{"fooocus_version.py": "version = '2.4.3'\n", "experiments_face.py": "import cv2\nimport extras.face_crop as cropper\n\n\nimg = cv2.imread('lena.png')\nresult = cropper.crop_image(img)\ncv2.imwrite('lena_result.png', result)\n", "launch.py": "import os\nimport ssl\nimport sys\n\nprint('[System ARGV] ' + str(sys.argv))\n\nroot = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(root)\nos.chdir(root)\n\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\nif \"GRADIO_SERVER_PORT\" not in os.environ:\n    os.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nimport platform\nimport fooocus_version\n\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met, delete_folder_content\nfrom modules.model_loader import load_file_from_url\n\nREINSTALL_ALL = False\nTRY_INSTALL_XFORMERS = False\n\n\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)\n\n    if TRY_INSTALL_XFORMERS:\n        if REINSTALL_ALL or not is_installed(\"xformers\"):\n            xformers_package = os.environ.get('XFORMERS_PACKAGE', 'xformers==0.0.23')\n            if platform.system() == \"Windows\":\n                if platform.python_version().startswith(\"3.10\"):\n                    run_pip(f\"install -U -I --no-deps {xformers_package}\", \"xformers\", live=True)\n                else:\n                    print(\"Installation of xformers is not supported in this version of Python.\")\n                    print(\n                        \"You can also check this and build manually: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers#building-xformers-on-windows-by-duckness\")\n                    if not is_installed(\"xformers\"):\n                        exit(0)\n            elif platform.system() == \"Linux\":\n                run_pip(f\"install -U -I --no-deps {xformers_package}\", \"xformers\")\n\n    if REINSTALL_ALL or not requirements_met(requirements_file):\n        run_pip(f\"install -r \\\"{requirements_file}\\\"\", \"requirements\")\n\n    return\n\n\nvae_approx_filenames = [\n    ('xlvaeapp.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth'),\n    ('vaeapp_sd15.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt'),\n    ('xl-to-v1_interposer-v4.0.safetensors',\n     'https://huggingface.co/mashb1t/misc/resolve/main/xl-to-v1_interposer-v4.0.safetensors')\n]\n\n\ndef ini_args():\n    from args_manager import args\n    return args\n\n\nprepare_environment()\nbuild_launcher()\nargs = ini_args()\n\nif args.gpu_device_id is not None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_device_id)\n    print(\"Set device to:\", args.gpu_device_id)\n\nif args.hf_mirror is not None : \n    os.environ['HF_MIRROR'] = str(args.hf_mirror)\n    print(\"Set hf_mirror to:\", args.hf_mirror)\n\nfrom modules import config\n\nos.environ['GRADIO_TEMP_DIR'] = config.temp_path\n\nif config.temp_path_cleanup_on_launch:\n    print(f'[Cleanup] Attempting to delete content of temp dir {config.temp_path}')\n    result = delete_folder_content(config.temp_path, '[Cleanup] ')\n    if result:\n        print(\"[Cleanup] Cleanup successful\")\n    else:\n        print(f\"[Cleanup] Failed to delete content of temp dir.\")\n\n\ndef download_models(default_model, previous_default_models, checkpoint_downloads, embeddings_downloads, lora_downloads):\n    for file_name, url in vae_approx_filenames:\n        load_file_from_url(url=url, model_dir=config.path_vae_approx, file_name=file_name)\n\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_expansion.bin',\n        model_dir=config.path_fooocus_expansion,\n        file_name='pytorch_model.bin'\n    )\n\n    if args.disable_preset_download:\n        print('Skipped model download.')\n        return default_model, checkpoint_downloads\n\n    if not args.always_download_new_model:\n        if not os.path.exists(os.path.join(config.paths_checkpoints[0], default_model)):\n            for alternative_model_name in previous_default_models:\n                if os.path.exists(os.path.join(config.paths_checkpoints[0], alternative_model_name)):\n                    print(f'You do not have [{default_model}] but you have [{alternative_model_name}].')\n                    print(f'Fooocus will use [{alternative_model_name}] to avoid downloading new models, '\n                          f'but you are not using the latest models.')\n                    print('Use --always-download-new-model to avoid fallback and always get new models.')\n                    checkpoint_downloads = {}\n                    default_model = alternative_model_name\n                    break\n\n    for file_name, url in checkpoint_downloads.items():\n        load_file_from_url(url=url, model_dir=config.paths_checkpoints[0], file_name=file_name)\n    for file_name, url in embeddings_downloads.items():\n        load_file_from_url(url=url, model_dir=config.path_embeddings, file_name=file_name)\n    for file_name, url in lora_downloads.items():\n        load_file_from_url(url=url, model_dir=config.paths_loras[0], file_name=file_name)\n\n    return default_model, checkpoint_downloads\n\n\nconfig.default_base_model_name, config.checkpoint_downloads = download_models(\n    config.default_base_model_name, config.previous_default_models, config.checkpoint_downloads,\n    config.embeddings_downloads, config.lora_downloads)\n\nfrom webui import *\n", "entry_with_update.py": "import os\nimport sys\n\n\nroot = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(root)\nos.chdir(root)\n\n\ntry:\n    import pygit2\n    pygit2.option(pygit2.GIT_OPT_SET_OWNER_VALIDATION, 0)\n\n    repo = pygit2.Repository(os.path.abspath(os.path.dirname(__file__)))\n\n    branch_name = repo.head.shorthand\n\n    remote_name = 'origin'\n    remote = repo.remotes[remote_name]\n\n    remote.fetch()\n\n    local_branch_ref = f'refs/heads/{branch_name}'\n    local_branch = repo.lookup_reference(local_branch_ref)\n\n    remote_reference = f'refs/remotes/{remote_name}/{branch_name}'\n    remote_commit = repo.revparse_single(remote_reference)\n\n    merge_result, _ = repo.merge_analysis(remote_commit.id)\n\n    if merge_result & pygit2.GIT_MERGE_ANALYSIS_UP_TO_DATE:\n        print(\"Already up-to-date\")\n    elif merge_result & pygit2.GIT_MERGE_ANALYSIS_FASTFORWARD:\n        local_branch.set_target(remote_commit.id)\n        repo.head.set_target(remote_commit.id)\n        repo.checkout_tree(repo.get(remote_commit.id))\n        repo.reset(local_branch.target, pygit2.GIT_RESET_HARD)\n        print(\"Fast-forward merge\")\n    elif merge_result & pygit2.GIT_MERGE_ANALYSIS_NORMAL:\n        print(\"Update failed - Did you modify any file?\")\nexcept Exception as e:\n    print('Update failed.')\n    print(str(e))\n\nprint('Update succeeded.')\nfrom launch import *\n", "build_launcher.py": "import os\n\nwin32_root = os.path.dirname(os.path.dirname(__file__))\npython_embeded_path = os.path.join(win32_root, 'python_embeded')\n\nis_win32_standalone_build = os.path.exists(python_embeded_path) and os.path.isdir(python_embeded_path)\n\nwin32_cmd = '''\n.\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\npause\n'''\n\n\ndef build_launcher():\n    if not is_win32_standalone_build:\n        return\n\n    presets = [None, 'anime', 'realistic']\n\n    for preset in presets:\n        win32_cmd_preset = win32_cmd.replace('{cmds}', '' if preset is None else f'--preset {preset}')\n        bat_path = os.path.join(win32_root, 'run.bat' if preset is None else f'run_{preset}.bat')\n        if not os.path.exists(bat_path):\n            with open(bat_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(win32_cmd_preset)\n    return\n", "experiments_interrogate.py": "import cv2\nfrom extras.interrogate import default_interrogator as default_interrogator_photo\nfrom extras.wd14tagger import default_interrogator as default_interrogator_anime\n\nimg = cv2.imread('./test_imgs/red_box.jpg')[:, :, ::-1].copy()\nprint(default_interrogator_photo(img))\nimg = cv2.imread('./test_imgs/miku.jpg')[:, :, ::-1].copy()\nprint(default_interrogator_anime(img))\n", "webui.py": "import gradio as gr\nimport random\nimport os\nimport json\nimport time\nimport shared\nimport modules.config\nimport fooocus_version\nimport modules.html\nimport modules.async_worker as worker\nimport modules.constants as constants\nimport modules.flags as flags\nimport modules.gradio_hijack as grh\nimport modules.style_sorter as style_sorter\nimport modules.meta_parser\nimport args_manager\nimport copy\nimport launch\n\nfrom modules.sdxl_styles import legal_style_names\nfrom modules.private_logger import get_current_html_path\nfrom modules.ui_gradio_extensions import reload_javascript\nfrom modules.auth import auth_enabled, check_auth\nfrom modules.util import is_json\n\ndef get_task(*args):\n    args = list(args)\n    args.pop(0)\n\n    return worker.AsyncTask(args=args)\n\ndef generate_clicked(task: worker.AsyncTask):\n    import ldm_patched.modules.model_management as model_management\n\n    with model_management.interrupt_processing_mutex:\n        model_management.interrupt_processing = False\n    # outputs=[progress_html, progress_window, progress_gallery, gallery]\n\n    if len(task.args) == 0:\n        return\n\n    execution_start_time = time.perf_counter()\n    finished = False\n\n    yield gr.update(visible=True, value=modules.html.make_progress_html(1, 'Waiting for task to start ...')), \\\n        gr.update(visible=True, value=None), \\\n        gr.update(visible=False, value=None), \\\n        gr.update(visible=False)\n\n    worker.async_tasks.append(task)\n\n    while not finished:\n        time.sleep(0.01)\n        if len(task.yields) > 0:\n            flag, product = task.yields.pop(0)\n            if flag == 'preview':\n\n                # help bad internet connection by skipping duplicated preview\n                if len(task.yields) > 0:  # if we have the next item\n                    if task.yields[0][0] == 'preview':   # if the next item is also a preview\n                        # print('Skipped one preview for better internet connection.')\n                        continue\n\n                percentage, title, image = product\n                yield gr.update(visible=True, value=modules.html.make_progress_html(percentage, title)), \\\n                    gr.update(visible=True, value=image) if image is not None else gr.update(), \\\n                    gr.update(), \\\n                    gr.update(visible=False)\n            if flag == 'results':\n                yield gr.update(visible=True), \\\n                    gr.update(visible=True), \\\n                    gr.update(visible=True, value=product), \\\n                    gr.update(visible=False)\n            if flag == 'finish':\n                yield gr.update(visible=False), \\\n                    gr.update(visible=False), \\\n                    gr.update(visible=False), \\\n                    gr.update(visible=True, value=product)\n                finished = True\n\n                # delete Fooocus temp images, only keep gradio temp images\n                if args_manager.args.disable_image_log:\n                    for filepath in product:\n                        if isinstance(filepath, str) and os.path.exists(filepath):\n                            os.remove(filepath)\n\n    execution_time = time.perf_counter() - execution_start_time\n    print(f'Total time: {execution_time:.2f} seconds')\n    return\n\n\nreload_javascript()\n\ntitle = f'Fooocus {fooocus_version.version}'\n\nif isinstance(args_manager.args.preset, str):\n    title += ' ' + args_manager.args.preset\n\nshared.gradio_root = gr.Blocks(title=title).queue()\n\nwith shared.gradio_root:\n    currentTask = gr.State(worker.AsyncTask(args=[]))\n    with gr.Row():\n        with gr.Column(scale=2):\n            with gr.Row():\n                progress_window = grh.Image(label='Preview', show_label=True, visible=False, height=768,\n                                            elem_classes=['main_view'])\n                progress_gallery = gr.Gallery(label='Finished Images', show_label=True, object_fit='contain',\n                                              height=768, visible=False, elem_classes=['main_view', 'image_gallery'])\n            progress_html = gr.HTML(value=modules.html.make_progress_html(32, 'Progress 32%'), visible=False,\n                                    elem_id='progress-bar', elem_classes='progress-bar')\n            gallery = gr.Gallery(label='Gallery', show_label=False, object_fit='contain', visible=True, height=768,\n                                 elem_classes=['resizable_area', 'main_view', 'final_gallery', 'image_gallery'],\n                                 elem_id='final_gallery')\n            with gr.Row():\n                with gr.Column(scale=17):\n                    prompt = gr.Textbox(show_label=False, placeholder=\"Type prompt here or paste parameters.\", elem_id='positive_prompt',\n                                        autofocus=True, lines=3)\n\n                    default_prompt = modules.config.default_prompt\n                    if isinstance(default_prompt, str) and default_prompt != '':\n                        shared.gradio_root.load(lambda: default_prompt, outputs=prompt)\n\n                with gr.Column(scale=3, min_width=0):\n                    generate_button = gr.Button(label=\"Generate\", value=\"Generate\", elem_classes='type_row', elem_id='generate_button', visible=True)\n                    reset_button = gr.Button(label=\"Reconnect\", value=\"Reconnect\", elem_classes='type_row', elem_id='reset_button', visible=False)\n                    load_parameter_button = gr.Button(label=\"Load Parameters\", value=\"Load Parameters\", elem_classes='type_row', elem_id='load_parameter_button', visible=False)\n                    skip_button = gr.Button(label=\"Skip\", value=\"Skip\", elem_classes='type_row_half', elem_id='skip_button', visible=False)\n                    stop_button = gr.Button(label=\"Stop\", value=\"Stop\", elem_classes='type_row_half', elem_id='stop_button', visible=False)\n\n                    def stop_clicked(currentTask):\n                        import ldm_patched.modules.model_management as model_management\n                        currentTask.last_stop = 'stop'\n                        if (currentTask.processing):\n                            model_management.interrupt_current_processing()\n                        return currentTask\n\n                    def skip_clicked(currentTask):\n                        import ldm_patched.modules.model_management as model_management\n                        currentTask.last_stop = 'skip'\n                        if (currentTask.processing):\n                            model_management.interrupt_current_processing()\n                        return currentTask\n\n                    stop_button.click(stop_clicked, inputs=currentTask, outputs=currentTask, queue=False, show_progress=False, _js='cancelGenerateForever')\n                    skip_button.click(skip_clicked, inputs=currentTask, outputs=currentTask, queue=False, show_progress=False)\n            with gr.Row(elem_classes='advanced_check_row'):\n                input_image_checkbox = gr.Checkbox(label='Input Image', value=False, container=False, elem_classes='min_check')\n                advanced_checkbox = gr.Checkbox(label='Advanced', value=modules.config.default_advanced_checkbox, container=False, elem_classes='min_check')\n            with gr.Row(visible=False) as image_input_panel:\n                with gr.Tabs():\n                    with gr.TabItem(label='Upscale or Variation') as uov_tab:\n                        with gr.Row():\n                            with gr.Column():\n                                uov_input_image = grh.Image(label='Image', source='upload', type='numpy', show_label=False)\n                            with gr.Column():\n                                uov_method = gr.Radio(label='Upscale or Variation:', choices=flags.uov_list, value=flags.disabled)\n                                gr.HTML('<a href=\"https://github.com/lllyasviel/Fooocus/discussions/390\" target=\"_blank\">\\U0001F4D4 Document</a>')\n                    with gr.TabItem(label='Image Prompt') as ip_tab:\n                        with gr.Row():\n                            ip_images = []\n                            ip_types = []\n                            ip_stops = []\n                            ip_weights = []\n                            ip_ctrls = []\n                            ip_ad_cols = []\n                            for _ in range(flags.controlnet_image_count):\n                                with gr.Column():\n                                    ip_image = grh.Image(label='Image', source='upload', type='numpy', show_label=False, height=300)\n                                    ip_images.append(ip_image)\n                                    ip_ctrls.append(ip_image)\n                                    with gr.Column(visible=False) as ad_col:\n                                        with gr.Row():\n                                            default_end, default_weight = flags.default_parameters[flags.default_ip]\n\n                                            ip_stop = gr.Slider(label='Stop At', minimum=0.0, maximum=1.0, step=0.001, value=default_end)\n                                            ip_stops.append(ip_stop)\n                                            ip_ctrls.append(ip_stop)\n\n                                            ip_weight = gr.Slider(label='Weight', minimum=0.0, maximum=2.0, step=0.001, value=default_weight)\n                                            ip_weights.append(ip_weight)\n                                            ip_ctrls.append(ip_weight)\n\n                                        ip_type = gr.Radio(label='Type', choices=flags.ip_list, value=flags.default_ip, container=False)\n                                        ip_types.append(ip_type)\n                                        ip_ctrls.append(ip_type)\n\n                                        ip_type.change(lambda x: flags.default_parameters[x], inputs=[ip_type], outputs=[ip_stop, ip_weight], queue=False, show_progress=False)\n                                    ip_ad_cols.append(ad_col)\n                        ip_advanced = gr.Checkbox(label='Advanced', value=False, container=False)\n                        gr.HTML('* \\\"Image Prompt\\\" is powered by Fooocus Image Mixture Engine (v1.0.1). <a href=\"https://github.com/lllyasviel/Fooocus/discussions/557\" target=\"_blank\">\\U0001F4D4 Document</a>')\n\n                        def ip_advance_checked(x):\n                            return [gr.update(visible=x)] * len(ip_ad_cols) + \\\n                                [flags.default_ip] * len(ip_types) + \\\n                                [flags.default_parameters[flags.default_ip][0]] * len(ip_stops) + \\\n                                [flags.default_parameters[flags.default_ip][1]] * len(ip_weights)\n\n                        ip_advanced.change(ip_advance_checked, inputs=ip_advanced,\n                                           outputs=ip_ad_cols + ip_types + ip_stops + ip_weights,\n                                           queue=False, show_progress=False)\n                    with gr.TabItem(label='Inpaint or Outpaint') as inpaint_tab:\n                        with gr.Row():\n                            inpaint_input_image = grh.Image(label='Image', source='upload', type='numpy', tool='sketch', height=500, brush_color=\"#FFFFFF\", elem_id='inpaint_canvas', show_label=False)\n                            inpaint_mask_image = grh.Image(label='Mask Upload', source='upload', type='numpy', height=500, visible=False)\n\n                        with gr.Row():\n                            inpaint_additional_prompt = gr.Textbox(placeholder=\"Describe what you want to inpaint.\", elem_id='inpaint_additional_prompt', label='Inpaint Additional Prompt', visible=False)\n                            outpaint_selections = gr.CheckboxGroup(choices=['Left', 'Right', 'Top', 'Bottom'], value=[], label='Outpaint Direction')\n                            inpaint_mode = gr.Dropdown(choices=modules.flags.inpaint_options, value=modules.flags.inpaint_option_default, label='Method')\n                        example_inpaint_prompts = gr.Dataset(samples=modules.config.example_inpaint_prompts, label='Additional Prompt Quick List', components=[inpaint_additional_prompt], visible=False)\n                        gr.HTML('* Powered by Fooocus Inpaint Engine <a href=\"https://github.com/lllyasviel/Fooocus/discussions/414\" target=\"_blank\">\\U0001F4D4 Document</a>')\n                        example_inpaint_prompts.click(lambda x: x[0], inputs=example_inpaint_prompts, outputs=inpaint_additional_prompt, show_progress=False, queue=False)\n                    with gr.TabItem(label='Describe') as desc_tab:\n                        with gr.Row():\n                            with gr.Column():\n                                desc_input_image = grh.Image(label='Image', source='upload', type='numpy', show_label=False)\n                            with gr.Column():\n                                desc_method = gr.Radio(\n                                    label='Content Type',\n                                    choices=[flags.desc_type_photo, flags.desc_type_anime],\n                                    value=flags.desc_type_photo)\n                                desc_btn = gr.Button(value='Describe this Image into Prompt')\n                                desc_image_size = gr.Textbox(label='Image Size and Recommended Size', elem_id='desc_image_size', visible=False)\n                                gr.HTML('<a href=\"https://github.com/lllyasviel/Fooocus/discussions/1363\" target=\"_blank\">\\U0001F4D4 Document</a>')\n\n                                def trigger_show_image_properties(image):\n                                    value = modules.util.get_image_size_info(image, modules.flags.sdxl_aspect_ratios)\n                                    return gr.update(value=value, visible=True)\n\n                                desc_input_image.upload(trigger_show_image_properties, inputs=desc_input_image,\n                                                        outputs=desc_image_size, show_progress=False, queue=False)\n\n                    with gr.TabItem(label='Metadata') as metadata_tab:\n                        with gr.Column():\n                            metadata_input_image = grh.Image(label='For images created by Fooocus', source='upload', type='filepath')\n                            metadata_json = gr.JSON(label='Metadata')\n                            metadata_import_button = gr.Button(value='Apply Metadata')\n\n                        def trigger_metadata_preview(filepath):\n                            parameters, metadata_scheme = modules.meta_parser.read_info_from_image(filepath)\n\n                            results = {}\n                            if parameters is not None:\n                                results['parameters'] = parameters\n\n                            if isinstance(metadata_scheme, flags.MetadataScheme):\n                                results['metadata_scheme'] = metadata_scheme.value\n\n                            return results\n\n                        metadata_input_image.upload(trigger_metadata_preview, inputs=metadata_input_image,\n                                                    outputs=metadata_json, queue=False, show_progress=True)\n\n            switch_js = \"(x) => {if(x){viewer_to_bottom(100);viewer_to_bottom(500);}else{viewer_to_top();} return x;}\"\n            down_js = \"() => {viewer_to_bottom();}\"\n\n            input_image_checkbox.change(lambda x: gr.update(visible=x), inputs=input_image_checkbox,\n                                        outputs=image_input_panel, queue=False, show_progress=False, _js=switch_js)\n            ip_advanced.change(lambda: None, queue=False, show_progress=False, _js=down_js)\n\n            current_tab = gr.Textbox(value='uov', visible=False)\n            uov_tab.select(lambda: 'uov', outputs=current_tab, queue=False, _js=down_js, show_progress=False)\n            inpaint_tab.select(lambda: 'inpaint', outputs=current_tab, queue=False, _js=down_js, show_progress=False)\n            ip_tab.select(lambda: 'ip', outputs=current_tab, queue=False, _js=down_js, show_progress=False)\n            desc_tab.select(lambda: 'desc', outputs=current_tab, queue=False, _js=down_js, show_progress=False)\n            metadata_tab.select(lambda: 'metadata', outputs=current_tab, queue=False, _js=down_js, show_progress=False)\n\n        with gr.Column(scale=1, visible=modules.config.default_advanced_checkbox) as advanced_column:\n            with gr.Tab(label='Setting'):\n                if not args_manager.args.disable_preset_selection:\n                    preset_selection = gr.Dropdown(label='Preset',\n                                                   choices=modules.config.available_presets,\n                                                   value=args_manager.args.preset if args_manager.args.preset else \"initial\",\n                                                   interactive=True)\n                performance_selection = gr.Radio(label='Performance',\n                                                 choices=flags.Performance.list(),\n                                                 value=modules.config.default_performance,\n                                                 elem_classes=['performance_selection'])\n                with gr.Accordion(label='Aspect Ratios', open=False, elem_id='aspect_ratios_accordion') as aspect_ratios_accordion:\n                    aspect_ratios_selection = gr.Radio(label='Aspect Ratios', show_label=False,\n                                                       choices=modules.config.available_aspect_ratios_labels,\n                                                       value=modules.config.default_aspect_ratio,\n                                                       info='width \u00d7 height',\n                                                       elem_classes='aspect_ratios')\n\n                    aspect_ratios_selection.change(lambda x: None, inputs=aspect_ratios_selection, queue=False, show_progress=False, _js='(x)=>{refresh_aspect_ratios_label(x);}')\n                    shared.gradio_root.load(lambda x: None, inputs=aspect_ratios_selection, queue=False, show_progress=False, _js='(x)=>{refresh_aspect_ratios_label(x);}')\n\n                image_number = gr.Slider(label='Image Number', minimum=1, maximum=modules.config.default_max_image_number, step=1, value=modules.config.default_image_number)\n\n                output_format = gr.Radio(label='Output Format',\n                                         choices=flags.OutputFormat.list(),\n                                         value=modules.config.default_output_format)\n\n                negative_prompt = gr.Textbox(label='Negative Prompt', show_label=True, placeholder=\"Type prompt here.\",\n                                             info='Describing what you do not want to see.', lines=2,\n                                             elem_id='negative_prompt',\n                                             value=modules.config.default_prompt_negative)\n                seed_random = gr.Checkbox(label='Random', value=True)\n                image_seed = gr.Textbox(label='Seed', value=0, max_lines=1, visible=False) # workaround for https://github.com/gradio-app/gradio/issues/5354\n\n                def random_checked(r):\n                    return gr.update(visible=not r)\n\n                def refresh_seed(r, seed_string):\n                    if r:\n                        return random.randint(constants.MIN_SEED, constants.MAX_SEED)\n                    else:\n                        try:\n                            seed_value = int(seed_string)\n                            if constants.MIN_SEED <= seed_value <= constants.MAX_SEED:\n                                return seed_value\n                        except ValueError:\n                            pass\n                        return random.randint(constants.MIN_SEED, constants.MAX_SEED)\n\n                seed_random.change(random_checked, inputs=[seed_random], outputs=[image_seed],\n                                   queue=False, show_progress=False)\n\n                def update_history_link():\n                    if args_manager.args.disable_image_log:\n                        return gr.update(value='')\n                    \n                    return gr.update(value=f'<a href=\"file={get_current_html_path(output_format)}\" target=\"_blank\">\\U0001F4DA History Log</a>')\n\n                history_link = gr.HTML()\n                shared.gradio_root.load(update_history_link, outputs=history_link, queue=False, show_progress=False)\n\n            with gr.Tab(label='Style', elem_classes=['style_selections_tab']):\n                style_sorter.try_load_sorted_styles(\n                    style_names=legal_style_names,\n                    default_selected=modules.config.default_styles)\n\n                style_search_bar = gr.Textbox(show_label=False, container=False,\n                                              placeholder=\"\\U0001F50E Type here to search styles ...\",\n                                              value=\"\",\n                                              label='Search Styles')\n                style_selections = gr.CheckboxGroup(show_label=False, container=False,\n                                                    choices=copy.deepcopy(style_sorter.all_styles),\n                                                    value=copy.deepcopy(modules.config.default_styles),\n                                                    label='Selected Styles',\n                                                    elem_classes=['style_selections'])\n                gradio_receiver_style_selections = gr.Textbox(elem_id='gradio_receiver_style_selections', visible=False)\n\n                shared.gradio_root.load(lambda: gr.update(choices=copy.deepcopy(style_sorter.all_styles)),\n                                        outputs=style_selections)\n\n                style_search_bar.change(style_sorter.search_styles,\n                                        inputs=[style_selections, style_search_bar],\n                                        outputs=style_selections,\n                                        queue=False,\n                                        show_progress=False).then(\n                    lambda: None, _js='()=>{refresh_style_localization();}')\n\n                gradio_receiver_style_selections.input(style_sorter.sort_styles,\n                                                       inputs=style_selections,\n                                                       outputs=style_selections,\n                                                       queue=False,\n                                                       show_progress=False).then(\n                    lambda: None, _js='()=>{refresh_style_localization();}')\n\n            with gr.Tab(label='Model'):\n                with gr.Group():\n                    with gr.Row():\n                        base_model = gr.Dropdown(label='Base Model (SDXL only)', choices=modules.config.model_filenames, value=modules.config.default_base_model_name, show_label=True)\n                        refiner_model = gr.Dropdown(label='Refiner (SDXL or SD 1.5)', choices=['None'] + modules.config.model_filenames, value=modules.config.default_refiner_model_name, show_label=True)\n\n                    refiner_switch = gr.Slider(label='Refiner Switch At', minimum=0.1, maximum=1.0, step=0.0001,\n                                               info='Use 0.4 for SD1.5 realistic models; '\n                                                    'or 0.667 for SD1.5 anime models; '\n                                                    'or 0.8 for XL-refiners; '\n                                                    'or any value for switching two SDXL models.',\n                                               value=modules.config.default_refiner_switch,\n                                               visible=modules.config.default_refiner_model_name != 'None')\n\n                    refiner_model.change(lambda x: gr.update(visible=x != 'None'),\n                                         inputs=refiner_model, outputs=refiner_switch, show_progress=False, queue=False)\n\n                with gr.Group():\n                    lora_ctrls = []\n\n                    for i, (enabled, filename, weight) in enumerate(modules.config.default_loras):\n                        with gr.Row():\n                            lora_enabled = gr.Checkbox(label='Enable', value=enabled,\n                                                       elem_classes=['lora_enable', 'min_check'], scale=1)\n                            lora_model = gr.Dropdown(label=f'LoRA {i + 1}',\n                                                     choices=['None'] + modules.config.lora_filenames, value=filename,\n                                                     elem_classes='lora_model', scale=5)\n                            lora_weight = gr.Slider(label='Weight', minimum=modules.config.default_loras_min_weight,\n                                                    maximum=modules.config.default_loras_max_weight, step=0.01, value=weight,\n                                                    elem_classes='lora_weight', scale=5)\n                            lora_ctrls += [lora_enabled, lora_model, lora_weight]\n\n                with gr.Row():\n                    refresh_files = gr.Button(label='Refresh', value='\\U0001f504 Refresh All Files', variant='secondary', elem_classes='refresh_button')\n            with gr.Tab(label='Advanced'):\n                guidance_scale = gr.Slider(label='Guidance Scale', minimum=1.0, maximum=30.0, step=0.01,\n                                           value=modules.config.default_cfg_scale,\n                                           info='Higher value means style is cleaner, vivider, and more artistic.')\n                sharpness = gr.Slider(label='Image Sharpness', minimum=0.0, maximum=30.0, step=0.001,\n                                      value=modules.config.default_sample_sharpness,\n                                      info='Higher value means image and texture are sharper.')\n                gr.HTML('<a href=\"https://github.com/lllyasviel/Fooocus/discussions/117\" target=\"_blank\">\\U0001F4D4 Document</a>')\n                dev_mode = gr.Checkbox(label='Developer Debug Mode', value=False, container=False)\n\n                with gr.Column(visible=False) as dev_tools:\n                    with gr.Tab(label='Debug Tools'):\n                        adm_scaler_positive = gr.Slider(label='Positive ADM Guidance Scaler', minimum=0.1, maximum=3.0,\n                                                        step=0.001, value=1.5, info='The scaler multiplied to positive ADM (use 1.0 to disable). ')\n                        adm_scaler_negative = gr.Slider(label='Negative ADM Guidance Scaler', minimum=0.1, maximum=3.0,\n                                                        step=0.001, value=0.8, info='The scaler multiplied to negative ADM (use 1.0 to disable). ')\n                        adm_scaler_end = gr.Slider(label='ADM Guidance End At Step', minimum=0.0, maximum=1.0,\n                                                   step=0.001, value=0.3,\n                                                   info='When to end the guidance from positive/negative ADM. ')\n\n                        refiner_swap_method = gr.Dropdown(label='Refiner swap method', value=flags.refiner_swap_method,\n                                                          choices=['joint', 'separate', 'vae'])\n\n                        adaptive_cfg = gr.Slider(label='CFG Mimicking from TSNR', minimum=1.0, maximum=30.0, step=0.01,\n                                                 value=modules.config.default_cfg_tsnr,\n                                                 info='Enabling Fooocus\\'s implementation of CFG mimicking for TSNR '\n                                                      '(effective when real CFG > mimicked CFG).')\n                        clip_skip = gr.Slider(label='CLIP Skip', minimum=1, maximum=flags.clip_skip_max, step=1,\n                                                 value=modules.config.default_clip_skip,\n                                                 info='Bypass CLIP layers to avoid overfitting (use 1 to not skip any layers, 2 is recommended).')\n                        sampler_name = gr.Dropdown(label='Sampler', choices=flags.sampler_list,\n                                                   value=modules.config.default_sampler)\n                        scheduler_name = gr.Dropdown(label='Scheduler', choices=flags.scheduler_list,\n                                                     value=modules.config.default_scheduler)\n                        vae_name = gr.Dropdown(label='VAE', choices=[modules.flags.default_vae] + modules.config.vae_filenames,\n                                                     value=modules.config.default_vae, show_label=True)\n\n                        generate_image_grid = gr.Checkbox(label='Generate Image Grid for Each Batch',\n                                                          info='(Experimental) This may cause performance problems on some computers and certain internet conditions.',\n                                                          value=False)\n\n                        overwrite_step = gr.Slider(label='Forced Overwrite of Sampling Step',\n                                                   minimum=-1, maximum=200, step=1,\n                                                   value=modules.config.default_overwrite_step,\n                                                   info='Set as -1 to disable. For developer debugging.')\n                        overwrite_switch = gr.Slider(label='Forced Overwrite of Refiner Switch Step',\n                                                     minimum=-1, maximum=200, step=1,\n                                                     value=modules.config.default_overwrite_switch,\n                                                     info='Set as -1 to disable. For developer debugging.')\n                        overwrite_width = gr.Slider(label='Forced Overwrite of Generating Width',\n                                                    minimum=-1, maximum=2048, step=1, value=-1,\n                                                    info='Set as -1 to disable. For developer debugging. '\n                                                         'Results will be worse for non-standard numbers that SDXL is not trained on.')\n                        overwrite_height = gr.Slider(label='Forced Overwrite of Generating Height',\n                                                     minimum=-1, maximum=2048, step=1, value=-1,\n                                                     info='Set as -1 to disable. For developer debugging. '\n                                                          'Results will be worse for non-standard numbers that SDXL is not trained on.')\n                        overwrite_vary_strength = gr.Slider(label='Forced Overwrite of Denoising Strength of \"Vary\"',\n                                                            minimum=-1, maximum=1.0, step=0.001, value=-1,\n                                                            info='Set as negative number to disable. For developer debugging.')\n                        overwrite_upscale_strength = gr.Slider(label='Forced Overwrite of Denoising Strength of \"Upscale\"',\n                                                               minimum=-1, maximum=1.0, step=0.001, value=-1,\n                                                               info='Set as negative number to disable. For developer debugging.')\n                        disable_preview = gr.Checkbox(label='Disable Preview', value=modules.config.default_black_out_nsfw,\n                                                      interactive=not modules.config.default_black_out_nsfw,\n                                                      info='Disable preview during generation.')\n                        disable_intermediate_results = gr.Checkbox(label='Disable Intermediate Results',\n                                                      value=flags.Performance.has_restricted_features(modules.config.default_performance),\n                                                      info='Disable intermediate results during generation, only show final gallery.')\n                        disable_seed_increment = gr.Checkbox(label='Disable seed increment',\n                                                             info='Disable automatic seed increment when image number is > 1.',\n                                                             value=False)\n                        read_wildcards_in_order = gr.Checkbox(label=\"Read wildcards in order\", value=False)\n\n                        black_out_nsfw = gr.Checkbox(label='Black Out NSFW',\n                                                     value=modules.config.default_black_out_nsfw,\n                                                     interactive=not modules.config.default_black_out_nsfw,\n                                                     info='Use black image if NSFW is detected.')\n\n                        black_out_nsfw.change(lambda x: gr.update(value=x, interactive=not x),\n                                              inputs=black_out_nsfw, outputs=disable_preview, queue=False,\n                                              show_progress=False)\n\n                        if not args_manager.args.disable_metadata:\n                            save_metadata_to_images = gr.Checkbox(label='Save Metadata to Images', value=modules.config.default_save_metadata_to_images,\n                                                                  info='Adds parameters to generated images allowing manual regeneration.')\n                            metadata_scheme = gr.Radio(label='Metadata Scheme', choices=flags.metadata_scheme, value=modules.config.default_metadata_scheme,\n                                                       info='Image Prompt parameters are not included. Use png and a1111 for compatibility with Civitai.',\n                                                       visible=modules.config.default_save_metadata_to_images)\n\n                            save_metadata_to_images.change(lambda x: gr.update(visible=x), inputs=[save_metadata_to_images], outputs=[metadata_scheme], \n                                                           queue=False, show_progress=False)\n\n                    with gr.Tab(label='Control'):\n                        debugging_cn_preprocessor = gr.Checkbox(label='Debug Preprocessors', value=False,\n                                                                info='See the results from preprocessors.')\n                        skipping_cn_preprocessor = gr.Checkbox(label='Skip Preprocessors', value=False,\n                                                               info='Do not preprocess images. (Inputs are already canny/depth/cropped-face/etc.)')\n\n                        mixing_image_prompt_and_vary_upscale = gr.Checkbox(label='Mixing Image Prompt and Vary/Upscale',\n                                                                           value=False)\n                        mixing_image_prompt_and_inpaint = gr.Checkbox(label='Mixing Image Prompt and Inpaint',\n                                                                      value=False)\n\n                        controlnet_softness = gr.Slider(label='Softness of ControlNet', minimum=0.0, maximum=1.0,\n                                                        step=0.001, value=0.25,\n                                                        info='Similar to the Control Mode in A1111 (use 0.0 to disable). ')\n\n                        with gr.Tab(label='Canny'):\n                            canny_low_threshold = gr.Slider(label='Canny Low Threshold', minimum=1, maximum=255,\n                                                            step=1, value=64)\n                            canny_high_threshold = gr.Slider(label='Canny High Threshold', minimum=1, maximum=255,\n                                                             step=1, value=128)\n\n                    with gr.Tab(label='Inpaint'):\n                        debugging_inpaint_preprocessor = gr.Checkbox(label='Debug Inpaint Preprocessing', value=False)\n                        inpaint_disable_initial_latent = gr.Checkbox(label='Disable initial latent in inpaint', value=False)\n                        inpaint_engine = gr.Dropdown(label='Inpaint Engine',\n                                                     value=modules.config.default_inpaint_engine_version,\n                                                     choices=flags.inpaint_engine_versions,\n                                                     info='Version of Fooocus inpaint model')\n                        inpaint_strength = gr.Slider(label='Inpaint Denoising Strength',\n                                                     minimum=0.0, maximum=1.0, step=0.001, value=1.0,\n                                                     info='Same as the denoising strength in A1111 inpaint. '\n                                                          'Only used in inpaint, not used in outpaint. '\n                                                          '(Outpaint always use 1.0)')\n                        inpaint_respective_field = gr.Slider(label='Inpaint Respective Field',\n                                                             minimum=0.0, maximum=1.0, step=0.001, value=0.618,\n                                                             info='The area to inpaint. '\n                                                                  'Value 0 is same as \"Only Masked\" in A1111. '\n                                                                  'Value 1 is same as \"Whole Image\" in A1111. '\n                                                                  'Only used in inpaint, not used in outpaint. '\n                                                                  '(Outpaint always use 1.0)')\n                        inpaint_erode_or_dilate = gr.Slider(label='Mask Erode or Dilate',\n                                                            minimum=-64, maximum=64, step=1, value=0,\n                                                            info='Positive value will make white area in the mask larger, '\n                                                                 'negative value will make white area smaller.'\n                                                                 '(default is 0, always process before any mask invert)')\n                        inpaint_mask_upload_checkbox = gr.Checkbox(label='Enable Mask Upload', value=False)\n                        invert_mask_checkbox = gr.Checkbox(label='Invert Mask', value=False)\n\n                        inpaint_mask_color = gr.ColorPicker(label='Inpaint brush color', value='#FFFFFF', elem_id='inpaint_brush_color')\n\n                        inpaint_ctrls = [debugging_inpaint_preprocessor, inpaint_disable_initial_latent, inpaint_engine,\n                                         inpaint_strength, inpaint_respective_field,\n                                         inpaint_mask_upload_checkbox, invert_mask_checkbox, inpaint_erode_or_dilate]\n\n                        inpaint_mask_upload_checkbox.change(lambda x: gr.update(visible=x),\n                                                            inputs=inpaint_mask_upload_checkbox,\n                                                            outputs=inpaint_mask_image, queue=False,\n                                                            show_progress=False)\n\n                        inpaint_mask_color.change(lambda x: gr.update(brush_color=x), inputs=inpaint_mask_color,\n                                                  outputs=inpaint_input_image,\n                                                  queue=False, show_progress=False)\n\n                    with gr.Tab(label='FreeU'):\n                        freeu_enabled = gr.Checkbox(label='Enabled', value=False)\n                        freeu_b1 = gr.Slider(label='B1', minimum=0, maximum=2, step=0.01, value=1.01)\n                        freeu_b2 = gr.Slider(label='B2', minimum=0, maximum=2, step=0.01, value=1.02)\n                        freeu_s1 = gr.Slider(label='S1', minimum=0, maximum=4, step=0.01, value=0.99)\n                        freeu_s2 = gr.Slider(label='S2', minimum=0, maximum=4, step=0.01, value=0.95)\n                        freeu_ctrls = [freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2]\n\n                def dev_mode_checked(r):\n                    return gr.update(visible=r)\n\n                dev_mode.change(dev_mode_checked, inputs=[dev_mode], outputs=[dev_tools],\n                                queue=False, show_progress=False)\n\n                def refresh_files_clicked():\n                    modules.config.update_files()\n                    results = [gr.update(choices=modules.config.model_filenames)]\n                    results += [gr.update(choices=['None'] + modules.config.model_filenames)]\n                    results += [gr.update(choices=[flags.default_vae] + modules.config.vae_filenames)]\n                    if not args_manager.args.disable_preset_selection:\n                        results += [gr.update(choices=modules.config.available_presets)]\n                    for i in range(modules.config.default_max_lora_number):\n                        results += [gr.update(interactive=True),\n                                    gr.update(choices=['None'] + modules.config.lora_filenames), gr.update()]\n                    return results\n\n                refresh_files_output = [base_model, refiner_model, vae_name]\n                if not args_manager.args.disable_preset_selection:\n                    refresh_files_output += [preset_selection]\n                refresh_files.click(refresh_files_clicked, [], refresh_files_output + lora_ctrls,\n                                    queue=False, show_progress=False)\n\n        state_is_generating = gr.State(False)\n\n        load_data_outputs = [advanced_checkbox, image_number, prompt, negative_prompt, style_selections,\n                             performance_selection, overwrite_step, overwrite_switch, aspect_ratios_selection,\n                             overwrite_width, overwrite_height, guidance_scale, sharpness, adm_scaler_positive,\n                             adm_scaler_negative, adm_scaler_end, refiner_swap_method, adaptive_cfg, clip_skip,\n                             base_model, refiner_model, refiner_switch, sampler_name, scheduler_name, vae_name,\n                             seed_random, image_seed, generate_button, load_parameter_button] + freeu_ctrls + lora_ctrls\n\n        if not args_manager.args.disable_preset_selection:\n            def preset_selection_change(preset, is_generating):\n                preset_content = modules.config.try_get_preset_content(preset) if preset != 'initial' else {}\n                preset_prepared = modules.meta_parser.parse_meta_from_preset(preset_content)\n\n                default_model = preset_prepared.get('base_model')\n                previous_default_models = preset_prepared.get('previous_default_models', [])\n                checkpoint_downloads = preset_prepared.get('checkpoint_downloads', {})\n                embeddings_downloads = preset_prepared.get('embeddings_downloads', {})\n                lora_downloads = preset_prepared.get('lora_downloads', {})\n\n                preset_prepared['base_model'], preset_prepared['lora_downloads'] = launch.download_models(\n                    default_model, previous_default_models, checkpoint_downloads, embeddings_downloads, lora_downloads)\n\n                if 'prompt' in preset_prepared and preset_prepared.get('prompt') == '':\n                    del preset_prepared['prompt']\n\n                return modules.meta_parser.load_parameter_button_click(json.dumps(preset_prepared), is_generating)\n\n            preset_selection.change(preset_selection_change, inputs=[preset_selection, state_is_generating], outputs=load_data_outputs, queue=False, show_progress=True) \\\n                .then(fn=style_sorter.sort_styles, inputs=style_selections, outputs=style_selections, queue=False, show_progress=False)\n\n        performance_selection.change(lambda x: [gr.update(interactive=not flags.Performance.has_restricted_features(x))] * 11 +\n                                               [gr.update(visible=not flags.Performance.has_restricted_features(x))] * 1 +\n                                               [gr.update(value=flags.Performance.has_restricted_features(x))] * 1,\n                                     inputs=performance_selection,\n                                     outputs=[\n                                         guidance_scale, sharpness, adm_scaler_end, adm_scaler_positive,\n                                         adm_scaler_negative, refiner_switch, refiner_model, sampler_name,\n                                         scheduler_name, adaptive_cfg, refiner_swap_method, negative_prompt, disable_intermediate_results\n                                     ], queue=False, show_progress=False)\n        \n        output_format.input(lambda x: gr.update(output_format=x), inputs=output_format)\n        \n        advanced_checkbox.change(lambda x: gr.update(visible=x), advanced_checkbox, advanced_column,\n                                 queue=False, show_progress=False) \\\n            .then(fn=lambda: None, _js='refresh_grid_delayed', queue=False, show_progress=False)\n\n        def inpaint_mode_change(mode):\n            assert mode in modules.flags.inpaint_options\n\n            # inpaint_additional_prompt, outpaint_selections, example_inpaint_prompts,\n            # inpaint_disable_initial_latent, inpaint_engine,\n            # inpaint_strength, inpaint_respective_field\n\n            if mode == modules.flags.inpaint_option_detail:\n                return [\n                    gr.update(visible=True), gr.update(visible=False, value=[]),\n                    gr.Dataset.update(visible=True, samples=modules.config.example_inpaint_prompts),\n                    False, 'None', 0.5, 0.0\n                ]\n\n            if mode == modules.flags.inpaint_option_modify:\n                return [\n                    gr.update(visible=True), gr.update(visible=False, value=[]),\n                    gr.Dataset.update(visible=False, samples=modules.config.example_inpaint_prompts),\n                    True, modules.config.default_inpaint_engine_version, 1.0, 0.0\n                ]\n\n            return [\n                gr.update(visible=False, value=''), gr.update(visible=True),\n                gr.Dataset.update(visible=False, samples=modules.config.example_inpaint_prompts),\n                False, modules.config.default_inpaint_engine_version, 1.0, 0.618\n            ]\n\n        inpaint_mode.input(inpaint_mode_change, inputs=inpaint_mode, outputs=[\n            inpaint_additional_prompt, outpaint_selections, example_inpaint_prompts,\n            inpaint_disable_initial_latent, inpaint_engine,\n            inpaint_strength, inpaint_respective_field\n        ], show_progress=False, queue=False)\n\n        ctrls = [currentTask, generate_image_grid]\n        ctrls += [\n            prompt, negative_prompt, style_selections,\n            performance_selection, aspect_ratios_selection, image_number, output_format, image_seed,\n            read_wildcards_in_order, sharpness, guidance_scale\n        ]\n\n        ctrls += [base_model, refiner_model, refiner_switch] + lora_ctrls\n        ctrls += [input_image_checkbox, current_tab]\n        ctrls += [uov_method, uov_input_image]\n        ctrls += [outpaint_selections, inpaint_input_image, inpaint_additional_prompt, inpaint_mask_image]\n        ctrls += [disable_preview, disable_intermediate_results, disable_seed_increment, black_out_nsfw]\n        ctrls += [adm_scaler_positive, adm_scaler_negative, adm_scaler_end, adaptive_cfg, clip_skip]\n        ctrls += [sampler_name, scheduler_name, vae_name]\n        ctrls += [overwrite_step, overwrite_switch, overwrite_width, overwrite_height, overwrite_vary_strength]\n        ctrls += [overwrite_upscale_strength, mixing_image_prompt_and_vary_upscale, mixing_image_prompt_and_inpaint]\n        ctrls += [debugging_cn_preprocessor, skipping_cn_preprocessor, canny_low_threshold, canny_high_threshold]\n        ctrls += [refiner_swap_method, controlnet_softness]\n        ctrls += freeu_ctrls\n        ctrls += inpaint_ctrls\n\n        if not args_manager.args.disable_metadata:\n            ctrls += [save_metadata_to_images, metadata_scheme]\n\n        ctrls += ip_ctrls\n\n        def parse_meta(raw_prompt_txt, is_generating):\n            loaded_json = None\n            if is_json(raw_prompt_txt):\n                loaded_json = json.loads(raw_prompt_txt)\n\n            if loaded_json is None:\n                if is_generating:\n                    return gr.update(), gr.update(), gr.update()\n                else:\n                    return gr.update(), gr.update(visible=True), gr.update(visible=False)\n\n            return json.dumps(loaded_json), gr.update(visible=False), gr.update(visible=True)\n\n        prompt.input(parse_meta, inputs=[prompt, state_is_generating], outputs=[prompt, generate_button, load_parameter_button], queue=False, show_progress=False)\n\n        load_parameter_button.click(modules.meta_parser.load_parameter_button_click, inputs=[prompt, state_is_generating], outputs=load_data_outputs, queue=False, show_progress=False)\n\n        def trigger_metadata_import(filepath, state_is_generating):\n            parameters, metadata_scheme = modules.meta_parser.read_info_from_image(filepath)\n            if parameters is None:\n                print('Could not find metadata in the image!')\n                parsed_parameters = {}\n            else:\n                metadata_parser = modules.meta_parser.get_metadata_parser(metadata_scheme)\n                parsed_parameters = metadata_parser.to_json(parameters)\n\n            return modules.meta_parser.load_parameter_button_click(parsed_parameters, state_is_generating)\n\n        metadata_import_button.click(trigger_metadata_import, inputs=[metadata_input_image, state_is_generating], outputs=load_data_outputs, queue=False, show_progress=True) \\\n            .then(style_sorter.sort_styles, inputs=style_selections, outputs=style_selections, queue=False, show_progress=False)\n\n        generate_button.click(lambda: (gr.update(visible=True, interactive=True), gr.update(visible=True, interactive=True), gr.update(visible=False, interactive=False), [], True),\n                              outputs=[stop_button, skip_button, generate_button, gallery, state_is_generating]) \\\n            .then(fn=refresh_seed, inputs=[seed_random, image_seed], outputs=image_seed) \\\n            .then(fn=get_task, inputs=ctrls, outputs=currentTask) \\\n            .then(fn=generate_clicked, inputs=currentTask, outputs=[progress_html, progress_window, progress_gallery, gallery]) \\\n            .then(lambda: (gr.update(visible=True, interactive=True), gr.update(visible=False, interactive=False), gr.update(visible=False, interactive=False), False),\n                  outputs=[generate_button, stop_button, skip_button, state_is_generating]) \\\n            .then(fn=update_history_link, outputs=history_link) \\\n            .then(fn=lambda: None, _js='playNotification').then(fn=lambda: None, _js='refresh_grid_delayed')\n\n        reset_button.click(lambda: [worker.AsyncTask(args=[]), False, gr.update(visible=True, interactive=True)] +\n                                   [gr.update(visible=False)] * 6 +\n                                   [gr.update(visible=True, value=[])],\n                           outputs=[currentTask, state_is_generating, generate_button,\n                                    reset_button, stop_button, skip_button,\n                                    progress_html, progress_window, progress_gallery, gallery],\n                           queue=False)\n\n        for notification_file in ['notification.ogg', 'notification.mp3']:\n            if os.path.exists(notification_file):\n                gr.Audio(interactive=False, value=notification_file, elem_id='audio_notification', visible=False)\n                break\n\n        def trigger_describe(mode, img):\n            if mode == flags.desc_type_photo:\n                from extras.interrogate import default_interrogator as default_interrogator_photo\n                return default_interrogator_photo(img), [\"Fooocus V2\", \"Fooocus Enhance\", \"Fooocus Sharp\"]\n            if mode == flags.desc_type_anime:\n                from extras.wd14tagger import default_interrogator as default_interrogator_anime\n                return default_interrogator_anime(img), [\"Fooocus V2\", \"Fooocus Masterpiece\"]\n            return mode, [\"Fooocus V2\"]\n\n        desc_btn.click(trigger_describe, inputs=[desc_method, desc_input_image],\n                       outputs=[prompt, style_selections], show_progress=True, queue=True)\n\n        if args_manager.args.enable_describe_uov_image:\n            def trigger_uov_describe(mode, img, prompt):\n                # keep prompt if not empty\n                if prompt == '':\n                    return trigger_describe(mode, img)\n                return gr.update(), gr.update()\n\n            uov_input_image.upload(trigger_uov_describe, inputs=[desc_method, uov_input_image, prompt],\n                           outputs=[prompt, style_selections], show_progress=True, queue=True)\n\ndef dump_default_english_config():\n    from modules.localization import dump_english_config\n    dump_english_config(grh.all_components)\n\n\n# dump_default_english_config()\n\nshared.gradio_root.launch(\n    inbrowser=args_manager.args.in_browser,\n    server_name=args_manager.args.listen,\n    server_port=args_manager.args.port,\n    share=args_manager.args.share,\n    auth=check_auth if (args_manager.args.share or args_manager.args.listen) and auth_enabled else None,\n    allowed_paths=[modules.config.path_outputs],\n    blocked_paths=[constants.AUTH_FILENAME]\n)\n", "args_manager.py": "import ldm_patched.modules.args_parser as args_parser\n\nargs_parser.parser.add_argument(\"--share\", action='store_true', help=\"Set whether to share on Gradio.\")\n\nargs_parser.parser.add_argument(\"--preset\", type=str, default=None, help=\"Apply specified UI preset.\")\nargs_parser.parser.add_argument(\"--disable-preset-selection\", action='store_true',\n                                help=\"Disables preset selection in Gradio.\")\n\nargs_parser.parser.add_argument(\"--language\", type=str, default='default',\n                                help=\"Translate UI using json files in [language] folder. \"\n                                  \"For example, [--language example] will use [language/example.json] for translation.\")\n\n# For example, https://github.com/lllyasviel/Fooocus/issues/849\nargs_parser.parser.add_argument(\"--disable-offload-from-vram\", action=\"store_true\",\n                                help=\"Force loading models to vram when the unload can be avoided. \"\n                                  \"Some Mac users may need this.\")\n\nargs_parser.parser.add_argument(\"--theme\", type=str, help=\"launches the UI with light or dark theme\", default=None)\nargs_parser.parser.add_argument(\"--disable-image-log\", action='store_true',\n                                help=\"Prevent writing images and logs to hard drive.\")\n\nargs_parser.parser.add_argument(\"--disable-analytics\", action='store_true',\n                                help=\"Disables analytics for Gradio.\")\n\nargs_parser.parser.add_argument(\"--disable-metadata\", action='store_true',\n                                help=\"Disables saving metadata to images.\")\n\nargs_parser.parser.add_argument(\"--disable-preset-download\", action='store_true',\n                                help=\"Disables downloading models for presets\", default=False)\n\nargs_parser.parser.add_argument(\"--enable-describe-uov-image\", action='store_true',\n                                help=\"Disables automatic description of uov images when prompt is empty\", default=False)\n\nargs_parser.parser.add_argument(\"--always-download-new-model\", action='store_true',\n                                help=\"Always download newer models \", default=False)\n\nargs_parser.parser.set_defaults(\n    disable_cuda_malloc=True,\n    in_browser=True,\n    port=None\n)\n\nargs_parser.args = args_parser.parser.parse_args()\n\n# (Disable by default because of issues like https://github.com/lllyasviel/Fooocus/issues/724)\nargs_parser.args.always_offload_from_vram = not args_parser.args.disable_offload_from_vram\n\nif args_parser.args.disable_analytics:\n    import os\n    os.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\"\n\nif args_parser.args.disable_in_browser:\n    args_parser.args.in_browser = False\n\nargs = args_parser.args\n", "experiments_expansion.py": "from modules.expansion import FooocusExpansion\n\nexpansion = FooocusExpansion()\n\ntext = 'a handsome man'\n\nfor i in range(64):\n    print(expansion(text, seed=i))\n", "shared.py": "gradio_root = None", "extras/wd14tagger.py": "# https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags\n# https://github.com/pythongosssss/ComfyUI-WD14-Tagger/blob/main/wd14tagger.py\n\n# {\n#     \"wd-v1-4-moat-tagger-v2\": \"https://huggingface.co/SmilingWolf/wd-v1-4-moat-tagger-v2\",\n#     \"wd-v1-4-convnextv2-tagger-v2\": \"https://huggingface.co/SmilingWolf/wd-v1-4-convnextv2-tagger-v2\",\n#     \"wd-v1-4-convnext-tagger-v2\": \"https://huggingface.co/SmilingWolf/wd-v1-4-convnext-tagger-v2\",\n#     \"wd-v1-4-convnext-tagger\": \"https://huggingface.co/SmilingWolf/wd-v1-4-convnext-tagger\",\n#     \"wd-v1-4-vit-tagger-v2\": \"https://huggingface.co/SmilingWolf/wd-v1-4-vit-tagger-v2\"\n# }\n\n\nimport numpy as np\nimport csv\nimport onnxruntime as ort\n\nfrom PIL import Image\nfrom onnxruntime import InferenceSession\nfrom modules.config import path_clip_vision\nfrom modules.model_loader import load_file_from_url\n\n\nglobal_model = None\nglobal_csv = None\n\n\ndef default_interrogator(image_rgb, threshold=0.35, character_threshold=0.85, exclude_tags=\"\"):\n    global global_model, global_csv\n\n    model_name = \"wd-v1-4-moat-tagger-v2\"\n\n    model_onnx_filename = load_file_from_url(\n        url=f'https://huggingface.co/lllyasviel/misc/resolve/main/{model_name}.onnx',\n        model_dir=path_clip_vision,\n        file_name=f'{model_name}.onnx',\n    )\n\n    model_csv_filename = load_file_from_url(\n        url=f'https://huggingface.co/lllyasviel/misc/resolve/main/{model_name}.csv',\n        model_dir=path_clip_vision,\n        file_name=f'{model_name}.csv',\n    )\n\n    if global_model is not None:\n        model = global_model\n    else:\n        model = InferenceSession(model_onnx_filename, providers=ort.get_available_providers())\n        global_model = model\n\n    input = model.get_inputs()[0]\n    height = input.shape[1]\n\n    image = Image.fromarray(image_rgb)  # RGB\n    ratio = float(height)/max(image.size)\n    new_size = tuple([int(x*ratio) for x in image.size])\n    image = image.resize(new_size, Image.LANCZOS)\n    square = Image.new(\"RGB\", (height, height), (255, 255, 255))\n    square.paste(image, ((height-new_size[0])//2, (height-new_size[1])//2))\n\n    image = np.array(square).astype(np.float32)\n    image = image[:, :, ::-1]  # RGB -> BGR\n    image = np.expand_dims(image, 0)\n\n    if global_csv is not None:\n        csv_lines = global_csv\n    else:\n        csv_lines = []\n        with open(model_csv_filename) as f:\n            reader = csv.reader(f)\n            next(reader)\n            for row in reader:\n                csv_lines.append(row)\n        global_csv = csv_lines\n\n    tags = []\n    general_index = None\n    character_index = None\n    for line_num, row in enumerate(csv_lines):\n        if general_index is None and row[2] == \"0\":\n            general_index = line_num\n        elif character_index is None and row[2] == \"4\":\n            character_index = line_num\n        tags.append(row[1])\n\n    label_name = model.get_outputs()[0].name\n    probs = model.run([label_name], {input.name: image})[0]\n\n    result = list(zip(tags, probs[0]))\n\n    general = [item for item in result[general_index:character_index] if item[1] > threshold]\n    character = [item for item in result[character_index:] if item[1] > character_threshold]\n\n    all = character + general\n    remove = [s.strip() for s in exclude_tags.lower().split(\",\")]\n    all = [tag for tag in all if tag[0] not in remove]\n\n    res = \", \".join((item[0].replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\") for item in all)).replace('_', ' ')\n    return res\n", "extras/ip_adapter.py": "import torch\nimport ldm_patched.modules.clip_vision\nimport safetensors.torch as sf\nimport ldm_patched.modules.model_management as model_management\nimport ldm_patched.ldm.modules.attention as attention\n\nfrom extras.resampler import Resampler\nfrom ldm_patched.modules.model_patcher import ModelPatcher\nfrom modules.core import numpy_to_pytorch\nfrom modules.ops import use_patched_ops\nfrom ldm_patched.modules.ops import manual_cast\n\n\nSD_V12_CHANNELS = [320] * 4 + [640] * 4 + [1280] * 4 + [1280] * 6 + [640] * 6 + [320] * 6 + [1280] * 2\nSD_XL_CHANNELS = [640] * 8 + [1280] * 40 + [1280] * 60 + [640] * 12 + [1280] * 20\n\n\ndef sdp(q, k, v, extra_options):\n    return attention.optimized_attention(q, k, v, heads=extra_options[\"n_heads\"], mask=None)\n\n\nclass ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n\n    def forward(self, image_embeds):\n        embeds = image_embeds\n        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens,\n                                                              self.cross_attention_dim)\n        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n        return clip_extra_context_tokens\n\n\nclass To_KV(torch.nn.Module):\n    def __init__(self, cross_attention_dim):\n        super().__init__()\n\n        channels = SD_XL_CHANNELS if cross_attention_dim == 2048 else SD_V12_CHANNELS\n        self.to_kvs = torch.nn.ModuleList(\n            [torch.nn.Linear(cross_attention_dim, channel, bias=False) for channel in channels])\n\n    def load_state_dict_ordered(self, sd):\n        state_dict = []\n        for i in range(4096):\n            for k in ['k', 'v']:\n                key = f'{i}.to_{k}_ip.weight'\n                if key in sd:\n                    state_dict.append(sd[key])\n        for i, v in enumerate(state_dict):\n            self.to_kvs[i].weight = torch.nn.Parameter(v, requires_grad=False)\n\n\nclass IPAdapterModel(torch.nn.Module):\n    def __init__(self, state_dict, plus, cross_attention_dim=768, clip_embeddings_dim=1024, clip_extra_context_tokens=4,\n                 sdxl_plus=False):\n        super().__init__()\n        self.plus = plus\n        if self.plus:\n            self.image_proj_model = Resampler(\n                dim=1280 if sdxl_plus else cross_attention_dim,\n                depth=4,\n                dim_head=64,\n                heads=20 if sdxl_plus else 12,\n                num_queries=clip_extra_context_tokens,\n                embedding_dim=clip_embeddings_dim,\n                output_dim=cross_attention_dim,\n                ff_mult=4\n            )\n        else:\n            self.image_proj_model = ImageProjModel(\n                cross_attention_dim=cross_attention_dim,\n                clip_embeddings_dim=clip_embeddings_dim,\n                clip_extra_context_tokens=clip_extra_context_tokens\n            )\n\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        self.ip_layers = To_KV(cross_attention_dim)\n        self.ip_layers.load_state_dict_ordered(state_dict[\"ip_adapter\"])\n\n\nclip_vision: ldm_patched.modules.clip_vision.ClipVisionModel = None\nip_negative: torch.Tensor = None\nip_adapters: dict = {}\n\n\ndef load_ip_adapter(clip_vision_path, ip_negative_path, ip_adapter_path):\n    global clip_vision, ip_negative, ip_adapters\n\n    if clip_vision is None and isinstance(clip_vision_path, str):\n        clip_vision = ldm_patched.modules.clip_vision.load(clip_vision_path)\n\n    if ip_negative is None and isinstance(ip_negative_path, str):\n        ip_negative = sf.load_file(ip_negative_path)['data']\n\n    if not isinstance(ip_adapter_path, str) or ip_adapter_path in ip_adapters:\n        return\n\n    load_device = model_management.get_torch_device()\n    offload_device = torch.device('cpu')\n\n    use_fp16 = model_management.should_use_fp16(device=load_device)\n    ip_state_dict = torch.load(ip_adapter_path, map_location=\"cpu\")\n    plus = \"latents\" in ip_state_dict[\"image_proj\"]\n    cross_attention_dim = ip_state_dict[\"ip_adapter\"][\"1.to_k_ip.weight\"].shape[1]\n    sdxl = cross_attention_dim == 2048\n    sdxl_plus = sdxl and plus\n\n    if plus:\n        clip_extra_context_tokens = ip_state_dict[\"image_proj\"][\"latents\"].shape[1]\n        clip_embeddings_dim = ip_state_dict[\"image_proj\"][\"latents\"].shape[2]\n    else:\n        clip_extra_context_tokens = ip_state_dict[\"image_proj\"][\"proj.weight\"].shape[0] // cross_attention_dim\n        clip_embeddings_dim = None\n\n    with use_patched_ops(manual_cast):\n        ip_adapter = IPAdapterModel(\n            ip_state_dict,\n            plus=plus,\n            cross_attention_dim=cross_attention_dim,\n            clip_embeddings_dim=clip_embeddings_dim,\n            clip_extra_context_tokens=clip_extra_context_tokens,\n            sdxl_plus=sdxl_plus\n        )\n\n    ip_adapter.sdxl = sdxl\n    ip_adapter.load_device = load_device\n    ip_adapter.offload_device = offload_device\n    ip_adapter.dtype = torch.float16 if use_fp16 else torch.float32\n    ip_adapter.to(offload_device, dtype=ip_adapter.dtype)\n\n    image_proj_model = ModelPatcher(model=ip_adapter.image_proj_model, load_device=load_device,\n                                    offload_device=offload_device)\n    ip_layers = ModelPatcher(model=ip_adapter.ip_layers, load_device=load_device,\n                             offload_device=offload_device)\n\n    ip_adapters[ip_adapter_path] = dict(\n        ip_adapter=ip_adapter,\n        image_proj_model=image_proj_model,\n        ip_layers=ip_layers,\n        ip_unconds=None\n    )\n\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_preprocess(image):\n    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=image.device, dtype=image.dtype).view([1, 3, 1, 1])\n    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=image.device, dtype=image.dtype).view([1, 3, 1, 1])\n    image = image.movedim(-1, 1)\n\n    # https://github.com/tencent-ailab/IP-Adapter/blob/d580c50a291566bbf9fc7ac0f760506607297e6d/README.md?plain=1#L75\n    B, C, H, W = image.shape\n    assert H == 224 and W == 224\n\n    return (image - mean) / std\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef preprocess(img, ip_adapter_path):\n    global ip_adapters\n    entry = ip_adapters[ip_adapter_path]\n\n    ldm_patched.modules.model_management.load_model_gpu(clip_vision.patcher)\n    pixel_values = clip_preprocess(numpy_to_pytorch(img).to(clip_vision.load_device))\n    outputs = clip_vision.model(pixel_values=pixel_values, output_hidden_states=True)\n\n    ip_adapter = entry['ip_adapter']\n    ip_layers = entry['ip_layers']\n    image_proj_model = entry['image_proj_model']\n    ip_unconds = entry['ip_unconds']\n\n    if ip_adapter.plus:\n        cond = outputs.hidden_states[-2]\n    else:\n        cond = outputs.image_embeds\n\n    cond = cond.to(device=ip_adapter.load_device, dtype=ip_adapter.dtype)\n\n    ldm_patched.modules.model_management.load_model_gpu(image_proj_model)\n    cond = image_proj_model.model(cond).to(device=ip_adapter.load_device, dtype=ip_adapter.dtype)\n\n    ldm_patched.modules.model_management.load_model_gpu(ip_layers)\n\n    if ip_unconds is None:\n        uncond = ip_negative.to(device=ip_adapter.load_device, dtype=ip_adapter.dtype)\n        ip_unconds = [m(uncond).cpu() for m in ip_layers.model.to_kvs]\n        entry['ip_unconds'] = ip_unconds\n\n    ip_conds = [m(cond).cpu() for m in ip_layers.model.to_kvs]\n\n    return ip_conds, ip_unconds\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef patch_model(model, tasks):\n    new_model = model.clone()\n\n    def make_attn_patcher(ip_index):\n        def patcher(n, context_attn2, value_attn2, extra_options):\n            org_dtype = n.dtype\n            current_step = float(model.model.diffusion_model.current_step.detach().cpu().numpy()[0])\n            cond_or_uncond = extra_options['cond_or_uncond']\n\n            q = n\n            k = [context_attn2]\n            v = [value_attn2]\n            b, _, _ = q.shape\n\n            for (cs, ucs), cn_stop, cn_weight in tasks:\n                if current_step < cn_stop:\n                    ip_k_c = cs[ip_index * 2].to(q)\n                    ip_v_c = cs[ip_index * 2 + 1].to(q)\n                    ip_k_uc = ucs[ip_index * 2].to(q)\n                    ip_v_uc = ucs[ip_index * 2 + 1].to(q)\n\n                    ip_k = torch.cat([(ip_k_c, ip_k_uc)[i] for i in cond_or_uncond], dim=0)\n                    ip_v = torch.cat([(ip_v_c, ip_v_uc)[i] for i in cond_or_uncond], dim=0)\n\n                    # Midjourney's attention formulation of image prompt (non-official reimplementation)\n                    # Written by Lvmin Zhang at Stanford University, 2023 Dec\n                    # For non-commercial use only - if you use this in commercial project then\n                    # probably it has some intellectual property issues.\n                    # Contact lvminzhang@acm.org if you are not sure.\n\n                    # Below is the sensitive part with potential intellectual property issues.\n\n                    ip_v_mean = torch.mean(ip_v, dim=1, keepdim=True)\n                    ip_v_offset = ip_v - ip_v_mean\n\n                    B, F, C = ip_k.shape\n                    channel_penalty = float(C) / 1280.0\n                    weight = cn_weight * channel_penalty\n\n                    ip_k = ip_k * weight\n                    ip_v = ip_v_offset + ip_v_mean * weight\n\n                    k.append(ip_k)\n                    v.append(ip_v)\n\n            k = torch.cat(k, dim=1)\n            v = torch.cat(v, dim=1)\n            out = sdp(q, k, v, extra_options)\n\n\n            return out.to(dtype=org_dtype)\n        return patcher\n\n    def set_model_patch_replace(model, number, key):\n        to = model.model_options[\"transformer_options\"]\n        if \"patches_replace\" not in to:\n            to[\"patches_replace\"] = {}\n        if \"attn2\" not in to[\"patches_replace\"]:\n            to[\"patches_replace\"][\"attn2\"] = {}\n        if key not in to[\"patches_replace\"][\"attn2\"]:\n            to[\"patches_replace\"][\"attn2\"][key] = make_attn_patcher(number)\n\n    number = 0\n\n    for id in [4, 5, 7, 8]:\n        block_indices = range(2) if id in [4, 5] else range(10)\n        for index in block_indices:\n            set_model_patch_replace(new_model, number, (\"input\", id, index))\n            number += 1\n\n    for id in range(6):\n        block_indices = range(2) if id in [3, 4, 5] else range(10)\n        for index in block_indices:\n            set_model_patch_replace(new_model, number, (\"output\", id, index))\n            number += 1\n\n    for index in range(10):\n        set_model_patch_replace(new_model, number, (\"middle\", 0, index))\n        number += 1\n\n    return new_model\n", "extras/face_crop.py": "import cv2\nimport numpy as np\nimport modules.config\n\n\nfaceRestoreHelper = None\n\n\ndef align_warp_face(self, landmark, border_mode='constant'):\n    affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n    self.affine_matrices.append(affine_matrix)\n    if border_mode == 'constant':\n        border_mode = cv2.BORDER_CONSTANT\n    elif border_mode == 'reflect101':\n        border_mode = cv2.BORDER_REFLECT101\n    elif border_mode == 'reflect':\n        border_mode = cv2.BORDER_REFLECT\n    input_img = self.input_img\n    cropped_face = cv2.warpAffine(input_img, affine_matrix, self.face_size,\n                                  borderMode=border_mode, borderValue=(135, 133, 132))\n    return cropped_face\n\n\ndef crop_image(img_rgb):\n    global faceRestoreHelper\n    \n    if faceRestoreHelper is None:\n        from extras.facexlib.utils.face_restoration_helper import FaceRestoreHelper\n        faceRestoreHelper = FaceRestoreHelper(\n            upscale_factor=1,\n            model_rootpath=modules.config.path_controlnet,\n            device='cpu'  # use cpu is safer since we are out of memory management\n        )\n\n    faceRestoreHelper.clean_all()\n    faceRestoreHelper.read_image(np.ascontiguousarray(img_rgb[:, :, ::-1].copy()))\n    faceRestoreHelper.get_face_landmarks_5()\n\n    landmarks = faceRestoreHelper.all_landmarks_5\n    # landmarks are already sorted with confidence.\n\n    if len(landmarks) == 0:\n        print('No face detected')\n        return img_rgb\n    else:\n        print(f'Detected {len(landmarks)} faces')\n\n    result = align_warp_face(faceRestoreHelper, landmarks[0])\n\n    return np.ascontiguousarray(result[:, :, ::-1].copy())\n", "extras/expansion.py": "# Fooocus GPT2 Expansion\n# Algorithm created by Lvmin Zhang at 2023, Stanford\n# If used inside Fooocus, any use is permitted.\n# If used outside Fooocus, only non-commercial use is permitted (CC-By NC 4.0).\n# This applies to the word list, vocab, model, and algorithm.\n\n\nimport os\nimport torch\nimport math\nimport ldm_patched.modules.model_management as model_management\n\nfrom transformers.generation.logits_process import LogitsProcessorList\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nfrom modules.config import path_fooocus_expansion\nfrom ldm_patched.modules.model_patcher import ModelPatcher\n\n\n# limitation of np.random.seed(), called from transformers.set_seed()\nSEED_LIMIT_NUMPY = 2**32\nneg_inf = - 8192.0\n\n\ndef safe_str(x):\n    x = str(x)\n    for _ in range(16):\n        x = x.replace('  ', ' ')\n    return x.strip(\",. \\r\\n\")\n\n\ndef remove_pattern(x, pattern):\n    for p in pattern:\n        x = x.replace(p, '')\n    return x\n\n\nclass FooocusExpansion:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(path_fooocus_expansion)\n\n        positive_words = open(os.path.join(path_fooocus_expansion, 'positive.txt'),\n                              encoding='utf-8').read().splitlines()\n        positive_words = ['\u0120' + x.lower() for x in positive_words if x != '']\n\n        self.logits_bias = torch.zeros((1, len(self.tokenizer.vocab)), dtype=torch.float32) + neg_inf\n\n        debug_list = []\n        for k, v in self.tokenizer.vocab.items():\n            if k in positive_words:\n                self.logits_bias[0, v] = 0\n                debug_list.append(k[1:])\n\n        print(f'Fooocus V2 Expansion: Vocab with {len(debug_list)} words.')\n\n        # debug_list = '\\n'.join(sorted(debug_list))\n        # print(debug_list)\n\n        # t11 = self.tokenizer(',', return_tensors=\"np\")\n        # t198 = self.tokenizer('\\n', return_tensors=\"np\")\n        # eos = self.tokenizer.eos_token_id\n\n        self.model = AutoModelForCausalLM.from_pretrained(path_fooocus_expansion)\n        self.model.eval()\n\n        load_device = model_management.text_encoder_device()\n        offload_device = model_management.text_encoder_offload_device()\n\n        # MPS hack\n        if model_management.is_device_mps(load_device):\n            load_device = torch.device('cpu')\n            offload_device = torch.device('cpu')\n\n        use_fp16 = model_management.should_use_fp16(device=load_device)\n\n        if use_fp16:\n            self.model.half()\n\n        self.patcher = ModelPatcher(self.model, load_device=load_device, offload_device=offload_device)\n        print(f'Fooocus Expansion engine loaded for {load_device}, use_fp16 = {use_fp16}.')\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def logits_processor(self, input_ids, scores):\n        assert scores.ndim == 2 and scores.shape[0] == 1\n        self.logits_bias = self.logits_bias.to(scores)\n\n        bias = self.logits_bias.clone()\n        bias[0, input_ids[0].to(bias.device).long()] = neg_inf\n        bias[0, 11] = 0\n\n        return scores + bias\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def __call__(self, prompt, seed):\n        if prompt == '':\n            return ''\n\n        if self.patcher.current_device != self.patcher.load_device:\n            print('Fooocus Expansion loaded by itself.')\n            model_management.load_model_gpu(self.patcher)\n\n        seed = int(seed) % SEED_LIMIT_NUMPY\n        set_seed(seed)\n        prompt = safe_str(prompt) + ','\n\n        tokenized_kwargs = self.tokenizer(prompt, return_tensors=\"pt\")\n        tokenized_kwargs.data['input_ids'] = tokenized_kwargs.data['input_ids'].to(self.patcher.load_device)\n        tokenized_kwargs.data['attention_mask'] = tokenized_kwargs.data['attention_mask'].to(self.patcher.load_device)\n\n        current_token_length = int(tokenized_kwargs.data['input_ids'].shape[1])\n        max_token_length = 75 * int(math.ceil(float(current_token_length) / 75.0))\n        max_new_tokens = max_token_length - current_token_length\n\n        if max_new_tokens == 0:\n            return prompt[:-1]\n\n        # https://huggingface.co/blog/introducing-csearch\n        # https://huggingface.co/docs/transformers/generation_strategies\n        features = self.model.generate(**tokenized_kwargs,\n                                       top_k=100,\n                                       max_new_tokens=max_new_tokens,\n                                       do_sample=True,\n                                       logits_processor=LogitsProcessorList([self.logits_processor]))\n\n        response = self.tokenizer.batch_decode(features, skip_special_tokens=True)\n        result = safe_str(response[0])\n\n        return result\n", "extras/censor.py": "import os\n\nimport numpy as np\nimport torch\nfrom transformers import CLIPConfig, CLIPImageProcessor\n\nimport ldm_patched.modules.model_management as model_management\nimport modules.config\nfrom extras.safety_checker.models.safety_checker import StableDiffusionSafetyChecker\nfrom ldm_patched.modules.model_patcher import ModelPatcher\n\nsafety_checker_repo_root = os.path.join(os.path.dirname(__file__), 'safety_checker')\nconfig_path = os.path.join(safety_checker_repo_root, \"configs\", \"config.json\")\npreprocessor_config_path = os.path.join(safety_checker_repo_root, \"configs\", \"preprocessor_config.json\")\n\n\nclass Censor:\n    def __init__(self):\n        self.safety_checker_model: ModelPatcher | None = None\n        self.clip_image_processor: CLIPImageProcessor | None = None\n        self.load_device = torch.device('cpu')\n        self.offload_device = torch.device('cpu')\n\n    def init(self):\n        if self.safety_checker_model is None and self.clip_image_processor is None:\n            safety_checker_model = modules.config.downloading_safety_checker_model()\n            self.clip_image_processor = CLIPImageProcessor.from_json_file(preprocessor_config_path)\n            clip_config = CLIPConfig.from_json_file(config_path)\n            model = StableDiffusionSafetyChecker.from_pretrained(safety_checker_model, config=clip_config)\n            model.eval()\n\n            self.load_device = model_management.text_encoder_device()\n            self.offload_device = model_management.text_encoder_offload_device()\n\n            model.to(self.offload_device)\n\n            self.safety_checker_model = ModelPatcher(model, load_device=self.load_device, offload_device=self.offload_device)\n\n    def censor(self, images: list | np.ndarray) -> list | np.ndarray:\n        self.init()\n        model_management.load_model_gpu(self.safety_checker_model)\n\n        single = False\n        if not isinstance(images, list) or isinstance(images, np.ndarray):\n            images = [images]\n            single = True\n\n        safety_checker_input = self.clip_image_processor(images, return_tensors=\"pt\")\n        safety_checker_input.to(device=self.load_device)\n        checked_images, has_nsfw_concept = self.safety_checker_model.model(images=images,\n                                                                           clip_input=safety_checker_input.pixel_values)\n        checked_images = [image.astype(np.uint8) for image in checked_images]\n\n        if single:\n            checked_images = checked_images[0]\n\n        return checked_images\n\n\ndefault_censor = Censor().censor\n", "extras/resampler.py": "# modified from https://github.com/mlfoundations/open_flamingo/blob/main/open_flamingo/src/helpers.py\nimport math\n\nimport torch\nimport torch.nn as nn\n\n\n# FFN\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n    \n    \ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    #(bs, length, width) --> (bs, length, n_heads, dim_per_head)\n    x = x.view(bs, length, heads, -1)\n    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n    x = x.transpose(1, 2)\n    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n    x = x.reshape(bs, heads, length, -1)\n    return x\n\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, n2, D)\n        \"\"\"\n        x = self.norm1(x)\n        latents = self.norm2(latents)\n        \n        b, l, _ = latents.shape\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n        \n        q = reshape_tensor(q, self.heads)\n        k = reshape_tensor(k, self.heads)\n        v = reshape_tensor(v, self.heads)\n\n        # attention\n        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n        weight = (q * scale) @ (k * scale).transpose(-2, -1) # More stable with f16 than dividing afterwards\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        out = weight @ v\n        \n        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n\n        return self.to_out(out)\n\n\nclass Resampler(nn.Module):\n    def __init__(\n        self,\n        dim=1024,\n        depth=8,\n        dim_head=64,\n        heads=16,\n        num_queries=8,\n        embedding_dim=768,\n        output_dim=1024,\n        ff_mult=4,\n    ):\n        super().__init__()\n        \n        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim**0.5)\n        \n        self.proj_in = nn.Linear(embedding_dim, dim)\n\n        self.proj_out = nn.Linear(dim, output_dim)\n        self.norm_out = nn.LayerNorm(output_dim)\n        \n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, x):\n        latents = self.latents.repeat(x.size(0), 1, 1).to(x)\n        \n        x = self.proj_in(x)\n        \n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n            \n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n", "extras/interrogate.py": "import os\nimport torch\nimport ldm_patched.modules.model_management as model_management\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom modules.model_loader import load_file_from_url\nfrom modules.config import path_clip_vision\nfrom ldm_patched.modules.model_patcher import ModelPatcher\nfrom extras.BLIP.models.blip import blip_decoder\n\n\nblip_image_eval_size = 384\nblip_repo_root = os.path.join(os.path.dirname(__file__), 'BLIP')\n\n\nclass Interrogator:\n    def __init__(self):\n        self.blip_model = None\n        self.load_device = torch.device('cpu')\n        self.offload_device = torch.device('cpu')\n        self.dtype = torch.float32\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def interrogate(self, img_rgb):\n        if self.blip_model is None:\n            filename = load_file_from_url(\n                url='https://huggingface.co/lllyasviel/misc/resolve/main/model_base_caption_capfilt_large.pth',\n                model_dir=path_clip_vision,\n                file_name='model_base_caption_capfilt_large.pth',\n            )\n\n            model = blip_decoder(pretrained=filename, image_size=blip_image_eval_size, vit='base',\n                                 med_config=os.path.join(blip_repo_root, \"configs\", \"med_config.json\"))\n            model.eval()\n\n            self.load_device = model_management.text_encoder_device()\n            self.offload_device = model_management.text_encoder_offload_device()\n            self.dtype = torch.float32\n\n            model.to(self.offload_device)\n\n            if model_management.should_use_fp16(device=self.load_device):\n                model.half()\n                self.dtype = torch.float16\n\n            self.blip_model = ModelPatcher(model, load_device=self.load_device, offload_device=self.offload_device)\n\n        model_management.load_model_gpu(self.blip_model)\n\n        gpu_image = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ])(img_rgb).unsqueeze(0).to(device=self.load_device, dtype=self.dtype)\n\n        caption = self.blip_model.model.generate(gpu_image, sample=True, num_beams=1, max_length=75)[0]\n\n        return caption\n\n\ndefault_interrogator = Interrogator().interrogate\n", "extras/vae_interpose.py": "# https://github.com/city96/SD-Latent-Interposer/blob/main/interposer.py\n\nimport os\n\nimport safetensors.torch as sf\nimport torch\nimport torch.nn as nn\n\nimport ldm_patched.modules.model_management\nfrom ldm_patched.modules.model_patcher import ModelPatcher\nfrom modules.config import path_vae_approx\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Block with residuals\"\"\"\n\n    def __init__(self, ch):\n        super().__init__()\n        self.join = nn.ReLU()\n        self.norm = nn.BatchNorm2d(ch)\n        self.long = nn.Sequential(\n            nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1),\n            nn.Dropout(0.1)\n        )\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.join(self.long(x) + x)\n\n\nclass ExtractBlock(nn.Module):\n    \"\"\"Increase no. of channels by [out/in]\"\"\"\n\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.join = nn.ReLU()\n        self.short = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n        self.long = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1),\n            nn.Dropout(0.1)\n        )\n\n    def forward(self, x):\n        return self.join(self.long(x) + self.short(x))\n\n\nclass InterposerModel(nn.Module):\n    \"\"\"Main neural network\"\"\"\n\n    def __init__(self, ch_in=4, ch_out=4, ch_mid=64, scale=1.0, blocks=12):\n        super().__init__()\n        self.ch_in = ch_in\n        self.ch_out = ch_out\n        self.ch_mid = ch_mid\n        self.blocks = blocks\n        self.scale = scale\n\n        self.head = ExtractBlock(self.ch_in, self.ch_mid)\n        self.core = nn.Sequential(\n            nn.Upsample(scale_factor=self.scale, mode=\"nearest\"),\n            *[ResBlock(self.ch_mid) for _ in range(blocks)],\n            nn.BatchNorm2d(self.ch_mid),\n            nn.SiLU(),\n        )\n        self.tail = nn.Conv2d(self.ch_mid, self.ch_out, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        y = self.head(x)\n        z = self.core(y)\n        return self.tail(z)\n\n\nvae_approx_model = None\nvae_approx_filename = os.path.join(path_vae_approx, 'xl-to-v1_interposer-v4.0.safetensors')\n\n\ndef parse(x):\n    global vae_approx_model\n\n    x_origin = x.clone()\n\n    if vae_approx_model is None:\n        model = InterposerModel()\n        model.eval()\n        sd = sf.load_file(vae_approx_filename)\n        model.load_state_dict(sd)\n        fp16 = ldm_patched.modules.model_management.should_use_fp16()\n        if fp16:\n            model = model.half()\n        vae_approx_model = ModelPatcher(\n            model=model,\n            load_device=ldm_patched.modules.model_management.get_torch_device(),\n            offload_device=torch.device('cpu')\n        )\n        vae_approx_model.dtype = torch.float16 if fp16 else torch.float32\n\n    ldm_patched.modules.model_management.load_model_gpu(vae_approx_model)\n\n    x = x_origin.to(device=vae_approx_model.load_device, dtype=vae_approx_model.dtype)\n    x = vae_approx_model.model(x).to(x_origin)\n    return x\n", "extras/preprocessors.py": "import cv2\nimport numpy as np\n\n\ndef centered_canny(x: np.ndarray, canny_low_threshold, canny_high_threshold):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 2 and x.dtype == np.uint8\n\n    y = cv2.Canny(x, int(canny_low_threshold), int(canny_high_threshold))\n    y = y.astype(np.float32) / 255.0\n    return y\n\n\ndef centered_canny_color(x: np.ndarray, canny_low_threshold, canny_high_threshold):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n\n    result = [centered_canny(x[..., i], canny_low_threshold, canny_high_threshold) for i in range(3)]\n    result = np.stack(result, axis=2)\n    return result\n\n\ndef pyramid_canny_color(x: np.ndarray, canny_low_threshold, canny_high_threshold):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n\n    H, W, C = x.shape\n    acc_edge = None\n\n    for k in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n        Hs, Ws = int(H * k), int(W * k)\n        small = cv2.resize(x, (Ws, Hs), interpolation=cv2.INTER_AREA)\n        edge = centered_canny_color(small, canny_low_threshold, canny_high_threshold)\n        if acc_edge is None:\n            acc_edge = edge\n        else:\n            acc_edge = cv2.resize(acc_edge, (edge.shape[1], edge.shape[0]), interpolation=cv2.INTER_LINEAR)\n            acc_edge = acc_edge * 0.75 + edge * 0.25\n\n    return acc_edge\n\n\ndef norm255(x, low=4, high=96):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 2 and x.dtype == np.float32\n\n    v_min = np.percentile(x, low)\n    v_max = np.percentile(x, high)\n\n    x -= v_min\n    x /= v_max - v_min\n\n    return x * 255.0\n\n\ndef canny_pyramid(x, canny_low_threshold, canny_high_threshold):\n    # For some reasons, SAI's Control-lora Canny seems to be trained on canny maps with non-standard resolutions.\n    # Then we use pyramid to use all resolutions to avoid missing any structure in specific resolutions.\n\n    color_canny = pyramid_canny_color(x, canny_low_threshold, canny_high_threshold)\n    result = np.sum(color_canny, axis=2)\n\n    return norm255(result, low=1, high=99).clip(0, 255).astype(np.uint8)\n\n\ndef cpds(x):\n    # cv2.decolor is not \"decolor\", it is Cewu Lu's method\n    # See http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html\n    # See https://docs.opencv.org/3.0-beta/modules/photo/doc/decolor.html\n\n    raw = cv2.GaussianBlur(x, (0, 0), 0.8)\n    density, boost = cv2.decolor(raw)\n\n    raw = raw.astype(np.float32)\n    density = density.astype(np.float32)\n    boost = boost.astype(np.float32)\n\n    offset = np.sum((raw - boost) ** 2.0, axis=2) ** 0.5\n    result = density + offset\n\n    return norm255(result, low=4, high=96).clip(0, 255).astype(np.uint8)\n", "extras/BLIP/models/blip_itm.py": "from extras.BLIP.models.med import BertConfig, BertModel\nfrom transformers import BertTokenizer\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom extras.BLIP.models.blip import create_vit, init_tokenizer, load_checkpoint\n\nclass BLIP_ITM(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 384,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                      \n                 embed_dim = 256,     \n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        self.tokenizer = init_tokenizer()   \n        med_config = BertConfig.from_json_file(med_config)\n        med_config.encoder_width = vision_width\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)          \n\n        text_width = self.text_encoder.config.hidden_size\n        \n        self.vision_proj = nn.Linear(vision_width, embed_dim)\n        self.text_proj = nn.Linear(text_width, embed_dim)\n\n        self.itm_head = nn.Linear(text_width, 2) \n        \n        \n    def forward(self, image, caption, match_head='itm'):\n\n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)        \n      \n        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, \n                              return_tensors=\"pt\").to(image.device) \n\n                 \n        if match_head=='itm':\n            output = self.text_encoder(text.input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )\n            itm_output = self.itm_head(output.last_hidden_state[:,0,:])     \n            return itm_output\n            \n        elif match_head=='itc':\n            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                            return_dict = True, mode = 'text')                     \n            image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1)   \n            text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1)    \n            \n            sim = image_feat @ text_feat.t()\n            return sim\n        \n        \ndef blip_itm(pretrained='',**kwargs):\n    model = BLIP_ITM(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n        assert(len(msg.missing_keys)==0)\n    return model         \n            ", "extras/BLIP/models/blip_vqa.py": "from extras.BLIP.models.med import BertConfig, BertModel, BertLMHeadModel\nfrom extras.BLIP.models.blip import create_vit, init_tokenizer, load_checkpoint\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer\nimport numpy as np\n\nclass BLIP_VQA(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 480,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                   \n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer, drop_path_rate=0.1)\n        self.tokenizer = init_tokenizer()  \n        \n        encoder_config = BertConfig.from_json_file(med_config)\n        encoder_config.encoder_width = vision_width\n        self.text_encoder = BertModel(config=encoder_config, add_pooling_layer=False) \n        \n        decoder_config = BertConfig.from_json_file(med_config)        \n        self.text_decoder = BertLMHeadModel(config=decoder_config)          \n\n\n    def forward(self, image, question, answer=None, n=None, weights=None, train=True, inference='rank', k_test=128):\n        \n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n        \n        question = self.tokenizer(question, padding='longest', truncation=True, max_length=35, \n                                  return_tensors=\"pt\").to(image.device) \n        question.input_ids[:,0] = self.tokenizer.enc_token_id\n        \n        if train:               \n            '''\n            n: number of answers for each question\n            weights: weight for each answer\n            '''                     \n            answer = self.tokenizer(answer, padding='longest', return_tensors=\"pt\").to(image.device) \n            answer.input_ids[:,0] = self.tokenizer.bos_token_id\n            answer_targets = answer.input_ids.masked_fill(answer.input_ids == self.tokenizer.pad_token_id, -100)      \n\n            question_output = self.text_encoder(question.input_ids, \n                                                attention_mask = question.attention_mask, \n                                                encoder_hidden_states = image_embeds,\n                                                encoder_attention_mask = image_atts,                             \n                                                return_dict = True)    \n\n            question_states = []                \n            question_atts = []  \n            for b, n in enumerate(n):\n                question_states += [question_output.last_hidden_state[b]]*n\n                question_atts += [question.attention_mask[b]]*n                \n            question_states = torch.stack(question_states,0)    \n            question_atts = torch.stack(question_atts,0)     \n\n            answer_output = self.text_decoder(answer.input_ids, \n                                              attention_mask = answer.attention_mask, \n                                              encoder_hidden_states = question_states,\n                                              encoder_attention_mask = question_atts,                  \n                                              labels = answer_targets,\n                                              return_dict = True,   \n                                              reduction = 'none',\n                                             )      \n            \n            loss = weights * answer_output.loss\n            loss = loss.sum()/image.size(0)\n\n            return loss\n            \n\n        else: \n            question_output = self.text_encoder(question.input_ids, \n                                                attention_mask = question.attention_mask, \n                                                encoder_hidden_states = image_embeds,\n                                                encoder_attention_mask = image_atts,                                    \n                                                return_dict = True) \n            \n            if inference=='generate':\n                num_beams = 3\n                question_states = question_output.last_hidden_state.repeat_interleave(num_beams,dim=0)\n                question_atts = torch.ones(question_states.size()[:-1],dtype=torch.long).to(question_states.device)\n                model_kwargs = {\"encoder_hidden_states\": question_states, \"encoder_attention_mask\":question_atts}\n                \n                bos_ids = torch.full((image.size(0),1),fill_value=self.tokenizer.bos_token_id,device=image.device)\n                \n                outputs = self.text_decoder.generate(input_ids=bos_ids,\n                                                     max_length=10,\n                                                     min_length=1,\n                                                     num_beams=num_beams,\n                                                     eos_token_id=self.tokenizer.sep_token_id,\n                                                     pad_token_id=self.tokenizer.pad_token_id, \n                                                     **model_kwargs)\n                \n                answers = []    \n                for output in outputs:\n                    answer = self.tokenizer.decode(output, skip_special_tokens=True)    \n                    answers.append(answer)\n                return answers\n            \n            elif inference=='rank':\n                max_ids = self.rank_answer(question_output.last_hidden_state, question.attention_mask, \n                                           answer.input_ids, answer.attention_mask, k_test) \n                return max_ids\n \n                \n                \n    def rank_answer(self, question_states, question_atts, answer_ids, answer_atts, k):\n        \n        num_ques = question_states.size(0)\n        start_ids = answer_ids[0,0].repeat(num_ques,1) # bos token\n        \n        start_output = self.text_decoder(start_ids, \n                                         encoder_hidden_states = question_states,\n                                         encoder_attention_mask = question_atts,                                      \n                                         return_dict = True,\n                                         reduction = 'none')              \n        logits = start_output.logits[:,0,:] # first token's logit\n        \n        # topk_probs: top-k probability \n        # topk_ids: [num_question, k]        \n        answer_first_token = answer_ids[:,1]\n        prob_first_token = F.softmax(logits,dim=1).index_select(dim=1, index=answer_first_token) \n        topk_probs, topk_ids = prob_first_token.topk(k,dim=1) \n        \n        # answer input: [num_question*k, answer_len]                 \n        input_ids = []\n        input_atts = []\n        for b, topk_id in enumerate(topk_ids):\n            input_ids.append(answer_ids.index_select(dim=0, index=topk_id))\n            input_atts.append(answer_atts.index_select(dim=0, index=topk_id))\n        input_ids = torch.cat(input_ids,dim=0)  \n        input_atts = torch.cat(input_atts,dim=0)  \n\n        targets_ids = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n\n        # repeat encoder's output for top-k answers\n        question_states = tile(question_states, 0, k)\n        question_atts = tile(question_atts, 0, k)\n        \n        output = self.text_decoder(input_ids, \n                                   attention_mask = input_atts, \n                                   encoder_hidden_states = question_states,\n                                   encoder_attention_mask = question_atts,     \n                                   labels = targets_ids,\n                                   return_dict = True, \n                                   reduction = 'none')   \n        \n        log_probs_sum = -output.loss\n        log_probs_sum = log_probs_sum.view(num_ques,k)\n\n        max_topk_ids = log_probs_sum.argmax(dim=1) \n        max_ids = topk_ids[max_topk_ids>=0,max_topk_ids]\n\n        return max_ids\n    \n    \ndef blip_vqa(pretrained='',**kwargs):\n    model = BLIP_VQA(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n#         assert(len(msg.missing_keys)==0)\n    return model  \n\n\ndef tile(x, dim, n_tile):\n    init_dim = x.size(dim)\n    repeat_idx = [1] * x.dim()\n    repeat_idx[dim] = n_tile\n    x = x.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n    return torch.index_select(x, dim, order_index.to(x.device))    \n        \n        ", "extras/BLIP/models/blip_retrieval.py": "from extras.BLIP.models.med import BertConfig, BertModel\nfrom transformers import BertTokenizer\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom extras.BLIP.models.blip import create_vit, init_tokenizer, load_checkpoint\n\nclass BLIP_Retrieval(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 384,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                      \n                 embed_dim = 256,     \n                 queue_size = 57600,\n                 momentum = 0.995,\n                 negative_all_rank = False,\n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        self.tokenizer = init_tokenizer()   \n        med_config = BertConfig.from_json_file(med_config)\n        med_config.encoder_width = vision_width\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)          \n\n        text_width = self.text_encoder.config.hidden_size\n        \n        self.vision_proj = nn.Linear(vision_width, embed_dim)\n        self.text_proj = nn.Linear(text_width, embed_dim)\n\n        self.itm_head = nn.Linear(text_width, 2) \n        \n        # create momentum encoders  \n        self.visual_encoder_m, vision_width = create_vit(vit,image_size)              \n        self.vision_proj_m = nn.Linear(vision_width, embed_dim)\n        self.text_encoder_m = BertModel(config=med_config, add_pooling_layer=False)    \n        self.text_proj_m = nn.Linear(text_width, embed_dim)\n        \n        self.model_pairs = [[self.visual_encoder,self.visual_encoder_m],\n                            [self.vision_proj,self.vision_proj_m],\n                            [self.text_encoder,self.text_encoder_m],\n                            [self.text_proj,self.text_proj_m],\n                           ]       \n        self.copy_params()\n\n        # create the queue\n        self.register_buffer(\"image_queue\", torch.randn(embed_dim, queue_size))\n        self.register_buffer(\"text_queue\", torch.randn(embed_dim, queue_size))\n        self.register_buffer(\"idx_queue\", torch.full((1,queue_size),-100))\n        self.register_buffer(\"ptr_queue\", torch.zeros(1, dtype=torch.long))  \n\n        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n        \n        self.queue_size = queue_size\n        self.momentum = momentum\n        self.temp = nn.Parameter(0.07*torch.ones([]))   \n        \n        self.negative_all_rank = negative_all_rank\n        \n        \n    def forward(self, image, caption, alpha, idx):\n        with torch.no_grad():\n            self.temp.clamp_(0.001,0.5)\n        \n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)        \n        image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1)    \n        \n        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, \n                              return_tensors=\"pt\").to(image.device) \n        \n        text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                        return_dict = True, mode = 'text')            \n        text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1)        \n        \n        ###============== Image-text Contrastive Learning ===================###\n        idx = idx.view(-1,1)\n        idx_all = torch.cat([idx.t(), self.idx_queue.clone().detach()],dim=1)  \n        pos_idx = torch.eq(idx, idx_all).float()       \n        sim_targets = pos_idx / pos_idx.sum(1,keepdim=True)   \n        \n        # get momentum features\n        with torch.no_grad():\n            self._momentum_update()\n            image_embeds_m = self.visual_encoder_m(image) \n            image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:,0,:]),dim=-1)  \n            image_feat_m_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1)                   \n            \n            text_output_m = self.text_encoder_m(text.input_ids, attention_mask = text.attention_mask,                      \n                                                return_dict = True, mode = 'text')    \n            text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:,0,:]),dim=-1) \n            text_feat_m_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1)\n\n            sim_i2t_m = image_feat_m @ text_feat_m_all / self.temp  \n            sim_t2i_m = text_feat_m @ image_feat_m_all / self.temp   \n\n            sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets\n            sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets        \n\n        sim_i2t = image_feat @ text_feat_m_all / self.temp \n        sim_t2i = text_feat @ image_feat_m_all / self.temp \n                             \n        loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean()\n        loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean() \n\n        loss_ita = (loss_i2t+loss_t2i)/2\n        \n        idxs = concat_all_gather(idx)\n        self._dequeue_and_enqueue(image_feat_m, text_feat_m, idxs)        \n\n        ###============== Image-text Matching ===================###\n        encoder_input_ids = text.input_ids.clone()\n        encoder_input_ids[:,0] = self.tokenizer.enc_token_id\n\n        # forward the positve image-text pair\n        bs = image.size(0)\n        output_pos = self.text_encoder(encoder_input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )  \n        \n        \n        if self.negative_all_rank:    \n            # compute sample similarity\n            with torch.no_grad():                \n                mask = torch.eq(idx, idxs.t())\n\n                image_feat_world = concat_all_gather(image_feat)\n                text_feat_world = concat_all_gather(text_feat)\n\n                sim_i2t = image_feat @ text_feat_world.t() / self.temp \n                sim_t2i = text_feat @ image_feat_world.t() / self.temp \n\n                weights_i2t = F.softmax(sim_i2t,dim=1)\n                weights_i2t.masked_fill_(mask, 0)            \n\n                weights_t2i = F.softmax(sim_t2i,dim=1)\n                weights_t2i.masked_fill_(mask, 0)     \n\n            image_embeds_world = all_gather_with_grad(image_embeds) \n\n            # select a negative image (from all ranks) for each text\n            image_embeds_neg = []    \n            for b in range(bs):\n                neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n                image_embeds_neg.append(image_embeds_world[neg_idx])\n            image_embeds_neg = torch.stack(image_embeds_neg,dim=0)   \n\n            # select a negative text (from all ranks) for each image\n            input_ids_world = concat_all_gather(encoder_input_ids)\n            att_mask_world = concat_all_gather(text.attention_mask)        \n\n            text_ids_neg = []\n            text_atts_neg = []\n            for b in range(bs):\n                neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n                text_ids_neg.append(input_ids_world[neg_idx])\n                text_atts_neg.append(att_mask_world[neg_idx])\n                \n        else:\n            with torch.no_grad():                \n                mask = torch.eq(idx, idx.t())\n                \n                sim_i2t = image_feat @ text_feat.t() / self.temp \n                sim_t2i = text_feat @ image_feat.t() / self.temp \n\n                weights_i2t = F.softmax(sim_i2t,dim=1)\n                weights_i2t.masked_fill_(mask, 0)            \n\n                weights_t2i = F.softmax(sim_t2i,dim=1)\n                weights_t2i.masked_fill_(mask, 0)     \n\n            # select a negative image (from same rank) for each text\n            image_embeds_neg = []    \n            for b in range(bs):\n                neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n                image_embeds_neg.append(image_embeds[neg_idx])\n            image_embeds_neg = torch.stack(image_embeds_neg,dim=0)   \n\n            # select a negative text (from same rank) for each image    \n            text_ids_neg = []\n            text_atts_neg = []\n            for b in range(bs):\n                neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n                text_ids_neg.append(encoder_input_ids[neg_idx])\n                text_atts_neg.append(text.attention_mask[neg_idx])            \n            \n        text_ids_neg = torch.stack(text_ids_neg,dim=0)   \n        text_atts_neg = torch.stack(text_atts_neg,dim=0)      \n\n        text_ids_all = torch.cat([encoder_input_ids, text_ids_neg],dim=0)     \n        text_atts_all = torch.cat([text.attention_mask, text_atts_neg],dim=0)     \n\n        image_embeds_all = torch.cat([image_embeds_neg,image_embeds],dim=0)\n        image_atts_all = torch.cat([image_atts,image_atts],dim=0)\n\n        output_neg = self.text_encoder(text_ids_all,\n                                       attention_mask = text_atts_all,\n                                       encoder_hidden_states = image_embeds_all,\n                                       encoder_attention_mask = image_atts_all,      \n                                       return_dict = True,\n                                      )                         \n          \n\n        vl_embeddings = torch.cat([output_pos.last_hidden_state[:,0,:], output_neg.last_hidden_state[:,0,:]],dim=0)\n        vl_output = self.itm_head(vl_embeddings)            \n\n        itm_labels = torch.cat([torch.ones(bs,dtype=torch.long),torch.zeros(2*bs,dtype=torch.long)],\n                               dim=0).to(image.device)\n        loss_itm = F.cross_entropy(vl_output, itm_labels)     \n\n        return loss_ita, loss_itm \n \n\n    @torch.no_grad()    \n    def copy_params(self):\n        for model_pair in self.model_pairs:           \n            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n                param_m.data.copy_(param.data)  # initialize\n                param_m.requires_grad = False  # not update by gradient    \n\n            \n    @torch.no_grad()        \n    def _momentum_update(self):\n        for model_pair in self.model_pairs:           \n            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n                param_m.data = param_m.data * self.momentum + param.data * (1. - self.momentum)\n                \n                \n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, image_feat, text_feat, idxs):\n        # gather keys before updating queue\n        image_feats = concat_all_gather(image_feat)\n        text_feats = concat_all_gather(text_feat)\n        \n\n        batch_size = image_feats.shape[0]\n\n        ptr = int(self.ptr_queue)\n        assert self.queue_size % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.image_queue[:, ptr:ptr + batch_size] = image_feats.T\n        self.text_queue[:, ptr:ptr + batch_size] = text_feats.T\n        self.idx_queue[:, ptr:ptr + batch_size] = idxs.T\n        ptr = (ptr + batch_size) % self.queue_size # move pointer\n\n        self.ptr_queue[0] = ptr  \n\n\ndef blip_retrieval(pretrained='',**kwargs):\n    model = BLIP_Retrieval(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n        print(\"missing keys:\")\n        print(msg.missing_keys)\n    return model \n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    tensors_gather = [torch.ones_like(tensor)\n        for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output      \n\n\nclass GatherLayer(torch.autograd.Function):\n    \"\"\"\n    Gather tensors from all workers with support for backward propagation:\n    This implementation does not cut the gradients as torch.distributed.all_gather does.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x):\n        output = [torch.zeros_like(x) for _ in range(torch.distributed.get_world_size())]\n        torch.distributed.all_gather(output, x)\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        all_gradients = torch.stack(grads)\n        torch.distributed.all_reduce(all_gradients)\n        return all_gradients[torch.distributed.get_rank()]\n\n\ndef all_gather_with_grad(tensors):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    Graph remains connected for backward grad computation.\n    \"\"\"\n    # Queue the gathered tensors\n    world_size = torch.distributed.get_world_size()\n    # There is no need for reduction in the single-proc case\n    if world_size == 1:\n        return tensors\n\n    tensor_all = GatherLayer.apply(tensors)\n\n    return torch.cat(tensor_all, dim=0)\n", "extras/BLIP/models/blip.py": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n'''\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom extras.BLIP.models.vit import VisionTransformer, interpolate_pos_embed\nfrom extras.BLIP.models.med import BertConfig, BertModel, BertLMHeadModel\nfrom transformers import BertTokenizer\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport os\nfrom urllib.parse import urlparse\nfrom timm.models.hub import download_cached_file\n\nclass BLIP_Base(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 224,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                 \n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        self.tokenizer = init_tokenizer()   \n        med_config = BertConfig.from_json_file(med_config)\n        med_config.encoder_width = vision_width\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n\n        \n    def forward(self, image, caption, mode):\n        \n        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n        text = self.tokenizer(caption, return_tensors=\"pt\").to(image.device) \n        \n        if mode=='image':    \n            # return image features\n            image_embeds = self.visual_encoder(image)             \n            return image_embeds\n        \n        elif mode=='text':\n            # return text features\n            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                            return_dict = True, mode = 'text')  \n            return text_output.last_hidden_state\n        \n        elif mode=='multimodal':\n            # return multimodel features\n            image_embeds = self.visual_encoder(image)    \n            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      \n            \n            text.input_ids[:,0] = self.tokenizer.enc_token_id\n            output = self.text_encoder(text.input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )              \n            return output.last_hidden_state\n        \n        \n        \nclass BLIP_Decoder(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 384,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,\n                 prompt = 'a picture of ',\n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"            \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        self.tokenizer = init_tokenizer()   \n        med_config = BertConfig.from_json_file(med_config)\n        med_config.encoder_width = vision_width\n        self.text_decoder = BertLMHeadModel(config=med_config)    \n        \n        self.prompt = prompt\n        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1\n\n        \n    def forward(self, image, caption):\n        \n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n        \n        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(image.device) \n        \n        text.input_ids[:,0] = self.tokenizer.bos_token_id\n        \n        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n        decoder_targets[:,:self.prompt_length] = -100\n     \n        decoder_output = self.text_decoder(text.input_ids, \n                                           attention_mask = text.attention_mask, \n                                           encoder_hidden_states = image_embeds,\n                                           encoder_attention_mask = image_atts,                  \n                                           labels = decoder_targets,\n                                           return_dict = True,   \n                                          )   \n        loss_lm = decoder_output.loss\n        \n        return loss_lm\n        \n    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n        image_embeds = self.visual_encoder(image)\n\n        if not sample:\n            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n            \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n        \n        prompt = [self.prompt] * image.size(0)\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \n        input_ids[:,0] = self.tokenizer.bos_token_id\n        input_ids = input_ids[:, :-1] \n\n        if sample:\n            #nucleus sampling\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  do_sample=True,\n                                                  top_p=top_p,\n                                                  num_return_sequences=1,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id, \n                                                  repetition_penalty=1.1,                                            \n                                                  **model_kwargs)\n        else:\n            #beam search\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  num_beams=num_beams,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id,     \n                                                  repetition_penalty=repetition_penalty,\n                                                  **model_kwargs)            \n            \n        captions = []    \n        for output in outputs:\n            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n            captions.append(caption[len(self.prompt):])\n        return captions\n    \n\ndef blip_decoder(pretrained='',**kwargs):\n    model = BLIP_Decoder(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n        assert(len(msg.missing_keys)==0)\n    return model    \n    \ndef blip_feature_extractor(pretrained='',**kwargs):\n    model = BLIP_Base(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n        assert(len(msg.missing_keys)==0)\n    return model        \n\ndef init_tokenizer():\n    tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"bert_tokenizer\")\n    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       \n    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  \n    return tokenizer\n\n\ndef create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n        \n    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n    if vit=='base':\n        vision_width = 768\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0 or drop_path_rate\n                                          )   \n    elif vit=='large':\n        vision_width = 1024\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0.1 or drop_path_rate\n                                          )   \n    return visual_encoder, vision_width\n\ndef is_url(url_or_filename):\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (\"http\", \"https\")\n\ndef load_checkpoint(model,url_or_filename):\n    if is_url(url_or_filename):\n        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n        checkpoint = torch.load(cached_file, map_location='cpu') \n    elif os.path.isfile(url_or_filename):        \n        checkpoint = torch.load(url_or_filename, map_location='cpu') \n    else:\n        raise RuntimeError('checkpoint url or path is invalid')\n        \n    state_dict = checkpoint['model']\n    \n    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n                                                                         model.visual_encoder_m)    \n    for key in model.state_dict().keys():\n        if key in state_dict.keys():\n            if state_dict[key].shape!=model.state_dict()[key].shape:\n                del state_dict[key]\n    \n    msg = model.load_state_dict(state_dict,strict=False)\n    print('load checkpoint from %s'%url_or_filename)  \n    return model,msg\n    \n", "extras/BLIP/models/blip_nlvr.py": "from extras.BLIP.models.med import BertConfig\nfrom extras.BLIP.models.nlvr_encoder import BertModel\nfrom extras.BLIP.models.vit import interpolate_pos_embed\nfrom extras.BLIP.models.blip import create_vit, init_tokenizer, is_url\n\nfrom timm.models.hub import download_cached_file\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer\nimport numpy as np\nimport os\n\n\nclass BLIP_NLVR(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 480,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                   \n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer, drop_path_rate=0.1)\n        self.tokenizer = init_tokenizer()   \n        med_config = BertConfig.from_json_file(med_config)\n        med_config.encoder_width = vision_width\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False) \n                    \n        self.cls_head = nn.Sequential(\n                  nn.Linear(self.text_encoder.config.hidden_size, self.text_encoder.config.hidden_size),\n                  nn.ReLU(),\n                  nn.Linear(self.text_encoder.config.hidden_size, 2)\n                )  \n\n    def forward(self, image, text, targets, train=True):\n        \n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)        \n        image0_embeds, image1_embeds = torch.split(image_embeds,targets.size(0))     \n\n        text = self.tokenizer(text, padding='longest', return_tensors=\"pt\").to(image.device) \n        text.input_ids[:,0] = self.tokenizer.enc_token_id        \n\n        output = self.text_encoder(text.input_ids, \n                                   attention_mask = text.attention_mask, \n                                   encoder_hidden_states = [image0_embeds,image1_embeds],\n                                   encoder_attention_mask = [image_atts[:image0_embeds.size(0)],\n                                                             image_atts[image0_embeds.size(0):]],        \n                                   return_dict = True,\n                                  )  \n        hidden_state = output.last_hidden_state[:,0,:]        \n        prediction = self.cls_head(hidden_state)\n\n        if train:            \n            loss = F.cross_entropy(prediction, targets)   \n            return loss\n        else:\n            return prediction\n    \ndef blip_nlvr(pretrained='',**kwargs):\n    model = BLIP_NLVR(**kwargs)\n    if pretrained:\n        model,msg = load_checkpoint(model,pretrained)\n        print(\"missing keys:\")\n        print(msg.missing_keys)\n    return model  \n\n        \ndef load_checkpoint(model,url_or_filename):\n    if is_url(url_or_filename):\n        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n        checkpoint = torch.load(cached_file, map_location='cpu') \n    elif os.path.isfile(url_or_filename):        \n        checkpoint = torch.load(url_or_filename, map_location='cpu') \n    else:\n        raise RuntimeError('checkpoint url or path is invalid')\n    state_dict = checkpoint['model']\n    \n    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n    \n    for key in list(state_dict.keys()):\n        if 'crossattention.self.' in key:\n            new_key0 = key.replace('self','self0')\n            new_key1 = key.replace('self','self1')\n            state_dict[new_key0] = state_dict[key]\n            state_dict[new_key1] = state_dict[key]\n        elif 'crossattention.output.dense.' in key:\n            new_key0 = key.replace('dense','dense0')\n            new_key1 = key.replace('dense','dense1')\n            state_dict[new_key0] = state_dict[key]\n            state_dict[new_key1] = state_dict[key]  \n                \n    msg = model.load_state_dict(state_dict,strict=False)\n    print('load checkpoint from %s'%url_or_filename)  \n    return model,msg\n            ", "extras/BLIP/models/nlvr_encoder.py": "import math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.models.bert.configuration_bert import BertConfig\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        \n        self.config = config\n\n    def forward(\n        self, input_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n\n        embeddings = inputs_embeds\n\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        \n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n        self.save_attention = False   \n            \n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n        \n    def get_attn_gradients(self):\n        return self.attn_gradients\n    \n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n        \n    def get_attention_map(self):\n        return self.attention_map\n    \n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        \n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)         \n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config, twin=False, merge=False):     \n        super().__init__()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)        \n        if twin:\n            self.dense0 = nn.Linear(config.hidden_size, config.hidden_size)\n            self.dense1 = nn.Linear(config.hidden_size, config.hidden_size)         \n        else:\n            self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if merge:\n            self.act =  ACT2FN[config.hidden_act]\n            self.merge_layer = nn.Linear(config.hidden_size * 2, config.hidden_size)\n            self.merge = True\n        else:\n            self.merge = False\n\n    def forward(self, hidden_states, input_tensor):\n        if type(hidden_states) == list:\n            hidden_states0 = self.dense0(hidden_states[0])\n            hidden_states1 = self.dense1(hidden_states[1])        \n            if self.merge:  \n                #hidden_states = self.merge_layer(self.act(torch.cat([hidden_states0,hidden_states1],dim=-1)))\n                hidden_states = self.merge_layer(torch.cat([hidden_states0,hidden_states1],dim=-1))\n            else:\n                hidden_states = (hidden_states0+hidden_states1)/2\n        else:    \n            hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False, layer_num=-1):\n        super().__init__()\n        if is_cross_attention:\n            self.self0 = BertSelfAttention(config, is_cross_attention)\n            self.self1 = BertSelfAttention(config, is_cross_attention)\n        else:    \n            self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config, twin=is_cross_attention, merge=(is_cross_attention and layer_num>=6))\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):        \n        if type(encoder_hidden_states)==list:   \n            self_outputs0 = self.self0(\n                hidden_states,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states[0],\n                encoder_attention_mask[0],\n                past_key_value,\n                output_attentions,\n            )\n            self_outputs1 = self.self1(\n                hidden_states,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states[1],\n                encoder_attention_mask[1],\n                past_key_value,\n                output_attentions,\n            )                        \n            attention_output = self.output([self_outputs0[0],self_outputs1[0]], hidden_states)\n    \n            outputs = (attention_output,) + self_outputs0[1:]  # add attentions if we output them\n        else:        \n            self_outputs = self.self(\n                hidden_states,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n            )\n            attention_output = self.output(self_outputs[0], hidden_states)\n            outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)      \n        self.layer_num = layer_num          \n        if self.config.add_cross_attention:\n            self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention, layer_num=layer_num)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n        mode=None,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        outputs = self_attention_outputs[1:-1]\n        present_key_value = self_attention_outputs[-1]\n\n        if mode=='multimodal':\n            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                output_attentions=output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights                               \n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config,i) for i in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n        mode='multimodal',\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n\n        next_decoder_cache = () if use_cache else None\n               \n        for i in range(self.config.num_hidden_layers):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    mode=mode,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                    mode=mode,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n        \n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n \n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    \n    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n   \n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )                     \n\n                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n    \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multimodal',\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n            device = input_ids.device\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = inputs_embeds.device\n        elif encoder_embeds is not None:    \n            input_shape = encoder_embeds.size()[:-1]\n            batch_size, seq_length = input_shape \n            device = encoder_embeds.device\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            \n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, \n                                                                                 device, is_decoder)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n            else:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            \n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n            else:    \n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n        \n        if encoder_embeds is None:\n            embedding_output = self.embeddings(\n                input_ids=input_ids,\n                position_ids=position_ids,\n                inputs_embeds=inputs_embeds,\n                past_key_values_length=past_key_values_length,\n            )\n        else:\n            embedding_output = encoder_embeds\n            \n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            mode=mode,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n\n", "extras/BLIP/models/med.py": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on huggingface code base\n * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n'''\n\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.models.bert.configuration_bert import BertConfig\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        \n        self.config = config\n\n    def forward(\n        self, input_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n\n        embeddings = inputs_embeds\n\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        \n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n        self.save_attention = False   \n            \n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n        \n    def get_attn_gradients(self):\n        return self.attn_gradients\n    \n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n        \n    def get_attention_map(self):\n        return self.attention_map\n    \n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        \n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)         \n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False):\n        super().__init__()\n        self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)      \n        self.layer_num = layer_num          \n        if self.config.add_cross_attention:\n            self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n        mode=None,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        outputs = self_attention_outputs[1:-1]\n        present_key_value = self_attention_outputs[-1]\n\n        if mode=='multimodal':\n            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                output_attentions=output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights                               \n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config,i) for i in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n        mode='multimodal',\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n\n        next_decoder_cache = () if use_cache else None\n               \n        for i in range(self.config.num_hidden_layers):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    mode=mode,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                    mode=mode,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n        \n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n \n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    \n    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n   \n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )                     \n\n                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n    \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multimodal',\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n            device = input_ids.device\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = inputs_embeds.device\n        elif encoder_embeds is not None:    \n            input_shape = encoder_embeds.size()[:-1]\n            batch_size, seq_length = input_shape \n            device = encoder_embeds.device\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            \n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, \n                                                                                 device, is_decoder)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n            else:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            \n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n            else:    \n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n        \n        if encoder_embeds is None:\n            embedding_output = self.embeddings(\n                input_ids=input_ids,\n                position_ids=position_ids,\n                inputs_embeds=inputs_embeds,\n                past_key_values_length=past_key_values_length,\n            )\n        else:\n            embedding_output = encoder_embeds\n            \n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            mode=mode,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n\n\n\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        return_logits=False,            \n        is_decoder=True,\n        reduction='mean',\n        mode='multimodal', \n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n        )\n        \n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n        \n        if return_logits:\n            return prediction_scores[:, :-1, :].contiguous()  \n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) \n            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            if reduction=='none':\n                lm_loss = lm_loss.view(prediction_scores.size(0),-1).sum(1)               \n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids, \n            \"attention_mask\": attention_mask, \n            \"past_key_values\": past,\n            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n", "extras/BLIP/models/blip_pretrain.py": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n'''\nfrom extras.BLIP.models.med import BertConfig, BertModel, BertLMHeadModel\nfrom transformers import BertTokenizer\nimport transformers\ntransformers.logging.set_verbosity_error()\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom extras.BLIP.models.blip import create_vit, init_tokenizer, load_checkpoint\n\nclass BLIP_Pretrain(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/bert_config.json',  \n                 image_size = 224,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,                    \n                 embed_dim = 256,     \n                 queue_size = 57600,\n                 momentum = 0.995,\n                 ):\n        \"\"\"\n        Args:\n            med_config (str): path for the mixture of encoder-decoder model's configuration file\n            image_size (int): input image size\n            vit (str): model size of vision transformer\n        \"\"\"               \n        super().__init__()\n        \n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer, 0)\n        \n        if vit=='base':\n            checkpoint = torch.hub.load_state_dict_from_url(\n                url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n                map_location=\"cpu\", check_hash=True)\n            state_dict = checkpoint[\"model\"]     \n            msg = self.visual_encoder.load_state_dict(state_dict,strict=False)\n        elif vit=='large':\n            from timm.models.helpers import load_custom_pretrained\n            from timm.models.vision_transformer import default_cfgs\n            load_custom_pretrained(self.visual_encoder,default_cfgs['vit_large_patch16_224_in21k'])        \n               \n        self.tokenizer = init_tokenizer()   \n        encoder_config = BertConfig.from_json_file(med_config)\n        encoder_config.encoder_width = vision_width\n        self.text_encoder = BertModel.from_pretrained('bert-base-uncased',config=encoder_config, add_pooling_layer=False)\n        self.text_encoder.resize_token_embeddings(len(self.tokenizer)) \n\n        text_width = self.text_encoder.config.hidden_size\n        \n        self.vision_proj = nn.Linear(vision_width, embed_dim)\n        self.text_proj = nn.Linear(text_width, embed_dim)\n\n        self.itm_head = nn.Linear(text_width, 2) \n        \n        # create momentum encoders  \n        self.visual_encoder_m, vision_width = create_vit(vit,image_size)              \n        self.vision_proj_m = nn.Linear(vision_width, embed_dim)\n        self.text_encoder_m = BertModel(config=encoder_config, add_pooling_layer=False)      \n        self.text_proj_m = nn.Linear(text_width, embed_dim)\n        \n        self.model_pairs = [[self.visual_encoder,self.visual_encoder_m],\n                            [self.vision_proj,self.vision_proj_m],\n                            [self.text_encoder,self.text_encoder_m],\n                            [self.text_proj,self.text_proj_m],\n                           ]       \n        self.copy_params()\n\n        # create the queue\n        self.register_buffer(\"image_queue\", torch.randn(embed_dim, queue_size))\n        self.register_buffer(\"text_queue\", torch.randn(embed_dim, queue_size))\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))  \n\n        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n        \n        self.queue_size = queue_size\n        self.momentum = momentum\n        self.temp = nn.Parameter(0.07*torch.ones([]))   \n        \n        # create the decoder\n        decoder_config = BertConfig.from_json_file(med_config)\n        decoder_config.encoder_width = vision_width        \n        self.text_decoder = BertLMHeadModel.from_pretrained('bert-base-uncased',config=decoder_config)    \n        self.text_decoder.resize_token_embeddings(len(self.tokenizer)) \n        tie_encoder_decoder_weights(self.text_encoder,self.text_decoder.bert,'','/attention')\n        \n        \n    def forward(self, image, caption, alpha):\n        with torch.no_grad():\n            self.temp.clamp_(0.001,0.5)\n        \n        image_embeds = self.visual_encoder(image) \n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)        \n        image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1)          \n        \n        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=30, \n                              return_tensors=\"pt\").to(image.device)  \n        text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                        return_dict = True, mode = 'text')            \n        text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1)                 \n             \n        # get momentum features\n        with torch.no_grad():\n            self._momentum_update()\n            image_embeds_m = self.visual_encoder_m(image) \n            image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:,0,:]),dim=-1)  \n            image_feat_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1)                   \n            \n            text_output_m = self.text_encoder_m(text.input_ids, attention_mask = text.attention_mask,                      \n                                                return_dict = True, mode = 'text')    \n            text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:,0,:]),dim=-1) \n            text_feat_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1)\n\n            sim_i2t_m = image_feat_m @ text_feat_all / self.temp  \n            sim_t2i_m = text_feat_m @ image_feat_all / self.temp \n\n            sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device)\n            sim_targets.fill_diagonal_(1)          \n\n            sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets\n            sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets        \n\n        sim_i2t = image_feat @ text_feat_all / self.temp\n        sim_t2i = text_feat @ image_feat_all / self.temp\n                             \n        loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean()\n        loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean() \n\n        loss_ita = (loss_i2t+loss_t2i)/2\n\n        self._dequeue_and_enqueue(image_feat_m, text_feat_m)        \n\n        ###============== Image-text Matching ===================###\n        encoder_input_ids = text.input_ids.clone()\n        encoder_input_ids[:,0] = self.tokenizer.enc_token_id\n        \n        # forward the positve image-text pair\n        bs = image.size(0)\n        output_pos = self.text_encoder(encoder_input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )            \n        with torch.no_grad():       \n            weights_t2i = F.softmax(sim_t2i[:,:bs],dim=1)+1e-4 \n            weights_t2i.fill_diagonal_(0)            \n            weights_i2t = F.softmax(sim_i2t[:,:bs],dim=1)+1e-4  \n            weights_i2t.fill_diagonal_(0)   \n            \n        # select a negative image for each text\n        image_embeds_neg = []    \n        for b in range(bs):\n            neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n            image_embeds_neg.append(image_embeds[neg_idx])\n        image_embeds_neg = torch.stack(image_embeds_neg,dim=0)   \n\n        # select a negative text for each image\n        text_ids_neg = []\n        text_atts_neg = []\n        for b in range(bs):\n            neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n            text_ids_neg.append(encoder_input_ids[neg_idx])\n            text_atts_neg.append(text.attention_mask[neg_idx])\n\n        text_ids_neg = torch.stack(text_ids_neg,dim=0)   \n        text_atts_neg = torch.stack(text_atts_neg,dim=0)      \n\n        text_ids_all = torch.cat([encoder_input_ids, text_ids_neg],dim=0)     \n        text_atts_all = torch.cat([text.attention_mask, text_atts_neg],dim=0)     \n\n        image_embeds_all = torch.cat([image_embeds_neg,image_embeds],dim=0)\n        image_atts_all = torch.cat([image_atts,image_atts],dim=0)\n\n        output_neg = self.text_encoder(text_ids_all,\n                                       attention_mask = text_atts_all,\n                                       encoder_hidden_states = image_embeds_all,\n                                       encoder_attention_mask = image_atts_all,      \n                                       return_dict = True,\n                                      )                            \n\n        vl_embeddings = torch.cat([output_pos.last_hidden_state[:,0,:], output_neg.last_hidden_state[:,0,:]],dim=0)\n        vl_output = self.itm_head(vl_embeddings)            \n\n        itm_labels = torch.cat([torch.ones(bs,dtype=torch.long),torch.zeros(2*bs,dtype=torch.long)],\n                               dim=0).to(image.device)\n        loss_itm = F.cross_entropy(vl_output, itm_labels)  \n        \n        ##================= LM ========================##     \n        decoder_input_ids = text.input_ids.clone()      \n        decoder_input_ids[:,0] = self.tokenizer.bos_token_id\n        decoder_targets = decoder_input_ids.masked_fill(decoder_input_ids == self.tokenizer.pad_token_id, -100) \n\n        decoder_output = self.text_decoder(decoder_input_ids, \n                                           attention_mask = text.attention_mask, \n                                           encoder_hidden_states = image_embeds,\n                                           encoder_attention_mask = image_atts,                  \n                                           labels = decoder_targets,\n                                           return_dict = True,   \n                                          )   \n          \n        loss_lm = decoder_output.loss                \n        return loss_ita, loss_itm, loss_lm\n \n\n\n    @torch.no_grad()    \n    def copy_params(self):\n        for model_pair in self.model_pairs:           \n            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n                param_m.data.copy_(param.data)  # initialize\n                param_m.requires_grad = False  # not update by gradient    \n\n            \n    @torch.no_grad()        \n    def _momentum_update(self):\n        for model_pair in self.model_pairs:           \n            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n                param_m.data = param_m.data * self.momentum + param.data * (1. - self.momentum)\n\n                        \n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, image_feat, text_feat):\n        # gather keys before updating queue\n        image_feats = concat_all_gather(image_feat)\n        text_feats = concat_all_gather(text_feat)\n\n        batch_size = image_feats.shape[0]\n\n        ptr = int(self.queue_ptr)\n        assert self.queue_size % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.image_queue[:, ptr:ptr + batch_size] = image_feats.T\n        self.text_queue[:, ptr:ptr + batch_size] = text_feats.T\n        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n\n        self.queue_ptr[0] = ptr \n\n\ndef blip_pretrain(**kwargs):\n    model = BLIP_Pretrain(**kwargs)\n    return model \n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    tensors_gather = [torch.ones_like(tensor)\n        for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output     \n\n\nfrom typing import List\ndef tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, skip_key:str):\n    uninitialized_encoder_weights: List[str] = []\n    if decoder.__class__ != encoder.__class__:\n        print(\n            f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\n        )\n\n    def tie_encoder_to_decoder_recursively(\n        decoder_pointer: nn.Module,\n        encoder_pointer: nn.Module,\n        module_name: str,\n        uninitialized_encoder_weights: List[str],\n        skip_key: str,\n        depth=0,\n    ):\n        assert isinstance(decoder_pointer, nn.Module) and isinstance(\n            encoder_pointer, nn.Module\n        ), f\"{decoder_pointer} and {encoder_pointer} have to be of type torch.nn.Module\"\n        if hasattr(decoder_pointer, \"weight\") and skip_key not in module_name:\n            assert hasattr(encoder_pointer, \"weight\")\n            encoder_pointer.weight = decoder_pointer.weight\n            if hasattr(decoder_pointer, \"bias\"):\n                assert hasattr(encoder_pointer, \"bias\")\n                encoder_pointer.bias = decoder_pointer.bias                \n            print(module_name+' is tied')    \n            return\n\n        encoder_modules = encoder_pointer._modules\n        decoder_modules = decoder_pointer._modules\n        if len(decoder_modules) > 0:\n            assert (\n                len(encoder_modules) > 0\n            ), f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n\n            all_encoder_weights = set([module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()])\n            encoder_layer_pos = 0\n            for name, module in decoder_modules.items():\n                if name.isdigit():\n                    encoder_name = str(int(name) + encoder_layer_pos)\n                    decoder_name = name\n                    if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n                        encoder_modules\n                    ) != len(decoder_modules):\n                        # this can happen if the name corresponds to the position in a list module list of layers\n                        # in this case the decoder has added a cross-attention that the encoder does not have\n                        # thus skip this step and subtract one layer pos from encoder\n                        encoder_layer_pos -= 1\n                        continue\n                elif name not in encoder_modules:\n                    continue\n                elif depth > 500:\n                    raise ValueError(\n                        \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is a circular dependency between two or more `nn.Modules` of your model.\"\n                    )\n                else:\n                    decoder_name = encoder_name = name\n                tie_encoder_to_decoder_recursively(\n                    decoder_modules[decoder_name],\n                    encoder_modules[encoder_name],\n                    module_name + \"/\" + name,\n                    uninitialized_encoder_weights,\n                    skip_key,\n                    depth=depth + 1,\n                )\n                all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n\n            uninitialized_encoder_weights += list(all_encoder_weights)\n\n    # tie weights recursively\n    tie_encoder_to_decoder_recursively(decoder, encoder, base_model_prefix, uninitialized_encoder_weights, skip_key)  \n", "extras/BLIP/models/vit.py": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on timm code base\n * https://github.com/rwightman/pytorch-image-models/tree/master/timm\n'''\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom timm.models.vision_transformer import _cfg, PatchEmbed\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.helpers import named_apply, adapt_input_conv\n\n\ndef checkpoint_wrapper(x):\n    return x\n\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.attn_gradients = None\n        self.attention_map = None\n        \n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n        \n    def get_attn_gradients(self):\n        return self.attn_gradients\n    \n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n        \n    def get_attention_map(self):\n        return self.attention_map\n    \n    def forward(self, x, register_hook=False):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n                \n        if register_hook:\n            self.save_attention_map(attn)\n            attn.register_hook(self.save_attn_gradients)        \n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_grad_checkpointing=False):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if use_grad_checkpointing:\n            self.attn = checkpoint_wrapper(self.attn)\n            self.mlp = checkpoint_wrapper(self.mlp)\n\n    def forward(self, x, register_hook=False):\n        x = x + self.drop_path(self.attn(self.norm1(x), register_hook=register_hook))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n    \nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n        https://arxiv.org/abs/2010.11929\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None, \n                 use_grad_checkpointing=False, ckpt_layer=0):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer\n        \"\"\"\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                use_grad_checkpointing=(use_grad_checkpointing and i>=depth-ckpt_layer)\n            )\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward(self, x, register_blk=-1):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n  \n        x = x + self.pos_embed[:,:x.size(1),:]\n        x = self.pos_drop(x)\n\n        for i,blk in enumerate(self.blocks):\n            x = blk(x, register_blk==i)\n        x = self.norm(x)\n        \n        return x\n\n    @torch.jit.ignore()\n    def load_pretrained(self, checkpoint_path, prefix=''):\n        _load_weights(self, checkpoint_path, prefix)\n        \n\n@torch.no_grad()\ndef _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n    \"\"\"\n    import numpy as np\n\n    def _n2p(w, t=True):\n        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n            w = w.flatten()\n        if t:\n            if w.ndim == 4:\n                w = w.transpose([3, 2, 0, 1])\n            elif w.ndim == 3:\n                w = w.transpose([2, 0, 1])\n            elif w.ndim == 2:\n                w = w.transpose([1, 0])\n        return torch.from_numpy(w)\n\n    w = np.load(checkpoint_path)\n    if not prefix and 'opt/target/embedding/kernel' in w:\n        prefix = 'opt/target/'\n\n    if hasattr(model.patch_embed, 'backbone'):\n        # hybrid\n        backbone = model.patch_embed.backbone\n        stem_only = not hasattr(backbone, 'stem')\n        stem = backbone if stem_only else backbone.stem\n        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n        if not stem_only:\n            for i, stage in enumerate(backbone.stages):\n                for j, block in enumerate(stage.blocks):\n                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n                    for r in range(3):\n                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n                    if block.downsample is not None:\n                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n    else:\n        embed_conv_w = adapt_input_conv(\n            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n    model.patch_embed.proj.weight.copy_(embed_conv_w)\n    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n    if pos_embed_w.shape != model.pos_embed.shape:\n        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n    model.pos_embed.copy_(pos_embed_w)\n    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n#     if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n#         model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n#         model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n#     if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n#         model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n#         model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n    for i, block in enumerate(model.blocks.children()):\n        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n        block.attn.qkv.weight.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n        block.attn.qkv.bias.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n        for r in range(2):\n            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n\n            \ndef interpolate_pos_embed(pos_embed_checkpoint, visual_encoder):        \n    # interpolate position embedding\n    embedding_size = pos_embed_checkpoint.shape[-1]\n    num_patches = visual_encoder.patch_embed.num_patches\n    num_extra_tokens = visual_encoder.pos_embed.shape[-2] - num_patches\n    # height (== width) for the checkpoint position embedding\n    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n    # height (== width) for the new position embedding\n    new_size = int(num_patches ** 0.5)\n\n    if orig_size!=new_size:\n        # class_token and dist_token are kept unchanged\n        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n        # only the position tokens are interpolated\n        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n        pos_tokens = torch.nn.functional.interpolate(\n            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n        print('reshape position embedding from %d to %d'%(orig_size ** 2,new_size ** 2))\n        \n        return new_pos_embed    \n    else:\n        return pos_embed_checkpoint", "extras/safety_checker/models/safety_checker.py": "# from https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py\n\n# Copyright 2024 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\nfrom transformers.utils import logging\n\nlogger = logging.get_logger(__name__)\n\n\ndef cosine_distance(image_embeds, text_embeds):\n    normalized_image_embeds = nn.functional.normalize(image_embeds)\n    normalized_text_embeds = nn.functional.normalize(text_embeds)\n    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())\n\n\nclass StableDiffusionSafetyChecker(PreTrainedModel):\n    config_class = CLIPConfig\n    main_input_name = \"clip_input\"\n\n    _no_split_modules = [\"CLIPEncoderLayer\"]\n\n    def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)\n\n    @torch.no_grad()\n    def forward(self, clip_input, images):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\n\n        result = []\n        batch_size = image_embeds.shape[0]\n        for i in range(batch_size):\n            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n\n            # increase this value to create a stronger `nfsw` filter\n            # at the cost of increasing the possibility of filtering benign images\n            adjustment = 0.0\n\n            for concept_idx in range(len(special_cos_dist[0])):\n                concept_cos = special_cos_dist[i][concept_idx]\n                concept_threshold = self.special_care_embeds_weights[concept_idx].item()\n                result_img[\"special_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"special_scores\"][concept_idx] > 0:\n                    result_img[\"special_care\"].append({concept_idx, result_img[\"special_scores\"][concept_idx]})\n                    adjustment = 0.01\n\n            for concept_idx in range(len(cos_dist[0])):\n                concept_cos = cos_dist[i][concept_idx]\n                concept_threshold = self.concept_embeds_weights[concept_idx].item()\n                result_img[\"concept_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"concept_scores\"][concept_idx] > 0:\n                    result_img[\"bad_concepts\"].append(concept_idx)\n\n            result.append(result_img)\n\n        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                if torch.is_tensor(images) or torch.is_tensor(images[0]):\n                    images[idx] = torch.zeros_like(images[idx])  # black image\n                else:\n                    images[idx] = np.zeros(images[idx].shape)  # black image\n\n        if any(has_nsfw_concepts):\n            logger.warning(\n                \"Potential NSFW content was detected in one or more images. A black image will be returned instead.\"\n                \" Try again with a different prompt and/or seed.\"\n            )\n\n        return images, has_nsfw_concepts\n\n    @torch.no_grad()\n    def forward_onnx(self, clip_input: torch.Tensor, images: torch.Tensor):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds)\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds)\n\n        # increase this value to create a stronger `nsfw` filter\n        # at the cost of increasing the possibility of filtering benign images\n        adjustment = 0.0\n\n        special_scores = special_cos_dist - self.special_care_embeds_weights + adjustment\n        # special_scores = special_scores.round(decimals=3)\n        special_care = torch.any(special_scores > 0, dim=1)\n        special_adjustment = special_care * 0.01\n        special_adjustment = special_adjustment.unsqueeze(1).expand(-1, cos_dist.shape[1])\n\n        concept_scores = (cos_dist - self.concept_embeds_weights) + special_adjustment\n        # concept_scores = concept_scores.round(decimals=3)\n        has_nsfw_concepts = torch.any(concept_scores > 0, dim=1)\n\n        images[has_nsfw_concepts] = 0.0  # black image\n\n        return images, has_nsfw_concepts\n", "extras/facexlib/parsing/resnet.py": "import torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_chan),\n            )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = F.relu(self.bn1(residual))\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum - 1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass ResNet18(nn.Module):\n\n    def __init__(self):\n        super(ResNet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x)  # 1/8\n        feat16 = self.layer3(feat8)  # 1/16\n        feat32 = self.layer4(feat16)  # 1/32\n        return feat8, feat16, feat32\n", "extras/facexlib/parsing/__init__.py": "import torch\n\nfrom extras.facexlib.utils import load_file_from_url\nfrom .bisenet import BiSeNet\nfrom .parsenet import ParseNet\n\n\ndef init_parsing_model(model_name='bisenet', half=False, device='cuda', model_rootpath=None):\n    if model_name == 'bisenet':\n        model = BiSeNet(num_class=19)\n        model_url = 'https://github.com/xinntao/facexlib/releases/download/v0.2.0/parsing_bisenet.pth'\n    elif model_name == 'parsenet':\n        model = ParseNet(in_size=512, out_size=512, parsing_ch=19)\n        model_url = 'https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth'\n    else:\n        raise NotImplementedError(f'{model_name} is not implemented.')\n\n    model_path = load_file_from_url(\n        url=model_url, model_dir='facexlib/weights', progress=True, file_name=None, save_dir=model_rootpath)\n    load_net = torch.load(model_path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(load_net, strict=True)\n    model.eval()\n    model = model.to(device)\n    return model\n", "extras/facexlib/parsing/bisenet.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import ResNet18\n\n\nclass ConvBNReLU(nn.Module):\n\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size=ks, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_chan)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\n\n\nclass BiSeNetOutput(nn.Module):\n\n    def __init__(self, in_chan, mid_chan, num_class):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, num_class, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        feat = self.conv(x)\n        out = self.conv_out(feat)\n        return out, feat\n\n\nclass AttentionRefinementModule(nn.Module):\n\n    def __init__(self, in_chan, out_chan):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size=1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n\nclass ContextPath(nn.Module):\n\n    def __init__(self):\n        super(ContextPath, self).__init__()\n        self.resnet = ResNet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n    def forward(self, x):\n        feat8, feat16, feat32 = self.resnet(x)\n        h8, w8 = feat8.size()[2:]\n        h16, w16 = feat16.size()[2:]\n        h32, w32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (h32, w32), mode='nearest')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (h16, w16), mode='nearest')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (h8, w8), mode='nearest')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\n\nclass FeatureFusionModule(nn.Module):\n\n    def __init__(self, in_chan, out_chan):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan, out_chan // 4, kernel_size=1, stride=1, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(out_chan // 4, out_chan, kernel_size=1, stride=1, padding=0, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n\nclass BiSeNet(nn.Module):\n\n    def __init__(self, num_class):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, num_class)\n        self.conv_out16 = BiSeNetOutput(128, 64, num_class)\n        self.conv_out32 = BiSeNetOutput(128, 64, num_class)\n\n    def forward(self, x, return_feat=False):\n        h, w = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # return res3b1 feature\n        feat_sp = feat_res8  # replace spatial path feature with res3b1 feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        out, feat = self.conv_out(feat_fuse)\n        out16, feat16 = self.conv_out16(feat_cp8)\n        out32, feat32 = self.conv_out32(feat_cp16)\n\n        out = F.interpolate(out, (h, w), mode='bilinear', align_corners=True)\n        out16 = F.interpolate(out16, (h, w), mode='bilinear', align_corners=True)\n        out32 = F.interpolate(out32, (h, w), mode='bilinear', align_corners=True)\n\n        if return_feat:\n            feat = F.interpolate(feat, (h, w), mode='bilinear', align_corners=True)\n            feat16 = F.interpolate(feat16, (h, w), mode='bilinear', align_corners=True)\n            feat32 = F.interpolate(feat32, (h, w), mode='bilinear', align_corners=True)\n            return out, out16, out32, feat, feat16, feat32\n        else:\n            return out, out16, out32\n", "extras/facexlib/parsing/parsenet.py": "\"\"\"Modified from https://github.com/chaofengc/PSFRGAN\n\"\"\"\nimport numpy as np\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass NormLayer(nn.Module):\n    \"\"\"Normalization Layers.\n\n    Args:\n        channels: input channels, for batch norm and instance norm.\n        input_size: input shape without batch size, for layer norm.\n    \"\"\"\n\n    def __init__(self, channels, normalize_shape=None, norm_type='bn'):\n        super(NormLayer, self).__init__()\n        norm_type = norm_type.lower()\n        self.norm_type = norm_type\n        if norm_type == 'bn':\n            self.norm = nn.BatchNorm2d(channels, affine=True)\n        elif norm_type == 'in':\n            self.norm = nn.InstanceNorm2d(channels, affine=False)\n        elif norm_type == 'gn':\n            self.norm = nn.GroupNorm(32, channels, affine=True)\n        elif norm_type == 'pixel':\n            self.norm = lambda x: F.normalize(x, p=2, dim=1)\n        elif norm_type == 'layer':\n            self.norm = nn.LayerNorm(normalize_shape)\n        elif norm_type == 'none':\n            self.norm = lambda x: x * 1.0\n        else:\n            assert 1 == 0, f'Norm type {norm_type} not support.'\n\n    def forward(self, x, ref=None):\n        if self.norm_type == 'spade':\n            return self.norm(x, ref)\n        else:\n            return self.norm(x)\n\n\nclass ReluLayer(nn.Module):\n    \"\"\"Relu Layer.\n\n    Args:\n        relu type: type of relu layer, candidates are\n            - ReLU\n            - LeakyReLU: default relu slope 0.2\n            - PRelu\n            - SELU\n            - none: direct pass\n    \"\"\"\n\n    def __init__(self, channels, relu_type='relu'):\n        super(ReluLayer, self).__init__()\n        relu_type = relu_type.lower()\n        if relu_type == 'relu':\n            self.func = nn.ReLU(True)\n        elif relu_type == 'leakyrelu':\n            self.func = nn.LeakyReLU(0.2, inplace=True)\n        elif relu_type == 'prelu':\n            self.func = nn.PReLU(channels)\n        elif relu_type == 'selu':\n            self.func = nn.SELU(True)\n        elif relu_type == 'none':\n            self.func = lambda x: x * 1.0\n        else:\n            assert 1 == 0, f'Relu type {relu_type} not support.'\n\n    def forward(self, x):\n        return self.func(x)\n\n\nclass ConvLayer(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 scale='none',\n                 norm_type='none',\n                 relu_type='none',\n                 use_pad=True,\n                 bias=True):\n        super(ConvLayer, self).__init__()\n        self.use_pad = use_pad\n        self.norm_type = norm_type\n        if norm_type in ['bn']:\n            bias = False\n\n        stride = 2 if scale == 'down' else 1\n\n        self.scale_func = lambda x: x\n        if scale == 'up':\n            self.scale_func = lambda x: nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n\n        self.reflection_pad = nn.ReflectionPad2d(int(np.ceil((kernel_size - 1.) / 2)))\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=bias)\n\n        self.relu = ReluLayer(out_channels, relu_type)\n        self.norm = NormLayer(out_channels, norm_type=norm_type)\n\n    def forward(self, x):\n        out = self.scale_func(x)\n        if self.use_pad:\n            out = self.reflection_pad(out)\n        out = self.conv2d(out)\n        out = self.norm(out)\n        out = self.relu(out)\n        return out\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    Residual block recommended in: http://torch.ch/blog/2016/02/04/resnets.html\n    \"\"\"\n\n    def __init__(self, c_in, c_out, relu_type='prelu', norm_type='bn', scale='none'):\n        super(ResidualBlock, self).__init__()\n\n        if scale == 'none' and c_in == c_out:\n            self.shortcut_func = lambda x: x\n        else:\n            self.shortcut_func = ConvLayer(c_in, c_out, 3, scale)\n\n        scale_config_dict = {'down': ['none', 'down'], 'up': ['up', 'none'], 'none': ['none', 'none']}\n        scale_conf = scale_config_dict[scale]\n\n        self.conv1 = ConvLayer(c_in, c_out, 3, scale_conf[0], norm_type=norm_type, relu_type=relu_type)\n        self.conv2 = ConvLayer(c_out, c_out, 3, scale_conf[1], norm_type=norm_type, relu_type='none')\n\n    def forward(self, x):\n        identity = self.shortcut_func(x)\n\n        res = self.conv1(x)\n        res = self.conv2(res)\n        return identity + res\n\n\nclass ParseNet(nn.Module):\n\n    def __init__(self,\n                 in_size=128,\n                 out_size=128,\n                 min_feat_size=32,\n                 base_ch=64,\n                 parsing_ch=19,\n                 res_depth=10,\n                 relu_type='LeakyReLU',\n                 norm_type='bn',\n                 ch_range=[32, 256]):\n        super().__init__()\n        self.res_depth = res_depth\n        act_args = {'norm_type': norm_type, 'relu_type': relu_type}\n        min_ch, max_ch = ch_range\n\n        ch_clip = lambda x: max(min_ch, min(x, max_ch))  # noqa: E731\n        min_feat_size = min(in_size, min_feat_size)\n\n        down_steps = int(np.log2(in_size // min_feat_size))\n        up_steps = int(np.log2(out_size // min_feat_size))\n\n        # =============== define encoder-body-decoder ====================\n        self.encoder = []\n        self.encoder.append(ConvLayer(3, base_ch, 3, 1))\n        head_ch = base_ch\n        for i in range(down_steps):\n            cin, cout = ch_clip(head_ch), ch_clip(head_ch * 2)\n            self.encoder.append(ResidualBlock(cin, cout, scale='down', **act_args))\n            head_ch = head_ch * 2\n\n        self.body = []\n        for i in range(res_depth):\n            self.body.append(ResidualBlock(ch_clip(head_ch), ch_clip(head_ch), **act_args))\n\n        self.decoder = []\n        for i in range(up_steps):\n            cin, cout = ch_clip(head_ch), ch_clip(head_ch // 2)\n            self.decoder.append(ResidualBlock(cin, cout, scale='up', **act_args))\n            head_ch = head_ch // 2\n\n        self.encoder = nn.Sequential(*self.encoder)\n        self.body = nn.Sequential(*self.body)\n        self.decoder = nn.Sequential(*self.decoder)\n        self.out_img_conv = ConvLayer(ch_clip(head_ch), 3)\n        self.out_mask_conv = ConvLayer(ch_clip(head_ch), parsing_ch)\n\n    def forward(self, x):\n        feat = self.encoder(x)\n        x = feat + self.body(feat)\n        x = self.decoder(x)\n        out_img = self.out_img_conv(x)\n        out_mask = self.out_mask_conv(x)\n        return out_mask, out_img\n", "extras/facexlib/utils/face_utils.py": "import cv2\nimport numpy as np\nimport torch\n\n\ndef compute_increased_bbox(bbox, increase_area, preserve_aspect=True):\n    left, top, right, bot = bbox\n    width = right - left\n    height = bot - top\n\n    if preserve_aspect:\n        width_increase = max(increase_area, ((1 + 2 * increase_area) * height - width) / (2 * width))\n        height_increase = max(increase_area, ((1 + 2 * increase_area) * width - height) / (2 * height))\n    else:\n        width_increase = height_increase = increase_area\n    left = int(left - width_increase * width)\n    top = int(top - height_increase * height)\n    right = int(right + width_increase * width)\n    bot = int(bot + height_increase * height)\n    return (left, top, right, bot)\n\n\ndef get_valid_bboxes(bboxes, h, w):\n    left = max(bboxes[0], 0)\n    top = max(bboxes[1], 0)\n    right = min(bboxes[2], w)\n    bottom = min(bboxes[3], h)\n    return (left, top, right, bottom)\n\n\ndef align_crop_face_landmarks(img,\n                              landmarks,\n                              output_size,\n                              transform_size=None,\n                              enable_padding=True,\n                              return_inverse_affine=False,\n                              shrink_ratio=(1, 1)):\n    \"\"\"Align and crop face with landmarks.\n\n    The output_size and transform_size are based on width. The height is\n    adjusted based on shrink_ratio_h/shring_ration_w.\n\n    Modified from:\n    https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n\n    Args:\n        img (Numpy array): Input image.\n        landmarks (Numpy array): 5 or 68 or 98 landmarks.\n        output_size (int): Output face size.\n        transform_size (ing): Transform size. Usually the four time of\n            output_size.\n        enable_padding (float): Default: True.\n        shrink_ratio (float | tuple[float] | list[float]): Shring the whole\n            face for height and width (crop larger area). Default: (1, 1).\n\n    Returns:\n        (Numpy array): Cropped face.\n    \"\"\"\n    lm_type = 'retinaface_5'  # Options: dlib_5, retinaface_5\n\n    if isinstance(shrink_ratio, (float, int)):\n        shrink_ratio = (shrink_ratio, shrink_ratio)\n    if transform_size is None:\n        transform_size = output_size * 4\n\n    # Parse landmarks\n    lm = np.array(landmarks)\n    if lm.shape[0] == 5 and lm_type == 'retinaface_5':\n        eye_left = lm[0]\n        eye_right = lm[1]\n        mouth_avg = (lm[3] + lm[4]) * 0.5\n    elif lm.shape[0] == 5 and lm_type == 'dlib_5':\n        lm_eye_left = lm[2:4]\n        lm_eye_right = lm[0:2]\n        eye_left = np.mean(lm_eye_left, axis=0)\n        eye_right = np.mean(lm_eye_right, axis=0)\n        mouth_avg = lm[4]\n    elif lm.shape[0] == 68:\n        lm_eye_left = lm[36:42]\n        lm_eye_right = lm[42:48]\n        eye_left = np.mean(lm_eye_left, axis=0)\n        eye_right = np.mean(lm_eye_right, axis=0)\n        mouth_avg = (lm[48] + lm[54]) * 0.5\n    elif lm.shape[0] == 98:\n        lm_eye_left = lm[60:68]\n        lm_eye_right = lm[68:76]\n        eye_left = np.mean(lm_eye_left, axis=0)\n        eye_right = np.mean(lm_eye_right, axis=0)\n        mouth_avg = (lm[76] + lm[82]) * 0.5\n\n    eye_avg = (eye_left + eye_right) * 0.5\n    eye_to_eye = eye_right - eye_left\n    eye_to_mouth = mouth_avg - eye_avg\n\n    # Get the oriented crop rectangle\n    # x: half width of the oriented crop rectangle\n    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n    #  - np.flipud(eye_to_mouth) * [-1, 1]: rotate 90 clockwise\n    # norm with the hypotenuse: get the direction\n    x /= np.hypot(*x)  # get the hypotenuse of a right triangle\n    rect_scale = 1  # TODO: you can edit it to get larger rect\n    x *= max(np.hypot(*eye_to_eye) * 2.0 * rect_scale, np.hypot(*eye_to_mouth) * 1.8 * rect_scale)\n    # y: half height of the oriented crop rectangle\n    y = np.flipud(x) * [-1, 1]\n\n    x *= shrink_ratio[1]  # width\n    y *= shrink_ratio[0]  # height\n\n    # c: center\n    c = eye_avg + eye_to_mouth * 0.1\n    # quad: (left_top, left_bottom, right_bottom, right_top)\n    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n    # qsize: side length of the square\n    qsize = np.hypot(*x) * 2\n\n    quad_ori = np.copy(quad)\n    # Shrink, for large face\n    # TODO: do we really need shrink\n    shrink = int(np.floor(qsize / output_size * 0.5))\n    if shrink > 1:\n        h, w = img.shape[0:2]\n        rsize = (int(np.rint(float(w) / shrink)), int(np.rint(float(h) / shrink)))\n        img = cv2.resize(img, rsize, interpolation=cv2.INTER_AREA)\n        quad /= shrink\n        qsize /= shrink\n\n    # Crop\n    h, w = img.shape[0:2]\n    border = max(int(np.rint(qsize * 0.1)), 3)\n    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n            int(np.ceil(max(quad[:, 1]))))\n    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, w), min(crop[3] + border, h))\n    if crop[2] - crop[0] < w or crop[3] - crop[1] < h:\n        img = img[crop[1]:crop[3], crop[0]:crop[2], :]\n        quad -= crop[0:2]\n\n    # Pad\n    # pad: (width_left, height_top, width_right, height_bottom)\n    h, w = img.shape[0:2]\n    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n           int(np.ceil(max(quad[:, 1]))))\n    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - w + border, 0), max(pad[3] - h + border, 0))\n    if enable_padding and max(pad) > border - 4:\n        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n        img = np.pad(img, ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n        h, w = img.shape[0:2]\n        y, x, _ = np.ogrid[:h, :w, :1]\n        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n                                           np.float32(w - 1 - x) / pad[2]),\n                          1.0 - np.minimum(np.float32(y) / pad[1],\n                                           np.float32(h - 1 - y) / pad[3]))\n        blur = int(qsize * 0.02)\n        if blur % 2 == 0:\n            blur += 1\n        blur_img = cv2.boxFilter(img, 0, ksize=(blur, blur))\n\n        img = img.astype('float32')\n        img += (blur_img - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n        img = np.clip(img, 0, 255)  # float32, [0, 255]\n        quad += pad[:2]\n\n    # Transform use cv2\n    h_ratio = shrink_ratio[0] / shrink_ratio[1]\n    dst_h, dst_w = int(transform_size * h_ratio), transform_size\n    template = np.array([[0, 0], [0, dst_h], [dst_w, dst_h], [dst_w, 0]])\n    # use cv2.LMEDS method for the equivalence to skimage transform\n    # ref: https://blog.csdn.net/yichxi/article/details/115827338\n    affine_matrix = cv2.estimateAffinePartial2D(quad, template, method=cv2.LMEDS)[0]\n    cropped_face = cv2.warpAffine(\n        img, affine_matrix, (dst_w, dst_h), borderMode=cv2.BORDER_CONSTANT, borderValue=(135, 133, 132))  # gray\n\n    if output_size < transform_size:\n        cropped_face = cv2.resize(\n            cropped_face, (output_size, int(output_size * h_ratio)), interpolation=cv2.INTER_LINEAR)\n\n    if return_inverse_affine:\n        dst_h, dst_w = int(output_size * h_ratio), output_size\n        template = np.array([[0, 0], [0, dst_h], [dst_w, dst_h], [dst_w, 0]])\n        # use cv2.LMEDS method for the equivalence to skimage transform\n        # ref: https://blog.csdn.net/yichxi/article/details/115827338\n        affine_matrix = cv2.estimateAffinePartial2D(\n            quad_ori, np.array([[0, 0], [0, output_size], [dst_w, dst_h], [dst_w, 0]]), method=cv2.LMEDS)[0]\n        inverse_affine = cv2.invertAffineTransform(affine_matrix)\n    else:\n        inverse_affine = None\n    return cropped_face, inverse_affine\n\n\ndef paste_face_back(img, face, inverse_affine):\n    h, w = img.shape[0:2]\n    face_h, face_w = face.shape[0:2]\n    inv_restored = cv2.warpAffine(face, inverse_affine, (w, h))\n    mask = np.ones((face_h, face_w, 3), dtype=np.float32)\n    inv_mask = cv2.warpAffine(mask, inverse_affine, (w, h))\n    # remove the black borders\n    inv_mask_erosion = cv2.erode(inv_mask, np.ones((2, 2), np.uint8))\n    inv_restored_remove_border = inv_mask_erosion * inv_restored\n    total_face_area = np.sum(inv_mask_erosion) // 3\n    # compute the fusion edge based on the area of face\n    w_edge = int(total_face_area**0.5) // 20\n    erosion_radius = w_edge * 2\n    inv_mask_center = cv2.erode(inv_mask_erosion, np.ones((erosion_radius, erosion_radius), np.uint8))\n    blur_size = w_edge * 2\n    inv_soft_mask = cv2.GaussianBlur(inv_mask_center, (blur_size + 1, blur_size + 1), 0)\n    img = inv_soft_mask * inv_restored_remove_border + (1 - inv_soft_mask) * img\n    # float32, [0, 255]\n    return img\n\n\nif __name__ == '__main__':\n    import os\n\n    from extras.facexlib.detection import init_detection_model\n    from extras.facexlib.utils.face_restoration_helper import get_largest_face\n    from extras.facexlib.visualization import visualize_detection\n\n    img_path = '/home/wxt/datasets/ffhq/ffhq_wild/00009.png'\n    img_name = os.splitext(os.path.basename(img_path))[0]\n\n    # initialize model\n    det_net = init_detection_model('retinaface_resnet50', half=False)\n    img_ori = cv2.imread(img_path)\n    h, w = img_ori.shape[0:2]\n    # if larger than 800, scale it\n    scale = max(h / 800, w / 800)\n    if scale > 1:\n        img = cv2.resize(img_ori, (int(w / scale), int(h / scale)), interpolation=cv2.INTER_LINEAR)\n\n    with torch.no_grad():\n        bboxes = det_net.detect_faces(img, 0.97)\n    if scale > 1:\n        bboxes *= scale  # the score is incorrect\n    bboxes = get_largest_face(bboxes, h, w)[0]\n    visualize_detection(img_ori, [bboxes], f'tmp/{img_name}_det.png')\n\n    landmarks = np.array([[bboxes[i], bboxes[i + 1]] for i in range(5, 15, 2)])\n\n    cropped_face, inverse_affine = align_crop_face_landmarks(\n        img_ori,\n        landmarks,\n        output_size=512,\n        transform_size=None,\n        enable_padding=True,\n        return_inverse_affine=True,\n        shrink_ratio=(1, 1))\n\n    cv2.imwrite(f'tmp/{img_name}_cropeed_face.png', cropped_face)\n    img = paste_face_back(img_ori, cropped_face, inverse_affine)\n    cv2.imwrite(f'tmp/{img_name}_back.png', img)\n", "extras/facexlib/utils/misc.py": "import cv2\nimport os\nimport os.path as osp\nimport torch\nfrom torch.hub import download_url_to_file, get_dir\nfrom urllib.parse import urlparse\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\ndef imwrite(img, file_path, params=None, auto_mkdir=True):\n    \"\"\"Write image to file.\n\n    Args:\n        img (ndarray): Image array to be written.\n        file_path (str): Image file path.\n        params (None or list): Same as opencv's :func:`imwrite` interface.\n        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n            whether to create it automatically.\n\n    Returns:\n        bool: Successful or not.\n    \"\"\"\n    if auto_mkdir:\n        dir_name = os.path.abspath(os.path.dirname(file_path))\n        os.makedirs(dir_name, exist_ok=True)\n    return cv2.imwrite(file_path, img, params)\n\n\ndef img2tensor(imgs, bgr2rgb=True, float32=True):\n    \"\"\"Numpy array to tensor.\n\n    Args:\n        imgs (list[ndarray] | ndarray): Input images.\n        bgr2rgb (bool): Whether to change bgr to rgb.\n        float32 (bool): Whether to change to float32.\n\n    Returns:\n        list[tensor] | tensor: Tensor images. If returned results only have\n            one element, just return tensor.\n    \"\"\"\n\n    def _totensor(img, bgr2rgb, float32):\n        if img.shape[2] == 3 and bgr2rgb:\n            if img.dtype == 'float64':\n                img = img.astype('float32')\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = torch.from_numpy(img.transpose(2, 0, 1))\n        if float32:\n            img = img.float()\n        return img\n\n    if isinstance(imgs, list):\n        return [_totensor(img, bgr2rgb, float32) for img in imgs]\n    else:\n        return _totensor(imgs, bgr2rgb, float32)\n\n\ndef load_file_from_url(url, model_dir=None, progress=True, file_name=None, save_dir=None):\n    \"\"\"Ref:https://github.com/1adrianb/face-alignment/blob/master/face_alignment/utils.py\n    \"\"\"\n    if model_dir is None:\n        hub_dir = get_dir()\n        model_dir = os.path.join(hub_dir, 'checkpoints')\n\n    if save_dir is None:\n        save_dir = os.path.join(ROOT_DIR, model_dir)\n    os.makedirs(save_dir, exist_ok=True)\n\n    parts = urlparse(url)\n    filename = os.path.basename(parts.path)\n    if file_name is not None:\n        filename = file_name\n    cached_file = os.path.abspath(os.path.join(save_dir, filename))\n    if not os.path.exists(cached_file):\n        print(f'Downloading: \"{url}\" to {cached_file}\\n')\n        download_url_to_file(url, cached_file, hash_prefix=None, progress=progress)\n    return cached_file\n\n\ndef scandir(dir_path, suffix=None, recursive=False, full_path=False):\n    \"\"\"Scan a directory to find the interested files.\n    Args:\n        dir_path (str): Path of the directory.\n        suffix (str | tuple(str), optional): File suffix that we are\n            interested in. Default: None.\n        recursive (bool, optional): If set to True, recursively scan the\n            directory. Default: False.\n        full_path (bool, optional): If set to True, include the dir_path.\n            Default: False.\n    Returns:\n        A generator for all the interested files with relative paths.\n    \"\"\"\n\n    if (suffix is not None) and not isinstance(suffix, (str, tuple)):\n        raise TypeError('\"suffix\" must be a string or tuple of strings')\n\n    root = dir_path\n\n    def _scandir(dir_path, suffix, recursive):\n        for entry in os.scandir(dir_path):\n            if not entry.name.startswith('.') and entry.is_file():\n                if full_path:\n                    return_path = entry.path\n                else:\n                    return_path = osp.relpath(entry.path, root)\n\n                if suffix is None:\n                    yield return_path\n                elif return_path.endswith(suffix):\n                    yield return_path\n            else:\n                if recursive:\n                    yield from _scandir(entry.path, suffix=suffix, recursive=recursive)\n                else:\n                    continue\n\n    return _scandir(dir_path, suffix=suffix, recursive=recursive)\n", "extras/facexlib/utils/face_restoration_helper.py": "import cv2\nimport numpy as np\nimport os\nimport torch\nfrom torchvision.transforms.functional import normalize\n\nfrom extras.facexlib.detection import init_detection_model\nfrom extras.facexlib.parsing import init_parsing_model\nfrom extras.facexlib.utils.misc import img2tensor, imwrite\n\n\ndef get_largest_face(det_faces, h, w):\n\n    def get_location(val, length):\n        if val < 0:\n            return 0\n        elif val > length:\n            return length\n        else:\n            return val\n\n    face_areas = []\n    for det_face in det_faces:\n        left = get_location(det_face[0], w)\n        right = get_location(det_face[2], w)\n        top = get_location(det_face[1], h)\n        bottom = get_location(det_face[3], h)\n        face_area = (right - left) * (bottom - top)\n        face_areas.append(face_area)\n    largest_idx = face_areas.index(max(face_areas))\n    return det_faces[largest_idx], largest_idx\n\n\ndef get_center_face(det_faces, h=0, w=0, center=None):\n    if center is not None:\n        center = np.array(center)\n    else:\n        center = np.array([w / 2, h / 2])\n    center_dist = []\n    for det_face in det_faces:\n        face_center = np.array([(det_face[0] + det_face[2]) / 2, (det_face[1] + det_face[3]) / 2])\n        dist = np.linalg.norm(face_center - center)\n        center_dist.append(dist)\n    center_idx = center_dist.index(min(center_dist))\n    return det_faces[center_idx], center_idx\n\n\nclass FaceRestoreHelper(object):\n    \"\"\"Helper for the face restoration pipeline (base class).\"\"\"\n\n    def __init__(self,\n                 upscale_factor,\n                 face_size=512,\n                 crop_ratio=(1, 1),\n                 det_model='retinaface_resnet50',\n                 save_ext='png',\n                 template_3points=False,\n                 pad_blur=False,\n                 use_parse=False,\n                 device=None,\n                 model_rootpath=None):\n        self.template_3points = template_3points  # improve robustness\n        self.upscale_factor = upscale_factor\n        # the cropped face ratio based on the square face\n        self.crop_ratio = crop_ratio  # (h, w)\n        assert (self.crop_ratio[0] >= 1 and self.crop_ratio[1] >= 1), 'crop ration only supports >=1'\n        self.face_size = (int(face_size * self.crop_ratio[1]), int(face_size * self.crop_ratio[0]))\n\n        if self.template_3points:\n            self.face_template = np.array([[192, 240], [319, 240], [257, 371]])\n        else:\n            # standard 5 landmarks for FFHQ faces with 512 x 512\n            self.face_template = np.array([[192.98138, 239.94708], [318.90277, 240.1936], [256.63416, 314.01935],\n                                           [201.26117, 371.41043], [313.08905, 371.15118]])\n        self.face_template = self.face_template * (face_size / 512.0)\n        if self.crop_ratio[0] > 1:\n            self.face_template[:, 1] += face_size * (self.crop_ratio[0] - 1) / 2\n        if self.crop_ratio[1] > 1:\n            self.face_template[:, 0] += face_size * (self.crop_ratio[1] - 1) / 2\n        self.save_ext = save_ext\n        self.pad_blur = pad_blur\n        if self.pad_blur is True:\n            self.template_3points = False\n\n        self.all_landmarks_5 = []\n        self.det_faces = []\n        self.affine_matrices = []\n        self.inverse_affine_matrices = []\n        self.cropped_faces = []\n        self.restored_faces = []\n        self.pad_input_imgs = []\n\n        if device is None:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            self.device = device\n\n        # init face detection model\n        self.face_det = init_detection_model(det_model, half=False, device=self.device, model_rootpath=model_rootpath)\n\n        # init face parsing model\n        self.use_parse = use_parse\n        self.face_parse = init_parsing_model(model_name='parsenet', device=self.device, model_rootpath=model_rootpath)\n\n    def set_upscale_factor(self, upscale_factor):\n        self.upscale_factor = upscale_factor\n\n    def read_image(self, img):\n        \"\"\"img can be image path or cv2 loaded image.\"\"\"\n        # self.input_img is Numpy array, (h, w, c), BGR, uint8, [0, 255]\n        if isinstance(img, str):\n            img = cv2.imread(img)\n\n        if np.max(img) > 256:  # 16-bit image\n            img = img / 65535 * 255\n        if len(img.shape) == 2:  # gray image\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        elif img.shape[2] == 4:  # RGBA image with alpha channel\n            img = img[:, :, 0:3]\n\n        self.input_img = img\n\n    def get_face_landmarks_5(self,\n                             only_keep_largest=False,\n                             only_center_face=False,\n                             resize=None,\n                             blur_ratio=0.01,\n                             eye_dist_threshold=None):\n        if resize is None:\n            scale = 1\n            input_img = self.input_img\n        else:\n            h, w = self.input_img.shape[0:2]\n            scale = min(h, w) / resize\n            h, w = int(h / scale), int(w / scale)\n            input_img = cv2.resize(self.input_img, (w, h), interpolation=cv2.INTER_LANCZOS4)\n\n        with torch.no_grad():\n            bboxes = self.face_det.detect_faces(input_img, 0.97) * scale\n        for bbox in bboxes:\n            # remove faces with too small eye distance: side faces or too small faces\n            eye_dist = np.linalg.norm([bbox[5] - bbox[7], bbox[6] - bbox[8]])\n            if eye_dist_threshold is not None and (eye_dist < eye_dist_threshold):\n                continue\n\n            if self.template_3points:\n                landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 11, 2)])\n            else:\n                landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 15, 2)])\n            self.all_landmarks_5.append(landmark)\n            self.det_faces.append(bbox[0:5])\n        if len(self.det_faces) == 0:\n            return 0\n        if only_keep_largest:\n            h, w, _ = self.input_img.shape\n            self.det_faces, largest_idx = get_largest_face(self.det_faces, h, w)\n            self.all_landmarks_5 = [self.all_landmarks_5[largest_idx]]\n        elif only_center_face:\n            h, w, _ = self.input_img.shape\n            self.det_faces, center_idx = get_center_face(self.det_faces, h, w)\n            self.all_landmarks_5 = [self.all_landmarks_5[center_idx]]\n\n        # pad blurry images\n        if self.pad_blur:\n            self.pad_input_imgs = []\n            for landmarks in self.all_landmarks_5:\n                # get landmarks\n                eye_left = landmarks[0, :]\n                eye_right = landmarks[1, :]\n                eye_avg = (eye_left + eye_right) * 0.5\n                mouth_avg = (landmarks[3, :] + landmarks[4, :]) * 0.5\n                eye_to_eye = eye_right - eye_left\n                eye_to_mouth = mouth_avg - eye_avg\n\n                # Get the oriented crop rectangle\n                # x: half width of the oriented crop rectangle\n                x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n                #  - np.flipud(eye_to_mouth) * [-1, 1]: rotate 90 clockwise\n                # norm with the hypotenuse: get the direction\n                x /= np.hypot(*x)  # get the hypotenuse of a right triangle\n                rect_scale = 1.5\n                x *= max(np.hypot(*eye_to_eye) * 2.0 * rect_scale, np.hypot(*eye_to_mouth) * 1.8 * rect_scale)\n                # y: half height of the oriented crop rectangle\n                y = np.flipud(x) * [-1, 1]\n\n                # c: center\n                c = eye_avg + eye_to_mouth * 0.1\n                # quad: (left_top, left_bottom, right_bottom, right_top)\n                quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n                # qsize: side length of the square\n                qsize = np.hypot(*x) * 2\n                border = max(int(np.rint(qsize * 0.1)), 3)\n\n                # get pad\n                # pad: (width_left, height_top, width_right, height_bottom)\n                pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n                       int(np.ceil(max(quad[:, 1]))))\n                pad = [\n                    max(-pad[0] + border, 1),\n                    max(-pad[1] + border, 1),\n                    max(pad[2] - self.input_img.shape[0] + border, 1),\n                    max(pad[3] - self.input_img.shape[1] + border, 1)\n                ]\n\n                if max(pad) > 1:\n                    # pad image\n                    pad_img = np.pad(self.input_img, ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n                    # modify landmark coords\n                    landmarks[:, 0] += pad[0]\n                    landmarks[:, 1] += pad[1]\n                    # blur pad images\n                    h, w, _ = pad_img.shape\n                    y, x, _ = np.ogrid[:h, :w, :1]\n                    mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n                                                       np.float32(w - 1 - x) / pad[2]),\n                                      1.0 - np.minimum(np.float32(y) / pad[1],\n                                                       np.float32(h - 1 - y) / pad[3]))\n                    blur = int(qsize * blur_ratio)\n                    if blur % 2 == 0:\n                        blur += 1\n                    blur_img = cv2.boxFilter(pad_img, 0, ksize=(blur, blur))\n                    # blur_img = cv2.GaussianBlur(pad_img, (blur, blur), 0)\n\n                    pad_img = pad_img.astype('float32')\n                    pad_img += (blur_img - pad_img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n                    pad_img += (np.median(pad_img, axis=(0, 1)) - pad_img) * np.clip(mask, 0.0, 1.0)\n                    pad_img = np.clip(pad_img, 0, 255)  # float32, [0, 255]\n                    self.pad_input_imgs.append(pad_img)\n                else:\n                    self.pad_input_imgs.append(np.copy(self.input_img))\n\n        return len(self.all_landmarks_5)\n\n    def align_warp_face(self, save_cropped_path=None, border_mode='constant'):\n        \"\"\"Align and warp faces with face template.\n        \"\"\"\n        if self.pad_blur:\n            assert len(self.pad_input_imgs) == len(\n                self.all_landmarks_5), f'Mismatched samples: {len(self.pad_input_imgs)} and {len(self.all_landmarks_5)}'\n        for idx, landmark in enumerate(self.all_landmarks_5):\n            # use 5 landmarks to get affine matrix\n            # use cv2.LMEDS method for the equivalence to skimage transform\n            # ref: https://blog.csdn.net/yichxi/article/details/115827338\n            affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n            self.affine_matrices.append(affine_matrix)\n            # warp and crop faces\n            if border_mode == 'constant':\n                border_mode = cv2.BORDER_CONSTANT\n            elif border_mode == 'reflect101':\n                border_mode = cv2.BORDER_REFLECT101\n            elif border_mode == 'reflect':\n                border_mode = cv2.BORDER_REFLECT\n            if self.pad_blur:\n                input_img = self.pad_input_imgs[idx]\n            else:\n                input_img = self.input_img\n            cropped_face = cv2.warpAffine(\n                input_img, affine_matrix, self.face_size, borderMode=border_mode, borderValue=(135, 133, 132))  # gray\n            self.cropped_faces.append(cropped_face)\n            # save the cropped face\n            if save_cropped_path is not None:\n                path = os.path.splitext(save_cropped_path)[0]\n                save_path = f'{path}_{idx:02d}.{self.save_ext}'\n                imwrite(cropped_face, save_path)\n\n    def get_inverse_affine(self, save_inverse_affine_path=None):\n        \"\"\"Get inverse affine matrix.\"\"\"\n        for idx, affine_matrix in enumerate(self.affine_matrices):\n            inverse_affine = cv2.invertAffineTransform(affine_matrix)\n            inverse_affine *= self.upscale_factor\n            self.inverse_affine_matrices.append(inverse_affine)\n            # save inverse affine matrices\n            if save_inverse_affine_path is not None:\n                path, _ = os.path.splitext(save_inverse_affine_path)\n                save_path = f'{path}_{idx:02d}.pth'\n                torch.save(inverse_affine, save_path)\n\n    def add_restored_face(self, face):\n        self.restored_faces.append(face)\n\n    def paste_faces_to_input_image(self, save_path=None, upsample_img=None):\n        h, w, _ = self.input_img.shape\n        h_up, w_up = int(h * self.upscale_factor), int(w * self.upscale_factor)\n\n        if upsample_img is None:\n            # simply resize the background\n            upsample_img = cv2.resize(self.input_img, (w_up, h_up), interpolation=cv2.INTER_LANCZOS4)\n        else:\n            upsample_img = cv2.resize(upsample_img, (w_up, h_up), interpolation=cv2.INTER_LANCZOS4)\n\n        assert len(self.restored_faces) == len(\n            self.inverse_affine_matrices), ('length of restored_faces and affine_matrices are different.')\n        for restored_face, inverse_affine in zip(self.restored_faces, self.inverse_affine_matrices):\n            # Add an offset to inverse affine matrix, for more precise back alignment\n            if self.upscale_factor > 1:\n                extra_offset = 0.5 * self.upscale_factor\n            else:\n                extra_offset = 0\n            inverse_affine[:, 2] += extra_offset\n            inv_restored = cv2.warpAffine(restored_face, inverse_affine, (w_up, h_up))\n\n            if self.use_parse:\n                # inference\n                face_input = cv2.resize(restored_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n                face_input = img2tensor(face_input.astype('float32') / 255., bgr2rgb=True, float32=True)\n                normalize(face_input, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n                face_input = torch.unsqueeze(face_input, 0).to(self.device)\n                with torch.no_grad():\n                    out = self.face_parse(face_input)[0]\n                out = out.argmax(dim=1).squeeze().cpu().numpy()\n\n                mask = np.zeros(out.shape)\n                MASK_COLORMAP = [0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 255, 0, 0, 0]\n                for idx, color in enumerate(MASK_COLORMAP):\n                    mask[out == idx] = color\n                #  blur the mask\n                mask = cv2.GaussianBlur(mask, (101, 101), 11)\n                mask = cv2.GaussianBlur(mask, (101, 101), 11)\n                # remove the black borders\n                thres = 10\n                mask[:thres, :] = 0\n                mask[-thres:, :] = 0\n                mask[:, :thres] = 0\n                mask[:, -thres:] = 0\n                mask = mask / 255.\n\n                mask = cv2.resize(mask, restored_face.shape[:2])\n                mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up), flags=3)\n                inv_soft_mask = mask[:, :, None]\n                pasted_face = inv_restored\n\n            else:  # use square parse maps\n                mask = np.ones(self.face_size, dtype=np.float32)\n                inv_mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up))\n                # remove the black borders\n                inv_mask_erosion = cv2.erode(\n                    inv_mask, np.ones((int(2 * self.upscale_factor), int(2 * self.upscale_factor)), np.uint8))\n                pasted_face = inv_mask_erosion[:, :, None] * inv_restored\n                total_face_area = np.sum(inv_mask_erosion)  # // 3\n                # compute the fusion edge based on the area of face\n                w_edge = int(total_face_area**0.5) // 20\n                erosion_radius = w_edge * 2\n                inv_mask_center = cv2.erode(inv_mask_erosion, np.ones((erosion_radius, erosion_radius), np.uint8))\n                blur_size = w_edge * 2\n                inv_soft_mask = cv2.GaussianBlur(inv_mask_center, (blur_size + 1, blur_size + 1), 0)\n                if len(upsample_img.shape) == 2:  # upsample_img is gray image\n                    upsample_img = upsample_img[:, :, None]\n                inv_soft_mask = inv_soft_mask[:, :, None]\n\n            if len(upsample_img.shape) == 3 and upsample_img.shape[2] == 4:  # alpha channel\n                alpha = upsample_img[:, :, 3:]\n                upsample_img = inv_soft_mask * pasted_face + (1 - inv_soft_mask) * upsample_img[:, :, 0:3]\n                upsample_img = np.concatenate((upsample_img, alpha), axis=2)\n            else:\n                upsample_img = inv_soft_mask * pasted_face + (1 - inv_soft_mask) * upsample_img\n\n        if np.max(upsample_img) > 256:  # 16-bit image\n            upsample_img = upsample_img.astype(np.uint16)\n        else:\n            upsample_img = upsample_img.astype(np.uint8)\n        if save_path is not None:\n            path = os.path.splitext(save_path)[0]\n            save_path = f'{path}.{self.save_ext}'\n            imwrite(upsample_img, save_path)\n        return upsample_img\n\n    def clean_all(self):\n        self.all_landmarks_5 = []\n        self.restored_faces = []\n        self.affine_matrices = []\n        self.cropped_faces = []\n        self.inverse_affine_matrices = []\n        self.det_faces = []\n        self.pad_input_imgs = []\n", "extras/facexlib/utils/__init__.py": "from .face_utils import align_crop_face_landmarks, compute_increased_bbox, get_valid_bboxes, paste_face_back\nfrom .misc import img2tensor, load_file_from_url, scandir\n\n__all__ = [\n    'align_crop_face_landmarks', 'compute_increased_bbox', 'get_valid_bboxes', 'load_file_from_url', 'paste_face_back',\n    'img2tensor', 'scandir'\n]\n", "extras/facexlib/detection/retinaface_utils.py": "import numpy as np\nimport torch\nimport torchvision\nfrom itertools import product as product\nfrom math import ceil\n\n\nclass PriorBox(object):\n\n    def __init__(self, cfg, image_size=None, phase='train'):\n        super(PriorBox, self).__init__()\n        self.min_sizes = cfg['min_sizes']\n        self.steps = cfg['steps']\n        self.clip = cfg['clip']\n        self.image_size = image_size\n        self.feature_maps = [[ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)] for step in self.steps]\n        self.name = 's'\n\n    def forward(self):\n        anchors = []\n        for k, f in enumerate(self.feature_maps):\n            min_sizes = self.min_sizes[k]\n            for i, j in product(range(f[0]), range(f[1])):\n                for min_size in min_sizes:\n                    s_kx = min_size / self.image_size[1]\n                    s_ky = min_size / self.image_size[0]\n                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n                    for cy, cx in product(dense_cy, dense_cx):\n                        anchors += [cx, cy, s_kx, s_ky]\n\n        # back to torch land\n        output = torch.Tensor(anchors).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n\n\ndef py_cpu_nms(dets, thresh):\n    \"\"\"Pure Python NMS baseline.\"\"\"\n    keep = torchvision.ops.nms(\n        boxes=torch.Tensor(dets[:, :4]),\n        scores=torch.Tensor(dets[:, 4]),\n        iou_threshold=thresh,\n    )\n\n    return list(keep)\n\n\ndef point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat(\n        (\n            boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n            boxes[:, :2] + boxes[:, 2:] / 2),\n        1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat(\n        (boxes[:, 2:] + boxes[:, :2]) / 2,  # cx, cy\n        boxes[:, 2:] - boxes[:, :2],\n        1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    \"\"\"\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2), box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2), box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \u2229 B / A \u222a B = A \u2229 B / (area(A) + area(B) - A \u2229 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef matrix_iou(a, b):\n    \"\"\"\n    return iou of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef matrix_iof(a, b):\n    \"\"\"\n    return iof of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n\n\ndef match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when matching boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, 4].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        landms: (tensor) Ground truth landms, Shape [num_obj, 10].\n        loc_t: (tensor) Tensor to be filled w/ encoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        landm_t: (tensor) Tensor to be filled w/ encoded landm targets.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location 2)confidence\n        3)landm preds.\n    \"\"\"\n    # jaccard index\n    overlaps = jaccard(truths, point_form(priors))\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n\n    # ignore hard gt\n    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n    if best_prior_idx_filter.shape[0] <= 0:\n        loc_t[idx] = 0\n        conf_t[idx] = 0\n        return\n\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_idx_filter.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):  # \u5224\u522b\u6b64anchor\u662f\u9884\u6d4b\u54ea\u4e00\u4e2aboxes\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]  # Shape: [num_priors,4] \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684bbox\u53d6\u51fa\u6765\n    conf = labels[best_truth_idx]  # Shape: [num_priors]      \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684label\u53d6\u51fa\u6765\n    conf[best_truth_overlap < threshold] = 0  # label as background   overlap<0.35\u7684\u5168\u90e8\u4f5c\u4e3a\u8d1f\u6837\u672c\n    loc = encode(matches, priors, variances)\n\n    matches_landm = landms[best_truth_idx]\n    landm = encode_landm(matches_landm, priors, variances)\n    loc_t[idx] = loc  # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n    landm_t[idx] = landm\n\n\ndef encode(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\ndef encode_landm(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 10].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded landm (tensor), Shape: [num_priors, 10]\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    matched = torch.reshape(matched, (matched.size(0), 5, 2))\n    priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n    priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n    priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n    priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n    priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2)\n    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, :, 2:])\n    # g_cxcy /= priors[:, :, 2:]\n    g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1)\n    # return target for smooth_l1_loss\n    return g_cxcy\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n\n    boxes = torch.cat((priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                       priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef decode_landm(pre, priors, variances):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded landm predictions\n    \"\"\"\n    tmp = (\n        priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n    )\n    landms = torch.cat(tmp, dim=1)\n    return landms\n\n\ndef batched_decode(b_loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        b_loc (tensor): location predictions for loc layers,\n            Shape: [num_batches,num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [1,num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n    boxes = (\n        priors[:, :, :2] + b_loc[:, :, :2] * variances[0] * priors[:, :, 2:],\n        priors[:, :, 2:] * torch.exp(b_loc[:, :, 2:] * variances[1]),\n    )\n    boxes = torch.cat(boxes, dim=2)\n\n    boxes[:, :, :2] -= boxes[:, :, 2:] / 2\n    boxes[:, :, 2:] += boxes[:, :, :2]\n    return boxes\n\n\ndef batched_decode_landm(pre, priors, variances):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_batches,num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [1,num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded landm predictions\n    \"\"\"\n    landms = (\n        priors[:, :, :2] + pre[:, :, :2] * variances[0] * priors[:, :, 2:],\n        priors[:, :, :2] + pre[:, :, 2:4] * variances[0] * priors[:, :, 2:],\n        priors[:, :, :2] + pre[:, :, 4:6] * variances[0] * priors[:, :, 2:],\n        priors[:, :, :2] + pre[:, :, 6:8] * variances[0] * priors[:, :, 2:],\n        priors[:, :, :2] + pre[:, :, 8:10] * variances[0] * priors[:, :, 2:],\n    )\n    landms = torch.cat(landms, dim=2)\n    return landms\n\n\ndef log_sum_exp(x):\n    \"\"\"Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    \"\"\"\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    \"\"\"\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w * h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter / union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n", "extras/facexlib/detection/align_trans.py": "import cv2\nimport numpy as np\n\nfrom .matlab_cp2tform import get_similarity_transform_for_cv2\n\n# reference facial points, a list of coordinates (x,y)\nREFERENCE_FACIAL_POINTS = [[30.29459953, 51.69630051], [65.53179932, 51.50139999], [48.02519989, 71.73660278],\n                           [33.54930115, 92.3655014], [62.72990036, 92.20410156]]\n\nDEFAULT_CROP_SIZE = (96, 112)\n\n\nclass FaceWarpException(Exception):\n\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\n\n\ndef get_reference_facial_points(output_size=None, inner_padding_factor=0.0, outer_padding=(0, 0), default_square=False):\n    \"\"\"\n    Function:\n    ----------\n        get reference 5 key points according to crop settings:\n        0. Set default crop_size:\n            if default_square:\n                crop_size = (112, 112)\n            else:\n                crop_size = (96, 112)\n        1. Pad the crop_size by inner_padding_factor in each side;\n        2. Resize crop_size into (output_size - outer_padding*2),\n            pad into output_size with outer_padding;\n        3. Output reference_5point;\n    Parameters:\n    ----------\n        @output_size: (w, h) or None\n            size of aligned face image\n        @inner_padding_factor: (w_factor, h_factor)\n            padding factor for inner (w, h)\n        @outer_padding: (w_pad, h_pad)\n            each row is a pair of coordinates (x, y)\n        @default_square: True or False\n            if True:\n                default crop_size = (112, 112)\n            else:\n                default crop_size = (96, 112);\n        !!! make sure, if output_size is not None:\n                (output_size - outer_padding)\n                = some_scale * (default crop_size * (1.0 +\n                inner_padding_factor))\n    Returns:\n    ----------\n        @reference_5point: 5x2 np.array\n            each row is a pair of transformed coordinates (x, y)\n    \"\"\"\n\n    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n\n    # 0) make the inner region a square\n    if default_square:\n        size_diff = max(tmp_crop_size) - tmp_crop_size\n        tmp_5pts += size_diff / 2\n        tmp_crop_size += size_diff\n\n    if (output_size and output_size[0] == tmp_crop_size[0] and output_size[1] == tmp_crop_size[1]):\n\n        return tmp_5pts\n\n    if (inner_padding_factor == 0 and outer_padding == (0, 0)):\n        if output_size is None:\n            return tmp_5pts\n        else:\n            raise FaceWarpException('No paddings to do, output_size must be None or {}'.format(tmp_crop_size))\n\n    # check output size\n    if not (0 <= inner_padding_factor <= 1.0):\n        raise FaceWarpException('Not (0 <= inner_padding_factor <= 1.0)')\n\n    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0) and output_size is None):\n        output_size = tmp_crop_size * \\\n            (1 + inner_padding_factor * 2).astype(np.int32)\n        output_size += np.array(outer_padding)\n    if not (outer_padding[0] < output_size[0] and outer_padding[1] < output_size[1]):\n        raise FaceWarpException('Not (outer_padding[0] < output_size[0] and outer_padding[1] < output_size[1])')\n\n    # 1) pad the inner region according inner_padding_factor\n    if inner_padding_factor > 0:\n        size_diff = tmp_crop_size * inner_padding_factor * 2\n        tmp_5pts += size_diff / 2\n        tmp_crop_size += np.round(size_diff).astype(np.int32)\n\n    # 2) resize the padded inner region\n    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n\n    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n        raise FaceWarpException('Must have (output_size - outer_padding)'\n                                '= some_scale * (crop_size * (1.0 + inner_padding_factor)')\n\n    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n    tmp_5pts = tmp_5pts * scale_factor\n    #    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n    #    tmp_5pts = tmp_5pts + size_diff / 2\n    tmp_crop_size = size_bf_outer_pad\n\n    # 3) add outer_padding to make output_size\n    reference_5point = tmp_5pts + np.array(outer_padding)\n    tmp_crop_size = output_size\n\n    return reference_5point\n\n\ndef get_affine_transform_matrix(src_pts, dst_pts):\n    \"\"\"\n    Function:\n    ----------\n        get affine transform matrix 'tfm' from src_pts to dst_pts\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points matrix, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points matrix, each row is a pair of coordinates (x, y)\n    Returns:\n    ----------\n        @tfm: 2x3 np.array\n            transform matrix from src_pts to dst_pts\n    \"\"\"\n\n    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n    n_pts = src_pts.shape[0]\n    ones = np.ones((n_pts, 1), src_pts.dtype)\n    src_pts_ = np.hstack([src_pts, ones])\n    dst_pts_ = np.hstack([dst_pts, ones])\n\n    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n\n    if rank == 3:\n        tfm = np.float32([[A[0, 0], A[1, 0], A[2, 0]], [A[0, 1], A[1, 1], A[2, 1]]])\n    elif rank == 2:\n        tfm = np.float32([[A[0, 0], A[1, 0], 0], [A[0, 1], A[1, 1], 0]])\n\n    return tfm\n\n\ndef warp_and_crop_face(src_img, facial_pts, reference_pts=None, crop_size=(96, 112), align_type='smilarity'):\n    \"\"\"\n    Function:\n    ----------\n        apply affine transform 'trans' to uv\n    Parameters:\n    ----------\n        @src_img: 3x3 np.array\n            input image\n        @facial_pts: could be\n            1)a list of K coordinates (x,y)\n        or\n            2) Kx2 or 2xK np.array\n            each row or col is a pair of coordinates (x, y)\n        @reference_pts: could be\n            1) a list of K coordinates (x,y)\n        or\n            2) Kx2 or 2xK np.array\n            each row or col is a pair of coordinates (x, y)\n        or\n            3) None\n            if None, use default reference facial points\n        @crop_size: (w, h)\n            output face image size\n        @align_type: transform type, could be one of\n            1) 'similarity': use similarity transform\n            2) 'cv2_affine': use the first 3 points to do affine transform,\n                    by calling cv2.getAffineTransform()\n            3) 'affine': use all points to do affine transform\n    Returns:\n    ----------\n        @face_img: output face image with size (w, h) = @crop_size\n    \"\"\"\n\n    if reference_pts is None:\n        if crop_size[0] == 96 and crop_size[1] == 112:\n            reference_pts = REFERENCE_FACIAL_POINTS\n        else:\n            default_square = False\n            inner_padding_factor = 0\n            outer_padding = (0, 0)\n            output_size = crop_size\n\n            reference_pts = get_reference_facial_points(output_size, inner_padding_factor, outer_padding,\n                                                        default_square)\n\n    ref_pts = np.float32(reference_pts)\n    ref_pts_shp = ref_pts.shape\n    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n        raise FaceWarpException('reference_pts.shape must be (K,2) or (2,K) and K>2')\n\n    if ref_pts_shp[0] == 2:\n        ref_pts = ref_pts.T\n\n    src_pts = np.float32(facial_pts)\n    src_pts_shp = src_pts.shape\n    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n        raise FaceWarpException('facial_pts.shape must be (K,2) or (2,K) and K>2')\n\n    if src_pts_shp[0] == 2:\n        src_pts = src_pts.T\n\n    if src_pts.shape != ref_pts.shape:\n        raise FaceWarpException('facial_pts and reference_pts must have the same shape')\n\n    if align_type == 'cv2_affine':\n        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n    elif align_type == 'affine':\n        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n    else:\n        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n\n    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n\n    return face_img\n", "extras/facexlib/detection/retinaface.py": "import cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision.models._utils import IntermediateLayerGetter as IntermediateLayerGetter\n\nfrom extras.facexlib.detection.align_trans import get_reference_facial_points, warp_and_crop_face\nfrom extras.facexlib.detection.retinaface_net import FPN, SSH, MobileNetV1, make_bbox_head, make_class_head, make_landmark_head\nfrom extras.facexlib.detection.retinaface_utils import (PriorBox, batched_decode, batched_decode_landm, decode, decode_landm,\n                                                 py_cpu_nms)\n\n\ndef generate_config(network_name):\n\n    cfg_mnet = {\n        'name': 'mobilenet0.25',\n        'min_sizes': [[16, 32], [64, 128], [256, 512]],\n        'steps': [8, 16, 32],\n        'variance': [0.1, 0.2],\n        'clip': False,\n        'loc_weight': 2.0,\n        'gpu_train': True,\n        'batch_size': 32,\n        'ngpu': 1,\n        'epoch': 250,\n        'decay1': 190,\n        'decay2': 220,\n        'image_size': 640,\n        'return_layers': {\n            'stage1': 1,\n            'stage2': 2,\n            'stage3': 3\n        },\n        'in_channel': 32,\n        'out_channel': 64\n    }\n\n    cfg_re50 = {\n        'name': 'Resnet50',\n        'min_sizes': [[16, 32], [64, 128], [256, 512]],\n        'steps': [8, 16, 32],\n        'variance': [0.1, 0.2],\n        'clip': False,\n        'loc_weight': 2.0,\n        'gpu_train': True,\n        'batch_size': 24,\n        'ngpu': 4,\n        'epoch': 100,\n        'decay1': 70,\n        'decay2': 90,\n        'image_size': 840,\n        'return_layers': {\n            'layer2': 1,\n            'layer3': 2,\n            'layer4': 3\n        },\n        'in_channel': 256,\n        'out_channel': 256\n    }\n\n    if network_name == 'mobile0.25':\n        return cfg_mnet\n    elif network_name == 'resnet50':\n        return cfg_re50\n    else:\n        raise NotImplementedError(f'network_name={network_name}')\n\n\nclass RetinaFace(nn.Module):\n\n    def __init__(self, network_name='resnet50', half=False, phase='test', device=None):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n\n        super(RetinaFace, self).__init__()\n        self.half_inference = half\n        cfg = generate_config(network_name)\n        self.backbone = cfg['name']\n\n        self.model_name = f'retinaface_{network_name}'\n        self.cfg = cfg\n        self.phase = phase\n        self.target_size, self.max_size = 1600, 2150\n        self.resize, self.scale, self.scale1 = 1., None, None\n        self.mean_tensor = torch.tensor([[[[104.]], [[117.]], [[123.]]]], device=self.device)\n        self.reference = get_reference_facial_points(default_square=True)\n        # Build network.\n        backbone = None\n        if cfg['name'] == 'mobilenet0.25':\n            backbone = MobileNetV1()\n            self.body = IntermediateLayerGetter(backbone, cfg['return_layers'])\n        elif cfg['name'] == 'Resnet50':\n            import torchvision.models as models\n            backbone = models.resnet50(weights=None)\n            self.body = IntermediateLayerGetter(backbone, cfg['return_layers'])\n\n        in_channels_stage2 = cfg['in_channel']\n        in_channels_list = [\n            in_channels_stage2 * 2,\n            in_channels_stage2 * 4,\n            in_channels_stage2 * 8,\n        ]\n\n        out_channels = cfg['out_channel']\n        self.fpn = FPN(in_channels_list, out_channels)\n        self.ssh1 = SSH(out_channels, out_channels)\n        self.ssh2 = SSH(out_channels, out_channels)\n        self.ssh3 = SSH(out_channels, out_channels)\n\n        self.ClassHead = make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n        self.BboxHead = make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n        self.LandmarkHead = make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n\n        self.to(self.device)\n        self.eval()\n        if self.half_inference:\n            self.half()\n\n    def forward(self, inputs):\n        out = self.body(inputs)\n\n        if self.backbone == 'mobilenet0.25' or self.backbone == 'Resnet50':\n            out = list(out.values())\n        # FPN\n        fpn = self.fpn(out)\n\n        # SSH\n        feature1 = self.ssh1(fpn[0])\n        feature2 = self.ssh2(fpn[1])\n        feature3 = self.ssh3(fpn[2])\n        features = [feature1, feature2, feature3]\n\n        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)], dim=1)\n        tmp = [self.LandmarkHead[i](feature) for i, feature in enumerate(features)]\n        ldm_regressions = (torch.cat(tmp, dim=1))\n\n        if self.phase == 'train':\n            output = (bbox_regressions, classifications, ldm_regressions)\n        else:\n            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n        return output\n\n    def __detect_faces(self, inputs):\n        # get scale\n        height, width = inputs.shape[2:]\n        self.scale = torch.tensor([width, height, width, height], dtype=torch.float32, device=self.device)\n        tmp = [width, height, width, height, width, height, width, height, width, height]\n        self.scale1 = torch.tensor(tmp, dtype=torch.float32, device=self.device)\n\n        # forawrd\n        inputs = inputs.to(self.device)\n        if self.half_inference:\n            inputs = inputs.half()\n        loc, conf, landmarks = self(inputs)\n\n        # get priorbox\n        priorbox = PriorBox(self.cfg, image_size=inputs.shape[2:])\n        priors = priorbox.forward().to(self.device)\n\n        return loc, conf, landmarks, priors\n\n    # single image detection\n    def transform(self, image, use_origin_size):\n        # convert to opencv format\n        if isinstance(image, Image.Image):\n            image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR)\n        image = image.astype(np.float32)\n\n        # testing scale\n        im_size_min = np.min(image.shape[0:2])\n        im_size_max = np.max(image.shape[0:2])\n        resize = float(self.target_size) / float(im_size_min)\n\n        # prevent bigger axis from being more than max_size\n        if np.round(resize * im_size_max) > self.max_size:\n            resize = float(self.max_size) / float(im_size_max)\n        resize = 1 if use_origin_size else resize\n\n        # resize\n        if resize != 1:\n            image = cv2.resize(image, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n\n        # convert to torch.tensor format\n        # image -= (104, 117, 123)\n        image = image.transpose(2, 0, 1)\n        image = torch.from_numpy(image).unsqueeze(0)\n\n        return image, resize\n\n    def detect_faces(\n        self,\n        image,\n        conf_threshold=0.8,\n        nms_threshold=0.4,\n        use_origin_size=True,\n    ):\n        image, self.resize = self.transform(image, use_origin_size)\n        image = image.to(self.device)\n        if self.half_inference:\n            image = image.half()\n        image = image - self.mean_tensor\n\n        loc, conf, landmarks, priors = self.__detect_faces(image)\n\n        boxes = decode(loc.data.squeeze(0), priors.data, self.cfg['variance'])\n        boxes = boxes * self.scale / self.resize\n        boxes = boxes.cpu().numpy()\n\n        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n\n        landmarks = decode_landm(landmarks.squeeze(0), priors, self.cfg['variance'])\n        landmarks = landmarks * self.scale1 / self.resize\n        landmarks = landmarks.cpu().numpy()\n\n        # ignore low scores\n        inds = np.where(scores > conf_threshold)[0]\n        boxes, landmarks, scores = boxes[inds], landmarks[inds], scores[inds]\n\n        # sort\n        order = scores.argsort()[::-1]\n        boxes, landmarks, scores = boxes[order], landmarks[order], scores[order]\n\n        # do NMS\n        bounding_boxes = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n        keep = py_cpu_nms(bounding_boxes, nms_threshold)\n        bounding_boxes, landmarks = bounding_boxes[keep, :], landmarks[keep]\n        # self.t['forward_pass'].toc()\n        # print(self.t['forward_pass'].average_time)\n        # import sys\n        # sys.stdout.flush()\n        return np.concatenate((bounding_boxes, landmarks), axis=1)\n\n    def __align_multi(self, image, boxes, landmarks, limit=None):\n\n        if len(boxes) < 1:\n            return [], []\n\n        if limit:\n            boxes = boxes[:limit]\n            landmarks = landmarks[:limit]\n\n        faces = []\n        for landmark in landmarks:\n            facial5points = [[landmark[2 * j], landmark[2 * j + 1]] for j in range(5)]\n\n            warped_face = warp_and_crop_face(np.array(image), facial5points, self.reference, crop_size=(112, 112))\n            faces.append(warped_face)\n\n        return np.concatenate((boxes, landmarks), axis=1), faces\n\n    def align_multi(self, img, conf_threshold=0.8, limit=None):\n\n        rlt = self.detect_faces(img, conf_threshold=conf_threshold)\n        boxes, landmarks = rlt[:, 0:5], rlt[:, 5:]\n\n        return self.__align_multi(img, boxes, landmarks, limit)\n\n    # batched detection\n    def batched_transform(self, frames, use_origin_size):\n        \"\"\"\n        Arguments:\n            frames: a list of PIL.Image, or torch.Tensor(shape=[n, h, w, c],\n                type=np.float32, BGR format).\n            use_origin_size: whether to use origin size.\n        \"\"\"\n        from_PIL = True if isinstance(frames[0], Image.Image) else False\n\n        # convert to opencv format\n        if from_PIL:\n            frames = [cv2.cvtColor(np.asarray(frame), cv2.COLOR_RGB2BGR) for frame in frames]\n            frames = np.asarray(frames, dtype=np.float32)\n\n        # testing scale\n        im_size_min = np.min(frames[0].shape[0:2])\n        im_size_max = np.max(frames[0].shape[0:2])\n        resize = float(self.target_size) / float(im_size_min)\n\n        # prevent bigger axis from being more than max_size\n        if np.round(resize * im_size_max) > self.max_size:\n            resize = float(self.max_size) / float(im_size_max)\n        resize = 1 if use_origin_size else resize\n\n        # resize\n        if resize != 1:\n            if not from_PIL:\n                frames = F.interpolate(frames, scale_factor=resize)\n            else:\n                frames = [\n                    cv2.resize(frame, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n                    for frame in frames\n                ]\n\n        # convert to torch.tensor format\n        if not from_PIL:\n            frames = frames.transpose(1, 2).transpose(1, 3).contiguous()\n        else:\n            frames = frames.transpose((0, 3, 1, 2))\n            frames = torch.from_numpy(frames)\n\n        return frames, resize\n\n    def batched_detect_faces(self, frames, conf_threshold=0.8, nms_threshold=0.4, use_origin_size=True):\n        \"\"\"\n        Arguments:\n            frames: a list of PIL.Image, or np.array(shape=[n, h, w, c],\n                type=np.uint8, BGR format).\n            conf_threshold: confidence threshold.\n            nms_threshold: nms threshold.\n            use_origin_size: whether to use origin size.\n        Returns:\n            final_bounding_boxes: list of np.array ([n_boxes, 5],\n                type=np.float32).\n            final_landmarks: list of np.array ([n_boxes, 10], type=np.float32).\n        \"\"\"\n        # self.t['forward_pass'].tic()\n        frames, self.resize = self.batched_transform(frames, use_origin_size)\n        frames = frames.to(self.device)\n        frames = frames - self.mean_tensor\n\n        b_loc, b_conf, b_landmarks, priors = self.__detect_faces(frames)\n\n        final_bounding_boxes, final_landmarks = [], []\n\n        # decode\n        priors = priors.unsqueeze(0)\n        b_loc = batched_decode(b_loc, priors, self.cfg['variance']) * self.scale / self.resize\n        b_landmarks = batched_decode_landm(b_landmarks, priors, self.cfg['variance']) * self.scale1 / self.resize\n        b_conf = b_conf[:, :, 1]\n\n        # index for selection\n        b_indice = b_conf > conf_threshold\n\n        # concat\n        b_loc_and_conf = torch.cat((b_loc, b_conf.unsqueeze(-1)), dim=2).float()\n\n        for pred, landm, inds in zip(b_loc_and_conf, b_landmarks, b_indice):\n\n            # ignore low scores\n            pred, landm = pred[inds, :], landm[inds, :]\n            if pred.shape[0] == 0:\n                final_bounding_boxes.append(np.array([], dtype=np.float32))\n                final_landmarks.append(np.array([], dtype=np.float32))\n                continue\n\n            # sort\n            # order = score.argsort(descending=True)\n            # box, landm, score = box[order], landm[order], score[order]\n\n            # to CPU\n            bounding_boxes, landm = pred.cpu().numpy(), landm.cpu().numpy()\n\n            # NMS\n            keep = py_cpu_nms(bounding_boxes, nms_threshold)\n            bounding_boxes, landmarks = bounding_boxes[keep, :], landm[keep]\n\n            # append\n            final_bounding_boxes.append(bounding_boxes)\n            final_landmarks.append(landmarks)\n        # self.t['forward_pass'].toc(average=True)\n        # self.batch_time += self.t['forward_pass'].diff\n        # self.total_frame += len(frames)\n        # print(self.batch_time / self.total_frame)\n\n        return final_bounding_boxes, final_landmarks\n", "extras/facexlib/detection/matlab_cp2tform.py": "import numpy as np\nfrom numpy.linalg import inv, lstsq\nfrom numpy.linalg import matrix_rank as rank\nfrom numpy.linalg import norm\n\n\nclass MatlabCp2tormException(Exception):\n\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\n\n\ndef tformfwd(trans, uv):\n    \"\"\"\n    Function:\n    ----------\n        apply affine transform 'trans' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of transformed coordinates (x, y)\n    \"\"\"\n    uv = np.hstack((uv, np.ones((uv.shape[0], 1))))\n    xy = np.dot(uv, trans)\n    xy = xy[:, 0:-1]\n    return xy\n\n\ndef tforminv(trans, uv):\n    \"\"\"\n    Function:\n    ----------\n        apply the inverse of affine transform 'trans' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of inverse-transformed coordinates (x, y)\n    \"\"\"\n    Tinv = inv(trans)\n    xy = tformfwd(Tinv, uv)\n    return xy\n\n\ndef findNonreflectiveSimilarity(uv, xy, options=None):\n    options = {'K': 2}\n\n    K = options['K']\n    M = xy.shape[0]\n    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n\n    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n    X = np.vstack((tmp1, tmp2))\n\n    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    U = np.vstack((u, v))\n\n    # We know that X * r = U\n    if rank(X) >= 2 * K:\n        r, _, _, _ = lstsq(X, U, rcond=-1)\n        r = np.squeeze(r)\n    else:\n        raise Exception('cp2tform:twoUniquePointsReq')\n    sc = r[0]\n    ss = r[1]\n    tx = r[2]\n    ty = r[3]\n\n    Tinv = np.array([[sc, -ss, 0], [ss, sc, 0], [tx, ty, 1]])\n    T = inv(Tinv)\n    T[:, 2] = np.array([0, 0, 1])\n\n    return T, Tinv\n\n\ndef findSimilarity(uv, xy, options=None):\n    options = {'K': 2}\n\n    #    uv = np.array(uv)\n    #    xy = np.array(xy)\n\n    # Solve for trans1\n    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n\n    # Solve for trans2\n\n    # manually reflect the xy data across the Y-axis\n    xyR = xy\n    xyR[:, 0] = -1 * xyR[:, 0]\n\n    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\n\n    # manually reflect the tform to undo the reflection done on xyR\n    TreflectY = np.array([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    trans2 = np.dot(trans2r, TreflectY)\n\n    # Figure out if trans1 or trans2 is better\n    xy1 = tformfwd(trans1, uv)\n    norm1 = norm(xy1 - xy)\n\n    xy2 = tformfwd(trans2, uv)\n    norm2 = norm(xy2 - xy)\n\n    if norm1 <= norm2:\n        return trans1, trans1_inv\n    else:\n        trans2_inv = inv(trans2)\n        return trans2, trans2_inv\n\n\ndef get_similarity_transform(src_pts, dst_pts, reflective=True):\n    \"\"\"\n    Function:\n    ----------\n        Find Similarity Transform Matrix 'trans':\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y, 1] = [u, v, 1] * trans\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        @reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n       @trans: 3x3 np.array\n            transform matrix from uv to xy\n        trans_inv: 3x3 np.array\n            inverse of trans, transform matrix from xy to uv\n    \"\"\"\n\n    if reflective:\n        trans, trans_inv = findSimilarity(src_pts, dst_pts)\n    else:\n        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\n\n    return trans, trans_inv\n\n\ndef cvt_tform_mat_for_cv2(trans):\n    \"\"\"\n    Function:\n    ----------\n        Convert Transform Matrix 'trans' into 'cv2_trans' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix from uv to xy\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    \"\"\"\n    cv2_trans = trans[:, 0:2].T\n\n    return cv2_trans\n\n\ndef get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\n    \"\"\"\n    Function:\n    ----------\n        Find Similarity Transform Matrix 'cv2_trans' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    \"\"\"\n    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\n    cv2_trans = cvt_tform_mat_for_cv2(trans)\n\n    return cv2_trans\n\n\nif __name__ == '__main__':\n    \"\"\"\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    # In Matlab, run:\n    #\n    #   uv = [u'; v'];\n    #   xy = [x'; y'];\n    #   tform_sim=cp2tform(uv,xy,'similarity');\n    #\n    #   trans = tform_sim.tdata.T\n    #   ans =\n    #       -0.0764   -1.6190         0\n    #        1.6190   -0.0764         0\n    #       -3.2156    0.0290    1.0000\n    #   trans_inv = tform_sim.tdata.Tinv\n    #    ans =\n    #\n    #       -0.0291    0.6163         0\n    #       -0.6163   -0.0291         0\n    #       -0.0756    1.9826    1.0000\n    #    xy_m=tformfwd(tform_sim, u,v)\n    #\n    #    xy_m =\n    #\n    #       -3.2156    0.0290\n    #        1.1833   -9.9143\n    #        5.0323    2.8853\n    #    uv_m=tforminv(tform_sim, x,y)\n    #\n    #    uv_m =\n    #\n    #        0.5698    1.3953\n    #        6.0872    2.2733\n    #       -2.6570    4.3314\n    \"\"\"\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    uv = np.array((u, v)).T\n    xy = np.array((x, y)).T\n\n    print('\\n--->uv:')\n    print(uv)\n    print('\\n--->xy:')\n    print(xy)\n\n    trans, trans_inv = get_similarity_transform(uv, xy)\n\n    print('\\n--->trans matrix:')\n    print(trans)\n\n    print('\\n--->trans_inv matrix:')\n    print(trans_inv)\n\n    print('\\n---> apply transform to uv')\n    print('\\nxy_m = uv_augmented * trans')\n    uv_aug = np.hstack((uv, np.ones((uv.shape[0], 1))))\n    xy_m = np.dot(uv_aug, trans)\n    print(xy_m)\n\n    print('\\nxy_m = tformfwd(trans, uv)')\n    xy_m = tformfwd(trans, uv)\n    print(xy_m)\n\n    print('\\n---> apply inverse transform to xy')\n    print('\\nuv_m = xy_augmented * trans_inv')\n    xy_aug = np.hstack((xy, np.ones((xy.shape[0], 1))))\n    uv_m = np.dot(xy_aug, trans_inv)\n    print(uv_m)\n\n    print('\\nuv_m = tformfwd(trans_inv, xy)')\n    uv_m = tformfwd(trans_inv, xy)\n    print(uv_m)\n\n    uv_m = tforminv(trans, xy)\n    print('\\nuv_m = tforminv(trans, xy)')\n    print(uv_m)\n", "extras/facexlib/detection/__init__.py": "import torch\nfrom copy import deepcopy\n\nfrom extras.facexlib.utils import load_file_from_url\nfrom .retinaface import RetinaFace\n\n\ndef init_detection_model(model_name, half=False, device='cuda', model_rootpath=None):\n    if model_name == 'retinaface_resnet50':\n        model = RetinaFace(network_name='resnet50', half=half, device=device)\n        model_url = 'https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth'\n    elif model_name == 'retinaface_mobile0.25':\n        model = RetinaFace(network_name='mobile0.25', half=half, device=device)\n        model_url = 'https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_mobilenet0.25_Final.pth'\n    else:\n        raise NotImplementedError(f'{model_name} is not implemented.')\n\n    model_path = load_file_from_url(\n        url=model_url, model_dir='facexlib/weights', progress=True, file_name=None, save_dir=model_rootpath)\n\n    # TODO: clean pretrained model\n    load_net = torch.load(model_path, map_location=lambda storage, loc: storage)\n    # remove unnecessary 'module.'\n    for k, v in deepcopy(load_net).items():\n        if k.startswith('module.'):\n            load_net[k[7:]] = v\n            load_net.pop(k)\n    model.load_state_dict(load_net, strict=True)\n    model.eval()\n    model = model.to(device)\n    return model\n", "extras/facexlib/detection/retinaface_net.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_bn(inp, oup, stride=1, leaky=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True))\n\n\ndef conv_bn_no_relu(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\n\ndef conv_bn1X1(inp, oup, stride, leaky=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False), nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True))\n\n\ndef conv_dw(inp, oup, stride, leaky=0.1):\n    return nn.Sequential(\n        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True),\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True),\n    )\n\n\nclass SSH(nn.Module):\n\n    def __init__(self, in_channel, out_channel):\n        super(SSH, self).__init__()\n        assert out_channel % 4 == 0\n        leaky = 0\n        if (out_channel <= 64):\n            leaky = 0.1\n        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel // 2, stride=1)\n\n        self.conv5X5_1 = conv_bn(in_channel, out_channel // 4, stride=1, leaky=leaky)\n        self.conv5X5_2 = conv_bn_no_relu(out_channel // 4, out_channel // 4, stride=1)\n\n        self.conv7X7_2 = conv_bn(out_channel // 4, out_channel // 4, stride=1, leaky=leaky)\n        self.conv7x7_3 = conv_bn_no_relu(out_channel // 4, out_channel // 4, stride=1)\n\n    def forward(self, input):\n        conv3X3 = self.conv3X3(input)\n\n        conv5X5_1 = self.conv5X5_1(input)\n        conv5X5 = self.conv5X5_2(conv5X5_1)\n\n        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n        conv7X7 = self.conv7x7_3(conv7X7_2)\n\n        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n        out = F.relu(out)\n        return out\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, in_channels_list, out_channels):\n        super(FPN, self).__init__()\n        leaky = 0\n        if (out_channels <= 64):\n            leaky = 0.1\n        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride=1, leaky=leaky)\n        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride=1, leaky=leaky)\n        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride=1, leaky=leaky)\n\n        self.merge1 = conv_bn(out_channels, out_channels, leaky=leaky)\n        self.merge2 = conv_bn(out_channels, out_channels, leaky=leaky)\n\n    def forward(self, input):\n        # names = list(input.keys())\n        # input = list(input.values())\n\n        output1 = self.output1(input[0])\n        output2 = self.output2(input[1])\n        output3 = self.output3(input[2])\n\n        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode='nearest')\n        output2 = output2 + up3\n        output2 = self.merge2(output2)\n\n        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode='nearest')\n        output1 = output1 + up2\n        output1 = self.merge1(output1)\n\n        out = [output1, output2, output3]\n        return out\n\n\nclass MobileNetV1(nn.Module):\n\n    def __init__(self):\n        super(MobileNetV1, self).__init__()\n        self.stage1 = nn.Sequential(\n            conv_bn(3, 8, 2, leaky=0.1),  # 3\n            conv_dw(8, 16, 1),  # 7\n            conv_dw(16, 32, 2),  # 11\n            conv_dw(32, 32, 1),  # 19\n            conv_dw(32, 64, 2),  # 27\n            conv_dw(64, 64, 1),  # 43\n        )\n        self.stage2 = nn.Sequential(\n            conv_dw(64, 128, 2),  # 43 + 16 = 59\n            conv_dw(128, 128, 1),  # 59 + 32 = 91\n            conv_dw(128, 128, 1),  # 91 + 32 = 123\n            conv_dw(128, 128, 1),  # 123 + 32 = 155\n            conv_dw(128, 128, 1),  # 155 + 32 = 187\n            conv_dw(128, 128, 1),  # 187 + 32 = 219\n        )\n        self.stage3 = nn.Sequential(\n            conv_dw(128, 256, 2),  # 219 +3 2 = 241\n            conv_dw(256, 256, 1),  # 241 + 64 = 301\n        )\n        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, 1000)\n\n    def forward(self, x):\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.avg(x)\n        # x = self.model(x)\n        x = x.view(-1, 256)\n        x = self.fc(x)\n        return x\n\n\nclass ClassHead(nn.Module):\n\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(ClassHead, self).__init__()\n        self.num_anchors = num_anchors\n        self.conv1x1 = nn.Conv2d(inchannels, self.num_anchors * 2, kernel_size=(1, 1), stride=1, padding=0)\n\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n\n        return out.view(out.shape[0], -1, 2)\n\n\nclass BboxHead(nn.Module):\n\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(BboxHead, self).__init__()\n        self.conv1x1 = nn.Conv2d(inchannels, num_anchors * 4, kernel_size=(1, 1), stride=1, padding=0)\n\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n\n        return out.view(out.shape[0], -1, 4)\n\n\nclass LandmarkHead(nn.Module):\n\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(LandmarkHead, self).__init__()\n        self.conv1x1 = nn.Conv2d(inchannels, num_anchors * 10, kernel_size=(1, 1), stride=1, padding=0)\n\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n\n        return out.view(out.shape[0], -1, 10)\n\n\ndef make_class_head(fpn_num=3, inchannels=64, anchor_num=2):\n    classhead = nn.ModuleList()\n    for i in range(fpn_num):\n        classhead.append(ClassHead(inchannels, anchor_num))\n    return classhead\n\n\ndef make_bbox_head(fpn_num=3, inchannels=64, anchor_num=2):\n    bboxhead = nn.ModuleList()\n    for i in range(fpn_num):\n        bboxhead.append(BboxHead(inchannels, anchor_num))\n    return bboxhead\n\n\ndef make_landmark_head(fpn_num=3, inchannels=64, anchor_num=2):\n    landmarkhead = nn.ModuleList()\n    for i in range(fpn_num):\n        landmarkhead.append(LandmarkHead(inchannels, anchor_num))\n    return landmarkhead\n", "ldm_patched/contrib/external_rebatch.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\n\nclass LatentRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"latents\": (\"LATENT\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n\n    FUNCTION = \"rebatch\"\n\n    CATEGORY = \"latent/batch\"\n\n    @staticmethod\n    def get_batch(latents, list_ind, offset):\n        '''prepare a batch out of the list of latents'''\n        samples = latents[list_ind]['samples']\n        shape = samples.shape\n        mask = latents[list_ind]['noise_mask'] if 'noise_mask' in latents[list_ind] else torch.ones((shape[0], 1, shape[2]*8, shape[3]*8), device='cpu')\n        if mask.shape[-1] != shape[-1] * 8 or mask.shape[-2] != shape[-2]:\n            torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(shape[-2]*8, shape[-1]*8), mode=\"bilinear\")\n        if mask.shape[0] < samples.shape[0]:\n            mask = mask.repeat((shape[0] - 1) // mask.shape[0] + 1, 1, 1, 1)[:shape[0]]\n        if 'batch_index' in latents[list_ind]:\n            batch_inds = latents[list_ind]['batch_index']\n        else:\n            batch_inds = [x+offset for x in range(shape[0])]\n        return samples, mask, batch_inds\n\n    @staticmethod\n    def get_slices(indexable, num, batch_size):\n        '''divides an indexable object into num slices of length batch_size, and a remainder'''\n        slices = []\n        for i in range(num):\n            slices.append(indexable[i*batch_size:(i+1)*batch_size])\n        if num * batch_size < len(indexable):\n            return slices, indexable[num * batch_size:]\n        else:\n            return slices, None\n    \n    @staticmethod\n    def slice_batch(batch, num, batch_size):\n        result = [LatentRebatch.get_slices(x, num, batch_size) for x in batch]\n        return list(zip(*result))\n\n    @staticmethod\n    def cat_batch(batch1, batch2):\n        if batch1[0] is None:\n            return batch2\n        result = [torch.cat((b1, b2)) if torch.is_tensor(b1) else b1 + b2 for b1, b2 in zip(batch1, batch2)]\n        return result\n\n    def rebatch(self, latents, batch_size):\n        batch_size = batch_size[0]\n\n        output_list = []\n        current_batch = (None, None, None)\n        processed = 0\n\n        for i in range(len(latents)):\n            # fetch new entry of list\n            #samples, masks, indices = self.get_batch(latents, i)\n            next_batch = self.get_batch(latents, i, processed)\n            processed += len(next_batch[2])\n            # set to current if current is None\n            if current_batch[0] is None:\n                current_batch = next_batch\n            # add previous to list if dimensions do not match\n            elif next_batch[0].shape[-1] != current_batch[0].shape[-1] or next_batch[0].shape[-2] != current_batch[0].shape[-2]:\n                sliced, _ = self.slice_batch(current_batch, 1, batch_size)\n                output_list.append({'samples': sliced[0][0], 'noise_mask': sliced[1][0], 'batch_index': sliced[2][0]})\n                current_batch = next_batch\n            # cat if everything checks out\n            else:\n                current_batch = self.cat_batch(current_batch, next_batch)\n\n            # add to list if dimensions gone above target batch size\n            if current_batch[0].shape[0] > batch_size:\n                num = current_batch[0].shape[0] // batch_size\n                sliced, remainder = self.slice_batch(current_batch, num, batch_size)\n                \n                for i in range(num):\n                    output_list.append({'samples': sliced[0][i], 'noise_mask': sliced[1][i], 'batch_index': sliced[2][i]})\n\n                current_batch = remainder\n\n        #add remainder\n        if current_batch[0] is not None:\n            sliced, _ = self.slice_batch(current_batch, 1, batch_size)\n            output_list.append({'samples': sliced[0][0], 'noise_mask': sliced[1][0], 'batch_index': sliced[2][0]})\n\n        #get rid of empty masks\n        for s in output_list:\n            if s['noise_mask'].mean() == 1.0:\n                del s['noise_mask']\n\n        return (output_list,)\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n\n    FUNCTION = \"rebatch\"\n\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]\n\n        output_list = []\n        all_images = []\n        for img in images:\n            for i in range(img.shape[0]):\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size):\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))\n\n        return (output_list,)\n\nNODE_CLASS_MAPPINGS = {\n    \"RebatchLatents\": LatentRebatch,\n    \"RebatchImages\": ImageRebatch,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"RebatchLatents\": \"Rebatch Latents\",\n    \"RebatchImages\": \"Rebatch Images\",\n}\n", "ldm_patched/contrib/external_model_downscale.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nimport ldm_patched.modules.utils\n\nclass PatchModelAddDownscale:\n    upscale_methods = [\"bicubic\", \"nearest-exact\", \"bilinear\", \"area\", \"bislerp\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"block_number\": (\"INT\", {\"default\": 3, \"min\": 1, \"max\": 32, \"step\": 1}),\n                              \"downscale_factor\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.1, \"max\": 9.0, \"step\": 0.001}),\n                              \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"end_percent\": (\"FLOAT\", {\"default\": 0.35, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"downscale_after_skip\": (\"BOOLEAN\", {\"default\": True}),\n                              \"downscale_method\": (s.upscale_methods,),\n                              \"upscale_method\": (s.upscale_methods,),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, block_number, downscale_factor, start_percent, end_percent, downscale_after_skip, downscale_method, upscale_method):\n        sigma_start = model.model.model_sampling.percent_to_sigma(start_percent)\n        sigma_end = model.model.model_sampling.percent_to_sigma(end_percent)\n\n        def input_block_patch(h, transformer_options):\n            if transformer_options[\"block\"][1] == block_number:\n                sigma = transformer_options[\"sigmas\"][0].item()\n                if sigma <= sigma_start and sigma >= sigma_end:\n                    h = ldm_patched.modules.utils.common_upscale(h, round(h.shape[-1] * (1.0 / downscale_factor)), round(h.shape[-2] * (1.0 / downscale_factor)), downscale_method, \"disabled\")\n            return h\n\n        def output_block_patch(h, hsp, transformer_options):\n            if h.shape[2] != hsp.shape[2]:\n                h = ldm_patched.modules.utils.common_upscale(h, hsp.shape[-1], hsp.shape[-2], upscale_method, \"disabled\")\n            return h, hsp\n\n        m = model.clone()\n        if downscale_after_skip:\n            m.set_model_input_block_patch_after_skip(input_block_patch)\n        else:\n            m.set_model_input_block_patch(input_block_patch)\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"PatchModelAddDownscale\": PatchModelAddDownscale,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"PatchModelAddDownscale\": \"PatchModelAddDownscale (Kohya Deep Shrink)\",\n}\n", "ldm_patched/contrib/external_upscale_model.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport os\nfrom ldm_patched.pfn import model_loading\nfrom ldm_patched.modules import model_management\nimport torch\nimport ldm_patched.modules.utils\nimport ldm_patched.utils.path_utils\n\nclass UpscaleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model_name\": (ldm_patched.utils.path_utils.get_filename_list(\"upscale_models\"), ),\n                             }}\n    RETURN_TYPES = (\"UPSCALE_MODEL\",)\n    FUNCTION = \"load_model\"\n\n    CATEGORY = \"loaders\"\n\n    def load_model(self, model_name):\n        model_path = ldm_patched.utils.path_utils.get_full_path(\"upscale_models\", model_name)\n        sd = ldm_patched.modules.utils.load_torch_file(model_path, safe_load=True)\n        if \"module.layers.0.residual_group.blocks.0.norm1.weight\" in sd:\n            sd = ldm_patched.modules.utils.state_dict_prefix_replace(sd, {\"module.\":\"\"})\n        out = model_loading.load_state_dict(sd).eval()\n        return (out, )\n\n\nclass ImageUpscaleWithModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"upscale_model\": (\"UPSCALE_MODEL\",),\n                              \"image\": (\"IMAGE\",),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, upscale_model, image):\n        device = model_management.get_torch_device()\n        upscale_model.to(device)\n        in_img = image.movedim(-1,-3).to(device)\n        free_memory = model_management.get_free_memory(device)\n\n        tile = 512\n        overlap = 32\n\n        oom = True\n        while oom:\n            try:\n                steps = in_img.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(in_img.shape[3], in_img.shape[2], tile_x=tile, tile_y=tile, overlap=overlap)\n                pbar = ldm_patched.modules.utils.ProgressBar(steps)\n                s = ldm_patched.modules.utils.tiled_scale(in_img, lambda a: upscale_model(a), tile_x=tile, tile_y=tile, overlap=overlap, upscale_amount=upscale_model.scale, pbar=pbar)\n                oom = False\n            except model_management.OOM_EXCEPTION as e:\n                tile //= 2\n                if tile < 128:\n                    raise e\n\n        upscale_model.cpu()\n        s = torch.clamp(s.movedim(-3,-1), min=0, max=1.0)\n        return (s,)\n\nNODE_CLASS_MAPPINGS = {\n    \"UpscaleModelLoader\": UpscaleModelLoader,\n    \"ImageUpscaleWithModel\": ImageUpscaleWithModel\n}\n", "ldm_patched/contrib/external_sag.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nfrom torch import einsum\nimport torch.nn.functional as F\nimport math\n\nfrom einops import rearrange, repeat\nimport os\nfrom ldm_patched.ldm.modules.attention import optimized_attention, _ATTN_PRECISION\nimport ldm_patched.modules.samplers\n\n# from ldm_patched.modules/ldm/modules/attention.py\n# but modified to return attention scores as well as output\ndef attention_basic_with_sim(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),\n    )\n\n    # force cast to fp32 to avoid overflowing\n    if _ATTN_PRECISION ==\"fp32\":\n        sim = einsum('b i d, b j d -> b i j', q.float(), k.float()) * scale\n    else:\n        sim = einsum('b i d, b j d -> b i j', q, k) * scale\n\n    del q, k\n\n    if mask is not None:\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n\n    # attention, what we cannot get enough of\n    sim = sim.softmax(dim=-1)\n\n    out = einsum('b i j, b j d -> b i d', sim.to(v.dtype), v)\n    out = (\n        out.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return (out, sim)\n\ndef create_blur_map(x0, attn, sigma=3.0, threshold=1.0):\n    # reshape and GAP the attention map\n    _, hw1, hw2 = attn.shape\n    b, _, lh, lw = x0.shape\n    attn = attn.reshape(b, -1, hw1, hw2)\n    # Global Average Pool\n    mask = attn.mean(1, keepdim=False).sum(1, keepdim=False) > threshold\n    ratio = 2**(math.ceil(math.sqrt(lh * lw / hw1)) - 1).bit_length()\n    mid_shape = [math.ceil(lh / ratio), math.ceil(lw / ratio)]\n\n    # Reshape\n    mask = (\n        mask.reshape(b, *mid_shape)\n        .unsqueeze(1)\n        .type(attn.dtype)\n    )\n    # Upsample\n    mask = F.interpolate(mask, (lh, lw))\n\n    blurred = gaussian_blur_2d(x0, kernel_size=9, sigma=sigma)\n    blurred = blurred * mask + x0 * (1 - mask)\n    return blurred\n\ndef gaussian_blur_2d(img, kernel_size, sigma):\n    ksize_half = (kernel_size - 1) * 0.5\n\n    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size)\n\n    pdf = torch.exp(-0.5 * (x / sigma).pow(2))\n\n    x_kernel = pdf / pdf.sum()\n    x_kernel = x_kernel.to(device=img.device, dtype=img.dtype)\n\n    kernel2d = torch.mm(x_kernel[:, None], x_kernel[None, :])\n    kernel2d = kernel2d.expand(img.shape[-3], 1, kernel2d.shape[0], kernel2d.shape[1])\n\n    padding = [kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2]\n\n    img = F.pad(img, padding, mode=\"reflect\")\n    img = F.conv2d(img, kernel2d, groups=img.shape[-3])\n    return img\n\nclass SelfAttentionGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"scale\": (\"FLOAT\", {\"default\": 0.5, \"min\": -2.0, \"max\": 5.0, \"step\": 0.1}),\n                             \"blur_sigma\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.1}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, scale, blur_sigma):\n        m = model.clone()\n\n        attn_scores = None\n\n        # TODO: make this work properly with chunked batches\n        #       currently, we can only save the attn from one UNet call\n        def attn_and_record(q, k, v, extra_options):\n            nonlocal attn_scores\n            # if uncond, save the attention scores\n            heads = extra_options[\"n_heads\"]\n            cond_or_uncond = extra_options[\"cond_or_uncond\"]\n            b = q.shape[0] // len(cond_or_uncond)\n            if 1 in cond_or_uncond:\n                uncond_index = cond_or_uncond.index(1)\n                # do the entire attention operation, but save the attention scores to attn_scores\n                (out, sim) = attention_basic_with_sim(q, k, v, heads=heads)\n                # when using a higher batch size, I BELIEVE the result batch dimension is [uc1, ... ucn, c1, ... cn]\n                n_slices = heads * b\n                attn_scores = sim[n_slices * uncond_index:n_slices * (uncond_index+1)]\n                return out\n            else:\n                return optimized_attention(q, k, v, heads=heads)\n\n        def post_cfg_function(args):\n            nonlocal attn_scores\n            uncond_attn = attn_scores\n\n            sag_scale = scale\n            sag_sigma = blur_sigma\n            sag_threshold = 1.0\n            model = args[\"model\"]\n            uncond_pred = args[\"uncond_denoised\"]\n            uncond = args[\"uncond\"]\n            cfg_result = args[\"denoised\"]\n            sigma = args[\"sigma\"]\n            model_options = args[\"model_options\"]\n            x = args[\"input\"]\n            if min(cfg_result.shape[2:]) <= 4: #skip when too small to add padding\n                return cfg_result\n\n            # create the adversarially blurred image\n            degraded = create_blur_map(uncond_pred, uncond_attn, sag_sigma, sag_threshold)\n            degraded_noised = degraded + x - uncond_pred\n            # call into the UNet\n            (sag, _) = ldm_patched.modules.samplers.calc_cond_uncond_batch(model, uncond, None, degraded_noised, sigma, model_options)\n            return cfg_result + (degraded - sag) * sag_scale\n\n        m.set_model_sampler_post_cfg_function(post_cfg_function, disable_cfg1_optimization=True)\n\n        # from diffusers:\n        # unet.mid_block.attentions[0].transformer_blocks[0].attn1.patch\n        m.set_model_attn1_replace(attn_and_record, \"middle\", 0, 0)\n\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"SelfAttentionGuidance\": SelfAttentionGuidance,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"SelfAttentionGuidance\": \"Self-Attention Guidance\",\n}\n", "ldm_patched/contrib/external_video_model.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.contrib.external\nimport torch\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.sd\nimport ldm_patched.utils.path_utils\nimport ldm_patched.contrib.external_model_merging\n\n\nclass ImageOnlyCheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (ldm_patched.utils.path_utils.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP_VISION\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders/video_models\"\n\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = ldm_patched.utils.path_utils.get_full_path(\"checkpoints\", ckpt_name)\n        out = ldm_patched.modules.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=False, output_clipvision=True, embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n        return (out[0], out[3], out[2])\n\n\nclass SVD_img2vid_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 576, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"video_frames\": (\"INT\", {\"default\": 14, \"min\": 1, \"max\": 4096}),\n                              \"motion_bucket_id\": (\"INT\", {\"default\": 127, \"min\": 1, \"max\": 1023}),\n                              \"fps\": (\"INT\", {\"default\": 6, \"min\": 1, \"max\": 1024}),\n                              \"augmentation_level\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, video_frames, motion_bucket_id, fps, augmentation_level):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = ldm_patched.modules.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        if augmentation_level > 0:\n            encode_pixels += torch.randn_like(pixels) * augmentation_level\n        t = vae.encode(encode_pixels)\n        positive = [[pooled, {\"motion_bucket_id\": motion_bucket_id, \"fps\": fps, \"augmentation_level\": augmentation_level, \"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"motion_bucket_id\": motion_bucket_id, \"fps\": fps, \"augmentation_level\": augmentation_level, \"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([video_frames, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent})\n\nclass VideoLinearCFGGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"min_cfg\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.5, \"round\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"sampling/video_models\"\n\n    def patch(self, model, min_cfg):\n        def linear_cfg(args):\n            cond = args[\"cond\"]\n            uncond = args[\"uncond\"]\n            cond_scale = args[\"cond_scale\"]\n\n            scale = torch.linspace(min_cfg, cond_scale, cond.shape[0], device=cond.device).reshape((cond.shape[0], 1, 1, 1))\n            return uncond + scale * (cond - uncond)\n\n        m = model.clone()\n        m.set_model_sampler_cfg_function(linear_cfg)\n        return (m, )\n\nclass ImageOnlyCheckpointSave(ldm_patched.contrib.external_model_merging.CheckpointSave):\n    CATEGORY = \"_for_testing\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip_vision\": (\"CLIP_VISION\",),\n                              \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"checkpoints/ldm_patched\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n\n    def save(self, model, clip_vision, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        ldm_patched.contrib.external_model_merging.save_checkpoint(model, clip_vision=clip_vision, vae=vae, filename_prefix=filename_prefix, output_dir=self.output_dir, prompt=prompt, extra_pnginfo=extra_pnginfo)\n        return {}\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageOnlyCheckpointLoader\": ImageOnlyCheckpointLoader,\n    \"SVD_img2vid_Conditioning\": SVD_img2vid_Conditioning,\n    \"VideoLinearCFGGuidance\": VideoLinearCFGGuidance,\n    \"ImageOnlyCheckpointSave\": ImageOnlyCheckpointSave,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"ImageOnlyCheckpointLoader\": \"Image Only Checkpoint Loader (img2vid model)\",\n}\n", "ldm_patched/contrib/external_mask.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport numpy as np\nimport scipy.ndimage\nimport torch\nimport ldm_patched.modules.utils\n\nfrom ldm_patched.contrib.external import MAX_RESOLUTION\n\ndef composite(destination, source, x, y, mask = None, multiplier = 8, resize_source = False):\n    source = source.to(destination.device)\n    if resize_source:\n        source = torch.nn.functional.interpolate(source, size=(destination.shape[2], destination.shape[3]), mode=\"bilinear\")\n\n    source = ldm_patched.modules.utils.repeat_to_batch_size(source, destination.shape[0])\n\n    x = max(-source.shape[3] * multiplier, min(x, destination.shape[3] * multiplier))\n    y = max(-source.shape[2] * multiplier, min(y, destination.shape[2] * multiplier))\n\n    left, top = (x // multiplier, y // multiplier)\n    right, bottom = (left + source.shape[3], top + source.shape[2],)\n\n    if mask is None:\n        mask = torch.ones_like(source)\n    else:\n        mask = mask.to(destination.device, copy=True)\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(source.shape[2], source.shape[3]), mode=\"bilinear\")\n        mask = ldm_patched.modules.utils.repeat_to_batch_size(mask, source.shape[0])\n\n    # calculate the bounds of the source that will be overlapping the destination\n    # this prevents the source trying to overwrite latent pixels that are out of bounds\n    # of the destination\n    visible_width, visible_height = (destination.shape[3] - left + min(0, x), destination.shape[2] - top + min(0, y),)\n\n    mask = mask[:, :, :visible_height, :visible_width]\n    inverse_mask = torch.ones_like(mask) - mask\n\n    source_portion = mask * source[:, :, :visible_height, :visible_width]\n    destination_portion = inverse_mask  * destination[:, :, top:bottom, left:right]\n\n    destination[:, :, top:bottom, left:right] = source_portion + destination_portion\n    return destination\n\nclass LatentCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"LATENT\",),\n                \"source\": (\"LATENT\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"latent\"\n\n    def composite(self, destination, source, x, y, resize_source, mask = None):\n        output = destination.copy()\n        destination = destination[\"samples\"].clone()\n        source = source[\"samples\"]\n        output[\"samples\"] = composite(destination, source, x, y, mask, 8, resize_source)\n        return (output,)\n\nclass ImageCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"IMAGE\",),\n                \"source\": (\"IMAGE\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"image\"\n\n    def composite(self, destination, source, x, y, resize_source, mask = None):\n        destination = destination.clone().movedim(-1, 1)\n        output = composite(destination, source.movedim(-1, 1), x, y, mask, 1, resize_source).movedim(1, -1)\n        return (output,)\n\nclass MaskToImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"mask\": (\"MASK\",),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mask_to_image\"\n\n    def mask_to_image(self, mask):\n        result = mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])).movedim(1, -1).expand(-1, -1, -1, 3)\n        return (result,)\n\nclass ImageToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"channel\": ([\"red\", \"green\", \"blue\", \"alpha\"],),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"image_to_mask\"\n\n    def image_to_mask(self, image, channel):\n        channels = [\"red\", \"green\", \"blue\", \"alpha\"]\n        mask = image[:, :, :, channels.index(channel)]\n        return (mask,)\n\nclass ImageColorToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"image_to_mask\"\n\n    def image_to_mask(self, image, color):\n        temp = (torch.clamp(image, 0, 1.0) * 255.0).round().to(torch.int)\n        temp = torch.bitwise_left_shift(temp[:,:,:,0], 16) + torch.bitwise_left_shift(temp[:,:,:,1], 8) + temp[:,:,:,2]\n        mask = torch.where(temp == color, 255, 0).float()\n        return (mask,)\n\nclass SolidMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"value\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"solid\"\n\n    def solid(self, value, width, height):\n        out = torch.full((1, height, width), value, dtype=torch.float32, device=\"cpu\")\n        return (out,)\n\nclass InvertMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"invert\"\n\n    def invert(self, mask):\n        out = 1.0 - mask\n        return (out,)\n\nclass CropMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"crop\"\n\n    def crop(self, mask, x, y, width, height):\n        mask = mask.reshape((-1, mask.shape[-2], mask.shape[-1]))\n        out = mask[:, y:y + height, x:x + width]\n        return (out,)\n\nclass MaskComposite:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"destination\": (\"MASK\",),\n                \"source\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"operation\": ([\"multiply\", \"add\", \"subtract\", \"and\", \"or\", \"xor\"],),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"combine\"\n\n    def combine(self, destination, source, x, y, operation):\n        output = destination.reshape((-1, destination.shape[-2], destination.shape[-1])).clone()\n        source = source.reshape((-1, source.shape[-2], source.shape[-1]))\n\n        left, top = (x, y,)\n        right, bottom = (min(left + source.shape[-1], destination.shape[-1]), min(top + source.shape[-2], destination.shape[-2]))\n        visible_width, visible_height = (right - left, bottom - top,)\n\n        source_portion = source[:, :visible_height, :visible_width]\n        destination_portion = destination[:, top:bottom, left:right]\n\n        if operation == \"multiply\":\n            output[:, top:bottom, left:right] = destination_portion * source_portion\n        elif operation == \"add\":\n            output[:, top:bottom, left:right] = destination_portion + source_portion\n        elif operation == \"subtract\":\n            output[:, top:bottom, left:right] = destination_portion - source_portion\n        elif operation == \"and\":\n            output[:, top:bottom, left:right] = torch.bitwise_and(destination_portion.round().bool(), source_portion.round().bool()).float()\n        elif operation == \"or\":\n            output[:, top:bottom, left:right] = torch.bitwise_or(destination_portion.round().bool(), source_portion.round().bool()).float()\n        elif operation == \"xor\":\n            output[:, top:bottom, left:right] = torch.bitwise_xor(destination_portion.round().bool(), source_portion.round().bool()).float()\n\n        output = torch.clamp(output, 0.0, 1.0)\n\n        return (output,)\n\nclass FeatherMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"feather\"\n\n    def feather(self, mask, left, top, right, bottom):\n        output = mask.reshape((-1, mask.shape[-2], mask.shape[-1])).clone()\n\n        left = min(left, output.shape[-1])\n        right = min(right, output.shape[-1])\n        top = min(top, output.shape[-2])\n        bottom = min(bottom, output.shape[-2])\n\n        for x in range(left):\n            feather_rate = (x + 1.0) / left\n            output[:, :, x] *= feather_rate\n\n        for x in range(right):\n            feather_rate = (x + 1) / right\n            output[:, :, -x] *= feather_rate\n\n        for y in range(top):\n            feather_rate = (y + 1) / top\n            output[:, y, :] *= feather_rate\n\n        for y in range(bottom):\n            feather_rate = (y + 1) / bottom\n            output[:, -y, :] *= feather_rate\n\n        return (output,)\n    \nclass GrowMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"expand\": (\"INT\", {\"default\": 0, \"min\": -MAX_RESOLUTION, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"tapered_corners\": (\"BOOLEAN\", {\"default\": True}),\n            },\n        }\n    \n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"expand_mask\"\n\n    def expand_mask(self, mask, expand, tapered_corners):\n        c = 0 if tapered_corners else 1\n        kernel = np.array([[c, 1, c],\n                           [1, 1, 1],\n                           [c, 1, c]])\n        mask = mask.reshape((-1, mask.shape[-2], mask.shape[-1]))\n        out = []\n        for m in mask:\n            output = m.numpy()\n            for _ in range(abs(expand)):\n                if expand < 0:\n                    output = scipy.ndimage.grey_erosion(output, footprint=kernel)\n                else:\n                    output = scipy.ndimage.grey_dilation(output, footprint=kernel)\n            output = torch.from_numpy(output)\n            out.append(output)\n        return (torch.stack(out, dim=0),)\n\n\n\nNODE_CLASS_MAPPINGS = {\n    \"LatentCompositeMasked\": LatentCompositeMasked,\n    \"ImageCompositeMasked\": ImageCompositeMasked,\n    \"MaskToImage\": MaskToImage,\n    \"ImageToMask\": ImageToMask,\n    \"ImageColorToMask\": ImageColorToMask,\n    \"SolidMask\": SolidMask,\n    \"InvertMask\": InvertMask,\n    \"CropMask\": CropMask,\n    \"MaskComposite\": MaskComposite,\n    \"FeatherMask\": FeatherMask,\n    \"GrowMask\": GrowMask,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"ImageToMask\": \"Convert Image to Mask\",\n    \"MaskToImage\": \"Convert Mask to Image\",\n}\n", "ldm_patched/contrib/external_perpneg.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.sample\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.utils\n\n\nclass PerpNeg:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"model\": (\"MODEL\", ),\n                             \"empty_conditioning\": (\"CONDITIONING\", ),\n                             \"neg_scale\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0}),\n                            }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, empty_conditioning, neg_scale):\n        m = model.clone()\n        nocond = ldm_patched.modules.sample.convert_cond(empty_conditioning)\n\n        def cfg_function(args):\n            model = args[\"model\"]\n            noise_pred_pos = args[\"cond_denoised\"]\n            noise_pred_neg = args[\"uncond_denoised\"]\n            cond_scale = args[\"cond_scale\"]\n            x = args[\"input\"]\n            sigma = args[\"sigma\"]\n            model_options = args[\"model_options\"]\n            nocond_processed = ldm_patched.modules.samplers.encode_model_conds(model.extra_conds, nocond, x, x.device, \"negative\")\n\n            (noise_pred_nocond, _) = ldm_patched.modules.samplers.calc_cond_uncond_batch(model, nocond_processed, None, x, sigma, model_options)\n\n            pos = noise_pred_pos - noise_pred_nocond\n            neg = noise_pred_neg - noise_pred_nocond\n            perp = ((torch.mul(pos, neg).sum())/(torch.norm(neg)**2)) * neg\n            perp_neg = perp * neg_scale\n            cfg_result = noise_pred_nocond + cond_scale*(pos - perp_neg)\n            cfg_result = x - cfg_result\n            return cfg_result\n\n        m.set_model_sampler_cfg_function(cfg_function)\n\n        return (m, )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"PerpNeg\": PerpNeg,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"PerpNeg\": \"Perp-Neg\",\n}\n", "ldm_patched/contrib/external_custom_sampler.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.sample\nfrom ldm_patched.k_diffusion import sampling as k_diffusion_sampling\nimport ldm_patched.utils.latent_visualization\nimport torch\nimport ldm_patched.modules.utils\n\n\nclass BasicScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"scheduler\": (ldm_patched.modules.samplers.SCHEDULER_NAMES, ),\n                     \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, scheduler, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            total_steps = int(steps/denoise)\n\n        ldm_patched.modules.model_management.load_models_gpu([model])\n        sigmas = ldm_patched.modules.samplers.calculate_sigmas_scheduler(model.model, scheduler, total_steps).cpu()\n        sigmas = sigmas[-(steps + 1):]\n        return (sigmas, )\n\n\nclass KarrasScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 7.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min, rho):\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)\n        return (sigmas, )\n\nclass ExponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min):\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=sigma_min, sigma_max=sigma_max)\n        return (sigmas, )\n\nclass PolyexponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min, rho):\n        sigmas = k_diffusion_sampling.get_sigmas_polyexponential(n=steps, sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)\n        return (sigmas, )\n\nclass SDTurboScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"steps\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 10}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, steps, denoise):\n        start_step = 10 - int(10 * denoise)\n        timesteps = torch.flip(torch.arange(1, 11) * 100 - 1, (0,))[start_step:start_step + steps]\n        sigmas = model.model_sampling.sigma(timesteps)\n        sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])\n        return (sigmas, )\n\nclass VPScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"beta_d\": (\"FLOAT\", {\"default\": 19.9, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}), #TODO: fix default values\n                     \"beta_min\": (\"FLOAT\", {\"default\": 0.1, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"eps_s\": (\"FLOAT\", {\"default\": 0.001, \"min\": 0.0, \"max\": 1.0, \"step\":0.0001, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, beta_d, beta_min, eps_s):\n        sigmas = k_diffusion_sampling.get_sigmas_vp(n=steps, beta_d=beta_d, beta_min=beta_min, eps_s=eps_s)\n        return (sigmas, )\n\nclass SplitSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                    \"step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",\"SIGMAS\")\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, sigmas, step):\n        sigmas1 = sigmas[:step + 1]\n        sigmas2 = sigmas[step:]\n        return (sigmas1, sigmas2)\n\nclass FlipSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, sigmas):\n        sigmas = sigmas.flip(0)\n        if sigmas[0] == 0:\n            sigmas[0] = 0.0001\n        return (sigmas,)\n\nclass KSamplerSelect:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sampler_name\": (ldm_patched.modules.samplers.SAMPLER_NAMES, ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, sampler_name):\n        sampler = ldm_patched.modules.samplers.sampler_object(sampler_name)\n        return (sampler, )\n\nclass SamplerDPMPP_2M_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"solver_type\": (['midpoint', 'heun'], ),\n                     \"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, solver_type, eta, s_noise, noise_device):\n        if noise_device == 'cpu':\n            sampler_name = \"dpmpp_2m_sde\"\n        else:\n            sampler_name = \"dpmpp_2m_sde_gpu\"\n        sampler = ldm_patched.modules.samplers.ksampler(sampler_name, {\"eta\": eta, \"s_noise\": s_noise, \"solver_type\": solver_type})\n        return (sampler, )\n\n\nclass SamplerDPMPP_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"r\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, eta, s_noise, r, noise_device):\n        if noise_device == 'cpu':\n            sampler_name = \"dpmpp_sde\"\n        else:\n            sampler_name = \"dpmpp_sde_gpu\"\n        sampler = ldm_patched.modules.samplers.ksampler(sampler_name, {\"eta\": eta, \"s_noise\": s_noise, \"r\": r})\n        return (sampler, )\n\n\nclass SamplerTCD:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"eta\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n            }\n        }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, eta=0.3):\n        sampler = ldm_patched.modules.samplers.ksampler(\"tcd\", {\"eta\": eta})\n        return (sampler, )\n\n\nclass SamplerCustom:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": (\"BOOLEAN\", {\"default\": True}),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"sampler\": (\"SAMPLER\", ),\n                    \"sigmas\": (\"SIGMAS\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",\"LATENT\")\n    RETURN_NAMES = (\"output\", \"denoised_output\")\n\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling/custom_sampling\"\n\n    def sample(self, model, add_noise, noise_seed, cfg, positive, negative, sampler, sigmas, latent_image):\n        latent = latent_image\n        latent_image = latent[\"samples\"]\n        if not add_noise:\n            noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n        else:\n            batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n            noise = ldm_patched.modules.sample.prepare_noise(latent_image, noise_seed, batch_inds)\n\n        noise_mask = None\n        if \"noise_mask\" in latent:\n            noise_mask = latent[\"noise_mask\"]\n\n        x0_output = {}\n        callback = ldm_patched.utils.latent_visualization.prepare_callback(model, sigmas.shape[-1] - 1, x0_output)\n\n        disable_pbar = not ldm_patched.modules.utils.PROGRESS_BAR_ENABLED\n        samples = ldm_patched.modules.sample.sample_custom(model, noise, cfg, sampler, sigmas, positive, negative, latent_image, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise_seed)\n\n        out = latent.copy()\n        out[\"samples\"] = samples\n        if \"x0\" in x0_output:\n            out_denoised = latent.copy()\n            out_denoised[\"samples\"] = model.model.process_latent_out(x0_output[\"x0\"].cpu())\n        else:\n            out_denoised = out\n        return (out, out_denoised)\n\nNODE_CLASS_MAPPINGS = {\n    \"SamplerCustom\": SamplerCustom,\n    \"BasicScheduler\": BasicScheduler,\n    \"KarrasScheduler\": KarrasScheduler,\n    \"ExponentialScheduler\": ExponentialScheduler,\n    \"PolyexponentialScheduler\": PolyexponentialScheduler,\n    \"VPScheduler\": VPScheduler,\n    \"SDTurboScheduler\": SDTurboScheduler,\n    \"KSamplerSelect\": KSamplerSelect,\n    \"SamplerDPMPP_2M_SDE\": SamplerDPMPP_2M_SDE,\n    \"SamplerDPMPP_SDE\": SamplerDPMPP_SDE,\n    \"SamplerTCD\": SamplerTCD,\n    \"SplitSigmas\": SplitSigmas,\n    \"FlipSigmas\": FlipSigmas,\n}\n", "ldm_patched/contrib/external_latent.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.modules.utils\nimport torch\n\ndef reshape_latent_to(target_shape, latent):\n    if latent.shape[1:] != target_shape[1:]:\n        latent = ldm_patched.modules.utils.common_upscale(latent, target_shape[3], target_shape[2], \"bilinear\", \"center\")\n    return ldm_patched.modules.utils.repeat_to_batch_size(latent, target_shape[0])\n\n\nclass LatentAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n        samples_out[\"samples\"] = s1 + s2\n        return (samples_out,)\n\nclass LatentSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n        samples_out[\"samples\"] = s1 - s2\n        return (samples_out,)\n\nclass LatentMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples, multiplier):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1 * multiplier\n        return (samples_out,)\n\nclass LatentInterpolate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",),\n                              \"samples2\": (\"LATENT\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2, ratio):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n\n        m1 = torch.linalg.vector_norm(s1, dim=(1))\n        m2 = torch.linalg.vector_norm(s2, dim=(1))\n\n        s1 = torch.nan_to_num(s1 / m1)\n        s2 = torch.nan_to_num(s2 / m2)\n\n        t = (s1 * ratio + s2 * (1.0 - ratio))\n        mt = torch.linalg.vector_norm(t, dim=(1))\n        st = torch.nan_to_num(t / mt)\n\n        samples_out[\"samples\"] = st * (m1 * ratio + m2 * (1.0 - ratio))\n        return (samples_out,)\n\nclass LatentBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"batch\"\n\n    CATEGORY = \"latent/batch\"\n\n    def batch(self, samples1, samples2):\n        samples_out = samples1.copy()\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        if s1.shape[1:] != s2.shape[1:]:\n            s2 = ldm_patched.modules.utils.common_upscale(s2, s1.shape[3], s1.shape[2], \"bilinear\", \"center\")\n        s = torch.cat((s1, s2), dim=0)\n        samples_out[\"samples\"] = s\n        samples_out[\"batch_index\"] = samples1.get(\"batch_index\", [x for x in range(0, s1.shape[0])]) + samples2.get(\"batch_index\", [x for x in range(0, s2.shape[0])])\n        return (samples_out,)\n\nclass LatentBatchSeedBehavior:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"seed_behavior\": ([\"random\", \"fixed\"],),}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples, seed_behavior):\n        samples_out = samples.copy()\n        latent = samples[\"samples\"]\n        if seed_behavior == \"random\":\n            if 'batch_index' in samples_out:\n                samples_out.pop('batch_index')\n        elif seed_behavior == \"fixed\":\n            batch_number = samples_out.get(\"batch_index\", [0])[0]\n            samples_out[\"batch_index\"] = [batch_number] * latent.shape[0]\n\n        return (samples_out,)\n\nNODE_CLASS_MAPPINGS = {\n    \"LatentAdd\": LatentAdd,\n    \"LatentSubtract\": LatentSubtract,\n    \"LatentMultiply\": LatentMultiply,\n    \"LatentInterpolate\": LatentInterpolate,\n    \"LatentBatch\": LatentBatch,\n    \"LatentBatchSeedBehavior\": LatentBatchSeedBehavior,\n}\n", "ldm_patched/contrib/external_sdupscale.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nimport ldm_patched.contrib.external\nimport ldm_patched.modules.utils\n\nclass SD_4XUpscale_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"positive\": (\"CONDITIONING\",),\n                              \"negative\": (\"CONDITIONING\",),\n                              \"scale_ratio\": (\"FLOAT\", {\"default\": 4.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/upscale_diffusion\"\n\n    def encode(self, images, positive, negative, scale_ratio, noise_augmentation):\n        width = max(1, round(images.shape[-2] * scale_ratio))\n        height = max(1, round(images.shape[-3] * scale_ratio))\n\n        pixels = ldm_patched.modules.utils.common_upscale((images.movedim(-1,1) * 2.0) - 1.0, width // 4, height // 4, \"bilinear\", \"center\")\n\n        out_cp = []\n        out_cn = []\n\n        for t in positive:\n            n = [t[0], t[1].copy()]\n            n[1]['concat_image'] = pixels\n            n[1]['noise_augmentation'] = noise_augmentation\n            out_cp.append(n)\n\n        for t in negative:\n            n = [t[0], t[1].copy()]\n            n[1]['concat_image'] = pixels\n            n[1]['noise_augmentation'] = noise_augmentation\n            out_cn.append(n)\n\n        latent = torch.zeros([images.shape[0], 4, height // 4, width // 4])\n        return (out_cp, out_cn, {\"samples\":latent})\n\nNODE_CLASS_MAPPINGS = {\n    \"SD_4XUpscale_Conditioning\": SD_4XUpscale_Conditioning,\n}\n", "ldm_patched/contrib/external_freelunch.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\n#code originally taken from: https://github.com/ChenyangSi/FreeU (under MIT License)\n\nimport torch\n\n\ndef Fourier_filter(x, threshold, scale):\n    # FFT\n    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))\n    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))\n\n    B, C, H, W = x_freq.shape\n    mask = torch.ones((B, C, H, W), device=x.device)\n\n    crow, ccol = H // 2, W //2\n    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale\n    x_freq = x_freq * mask\n\n    # IFFT\n    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))\n    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real\n\n    return x_filtered.to(x.dtype)\n\n\nclass FreeU:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.1, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, b1, b2, s1, s2):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n        scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}\n        on_cpu_devices = {}\n\n        def output_block_patch(h, hsp, transformer_options):\n            scale = scale_dict.get(h.shape[1], None)\n            if scale is not None:\n                h[:,:h.shape[1] // 2] = h[:,:h.shape[1] // 2] * scale[0]\n                if hsp.device not in on_cpu_devices:\n                    try:\n                        hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])\n                    except:\n                        print(\"Device\", hsp.device, \"does not support the torch.fft functions used in the FreeU node, switching to CPU.\")\n                        on_cpu_devices[hsp.device] = True\n                        hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n                else:\n                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n\n            return h, hsp\n\n        m = model.clone()\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nclass FreeU_V2:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.3, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.4, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, b1, b2, s1, s2):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n        scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}\n        on_cpu_devices = {}\n\n        def output_block_patch(h, hsp, transformer_options):\n            scale = scale_dict.get(h.shape[1], None)\n            if scale is not None:\n                hidden_mean = h.mean(1).unsqueeze(1)\n                B = hidden_mean.shape[0]\n                hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)\n                hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)\n                hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)\n\n                h[:,:h.shape[1] // 2] = h[:,:h.shape[1] // 2] * ((scale[0] - 1 ) * hidden_mean + 1)\n\n                if hsp.device not in on_cpu_devices:\n                    try:\n                        hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])\n                    except:\n                        print(\"Device\", hsp.device, \"does not support the torch.fft functions used in the FreeU node, switching to CPU.\")\n                        on_cpu_devices[hsp.device] = True\n                        hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n                else:\n                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n\n            return h, hsp\n\n        m = model.clone()\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"FreeU\": FreeU,\n    \"FreeU_V2\": FreeU_V2,\n}\n", "ldm_patched/contrib/external_model_merging.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.modules.sd\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.model_base\nimport ldm_patched.modules.model_management\n\nimport ldm_patched.utils.path_utils\nimport json\nimport os\n\nfrom ldm_patched.modules.args_parser import args\n\nclass ModelMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, ratio):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\nclass ModelSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, multiplier):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, - multiplier, multiplier)\n        return (m, )\n\nclass ModelAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, 1.0, 1.0)\n        return (m, )\n\n\nclass CLIPMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip1\": (\"CLIP\",),\n                              \"clip2\": (\"CLIP\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, clip1, clip2, ratio):\n        m = clip1.clone()\n        kp = clip2.get_key_patches()\n        for k in kp:\n            if k.endswith(\".position_ids\") or k.endswith(\".logit_scale\"):\n                continue\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\nclass ModelMergeBlocks:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"input\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"middle\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, **kwargs):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        default_ratio = next(iter(kwargs.values()))\n\n        for k in kp:\n            ratio = default_ratio\n            k_unet = k[len(\"diffusion_model.\"):]\n\n            last_arg_size = 0\n            for arg in kwargs:\n                if k_unet.startswith(arg) and last_arg_size < len(arg):\n                    ratio = kwargs[arg]\n                    last_arg_size = len(arg)\n\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\ndef save_checkpoint(model, clip=None, vae=None, clip_vision=None, filename_prefix=None, output_dir=None, prompt=None, extra_pnginfo=None):\n    full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, output_dir)\n    prompt_info = \"\"\n    if prompt is not None:\n        prompt_info = json.dumps(prompt)\n\n    metadata = {}\n\n    enable_modelspec = True\n    if isinstance(model.model, ldm_patched.modules.model_base.SDXL):\n        metadata[\"modelspec.architecture\"] = \"stable-diffusion-xl-v1-base\"\n    elif isinstance(model.model, ldm_patched.modules.model_base.SDXLRefiner):\n        metadata[\"modelspec.architecture\"] = \"stable-diffusion-xl-v1-refiner\"\n    else:\n        enable_modelspec = False\n\n    if enable_modelspec:\n        metadata[\"modelspec.sai_model_spec\"] = \"1.0.0\"\n        metadata[\"modelspec.implementation\"] = \"sgm\"\n        metadata[\"modelspec.title\"] = \"{} {}\".format(filename, counter)\n\n    #TODO:\n    # \"stable-diffusion-v1\", \"stable-diffusion-v1-inpainting\", \"stable-diffusion-v2-512\",\n    # \"stable-diffusion-v2-768-v\", \"stable-diffusion-v2-unclip-l\", \"stable-diffusion-v2-unclip-h\",\n    # \"v2-inpainting\"\n\n    if model.model.model_type == ldm_patched.modules.model_base.ModelType.EPS:\n        metadata[\"modelspec.predict_key\"] = \"epsilon\"\n    elif model.model.model_type == ldm_patched.modules.model_base.ModelType.V_PREDICTION:\n        metadata[\"modelspec.predict_key\"] = \"v\"\n\n    if not args.disable_server_info:\n        metadata[\"prompt\"] = prompt_info\n        if extra_pnginfo is not None:\n            for x in extra_pnginfo:\n                metadata[x] = json.dumps(extra_pnginfo[x])\n\n    output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n    output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n    ldm_patched.modules.sd.save_checkpoint(output_checkpoint, model, clip, vae, clip_vision, metadata=metadata)\n\nclass CheckpointSave:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\",),\n                              \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"checkpoints/ldm_patched\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, model, clip, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        save_checkpoint(model, clip=clip, vae=vae, filename_prefix=filename_prefix, output_dir=self.output_dir, prompt=prompt, extra_pnginfo=extra_pnginfo)\n        return {}\n\nclass CLIPSave:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"clip/ldm_patched\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, clip, filename_prefix, prompt=None, extra_pnginfo=None):\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = {}\n        if not args.disable_server_info:\n            metadata[\"prompt\"] = prompt_info\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        ldm_patched.modules.model_management.load_models_gpu([clip.load_model()])\n        clip_sd = clip.get_sd()\n\n        for prefix in [\"clip_l.\", \"clip_g.\", \"\"]:\n            k = list(filter(lambda a: a.startswith(prefix), clip_sd.keys()))\n            current_clip_sd = {}\n            for x in k:\n                current_clip_sd[x] = clip_sd.pop(x)\n            if len(current_clip_sd) == 0:\n                continue\n\n            p = prefix[:-1]\n            replace_prefix = {}\n            filename_prefix_ = filename_prefix\n            if len(p) > 0:\n                filename_prefix_ = \"{}_{}\".format(filename_prefix_, p)\n                replace_prefix[prefix] = \"\"\n            replace_prefix[\"transformer.\"] = \"\"\n\n            full_output_folder, filename, counter, subfolder, filename_prefix_ = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix_, self.output_dir)\n\n            output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n            output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n            current_clip_sd = ldm_patched.modules.utils.state_dict_prefix_replace(current_clip_sd, replace_prefix)\n\n            ldm_patched.modules.utils.save_torch_file(current_clip_sd, output_checkpoint, metadata=metadata)\n        return {}\n\nclass VAESave:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"vae/ldm_patched_vae\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, self.output_dir)\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = {}\n        if not args.disable_server_info:\n            metadata[\"prompt\"] = prompt_info\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n        output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n        ldm_patched.modules.utils.save_torch_file(vae.get_sd(), output_checkpoint, metadata=metadata)\n        return {}\n\nNODE_CLASS_MAPPINGS = {\n    \"ModelMergeSimple\": ModelMergeSimple,\n    \"ModelMergeBlocks\": ModelMergeBlocks,\n    \"ModelMergeSubtract\": ModelSubtract,\n    \"ModelMergeAdd\": ModelAdd,\n    \"CheckpointSave\": CheckpointSave,\n    \"CLIPMergeSimple\": CLIPMergeSimple,\n    \"CLIPSave\": CLIPSave,\n    \"VAESave\": VAESave,\n}\n", "ldm_patched/contrib/external_align_your_steps.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py\n\n#from: https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/howto.html\nimport numpy as np\nimport torch\n\ndef loglinear_interp(t_steps, num_steps):\n    \"\"\"\n    Performs log-linear interpolation of a given array of decreasing numbers.\n    \"\"\"\n    xs = np.linspace(0, 1, len(t_steps))\n    ys = np.log(t_steps[::-1])\n\n    new_xs = np.linspace(0, 1, num_steps)\n    new_ys = np.interp(new_xs, xs, ys)\n\n    interped_ys = np.exp(new_ys)[::-1].copy()\n    return interped_ys\n\nNOISE_LEVELS = {\"SD1\": [14.6146412293, 6.4745760956,  3.8636745985,  2.6946151520, 1.8841921177,  1.3943805092,  0.9642583904,  0.6523686016, 0.3977456272,  0.1515232662,  0.0291671582],\n                \"SDXL\":[14.6146412293, 6.3184485287,  3.7681790315,  2.1811480769, 1.3405244945,  0.8620721141,  0.5550693289,  0.3798540708, 0.2332364134,  0.1114188177,  0.0291671582],\n                \"SVD\": [700.00, 54.5, 15.886, 7.977, 4.248, 1.789, 0.981, 0.403, 0.173, 0.034, 0.002]}\n\nclass AlignYourStepsScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model_type\": ([\"SD1\", \"SDXL\", \"SVD\"], ),\n                     \"steps\": (\"INT\", {\"default\": 10, \"min\": 10, \"max\": 10000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model_type, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            if denoise <= 0.0:\n                return (torch.FloatTensor([]),)\n            total_steps = round(steps * denoise)\n\n        sigmas = NOISE_LEVELS[model_type][:]\n        if (steps + 1) != len(sigmas):\n            sigmas = loglinear_interp(sigmas, steps + 1)\n\n        sigmas = sigmas[-(total_steps + 1):]\n        sigmas[-1] = 0\n        return (torch.FloatTensor(sigmas), )\n\nNODE_CLASS_MAPPINGS = {\n    \"AlignYourStepsScheduler\": AlignYourStepsScheduler,\n}", "ldm_patched/contrib/external_canny.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\n#From https://github.com/kornia/kornia\nimport math\n\nimport torch\nimport torch.nn.functional as F\nimport ldm_patched.modules.model_management\n\ndef get_canny_nms_kernel(device=None, dtype=None):\n    \"\"\"Utility function that returns 3x3 kernels for the Canny Non-maximal suppression.\"\"\"\n    return torch.tensor(\n        [\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, -1.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, -1.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [-1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[-1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, -1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, -1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],\n        ],\n        device=device,\n        dtype=dtype,\n    )\n\n\ndef get_hysteresis_kernel(device=None, dtype=None):\n    \"\"\"Utility function that returns the 3x3 kernels for the Canny hysteresis.\"\"\"\n    return torch.tensor(\n        [\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 1.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],\n        ],\n        device=device,\n        dtype=dtype,\n    )\n\ndef gaussian_blur_2d(img, kernel_size, sigma):\n    ksize_half = (kernel_size - 1) * 0.5\n\n    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size)\n\n    pdf = torch.exp(-0.5 * (x / sigma).pow(2))\n\n    x_kernel = pdf / pdf.sum()\n    x_kernel = x_kernel.to(device=img.device, dtype=img.dtype)\n\n    kernel2d = torch.mm(x_kernel[:, None], x_kernel[None, :])\n    kernel2d = kernel2d.expand(img.shape[-3], 1, kernel2d.shape[0], kernel2d.shape[1])\n\n    padding = [kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2]\n\n    img = torch.nn.functional.pad(img, padding, mode=\"reflect\")\n    img = torch.nn.functional.conv2d(img, kernel2d, groups=img.shape[-3])\n\n    return img\n\ndef get_sobel_kernel2d(device=None, dtype=None):\n    kernel_x = torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]], device=device, dtype=dtype)\n    kernel_y = kernel_x.transpose(0, 1)\n    return torch.stack([kernel_x, kernel_y])\n\ndef spatial_gradient(input, normalized: bool = True):\n    r\"\"\"Compute the first order image derivative in both x and y using a Sobel operator.\n    .. image:: _static/img/spatial_gradient.png\n    Args:\n        input: input image tensor with shape :math:`(B, C, H, W)`.\n        mode: derivatives modality, can be: `sobel` or `diff`.\n        order: the order of the derivatives.\n        normalized: whether the output is normalized.\n    Return:\n        the derivatives of the input feature map. with shape :math:`(B, C, 2, H, W)`.\n    .. note::\n       See a working example `here <https://kornia.readthedocs.io/en/latest/\n       filtering_edges.html>`__.\n    Examples:\n        >>> input = torch.rand(1, 3, 4, 4)\n        >>> output = spatial_gradient(input)  # 1x3x2x4x4\n        >>> output.shape\n        torch.Size([1, 3, 2, 4, 4])\n    \"\"\"\n    # KORNIA_CHECK_IS_TENSOR(input)\n    # KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])\n\n    # allocate kernel\n    kernel = get_sobel_kernel2d(device=input.device, dtype=input.dtype)\n    if normalized:\n        kernel = normalize_kernel2d(kernel)\n\n    # prepare kernel\n    b, c, h, w = input.shape\n    tmp_kernel = kernel[:, None, ...]\n\n    # Pad with \"replicate for spatial dims, but with zeros for channel\n    spatial_pad = [kernel.size(1) // 2, kernel.size(1) // 2, kernel.size(2) // 2, kernel.size(2) // 2]\n    out_channels: int = 2\n    padded_inp = torch.nn.functional.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')\n    out = F.conv2d(padded_inp, tmp_kernel, groups=1, padding=0, stride=1)\n    return out.reshape(b, c, out_channels, h, w)\n\ndef rgb_to_grayscale(image, rgb_weights = None):\n    r\"\"\"Convert a RGB image to grayscale version of image.\n\n    .. image:: _static/img/rgb_to_grayscale.png\n\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        image: RGB image to be converted to grayscale with shape :math:`(*,3,H,W)`.\n        rgb_weights: Weights that will be applied on each channel (RGB).\n            The sum of the weights should add up to one.\n    Returns:\n        grayscale version of the image with shape :math:`(*,1,H,W)`.\n\n    .. note::\n       See a working example `here <https://kornia.readthedocs.io/en/latest/\n       color_conversions.html>`__.\n\n    Example:\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> gray = rgb_to_grayscale(input) # 2x1x4x5\n    \"\"\"\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(f\"Input size must have a shape of (*, 3, H, W). Got {image.shape}\")\n\n    if rgb_weights is None:\n        # 8 bit images\n        if image.dtype == torch.uint8:\n            rgb_weights = torch.tensor([76, 150, 29], device=image.device, dtype=torch.uint8)\n        # floating point images\n        elif image.dtype in (torch.float16, torch.float32, torch.float64):\n            rgb_weights = torch.tensor([0.299, 0.587, 0.114], device=image.device, dtype=image.dtype)\n        else:\n            raise TypeError(f\"Unknown data type: {image.dtype}\")\n    else:\n        # is tensor that we make sure is in the same device/dtype\n        rgb_weights = rgb_weights.to(image)\n\n    # unpack the color image channels with RGB order\n    r: Tensor = image[..., 0:1, :, :]\n    g: Tensor = image[..., 1:2, :, :]\n    b: Tensor = image[..., 2:3, :, :]\n\n    w_r, w_g, w_b = rgb_weights.unbind()\n    return w_r * r + w_g * g + w_b * b\n\ndef canny(\n    input,\n    low_threshold = 0.1,\n    high_threshold = 0.2,\n    kernel_size  = 5,\n    sigma = 1,\n    hysteresis = True,\n    eps = 1e-6,\n):\n    r\"\"\"Find edges of the input image and filters them using the Canny algorithm.\n    .. image:: _static/img/canny.png\n    Args:\n        input: input image tensor with shape :math:`(B,C,H,W)`.\n        low_threshold: lower threshold for the hysteresis procedure.\n        high_threshold: upper threshold for the hysteresis procedure.\n        kernel_size: the size of the kernel for the gaussian blur.\n        sigma: the standard deviation of the kernel for the gaussian blur.\n        hysteresis: if True, applies the hysteresis edge tracking.\n            Otherwise, the edges are divided between weak (0.5) and strong (1) edges.\n        eps: regularization number to avoid NaN during backprop.\n    Returns:\n        - the canny edge magnitudes map, shape of :math:`(B,1,H,W)`.\n        - the canny edge detection filtered by thresholds and hysteresis, shape of :math:`(B,1,H,W)`.\n    .. note::\n       See a working example `here <https://kornia.readthedocs.io/en/latest/\n       canny.html>`__.\n    Example:\n        >>> input = torch.rand(5, 3, 4, 4)\n        >>> magnitude, edges = canny(input)  # 5x3x4x4\n        >>> magnitude.shape\n        torch.Size([5, 1, 4, 4])\n        >>> edges.shape\n        torch.Size([5, 1, 4, 4])\n    \"\"\"\n    # KORNIA_CHECK_IS_TENSOR(input)\n    # KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])\n    # KORNIA_CHECK(\n    #     low_threshold <= high_threshold,\n    #     \"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: \"\n    #     f\"{low_threshold}>{high_threshold}\",\n    # )\n    # KORNIA_CHECK(0 < low_threshold < 1, f'Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}')\n    # KORNIA_CHECK(0 < high_threshold < 1, f'Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}')\n\n    device = input.device\n    dtype = input.dtype\n\n    # To Grayscale\n    if input.shape[1] == 3:\n        input = rgb_to_grayscale(input)\n\n    # Gaussian filter\n    blurred: Tensor = gaussian_blur_2d(input, kernel_size, sigma)\n\n    # Compute the gradients\n    gradients: Tensor = spatial_gradient(blurred, normalized=False)\n\n    # Unpack the edges\n    gx: Tensor = gradients[:, :, 0]\n    gy: Tensor = gradients[:, :, 1]\n\n    # Compute gradient magnitude and angle\n    magnitude: Tensor = torch.sqrt(gx * gx + gy * gy + eps)\n    angle: Tensor = torch.atan2(gy, gx)\n\n    # Radians to Degrees\n    angle = 180.0 * angle / math.pi\n\n    # Round angle to the nearest 45 degree\n    angle = torch.round(angle / 45) * 45\n\n    # Non-maximal suppression\n    nms_kernels: Tensor = get_canny_nms_kernel(device, dtype)\n    nms_magnitude: Tensor = F.conv2d(magnitude, nms_kernels, padding=nms_kernels.shape[-1] // 2)\n\n    # Get the indices for both directions\n    positive_idx: Tensor = (angle / 45) % 8\n    positive_idx = positive_idx.long()\n\n    negative_idx: Tensor = ((angle / 45) + 4) % 8\n    negative_idx = negative_idx.long()\n\n    # Apply the non-maximum suppression to the different directions\n    channel_select_filtered_positive: Tensor = torch.gather(nms_magnitude, 1, positive_idx)\n    channel_select_filtered_negative: Tensor = torch.gather(nms_magnitude, 1, negative_idx)\n\n    channel_select_filtered: Tensor = torch.stack(\n        [channel_select_filtered_positive, channel_select_filtered_negative], 1\n    )\n\n    is_max: Tensor = channel_select_filtered.min(dim=1)[0] > 0.0\n\n    magnitude = magnitude * is_max\n\n    # Threshold\n    edges: Tensor = F.threshold(magnitude, low_threshold, 0.0)\n\n    low: Tensor = magnitude > low_threshold\n    high: Tensor = magnitude > high_threshold\n\n    edges = low * 0.5 + high * 0.5\n    edges = edges.to(dtype)\n\n    # Hysteresis\n    if hysteresis:\n        edges_old: Tensor = -torch.ones(edges.shape, device=edges.device, dtype=dtype)\n        hysteresis_kernels: Tensor = get_hysteresis_kernel(device, dtype)\n\n        while ((edges_old - edges).abs() != 0).any():\n            weak: Tensor = (edges == 0.5).float()\n            strong: Tensor = (edges == 1).float()\n\n            hysteresis_magnitude: Tensor = F.conv2d(\n                edges, hysteresis_kernels, padding=hysteresis_kernels.shape[-1] // 2\n            )\n            hysteresis_magnitude = (hysteresis_magnitude == 1).any(1, keepdim=True).to(dtype)\n            hysteresis_magnitude = hysteresis_magnitude * weak + strong\n\n            edges_old = edges.clone()\n            edges = hysteresis_magnitude + (hysteresis_magnitude == 0) * weak * 0.5\n\n        edges = hysteresis_magnitude\n\n    return magnitude, edges\n\n\nclass Canny:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"image\": (\"IMAGE\",),\n                                \"low_threshold\": (\"FLOAT\", {\"default\": 0.4, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01}),\n                                \"high_threshold\": (\"FLOAT\", {\"default\": 0.8, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01})\n                                }}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"detect_edge\"\n\n    CATEGORY = \"image/preprocessors\"\n\n    def detect_edge(self, image, low_threshold, high_threshold):\n        output = canny(image.to(ldm_patched.modules.model_management.get_torch_device()).movedim(-1, 1), low_threshold, high_threshold)\n        img_out = output[1].to(ldm_patched.modules.model_management.intermediate_device()).repeat(1, 3, 1, 1).movedim(1, -1)\n        return (img_out,)\n\nNODE_CLASS_MAPPINGS = {\n    \"Canny\": Canny,\n}\n", "ldm_patched/contrib/external_hypernetwork.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.modules.utils\nimport ldm_patched.utils.path_utils\nimport torch\n\ndef load_hypernetwork_patch(path, strength):\n    sd = ldm_patched.modules.utils.load_torch_file(path, safe_load=True)\n    activation_func = sd.get('activation_func', 'linear')\n    is_layer_norm = sd.get('is_layer_norm', False)\n    use_dropout = sd.get('use_dropout', False)\n    activate_output = sd.get('activate_output', False)\n    last_layer_dropout = sd.get('last_layer_dropout', False)\n\n    valid_activation = {\n        \"linear\": torch.nn.Identity,\n        \"relu\": torch.nn.ReLU,\n        \"leakyrelu\": torch.nn.LeakyReLU,\n        \"elu\": torch.nn.ELU,\n        \"swish\": torch.nn.Hardswish,\n        \"tanh\": torch.nn.Tanh,\n        \"sigmoid\": torch.nn.Sigmoid,\n        \"softsign\": torch.nn.Softsign,\n        \"mish\": torch.nn.Mish,\n    }\n\n    if activation_func not in valid_activation:\n        print(\"Unsupported Hypernetwork format, if you report it I might implement it.\", path, \" \", activation_func, is_layer_norm, use_dropout, activate_output, last_layer_dropout)\n        return None\n\n    out = {}\n\n    for d in sd:\n        try:\n            dim = int(d)\n        except:\n            continue\n\n        output = []\n        for index in [0, 1]:\n            attn_weights = sd[dim][index]\n            keys = attn_weights.keys()\n\n            linears = filter(lambda a: a.endswith(\".weight\"), keys)\n            linears = list(map(lambda a: a[:-len(\".weight\")], linears))\n            layers = []\n\n            i = 0\n            while i < len(linears):\n                lin_name = linears[i]\n                last_layer = (i == (len(linears) - 1))\n                penultimate_layer = (i == (len(linears) - 2))\n\n                lin_weight = attn_weights['{}.weight'.format(lin_name)]\n                lin_bias = attn_weights['{}.bias'.format(lin_name)]\n                layer = torch.nn.Linear(lin_weight.shape[1], lin_weight.shape[0])\n                layer.load_state_dict({\"weight\": lin_weight, \"bias\": lin_bias})\n                layers.append(layer)\n                if activation_func != \"linear\":\n                    if (not last_layer) or (activate_output):\n                        layers.append(valid_activation[activation_func]())\n                if is_layer_norm:\n                    i += 1\n                    ln_name = linears[i]\n                    ln_weight = attn_weights['{}.weight'.format(ln_name)]\n                    ln_bias = attn_weights['{}.bias'.format(ln_name)]\n                    ln = torch.nn.LayerNorm(ln_weight.shape[0])\n                    ln.load_state_dict({\"weight\": ln_weight, \"bias\": ln_bias})\n                    layers.append(ln)\n                if use_dropout:\n                    if (not last_layer) and (not penultimate_layer or last_layer_dropout):\n                        layers.append(torch.nn.Dropout(p=0.3))\n                i += 1\n\n            output.append(torch.nn.Sequential(*layers))\n        out[dim] = torch.nn.ModuleList(output)\n\n    class hypernetwork_patch:\n        def __init__(self, hypernet, strength):\n            self.hypernet = hypernet\n            self.strength = strength\n        def __call__(self, q, k, v, extra_options):\n            dim = k.shape[-1]\n            if dim in self.hypernet:\n                hn = self.hypernet[dim]\n                k = k + hn[0](k) * self.strength\n                v = v + hn[1](v) * self.strength\n\n            return q, k, v\n\n        def to(self, device):\n            for d in self.hypernet.keys():\n                self.hypernet[d] = self.hypernet[d].to(device)\n            return self\n\n    return hypernetwork_patch(out, strength)\n\nclass HypernetworkLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"hypernetwork_name\": (ldm_patched.utils.path_utils.get_filename_list(\"hypernetworks\"), ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_hypernetwork\"\n\n    CATEGORY = \"loaders\"\n\n    def load_hypernetwork(self, model, hypernetwork_name, strength):\n        hypernetwork_path = ldm_patched.utils.path_utils.get_full_path(\"hypernetworks\", hypernetwork_name)\n        model_hypernetwork = model.clone()\n        patch = load_hypernetwork_patch(hypernetwork_path, strength)\n        if patch is not None:\n            model_hypernetwork.set_model_attn1_patch(patch)\n            model_hypernetwork.set_model_attn2_patch(patch)\n        return (model_hypernetwork,)\n\nNODE_CLASS_MAPPINGS = {\n    \"HypernetworkLoader\": HypernetworkLoader\n}\n", "ldm_patched/contrib/external_compositing.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport numpy as np\nimport torch\nimport ldm_patched.modules.utils\nfrom enum import Enum\n\ndef resize_mask(mask, shape):\n    return torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(shape[0], shape[1]), mode=\"bilinear\").squeeze(1)\n\nclass PorterDuffMode(Enum):\n    ADD = 0\n    CLEAR = 1\n    DARKEN = 2\n    DST = 3\n    DST_ATOP = 4\n    DST_IN = 5\n    DST_OUT = 6\n    DST_OVER = 7\n    LIGHTEN = 8\n    MULTIPLY = 9\n    OVERLAY = 10\n    SCREEN = 11\n    SRC = 12\n    SRC_ATOP = 13\n    SRC_IN = 14\n    SRC_OUT = 15\n    SRC_OVER = 16\n    XOR = 17\n\n\ndef porter_duff_composite(src_image: torch.Tensor, src_alpha: torch.Tensor, dst_image: torch.Tensor, dst_alpha: torch.Tensor, mode: PorterDuffMode):\n    if mode == PorterDuffMode.ADD:\n        out_alpha = torch.clamp(src_alpha + dst_alpha, 0, 1)\n        out_image = torch.clamp(src_image + dst_image, 0, 1)\n    elif mode == PorterDuffMode.CLEAR:\n        out_alpha = torch.zeros_like(dst_alpha)\n        out_image = torch.zeros_like(dst_image)\n    elif mode == PorterDuffMode.DARKEN:\n        out_alpha = src_alpha + dst_alpha  - src_alpha * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image + torch.min(src_image, dst_image)\n    elif mode == PorterDuffMode.DST:\n        out_alpha = dst_alpha\n        out_image = dst_image\n    elif mode == PorterDuffMode.DST_ATOP:\n        out_alpha = src_alpha\n        out_image = src_alpha * dst_image + (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.DST_IN:\n        out_alpha = src_alpha * dst_alpha\n        out_image = dst_image * src_alpha\n    elif mode == PorterDuffMode.DST_OUT:\n        out_alpha = (1 - src_alpha) * dst_alpha\n        out_image = (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.DST_OVER:\n        out_alpha = dst_alpha + (1 - dst_alpha) * src_alpha\n        out_image = dst_image + (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.LIGHTEN:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image + torch.max(src_image, dst_image)\n    elif mode == PorterDuffMode.MULTIPLY:\n        out_alpha = src_alpha * dst_alpha\n        out_image = src_image * dst_image\n    elif mode == PorterDuffMode.OVERLAY:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = torch.where(2 * dst_image < dst_alpha, 2 * src_image * dst_image,\n            src_alpha * dst_alpha - 2 * (dst_alpha - src_image) * (src_alpha - dst_image))\n    elif mode == PorterDuffMode.SCREEN:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = src_image + dst_image - src_image * dst_image\n    elif mode == PorterDuffMode.SRC:\n        out_alpha = src_alpha\n        out_image = src_image\n    elif mode == PorterDuffMode.SRC_ATOP:\n        out_alpha = dst_alpha\n        out_image = dst_alpha * src_image + (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.SRC_IN:\n        out_alpha = src_alpha * dst_alpha\n        out_image = src_image * dst_alpha\n    elif mode == PorterDuffMode.SRC_OUT:\n        out_alpha = (1 - dst_alpha) * src_alpha\n        out_image = (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.SRC_OVER:\n        out_alpha = src_alpha + (1 - src_alpha) * dst_alpha\n        out_image = src_image + (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.XOR:\n        out_alpha = (1 - dst_alpha) * src_alpha + (1 - src_alpha) * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image\n    else:\n        out_alpha = None\n        out_image = None\n    return out_image, out_alpha\n\n\nclass PorterDuffImageComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"source\": (\"IMAGE\",),\n                \"source_alpha\": (\"MASK\",),\n                \"destination\": (\"IMAGE\",),\n                \"destination_alpha\": (\"MASK\",),\n                \"mode\": ([mode.name for mode in PorterDuffMode], {\"default\": PorterDuffMode.DST.name}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"composite\"\n    CATEGORY = \"mask/compositing\"\n\n    def composite(self, source: torch.Tensor, source_alpha: torch.Tensor, destination: torch.Tensor, destination_alpha: torch.Tensor, mode):\n        batch_size = min(len(source), len(source_alpha), len(destination), len(destination_alpha))\n        out_images = []\n        out_alphas = []\n\n        for i in range(batch_size):\n            src_image = source[i]\n            dst_image = destination[i]\n\n            assert src_image.shape[2] == dst_image.shape[2] # inputs need to have same number of channels\n\n            src_alpha = source_alpha[i].unsqueeze(2)\n            dst_alpha = destination_alpha[i].unsqueeze(2)\n\n            if dst_alpha.shape[:2] != dst_image.shape[:2]:\n                upscale_input = dst_alpha.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = ldm_patched.modules.utils.common_upscale(upscale_input, dst_image.shape[1], dst_image.shape[0], upscale_method='bicubic', crop='center')\n                dst_alpha = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n            if src_image.shape != dst_image.shape:\n                upscale_input = src_image.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = ldm_patched.modules.utils.common_upscale(upscale_input, dst_image.shape[1], dst_image.shape[0], upscale_method='bicubic', crop='center')\n                src_image = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n            if src_alpha.shape != dst_alpha.shape:\n                upscale_input = src_alpha.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = ldm_patched.modules.utils.common_upscale(upscale_input, dst_alpha.shape[1], dst_alpha.shape[0], upscale_method='bicubic', crop='center')\n                src_alpha = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n\n            out_image, out_alpha = porter_duff_composite(src_image, src_alpha, dst_image, dst_alpha, PorterDuffMode[mode])\n\n            out_images.append(out_image)\n            out_alphas.append(out_alpha.squeeze(2))\n\n        result = (torch.stack(out_images), torch.stack(out_alphas))\n        return result\n\n\nclass SplitImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                }\n        }\n\n    CATEGORY = \"mask/compositing\"\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"split_image_with_alpha\"\n\n    def split_image_with_alpha(self, image: torch.Tensor):\n        out_images = [i[:,:,:3] for i in image]\n        out_alphas = [i[:,:,3] if i.shape[2] > 3 else torch.ones_like(i[:,:,0]) for i in image]\n        result = (torch.stack(out_images), 1.0 - torch.stack(out_alphas))\n        return result\n\n\nclass JoinImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"alpha\": (\"MASK\",),\n                }\n        }\n\n    CATEGORY = \"mask/compositing\"\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"join_image_with_alpha\"\n\n    def join_image_with_alpha(self, image: torch.Tensor, alpha: torch.Tensor):\n        batch_size = min(len(image), len(alpha))\n        out_images = []\n\n        alpha = 1.0 - resize_mask(alpha, image.shape[1:])\n        for i in range(batch_size):\n           out_images.append(torch.cat((image[i][:,:,:3], alpha[i].unsqueeze(2)), dim=2))\n\n        result = (torch.stack(out_images),)\n        return result\n\n\nNODE_CLASS_MAPPINGS = {\n    \"PorterDuffImageComposite\": PorterDuffImageComposite,\n    \"SplitImageWithAlpha\": SplitImageWithAlpha,\n    \"JoinImageWithAlpha\": JoinImageWithAlpha,\n}\n\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"PorterDuffImageComposite\": \"Porter-Duff Image Composite\",\n    \"SplitImageWithAlpha\": \"Split Image with Alpha\",\n    \"JoinImageWithAlpha\": \"Join Image with Alpha\",\n}\n", "ldm_patched/contrib/external_clip_sdxl.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nfrom ldm_patched.contrib.external import MAX_RESOLUTION\n\nclass CLIPTextEncodeSDXLRefiner:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"ascore\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", ),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def encode(self, clip, ascore, width, height, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled, \"aesthetic_score\": ascore, \"width\": width,\"height\": height}]], )\n\nclass CLIPTextEncodeSDXL:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_w\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_h\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"text_g\": (\"STRING\", {\"multiline\": True, \"default\": \"CLIP_G\"}), \"clip\": (\"CLIP\", ),\n            \"text_l\": (\"STRING\", {\"multiline\": True, \"default\": \"CLIP_L\"}), \"clip\": (\"CLIP\", ),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def encode(self, clip, width, height, crop_w, crop_h, target_width, target_height, text_g, text_l):\n        tokens = clip.tokenize(text_g)\n        tokens[\"l\"] = clip.tokenize(text_l)[\"l\"]\n        if len(tokens[\"l\"]) != len(tokens[\"g\"]):\n            empty = clip.tokenize(\"\")\n            while len(tokens[\"l\"]) < len(tokens[\"g\"]):\n                tokens[\"l\"] += empty[\"l\"]\n            while len(tokens[\"l\"]) > len(tokens[\"g\"]):\n                tokens[\"g\"] += empty[\"g\"]\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled, \"width\": width, \"height\": height, \"crop_w\": crop_w, \"crop_h\": crop_h, \"target_width\": target_width, \"target_height\": target_height}]], )\n\nNODE_CLASS_MAPPINGS = {\n    \"CLIPTextEncodeSDXLRefiner\": CLIPTextEncodeSDXLRefiner,\n    \"CLIPTextEncodeSDXL\": CLIPTextEncodeSDXL,\n}\n", "ldm_patched/contrib/external_post_processing.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport math\n\nimport ldm_patched.modules.utils\n\n\nclass Blend:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",),\n                \"image2\": (\"IMAGE\",),\n                \"blend_factor\": (\"FLOAT\", {\n                    \"default\": 0.5,\n                    \"min\": 0.0,\n                    \"max\": 1.0,\n                    \"step\": 0.01\n                }),\n                \"blend_mode\": ([\"normal\", \"multiply\", \"screen\", \"overlay\", \"soft_light\", \"difference\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"blend_images\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def blend_images(self, image1: torch.Tensor, image2: torch.Tensor, blend_factor: float, blend_mode: str):\n        image2 = image2.to(image1.device)\n        if image1.shape != image2.shape:\n            image2 = image2.permute(0, 3, 1, 2)\n            image2 = ldm_patched.modules.utils.common_upscale(image2, image1.shape[2], image1.shape[1], upscale_method='bicubic', crop='center')\n            image2 = image2.permute(0, 2, 3, 1)\n\n        blended_image = self.blend_mode(image1, image2, blend_mode)\n        blended_image = image1 * (1 - blend_factor) + blended_image * blend_factor\n        blended_image = torch.clamp(blended_image, 0, 1)\n        return (blended_image,)\n\n    def blend_mode(self, img1, img2, mode):\n        if mode == \"normal\":\n            return img2\n        elif mode == \"multiply\":\n            return img1 * img2\n        elif mode == \"screen\":\n            return 1 - (1 - img1) * (1 - img2)\n        elif mode == \"overlay\":\n            return torch.where(img1 <= 0.5, 2 * img1 * img2, 1 - 2 * (1 - img1) * (1 - img2))\n        elif mode == \"soft_light\":\n            return torch.where(img2 <= 0.5, img1 - (1 - 2 * img2) * img1 * (1 - img1), img1 + (2 * img2 - 1) * (self.g(img1) - img1))\n        elif mode == \"difference\":\n            return img1 - img2\n        else:\n            raise ValueError(f\"Unsupported blend mode: {mode}\")\n\n    def g(self, x):\n        return torch.where(x <= 0.25, ((16 * x - 12) * x + 4) * x, torch.sqrt(x))\n\ndef gaussian_kernel(kernel_size: int, sigma: float, device=None):\n    x, y = torch.meshgrid(torch.linspace(-1, 1, kernel_size, device=device), torch.linspace(-1, 1, kernel_size, device=device), indexing=\"ij\")\n    d = torch.sqrt(x * x + y * y)\n    g = torch.exp(-(d * d) / (2.0 * sigma * sigma))\n    return g / g.sum()\n\nclass Blur:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"blur_radius\": (\"INT\", {\n                    \"default\": 1,\n                    \"min\": 1,\n                    \"max\": 31,\n                    \"step\": 1\n                }),\n                \"sigma\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.1,\n                    \"max\": 10.0,\n                    \"step\": 0.1\n                }),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"blur\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def blur(self, image: torch.Tensor, blur_radius: int, sigma: float):\n        if blur_radius == 0:\n            return (image,)\n\n        batch_size, height, width, channels = image.shape\n\n        kernel_size = blur_radius * 2 + 1\n        kernel = gaussian_kernel(kernel_size, sigma, device=image.device).repeat(channels, 1, 1).unsqueeze(1)\n\n        image = image.permute(0, 3, 1, 2) # Torch wants (B, C, H, W) we use (B, H, W, C)\n        padded_image = F.pad(image, (blur_radius,blur_radius,blur_radius,blur_radius), 'reflect')\n        blurred = F.conv2d(padded_image, kernel, padding=kernel_size // 2, groups=channels)[:,:,blur_radius:-blur_radius, blur_radius:-blur_radius]\n        blurred = blurred.permute(0, 2, 3, 1)\n\n        return (blurred,)\n\nclass Quantize:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"colors\": (\"INT\", {\n                    \"default\": 256,\n                    \"min\": 1,\n                    \"max\": 256,\n                    \"step\": 1\n                }),\n                \"dither\": ([\"none\", \"floyd-steinberg\", \"bayer-2\", \"bayer-4\", \"bayer-8\", \"bayer-16\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"quantize\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def bayer(im, pal_im, order):\n        def normalized_bayer_matrix(n):\n            if n == 0:\n                return np.zeros((1,1), \"float32\")\n            else:\n                q = 4 ** n\n                m = q * normalized_bayer_matrix(n - 1)\n                return np.bmat(((m-1.5, m+0.5), (m+1.5, m-0.5))) / q\n\n        num_colors = len(pal_im.getpalette()) // 3\n        spread = 2 * 256 / num_colors\n        bayer_n = int(math.log2(order))\n        bayer_matrix = torch.from_numpy(spread * normalized_bayer_matrix(bayer_n) + 0.5)\n\n        result = torch.from_numpy(np.array(im).astype(np.float32))\n        tw = math.ceil(result.shape[0] / bayer_matrix.shape[0])\n        th = math.ceil(result.shape[1] / bayer_matrix.shape[1])\n        tiled_matrix = bayer_matrix.tile(tw, th).unsqueeze(-1)\n        result.add_(tiled_matrix[:result.shape[0],:result.shape[1]]).clamp_(0, 255)\n        result = result.to(dtype=torch.uint8)\n\n        im = Image.fromarray(result.cpu().numpy())\n        im = im.quantize(palette=pal_im, dither=Image.Dither.NONE)\n        return im\n\n    def quantize(self, image: torch.Tensor, colors: int, dither: str):\n        batch_size, height, width, _ = image.shape\n        result = torch.zeros_like(image)\n\n        for b in range(batch_size):\n            im = Image.fromarray((image[b] * 255).to(torch.uint8).numpy(), mode='RGB')\n\n            pal_im = im.quantize(colors=colors) # Required as described in https://github.com/python-pillow/Pillow/issues/5836\n\n            if dither == \"none\":\n                quantized_image = im.quantize(palette=pal_im, dither=Image.Dither.NONE)\n            elif dither == \"floyd-steinberg\":\n                quantized_image = im.quantize(palette=pal_im, dither=Image.Dither.FLOYDSTEINBERG)\n            elif dither.startswith(\"bayer\"):\n                order = int(dither.split('-')[-1])\n                quantized_image = Quantize.bayer(im, pal_im, order)\n\n            quantized_array = torch.tensor(np.array(quantized_image.convert(\"RGB\"))).float() / 255\n            result[b] = quantized_array\n\n        return (result,)\n\nclass Sharpen:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"sharpen_radius\": (\"INT\", {\n                    \"default\": 1,\n                    \"min\": 1,\n                    \"max\": 31,\n                    \"step\": 1\n                }),\n                \"sigma\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.1,\n                    \"max\": 10.0,\n                    \"step\": 0.1\n                }),\n                \"alpha\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.0,\n                    \"max\": 5.0,\n                    \"step\": 0.1\n                }),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"sharpen\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def sharpen(self, image: torch.Tensor, sharpen_radius: int, sigma:float, alpha: float):\n        if sharpen_radius == 0:\n            return (image,)\n\n        batch_size, height, width, channels = image.shape\n\n        kernel_size = sharpen_radius * 2 + 1\n        kernel = gaussian_kernel(kernel_size, sigma, device=image.device) * -(alpha*10)\n        center = kernel_size // 2\n        kernel[center, center] = kernel[center, center] - kernel.sum() + 1.0\n        kernel = kernel.repeat(channels, 1, 1).unsqueeze(1)\n\n        tensor_image = image.permute(0, 3, 1, 2) # Torch wants (B, C, H, W) we use (B, H, W, C)\n        tensor_image = F.pad(tensor_image, (sharpen_radius,sharpen_radius,sharpen_radius,sharpen_radius), 'reflect')\n        sharpened = F.conv2d(tensor_image, kernel, padding=center, groups=channels)[:,:,sharpen_radius:-sharpen_radius, sharpen_radius:-sharpen_radius]\n        sharpened = sharpened.permute(0, 2, 3, 1)\n\n        result = torch.clamp(sharpened, 0, 1)\n\n        return (result,)\n\nclass ImageScaleToTotalPixels:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"megapixels\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 16.0, \"step\": 0.01}),\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, megapixels):\n        samples = image.movedim(-1,1)\n        total = int(megapixels * 1024 * 1024)\n\n        scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))\n        width = round(samples.shape[3] * scale_by)\n        height = round(samples.shape[2] * scale_by)\n\n        s = ldm_patched.modules.utils.common_upscale(samples, width, height, upscale_method, \"disabled\")\n        s = s.movedim(1,-1)\n        return (s,)\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageBlend\": Blend,\n    \"ImageBlur\": Blur,\n    \"ImageQuantize\": Quantize,\n    \"ImageSharpen\": Sharpen,\n    \"ImageScaleToTotalPixels\": ImageScaleToTotalPixels,\n}\n", "ldm_patched/contrib/external_photomaker.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nimport torch.nn as nn\nimport ldm_patched.utils.path_utils\nimport ldm_patched.modules.clip_model\nimport ldm_patched.modules.clip_vision\nimport ldm_patched.modules.ops\n\n# code for model from: https://github.com/TencentARC/PhotoMaker/blob/main/photomaker/model.py under Apache License Version 2.0\nVISION_CONFIG_DICT = {\n    \"hidden_size\": 1024,\n    \"image_size\": 224,\n    \"intermediate_size\": 4096,\n    \"num_attention_heads\": 16,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 24,\n    \"patch_size\": 14,\n    \"projection_dim\": 768,\n    \"hidden_act\": \"quick_gelu\",\n}\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim, use_residual=True, operations=ldm_patched.modules.ops):\n        super().__init__()\n        if use_residual:\n            assert in_dim == out_dim\n        self.layernorm = operations.LayerNorm(in_dim)\n        self.fc1 = operations.Linear(in_dim, hidden_dim)\n        self.fc2 = operations.Linear(hidden_dim, out_dim)\n        self.use_residual = use_residual\n        self.act_fn = nn.GELU()\n\n    def forward(self, x):\n        residual = x\n        x = self.layernorm(x)\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.fc2(x)\n        if self.use_residual:\n            x = x + residual\n        return x\n\n\nclass FuseModule(nn.Module):\n    def __init__(self, embed_dim, operations):\n        super().__init__()\n        self.mlp1 = MLP(embed_dim * 2, embed_dim, embed_dim, use_residual=False, operations=operations)\n        self.mlp2 = MLP(embed_dim, embed_dim, embed_dim, use_residual=True, operations=operations)\n        self.layer_norm = operations.LayerNorm(embed_dim)\n\n    def fuse_fn(self, prompt_embeds, id_embeds):\n        stacked_id_embeds = torch.cat([prompt_embeds, id_embeds], dim=-1)\n        stacked_id_embeds = self.mlp1(stacked_id_embeds) + prompt_embeds\n        stacked_id_embeds = self.mlp2(stacked_id_embeds)\n        stacked_id_embeds = self.layer_norm(stacked_id_embeds)\n        return stacked_id_embeds\n\n    def forward(\n        self,\n        prompt_embeds,\n        id_embeds,\n        class_tokens_mask,\n    ) -> torch.Tensor:\n        # id_embeds shape: [b, max_num_inputs, 1, 2048]\n        id_embeds = id_embeds.to(prompt_embeds.dtype)\n        num_inputs = class_tokens_mask.sum().unsqueeze(0) # TODO: check for training case\n        batch_size, max_num_inputs = id_embeds.shape[:2]\n        # seq_length: 77\n        seq_length = prompt_embeds.shape[1]\n        # flat_id_embeds shape: [b*max_num_inputs, 1, 2048]\n        flat_id_embeds = id_embeds.view(\n            -1, id_embeds.shape[-2], id_embeds.shape[-1]\n        )\n        # valid_id_mask [b*max_num_inputs]\n        valid_id_mask = (\n            torch.arange(max_num_inputs, device=flat_id_embeds.device)[None, :]\n            < num_inputs[:, None]\n        )\n        valid_id_embeds = flat_id_embeds[valid_id_mask.flatten()]\n\n        prompt_embeds = prompt_embeds.view(-1, prompt_embeds.shape[-1])\n        class_tokens_mask = class_tokens_mask.view(-1)\n        valid_id_embeds = valid_id_embeds.view(-1, valid_id_embeds.shape[-1])\n        # slice out the image token embeddings\n        image_token_embeds = prompt_embeds[class_tokens_mask]\n        stacked_id_embeds = self.fuse_fn(image_token_embeds, valid_id_embeds)\n        assert class_tokens_mask.sum() == stacked_id_embeds.shape[0], f\"{class_tokens_mask.sum()} != {stacked_id_embeds.shape[0]}\"\n        prompt_embeds.masked_scatter_(class_tokens_mask[:, None], stacked_id_embeds.to(prompt_embeds.dtype))\n        updated_prompt_embeds = prompt_embeds.view(batch_size, seq_length, -1)\n        return updated_prompt_embeds\n\nclass PhotoMakerIDEncoder(ldm_patched.modules.clip_model.CLIPVisionModelProjection):\n    def __init__(self):\n        self.load_device = ldm_patched.modules.model_management.text_encoder_device()\n        offload_device = ldm_patched.modules.model_management.text_encoder_offload_device()\n        dtype = ldm_patched.modules.model_management.text_encoder_dtype(self.load_device)\n\n        super().__init__(VISION_CONFIG_DICT, dtype, offload_device, ldm_patched.modules.ops.manual_cast)\n        self.visual_projection_2 = ldm_patched.modules.ops.manual_cast.Linear(1024, 1280, bias=False)\n        self.fuse_module = FuseModule(2048, ldm_patched.modules.ops.manual_cast)\n\n    def forward(self, id_pixel_values, prompt_embeds, class_tokens_mask):\n        b, num_inputs, c, h, w = id_pixel_values.shape\n        id_pixel_values = id_pixel_values.view(b * num_inputs, c, h, w)\n\n        shared_id_embeds = self.vision_model(id_pixel_values)[2]\n        id_embeds = self.visual_projection(shared_id_embeds)\n        id_embeds_2 = self.visual_projection_2(shared_id_embeds)\n\n        id_embeds = id_embeds.view(b, num_inputs, 1, -1)\n        id_embeds_2 = id_embeds_2.view(b, num_inputs, 1, -1)\n\n        id_embeds = torch.cat((id_embeds, id_embeds_2), dim=-1)\n        updated_prompt_embeds = self.fuse_module(prompt_embeds, id_embeds, class_tokens_mask)\n\n        return updated_prompt_embeds\n\n\nclass PhotoMakerLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"photomaker_model_name\": (ldm_patched.utils.path_utils.get_filename_list(\"photomaker\"), )}}\n\n    RETURN_TYPES = (\"PHOTOMAKER\",)\n    FUNCTION = \"load_photomaker_model\"\n\n    CATEGORY = \"_for_testing/photomaker\"\n\n    def load_photomaker_model(self, photomaker_model_name):\n        photomaker_model_path = ldm_patched.utils.path_utils.get_full_path(\"photomaker\", photomaker_model_name)\n        photomaker_model = PhotoMakerIDEncoder()\n        data = ldm_patched.modules.utils.load_torch_file(photomaker_model_path, safe_load=True)\n        if \"id_encoder\" in data:\n            data = data[\"id_encoder\"]\n        photomaker_model.load_state_dict(data)\n        return (photomaker_model,)\n\n\nclass PhotoMakerEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"photomaker\": (\"PHOTOMAKER\",),\n                              \"image\": (\"IMAGE\",),\n                              \"clip\": (\"CLIP\", ),\n                              \"text\": (\"STRING\", {\"multiline\": True, \"default\": \"photograph of photomaker\"}),\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_photomaker\"\n\n    CATEGORY = \"_for_testing/photomaker\"\n\n    def apply_photomaker(self, photomaker, image, clip, text):\n        special_token = \"photomaker\"\n        pixel_values = ldm_patched.modules.clip_vision.clip_preprocess(image.to(photomaker.load_device)).float()\n        try:\n            index = text.split(\" \").index(special_token) + 1\n        except ValueError:\n            index = -1\n        tokens = clip.tokenize(text, return_word_ids=True)\n        out_tokens = {}\n        for k in tokens:\n            out_tokens[k] = []\n            for t in tokens[k]:\n                f = list(filter(lambda x: x[2] != index, t))\n                while len(f) < len(t):\n                    f.append(t[-1])\n                out_tokens[k].append(f)\n\n        cond, pooled = clip.encode_from_tokens(out_tokens, return_pooled=True)\n\n        if index > 0:\n            token_index = index - 1\n            num_id_images = 1\n            class_tokens_mask = [True if token_index <= i < token_index+num_id_images else False for i in range(77)]\n            out = photomaker(id_pixel_values=pixel_values.unsqueeze(0), prompt_embeds=cond.to(photomaker.load_device),\n                            class_tokens_mask=torch.tensor(class_tokens_mask, dtype=torch.bool, device=photomaker.load_device).unsqueeze(0))\n        else:\n            out = cond\n\n        return ([[out, {\"pooled_output\": pooled}]], )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"PhotoMakerLoader\": PhotoMakerLoader,\n    \"PhotoMakerEncode\": PhotoMakerEncode,\n}\n\n", "ldm_patched/contrib/external.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\n\nimport os\nimport sys\nimport json\nimport hashlib\nimport traceback\nimport math\nimport time\nimport random\n\nfrom PIL import Image, ImageOps, ImageSequence\nfrom PIL.PngImagePlugin import PngInfo\nimport numpy as np\nimport safetensors.torch\n\npass  # sys.path.insert(0, os.path.join(os.path.dirname(os.path.realpath(__file__)), \"ldm_patched\"))\n\n\nimport ldm_patched.modules.diffusers_load\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.sample\nimport ldm_patched.modules.sd\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.controlnet\n\nimport ldm_patched.modules.clip_vision\n\nimport ldm_patched.modules.model_management\nfrom ldm_patched.modules.args_parser import args\n\nimport importlib\n\nimport ldm_patched.utils.path_utils\nimport ldm_patched.utils.latent_visualization\n\ndef before_node_execution():\n    ldm_patched.modules.model_management.throw_exception_if_processing_interrupted()\n\ndef interrupt_processing(value=True):\n    ldm_patched.modules.model_management.interrupt_current_processing(value)\n\nMAX_RESOLUTION=8192\n\nclass CLIPTextEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning\"\n\n    def encode(self, clip, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled}]], )\n\nclass ConditioningCombine:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_1\": (\"CONDITIONING\", ), \"conditioning_2\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"combine\"\n\n    CATEGORY = \"conditioning\"\n\n    def combine(self, conditioning_1, conditioning_2):\n        return (conditioning_1 + conditioning_2, )\n\nclass ConditioningAverage :\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ), \"conditioning_from\": (\"CONDITIONING\", ),\n                              \"conditioning_to_strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"addWeighted\"\n\n    CATEGORY = \"conditioning\"\n\n    def addWeighted(self, conditioning_to, conditioning_from, conditioning_to_strength):\n        out = []\n\n        if len(conditioning_from) > 1:\n            print(\"Warning: ConditioningAverage conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n\n        cond_from = conditioning_from[0][0]\n        pooled_output_from = conditioning_from[0][1].get(\"pooled_output\", None)\n\n        for i in range(len(conditioning_to)):\n            t1 = conditioning_to[i][0]\n            pooled_output_to = conditioning_to[i][1].get(\"pooled_output\", pooled_output_from)\n            t0 = cond_from[:,:t1.shape[1]]\n            if t0.shape[1] < t1.shape[1]:\n                t0 = torch.cat([t0] + [torch.zeros((1, (t1.shape[1] - t0.shape[1]), t1.shape[2]))], dim=1)\n\n            tw = torch.mul(t1, conditioning_to_strength) + torch.mul(t0, (1.0 - conditioning_to_strength))\n            t_to = conditioning_to[i][1].copy()\n            if pooled_output_from is not None and pooled_output_to is not None:\n                t_to[\"pooled_output\"] = torch.mul(pooled_output_to, conditioning_to_strength) + torch.mul(pooled_output_from, (1.0 - conditioning_to_strength))\n            elif pooled_output_from is not None:\n                t_to[\"pooled_output\"] = pooled_output_from\n\n            n = [tw, t_to]\n            out.append(n)\n        return (out, )\n\nclass ConditioningConcat:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"conditioning_to\": (\"CONDITIONING\",),\n            \"conditioning_from\": (\"CONDITIONING\",),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"concat\"\n\n    CATEGORY = \"conditioning\"\n\n    def concat(self, conditioning_to, conditioning_from):\n        out = []\n\n        if len(conditioning_from) > 1:\n            print(\"Warning: ConditioningConcat conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n\n        cond_from = conditioning_from[0][0]\n\n        for i in range(len(conditioning_to)):\n            t1 = conditioning_to[i][0]\n            tw = torch.cat((t1, cond_from),1)\n            n = [tw, conditioning_to[i][1].copy()]\n            out.append(n)\n\n        return (out, )\n\nclass ConditioningSetArea:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, width, height, x, y, strength):\n        c = []\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            n[1]['area'] = (height // 8, width // 8, y // 8, x // 8)\n            n[1]['strength'] = strength\n            n[1]['set_area_to_bounds'] = False\n            c.append(n)\n        return (c, )\n\nclass ConditioningSetAreaPercentage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"height\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"x\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"y\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, width, height, x, y, strength):\n        c = []\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            n[1]['area'] = (\"percentage\", height, width, y, x)\n            n[1]['strength'] = strength\n            n[1]['set_area_to_bounds'] = False\n            c.append(n)\n        return (c, )\n\nclass ConditioningSetMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"mask\": (\"MASK\", ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"set_cond_area\": ([\"default\", \"mask bounds\"],),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, mask, set_cond_area, strength):\n        c = []\n        set_area_to_bounds = False\n        if set_cond_area != \"default\":\n            set_area_to_bounds = True\n        if len(mask.shape) < 3:\n            mask = mask.unsqueeze(0)\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            _, h, w = mask.shape\n            n[1]['mask'] = mask\n            n[1]['set_area_to_bounds'] = set_area_to_bounds\n            n[1]['mask_strength'] = strength\n            c.append(n)\n        return (c, )\n\nclass ConditioningZeroOut:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"zero_out\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def zero_out(self, conditioning):\n        c = []\n        for t in conditioning:\n            d = t[1].copy()\n            if \"pooled_output\" in d:\n                d[\"pooled_output\"] = torch.zeros_like(d[\"pooled_output\"])\n            n = [torch.zeros_like(t[0]), d]\n            c.append(n)\n        return (c, )\n\nclass ConditioningSetTimestepRange:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"start\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"set_range\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def set_range(self, conditioning, start, end):\n        c = []\n        for t in conditioning:\n            d = t[1].copy()\n            d['start_percent'] = start\n            d['end_percent'] = end\n            n = [t[0], d]\n            c.append(n)\n        return (c, )\n\nclass VAEDecode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"latent\"\n\n    def decode(self, vae, samples):\n        return (vae.decode(samples[\"samples\"]), )\n\nclass VAEDecodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"_for_testing\"\n\n    def decode(self, vae, samples, tile_size):\n        return (vae.decode_tiled(samples[\"samples\"], tile_x=tile_size // 8, tile_y=tile_size // 8, ), )\n\nclass VAEEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent\"\n\n    @staticmethod\n    def vae_encode_crop_pixels(pixels):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:, x_offset:x + x_offset, y_offset:y + y_offset, :]\n        return pixels\n\n    def encode(self, vae, pixels):\n        pixels = self.vae_encode_crop_pixels(pixels)\n        t = vae.encode(pixels[:,:,:,:3])\n        return ({\"samples\":t}, )\n\nclass VAEEncodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"_for_testing\"\n\n    def encode(self, vae, pixels, tile_size):\n        pixels = VAEEncode.vae_encode_crop_pixels(pixels)\n        t = vae.encode_tiled(pixels[:,:,:,:3], tile_x=tile_size, tile_y=tile_size, )\n        return ({\"samples\":t}, )\n\nclass VAEEncodeForInpaint:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ), \"mask\": (\"MASK\", ), \"grow_mask_by\": (\"INT\", {\"default\": 6, \"min\": 0, \"max\": 64, \"step\": 1}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent/inpaint\"\n\n    def encode(self, vae, pixels, mask, grow_mask_by=6):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n\n        pixels = pixels.clone()\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n\n        #grow mask by a few pixels to keep things seamless in latent space\n        if grow_mask_by == 0:\n            mask_erosion = mask\n        else:\n            kernel_tensor = torch.ones((1, 1, grow_mask_by, grow_mask_by))\n            padding = math.ceil((grow_mask_by - 1) / 2)\n\n            mask_erosion = torch.clamp(torch.nn.functional.conv2d(mask.round(), kernel_tensor, padding=padding), 0, 1)\n\n        m = (1.0 - mask.round()).squeeze(1)\n        for i in range(3):\n            pixels[:,:,:,i] -= 0.5\n            pixels[:,:,:,i] *= m\n            pixels[:,:,:,i] += 0.5\n        t = vae.encode(pixels)\n\n        return ({\"samples\":t, \"noise_mask\": (mask_erosion[:,:,:x,:y].round())}, )\n\n\nclass InpaintModelConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"pixels\": (\"IMAGE\", ),\n                             \"mask\": (\"MASK\", ),\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\",\"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/inpaint\"\n\n    def encode(self, positive, negative, pixels, vae, mask):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n\n        orig_pixels = pixels\n        pixels = orig_pixels.clone()\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n\n        m = (1.0 - mask.round()).squeeze(1)\n        for i in range(3):\n            pixels[:,:,:,i] -= 0.5\n            pixels[:,:,:,i] *= m\n            pixels[:,:,:,i] += 0.5\n        concat_latent = vae.encode(pixels)\n        orig_latent = vae.encode(orig_pixels)\n\n        out_latent = {}\n\n        out_latent[\"samples\"] = orig_latent\n        out_latent[\"noise_mask\"] = mask\n\n        out = []\n        for conditioning in [positive, negative]:\n            c = []\n            for t in conditioning:\n                d = t[1].copy()\n                d[\"concat_latent_image\"] = concat_latent\n                d[\"concat_mask\"] = mask\n                n = [t[0], d]\n                c.append(n)\n            out.append(c)\n        return (out[0], out[1], out_latent)\n\n\nclass SaveLatent:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"latents/ldm_patched\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"_for_testing\"\n\n    def save(self, samples, filename_prefix=\"ldm_patched\", prompt=None, extra_pnginfo=None):\n        full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, self.output_dir)\n\n        # support save metadata for latent sharing\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = None\n        if not args.disable_server_info:\n            metadata = {\"prompt\": prompt_info}\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        file = f\"{filename}_{counter:05}_.latent\"\n\n        results = list()\n        results.append({\n            \"filename\": file,\n            \"subfolder\": subfolder,\n            \"type\": \"output\"\n        })\n\n        file = os.path.join(full_output_folder, file)\n\n        output = {}\n        output[\"latent_tensor\"] = samples[\"samples\"]\n        output[\"latent_format_version_0\"] = torch.tensor([])\n\n        ldm_patched.modules.utils.save_torch_file(output, file, metadata=metadata)\n        return { \"ui\": { \"latents\": results } }\n\n\nclass LoadLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = ldm_patched.utils.path_utils.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.endswith(\".latent\")]\n        return {\"required\": {\"latent\": [sorted(files), ]}, }\n\n    CATEGORY = \"_for_testing\"\n\n    RETURN_TYPES = (\"LATENT\", )\n    FUNCTION = \"load\"\n\n    def load(self, latent):\n        latent_path = ldm_patched.utils.path_utils.get_annotated_filepath(latent)\n        latent = safetensors.torch.load_file(latent_path, device=\"cpu\")\n        multiplier = 1.0\n        if \"latent_format_version_0\" not in latent:\n            multiplier = 1.0 / 0.18215\n        samples = {\"samples\": latent[\"latent_tensor\"].float() * multiplier}\n        return (samples, )\n\n    @classmethod\n    def IS_CHANGED(s, latent):\n        image_path = ldm_patched.utils.path_utils.get_annotated_filepath(latent)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, latent):\n        if not ldm_patched.utils.path_utils.exists_annotated_filepath(latent):\n            return \"Invalid latent file: {}\".format(latent)\n        return True\n\n\nclass CheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"config_name\": (ldm_patched.utils.path_utils.get_filename_list(\"configs\"), ),\n                              \"ckpt_name\": (ldm_patched.utils.path_utils.get_filename_list(\"checkpoints\"), )}}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_checkpoint(self, config_name, ckpt_name, output_vae=True, output_clip=True):\n        config_path = ldm_patched.utils.path_utils.get_full_path(\"configs\", config_name)\n        ckpt_path = ldm_patched.utils.path_utils.get_full_path(\"checkpoints\", ckpt_name)\n        return ldm_patched.modules.sd.load_checkpoint(config_path, ckpt_path, output_vae=True, output_clip=True, embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n\nclass CheckpointLoaderSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (ldm_patched.utils.path_utils.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders\"\n\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = ldm_patched.utils.path_utils.get_full_path(\"checkpoints\", ckpt_name)\n        out = ldm_patched.modules.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n        return out[:3]\n\nclass DiffusersLoader:\n    @classmethod\n    def INPUT_TYPES(cls):\n        paths = []\n        for search_path in ldm_patched.utils.path_utils.get_folder_paths(\"diffusers\"):\n            if os.path.exists(search_path):\n                for root, subdir, files in os.walk(search_path, followlinks=True):\n                    if \"model_index.json\" in files:\n                        paths.append(os.path.relpath(root, start=search_path))\n\n        return {\"required\": {\"model_path\": (paths,), }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"advanced/loaders/deprecated\"\n\n    def load_checkpoint(self, model_path, output_vae=True, output_clip=True):\n        for search_path in ldm_patched.utils.path_utils.get_folder_paths(\"diffusers\"):\n            if os.path.exists(search_path):\n                path = os.path.join(search_path, model_path)\n                if os.path.exists(path):\n                    model_path = path\n                    break\n\n        return ldm_patched.modules.diffusers_load.load_diffusers(model_path, output_vae=output_vae, output_clip=output_clip, embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n\n\nclass unCLIPCheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (ldm_patched.utils.path_utils.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\", \"CLIP_VISION\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders\"\n\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = ldm_patched.utils.path_utils.get_full_path(\"checkpoints\", ckpt_name)\n        out = ldm_patched.modules.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=True, embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n        return out\n\nclass CLIPSetLastLayer:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\", ),\n                              \"stop_at_clip_layer\": (\"INT\", {\"default\": -1, \"min\": -24, \"max\": -1, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"set_last_layer\"\n\n    CATEGORY = \"conditioning\"\n\n    def set_last_layer(self, clip, stop_at_clip_layer):\n        clip = clip.clone()\n        clip.clip_layer(stop_at_clip_layer)\n        return (clip,)\n\nclass LoraLoader:\n    def __init__(self):\n        self.loaded_lora = None\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\", ),\n                              \"lora_name\": (ldm_patched.utils.path_utils.get_filename_list(\"loras\"), ),\n                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -20.0, \"max\": 20.0, \"step\": 0.01}),\n                              \"strength_clip\": (\"FLOAT\", {\"default\": 1.0, \"min\": -20.0, \"max\": 20.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\")\n    FUNCTION = \"load_lora\"\n\n    CATEGORY = \"loaders\"\n\n    def load_lora(self, model, clip, lora_name, strength_model, strength_clip):\n        if strength_model == 0 and strength_clip == 0:\n            return (model, clip)\n\n        lora_path = ldm_patched.utils.path_utils.get_full_path(\"loras\", lora_name)\n        lora = None\n        if self.loaded_lora is not None:\n            if self.loaded_lora[0] == lora_path:\n                lora = self.loaded_lora[1]\n            else:\n                temp = self.loaded_lora\n                self.loaded_lora = None\n                del temp\n\n        if lora is None:\n            lora = ldm_patched.modules.utils.load_torch_file(lora_path, safe_load=True)\n            self.loaded_lora = (lora_path, lora)\n\n        model_lora, clip_lora = ldm_patched.modules.sd.load_lora_for_models(model, clip, lora, strength_model, strength_clip)\n        return (model_lora, clip_lora)\n\nclass LoraLoaderModelOnly(LoraLoader):\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"lora_name\": (ldm_patched.utils.path_utils.get_filename_list(\"loras\"), ),\n                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -20.0, \"max\": 20.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_lora_model_only\"\n\n    def load_lora_model_only(self, model, lora_name, strength_model):\n        return (self.load_lora(model, None, lora_name, strength_model, 0)[0],)\n\nclass VAELoader:\n    @staticmethod\n    def vae_list():\n        vaes = ldm_patched.utils.path_utils.get_filename_list(\"vae\")\n        approx_vaes = ldm_patched.utils.path_utils.get_filename_list(\"vae_approx\")\n        sdxl_taesd_enc = False\n        sdxl_taesd_dec = False\n        sd1_taesd_enc = False\n        sd1_taesd_dec = False\n\n        for v in approx_vaes:\n            if v.startswith(\"taesd_decoder.\"):\n                sd1_taesd_dec = True\n            elif v.startswith(\"taesd_encoder.\"):\n                sd1_taesd_enc = True\n            elif v.startswith(\"taesdxl_decoder.\"):\n                sdxl_taesd_dec = True\n            elif v.startswith(\"taesdxl_encoder.\"):\n                sdxl_taesd_enc = True\n        if sd1_taesd_dec and sd1_taesd_enc:\n            vaes.append(\"taesd\")\n        if sdxl_taesd_dec and sdxl_taesd_enc:\n            vaes.append(\"taesdxl\")\n        return vaes\n\n    @staticmethod\n    def load_taesd(name):\n        sd = {}\n        approx_vaes = ldm_patched.utils.path_utils.get_filename_list(\"vae_approx\")\n\n        encoder = next(filter(lambda a: a.startswith(\"{}_encoder.\".format(name)), approx_vaes))\n        decoder = next(filter(lambda a: a.startswith(\"{}_decoder.\".format(name)), approx_vaes))\n\n        enc = ldm_patched.modules.utils.load_torch_file(ldm_patched.utils.path_utils.get_full_path(\"vae_approx\", encoder))\n        for k in enc:\n            sd[\"taesd_encoder.{}\".format(k)] = enc[k]\n\n        dec = ldm_patched.modules.utils.load_torch_file(ldm_patched.utils.path_utils.get_full_path(\"vae_approx\", decoder))\n        for k in dec:\n            sd[\"taesd_decoder.{}\".format(k)] = dec[k]\n\n        if name == \"taesd\":\n            sd[\"vae_scale\"] = torch.tensor(0.18215)\n        elif name == \"taesdxl\":\n            sd[\"vae_scale\"] = torch.tensor(0.13025)\n        return sd\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae_name\": (s.vae_list(), )}}\n    RETURN_TYPES = (\"VAE\",)\n    FUNCTION = \"load_vae\"\n\n    CATEGORY = \"loaders\"\n\n    #TODO: scale factor?\n    def load_vae(self, vae_name):\n        if vae_name in [\"taesd\", \"taesdxl\"]:\n            sd = self.load_taesd(vae_name)\n        else:\n            vae_path = ldm_patched.utils.path_utils.get_full_path(\"vae\", vae_name)\n            sd = ldm_patched.modules.utils.load_torch_file(vae_path)\n        vae = ldm_patched.modules.sd.VAE(sd=sd)\n        return (vae,)\n\nclass ControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"control_net_name\": (ldm_patched.utils.path_utils.get_filename_list(\"controlnet\"), )}}\n\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n\n    CATEGORY = \"loaders\"\n\n    def load_controlnet(self, control_net_name):\n        controlnet_path = ldm_patched.utils.path_utils.get_full_path(\"controlnet\", control_net_name)\n        controlnet = ldm_patched.modules.controlnet.load_controlnet(controlnet_path)\n        return (controlnet,)\n\nclass DiffControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"control_net_name\": (ldm_patched.utils.path_utils.get_filename_list(\"controlnet\"), )}}\n\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n\n    CATEGORY = \"loaders\"\n\n    def load_controlnet(self, model, control_net_name):\n        controlnet_path = ldm_patched.utils.path_utils.get_full_path(\"controlnet\", control_net_name)\n        controlnet = ldm_patched.modules.controlnet.load_controlnet(controlnet_path, model)\n        return (controlnet,)\n\n\nclass ControlNetApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_controlnet\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_controlnet(self, conditioning, control_net, image, strength):\n        if strength == 0:\n            return (conditioning, )\n\n        c = []\n        control_hint = image.movedim(-1,1)\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            c_net = control_net.copy().set_cond_hint(control_hint, strength)\n            if 'control' in t[1]:\n                c_net.set_previous_controlnet(t[1]['control'])\n            n[1]['control'] = c_net\n            n[1]['control_apply_to_uncond'] = True\n            c.append(n)\n        return (c, )\n\n\nclass ControlNetApplyAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end_percent\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\")\n    RETURN_NAMES = (\"positive\", \"negative\")\n    FUNCTION = \"apply_controlnet\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_controlnet(self, positive, negative, control_net, image, strength, start_percent, end_percent):\n        if strength == 0:\n            return (positive, negative)\n\n        control_hint = image.movedim(-1,1)\n        cnets = {}\n\n        out = []\n        for conditioning in [positive, negative]:\n            c = []\n            for t in conditioning:\n                d = t[1].copy()\n\n                prev_cnet = d.get('control', None)\n                if prev_cnet in cnets:\n                    c_net = cnets[prev_cnet]\n                else:\n                    c_net = control_net.copy().set_cond_hint(control_hint, strength, (start_percent, end_percent))\n                    c_net.set_previous_controlnet(prev_cnet)\n                    cnets[prev_cnet] = c_net\n\n                d['control'] = c_net\n                d['control_apply_to_uncond'] = False\n                n = [t[0], d]\n                c.append(n)\n            out.append(c)\n        return (out[0], out[1])\n\n\nclass UNETLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"unet_name\": (ldm_patched.utils.path_utils.get_filename_list(\"unet\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_unet\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_unet(self, unet_name):\n        unet_path = ldm_patched.utils.path_utils.get_full_path(\"unet\", unet_name)\n        model = ldm_patched.modules.sd.load_unet(unet_path)\n        return (model,)\n\nclass CLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (ldm_patched.utils.path_utils.get_filename_list(\"clip\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_clip(self, clip_name):\n        clip_path = ldm_patched.utils.path_utils.get_full_path(\"clip\", clip_name)\n        clip = ldm_patched.modules.sd.load_clip(ckpt_paths=[clip_path], embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n        return (clip,)\n\nclass DualCLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name1\": (ldm_patched.utils.path_utils.get_filename_list(\"clip\"), ), \"clip_name2\": (ldm_patched.utils.path_utils.get_filename_list(\"clip\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_clip(self, clip_name1, clip_name2):\n        clip_path1 = ldm_patched.utils.path_utils.get_full_path(\"clip\", clip_name1)\n        clip_path2 = ldm_patched.utils.path_utils.get_full_path(\"clip\", clip_name2)\n        clip = ldm_patched.modules.sd.load_clip(ckpt_paths=[clip_path1, clip_path2], embedding_directory=ldm_patched.utils.path_utils.get_folder_paths(\"embeddings\"))\n        return (clip,)\n\nclass CLIPVisionLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (ldm_patched.utils.path_utils.get_filename_list(\"clip_vision\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"loaders\"\n\n    def load_clip(self, clip_name):\n        clip_path = ldm_patched.utils.path_utils.get_full_path(\"clip_vision\", clip_name)\n        clip_vision = ldm_patched.modules.clip_vision.load(clip_path)\n        return (clip_vision,)\n\nclass CLIPVisionEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"image\": (\"IMAGE\",)\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION_OUTPUT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning\"\n\n    def encode(self, clip_vision, image):\n        output = clip_vision.encode_image(image)\n        return (output,)\n\nclass StyleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"style_model_name\": (ldm_patched.utils.path_utils.get_filename_list(\"style_models\"), )}}\n\n    RETURN_TYPES = (\"STYLE_MODEL\",)\n    FUNCTION = \"load_style_model\"\n\n    CATEGORY = \"loaders\"\n\n    def load_style_model(self, style_model_name):\n        style_model_path = ldm_patched.utils.path_utils.get_full_path(\"style_models\", style_model_name)\n        style_model = ldm_patched.modules.sd.load_style_model(style_model_path)\n        return (style_model,)\n\n\nclass StyleModelApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"style_model\": (\"STYLE_MODEL\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_stylemodel\"\n\n    CATEGORY = \"conditioning/style_model\"\n\n    def apply_stylemodel(self, clip_vision_output, style_model, conditioning):\n        cond = style_model.get_cond(clip_vision_output).flatten(start_dim=0, end_dim=1).unsqueeze(dim=0)\n        c = []\n        for t in conditioning:\n            n = [torch.cat((t[0], cond), dim=1), t[1].copy()]\n            c.append(n)\n        return (c, )\n\nclass unCLIPConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_adm\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_adm(self, conditioning, clip_vision_output, strength, noise_augmentation):\n        if strength == 0:\n            return (conditioning, )\n\n        c = []\n        for t in conditioning:\n            o = t[1].copy()\n            x = {\"clip_vision_output\": clip_vision_output, \"strength\": strength, \"noise_augmentation\": noise_augmentation}\n            if \"unclip_conditioning\" in o:\n                o[\"unclip_conditioning\"] = o[\"unclip_conditioning\"][:] + [x]\n            else:\n                o[\"unclip_conditioning\"] = [x]\n            n = [t[0], o]\n            c.append(n)\n        return (c, )\n\nclass GLIGENLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"gligen_name\": (ldm_patched.utils.path_utils.get_filename_list(\"gligen\"), )}}\n\n    RETURN_TYPES = (\"GLIGEN\",)\n    FUNCTION = \"load_gligen\"\n\n    CATEGORY = \"loaders\"\n\n    def load_gligen(self, gligen_name):\n        gligen_path = ldm_patched.utils.path_utils.get_full_path(\"gligen\", gligen_name)\n        gligen = ldm_patched.modules.sd.load_gligen(gligen_path)\n        return (gligen,)\n\nclass GLIGENTextBoxApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ),\n                              \"clip\": (\"CLIP\", ),\n                              \"gligen_textbox_model\": (\"GLIGEN\", ),\n                              \"text\": (\"STRING\", {\"multiline\": True}),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning/gligen\"\n\n    def append(self, conditioning_to, clip, gligen_textbox_model, text, width, height, x, y):\n        c = []\n        cond, cond_pooled = clip.encode_from_tokens(clip.tokenize(text), return_pooled=True)\n        for t in conditioning_to:\n            n = [t[0], t[1].copy()]\n            position_params = [(cond_pooled, height // 8, width // 8, y // 8, x // 8)]\n            prev = []\n            if \"gligen\" in n[1]:\n                prev = n[1]['gligen'][2]\n\n            n[1]['gligen'] = (\"position\", gligen_textbox_model, prev + position_params)\n            c.append(n)\n        return (c, )\n\nclass EmptyLatentImage:\n    def __init__(self):\n        self.device = ldm_patched.modules.model_management.intermediate_device()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent\"\n\n    def generate(self, width, height, batch_size=1):\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8], device=self.device)\n        return ({\"samples\":latent}, )\n\n\nclass LatentFromBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"batch_index\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 63}),\n                              \"length\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"frombatch\"\n\n    CATEGORY = \"latent/batch\"\n\n    def frombatch(self, samples, batch_index, length):\n        s = samples.copy()\n        s_in = samples[\"samples\"]\n        batch_index = min(s_in.shape[0] - 1, batch_index)\n        length = min(s_in.shape[0] - batch_index, length)\n        s[\"samples\"] = s_in[batch_index:batch_index + length].clone()\n        if \"noise_mask\" in samples:\n            masks = samples[\"noise_mask\"]\n            if masks.shape[0] == 1:\n                s[\"noise_mask\"] = masks.clone()\n            else:\n                if masks.shape[0] < s_in.shape[0]:\n                    masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n                s[\"noise_mask\"] = masks[batch_index:batch_index + length].clone()\n        if \"batch_index\" not in s:\n            s[\"batch_index\"] = [x for x in range(batch_index, batch_index+length)]\n        else:\n            s[\"batch_index\"] = samples[\"batch_index\"][batch_index:batch_index + length]\n        return (s,)\n    \nclass RepeatLatentBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"repeat\"\n\n    CATEGORY = \"latent/batch\"\n\n    def repeat(self, samples, amount):\n        s = samples.copy()\n        s_in = samples[\"samples\"]\n        \n        s[\"samples\"] = s_in.repeat((amount, 1,1,1))\n        if \"noise_mask\" in samples and samples[\"noise_mask\"].shape[0] > 1:\n            masks = samples[\"noise_mask\"]\n            if masks.shape[0] < s_in.shape[0]:\n                masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n            s[\"noise_mask\"] = samples[\"noise_mask\"].repeat((amount, 1,1,1))\n        if \"batch_index\" in s:\n            offset = max(s[\"batch_index\"]) - min(s[\"batch_index\"]) + 1\n            s[\"batch_index\"] = s[\"batch_index\"] + [x + (i * offset) for i in range(1, amount) for x in s[\"batch_index\"]]\n        return (s,)\n\nclass LatentUpscale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"latent\"\n\n    def upscale(self, samples, upscale_method, width, height, crop):\n        if width == 0 and height == 0:\n            s = samples\n        else:\n            s = samples.copy()\n\n            if width == 0:\n                height = max(64, height)\n                width = max(64, round(samples[\"samples\"].shape[3] * height / samples[\"samples\"].shape[2]))\n            elif height == 0:\n                width = max(64, width)\n                height = max(64, round(samples[\"samples\"].shape[2] * width / samples[\"samples\"].shape[3]))\n            else:\n                width = max(64, width)\n                height = max(64, height)\n\n            s[\"samples\"] = ldm_patched.modules.utils.common_upscale(samples[\"samples\"], width // 8, height // 8, upscale_method, crop)\n        return (s,)\n\nclass LatentUpscaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.5, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"latent\"\n\n    def upscale(self, samples, upscale_method, scale_by):\n        s = samples.copy()\n        width = round(samples[\"samples\"].shape[3] * scale_by)\n        height = round(samples[\"samples\"].shape[2] * scale_by)\n        s[\"samples\"] = ldm_patched.modules.utils.common_upscale(samples[\"samples\"], width, height, upscale_method, \"disabled\")\n        return (s,)\n\nclass LatentRotate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"rotation\": ([\"none\", \"90 degrees\", \"180 degrees\", \"270 degrees\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"rotate\"\n\n    CATEGORY = \"latent/transform\"\n\n    def rotate(self, samples, rotation):\n        s = samples.copy()\n        rotate_by = 0\n        if rotation.startswith(\"90\"):\n            rotate_by = 1\n        elif rotation.startswith(\"180\"):\n            rotate_by = 2\n        elif rotation.startswith(\"270\"):\n            rotate_by = 3\n\n        s[\"samples\"] = torch.rot90(samples[\"samples\"], k=rotate_by, dims=[3, 2])\n        return (s,)\n\nclass LatentFlip:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"flip_method\": ([\"x-axis: vertically\", \"y-axis: horizontally\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"flip\"\n\n    CATEGORY = \"latent/transform\"\n\n    def flip(self, samples, flip_method):\n        s = samples.copy()\n        if flip_method.startswith(\"x\"):\n            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[2])\n        elif flip_method.startswith(\"y\"):\n            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[3])\n\n        return (s,)\n\nclass LatentComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples_to\": (\"LATENT\",),\n                              \"samples_from\": (\"LATENT\",),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"feather\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"latent\"\n\n    def composite(self, samples_to, samples_from, x, y, composite_method=\"normal\", feather=0):\n        x =  x // 8\n        y = y // 8\n        feather = feather // 8\n        samples_out = samples_to.copy()\n        s = samples_to[\"samples\"].clone()\n        samples_to = samples_to[\"samples\"]\n        samples_from = samples_from[\"samples\"]\n        if feather == 0:\n            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n        else:\n            samples_from = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n            mask = torch.ones_like(samples_from)\n            for t in range(feather):\n                if y != 0:\n                    mask[:,:,t:1+t,:] *= ((1.0/feather) * (t + 1))\n\n                if y + samples_from.shape[2] < samples_to.shape[2]:\n                    mask[:,:,mask.shape[2] -1 -t: mask.shape[2]-t,:] *= ((1.0/feather) * (t + 1))\n                if x != 0:\n                    mask[:,:,:,t:1+t] *= ((1.0/feather) * (t + 1))\n                if x + samples_from.shape[3] < samples_to.shape[3]:\n                    mask[:,:,:,mask.shape[3]- 1 - t: mask.shape[3]- t] *= ((1.0/feather) * (t + 1))\n            rev_mask = torch.ones_like(mask) - mask\n            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x] * mask + s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] * rev_mask\n        samples_out[\"samples\"] = s\n        return (samples_out,)\n\nclass LatentBlend:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"samples1\": (\"LATENT\",),\n            \"samples2\": (\"LATENT\",),\n            \"blend_factor\": (\"FLOAT\", {\n                \"default\": 0.5,\n                \"min\": 0,\n                \"max\": 1,\n                \"step\": 0.01\n            }),\n        }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"blend\"\n\n    CATEGORY = \"_for_testing\"\n\n    def blend(self, samples1, samples2, blend_factor:float, blend_mode: str=\"normal\"):\n\n        samples_out = samples1.copy()\n        samples1 = samples1[\"samples\"]\n        samples2 = samples2[\"samples\"]\n\n        if samples1.shape != samples2.shape:\n            samples2.permute(0, 3, 1, 2)\n            samples2 = ldm_patched.modules.utils.common_upscale(samples2, samples1.shape[3], samples1.shape[2], 'bicubic', crop='center')\n            samples2.permute(0, 2, 3, 1)\n\n        samples_blended = self.blend_mode(samples1, samples2, blend_mode)\n        samples_blended = samples1 * blend_factor + samples_blended * (1 - blend_factor)\n        samples_out[\"samples\"] = samples_blended\n        return (samples_out,)\n\n    def blend_mode(self, img1, img2, mode):\n        if mode == \"normal\":\n            return img2\n        else:\n            raise ValueError(f\"Unsupported blend mode: {mode}\")\n\nclass LatentCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"crop\"\n\n    CATEGORY = \"latent/transform\"\n\n    def crop(self, samples, width, height, x, y):\n        s = samples.copy()\n        samples = samples['samples']\n        x =  x // 8\n        y = y // 8\n\n        #enfonce minimum size of 64\n        if x > (samples.shape[3] - 8):\n            x = samples.shape[3] - 8\n        if y > (samples.shape[2] - 8):\n            y = samples.shape[2] - 8\n\n        new_height = height // 8\n        new_width = width // 8\n        to_x = new_width + x\n        to_y = new_height + y\n        s['samples'] = samples[:,:,y:to_y, x:to_x]\n        return (s,)\n\nclass SetLatentNoiseMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"mask\": (\"MASK\",),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"set_mask\"\n\n    CATEGORY = \"latent/inpaint\"\n\n    def set_mask(self, samples, mask):\n        s = samples.copy()\n        s[\"noise_mask\"] = mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1]))\n        return (s,)\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = ldm_patched.modules.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = ldm_patched.utils.latent_visualization.prepare_callback(model, steps)\n    disable_pbar = not ldm_patched.modules.utils.PROGRESS_BAR_ENABLED\n    samples = ldm_patched.modules.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (ldm_patched.modules.samplers.KSampler.SAMPLERS, ),\n                    \"scheduler\": (ldm_patched.modules.samplers.KSampler.SCHEDULERS, ),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                    \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\nclass KSamplerAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": ([\"enable\", \"disable\"], ),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (ldm_patched.modules.samplers.KSampler.SAMPLERS, ),\n                    \"scheduler\": (ldm_patched.modules.samplers.KSampler.SCHEDULERS, ),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                    \"start_at_step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n                    \"end_at_step\": (\"INT\", {\"default\": 10000, \"min\": 0, \"max\": 10000}),\n                    \"return_with_leftover_noise\": ([\"disable\", \"enable\"], ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n\n    def sample(self, model, add_noise, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, start_at_step, end_at_step, return_with_leftover_noise, denoise=1.0):\n        force_full_denoise = True\n        if return_with_leftover_noise == \"enable\":\n            force_full_denoise = False\n        disable_noise = False\n        if add_noise == \"disable\":\n            disable_noise = True\n        return common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise, disable_noise=disable_noise, start_step=start_at_step, last_step=end_at_step, force_full_denoise=force_full_denoise)\n\nclass SaveImage:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": \n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ldm_patched\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image\"\n\n    def save_images(self, images, filename_prefix=\"ldm_patched\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            metadata = None\n            if not args.disable_server_info:\n                metadata = PngInfo()\n                if prompt is not None:\n                    metadata.add_text(\"prompt\", json.dumps(prompt))\n                if extra_pnginfo is not None:\n                    for x in extra_pnginfo:\n                        metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n            file = f\"{filename}_{counter:05}_.png\"\n            img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=self.compress_level)\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n\n        return { \"ui\": { \"images\": results } }\n\nclass PreviewImage(SaveImage):\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + ''.join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n        self.compress_level = 1\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ), },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\nclass LoadImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = ldm_patched.utils.path_utils.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True})},\n                }\n\n    CATEGORY = \"image\"\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"load_image\"\n    def load_image(self, image):\n        image_path = ldm_patched.utils.path_utils.get_annotated_filepath(image)\n        img = Image.open(image_path)\n        output_images = []\n        output_masks = []\n        for i in ImageSequence.Iterator(img):\n            i = ImageOps.exif_transpose(i)\n            if i.mode == 'I':\n                i = i.point(lambda i: i * (1 / 255))\n            image = i.convert(\"RGB\")\n            image = np.array(image).astype(np.float32) / 255.0\n            image = torch.from_numpy(image)[None,]\n            if 'A' in i.getbands():\n                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n                mask = 1. - torch.from_numpy(mask)\n            else:\n                mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n            output_images.append(image)\n            output_masks.append(mask.unsqueeze(0))\n\n        if len(output_images) > 1:\n            output_image = torch.cat(output_images, dim=0)\n            output_mask = torch.cat(output_masks, dim=0)\n        else:\n            output_image = output_images[0]\n            output_mask = output_masks[0]\n\n        return (output_image, output_mask)\n\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = ldm_patched.utils.path_utils.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, image):\n        if not ldm_patched.utils.path_utils.exists_annotated_filepath(image):\n            return \"Invalid image file: {}\".format(image)\n\n        return True\n\nclass LoadImageMask:\n    _color_channels = [\"alpha\", \"red\", \"green\", \"blue\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = ldm_patched.utils.path_utils.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True}),\n                     \"channel\": (s._color_channels, ), }\n                }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"load_image\"\n    def load_image(self, image, channel):\n        image_path = ldm_patched.utils.path_utils.get_annotated_filepath(image)\n        i = Image.open(image_path)\n        i = ImageOps.exif_transpose(i)\n        if i.getbands() != (\"R\", \"G\", \"B\", \"A\"):\n            if i.mode == 'I':\n                i = i.point(lambda i: i * (1 / 255))\n            i = i.convert(\"RGBA\")\n        mask = None\n        c = channel[0].upper()\n        if c in i.getbands():\n            mask = np.array(i.getchannel(c)).astype(np.float32) / 255.0\n            mask = torch.from_numpy(mask)\n            if c == 'A':\n                mask = 1. - mask\n        else:\n            mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n        return (mask.unsqueeze(0),)\n\n    @classmethod\n    def IS_CHANGED(s, image, channel):\n        image_path = ldm_patched.utils.path_utils.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, image):\n        if not ldm_patched.utils.path_utils.exists_annotated_filepath(image):\n            return \"Invalid image file: {}\".format(image)\n\n        return True\n\nclass ImageScale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, width, height, crop):\n        if width == 0 and height == 0:\n            s = image\n        else:\n            samples = image.movedim(-1,1)\n\n            if width == 0:\n                width = max(1, round(samples.shape[3] * height / samples.shape[2]))\n            elif height == 0:\n                height = max(1, round(samples.shape[2] * width / samples.shape[3]))\n\n            s = ldm_patched.modules.utils.common_upscale(samples, width, height, upscale_method, crop)\n            s = s.movedim(1,-1)\n        return (s,)\n\nclass ImageScaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, scale_by):\n        samples = image.movedim(-1,1)\n        width = round(samples.shape[3] * scale_by)\n        height = round(samples.shape[2] * scale_by)\n        s = ldm_patched.modules.utils.common_upscale(samples, width, height, upscale_method, \"disabled\")\n        s = s.movedim(1,-1)\n        return (s,)\n\nclass ImageInvert:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"invert\"\n\n    CATEGORY = \"image\"\n\n    def invert(self, image):\n        s = 1.0 - image\n        return (s,)\n\nclass ImageBatch:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image1\": (\"IMAGE\",), \"image2\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"batch\"\n\n    CATEGORY = \"image\"\n\n    def batch(self, image1, image2):\n        if image1.shape[1:] != image2.shape[1:]:\n            image2 = ldm_patched.modules.utils.common_upscale(image2.movedim(-1,1), image1.shape[2], image1.shape[1], \"bilinear\", \"center\").movedim(1,-1)\n        s = torch.cat((image1, image2), dim=0)\n        return (s,)\n\nclass EmptyImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"image\"\n\n    def generate(self, width, height, batch_size=1, color=0):\n        r = torch.full([batch_size, height, width, 1], ((color >> 16) & 0xFF) / 0xFF)\n        g = torch.full([batch_size, height, width, 1], ((color >> 8) & 0xFF) / 0xFF)\n        b = torch.full([batch_size, height, width, 1], ((color) & 0xFF) / 0xFF)\n        return (torch.cat((r, g, b), dim=-1), )\n\nclass ImagePadForOutpaint:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"feathering\": (\"INT\", {\"default\": 40, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"expand_image\"\n\n    CATEGORY = \"image\"\n\n    def expand_image(self, image, left, top, right, bottom, feathering):\n        d1, d2, d3, d4 = image.size()\n\n        new_image = torch.ones(\n            (d1, d2 + top + bottom, d3 + left + right, d4),\n            dtype=torch.float32,\n        ) * 0.5\n\n        new_image[:, top:top + d2, left:left + d3, :] = image\n\n        mask = torch.ones(\n            (d2 + top + bottom, d3 + left + right),\n            dtype=torch.float32,\n        )\n\n        t = torch.zeros(\n            (d2, d3),\n            dtype=torch.float32\n        )\n\n        if feathering > 0 and feathering * 2 < d2 and feathering * 2 < d3:\n\n            for i in range(d2):\n                for j in range(d3):\n                    dt = i if top != 0 else d2\n                    db = d2 - i if bottom != 0 else d2\n\n                    dl = j if left != 0 else d3\n                    dr = d3 - j if right != 0 else d3\n\n                    d = min(dt, db, dl, dr)\n\n                    if d >= feathering:\n                        continue\n\n                    v = (feathering - d) / feathering\n\n                    t[i, j] = v * v\n\n        mask[top:top + d2, left:left + d3] = t\n\n        return (new_image, mask)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"KSampler\": KSampler,\n    \"CheckpointLoaderSimple\": CheckpointLoaderSimple,\n    \"CLIPTextEncode\": CLIPTextEncode,\n    \"CLIPSetLastLayer\": CLIPSetLastLayer,\n    \"VAEDecode\": VAEDecode,\n    \"VAEEncode\": VAEEncode,\n    \"VAEEncodeForInpaint\": VAEEncodeForInpaint,\n    \"VAELoader\": VAELoader,\n    \"EmptyLatentImage\": EmptyLatentImage,\n    \"LatentUpscale\": LatentUpscale,\n    \"LatentUpscaleBy\": LatentUpscaleBy,\n    \"LatentFromBatch\": LatentFromBatch,\n    \"RepeatLatentBatch\": RepeatLatentBatch,\n    \"SaveImage\": SaveImage,\n    \"PreviewImage\": PreviewImage,\n    \"LoadImage\": LoadImage,\n    \"LoadImageMask\": LoadImageMask,\n    \"ImageScale\": ImageScale,\n    \"ImageScaleBy\": ImageScaleBy,\n    \"ImageInvert\": ImageInvert,\n    \"ImageBatch\": ImageBatch,\n    \"ImagePadForOutpaint\": ImagePadForOutpaint,\n    \"EmptyImage\": EmptyImage,\n    \"ConditioningAverage\": ConditioningAverage ,\n    \"ConditioningCombine\": ConditioningCombine,\n    \"ConditioningConcat\": ConditioningConcat,\n    \"ConditioningSetArea\": ConditioningSetArea,\n    \"ConditioningSetAreaPercentage\": ConditioningSetAreaPercentage,\n    \"ConditioningSetMask\": ConditioningSetMask,\n    \"KSamplerAdvanced\": KSamplerAdvanced,\n    \"SetLatentNoiseMask\": SetLatentNoiseMask,\n    \"LatentComposite\": LatentComposite,\n    \"LatentBlend\": LatentBlend,\n    \"LatentRotate\": LatentRotate,\n    \"LatentFlip\": LatentFlip,\n    \"LatentCrop\": LatentCrop,\n    \"LoraLoader\": LoraLoader,\n    \"CLIPLoader\": CLIPLoader,\n    \"UNETLoader\": UNETLoader,\n    \"DualCLIPLoader\": DualCLIPLoader,\n    \"CLIPVisionEncode\": CLIPVisionEncode,\n    \"StyleModelApply\": StyleModelApply,\n    \"unCLIPConditioning\": unCLIPConditioning,\n    \"ControlNetApply\": ControlNetApply,\n    \"ControlNetApplyAdvanced\": ControlNetApplyAdvanced,\n    \"ControlNetLoader\": ControlNetLoader,\n    \"DiffControlNetLoader\": DiffControlNetLoader,\n    \"StyleModelLoader\": StyleModelLoader,\n    \"CLIPVisionLoader\": CLIPVisionLoader,\n    \"VAEDecodeTiled\": VAEDecodeTiled,\n    \"VAEEncodeTiled\": VAEEncodeTiled,\n    \"unCLIPCheckpointLoader\": unCLIPCheckpointLoader,\n    \"GLIGENLoader\": GLIGENLoader,\n    \"GLIGENTextBoxApply\": GLIGENTextBoxApply,\n    \"InpaintModelConditioning\": InpaintModelConditioning,\n\n    \"CheckpointLoader\": CheckpointLoader,\n    \"DiffusersLoader\": DiffusersLoader,\n\n    \"LoadLatent\": LoadLatent,\n    \"SaveLatent\": SaveLatent,\n\n    \"ConditioningZeroOut\": ConditioningZeroOut,\n    \"ConditioningSetTimestepRange\": ConditioningSetTimestepRange,\n    \"LoraLoaderModelOnly\": LoraLoaderModelOnly,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"KSampler\": \"KSampler\",\n    \"KSamplerAdvanced\": \"KSampler (Advanced)\",\n    # Loaders\n    \"CheckpointLoader\": \"Load Checkpoint With Config (DEPRECATED)\",\n    \"CheckpointLoaderSimple\": \"Load Checkpoint\",\n    \"VAELoader\": \"Load VAE\",\n    \"LoraLoader\": \"Load LoRA\",\n    \"CLIPLoader\": \"Load CLIP\",\n    \"ControlNetLoader\": \"Load ControlNet Model\",\n    \"DiffControlNetLoader\": \"Load ControlNet Model (diff)\",\n    \"StyleModelLoader\": \"Load Style Model\",\n    \"CLIPVisionLoader\": \"Load CLIP Vision\",\n    \"UpscaleModelLoader\": \"Load Upscale Model\",\n    # Conditioning\n    \"CLIPVisionEncode\": \"CLIP Vision Encode\",\n    \"StyleModelApply\": \"Apply Style Model\",\n    \"CLIPTextEncode\": \"CLIP Text Encode (Prompt)\",\n    \"CLIPSetLastLayer\": \"CLIP Set Last Layer\",\n    \"ConditioningCombine\": \"Conditioning (Combine)\",\n    \"ConditioningAverage \": \"Conditioning (Average)\",\n    \"ConditioningConcat\": \"Conditioning (Concat)\",\n    \"ConditioningSetArea\": \"Conditioning (Set Area)\",\n    \"ConditioningSetAreaPercentage\": \"Conditioning (Set Area with Percentage)\",\n    \"ConditioningSetMask\": \"Conditioning (Set Mask)\",\n    \"ControlNetApply\": \"Apply ControlNet\",\n    \"ControlNetApplyAdvanced\": \"Apply ControlNet (Advanced)\",\n    # Latent\n    \"VAEEncodeForInpaint\": \"VAE Encode (for Inpainting)\",\n    \"SetLatentNoiseMask\": \"Set Latent Noise Mask\",\n    \"VAEDecode\": \"VAE Decode\",\n    \"VAEEncode\": \"VAE Encode\",\n    \"LatentRotate\": \"Rotate Latent\",\n    \"LatentFlip\": \"Flip Latent\",\n    \"LatentCrop\": \"Crop Latent\",\n    \"EmptyLatentImage\": \"Empty Latent Image\",\n    \"LatentUpscale\": \"Upscale Latent\",\n    \"LatentUpscaleBy\": \"Upscale Latent By\",\n    \"LatentComposite\": \"Latent Composite\",\n    \"LatentBlend\": \"Latent Blend\",\n    \"LatentFromBatch\" : \"Latent From Batch\",\n    \"RepeatLatentBatch\": \"Repeat Latent Batch\",\n    # Image\n    \"SaveImage\": \"Save Image\",\n    \"PreviewImage\": \"Preview Image\",\n    \"LoadImage\": \"Load Image\",\n    \"LoadImageMask\": \"Load Image (as Mask)\",\n    \"ImageScale\": \"Upscale Image\",\n    \"ImageScaleBy\": \"Upscale Image By\",\n    \"ImageUpscaleWithModel\": \"Upscale Image (using Model)\",\n    \"ImageInvert\": \"Invert Image\",\n    \"ImagePadForOutpaint\": \"Pad Image for Outpainting\",\n    \"ImageBatch\": \"Batch Images\",\n    # _for_testing\n    \"VAEDecodeTiled\": \"VAE Decode (Tiled)\",\n    \"VAEEncodeTiled\": \"VAE Encode (Tiled)\",\n}\n\nEXTENSION_WEB_DIRS = {}\n\ndef load_custom_node(module_path, ignore=set()):\n    module_name = os.path.basename(module_path)\n    if os.path.isfile(module_path):\n        sp = os.path.splitext(module_path)\n        module_name = sp[0]\n    try:\n        if os.path.isfile(module_path):\n            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n            module_dir = os.path.split(module_path)[0]\n        else:\n            module_spec = importlib.util.spec_from_file_location(module_name, os.path.join(module_path, \"__init__.py\"))\n            module_dir = module_path\n\n        module = importlib.util.module_from_spec(module_spec)\n        sys.modules[module_name] = module\n        module_spec.loader.exec_module(module)\n\n        if hasattr(module, \"WEB_DIRECTORY\") and getattr(module, \"WEB_DIRECTORY\") is not None:\n            web_dir = os.path.abspath(os.path.join(module_dir, getattr(module, \"WEB_DIRECTORY\")))\n            if os.path.isdir(web_dir):\n                EXTENSION_WEB_DIRS[module_name] = web_dir\n\n        if hasattr(module, \"NODE_CLASS_MAPPINGS\") and getattr(module, \"NODE_CLASS_MAPPINGS\") is not None:\n            for name in module.NODE_CLASS_MAPPINGS:\n                if name not in ignore:\n                    NODE_CLASS_MAPPINGS[name] = module.NODE_CLASS_MAPPINGS[name]\n            if hasattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") and getattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") is not None:\n                NODE_DISPLAY_NAME_MAPPINGS.update(module.NODE_DISPLAY_NAME_MAPPINGS)\n            return True\n        else:\n            print(f\"Skip {module_path} module for custom nodes due to the lack of NODE_CLASS_MAPPINGS.\")\n            return False\n    except Exception as e:\n        print(traceback.format_exc())\n        print(f\"Cannot import {module_path} module for custom nodes:\", e)\n        return False\n\ndef load_custom_nodes():\n    base_node_names = set(NODE_CLASS_MAPPINGS.keys())\n    node_paths = ldm_patched.utils.path_utils.get_folder_paths(\"custom_nodes\")\n    node_import_times = []\n    for custom_node_path in node_paths:\n        possible_modules = os.listdir(os.path.realpath(custom_node_path))\n        if \"__pycache__\" in possible_modules:\n            possible_modules.remove(\"__pycache__\")\n\n        for possible_module in possible_modules:\n            module_path = os.path.join(custom_node_path, possible_module)\n            if os.path.isfile(module_path) and os.path.splitext(module_path)[1] != \".py\": continue\n            if module_path.endswith(\".disabled\"): continue\n            time_before = time.perf_counter()\n            success = load_custom_node(module_path, base_node_names)\n            node_import_times.append((time.perf_counter() - time_before, module_path, success))\n\n    if len(node_import_times) > 0:\n        print(\"\\nImport times for custom nodes:\")\n        for n in sorted(node_import_times):\n            if n[2]:\n                import_message = \"\"\n            else:\n                import_message = \" (IMPORT FAILED)\"\n            print(\"{:6.1f} seconds{}:\".format(n[0], import_message), n[1])\n        print()\n\ndef init_custom_nodes():\n    extras_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"ldm_patched_extras\")\n    extras_files = [\n        \"nodes_latent.py\",\n        \"nodes_hypernetwork.py\",\n        \"nodes_upscale_model.py\",\n        \"nodes_post_processing.py\",\n        \"nodes_mask.py\",\n        \"nodes_compositing.py\",\n        \"nodes_rebatch.py\",\n        \"nodes_model_merging.py\",\n        \"nodes_tomesd.py\",\n        \"nodes_clip_sdxl.py\",\n        \"nodes_canny.py\",\n        \"nodes_freelunch.py\",\n        \"nodes_custom_sampler.py\",\n        \"nodes_hypertile.py\",\n        \"nodes_model_advanced.py\",\n        \"nodes_model_downscale.py\",\n        \"nodes_images.py\",\n        \"nodes_video_model.py\",\n        \"nodes_sag.py\",\n        \"nodes_perpneg.py\",\n        \"nodes_stable3d.py\",\n        \"nodes_sdupscale.py\",\n        \"nodes_photomaker.py\",\n    ]\n\n    for node_file in extras_files:\n        load_custom_node(os.path.join(extras_dir, node_file))\n\n    load_custom_nodes()\n", "ldm_patched/contrib/external_stable3d.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport torch\nimport ldm_patched.contrib.external\nimport ldm_patched.modules.utils\n\ndef camera_embeddings(elevation, azimuth):\n    elevation = torch.as_tensor([elevation])\n    azimuth = torch.as_tensor([azimuth])\n    embeddings = torch.stack(\n        [\n                torch.deg2rad(\n                    (90 - elevation) - (90)\n                ),  # Zero123 polar is 90-elevation\n                torch.sin(torch.deg2rad(azimuth)),\n                torch.cos(torch.deg2rad(azimuth)),\n                torch.deg2rad(\n                    90 - torch.full_like(elevation, 0)\n                ),\n        ], dim=-1).unsqueeze(1)\n\n    return embeddings\n\n\nclass StableZero123_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"elevation\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                              \"azimuth\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/3d_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, batch_size, elevation, azimuth):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = ldm_patched.modules.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        t = vae.encode(encode_pixels)\n        cam_embeds = camera_embeddings(elevation, azimuth)\n        cond = torch.cat([pooled, cam_embeds.to(pooled.device).repeat((pooled.shape[0], 1, 1))], dim=-1)\n\n        positive = [[cond, {\"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent})\n\nclass StableZero123_Conditioning_Batched:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": ldm_patched.contrib.external.MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"elevation\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                              \"azimuth\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                              \"elevation_batch_increment\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                              \"azimuth_batch_increment\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/3d_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, batch_size, elevation, azimuth, elevation_batch_increment, azimuth_batch_increment):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = ldm_patched.modules.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        t = vae.encode(encode_pixels)\n\n        cam_embeds = []\n        for i in range(batch_size):\n            cam_embeds.append(camera_embeddings(elevation, azimuth))\n            elevation += elevation_batch_increment\n            azimuth += azimuth_batch_increment\n\n        cam_embeds = torch.cat(cam_embeds, dim=0)\n        cond = torch.cat([ldm_patched.modules.utils.repeat_to_batch_size(pooled, batch_size), cam_embeds], dim=-1)\n\n        positive = [[cond, {\"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent, \"batch_index\": [0] * batch_size})\n\n\nNODE_CLASS_MAPPINGS = {\n    \"StableZero123_Conditioning\": StableZero123_Conditioning,\n    \"StableZero123_Conditioning_Batched\": StableZero123_Conditioning_Batched,\n}\n", "ldm_patched/contrib/external_images.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.contrib.external\nimport ldm_patched.utils.path_utils\nfrom ldm_patched.modules.args_parser import args\n\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\n\nimport numpy as np\nimport json\nimport os\n\nMAX_RESOLUTION = ldm_patched.contrib.external.MAX_RESOLUTION\n\nclass ImageCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"crop\"\n\n    CATEGORY = \"image/transform\"\n\n    def crop(self, image, width, height, x, y):\n        x = min(x, image.shape[2] - 1)\n        y = min(y, image.shape[1] - 1)\n        to_x = width + x\n        to_y = height + y\n        img = image[:,y:to_y, x:to_x, :]\n        return (img,)\n\nclass RepeatImageBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"repeat\"\n\n    CATEGORY = \"image/batch\"\n\n    def repeat(self, image, amount):\n        s = image.repeat((amount, 1,1,1))\n        return (s,)\n\nclass SaveAnimatedWEBP:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    methods = {\"default\": 4, \"fastest\": 0, \"slowest\": 6}\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ldm_patched\"}),\n                     \"fps\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.01, \"max\": 1000.0, \"step\": 0.01}),\n                     \"lossless\": (\"BOOLEAN\", {\"default\": True}),\n                     \"quality\": (\"INT\", {\"default\": 80, \"min\": 0, \"max\": 100}),\n                     \"method\": (list(s.methods.keys()),),\n                     # \"num_frames\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 8192}),\n                     },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image/animation\"\n\n    def save_images(self, images, fps, filename_prefix, lossless, quality, method, num_frames=0, prompt=None, extra_pnginfo=None):\n        method = self.methods.get(method)\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        pil_images = []\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            pil_images.append(img)\n\n        metadata = pil_images[0].getexif()\n        if not args.disable_server_info:\n            if prompt is not None:\n                metadata[0x0110] = \"prompt:{}\".format(json.dumps(prompt))\n            if extra_pnginfo is not None:\n                inital_exif = 0x010f\n                for x in extra_pnginfo:\n                    metadata[inital_exif] = \"{}:{}\".format(x, json.dumps(extra_pnginfo[x]))\n                    inital_exif -= 1\n\n        if num_frames == 0:\n            num_frames = len(pil_images)\n\n        c = len(pil_images)\n        for i in range(0, c, num_frames):\n            file = f\"{filename}_{counter:05}_.webp\"\n            pil_images[i].save(os.path.join(full_output_folder, file), save_all=True, duration=int(1000.0/fps), append_images=pil_images[i + 1:i + num_frames], exif=metadata, lossless=lossless, quality=quality, method=method)\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n\n        animated = num_frames != 1\n        return { \"ui\": { \"images\": results, \"animated\": (animated,) } }\n\nclass SaveAnimatedPNG:\n    def __init__(self):\n        self.output_dir = ldm_patched.utils.path_utils.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ldm_patched\"}),\n                     \"fps\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.01, \"max\": 1000.0, \"step\": 0.01}),\n                     \"compress_level\": (\"INT\", {\"default\": 4, \"min\": 0, \"max\": 9})\n                     },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image/animation\"\n\n    def save_images(self, images, fps, compress_level, filename_prefix=\"ldm_patched\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = ldm_patched.utils.path_utils.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        pil_images = []\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            pil_images.append(img)\n\n        metadata = None\n        if not args.disable_server_info:\n            metadata = PngInfo()\n            if prompt is not None:\n                metadata.add(b\"ldm_patched\", \"prompt\".encode(\"latin-1\", \"strict\") + b\"\\0\" + json.dumps(prompt).encode(\"latin-1\", \"strict\"), after_idat=True)\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata.add(b\"ldm_patched\", x.encode(\"latin-1\", \"strict\") + b\"\\0\" + json.dumps(extra_pnginfo[x]).encode(\"latin-1\", \"strict\"), after_idat=True)\n\n        file = f\"{filename}_{counter:05}_.png\"\n        pil_images[0].save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=compress_level, save_all=True, duration=int(1000.0/fps), append_images=pil_images[1:])\n        results.append({\n            \"filename\": file,\n            \"subfolder\": subfolder,\n            \"type\": self.type\n        })\n\n        return { \"ui\": { \"images\": results, \"animated\": (True,)} }\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageCrop\": ImageCrop,\n    \"RepeatImageBatch\": RepeatImageBatch,\n    \"SaveAnimatedWEBP\": SaveAnimatedWEBP,\n    \"SaveAnimatedPNG\": SaveAnimatedPNG,\n}\n", "ldm_patched/contrib/external_hypertile.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\n#Taken from: https://github.com/tfernd/HyperTile/\n\nimport math\nfrom einops import rearrange\n# Use torch rng for consistency across generations\nfrom torch import randint\n\ndef random_divisor(value: int, min_value: int, /, max_options: int = 1) -> int:\n    min_value = min(min_value, value)\n\n    # All big divisors of value (inclusive)\n    divisors = [i for i in range(min_value, value + 1) if value % i == 0]\n\n    ns = [value // i for i in divisors[:max_options]]  # has at least 1 element\n\n    if len(ns) - 1 > 0:\n        idx = randint(low=0, high=len(ns) - 1, size=(1,)).item()\n    else:\n        idx = 0\n\n    return ns[idx]\n\nclass HyperTile:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"tile_size\": (\"INT\", {\"default\": 256, \"min\": 1, \"max\": 2048}),\n                             \"swap_size\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 128}),\n                             \"max_depth\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10}),\n                             \"scale_depth\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, tile_size, swap_size, max_depth, scale_depth):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n\n        latent_tile_size = max(32, tile_size) // 8\n        self.temp = None\n\n        def hypertile_in(q, k, v, extra_options):\n            model_chans = q.shape[-2]\n            orig_shape = extra_options['original_shape']\n            apply_to = []\n            for i in range(max_depth + 1):\n                apply_to.append((orig_shape[-2] / (2 ** i)) * (orig_shape[-1] / (2 ** i)))\n\n            if model_chans in apply_to:\n                shape = extra_options[\"original_shape\"]\n                aspect_ratio = shape[-1] / shape[-2]\n\n                hw = q.size(1)\n                h, w = round(math.sqrt(hw * aspect_ratio)), round(math.sqrt(hw / aspect_ratio))\n\n                factor = (2 ** apply_to.index(model_chans)) if scale_depth else 1\n                nh = random_divisor(h, latent_tile_size * factor, swap_size)\n                nw = random_divisor(w, latent_tile_size * factor, swap_size)\n\n                if nh * nw > 1:\n                    q = rearrange(q, \"b (nh h nw w) c -> (b nh nw) (h w) c\", h=h // nh, w=w // nw, nh=nh, nw=nw)\n                    self.temp = (nh, nw, h, w)\n                return q, k, v\n\n            return q, k, v\n        def hypertile_out(out, extra_options):\n            if self.temp is not None:\n                nh, nw, h, w = self.temp\n                self.temp = None\n                out = rearrange(out, \"(b nh nw) hw c -> b nh nw hw c\", nh=nh, nw=nw)\n                out = rearrange(out, \"b nh nw (h w) c -> b (nh h nw w) c\", h=h // nh, w=w // nw)\n            return out\n\n\n        m = model.clone()\n        m.set_model_attn1_patch(hypertile_in)\n        m.set_model_attn1_output_patch(hypertile_out)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"HyperTile\": HyperTile,\n}\n", "ldm_patched/contrib/external_tomesd.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\n#Taken from: https://github.com/dbolya/tomesd\n\nimport torch\nfrom typing import Tuple, Callable\nimport math\n\ndef do_nothing(x: torch.Tensor, mode:str=None):\n    return x\n\n\ndef mps_gather_workaround(input, dim, index):\n    if input.shape[-1] == 1:\n        return torch.gather(\n            input.unsqueeze(-1),\n            dim - 1 if dim < 0 else dim,\n            index.unsqueeze(-1)\n        ).squeeze(-1)\n    else:\n        return torch.gather(input, dim, index)\n\n\ndef bipartite_soft_matching_random2d(metric: torch.Tensor,\n                                     w: int, h: int, sx: int, sy: int, r: int,\n                                     no_rand: bool = False) -> Tuple[Callable, Callable]:\n    \"\"\"\n    Partitions the tokens into src and dst and merges r tokens from src to dst.\n    Dst tokens are partitioned by choosing one randomy in each (sx, sy) region.\n    Args:\n     - metric [B, N, C]: metric to use for similarity\n     - w: image width in tokens\n     - h: image height in tokens\n     - sx: stride in the x dimension for dst, must divide w\n     - sy: stride in the y dimension for dst, must divide h\n     - r: number of tokens to remove (by merging)\n     - no_rand: if true, disable randomness (use top left corner only)\n    \"\"\"\n    B, N, _ = metric.shape\n\n    if r <= 0 or w == 1 or h == 1:\n        return do_nothing, do_nothing\n\n    gather = mps_gather_workaround if metric.device.type == \"mps\" else torch.gather\n    \n    with torch.no_grad():\n        \n        hsy, wsx = h // sy, w // sx\n\n        # For each sy by sx kernel, randomly assign one token to be dst and the rest src\n        if no_rand:\n            rand_idx = torch.zeros(hsy, wsx, 1, device=metric.device, dtype=torch.int64)\n        else:\n            rand_idx = torch.randint(sy*sx, size=(hsy, wsx, 1), device=metric.device)\n        \n        # The image might not divide sx and sy, so we need to work on a view of the top left if the idx buffer instead\n        idx_buffer_view = torch.zeros(hsy, wsx, sy*sx, device=metric.device, dtype=torch.int64)\n        idx_buffer_view.scatter_(dim=2, index=rand_idx, src=-torch.ones_like(rand_idx, dtype=rand_idx.dtype))\n        idx_buffer_view = idx_buffer_view.view(hsy, wsx, sy, sx).transpose(1, 2).reshape(hsy * sy, wsx * sx)\n\n        # Image is not divisible by sx or sy so we need to move it into a new buffer\n        if (hsy * sy) < h or (wsx * sx) < w:\n            idx_buffer = torch.zeros(h, w, device=metric.device, dtype=torch.int64)\n            idx_buffer[:(hsy * sy), :(wsx * sx)] = idx_buffer_view\n        else:\n            idx_buffer = idx_buffer_view\n\n        # We set dst tokens to be -1 and src to be 0, so an argsort gives us dst|src indices\n        rand_idx = idx_buffer.reshape(1, -1, 1).argsort(dim=1)\n\n        # We're finished with these\n        del idx_buffer, idx_buffer_view\n\n        # rand_idx is currently dst|src, so split them\n        num_dst = hsy * wsx\n        a_idx = rand_idx[:, num_dst:, :] # src\n        b_idx = rand_idx[:, :num_dst, :] # dst\n\n        def split(x):\n            C = x.shape[-1]\n            src = gather(x, dim=1, index=a_idx.expand(B, N - num_dst, C))\n            dst = gather(x, dim=1, index=b_idx.expand(B, num_dst, C))\n            return src, dst\n\n        # Cosine similarity between A and B\n        metric = metric / metric.norm(dim=-1, keepdim=True)\n        a, b = split(metric)\n        scores = a @ b.transpose(-1, -2)\n\n        # Can't reduce more than the # tokens in src\n        r = min(a.shape[1], r)\n\n        # Find the most similar greedily\n        node_max, node_idx = scores.max(dim=-1)\n        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n\n        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n        dst_idx = gather(node_idx[..., None], dim=-2, index=src_idx)\n\n    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n        src, dst = split(x)\n        n, t1, c = src.shape\n        \n        unm = gather(src, dim=-2, index=unm_idx.expand(n, t1 - r, c))\n        src = gather(src, dim=-2, index=src_idx.expand(n, r, c))\n        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n\n        return torch.cat([unm, dst], dim=1)\n\n    def unmerge(x: torch.Tensor) -> torch.Tensor:\n        unm_len = unm_idx.shape[1]\n        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n        _, _, c = unm.shape\n\n        src = gather(dst, dim=-2, index=dst_idx.expand(B, r, c))\n\n        # Combine back to the original shape\n        out = torch.zeros(B, N, c, device=x.device, dtype=x.dtype)\n        out.scatter_(dim=-2, index=b_idx.expand(B, num_dst, c), src=dst)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=unm_idx).expand(B, unm_len, c), src=unm)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=src_idx).expand(B, r, c), src=src)\n\n        return out\n\n    return merge, unmerge\n\n\ndef get_functions(x, ratio, original_shape):\n    b, c, original_h, original_w = original_shape\n    original_tokens = original_h * original_w\n    downsample = int(math.ceil(math.sqrt(original_tokens // x.shape[1])))\n    stride_x = 2\n    stride_y = 2\n    max_downsample = 1\n\n    if downsample <= max_downsample:\n        w = int(math.ceil(original_w / downsample))\n        h = int(math.ceil(original_h / downsample))\n        r = int(x.shape[1] * ratio)\n        no_rand = False\n        m, u = bipartite_soft_matching_random2d(x, w, h, stride_x, stride_y, r, no_rand)\n        return m, u\n\n    nothing = lambda y: y\n    return nothing, nothing\n\n\n\nclass TomePatchModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, ratio):\n        self.u = None\n        def tomesd_m(q, k, v, extra_options):\n            #NOTE: In the reference code get_functions takes x (input of the transformer block) as the argument instead of q\n            #however from my basic testing it seems that using q instead gives better results\n            m, self.u = get_functions(q, ratio, extra_options[\"original_shape\"])\n            return m(q), k, v\n        def tomesd_u(n, extra_options):\n            return self.u(n)\n\n        m = model.clone()\n        m.set_model_attn1_patch(tomesd_m)\n        m.set_model_attn1_output_patch(tomesd_u)\n        return (m, )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"TomePatchModel\": TomePatchModel,\n}\n", "ldm_patched/contrib/external_model_advanced.py": "# https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py \n\nimport ldm_patched.utils.path_utils\nimport ldm_patched.modules.sd\nimport ldm_patched.modules.model_sampling\nimport torch\n\nclass LCM(ldm_patched.modules.model_sampling.EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        timestep = self.timestep(sigma).view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        x0 = model_input - model_output * sigma\n\n        sigma_data = 0.5\n        scaled_timestep = timestep * 10.0 #timestep_scaling\n\n        c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n        c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n\n        return c_out * x0 + c_skip * model_input\n\nclass ModelSamplingDiscreteDistilled(ldm_patched.modules.model_sampling.ModelSamplingDiscrete):\n    original_timesteps = 50\n\n    def __init__(self, model_config=None):\n        super().__init__(model_config)\n\n        self.skip_steps = self.num_timesteps // self.original_timesteps\n\n        sigmas_valid = torch.zeros((self.original_timesteps), dtype=torch.float32)\n        for x in range(self.original_timesteps):\n            sigmas_valid[self.original_timesteps - 1 - x] = self.sigmas[self.num_timesteps - 1 - x * self.skip_steps]\n\n        self.set_sigmas(sigmas_valid)\n\n    def timestep(self, sigma):\n        log_sigma = sigma.log()\n        dists = log_sigma.to(self.log_sigmas.device) - self.log_sigmas[:, None]\n        return (dists.abs().argmin(dim=0).view(sigma.shape) * self.skip_steps + (self.skip_steps - 1)).to(sigma.device)\n\n    def sigma(self, timestep):\n        t = torch.clamp(((timestep.float().to(self.log_sigmas.device) - (self.skip_steps - 1)) / self.skip_steps).float(), min=0, max=(len(self.sigmas) - 1))\n        low_idx = t.floor().long()\n        high_idx = t.ceil().long()\n        w = t.frac()\n        log_sigma = (1 - w) * self.log_sigmas[low_idx] + w * self.log_sigmas[high_idx]\n        return log_sigma.exp().to(timestep.device)\n\n\ndef rescale_zero_terminal_snr_sigmas(sigmas):\n    alphas_cumprod = 1 / ((sigmas * sigmas) + 1)\n    alphas_bar_sqrt = alphas_cumprod.sqrt()\n\n    # Store old values.\n    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n\n    # Shift so the last timestep is zero.\n    alphas_bar_sqrt -= (alphas_bar_sqrt_T)\n\n    # Scale so the first timestep is back to the old value.\n    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n\n    # Convert alphas_bar_sqrt to betas\n    alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\n    alphas_bar[-1] = 4.8973451890853435e-08\n    return ((1 - alphas_bar) / alphas_bar) ** 0.5\n\nclass ModelSamplingDiscrete:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"eps\", \"v_prediction\", \"lcm\", \"tcd\"]),\n                              \"zsnr\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, sampling, zsnr):\n        m = model.clone()\n\n        sampling_base = ldm_patched.modules.model_sampling.ModelSamplingDiscrete\n        if sampling == \"eps\":\n            sampling_type = ldm_patched.modules.model_sampling.EPS\n        elif sampling == \"v_prediction\":\n            sampling_type = ldm_patched.modules.model_sampling.V_PREDICTION\n        elif sampling == \"lcm\":\n            sampling_type = LCM\n            sampling_base = ModelSamplingDiscreteDistilled\n        elif sampling == \"tcd\":\n            sampling_type = ldm_patched.modules.model_sampling.EPS\n            sampling_base = ModelSamplingDiscreteDistilled\n\n        class ModelSamplingAdvanced(sampling_base, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        if zsnr:\n            model_sampling.set_sigmas(rescale_zero_terminal_snr_sigmas(model_sampling.sigmas))\n\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        return (m, )\n\nclass ModelSamplingContinuousEDM:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"v_prediction\", \"edm_playground_v2.5\", \"eps\"],),\n                              \"sigma_max\": (\"FLOAT\", {\"default\": 120.0, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              \"sigma_min\": (\"FLOAT\", {\"default\": 0.002, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, sampling, sigma_max, sigma_min):\n        m = model.clone()\n\n        latent_format = None\n        sigma_data = 1.0\n        if sampling == \"eps\":\n            sampling_type = ldm_patched.modules.model_sampling.EPS\n        elif sampling == \"v_prediction\":\n            sampling_type = ldm_patched.modules.model_sampling.V_PREDICTION\n        elif sampling == \"edm_playground_v2.5\":\n            sampling_type = ldm_patched.modules.model_sampling.EDM\n            sigma_data = 0.5\n            latent_format = ldm_patched.modules.latent_formats.SDXL_Playground_2_5()\n\n        class ModelSamplingAdvanced(ldm_patched.modules.model_sampling.ModelSamplingContinuousEDM, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        model_sampling.set_parameters(sigma_min, sigma_max, sigma_data)\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        if latent_format is not None:\n            m.add_object_patch(\"latent_format\", latent_format)\n        return (m, )\n\nclass RescaleCFG:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 0.7, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, multiplier):\n        def rescale_cfg(args):\n            cond = args[\"cond\"]\n            uncond = args[\"uncond\"]\n            cond_scale = args[\"cond_scale\"]\n            sigma = args[\"sigma\"]\n            sigma = sigma.view(sigma.shape[:1] + (1,) * (cond.ndim - 1))\n            x_orig = args[\"input\"]\n\n            #rescale cfg has to be done on v-pred model output\n            x = x_orig / (sigma * sigma + 1.0)\n            cond = ((x - (x_orig - cond)) * (sigma ** 2 + 1.0) ** 0.5) / (sigma)\n            uncond = ((x - (x_orig - uncond)) * (sigma ** 2 + 1.0) ** 0.5) / (sigma)\n\n            #rescalecfg\n            x_cfg = uncond + cond_scale * (cond - uncond)\n            ro_pos = torch.std(cond, dim=(1,2,3), keepdim=True)\n            ro_cfg = torch.std(x_cfg, dim=(1,2,3), keepdim=True)\n\n            x_rescaled = x_cfg * (ro_pos / ro_cfg)\n            x_final = multiplier * x_rescaled + (1.0 - multiplier) * x_cfg\n\n            return x_orig - (x - x_final * sigma / (sigma * sigma + 1.0) ** 0.5)\n\n        m = model.clone()\n        m.set_model_sampler_cfg_function(rescale_cfg)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"ModelSamplingDiscrete\": ModelSamplingDiscrete,\n    \"ModelSamplingContinuousEDM\": ModelSamplingContinuousEDM,\n    \"RescaleCFG\": RescaleCFG,\n}\n", "ldm_patched/utils/path_utils.py": "import os\nimport time\n\nsupported_pt_extensions = set(['.ckpt', '.pt', '.bin', '.pth', '.safetensors'])\n\nfolder_names_and_paths = {}\n\nbase_path = os.getcwd()\nmodels_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\n\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\n\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"photomaker\"] = ([os.path.join(models_dir, \"photomaker\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\n\noutput_directory = os.path.join(os.getcwd(), \"output\")\ntemp_directory = os.path.join(os.getcwd(), \"temp\")\ninput_directory = os.path.join(os.getcwd(), \"input\")\nuser_directory = os.path.join(os.getcwd(), \"user\")\n\nfilename_list_cache = {}\n\nif not os.path.exists(input_directory):\n    try:\n        pass  # os.makedirs(input_directory)\n    except:\n        print(\"Failed to create input directory\")\n\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\n\ndef set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir\n\ndef set_input_directory(input_dir):\n    global input_directory\n    input_directory = input_dir\n\ndef get_output_directory():\n    global output_directory\n    return output_directory\n\ndef get_temp_directory():\n    global temp_directory\n    return temp_directory\n\ndef get_input_directory():\n    global input_directory\n    return input_directory\n\n\n#NOTE: used in http server so don't put folders that should not be accessed remotely\ndef get_directory_by_type(type_name):\n    if type_name == \"output\":\n        return get_output_directory()\n    if type_name == \"temp\":\n        return get_temp_directory()\n    if type_name == \"input\":\n        return get_input_directory()\n    return None\n\n\n# determine base_dir rely on annotation if name is 'filename.ext [annotation]' format\n# otherwise use default_path as base_dir\ndef annotated_filepath(name):\n    if name.endswith(\"[output]\"):\n        base_dir = get_output_directory()\n        name = name[:-9]\n    elif name.endswith(\"[input]\"):\n        base_dir = get_input_directory()\n        name = name[:-8]\n    elif name.endswith(\"[temp]\"):\n        base_dir = get_temp_directory()\n        name = name[:-7]\n    else:\n        return name, None\n\n    return name, base_dir\n\n\ndef get_annotated_filepath(name, default_dir=None):\n    name, base_dir = annotated_filepath(name)\n\n    if base_dir is None:\n        if default_dir is not None:\n            base_dir = default_dir\n        else:\n            base_dir = get_input_directory()  # fallback path\n\n    return os.path.join(base_dir, name)\n\n\ndef exists_annotated_filepath(name):\n    name, base_dir = annotated_filepath(name)\n\n    if base_dir is None:\n        base_dir = get_input_directory()  # fallback path\n\n    filepath = os.path.join(base_dir, name)\n    return os.path.exists(filepath)\n\n\ndef add_model_folder_path(folder_name, full_folder_path):\n    global folder_names_and_paths\n    if folder_name in folder_names_and_paths:\n        folder_names_and_paths[folder_name][0].append(full_folder_path)\n    else:\n        folder_names_and_paths[folder_name] = ([full_folder_path], set())\n\ndef get_folder_paths(folder_name):\n    return folder_names_and_paths[folder_name][0][:]\n\ndef recursive_search(directory, excluded_dir_names=None):\n    if not os.path.isdir(directory):\n        return [], {}\n\n    if excluded_dir_names is None:\n        excluded_dir_names = []\n\n    result = []\n    dirs = {}\n\n    # Attempt to add the initial directory to dirs with error handling\n    try:\n        dirs[directory] = os.path.getmtime(directory)\n    except FileNotFoundError:\n        print(f\"Warning: Unable to access {directory}. Skipping this path.\")\n        \n    for dirpath, subdirs, filenames in os.walk(directory, followlinks=True, topdown=True):\n        subdirs[:] = [d for d in subdirs if d not in excluded_dir_names]\n        for file_name in filenames:\n            relative_path = os.path.relpath(os.path.join(dirpath, file_name), directory)\n            result.append(relative_path)\n        \n        for d in subdirs:\n            path = os.path.join(dirpath, d)\n            try:\n                dirs[path] = os.path.getmtime(path)\n            except FileNotFoundError:\n                print(f\"Warning: Unable to access {path}. Skipping this path.\")\n                continue\n    return result, dirs\n\ndef filter_files_extensions(files, extensions):\n    return sorted(list(filter(lambda a: os.path.splitext(a)[-1].lower() in extensions or len(extensions) == 0, files)))\n\n\n\ndef get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)\n        if os.path.isfile(full_path):\n            return full_path\n\n    return None\n\ndef get_filename_list_(folder_name):\n    global folder_names_and_paths\n    output_list = set()\n    folders = folder_names_and_paths[folder_name]\n    output_folders = {}\n    for x in folders[0]:\n        files, folders_all = recursive_search(x, excluded_dir_names=[\".git\"])\n        output_list.update(filter_files_extensions(files, folders[1]))\n        output_folders = {**output_folders, **folders_all}\n\n    return (sorted(list(output_list)), output_folders, time.perf_counter())\n\ndef cached_filename_list_(folder_name):\n    global filename_list_cache\n    global folder_names_and_paths\n    if folder_name not in filename_list_cache:\n        return None\n    out = filename_list_cache[folder_name]\n\n    for x in out[1]:\n        time_modified = out[1][x]\n        folder = x\n        if os.path.getmtime(folder) != time_modified:\n            return None\n\n    folders = folder_names_and_paths[folder_name]\n    for x in folders[0]:\n        if os.path.isdir(x):\n            if x not in out[1]:\n                return None\n\n    return out\n\ndef get_filename_list(folder_name):\n    out = cached_filename_list_(folder_name)\n    if out is None:\n        out = get_filename_list_(folder_name)\n        global filename_list_cache\n        filename_list_cache[folder_name] = out\n    return list(out[0])\n\ndef get_save_image_path(filename_prefix, output_dir, image_width=0, image_height=0):\n    def map_filename(filename):\n        prefix_len = len(os.path.basename(filename_prefix))\n        prefix = filename[:prefix_len + 1]\n        try:\n            digits = int(filename[prefix_len + 1:].split('_')[0])\n        except:\n            digits = 0\n        return (digits, prefix)\n\n    def compute_vars(input, image_width, image_height):\n        input = input.replace(\"%width%\", str(image_width))\n        input = input.replace(\"%height%\", str(image_height))\n        return input\n\n    filename_prefix = compute_vars(filename_prefix, image_width, image_height)\n\n    subfolder = os.path.dirname(os.path.normpath(filename_prefix))\n    filename = os.path.basename(os.path.normpath(filename_prefix))\n\n    full_output_folder = os.path.join(output_dir, subfolder)\n\n    if os.path.commonpath((output_dir, os.path.abspath(full_output_folder))) != output_dir:\n        err = \"**** ERROR: Saving image outside the output folder is not allowed.\" + \\\n              \"\\n full_output_folder: \" + os.path.abspath(full_output_folder) + \\\n              \"\\n         output_dir: \" + output_dir + \\\n              \"\\n         commonpath: \" + os.path.commonpath((output_dir, os.path.abspath(full_output_folder))) \n        print(err)\n        raise Exception(err)\n\n    try:\n        counter = max(filter(lambda a: a[1][:-1] == filename and a[1][-1] == \"_\", map(map_filename, os.listdir(full_output_folder))))[0] + 1\n    except ValueError:\n        counter = 1\n    except FileNotFoundError:\n        os.makedirs(full_output_folder, exist_ok=True)\n        counter = 1\n    return full_output_folder, filename, counter, subfolder, filename_prefix\n", "ldm_patched/utils/latent_visualization.py": "import torch\nfrom PIL import Image\nimport struct\nimport numpy as np\nfrom ldm_patched.modules.args_parser import args, LatentPreviewMethod\nfrom ldm_patched.taesd.taesd import TAESD\nimport ldm_patched.utils.path_utils\nimport ldm_patched.modules.utils\n\nMAX_PREVIEW_RESOLUTION = 512\n\nclass LatentPreviewer:\n    def decode_latent_to_preview(self, x0):\n        pass\n\n    def decode_latent_to_preview_image(self, preview_format, x0):\n        preview_image = self.decode_latent_to_preview(x0)\n        return (\"JPEG\", preview_image, MAX_PREVIEW_RESOLUTION)\n\nclass TAESDPreviewerImpl(LatentPreviewer):\n    def __init__(self, taesd):\n        self.taesd = taesd\n\n    def decode_latent_to_preview(self, x0):\n        x_sample = self.taesd.decode(x0[:1])[0].detach()\n        x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n        x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n        x_sample = x_sample.astype(np.uint8)\n\n        preview_image = Image.fromarray(x_sample)\n        return preview_image\n\n\nclass Latent2RGBPreviewer(LatentPreviewer):\n    def __init__(self, latent_rgb_factors):\n        self.latent_rgb_factors = torch.tensor(latent_rgb_factors, device=\"cpu\")\n\n    def decode_latent_to_preview(self, x0):\n        latent_image = x0[0].permute(1, 2, 0).cpu() @ self.latent_rgb_factors\n\n        latents_ubyte = (((latent_image + 1) / 2)\n                            .clamp(0, 1)  # change scale from -1..1 to 0..1\n                            .mul(0xFF)  # to 0..255\n                            .byte()).cpu()\n\n        return Image.fromarray(latents_ubyte.numpy())\n\n\ndef get_previewer(device, latent_format):\n    previewer = None\n    method = args.preview_option\n    if method != LatentPreviewMethod.NoPreviews:\n        # TODO previewer methods\n        taesd_decoder_path = None\n        if latent_format.taesd_decoder_name is not None:\n            taesd_decoder_path = next(\n                (fn for fn in ldm_patched.utils.path_utils.get_filename_list(\"vae_approx\")\n                    if fn.startswith(latent_format.taesd_decoder_name)),\n                \"\"\n            )\n            taesd_decoder_path = ldm_patched.utils.path_utils.get_full_path(\"vae_approx\", taesd_decoder_path)\n\n        if method == LatentPreviewMethod.Auto:\n            method = LatentPreviewMethod.Latent2RGB\n            if taesd_decoder_path:\n                method = LatentPreviewMethod.TAESD\n\n        if method == LatentPreviewMethod.TAESD:\n            if taesd_decoder_path:\n                taesd = TAESD(None, taesd_decoder_path).to(device)\n                previewer = TAESDPreviewerImpl(taesd)\n            else:\n                print(\"Warning: TAESD previews enabled, but could not find models/vae_approx/{}\".format(latent_format.taesd_decoder_name))\n\n        if previewer is None:\n            if latent_format.latent_rgb_factors is not None:\n                previewer = Latent2RGBPreviewer(latent_format.latent_rgb_factors)\n    return previewer\n\ndef prepare_callback(model, steps, x0_output_dict=None):\n    preview_format = \"JPEG\"\n    if preview_format not in [\"JPEG\", \"PNG\"]:\n        preview_format = \"JPEG\"\n\n    previewer = get_previewer(model.load_device, model.model.latent_format)\n\n    pbar = ldm_patched.modules.utils.ProgressBar(steps)\n    def callback(step, x0, x, total_steps):\n        if x0_output_dict is not None:\n            x0_output_dict[\"x0\"] = x0\n\n        preview_bytes = None\n        if previewer:\n            preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\n        pbar.update_absolute(step + 1, total_steps, preview_bytes)\n    return callback\n\n", "ldm_patched/ldm/util.py": "import importlib\n\nimport torch\nfrom torch import optim\nimport numpy as np\n\nfrom inspect import isfunction\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))\n        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n\n        try:\n            draw.text((0, 0), lines, fill=\"black\", font=font)\n        except UnicodeEncodeError:\n            print(\"Cant encode string for logging. Skipping.\")\n\n        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n        txts.append(txt)\n    txts = np.stack(txts)\n    txts = torch.tensor(txts)\n    return txts\n\n\ndef ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\n\n\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\n\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n\n\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\n\n\nclass AdamWwithEMAandWings(optim.Optimizer):\n    # credit to https://gist.github.com/crowsonkb/65f7265353f403714fce3b2595e0b298\n    def __init__(self, params, lr=1.e-3, betas=(0.9, 0.999), eps=1.e-8,  # TODO: check hyperparameters before using\n                 weight_decay=1.e-2, amsgrad=False, ema_decay=0.9999,   # ema decay to match previous code\n                 ema_power=1., param_names=()):\n        \"\"\"AdamW that saves EMA versions of the parameters.\"\"\"\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= ema_decay <= 1.0:\n            raise ValueError(\"Invalid ema_decay value: {}\".format(ema_decay))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad, ema_decay=ema_decay,\n                        ema_power=ema_power, param_names=param_names)\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            exp_avg_sqs = []\n            ema_params_with_grad = []\n            state_sums = []\n            max_exp_avg_sqs = []\n            state_steps = []\n            amsgrad = group['amsgrad']\n            beta1, beta2 = group['betas']\n            ema_decay = group['ema_decay']\n            ema_power = group['ema_power']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of parameter values\n                    state['param_exp_avg'] = p.detach().float().clone()\n\n                exp_avgs.append(state['exp_avg'])\n                exp_avg_sqs.append(state['exp_avg_sq'])\n                ema_params_with_grad.append(state['param_exp_avg'])\n\n                if amsgrad:\n                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n\n                # update the steps for each param group update\n                state['step'] += 1\n                # record the step after step update\n                state_steps.append(state['step'])\n\n            optim._functional.adamw(params_with_grad,\n                    grads,\n                    exp_avgs,\n                    exp_avg_sqs,\n                    max_exp_avg_sqs,\n                    state_steps,\n                    amsgrad=amsgrad,\n                    beta1=beta1,\n                    beta2=beta2,\n                    lr=group['lr'],\n                    weight_decay=group['weight_decay'],\n                    eps=group['eps'],\n                    maximize=False)\n\n            cur_ema_decay = min(ema_decay, 1 - state['step'] ** -ema_power)\n            for param, ema_param in zip(params_with_grad, ema_params_with_grad):\n                ema_param.mul_(cur_ema_decay).add_(param.float(), alpha=1 - cur_ema_decay)\n\n        return loss", "ldm_patched/ldm/models/autoencoder.py": "import torch\n# import pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom ldm_patched.ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n\nfrom ldm_patched.ldm.util import instantiate_from_config\nfrom ldm_patched.ldm.modules.ema import LitEma\nimport ldm_patched.modules.ops\n\nclass DiagonalGaussianRegularizer(torch.nn.Module):\n    def __init__(self, sample: bool = True):\n        super().__init__()\n        self.sample = sample\n\n    def get_trainable_parameters(self) -> Any:\n        yield from ()\n\n    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n        log = dict()\n        posterior = DiagonalGaussianDistribution(z)\n        if self.sample:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        kl_loss = posterior.kl()\n        kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n        log[\"kl_loss\"] = kl_loss\n        return z, log\n\n\nclass AbstractAutoencoder(torch.nn.Module):\n    \"\"\"\n    This is the base class for all autoencoders, including image autoencoders, image autoencoders with discriminators,\n    unCLIP models, etc. Hence, it is fairly general, and specific features\n    (e.g. discriminator training, encoding, decoding) must be implemented in subclasses.\n    \"\"\"\n\n    def __init__(\n        self,\n        ema_decay: Union[None, float] = None,\n        monitor: Union[None, str] = None,\n        input_key: str = \"jpg\",\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.input_key = input_key\n        self.use_ema = ema_decay is not None\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema:\n            self.model_ema = LitEma(self, decay=ema_decay)\n            logpy.info(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n    def get_input(self, batch) -> Any:\n        raise NotImplementedError()\n\n    def on_train_batch_end(self, *args, **kwargs):\n        # for EMA computation\n        if self.use_ema:\n            self.model_ema(self)\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.parameters())\n            self.model_ema.copy_to(self)\n            if context is not None:\n                logpy.info(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.parameters())\n                if context is not None:\n                    logpy.info(f\"{context}: Restored training weights\")\n\n    def encode(self, *args, **kwargs) -> torch.Tensor:\n        raise NotImplementedError(\"encode()-method of abstract base class called\")\n\n    def decode(self, *args, **kwargs) -> torch.Tensor:\n        raise NotImplementedError(\"decode()-method of abstract base class called\")\n\n    def instantiate_optimizer_from_config(self, params, lr, cfg):\n        logpy.info(f\"loading >>> {cfg['target']} <<< optimizer from config\")\n        return get_obj_from_str(cfg[\"target\"])(\n            params, lr=lr, **cfg.get(\"params\", dict())\n        )\n\n    def configure_optimizers(self) -> Any:\n        raise NotImplementedError()\n\n\nclass AutoencodingEngine(AbstractAutoencoder):\n    \"\"\"\n    Base class for all image autoencoders that we train, like VQGAN or AutoencoderKL\n    (we also restore them explicitly as special cases for legacy reasons).\n    Regularizations such as KL or VQ are moved to the regularizer class.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        encoder_config: Dict,\n        decoder_config: Dict,\n        regularizer_config: Dict,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n        self.encoder: torch.nn.Module = instantiate_from_config(encoder_config)\n        self.decoder: torch.nn.Module = instantiate_from_config(decoder_config)\n        self.regularization: AbstractRegularizer = instantiate_from_config(\n            regularizer_config\n        )\n\n    def get_last_layer(self):\n        return self.decoder.get_last_layer()\n\n    def encode(\n        self,\n        x: torch.Tensor,\n        return_reg_log: bool = False,\n        unregularized: bool = False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, dict]]:\n        z = self.encoder(x)\n        if unregularized:\n            return z, dict()\n        z, reg_log = self.regularization(z)\n        if return_reg_log:\n            return z, reg_log\n        return z\n\n    def decode(self, z: torch.Tensor, **kwargs) -> torch.Tensor:\n        x = self.decoder(z, **kwargs)\n        return x\n\n    def forward(\n        self, x: torch.Tensor, **additional_decode_kwargs\n    ) -> Tuple[torch.Tensor, torch.Tensor, dict]:\n        z, reg_log = self.encode(x, return_reg_log=True)\n        dec = self.decode(z, **additional_decode_kwargs)\n        return z, dec, reg_log\n\n\nclass AutoencodingEngineLegacy(AutoencodingEngine):\n    def __init__(self, embed_dim: int, **kwargs):\n        self.max_batch_size = kwargs.pop(\"max_batch_size\", None)\n        ddconfig = kwargs.pop(\"ddconfig\")\n        super().__init__(\n            encoder_config={\n                \"target\": \"ldm_patched.ldm.modules.diffusionmodules.model.Encoder\",\n                \"params\": ddconfig,\n            },\n            decoder_config={\n                \"target\": \"ldm_patched.ldm.modules.diffusionmodules.model.Decoder\",\n                \"params\": ddconfig,\n            },\n            **kwargs,\n        )\n        self.quant_conv = ldm_patched.modules.ops.disable_weight_init.Conv2d(\n            (1 + ddconfig[\"double_z\"]) * ddconfig[\"z_channels\"],\n            (1 + ddconfig[\"double_z\"]) * embed_dim,\n            1,\n        )\n        self.post_quant_conv = ldm_patched.modules.ops.disable_weight_init.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n\n    def get_autoencoder_params(self) -> list:\n        params = super().get_autoencoder_params()\n        return params\n\n    def encode(\n        self, x: torch.Tensor, return_reg_log: bool = False\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, dict]]:\n        if self.max_batch_size is None:\n            z = self.encoder(x)\n            z = self.quant_conv(z)\n        else:\n            N = x.shape[0]\n            bs = self.max_batch_size\n            n_batches = int(math.ceil(N / bs))\n            z = list()\n            for i_batch in range(n_batches):\n                z_batch = self.encoder(x[i_batch * bs : (i_batch + 1) * bs])\n                z_batch = self.quant_conv(z_batch)\n                z.append(z_batch)\n            z = torch.cat(z, 0)\n\n        z, reg_log = self.regularization(z)\n        if return_reg_log:\n            return z, reg_log\n        return z\n\n    def decode(self, z: torch.Tensor, **decoder_kwargs) -> torch.Tensor:\n        if self.max_batch_size is None:\n            dec = self.post_quant_conv(z)\n            dec = self.decoder(dec, **decoder_kwargs)\n        else:\n            N = z.shape[0]\n            bs = self.max_batch_size\n            n_batches = int(math.ceil(N / bs))\n            dec = list()\n            for i_batch in range(n_batches):\n                dec_batch = self.post_quant_conv(z[i_batch * bs : (i_batch + 1) * bs])\n                dec_batch = self.decoder(dec_batch, **decoder_kwargs)\n                dec.append(dec_batch)\n            dec = torch.cat(dec, 0)\n\n        return dec\n\n\nclass AutoencoderKL(AutoencodingEngineLegacy):\n    def __init__(self, **kwargs):\n        if \"lossconfig\" in kwargs:\n            kwargs[\"loss_config\"] = kwargs.pop(\"lossconfig\")\n        super().__init__(\n            regularizer_config={\n                \"target\": (\n                    \"ldm_patched.ldm.models.autoencoder.DiagonalGaussianRegularizer\"\n                )\n            },\n            **kwargs,\n        )\n", "ldm_patched/ldm/modules/attention.py": "import math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom typing import Optional, Any\n\nfrom .diffusionmodules.util import checkpoint, AlphaBlender, timestep_embedding\nfrom .sub_quadratic_attention import efficient_dot_product_attention\n\nfrom ldm_patched.modules import model_management\n\nif model_management.xformers_enabled():\n    import xformers\n    import xformers.ops\n\nfrom ldm_patched.modules.args_parser import args\nimport ldm_patched.modules.ops\nops = ldm_patched.modules.ops.disable_weight_init\n\n# CrossAttn precision handling\nif args.disable_attention_upcast:\n    print(\"disabling upcasting of attention\")\n    _ATTN_PRECISION = \"fp16\"\nelse:\n    _ATTN_PRECISION = \"fp32\"\n\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d\n\n\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\n\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.proj = operations.Linear(dim_in, dim_out * 2, dtype=dtype, device=device)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0., dtype=None, device=None, operations=ops):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            operations.Linear(dim, inner_dim, dtype=dtype, device=device),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim, dtype=dtype, device=device, operations=operations)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            operations.Linear(inner_dim, dim_out, dtype=dtype, device=device)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Normalize(in_channels, dtype=None, device=None):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\n\ndef attention_basic(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),\n    )\n\n    # force cast to fp32 to avoid overflowing\n    if _ATTN_PRECISION ==\"fp32\":\n        sim = einsum('b i d, b j d -> b i j', q.float(), k.float()) * scale\n    else:\n        sim = einsum('b i d, b j d -> b i j', q, k) * scale\n\n    del q, k\n\n    if exists(mask):\n        if mask.dtype == torch.bool:\n            mask = rearrange(mask, 'b ... -> b (...)') #TODO: check if this bool part matches pytorch attention\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n        else:\n            sim += mask\n\n    # attention, what we cannot get enough of\n    sim = sim.softmax(dim=-1)\n\n    out = einsum('b i j, b j d -> b i d', sim.to(v.dtype), v)\n    out = (\n        out.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return out\n\n\ndef attention_sub_quad(query, key, value, heads, mask=None):\n    b, _, dim_head = query.shape\n    dim_head //= heads\n\n    scale = dim_head ** -0.5\n    query = query.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n    value = value.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n\n    key = key.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 3, 1).reshape(b * heads, dim_head, -1)\n\n    dtype = query.dtype\n    upcast_attention = _ATTN_PRECISION ==\"fp32\" and query.dtype != torch.float32\n    if upcast_attention:\n        bytes_per_token = torch.finfo(torch.float32).bits//8\n    else:\n        bytes_per_token = torch.finfo(query.dtype).bits//8\n    batch_x_heads, q_tokens, _ = query.shape\n    _, _, k_tokens = key.shape\n    qk_matmul_size_bytes = batch_x_heads * bytes_per_token * q_tokens * k_tokens\n\n    mem_free_total, mem_free_torch = model_management.get_free_memory(query.device, True)\n\n    kv_chunk_size_min = None\n    kv_chunk_size = None\n    query_chunk_size = None\n\n    for x in [4096, 2048, 1024, 512, 256]:\n        count = mem_free_total / (batch_x_heads * bytes_per_token * x * 4.0)\n        if count >= k_tokens:\n            kv_chunk_size = k_tokens\n            query_chunk_size = x\n            break\n\n    if query_chunk_size is None:\n        query_chunk_size = 512\n\n    hidden_states = efficient_dot_product_attention(\n        query,\n        key,\n        value,\n        query_chunk_size=query_chunk_size,\n        kv_chunk_size=kv_chunk_size,\n        kv_chunk_size_min=kv_chunk_size_min,\n        use_checkpoint=False,\n        upcast_attention=upcast_attention,\n        mask=mask,\n    )\n\n    hidden_states = hidden_states.to(dtype)\n\n    hidden_states = hidden_states.unflatten(0, (-1, heads)).transpose(1,2).flatten(start_dim=2)\n    return hidden_states\n\ndef attention_split(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),\n    )\n\n    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n\n    mem_free_total = model_management.get_free_memory(q.device)\n\n    if _ATTN_PRECISION ==\"fp32\":\n        element_size = 4\n    else:\n        element_size = q.element_size()\n\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * element_size\n    modifier = 3\n    mem_required = tensor_size * modifier\n    steps = 1\n\n\n    if mem_required > mem_free_total:\n        steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n        # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n        #      f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n\n    if steps > 64:\n        max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n        raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n                            f'Need: {mem_required/64/gb:0.1f}GB free, Have:{mem_free_total/gb:0.1f}GB free')\n\n    # print(\"steps\", steps, mem_required, mem_free_total, modifier, q.element_size(), tensor_size)\n    first_op_done = False\n    cleared_cache = False\n    while True:\n        try:\n            slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n            for i in range(0, q.shape[1], slice_size):\n                end = i + slice_size\n                if _ATTN_PRECISION ==\"fp32\":\n                    with torch.autocast(enabled=False, device_type = 'cuda'):\n                        s1 = einsum('b i d, b j d -> b i j', q[:, i:end].float(), k.float()) * scale\n                else:\n                    s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k) * scale\n\n                if mask is not None:\n                    if len(mask.shape) == 2:\n                        s1 += mask[i:end]\n                    else:\n                        s1 += mask[:, i:end]\n\n                s2 = s1.softmax(dim=-1).to(v.dtype)\n                del s1\n                first_op_done = True\n\n                r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n                del s2\n            break\n        except model_management.OOM_EXCEPTION as e:\n            if first_op_done == False:\n                model_management.soft_empty_cache(True)\n                if cleared_cache == False:\n                    cleared_cache = True\n                    print(\"out of memory error, emptying cache and trying again\")\n                    continue\n                steps *= 2\n                if steps > 64:\n                    raise e\n                print(\"out of memory error, increasing steps and trying again\", steps)\n            else:\n                raise e\n\n    del q, k, v\n\n    r1 = (\n        r1.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return r1\n\nBROKEN_XFORMERS = False\ntry:\n    x_vers = xformers.__version__\n    #I think 0.0.23 is also broken (q with bs bigger than 65535 gives CUDA error)\n    BROKEN_XFORMERS = x_vers.startswith(\"0.0.21\") or x_vers.startswith(\"0.0.22\") or x_vers.startswith(\"0.0.23\")\nexcept:\n    pass\n\ndef attention_xformers(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    if BROKEN_XFORMERS:\n        if b * heads > 65535:\n            return attention_pytorch(q, k, v, heads, mask)\n\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),\n    )\n\n    if mask is not None:\n        pad = 8 - q.shape[1] % 8\n        mask_out = torch.empty([q.shape[0], q.shape[1], q.shape[1] + pad], dtype=q.dtype, device=q.device)\n        mask_out[:, :, :mask.shape[-1]] = mask\n        mask = mask_out[:, :, :mask.shape[-1]]\n\n    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=mask)\n\n    out = (\n        out.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return out\n\ndef attention_pytorch(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    q, k, v = map(\n        lambda t: t.view(b, -1, heads, dim_head).transpose(1, 2),\n        (q, k, v),\n    )\n\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)\n    out = (\n        out.transpose(1, 2).reshape(b, -1, heads * dim_head)\n    )\n    return out\n\n\noptimized_attention = attention_basic\n\nif model_management.xformers_enabled():\n    print(\"Using xformers cross attention\")\n    optimized_attention = attention_xformers\nelif model_management.pytorch_attention_enabled():\n    print(\"Using pytorch cross attention\")\n    optimized_attention = attention_pytorch\nelse:\n    if args.attention_split:\n        print(\"Using split optimization for cross attention\")\n        optimized_attention = attention_split\n    else:\n        print(\"Using sub quadratic optimization for cross attention, if you have memory or speed issues try using: --attention-split\")\n        optimized_attention = attention_sub_quad\n\noptimized_attention_masked = optimized_attention\n\ndef optimized_attention_for_device(device, mask=False, small_input=False):\n    if small_input:\n        if model_management.pytorch_attention_enabled():\n            return attention_pytorch #TODO: need to confirm but this is probably slightly faster for small inputs in all cases\n        else:\n            return attention_basic\n\n    if device == torch.device(\"cpu\"):\n        return attention_sub_quad\n\n    if mask:\n        return optimized_attention_masked\n\n    return optimized_attention\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., dtype=None, device=None, operations=ops):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = operations.Linear(query_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_k = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_v = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)\n\n        self.to_out = nn.Sequential(operations.Linear(inner_dim, query_dim, dtype=dtype, device=device), nn.Dropout(dropout))\n\n    def forward(self, x, context=None, value=None, mask=None):\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        if value is not None:\n            v = self.to_v(value)\n            del value\n        else:\n            v = self.to_v(context)\n\n        if mask is None:\n            out = optimized_attention(q, k, v, self.heads)\n        else:\n            out = optimized_attention_masked(q, k, v, self.heads, mask)\n        return self.to_out(out)\n\n\nclass BasicTransformerBlock(nn.Module):\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True, ff_in=False, inner_dim=None,\n                 disable_self_attn=False, disable_temporal_crossattention=False, switch_temporal_ca_to_sa=False, dtype=None, device=None, operations=ops):\n        super().__init__()\n\n        self.ff_in = ff_in or inner_dim is not None\n        if inner_dim is None:\n            inner_dim = dim\n\n        self.is_res = inner_dim == dim\n\n        if self.ff_in:\n            self.norm_in = operations.LayerNorm(dim, dtype=dtype, device=device)\n            self.ff_in = FeedForward(dim, dim_out=inner_dim, dropout=dropout, glu=gated_ff, dtype=dtype, device=device, operations=operations)\n\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = CrossAttention(query_dim=inner_dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                              context_dim=context_dim if self.disable_self_attn else None, dtype=dtype, device=device, operations=operations)  # is a self-attention if not self.disable_self_attn\n        self.ff = FeedForward(inner_dim, dim_out=dim, dropout=dropout, glu=gated_ff, dtype=dtype, device=device, operations=operations)\n\n        if disable_temporal_crossattention:\n            if switch_temporal_ca_to_sa:\n                raise ValueError\n            else:\n                self.attn2 = None\n        else:\n            context_dim_attn2 = None\n            if not switch_temporal_ca_to_sa:\n                context_dim_attn2 = context_dim\n\n            self.attn2 = CrossAttention(query_dim=inner_dim, context_dim=context_dim_attn2,\n                                heads=n_heads, dim_head=d_head, dropout=dropout, dtype=dtype, device=device, operations=operations)  # is self-attn if context is none\n            self.norm2 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n\n        self.norm1 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n        self.norm3 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n        self.checkpoint = checkpoint\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.switch_temporal_ca_to_sa = switch_temporal_ca_to_sa\n\n    def forward(self, x, context=None, transformer_options={}):\n        return checkpoint(self._forward, (x, context, transformer_options), self.parameters(), self.checkpoint)\n\n    def _forward(self, x, context=None, transformer_options={}):\n        extra_options = {}\n        block = transformer_options.get(\"block\", None)\n        block_index = transformer_options.get(\"block_index\", 0)\n        transformer_patches = {}\n        transformer_patches_replace = {}\n\n        for k in transformer_options:\n            if k == \"patches\":\n                transformer_patches = transformer_options[k]\n            elif k == \"patches_replace\":\n                transformer_patches_replace = transformer_options[k]\n            else:\n                extra_options[k] = transformer_options[k]\n\n        extra_options[\"n_heads\"] = self.n_heads\n        extra_options[\"dim_head\"] = self.d_head\n\n        if self.ff_in:\n            x_skip = x\n            x = self.ff_in(self.norm_in(x))\n            if self.is_res:\n                x += x_skip\n\n        n = self.norm1(x)\n        if self.disable_self_attn:\n            context_attn1 = context\n        else:\n            context_attn1 = None\n        value_attn1 = None\n\n        if \"attn1_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn1_patch\"]\n            if context_attn1 is None:\n                context_attn1 = n\n            value_attn1 = context_attn1\n            for p in patch:\n                n, context_attn1, value_attn1 = p(n, context_attn1, value_attn1, extra_options)\n\n        if block is not None:\n            transformer_block = (block[0], block[1], block_index)\n        else:\n            transformer_block = None\n        attn1_replace_patch = transformer_patches_replace.get(\"attn1\", {})\n        block_attn1 = transformer_block\n        if block_attn1 not in attn1_replace_patch:\n            block_attn1 = block\n\n        if block_attn1 in attn1_replace_patch:\n            if context_attn1 is None:\n                context_attn1 = n\n                value_attn1 = n\n            n = self.attn1.to_q(n)\n            context_attn1 = self.attn1.to_k(context_attn1)\n            value_attn1 = self.attn1.to_v(value_attn1)\n            n = attn1_replace_patch[block_attn1](n, context_attn1, value_attn1, extra_options)\n            n = self.attn1.to_out(n)\n        else:\n            n = self.attn1(n, context=context_attn1, value=value_attn1)\n\n        if \"attn1_output_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn1_output_patch\"]\n            for p in patch:\n                n = p(n, extra_options)\n\n        x += n\n        if \"middle_patch\" in transformer_patches:\n            patch = transformer_patches[\"middle_patch\"]\n            for p in patch:\n                x = p(x, extra_options)\n\n        if self.attn2 is not None:\n            n = self.norm2(x)\n            if self.switch_temporal_ca_to_sa:\n                context_attn2 = n\n            else:\n                context_attn2 = context\n            value_attn2 = None\n            if \"attn2_patch\" in transformer_patches:\n                patch = transformer_patches[\"attn2_patch\"]\n                value_attn2 = context_attn2\n                for p in patch:\n                    n, context_attn2, value_attn2 = p(n, context_attn2, value_attn2, extra_options)\n\n            attn2_replace_patch = transformer_patches_replace.get(\"attn2\", {})\n            block_attn2 = transformer_block\n            if block_attn2 not in attn2_replace_patch:\n                block_attn2 = block\n\n            if block_attn2 in attn2_replace_patch:\n                if value_attn2 is None:\n                    value_attn2 = context_attn2\n                n = self.attn2.to_q(n)\n                context_attn2 = self.attn2.to_k(context_attn2)\n                value_attn2 = self.attn2.to_v(value_attn2)\n                n = attn2_replace_patch[block_attn2](n, context_attn2, value_attn2, extra_options)\n                n = self.attn2.to_out(n)\n            else:\n                n = self.attn2(n, context=context_attn2, value=value_attn2)\n\n        if \"attn2_output_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn2_output_patch\"]\n            for p in patch:\n                n = p(n, extra_options)\n\n        x += n\n        if self.is_res:\n            x_skip = x\n        x = self.ff(self.norm3(x))\n        if self.is_res:\n            x += x_skip\n\n        return x\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head,\n                 depth=1, dropout=0., context_dim=None,\n                 disable_self_attn=False, use_linear=False,\n                 use_checkpoint=True, dtype=None, device=None, operations=ops):\n        super().__init__()\n        if exists(context_dim) and not isinstance(context_dim, list):\n            context_dim = [context_dim] * depth\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = operations.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\n        if not use_linear:\n            self.proj_in = operations.Conv2d(in_channels,\n                                     inner_dim,\n                                     kernel_size=1,\n                                     stride=1,\n                                     padding=0, dtype=dtype, device=device)\n        else:\n            self.proj_in = operations.Linear(in_channels, inner_dim, dtype=dtype, device=device)\n\n        self.transformer_blocks = nn.ModuleList(\n            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim[d],\n                                   disable_self_attn=disable_self_attn, checkpoint=use_checkpoint, dtype=dtype, device=device, operations=operations)\n                for d in range(depth)]\n        )\n        if not use_linear:\n            self.proj_out = operations.Conv2d(inner_dim,in_channels,\n                                                  kernel_size=1,\n                                                  stride=1,\n                                                  padding=0, dtype=dtype, device=device)\n        else:\n            self.proj_out = operations.Linear(in_channels, inner_dim, dtype=dtype, device=device)\n        self.use_linear = use_linear\n\n    def forward(self, x, context=None, transformer_options={}):\n        # note: if no context is given, cross-attention defaults to self-attention\n        if not isinstance(context, list):\n            context = [context] * len(self.transformer_blocks)\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, 'b c h w -> b (h w) c').contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        for i, block in enumerate(self.transformer_blocks):\n            transformer_options[\"block_index\"] = i\n            x = block(x, context=context[i], transformer_options=transformer_options)\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = self.proj_out(x)\n        return x + x_in\n\n\nclass SpatialVideoTransformer(SpatialTransformer):\n    def __init__(\n        self,\n        in_channels,\n        n_heads,\n        d_head,\n        depth=1,\n        dropout=0.0,\n        use_linear=False,\n        context_dim=None,\n        use_spatial_context=False,\n        timesteps=None,\n        merge_strategy: str = \"fixed\",\n        merge_factor: float = 0.5,\n        time_context_dim=None,\n        ff_in=False,\n        checkpoint=False,\n        time_depth=1,\n        disable_self_attn=False,\n        disable_temporal_crossattention=False,\n        max_time_embed_period: int = 10000,\n        dtype=None, device=None, operations=ops\n    ):\n        super().__init__(\n            in_channels,\n            n_heads,\n            d_head,\n            depth=depth,\n            dropout=dropout,\n            use_checkpoint=checkpoint,\n            context_dim=context_dim,\n            use_linear=use_linear,\n            disable_self_attn=disable_self_attn,\n            dtype=dtype, device=device, operations=operations\n        )\n        self.time_depth = time_depth\n        self.depth = depth\n        self.max_time_embed_period = max_time_embed_period\n\n        time_mix_d_head = d_head\n        n_time_mix_heads = n_heads\n\n        time_mix_inner_dim = int(time_mix_d_head * n_time_mix_heads)\n\n        inner_dim = n_heads * d_head\n        if use_spatial_context:\n            time_context_dim = context_dim\n\n        self.time_stack = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    inner_dim,\n                    n_time_mix_heads,\n                    time_mix_d_head,\n                    dropout=dropout,\n                    context_dim=time_context_dim,\n                    # timesteps=timesteps,\n                    checkpoint=checkpoint,\n                    ff_in=ff_in,\n                    inner_dim=time_mix_inner_dim,\n                    disable_self_attn=disable_self_attn,\n                    disable_temporal_crossattention=disable_temporal_crossattention,\n                    dtype=dtype, device=device, operations=operations\n                )\n                for _ in range(self.depth)\n            ]\n        )\n\n        assert len(self.time_stack) == len(self.transformer_blocks)\n\n        self.use_spatial_context = use_spatial_context\n        self.in_channels = in_channels\n\n        time_embed_dim = self.in_channels * 4\n        self.time_pos_embed = nn.Sequential(\n            operations.Linear(self.in_channels, time_embed_dim, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, self.in_channels, dtype=dtype, device=device),\n        )\n\n        self.time_mixer = AlphaBlender(\n            alpha=merge_factor, merge_strategy=merge_strategy\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        context: Optional[torch.Tensor] = None,\n        time_context: Optional[torch.Tensor] = None,\n        timesteps: Optional[int] = None,\n        image_only_indicator: Optional[torch.Tensor] = None,\n        transformer_options={}\n    ) -> torch.Tensor:\n        _, _, h, w = x.shape\n        x_in = x\n        spatial_context = None\n        if exists(context):\n            spatial_context = context\n\n        if self.use_spatial_context:\n            assert (\n                context.ndim == 3\n            ), f\"n dims of spatial context should be 3 but are {context.ndim}\"\n\n            if time_context is None:\n                time_context = context\n            time_context_first_timestep = time_context[::timesteps]\n            time_context = repeat(\n                time_context_first_timestep, \"b ... -> (b n) ...\", n=h * w\n            )\n        elif time_context is not None and not self.use_spatial_context:\n            time_context = repeat(time_context, \"b ... -> (b n) ...\", n=h * w)\n            if time_context.ndim == 2:\n                time_context = rearrange(time_context, \"b c -> b 1 c\")\n\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n        if self.use_linear:\n            x = self.proj_in(x)\n\n        num_frames = torch.arange(timesteps, device=x.device)\n        num_frames = repeat(num_frames, \"t -> b t\", b=x.shape[0] // timesteps)\n        num_frames = rearrange(num_frames, \"b t -> (b t)\")\n        t_emb = timestep_embedding(num_frames, self.in_channels, repeat_only=False, max_period=self.max_time_embed_period).to(x.dtype)\n        emb = self.time_pos_embed(t_emb)\n        emb = emb[:, None, :]\n\n        for it_, (block, mix_block) in enumerate(\n            zip(self.transformer_blocks, self.time_stack)\n        ):\n            transformer_options[\"block_index\"] = it_\n            x = block(\n                x,\n                context=spatial_context,\n                transformer_options=transformer_options,\n            )\n\n            x_mix = x\n            x_mix = x_mix + emb\n\n            B, S, C = x_mix.shape\n            x_mix = rearrange(x_mix, \"(b t) s c -> (b s) t c\", t=timesteps)\n            x_mix = mix_block(x_mix, context=time_context) #TODO: transformer_options\n            x_mix = rearrange(\n                x_mix, \"(b s) t c -> (b t) s c\", s=S, b=B // timesteps, c=C, t=timesteps\n            )\n\n            x = self.time_mixer(x_spatial=x, x_temporal=x_mix, image_only_indicator=image_only_indicator)\n\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n        if not self.use_linear:\n            x = self.proj_out(x)\n        out = x + x_in\n        return out\n\n\n", "ldm_patched/ldm/modules/sub_quadratic_attention.py": "# original source:\n#   https://github.com/AminRezaei0x443/memory-efficient-attention/blob/1bc0d9e6ac5f82ea43a375135c4e1d3896ee1694/memory_efficient_attention/attention_torch.py\n# license:\n#   MIT\n# credit:\n#   Amin Rezaei (original author)\n#   Alex Birch (optimized algorithm for 3D tensors, at the expense of removing bias, masking and callbacks)\n# implementation of:\n#   Self-attention Does Not Need O(n2) Memory\":\n#   https://arxiv.org/abs/2112.05682v2\n\nfrom functools import partial\nimport torch\nfrom torch import Tensor\nfrom torch.utils.checkpoint import checkpoint\nimport math\n\ntry:\n\tfrom typing import Optional, NamedTuple, List, Protocol\nexcept ImportError:\n\tfrom typing import Optional, NamedTuple, List\n\tfrom typing_extensions import Protocol\n\nfrom torch import Tensor\nfrom typing import List\n\nfrom ldm_patched.modules import model_management\n\ndef dynamic_slice(\n    x: Tensor,\n    starts: List[int],\n    sizes: List[int],\n) -> Tensor:\n    slicing = [slice(start, start + size) for start, size in zip(starts, sizes)]\n    return x[slicing]\n\nclass AttnChunk(NamedTuple):\n    exp_values: Tensor\n    exp_weights_sum: Tensor\n    max_score: Tensor\n\nclass SummarizeChunk(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> AttnChunk: ...\n\nclass ComputeQueryChunkAttn(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> Tensor: ...\n\ndef _summarize_chunk(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    scale: float,\n    upcast_attention: bool,\n    mask,\n) -> AttnChunk:\n    if upcast_attention:\n        with torch.autocast(enabled=False, device_type = 'cuda'):\n            query = query.float()\n            key_t = key_t.float()\n            attn_weights = torch.baddbmm(\n                torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n                query,\n                key_t,\n                alpha=scale,\n                beta=0,\n            )\n    else:\n        attn_weights = torch.baddbmm(\n            torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n            query,\n            key_t,\n            alpha=scale,\n            beta=0,\n        )\n    max_score, _ = torch.max(attn_weights, -1, keepdim=True)\n    max_score = max_score.detach()\n    attn_weights -= max_score\n    if mask is not None:\n        attn_weights += mask\n    torch.exp(attn_weights, out=attn_weights)\n    exp_weights = attn_weights.to(value.dtype)\n    exp_values = torch.bmm(exp_weights, value)\n    max_score = max_score.squeeze(-1)\n    return AttnChunk(exp_values, exp_weights.sum(dim=-1), max_score)\n\ndef _query_chunk_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    summarize_chunk: SummarizeChunk,\n    kv_chunk_size: int,\n    mask,\n) -> Tensor:\n    batch_x_heads, k_channels_per_head, k_tokens = key_t.shape\n    _, _, v_channels_per_head = value.shape\n\n    def chunk_scanner(chunk_idx: int, mask) -> AttnChunk:\n        key_chunk = dynamic_slice(\n            key_t,\n            (0, 0, chunk_idx),\n            (batch_x_heads, k_channels_per_head, kv_chunk_size)\n        )\n        value_chunk = dynamic_slice(\n            value,\n            (0, chunk_idx, 0),\n            (batch_x_heads, kv_chunk_size, v_channels_per_head)\n        )\n        if mask is not None:\n            mask = mask[:,:,chunk_idx:chunk_idx + kv_chunk_size]\n\n        return summarize_chunk(query, key_chunk, value_chunk, mask=mask)\n\n    chunks: List[AttnChunk] = [\n        chunk_scanner(chunk, mask) for chunk in torch.arange(0, k_tokens, kv_chunk_size)\n    ]\n    acc_chunk = AttnChunk(*map(torch.stack, zip(*chunks)))\n    chunk_values, chunk_weights, chunk_max = acc_chunk\n\n    global_max, _ = torch.max(chunk_max, 0, keepdim=True)\n    max_diffs = torch.exp(chunk_max - global_max)\n    chunk_values *= torch.unsqueeze(max_diffs, -1)\n    chunk_weights *= max_diffs\n\n    all_values = chunk_values.sum(dim=0)\n    all_weights = torch.unsqueeze(chunk_weights, -1).sum(dim=0)\n    return all_values / all_weights\n\n# TODO: refactor CrossAttention#get_attention_scores to share code with this\ndef _get_attention_scores_no_kv_chunking(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    scale: float,\n    upcast_attention: bool,\n    mask,\n) -> Tensor:\n    if upcast_attention:\n        with torch.autocast(enabled=False, device_type = 'cuda'):\n            query = query.float()\n            key_t = key_t.float()\n            attn_scores = torch.baddbmm(\n                torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n                query,\n                key_t,\n                alpha=scale,\n                beta=0,\n            )\n    else:\n        attn_scores = torch.baddbmm(\n            torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n            query,\n            key_t,\n            alpha=scale,\n            beta=0,\n        )\n\n    if mask is not None:\n        attn_scores += mask\n    try:\n        attn_probs = attn_scores.softmax(dim=-1)\n        del attn_scores\n    except model_management.OOM_EXCEPTION:\n        print(\"ran out of memory while running softmax in  _get_attention_scores_no_kv_chunking, trying slower in place softmax instead\")\n        attn_scores -= attn_scores.max(dim=-1, keepdim=True).values\n        torch.exp(attn_scores, out=attn_scores)\n        summed = torch.sum(attn_scores, dim=-1, keepdim=True)\n        attn_scores /= summed\n        attn_probs = attn_scores\n\n    hidden_states_slice = torch.bmm(attn_probs.to(value.dtype), value)\n    return hidden_states_slice\n\nclass ScannedChunk(NamedTuple):\n    chunk_idx: int\n    attn_chunk: AttnChunk\n\ndef efficient_dot_product_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    query_chunk_size=1024,\n    kv_chunk_size: Optional[int] = None,\n    kv_chunk_size_min: Optional[int] = None,\n    use_checkpoint=True,\n    upcast_attention=False,\n    mask = None,\n):\n    \"\"\"Computes efficient dot-product attention given query, transposed key, and value.\n      This is efficient version of attention presented in\n      https://arxiv.org/abs/2112.05682v2 which comes with O(sqrt(n)) memory requirements.\n      Args:\n        query: queries for calculating attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        key_t: keys for calculating attention with shape of\n          `[batch * num_heads, channels_per_head, tokens]`.\n        value: values to be used in attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        query_chunk_size: int: query chunks size\n        kv_chunk_size: Optional[int]: key/value chunks size. if None: defaults to sqrt(key_tokens)\n        kv_chunk_size_min: Optional[int]: key/value minimum chunk size. only considered when kv_chunk_size is None. changes `sqrt(key_tokens)` into `max(sqrt(key_tokens), kv_chunk_size_min)`, to ensure our chunk sizes don't get too small (smaller chunks = more chunks = less concurrent work done).\n        use_checkpoint: bool: whether to use checkpointing (recommended True for training, False for inference)\n      Returns:\n        Output of shape `[batch * num_heads, query_tokens, channels_per_head]`.\n      \"\"\"\n    batch_x_heads, q_tokens, q_channels_per_head = query.shape\n    _, _, k_tokens = key_t.shape\n    scale = q_channels_per_head ** -0.5\n\n    kv_chunk_size = min(kv_chunk_size or int(math.sqrt(k_tokens)), k_tokens)\n    if kv_chunk_size_min is not None:\n        kv_chunk_size = max(kv_chunk_size, kv_chunk_size_min)\n\n    if mask is not None and len(mask.shape) == 2:\n        mask = mask.unsqueeze(0)\n\n    def get_query_chunk(chunk_idx: int) -> Tensor:\n        return dynamic_slice(\n            query,\n            (0, chunk_idx, 0),\n            (batch_x_heads, min(query_chunk_size, q_tokens), q_channels_per_head)\n        )\n\n    def get_mask_chunk(chunk_idx: int) -> Tensor:\n        if mask is None:\n            return None\n        chunk = min(query_chunk_size, q_tokens)\n        return mask[:,chunk_idx:chunk_idx + chunk]\n\n    summarize_chunk: SummarizeChunk = partial(_summarize_chunk, scale=scale, upcast_attention=upcast_attention)\n    summarize_chunk: SummarizeChunk = partial(checkpoint, summarize_chunk) if use_checkpoint else summarize_chunk\n    compute_query_chunk_attn: ComputeQueryChunkAttn = partial(\n        _get_attention_scores_no_kv_chunking,\n        scale=scale,\n        upcast_attention=upcast_attention\n    ) if k_tokens <= kv_chunk_size else (\n        # fast-path for when there's just 1 key-value chunk per query chunk (this is just sliced attention btw)\n        partial(\n            _query_chunk_attention,\n            kv_chunk_size=kv_chunk_size,\n            summarize_chunk=summarize_chunk,\n        )\n    )\n\n    if q_tokens <= query_chunk_size:\n        # fast-path for when there's just 1 query chunk\n        return compute_query_chunk_attn(\n            query=query,\n            key_t=key_t,\n            value=value,\n            mask=mask,\n        )\n    \n    # TODO: maybe we should use torch.empty_like(query) to allocate storage in-advance,\n    # and pass slices to be mutated, instead of torch.cat()ing the returned slices\n    res = torch.cat([\n        compute_query_chunk_attn(\n            query=get_query_chunk(i * query_chunk_size),\n            key_t=key_t,\n            value=value,\n            mask=get_mask_chunk(i * query_chunk_size)\n        ) for i in range(math.ceil(q_tokens / query_chunk_size))\n    ], dim=1)\n    return res\n", "ldm_patched/ldm/modules/temporal_ae.py": "import functools\nfrom typing import Callable, Iterable, Union\n\nimport torch\nfrom einops import rearrange, repeat\n\nimport ldm_patched.modules.ops\nops = ldm_patched.modules.ops.disable_weight_init\n\nfrom .diffusionmodules.model import (\n    AttnBlock,\n    Decoder,\n    ResnetBlock,\n)\nfrom .diffusionmodules.openaimodel import ResBlock, timestep_embedding\nfrom .attention import BasicTransformerBlock\n\ndef partialclass(cls, *args, **kwargs):\n    class NewCls(cls):\n        __init__ = functools.partialmethod(cls.__init__, *args, **kwargs)\n\n    return NewCls\n\n\nclass VideoResBlock(ResnetBlock):\n    def __init__(\n        self,\n        out_channels,\n        *args,\n        dropout=0.0,\n        video_kernel_size=3,\n        alpha=0.0,\n        merge_strategy=\"learned\",\n        **kwargs,\n    ):\n        super().__init__(out_channels=out_channels, dropout=dropout, *args, **kwargs)\n        if video_kernel_size is None:\n            video_kernel_size = [3, 1, 1]\n        self.time_stack = ResBlock(\n            channels=out_channels,\n            emb_channels=0,\n            dropout=dropout,\n            dims=3,\n            use_scale_shift_norm=False,\n            use_conv=False,\n            up=False,\n            down=False,\n            kernel_size=video_kernel_size,\n            use_checkpoint=False,\n            skip_t_emb=True,\n        )\n\n        self.merge_strategy = merge_strategy\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif self.merge_strategy == \"learned\":\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def get_alpha(self, bs):\n        if self.merge_strategy == \"fixed\":\n            return self.mix_factor\n        elif self.merge_strategy == \"learned\":\n            return torch.sigmoid(self.mix_factor)\n        else:\n            raise NotImplementedError()\n\n    def forward(self, x, temb, skip_video=False, timesteps=None):\n        b, c, h, w = x.shape\n        if timesteps is None:\n            timesteps = b\n\n        x = super().forward(x, temb)\n\n        if not skip_video:\n            x_mix = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n\n            x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n\n            x = self.time_stack(x, temb)\n\n            alpha = self.get_alpha(bs=b // timesteps).to(x.device)\n            x = alpha * x + (1.0 - alpha) * x_mix\n\n            x = rearrange(x, \"b c t h w -> (b t) c h w\")\n        return x\n\n\nclass AE3DConv(ops.Conv2d):\n    def __init__(self, in_channels, out_channels, video_kernel_size=3, *args, **kwargs):\n        super().__init__(in_channels, out_channels, *args, **kwargs)\n        if isinstance(video_kernel_size, Iterable):\n            padding = [int(k // 2) for k in video_kernel_size]\n        else:\n            padding = int(video_kernel_size // 2)\n\n        self.time_mix_conv = ops.Conv3d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=video_kernel_size,\n            padding=padding,\n        )\n\n    def forward(self, input, timesteps=None, skip_video=False):\n        if timesteps is None:\n            timesteps = input.shape[0]\n        x = super().forward(input)\n        if skip_video:\n            return x\n        x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n        x = self.time_mix_conv(x)\n        return rearrange(x, \"b c t h w -> (b t) c h w\")\n\n\nclass AttnVideoBlock(AttnBlock):\n    def __init__(\n        self, in_channels: int, alpha: float = 0, merge_strategy: str = \"learned\"\n    ):\n        super().__init__(in_channels)\n        # no context, single headed, as in base class\n        self.time_mix_block = BasicTransformerBlock(\n            dim=in_channels,\n            n_heads=1,\n            d_head=in_channels,\n            checkpoint=False,\n            ff_in=True,\n        )\n\n        time_embed_dim = self.in_channels * 4\n        self.video_time_embed = torch.nn.Sequential(\n            ops.Linear(self.in_channels, time_embed_dim),\n            torch.nn.SiLU(),\n            ops.Linear(time_embed_dim, self.in_channels),\n        )\n\n        self.merge_strategy = merge_strategy\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif self.merge_strategy == \"learned\":\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def forward(self, x, timesteps=None, skip_time_block=False):\n        if skip_time_block:\n            return super().forward(x)\n\n        if timesteps is None:\n            timesteps = x.shape[0]\n\n        x_in = x\n        x = self.attention(x)\n        h, w = x.shape[2:]\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n\n        x_mix = x\n        num_frames = torch.arange(timesteps, device=x.device)\n        num_frames = repeat(num_frames, \"t -> b t\", b=x.shape[0] // timesteps)\n        num_frames = rearrange(num_frames, \"b t -> (b t)\")\n        t_emb = timestep_embedding(num_frames, self.in_channels, repeat_only=False)\n        emb = self.video_time_embed(t_emb)  # b, n_channels\n        emb = emb[:, None, :]\n        x_mix = x_mix + emb\n\n        alpha = self.get_alpha().to(x.device)\n        x_mix = self.time_mix_block(x_mix, timesteps=timesteps)\n        x = alpha * x + (1.0 - alpha) * x_mix  # alpha merge\n\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n        x = self.proj_out(x)\n\n        return x_in + x\n\n    def get_alpha(\n        self,\n    ):\n        if self.merge_strategy == \"fixed\":\n            return self.mix_factor\n        elif self.merge_strategy == \"learned\":\n            return torch.sigmoid(self.mix_factor)\n        else:\n            raise NotImplementedError(f\"unknown merge strategy {self.merge_strategy}\")\n\n\n\ndef make_time_attn(\n    in_channels,\n    attn_type=\"vanilla\",\n    attn_kwargs=None,\n    alpha: float = 0,\n    merge_strategy: str = \"learned\",\n):\n    return partialclass(\n        AttnVideoBlock, in_channels, alpha=alpha, merge_strategy=merge_strategy\n    )\n\n\nclass Conv2DWrapper(torch.nn.Conv2d):\n    def forward(self, input: torch.Tensor, **kwargs) -> torch.Tensor:\n        return super().forward(input)\n\n\nclass VideoDecoder(Decoder):\n    available_time_modes = [\"all\", \"conv-only\", \"attn-only\"]\n\n    def __init__(\n        self,\n        *args,\n        video_kernel_size: Union[int, list] = 3,\n        alpha: float = 0.0,\n        merge_strategy: str = \"learned\",\n        time_mode: str = \"conv-only\",\n        **kwargs,\n    ):\n        self.video_kernel_size = video_kernel_size\n        self.alpha = alpha\n        self.merge_strategy = merge_strategy\n        self.time_mode = time_mode\n        assert (\n            self.time_mode in self.available_time_modes\n        ), f\"time_mode parameter has to be in {self.available_time_modes}\"\n\n        if self.time_mode != \"attn-only\":\n            kwargs[\"conv_out_op\"] = partialclass(AE3DConv, video_kernel_size=self.video_kernel_size)\n        if self.time_mode not in [\"conv-only\", \"only-last-conv\"]:\n            kwargs[\"attn_op\"] = partialclass(make_time_attn, alpha=self.alpha, merge_strategy=self.merge_strategy)\n        if self.time_mode not in [\"attn-only\", \"only-last-conv\"]:\n            kwargs[\"resnet_op\"] = partialclass(VideoResBlock, video_kernel_size=self.video_kernel_size, alpha=self.alpha, merge_strategy=self.merge_strategy)\n\n        super().__init__(*args, **kwargs)\n\n    def get_last_layer(self, skip_time_mix=False, **kwargs):\n        if self.time_mode == \"attn-only\":\n            raise NotImplementedError(\"TODO\")\n        else:\n            return (\n                self.conv_out.time_mix_conv.weight\n                if not skip_time_mix\n                else self.conv_out.weight\n            )\n", "ldm_patched/ldm/modules/ema.py": "import torch\nfrom torch import nn\n\n\nclass LitEma(nn.Module):\n    def __init__(self, model, decay=0.9999, use_num_upates=True):\n        super().__init__()\n        if decay < 0.0 or decay > 1.0:\n            raise ValueError('Decay must be between 0 and 1')\n\n        self.m_name2s_name = {}\n        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int) if use_num_upates\n        else torch.tensor(-1, dtype=torch.int))\n\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                # remove as '.'-character is not allowed in buffers\n                s_name = name.replace('.', '')\n                self.m_name2s_name.update({name: s_name})\n                self.register_buffer(s_name, p.clone().detach().data)\n\n        self.collected_params = []\n\n    def reset_num_updates(self):\n        del self.num_updates\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int))\n\n    def forward(self, model):\n        decay = self.decay\n\n        if self.num_updates >= 0:\n            self.num_updates += 1\n            decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n\n        one_minus_decay = 1.0 - decay\n\n        with torch.no_grad():\n            m_param = dict(model.named_parameters())\n            shadow_params = dict(self.named_buffers())\n\n            for key in m_param:\n                if m_param[key].requires_grad:\n                    sname = self.m_name2s_name[key]\n                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n                else:\n                    assert not key in self.m_name2s_name\n\n    def copy_to(self, model):\n        m_param = dict(model.named_parameters())\n        shadow_params = dict(self.named_buffers())\n        for key in m_param:\n            if m_param[key].requires_grad:\n                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n            else:\n                assert not key in self.m_name2s_name\n\n    def store(self, parameters):\n        \"\"\"\n        Save the current parameters for restoring later.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            temporarily stored.\n        \"\"\"\n        self.collected_params = [param.clone() for param in parameters]\n\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process. Store the parameters before the\n        `copy_to` method. After validation (or model saving), use this to\n        restore the former parameters.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            updated with the stored parameters.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)\n", "ldm_patched/ldm/modules/encoders/noise_aug_modules.py": "from ..diffusionmodules.upscaling import ImageConcatWithNoiseAugmentation\nfrom ..diffusionmodules.openaimodel import Timestep\nimport torch\n\nclass CLIPEmbeddingNoiseAugmentation(ImageConcatWithNoiseAugmentation):\n    def __init__(self, *args, clip_stats_path=None, timestep_dim=256, **kwargs):\n        super().__init__(*args, **kwargs)\n        if clip_stats_path is None:\n            clip_mean, clip_std = torch.zeros(timestep_dim), torch.ones(timestep_dim)\n        else:\n            clip_mean, clip_std = torch.load(clip_stats_path, map_location=\"cpu\")\n        self.register_buffer(\"data_mean\", clip_mean[None, :], persistent=False)\n        self.register_buffer(\"data_std\", clip_std[None, :], persistent=False)\n        self.time_embed = Timestep(timestep_dim)\n\n    def scale(self, x):\n        # re-normalize to centered mean and unit variance\n        x = (x - self.data_mean.to(x.device)) * 1. / self.data_std.to(x.device)\n        return x\n\n    def unscale(self, x):\n        # back to original data stats\n        x = (x * self.data_std.to(x.device)) + self.data_mean.to(x.device)\n        return x\n\n    def forward(self, x, noise_level=None, seed=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        x = self.scale(x)\n        z = self.q_sample(x, noise_level, seed=seed)\n        z = self.unscale(z)\n        noise_level = self.time_embed(noise_level)\n        return z, noise_level\n", "ldm_patched/ldm/modules/encoders/__init__.py": "", "ldm_patched/ldm/modules/diffusionmodules/model.py": "# pytorch_diffusion + derived encoder decoder\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import rearrange\nfrom typing import Optional, Any\n\nfrom ldm_patched.modules import model_management\nimport ldm_patched.modules.ops\nops = ldm_patched.modules.ops.disable_weight_init\n\nif model_management.xformers_enabled_vae():\n    import xformers\n    import xformers.ops\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, num_groups=32):\n    return ops.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        try:\n            x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        except: #operation not implemented for bf16\n            b, c, h, w = x.shape\n            out = torch.empty((b, c, h*2, w*2), dtype=x.dtype, layout=x.layout, device=x.device)\n            split = 8\n            l = out.shape[1] // split\n            for i in range(0, out.shape[1], l):\n                out[:,i:i+l] = torch.nn.functional.interpolate(x[:,i:i+l].to(torch.float32), scale_factor=2.0, mode=\"nearest\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.swish = torch.nn.SiLU(inplace=True)\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = ops.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = ops.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout, inplace=True)\n        self.conv2 = ops.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = ops.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = ops.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = self.swish(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(self.swish(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = self.swish(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h\n\ndef slice_attention(q, k, v):\n    r1 = torch.zeros_like(k, device=q.device)\n    scale = (int(q.shape[-1])**(-0.5))\n\n    mem_free_total = model_management.get_free_memory(q.device)\n\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n    modifier = 3 if q.element_size() == 2 else 2.5\n    mem_required = tensor_size * modifier\n    steps = 1\n\n    if mem_required > mem_free_total:\n        steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n\n    while True:\n        try:\n            slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n            for i in range(0, q.shape[1], slice_size):\n                end = i + slice_size\n                s1 = torch.bmm(q[:, i:end], k) * scale\n\n                s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n                del s1\n\n                r1[:, :, i:end] = torch.bmm(v, s2)\n                del s2\n            break\n        except model_management.OOM_EXCEPTION as e:\n            model_management.soft_empty_cache(True)\n            steps *= 2\n            if steps > 128:\n                raise e\n            print(\"out of memory error, increasing steps and trying again\", steps)\n\n    return r1\n\ndef normal_attention(q, k, v):\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n\ndef xformers_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\ndef pytorch_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, 1, C, -1).transpose(2, 3).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(2, 3).reshape(B, C, H, W)\n    except model_management.OOM_EXCEPTION as e:\n        print(\"scaled_dot_product_attention OOMed: switched to slice attention\")\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n        if model_management.xformers_enabled_vae():\n            print(\"Using xformers attention in VAE\")\n            self.optimized_attention = xformers_attention\n        elif model_management.pytorch_attention_enabled():\n            print(\"Using pytorch attention in VAE\")\n            self.optimized_attention = pytorch_attention\n        else:\n            print(\"Using split attention in VAE\")\n            self.optimized_attention = normal_attention\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        h_ = self.optimized_attention(q, k, v)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\ndef make_attn(in_channels, attn_type=\"vanilla\", attn_kwargs=None):\n    return AttnBlock(in_channels)\n\n\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        self.use_timestep = use_timestep\n        if self.use_timestep:\n            # timestep embedding\n            self.temb = nn.Module()\n            self.temb.dense = nn.ModuleList([\n                ops.Linear(self.ch,\n                                self.temb_ch),\n                ops.Linear(self.temb_ch,\n                                self.temb_ch),\n            ])\n\n        # downsampling\n        self.conv_in = ops.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            skip_in = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch*in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in+skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = ops.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x, t=None, context=None):\n        #assert x.shape[2] == x.shape[3] == self.resolution\n        if context is not None:\n            # assume aligned context, cat along channel axis\n            x = torch.cat((x, context), dim=1)\n        if self.use_timestep:\n            # timestep embedding\n            assert t is not None\n            temb = get_timestep_embedding(t, self.ch)\n            temb = self.temb.dense[0](temb)\n            temb = nonlinearity(temb)\n            temb = self.temb.dense[1](temb)\n        else:\n            temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n    def get_last_layer(self):\n        return self.conv_out.weight\n\n\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # downsampling\n        self.conv_in = ops.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.in_ch_mult = in_ch_mult\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = ops.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        # timestep embedding\n        temb = None\n        # downsampling\n        h = self.conv_in(x)\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n            if i_level != self.num_resolutions-1:\n                h = self.down[i_level].downsample(h)\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 conv_out_op=ops.Conv2d,\n                 resnet_op=ResnetBlock,\n                 attn_op=AttnBlock,\n                **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.tanh_out = tanh_out\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = ops.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = resnet_op(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = attn_op(block_in)\n        self.mid.block_2 = resnet_op(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(resnet_op(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(attn_op(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = conv_out_op(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z, **kwargs):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        h = self.mid.block_1(h, temb, **kwargs)\n        h = self.mid.attn_1(h, **kwargs)\n        h = self.mid.block_2(h, temb, **kwargs)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb, **kwargs)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h, **kwargs)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h, **kwargs)\n        if self.tanh_out:\n            h = torch.tanh(h)\n        return h\n", "ldm_patched/ldm/modules/diffusionmodules/util.py": "# adopted from\n# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n# and\n# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n# and\n# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n#\n# thanks!\n\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import repeat, rearrange\n\nfrom ldm_patched.ldm.util import instantiate_from_config\n\nclass AlphaBlender(nn.Module):\n    strategies = [\"learned\", \"fixed\", \"learned_with_images\"]\n\n    def __init__(\n        self,\n        alpha: float,\n        merge_strategy: str = \"learned_with_images\",\n        rearrange_pattern: str = \"b t -> (b t) 1 1\",\n    ):\n        super().__init__()\n        self.merge_strategy = merge_strategy\n        self.rearrange_pattern = rearrange_pattern\n\n        assert (\n            merge_strategy in self.strategies\n        ), f\"merge_strategy needs to be in {self.strategies}\"\n\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif (\n            self.merge_strategy == \"learned\"\n            or self.merge_strategy == \"learned_with_images\"\n        ):\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def get_alpha(self, image_only_indicator: torch.Tensor) -> torch.Tensor:\n        # skip_time_mix = rearrange(repeat(skip_time_mix, 'b -> (b t) () () ()', t=t), '(b t) 1 ... -> b 1 t ...', t=t)\n        if self.merge_strategy == \"fixed\":\n            # make shape compatible\n            # alpha = repeat(self.mix_factor, '1 -> b () t  () ()', t=t, b=bs)\n            alpha = self.mix_factor.to(image_only_indicator.device)\n        elif self.merge_strategy == \"learned\":\n            alpha = torch.sigmoid(self.mix_factor.to(image_only_indicator.device))\n            # make shape compatible\n            # alpha = repeat(alpha, '1 -> s () ()', s = t * bs)\n        elif self.merge_strategy == \"learned_with_images\":\n            assert image_only_indicator is not None, \"need image_only_indicator ...\"\n            alpha = torch.where(\n                image_only_indicator.bool(),\n                torch.ones(1, 1, device=image_only_indicator.device),\n                rearrange(torch.sigmoid(self.mix_factor.to(image_only_indicator.device)), \"... -> ... 1\"),\n            )\n            alpha = rearrange(alpha, self.rearrange_pattern)\n            # make shape compatible\n            # alpha = repeat(alpha, '1 -> s () ()', s = t * bs)\n        else:\n            raise NotImplementedError()\n        return alpha\n\n    def forward(\n        self,\n        x_spatial,\n        x_temporal,\n        image_only_indicator=None,\n    ) -> torch.Tensor:\n        alpha = self.get_alpha(image_only_indicator)\n        x = (\n            alpha.to(x_spatial.dtype) * x_spatial\n            + (1.0 - alpha).to(x_spatial.dtype) * x_temporal\n        )\n        return x\n\n\ndef make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    if schedule == \"linear\":\n        betas = (\n                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n        )\n\n    elif schedule == \"cosine\":\n        timesteps = (\n                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n        alphas = torch.cos(alphas).pow(2)\n        alphas = alphas / alphas[0]\n        betas = 1 - alphas[1:] / alphas[:-1]\n        betas = np.clip(betas, a_min=0, a_max=0.999)\n\n    elif schedule == \"squaredcos_cap_v2\":  # used for karlo prior\n        # return early\n        return betas_for_alpha_bar(\n            n_timestep,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n\n    elif schedule == \"sqrt_linear\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n    elif schedule == \"sqrt\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n    else:\n        raise ValueError(f\"schedule '{schedule}' unknown.\")\n    return betas.numpy()\n\n\ndef make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n    if ddim_discr_method == 'uniform':\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == 'quad':\n        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n    else:\n        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n\n    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n    steps_out = ddim_timesteps + 1\n    if verbose:\n        print(f'Selected timesteps for ddim sampler: {steps_out}')\n    return steps_out\n\n\ndef make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n    if verbose:\n        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n        print(f'For the chosen value of eta, which is {eta}, '\n              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n    return sigmas, alphas, alphas_prev\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\ndef extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        ctx.gpu_autocast_kwargs = {\"enabled\": torch.is_autocast_enabled(),\n                                   \"dtype\": torch.get_autocast_gpu_dtype(),\n                                   \"cache_enabled\": torch.is_autocast_cache_enabled()}\n        with torch.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with torch.enable_grad(), \\\n                torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs):\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = torch.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32, device=timesteps.device) / half\n        )\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = repeat(timesteps, 'b -> b d', d=dim)\n    return embedding\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass HybridConditioner(nn.Module):\n\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n\n\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n", "ldm_patched/ldm/modules/diffusionmodules/upscaling.py": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom functools import partial\n\nfrom .util import extract_into_tensor, make_beta_schedule\nfrom ldm_patched.ldm.util import default\n\n\nclass AbstractLowScaleModel(nn.Module):\n    # for concatenating a downsampled image to the latent representation\n    def __init__(self, noise_schedule_config=None):\n        super(AbstractLowScaleModel, self).__init__()\n        if noise_schedule_config is not None:\n            self.register_schedule(**noise_schedule_config)\n\n    def register_schedule(self, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                   cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n    def q_sample(self, x_start, t, noise=None, seed=None):\n        if noise is None:\n            if seed is None:\n                noise = torch.randn_like(x_start)\n            else:\n                noise = torch.randn(x_start.size(), dtype=x_start.dtype, layout=x_start.layout, generator=torch.manual_seed(seed)).to(x_start.device)\n        return (extract_into_tensor(self.sqrt_alphas_cumprod.to(x_start.device), t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod.to(x_start.device), t, x_start.shape) * noise)\n\n    def forward(self, x):\n        return x, None\n\n    def decode(self, x):\n        return x\n\n\nclass SimpleImageConcat(AbstractLowScaleModel):\n    # no noise level conditioning\n    def __init__(self):\n        super(SimpleImageConcat, self).__init__(noise_schedule_config=None)\n        self.max_noise_level = 0\n\n    def forward(self, x):\n        # fix to constant noise level\n        return x, torch.zeros(x.shape[0], device=x.device).long()\n\n\nclass ImageConcatWithNoiseAugmentation(AbstractLowScaleModel):\n    def __init__(self, noise_schedule_config, max_noise_level=1000, to_cuda=False):\n        super().__init__(noise_schedule_config=noise_schedule_config)\n        self.max_noise_level = max_noise_level\n\n    def forward(self, x, noise_level=None, seed=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        z = self.q_sample(x, noise_level, seed=seed)\n        return z, noise_level\n\n\n\n", "ldm_patched/ldm/modules/diffusionmodules/__init__.py": "", "ldm_patched/ldm/modules/diffusionmodules/openaimodel.py": "from abc import abstractmethod\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nfrom .util import (\n    checkpoint,\n    avg_pool_nd,\n    zero_module,\n    timestep_embedding,\n    AlphaBlender,\n)\nfrom ..attention import SpatialTransformer, SpatialVideoTransformer, default\nfrom ldm_patched.ldm.util import exists\nimport ldm_patched.modules.ops\nops = ldm_patched.modules.ops.disable_weight_init\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n#This is needed because accelerate makes a copy of transformer_options which breaks \"transformer_index\"\ndef forward_timestep_embed(ts, x, emb, context=None, transformer_options={}, output_shape=None, time_context=None, num_video_frames=None, image_only_indicator=None):\n    for layer in ts:\n        if isinstance(layer, VideoResBlock):\n            x = layer(x, emb, num_video_frames, image_only_indicator)\n        elif isinstance(layer, TimestepBlock):\n            x = layer(x, emb)\n        elif isinstance(layer, SpatialVideoTransformer):\n            x = layer(x, context, time_context, num_video_frames, image_only_indicator, transformer_options)\n            if \"transformer_index\" in transformer_options:\n                transformer_options[\"transformer_index\"] += 1\n        elif isinstance(layer, SpatialTransformer):\n            x = layer(x, context, transformer_options)\n            if \"transformer_index\" in transformer_options:\n                transformer_options[\"transformer_index\"] += 1\n        elif isinstance(layer, Upsample):\n            x = layer(x, output_shape=output_shape)\n        else:\n            x = layer(x)\n    return x\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, *args, **kwargs):\n        return forward_timestep_embed(self, *args, **kwargs)\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = operations.conv_nd(dims, self.channels, self.out_channels, 3, padding=padding, dtype=dtype, device=device)\n\n    def forward(self, x, output_shape=None):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            shape = [x.shape[2], x.shape[3] * 2, x.shape[4] * 2]\n            if output_shape is not None:\n                shape[1] = output_shape[3]\n                shape[2] = output_shape[4]\n        else:\n            shape = [x.shape[2] * 2, x.shape[3] * 2]\n            if output_shape is not None:\n                shape[0] = output_shape[2]\n                shape[1] = output_shape[3]\n\n        x = F.interpolate(x, size=shape, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = operations.conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n        kernel_size=3,\n        exchange_temb_dims=False,\n        skip_t_emb=False,\n        dtype=None,\n        device=None,\n        operations=ops\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.exchange_temb_dims = exchange_temb_dims\n\n        if isinstance(kernel_size, list):\n            padding = [k // 2 for k in kernel_size]\n        else:\n            padding = kernel_size // 2\n\n        self.in_layers = nn.Sequential(\n            operations.GroupNorm(32, channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = rearrange(emb_out, \"b t c ... -> b c t ...\")\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass VideoResBlock(ResBlock):\n    def __init__(\n        self,\n        channels: int,\n        emb_channels: int,\n        dropout: float,\n        video_kernel_size=3,\n        merge_strategy: str = \"fixed\",\n        merge_factor: float = 0.5,\n        out_channels=None,\n        use_conv: bool = False,\n        use_scale_shift_norm: bool = False,\n        dims: int = 2,\n        use_checkpoint: bool = False,\n        up: bool = False,\n        down: bool = False,\n        dtype=None,\n        device=None,\n        operations=ops\n    ):\n        super().__init__(\n            channels,\n            emb_channels,\n            dropout,\n            out_channels=out_channels,\n            use_conv=use_conv,\n            use_scale_shift_norm=use_scale_shift_norm,\n            dims=dims,\n            use_checkpoint=use_checkpoint,\n            up=up,\n            down=down,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n\n        self.time_stack = ResBlock(\n            default(out_channels, channels),\n            emb_channels,\n            dropout=dropout,\n            dims=3,\n            out_channels=default(out_channels, channels),\n            use_scale_shift_norm=False,\n            use_conv=False,\n            up=False,\n            down=False,\n            kernel_size=video_kernel_size,\n            use_checkpoint=use_checkpoint,\n            exchange_temb_dims=True,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n        self.time_mixer = AlphaBlender(\n            alpha=merge_factor,\n            merge_strategy=merge_strategy,\n            rearrange_pattern=\"b t -> b 1 t 1 1\",\n        )\n\n    def forward(\n        self,\n        x: th.Tensor,\n        emb: th.Tensor,\n        num_video_frames: int,\n        image_only_indicator = None,\n    ) -> th.Tensor:\n        x = super().forward(x, emb)\n\n        x_mix = rearrange(x, \"(b t) c h w -> b c t h w\", t=num_video_frames)\n        x = rearrange(x, \"(b t) c h w -> b c t h w\", t=num_video_frames)\n\n        x = self.time_stack(\n            x, rearrange(emb, \"(b t) ... -> b t ...\", t=num_video_frames)\n        )\n        x = self.time_mixer(\n            x_spatial=x_mix, x_temporal=x, image_only_indicator=image_only_indicator\n        )\n        x = rearrange(x, \"b c t h w -> (b t) c h w\")\n        return x\n\n\nclass Timestep(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        return timestep_embedding(t, self.dim)\n\ndef apply_control(h, control, name):\n    if control is not None and name in control and len(control[name]) > 0:\n        ctrl = control[name].pop()\n        if ctrl is not None:\n            try:\n                h += ctrl\n            except:\n                print(\"warning control could not be applied\", h.shape, ctrl.shape)\n    return h\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        dtype=th.float32,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        adm_in_channels=None,\n        transformer_depth_middle=None,\n        transformer_depth_output=None,\n        use_temporal_resblock=False,\n        use_temporal_attention=False,\n        time_context_dim=None,\n        extra_ff_mix_layer=False,\n        use_spatial_context=False,\n        merge_strategy=None,\n        merge_factor=0.0,\n        video_kernel_size=None,\n        disable_temporal_crossattention=False,\n        max_ddpm_temb_period=10000,\n        device=None,\n        operations=ops,\n    ):\n        super().__init__()\n\n        if context_dim is not None:\n            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n            # from omegaconf.listconfig import ListConfig\n            # if type(context_dim) == ListConfig:\n            #     context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n            self.num_res_blocks = num_res_blocks\n\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n\n        transformer_depth = transformer_depth[:]\n        transformer_depth_output = transformer_depth_output[:]\n\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = dtype\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.use_temporal_resblocks = use_temporal_resblock\n        self.predict_codebook_ids = n_embed is not None\n\n        self.default_num_video_frames = None\n        self.default_image_only_indicator = None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            operations.Linear(model_channels, time_embed_dim, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n        )\n\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim, dtype=self.dtype, device=device)\n            elif self.num_classes == \"continuous\":\n                print(\"setting up linear c_adm embedding layer\")\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            elif self.num_classes == \"sequential\":\n                assert adm_in_channels is not None\n                self.label_emb = nn.Sequential(\n                    nn.Sequential(\n                        operations.Linear(adm_in_channels, time_embed_dim, dtype=self.dtype, device=device),\n                        nn.SiLU(),\n                        operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n                    )\n                )\n            else:\n                raise ValueError()\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    operations.conv_nd(dims, in_channels, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n\n        def get_attention_layer(\n            ch,\n            num_heads,\n            dim_head,\n            depth=1,\n            context_dim=None,\n            use_checkpoint=False,\n            disable_self_attn=False,\n        ):\n            if use_temporal_attention:\n                return SpatialVideoTransformer(\n                    ch,\n                    num_heads,\n                    dim_head,\n                    depth=depth,\n                    context_dim=context_dim,\n                    time_context_dim=time_context_dim,\n                    dropout=dropout,\n                    ff_in=extra_ff_mix_layer,\n                    use_spatial_context=use_spatial_context,\n                    merge_strategy=merge_strategy,\n                    merge_factor=merge_factor,\n                    checkpoint=use_checkpoint,\n                    use_linear=use_linear_in_transformer,\n                    disable_self_attn=disable_self_attn,\n                    disable_temporal_crossattention=disable_temporal_crossattention,\n                    max_time_embed_period=max_ddpm_temb_period,\n                    dtype=self.dtype, device=device, operations=operations\n                )\n            else:\n                return SpatialTransformer(\n                                ch, num_heads, dim_head, depth=depth, context_dim=context_dim,\n                                disable_self_attn=disable_self_attn, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint, dtype=self.dtype, device=device, operations=operations\n                            )\n\n        def get_resblock(\n            merge_factor,\n            merge_strategy,\n            video_kernel_size,\n            ch,\n            time_embed_dim,\n            dropout,\n            out_channels,\n            dims,\n            use_checkpoint,\n            use_scale_shift_norm,\n            down=False,\n            up=False,\n            dtype=None,\n            device=None,\n            operations=ops\n        ):\n            if self.use_temporal_resblocks:\n                return VideoResBlock(\n                    merge_factor=merge_factor,\n                    merge_strategy=merge_strategy,\n                    video_kernel_size=video_kernel_size,\n                    channels=ch,\n                    emb_channels=time_embed_dim,\n                    dropout=dropout,\n                    out_channels=out_channels,\n                    dims=dims,\n                    use_checkpoint=use_checkpoint,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                    down=down,\n                    up=up,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n            else:\n                return ResBlock(\n                    channels=ch,\n                    emb_channels=time_embed_dim,\n                    dropout=dropout,\n                    out_channels=out_channels,\n                    use_checkpoint=use_checkpoint,\n                    dims=dims,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                    down=down,\n                    up=up,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    get_resblock(\n                        merge_factor=merge_factor,\n                        merge_strategy=merge_strategy,\n                        video_kernel_size=video_kernel_size,\n                        ch=ch,\n                        time_embed_dim=time_embed_dim,\n                        dropout=dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations,\n                    )\n                ]\n                ch = mult * model_channels\n                num_transformers = transformer_depth.pop(0)\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or nr < num_attention_blocks[level]:\n                        layers.append(get_attention_layer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_checkpoint=use_checkpoint)\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        get_resblock(\n                            merge_factor=merge_factor,\n                            merge_strategy=merge_strategy,\n                            video_kernel_size=video_kernel_size,\n                            ch=ch,\n                            time_embed_dim=time_embed_dim,\n                            dropout=dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        mid_block = [\n            get_resblock(\n                merge_factor=merge_factor,\n                merge_strategy=merge_strategy,\n                video_kernel_size=video_kernel_size,\n                ch=ch,\n                time_embed_dim=time_embed_dim,\n                dropout=dropout,\n                out_channels=None,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        if transformer_depth_middle >= 0:\n            mid_block += [get_attention_layer(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth_middle, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_checkpoint=use_checkpoint\n                        ),\n            get_resblock(\n                merge_factor=merge_factor,\n                merge_strategy=merge_strategy,\n                video_kernel_size=video_kernel_size,\n                ch=ch,\n                time_embed_dim=time_embed_dim,\n                dropout=dropout,\n                out_channels=None,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        self.middle_block = TimestepEmbedSequential(*mid_block)\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(self.num_res_blocks[level] + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    get_resblock(\n                        merge_factor=merge_factor,\n                        merge_strategy=merge_strategy,\n                        video_kernel_size=video_kernel_size,\n                        ch=ch + ich,\n                        time_embed_dim=time_embed_dim,\n                        dropout=dropout,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations\n                    )\n                ]\n                ch = model_channels * mult\n                num_transformers = transformer_depth_output.pop()\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or i < num_attention_blocks[level]:\n                        layers.append(\n                            get_attention_layer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_checkpoint=use_checkpoint\n                            )\n                        )\n                if level and i == self.num_res_blocks[level]:\n                    out_ch = ch\n                    layers.append(\n                        get_resblock(\n                            merge_factor=merge_factor,\n                            merge_strategy=merge_strategy,\n                            video_kernel_size=video_kernel_size,\n                            ch=ch,\n                            time_embed_dim=time_embed_dim,\n                            dropout=dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            operations.GroupNorm(32, ch, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            zero_module(operations.conv_nd(dims, model_channels, out_channels, 3, padding=1, dtype=self.dtype, device=device)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n            operations.GroupNorm(32, ch, dtype=self.dtype, device=device),\n            operations.conv_nd(dims, model_channels, n_embed, 1, dtype=self.dtype, device=device),\n            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n        )\n\n    def forward(self, x, timesteps=None, context=None, y=None, control=None, transformer_options={}, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        transformer_options[\"original_shape\"] = list(x.shape)\n        transformer_options[\"transformer_index\"] = 0\n        transformer_patches = transformer_options.get(\"patches\", {})\n\n        num_video_frames = kwargs.get(\"num_video_frames\", self.default_num_video_frames)\n        image_only_indicator = kwargs.get(\"image_only_indicator\", self.default_image_only_indicator)\n        time_context = kwargs.get(\"time_context\", None)\n\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n        hs = []\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n        emb = self.time_embed(t_emb)\n\n        if self.num_classes is not None:\n            assert y.shape[0] == x.shape[0]\n            emb = emb + self.label_emb(y)\n\n        h = x\n        for id, module in enumerate(self.input_blocks):\n            transformer_options[\"block\"] = (\"input\", id)\n            h = forward_timestep_embed(module, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n            h = apply_control(h, control, 'input')\n            if \"input_block_patch\" in transformer_patches:\n                patch = transformer_patches[\"input_block_patch\"]\n                for p in patch:\n                    h = p(h, transformer_options)\n\n            hs.append(h)\n            if \"input_block_patch_after_skip\" in transformer_patches:\n                patch = transformer_patches[\"input_block_patch_after_skip\"]\n                for p in patch:\n                    h = p(h, transformer_options)\n\n        transformer_options[\"block\"] = (\"middle\", 0)\n        h = forward_timestep_embed(self.middle_block, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n        h = apply_control(h, control, 'middle')\n\n\n        for id, module in enumerate(self.output_blocks):\n            transformer_options[\"block\"] = (\"output\", id)\n            hsp = hs.pop()\n            hsp = apply_control(hsp, control, 'output')\n\n            if \"output_block_patch\" in transformer_patches:\n                patch = transformer_patches[\"output_block_patch\"]\n                for p in patch:\n                    h, hsp = p(h, hsp, transformer_options)\n\n            h = th.cat([h, hsp], dim=1)\n            del hsp\n            if len(hs) > 0:\n                output_shape = hs[-1].shape\n            else:\n                output_shape = None\n            h = forward_timestep_embed(module, h, emb, context, transformer_options, output_shape, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n        h = h.type(x.dtype)\n        if self.predict_codebook_ids:\n            return self.id_predictor(h)\n        else:\n            return self.out(h)\n", "ldm_patched/ldm/modules/distributions/distributions.py": "import torch\nimport numpy as np\n\n\nclass AbstractDistribution:\n    def sample(self):\n        raise NotImplementedError()\n\n    def mode(self):\n        raise NotImplementedError()\n\n\nclass DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n\n    def sample(self):\n        return self.value\n\n    def mode(self):\n        return self.value\n\n\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n\n    def sample(self):\n        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n        return x\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n                                       + self.var - 1.0 - self.logvar,\n                                       dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n                    dim=[1, 2, 3])\n\n    def nll(self, sample, dims=[1,2,3]):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(\n            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n            dim=dims)\n\n    def mode(self):\n        return self.mean\n\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + torch.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n", "ldm_patched/ldm/modules/distributions/__init__.py": "", "ldm_patched/unipc/uni_pc.py": "#code taken from: https://github.com/wl-zhao/UniPC and modified\n\nimport torch\nimport torch.nn.functional as F\nimport math\n\nfrom tqdm.auto import trange, tqdm\n\n\nclass NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n        ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n\n            t = self.inverse_lambda(lambda_t)\n\n        ===============================================================\n\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n\n        1. For discrete-time DPMs:\n\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n        \n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \"\"\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == 'linear':\n            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n\ndef model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.,\n    classifier_fn=None,\n    classifier_kwargs={},\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n\n    We support four types of the diffusion model by setting `model_type`:\n\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n    \n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            `` \n\n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            `` \n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n        \n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)         \n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \"classifier-free\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn\n\n\nclass UniPC:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        predict_x0=True,\n        thresholding=False,\n        max_val=1.,\n        variant='bh1',\n        noise_mask=None,\n        masked_image=None,\n        noise=None,\n    ):\n        \"\"\"Construct a UniPC. \n\n        We support both data_prediction and noise_prediction.\n        \"\"\"\n        self.model = model_fn\n        self.noise_schedule = noise_schedule\n        self.variant = variant\n        self.predict_x0 = predict_x0\n        self.thresholding = thresholding\n        self.max_val = max_val\n        self.noise_mask = noise_mask\n        self.masked_image = masked_image\n        self.noise = noise\n\n    def dynamic_thresholding_fn(self, x0, t=None):\n        \"\"\"\n        The dynamic thresholding method. \n        \"\"\"\n        dims = x0.dim()\n        p = self.dynamic_thresholding_ratio\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n        x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        if self.noise_mask is not None:\n            return self.model(x, t) * self.noise_mask\n        else:\n            return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with thresholding).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n        if self.thresholding:\n            p = 0.995   # A hyperparameter in the paper of \"Imagen\" [1].\n            s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n            s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n            x0 = torch.clamp(x0, -s, s) / s\n        if self.noise_mask is not None:\n            x0 = x0 * self.noise_mask + (1. - self.noise_mask) * self.masked_image\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model. \n        \"\"\"\n        if self.predict_x0:\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n        \"\"\"\n        if skip_type == 'logSNR':\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == 'time_uniform':\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == 'time_quadratic':\n            t_order = 2\n            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(\"Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\".format(skip_type))\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [3,] * (K - 2) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [3,] * (K - 1) + [1]\n            else:\n                orders = [3,] * (K - 1) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [2,] * K\n            else:\n                K = steps // 2 + 1\n                orders = [2,] * (K - 1) + [1]\n        elif order == 1:\n            K = steps\n            orders = [1,] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == 'logSNR':\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization. \n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def multistep_uni_pc_update(self, x, model_prev_list, t_prev_list, t, order, **kwargs):\n        if len(t.shape) == 0:\n            t = t.view(-1)\n        if 'bh' in self.variant:\n            return self.multistep_uni_pc_bh_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n        else:\n            assert self.variant == 'vary_coeff'\n            return self.multistep_uni_pc_vary_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n\n    def multistep_uni_pc_vary_update(self, x, model_prev_list, t_prev_list, t, order, use_corrector=True):\n        print(f'using unified predictor-corrector with order {order} (solver type: vary coeff)')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_t = ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = (lambda_prev_i - lambda_prev_0) / h\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        K = len(rks)\n        # build C matrix\n        C = []\n\n        col = torch.ones_like(rks)\n        for k in range(1, K + 1):\n            C.append(col)\n            col = col * rks / (k + 1) \n        C = torch.stack(C, dim=1)\n\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            C_inv_p = torch.linalg.inv(C[:-1, :-1])\n            A_p = C_inv_p\n\n        if use_corrector:\n            print('using corrector')\n            C_inv = torch.linalg.inv(C)\n            A_c = C_inv\n\n        hh = -h if self.predict_x0 else h\n        h_phi_1 = torch.expm1(hh)\n        h_phi_ks = []\n        factorial_k = 1\n        h_phi_k = h_phi_1\n        for k in range(1, K + 2):\n            h_phi_ks.append(h_phi_k)\n            h_phi_k = h_phi_k / hh - 1 / factorial_k\n            factorial_k *= (k + 1)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                sigma_t / sigma_prev_0 * x\n                - alpha_t * h_phi_1 * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - alpha_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        else:\n            log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n            x_t_ = (\n                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                - (sigma_t * h_phi_1) * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - sigma_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        return x_t, model_t\n\n    def multistep_uni_pc_bh_update(self, x, model_prev_list, t_prev_list, t, order, x_t=None, use_corrector=True):\n        # print(f'using unified predictor-corrector with order {order} (solver type: B(h))')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n        dims = x.dim()\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = ((lambda_prev_i - lambda_prev_0) / h)[0]\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        R = []\n        b = []\n\n        hh = -h[0] if self.predict_x0 else h[0]\n        h_phi_1 = torch.expm1(hh) # h\\phi_1(h) = e^h - 1\n        h_phi_k = h_phi_1 / hh - 1\n\n        factorial_i = 1\n\n        if self.variant == 'bh1':\n            B_h = hh\n        elif self.variant == 'bh2':\n            B_h = torch.expm1(hh)\n        else:\n            raise NotImplementedError()\n            \n        for i in range(1, order + 1):\n            R.append(torch.pow(rks, i - 1))\n            b.append(h_phi_k * factorial_i / B_h)\n            factorial_i *= (i + 1)\n            h_phi_k = h_phi_k / hh - 1 / factorial_i \n\n        R = torch.stack(R)\n        b = torch.tensor(b, device=x.device)\n\n        # now predictor\n        use_predictor = len(D1s) > 0 and x_t is None\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            if x_t is None:\n                # for order 2, we use a simplified version\n                if order == 2:\n                    rhos_p = torch.tensor([0.5], device=b.device)\n                else:\n                    rhos_p = torch.linalg.solve(R[:-1, :-1], b[:-1])\n        else:\n            D1s = None\n\n        if use_corrector:\n            # print('using corrector')\n            # for order 1, we use a simplified version\n            if order == 1:\n                rhos_c = torch.tensor([0.5], device=b.device)\n            else:\n                rhos_c = torch.linalg.solve(R, b)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                expand_dims(sigma_t / sigma_prev_0, dims) * x\n                - expand_dims(alpha_t * h_phi_1, dims)* model_prev_0\n            )\n\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        else:\n            x_t_ = (\n                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                - expand_dims(sigma_t * h_phi_1, dims) * model_prev_0\n            )\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        return x_t, model_t\n\n\n    def sample(self, x, timesteps, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n        method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n        atol=0.0078, rtol=0.05, corrector=False, callback=None, disable_pbar=False\n    ):\n        # t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n        # t_T = self.noise_schedule.T if t_start is None else t_start\n        device = x.device\n        steps = len(timesteps) - 1\n        if method == 'multistep':\n            assert steps >= order\n            # timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n            assert timesteps.shape[0] - 1 == steps\n            # with torch.no_grad():\n            for step_index in trange(steps, disable=disable_pbar):\n                if self.noise_mask is not None:\n                    x = x * self.noise_mask + (1. - self.noise_mask) * (self.masked_image * self.noise_schedule.marginal_alpha(timesteps[step_index]) + self.noise * self.noise_schedule.marginal_std(timesteps[step_index]))\n                if step_index == 0:\n                    vec_t = timesteps[0].expand((x.shape[0]))\n                    model_prev_list = [self.model_fn(x, vec_t)]\n                    t_prev_list = [vec_t]\n                elif step_index < order:\n                    init_order = step_index\n                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback(step_index, model_prev_list[-1], x, steps)\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n\n#############################################################\n# other utility functions\n#############################################################\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand\n\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]\n\n\nclass SigmaConvert:\n    schedule = \"\"\n    def marginal_log_mean_coeff(self, sigma):\n        return 0.5 * torch.log(1 / ((sigma * sigma) + 1))\n\n    def marginal_alpha(self, t):\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\ndef predict_eps_sigma(model, input, sigma_in, **kwargs):\n    sigma = sigma_in.view(sigma_in.shape[:1] + (1,) * (input.ndim - 1))\n    input = input * ((sigma ** 2 + 1.0) ** 0.5)\n    return  (input - model(input, sigma_in, **kwargs)) / sigma\n\n\ndef sample_unipc(model, noise, image, sigmas, max_denoise, extra_args=None, callback=None, disable=False, noise_mask=None, variant='bh1'):\n        timesteps = sigmas.clone()\n        if sigmas[-1] == 0:\n            timesteps = sigmas[:]\n            timesteps[-1] = 0.001\n        else:\n            timesteps = sigmas.clone()\n        ns = SigmaConvert()\n\n        if image is not None:\n            img = image * ns.marginal_alpha(timesteps[0])\n            if max_denoise:\n                noise_mult = 1.0\n            else:\n                noise_mult = ns.marginal_std(timesteps[0])\n            img += noise * noise_mult\n        else:\n            img = noise\n\n        model_type = \"noise\"\n\n        model_fn = model_wrapper(\n            lambda input, sigma, **kwargs: predict_eps_sigma(model, input, sigma, **kwargs),\n            ns,\n            model_type=model_type,\n            guidance_type=\"uncond\",\n            model_kwargs=extra_args,\n        )\n\n        order = min(3, len(timesteps) - 2)\n        uni_pc = UniPC(model_fn, ns, predict_x0=True, thresholding=False, noise_mask=noise_mask, masked_image=image, noise=noise, variant=variant)\n        x = uni_pc.sample(img, timesteps=timesteps, skip_type=\"time_uniform\", method=\"multistep\", order=order, lower_order_final=True, callback=callback, disable_pbar=disable)\n        x /= ns.marginal_alpha(timesteps[-1])\n        return x\n", "ldm_patched/k_diffusion/sampling.py": "import math\n\nfrom scipy import integrate\nimport torch\nfrom torch import nn\nimport torchsde\nfrom tqdm.auto import trange, tqdm\n\nfrom . import utils\n\n\ndef append_zero(x):\n    return torch.cat([x, x.new_zeros([1])])\n\n\ndef get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n    ramp = torch.linspace(0, 1, n, device=device)\n    min_inv_rho = sigma_min ** (1 / rho)\n    max_inv_rho = sigma_max ** (1 / rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n    return append_zero(sigmas).to(device)\n\n\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n    return append_zero(sigmas)\n\n\ndef get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n    t = torch.linspace(1, eps_s, n, device=device)\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n    return append_zero(sigmas)\n\n\ndef to_d(x, sigma, denoised):\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\n\n\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n    if not eta:\n        return sigma_to, 0.\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n    return sigma_down, sigma_up\n\n\ndef default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)\n\n\nclass BatchedBrownianTree:\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\n\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\n        self.cpu_tree = True\n        if \"cpu\" in kwargs:\n            self.cpu_tree = kwargs.pop(\"cpu\")\n        t0, t1, self.sign = self.sort(t0, t1)\n        w0 = kwargs.get('w0', torch.zeros_like(x))\n        if seed is None:\n            seed = torch.randint(0, 2 ** 63 - 1, []).item()\n        self.batched = True\n        try:\n            assert len(seed) == x.shape[0]\n            w0 = w0[0]\n        except TypeError:\n            seed = [seed]\n            self.batched = False\n        if self.cpu_tree:\n            self.trees = [torchsde.BrownianTree(t0.cpu(), w0.cpu(), t1.cpu(), entropy=s, **kwargs) for s in seed]\n        else:\n            self.trees = [torchsde.BrownianTree(t0, w0, t1, entropy=s, **kwargs) for s in seed]\n\n    @staticmethod\n    def sort(a, b):\n        return (a, b, 1) if a < b else (b, a, -1)\n\n    def __call__(self, t0, t1):\n        t0, t1, sign = self.sort(t0, t1)\n        if self.cpu_tree:\n            w = torch.stack([tree(t0.cpu().float(), t1.cpu().float()).to(t0.dtype).to(t0.device) for tree in self.trees]) * (self.sign * sign)\n        else:\n            w = torch.stack([tree(t0, t1) for tree in self.trees]) * (self.sign * sign)\n\n        return w if self.batched else w[0]\n\n\nclass BrownianTreeNoiseSampler:\n    \"\"\"A noise sampler backed by a torchsde.BrownianTree.\n\n    Args:\n        x (Tensor): The tensor whose shape, device and dtype to use to generate\n            random samples.\n        sigma_min (float): The low end of the valid interval.\n        sigma_max (float): The high end of the valid interval.\n        seed (int or List[int]): The random seed. If a list of seeds is\n            supplied instead of a single integer, then the noise sampler will\n            use one BrownianTree per batch item, each with its own seed.\n        transform (callable): A function that maps sigma to the sampler's\n            internal timestep.\n    \"\"\"\n\n    def __init__(self, x, sigma_min, sigma_max, seed=None, transform=lambda x: x, cpu=False):\n        self.transform = transform\n        t0, t1 = self.transform(torch.as_tensor(sigma_min)), self.transform(torch.as_tensor(sigma_max))\n        self.tree = BatchedBrownianTree(x, t0, t1, seed, cpu=cpu)\n\n    def __call__(self, sigma, sigma_next):\n        t0, t1 = self.transform(torch.as_tensor(sigma)), self.transform(torch.as_tensor(sigma_next))\n        return self.tree(t0, t1) / (t1 - t0).abs().sqrt()\n\n\n@torch.no_grad()\ndef sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + d * dt\n    return x\n\n\n@torch.no_grad()\ndef sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\n@torch.no_grad()\ndef sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        if sigmas[i + 1] == 0:\n            # Euler method\n            x = x + d * dt\n        else:\n            # Heun's method\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n            d_prime = (d + d_2) / 2\n            x = x + d_prime * dt\n    return x\n\n\n@torch.no_grad()\ndef sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Euler method\n            dt = sigmas[i + 1] - sigma_hat\n            x = x + d * dt\n        else:\n            # DPM-Solver-2\n            sigma_mid = sigma_hat.log().lerp(sigmas[i + 1].log(), 0.5).exp()\n            dt_1 = sigma_mid - sigma_hat\n            dt_2 = sigmas[i + 1] - sigma_hat\n            x_2 = x + d * dt_1\n            denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n    return x\n\n\n@torch.no_grad()\ndef sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        if sigma_down == 0:\n            # Euler method\n            dt = sigma_down - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver-2\n            sigma_mid = sigmas[i].log().lerp(sigma_down.log(), 0.5).exp()\n            dt_1 = sigma_mid - sigmas[i]\n            dt_2 = sigma_down - sigmas[i]\n            x_2 = x + d * dt_1\n            denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\ndef linear_multistep_coeff(order, t, i, j):\n    if order - 1 > i:\n        raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k:\n                continue\n            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n        return prod\n    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]\n\n\n@torch.no_grad()\ndef sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigmas_cpu = sigmas.detach().cpu().numpy()\n    ds = []\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        ds.append(d)\n        if len(ds) > order:\n            ds.pop(0)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        cur_order = min(i + 1, order)\n        coeffs = [linear_multistep_coeff(cur_order, sigmas_cpu, i, j) for j in range(cur_order)]\n        x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n    return x\n\n\nclass PIDStepSizeController:\n    \"\"\"A PID controller for ODE adaptive step size control.\"\"\"\n    def __init__(self, h, pcoeff, icoeff, dcoeff, order=1, accept_safety=0.81, eps=1e-8):\n        self.h = h\n        self.b1 = (pcoeff + icoeff + dcoeff) / order\n        self.b2 = -(pcoeff + 2 * dcoeff) / order\n        self.b3 = dcoeff / order\n        self.accept_safety = accept_safety\n        self.eps = eps\n        self.errs = []\n\n    def limiter(self, x):\n        return 1 + math.atan(x - 1)\n\n    def propose_step(self, error):\n        inv_error = 1 / (float(error) + self.eps)\n        if not self.errs:\n            self.errs = [inv_error, inv_error, inv_error]\n        self.errs[0] = inv_error\n        factor = self.errs[0] ** self.b1 * self.errs[1] ** self.b2 * self.errs[2] ** self.b3\n        factor = self.limiter(factor)\n        accept = factor >= self.accept_safety\n        if accept:\n            self.errs[2] = self.errs[1]\n            self.errs[1] = self.errs[0]\n        self.h *= factor\n        return accept\n\n\nclass DPMSolver(nn.Module):\n    \"\"\"DPM-Solver. See https://arxiv.org/abs/2206.00927.\"\"\"\n\n    def __init__(self, model, extra_args=None, eps_callback=None, info_callback=None):\n        super().__init__()\n        self.model = model\n        self.extra_args = {} if extra_args is None else extra_args\n        self.eps_callback = eps_callback\n        self.info_callback = info_callback\n\n    def t(self, sigma):\n        return -sigma.log()\n\n    def sigma(self, t):\n        return t.neg().exp()\n\n    def eps(self, eps_cache, key, x, t, *args, **kwargs):\n        if key in eps_cache:\n            return eps_cache[key], eps_cache\n        sigma = self.sigma(t) * x.new_ones([x.shape[0]])\n        eps = (x - self.model(x, sigma, *args, **self.extra_args, **kwargs)) / self.sigma(t)\n        if self.eps_callback is not None:\n            self.eps_callback()\n        return eps, {key: eps, **eps_cache}\n\n    def dpm_solver_1_step(self, x, t, t_next, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        x_1 = x - self.sigma(t_next) * h.expm1() * eps\n        return x_1, eps_cache\n\n    def dpm_solver_2_step(self, x, t, t_next, r1=1 / 2, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        s1 = t + r1 * h\n        u1 = x - self.sigma(s1) * (r1 * h).expm1() * eps\n        eps_r1, eps_cache = self.eps(eps_cache, 'eps_r1', u1, s1)\n        x_2 = x - self.sigma(t_next) * h.expm1() * eps - self.sigma(t_next) / (2 * r1) * h.expm1() * (eps_r1 - eps)\n        return x_2, eps_cache\n\n    def dpm_solver_3_step(self, x, t, t_next, r1=1 / 3, r2=2 / 3, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        s1 = t + r1 * h\n        s2 = t + r2 * h\n        u1 = x - self.sigma(s1) * (r1 * h).expm1() * eps\n        eps_r1, eps_cache = self.eps(eps_cache, 'eps_r1', u1, s1)\n        u2 = x - self.sigma(s2) * (r2 * h).expm1() * eps - self.sigma(s2) * (r2 / r1) * ((r2 * h).expm1() / (r2 * h) - 1) * (eps_r1 - eps)\n        eps_r2, eps_cache = self.eps(eps_cache, 'eps_r2', u2, s2)\n        x_3 = x - self.sigma(t_next) * h.expm1() * eps - self.sigma(t_next) / r2 * (h.expm1() / h - 1) * (eps_r2 - eps)\n        return x_3, eps_cache\n\n    def dpm_solver_fast(self, x, t_start, t_end, nfe, eta=0., s_noise=1., noise_sampler=None):\n        noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n        if not t_end > t_start and eta:\n            raise ValueError('eta must be 0 for reverse sampling')\n\n        m = math.floor(nfe / 3) + 1\n        ts = torch.linspace(t_start, t_end, m + 1, device=x.device)\n\n        if nfe % 3 == 0:\n            orders = [3] * (m - 2) + [2, 1]\n        else:\n            orders = [3] * (m - 1) + [nfe % 3]\n\n        for i in range(len(orders)):\n            eps_cache = {}\n            t, t_next = ts[i], ts[i + 1]\n            if eta:\n                sd, su = get_ancestral_step(self.sigma(t), self.sigma(t_next), eta)\n                t_next_ = torch.minimum(t_end, self.t(sd))\n                su = (self.sigma(t_next) ** 2 - self.sigma(t_next_) ** 2) ** 0.5\n            else:\n                t_next_, su = t_next, 0.\n\n            eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n            denoised = x - self.sigma(t) * eps\n            if self.info_callback is not None:\n                self.info_callback({'x': x, 'i': i, 't': ts[i], 't_up': t, 'denoised': denoised})\n\n            if orders[i] == 1:\n                x, eps_cache = self.dpm_solver_1_step(x, t, t_next_, eps_cache=eps_cache)\n            elif orders[i] == 2:\n                x, eps_cache = self.dpm_solver_2_step(x, t, t_next_, eps_cache=eps_cache)\n            else:\n                x, eps_cache = self.dpm_solver_3_step(x, t, t_next_, eps_cache=eps_cache)\n\n            x = x + su * s_noise * noise_sampler(self.sigma(t), self.sigma(t_next))\n\n        return x\n\n    def dpm_solver_adaptive(self, x, t_start, t_end, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None):\n        noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n        if order not in {2, 3}:\n            raise ValueError('order should be 2 or 3')\n        forward = t_end > t_start\n        if not forward and eta:\n            raise ValueError('eta must be 0 for reverse sampling')\n        h_init = abs(h_init) * (1 if forward else -1)\n        atol = torch.tensor(atol)\n        rtol = torch.tensor(rtol)\n        s = t_start\n        x_prev = x\n        accept = True\n        pid = PIDStepSizeController(h_init, pcoeff, icoeff, dcoeff, 1.5 if eta else order, accept_safety)\n        info = {'steps': 0, 'nfe': 0, 'n_accept': 0, 'n_reject': 0}\n\n        while s < t_end - 1e-5 if forward else s > t_end + 1e-5:\n            eps_cache = {}\n            t = torch.minimum(t_end, s + pid.h) if forward else torch.maximum(t_end, s + pid.h)\n            if eta:\n                sd, su = get_ancestral_step(self.sigma(s), self.sigma(t), eta)\n                t_ = torch.minimum(t_end, self.t(sd))\n                su = (self.sigma(t) ** 2 - self.sigma(t_) ** 2) ** 0.5\n            else:\n                t_, su = t, 0.\n\n            eps, eps_cache = self.eps(eps_cache, 'eps', x, s)\n            denoised = x - self.sigma(s) * eps\n\n            if order == 2:\n                x_low, eps_cache = self.dpm_solver_1_step(x, s, t_, eps_cache=eps_cache)\n                x_high, eps_cache = self.dpm_solver_2_step(x, s, t_, eps_cache=eps_cache)\n            else:\n                x_low, eps_cache = self.dpm_solver_2_step(x, s, t_, r1=1 / 3, eps_cache=eps_cache)\n                x_high, eps_cache = self.dpm_solver_3_step(x, s, t_, eps_cache=eps_cache)\n            delta = torch.maximum(atol, rtol * torch.maximum(x_low.abs(), x_prev.abs()))\n            error = torch.linalg.norm((x_low - x_high) / delta) / x.numel() ** 0.5\n            accept = pid.propose_step(error)\n            if accept:\n                x_prev = x_low\n                x = x_high + su * s_noise * noise_sampler(self.sigma(s), self.sigma(t))\n                s = t\n                info['n_accept'] += 1\n            else:\n                info['n_reject'] += 1\n            info['nfe'] += order\n            info['steps'] += 1\n\n            if self.info_callback is not None:\n                self.info_callback({'x': x, 'i': info['steps'] - 1, 't': s, 't_up': s, 'denoised': denoised, 'error': error, 'h': pid.h, **info})\n\n        return x, info\n\n\n@torch.no_grad()\ndef sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(total=n, disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)\n\n\n@torch.no_grad()\ndef sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):\n    \"\"\"DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)\n    if return_info:\n        return x, info\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_2s_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver++(2S) second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigma_down == 0:\n            # Euler method\n            d = to_d(x, sigmas[i], denoised)\n            dt = sigma_down - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver++(2S)\n            t, t_next = t_fn(sigmas[i]), t_fn(sigma_down)\n            r = 1 / 2\n            h = t_next - t\n            s = t + r * h\n            x_2 = (sigma_fn(s) / sigma_fn(t)) * x - (-h * r).expm1() * denoised\n            denoised_2 = model(x_2, sigma_fn(s) * s_in, **extra_args)\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised_2\n        # Noise addition\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    \"\"\"DPM-Solver++ (stochastic).\"\"\"\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    seed = extra_args.get(\"seed\", None)\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Euler method\n            d = to_d(x, sigmas[i], denoised)\n            dt = sigmas[i + 1] - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver++\n            t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])\n            h = t_next - t\n            s = t + h * r\n            fac = 1 / (2 * r)\n\n            # Step 1\n            sd, su = get_ancestral_step(sigma_fn(t), sigma_fn(s), eta)\n            s_ = t_fn(sd)\n            x_2 = (sigma_fn(s_) / sigma_fn(t)) * x - (t - s_).expm1() * denoised\n            x_2 = x_2 + noise_sampler(sigma_fn(t), sigma_fn(s)) * s_noise * su\n            denoised_2 = model(x_2, sigma_fn(s) * s_in, **extra_args)\n\n            # Step 2\n            sd, su = get_ancestral_step(sigma_fn(t), sigma_fn(t_next), eta)\n            t_next_ = t_fn(sd)\n            denoised_d = (1 - fac) * denoised + fac * denoised_2\n            x = (sigma_fn(t_next_) / sigma_fn(t)) * x - (t - t_next_).expm1() * denoised_d\n            x = x + noise_sampler(sigma_fn(t), sigma_fn(t_next)) * s_noise * su\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_2m(model, x, sigmas, extra_args=None, callback=None, disable=None):\n    \"\"\"DPM-Solver++(2M).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n    old_denoised = None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])\n        h = t_next - t\n        if old_denoised is None or sigmas[i + 1] == 0:\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised\n        else:\n            h_last = t - t_fn(sigmas[i - 1])\n            r = h_last / h\n            denoised_d = (1 + 1 / (2 * r)) * denoised - (1 / (2 * r)) * old_denoised\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised_d\n        old_denoised = denoised\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_2m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    \"\"\"DPM-Solver++(2M) SDE.\"\"\"\n\n    if solver_type not in {'heun', 'midpoint'}:\n        raise ValueError('solver_type must be \\'heun\\' or \\'midpoint\\'')\n\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    old_denoised = None\n    h_last = None\n    h = None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            # DPM-Solver++(2M) SDE\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            eta_h = eta * h\n\n            x = sigmas[i + 1] / sigmas[i] * (-eta_h).exp() * x + (-h - eta_h).expm1().neg() * denoised\n\n            if old_denoised is not None:\n                r = h_last / h\n                if solver_type == 'heun':\n                    x = x + ((-h - eta_h).expm1().neg() / (-h - eta_h) + 1) * (1 / r) * (denoised - old_denoised)\n                elif solver_type == 'midpoint':\n                    x = x + 0.5 * (-h - eta_h).expm1().neg() * (1 / r) * (denoised - old_denoised)\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * eta_h).expm1().neg().sqrt() * s_noise\n\n        old_denoised = denoised\n        h_last = h\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_3m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver++(3M) SDE.\"\"\"\n\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    denoised_1, denoised_2 = None, None\n    h, h_1, h_2 = None, None, None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_3m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_3m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler)\n\n@torch.no_grad()\ndef sample_dpmpp_2m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_2m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, solver_type=solver_type)\n\n@torch.no_grad()\ndef sample_dpmpp_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, r=r)\n\n\ndef DDPMSampler_step(x, sigma, sigma_prev, noise, noise_sampler):\n    alpha_cumprod = 1 / ((sigma * sigma) + 1)\n    alpha_cumprod_prev = 1 / ((sigma_prev * sigma_prev) + 1)\n    alpha = (alpha_cumprod / alpha_cumprod_prev)\n\n    mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n    if sigma_prev > 0:\n        mu += ((1 - alpha) * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)).sqrt() * noise_sampler(sigma, sigma_prev)\n    return mu\n\ndef generic_step_sampler(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None, step_function=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = step_function(x / torch.sqrt(1.0 + sigmas[i] ** 2.0), sigmas[i], sigmas[i + 1], (x - denoised) / sigmas[i], noise_sampler)\n        if sigmas[i + 1] != 0:\n            x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2.0)\n    return x\n\n\n@torch.no_grad()\ndef sample_ddpm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    return generic_step_sampler(model, x, sigmas, extra_args, callback, disable, noise_sampler, DDPMSampler_step)\n\n@torch.no_grad()\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        x = denoised\n        if sigmas[i + 1] > 0:\n            x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\n    return x\n\n\n@torch.no_grad()\ndef sample_heunpp2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    # From MIT licensed: https://github.com/Carzit/sd-webui-samplers-scheduler/\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    s_end = sigmas[-1]\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        eps = torch.randn_like(x) * s_noise\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        if sigmas[i + 1] == s_end:\n            # Euler method\n            x = x + d * dt\n        elif sigmas[i + 2] == s_end:\n\n            # Heun's method\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n\n            w = 2 * sigmas[0]\n            w2 = sigmas[i+1]/w\n            w1 = 1 - w2\n\n            d_prime = d * w1 + d_2 * w2\n\n\n            x = x + d_prime * dt\n\n        else:\n            # Heun++\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n            dt_2 = sigmas[i + 2] - sigmas[i + 1]\n\n            x_3 = x_2 + d_2 * dt_2\n            denoised_3 = model(x_3, sigmas[i + 2] * s_in, **extra_args)\n            d_3 = to_d(x_3, sigmas[i + 2], denoised_3)\n\n            w = 3 * sigmas[0]\n            w2 = sigmas[i + 1] / w\n            w3 = sigmas[i + 2] / w\n            w1 = 1 - w2 - w3\n\n            d_prime = w1 * d + w2 * d_2 + w3 * d_3\n            x = x + d_prime * dt\n    return x\n\n\n@torch.no_grad()\ndef sample_tcd(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None, eta=0.3):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    model_sampling = model.inner_model.inner_model.model_sampling\n    timesteps_s = torch.floor((1 - eta) * model_sampling.timestep(sigmas)).to(dtype=torch.long).detach().cpu()\n    timesteps_s[-1] = 0\n    alpha_prod_s = model_sampling.alphas_cumprod[timesteps_s]\n    beta_prod_s = 1 - alpha_prod_s\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)  # predicted_original_sample\n        eps = (x - denoised) / sigmas[i]\n        denoised = alpha_prod_s[i + 1].sqrt() * denoised + beta_prod_s[i + 1].sqrt() * eps\n\n        if callback is not None:\n            callback({\"x\": x, \"i\": i, \"sigma\": sigmas[i], \"sigma_hat\": sigmas[i], \"denoised\": denoised})\n\n        x = denoised\n        if eta > 0 and sigmas[i + 1] > 0:\n            noise = noise_sampler(sigmas[i], sigmas[i + 1])\n            x = x / alpha_prod_s[i+1].sqrt() + noise * (sigmas[i+1]**2 + 1 - 1/alpha_prod_s[i+1]).sqrt()\n        else:\n            x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2)\n\n    return x", "ldm_patched/k_diffusion/utils.py": "from contextlib import contextmanager\nimport hashlib\nimport math\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport warnings\n\nfrom PIL import Image\nimport torch\nfrom torch import nn, optim\nfrom torch.utils import data\n\n\ndef hf_datasets_augs_helper(examples, transform, image_key, mode='RGB'):\n    \"\"\"Apply passed in transforms for HuggingFace Datasets.\"\"\"\n    images = [transform(image.convert(mode)) for image in examples[image_key]]\n    return {image_key: images}\n\n\ndef append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    expanded = x[(...,) + (None,) * dims_to_append]\n    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.\n    # https://github.com/pytorch/pytorch/issues/84364\n    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded\n\n\ndef n_params(module):\n    \"\"\"Returns the number of trainable parameters in a module.\"\"\"\n    return sum(p.numel() for p in module.parameters())\n\n\ndef download_file(path, url, digest=None):\n    \"\"\"Downloads a file if it does not exist, optionally checking its SHA-256 hash.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if digest != file_digest:\n            raise OSError(f'hash of {path} (url: {url}) failed to validate')\n    return path\n\n\n@contextmanager\ndef train_mode(model, mode=True):\n    \"\"\"A context manager that places a model into training mode and restores\n    the previous mode on exit.\"\"\"\n    modes = [module.training for module in model.modules()]\n    try:\n        yield model.train(mode)\n    finally:\n        for i, module in enumerate(model.modules()):\n            module.training = modes[i]\n\n\ndef eval_mode(model):\n    \"\"\"A context manager that places a model into evaluation mode and restores\n    the previous mode on exit.\"\"\"\n    return train_mode(model, False)\n\n\n@torch.no_grad()\ndef ema_update(model, averaged_model, decay):\n    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n    version of a model. It should be called after each optimizer step.\"\"\"\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    assert model_params.keys() == averaged_params.keys()\n\n    for name, param in model_params.items():\n        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    assert model_buffers.keys() == averaged_buffers.keys()\n\n    for name, buf in model_buffers.items():\n        averaged_buffers[name].copy_(buf)\n\n\nclass EMAWarmup:\n    \"\"\"Implements an EMA warmup using an inverse decay schedule.\n    If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n    good values for models you plan to train for a million or more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \"\"\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \"\"\"Returns the state of the class as a :class:`dict`.\"\"\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \"\"\"Gets the current EMA decay rate.\"\"\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \"\"\"Updates the step count.\"\"\"\n        self.last_epoch += 1\n\n\nclass InverseLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an inverse decay learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr.\n    inv_gamma is the number of steps/epochs required for the learning rate to decay to\n    (1 / 2)**power of its original value.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        inv_gamma (float): Inverse multiplicative factor of learning rate decay. Default: 1.\n        power (float): Exponential factor of learning rate decay. Default: 1.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)\n            Default: 0.\n        min_lr (float): The minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n    \"\"\"\n\n    def __init__(self, optimizer, inv_gamma=1., power=1., warmup=0., min_lr=0.,\n                 last_epoch=-1, verbose=False):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        if not 0. <= warmup < 1:\n            raise ValueError('Invalid value for warmup')\n        self.warmup = warmup\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\")\n\n        return self._get_closed_form_lr()\n\n    def _get_closed_form_lr(self):\n        warmup = 1 - self.warmup ** (self.last_epoch + 1)\n        lr_mult = (1 + self.last_epoch / self.inv_gamma) ** -self.power\n        return [warmup * max(self.min_lr, base_lr * lr_mult)\n                for base_lr in self.base_lrs]\n\n\nclass ExponentialLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an exponential learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr. Decays the learning rate\n    continuously by decay (default 0.5) every num_steps steps.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        num_steps (float): The number of steps to decay the learning rate by decay in.\n        decay (float): The factor by which to decay the learning rate every num_steps\n            steps. Default: 0.5.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)\n            Default: 0.\n        min_lr (float): The minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n    \"\"\"\n\n    def __init__(self, optimizer, num_steps, decay=0.5, warmup=0., min_lr=0.,\n                 last_epoch=-1, verbose=False):\n        self.num_steps = num_steps\n        self.decay = decay\n        if not 0. <= warmup < 1:\n            raise ValueError('Invalid value for warmup')\n        self.warmup = warmup\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\")\n\n        return self._get_closed_form_lr()\n\n    def _get_closed_form_lr(self):\n        warmup = 1 - self.warmup ** (self.last_epoch + 1)\n        lr_mult = (self.decay ** (1 / self.num_steps)) ** self.last_epoch\n        return [warmup * max(self.min_lr, base_lr * lr_mult)\n                for base_lr in self.base_lrs]\n\n\ndef rand_log_normal(shape, loc=0., scale=1., device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an lognormal distribution.\"\"\"\n    return (torch.randn(shape, device=device, dtype=dtype) * scale + loc).exp()\n\n\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an optionally truncated log-logistic distribution.\"\"\"\n    min_value = torch.as_tensor(min_value, device=device, dtype=torch.float64)\n    max_value = torch.as_tensor(max_value, device=device, dtype=torch.float64)\n    min_cdf = min_value.log().sub(loc).div(scale).sigmoid()\n    max_cdf = max_value.log().sub(loc).div(scale).sigmoid()\n    u = torch.rand(shape, device=device, dtype=torch.float64) * (max_cdf - min_cdf) + min_cdf\n    return u.logit().mul(scale).add(loc).exp().to(dtype)\n\n\ndef rand_log_uniform(shape, min_value, max_value, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an log-uniform distribution.\"\"\"\n    min_value = math.log(min_value)\n    max_value = math.log(max_value)\n    return (torch.rand(shape, device=device, dtype=dtype) * (max_value - min_value) + min_value).exp()\n\n\ndef rand_v_diffusion(shape, sigma_data=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a truncated v-diffusion training timestep distribution.\"\"\"\n    min_cdf = math.atan(min_value / sigma_data) * 2 / math.pi\n    max_cdf = math.atan(max_value / sigma_data) * 2 / math.pi\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return torch.tan(u * math.pi / 2) * sigma_data\n\n\ndef rand_split_log_normal(shape, loc, scale_1, scale_2, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a split lognormal distribution.\"\"\"\n    n = torch.randn(shape, device=device, dtype=dtype).abs()\n    u = torch.rand(shape, device=device, dtype=dtype)\n    n_left = n * -scale_1 + loc\n    n_right = n * scale_2 + loc\n    ratio = scale_1 / (scale_1 + scale_2)\n    return torch.where(u < ratio, n_left, n_right).exp()\n\n\nclass FolderOfImages(data.Dataset):\n    \"\"\"Recursively finds all images in a directory. It does not support\n    classes/targets.\"\"\"\n\n    IMG_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'}\n\n    def __init__(self, root, transform=None):\n        super().__init__()\n        self.root = Path(root)\n        self.transform = nn.Identity() if transform is None else transform\n        self.paths = sorted(path for path in self.root.rglob('*') if path.suffix.lower() in self.IMG_EXTENSIONS)\n\n    def __repr__(self):\n        return f'FolderOfImages(root=\"{self.root}\", len: {len(self)})'\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, key):\n        path = self.paths[key]\n        with open(path, 'rb') as f:\n            image = Image.open(f).convert('RGB')\n        image = self.transform(image)\n        return image,\n\n\nclass CSVLogger:\n    def __init__(self, filename, columns):\n        self.filename = Path(filename)\n        self.columns = columns\n        if self.filename.exists():\n            self.file = open(self.filename, 'a')\n        else:\n            self.file = open(self.filename, 'w')\n            self.write(*self.columns)\n\n    def write(self, *args):\n        print(*args, sep=',', file=self.file, flush=True)\n\n\n@contextmanager\ndef tf32_mode(cudnn=None, matmul=None):\n    \"\"\"A context manager that sets whether TF32 is allowed on cuDNN or matmul.\"\"\"\n    cudnn_old = torch.backends.cudnn.allow_tf32\n    matmul_old = torch.backends.cuda.matmul.allow_tf32\n    try:\n        if cudnn is not None:\n            torch.backends.cudnn.allow_tf32 = cudnn\n        if matmul is not None:\n            torch.backends.cuda.matmul.allow_tf32 = matmul\n        yield\n    finally:\n        if cudnn is not None:\n            torch.backends.cudnn.allow_tf32 = cudnn_old\n        if matmul is not None:\n            torch.backends.cuda.matmul.allow_tf32 = matmul_old\n", "ldm_patched/pfn/model_loading.py": "import logging as logger\n\nfrom .architecture.DAT import DAT\nfrom .architecture.face.codeformer import CodeFormer\nfrom .architecture.face.gfpganv1_clean_arch import GFPGANv1Clean\nfrom .architecture.face.restoreformer_arch import RestoreFormer\nfrom .architecture.HAT import HAT\nfrom .architecture.LaMa import LaMa\nfrom .architecture.OmniSR.OmniSR import OmniSR\nfrom .architecture.RRDB import RRDBNet as ESRGAN\nfrom .architecture.SCUNet import SCUNet\nfrom .architecture.SPSR import SPSRNet as SPSR\nfrom .architecture.SRVGG import SRVGGNetCompact as RealESRGANv2\nfrom .architecture.SwiftSRGAN import Generator as SwiftSRGAN\nfrom .architecture.Swin2SR import Swin2SR\nfrom .architecture.SwinIR import SwinIR\nfrom .types import PyTorchModel\n\n\nclass UnsupportedModel(Exception):\n    pass\n\n\ndef load_state_dict(state_dict) -> PyTorchModel:\n    logger.debug(f\"Loading state dict into pytorch model arch\")\n\n    state_dict_keys = list(state_dict.keys())\n\n    if \"params_ema\" in state_dict_keys:\n        state_dict = state_dict[\"params_ema\"]\n    elif \"params-ema\" in state_dict_keys:\n        state_dict = state_dict[\"params-ema\"]\n    elif \"params\" in state_dict_keys:\n        state_dict = state_dict[\"params\"]\n\n    state_dict_keys = list(state_dict.keys())\n    # SRVGGNet Real-ESRGAN (v2)\n    if \"body.0.weight\" in state_dict_keys and \"body.1.weight\" in state_dict_keys:\n        model = RealESRGANv2(state_dict)\n    # SPSR (ESRGAN with lots of extra layers)\n    elif \"f_HR_conv1.0.weight\" in state_dict:\n        model = SPSR(state_dict)\n    # Swift-SRGAN\n    elif (\n        \"model\" in state_dict_keys\n        and \"initial.cnn.depthwise.weight\" in state_dict[\"model\"].keys()\n    ):\n        model = SwiftSRGAN(state_dict)\n    # SwinIR, Swin2SR, HAT\n    elif \"layers.0.residual_group.blocks.0.norm1.weight\" in state_dict_keys:\n        if (\n            \"layers.0.residual_group.blocks.0.conv_block.cab.0.weight\"\n            in state_dict_keys\n        ):\n            model = HAT(state_dict)\n        elif \"patch_embed.proj.weight\" in state_dict_keys:\n            model = Swin2SR(state_dict)\n        else:\n            model = SwinIR(state_dict)\n    # GFPGAN\n    elif (\n        \"toRGB.0.weight\" in state_dict_keys\n        and \"stylegan_decoder.style_mlp.1.weight\" in state_dict_keys\n    ):\n        model = GFPGANv1Clean(state_dict)\n    # RestoreFormer\n    elif (\n        \"encoder.conv_in.weight\" in state_dict_keys\n        and \"encoder.down.0.block.0.norm1.weight\" in state_dict_keys\n    ):\n        model = RestoreFormer(state_dict)\n    elif (\n        \"encoder.blocks.0.weight\" in state_dict_keys\n        and \"quantize.embedding.weight\" in state_dict_keys\n    ):\n        model = CodeFormer(state_dict)\n    # LaMa\n    elif (\n        \"model.model.1.bn_l.running_mean\" in state_dict_keys\n        or \"generator.model.1.bn_l.running_mean\" in state_dict_keys\n    ):\n        model = LaMa(state_dict)\n    # Omni-SR\n    elif \"residual_layer.0.residual_layer.0.layer.0.fn.0.weight\" in state_dict_keys:\n        model = OmniSR(state_dict)\n    # SCUNet\n    elif \"m_head.0.weight\" in state_dict_keys and \"m_tail.0.weight\" in state_dict_keys:\n        model = SCUNet(state_dict)\n    # DAT\n    elif \"layers.0.blocks.2.attn.attn_mask_0\" in state_dict_keys:\n        model = DAT(state_dict)\n    # Regular ESRGAN, \"new-arch\" ESRGAN, Real-ESRGAN v1\n    else:\n        try:\n            model = ESRGAN(state_dict)\n        except:\n            # pylint: disable=raise-missing-from\n            raise UnsupportedModel\n    return model\n", "ldm_patched/pfn/types.py": "from typing import Union\n\nfrom .architecture.DAT import DAT\nfrom .architecture.face.codeformer import CodeFormer\nfrom .architecture.face.gfpganv1_clean_arch import GFPGANv1Clean\nfrom .architecture.face.restoreformer_arch import RestoreFormer\nfrom .architecture.HAT import HAT\nfrom .architecture.LaMa import LaMa\nfrom .architecture.OmniSR.OmniSR import OmniSR\nfrom .architecture.RRDB import RRDBNet as ESRGAN\nfrom .architecture.SCUNet import SCUNet\nfrom .architecture.SPSR import SPSRNet as SPSR\nfrom .architecture.SRVGG import SRVGGNetCompact as RealESRGANv2\nfrom .architecture.SwiftSRGAN import Generator as SwiftSRGAN\nfrom .architecture.Swin2SR import Swin2SR\nfrom .architecture.SwinIR import SwinIR\n\nPyTorchSRModels = (\n    RealESRGANv2,\n    SPSR,\n    SwiftSRGAN,\n    ESRGAN,\n    SwinIR,\n    Swin2SR,\n    HAT,\n    OmniSR,\n    SCUNet,\n    DAT,\n)\nPyTorchSRModel = Union[\n    RealESRGANv2,\n    SPSR,\n    SwiftSRGAN,\n    ESRGAN,\n    SwinIR,\n    Swin2SR,\n    HAT,\n    OmniSR,\n    SCUNet,\n    DAT,\n]\n\n\ndef is_pytorch_sr_model(model: object):\n    return isinstance(model, PyTorchSRModels)\n\n\nPyTorchFaceModels = (GFPGANv1Clean, RestoreFormer, CodeFormer)\nPyTorchFaceModel = Union[GFPGANv1Clean, RestoreFormer, CodeFormer]\n\n\ndef is_pytorch_face_model(model: object):\n    return isinstance(model, PyTorchFaceModels)\n\n\nPyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\n\n\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\n\n\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\n\n\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)\n", "ldm_patched/pfn/__init__.py": "", "ldm_patched/pfn/architecture/block.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nfrom collections import OrderedDict\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal\n\nimport torch\nimport torch.nn as nn\n\n####################\n# Basic blocks\n####################\n\n\ndef act(act_type: str, inplace=True, neg_slope=0.2, n_prelu=1):\n    # helper selecting activation\n    # neg_slope: for leakyrelu and init of prelu\n    # n_prelu: for p_relu num_parameters\n    act_type = act_type.lower()\n    if act_type == \"relu\":\n        layer = nn.ReLU(inplace)\n    elif act_type == \"leakyrelu\":\n        layer = nn.LeakyReLU(neg_slope, inplace)\n    elif act_type == \"prelu\":\n        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n    else:\n        raise NotImplementedError(\n            \"activation layer [{:s}] is not found\".format(act_type)\n        )\n    return layer\n\n\ndef norm(norm_type: str, nc: int):\n    # helper selecting normalization layer\n    norm_type = norm_type.lower()\n    if norm_type == \"batch\":\n        layer = nn.BatchNorm2d(nc, affine=True)\n    elif norm_type == \"instance\":\n        layer = nn.InstanceNorm2d(nc, affine=False)\n    else:\n        raise NotImplementedError(\n            \"normalization layer [{:s}] is not found\".format(norm_type)\n        )\n    return layer\n\n\ndef pad(pad_type: str, padding):\n    # helper selecting padding layer\n    # if padding is 'zero', do by conv layers\n    pad_type = pad_type.lower()\n    if padding == 0:\n        return None\n    if pad_type == \"reflect\":\n        layer = nn.ReflectionPad2d(padding)\n    elif pad_type == \"replicate\":\n        layer = nn.ReplicationPad2d(padding)\n    else:\n        raise NotImplementedError(\n            \"padding layer [{:s}] is not implemented\".format(pad_type)\n        )\n    return layer\n\n\ndef get_valid_padding(kernel_size, dilation):\n    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n    padding = (kernel_size - 1) // 2\n    return padding\n\n\nclass ConcatBlock(nn.Module):\n    # Concat the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ConcatBlock, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        output = torch.cat((x, self.sub(x)), dim=1)\n        return output\n\n    def __repr__(self):\n        tmpstr = \"Identity .. \\n|\"\n        modstr = self.sub.__repr__().replace(\"\\n\", \"\\n|\")\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\nclass ShortcutBlock(nn.Module):\n    # Elementwise sum the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ShortcutBlock, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        output = x + self.sub(x)\n        return output\n\n    def __repr__(self):\n        tmpstr = \"Identity + \\n|\"\n        modstr = self.sub.__repr__().replace(\"\\n\", \"\\n|\")\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\nclass ShortcutBlockSPSR(nn.Module):\n    # Elementwise sum the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ShortcutBlockSPSR, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        return x, self.sub\n\n    def __repr__(self):\n        tmpstr = \"Identity + \\n|\"\n        modstr = self.sub.__repr__().replace(\"\\n\", \"\\n|\")\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\ndef sequential(*args):\n    # Flatten Sequential. It unwraps nn.Sequential.\n    if len(args) == 1:\n        if isinstance(args[0], OrderedDict):\n            raise NotImplementedError(\"sequential does not support OrderedDict input.\")\n        return args[0]  # No sequential is needed.\n    modules = []\n    for module in args:\n        if isinstance(module, nn.Sequential):\n            for submodule in module.children():\n                modules.append(submodule)\n        elif isinstance(module, nn.Module):\n            modules.append(module)\n    return nn.Sequential(*modules)\n\n\nConvMode = Literal[\"CNA\", \"NAC\", \"CNAC\"]\n\n\n# 2x2x2 Conv Block\ndef conv_block_2c2(\n    in_nc,\n    out_nc,\n    act_type=\"relu\",\n):\n    return sequential(\n        nn.Conv2d(in_nc, out_nc, kernel_size=2, padding=1),\n        nn.Conv2d(out_nc, out_nc, kernel_size=2, padding=0),\n        act(act_type) if act_type else None,\n    )\n\n\ndef conv_block(\n    in_nc: int,\n    out_nc: int,\n    kernel_size,\n    stride=1,\n    dilation=1,\n    groups=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,\n    act_type: str | None = \"relu\",\n    mode: ConvMode = \"CNA\",\n    c2x2=False,\n):\n    \"\"\"\n    Conv layer with padding, normalization, activation\n    mode: CNA --> Conv -> Norm -> Act\n        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n    \"\"\"\n\n    if c2x2:\n        return conv_block_2c2(in_nc, out_nc, act_type=act_type)\n\n    assert mode in (\"CNA\", \"NAC\", \"CNAC\"), \"Wrong conv mode [{:s}]\".format(mode)\n    padding = get_valid_padding(kernel_size, dilation)\n    p = pad(pad_type, padding) if pad_type and pad_type != \"zero\" else None\n    padding = padding if pad_type == \"zero\" else 0\n\n    c = nn.Conv2d(\n        in_nc,\n        out_nc,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        bias=bias,\n        groups=groups,\n    )\n    a = act(act_type) if act_type else None\n    if mode in (\"CNA\", \"CNAC\"):\n        n = norm(norm_type, out_nc) if norm_type else None\n        return sequential(p, c, n, a)\n    elif mode == \"NAC\":\n        if norm_type is None and act_type is not None:\n            a = act(act_type, inplace=False)\n            # Important!\n            # input----ReLU(inplace)----Conv--+----output\n            #        |________________________|\n            # inplace ReLU will modify the input, therefore wrong output\n        n = norm(norm_type, in_nc) if norm_type else None\n        return sequential(n, a, p, c)\n    else:\n        assert False, f\"Invalid conv mode {mode}\"\n\n\n####################\n# Useful blocks\n####################\n\n\nclass ResNetBlock(nn.Module):\n    \"\"\"\n    ResNet Block, 3-3 style\n    with extra residual scaling used in EDSR\n    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_nc,\n        mid_nc,\n        out_nc,\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        groups=1,\n        bias=True,\n        pad_type=\"zero\",\n        norm_type=None,\n        act_type=\"relu\",\n        mode: ConvMode = \"CNA\",\n        res_scale=1,\n    ):\n        super(ResNetBlock, self).__init__()\n        conv0 = conv_block(\n            in_nc,\n            mid_nc,\n            kernel_size,\n            stride,\n            dilation,\n            groups,\n            bias,\n            pad_type,\n            norm_type,\n            act_type,\n            mode,\n        )\n        if mode == \"CNA\":\n            act_type = None\n        if mode == \"CNAC\":  # Residual path: |-CNAC-|\n            act_type = None\n            norm_type = None\n        conv1 = conv_block(\n            mid_nc,\n            out_nc,\n            kernel_size,\n            stride,\n            dilation,\n            groups,\n            bias,\n            pad_type,\n            norm_type,\n            act_type,\n            mode,\n        )\n        # if in_nc != out_nc:\n        #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \\\n        #         None, None)\n        #     print('Need a projecter in ResNetBlock.')\n        # else:\n        #     self.project = lambda x:x\n        self.res = sequential(conv0, conv1)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.res(x).mul(self.res_scale)\n        return x + res\n\n\nclass RRDB(nn.Module):\n    \"\"\"\n    Residual in Residual Dense Block\n    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n    \"\"\"\n\n    def __init__(\n        self,\n        nf,\n        kernel_size=3,\n        gc=32,\n        stride=1,\n        bias: bool = True,\n        pad_type=\"zero\",\n        norm_type=None,\n        act_type=\"leakyrelu\",\n        mode: ConvMode = \"CNA\",\n        _convtype=\"Conv2D\",\n        _spectral_norm=False,\n        plus=False,\n        c2x2=False,\n    ):\n        super(RRDB, self).__init__()\n        self.RDB1 = ResidualDenseBlock_5C(\n            nf,\n            kernel_size,\n            gc,\n            stride,\n            bias,\n            pad_type,\n            norm_type,\n            act_type,\n            mode,\n            plus=plus,\n            c2x2=c2x2,\n        )\n        self.RDB2 = ResidualDenseBlock_5C(\n            nf,\n            kernel_size,\n            gc,\n            stride,\n            bias,\n            pad_type,\n            norm_type,\n            act_type,\n            mode,\n            plus=plus,\n            c2x2=c2x2,\n        )\n        self.RDB3 = ResidualDenseBlock_5C(\n            nf,\n            kernel_size,\n            gc,\n            stride,\n            bias,\n            pad_type,\n            norm_type,\n            act_type,\n            mode,\n            plus=plus,\n            c2x2=c2x2,\n        )\n\n    def forward(self, x):\n        out = self.RDB1(x)\n        out = self.RDB2(out)\n        out = self.RDB3(out)\n        return out * 0.2 + x\n\n\nclass ResidualDenseBlock_5C(nn.Module):\n    \"\"\"\n    Residual Dense Block\n    style: 5 convs\n    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n    Modified options that can be used:\n        - \"Partial Convolution based Padding\" arXiv:1811.11718\n        - \"Spectral normalization\" arXiv:1802.05957\n        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C.\n            {Rakotonirina} and A. {Rasoanaivo}\n\n    Args:\n        nf (int): Channel number of intermediate features (num_feat).\n        gc (int): Channels for each growth (num_grow_ch: growth channel,\n            i.e. intermediate channels).\n        convtype (str): the type of convolution to use. Default: 'Conv2D'\n        gaussian_noise (bool): enable the ESRGAN+ gaussian noise (no new\n            trainable parameters)\n        plus (bool): enable the additional residual paths from ESRGAN+\n            (adds trainable parameters)\n    \"\"\"\n\n    def __init__(\n        self,\n        nf=64,\n        kernel_size=3,\n        gc=32,\n        stride=1,\n        bias: bool = True,\n        pad_type=\"zero\",\n        norm_type=None,\n        act_type=\"leakyrelu\",\n        mode: ConvMode = \"CNA\",\n        plus=False,\n        c2x2=False,\n    ):\n        super(ResidualDenseBlock_5C, self).__init__()\n\n        ## +\n        self.conv1x1 = conv1x1(nf, gc) if plus else None\n        ## +\n\n        self.conv1 = conv_block(\n            nf,\n            gc,\n            kernel_size,\n            stride,\n            bias=bias,\n            pad_type=pad_type,\n            norm_type=norm_type,\n            act_type=act_type,\n            mode=mode,\n            c2x2=c2x2,\n        )\n        self.conv2 = conv_block(\n            nf + gc,\n            gc,\n            kernel_size,\n            stride,\n            bias=bias,\n            pad_type=pad_type,\n            norm_type=norm_type,\n            act_type=act_type,\n            mode=mode,\n            c2x2=c2x2,\n        )\n        self.conv3 = conv_block(\n            nf + 2 * gc,\n            gc,\n            kernel_size,\n            stride,\n            bias=bias,\n            pad_type=pad_type,\n            norm_type=norm_type,\n            act_type=act_type,\n            mode=mode,\n            c2x2=c2x2,\n        )\n        self.conv4 = conv_block(\n            nf + 3 * gc,\n            gc,\n            kernel_size,\n            stride,\n            bias=bias,\n            pad_type=pad_type,\n            norm_type=norm_type,\n            act_type=act_type,\n            mode=mode,\n            c2x2=c2x2,\n        )\n        if mode == \"CNA\":\n            last_act = None\n        else:\n            last_act = act_type\n        self.conv5 = conv_block(\n            nf + 4 * gc,\n            nf,\n            3,\n            stride,\n            bias=bias,\n            pad_type=pad_type,\n            norm_type=norm_type,\n            act_type=last_act,\n            mode=mode,\n            c2x2=c2x2,\n        )\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.conv2(torch.cat((x, x1), 1))\n        if self.conv1x1:\n            # pylint: disable=not-callable\n            x2 = x2 + self.conv1x1(x)  # +\n        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n        if self.conv1x1:\n            x4 = x4 + x2  # +\n        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n        return x5 * 0.2 + x\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\n####################\n# Upsampler\n####################\n\n\ndef pixelshuffle_block(\n    in_nc: int,\n    out_nc: int,\n    upscale_factor=2,\n    kernel_size=3,\n    stride=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,\n    act_type=\"relu\",\n):\n    \"\"\"\n    Pixel shuffle layer\n    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n    Neural Network, CVPR17)\n    \"\"\"\n    conv = conv_block(\n        in_nc,\n        out_nc * (upscale_factor**2),\n        kernel_size,\n        stride,\n        bias=bias,\n        pad_type=pad_type,\n        norm_type=None,\n        act_type=None,\n    )\n    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    n = norm(norm_type, out_nc) if norm_type else None\n    a = act(act_type) if act_type else None\n    return sequential(conv, pixel_shuffle, n, a)\n\n\ndef upconv_block(\n    in_nc: int,\n    out_nc: int,\n    upscale_factor=2,\n    kernel_size=3,\n    stride=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,\n    act_type=\"relu\",\n    mode=\"nearest\",\n    c2x2=False,\n):\n    # Up conv\n    # described in https://distill.pub/2016/deconv-checkerboard/\n    upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n    conv = conv_block(\n        in_nc,\n        out_nc,\n        kernel_size,\n        stride,\n        bias=bias,\n        pad_type=pad_type,\n        norm_type=norm_type,\n        act_type=act_type,\n        c2x2=c2x2,\n    )\n    return sequential(upsample, conv)\n", "ldm_patched/pfn/architecture/DAT.py": "# pylint: skip-file\nimport math\nimport re\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\nfrom torch import Tensor\nfrom torch.nn import functional as F\n\nfrom .timm.drop import DropPath\nfrom .timm.weight_init import trunc_normal_\n\n\ndef img2windows(img, H_sp, W_sp):\n    \"\"\"\n    Input: Image (B, C, H, W)\n    Output: Window Partition (B', N, C)\n    \"\"\"\n    B, C, H, W = img.shape\n    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n    img_perm = (\n        img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp * W_sp, C)\n    )\n    return img_perm\n\n\ndef windows2img(img_splits_hw, H_sp, W_sp, H, W):\n    \"\"\"\n    Input: Window Partition (B', N, C)\n    Output: Image (B, H, W, C)\n    \"\"\"\n    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n\n    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return img\n\n\nclass SpatialGate(nn.Module):\n    \"\"\"Spatial-Gate.\n    Args:\n        dim (int): Half of input channels.\n    \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.conv = nn.Conv2d(\n            dim, dim, kernel_size=3, stride=1, padding=1, groups=dim\n        )  # DW Conv\n\n    def forward(self, x, H, W):\n        # Split\n        x1, x2 = x.chunk(2, dim=-1)\n        B, N, C = x.shape\n        x2 = (\n            self.conv(self.norm(x2).transpose(1, 2).contiguous().view(B, C // 2, H, W))\n            .flatten(2)\n            .transpose(-1, -2)\n            .contiguous()\n        )\n\n        return x1 * x2\n\n\nclass SGFN(nn.Module):\n    \"\"\"Spatial-Gate Feed-Forward Network.\n    Args:\n        in_features (int): Number of input channels.\n        hidden_features (int | None): Number of hidden channels. Default: None\n        out_features (int | None): Number of output channels. Default: None\n        act_layer (nn.Module): Activation layer. Default: nn.GELU\n        drop (float): Dropout rate. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.sg = SpatialGate(hidden_features // 2)\n        self.fc2 = nn.Linear(hidden_features // 2, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x, H, W):\n        \"\"\"\n        Input: x: (B, H*W, C), H, W\n        Output: x: (B, H*W, C)\n        \"\"\"\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n\n        x = self.sg(x, H, W)\n        x = self.drop(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass DynamicPosBias(nn.Module):\n    # The implementation builds on Crossformer code https://github.com/cheerss/CrossFormer/blob/main/models/crossformer.py\n    \"\"\"Dynamic Relative Position Bias.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        residual (bool):  If True, use residual strage to connect conv.\n    \"\"\"\n\n    def __init__(self, dim, num_heads, residual):\n        super().__init__()\n        self.residual = residual\n        self.num_heads = num_heads\n        self.pos_dim = dim // 4\n        self.pos_proj = nn.Linear(2, self.pos_dim)\n        self.pos1 = nn.Sequential(\n            nn.LayerNorm(self.pos_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.pos_dim, self.pos_dim),\n        )\n        self.pos2 = nn.Sequential(\n            nn.LayerNorm(self.pos_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.pos_dim, self.pos_dim),\n        )\n        self.pos3 = nn.Sequential(\n            nn.LayerNorm(self.pos_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.pos_dim, self.num_heads),\n        )\n\n    def forward(self, biases):\n        if self.residual:\n            pos = self.pos_proj(biases)  # 2Gh-1 * 2Gw-1, heads\n            pos = pos + self.pos1(pos)\n            pos = pos + self.pos2(pos)\n            pos = self.pos3(pos)\n        else:\n            pos = self.pos3(self.pos2(self.pos1(self.pos_proj(biases))))\n        return pos\n\n\nclass Spatial_Attention(nn.Module):\n    \"\"\"Spatial Window Self-Attention.\n    It supports rectangle window (containing square window).\n    Args:\n        dim (int): Number of input channels.\n        idx (int): The indentix of window. (0/1)\n        split_size (tuple(int)): Height and Width of spatial window.\n        dim_out (int | None): The dimension of the attention output. Default: None\n        num_heads (int): Number of attention heads. Default: 6\n        attn_drop (float): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float): Dropout ratio of output. Default: 0.0\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set\n        position_bias (bool): The dynamic relative position bias. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        idx,\n        split_size=[8, 8],\n        dim_out=None,\n        num_heads=6,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        qk_scale=None,\n        position_bias=True,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.dim_out = dim_out or dim\n        self.split_size = split_size\n        self.num_heads = num_heads\n        self.idx = idx\n        self.position_bias = position_bias\n\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        if idx == 0:\n            H_sp, W_sp = self.split_size[0], self.split_size[1]\n        elif idx == 1:\n            W_sp, H_sp = self.split_size[0], self.split_size[1]\n        else:\n            print(\"ERROR MODE\", idx)\n            exit(0)\n        self.H_sp = H_sp\n        self.W_sp = W_sp\n\n        if self.position_bias:\n            self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)\n            # generate mother-set\n            position_bias_h = torch.arange(1 - self.H_sp, self.H_sp)\n            position_bias_w = torch.arange(1 - self.W_sp, self.W_sp)\n            biases = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))\n            biases = biases.flatten(1).transpose(0, 1).contiguous().float()\n            self.register_buffer(\"rpe_biases\", biases)\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(self.H_sp)\n            coords_w = torch.arange(self.W_sp)\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n            coords_flatten = torch.flatten(coords, 1)\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n            relative_coords[:, :, 0] += self.H_sp - 1\n            relative_coords[:, :, 1] += self.W_sp - 1\n            relative_coords[:, :, 0] *= 2 * self.W_sp - 1\n            relative_position_index = relative_coords.sum(-1)\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n\n    def im2win(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n        x = img2windows(x, self.H_sp, self.W_sp)\n        x = (\n            x.reshape(-1, self.H_sp * self.W_sp, self.num_heads, C // self.num_heads)\n            .permute(0, 2, 1, 3)\n            .contiguous()\n        )\n        return x\n\n    def forward(self, qkv, H, W, mask=None):\n        \"\"\"\n        Input: qkv: (B, 3*L, C), H, W, mask: (B, N, N), N is the window size\n        Output: x (B, H, W, C)\n        \"\"\"\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        B, L, C = q.shape\n        assert L == H * W, \"flatten img_tokens has wrong size\"\n\n        # partition the q,k,v, image to window\n        q = self.im2win(q, H, W)\n        k = self.im2win(k, H, W)\n        v = self.im2win(v, H, W)\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)  # B head N C @ B head C N --> B head N N\n\n        # calculate drpe\n        if self.position_bias:\n            pos = self.pos(self.rpe_biases)\n            # select position bias\n            relative_position_bias = pos[self.relative_position_index.view(-1)].view(\n                self.H_sp * self.W_sp, self.H_sp * self.W_sp, -1\n            )\n            relative_position_bias = relative_position_bias.permute(\n                2, 0, 1\n            ).contiguous()\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        N = attn.shape[3]\n\n        # use mask for shift window\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(\n                0\n            )\n            attn = attn.view(-1, self.num_heads, N, N)\n\n        attn = nn.functional.softmax(attn, dim=-1, dtype=attn.dtype)\n        attn = self.attn_drop(attn)\n\n        x = attn @ v\n        x = x.transpose(1, 2).reshape(\n            -1, self.H_sp * self.W_sp, C\n        )  # B head N N @ B head N C\n\n        # merge the window, window to image\n        x = windows2img(x, self.H_sp, self.W_sp, H, W)  # B H' W' C\n\n        return x\n\n\nclass Adaptive_Spatial_Attention(nn.Module):\n    # The implementation builds on CAT code https://github.com/Zhengchen1999/CAT\n    \"\"\"Adaptive Spatial Self-Attention\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads. Default: 6\n        split_size (tuple(int)): Height and Width of spatial window.\n        shift_size (tuple(int)): Shift size for spatial window.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float): Dropout rate. Default: 0.0\n        attn_drop (float): Attention dropout rate. Default: 0.0\n        rg_idx (int): The indentix of Residual Group (RG)\n        b_idx (int): The indentix of Block in each RG\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        reso=64,\n        split_size=[8, 8],\n        shift_size=[1, 2],\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        rg_idx=0,\n        b_idx=0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.split_size = split_size\n        self.shift_size = shift_size\n        self.b_idx = b_idx\n        self.rg_idx = rg_idx\n        self.patches_resolution = reso\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\n        assert (\n            0 <= self.shift_size[0] < self.split_size[0]\n        ), \"shift_size must in 0-split_size0\"\n        assert (\n            0 <= self.shift_size[1] < self.split_size[1]\n        ), \"shift_size must in 0-split_size1\"\n\n        self.branch_num = 2\n\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(drop)\n\n        self.attns = nn.ModuleList(\n            [\n                Spatial_Attention(\n                    dim // 2,\n                    idx=i,\n                    split_size=split_size,\n                    num_heads=num_heads // 2,\n                    dim_out=dim // 2,\n                    qk_scale=qk_scale,\n                    attn_drop=attn_drop,\n                    proj_drop=drop,\n                    position_bias=True,\n                )\n                for i in range(self.branch_num)\n            ]\n        )\n\n        if (self.rg_idx % 2 == 0 and self.b_idx > 0 and (self.b_idx - 2) % 4 == 0) or (\n            self.rg_idx % 2 != 0 and self.b_idx % 4 == 0\n        ):\n            attn_mask = self.calculate_mask(\n                self.patches_resolution, self.patches_resolution\n            )\n            self.register_buffer(\"attn_mask_0\", attn_mask[0])\n            self.register_buffer(\"attn_mask_1\", attn_mask[1])\n        else:\n            attn_mask = None\n            self.register_buffer(\"attn_mask_0\", None)\n            self.register_buffer(\"attn_mask_1\", None)\n\n        self.dwconv = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim),\n            nn.BatchNorm2d(dim),\n            nn.GELU(),\n        )\n        self.channel_interaction = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, dim // 8, kernel_size=1),\n            nn.BatchNorm2d(dim // 8),\n            nn.GELU(),\n            nn.Conv2d(dim // 8, dim, kernel_size=1),\n        )\n        self.spatial_interaction = nn.Sequential(\n            nn.Conv2d(dim, dim // 16, kernel_size=1),\n            nn.BatchNorm2d(dim // 16),\n            nn.GELU(),\n            nn.Conv2d(dim // 16, 1, kernel_size=1),\n        )\n\n    def calculate_mask(self, H, W):\n        # The implementation builds on Swin Transformer code https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py\n        # calculate attention mask for shift window\n        img_mask_0 = torch.zeros((1, H, W, 1))  # 1 H W 1 idx=0\n        img_mask_1 = torch.zeros((1, H, W, 1))  # 1 H W 1 idx=1\n        h_slices_0 = (\n            slice(0, -self.split_size[0]),\n            slice(-self.split_size[0], -self.shift_size[0]),\n            slice(-self.shift_size[0], None),\n        )\n        w_slices_0 = (\n            slice(0, -self.split_size[1]),\n            slice(-self.split_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n\n        h_slices_1 = (\n            slice(0, -self.split_size[1]),\n            slice(-self.split_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices_1 = (\n            slice(0, -self.split_size[0]),\n            slice(-self.split_size[0], -self.shift_size[0]),\n            slice(-self.shift_size[0], None),\n        )\n        cnt = 0\n        for h in h_slices_0:\n            for w in w_slices_0:\n                img_mask_0[:, h, w, :] = cnt\n                cnt += 1\n        cnt = 0\n        for h in h_slices_1:\n            for w in w_slices_1:\n                img_mask_1[:, h, w, :] = cnt\n                cnt += 1\n\n        # calculate mask for window-0\n        img_mask_0 = img_mask_0.view(\n            1,\n            H // self.split_size[0],\n            self.split_size[0],\n            W // self.split_size[1],\n            self.split_size[1],\n            1,\n        )\n        img_mask_0 = (\n            img_mask_0.permute(0, 1, 3, 2, 4, 5)\n            .contiguous()\n            .view(-1, self.split_size[0], self.split_size[1], 1)\n        )  # nW, sw[0], sw[1], 1\n        mask_windows_0 = img_mask_0.view(-1, self.split_size[0] * self.split_size[1])\n        attn_mask_0 = mask_windows_0.unsqueeze(1) - mask_windows_0.unsqueeze(2)\n        attn_mask_0 = attn_mask_0.masked_fill(\n            attn_mask_0 != 0, float(-100.0)\n        ).masked_fill(attn_mask_0 == 0, float(0.0))\n\n        # calculate mask for window-1\n        img_mask_1 = img_mask_1.view(\n            1,\n            H // self.split_size[1],\n            self.split_size[1],\n            W // self.split_size[0],\n            self.split_size[0],\n            1,\n        )\n        img_mask_1 = (\n            img_mask_1.permute(0, 1, 3, 2, 4, 5)\n            .contiguous()\n            .view(-1, self.split_size[1], self.split_size[0], 1)\n        )  # nW, sw[1], sw[0], 1\n        mask_windows_1 = img_mask_1.view(-1, self.split_size[1] * self.split_size[0])\n        attn_mask_1 = mask_windows_1.unsqueeze(1) - mask_windows_1.unsqueeze(2)\n        attn_mask_1 = attn_mask_1.masked_fill(\n            attn_mask_1 != 0, float(-100.0)\n        ).masked_fill(attn_mask_1 == 0, float(0.0))\n\n        return attn_mask_0, attn_mask_1\n\n    def forward(self, x, H, W):\n        \"\"\"\n        Input: x: (B, H*W, C), H, W\n        Output: x: (B, H*W, C)\n        \"\"\"\n        B, L, C = x.shape\n        assert L == H * W, \"flatten img_tokens has wrong size\"\n\n        qkv = self.qkv(x).reshape(B, -1, 3, C).permute(2, 0, 1, 3)  # 3, B, HW, C\n        # V without partition\n        v = qkv[2].transpose(-2, -1).contiguous().view(B, C, H, W)\n\n        # image padding\n        max_split_size = max(self.split_size[0], self.split_size[1])\n        pad_l = pad_t = 0\n        pad_r = (max_split_size - W % max_split_size) % max_split_size\n        pad_b = (max_split_size - H % max_split_size) % max_split_size\n\n        qkv = qkv.reshape(3 * B, H, W, C).permute(0, 3, 1, 2)  # 3B C H W\n        qkv = (\n            F.pad(qkv, (pad_l, pad_r, pad_t, pad_b))\n            .reshape(3, B, C, -1)\n            .transpose(-2, -1)\n        )  # l r t b\n        _H = pad_b + H\n        _W = pad_r + W\n        _L = _H * _W\n\n        # window-0 and window-1 on split channels [C/2, C/2]; for square windows (e.g., 8x8), window-0 and window-1 can be merged\n        # shift in block: (0, 4, 8, ...), (2, 6, 10, ...), (0, 4, 8, ...), (2, 6, 10, ...), ...\n        if (self.rg_idx % 2 == 0 and self.b_idx > 0 and (self.b_idx - 2) % 4 == 0) or (\n            self.rg_idx % 2 != 0 and self.b_idx % 4 == 0\n        ):\n            qkv = qkv.view(3, B, _H, _W, C)\n            qkv_0 = torch.roll(\n                qkv[:, :, :, :, : C // 2],\n                shifts=(-self.shift_size[0], -self.shift_size[1]),\n                dims=(2, 3),\n            )\n            qkv_0 = qkv_0.view(3, B, _L, C // 2)\n            qkv_1 = torch.roll(\n                qkv[:, :, :, :, C // 2 :],\n                shifts=(-self.shift_size[1], -self.shift_size[0]),\n                dims=(2, 3),\n            )\n            qkv_1 = qkv_1.view(3, B, _L, C // 2)\n\n            if self.patches_resolution != _H or self.patches_resolution != _W:\n                mask_tmp = self.calculate_mask(_H, _W)\n                x1_shift = self.attns[0](qkv_0, _H, _W, mask=mask_tmp[0].to(x.device))\n                x2_shift = self.attns[1](qkv_1, _H, _W, mask=mask_tmp[1].to(x.device))\n            else:\n                x1_shift = self.attns[0](qkv_0, _H, _W, mask=self.attn_mask_0)\n                x2_shift = self.attns[1](qkv_1, _H, _W, mask=self.attn_mask_1)\n\n            x1 = torch.roll(\n                x1_shift, shifts=(self.shift_size[0], self.shift_size[1]), dims=(1, 2)\n            )\n            x2 = torch.roll(\n                x2_shift, shifts=(self.shift_size[1], self.shift_size[0]), dims=(1, 2)\n            )\n            x1 = x1[:, :H, :W, :].reshape(B, L, C // 2)\n            x2 = x2[:, :H, :W, :].reshape(B, L, C // 2)\n            # attention output\n            attened_x = torch.cat([x1, x2], dim=2)\n\n        else:\n            x1 = self.attns[0](qkv[:, :, :, : C // 2], _H, _W)[:, :H, :W, :].reshape(\n                B, L, C // 2\n            )\n            x2 = self.attns[1](qkv[:, :, :, C // 2 :], _H, _W)[:, :H, :W, :].reshape(\n                B, L, C // 2\n            )\n            # attention output\n            attened_x = torch.cat([x1, x2], dim=2)\n\n        # convolution output\n        conv_x = self.dwconv(v)\n\n        # Adaptive Interaction Module (AIM)\n        # C-Map (before sigmoid)\n        channel_map = (\n            self.channel_interaction(conv_x)\n            .permute(0, 2, 3, 1)\n            .contiguous()\n            .view(B, 1, C)\n        )\n        # S-Map (before sigmoid)\n        attention_reshape = attened_x.transpose(-2, -1).contiguous().view(B, C, H, W)\n        spatial_map = self.spatial_interaction(attention_reshape)\n\n        # C-I\n        attened_x = attened_x * torch.sigmoid(channel_map)\n        # S-I\n        conv_x = torch.sigmoid(spatial_map) * conv_x\n        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(B, L, C)\n\n        x = attened_x + conv_x\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Adaptive_Channel_Attention(nn.Module):\n    # The implementation builds on XCiT code https://github.com/facebookresearch/xcit\n    \"\"\"Adaptive Channel Self-Attention\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads. Default: 6\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set.\n        attn_drop (float): Attention dropout rate. Default: 0.0\n        drop_path (float): Stochastic depth rate. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.dwconv = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim),\n            nn.BatchNorm2d(dim),\n            nn.GELU(),\n        )\n        self.channel_interaction = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, dim // 8, kernel_size=1),\n            nn.BatchNorm2d(dim // 8),\n            nn.GELU(),\n            nn.Conv2d(dim // 8, dim, kernel_size=1),\n        )\n        self.spatial_interaction = nn.Sequential(\n            nn.Conv2d(dim, dim // 16, kernel_size=1),\n            nn.BatchNorm2d(dim // 16),\n            nn.GELU(),\n            nn.Conv2d(dim // 16, 1, kernel_size=1),\n        )\n\n    def forward(self, x, H, W):\n        \"\"\"\n        Input: x: (B, H*W, C), H, W\n        Output: x: (B, H*W, C)\n        \"\"\"\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n\n        v_ = v.reshape(B, C, N).contiguous().view(B, C, H, W)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        # attention output\n        attened_x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n\n        # convolution output\n        conv_x = self.dwconv(v_)\n\n        # Adaptive Interaction Module (AIM)\n        # C-Map (before sigmoid)\n        attention_reshape = attened_x.transpose(-2, -1).contiguous().view(B, C, H, W)\n        channel_map = self.channel_interaction(attention_reshape)\n        # S-Map (before sigmoid)\n        spatial_map = (\n            self.spatial_interaction(conv_x)\n            .permute(0, 2, 3, 1)\n            .contiguous()\n            .view(B, N, 1)\n        )\n\n        # S-I\n        attened_x = attened_x * torch.sigmoid(spatial_map)\n        # C-I\n        conv_x = conv_x * torch.sigmoid(channel_map)\n        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(B, N, C)\n\n        x = attened_x + conv_x\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass DATB(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        reso=64,\n        split_size=[2, 4],\n        shift_size=[1, 2],\n        expansion_factor=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        rg_idx=0,\n        b_idx=0,\n    ):\n        super().__init__()\n\n        self.norm1 = norm_layer(dim)\n\n        if b_idx % 2 == 0:\n            # DSTB\n            self.attn = Adaptive_Spatial_Attention(\n                dim,\n                num_heads=num_heads,\n                reso=reso,\n                split_size=split_size,\n                shift_size=shift_size,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop,\n                attn_drop=attn_drop,\n                rg_idx=rg_idx,\n                b_idx=b_idx,\n            )\n        else:\n            # DCTB\n            self.attn = Adaptive_Channel_Attention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                proj_drop=drop,\n            )\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        ffn_hidden_dim = int(dim * expansion_factor)\n        self.ffn = SGFN(\n            in_features=dim,\n            hidden_features=ffn_hidden_dim,\n            out_features=dim,\n            act_layer=act_layer,\n        )\n        self.norm2 = norm_layer(dim)\n\n    def forward(self, x, x_size):\n        \"\"\"\n        Input: x: (B, H*W, C), x_size: (H, W)\n        Output: x: (B, H*W, C)\n        \"\"\"\n        H, W = x_size\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.ffn(self.norm2(x), H, W))\n\n        return x\n\n\nclass ResidualGroup(nn.Module):\n    \"\"\"ResidualGroup\n    Args:\n        dim (int): Number of input channels.\n        reso (int): Input resolution.\n        num_heads (int): Number of attention heads.\n        split_size (tuple(int)): Height and Width of spatial window.\n        expansion_factor (float): Ratio of ffn hidden dim to embedding dim.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop (float): Dropout rate. Default: 0\n        attn_drop(float): Attention dropout rate. Default: 0\n        drop_paths (float | None): Stochastic depth rate.\n        act_layer (nn.Module): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm\n        depth (int): Number of dual aggregation Transformer blocks in residual group.\n        use_chk (bool): Whether to use checkpointing to save memory.\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        reso,\n        num_heads,\n        split_size=[2, 4],\n        expansion_factor=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_paths=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        depth=2,\n        use_chk=False,\n        resi_connection=\"1conv\",\n        rg_idx=0,\n    ):\n        super().__init__()\n        self.use_chk = use_chk\n        self.reso = reso\n\n        self.blocks = nn.ModuleList(\n            [\n                DATB(\n                    dim=dim,\n                    num_heads=num_heads,\n                    reso=reso,\n                    split_size=split_size,\n                    shift_size=[split_size[0] // 2, split_size[1] // 2],\n                    expansion_factor=expansion_factor,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_paths[i],\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    rg_idx=rg_idx,\n                    b_idx=i,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        if resi_connection == \"1conv\":\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            self.conv = nn.Sequential(\n                nn.Conv2d(dim, dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim, 3, 1, 1),\n            )\n\n    def forward(self, x, x_size):\n        \"\"\"\n        Input: x: (B, H*W, C), x_size: (H, W)\n        Output: x: (B, H*W, C)\n        \"\"\"\n        H, W = x_size\n        res = x\n        for blk in self.blocks:\n            if self.use_chk:\n                x = checkpoint.checkpoint(blk, x, x_size)\n            else:\n                x = blk(x, x_size)\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=H, w=W)\n        x = self.conv(x)\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n        x = res + x\n\n        return x\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n            )\n        super(Upsample, self).__init__(*m)\n\n\nclass UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n\n    \"\"\"\n\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution\n        m = []\n        m.append(nn.Conv2d(num_feat, (scale**2) * num_out_ch, 3, 1, 1))\n        m.append(nn.PixelShuffle(scale))\n        super(UpsampleOneStep, self).__init__(*m)\n\n    def flops(self):\n        h, w = self.input_resolution\n        flops = h * w * self.num_feat * 3 * 9\n        return flops\n\n\nclass DAT(nn.Module):\n    \"\"\"Dual Aggregation Transformer\n    Args:\n        img_size (int): Input image size. Default: 64\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 180\n        depths (tuple(int)): Depth of each residual group (number of DATB in each RG).\n        split_size (tuple(int)): Height and Width of spatial window.\n        num_heads (tuple(int)): Number of attention heads in different residual groups.\n        expansion_factor (float): Ratio of ffn hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        act_layer (nn.Module): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm\n        use_chk (bool): Whether to use checkpointing to save memory.\n        upscale: Upscale factor. 2/3/4 for image SR\n        img_range: Image range. 1. or 255.\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(self, state_dict):\n        super().__init__()\n\n        # defaults\n        img_size = 64\n        in_chans = 3\n        embed_dim = 180\n        split_size = [2, 4]\n        depth = [2, 2, 2, 2]\n        num_heads = [2, 2, 2, 2]\n        expansion_factor = 4.0\n        qkv_bias = True\n        qk_scale = None\n        drop_rate = 0.0\n        attn_drop_rate = 0.0\n        drop_path_rate = 0.1\n        act_layer = nn.GELU\n        norm_layer = nn.LayerNorm\n        use_chk = False\n        upscale = 2\n        img_range = 1.0\n        resi_connection = \"1conv\"\n        upsampler = \"pixelshuffle\"\n\n        self.model_arch = \"DAT\"\n        self.sub_type = \"SR\"\n        self.state = state_dict\n\n        state_keys = state_dict.keys()\n        if \"conv_before_upsample.0.weight\" in state_keys:\n            if \"conv_up1.weight\" in state_keys:\n                upsampler = \"nearest+conv\"\n            else:\n                upsampler = \"pixelshuffle\"\n                supports_fp16 = False\n        elif \"upsample.0.weight\" in state_keys:\n            upsampler = \"pixelshuffledirect\"\n        else:\n            upsampler = \"\"\n\n        num_feat = (\n            state_dict.get(\"conv_before_upsample.0.weight\", None).shape[1]\n            if state_dict.get(\"conv_before_upsample.weight\", None)\n            else 64\n        )\n\n        num_in_ch = state_dict[\"conv_first.weight\"].shape[1]\n        in_chans = num_in_ch\n        if \"conv_last.weight\" in state_keys:\n            num_out_ch = state_dict[\"conv_last.weight\"].shape[0]\n        else:\n            num_out_ch = num_in_ch\n\n        upscale = 1\n        if upsampler == \"nearest+conv\":\n            upsample_keys = [\n                x for x in state_keys if \"conv_up\" in x and \"bias\" not in x\n            ]\n\n            for upsample_key in upsample_keys:\n                upscale *= 2\n        elif upsampler == \"pixelshuffle\":\n            upsample_keys = [\n                x\n                for x in state_keys\n                if \"upsample\" in x and \"conv\" not in x and \"bias\" not in x\n            ]\n            for upsample_key in upsample_keys:\n                shape = state_dict[upsample_key].shape[0]\n                upscale *= math.sqrt(shape // num_feat)\n            upscale = int(upscale)\n        elif upsampler == \"pixelshuffledirect\":\n            upscale = int(\n                math.sqrt(state_dict[\"upsample.0.bias\"].shape[0] // num_out_ch)\n            )\n\n        max_layer_num = 0\n        max_block_num = 0\n        for key in state_keys:\n            result = re.match(r\"layers.(\\d*).blocks.(\\d*).norm1.weight\", key)\n            if result:\n                layer_num, block_num = result.groups()\n                max_layer_num = max(max_layer_num, int(layer_num))\n                max_block_num = max(max_block_num, int(block_num))\n\n        depth = [max_block_num + 1 for _ in range(max_layer_num + 1)]\n\n        if \"layers.0.blocks.1.attn.temperature\" in state_keys:\n            num_heads_num = state_dict[\"layers.0.blocks.1.attn.temperature\"].shape[0]\n            num_heads = [num_heads_num for _ in range(max_layer_num + 1)]\n        else:\n            num_heads = depth\n\n        embed_dim = state_dict[\"conv_first.weight\"].shape[0]\n        expansion_factor = float(\n            state_dict[\"layers.0.blocks.0.ffn.fc1.weight\"].shape[0] / embed_dim\n        )\n\n        # TODO: could actually count the layers, but this should do\n        if \"layers.0.conv.4.weight\" in state_keys:\n            resi_connection = \"3conv\"\n        else:\n            resi_connection = \"1conv\"\n\n        if \"layers.0.blocks.2.attn.attn_mask_0\" in state_keys:\n            attn_mask_0_x, attn_mask_0_y, attn_mask_0_z = state_dict[\n                \"layers.0.blocks.2.attn.attn_mask_0\"\n            ].shape\n\n            img_size = int(math.sqrt(attn_mask_0_x * attn_mask_0_y))\n\n        if \"layers.0.blocks.0.attn.attns.0.rpe_biases\" in state_keys:\n            split_sizes = (\n                state_dict[\"layers.0.blocks.0.attn.attns.0.rpe_biases\"][-1] + 1\n            )\n            split_size = [int(x) for x in split_sizes]\n\n        self.in_nc = num_in_ch\n        self.out_nc = num_out_ch\n        self.num_feat = num_feat\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.depth = depth\n        self.scale = upscale\n        self.upsampler = upsampler\n        self.img_size = img_size\n        self.img_range = img_range\n        self.expansion_factor = expansion_factor\n        self.resi_connection = resi_connection\n        self.split_size = split_size\n\n        self.supports_fp16 = False  # Too much weirdness to support this at the moment\n        self.supports_bfp16 = True\n        self.min_size_restriction = 16\n\n        num_in_ch = in_chans\n        num_out_ch = in_chans\n        num_feat = 64\n        self.img_range = img_range\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n\n        # ------------------------- 1, Shallow Feature Extraction ------------------------- #\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        # ------------------------- 2, Deep Feature Extraction ------------------------- #\n        self.num_layers = len(depth)\n        self.use_chk = use_chk\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n        heads = num_heads\n\n        self.before_RG = nn.Sequential(\n            Rearrange(\"b c h w -> b (h w) c\"), nn.LayerNorm(embed_dim)\n        )\n\n        curr_dim = embed_dim\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))\n        ]  # stochastic depth decay rule\n\n        self.layers = nn.ModuleList()\n        for i in range(self.num_layers):\n            layer = ResidualGroup(\n                dim=embed_dim,\n                num_heads=heads[i],\n                reso=img_size,\n                split_size=split_size,\n                expansion_factor=expansion_factor,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_paths=dpr[sum(depth[:i]) : sum(depth[: i + 1])],\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                depth=depth[i],\n                use_chk=use_chk,\n                resi_connection=resi_connection,\n                rg_idx=i,\n            )\n            self.layers.append(layer)\n\n        self.norm = norm_layer(curr_dim)\n        # build the last conv layer in deep feature extraction\n        if resi_connection == \"1conv\":\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            # to save parameters and memory\n            self.conv_after_body = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1),\n            )\n\n        # ------------------------- 3, Reconstruction ------------------------- #\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR (to save parameters)\n            self.upsample = UpsampleOneStep(\n                upscale, embed_dim, num_out_ch, (img_size, img_size)\n            )\n\n        self.apply(self._init_weights)\n        self.load_state_dict(state_dict, strict=True)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(\n            m, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm, nn.InstanceNorm2d)\n        ):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_features(self, x):\n        _, _, H, W = x.shape\n        x_size = [H, W]\n        x = self.before_RG(x)\n        for layer in self.layers:\n            x = layer(x, x_size)\n        x = self.norm(x)\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=H, w=W)\n\n        return x\n\n    def forward(self, x):\n        \"\"\"\n        Input: x: (B, C, H, W)\n        \"\"\"\n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n\n        if self.upsampler == \"pixelshuffle\":\n            # for image SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.upsample(x)\n\n        x = x / self.img_range + self.mean\n        return x\n", "ldm_patched/pfn/architecture/RRDB.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport functools\nimport math\nimport re\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import block as B\n\n\n# Borrowed from https://github.com/rlaphoenix/VSGAN/blob/master/vsgan/archs/esrgan.py\n# Which enhanced stuff that was already here\nclass RRDBNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        norm=None,\n        act: str = \"leakyrelu\",\n        upsampler: str = \"upconv\",\n        mode: B.ConvMode = \"CNA\",\n    ) -> None:\n        \"\"\"\n        ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks.\n        By Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao,\n        and Chen Change Loy.\n        This is old-arch Residual in Residual Dense Block Network and is not\n        the newest revision that's available at github.com/xinntao/ESRGAN.\n        This is on purpose, the newest Network has severely limited the\n        potential use of the Network with no benefits.\n        This network supports model files from both new and old-arch.\n        Args:\n            norm: Normalization layer\n            act: Activation layer\n            upsampler: Upsample layer. upconv, pixel_shuffle\n            mode: Convolution mode\n        \"\"\"\n        super(RRDBNet, self).__init__()\n        self.model_arch = \"ESRGAN\"\n        self.sub_type = \"SR\"\n\n        self.state = state_dict\n        self.norm = norm\n        self.act = act\n        self.upsampler = upsampler\n        self.mode = mode\n\n        self.state_map = {\n            # currently supports old, new, and newer RRDBNet arch models\n            # ESRGAN, BSRGAN/RealSR, Real-ESRGAN\n            \"model.0.weight\": (\"conv_first.weight\",),\n            \"model.0.bias\": (\"conv_first.bias\",),\n            \"model.1.sub./NB/.weight\": (\"trunk_conv.weight\", \"conv_body.weight\"),\n            \"model.1.sub./NB/.bias\": (\"trunk_conv.bias\", \"conv_body.bias\"),\n            r\"model.1.sub.\\1.RDB\\2.conv\\3.0.\\4\": (\n                r\"RRDB_trunk\\.(\\d+)\\.RDB(\\d)\\.conv(\\d+)\\.(weight|bias)\",\n                r\"body\\.(\\d+)\\.rdb(\\d)\\.conv(\\d+)\\.(weight|bias)\",\n            ),\n        }\n        if \"params_ema\" in self.state:\n            self.state = self.state[\"params_ema\"]\n            # self.model_arch = \"RealESRGAN\"\n        self.num_blocks = self.get_num_blocks()\n        self.plus = any(\"conv1x1\" in k for k in self.state.keys())\n        if self.plus:\n            self.model_arch = \"ESRGAN+\"\n\n        self.state = self.new_to_old_arch(self.state)\n\n        self.key_arr = list(self.state.keys())\n\n        self.in_nc: int = self.state[self.key_arr[0]].shape[1]\n        self.out_nc: int = self.state[self.key_arr[-1]].shape[0]\n\n        self.scale: int = self.get_scale()\n        self.num_filters: int = self.state[self.key_arr[0]].shape[0]\n\n        c2x2 = False\n        if self.state[\"model.0.weight\"].shape[-2] == 2:\n            c2x2 = True\n            self.scale = round(math.sqrt(self.scale / 4))\n            self.model_arch = \"ESRGAN-2c2\"\n\n        self.supports_fp16 = True\n        self.supports_bfp16 = True\n        self.min_size_restriction = None\n\n        # Detect if pixelunshuffle was used (Real-ESRGAN)\n        if self.in_nc in (self.out_nc * 4, self.out_nc * 16) and self.out_nc in (\n            self.in_nc / 4,\n            self.in_nc / 16,\n        ):\n            self.shuffle_factor = int(math.sqrt(self.in_nc / self.out_nc))\n        else:\n            self.shuffle_factor = None\n\n        upsample_block = {\n            \"upconv\": B.upconv_block,\n            \"pixel_shuffle\": B.pixelshuffle_block,\n        }.get(self.upsampler)\n        if upsample_block is None:\n            raise NotImplementedError(f\"Upsample mode [{self.upsampler}] is not found\")\n\n        if self.scale == 3:\n            upsample_blocks = upsample_block(\n                in_nc=self.num_filters,\n                out_nc=self.num_filters,\n                upscale_factor=3,\n                act_type=self.act,\n                c2x2=c2x2,\n            )\n        else:\n            upsample_blocks = [\n                upsample_block(\n                    in_nc=self.num_filters,\n                    out_nc=self.num_filters,\n                    act_type=self.act,\n                    c2x2=c2x2,\n                )\n                for _ in range(int(math.log(self.scale, 2)))\n            ]\n\n        self.model = B.sequential(\n            # fea conv\n            B.conv_block(\n                in_nc=self.in_nc,\n                out_nc=self.num_filters,\n                kernel_size=3,\n                norm_type=None,\n                act_type=None,\n                c2x2=c2x2,\n            ),\n            B.ShortcutBlock(\n                B.sequential(\n                    # rrdb blocks\n                    *[\n                        B.RRDB(\n                            nf=self.num_filters,\n                            kernel_size=3,\n                            gc=32,\n                            stride=1,\n                            bias=True,\n                            pad_type=\"zero\",\n                            norm_type=self.norm,\n                            act_type=self.act,\n                            mode=\"CNA\",\n                            plus=self.plus,\n                            c2x2=c2x2,\n                        )\n                        for _ in range(self.num_blocks)\n                    ],\n                    # lr conv\n                    B.conv_block(\n                        in_nc=self.num_filters,\n                        out_nc=self.num_filters,\n                        kernel_size=3,\n                        norm_type=self.norm,\n                        act_type=None,\n                        mode=self.mode,\n                        c2x2=c2x2,\n                    ),\n                )\n            ),\n            *upsample_blocks,\n            # hr_conv0\n            B.conv_block(\n                in_nc=self.num_filters,\n                out_nc=self.num_filters,\n                kernel_size=3,\n                norm_type=None,\n                act_type=self.act,\n                c2x2=c2x2,\n            ),\n            # hr_conv1\n            B.conv_block(\n                in_nc=self.num_filters,\n                out_nc=self.out_nc,\n                kernel_size=3,\n                norm_type=None,\n                act_type=None,\n                c2x2=c2x2,\n            ),\n        )\n\n        # Adjust these properties for calculations outside of the model\n        if self.shuffle_factor:\n            self.in_nc //= self.shuffle_factor**2\n            self.scale //= self.shuffle_factor\n\n        self.load_state_dict(self.state, strict=False)\n\n    def new_to_old_arch(self, state):\n        \"\"\"Convert a new-arch model state dictionary to an old-arch dictionary.\"\"\"\n        if \"params_ema\" in state:\n            state = state[\"params_ema\"]\n\n        if \"conv_first.weight\" not in state:\n            # model is already old arch, this is a loose check, but should be sufficient\n            return state\n\n        # add nb to state keys\n        for kind in (\"weight\", \"bias\"):\n            self.state_map[f\"model.1.sub.{self.num_blocks}.{kind}\"] = self.state_map[\n                f\"model.1.sub./NB/.{kind}\"\n            ]\n            del self.state_map[f\"model.1.sub./NB/.{kind}\"]\n\n        old_state = OrderedDict()\n        for old_key, new_keys in self.state_map.items():\n            for new_key in new_keys:\n                if r\"\\1\" in old_key:\n                    for k, v in state.items():\n                        sub = re.sub(new_key, old_key, k)\n                        if sub != k:\n                            old_state[sub] = v\n                else:\n                    if new_key in state:\n                        old_state[old_key] = state[new_key]\n\n        # upconv layers\n        max_upconv = 0\n        for key in state.keys():\n            match = re.match(r\"(upconv|conv_up)(\\d)\\.(weight|bias)\", key)\n            if match is not None:\n                _, key_num, key_type = match.groups()\n                old_state[f\"model.{int(key_num) * 3}.{key_type}\"] = state[key]\n                max_upconv = max(max_upconv, int(key_num) * 3)\n\n        # final layers\n        for key in state.keys():\n            if key in (\"HRconv.weight\", \"conv_hr.weight\"):\n                old_state[f\"model.{max_upconv + 2}.weight\"] = state[key]\n            elif key in (\"HRconv.bias\", \"conv_hr.bias\"):\n                old_state[f\"model.{max_upconv + 2}.bias\"] = state[key]\n            elif key in (\"conv_last.weight\",):\n                old_state[f\"model.{max_upconv + 4}.weight\"] = state[key]\n            elif key in (\"conv_last.bias\",):\n                old_state[f\"model.{max_upconv + 4}.bias\"] = state[key]\n\n        # Sort by first numeric value of each layer\n        def compare(item1, item2):\n            parts1 = item1.split(\".\")\n            parts2 = item2.split(\".\")\n            int1 = int(parts1[1])\n            int2 = int(parts2[1])\n            return int1 - int2\n\n        sorted_keys = sorted(old_state.keys(), key=functools.cmp_to_key(compare))\n\n        # Rebuild the output dict in the right order\n        out_dict = OrderedDict((k, old_state[k]) for k in sorted_keys)\n\n        return out_dict\n\n    def get_scale(self, min_part: int = 6) -> int:\n        n = 0\n        for part in list(self.state):\n            parts = part.split(\".\")[1:]\n            if len(parts) == 2:\n                part_num = int(parts[0])\n                if part_num > min_part and parts[1] == \"weight\":\n                    n += 1\n        return 2**n\n\n    def get_num_blocks(self) -> int:\n        nbs = []\n        state_keys = self.state_map[r\"model.1.sub.\\1.RDB\\2.conv\\3.0.\\4\"] + (\n            r\"model\\.\\d+\\.sub\\.(\\d+)\\.RDB(\\d+)\\.conv(\\d+)\\.0\\.(weight|bias)\",\n        )\n        for state_key in state_keys:\n            for k in self.state:\n                m = re.search(state_key, k)\n                if m:\n                    nbs.append(int(m.group(1)))\n            if nbs:\n                break\n        return max(*nbs) + 1\n\n    def forward(self, x):\n        if self.shuffle_factor:\n            _, _, h, w = x.size()\n            mod_pad_h = (\n                self.shuffle_factor - h % self.shuffle_factor\n            ) % self.shuffle_factor\n            mod_pad_w = (\n                self.shuffle_factor - w % self.shuffle_factor\n            ) % self.shuffle_factor\n            x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"reflect\")\n            x = torch.pixel_unshuffle(x, downscale_factor=self.shuffle_factor)\n            x = self.model(x)\n            return x[:, :, : h * self.scale, : w * self.scale]\n        return self.model(x)\n", "ldm_patched/pfn/architecture/SwinIR.py": "# pylint: skip-file\n# -----------------------------------------------------------------------------------\n# SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257\n# Originally Written by Ze Liu, Modified by Jingyun Liang.\n# -----------------------------------------------------------------------------------\n\nimport math\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\n# Originally from the timm package\nfrom .timm.drop import DropPath\nfrom .timm.helpers import to_2tuple\nfrom .timm.weight_init import trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(\n        B, H // window_size, W // window_size, window_size, window_size, -1\n    )\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=True,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(  # type: ignore\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n        )  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index.view(-1)  # type: ignore\n        ].view(\n            self.window_size[0] * self.window_size[1],\n            self.window_size[0] * self.window_size[1],\n            -1,\n        )  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert (\n            0 <= self.shift_size < self.window_size\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim,\n            window_size=to_2tuple(self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n        if self.shift_size > 0:\n            attn_mask = self.calculate_mask(self.input_resolution)\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def calculate_mask(self, x_size):\n        # calculate attention mask for SW-MSA\n        H, W = x_size\n        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n        h_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        w_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        return attn_mask\n\n    def forward(self, x, x_size):\n        H, W = x_size\n        B, L, C = x.shape\n        # assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(\n                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n            )\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(\n            shifted_x, self.window_size\n        )  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(\n            -1, self.window_size * self.window_size, C\n        )  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n        if self.input_resolution == x_size:\n            attn_windows = self.attn(\n                x_windows, mask=self.attn_mask\n            )  # nW*B, window_size*window_size, C\n        else:\n            attn_windows = self.attn(\n                x_windows, mask=self.calculate_mask(x_size).to(x.device)\n            )\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(\n                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n            )\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return (\n            f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \"\n            f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n        )\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.dim\n        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock(\n                    dim=dim,\n                    input_resolution=input_resolution,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, norm_layer=norm_layer\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x, x_size):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, x_size)\n            else:\n                x = blk(x, x_size)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()  # type: ignore\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n\nclass RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        img_size: Input image size.\n        patch_size: Patch size.\n        resi_connection: The convolutional block before residual connection.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n        img_size=224,\n        patch_size=4,\n        resi_connection=\"1conv\",\n    ):\n        super(RSTB, self).__init__()\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n\n        self.residual_group = BasicLayer(\n            dim=dim,\n            input_resolution=input_resolution,\n            depth=depth,\n            num_heads=num_heads,\n            window_size=window_size,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            norm_layer=norm_layer,\n            downsample=downsample,\n            use_checkpoint=use_checkpoint,\n        )\n\n        if resi_connection == \"1conv\":\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            # to save parameters and memory\n            self.conv = nn.Sequential(\n                nn.Conv2d(dim, dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim, 3, 1, 1),\n            )\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=0,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=0,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n    def forward(self, x, x_size):\n        return (\n            self.patch_embed(\n                self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))\n            )\n            + x\n        )\n\n    def flops(self):\n        flops = 0\n        flops += self.residual_group.flops()\n        H, W = self.input_resolution\n        flops += H * W * self.dim * self.dim * 9\n        flops += self.patch_embed.flops()\n        flops += self.patch_unembed.flops()\n\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [\n            img_size[0] // patch_size[0],  # type: ignore\n            img_size[1] // patch_size[1],  # type: ignore\n        ]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        flops = 0\n        H, W = self.img_size\n        if self.norm is not None:\n            flops += H * W * self.embed_dim  # type: ignore\n        return flops\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [\n            img_size[0] // patch_size[0],  # type: ignore\n            img_size[1] // patch_size[1],  # type: ignore\n        ]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n    def forward(self, x, x_size):\n        B, HW, C = x.shape\n        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n        return x\n\n    def flops(self):\n        flops = 0\n        return flops\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n            )\n        super(Upsample, self).__init__(*m)\n\n\nclass UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n\n    \"\"\"\n\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution\n        m = []\n        m.append(nn.Conv2d(num_feat, (scale**2) * num_out_ch, 3, 1, 1))\n        m.append(nn.PixelShuffle(scale))\n        super(UpsampleOneStep, self).__init__(*m)\n\n    def flops(self):\n        H, W = self.input_resolution  # type: ignore\n        flops = H * W * self.num_feat * 3 * 9\n        return flops\n\n\nclass SwinIR(nn.Module):\n    r\"\"\"SwinIR\n        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n        img_range: Image range. 1. or 255.\n        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n        **kwargs,\n    ):\n        super(SwinIR, self).__init__()\n\n        # Defaults\n        img_size = 64\n        patch_size = 1\n        in_chans = 3\n        embed_dim = 96\n        depths = [6, 6, 6, 6]\n        num_heads = [6, 6, 6, 6]\n        window_size = 7\n        mlp_ratio = 4.0\n        qkv_bias = True\n        qk_scale = None\n        drop_rate = 0.0\n        attn_drop_rate = 0.0\n        drop_path_rate = 0.1\n        norm_layer = nn.LayerNorm\n        ape = False\n        patch_norm = True\n        use_checkpoint = False\n        upscale = 2\n        img_range = 1.0\n        upsampler = \"\"\n        resi_connection = \"1conv\"\n        num_feat = 64\n        num_in_ch = in_chans\n        num_out_ch = in_chans\n        supports_fp16 = True\n        self.start_unshuffle = 1\n\n        self.model_arch = \"SwinIR\"\n        self.sub_type = \"SR\"\n        self.state = state_dict\n        if \"params_ema\" in self.state:\n            self.state = self.state[\"params_ema\"]\n        elif \"params\" in self.state:\n            self.state = self.state[\"params\"]\n\n        state_keys = self.state.keys()\n\n        if \"conv_before_upsample.0.weight\" in state_keys:\n            if \"conv_up1.weight\" in state_keys:\n                upsampler = \"nearest+conv\"\n            else:\n                upsampler = \"pixelshuffle\"\n                supports_fp16 = False\n        elif \"upsample.0.weight\" in state_keys:\n            upsampler = \"pixelshuffledirect\"\n        else:\n            upsampler = \"\"\n\n        num_feat = (\n            self.state.get(\"conv_before_upsample.0.weight\", None).shape[1]\n            if self.state.get(\"conv_before_upsample.weight\", None)\n            else 64\n        )\n\n        if \"conv_first.1.weight\" in self.state:\n            self.state[\"conv_first.weight\"] = self.state.pop(\"conv_first.1.weight\")\n            self.state[\"conv_first.bias\"] = self.state.pop(\"conv_first.1.bias\")\n            self.start_unshuffle = round(math.sqrt(self.state[\"conv_first.weight\"].shape[1] // 3))\n\n        num_in_ch = self.state[\"conv_first.weight\"].shape[1]\n        in_chans = num_in_ch\n        if \"conv_last.weight\" in state_keys:\n            num_out_ch = self.state[\"conv_last.weight\"].shape[0]\n        else:\n            num_out_ch = num_in_ch\n\n        upscale = 1\n        if upsampler == \"nearest+conv\":\n            upsample_keys = [\n                x for x in state_keys if \"conv_up\" in x and \"bias\" not in x\n            ]\n\n            for upsample_key in upsample_keys:\n                upscale *= 2\n        elif upsampler == \"pixelshuffle\":\n            upsample_keys = [\n                x\n                for x in state_keys\n                if \"upsample\" in x and \"conv\" not in x and \"bias\" not in x\n            ]\n            for upsample_key in upsample_keys:\n                shape = self.state[upsample_key].shape[0]\n                upscale *= math.sqrt(shape // num_feat)\n            upscale = int(upscale)\n        elif upsampler == \"pixelshuffledirect\":\n            upscale = int(\n                math.sqrt(self.state[\"upsample.0.bias\"].shape[0] // num_out_ch)\n            )\n\n        max_layer_num = 0\n        max_block_num = 0\n        for key in state_keys:\n            result = re.match(\n                r\"layers.(\\d*).residual_group.blocks.(\\d*).norm1.weight\", key\n            )\n            if result:\n                layer_num, block_num = result.groups()\n                max_layer_num = max(max_layer_num, int(layer_num))\n                max_block_num = max(max_block_num, int(block_num))\n\n        depths = [max_block_num + 1 for _ in range(max_layer_num + 1)]\n\n        if (\n            \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            in state_keys\n        ):\n            num_heads_num = self.state[\n                \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            ].shape[-1]\n            num_heads = [num_heads_num for _ in range(max_layer_num + 1)]\n        else:\n            num_heads = depths\n\n        embed_dim = self.state[\"conv_first.weight\"].shape[0]\n\n        mlp_ratio = float(\n            self.state[\"layers.0.residual_group.blocks.0.mlp.fc1.bias\"].shape[0]\n            / embed_dim\n        )\n\n        # TODO: could actually count the layers, but this should do\n        if \"layers.0.conv.4.weight\" in state_keys:\n            resi_connection = \"3conv\"\n        else:\n            resi_connection = \"1conv\"\n\n        window_size = int(\n            math.sqrt(\n                self.state[\n                    \"layers.0.residual_group.blocks.0.attn.relative_position_index\"\n                ].shape[0]\n            )\n        )\n\n        if \"layers.0.residual_group.blocks.1.attn_mask\" in state_keys:\n            img_size = int(\n                math.sqrt(\n                    self.state[\"layers.0.residual_group.blocks.1.attn_mask\"].shape[0]\n                )\n                * window_size\n            )\n\n        # The JPEG models are the only ones with window-size 7, and they also use this range\n        img_range = 255.0 if window_size == 7 else 1.0\n\n        self.in_nc = num_in_ch\n        self.out_nc = num_out_ch\n        self.num_feat = num_feat\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.depths = depths\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n        self.scale = upscale / self.start_unshuffle\n        self.upsampler = upsampler\n        self.img_size = img_size\n        self.img_range = img_range\n        self.resi_connection = resi_connection\n\n        self.supports_fp16 = False  # Too much weirdness to support this at the moment\n        self.supports_bfp16 = True\n        self.min_size_restriction = 16\n\n        self.img_range = img_range\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n        self.window_size = window_size\n\n        #####################################################################################################\n        ################################### 1, shallow feature extraction ###################################\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        #####################################################################################################\n        ################################### 2, deep feature extraction ######################################\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # merge non-overlapping patches into image\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(  # type: ignore\n                torch.zeros(1, num_patches, embed_dim)\n            )\n            trunc_normal_(self.absolute_pos_embed, std=0.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build Residual Swin Transformer blocks (RSTB)\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = RSTB(\n                dim=embed_dim,\n                input_resolution=(patches_resolution[0], patches_resolution[1]),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=self.mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[\n                    sum(depths[:i_layer]) : sum(depths[: i_layer + 1])  # type: ignore\n                ],  # no impact on SR results\n                norm_layer=norm_layer,\n                downsample=None,\n                use_checkpoint=use_checkpoint,\n                img_size=img_size,\n                patch_size=patch_size,\n                resi_connection=resi_connection,\n            )\n            self.layers.append(layer)\n        self.norm = norm_layer(self.num_features)\n\n        # build the last conv layer in deep feature extraction\n        if resi_connection == \"1conv\":\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            # to save parameters and memory\n            self.conv_after_body = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1),\n            )\n\n        #####################################################################################################\n        ################################ 3, high quality image reconstruction ################################\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR (to save parameters)\n            self.upsample = UpsampleOneStep(\n                upscale,\n                embed_dim,\n                num_out_ch,\n                (patches_resolution[0], patches_resolution[1]),\n            )\n        elif self.upsampler == \"nearest+conv\":\n            # for real-world SR (less artifacts)\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            if self.upscale == 4:\n                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            elif self.upscale == 8:\n                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n                self.conv_up3 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n\n        self.apply(self._init_weights)\n        self.load_state_dict(self.state, strict=False)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay(self):\n        return {\"absolute_pos_embed\"}\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay_keywords(self):\n        return {\"relative_position_bias_table\"}\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"reflect\")\n        return x\n\n    def forward_features(self, x):\n        x_size = (x.shape[2], x.shape[3])\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x, x_size)\n\n        x = self.norm(x)  # B L C\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        x = self.check_image_size(x)\n\n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n\n        if self.start_unshuffle > 1:\n            x = torch.nn.functional.pixel_unshuffle(x, self.start_unshuffle)\n\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.upsample(x)\n        elif self.upsampler == \"nearest+conv\":\n            # for real-world SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.lrelu(\n                self.conv_up1(\n                    torch.nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")  # type: ignore\n                )\n            )\n            if self.upscale == 4:\n                x = self.lrelu(\n                    self.conv_up2(\n                        torch.nn.functional.interpolate(  # type: ignore\n                            x, scale_factor=2, mode=\"nearest\"\n                        )\n                    )\n                )\n            elif self.upscale == 8:\n                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n                x = self.lrelu(self.conv_up3(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            x_first = self.conv_first(x)\n            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n            x = x + self.conv_last(res)\n\n        x = x / self.img_range + self.mean\n\n        return x[:, :, : H * self.upscale, : W * self.upscale]\n\n    def flops(self):\n        flops = 0\n        H, W = self.patches_resolution\n        flops += H * W * 3 * self.embed_dim * 9\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()  # type: ignore\n        flops += H * W * 3 * self.embed_dim * self.embed_dim\n        flops += self.upsample.flops()  # type: ignore\n        return flops\n", "ldm_patched/pfn/architecture/SRVGG.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SRVGGNetCompact(nn.Module):\n    \"\"\"A compact VGG-style network structure for super-resolution.\n    It is a compact network structure, which performs upsampling in the last layer and no convolution is\n    conducted on the HR feature space.\n    Args:\n        num_in_ch (int): Channel number of inputs. Default: 3.\n        num_out_ch (int): Channel number of outputs. Default: 3.\n        num_feat (int): Channel number of intermediate features. Default: 64.\n        num_conv (int): Number of convolution layers in the body network. Default: 16.\n        upscale (int): Upsampling factor. Default: 4.\n        act_type (str): Activation type, options: 'relu', 'prelu', 'leakyrelu'. Default: prelu.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n        act_type: str = \"prelu\",\n    ):\n        super(SRVGGNetCompact, self).__init__()\n        self.model_arch = \"SRVGG (RealESRGAN)\"\n        self.sub_type = \"SR\"\n\n        self.act_type = act_type\n\n        self.state = state_dict\n\n        if \"params\" in self.state:\n            self.state = self.state[\"params\"]\n\n        self.key_arr = list(self.state.keys())\n\n        self.in_nc = self.get_in_nc()\n        self.num_feat = self.get_num_feats()\n        self.num_conv = self.get_num_conv()\n        self.out_nc = self.in_nc  # :(\n        self.pixelshuffle_shape = None  # Defined in get_scale()\n        self.scale = self.get_scale()\n\n        self.supports_fp16 = True\n        self.supports_bfp16 = True\n        self.min_size_restriction = None\n\n        self.body = nn.ModuleList()\n        # the first conv\n        self.body.append(nn.Conv2d(self.in_nc, self.num_feat, 3, 1, 1))\n        # the first activation\n        if act_type == \"relu\":\n            activation = nn.ReLU(inplace=True)\n        elif act_type == \"prelu\":\n            activation = nn.PReLU(num_parameters=self.num_feat)\n        elif act_type == \"leakyrelu\":\n            activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        self.body.append(activation)  # type: ignore\n\n        # the body structure\n        for _ in range(self.num_conv):\n            self.body.append(nn.Conv2d(self.num_feat, self.num_feat, 3, 1, 1))\n            # activation\n            if act_type == \"relu\":\n                activation = nn.ReLU(inplace=True)\n            elif act_type == \"prelu\":\n                activation = nn.PReLU(num_parameters=self.num_feat)\n            elif act_type == \"leakyrelu\":\n                activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n            self.body.append(activation)  # type: ignore\n\n        # the last conv\n        self.body.append(nn.Conv2d(self.num_feat, self.pixelshuffle_shape, 3, 1, 1))  # type: ignore\n        # upsample\n        self.upsampler = nn.PixelShuffle(self.scale)\n\n        self.load_state_dict(self.state, strict=False)\n\n    def get_num_conv(self) -> int:\n        return (int(self.key_arr[-1].split(\".\")[1]) - 2) // 2\n\n    def get_num_feats(self) -> int:\n        return self.state[self.key_arr[0]].shape[0]\n\n    def get_in_nc(self) -> int:\n        return self.state[self.key_arr[0]].shape[1]\n\n    def get_scale(self) -> int:\n        self.pixelshuffle_shape = self.state[self.key_arr[-1]].shape[0]\n        # Assume out_nc is the same as in_nc\n        # I cant think of a better way to do that\n        self.out_nc = self.in_nc\n        scale = math.sqrt(self.pixelshuffle_shape / self.out_nc)\n        if scale - int(scale) > 0:\n            print(\n                \"out_nc is probably different than in_nc, scale calculation might be wrong\"\n            )\n        scale = int(scale)\n        return scale\n\n    def forward(self, x):\n        out = x\n        for i in range(0, len(self.body)):\n            out = self.body[i](out)\n\n        out = self.upsampler(out)\n        # add the nearest upsampled image, so that the network learns the residual\n        base = F.interpolate(x, scale_factor=self.scale, mode=\"nearest\")\n        out += base\n        return out\n", "ldm_patched/pfn/architecture/SCUNet.py": "# pylint: skip-file\n# -----------------------------------------------------------------------------------\n# SCUNet: Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis, https://arxiv.org/abs/2203.13278\n# Zhang, Kai and Li, Yawei and Liang, Jingyun and Cao, Jiezhang and Zhang, Yulun and Tang, Hao and Timofte, Radu and Van Gool, Luc\n# -----------------------------------------------------------------------------------\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\nfrom .timm.drop import DropPath\nfrom .timm.weight_init import trunc_normal_\n\n\n# Borrowed from https://github.com/cszn/SCUNet/blob/main/models/network_scunet.py\nclass WMSA(nn.Module):\n    \"\"\"Self-attention module in Swin Transformer\"\"\"\n\n    def __init__(self, input_dim, output_dim, head_dim, window_size, type):\n        super(WMSA, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.head_dim = head_dim\n        self.scale = self.head_dim**-0.5\n        self.n_heads = input_dim // head_dim\n        self.window_size = window_size\n        self.type = type\n        self.embedding_layer = nn.Linear(self.input_dim, 3 * self.input_dim, bias=True)\n\n        self.relative_position_params = nn.Parameter(\n            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), self.n_heads)\n        )\n        # TODO recover\n        # self.relative_position_params = nn.Parameter(torch.zeros(self.n_heads, 2 * window_size - 1, 2 * window_size -1))\n        self.relative_position_params = nn.Parameter(\n            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), self.n_heads)\n        )\n\n        self.linear = nn.Linear(self.input_dim, self.output_dim)\n\n        trunc_normal_(self.relative_position_params, std=0.02)\n        self.relative_position_params = torch.nn.Parameter(\n            self.relative_position_params.view(\n                2 * window_size - 1, 2 * window_size - 1, self.n_heads\n            )\n            .transpose(1, 2)\n            .transpose(0, 1)\n        )\n\n    def generate_mask(self, h, w, p, shift):\n        \"\"\"generating the mask of SW-MSA\n        Args:\n            shift: shift parameters in CyclicShift.\n        Returns:\n            attn_mask: should be (1 1 w p p),\n        \"\"\"\n        # supporting square.\n        attn_mask = torch.zeros(\n            h,\n            w,\n            p,\n            p,\n            p,\n            p,\n            dtype=torch.bool,\n            device=self.relative_position_params.device,\n        )\n        if self.type == \"W\":\n            return attn_mask\n\n        s = p - shift\n        attn_mask[-1, :, :s, :, s:, :] = True\n        attn_mask[-1, :, s:, :, :s, :] = True\n        attn_mask[:, -1, :, :s, :, s:] = True\n        attn_mask[:, -1, :, s:, :, :s] = True\n        attn_mask = rearrange(\n            attn_mask, \"w1 w2 p1 p2 p3 p4 -> 1 1 (w1 w2) (p1 p2) (p3 p4)\"\n        )\n        return attn_mask\n\n    def forward(self, x):\n        \"\"\"Forward pass of Window Multi-head Self-attention module.\n        Args:\n            x: input tensor with shape of [b h w c];\n            attn_mask: attention mask, fill -inf where the value is True;\n        Returns:\n            output: tensor shape [b h w c]\n        \"\"\"\n        if self.type != \"W\":\n            x = torch.roll(\n                x,\n                shifts=(-(self.window_size // 2), -(self.window_size // 2)),\n                dims=(1, 2),\n            )\n\n        x = rearrange(\n            x,\n            \"b (w1 p1) (w2 p2) c -> b w1 w2 p1 p2 c\",\n            p1=self.window_size,\n            p2=self.window_size,\n        )\n        h_windows = x.size(1)\n        w_windows = x.size(2)\n        # square validation\n        # assert h_windows == w_windows\n\n        x = rearrange(\n            x,\n            \"b w1 w2 p1 p2 c -> b (w1 w2) (p1 p2) c\",\n            p1=self.window_size,\n            p2=self.window_size,\n        )\n        qkv = self.embedding_layer(x)\n        q, k, v = rearrange(\n            qkv, \"b nw np (threeh c) -> threeh b nw np c\", c=self.head_dim\n        ).chunk(3, dim=0)\n        sim = torch.einsum(\"hbwpc,hbwqc->hbwpq\", q, k) * self.scale\n        # Adding learnable relative embedding\n        sim = sim + rearrange(self.relative_embedding(), \"h p q -> h 1 1 p q\")\n        # Using Attn Mask to distinguish different subwindows.\n        if self.type != \"W\":\n            attn_mask = self.generate_mask(\n                h_windows, w_windows, self.window_size, shift=self.window_size // 2\n            )\n            sim = sim.masked_fill_(attn_mask, float(\"-inf\"))\n\n        probs = nn.functional.softmax(sim, dim=-1)\n        output = torch.einsum(\"hbwij,hbwjc->hbwic\", probs, v)\n        output = rearrange(output, \"h b w p c -> b w p (h c)\")\n        output = self.linear(output)\n        output = rearrange(\n            output,\n            \"b (w1 w2) (p1 p2) c -> b (w1 p1) (w2 p2) c\",\n            w1=h_windows,\n            p1=self.window_size,\n        )\n\n        if self.type != \"W\":\n            output = torch.roll(\n                output,\n                shifts=(self.window_size // 2, self.window_size // 2),\n                dims=(1, 2),\n            )\n\n        return output\n\n    def relative_embedding(self):\n        cord = torch.tensor(\n            np.array(\n                [\n                    [i, j]\n                    for i in range(self.window_size)\n                    for j in range(self.window_size)\n                ]\n            )\n        )\n        relation = cord[:, None, :] - cord[None, :, :] + self.window_size - 1\n        # negative is allowed\n        return self.relative_position_params[\n            :, relation[:, :, 0].long(), relation[:, :, 1].long()\n        ]\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        head_dim,\n        window_size,\n        drop_path,\n        type=\"W\",\n        input_resolution=None,\n    ):\n        \"\"\"SwinTransformer Block\"\"\"\n        super(Block, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        assert type in [\"W\", \"SW\"]\n        self.type = type\n        if input_resolution <= window_size:\n            self.type = \"W\"\n\n        self.ln1 = nn.LayerNorm(input_dim)\n        self.msa = WMSA(input_dim, input_dim, head_dim, window_size, self.type)\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.ln2 = nn.LayerNorm(input_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, 4 * input_dim),\n            nn.GELU(),\n            nn.Linear(4 * input_dim, output_dim),\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.msa(self.ln1(x)))\n        x = x + self.drop_path(self.mlp(self.ln2(x)))\n        return x\n\n\nclass ConvTransBlock(nn.Module):\n    def __init__(\n        self,\n        conv_dim,\n        trans_dim,\n        head_dim,\n        window_size,\n        drop_path,\n        type=\"W\",\n        input_resolution=None,\n    ):\n        \"\"\"SwinTransformer and Conv Block\"\"\"\n        super(ConvTransBlock, self).__init__()\n        self.conv_dim = conv_dim\n        self.trans_dim = trans_dim\n        self.head_dim = head_dim\n        self.window_size = window_size\n        self.drop_path = drop_path\n        self.type = type\n        self.input_resolution = input_resolution\n\n        assert self.type in [\"W\", \"SW\"]\n        if self.input_resolution <= self.window_size:\n            self.type = \"W\"\n\n        self.trans_block = Block(\n            self.trans_dim,\n            self.trans_dim,\n            self.head_dim,\n            self.window_size,\n            self.drop_path,\n            self.type,\n            self.input_resolution,\n        )\n        self.conv1_1 = nn.Conv2d(\n            self.conv_dim + self.trans_dim,\n            self.conv_dim + self.trans_dim,\n            1,\n            1,\n            0,\n            bias=True,\n        )\n        self.conv1_2 = nn.Conv2d(\n            self.conv_dim + self.trans_dim,\n            self.conv_dim + self.trans_dim,\n            1,\n            1,\n            0,\n            bias=True,\n        )\n\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(self.conv_dim, self.conv_dim, 3, 1, 1, bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(self.conv_dim, self.conv_dim, 3, 1, 1, bias=False),\n        )\n\n    def forward(self, x):\n        conv_x, trans_x = torch.split(\n            self.conv1_1(x), (self.conv_dim, self.trans_dim), dim=1\n        )\n        conv_x = self.conv_block(conv_x) + conv_x\n        trans_x = Rearrange(\"b c h w -> b h w c\")(trans_x)\n        trans_x = self.trans_block(trans_x)\n        trans_x = Rearrange(\"b h w c -> b c h w\")(trans_x)\n        res = self.conv1_2(torch.cat((conv_x, trans_x), dim=1))\n        x = x + res\n\n        return x\n\n\nclass SCUNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        in_nc=3,\n        config=[4, 4, 4, 4, 4, 4, 4],\n        dim=64,\n        drop_path_rate=0.0,\n        input_resolution=256,\n    ):\n        super(SCUNet, self).__init__()\n        self.model_arch = \"SCUNet\"\n        self.sub_type = \"SR\"\n\n        self.num_filters: int = 0\n\n        self.state = state_dict\n        self.config = config\n        self.dim = dim\n        self.head_dim = 32\n        self.window_size = 8\n\n        self.in_nc = in_nc\n        self.out_nc = self.in_nc\n        self.scale = 1\n        self.supports_fp16 = True\n\n        # drop path rate for each layer\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(config))]\n\n        self.m_head = [nn.Conv2d(in_nc, dim, 3, 1, 1, bias=False)]\n\n        begin = 0\n        self.m_down1 = [\n            ConvTransBlock(\n                dim // 2,\n                dim // 2,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution,\n            )\n            for i in range(config[0])\n        ] + [nn.Conv2d(dim, 2 * dim, 2, 2, 0, bias=False)]\n\n        begin += config[0]\n        self.m_down2 = [\n            ConvTransBlock(\n                dim,\n                dim,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution // 2,\n            )\n            for i in range(config[1])\n        ] + [nn.Conv2d(2 * dim, 4 * dim, 2, 2, 0, bias=False)]\n\n        begin += config[1]\n        self.m_down3 = [\n            ConvTransBlock(\n                2 * dim,\n                2 * dim,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution // 4,\n            )\n            for i in range(config[2])\n        ] + [nn.Conv2d(4 * dim, 8 * dim, 2, 2, 0, bias=False)]\n\n        begin += config[2]\n        self.m_body = [\n            ConvTransBlock(\n                4 * dim,\n                4 * dim,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution // 8,\n            )\n            for i in range(config[3])\n        ]\n\n        begin += config[3]\n        self.m_up3 = [\n            nn.ConvTranspose2d(8 * dim, 4 * dim, 2, 2, 0, bias=False),\n        ] + [\n            ConvTransBlock(\n                2 * dim,\n                2 * dim,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution // 4,\n            )\n            for i in range(config[4])\n        ]\n\n        begin += config[4]\n        self.m_up2 = [\n            nn.ConvTranspose2d(4 * dim, 2 * dim, 2, 2, 0, bias=False),\n        ] + [\n            ConvTransBlock(\n                dim,\n                dim,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution // 2,\n            )\n            for i in range(config[5])\n        ]\n\n        begin += config[5]\n        self.m_up1 = [\n            nn.ConvTranspose2d(2 * dim, dim, 2, 2, 0, bias=False),\n        ] + [\n            ConvTransBlock(\n                dim // 2,\n                dim // 2,\n                self.head_dim,\n                self.window_size,\n                dpr[i + begin],\n                \"W\" if not i % 2 else \"SW\",\n                input_resolution,\n            )\n            for i in range(config[6])\n        ]\n\n        self.m_tail = [nn.Conv2d(dim, in_nc, 3, 1, 1, bias=False)]\n\n        self.m_head = nn.Sequential(*self.m_head)\n        self.m_down1 = nn.Sequential(*self.m_down1)\n        self.m_down2 = nn.Sequential(*self.m_down2)\n        self.m_down3 = nn.Sequential(*self.m_down3)\n        self.m_body = nn.Sequential(*self.m_body)\n        self.m_up3 = nn.Sequential(*self.m_up3)\n        self.m_up2 = nn.Sequential(*self.m_up2)\n        self.m_up1 = nn.Sequential(*self.m_up1)\n        self.m_tail = nn.Sequential(*self.m_tail)\n        # self.apply(self._init_weights)\n        self.load_state_dict(state_dict, strict=True)\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (64 - h % 64) % 64\n        mod_pad_w = (64 - w % 64) % 64\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"reflect\")\n        return x\n\n    def forward(self, x0):\n        h, w = x0.size()[-2:]\n        x0 = self.check_image_size(x0)\n\n        x1 = self.m_head(x0)\n        x2 = self.m_down1(x1)\n        x3 = self.m_down2(x2)\n        x4 = self.m_down3(x3)\n        x = self.m_body(x4)\n        x = self.m_up3(x + x4)\n        x = self.m_up2(x + x3)\n        x = self.m_up1(x + x2)\n        x = self.m_tail(x + x1)\n\n        x = x[:, :, :h, :w]\n        return x\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n", "ldm_patched/pfn/architecture/HAT.py": "# pylint: skip-file\n# HAT from https://github.com/XPixelGroup/HAT/blob/main/hat/archs/hat_arch.py\nimport math\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nfrom .timm.helpers import to_2tuple\nfrom .timm.weight_init import trunc_normal_\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    From: https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/drop.py\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    From: https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/drop.py\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)  # type: ignore\n\n\nclass ChannelAttention(nn.Module):\n    \"\"\"Channel attention used in RCAN.\n    Args:\n        num_feat (int): Channel number of intermediate features.\n        squeeze_factor (int): Channel squeeze factor. Default: 16.\n    \"\"\"\n\n    def __init__(self, num_feat, squeeze_factor=16):\n        super(ChannelAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        y = self.attention(x)\n        return x * y\n\n\nclass CAB(nn.Module):\n    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n        super(CAB, self).__init__()\n\n        self.cab = nn.Sequential(\n            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n            nn.GELU(),\n            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n            ChannelAttention(num_feat, squeeze_factor),\n        )\n\n    def forward(self, x):\n        return self.cab(x)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (b, h, w, c)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*b, window_size, window_size, c)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, c)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, h, w):\n    \"\"\"\n    Args:\n        windows: (num_windows*b, window_size, window_size, c)\n        window_size (int): Window size\n        h (int): Height of image\n        w (int): Width of image\n    Returns:\n        x: (b, h, w, c)\n    \"\"\"\n    b = int(windows.shape[0] / (h * w / window_size / window_size))\n    x = windows.view(\n        b, h // window_size, w // window_size, window_size, window_size, -1\n    )\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=True,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(  # type: ignore\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n        )  # 2*Wh-1 * 2*Ww-1, nH\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, rpi, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*b, n, c)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        b_, n, c = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(b_, n, 3, self.num_heads, c // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n            self.window_size[0] * self.window_size[1],\n            self.window_size[0] * self.window_size[1],\n            -1,\n        )  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nw = mask.shape[0]\n            attn = attn.view(b_ // nw, nw, self.num_heads, n, n) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, n, n)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(b_, n, c)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass HAB(nn.Module):\n    r\"\"\"Hybrid Attention Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        compress_ratio=3,\n        squeeze_factor=30,\n        conv_scale=0.01,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert (\n            0 <= self.shift_size < self.window_size\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim,\n            window_size=to_2tuple(self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.conv_scale = conv_scale\n        self.conv_block = CAB(\n            num_feat=dim, compress_ratio=compress_ratio, squeeze_factor=squeeze_factor\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x, x_size, rpi_sa, attn_mask):\n        h, w = x_size\n        b, _, c = x.shape\n        # assert seq_len == h * w, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(b, h, w, c)\n\n        # Conv_X\n        conv_x = self.conv_block(x.permute(0, 3, 1, 2))\n        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(b, h * w, c)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(\n                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n            )\n            attn_mask = attn_mask\n        else:\n            shifted_x = x\n            attn_mask = None\n\n        # partition windows\n        x_windows = window_partition(\n            shifted_x, self.window_size\n        )  # nw*b, window_size, window_size, c\n        x_windows = x_windows.view(\n            -1, self.window_size * self.window_size, c\n        )  # nw*b, window_size*window_size, c\n\n        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n        attn_windows = self.attn(x_windows, rpi=rpi_sa, mask=attn_mask)\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, c)\n        shifted_x = window_reverse(attn_windows, self.window_size, h, w)  # b h' w' c\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            attn_x = torch.roll(\n                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n            )\n        else:\n            attn_x = shifted_x\n        attn_x = attn_x.view(b, h * w, c)\n\n        # FFN\n        x = shortcut + self.drop_path(attn_x) + conv_x * self.conv_scale\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: b, h*w, c\n        \"\"\"\n        h, w = self.input_resolution\n        b, seq_len, c = x.shape\n        assert seq_len == h * w, \"input feature has wrong size\"\n        assert h % 2 == 0 and w % 2 == 0, f\"x size ({h}*{w}) are not even.\"\n\n        x = x.view(b, h, w, c)\n\n        x0 = x[:, 0::2, 0::2, :]  # b h/2 w/2 c\n        x1 = x[:, 1::2, 0::2, :]  # b h/2 w/2 c\n        x2 = x[:, 0::2, 1::2, :]  # b h/2 w/2 c\n        x3 = x[:, 1::2, 1::2, :]  # b h/2 w/2 c\n        x = torch.cat([x0, x1, x2, x3], -1)  # b h/2 w/2 4*c\n        x = x.view(b, -1, 4 * c)  # b h/2*w/2 4*c\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\nclass OCAB(nn.Module):\n    # overlapping cross-attention block\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        window_size,\n        overlap_ratio,\n        num_heads,\n        qkv_bias=True,\n        qk_scale=None,\n        mlp_ratio=2,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n        self.overlap_win_size = int(window_size * overlap_ratio) + window_size\n\n        self.norm1 = norm_layer(dim)\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.unfold = nn.Unfold(\n            kernel_size=(self.overlap_win_size, self.overlap_win_size),\n            stride=window_size,\n            padding=(self.overlap_win_size - window_size) // 2,\n        )\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(  # type: ignore\n            torch.zeros(\n                (window_size + self.overlap_win_size - 1)\n                * (window_size + self.overlap_win_size - 1),\n                num_heads,\n            )\n        )  # 2*Wh-1 * 2*Ww-1, nH\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.proj = nn.Linear(dim, dim)\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU\n        )\n\n    def forward(self, x, x_size, rpi):\n        h, w = x_size\n        b, _, c = x.shape\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(b, h, w, c)\n\n        qkv = self.qkv(x).reshape(b, h, w, 3, c).permute(3, 0, 4, 1, 2)  # 3, b, c, h, w\n        q = qkv[0].permute(0, 2, 3, 1)  # b, h, w, c\n        kv = torch.cat((qkv[1], qkv[2]), dim=1)  # b, 2*c, h, w\n\n        # partition windows\n        q_windows = window_partition(\n            q, self.window_size\n        )  # nw*b, window_size, window_size, c\n        q_windows = q_windows.view(\n            -1, self.window_size * self.window_size, c\n        )  # nw*b, window_size*window_size, c\n\n        kv_windows = self.unfold(kv)  # b, c*w*w, nw\n        kv_windows = rearrange(\n            kv_windows,\n            \"b (nc ch owh oww) nw -> nc (b nw) (owh oww) ch\",\n            nc=2,\n            ch=c,\n            owh=self.overlap_win_size,\n            oww=self.overlap_win_size,\n        ).contiguous()  # 2, nw*b, ow*ow, c\n        # Do the above rearrangement without the rearrange function\n        # kv_windows = kv_windows.view(\n        #     2, b, self.overlap_win_size, self.overlap_win_size, c, -1\n        # )\n        # kv_windows = kv_windows.permute(0, 5, 1, 2, 3, 4).contiguous()\n        # kv_windows = kv_windows.view(\n        #     2, -1, self.overlap_win_size * self.overlap_win_size, c\n        # )\n\n        k_windows, v_windows = kv_windows[0], kv_windows[1]  # nw*b, ow*ow, c\n\n        b_, nq, _ = q_windows.shape\n        _, n, _ = k_windows.shape\n        d = self.dim // self.num_heads\n        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(\n            0, 2, 1, 3\n        )  # nw*b, nH, nq, d\n        k = k_windows.reshape(b_, n, self.num_heads, d).permute(\n            0, 2, 1, 3\n        )  # nw*b, nH, n, d\n        v = v_windows.reshape(b_, n, self.num_heads, d).permute(\n            0, 2, 1, 3\n        )  # nw*b, nH, n, d\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n            self.window_size * self.window_size,\n            self.overlap_win_size * self.overlap_win_size,\n            -1,\n        )  # ws*ws, wse*wse, nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, ws*ws, wse*wse\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        attn = self.softmax(attn)\n        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)\n\n        # merge windows\n        attn_windows = attn_windows.view(\n            -1, self.window_size, self.window_size, self.dim\n        )\n        x = window_reverse(attn_windows, self.window_size, h, w)  # b h w c\n        x = x.view(b, h * w, self.dim)\n\n        x = self.proj(x) + shortcut\n\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass AttenBlocks(nn.Module):\n    \"\"\"A series of attention blocks for one RHAG.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        compress_ratio,\n        squeeze_factor,\n        conv_scale,\n        overlap_ratio,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                HAB(\n                    dim=dim,\n                    input_resolution=input_resolution,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n                    compress_ratio=compress_ratio,\n                    squeeze_factor=squeeze_factor,\n                    conv_scale=conv_scale,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # OCAB\n        self.overlap_attn = OCAB(\n            dim=dim,\n            input_resolution=input_resolution,\n            window_size=window_size,\n            overlap_ratio=overlap_ratio,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            mlp_ratio=mlp_ratio,  # type: ignore\n            norm_layer=norm_layer,\n        )\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, norm_layer=norm_layer\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x, x_size, params):\n        for blk in self.blocks:\n            x = blk(x, x_size, params[\"rpi_sa\"], params[\"attn_mask\"])\n\n        x = self.overlap_attn(x, x_size, params[\"rpi_oca\"])\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n\nclass RHAG(nn.Module):\n    \"\"\"Residual Hybrid Attention Group (RHAG).\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        img_size: Input image size.\n        patch_size: Patch size.\n        resi_connection: The convolutional block before residual connection.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        compress_ratio,\n        squeeze_factor,\n        conv_scale,\n        overlap_ratio,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n        img_size=224,\n        patch_size=4,\n        resi_connection=\"1conv\",\n    ):\n        super(RHAG, self).__init__()\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n\n        self.residual_group = AttenBlocks(\n            dim=dim,\n            input_resolution=input_resolution,\n            depth=depth,\n            num_heads=num_heads,\n            window_size=window_size,\n            compress_ratio=compress_ratio,\n            squeeze_factor=squeeze_factor,\n            conv_scale=conv_scale,\n            overlap_ratio=overlap_ratio,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            norm_layer=norm_layer,\n            downsample=downsample,\n            use_checkpoint=use_checkpoint,\n        )\n\n        if resi_connection == \"1conv\":\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == \"identity\":\n            self.conv = nn.Identity()\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=0,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=0,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n    def forward(self, x, x_size, params):\n        return (\n            self.patch_embed(\n                self.conv(\n                    self.patch_unembed(self.residual_group(x, x_size, params), x_size)\n                )\n            )\n            + x\n        )\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [\n            img_size[0] // patch_size[0],  # type: ignore\n            img_size[1] // patch_size[1],  # type: ignore\n        ]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)  # b Ph*Pw c\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [\n            img_size[0] // patch_size[0],  # type: ignore\n            img_size[1] // patch_size[1],  # type: ignore\n        ]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n    def forward(self, x, x_size):\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(x.shape[0], self.embed_dim, x_size[0], x_size[1])\n        )  # b Ph*Pw c\n        return x\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n            )\n        super(Upsample, self).__init__(*m)\n\n\nclass HAT(nn.Module):\n    r\"\"\"Hybrid Attention Transformer\n        A PyTorch implementation of : `Activating More Pixels in Image Super-Resolution Transformer`.\n        Some codes are based on SwinIR.\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n        img_range: Image range. 1. or 255.\n        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n        **kwargs,\n    ):\n        super(HAT, self).__init__()\n\n        # Defaults\n        img_size = 64\n        patch_size = 1\n        in_chans = 3\n        embed_dim = 96\n        depths = (6, 6, 6, 6)\n        num_heads = (6, 6, 6, 6)\n        window_size = 7\n        compress_ratio = 3\n        squeeze_factor = 30\n        conv_scale = 0.01\n        overlap_ratio = 0.5\n        mlp_ratio = 4.0\n        qkv_bias = True\n        qk_scale = None\n        drop_rate = 0.0\n        attn_drop_rate = 0.0\n        drop_path_rate = 0.1\n        norm_layer = nn.LayerNorm\n        ape = False\n        patch_norm = True\n        use_checkpoint = False\n        upscale = 2\n        img_range = 1.0\n        upsampler = \"\"\n        resi_connection = \"1conv\"\n\n        self.state = state_dict\n        self.model_arch = \"HAT\"\n        self.sub_type = \"SR\"\n        self.supports_fp16 = False\n        self.support_bf16 = True\n        self.min_size_restriction = 16\n\n        state_keys = list(state_dict.keys())\n\n        num_feat = state_dict[\"conv_last.weight\"].shape[1]\n        in_chans = state_dict[\"conv_first.weight\"].shape[1]\n        num_out_ch = state_dict[\"conv_last.weight\"].shape[0]\n        embed_dim = state_dict[\"conv_first.weight\"].shape[0]\n\n        if \"conv_before_upsample.0.weight\" in state_keys:\n            if \"conv_up1.weight\" in state_keys:\n                upsampler = \"nearest+conv\"\n            else:\n                upsampler = \"pixelshuffle\"\n                supports_fp16 = False\n        elif \"upsample.0.weight\" in state_keys:\n            upsampler = \"pixelshuffledirect\"\n        else:\n            upsampler = \"\"\n        upscale = 1\n        if upsampler == \"nearest+conv\":\n            upsample_keys = [\n                x for x in state_keys if \"conv_up\" in x and \"bias\" not in x\n            ]\n\n            for upsample_key in upsample_keys:\n                upscale *= 2\n        elif upsampler == \"pixelshuffle\":\n            upsample_keys = [\n                x\n                for x in state_keys\n                if \"upsample\" in x and \"conv\" not in x and \"bias\" not in x\n            ]\n            for upsample_key in upsample_keys:\n                shape = self.state[upsample_key].shape[0]\n                upscale *= math.sqrt(shape // num_feat)\n            upscale = int(upscale)\n        elif upsampler == \"pixelshuffledirect\":\n            upscale = int(\n                math.sqrt(self.state[\"upsample.0.bias\"].shape[0] // num_out_ch)\n            )\n\n        max_layer_num = 0\n        max_block_num = 0\n        for key in state_keys:\n            result = re.match(\n                r\"layers.(\\d*).residual_group.blocks.(\\d*).conv_block.cab.0.weight\", key\n            )\n            if result:\n                layer_num, block_num = result.groups()\n                max_layer_num = max(max_layer_num, int(layer_num))\n                max_block_num = max(max_block_num, int(block_num))\n\n        depths = [max_block_num + 1 for _ in range(max_layer_num + 1)]\n\n        if (\n            \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            in state_keys\n        ):\n            num_heads_num = self.state[\n                \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            ].shape[-1]\n            num_heads = [num_heads_num for _ in range(max_layer_num + 1)]\n        else:\n            num_heads = depths\n\n        mlp_ratio = float(\n            self.state[\"layers.0.residual_group.blocks.0.mlp.fc1.bias\"].shape[0]\n            / embed_dim\n        )\n\n        # TODO: could actually count the layers, but this should do\n        if \"layers.0.conv.4.weight\" in state_keys:\n            resi_connection = \"3conv\"\n        else:\n            resi_connection = \"1conv\"\n\n        window_size = int(math.sqrt(self.state[\"relative_position_index_SA\"].shape[0]))\n\n        # Not sure if this is needed or used at all anywhere in HAT's config\n        if \"layers.0.residual_group.blocks.1.attn_mask\" in state_keys:\n            img_size = int(\n                math.sqrt(\n                    self.state[\"layers.0.residual_group.blocks.1.attn_mask\"].shape[0]\n                )\n                * window_size\n            )\n\n        self.window_size = window_size\n        self.shift_size = window_size // 2\n        self.overlap_ratio = overlap_ratio\n\n        self.in_nc = in_chans\n        self.out_nc = num_out_ch\n        self.num_feat = num_feat\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.depths = depths\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n        self.scale = upscale\n        self.upsampler = upsampler\n        self.img_size = img_size\n        self.img_range = img_range\n        self.resi_connection = resi_connection\n\n        num_in_ch = in_chans\n        # num_out_ch = in_chans\n        # num_feat = 64\n        self.img_range = img_range\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n\n        # relative position index\n        relative_position_index_SA = self.calculate_rpi_sa()\n        relative_position_index_OCA = self.calculate_rpi_oca()\n        self.register_buffer(\"relative_position_index_SA\", relative_position_index_SA)\n        self.register_buffer(\"relative_position_index_OCA\", relative_position_index_OCA)\n\n        # ------------------------- 1, shallow feature extraction ------------------------- #\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        # ------------------------- 2, deep feature extraction ------------------------- #\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # merge non-overlapping patches into image\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(  # type: ignore[arg-type]\n                torch.zeros(1, num_patches, embed_dim)\n            )\n            trunc_normal_(self.absolute_pos_embed, std=0.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build Residual Hybrid Attention Groups (RHAG)\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = RHAG(\n                dim=embed_dim,\n                input_resolution=(patches_resolution[0], patches_resolution[1]),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                compress_ratio=compress_ratio,\n                squeeze_factor=squeeze_factor,\n                conv_scale=conv_scale,\n                overlap_ratio=overlap_ratio,\n                mlp_ratio=self.mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[\n                    sum(depths[:i_layer]) : sum(depths[: i_layer + 1])  # type: ignore\n                ],  # no impact on SR results\n                norm_layer=norm_layer,\n                downsample=None,\n                use_checkpoint=use_checkpoint,\n                img_size=img_size,\n                patch_size=patch_size,\n                resi_connection=resi_connection,\n            )\n            self.layers.append(layer)\n        self.norm = norm_layer(self.num_features)\n\n        # build the last conv layer in deep feature extraction\n        if resi_connection == \"1conv\":\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == \"identity\":\n            self.conv_after_body = nn.Identity()\n\n        # ------------------------- 3, high quality image reconstruction ------------------------- #\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n\n        self.apply(self._init_weights)\n        self.load_state_dict(self.state, strict=False)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def calculate_rpi_sa(self):\n        # calculate relative position index for SA\n        coords_h = torch.arange(self.window_size)\n        coords_w = torch.arange(self.window_size)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        return relative_position_index\n\n    def calculate_rpi_oca(self):\n        # calculate relative position index for OCA\n        window_size_ori = self.window_size\n        window_size_ext = self.window_size + int(self.overlap_ratio * self.window_size)\n\n        coords_h = torch.arange(window_size_ori)\n        coords_w = torch.arange(window_size_ori)\n        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws\n        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws\n\n        coords_h = torch.arange(window_size_ext)\n        coords_w = torch.arange(window_size_ext)\n        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse\n        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse\n\n        relative_coords = (\n            coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]\n        )  # 2, ws*ws, wse*wse\n\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # ws*ws, wse*wse, 2\n        relative_coords[:, :, 0] += (\n            window_size_ori - window_size_ext + 1\n        )  # shift to start from 0\n        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1\n\n        relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1\n        relative_position_index = relative_coords.sum(-1)\n        return relative_position_index\n\n    def calculate_mask(self, x_size):\n        # calculate attention mask for SW-MSA\n        h, w = x_size\n        img_mask = torch.zeros((1, h, w, 1))  # 1 h w 1\n        h_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        w_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(\n            img_mask, self.window_size\n        )  # nw, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        return attn_mask\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay(self):\n        return {\"absolute_pos_embed\"}\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay_keywords(self):\n        return {\"relative_position_bias_table\"}\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"reflect\")\n        return x\n\n    def forward_features(self, x):\n        x_size = (x.shape[2], x.shape[3])\n\n        # Calculate attention mask and relative position index in advance to speed up inference.\n        # The original code is very time-cosuming for large window size.\n        attn_mask = self.calculate_mask(x_size).to(x.device)\n        params = {\n            \"attn_mask\": attn_mask,\n            \"rpi_sa\": self.relative_position_index_SA,\n            \"rpi_oca\": self.relative_position_index_OCA,\n        }\n\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x, x_size, params)\n\n        x = self.norm(x)  # b seq_len c\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n        x = self.check_image_size(x)\n\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n\n        x = x / self.img_range + self.mean\n\n        return x[:, :, : H * self.upscale, : W * self.upscale]\n", "ldm_patched/pfn/architecture/Swin2SR.py": "# pylint: skip-file\n# -----------------------------------------------------------------------------------\n# Swin2SR: Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration, https://arxiv.org/abs/2209.11345\n# Written by Conde and Choi et al.\n# From: https://raw.githubusercontent.com/mv-lab/swin2sr/main/models/network_swin2sr.py\n# -----------------------------------------------------------------------------------\n\nimport math\nimport re\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\n# Originally from the timm package\nfrom .timm.drop import DropPath\nfrom .timm.helpers import to_2tuple\nfrom .timm.weight_init import trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(\n        B, H // window_size, W // window_size, window_size, window_size, -1\n    )\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=True,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        pretrained_window_size=[0, 0],\n    ):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.pretrained_window_size = pretrained_window_size\n        self.num_heads = num_heads\n\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)  # type: ignore\n\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(\n            nn.Linear(2, 512, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, num_heads, bias=False),\n        )\n\n        # get relative_coords_table\n        relative_coords_h = torch.arange(\n            -(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32\n        )\n        relative_coords_w = torch.arange(\n            -(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32\n        )\n        relative_coords_table = (\n            torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w]))\n            .permute(1, 2, 0)\n            .contiguous()\n            .unsqueeze(0)\n        )  # 1, 2*Wh-1, 2*Ww-1, 2\n        if pretrained_window_size[0] > 0:\n            relative_coords_table[:, :, :, 0] /= pretrained_window_size[0] - 1\n            relative_coords_table[:, :, :, 1] /= pretrained_window_size[1] - 1\n        else:\n            relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n            relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = (\n            torch.sign(relative_coords_table)\n            * torch.log2(torch.abs(relative_coords_table) + 1.0)\n            / np.log2(8)\n        )\n\n        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))  # type: ignore\n            self.v_bias = nn.Parameter(torch.zeros(dim))  # type: ignore\n        else:\n            self.q_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))  # type: ignore\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        # cosine attention\n        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)\n        logit_scale = torch.clamp(\n            self.logit_scale,\n            max=torch.log(torch.tensor(1.0 / 0.01)).to(self.logit_scale.device),\n        ).exp()\n        attn = attn * logit_scale\n\n        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(\n            -1, self.num_heads\n        )\n        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(  # type: ignore\n            self.window_size[0] * self.window_size[1],\n            self.window_size[0] * self.window_size[1],\n            -1,\n        )  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return (\n            f\"dim={self.dim}, window_size={self.window_size}, \"\n            f\"pretrained_window_size={self.pretrained_window_size}, num_heads={self.num_heads}\"\n        )\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        pretrained_window_size (int): Window size in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        pretrained_window_size=0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert (\n            0 <= self.shift_size < self.window_size\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim,\n            window_size=to_2tuple(self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n            pretrained_window_size=to_2tuple(pretrained_window_size),\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n        if self.shift_size > 0:\n            attn_mask = self.calculate_mask(self.input_resolution)\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def calculate_mask(self, x_size):\n        # calculate attention mask for SW-MSA\n        H, W = x_size\n        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n        h_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        w_slices = (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        return attn_mask\n\n    def forward(self, x, x_size):\n        H, W = x_size\n        B, L, C = x.shape\n        # assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(\n                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n            )\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(\n            shifted_x, self.window_size\n        )  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(\n            -1, self.window_size * self.window_size, C\n        )  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n        if self.input_resolution == x_size:\n            attn_windows = self.attn(\n                x_windows, mask=self.attn_mask\n            )  # nW*B, window_size*window_size, C\n        else:\n            attn_windows = self.attn(\n                x_windows, mask=self.calculate_mask(x_size).to(x.device)\n            )\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(\n                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n            )\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(self.norm1(x))\n\n        # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return (\n            f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \"\n            f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n        )\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(2 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.reduction(x)\n        x = self.norm(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        flops += H * W * self.dim // 2\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n        pretrained_window_size=0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock(\n                    dim=dim,\n                    input_resolution=input_resolution,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                    pretrained_window_size=pretrained_window_size,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, norm_layer=norm_layer\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x, x_size):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, x_size)\n            else:\n                x = blk(x, x_size)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()  # type: ignore\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n    def _init_respostnorm(self):\n        for blk in self.blocks:\n            nn.init.constant_(blk.norm1.bias, 0)  # type: ignore\n            nn.init.constant_(blk.norm1.weight, 0)  # type: ignore\n            nn.init.constant_(blk.norm2.bias, 0)  # type: ignore\n            nn.init.constant_(blk.norm2.weight, 0)  # type: ignore\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]  # type: ignore\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size  # type: ignore\n        )\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        # assert H == self.img_size[0] and W == self.img_size[1],\n        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])  # type: ignore\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        img_size: Input image size.\n        patch_size: Patch size.\n        resi_connection: The convolutional block before residual connection.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        depth,\n        num_heads,\n        window_size,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n        use_checkpoint=False,\n        img_size=224,\n        patch_size=4,\n        resi_connection=\"1conv\",\n    ):\n        super(RSTB, self).__init__()\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n\n        self.residual_group = BasicLayer(\n            dim=dim,\n            input_resolution=input_resolution,\n            depth=depth,\n            num_heads=num_heads,\n            window_size=window_size,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            norm_layer=norm_layer,\n            downsample=downsample,\n            use_checkpoint=use_checkpoint,\n        )\n\n        if resi_connection == \"1conv\":\n            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            # to save parameters and memory\n            self.conv = nn.Sequential(\n                nn.Conv2d(dim, dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(dim // 4, dim, 3, 1, 1),\n            )\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=dim,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=dim,\n            embed_dim=dim,\n            norm_layer=None,\n        )\n\n    def forward(self, x, x_size):\n        return (\n            self.patch_embed(\n                self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))\n            )\n            + x\n        )\n\n    def flops(self):\n        flops = 0\n        flops += self.residual_group.flops()\n        H, W = self.input_resolution\n        flops += H * W * self.dim * self.dim * 9\n        flops += self.patch_embed.flops()\n        flops += self.patch_unembed.flops()\n\n        return flops\n\n\nclass PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]  # type: ignore\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n    def forward(self, x, x_size):\n        B, HW, C = x.shape\n        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n        return x\n\n    def flops(self):\n        flops = 0\n        return flops\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n            )\n        super(Upsample, self).__init__(*m)\n\n\nclass Upsample_hf(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(\n                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n            )\n        super(Upsample_hf, self).__init__(*m)\n\n\nclass UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n\n    \"\"\"\n\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution\n        m = []\n        m.append(nn.Conv2d(num_feat, (scale**2) * num_out_ch, 3, 1, 1))\n        m.append(nn.PixelShuffle(scale))\n        super(UpsampleOneStep, self).__init__(*m)\n\n    def flops(self):\n        H, W = self.input_resolution  # type: ignore\n        flops = H * W * self.num_feat * 3 * 9\n        return flops\n\n\nclass Swin2SR(nn.Module):\n    r\"\"\"Swin2SR\n        A PyTorch impl of : `Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration`.\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n        img_range: Image range. 1. or 255.\n        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n        **kwargs,\n    ):\n        super(Swin2SR, self).__init__()\n\n        # Defaults\n        img_size = 128\n        patch_size = 1\n        in_chans = 3\n        embed_dim = 96\n        depths = [6, 6, 6, 6]\n        num_heads = [6, 6, 6, 6]\n        window_size = 7\n        mlp_ratio = 4.0\n        qkv_bias = True\n        drop_rate = 0.0\n        attn_drop_rate = 0.0\n        drop_path_rate = 0.1\n        norm_layer = nn.LayerNorm\n        ape = False\n        patch_norm = True\n        use_checkpoint = False\n        upscale = 2\n        img_range = 1.0\n        upsampler = \"\"\n        resi_connection = \"1conv\"\n        num_in_ch = in_chans\n        num_out_ch = in_chans\n        num_feat = 64\n\n        self.model_arch = \"Swin2SR\"\n        self.sub_type = \"SR\"\n        self.state = state_dict\n        if \"params_ema\" in self.state:\n            self.state = self.state[\"params_ema\"]\n        elif \"params\" in self.state:\n            self.state = self.state[\"params\"]\n\n        state_keys = self.state.keys()\n\n        if \"conv_before_upsample.0.weight\" in state_keys:\n            if \"conv_aux.weight\" in state_keys:\n                upsampler = \"pixelshuffle_aux\"\n            elif \"conv_up1.weight\" in state_keys:\n                upsampler = \"nearest+conv\"\n            else:\n                upsampler = \"pixelshuffle\"\n                supports_fp16 = False\n        elif \"upsample.0.weight\" in state_keys:\n            upsampler = \"pixelshuffledirect\"\n        else:\n            upsampler = \"\"\n\n        num_feat = (\n            self.state.get(\"conv_before_upsample.0.weight\", None).shape[1]\n            if self.state.get(\"conv_before_upsample.weight\", None)\n            else 64\n        )\n\n        num_in_ch = self.state[\"conv_first.weight\"].shape[1]\n        in_chans = num_in_ch\n        if \"conv_last.weight\" in state_keys:\n            num_out_ch = self.state[\"conv_last.weight\"].shape[0]\n        else:\n            num_out_ch = num_in_ch\n\n        upscale = 1\n        if upsampler == \"nearest+conv\":\n            upsample_keys = [\n                x for x in state_keys if \"conv_up\" in x and \"bias\" not in x\n            ]\n\n            for upsample_key in upsample_keys:\n                upscale *= 2\n        elif upsampler == \"pixelshuffle\" or upsampler == \"pixelshuffle_aux\":\n            upsample_keys = [\n                x\n                for x in state_keys\n                if \"upsample\" in x and \"conv\" not in x and \"bias\" not in x\n            ]\n            for upsample_key in upsample_keys:\n                shape = self.state[upsample_key].shape[0]\n                upscale *= math.sqrt(shape // num_feat)\n            upscale = int(upscale)\n        elif upsampler == \"pixelshuffledirect\":\n            upscale = int(\n                math.sqrt(self.state[\"upsample.0.bias\"].shape[0] // num_out_ch)\n            )\n\n        max_layer_num = 0\n        max_block_num = 0\n        for key in state_keys:\n            result = re.match(\n                r\"layers.(\\d*).residual_group.blocks.(\\d*).norm1.weight\", key\n            )\n            if result:\n                layer_num, block_num = result.groups()\n                max_layer_num = max(max_layer_num, int(layer_num))\n                max_block_num = max(max_block_num, int(block_num))\n\n        depths = [max_block_num + 1 for _ in range(max_layer_num + 1)]\n\n        if (\n            \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            in state_keys\n        ):\n            num_heads_num = self.state[\n                \"layers.0.residual_group.blocks.0.attn.relative_position_bias_table\"\n            ].shape[-1]\n            num_heads = [num_heads_num for _ in range(max_layer_num + 1)]\n        else:\n            num_heads = depths\n\n        embed_dim = self.state[\"conv_first.weight\"].shape[0]\n\n        mlp_ratio = float(\n            self.state[\"layers.0.residual_group.blocks.0.mlp.fc1.bias\"].shape[0]\n            / embed_dim\n        )\n\n        # TODO: could actually count the layers, but this should do\n        if \"layers.0.conv.4.weight\" in state_keys:\n            resi_connection = \"3conv\"\n        else:\n            resi_connection = \"1conv\"\n\n        window_size = int(\n            math.sqrt(\n                self.state[\n                    \"layers.0.residual_group.blocks.0.attn.relative_position_index\"\n                ].shape[0]\n            )\n        )\n\n        if \"layers.0.residual_group.blocks.1.attn_mask\" in state_keys:\n            img_size = int(\n                math.sqrt(\n                    self.state[\"layers.0.residual_group.blocks.1.attn_mask\"].shape[0]\n                )\n                * window_size\n            )\n\n        # The JPEG models are the only ones with window-size 7, and they also use this range\n        img_range = 255.0 if window_size == 7 else 1.0\n\n        self.in_nc = num_in_ch\n        self.out_nc = num_out_ch\n        self.num_feat = num_feat\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.depths = depths\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n        self.scale = upscale\n        self.upsampler = upsampler\n        self.img_size = img_size\n        self.img_range = img_range\n        self.resi_connection = resi_connection\n\n        self.supports_fp16 = False  # Too much weirdness to support this at the moment\n        self.supports_bfp16 = True\n        self.min_size_restriction = 16\n\n        ## END AUTO DETECTION\n\n        if in_chans == 3:\n            rgb_mean = (0.4488, 0.4371, 0.4040)\n            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n        else:\n            self.mean = torch.zeros(1, 1, 1, 1)\n        self.upscale = upscale\n        self.upsampler = upsampler\n        self.window_size = window_size\n\n        #####################################################################################################\n        ################################### 1, shallow feature extraction ###################################\n        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n\n        #####################################################################################################\n        ################################### 2, deep feature extraction ######################################\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # merge non-overlapping patches into image\n        self.patch_unembed = PatchUnEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=embed_dim,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))  # type: ignore\n            trunc_normal_(self.absolute_pos_embed, std=0.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build Residual Swin Transformer blocks (RSTB)\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = RSTB(\n                dim=embed_dim,\n                input_resolution=(patches_resolution[0], patches_resolution[1]),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=self.mlp_ratio,\n                qkv_bias=qkv_bias,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],  # type: ignore    # no impact on SR results\n                norm_layer=norm_layer,\n                downsample=None,\n                use_checkpoint=use_checkpoint,\n                img_size=img_size,\n                patch_size=patch_size,\n                resi_connection=resi_connection,\n            )\n            self.layers.append(layer)\n\n        if self.upsampler == \"pixelshuffle_hf\":\n            self.layers_hf = nn.ModuleList()\n            for i_layer in range(self.num_layers):\n                layer = RSTB(\n                    dim=embed_dim,\n                    input_resolution=(patches_resolution[0], patches_resolution[1]),\n                    depth=depths[i_layer],\n                    num_heads=num_heads[i_layer],\n                    window_size=window_size,\n                    mlp_ratio=self.mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],  # type: ignore    # no impact on SR results # type: ignore\n                    norm_layer=norm_layer,\n                    downsample=None,\n                    use_checkpoint=use_checkpoint,\n                    img_size=img_size,\n                    patch_size=patch_size,\n                    resi_connection=resi_connection,\n                )\n                self.layers_hf.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n\n        # build the last conv layer in deep feature extraction\n        if resi_connection == \"1conv\":\n            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n        elif resi_connection == \"3conv\":\n            # to save parameters and memory\n            self.conv_after_body = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1),\n            )\n\n        #####################################################################################################\n        ################################ 3, high quality image reconstruction ################################\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n        elif self.upsampler == \"pixelshuffle_aux\":\n            self.conv_bicubic = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.conv_aux = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.conv_after_aux = nn.Sequential(\n                nn.Conv2d(3, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n\n        elif self.upsampler == \"pixelshuffle_hf\":\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.upsample = Upsample(upscale, num_feat)\n            self.upsample_hf = Upsample_hf(upscale, num_feat)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.conv_first_hf = nn.Sequential(\n                nn.Conv2d(num_feat, embed_dim, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.conv_after_body_hf = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n            self.conv_before_upsample_hf = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.conv_last_hf = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR (to save parameters)\n            self.upsample = UpsampleOneStep(\n                upscale,\n                embed_dim,\n                num_out_ch,\n                (patches_resolution[0], patches_resolution[1]),\n            )\n        elif self.upsampler == \"nearest+conv\":\n            # for real-world SR (less artifacts)\n            assert self.upscale == 4, \"only support x4 now.\"\n            self.conv_before_upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n            )\n            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n\n        self.apply(self._init_weights)\n\n        self.load_state_dict(state_dict)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay(self):\n        return {\"absolute_pos_embed\"}\n\n    @torch.jit.ignore  # type: ignore\n    def no_weight_decay_keywords(self):\n        return {\"relative_position_bias_table\"}\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"reflect\")\n        return x\n\n    def forward_features(self, x):\n        x_size = (x.shape[2], x.shape[3])\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x, x_size)\n\n        x = self.norm(x)  # B L C\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward_features_hf(self, x):\n        x_size = (x.shape[2], x.shape[3])\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers_hf:\n            x = layer(x, x_size)\n\n        x = self.norm(x)  # B L C\n        x = self.patch_unembed(x, x_size)\n\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        x = self.check_image_size(x)\n\n        self.mean = self.mean.type_as(x)\n        x = (x - self.mean) * self.img_range\n\n        if self.upsampler == \"pixelshuffle\":\n            # for classical SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.conv_last(self.upsample(x))\n        elif self.upsampler == \"pixelshuffle_aux\":\n            bicubic = F.interpolate(\n                x,\n                size=(H * self.upscale, W * self.upscale),\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n            bicubic = self.conv_bicubic(bicubic)\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            aux = self.conv_aux(x)  # b, 3, LR_H, LR_W\n            x = self.conv_after_aux(aux)\n            x = (\n                self.upsample(x)[:, :, : H * self.upscale, : W * self.upscale]\n                + bicubic[:, :, : H * self.upscale, : W * self.upscale]\n            )\n            x = self.conv_last(x)\n            aux = aux / self.img_range + self.mean\n        elif self.upsampler == \"pixelshuffle_hf\":\n            # for classical SR with HF\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x_before = self.conv_before_upsample(x)\n            x_out = self.conv_last(self.upsample(x_before))\n\n            x_hf = self.conv_first_hf(x_before)\n            x_hf = self.conv_after_body_hf(self.forward_features_hf(x_hf)) + x_hf\n            x_hf = self.conv_before_upsample_hf(x_hf)\n            x_hf = self.conv_last_hf(self.upsample_hf(x_hf))\n            x = x_out + x_hf\n            x_hf = x_hf / self.img_range + self.mean\n\n        elif self.upsampler == \"pixelshuffledirect\":\n            # for lightweight SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.upsample(x)\n        elif self.upsampler == \"nearest+conv\":\n            # for real-world SR\n            x = self.conv_first(x)\n            x = self.conv_after_body(self.forward_features(x)) + x\n            x = self.conv_before_upsample(x)\n            x = self.lrelu(\n                self.conv_up1(\n                    torch.nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n                )\n            )\n            x = self.lrelu(\n                self.conv_up2(\n                    torch.nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n                )\n            )\n            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n        else:\n            # for image denoising and JPEG compression artifact reduction\n            x_first = self.conv_first(x)\n            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n            x = x + self.conv_last(res)\n\n        x = x / self.img_range + self.mean\n        if self.upsampler == \"pixelshuffle_aux\":\n            # NOTE: I removed an \"aux\" output here. not sure what that was for\n            return x[:, :, : H * self.upscale, : W * self.upscale]  # type: ignore\n\n        elif self.upsampler == \"pixelshuffle_hf\":\n            x_out = x_out / self.img_range + self.mean  # type: ignore\n            return x_out[:, :, : H * self.upscale, : W * self.upscale], x[:, :, : H * self.upscale, : W * self.upscale], x_hf[:, :, : H * self.upscale, : W * self.upscale]  # type: ignore\n\n        else:\n            return x[:, :, : H * self.upscale, : W * self.upscale]\n\n    def flops(self):\n        flops = 0\n        H, W = self.patches_resolution\n        flops += H * W * 3 * self.embed_dim * 9\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()  # type: ignore\n        flops += H * W * 3 * self.embed_dim * self.embed_dim\n        flops += self.upsample.flops()  # type: ignore\n        return flops\n", "ldm_patched/pfn/architecture/__init__.py": "", "ldm_patched/pfn/architecture/SPSR.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import block as B\n\n\nclass Get_gradient_nopadding(nn.Module):\n    def __init__(self):\n        super(Get_gradient_nopadding, self).__init__()\n        kernel_v = [[0, -1, 0], [0, 0, 0], [0, 1, 0]]\n        kernel_h = [[0, 0, 0], [-1, 0, 1], [0, 0, 0]]\n        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)\n        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)\n        self.weight_h = nn.Parameter(data=kernel_h, requires_grad=False)  # type: ignore\n\n        self.weight_v = nn.Parameter(data=kernel_v, requires_grad=False)  # type: ignore\n\n    def forward(self, x):\n        x_list = []\n        for i in range(x.shape[1]):\n            x_i = x[:, i]\n            x_i_v = F.conv2d(x_i.unsqueeze(1), self.weight_v, padding=1)\n            x_i_h = F.conv2d(x_i.unsqueeze(1), self.weight_h, padding=1)\n            x_i = torch.sqrt(torch.pow(x_i_v, 2) + torch.pow(x_i_h, 2) + 1e-6)\n            x_list.append(x_i)\n\n        x = torch.cat(x_list, dim=1)\n\n        return x\n\n\nclass SPSRNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        norm=None,\n        act: str = \"leakyrelu\",\n        upsampler: str = \"upconv\",\n        mode: B.ConvMode = \"CNA\",\n    ):\n        super(SPSRNet, self).__init__()\n        self.model_arch = \"SPSR\"\n        self.sub_type = \"SR\"\n\n        self.state = state_dict\n        self.norm = norm\n        self.act = act\n        self.upsampler = upsampler\n        self.mode = mode\n\n        self.num_blocks = self.get_num_blocks()\n\n        self.in_nc: int = self.state[\"model.0.weight\"].shape[1]\n        self.out_nc: int = self.state[\"f_HR_conv1.0.bias\"].shape[0]\n\n        self.scale = self.get_scale(4)\n        self.num_filters: int = self.state[\"model.0.weight\"].shape[0]\n\n        self.supports_fp16 = True\n        self.supports_bfp16 = True\n        self.min_size_restriction = None\n\n        n_upscale = int(math.log(self.scale, 2))\n        if self.scale == 3:\n            n_upscale = 1\n\n        fea_conv = B.conv_block(\n            self.in_nc, self.num_filters, kernel_size=3, norm_type=None, act_type=None\n        )\n        rb_blocks = [\n            B.RRDB(\n                self.num_filters,\n                kernel_size=3,\n                gc=32,\n                stride=1,\n                bias=True,\n                pad_type=\"zero\",\n                norm_type=norm,\n                act_type=act,\n                mode=\"CNA\",\n            )\n            for _ in range(self.num_blocks)\n        ]\n        LR_conv = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=norm,\n            act_type=None,\n            mode=mode,\n        )\n\n        if upsampler == \"upconv\":\n            upsample_block = B.upconv_block\n        elif upsampler == \"pixelshuffle\":\n            upsample_block = B.pixelshuffle_block\n        else:\n            raise NotImplementedError(f\"upsample mode [{upsampler}] is not found\")\n        if self.scale == 3:\n            a_upsampler = upsample_block(\n                self.num_filters, self.num_filters, 3, act_type=act\n            )\n        else:\n            a_upsampler = [\n                upsample_block(self.num_filters, self.num_filters, act_type=act)\n                for _ in range(n_upscale)\n            ]\n        self.HR_conv0_new = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=act,\n        )\n        self.HR_conv1_new = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n\n        self.model = B.sequential(\n            fea_conv,\n            B.ShortcutBlockSPSR(B.sequential(*rb_blocks, LR_conv)),\n            *a_upsampler,\n            self.HR_conv0_new,\n        )\n\n        self.get_g_nopadding = Get_gradient_nopadding()\n\n        self.b_fea_conv = B.conv_block(\n            self.in_nc, self.num_filters, kernel_size=3, norm_type=None, act_type=None\n        )\n\n        self.b_concat_1 = B.conv_block(\n            2 * self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n        self.b_block_1 = B.RRDB(\n            self.num_filters * 2,\n            kernel_size=3,\n            gc=32,\n            stride=1,\n            bias=True,\n            pad_type=\"zero\",\n            norm_type=norm,\n            act_type=act,\n            mode=\"CNA\",\n        )\n\n        self.b_concat_2 = B.conv_block(\n            2 * self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n        self.b_block_2 = B.RRDB(\n            self.num_filters * 2,\n            kernel_size=3,\n            gc=32,\n            stride=1,\n            bias=True,\n            pad_type=\"zero\",\n            norm_type=norm,\n            act_type=act,\n            mode=\"CNA\",\n        )\n\n        self.b_concat_3 = B.conv_block(\n            2 * self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n        self.b_block_3 = B.RRDB(\n            self.num_filters * 2,\n            kernel_size=3,\n            gc=32,\n            stride=1,\n            bias=True,\n            pad_type=\"zero\",\n            norm_type=norm,\n            act_type=act,\n            mode=\"CNA\",\n        )\n\n        self.b_concat_4 = B.conv_block(\n            2 * self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n        self.b_block_4 = B.RRDB(\n            self.num_filters * 2,\n            kernel_size=3,\n            gc=32,\n            stride=1,\n            bias=True,\n            pad_type=\"zero\",\n            norm_type=norm,\n            act_type=act,\n            mode=\"CNA\",\n        )\n\n        self.b_LR_conv = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=norm,\n            act_type=None,\n            mode=mode,\n        )\n\n        if upsampler == \"upconv\":\n            upsample_block = B.upconv_block\n        elif upsampler == \"pixelshuffle\":\n            upsample_block = B.pixelshuffle_block\n        else:\n            raise NotImplementedError(f\"upsample mode [{upsampler}] is not found\")\n        if self.scale == 3:\n            b_upsampler = upsample_block(\n                self.num_filters, self.num_filters, 3, act_type=act\n            )\n        else:\n            b_upsampler = [\n                upsample_block(self.num_filters, self.num_filters, act_type=act)\n                for _ in range(n_upscale)\n            ]\n\n        b_HR_conv0 = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=act,\n        )\n        b_HR_conv1 = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n\n        self.b_module = B.sequential(*b_upsampler, b_HR_conv0, b_HR_conv1)\n\n        self.conv_w = B.conv_block(\n            self.num_filters, self.out_nc, kernel_size=1, norm_type=None, act_type=None\n        )\n\n        self.f_concat = B.conv_block(\n            self.num_filters * 2,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=None,\n        )\n\n        self.f_block = B.RRDB(\n            self.num_filters * 2,\n            kernel_size=3,\n            gc=32,\n            stride=1,\n            bias=True,\n            pad_type=\"zero\",\n            norm_type=norm,\n            act_type=act,\n            mode=\"CNA\",\n        )\n\n        self.f_HR_conv0 = B.conv_block(\n            self.num_filters,\n            self.num_filters,\n            kernel_size=3,\n            norm_type=None,\n            act_type=act,\n        )\n        self.f_HR_conv1 = B.conv_block(\n            self.num_filters, self.out_nc, kernel_size=3, norm_type=None, act_type=None\n        )\n\n        self.load_state_dict(self.state, strict=False)\n\n    def get_scale(self, min_part: int = 4) -> int:\n        n = 0\n        for part in list(self.state):\n            parts = part.split(\".\")\n            if len(parts) == 3:\n                part_num = int(parts[1])\n                if part_num > min_part and parts[0] == \"model\" and parts[2] == \"weight\":\n                    n += 1\n        return 2**n\n\n    def get_num_blocks(self) -> int:\n        nb = 0\n        for part in list(self.state):\n            parts = part.split(\".\")\n            n_parts = len(parts)\n            if n_parts == 5 and parts[2] == \"sub\":\n                nb = int(parts[3])\n        return nb\n\n    def forward(self, x):\n        x_grad = self.get_g_nopadding(x)\n        x = self.model[0](x)\n\n        x, block_list = self.model[1](x)\n\n        x_ori = x\n        for i in range(5):\n            x = block_list[i](x)\n        x_fea1 = x\n\n        for i in range(5):\n            x = block_list[i + 5](x)\n        x_fea2 = x\n\n        for i in range(5):\n            x = block_list[i + 10](x)\n        x_fea3 = x\n\n        for i in range(5):\n            x = block_list[i + 15](x)\n        x_fea4 = x\n\n        x = block_list[20:](x)\n        # short cut\n        x = x_ori + x\n        x = self.model[2:](x)\n        x = self.HR_conv1_new(x)\n\n        x_b_fea = self.b_fea_conv(x_grad)\n        x_cat_1 = torch.cat([x_b_fea, x_fea1], dim=1)\n\n        x_cat_1 = self.b_block_1(x_cat_1)\n        x_cat_1 = self.b_concat_1(x_cat_1)\n\n        x_cat_2 = torch.cat([x_cat_1, x_fea2], dim=1)\n\n        x_cat_2 = self.b_block_2(x_cat_2)\n        x_cat_2 = self.b_concat_2(x_cat_2)\n\n        x_cat_3 = torch.cat([x_cat_2, x_fea3], dim=1)\n\n        x_cat_3 = self.b_block_3(x_cat_3)\n        x_cat_3 = self.b_concat_3(x_cat_3)\n\n        x_cat_4 = torch.cat([x_cat_3, x_fea4], dim=1)\n\n        x_cat_4 = self.b_block_4(x_cat_4)\n        x_cat_4 = self.b_concat_4(x_cat_4)\n\n        x_cat_4 = self.b_LR_conv(x_cat_4)\n\n        # short cut\n        x_cat_4 = x_cat_4 + x_b_fea\n        x_branch = self.b_module(x_cat_4)\n\n        # x_out_branch = self.conv_w(x_branch)\n        ########\n        x_branch_d = x_branch\n        x_f_cat = torch.cat([x_branch_d, x], dim=1)\n        x_f_cat = self.f_block(x_f_cat)\n        x_out = self.f_concat(x_f_cat)\n        x_out = self.f_HR_conv0(x_out)\n        x_out = self.f_HR_conv1(x_out)\n\n        #########\n        # return x_out_branch, x_out, x_grad\n        return x_out\n", "ldm_patched/pfn/architecture/LaMa.py": "# pylint: skip-file\n\"\"\"\nModel adapted from advimman's lama project: https://github.com/advimman/lama\n\"\"\"\n\n# Fast Fourier Convolution NeurIPS 2020\n# original implementation https://github.com/pkumivision/FFC/blob/main/model_zoo/ffc.py\n# paper https://proceedings.neurips.cc/paper/2020/file/2fd5d41ec6cfab47e32164d5624269b1-Paper.pdf\n\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms.functional import InterpolationMode, rotate\n\n\nclass LearnableSpatialTransformWrapper(nn.Module):\n    def __init__(self, impl, pad_coef=0.5, angle_init_range=80, train_angle=True):\n        super().__init__()\n        self.impl = impl\n        self.angle = torch.rand(1) * angle_init_range\n        if train_angle:\n            self.angle = nn.Parameter(self.angle, requires_grad=True)\n        self.pad_coef = pad_coef\n\n    def forward(self, x):\n        if torch.is_tensor(x):\n            return self.inverse_transform(self.impl(self.transform(x)), x)\n        elif isinstance(x, tuple):\n            x_trans = tuple(self.transform(elem) for elem in x)\n            y_trans = self.impl(x_trans)\n            return tuple(\n                self.inverse_transform(elem, orig_x) for elem, orig_x in zip(y_trans, x)\n            )\n        else:\n            raise ValueError(f\"Unexpected input type {type(x)}\")\n\n    def transform(self, x):\n        height, width = x.shape[2:]\n        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n        x_padded = F.pad(x, [pad_w, pad_w, pad_h, pad_h], mode=\"reflect\")\n        x_padded_rotated = rotate(\n            x_padded, self.angle.to(x_padded), InterpolationMode.BILINEAR, fill=0\n        )\n\n        return x_padded_rotated\n\n    def inverse_transform(self, y_padded_rotated, orig_x):\n        height, width = orig_x.shape[2:]\n        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n\n        y_padded = rotate(\n            y_padded_rotated,\n            -self.angle.to(y_padded_rotated),\n            InterpolationMode.BILINEAR,\n            fill=0,\n        )\n        y_height, y_width = y_padded.shape[2:]\n        y = y_padded[:, :, pad_h : y_height - pad_h, pad_w : y_width - pad_w]\n        return y\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        res = x * y.expand_as(x)\n        return res\n\n\nclass FourierUnit(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        groups=1,\n        spatial_scale_factor=None,\n        spatial_scale_mode=\"bilinear\",\n        spectral_pos_encoding=False,\n        use_se=False,\n        se_kwargs=None,\n        ffc3d=False,\n        fft_norm=\"ortho\",\n    ):\n        # bn_layer not used\n        super(FourierUnit, self).__init__()\n        self.groups = groups\n\n        self.conv_layer = torch.nn.Conv2d(\n            in_channels=in_channels * 2 + (2 if spectral_pos_encoding else 0),\n            out_channels=out_channels * 2,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            groups=self.groups,\n            bias=False,\n        )\n        self.bn = torch.nn.BatchNorm2d(out_channels * 2)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n        # squeeze and excitation block\n        self.use_se = use_se\n        if use_se:\n            if se_kwargs is None:\n                se_kwargs = {}\n            self.se = SELayer(self.conv_layer.in_channels, **se_kwargs)\n\n        self.spatial_scale_factor = spatial_scale_factor\n        self.spatial_scale_mode = spatial_scale_mode\n        self.spectral_pos_encoding = spectral_pos_encoding\n        self.ffc3d = ffc3d\n        self.fft_norm = fft_norm\n\n    def forward(self, x):\n        half_check = False\n        if x.type() == \"torch.cuda.HalfTensor\":\n            # half only works on gpu anyway\n            half_check = True\n\n        batch = x.shape[0]\n\n        if self.spatial_scale_factor is not None:\n            orig_size = x.shape[-2:]\n            x = F.interpolate(\n                x,\n                scale_factor=self.spatial_scale_factor,\n                mode=self.spatial_scale_mode,\n                align_corners=False,\n            )\n\n        # (batch, c, h, w/2+1, 2)\n        fft_dim = (-3, -2, -1) if self.ffc3d else (-2, -1)\n        if half_check == True:\n            ffted = torch.fft.rfftn(\n                x.float(), dim=fft_dim, norm=self.fft_norm\n            )  # .type(torch.cuda.HalfTensor)\n        else:\n            ffted = torch.fft.rfftn(x, dim=fft_dim, norm=self.fft_norm)\n\n        ffted = torch.stack((ffted.real, ffted.imag), dim=-1)\n        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()  # (batch, c, 2, h, w/2+1)\n        ffted = ffted.view(\n            (\n                batch,\n                -1,\n            )\n            + ffted.size()[3:]\n        )\n\n        if self.spectral_pos_encoding:\n            height, width = ffted.shape[-2:]\n            coords_vert = (\n                torch.linspace(0, 1, height)[None, None, :, None]\n                .expand(batch, 1, height, width)\n                .to(ffted)\n            )\n            coords_hor = (\n                torch.linspace(0, 1, width)[None, None, None, :]\n                .expand(batch, 1, height, width)\n                .to(ffted)\n            )\n            ffted = torch.cat((coords_vert, coords_hor, ffted), dim=1)\n\n        if self.use_se:\n            ffted = self.se(ffted)\n\n        if half_check == True:\n            ffted = self.conv_layer(ffted.half())  # (batch, c*2, h, w/2+1)\n        else:\n            ffted = self.conv_layer(\n                ffted\n            )  # .type(torch.cuda.FloatTensor)  # (batch, c*2, h, w/2+1)\n\n        ffted = self.relu(self.bn(ffted))\n        # forcing to be always float\n        ffted = ffted.float()\n\n        ffted = (\n            ffted.view(\n                (\n                    batch,\n                    -1,\n                    2,\n                )\n                + ffted.size()[2:]\n            )\n            .permute(0, 1, 3, 4, 2)\n            .contiguous()\n        )  # (batch,c, t, h, w/2+1, 2)\n\n        ffted = torch.complex(ffted[..., 0], ffted[..., 1])\n\n        ifft_shape_slice = x.shape[-3:] if self.ffc3d else x.shape[-2:]\n        output = torch.fft.irfftn(\n            ffted, s=ifft_shape_slice, dim=fft_dim, norm=self.fft_norm\n        )\n\n        if half_check == True:\n            output = output.half()\n\n        if self.spatial_scale_factor is not None:\n            output = F.interpolate(\n                output,\n                size=orig_size,\n                mode=self.spatial_scale_mode,\n                align_corners=False,\n            )\n\n        return output\n\n\nclass SpectralTransform(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride=1,\n        groups=1,\n        enable_lfu=True,\n        separable_fu=False,\n        **fu_kwargs,\n    ):\n        # bn_layer not used\n        super(SpectralTransform, self).__init__()\n        self.enable_lfu = enable_lfu\n        if stride == 2:\n            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n        else:\n            self.downsample = nn.Identity()\n\n        self.stride = stride\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels // 2, kernel_size=1, groups=groups, bias=False\n            ),\n            nn.BatchNorm2d(out_channels // 2),\n            nn.ReLU(inplace=True),\n        )\n        fu_class = FourierUnit\n        self.fu = fu_class(out_channels // 2, out_channels // 2, groups, **fu_kwargs)\n        if self.enable_lfu:\n            self.lfu = fu_class(out_channels // 2, out_channels // 2, groups)\n        self.conv2 = torch.nn.Conv2d(\n            out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False\n        )\n\n    def forward(self, x):\n        x = self.downsample(x)\n        x = self.conv1(x)\n        output = self.fu(x)\n\n        if self.enable_lfu:\n            _, c, h, _ = x.shape\n            split_no = 2\n            split_s = h // split_no\n            xs = torch.cat(\n                torch.split(x[:, : c // 4], split_s, dim=-2), dim=1\n            ).contiguous()\n            xs = torch.cat(torch.split(xs, split_s, dim=-1), dim=1).contiguous()\n            xs = self.lfu(xs)\n            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n        else:\n            xs = 0\n\n        output = self.conv2(x + output + xs)\n\n        return output\n\n\nclass FFC(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        ratio_gin,\n        ratio_gout,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=False,\n        enable_lfu=True,\n        padding_type=\"reflect\",\n        gated=False,\n        **spectral_kwargs,\n    ):\n        super(FFC, self).__init__()\n\n        assert stride == 1 or stride == 2, \"Stride should be 1 or 2.\"\n        self.stride = stride\n\n        in_cg = int(in_channels * ratio_gin)\n        in_cl = in_channels - in_cg\n        out_cg = int(out_channels * ratio_gout)\n        out_cl = out_channels - out_cg\n        # groups_g = 1 if groups == 1 else int(groups * ratio_gout)\n        # groups_l = 1 if groups == 1 else groups - groups_g\n\n        self.ratio_gin = ratio_gin\n        self.ratio_gout = ratio_gout\n        self.global_in_num = in_cg\n\n        module = nn.Identity if in_cl == 0 or out_cl == 0 else nn.Conv2d\n        self.convl2l = module(\n            in_cl,\n            out_cl,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode=padding_type,\n        )\n        module = nn.Identity if in_cl == 0 or out_cg == 0 else nn.Conv2d\n        self.convl2g = module(\n            in_cl,\n            out_cg,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode=padding_type,\n        )\n        module = nn.Identity if in_cg == 0 or out_cl == 0 else nn.Conv2d\n        self.convg2l = module(\n            in_cg,\n            out_cl,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode=padding_type,\n        )\n        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform\n        self.convg2g = module(\n            in_cg,\n            out_cg,\n            stride,\n            1 if groups == 1 else groups // 2,\n            enable_lfu,\n            **spectral_kwargs,\n        )\n\n        self.gated = gated\n        module = (\n            nn.Identity if in_cg == 0 or out_cl == 0 or not self.gated else nn.Conv2d\n        )\n        self.gate = module(in_channels, 2, 1)\n\n    def forward(self, x):\n        x_l, x_g = x if type(x) is tuple else (x, 0)\n        out_xl, out_xg = 0, 0\n\n        if self.gated:\n            total_input_parts = [x_l]\n            if torch.is_tensor(x_g):\n                total_input_parts.append(x_g)\n            total_input = torch.cat(total_input_parts, dim=1)\n\n            gates = torch.sigmoid(self.gate(total_input))\n            g2l_gate, l2g_gate = gates.chunk(2, dim=1)\n        else:\n            g2l_gate, l2g_gate = 1, 1\n\n        if self.ratio_gout != 1:\n            out_xl = self.convl2l(x_l) + self.convg2l(x_g) * g2l_gate\n        if self.ratio_gout != 0:\n            out_xg = self.convl2g(x_l) * l2g_gate + self.convg2g(x_g)\n\n        return out_xl, out_xg\n\n\nclass FFC_BN_ACT(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        ratio_gin,\n        ratio_gout,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=False,\n        norm_layer=nn.BatchNorm2d,\n        activation_layer=nn.Identity,\n        padding_type=\"reflect\",\n        enable_lfu=True,\n        **kwargs,\n    ):\n        super(FFC_BN_ACT, self).__init__()\n        self.ffc = FFC(\n            in_channels,\n            out_channels,\n            kernel_size,\n            ratio_gin,\n            ratio_gout,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            enable_lfu,\n            padding_type=padding_type,\n            **kwargs,\n        )\n        lnorm = nn.Identity if ratio_gout == 1 else norm_layer\n        gnorm = nn.Identity if ratio_gout == 0 else norm_layer\n        global_channels = int(out_channels * ratio_gout)\n        self.bn_l = lnorm(out_channels - global_channels)\n        self.bn_g = gnorm(global_channels)\n\n        lact = nn.Identity if ratio_gout == 1 else activation_layer\n        gact = nn.Identity if ratio_gout == 0 else activation_layer\n        self.act_l = lact(inplace=True)\n        self.act_g = gact(inplace=True)\n\n    def forward(self, x):\n        x_l, x_g = self.ffc(x)\n        x_l = self.act_l(self.bn_l(x_l))\n        x_g = self.act_g(self.bn_g(x_g))\n        return x_l, x_g\n\n\nclass FFCResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        padding_type,\n        norm_layer,\n        activation_layer=nn.ReLU,\n        dilation=1,\n        spatial_transform_kwargs=None,\n        inline=False,\n        **conv_kwargs,\n    ):\n        super().__init__()\n        self.conv1 = FFC_BN_ACT(\n            dim,\n            dim,\n            kernel_size=3,\n            padding=dilation,\n            dilation=dilation,\n            norm_layer=norm_layer,\n            activation_layer=activation_layer,\n            padding_type=padding_type,\n            **conv_kwargs,\n        )\n        self.conv2 = FFC_BN_ACT(\n            dim,\n            dim,\n            kernel_size=3,\n            padding=dilation,\n            dilation=dilation,\n            norm_layer=norm_layer,\n            activation_layer=activation_layer,\n            padding_type=padding_type,\n            **conv_kwargs,\n        )\n        if spatial_transform_kwargs is not None:\n            self.conv1 = LearnableSpatialTransformWrapper(\n                self.conv1, **spatial_transform_kwargs\n            )\n            self.conv2 = LearnableSpatialTransformWrapper(\n                self.conv2, **spatial_transform_kwargs\n            )\n        self.inline = inline\n\n    def forward(self, x):\n        if self.inline:\n            x_l, x_g = (\n                x[:, : -self.conv1.ffc.global_in_num],\n                x[:, -self.conv1.ffc.global_in_num :],\n            )\n        else:\n            x_l, x_g = x if type(x) is tuple else (x, 0)\n\n        id_l, id_g = x_l, x_g\n\n        x_l, x_g = self.conv1((x_l, x_g))\n        x_l, x_g = self.conv2((x_l, x_g))\n\n        x_l, x_g = id_l + x_l, id_g + x_g\n        out = x_l, x_g\n        if self.inline:\n            out = torch.cat(out, dim=1)\n        return out\n\n\nclass ConcatTupleLayer(nn.Module):\n    def forward(self, x):\n        assert isinstance(x, tuple)\n        x_l, x_g = x\n        assert torch.is_tensor(x_l) or torch.is_tensor(x_g)\n        if not torch.is_tensor(x_g):\n            return x_l\n        return torch.cat(x, dim=1)\n\n\nclass FFCResNetGenerator(nn.Module):\n    def __init__(\n        self,\n        input_nc,\n        output_nc,\n        ngf=64,\n        n_downsampling=3,\n        n_blocks=18,\n        norm_layer=nn.BatchNorm2d,\n        padding_type=\"reflect\",\n        activation_layer=nn.ReLU,\n        up_norm_layer=nn.BatchNorm2d,\n        up_activation=nn.ReLU(True),\n        init_conv_kwargs={},\n        downsample_conv_kwargs={},\n        resnet_conv_kwargs={},\n        spatial_transform_layers=None,\n        spatial_transform_kwargs={},\n        max_features=1024,\n        out_ffc=False,\n        out_ffc_kwargs={},\n    ):\n        assert n_blocks >= 0\n        super().__init__()\n        \"\"\"\n        init_conv_kwargs = {'ratio_gin': 0, 'ratio_gout': 0, 'enable_lfu': False}\n        downsample_conv_kwargs = {'ratio_gin': '${generator.init_conv_kwargs.ratio_gout}', 'ratio_gout': '${generator.downsample_conv_kwargs.ratio_gin}', 'enable_lfu': False}\n        resnet_conv_kwargs = {'ratio_gin': 0.75, 'ratio_gout': '${generator.resnet_conv_kwargs.ratio_gin}', 'enable_lfu': False}\n        spatial_transform_kwargs = {}\n        out_ffc_kwargs = {}\n        \"\"\"\n        \"\"\"\n        print(input_nc, output_nc, ngf, n_downsampling, n_blocks, norm_layer,\n                padding_type, activation_layer,\n                up_norm_layer, up_activation,\n                spatial_transform_layers,\n                add_out_act, max_features, out_ffc, file=sys.stderr)\n\n        4 3 64 3 18 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n        reflect <class 'torch.nn.modules.activation.ReLU'>\n        <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n        ReLU(inplace=True)\n        None sigmoid 1024 False\n        \"\"\"\n        init_conv_kwargs = {\"ratio_gin\": 0, \"ratio_gout\": 0, \"enable_lfu\": False}\n        downsample_conv_kwargs = {\"ratio_gin\": 0, \"ratio_gout\": 0, \"enable_lfu\": False}\n        resnet_conv_kwargs = {\n            \"ratio_gin\": 0.75,\n            \"ratio_gout\": 0.75,\n            \"enable_lfu\": False,\n        }\n        spatial_transform_kwargs = {}\n        out_ffc_kwargs = {}\n\n        model = [\n            nn.ReflectionPad2d(3),\n            FFC_BN_ACT(\n                input_nc,\n                ngf,\n                kernel_size=7,\n                padding=0,\n                norm_layer=norm_layer,\n                activation_layer=activation_layer,\n                **init_conv_kwargs,\n            ),\n        ]\n\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            if i == n_downsampling - 1:\n                cur_conv_kwargs = dict(downsample_conv_kwargs)\n                cur_conv_kwargs[\"ratio_gout\"] = resnet_conv_kwargs.get(\"ratio_gin\", 0)\n            else:\n                cur_conv_kwargs = downsample_conv_kwargs\n            model += [\n                FFC_BN_ACT(\n                    min(max_features, ngf * mult),\n                    min(max_features, ngf * mult * 2),\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                    **cur_conv_kwargs,\n                )\n            ]\n\n        mult = 2**n_downsampling\n        feats_num_bottleneck = min(max_features, ngf * mult)\n\n        ### resnet blocks\n        for i in range(n_blocks):\n            cur_resblock = FFCResnetBlock(\n                feats_num_bottleneck,\n                padding_type=padding_type,\n                activation_layer=activation_layer,\n                norm_layer=norm_layer,\n                **resnet_conv_kwargs,\n            )\n            if spatial_transform_layers is not None and i in spatial_transform_layers:\n                cur_resblock = LearnableSpatialTransformWrapper(\n                    cur_resblock, **spatial_transform_kwargs\n                )\n            model += [cur_resblock]\n\n        model += [ConcatTupleLayer()]\n\n        ### upsample\n        for i in range(n_downsampling):\n            mult = 2 ** (n_downsampling - i)\n            model += [\n                nn.ConvTranspose2d(\n                    min(max_features, ngf * mult),\n                    min(max_features, int(ngf * mult / 2)),\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                ),\n                up_norm_layer(min(max_features, int(ngf * mult / 2))),\n                up_activation,\n            ]\n\n        if out_ffc:\n            model += [\n                FFCResnetBlock(\n                    ngf,\n                    padding_type=padding_type,\n                    activation_layer=activation_layer,\n                    norm_layer=norm_layer,\n                    inline=True,\n                    **out_ffc_kwargs,\n                )\n            ]\n\n        model += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n        ]\n        model.append(nn.Sigmoid())\n        self.model = nn.Sequential(*model)\n\n    def forward(self, image, mask):\n        return self.model(torch.cat([image, mask], dim=1))\n\n\nclass LaMa(nn.Module):\n    def __init__(self, state_dict) -> None:\n        super(LaMa, self).__init__()\n        self.model_arch = \"LaMa\"\n        self.sub_type = \"Inpaint\"\n        self.in_nc = 4\n        self.out_nc = 3\n        self.scale = 1\n\n        self.min_size = None\n        self.pad_mod = 8\n        self.pad_to_square = False\n\n        self.model = FFCResNetGenerator(self.in_nc, self.out_nc)\n        self.state = {\n            k.replace(\"generator.model\", \"model.model\"): v\n            for k, v in state_dict.items()\n        }\n\n        self.supports_fp16 = False\n        self.support_bf16 = True\n\n        self.load_state_dict(self.state, strict=False)\n\n    def forward(self, img, mask):\n        masked_img = img * (1 - mask)\n        inpainted_mask = mask * self.model.forward(masked_img, mask)\n        result = inpainted_mask + (1 - mask) * img\n        return result\n", "ldm_patched/pfn/architecture/SwiftSRGAN.py": "# From https://github.com/Koushik0901/Swift-SRGAN/blob/master/swift-srgan/models.py\n\nimport torch\nfrom torch import nn\n\n\nclass SeperableConv2d(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, padding=1, bias=True\n    ):\n        super(SeperableConv2d, self).__init__()\n        self.depthwise = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            groups=in_channels,\n            bias=bias,\n            padding=padding,\n        )\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        return self.pointwise(self.depthwise(x))\n\n\nclass ConvBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        use_act=True,\n        use_bn=True,\n        discriminator=False,\n        **kwargs,\n    ):\n        super(ConvBlock, self).__init__()\n\n        self.use_act = use_act\n        self.cnn = SeperableConv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n        self.act = (\n            nn.LeakyReLU(0.2, inplace=True)\n            if discriminator\n            else nn.PReLU(num_parameters=out_channels)\n        )\n\n    def forward(self, x):\n        return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))\n\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self, in_channels, scale_factor):\n        super(UpsampleBlock, self).__init__()\n\n        self.conv = SeperableConv2d(\n            in_channels,\n            in_channels * scale_factor**2,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        )\n        self.ps = nn.PixelShuffle(\n            scale_factor\n        )  # (in_channels * 4, H, W) -> (in_channels, H*2, W*2)\n        self.act = nn.PReLU(num_parameters=in_channels)\n\n    def forward(self, x):\n        return self.act(self.ps(self.conv(x)))\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n\n        self.block1 = ConvBlock(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.block2 = ConvBlock(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1, use_act=False\n        )\n\n    def forward(self, x):\n        out = self.block1(x)\n        out = self.block2(out)\n        return out + x\n\n\nclass Generator(nn.Module):\n    \"\"\"Swift-SRGAN Generator\n    Args:\n        in_channels (int): number of input image channels.\n        num_channels (int): number of hidden channels.\n        num_blocks (int): number of residual blocks.\n        upscale_factor (int): factor to upscale the image [2x, 4x, 8x].\n    Returns:\n        torch.Tensor: super resolution image\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n    ):\n        super(Generator, self).__init__()\n        self.model_arch = \"Swift-SRGAN\"\n        self.sub_type = \"SR\"\n        self.state = state_dict\n        if \"model\" in self.state:\n            self.state = self.state[\"model\"]\n\n        self.in_nc: int = self.state[\"initial.cnn.depthwise.weight\"].shape[0]\n        self.out_nc: int = self.state[\"final_conv.pointwise.weight\"].shape[0]\n        self.num_filters: int = self.state[\"initial.cnn.pointwise.weight\"].shape[0]\n        self.num_blocks = len(\n            set([x.split(\".\")[1] for x in self.state.keys() if \"residual\" in x])\n        )\n        self.scale: int = 2 ** len(\n            set([x.split(\".\")[1] for x in self.state.keys() if \"upsampler\" in x])\n        )\n\n        in_channels = self.in_nc\n        num_channels = self.num_filters\n        num_blocks = self.num_blocks\n        upscale_factor = self.scale\n\n        self.supports_fp16 = True\n        self.supports_bfp16 = True\n        self.min_size_restriction = None\n\n        self.initial = ConvBlock(\n            in_channels, num_channels, kernel_size=9, stride=1, padding=4, use_bn=False\n        )\n        self.residual = nn.Sequential(\n            *[ResidualBlock(num_channels) for _ in range(num_blocks)]\n        )\n        self.convblock = ConvBlock(\n            num_channels,\n            num_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            use_act=False,\n        )\n        self.upsampler = nn.Sequential(\n            *[\n                UpsampleBlock(num_channels, scale_factor=2)\n                for _ in range(upscale_factor // 2)\n            ]\n        )\n        self.final_conv = SeperableConv2d(\n            num_channels, in_channels, kernel_size=9, stride=1, padding=4\n        )\n\n        self.load_state_dict(self.state, strict=False)\n\n    def forward(self, x):\n        initial = self.initial(x)\n        x = self.residual(initial)\n        x = self.convblock(x) + initial\n        x = self.upsampler(x)\n        return (torch.tanh(self.final_conv(x)) + 1) / 2\n", "ldm_patched/pfn/architecture/OmniSR/ChannelAttention.py": "import math\n\nimport torch.nn as nn\n\n\nclass CA_layer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(CA_layer, self).__init__()\n        # global average pooling\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channel, channel // reduction, kernel_size=(1, 1), bias=False),\n            nn.GELU(),\n            nn.Conv2d(channel // reduction, channel, kernel_size=(1, 1), bias=False),\n            # nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.fc(self.gap(x))\n        return x * y.expand_as(x)\n\n\nclass Simple_CA_layer(nn.Module):\n    def __init__(self, channel):\n        super(Simple_CA_layer, self).__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(\n            in_channels=channel,\n            out_channels=channel,\n            kernel_size=1,\n            padding=0,\n            stride=1,\n            groups=1,\n            bias=True,\n        )\n\n    def forward(self, x):\n        return x * self.fc(self.gap(x))\n\n\nclass ECA_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n\n    def __init__(self, channel):\n        super(ECA_layer, self).__init__()\n\n        b = 1\n        gamma = 2\n        k_size = int(abs(math.log(channel, 2) + b) / gamma)\n        k_size = k_size if k_size % 2 else k_size + 1\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(\n            1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False\n        )\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: input features with shape [b, c, h, w]\n        # b, c, h, w = x.size()\n\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        # y = self.sigmoid(y)\n\n        return x * y.expand_as(x)\n\n\nclass ECA_MaxPool_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n\n    def __init__(self, channel):\n        super(ECA_MaxPool_layer, self).__init__()\n\n        b = 1\n        gamma = 2\n        k_size = int(abs(math.log(channel, 2) + b) / gamma)\n        k_size = k_size if k_size % 2 else k_size + 1\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.conv = nn.Conv1d(\n            1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False\n        )\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: input features with shape [b, c, h, w]\n        # b, c, h, w = x.size()\n\n        # feature descriptor on the global spatial information\n        y = self.max_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        # y = self.sigmoid(y)\n\n        return x * y.expand_as(x)\n", "ldm_patched/pfn/architecture/OmniSR/OmniSR.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: OmniSR.py\n# Created Date: Tuesday April 28th 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Sunday, 23rd April 2023 3:06:36 pm\n# Modified By: Chen Xuanhong\n# Copyright (c) 2020 Shanghai Jiao Tong University\n#############################################################\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .OSAG import OSAG\nfrom .pixelshuffle import pixelshuffle_block\n\n\nclass OmniSR(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        **kwargs,\n    ):\n        super(OmniSR, self).__init__()\n        self.state = state_dict\n\n        bias = True  # Fine to assume this for now\n        block_num = 1  # Fine to assume this for now\n        ffn_bias = True\n        pe = True\n\n        num_feat = state_dict[\"input.weight\"].shape[0] or 64\n        num_in_ch = state_dict[\"input.weight\"].shape[1] or 3\n        num_out_ch = num_in_ch  # we can just assume this for now. pixelshuffle smh\n\n        pixelshuffle_shape = state_dict[\"up.0.weight\"].shape[0]\n        up_scale = math.sqrt(pixelshuffle_shape / num_out_ch)\n        if up_scale - int(up_scale) > 0:\n            print(\n                \"out_nc is probably different than in_nc, scale calculation might be wrong\"\n            )\n        up_scale = int(up_scale)\n        res_num = 0\n        for key in state_dict.keys():\n            if \"residual_layer\" in key:\n                temp_res_num = int(key.split(\".\")[1])\n                if temp_res_num > res_num:\n                    res_num = temp_res_num\n        res_num = res_num + 1  # zero-indexed\n\n        residual_layer = []\n        self.res_num = res_num\n\n        if (\n            \"residual_layer.0.residual_layer.0.layer.2.fn.rel_pos_bias.weight\"\n            in state_dict.keys()\n        ):\n            rel_pos_bias_weight = state_dict[\n                \"residual_layer.0.residual_layer.0.layer.2.fn.rel_pos_bias.weight\"\n            ].shape[0]\n            self.window_size = int((math.sqrt(rel_pos_bias_weight) + 1) / 2)\n        else:\n            self.window_size = 8\n\n        self.up_scale = up_scale\n\n        for _ in range(res_num):\n            temp_res = OSAG(\n                channel_num=num_feat,\n                bias=bias,\n                block_num=block_num,\n                ffn_bias=ffn_bias,\n                window_size=self.window_size,\n                pe=pe,\n            )\n            residual_layer.append(temp_res)\n        self.residual_layer = nn.Sequential(*residual_layer)\n        self.input = nn.Conv2d(\n            in_channels=num_in_ch,\n            out_channels=num_feat,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=bias,\n        )\n        self.output = nn.Conv2d(\n            in_channels=num_feat,\n            out_channels=num_feat,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=bias,\n        )\n        self.up = pixelshuffle_block(num_feat, num_out_ch, up_scale, bias=bias)\n\n        # self.tail   = pixelshuffle_block(num_feat,num_out_ch,up_scale,bias=bias)\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, sqrt(2. / n))\n\n        # chaiNNer specific stuff\n        self.model_arch = \"OmniSR\"\n        self.sub_type = \"SR\"\n        self.in_nc = num_in_ch\n        self.out_nc = num_out_ch\n        self.num_feat = num_feat\n        self.scale = up_scale\n\n        self.supports_fp16 = True  # TODO: Test this\n        self.supports_bfp16 = True\n        self.min_size_restriction = 16\n\n        self.load_state_dict(state_dict, strict=False)\n\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        # import pdb; pdb.set_trace()\n        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n        # x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), \"constant\", 0)\n        return x\n\n    def forward(self, x):\n        H, W = x.shape[2:]\n        x = self.check_image_size(x)\n\n        residual = self.input(x)\n        out = self.residual_layer(residual)\n\n        # origin\n        out = torch.add(self.output(out), residual)\n        out = self.up(out)\n\n        out = out[:, :, : H * self.up_scale, : W * self.up_scale]\n        return out\n", "ldm_patched/pfn/architecture/OmniSR/esa.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: esa.py\n# Created Date: Tuesday April 28th 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Thursday, 20th April 2023 9:28:06 am\n# Modified By: Chen Xuanhong\n# Copyright (c) 2020 Shanghai Jiao Tong University\n#############################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .layernorm import LayerNorm2d\n\n\ndef moment(x, dim=(2, 3), k=2):\n    assert len(x.size()) == 4\n    mean = torch.mean(x, dim=dim).unsqueeze(-1).unsqueeze(-1)\n    mk = (1 / (x.size(2) * x.size(3))) * torch.sum(torch.pow(x - mean, k), dim=dim)\n    return mk\n\n\nclass ESA(nn.Module):\n    \"\"\"\n    Modification of Enhanced Spatial Attention (ESA), which is proposed by\n    `Residual Feature Aggregation Network for Image Super-Resolution`\n    Note: `conv_max` and `conv3_` are NOT used here, so the corresponding codes\n    are deleted.\n    \"\"\"\n\n    def __init__(self, esa_channels, n_feats, conv=nn.Conv2d):\n        super(ESA, self).__init__()\n        f = esa_channels\n        self.conv1 = conv(n_feats, f, kernel_size=1)\n        self.conv_f = conv(f, f, kernel_size=1)\n        self.conv2 = conv(f, f, kernel_size=3, stride=2, padding=0)\n        self.conv3 = conv(f, f, kernel_size=3, padding=1)\n        self.conv4 = conv(f, n_feats, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        c1_ = self.conv1(x)\n        c1 = self.conv2(c1_)\n        v_max = F.max_pool2d(c1, kernel_size=7, stride=3)\n        c3 = self.conv3(v_max)\n        c3 = F.interpolate(\n            c3, (x.size(2), x.size(3)), mode=\"bilinear\", align_corners=False\n        )\n        cf = self.conv_f(c1_)\n        c4 = self.conv4(c3 + cf)\n        m = self.sigmoid(c4)\n        return x * m\n\n\nclass LK_ESA(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(LK_ESA, self).__init__()\n        f = esa_channels\n        self.conv1 = conv(n_feats, f, kernel_size=1)\n        self.conv_f = conv(f, f, kernel_size=1)\n\n        kernel_size = 17\n        kernel_expand = kernel_expand\n        padding = kernel_size // 2\n\n        self.vec_conv = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(1, kernel_size),\n            padding=(0, padding),\n            groups=2,\n            bias=bias,\n        )\n        self.vec_conv3x1 = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(1, 3),\n            padding=(0, 1),\n            groups=2,\n            bias=bias,\n        )\n\n        self.hor_conv = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(kernel_size, 1),\n            padding=(padding, 0),\n            groups=2,\n            bias=bias,\n        )\n        self.hor_conv1x3 = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(3, 1),\n            padding=(1, 0),\n            groups=2,\n            bias=bias,\n        )\n\n        self.conv4 = conv(f, n_feats, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        c1_ = self.conv1(x)\n\n        res = self.vec_conv(c1_) + self.vec_conv3x1(c1_)\n        res = self.hor_conv(res) + self.hor_conv1x3(res)\n\n        cf = self.conv_f(c1_)\n        c4 = self.conv4(res + cf)\n        m = self.sigmoid(c4)\n        return x * m\n\n\nclass LK_ESA_LN(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(LK_ESA_LN, self).__init__()\n        f = esa_channels\n        self.conv1 = conv(n_feats, f, kernel_size=1)\n        self.conv_f = conv(f, f, kernel_size=1)\n\n        kernel_size = 17\n        kernel_expand = kernel_expand\n        padding = kernel_size // 2\n\n        self.norm = LayerNorm2d(n_feats)\n\n        self.vec_conv = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(1, kernel_size),\n            padding=(0, padding),\n            groups=2,\n            bias=bias,\n        )\n        self.vec_conv3x1 = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(1, 3),\n            padding=(0, 1),\n            groups=2,\n            bias=bias,\n        )\n\n        self.hor_conv = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(kernel_size, 1),\n            padding=(padding, 0),\n            groups=2,\n            bias=bias,\n        )\n        self.hor_conv1x3 = nn.Conv2d(\n            in_channels=f * kernel_expand,\n            out_channels=f * kernel_expand,\n            kernel_size=(3, 1),\n            padding=(1, 0),\n            groups=2,\n            bias=bias,\n        )\n\n        self.conv4 = conv(f, n_feats, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        c1_ = self.norm(x)\n        c1_ = self.conv1(c1_)\n\n        res = self.vec_conv(c1_) + self.vec_conv3x1(c1_)\n        res = self.hor_conv(res) + self.hor_conv1x3(res)\n\n        cf = self.conv_f(c1_)\n        c4 = self.conv4(res + cf)\n        m = self.sigmoid(c4)\n        return x * m\n\n\nclass AdaGuidedFilter(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(AdaGuidedFilter, self).__init__()\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(\n            in_channels=n_feats,\n            out_channels=1,\n            kernel_size=1,\n            padding=0,\n            stride=1,\n            groups=1,\n            bias=True,\n        )\n\n        self.r = 5\n\n    def box_filter(self, x, r):\n        channel = x.shape[1]\n        kernel_size = 2 * r + 1\n        weight = 1.0 / (kernel_size**2)\n        box_kernel = weight * torch.ones(\n            (channel, 1, kernel_size, kernel_size), dtype=torch.float32, device=x.device\n        )\n        output = F.conv2d(x, weight=box_kernel, stride=1, padding=r, groups=channel)\n        return output\n\n    def forward(self, x):\n        _, _, H, W = x.shape\n        N = self.box_filter(\n            torch.ones((1, 1, H, W), dtype=x.dtype, device=x.device), self.r\n        )\n\n        # epsilon = self.fc(self.gap(x))\n        # epsilon = torch.pow(epsilon, 2)\n        epsilon = 1e-2\n\n        mean_x = self.box_filter(x, self.r) / N\n        var_x = self.box_filter(x * x, self.r) / N - mean_x * mean_x\n\n        A = var_x / (var_x + epsilon)\n        b = (1 - A) * mean_x\n        m = A * x + b\n\n        # mean_A = self.box_filter(A, self.r) / N\n        # mean_b = self.box_filter(b, self.r) / N\n        # m = mean_A * x + mean_b\n        return x * m\n\n\nclass AdaConvGuidedFilter(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(AdaConvGuidedFilter, self).__init__()\n        f = esa_channels\n\n        self.conv_f = conv(f, f, kernel_size=1)\n\n        kernel_size = 17\n        kernel_expand = kernel_expand\n        padding = kernel_size // 2\n\n        self.vec_conv = nn.Conv2d(\n            in_channels=f,\n            out_channels=f,\n            kernel_size=(1, kernel_size),\n            padding=(0, padding),\n            groups=f,\n            bias=bias,\n        )\n\n        self.hor_conv = nn.Conv2d(\n            in_channels=f,\n            out_channels=f,\n            kernel_size=(kernel_size, 1),\n            padding=(padding, 0),\n            groups=f,\n            bias=bias,\n        )\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(\n            in_channels=f,\n            out_channels=f,\n            kernel_size=1,\n            padding=0,\n            stride=1,\n            groups=1,\n            bias=True,\n        )\n\n    def forward(self, x):\n        y = self.vec_conv(x)\n        y = self.hor_conv(y)\n\n        sigma = torch.pow(y, 2)\n        epsilon = self.fc(self.gap(y))\n\n        weight = sigma / (sigma + epsilon)\n\n        m = weight * x + (1 - weight)\n\n        return x * m\n", "ldm_patched/pfn/architecture/OmniSR/pixelshuffle.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: pixelshuffle.py\n# Created Date: Friday July 1st 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Friday, 1st July 2022 10:18:39 am\n# Modified By: Chen Xuanhong\n# Copyright (c) 2022 Shanghai Jiao Tong University\n#############################################################\n\nimport torch.nn as nn\n\n\ndef pixelshuffle_block(\n    in_channels, out_channels, upscale_factor=2, kernel_size=3, bias=False\n):\n    \"\"\"\n    Upsample features according to `upscale_factor`.\n    \"\"\"\n    padding = kernel_size // 2\n    conv = nn.Conv2d(\n        in_channels,\n        out_channels * (upscale_factor**2),\n        kernel_size,\n        padding=1,\n        bias=bias,\n    )\n    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n    return nn.Sequential(*[conv, pixel_shuffle])\n", "ldm_patched/pfn/architecture/OmniSR/layernorm.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: layernorm.py\n# Created Date: Tuesday April 28th 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Thursday, 20th April 2023 9:28:20 am\n# Modified By: Chen Xuanhong\n# Copyright (c) 2020 Shanghai Jiao Tong University\n#############################################################\n\nimport torch\nimport torch.nn as nn\n\n\nclass LayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, eps):\n        ctx.eps = eps\n        N, C, H, W = x.size()\n        mu = x.mean(1, keepdim=True)\n        var = (x - mu).pow(2).mean(1, keepdim=True)\n        y = (x - mu) / (var + eps).sqrt()\n        ctx.save_for_backward(y, var, weight)\n        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        eps = ctx.eps\n\n        N, C, H, W = grad_output.size()\n        y, var, weight = ctx.saved_variables\n        g = grad_output * weight.view(1, C, 1, 1)\n        mean_g = g.mean(dim=1, keepdim=True)\n\n        mean_gy = (g * y).mean(dim=1, keepdim=True)\n        gx = 1.0 / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n        return (\n            gx,\n            (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0),\n            grad_output.sum(dim=3).sum(dim=2).sum(dim=0),\n            None,\n        )\n\n\nclass LayerNorm2d(nn.Module):\n    def __init__(self, channels, eps=1e-6):\n        super(LayerNorm2d, self).__init__()\n        self.register_parameter(\"weight\", nn.Parameter(torch.ones(channels)))\n        self.register_parameter(\"bias\", nn.Parameter(torch.zeros(channels)))\n        self.eps = eps\n\n    def forward(self, x):\n        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n\n\nclass GRN(nn.Module):\n    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n\n    def forward(self, x):\n        Gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n        Nx = Gx / (Gx.mean(dim=1, keepdim=True) + 1e-6)\n        return self.gamma * (x * Nx) + self.beta + x\n", "ldm_patched/pfn/architecture/OmniSR/OSAG.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: OSAG.py\n# Created Date: Tuesday April 28th 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Sunday, 23rd April 2023 3:08:49 pm\n# Modified By: Chen Xuanhong\n# Copyright (c) 2020 Shanghai Jiao Tong University\n#############################################################\n\n\nimport torch.nn as nn\n\nfrom .esa import ESA\nfrom .OSA import OSA_Block\n\n\nclass OSAG(nn.Module):\n    def __init__(\n        self,\n        channel_num=64,\n        bias=True,\n        block_num=4,\n        ffn_bias=False,\n        window_size=0,\n        pe=False,\n    ):\n        super(OSAG, self).__init__()\n\n        # print(\"window_size: %d\" % (window_size))\n        # print(\"with_pe\", pe)\n        # print(\"ffn_bias: %d\" % (ffn_bias))\n\n        # block_script_name = kwargs.get(\"block_script_name\", \"OSA\")\n        # block_class_name = kwargs.get(\"block_class_name\", \"OSA_Block\")\n\n        # script_name = \".\" + block_script_name\n        # package = __import__(script_name, fromlist=True)\n        block_class = OSA_Block  # getattr(package, block_class_name)\n        group_list = []\n        for _ in range(block_num):\n            temp_res = block_class(\n                channel_num,\n                bias,\n                ffn_bias=ffn_bias,\n                window_size=window_size,\n                with_pe=pe,\n            )\n            group_list.append(temp_res)\n        group_list.append(nn.Conv2d(channel_num, channel_num, 1, 1, 0, bias=bias))\n        self.residual_layer = nn.Sequential(*group_list)\n        esa_channel = max(channel_num // 4, 16)\n        self.esa = ESA(esa_channel, channel_num)\n\n    def forward(self, x):\n        out = self.residual_layer(x)\n        out = out + x\n        return self.esa(out)\n", "ldm_patched/pfn/architecture/OmniSR/OSA.py": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n#############################################################\n# File: OSA.py\n# Created Date: Tuesday April 28th 2022\n# Author: Chen Xuanhong\n# Email: chenxuanhongzju@outlook.com\n# Last Modified:  Sunday, 23rd April 2023 3:07:42 pm\n# Modified By: Chen Xuanhong\n# Copyright (c) 2020 Shanghai Jiao Tong University\n#############################################################\n\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange, Reduce\nfrom torch import einsum, nn\n\nfrom .layernorm import LayerNorm2d\n\n# helpers\n\n\ndef exists(val):\n    return val is not None\n\n\ndef default(val, d):\n    return val if exists(val) else d\n\n\ndef cast_tuple(val, length=1):\n    return val if isinstance(val, tuple) else ((val,) * length)\n\n\n# helper classes\n\n\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\n\n\nclass Conv_PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = LayerNorm2d(dim)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult=2, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Conv_FeedForward(nn.Module):\n    def __init__(self, dim, mult=2, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Conv2d(dim, inner_dim, 1, 1, 0),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(inner_dim, dim, 1, 1, 0),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Gated_Conv_FeedForward(nn.Module):\n    def __init__(self, dim, mult=1, bias=False, dropout=0.0):\n        super().__init__()\n\n        hidden_features = int(dim * mult)\n\n        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n\n        self.dwconv = nn.Conv2d(\n            hidden_features * 2,\n            hidden_features * 2,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=hidden_features * 2,\n            bias=bias,\n        )\n\n        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        x = self.project_in(x)\n        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n        x = F.gelu(x1) * x2\n        x = self.project_out(x)\n        return x\n\n\n# MBConv\n\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, dim, shrinkage_rate=0.25):\n        super().__init__()\n        hidden_dim = int(dim * shrinkage_rate)\n\n        self.gate = nn.Sequential(\n            Reduce(\"b c h w -> b c\", \"mean\"),\n            nn.Linear(dim, hidden_dim, bias=False),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, dim, bias=False),\n            nn.Sigmoid(),\n            Rearrange(\"b c -> b c 1 1\"),\n        )\n\n    def forward(self, x):\n        return x * self.gate(x)\n\n\nclass MBConvResidual(nn.Module):\n    def __init__(self, fn, dropout=0.0):\n        super().__init__()\n        self.fn = fn\n        self.dropsample = Dropsample(dropout)\n\n    def forward(self, x):\n        out = self.fn(x)\n        out = self.dropsample(out)\n        return out + x\n\n\nclass Dropsample(nn.Module):\n    def __init__(self, prob=0):\n        super().__init__()\n        self.prob = prob\n\n    def forward(self, x):\n        device = x.device\n\n        if self.prob == 0.0 or (not self.training):\n            return x\n\n        keep_mask = (\n            torch.FloatTensor((x.shape[0], 1, 1, 1), device=device).uniform_()\n            > self.prob\n        )\n        return x * keep_mask / (1 - self.prob)\n\n\ndef MBConv(\n    dim_in, dim_out, *, downsample, expansion_rate=4, shrinkage_rate=0.25, dropout=0.0\n):\n    hidden_dim = int(expansion_rate * dim_out)\n    stride = 2 if downsample else 1\n\n    net = nn.Sequential(\n        nn.Conv2d(dim_in, hidden_dim, 1),\n        # nn.BatchNorm2d(hidden_dim),\n        nn.GELU(),\n        nn.Conv2d(\n            hidden_dim, hidden_dim, 3, stride=stride, padding=1, groups=hidden_dim\n        ),\n        # nn.BatchNorm2d(hidden_dim),\n        nn.GELU(),\n        SqueezeExcitation(hidden_dim, shrinkage_rate=shrinkage_rate),\n        nn.Conv2d(hidden_dim, dim_out, 1),\n        # nn.BatchNorm2d(dim_out)\n    )\n\n    if dim_in == dim_out and not downsample:\n        net = MBConvResidual(net, dropout=dropout)\n\n    return net\n\n\n# attention related classes\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=32,\n        dropout=0.0,\n        window_size=7,\n        with_pe=True,\n    ):\n        super().__init__()\n        assert (\n            dim % dim_head\n        ) == 0, \"dimension should be divisible by dimension per head\"\n\n        self.heads = dim // dim_head\n        self.scale = dim_head**-0.5\n        self.with_pe = with_pe\n\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n\n        self.attend = nn.Sequential(nn.Softmax(dim=-1), nn.Dropout(dropout))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(dim, dim, bias=False), nn.Dropout(dropout)\n        )\n\n        # relative positional bias\n        if self.with_pe:\n            self.rel_pos_bias = nn.Embedding((2 * window_size - 1) ** 2, self.heads)\n\n            pos = torch.arange(window_size)\n            grid = torch.stack(torch.meshgrid(pos, pos))\n            grid = rearrange(grid, \"c i j -> (i j) c\")\n            rel_pos = rearrange(grid, \"i ... -> i 1 ...\") - rearrange(\n                grid, \"j ... -> 1 j ...\"\n            )\n            rel_pos += window_size - 1\n            rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(\n                dim=-1\n            )\n\n            self.register_buffer(\"rel_pos_indices\", rel_pos_indices, persistent=False)\n\n    def forward(self, x):\n        batch, height, width, window_height, window_width, _, device, h = (\n            *x.shape,\n            x.device,\n            self.heads,\n        )\n\n        # flatten\n\n        x = rearrange(x, \"b x y w1 w2 d -> (b x y) (w1 w2) d\")\n\n        # project for queries, keys, values\n\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n\n        # split heads\n\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d ) -> b h n d\", h=h), (q, k, v))\n\n        # scale\n\n        q = q * self.scale\n\n        # sim\n\n        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k)\n\n        # add positional bias\n        if self.with_pe:\n            bias = self.rel_pos_bias(self.rel_pos_indices)\n            sim = sim + rearrange(bias, \"i j h -> h i j\")\n\n        # attention\n\n        attn = self.attend(sim)\n\n        # aggregate\n\n        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n\n        # merge heads\n\n        out = rearrange(\n            out, \"b h (w1 w2) d -> b w1 w2 (h d)\", w1=window_height, w2=window_width\n        )\n\n        # combine heads out\n\n        out = self.to_out(out)\n        return rearrange(out, \"(b x y) ... -> b x y ...\", x=height, y=width)\n\n\nclass Block_Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=32,\n        bias=False,\n        dropout=0.0,\n        window_size=7,\n        with_pe=True,\n    ):\n        super().__init__()\n        assert (\n            dim % dim_head\n        ) == 0, \"dimension should be divisible by dimension per head\"\n\n        self.heads = dim // dim_head\n        self.ps = window_size\n        self.scale = dim_head**-0.5\n        self.with_pe = with_pe\n\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(\n            dim * 3,\n            dim * 3,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=dim * 3,\n            bias=bias,\n        )\n\n        self.attend = nn.Sequential(nn.Softmax(dim=-1), nn.Dropout(dropout))\n\n        self.to_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        # project for queries, keys, values\n        b, c, h, w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        q, k, v = qkv.chunk(3, dim=1)\n\n        # split heads\n\n        q, k, v = map(\n            lambda t: rearrange(\n                t,\n                \"b (h d) (x w1) (y w2) -> (b x y) h (w1 w2) d\",\n                h=self.heads,\n                w1=self.ps,\n                w2=self.ps,\n            ),\n            (q, k, v),\n        )\n\n        # scale\n\n        q = q * self.scale\n\n        # sim\n\n        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k)\n\n        # attention\n        attn = self.attend(sim)\n\n        # aggregate\n\n        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n\n        # merge heads\n        out = rearrange(\n            out,\n            \"(b x y) head (w1 w2) d -> b (head d) (x w1) (y w2)\",\n            x=h // self.ps,\n            y=w // self.ps,\n            head=self.heads,\n            w1=self.ps,\n            w2=self.ps,\n        )\n\n        out = self.to_out(out)\n        return out\n\n\nclass Channel_Attention(nn.Module):\n    def __init__(self, dim, heads, bias=False, dropout=0.0, window_size=7):\n        super(Channel_Attention, self).__init__()\n        self.heads = heads\n\n        self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n\n        self.ps = window_size\n\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(\n            dim * 3,\n            dim * 3,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=dim * 3,\n            bias=bias,\n        )\n        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        qkv = qkv.chunk(3, dim=1)\n\n        q, k, v = map(\n            lambda t: rearrange(\n                t,\n                \"b (head d) (h ph) (w pw) -> b (h w) head d (ph pw)\",\n                ph=self.ps,\n                pw=self.ps,\n                head=self.heads,\n            ),\n            qkv,\n        )\n\n        q = F.normalize(q, dim=-1)\n        k = F.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n        out = attn @ v\n\n        out = rearrange(\n            out,\n            \"b (h w) head d (ph pw) -> b (head d) (h ph) (w pw)\",\n            h=h // self.ps,\n            w=w // self.ps,\n            ph=self.ps,\n            pw=self.ps,\n            head=self.heads,\n        )\n\n        out = self.project_out(out)\n\n        return out\n\n\nclass Channel_Attention_grid(nn.Module):\n    def __init__(self, dim, heads, bias=False, dropout=0.0, window_size=7):\n        super(Channel_Attention_grid, self).__init__()\n        self.heads = heads\n\n        self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n\n        self.ps = window_size\n\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(\n            dim * 3,\n            dim * 3,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=dim * 3,\n            bias=bias,\n        )\n        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        qkv = qkv.chunk(3, dim=1)\n\n        q, k, v = map(\n            lambda t: rearrange(\n                t,\n                \"b (head d) (h ph) (w pw) -> b (ph pw) head d (h w)\",\n                ph=self.ps,\n                pw=self.ps,\n                head=self.heads,\n            ),\n            qkv,\n        )\n\n        q = F.normalize(q, dim=-1)\n        k = F.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n        out = attn @ v\n\n        out = rearrange(\n            out,\n            \"b (ph pw) head d (h w) -> b (head d) (h ph) (w pw)\",\n            h=h // self.ps,\n            w=w // self.ps,\n            ph=self.ps,\n            pw=self.ps,\n            head=self.heads,\n        )\n\n        out = self.project_out(out)\n\n        return out\n\n\nclass OSA_Block(nn.Module):\n    def __init__(\n        self,\n        channel_num=64,\n        bias=True,\n        ffn_bias=True,\n        window_size=8,\n        with_pe=False,\n        dropout=0.0,\n    ):\n        super(OSA_Block, self).__init__()\n\n        w = window_size\n\n        self.layer = nn.Sequential(\n            MBConv(\n                channel_num,\n                channel_num,\n                downsample=False,\n                expansion_rate=1,\n                shrinkage_rate=0.25,\n            ),\n            Rearrange(\n                \"b d (x w1) (y w2) -> b x y w1 w2 d\", w1=w, w2=w\n            ),  # block-like attention\n            PreNormResidual(\n                channel_num,\n                Attention(\n                    dim=channel_num,\n                    dim_head=channel_num // 4,\n                    dropout=dropout,\n                    window_size=window_size,\n                    with_pe=with_pe,\n                ),\n            ),\n            Rearrange(\"b x y w1 w2 d -> b d (x w1) (y w2)\"),\n            Conv_PreNormResidual(\n                channel_num, Gated_Conv_FeedForward(dim=channel_num, dropout=dropout)\n            ),\n            # channel-like attention\n            Conv_PreNormResidual(\n                channel_num,\n                Channel_Attention(\n                    dim=channel_num, heads=4, dropout=dropout, window_size=window_size\n                ),\n            ),\n            Conv_PreNormResidual(\n                channel_num, Gated_Conv_FeedForward(dim=channel_num, dropout=dropout)\n            ),\n            Rearrange(\n                \"b d (w1 x) (w2 y) -> b x y w1 w2 d\", w1=w, w2=w\n            ),  # grid-like attention\n            PreNormResidual(\n                channel_num,\n                Attention(\n                    dim=channel_num,\n                    dim_head=channel_num // 4,\n                    dropout=dropout,\n                    window_size=window_size,\n                    with_pe=with_pe,\n                ),\n            ),\n            Rearrange(\"b x y w1 w2 d -> b d (w1 x) (w2 y)\"),\n            Conv_PreNormResidual(\n                channel_num, Gated_Conv_FeedForward(dim=channel_num, dropout=dropout)\n            ),\n            # channel-like attention\n            Conv_PreNormResidual(\n                channel_num,\n                Channel_Attention_grid(\n                    dim=channel_num, heads=4, dropout=dropout, window_size=window_size\n                ),\n            ),\n            Conv_PreNormResidual(\n                channel_num, Gated_Conv_FeedForward(dim=channel_num, dropout=dropout)\n            ),\n        )\n\n    def forward(self, x):\n        out = self.layer(x)\n        return out\n", "ldm_patched/pfn/architecture/timm/weight_init.py": "import math\nimport warnings\n\nimport torch\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\n            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n            \"The distribution of values may be incorrect.\",\n            stacklevel=2,\n        )\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.0))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(\n    tensor: torch.Tensor, mean=0.0, std=1.0, a=-2.0, b=2.0\n) -> torch.Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n    applied while sampling the normal with mean/std applied, therefore a, b args\n    should be adjusted to match the range of mean, std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n\ndef trunc_normal_tf_(\n    tensor: torch.Tensor, mean=0.0, std=1.0, a=-2.0, b=2.0\n) -> torch.Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n    and the result is subsquently scaled and shifted by the mean and std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    _no_grad_trunc_normal_(tensor, 0, 1.0, a, b)\n    with torch.no_grad():\n        tensor.mul_(std).add_(mean)\n    return tensor\n\n\ndef variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    if mode == \"fan_in\":\n        denom = fan_in\n    elif mode == \"fan_out\":\n        denom = fan_out\n    elif mode == \"fan_avg\":\n        denom = (fan_in + fan_out) / 2\n\n    variance = scale / denom  # type: ignore\n\n    if distribution == \"truncated_normal\":\n        # constant is stddev of standard normal truncated to (-2, 2)\n        trunc_normal_tf_(tensor, std=math.sqrt(variance) / 0.87962566103423978)\n    elif distribution == \"normal\":\n        tensor.normal_(std=math.sqrt(variance))\n    elif distribution == \"uniform\":\n        bound = math.sqrt(3 * variance)\n        # pylint: disable=invalid-unary-operand-type\n        tensor.uniform_(-bound, bound)\n    else:\n        raise ValueError(f\"invalid distribution {distribution}\")\n\n\ndef lecun_normal_(tensor):\n    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"truncated_normal\")\n", "ldm_patched/pfn/architecture/timm/helpers.py": "\"\"\" Layer/Module Helpers\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport collections.abc\nfrom itertools import repeat\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v\n", "ldm_patched/pfn/architecture/timm/drop.py": "\"\"\" DropBlock, DropPath\n\nPyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.\n\nPapers:\nDropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)\n\nDeep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)\n\nCode:\nDropBlock impl inspired by two Tensorflow impl that I liked:\n - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74\n - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef drop_block_2d(\n    x,\n    drop_prob: float = 0.1,\n    block_size: int = 7,\n    gamma_scale: float = 1.0,\n    with_noise: bool = False,\n    inplace: bool = False,\n    batchwise: bool = False,\n):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n    \"\"\"\n    _, C, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    # seed_drop_rate, the gamma parameter\n    gamma = (\n        gamma_scale\n        * drop_prob\n        * total_size\n        / clipped_block_size**2\n        / ((W - block_size + 1) * (H - block_size + 1))\n    )\n\n    # Forces the block to be inside the feature map.\n    w_i, h_i = torch.meshgrid(\n        torch.arange(W).to(x.device), torch.arange(H).to(x.device)\n    )\n    valid_block = (\n        (w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)\n    ) & ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))\n    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)\n\n    if batchwise:\n        # one mask for whole batch, quite a bit faster\n        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)\n    else:\n        uniform_noise = torch.rand_like(x)\n    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)\n    block_mask = -F.max_pool2d(\n        -block_mask,\n        kernel_size=clipped_block_size,  # block_size,\n        stride=1,\n        padding=clipped_block_size // 2,\n    )\n\n    if with_noise:\n        normal_noise = (\n            torch.randn((1, C, H, W), dtype=x.dtype, device=x.device)\n            if batchwise\n            else torch.randn_like(x)\n        )\n        if inplace:\n            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))\n        else:\n            x = x * block_mask + normal_noise * (1 - block_mask)\n    else:\n        normalize_scale = (\n            block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)\n        ).to(x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\ndef drop_block_fast_2d(\n    x: torch.Tensor,\n    drop_prob: float = 0.1,\n    block_size: int = 7,\n    gamma_scale: float = 1.0,\n    with_noise: bool = False,\n    inplace: bool = False,\n):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n    block mask at edges.\n    \"\"\"\n    _, _, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    gamma = (\n        gamma_scale\n        * drop_prob\n        * total_size\n        / clipped_block_size**2\n        / ((W - block_size + 1) * (H - block_size + 1))\n    )\n\n    block_mask = torch.empty_like(x).bernoulli_(gamma)\n    block_mask = F.max_pool2d(\n        block_mask.to(x.dtype),\n        kernel_size=clipped_block_size,\n        stride=1,\n        padding=clipped_block_size // 2,\n    )\n\n    if with_noise:\n        normal_noise = torch.empty_like(x).normal_()\n        if inplace:\n            x.mul_(1.0 - block_mask).add_(normal_noise * block_mask)\n        else:\n            x = x * (1.0 - block_mask) + normal_noise * block_mask\n    else:\n        block_mask = 1 - block_mask\n        normalize_scale = (\n            block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-6)\n        ).to(dtype=x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\nclass DropBlock2d(nn.Module):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n\n    def __init__(\n        self,\n        drop_prob: float = 0.1,\n        block_size: int = 7,\n        gamma_scale: float = 1.0,\n        with_noise: bool = False,\n        inplace: bool = False,\n        batchwise: bool = False,\n        fast: bool = True,\n    ):\n        super(DropBlock2d, self).__init__()\n        self.drop_prob = drop_prob\n        self.gamma_scale = gamma_scale\n        self.block_size = block_size\n        self.with_noise = with_noise\n        self.inplace = inplace\n        self.batchwise = batchwise\n        self.fast = fast  # FIXME finish comparisons of fast vs not\n\n    def forward(self, x):\n        if not self.training or not self.drop_prob:\n            return x\n        if self.fast:\n            return drop_block_fast_2d(\n                x,\n                self.drop_prob,\n                self.block_size,\n                self.gamma_scale,\n                self.with_noise,\n                self.inplace,\n            )\n        else:\n            return drop_block_2d(\n                x,\n                self.drop_prob,\n                self.block_size,\n                self.gamma_scale,\n                self.with_noise,\n                self.inplace,\n                self.batchwise,\n            )\n\n\ndef drop_path(\n    x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True\n):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob: float = 0.0, scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n    def extra_repr(self):\n        return f\"drop_prob={round(self.drop_prob,3):0.3f}\"\n", "ldm_patched/pfn/architecture/face/codeformer.py": "\"\"\"\nModified from https://github.com/sczhou/CodeFormer\nVQGAN code, adapted from the original created by the Unleashing Transformers authors:\nhttps://github.com/samb-t/unleashing-transformers/blob/master/models/vqgan.py\nThis version of the arch specifically was gathered from an old version of GFPGAN. If this is a problem, please contact me.\n\"\"\"\nimport math\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging as logger\nfrom torch import Tensor\n\n\nclass VectorQuantizer(nn.Module):\n    def __init__(self, codebook_size, emb_dim, beta):\n        super(VectorQuantizer, self).__init__()\n        self.codebook_size = codebook_size  # number of embeddings\n        self.emb_dim = emb_dim  # dimension of embedding\n        self.beta = beta  # commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n        self.embedding = nn.Embedding(self.codebook_size, self.emb_dim)\n        self.embedding.weight.data.uniform_(\n            -1.0 / self.codebook_size, 1.0 / self.codebook_size\n        )\n\n    def forward(self, z):\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.emb_dim)\n\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n        d = (\n            (z_flattened**2).sum(dim=1, keepdim=True)\n            + (self.embedding.weight**2).sum(1)\n            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n        )\n\n        mean_distance = torch.mean(d)\n        # find closest encodings\n        # min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n        min_encoding_scores, min_encoding_indices = torch.topk(\n            d, 1, dim=1, largest=False\n        )\n        # [0-1], higher score, higher confidence\n        min_encoding_scores = torch.exp(-min_encoding_scores / 10)\n\n        min_encodings = torch.zeros(\n            min_encoding_indices.shape[0], self.codebook_size\n        ).to(z)\n        min_encodings.scatter_(1, min_encoding_indices, 1)\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n        # compute loss for embedding\n        loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean(\n            (z_q - z.detach()) ** 2\n        )\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # perplexity\n        e_mean = torch.mean(min_encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return (\n            z_q,\n            loss,\n            {\n                \"perplexity\": perplexity,\n                \"min_encodings\": min_encodings,\n                \"min_encoding_indices\": min_encoding_indices,\n                \"min_encoding_scores\": min_encoding_scores,\n                \"mean_distance\": mean_distance,\n            },\n        )\n\n    def get_codebook_feat(self, indices, shape):\n        # input indices: batch*token_num -> (batch*token_num)*1\n        # shape: batch, height, width, channel\n        indices = indices.view(-1, 1)\n        min_encodings = torch.zeros(indices.shape[0], self.codebook_size).to(indices)\n        min_encodings.scatter_(1, indices, 1)\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n\n        if shape is not None:  # reshape back to match original input shape\n            z_q = z_q.view(shape).permute(0, 3, 1, 2).contiguous()\n\n        return z_q\n\n\nclass GumbelQuantizer(nn.Module):\n    def __init__(\n        self,\n        codebook_size,\n        emb_dim,\n        num_hiddens,\n        straight_through=False,\n        kl_weight=5e-4,\n        temp_init=1.0,\n    ):\n        super().__init__()\n        self.codebook_size = codebook_size  # number of embeddings\n        self.emb_dim = emb_dim  # dimension of embedding\n        self.straight_through = straight_through\n        self.temperature = temp_init\n        self.kl_weight = kl_weight\n        self.proj = nn.Conv2d(\n            num_hiddens, codebook_size, 1\n        )  # projects last encoder layer to quantized logits\n        self.embed = nn.Embedding(codebook_size, emb_dim)\n\n    def forward(self, z):\n        hard = self.straight_through if self.training else True\n\n        logits = self.proj(z)\n\n        soft_one_hot = F.gumbel_softmax(logits, tau=self.temperature, dim=1, hard=hard)\n\n        z_q = torch.einsum(\"b n h w, n d -> b d h w\", soft_one_hot, self.embed.weight)\n\n        # + kl divergence to the prior loss\n        qy = F.softmax(logits, dim=1)\n        diff = (\n            self.kl_weight\n            * torch.sum(qy * torch.log(qy * self.codebook_size + 1e-10), dim=1).mean()\n        )\n        min_encoding_indices = soft_one_hot.argmax(dim=1)\n\n        return z_q, diff, {\"min_encoding_indices\": min_encoding_indices}\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=2, padding=0\n        )\n\n    def forward(self, x):\n        pad = (0, 1, 0, 1)\n        x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n        x = self.conv(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        x = self.conv(x)\n\n        return x\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = normalize(in_channels)\n        self.q = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.k = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.v = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.proj_out = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b, c, h, w = q.shape\n        q = q.reshape(b, c, h * w)\n        q = q.permute(0, 2, 1)\n        k = k.reshape(b, c, h * w)\n        w_ = torch.bmm(q, k)\n        w_ = w_ * (int(c) ** (-0.5))\n        w_ = F.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b, c, h * w)\n        w_ = w_.permute(0, 2, 1)\n        h_ = torch.bmm(v, w_)\n        h_ = h_.reshape(b, c, h, w)\n\n        h_ = self.proj_out(h_)\n\n        return x + h_\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        nf,\n        out_channels,\n        ch_mult,\n        num_res_blocks,\n        resolution,\n        attn_resolutions,\n    ):\n        super().__init__()\n        self.nf = nf\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.attn_resolutions = attn_resolutions\n\n        curr_res = self.resolution\n        in_ch_mult = (1,) + tuple(ch_mult)\n\n        blocks = []\n        # initial convultion\n        blocks.append(nn.Conv2d(in_channels, nf, kernel_size=3, stride=1, padding=1))\n\n        # residual and downsampling blocks, with attention on smaller res (16x16)\n        for i in range(self.num_resolutions):\n            block_in_ch = nf * in_ch_mult[i]\n            block_out_ch = nf * ch_mult[i]\n            for _ in range(self.num_res_blocks):\n                blocks.append(ResBlock(block_in_ch, block_out_ch))\n                block_in_ch = block_out_ch\n                if curr_res in attn_resolutions:\n                    blocks.append(AttnBlock(block_in_ch))\n\n            if i != self.num_resolutions - 1:\n                blocks.append(Downsample(block_in_ch))\n                curr_res = curr_res // 2\n\n        # non-local attention block\n        blocks.append(ResBlock(block_in_ch, block_in_ch))  # type: ignore\n        blocks.append(AttnBlock(block_in_ch))  # type: ignore\n        blocks.append(ResBlock(block_in_ch, block_in_ch))  # type: ignore\n\n        # normalise and convert to latent size\n        blocks.append(normalize(block_in_ch))  # type: ignore\n        blocks.append(\n            nn.Conv2d(block_in_ch, out_channels, kernel_size=3, stride=1, padding=1)  # type: ignore\n        )\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self, nf, ch_mult, res_blocks, img_size, attn_resolutions, emb_dim):\n        super().__init__()\n        self.nf = nf\n        self.ch_mult = ch_mult\n        self.num_resolutions = len(self.ch_mult)\n        self.num_res_blocks = res_blocks\n        self.resolution = img_size\n        self.attn_resolutions = attn_resolutions\n        self.in_channels = emb_dim\n        self.out_channels = 3\n        block_in_ch = self.nf * self.ch_mult[-1]\n        curr_res = self.resolution // 2 ** (self.num_resolutions - 1)\n\n        blocks = []\n        # initial conv\n        blocks.append(\n            nn.Conv2d(self.in_channels, block_in_ch, kernel_size=3, stride=1, padding=1)\n        )\n\n        # non-local attention block\n        blocks.append(ResBlock(block_in_ch, block_in_ch))\n        blocks.append(AttnBlock(block_in_ch))\n        blocks.append(ResBlock(block_in_ch, block_in_ch))\n\n        for i in reversed(range(self.num_resolutions)):\n            block_out_ch = self.nf * self.ch_mult[i]\n\n            for _ in range(self.num_res_blocks):\n                blocks.append(ResBlock(block_in_ch, block_out_ch))\n                block_in_ch = block_out_ch\n\n                if curr_res in self.attn_resolutions:\n                    blocks.append(AttnBlock(block_in_ch))\n\n            if i != 0:\n                blocks.append(Upsample(block_in_ch))\n                curr_res = curr_res * 2\n\n        blocks.append(normalize(block_in_ch))\n        blocks.append(\n            nn.Conv2d(\n                block_in_ch, self.out_channels, kernel_size=3, stride=1, padding=1\n            )\n        )\n\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n\n        return x\n\n\nclass VQAutoEncoder(nn.Module):\n    def __init__(\n        self,\n        img_size,\n        nf,\n        ch_mult,\n        quantizer=\"nearest\",\n        res_blocks=2,\n        attn_resolutions=[16],\n        codebook_size=1024,\n        emb_dim=256,\n        beta=0.25,\n        gumbel_straight_through=False,\n        gumbel_kl_weight=1e-8,\n        model_path=None,\n    ):\n        super().__init__()\n        self.in_channels = 3\n        self.nf = nf\n        self.n_blocks = res_blocks\n        self.codebook_size = codebook_size\n        self.embed_dim = emb_dim\n        self.ch_mult = ch_mult\n        self.resolution = img_size\n        self.attn_resolutions = attn_resolutions\n        self.quantizer_type = quantizer\n        self.encoder = Encoder(\n            self.in_channels,\n            self.nf,\n            self.embed_dim,\n            self.ch_mult,\n            self.n_blocks,\n            self.resolution,\n            self.attn_resolutions,\n        )\n        if self.quantizer_type == \"nearest\":\n            self.beta = beta  # 0.25\n            self.quantize = VectorQuantizer(\n                self.codebook_size, self.embed_dim, self.beta\n            )\n        elif self.quantizer_type == \"gumbel\":\n            self.gumbel_num_hiddens = emb_dim\n            self.straight_through = gumbel_straight_through\n            self.kl_weight = gumbel_kl_weight\n            self.quantize = GumbelQuantizer(\n                self.codebook_size,\n                self.embed_dim,\n                self.gumbel_num_hiddens,\n                self.straight_through,\n                self.kl_weight,\n            )\n        self.generator = Generator(\n            nf, ch_mult, res_blocks, img_size, attn_resolutions, emb_dim\n        )\n\n        if model_path is not None:\n            chkpt = torch.load(model_path, map_location=\"cpu\")\n            if \"params_ema\" in chkpt:\n                self.load_state_dict(\n                    torch.load(model_path, map_location=\"cpu\")[\"params_ema\"]\n                )\n                logger.info(f\"vqgan is loaded from: {model_path} [params_ema]\")\n            elif \"params\" in chkpt:\n                self.load_state_dict(\n                    torch.load(model_path, map_location=\"cpu\")[\"params\"]\n                )\n                logger.info(f\"vqgan is loaded from: {model_path} [params]\")\n            else:\n                raise ValueError(\"Wrong params!\")\n\n    def forward(self, x):\n        x = self.encoder(x)\n        quant, codebook_loss, quant_stats = self.quantize(x)\n        x = self.generator(quant)\n        return x, codebook_loss, quant_stats\n\n\ndef calc_mean_std(feat, eps=1e-5):\n    \"\"\"Calculate mean and std for adaptive_instance_normalization.\n    Args:\n        feat (Tensor): 4D tensor.\n        eps (float): A small value added to the variance to avoid\n            divide-by-zero. Default: 1e-5.\n    \"\"\"\n    size = feat.size()\n    assert len(size) == 4, \"The input feature should be 4D tensor.\"\n    b, c = size[:2]\n    feat_var = feat.view(b, c, -1).var(dim=2) + eps\n    feat_std = feat_var.sqrt().view(b, c, 1, 1)\n    feat_mean = feat.view(b, c, -1).mean(dim=2).view(b, c, 1, 1)\n    return feat_mean, feat_std\n\n\ndef adaptive_instance_normalization(content_feat, style_feat):\n    \"\"\"Adaptive instance normalization.\n    Adjust the reference features to have the similar color and illuminations\n    as those in the degradate features.\n    Args:\n        content_feat (Tensor): The reference feature.\n        style_feat (Tensor): The degradate features.\n    \"\"\"\n    size = content_feat.size()\n    style_mean, style_std = calc_mean_std(style_feat)\n    content_mean, content_std = calc_mean_std(content_feat)\n    normalized_feat = (content_feat - content_mean.expand(size)) / content_std.expand(\n        size\n    )\n    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n\n\nclass PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n\n    def __init__(\n        self, num_pos_feats=64, temperature=10000, normalize=False, scale=None\n    ):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, x, mask=None):\n        if mask is None:\n            mask = torch.zeros(\n                (x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool\n            )\n        not_mask = ~mask  # pylint: disable=invalid-unary-operand-type\n        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack(\n            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos_y = torch.stack(\n            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\n\n\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n\n\nclass TransformerSALayer(nn.Module):\n    def __init__(\n        self, embed_dim, nhead=8, dim_mlp=2048, dropout=0.0, activation=\"gelu\"\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(embed_dim, nhead, dropout=dropout)\n        # Implementation of Feedforward model - MLP\n        self.linear1 = nn.Linear(embed_dim, dim_mlp)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_mlp, embed_dim)\n\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward(\n        self,\n        tgt,\n        tgt_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        # self attention\n        tgt2 = self.norm1(tgt)\n        q = k = self.with_pos_embed(tgt2, query_pos)\n        tgt2 = self.self_attn(\n            q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout1(tgt2)\n\n        # ffn\n        tgt2 = self.norm2(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n        tgt = tgt + self.dropout2(tgt2)\n        return tgt\n\n\ndef normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\n\n\n@torch.jit.script  # type: ignore\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None):\n        super(ResBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        self.norm1 = normalize(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1  # type: ignore\n        )\n        self.norm2 = normalize(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1  # type: ignore\n        )\n        if self.in_channels != self.out_channels:\n            self.conv_out = nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0  # type: ignore\n            )\n\n    def forward(self, x_in):\n        x = x_in\n        x = self.norm1(x)\n        x = swish(x)\n        x = self.conv1(x)\n        x = self.norm2(x)\n        x = swish(x)\n        x = self.conv2(x)\n        if self.in_channels != self.out_channels:\n            x_in = self.conv_out(x_in)\n\n        return x + x_in\n\n\nclass Fuse_sft_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.encode_enc = ResBlock(2 * in_ch, out_ch)\n\n        self.scale = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n        )\n\n        self.shift = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n        )\n\n    def forward(self, enc_feat, dec_feat, w=1):\n        enc_feat = self.encode_enc(torch.cat([enc_feat, dec_feat], dim=1))\n        scale = self.scale(enc_feat)\n        shift = self.shift(enc_feat)\n        residual = w * (dec_feat * scale + shift)\n        out = dec_feat + residual\n        return out\n\n\nclass CodeFormer(VQAutoEncoder):\n    def __init__(self, state_dict):\n        dim_embd = 512\n        n_head = 8\n        n_layers = 9\n        codebook_size = 1024\n        latent_size = 256\n        connect_list = [\"32\", \"64\", \"128\", \"256\"]\n        fix_modules = [\"quantize\", \"generator\"]\n\n        # This is just a guess as I only have one model to look at\n        position_emb = state_dict[\"position_emb\"]\n        dim_embd = position_emb.shape[1]\n        latent_size = position_emb.shape[0]\n\n        try:\n            n_layers = len(\n                set([x.split(\".\")[1] for x in state_dict.keys() if \"ft_layers\" in x])\n            )\n        except:\n            pass\n\n        codebook_size = state_dict[\"quantize.embedding.weight\"].shape[0]\n\n        # This is also just another guess\n        n_head_exp = (\n            state_dict[\"ft_layers.0.self_attn.in_proj_weight\"].shape[0] // dim_embd\n        )\n        n_head = 2**n_head_exp\n\n        in_nc = state_dict[\"encoder.blocks.0.weight\"].shape[1]\n\n        self.model_arch = \"CodeFormer\"\n        self.sub_type = \"Face SR\"\n        self.scale = 8\n        self.in_nc = in_nc\n        self.out_nc = in_nc\n\n        self.state = state_dict\n\n        self.supports_fp16 = False\n        self.supports_bf16 = True\n        self.min_size_restriction = 16\n\n        super(CodeFormer, self).__init__(\n            512, 64, [1, 2, 2, 4, 4, 8], \"nearest\", 2, [16], codebook_size\n        )\n\n        if fix_modules is not None:\n            for module in fix_modules:\n                for param in getattr(self, module).parameters():\n                    param.requires_grad = False\n\n        self.connect_list = connect_list\n        self.n_layers = n_layers\n        self.dim_embd = dim_embd\n        self.dim_mlp = dim_embd * 2\n\n        self.position_emb = nn.Parameter(torch.zeros(latent_size, self.dim_embd))  # type: ignore\n        self.feat_emb = nn.Linear(256, self.dim_embd)\n\n        # transformer\n        self.ft_layers = nn.Sequential(\n            *[\n                TransformerSALayer(\n                    embed_dim=dim_embd, nhead=n_head, dim_mlp=self.dim_mlp, dropout=0.0\n                )\n                for _ in range(self.n_layers)\n            ]\n        )\n\n        # logits_predict head\n        self.idx_pred_layer = nn.Sequential(\n            nn.LayerNorm(dim_embd), nn.Linear(dim_embd, codebook_size, bias=False)\n        )\n\n        self.channels = {\n            \"16\": 512,\n            \"32\": 256,\n            \"64\": 256,\n            \"128\": 128,\n            \"256\": 128,\n            \"512\": 64,\n        }\n\n        # after second residual block for > 16, before attn layer for ==16\n        self.fuse_encoder_block = {\n            \"512\": 2,\n            \"256\": 5,\n            \"128\": 8,\n            \"64\": 11,\n            \"32\": 14,\n            \"16\": 18,\n        }\n        # after first residual block for > 16, before attn layer for ==16\n        self.fuse_generator_block = {\n            \"16\": 6,\n            \"32\": 9,\n            \"64\": 12,\n            \"128\": 15,\n            \"256\": 18,\n            \"512\": 21,\n        }\n\n        # fuse_convs_dict\n        self.fuse_convs_dict = nn.ModuleDict()\n        for f_size in self.connect_list:\n            in_ch = self.channels[f_size]\n            self.fuse_convs_dict[f_size] = Fuse_sft_block(in_ch, in_ch)\n\n        self.load_state_dict(state_dict)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, x, weight=0.5, **kwargs):\n        detach_16 = True\n        code_only = False\n        adain = True\n        # ################### Encoder #####################\n        enc_feat_dict = {}\n        out_list = [self.fuse_encoder_block[f_size] for f_size in self.connect_list]\n        for i, block in enumerate(self.encoder.blocks):\n            x = block(x)\n            if i in out_list:\n                enc_feat_dict[str(x.shape[-1])] = x.clone()\n\n        lq_feat = x\n        # ################# Transformer ###################\n        # quant_feat, codebook_loss, quant_stats = self.quantize(lq_feat)\n        pos_emb = self.position_emb.unsqueeze(1).repeat(1, x.shape[0], 1)\n        # BCHW -> BC(HW) -> (HW)BC\n        feat_emb = self.feat_emb(lq_feat.flatten(2).permute(2, 0, 1))\n        query_emb = feat_emb\n        # Transformer encoder\n        for layer in self.ft_layers:\n            query_emb = layer(query_emb, query_pos=pos_emb)\n\n        # output logits\n        logits = self.idx_pred_layer(query_emb)  # (hw)bn\n        logits = logits.permute(1, 0, 2)  # (hw)bn -> b(hw)n\n\n        if code_only:  # for training stage II\n            # logits doesn't need softmax before cross_entropy loss\n            return logits, lq_feat\n\n        # ################# Quantization ###################\n        # if self.training:\n        #     quant_feat = torch.einsum('btn,nc->btc', [soft_one_hot, self.quantize.embedding.weight])\n        #     # b(hw)c -> bc(hw) -> bchw\n        #     quant_feat = quant_feat.permute(0,2,1).view(lq_feat.shape)\n        # ------------\n        soft_one_hot = F.softmax(logits, dim=2)\n        _, top_idx = torch.topk(soft_one_hot, 1, dim=2)\n        quant_feat = self.quantize.get_codebook_feat(\n            top_idx, shape=[x.shape[0], 16, 16, 256]  # type: ignore\n        )\n        # preserve gradients\n        # quant_feat = lq_feat + (quant_feat - lq_feat).detach()\n\n        if detach_16:\n            quant_feat = quant_feat.detach()  # for training stage III\n        if adain:\n            quant_feat = adaptive_instance_normalization(quant_feat, lq_feat)\n\n        # ################## Generator ####################\n        x = quant_feat\n        fuse_list = [self.fuse_generator_block[f_size] for f_size in self.connect_list]\n\n        for i, block in enumerate(self.generator.blocks):\n            x = block(x)\n            if i in fuse_list:  # fuse after i-th block\n                f_size = str(x.shape[-1])\n                if weight > 0:\n                    x = self.fuse_convs_dict[f_size](\n                        enc_feat_dict[f_size].detach(), x, weight\n                    )\n        out = x\n        # logits doesn't need softmax before cross_entropy loss\n        # return out, logits, lq_feat\n        return out, logits\n", "ldm_patched/pfn/architecture/face/gfpganv1_clean_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .stylegan2_clean_arch import StyleGAN2GeneratorClean\n\n\nclass StyleGAN2GeneratorCSFT(StyleGAN2GeneratorClean):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        num_mlp=8,\n        channel_multiplier=2,\n        narrow=1,\n        sft_half=False,\n    ):\n        super(StyleGAN2GeneratorCSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            narrow=narrow,\n        )\n        self.sft_half = sft_half\n\n    def forward(\n        self,\n        styles,\n        conditions,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2GeneratorCSFT.\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual block with bilinear upsampling/downsampling.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        mode (str): Upsampling/downsampling mode. Options: down | up. Default: down.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, mode=\"down\"):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n        self.skip = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n        if mode == \"down\":\n            self.scale_factor = 0.5\n        elif mode == \"up\":\n            self.scale_factor = 2\n\n    def forward(self, x):\n        out = F.leaky_relu_(self.conv1(x), negative_slope=0.2)\n        # upsample/downsample\n        out = F.interpolate(\n            out, scale_factor=self.scale_factor, mode=\"bilinear\", align_corners=False\n        )\n        out = F.leaky_relu_(self.conv2(out), negative_slope=0.2)\n        # skip\n        x = F.interpolate(\n            x, scale_factor=self.scale_factor, mode=\"bilinear\", align_corners=False\n        )\n        skip = self.skip(x)\n        out = out + skip\n        return out\n\n\nclass GFPGANv1Clean(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dict,\n    ):\n        super(GFPGANv1Clean, self).__init__()\n\n        out_size = 512\n        num_style_feat = 512\n        channel_multiplier = 2\n        decoder_load_path = None\n        fix_decoder = False\n        num_mlp = 8\n        input_is_latent = True\n        different_w = True\n        narrow = 1\n        sft_half = True\n\n        self.model_arch = \"GFPGAN\"\n        self.sub_type = \"Face SR\"\n        self.scale = 8\n        self.in_nc = 3\n        self.out_nc = 3\n        self.state = state_dict\n\n        self.supports_fp16 = False\n        self.supports_bf16 = True\n        self.min_size_restriction = 512\n\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            \"4\": int(512 * unet_narrow),\n            \"8\": int(512 * unet_narrow),\n            \"16\": int(512 * unet_narrow),\n            \"32\": int(512 * unet_narrow),\n            \"64\": int(256 * channel_multiplier * unet_narrow),\n            \"128\": int(128 * channel_multiplier * unet_narrow),\n            \"256\": int(64 * channel_multiplier * unet_narrow),\n            \"512\": int(32 * channel_multiplier * unet_narrow),\n            \"1024\": int(16 * channel_multiplier * unet_narrow),\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2 ** (int(math.log(out_size, 2)))\n\n        self.conv_body_first = nn.Conv2d(3, channels[f\"{first_out_size}\"], 1)\n\n        # downsample\n        in_channels = channels[f\"{first_out_size}\"]\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f\"{2**(i - 1)}\"]\n            self.conv_body_down.append(ResBlock(in_channels, out_channels, mode=\"down\"))\n            in_channels = out_channels\n\n        self.final_conv = nn.Conv2d(in_channels, channels[\"4\"], 3, 1, 1)\n\n        # upsample\n        in_channels = channels[\"4\"]\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.conv_body_up.append(ResBlock(in_channels, out_channels, mode=\"up\"))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(nn.Conv2d(channels[f\"{2**i}\"], 3, 1))\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = nn.Linear(channels[\"4\"] * 4 * 4, linear_out_channel)\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorCSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            narrow=narrow,\n            sft_half=sft_half,\n        )\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(\n                    decoder_load_path, map_location=lambda storage, loc: storage\n                )[\"params_ema\"]\n            )\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n                    nn.LeakyReLU(0.2, True),\n                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1),\n                )\n            )\n            self.condition_shift.append(\n                nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n                    nn.LeakyReLU(0.2, True),\n                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1),\n                )\n            )\n        self.load_state_dict(state_dict)\n\n    def forward(\n        self, x, return_latents=False, return_rgb=True, randomize_noise=True, **kwargs\n    ):\n        \"\"\"Forward function for GFPGANv1Clean.\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = F.leaky_relu_(self.conv_body_first(x), negative_slope=0.2)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n        feat = F.leaky_relu_(self.final_conv(feat), negative_slope=0.2)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder(\n            [style_code],\n            conditions,\n            return_latents=return_latents,\n            input_is_latent=self.input_is_latent,\n            randomize_noise=randomize_noise,\n        )\n\n        return image, out_rgbs\n", "ldm_patched/pfn/architecture/face/gfpganv1_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .fused_act import FusedLeakyReLU\nfrom .stylegan2_arch import (\n    ConvLayer,\n    EqualConv2d,\n    EqualLinear,\n    ResBlock,\n    ScaledLeakyReLU,\n    StyleGAN2Generator,\n)\n\n\nclass StyleGAN2GeneratorSFT(StyleGAN2Generator):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        num_mlp=8,\n        channel_multiplier=2,\n        resample_kernel=(1, 3, 3, 1),\n        lr_mlp=0.01,\n        narrow=1,\n        sft_half=False,\n    ):\n        super(StyleGAN2GeneratorSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            resample_kernel=resample_kernel,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n        )\n        self.sft_half = sft_half\n\n    def forward(\n        self,\n        styles,\n        conditions,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2GeneratorSFT.\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ConvUpLayer(nn.Module):\n    \"\"\"Convolutional upsampling layer. It uses bilinear upsampler + Conv.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input. Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        activate (bool): Whether use activateion. Default: True.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        bias=True,\n        bias_init_val=0,\n        activate=True,\n    ):\n        super(ConvUpLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        # self.scale is used to scale the convolution weights, which is related to the common initializations.\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(\n            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n        )\n\n        if bias and not activate:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        # activation\n        if activate:\n            if bias:\n                self.activation = FusedLeakyReLU(out_channels)\n            else:\n                self.activation = ScaledLeakyReLU(0.2)\n        else:\n            self.activation = None\n\n    def forward(self, x):\n        # bilinear upsample\n        out = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False)\n        # conv\n        out = F.conv2d(\n            out,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        # activation\n        if self.activation is not None:\n            out = self.activation(out)\n        return out\n\n\nclass ResUpBlock(nn.Module):\n    \"\"\"Residual block with upsampling.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super(ResUpBlock, self).__init__()\n\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvUpLayer(\n            in_channels, out_channels, 3, stride=1, padding=1, bias=True, activate=True\n        )\n        self.skip = ConvUpLayer(\n            in_channels, out_channels, 1, bias=False, activate=False\n        )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        skip = self.skip(x)\n        out = (out + skip) / math.sqrt(2)\n        return out\n\n\nclass GFPGANv1(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        channel_multiplier=1,\n        resample_kernel=(1, 3, 3, 1),\n        decoder_load_path=None,\n        fix_decoder=True,\n        # for stylegan decoder\n        num_mlp=8,\n        lr_mlp=0.01,\n        input_is_latent=False,\n        different_w=False,\n        narrow=1,\n        sft_half=False,\n    ):\n        super(GFPGANv1, self).__init__()\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            \"4\": int(512 * unet_narrow),\n            \"8\": int(512 * unet_narrow),\n            \"16\": int(512 * unet_narrow),\n            \"32\": int(512 * unet_narrow),\n            \"64\": int(256 * channel_multiplier * unet_narrow),\n            \"128\": int(128 * channel_multiplier * unet_narrow),\n            \"256\": int(64 * channel_multiplier * unet_narrow),\n            \"512\": int(32 * channel_multiplier * unet_narrow),\n            \"1024\": int(16 * channel_multiplier * unet_narrow),\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2 ** (int(math.log(out_size, 2)))\n\n        self.conv_body_first = ConvLayer(\n            3, channels[f\"{first_out_size}\"], 1, bias=True, activate=True\n        )\n\n        # downsample\n        in_channels = channels[f\"{first_out_size}\"]\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f\"{2**(i - 1)}\"]\n            self.conv_body_down.append(\n                ResBlock(in_channels, out_channels, resample_kernel)\n            )\n            in_channels = out_channels\n\n        self.final_conv = ConvLayer(\n            in_channels, channels[\"4\"], 3, bias=True, activate=True\n        )\n\n        # upsample\n        in_channels = channels[\"4\"]\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.conv_body_up.append(ResUpBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(\n                EqualConv2d(\n                    channels[f\"{2**i}\"],\n                    3,\n                    1,\n                    stride=1,\n                    padding=0,\n                    bias=True,\n                    bias_init_val=0,\n                )\n            )\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = EqualLinear(\n            channels[\"4\"] * 4 * 4,\n            linear_out_channel,\n            bias=True,\n            bias_init_val=0,\n            lr_mul=1,\n            activation=None,\n        )\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            resample_kernel=resample_kernel,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n            sft_half=sft_half,\n        )\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(\n                    decoder_load_path, map_location=lambda storage, loc: storage\n                )[\"params_ema\"]\n            )\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    EqualConv2d(\n                        out_channels,\n                        out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(\n                        out_channels,\n                        sft_out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=1,\n                    ),\n                )\n            )\n            self.condition_shift.append(\n                nn.Sequential(\n                    EqualConv2d(\n                        out_channels,\n                        out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(\n                        out_channels,\n                        sft_out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                )\n            )\n\n    def forward(\n        self, x, return_latents=False, return_rgb=True, randomize_noise=True, **kwargs\n    ):\n        \"\"\"Forward function for GFPGANv1.\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = self.conv_body_first(x)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n\n        feat = self.final_conv(feat)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder(\n            [style_code],\n            conditions,\n            return_latents=return_latents,\n            input_is_latent=self.input_is_latent,\n            randomize_noise=randomize_noise,\n        )\n\n        return image, out_rgbs\n\n\nclass FacialComponentDiscriminator(nn.Module):\n    \"\"\"Facial component (eyes, mouth, noise) discriminator used in GFPGAN.\"\"\"\n\n    def __init__(self):\n        super(FacialComponentDiscriminator, self).__init__()\n        # It now uses a VGG-style architectrue with fixed model size\n        self.conv1 = ConvLayer(\n            3,\n            64,\n            3,\n            downsample=False,\n            resample_kernel=(1, 3, 3, 1),\n            bias=True,\n            activate=True,\n        )\n        self.conv2 = ConvLayer(\n            64,\n            128,\n            3,\n            downsample=True,\n            resample_kernel=(1, 3, 3, 1),\n            bias=True,\n            activate=True,\n        )\n        self.conv3 = ConvLayer(\n            128,\n            128,\n            3,\n            downsample=False,\n            resample_kernel=(1, 3, 3, 1),\n            bias=True,\n            activate=True,\n        )\n        self.conv4 = ConvLayer(\n            128,\n            256,\n            3,\n            downsample=True,\n            resample_kernel=(1, 3, 3, 1),\n            bias=True,\n            activate=True,\n        )\n        self.conv5 = ConvLayer(\n            256,\n            256,\n            3,\n            downsample=False,\n            resample_kernel=(1, 3, 3, 1),\n            bias=True,\n            activate=True,\n        )\n        self.final_conv = ConvLayer(256, 1, 3, bias=True, activate=False)\n\n    def forward(self, x, return_feats=False, **kwargs):\n        \"\"\"Forward function for FacialComponentDiscriminator.\n        Args:\n            x (Tensor): Input images.\n            return_feats (bool): Whether to return intermediate features. Default: False.\n        \"\"\"\n        feat = self.conv1(x)\n        feat = self.conv3(self.conv2(feat))\n        rlt_feats = []\n        if return_feats:\n            rlt_feats.append(feat.clone())\n        feat = self.conv5(self.conv4(feat))\n        if return_feats:\n            rlt_feats.append(feat.clone())\n        out = self.final_conv(feat)\n\n        if return_feats:\n            return out, rlt_feats\n        else:\n            return out, None\n", "ldm_patched/pfn/architecture/face/stylegan2_clean_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\n\n@torch.no_grad()\ndef default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n    \"\"\"Initialize network weights.\n    Args:\n        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n        scale (float): Scale initialized weights, especially for residual\n            blocks. Default: 1.\n        bias_fill (float): The value to fill bias. Default: 0\n        kwargs (dict): Other arguments for initialization function.\n    \"\"\"\n    if not isinstance(module_list, list):\n        module_list = [module_list]\n    for module in module_list:\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, _BatchNorm):\n                init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n\n\nclass NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\n\nclass ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n    There is no bias in ModulatedConv2d.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n        eps (float): A value added to the denominator for numerical stability. Default: 1e-8.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n        eps=1e-8,\n    ):\n        super(ModulatedConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.demodulate = demodulate\n        self.sample_mode = sample_mode\n        self.eps = eps\n\n        # modulation inside each modulated conv\n        self.modulation = nn.Linear(num_style_feat, in_channels, bias=True)\n        # initialization\n        default_init_weights(\n            self.modulation,\n            scale=1,\n            bias_fill=1,\n            a=0,\n            mode=\"fan_in\",\n            nonlinearity=\"linear\",\n        )\n\n        self.weight = nn.Parameter(\n            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n            / math.sqrt(in_channels * kernel_size**2)\n        )\n        self.padding = kernel_size // 2\n\n    def forward(self, x, style):\n        \"\"\"Forward function.\n        Args:\n            x (Tensor): Tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n        Returns:\n            Tensor: Modulated tensor after convolution.\n        \"\"\"\n        b, c, h, w = x.shape  # c = c_in\n        # weight modulation\n        style = self.modulation(style).view(b, 1, c, 1, 1)\n        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n        weight = self.weight * style  # (b, c_out, c_in, k, k)\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n\n        weight = weight.view(\n            b * self.out_channels, c, self.kernel_size, self.kernel_size\n        )\n\n        # upsample or downsample if necessary\n        if self.sample_mode == \"upsample\":\n            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False)\n        elif self.sample_mode == \"downsample\":\n            x = F.interpolate(x, scale_factor=0.5, mode=\"bilinear\", align_corners=False)\n\n        b, c, h, w = x.shape\n        x = x.view(1, b * c, h, w)\n        # weight: (b*c_out, c_in, k, k), groups=b\n        out = F.conv2d(x, weight, padding=self.padding, groups=b)\n        out = out.view(b, self.out_channels, *out.shape[2:4])\n\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, out_channels={self.out_channels}, \"\n            f\"kernel_size={self.kernel_size}, demodulate={self.demodulate}, sample_mode={self.sample_mode})\"\n        )\n\n\nclass StyleConv(nn.Module):\n    \"\"\"Style conv used in StyleGAN2.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n    ):\n        super(StyleConv, self).__init__()\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            num_style_feat,\n            demodulate=demodulate,\n            sample_mode=sample_mode,\n        )\n        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n        self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n\nclass ToRGB(nn.Module):\n    \"\"\"To RGB (image space) from features.\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n\n    def __init__(self, in_channels, num_style_feat, upsample=True):\n        super(ToRGB, self).__init__()\n        self.upsample = upsample\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            3,\n            kernel_size=1,\n            num_style_feat=num_style_feat,\n            demodulate=False,\n            sample_mode=None,\n        )\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, x, style, skip=None):\n        \"\"\"Forward function.\n        Args:\n            x (Tensor): Feature tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n            skip (Tensor): Base/skip tensor. Default: None.\n        Returns:\n            Tensor: RGB images.\n        \"\"\"\n        out = self.modulated_conv(x, style)\n        out = out + self.bias\n        if skip is not None:\n            if self.upsample:\n                skip = F.interpolate(\n                    skip, scale_factor=2, mode=\"bilinear\", align_corners=False\n                )\n            out = out + skip\n        return out\n\n\nclass ConstantInput(nn.Module):\n    \"\"\"Constant input.\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n\n    def forward(self, batch):\n        out = self.weight.repeat(batch, 1, 1, 1)\n        return out\n\n\nclass StyleGAN2GeneratorClean(nn.Module):\n    \"\"\"Clean version of StyleGAN2 Generator.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(\n        self, out_size, num_style_feat=512, num_mlp=8, channel_multiplier=2, narrow=1\n    ):\n        super(StyleGAN2GeneratorClean, self).__init__()\n        # Style MLP layers\n        self.num_style_feat = num_style_feat\n        style_mlp_layers = [NormStyleCode()]\n        for i in range(num_mlp):\n            style_mlp_layers.extend(\n                [\n                    nn.Linear(num_style_feat, num_style_feat, bias=True),\n                    nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                ]\n            )\n        self.style_mlp = nn.Sequential(*style_mlp_layers)\n        # initialization\n        default_init_weights(\n            self.style_mlp,\n            scale=1,\n            bias_fill=0,\n            a=0.2,\n            mode=\"fan_in\",\n            nonlinearity=\"leaky_relu\",\n        )\n\n        # channel list\n        channels = {\n            \"4\": int(512 * narrow),\n            \"8\": int(512 * narrow),\n            \"16\": int(512 * narrow),\n            \"32\": int(512 * narrow),\n            \"64\": int(256 * channel_multiplier * narrow),\n            \"128\": int(128 * channel_multiplier * narrow),\n            \"256\": int(64 * channel_multiplier * narrow),\n            \"512\": int(32 * channel_multiplier * narrow),\n            \"1024\": int(16 * channel_multiplier * narrow),\n        }\n        self.channels = channels\n\n        self.constant_input = ConstantInput(channels[\"4\"], size=4)\n        self.style_conv1 = StyleConv(\n            channels[\"4\"],\n            channels[\"4\"],\n            kernel_size=3,\n            num_style_feat=num_style_feat,\n            demodulate=True,\n            sample_mode=None,\n        )\n        self.to_rgb1 = ToRGB(channels[\"4\"], num_style_feat, upsample=False)\n\n        self.log_size = int(math.log(out_size, 2))\n        self.num_layers = (self.log_size - 2) * 2 + 1\n        self.num_latent = self.log_size * 2 - 2\n\n        self.style_convs = nn.ModuleList()\n        self.to_rgbs = nn.ModuleList()\n        self.noises = nn.Module()\n\n        in_channels = channels[\"4\"]\n        # noise\n        for layer_idx in range(self.num_layers):\n            resolution = 2 ** ((layer_idx + 5) // 2)\n            shape = [1, 1, resolution, resolution]\n            self.noises.register_buffer(f\"noise{layer_idx}\", torch.randn(*shape))\n        # style convs and to_rgbs\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.style_convs.append(\n                StyleConv(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=\"upsample\",\n                )\n            )\n            self.style_convs.append(\n                StyleConv(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=None,\n                )\n            )\n            self.to_rgbs.append(ToRGB(out_channels, num_style_feat, upsample=True))\n            in_channels = out_channels\n\n    def make_noise(self):\n        \"\"\"Make noise for noise injection.\"\"\"\n        device = self.constant_input.weight.device\n        noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n        for i in range(3, self.log_size + 1):\n            for _ in range(2):\n                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n        return noises\n\n    def get_latent(self, x):\n        return self.style_mlp(x)\n\n    def mean_latent(self, num_latent):\n        latent_in = torch.randn(\n            num_latent, self.num_style_feat, device=self.constant_input.weight.device\n        )\n        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n        return latent\n\n    def forward(\n        self,\n        styles,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2GeneratorClean.\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n", "ldm_patched/pfn/architecture/face/upfirdn2d.py": "# pylint: skip-file\n# type: ignore\n# modify from https://github.com/rosinality/stylegan2-pytorch/blob/master/op/upfirdn2d.py  # noqa:E501\n\nimport os\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.nn import functional as F\n\nupfirdn2d_ext = None\n\n\nclass UpFirDn2dBackward(Function):\n    @staticmethod\n    def forward(\n        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n    ):\n        up_x, up_y = up\n        down_x, down_y = down\n        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n\n        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n\n        grad_input = upfirdn2d_ext.upfirdn2d(\n            grad_output,\n            grad_kernel,\n            down_x,\n            down_y,\n            up_x,\n            up_y,\n            g_pad_x0,\n            g_pad_x1,\n            g_pad_y0,\n            g_pad_y1,\n        )\n        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])\n\n        ctx.save_for_backward(kernel)\n\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        ctx.up_x = up_x\n        ctx.up_y = up_y\n        ctx.down_x = down_x\n        ctx.down_y = down_y\n        ctx.pad_x0 = pad_x0\n        ctx.pad_x1 = pad_x1\n        ctx.pad_y0 = pad_y0\n        ctx.pad_y1 = pad_y1\n        ctx.in_size = in_size\n        ctx.out_size = out_size\n\n        return grad_input\n\n    @staticmethod\n    def backward(ctx, gradgrad_input):\n        (kernel,) = ctx.saved_tensors\n\n        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)\n\n        gradgrad_out = upfirdn2d_ext.upfirdn2d(\n            gradgrad_input,\n            kernel,\n            ctx.up_x,\n            ctx.up_y,\n            ctx.down_x,\n            ctx.down_y,\n            ctx.pad_x0,\n            ctx.pad_x1,\n            ctx.pad_y0,\n            ctx.pad_y1,\n        )\n        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0],\n        #                                  ctx.out_size[1], ctx.in_size[3])\n        gradgrad_out = gradgrad_out.view(\n            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]\n        )\n\n        return gradgrad_out, None, None, None, None, None, None, None, None\n\n\nclass UpFirDn2d(Function):\n    @staticmethod\n    def forward(ctx, input, kernel, up, down, pad):\n        up_x, up_y = up\n        down_x, down_y = down\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        kernel_h, kernel_w = kernel.shape\n        _, channel, in_h, in_w = input.shape\n        ctx.in_size = input.shape\n\n        input = input.reshape(-1, in_h, in_w, 1)\n\n        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))\n\n        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n        ctx.out_size = (out_h, out_w)\n\n        ctx.up = (up_x, up_y)\n        ctx.down = (down_x, down_y)\n        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)\n\n        g_pad_x0 = kernel_w - pad_x0 - 1\n        g_pad_y0 = kernel_h - pad_y0 - 1\n        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1\n        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1\n\n        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)\n\n        out = upfirdn2d_ext.upfirdn2d(\n            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n        )\n        # out = out.view(major, out_h, out_w, minor)\n        out = out.view(-1, channel, out_h, out_w)\n\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        kernel, grad_kernel = ctx.saved_tensors\n\n        grad_input = UpFirDn2dBackward.apply(\n            grad_output,\n            kernel,\n            grad_kernel,\n            ctx.up,\n            ctx.down,\n            ctx.pad,\n            ctx.g_pad,\n            ctx.in_size,\n            ctx.out_size,\n        )\n\n        return grad_input, None, None, None, None\n\n\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    if input.device.type == \"cpu\":\n        out = upfirdn2d_native(\n            input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n        )\n    else:\n        out = UpFirDn2d.apply(\n            input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])\n        )\n\n    return out\n\n\ndef upfirdn2d_native(\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n):\n    _, channel, in_h, in_w = input.shape\n    input = input.reshape(-1, in_h, in_w, 1)\n\n    _, in_h, in_w, minor = input.shape\n    kernel_h, kernel_w = kernel.shape\n\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n    out = F.pad(\n        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n    )\n    out = out[\n        :,\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n        :,\n    ]\n\n    out = out.permute(0, 3, 1, 2)\n    out = out.reshape(\n        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n    )\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n    out = F.conv2d(out, w)\n    out = out.reshape(\n        -1,\n        minor,\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n    )\n    out = out.permute(0, 2, 3, 1)\n    out = out[:, ::down_y, ::down_x, :]\n\n    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\n    return out.view(-1, channel, out_h, out_w)\n", "ldm_patched/pfn/architecture/face/gfpgan_bilinear_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\nimport random\n\nimport torch\nfrom torch import nn\n\nfrom .gfpganv1_arch import ResUpBlock\nfrom .stylegan2_bilinear_arch import (\n    ConvLayer,\n    EqualConv2d,\n    EqualLinear,\n    ResBlock,\n    ScaledLeakyReLU,\n    StyleGAN2GeneratorBilinear,\n)\n\n\nclass StyleGAN2GeneratorBilinearSFT(StyleGAN2GeneratorBilinear):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    It is the bilinear version. It does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: StyleGAN2GeneratorCSFT.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        num_mlp=8,\n        channel_multiplier=2,\n        lr_mlp=0.01,\n        narrow=1,\n        sft_half=False,\n    ):\n        super(StyleGAN2GeneratorBilinearSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n        )\n        self.sft_half = sft_half\n\n    def forward(\n        self,\n        styles,\n        conditions,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2GeneratorBilinearSFT.\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass GFPGANBilinear(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    It is the bilinear version and it does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: GFPGANv1Clean.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        channel_multiplier=1,\n        decoder_load_path=None,\n        fix_decoder=True,\n        # for stylegan decoder\n        num_mlp=8,\n        lr_mlp=0.01,\n        input_is_latent=False,\n        different_w=False,\n        narrow=1,\n        sft_half=False,\n    ):\n        super(GFPGANBilinear, self).__init__()\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n        self.min_size_restriction = 512\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            \"4\": int(512 * unet_narrow),\n            \"8\": int(512 * unet_narrow),\n            \"16\": int(512 * unet_narrow),\n            \"32\": int(512 * unet_narrow),\n            \"64\": int(256 * channel_multiplier * unet_narrow),\n            \"128\": int(128 * channel_multiplier * unet_narrow),\n            \"256\": int(64 * channel_multiplier * unet_narrow),\n            \"512\": int(32 * channel_multiplier * unet_narrow),\n            \"1024\": int(16 * channel_multiplier * unet_narrow),\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2 ** (int(math.log(out_size, 2)))\n\n        self.conv_body_first = ConvLayer(\n            3, channels[f\"{first_out_size}\"], 1, bias=True, activate=True\n        )\n\n        # downsample\n        in_channels = channels[f\"{first_out_size}\"]\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f\"{2**(i - 1)}\"]\n            self.conv_body_down.append(ResBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        self.final_conv = ConvLayer(\n            in_channels, channels[\"4\"], 3, bias=True, activate=True\n        )\n\n        # upsample\n        in_channels = channels[\"4\"]\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.conv_body_up.append(ResUpBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(\n                EqualConv2d(\n                    channels[f\"{2**i}\"],\n                    3,\n                    1,\n                    stride=1,\n                    padding=0,\n                    bias=True,\n                    bias_init_val=0,\n                )\n            )\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = EqualLinear(\n            channels[\"4\"] * 4 * 4,\n            linear_out_channel,\n            bias=True,\n            bias_init_val=0,\n            lr_mul=1,\n            activation=None,\n        )\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorBilinearSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n            sft_half=sft_half,\n        )\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(\n                    decoder_load_path, map_location=lambda storage, loc: storage\n                )[\"params_ema\"]\n            )\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    EqualConv2d(\n                        out_channels,\n                        out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(\n                        out_channels,\n                        sft_out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=1,\n                    ),\n                )\n            )\n            self.condition_shift.append(\n                nn.Sequential(\n                    EqualConv2d(\n                        out_channels,\n                        out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(\n                        out_channels,\n                        sft_out_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        bias_init_val=0,\n                    ),\n                )\n            )\n\n    def forward(self, x, return_latents=False, return_rgb=True, randomize_noise=True):\n        \"\"\"Forward function for GFPGANBilinear.\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = self.conv_body_first(x)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n\n        feat = self.final_conv(feat)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder(\n            [style_code],\n            conditions,\n            return_latents=return_latents,\n            input_is_latent=self.input_is_latent,\n            randomize_noise=randomize_noise,\n        )\n\n        return image, out_rgbs\n", "ldm_patched/pfn/architecture/face/stylegan2_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .fused_act import FusedLeakyReLU, fused_leaky_relu\nfrom .upfirdn2d import upfirdn2d\n\n\nclass NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\n\ndef make_resample_kernel(k):\n    \"\"\"Make resampling kernel for UpFirDn.\n\n    Args:\n        k (list[int]): A list indicating the 1D resample kernel magnitude.\n\n    Returns:\n        Tensor: 2D resampled kernel.\n    \"\"\"\n    k = torch.tensor(k, dtype=torch.float32)\n    if k.ndim == 1:\n        k = k[None, :] * k[:, None]  # to 2D kernel, outer product\n    # normalize\n    k /= k.sum()\n    return k\n\n\nclass UpFirDnUpsample(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (upsampole version).\n\n    References:\n    1. https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.upfirdn.html  # noqa: E501\n    2. http://www.ece.northwestern.edu/local-apps/matlabhelp/toolbox/signal/upfirdn.html  # noqa: E501\n\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        factor (int): Upsampling scale factor. Default: 2.\n    \"\"\"\n\n    def __init__(self, resample_kernel, factor=2):\n        super(UpFirDnUpsample, self).__init__()\n        self.kernel = make_resample_kernel(resample_kernel) * (factor**2)\n        self.factor = factor\n\n        pad = self.kernel.shape[0] - factor\n        self.pad = ((pad + 1) // 2 + factor - 1, pad // 2)\n\n    def forward(self, x):\n        out = upfirdn2d(x, self.kernel.type_as(x), up=self.factor, down=1, pad=self.pad)\n        return out\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(factor={self.factor})\"\n\n\nclass UpFirDnDownsample(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (downsampole version).\n\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        factor (int): Downsampling scale factor. Default: 2.\n    \"\"\"\n\n    def __init__(self, resample_kernel, factor=2):\n        super(UpFirDnDownsample, self).__init__()\n        self.kernel = make_resample_kernel(resample_kernel)\n        self.factor = factor\n\n        pad = self.kernel.shape[0] - factor\n        self.pad = ((pad + 1) // 2, pad // 2)\n\n    def forward(self, x):\n        out = upfirdn2d(x, self.kernel.type_as(x), up=1, down=self.factor, pad=self.pad)\n        return out\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(factor={self.factor})\"\n\n\nclass UpFirDnSmooth(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (smooth version).\n\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        upsample_factor (int): Upsampling scale factor. Default: 1.\n        downsample_factor (int): Downsampling scale factor. Default: 1.\n        kernel_size (int): Kernel size: Default: 1.\n    \"\"\"\n\n    def __init__(\n        self, resample_kernel, upsample_factor=1, downsample_factor=1, kernel_size=1\n    ):\n        super(UpFirDnSmooth, self).__init__()\n        self.upsample_factor = upsample_factor\n        self.downsample_factor = downsample_factor\n        self.kernel = make_resample_kernel(resample_kernel)\n        if upsample_factor > 1:\n            self.kernel = self.kernel * (upsample_factor**2)\n\n        if upsample_factor > 1:\n            pad = (self.kernel.shape[0] - upsample_factor) - (kernel_size - 1)\n            self.pad = ((pad + 1) // 2 + upsample_factor - 1, pad // 2 + 1)\n        elif downsample_factor > 1:\n            pad = (self.kernel.shape[0] - downsample_factor) + (kernel_size - 1)\n            self.pad = ((pad + 1) // 2, pad // 2)\n        else:\n            raise NotImplementedError\n\n    def forward(self, x):\n        out = upfirdn2d(x, self.kernel.type_as(x), up=1, down=1, pad=self.pad)\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(upsample_factor={self.upsample_factor}\"\n            f\", downsample_factor={self.downsample_factor})\"\n        )\n\n\nclass EqualLinear(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n\n    Args:\n        in_channels (int): Size of each sample.\n        out_channels (int): Size of each output sample.\n        bias (bool): If set to ``False``, the layer will not learn an additive\n            bias. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        lr_mul (float): Learning rate multiplier. Default: 1.\n        activation (None | str): The activation after ``linear`` operation.\n            Supported: 'fused_lrelu', None. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        bias=True,\n        bias_init_val=0,\n        lr_mul=1,\n        activation=None,\n    ):\n        super(EqualLinear, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.lr_mul = lr_mul\n        self.activation = activation\n        if self.activation not in [\"fused_lrelu\", None]:\n            raise ValueError(\n                f\"Wrong activation value in EqualLinear: {activation}\"\n                \"Supported ones are: ['fused_lrelu', None].\"\n            )\n        self.scale = (1 / math.sqrt(in_channels)) * lr_mul\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels).div_(lr_mul))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x):\n        if self.bias is None:\n            bias = None\n        else:\n            bias = self.bias * self.lr_mul\n        if self.activation == \"fused_lrelu\":\n            out = F.linear(x, self.weight * self.scale)\n            out = fused_leaky_relu(out, bias)\n        else:\n            out = F.linear(x, self.weight * self.scale, bias=bias)\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, bias={self.bias is not None})\"\n        )\n\n\nclass ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n\n    There is no bias in ModulatedConv2d.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer.\n            Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. Default: (1, 3, 3, 1).\n        eps (float): A value added to the denominator for numerical stability.\n            Default: 1e-8.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n        resample_kernel=(1, 3, 3, 1),\n        eps=1e-8,\n    ):\n        super(ModulatedConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.demodulate = demodulate\n        self.sample_mode = sample_mode\n        self.eps = eps\n\n        if self.sample_mode == \"upsample\":\n            self.smooth = UpFirDnSmooth(\n                resample_kernel,\n                upsample_factor=2,\n                downsample_factor=1,\n                kernel_size=kernel_size,\n            )\n        elif self.sample_mode == \"downsample\":\n            self.smooth = UpFirDnSmooth(\n                resample_kernel,\n                upsample_factor=1,\n                downsample_factor=2,\n                kernel_size=kernel_size,\n            )\n        elif self.sample_mode is None:\n            pass\n        else:\n            raise ValueError(\n                f\"Wrong sample mode {self.sample_mode}, \"\n                \"supported ones are ['upsample', 'downsample', None].\"\n            )\n\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n        # modulation inside each modulated conv\n        self.modulation = EqualLinear(\n            num_style_feat,\n            in_channels,\n            bias=True,\n            bias_init_val=1,\n            lr_mul=1,\n            activation=None,\n        )\n\n        self.weight = nn.Parameter(\n            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n        )\n        self.padding = kernel_size // 2\n\n    def forward(self, x, style):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n\n        Returns:\n            Tensor: Modulated tensor after convolution.\n        \"\"\"\n        b, c, h, w = x.shape  # c = c_in\n        # weight modulation\n        style = self.modulation(style).view(b, 1, c, 1, 1)\n        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n        weight = self.scale * self.weight * style  # (b, c_out, c_in, k, k)\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n\n        weight = weight.view(\n            b * self.out_channels, c, self.kernel_size, self.kernel_size\n        )\n\n        if self.sample_mode == \"upsample\":\n            x = x.view(1, b * c, h, w)\n            weight = weight.view(\n                b, self.out_channels, c, self.kernel_size, self.kernel_size\n            )\n            weight = weight.transpose(1, 2).reshape(\n                b * c, self.out_channels, self.kernel_size, self.kernel_size\n            )\n            out = F.conv_transpose2d(x, weight, padding=0, stride=2, groups=b)\n            out = out.view(b, self.out_channels, *out.shape[2:4])\n            out = self.smooth(out)\n        elif self.sample_mode == \"downsample\":\n            x = self.smooth(x)\n            x = x.view(1, b * c, *x.shape[2:4])\n            out = F.conv2d(x, weight, padding=0, stride=2, groups=b)\n            out = out.view(b, self.out_channels, *out.shape[2:4])\n        else:\n            x = x.view(1, b * c, h, w)\n            # weight: (b*c_out, c_in, k, k), groups=b\n            out = F.conv2d(x, weight, padding=self.padding, groups=b)\n            out = out.view(b, self.out_channels, *out.shape[2:4])\n\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, \"\n            f\"kernel_size={self.kernel_size}, \"\n            f\"demodulate={self.demodulate}, sample_mode={self.sample_mode})\"\n        )\n\n\nclass StyleConv(nn.Module):\n    \"\"\"Style conv.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. Default: (1, 3, 3, 1).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n        resample_kernel=(1, 3, 3, 1),\n    ):\n        super(StyleConv, self).__init__()\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            num_style_feat,\n            demodulate=demodulate,\n            sample_mode=sample_mode,\n            resample_kernel=resample_kernel,\n        )\n        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n        self.activate = FusedLeakyReLU(out_channels)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style)\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # activation (with bias)\n        out = self.activate(out)\n        return out\n\n\nclass ToRGB(nn.Module):\n    \"\"\"To RGB from features.\n\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. Default: (1, 3, 3, 1).\n    \"\"\"\n\n    def __init__(\n        self, in_channels, num_style_feat, upsample=True, resample_kernel=(1, 3, 3, 1)\n    ):\n        super(ToRGB, self).__init__()\n        if upsample:\n            self.upsample = UpFirDnUpsample(resample_kernel, factor=2)\n        else:\n            self.upsample = None\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            3,\n            kernel_size=1,\n            num_style_feat=num_style_feat,\n            demodulate=False,\n            sample_mode=None,\n        )\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, x, style, skip=None):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Feature tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n            skip (Tensor): Base/skip tensor. Default: None.\n\n        Returns:\n            Tensor: RGB images.\n        \"\"\"\n        out = self.modulated_conv(x, style)\n        out = out + self.bias\n        if skip is not None:\n            if self.upsample:\n                skip = self.upsample(skip)\n            out = out + skip\n        return out\n\n\nclass ConstantInput(nn.Module):\n    \"\"\"Constant input.\n\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n\n    def forward(self, batch):\n        out = self.weight.repeat(batch, 1, 1, 1)\n        return out\n\n\nclass StyleGAN2Generator(nn.Module):\n    \"\"\"StyleGAN2 Generator.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of\n            StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. A cross production will be applied to extent 1D resample\n            kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        num_mlp=8,\n        channel_multiplier=2,\n        resample_kernel=(1, 3, 3, 1),\n        lr_mlp=0.01,\n        narrow=1,\n    ):\n        super(StyleGAN2Generator, self).__init__()\n        # Style MLP layers\n        self.num_style_feat = num_style_feat\n        style_mlp_layers = [NormStyleCode()]\n        for i in range(num_mlp):\n            style_mlp_layers.append(\n                EqualLinear(\n                    num_style_feat,\n                    num_style_feat,\n                    bias=True,\n                    bias_init_val=0,\n                    lr_mul=lr_mlp,\n                    activation=\"fused_lrelu\",\n                )\n            )\n        self.style_mlp = nn.Sequential(*style_mlp_layers)\n\n        channels = {\n            \"4\": int(512 * narrow),\n            \"8\": int(512 * narrow),\n            \"16\": int(512 * narrow),\n            \"32\": int(512 * narrow),\n            \"64\": int(256 * channel_multiplier * narrow),\n            \"128\": int(128 * channel_multiplier * narrow),\n            \"256\": int(64 * channel_multiplier * narrow),\n            \"512\": int(32 * channel_multiplier * narrow),\n            \"1024\": int(16 * channel_multiplier * narrow),\n        }\n        self.channels = channels\n\n        self.constant_input = ConstantInput(channels[\"4\"], size=4)\n        self.style_conv1 = StyleConv(\n            channels[\"4\"],\n            channels[\"4\"],\n            kernel_size=3,\n            num_style_feat=num_style_feat,\n            demodulate=True,\n            sample_mode=None,\n            resample_kernel=resample_kernel,\n        )\n        self.to_rgb1 = ToRGB(\n            channels[\"4\"],\n            num_style_feat,\n            upsample=False,\n            resample_kernel=resample_kernel,\n        )\n\n        self.log_size = int(math.log(out_size, 2))\n        self.num_layers = (self.log_size - 2) * 2 + 1\n        self.num_latent = self.log_size * 2 - 2\n\n        self.style_convs = nn.ModuleList()\n        self.to_rgbs = nn.ModuleList()\n        self.noises = nn.Module()\n\n        in_channels = channels[\"4\"]\n        # noise\n        for layer_idx in range(self.num_layers):\n            resolution = 2 ** ((layer_idx + 5) // 2)\n            shape = [1, 1, resolution, resolution]\n            self.noises.register_buffer(f\"noise{layer_idx}\", torch.randn(*shape))\n        # style convs and to_rgbs\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.style_convs.append(\n                StyleConv(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=\"upsample\",\n                    resample_kernel=resample_kernel,\n                )\n            )\n            self.style_convs.append(\n                StyleConv(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=None,\n                    resample_kernel=resample_kernel,\n                )\n            )\n            self.to_rgbs.append(\n                ToRGB(\n                    out_channels,\n                    num_style_feat,\n                    upsample=True,\n                    resample_kernel=resample_kernel,\n                )\n            )\n            in_channels = out_channels\n\n    def make_noise(self):\n        \"\"\"Make noise for noise injection.\"\"\"\n        device = self.constant_input.weight.device\n        noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n        for i in range(3, self.log_size + 1):\n            for _ in range(2):\n                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n        return noises\n\n    def get_latent(self, x):\n        return self.style_mlp(x)\n\n    def mean_latent(self, num_latent):\n        latent_in = torch.randn(\n            num_latent, self.num_style_feat, device=self.constant_input.weight.device\n        )\n        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n        return latent\n\n    def forward(\n        self,\n        styles,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2Generator.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style.\n                Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is\n                False. Default: True.\n            truncation (float): TODO. Default: 1.\n            truncation_latent (Tensor | None): TODO. Default: None.\n            inject_index (int | None): The injection index for mixing noise.\n                Default: None.\n            return_latents (bool): Whether to return style latents.\n                Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latent with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ScaledLeakyReLU(nn.Module):\n    \"\"\"Scaled LeakyReLU.\n\n    Args:\n        negative_slope (float): Negative slope. Default: 0.2.\n    \"\"\"\n\n    def __init__(self, negative_slope=0.2):\n        super(ScaledLeakyReLU, self).__init__()\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        out = F.leaky_relu(x, negative_slope=self.negative_slope)\n        return out * math.sqrt(2)\n\n\nclass EqualConv2d(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input.\n            Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output.\n            Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        bias=True,\n        bias_init_val=0,\n    ):\n        super(EqualConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(\n            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n        )\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x):\n        out = F.conv2d(\n            x,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, \"\n            f\"kernel_size={self.kernel_size},\"\n            f\" stride={self.stride}, padding={self.padding}, \"\n            f\"bias={self.bias is not None})\"\n        )\n\n\nclass ConvLayer(nn.Sequential):\n    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Kernel size.\n        downsample (bool): Whether downsample by a factor of 2.\n            Default: False.\n        resample_kernel (list[int]): A list indicating the 1D resample\n            kernel magnitude. A cross production will be applied to\n            extent 1D resample kernel to 2D resample kernel.\n            Default: (1, 3, 3, 1).\n        bias (bool): Whether with bias. Default: True.\n        activate (bool): Whether use activateion. Default: True.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        downsample=False,\n        resample_kernel=(1, 3, 3, 1),\n        bias=True,\n        activate=True,\n    ):\n        layers = []\n        # downsample\n        if downsample:\n            layers.append(\n                UpFirDnSmooth(\n                    resample_kernel,\n                    upsample_factor=1,\n                    downsample_factor=2,\n                    kernel_size=kernel_size,\n                )\n            )\n            stride = 2\n            self.padding = 0\n        else:\n            stride = 1\n            self.padding = kernel_size // 2\n        # conv\n        layers.append(\n            EqualConv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=self.padding,\n                bias=bias and not activate,\n            )\n        )\n        # activation\n        if activate:\n            if bias:\n                layers.append(FusedLeakyReLU(out_channels))\n            else:\n                layers.append(ScaledLeakyReLU(0.2))\n\n        super(ConvLayer, self).__init__(*layers)\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual block used in StyleGAN2 Discriminator.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        resample_kernel (list[int]): A list indicating the 1D resample\n            kernel magnitude. A cross production will be applied to\n            extent 1D resample kernel to 2D resample kernel.\n            Default: (1, 3, 3, 1).\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, resample_kernel=(1, 3, 3, 1)):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvLayer(\n            in_channels,\n            out_channels,\n            3,\n            downsample=True,\n            resample_kernel=resample_kernel,\n            bias=True,\n            activate=True,\n        )\n        self.skip = ConvLayer(\n            in_channels,\n            out_channels,\n            1,\n            downsample=True,\n            resample_kernel=resample_kernel,\n            bias=False,\n            activate=False,\n        )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        skip = self.skip(x)\n        out = (out + skip) / math.sqrt(2)\n        return out\n", "ldm_patched/pfn/architecture/face/fused_act.py": "# pylint: skip-file\n# type: ignore\n# modify from https://github.com/rosinality/stylegan2-pytorch/blob/master/op/fused_act.py # noqa:E501\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\n\nfused_act_ext = None\n\n\nclass FusedLeakyReLUFunctionBackward(Function):\n    @staticmethod\n    def forward(ctx, grad_output, out, negative_slope, scale):\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        empty = grad_output.new_empty(0)\n\n        grad_input = fused_act_ext.fused_bias_act(\n            grad_output, empty, out, 3, 1, negative_slope, scale\n        )\n\n        dim = [0]\n\n        if grad_input.ndim > 2:\n            dim += list(range(2, grad_input.ndim))\n\n        grad_bias = grad_input.sum(dim).detach()\n\n        return grad_input, grad_bias\n\n    @staticmethod\n    def backward(ctx, gradgrad_input, gradgrad_bias):\n        (out,) = ctx.saved_tensors\n        gradgrad_out = fused_act_ext.fused_bias_act(\n            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n        )\n\n        return gradgrad_out, None, None, None\n\n\nclass FusedLeakyReLUFunction(Function):\n    @staticmethod\n    def forward(ctx, input, bias, negative_slope, scale):\n        empty = input.new_empty(0)\n        out = fused_act_ext.fused_bias_act(\n            input, bias, empty, 3, 0, negative_slope, scale\n        )\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        (out,) = ctx.saved_tensors\n\n        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n            grad_output, out, ctx.negative_slope, ctx.scale\n        )\n\n        return grad_input, grad_bias, None, None\n\n\nclass FusedLeakyReLU(nn.Module):\n    def __init__(self, channel, negative_slope=0.2, scale=2**0.5):\n        super().__init__()\n\n        self.bias = nn.Parameter(torch.zeros(channel))\n        self.negative_slope = negative_slope\n        self.scale = scale\n\n    def forward(self, input):\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n\n\ndef fused_leaky_relu(input, bias, negative_slope=0.2, scale=2**0.5):\n    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)\n", "ldm_patched/pfn/architecture/face/stylegan2_bilinear_arch.py": "# pylint: skip-file\n# type: ignore\nimport math\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .fused_act import FusedLeakyReLU, fused_leaky_relu\n\n\nclass NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\n\nclass EqualLinear(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Size of each sample.\n        out_channels (int): Size of each output sample.\n        bias (bool): If set to ``False``, the layer will not learn an additive\n            bias. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        lr_mul (float): Learning rate multiplier. Default: 1.\n        activation (None | str): The activation after ``linear`` operation.\n            Supported: 'fused_lrelu', None. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        bias=True,\n        bias_init_val=0,\n        lr_mul=1,\n        activation=None,\n    ):\n        super(EqualLinear, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.lr_mul = lr_mul\n        self.activation = activation\n        if self.activation not in [\"fused_lrelu\", None]:\n            raise ValueError(\n                f\"Wrong activation value in EqualLinear: {activation}\"\n                \"Supported ones are: ['fused_lrelu', None].\"\n            )\n        self.scale = (1 / math.sqrt(in_channels)) * lr_mul\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels).div_(lr_mul))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x):\n        if self.bias is None:\n            bias = None\n        else:\n            bias = self.bias * self.lr_mul\n        if self.activation == \"fused_lrelu\":\n            out = F.linear(x, self.weight * self.scale)\n            out = fused_leaky_relu(out, bias)\n        else:\n            out = F.linear(x, self.weight * self.scale, bias=bias)\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, bias={self.bias is not None})\"\n        )\n\n\nclass ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n    There is no bias in ModulatedConv2d.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer.\n            Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n        eps (float): A value added to the denominator for numerical stability.\n            Default: 1e-8.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n        eps=1e-8,\n        interpolation_mode=\"bilinear\",\n    ):\n        super(ModulatedConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.demodulate = demodulate\n        self.sample_mode = sample_mode\n        self.eps = eps\n        self.interpolation_mode = interpolation_mode\n        if self.interpolation_mode == \"nearest\":\n            self.align_corners = None\n        else:\n            self.align_corners = False\n\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n        # modulation inside each modulated conv\n        self.modulation = EqualLinear(\n            num_style_feat,\n            in_channels,\n            bias=True,\n            bias_init_val=1,\n            lr_mul=1,\n            activation=None,\n        )\n\n        self.weight = nn.Parameter(\n            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n        )\n        self.padding = kernel_size // 2\n\n    def forward(self, x, style):\n        \"\"\"Forward function.\n        Args:\n            x (Tensor): Tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n        Returns:\n            Tensor: Modulated tensor after convolution.\n        \"\"\"\n        b, c, h, w = x.shape  # c = c_in\n        # weight modulation\n        style = self.modulation(style).view(b, 1, c, 1, 1)\n        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n        weight = self.scale * self.weight * style  # (b, c_out, c_in, k, k)\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n\n        weight = weight.view(\n            b * self.out_channels, c, self.kernel_size, self.kernel_size\n        )\n\n        if self.sample_mode == \"upsample\":\n            x = F.interpolate(\n                x,\n                scale_factor=2,\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        elif self.sample_mode == \"downsample\":\n            x = F.interpolate(\n                x,\n                scale_factor=0.5,\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n\n        b, c, h, w = x.shape\n        x = x.view(1, b * c, h, w)\n        # weight: (b*c_out, c_in, k, k), groups=b\n        out = F.conv2d(x, weight, padding=self.padding, groups=b)\n        out = out.view(b, self.out_channels, *out.shape[2:4])\n\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, \"\n            f\"kernel_size={self.kernel_size}, \"\n            f\"demodulate={self.demodulate}, sample_mode={self.sample_mode})\"\n        )\n\n\nclass StyleConv(nn.Module):\n    \"\"\"Style conv.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        num_style_feat,\n        demodulate=True,\n        sample_mode=None,\n        interpolation_mode=\"bilinear\",\n    ):\n        super(StyleConv, self).__init__()\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            num_style_feat,\n            demodulate=demodulate,\n            sample_mode=sample_mode,\n            interpolation_mode=interpolation_mode,\n        )\n        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n        self.activate = FusedLeakyReLU(out_channels)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style)\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # activation (with bias)\n        out = self.activate(out)\n        return out\n\n\nclass ToRGB(nn.Module):\n    \"\"\"To RGB from features.\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n\n    def __init__(\n        self, in_channels, num_style_feat, upsample=True, interpolation_mode=\"bilinear\"\n    ):\n        super(ToRGB, self).__init__()\n        self.upsample = upsample\n        self.interpolation_mode = interpolation_mode\n        if self.interpolation_mode == \"nearest\":\n            self.align_corners = None\n        else:\n            self.align_corners = False\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            3,\n            kernel_size=1,\n            num_style_feat=num_style_feat,\n            demodulate=False,\n            sample_mode=None,\n            interpolation_mode=interpolation_mode,\n        )\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, x, style, skip=None):\n        \"\"\"Forward function.\n        Args:\n            x (Tensor): Feature tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n            skip (Tensor): Base/skip tensor. Default: None.\n        Returns:\n            Tensor: RGB images.\n        \"\"\"\n        out = self.modulated_conv(x, style)\n        out = out + self.bias\n        if skip is not None:\n            if self.upsample:\n                skip = F.interpolate(\n                    skip,\n                    scale_factor=2,\n                    mode=self.interpolation_mode,\n                    align_corners=self.align_corners,\n                )\n            out = out + skip\n        return out\n\n\nclass ConstantInput(nn.Module):\n    \"\"\"Constant input.\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n\n    def forward(self, batch):\n        out = self.weight.repeat(batch, 1, 1, 1)\n        return out\n\n\nclass StyleGAN2GeneratorBilinear(nn.Module):\n    \"\"\"StyleGAN2 Generator.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of\n            StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_size,\n        num_style_feat=512,\n        num_mlp=8,\n        channel_multiplier=2,\n        lr_mlp=0.01,\n        narrow=1,\n        interpolation_mode=\"bilinear\",\n    ):\n        super(StyleGAN2GeneratorBilinear, self).__init__()\n        # Style MLP layers\n        self.num_style_feat = num_style_feat\n        style_mlp_layers = [NormStyleCode()]\n        for i in range(num_mlp):\n            style_mlp_layers.append(\n                EqualLinear(\n                    num_style_feat,\n                    num_style_feat,\n                    bias=True,\n                    bias_init_val=0,\n                    lr_mul=lr_mlp,\n                    activation=\"fused_lrelu\",\n                )\n            )\n        self.style_mlp = nn.Sequential(*style_mlp_layers)\n\n        channels = {\n            \"4\": int(512 * narrow),\n            \"8\": int(512 * narrow),\n            \"16\": int(512 * narrow),\n            \"32\": int(512 * narrow),\n            \"64\": int(256 * channel_multiplier * narrow),\n            \"128\": int(128 * channel_multiplier * narrow),\n            \"256\": int(64 * channel_multiplier * narrow),\n            \"512\": int(32 * channel_multiplier * narrow),\n            \"1024\": int(16 * channel_multiplier * narrow),\n        }\n        self.channels = channels\n\n        self.constant_input = ConstantInput(channels[\"4\"], size=4)\n        self.style_conv1 = StyleConv(\n            channels[\"4\"],\n            channels[\"4\"],\n            kernel_size=3,\n            num_style_feat=num_style_feat,\n            demodulate=True,\n            sample_mode=None,\n            interpolation_mode=interpolation_mode,\n        )\n        self.to_rgb1 = ToRGB(\n            channels[\"4\"],\n            num_style_feat,\n            upsample=False,\n            interpolation_mode=interpolation_mode,\n        )\n\n        self.log_size = int(math.log(out_size, 2))\n        self.num_layers = (self.log_size - 2) * 2 + 1\n        self.num_latent = self.log_size * 2 - 2\n\n        self.style_convs = nn.ModuleList()\n        self.to_rgbs = nn.ModuleList()\n        self.noises = nn.Module()\n\n        in_channels = channels[\"4\"]\n        # noise\n        for layer_idx in range(self.num_layers):\n            resolution = 2 ** ((layer_idx + 5) // 2)\n            shape = [1, 1, resolution, resolution]\n            self.noises.register_buffer(f\"noise{layer_idx}\", torch.randn(*shape))\n        # style convs and to_rgbs\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f\"{2**i}\"]\n            self.style_convs.append(\n                StyleConv(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=\"upsample\",\n                    interpolation_mode=interpolation_mode,\n                )\n            )\n            self.style_convs.append(\n                StyleConv(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=None,\n                    interpolation_mode=interpolation_mode,\n                )\n            )\n            self.to_rgbs.append(\n                ToRGB(\n                    out_channels,\n                    num_style_feat,\n                    upsample=True,\n                    interpolation_mode=interpolation_mode,\n                )\n            )\n            in_channels = out_channels\n\n    def make_noise(self):\n        \"\"\"Make noise for noise injection.\"\"\"\n        device = self.constant_input.weight.device\n        noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n        for i in range(3, self.log_size + 1):\n            for _ in range(2):\n                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n        return noises\n\n    def get_latent(self, x):\n        return self.style_mlp(x)\n\n    def mean_latent(self, num_latent):\n        latent_in = torch.randn(\n            num_latent, self.num_style_feat, device=self.constant_input.weight.device\n        )\n        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n        return latent\n\n    def forward(\n        self,\n        styles,\n        input_is_latent=False,\n        noise=None,\n        randomize_noise=True,\n        truncation=1,\n        truncation_latent=None,\n        inject_index=None,\n        return_latents=False,\n    ):\n        \"\"\"Forward function for StyleGAN2Generator.\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style.\n                Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is\n                False. Default: True.\n            truncation (float): TODO. Default: 1.\n            truncation_latent (Tensor | None): TODO. Default: None.\n            inject_index (int | None): The injection index for mixing noise.\n                Default: None.\n            return_latents (bool): Whether to return style latents.\n                Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [\n                    getattr(self.noises, f\"noise{i}\") for i in range(self.num_layers)\n                ]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(\n                    truncation_latent + truncation * (style - truncation_latent)\n                )\n            styles = style_truncation\n        # get style latent with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = (\n                styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            )\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\n            self.style_convs[::2],\n            self.style_convs[1::2],\n            noise[1::2],\n            noise[2::2],\n            self.to_rgbs,\n        ):\n            out = conv1(out, latent[:, i], noise=noise1)\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ScaledLeakyReLU(nn.Module):\n    \"\"\"Scaled LeakyReLU.\n    Args:\n        negative_slope (float): Negative slope. Default: 0.2.\n    \"\"\"\n\n    def __init__(self, negative_slope=0.2):\n        super(ScaledLeakyReLU, self).__init__()\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        out = F.leaky_relu(x, negative_slope=self.negative_slope)\n        return out * math.sqrt(2)\n\n\nclass EqualConv2d(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input.\n            Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output.\n            Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        bias=True,\n        bias_init_val=0,\n    ):\n        super(EqualConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(\n            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n        )\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x):\n        out = F.conv2d(\n            x,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n        return out\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(in_channels={self.in_channels}, \"\n            f\"out_channels={self.out_channels}, \"\n            f\"kernel_size={self.kernel_size},\"\n            f\" stride={self.stride}, padding={self.padding}, \"\n            f\"bias={self.bias is not None})\"\n        )\n\n\nclass ConvLayer(nn.Sequential):\n    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Kernel size.\n        downsample (bool): Whether downsample by a factor of 2.\n            Default: False.\n        bias (bool): Whether with bias. Default: True.\n        activate (bool): Whether use activateion. Default: True.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        downsample=False,\n        bias=True,\n        activate=True,\n        interpolation_mode=\"bilinear\",\n    ):\n        layers = []\n        self.interpolation_mode = interpolation_mode\n        # downsample\n        if downsample:\n            if self.interpolation_mode == \"nearest\":\n                self.align_corners = None\n            else:\n                self.align_corners = False\n\n            layers.append(\n                torch.nn.Upsample(\n                    scale_factor=0.5,\n                    mode=interpolation_mode,\n                    align_corners=self.align_corners,\n                )\n            )\n        stride = 1\n        self.padding = kernel_size // 2\n        # conv\n        layers.append(\n            EqualConv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=self.padding,\n                bias=bias and not activate,\n            )\n        )\n        # activation\n        if activate:\n            if bias:\n                layers.append(FusedLeakyReLU(out_channels))\n            else:\n                layers.append(ScaledLeakyReLU(0.2))\n\n        super(ConvLayer, self).__init__(*layers)\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual block used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, interpolation_mode=\"bilinear\"):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvLayer(\n            in_channels,\n            out_channels,\n            3,\n            downsample=True,\n            interpolation_mode=interpolation_mode,\n            bias=True,\n            activate=True,\n        )\n        self.skip = ConvLayer(\n            in_channels,\n            out_channels,\n            1,\n            downsample=True,\n            interpolation_mode=interpolation_mode,\n            bias=False,\n            activate=False,\n        )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        skip = self.skip(x)\n        out = (out + skip) / math.sqrt(2)\n        return out\n", "ldm_patched/pfn/architecture/face/arcface_arch.py": "import torch.nn as nn\n\n\ndef conv3x3(inplanes, outplanes, stride=1):\n    \"\"\"A simple wrapper for 3x3 convolution with padding.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        outplanes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n    \"\"\"\n    return nn.Conv2d(\n        inplanes, outplanes, kernel_size=3, stride=stride, padding=1, bias=False\n    )\n\n\nclass BasicBlock(nn.Module):\n    \"\"\"Basic residual block used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n\n    expansion = 1  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass IRBlock(nn.Module):\n    \"\"\"Improved residual block (IR Block) used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n\n    expansion = 1  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n        super(IRBlock, self).__init__()\n        self.bn0 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, inplanes)\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.prelu = nn.PReLU()\n        self.conv2 = conv3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.use_se = use_se\n        if self.use_se:\n            self.se = SEBlock(planes)\n\n    def forward(self, x):\n        residual = x\n        out = self.bn0(x)\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.prelu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.use_se:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.prelu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"Bottleneck block used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n\n    expansion = 4  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBlock(nn.Module):\n    \"\"\"The squeeze-and-excitation block (SEBlock) used in the IRBlock.\n\n    Args:\n        channel (int): Channel number of inputs.\n        reduction (int): Channel reduction ration. Default: 16.\n    \"\"\"\n\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            1\n        )  # pool to 1x1 without spatial information\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.PReLU(),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass ResNetArcFace(nn.Module):\n    \"\"\"ArcFace with ResNet architectures.\n\n    Ref: ArcFace: Additive Angular Margin Loss for Deep Face Recognition.\n\n    Args:\n        block (str): Block used in the ArcFace architecture.\n        layers (tuple(int)): Block numbers in each layer.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n\n    def __init__(self, block, layers, use_se=True):\n        if block == \"IRBlock\":\n            block = IRBlock\n        self.inplanes = 64\n        self.use_se = use_se\n        super(ResNetArcFace, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.prelu = nn.PReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.dropout = nn.Dropout()\n        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n        self.bn5 = nn.BatchNorm1d(512)\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, num_blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, use_se=self.use_se)\n        )\n        self.inplanes = planes\n        for _ in range(1, num_blocks):\n            layers.append(block(self.inplanes, planes, use_se=self.use_se))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.bn4(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n        x = self.bn5(x)\n\n        return x\n", "ldm_patched/pfn/architecture/face/restoreformer_arch.py": "# pylint: skip-file\n# type: ignore\n\"\"\"Modified from https://github.com/wzhouxiff/RestoreFormer\n\"\"\"\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass VectorQuantizer(nn.Module):\n    \"\"\"\n    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n    ____________________________________________\n    Discretization bottleneck part of the VQ-VAE.\n    Inputs:\n    - n_e : number of embeddings\n    - e_dim : dimension of embedding\n    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n    _____________________________________________\n    \"\"\"\n\n    def __init__(self, n_e, e_dim, beta):\n        super(VectorQuantizer, self).__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n    def forward(self, z):\n        \"\"\"\n        Inputs the output of the encoder network z and maps it to a discrete\n        one-hot vector that is the index of the closest embedding vector e_j\n        z (continuous) -> z_q (discrete)\n        z.shape = (batch, channel, height, width)\n        quantization pipeline:\n            1. get encoder input (B,C,H,W)\n            2. flatten input to (B*H*W,C)\n        \"\"\"\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = (\n            torch.sum(z_flattened**2, dim=1, keepdim=True)\n            + torch.sum(self.embedding.weight**2, dim=1)\n            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n        )\n\n        # could possible replace this here\n        # #\\start...\n        # find closest encodings\n\n        min_value, min_encoding_indices = torch.min(d, dim=1)\n\n        min_encoding_indices = min_encoding_indices.unsqueeze(1)\n\n        min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(z)\n        min_encodings.scatter_(1, min_encoding_indices, 1)\n\n        # dtype min encodings: torch.float32\n        # min_encodings shape: torch.Size([2048, 512])\n        # min_encoding_indices.shape: torch.Size([2048, 1])\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n        # .........\\end\n\n        # with:\n        # .........\\start\n        # min_encoding_indices = torch.argmin(d, dim=1)\n        # z_q = self.embedding(min_encoding_indices)\n        # ......\\end......... (TODO)\n\n        # compute loss for embedding\n        loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean(\n            (z_q - z.detach()) ** 2\n        )\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # perplexity\n\n        e_mean = torch.mean(min_encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices, d)\n\n    def get_codebook_entry(self, indices, shape):\n        # shape specifying (batch, height, width, channel)\n        # TODO: check for more easy handling with nn.Embedding\n        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n        min_encodings.scatter_(1, indices[:, None], 1)\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q\n\n\n# pytorch_diffusion + derived encoder decoder\ndef nonlinearity(x):\n    # swish\n    return x * torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(\n                in_channels, in_channels, kernel_size=3, stride=1, padding=1\n            )\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(\n                in_channels, in_channels, kernel_size=3, stride=2, padding=0\n            )\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0, 1, 0, 1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout,\n        temb_channels=512\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(\n                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n                )\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(\n                    in_channels, out_channels, kernel_size=1, stride=1, padding=0\n                )\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x + h\n\n\nclass MultiHeadAttnBlock(nn.Module):\n    def __init__(self, in_channels, head_size=1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.head_size = head_size\n        self.att_size = in_channels // head_size\n        assert (\n            in_channels % head_size == 0\n        ), \"The size of head should be divided by the number of channels.\"\n\n        self.norm1 = Normalize(in_channels)\n        self.norm2 = Normalize(in_channels)\n\n        self.q = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.k = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.v = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.proj_out = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.num = 0\n\n    def forward(self, x, y=None):\n        h_ = x\n        h_ = self.norm1(h_)\n        if y is None:\n            y = h_\n        else:\n            y = self.norm2(y)\n\n        q = self.q(y)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b, c, h, w = q.shape\n        q = q.reshape(b, self.head_size, self.att_size, h * w)\n        q = q.permute(0, 3, 1, 2)  # b, hw, head, att\n\n        k = k.reshape(b, self.head_size, self.att_size, h * w)\n        k = k.permute(0, 3, 1, 2)\n\n        v = v.reshape(b, self.head_size, self.att_size, h * w)\n        v = v.permute(0, 3, 1, 2)\n\n        q = q.transpose(1, 2)\n        v = v.transpose(1, 2)\n        k = k.transpose(1, 2).transpose(2, 3)\n\n        scale = int(self.att_size) ** (-0.5)\n        q.mul_(scale)\n        w_ = torch.matmul(q, k)\n        w_ = F.softmax(w_, dim=3)\n\n        w_ = w_.matmul(v)\n\n        w_ = w_.transpose(1, 2).contiguous()  # [b, h*w, head, att]\n        w_ = w_.view(b, h, w, -1)\n        w_ = w_.permute(0, 3, 1, 2)\n\n        w_ = self.proj_out(w_)\n\n        return x + w_\n\n\nclass MultiHeadEncoder(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,\n        in_channels=3,\n        resolution=512,\n        z_channels=256,\n        double_z=True,\n        enable_mid=True,\n        head_size=1,\n        **ignore_kwargs\n    ):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.enable_mid = enable_mid\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(\n            in_channels, self.ch, kernel_size=3, stride=1, padding=1\n        )\n\n        curr_res = resolution\n        in_ch_mult = (1,) + tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch * in_ch_mult[i_level]\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in,\n                        out_channels=block_out,\n                        temb_channels=self.temb_ch,\n                        dropout=dropout,\n                    )\n                )\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions - 1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(\n            block_in,\n            2 * z_channels if double_z else z_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        )\n\n    def forward(self, x):\n        hs = {}\n        # timestep embedding\n        temb = None\n\n        # downsampling\n        h = self.conv_in(x)\n        hs[\"in\"] = h\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n\n            if i_level != self.num_resolutions - 1:\n                # hs.append(h)\n                hs[\"block_\" + str(i_level)] = h\n                h = self.down[i_level].downsample(h)\n\n        # middle\n        # h = hs[-1]\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            hs[\"block_\" + str(i_level) + \"_atten\"] = h\n            h = self.mid.attn_1(h)\n            h = self.mid.block_2(h, temb)\n            hs[\"mid_atten\"] = h\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # hs.append(h)\n        hs[\"out\"] = h\n\n        return hs\n\n\nclass MultiHeadDecoder(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,\n        in_channels=3,\n        resolution=512,\n        z_channels=256,\n        give_pre_end=False,\n        enable_mid=True,\n        head_size=1,\n        **ignorekwargs\n    ):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.enable_mid = enable_mid\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        block_in = ch * ch_mult[self.num_resolutions - 1]\n        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n        self.z_shape = (1, z_channels, curr_res, curr_res)\n        print(\n            \"Working with z of shape {} = {} dimensions.\".format(\n                self.z_shape, np.prod(self.z_shape)\n            )\n        )\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(\n            z_channels, block_in, kernel_size=3, stride=1, padding=1\n        )\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in,\n                        out_channels=block_out,\n                        temb_channels=self.temb_ch,\n                        dropout=dropout,\n                    )\n                )\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up)  # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(\n            block_in, out_ch, kernel_size=3, stride=1, padding=1\n        )\n\n    def forward(self, z):\n        # assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            h = self.mid.attn_1(h)\n            h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass MultiHeadDecoderTransformer(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,\n        in_channels=3,\n        resolution=512,\n        z_channels=256,\n        give_pre_end=False,\n        enable_mid=True,\n        head_size=1,\n        **ignorekwargs\n    ):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.enable_mid = enable_mid\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        block_in = ch * ch_mult[self.num_resolutions - 1]\n        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n        self.z_shape = (1, z_channels, curr_res, curr_res)\n        print(\n            \"Working with z of shape {} = {} dimensions.\".format(\n                self.z_shape, np.prod(self.z_shape)\n            )\n        )\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(\n            z_channels, block_in, kernel_size=3, stride=1, padding=1\n        )\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in,\n                out_channels=block_in,\n                temb_channels=self.temb_ch,\n                dropout=dropout,\n            )\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in,\n                        out_channels=block_out,\n                        temb_channels=self.temb_ch,\n                        dropout=dropout,\n                    )\n                )\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up)  # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(\n            block_in, out_ch, kernel_size=3, stride=1, padding=1\n        )\n\n    def forward(self, z, hs):\n        # assert z.shape[1:] == self.z_shape[1:]\n        # self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            h = self.mid.attn_1(h, hs[\"mid_atten\"])\n            h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](\n                        h, hs[\"block_\" + str(i_level) + \"_atten\"]\n                    )\n                    # hfeature = h.clone()\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass RestoreFormer(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n    ):\n        super(RestoreFormer, self).__init__()\n\n        n_embed = 1024\n        embed_dim = 256\n        ch = 64\n        out_ch = 3\n        ch_mult = (1, 2, 2, 4, 4, 8)\n        num_res_blocks = 2\n        attn_resolutions = (16,)\n        dropout = 0.0\n        in_channels = 3\n        resolution = 512\n        z_channels = 256\n        double_z = False\n        enable_mid = True\n        fix_decoder = False\n        fix_codebook = True\n        fix_encoder = False\n        head_size = 8\n\n        self.model_arch = \"RestoreFormer\"\n        self.sub_type = \"Face SR\"\n        self.scale = 8\n        self.in_nc = 3\n        self.out_nc = out_ch\n        self.state = state_dict\n\n        self.supports_fp16 = False\n        self.supports_bf16 = True\n        self.min_size_restriction = 16\n\n        self.encoder = MultiHeadEncoder(\n            ch=ch,\n            out_ch=out_ch,\n            ch_mult=ch_mult,\n            num_res_blocks=num_res_blocks,\n            attn_resolutions=attn_resolutions,\n            dropout=dropout,\n            in_channels=in_channels,\n            resolution=resolution,\n            z_channels=z_channels,\n            double_z=double_z,\n            enable_mid=enable_mid,\n            head_size=head_size,\n        )\n        self.decoder = MultiHeadDecoderTransformer(\n            ch=ch,\n            out_ch=out_ch,\n            ch_mult=ch_mult,\n            num_res_blocks=num_res_blocks,\n            attn_resolutions=attn_resolutions,\n            dropout=dropout,\n            in_channels=in_channels,\n            resolution=resolution,\n            z_channels=z_channels,\n            enable_mid=enable_mid,\n            head_size=head_size,\n        )\n\n        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n\n        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n\n        if fix_decoder:\n            for _, param in self.decoder.named_parameters():\n                param.requires_grad = False\n            for _, param in self.post_quant_conv.named_parameters():\n                param.requires_grad = False\n            for _, param in self.quantize.named_parameters():\n                param.requires_grad = False\n        elif fix_codebook:\n            for _, param in self.quantize.named_parameters():\n                param.requires_grad = False\n\n        if fix_encoder:\n            for _, param in self.encoder.named_parameters():\n                param.requires_grad = False\n\n        self.load_state_dict(state_dict)\n\n    def encode(self, x):\n        hs = self.encoder(x)\n        h = self.quant_conv(hs[\"out\"])\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info, hs\n\n    def decode(self, quant, hs):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant, hs)\n\n        return dec\n\n    def forward(self, input, **kwargs):\n        quant, diff, info, hs = self.encode(input)\n        dec = self.decode(quant, hs)\n\n        return dec, None\n", "ldm_patched/t2ia/adapter.py": "#taken from https://github.com/TencentARC/T2I-Adapter\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if not self.use_conv:\n            padding = [x.shape[2] % 2, x.shape[3] % 2]\n            self.op.padding = padding\n\n        x = self.op(x)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_c, out_c, down, ksize=3, sk=False, use_conv=True):\n        super().__init__()\n        ps = ksize // 2\n        if in_c != out_c or sk == False:\n            self.in_conv = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            # print('n_in')\n            self.in_conv = None\n        self.block1 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(out_c, out_c, ksize, 1, ps)\n        if sk == False:\n            self.skep = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            self.skep = None\n\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=use_conv)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        if self.in_conv is not None:  # edit\n            x = self.in_conv(x)\n\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n        if self.skep is not None:\n            return h + self.skep(x)\n        else:\n            return h + x\n\n\nclass Adapter(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64, ksize=3, sk=False, use_conv=True, xl=True):\n        super(Adapter, self).__init__()\n        self.unshuffle_amount = 8\n        resblock_no_downsample = []\n        resblock_downsample = [3, 2, 1]\n        self.xl = xl\n        if self.xl:\n            self.unshuffle_amount = 16\n            resblock_no_downsample = [1]\n            resblock_downsample = [2]\n\n        self.input_channels = cin // (self.unshuffle_amount * self.unshuffle_amount)\n        self.unshuffle = nn.PixelUnshuffle(self.unshuffle_amount)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        for i in range(len(channels)):\n            for j in range(nums_rb):\n                if (i in resblock_downsample) and (j == 0):\n                    self.body.append(\n                        ResnetBlock(channels[i - 1], channels[i], down=True, ksize=ksize, sk=sk, use_conv=use_conv))\n                elif (i in resblock_no_downsample) and (j == 0):\n                    self.body.append(\n                        ResnetBlock(channels[i - 1], channels[i], down=False, ksize=ksize, sk=sk, use_conv=use_conv))\n                else:\n                    self.body.append(\n                        ResnetBlock(channels[i], channels[i], down=False, ksize=ksize, sk=sk, use_conv=use_conv))\n        self.body = nn.ModuleList(self.body)\n        self.conv_in = nn.Conv2d(cin, channels[0], 3, 1, 1)\n\n    def forward(self, x):\n        # unshuffle\n        x = self.unshuffle(x)\n        # extract features\n        features = []\n        x = self.conv_in(x)\n        for i in range(len(self.channels)):\n            for j in range(self.nums_rb):\n                idx = i * self.nums_rb + j\n                x = self.body[idx](x)\n            if self.xl:\n                features.append(None)\n                if i == 0:\n                    features.append(None)\n                    features.append(None)\n                if i == 2:\n                    features.append(None)\n            else:\n                features.append(None)\n                features.append(None)\n            features.append(x)\n\n        return features\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict([(\"c_fc\", nn.Linear(d_model, d_model * 4)), (\"gelu\", QuickGELU()),\n                         (\"c_proj\", nn.Linear(d_model * 4, d_model))]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass StyleAdapter(nn.Module):\n\n    def __init__(self, width=1024, context_dim=768, num_head=8, n_layes=3, num_token=4):\n        super().__init__()\n\n        scale = width ** -0.5\n        self.transformer_layes = nn.Sequential(*[ResidualAttentionBlock(width, num_head) for _ in range(n_layes)])\n        self.num_token = num_token\n        self.style_embedding = nn.Parameter(torch.randn(1, num_token, width) * scale)\n        self.ln_post = LayerNorm(width)\n        self.ln_pre = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, context_dim))\n\n    def forward(self, x):\n        # x shape [N, HW+1, C]\n        style_embedding = self.style_embedding + torch.zeros(\n            (x.shape[0], self.num_token, self.style_embedding.shape[-1]), device=x.device)\n        x = torch.cat([x, style_embedding], dim=1)\n        x = self.ln_pre(x)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer_layes(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, -self.num_token:, :])\n        x = x @ self.proj\n\n        return x\n\n\nclass ResnetBlock_light(nn.Module):\n    def __init__(self, in_c):\n        super().__init__()\n        self.block1 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n\n    def forward(self, x):\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n\n        return h + x\n\n\nclass extractor(nn.Module):\n    def __init__(self, in_c, inter_c, out_c, nums_rb, down=False):\n        super().__init__()\n        self.in_conv = nn.Conv2d(in_c, inter_c, 1, 1, 0)\n        self.body = []\n        for _ in range(nums_rb):\n            self.body.append(ResnetBlock_light(inter_c))\n        self.body = nn.Sequential(*self.body)\n        self.out_conv = nn.Conv2d(inter_c, out_c, 1, 1, 0)\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=False)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        x = self.in_conv(x)\n        x = self.body(x)\n        x = self.out_conv(x)\n\n        return x\n\n\nclass Adapter_light(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64):\n        super(Adapter_light, self).__init__()\n        self.unshuffle_amount = 8\n        self.unshuffle = nn.PixelUnshuffle(self.unshuffle_amount)\n        self.input_channels = cin // (self.unshuffle_amount * self.unshuffle_amount)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        self.xl = False\n\n        for i in range(len(channels)):\n            if i == 0:\n                self.body.append(extractor(in_c=cin, inter_c=channels[i]//4, out_c=channels[i], nums_rb=nums_rb, down=False))\n            else:\n                self.body.append(extractor(in_c=channels[i-1], inter_c=channels[i]//4, out_c=channels[i], nums_rb=nums_rb, down=True))\n        self.body = nn.ModuleList(self.body)\n\n    def forward(self, x):\n        # unshuffle\n        x = self.unshuffle(x)\n        # extract features\n        features = []\n        for i in range(len(self.channels)):\n            x = self.body[i](x)\n            features.append(None)\n            features.append(None)\n            features.append(x)\n\n        return features\n", "ldm_patched/controlnet/cldm.py": "#taken from: https://github.com/lllyasviel/ControlNet\n#and modified\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\nfrom ldm_patched.ldm.modules.diffusionmodules.util import (\n    zero_module,\n    timestep_embedding,\n)\n\nfrom ldm_patched.ldm.modules.attention import SpatialTransformer\nfrom ldm_patched.ldm.modules.diffusionmodules.openaimodel import UNetModel, TimestepEmbedSequential, ResBlock, Downsample\nfrom ldm_patched.ldm.util import exists\nimport ldm_patched.modules.ops\n\nclass ControlledUnetModel(UNetModel):\n    #implemented in the ldm unet\n    pass\n\nclass ControlNet(nn.Module):\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        hint_channels,\n        num_res_blocks,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        dtype=torch.float32,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        adm_in_channels=None,\n        transformer_depth_middle=None,\n        transformer_depth_output=None,\n        device=None,\n        operations=ldm_patched.modules.ops.disable_weight_init,\n        **kwargs,\n    ):\n        super().__init__()\n        assert use_spatial_transformer == True, \"use_spatial_transformer has to be true\"\n        if use_spatial_transformer:\n            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n\n        if context_dim is not None:\n            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n            # from omegaconf.listconfig import ListConfig\n            # if type(context_dim) == ListConfig:\n            #     context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.dims = dims\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n            self.num_res_blocks = num_res_blocks\n\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] >= num_attention_blocks[i], range(len(num_attention_blocks))))\n\n        transformer_depth = transformer_depth[:]\n\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = dtype\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            operations.Linear(model_channels, time_embed_dim, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n        )\n\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n            elif self.num_classes == \"continuous\":\n                print(\"setting up linear c_adm embedding layer\")\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            elif self.num_classes == \"sequential\":\n                assert adm_in_channels is not None\n                self.label_emb = nn.Sequential(\n                    nn.Sequential(\n                        operations.Linear(adm_in_channels, time_embed_dim, dtype=self.dtype, device=device),\n                        nn.SiLU(),\n                        operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n                    )\n                )\n            else:\n                raise ValueError()\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    operations.conv_nd(dims, in_channels, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n                )\n            ]\n        )\n        self.zero_convs = nn.ModuleList([self.make_zero_conv(model_channels, operations=operations, dtype=self.dtype, device=device)])\n\n        self.input_hint_block = TimestepEmbedSequential(\n                    operations.conv_nd(dims, hint_channels, 16, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 16, 16, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 16, 32, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 32, 32, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 32, 96, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 96, 96, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 96, 256, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 256, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n        )\n\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations,\n                    )\n                ]\n                ch = mult * model_channels\n                num_transformers = transformer_depth.pop(0)\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or nr < num_attention_blocks[level]:\n                        layers.append(\n                            SpatialTransformer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint, dtype=self.dtype, device=device, operations=operations\n                            )\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self.zero_convs.append(self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                self.zero_convs.append(self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device))\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        mid_block = [\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        if transformer_depth_middle >= 0:\n            mid_block += [SpatialTransformer(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth_middle, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_linear=use_linear_in_transformer,\n                            use_checkpoint=use_checkpoint, dtype=self.dtype, device=device, operations=operations\n                        ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        self.middle_block = TimestepEmbedSequential(*mid_block)\n        self.middle_block_out = self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device)\n        self._feature_size += ch\n\n    def make_zero_conv(self, channels, operations=None, dtype=None, device=None):\n        return TimestepEmbedSequential(operations.conv_nd(self.dims, channels, channels, 1, padding=0, dtype=dtype, device=device))\n\n    def forward(self, x, hint, timesteps, context, y=None, **kwargs):\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n        emb = self.time_embed(t_emb)\n\n        guided_hint = self.input_hint_block(hint, emb, context)\n\n        outs = []\n\n        hs = []\n        if self.num_classes is not None:\n            assert y.shape[0] == x.shape[0]\n            emb = emb + self.label_emb(y)\n\n        h = x\n        for module, zero_conv in zip(self.input_blocks, self.zero_convs):\n            if guided_hint is not None:\n                h = module(h, emb, context)\n                h += guided_hint\n                guided_hint = None\n            else:\n                h = module(h, emb, context)\n            outs.append(zero_conv(h, emb, context))\n\n        h = self.middle_block(h, emb, context)\n        outs.append(self.middle_block_out(h, emb, context))\n\n        return outs\n\n", "ldm_patched/taesd/taesd.py": "#!/usr/bin/env python3\n\"\"\"\nTiny AutoEncoder for Stable Diffusion\n(DNN for encoding / decoding SD's latent space)\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.ops\n\ndef conv(n_in, n_out, **kwargs):\n    return ldm_patched.modules.ops.disable_weight_init.Conv2d(n_in, n_out, 3, padding=1, **kwargs)\n\nclass Clamp(nn.Module):\n    def forward(self, x):\n        return torch.tanh(x / 3) * 3\n\nclass Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = ldm_patched.modules.ops.disable_weight_init.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()\n        self.fuse = nn.ReLU()\n    def forward(self, x):\n        return self.fuse(self.conv(x) + self.skip(x))\n\ndef Encoder():\n    return nn.Sequential(\n        conv(3, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 4),\n    )\n\ndef Decoder():\n    return nn.Sequential(\n        Clamp(), conv(4, 64), nn.ReLU(),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), conv(64, 3),\n    )\n\nclass TAESD(nn.Module):\n    latent_magnitude = 3\n    latent_shift = 0.5\n\n    def __init__(self, encoder_path=None, decoder_path=None):\n        \"\"\"Initialize pretrained TAESD on the given device from the given checkpoints.\"\"\"\n        super().__init__()\n        self.taesd_encoder = Encoder()\n        self.taesd_decoder = Decoder()\n        self.vae_scale = torch.nn.Parameter(torch.tensor(1.0))\n        if encoder_path is not None:\n            self.taesd_encoder.load_state_dict(ldm_patched.modules.utils.load_torch_file(encoder_path, safe_load=True))\n        if decoder_path is not None:\n            self.taesd_decoder.load_state_dict(ldm_patched.modules.utils.load_torch_file(decoder_path, safe_load=True))\n\n    @staticmethod\n    def scale_latents(x):\n        \"\"\"raw latents -> [0, 1]\"\"\"\n        return x.div(2 * TAESD.latent_magnitude).add(TAESD.latent_shift).clamp(0, 1)\n\n    @staticmethod\n    def unscale_latents(x):\n        \"\"\"[0, 1] -> raw latents\"\"\"\n        return x.sub(TAESD.latent_shift).mul(2 * TAESD.latent_magnitude)\n\n    def decode(self, x):\n        x_sample = self.taesd_decoder(x * self.vae_scale)\n        x_sample = x_sample.sub(0.5).mul(2)\n        return x_sample\n\n    def encode(self, x):\n        return self.taesd_encoder(x * 0.5 + 0.5) / self.vae_scale\n", "ldm_patched/modules/sd1_clip.py": "import os\n\nfrom transformers import CLIPTokenizer\nimport ldm_patched.modules.ops\nimport torch\nimport traceback\nimport zipfile\nfrom . import model_management\nimport ldm_patched.modules.clip_model\nimport json\n\ndef gen_empty_tokens(special_tokens, length):\n    start_token = special_tokens.get(\"start\", None)\n    end_token = special_tokens.get(\"end\", None)\n    pad_token = special_tokens.get(\"pad\")\n    output = []\n    if start_token is not None:\n        output.append(start_token)\n    if end_token is not None:\n        output.append(end_token)\n    output += [pad_token] * (length - len(output))\n    return output\n\nclass ClipTokenWeightEncoder:\n    def encode_token_weights(self, token_weight_pairs):\n        to_encode = list()\n        max_token_len = 0\n        has_weights = False\n        for x in token_weight_pairs:\n            tokens = list(map(lambda a: a[0], x))\n            max_token_len = max(len(tokens), max_token_len)\n            has_weights = has_weights or not all(map(lambda a: a[1] == 1.0, x))\n            to_encode.append(tokens)\n\n        sections = len(to_encode)\n        if has_weights or sections == 0:\n            to_encode.append(gen_empty_tokens(self.special_tokens, max_token_len))\n\n        out, pooled = self.encode(to_encode)\n        if pooled is not None:\n            first_pooled = pooled[0:1].to(model_management.intermediate_device())\n        else:\n            first_pooled = pooled\n\n        output = []\n        for k in range(0, sections):\n            z = out[k:k+1]\n            if has_weights:\n                z_empty = out[-1]\n                for i in range(len(z)):\n                    for j in range(len(z[i])):\n                        weight = token_weight_pairs[k][j][1]\n                        if weight != 1.0:\n                            z[i][j] = (z[i][j] - z_empty[j]) * weight + z_empty[j]\n            output.append(z)\n\n        if (len(output) == 0):\n            return out[-1:].to(model_management.intermediate_device()), first_pooled\n        return torch.cat(output, dim=-2).to(model_management.intermediate_device()), first_pooled\n\nclass SDClipModel(torch.nn.Module, ClipTokenWeightEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    LAYERS = [\n        \"last\",\n        \"pooled\",\n        \"hidden\"\n    ]\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cpu\", max_length=77,\n                 freeze=True, layer=\"last\", layer_idx=None, textmodel_json_config=None, dtype=None, model_class=ldm_patched.modules.clip_model.CLIPTextModel,\n                 special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 49407}, layer_norm_hidden_state=True):  # clip-vit-base-patch32\n        super().__init__()\n        assert layer in self.LAYERS\n\n        if textmodel_json_config is None:\n            textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd1_clip_config.json\")\n\n        with open(textmodel_json_config) as f:\n            config = json.load(f)\n\n        self.transformer = model_class(config, dtype, device, ldm_patched.modules.ops.manual_cast)\n        self.num_layers = self.transformer.num_layers\n\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        self.layer_idx = None\n        self.special_tokens = special_tokens\n        self.text_projection = torch.nn.Parameter(torch.eye(self.transformer.get_input_embeddings().weight.shape[1]))\n        self.logit_scale = torch.nn.Parameter(torch.tensor(4.6055))\n        self.enable_attention_masks = False\n\n        self.layer_norm_hidden_state = layer_norm_hidden_state\n        if layer == \"hidden\":\n            assert layer_idx is not None\n            assert abs(layer_idx) < self.num_layers\n            self.clip_layer(layer_idx)\n        self.layer_default = (self.layer, self.layer_idx)\n\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        #self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def clip_layer(self, layer_idx):\n        if abs(layer_idx) > self.num_layers:\n            self.layer = \"last\"\n        else:\n            self.layer = \"hidden\"\n            self.layer_idx = layer_idx\n\n    def reset_clip_layer(self):\n        self.layer = self.layer_default[0]\n        self.layer_idx = self.layer_default[1]\n\n    def set_up_textual_embeddings(self, tokens, current_embeds):\n        out_tokens = []\n        next_new_token = token_dict_size = current_embeds.weight.shape[0] - 1\n        embedding_weights = []\n\n        for x in tokens:\n            tokens_temp = []\n            for y in x:\n                if isinstance(y, int):\n                    if y == token_dict_size: #EOS token\n                        y = -1\n                    tokens_temp += [y]\n                else:\n                    if y.shape[0] == current_embeds.weight.shape[1]:\n                        embedding_weights += [y]\n                        tokens_temp += [next_new_token]\n                        next_new_token += 1\n                    else:\n                        print(\"WARNING: shape mismatch when trying to apply embedding, embedding will be ignored\", y.shape[0], current_embeds.weight.shape[1])\n            while len(tokens_temp) < len(x):\n                tokens_temp += [self.special_tokens[\"pad\"]]\n            out_tokens += [tokens_temp]\n\n        n = token_dict_size\n        if len(embedding_weights) > 0:\n            new_embedding = torch.nn.Embedding(next_new_token + 1, current_embeds.weight.shape[1], device=current_embeds.weight.device, dtype=current_embeds.weight.dtype)\n            new_embedding.weight[:token_dict_size] = current_embeds.weight[:-1]\n            for x in embedding_weights:\n                new_embedding.weight[n] = x\n                n += 1\n            new_embedding.weight[n] = current_embeds.weight[-1] #EOS embedding\n            self.transformer.set_input_embeddings(new_embedding)\n\n        processed_tokens = []\n        for x in out_tokens:\n            processed_tokens += [list(map(lambda a: n if a == -1 else a, x))] #The EOS token should always be the largest one\n\n        return processed_tokens\n\n    def forward(self, tokens):\n        backup_embeds = self.transformer.get_input_embeddings()\n        device = backup_embeds.weight.device\n        tokens = self.set_up_textual_embeddings(tokens, backup_embeds)\n        tokens = torch.LongTensor(tokens).to(device)\n\n        attention_mask = None\n        if self.enable_attention_masks:\n            attention_mask = torch.zeros_like(tokens)\n            max_token = self.transformer.get_input_embeddings().weight.shape[0] - 1\n            for x in range(attention_mask.shape[0]):\n                for y in range(attention_mask.shape[1]):\n                    attention_mask[x, y] = 1\n                    if tokens[x, y] == max_token:\n                        break\n\n        outputs = self.transformer(tokens, attention_mask, intermediate_output=self.layer_idx, final_layer_norm_intermediate=self.layer_norm_hidden_state)\n        self.transformer.set_input_embeddings(backup_embeds)\n\n        if self.layer == \"last\":\n            z = outputs[0]\n        else:\n            z = outputs[1]\n\n        if outputs[2] is not None:\n            pooled_output = outputs[2].float()\n        else:\n            pooled_output = None\n\n        if self.text_projection is not None and pooled_output is not None:\n            pooled_output = pooled_output.float().to(self.text_projection.device) @ self.text_projection.float()\n        return z.float(), pooled_output\n\n    def encode(self, tokens):\n        return self(tokens)\n\n    def load_sd(self, sd):\n        if \"text_projection\" in sd:\n            self.text_projection[:] = sd.pop(\"text_projection\")\n        if \"text_projection.weight\" in sd:\n            self.text_projection[:] = sd.pop(\"text_projection.weight\").transpose(0, 1)\n        return self.transformer.load_state_dict(sd, strict=False)\n\ndef parse_parentheses(string):\n    result = []\n    current_item = \"\"\n    nesting_level = 0\n    for char in string:\n        if char == \"(\":\n            if nesting_level == 0:\n                if current_item:\n                    result.append(current_item)\n                    current_item = \"(\"\n                else:\n                    current_item = \"(\"\n            else:\n                current_item += char\n            nesting_level += 1\n        elif char == \")\":\n            nesting_level -= 1\n            if nesting_level == 0:\n                result.append(current_item + \")\")\n                current_item = \"\"\n            else:\n                current_item += char\n        else:\n            current_item += char\n    if current_item:\n        result.append(current_item)\n    return result\n\ndef token_weights(string, current_weight):\n    a = parse_parentheses(string)\n    out = []\n    for x in a:\n        weight = current_weight\n        if len(x) >= 2 and x[-1] == ')' and x[0] == '(':\n            x = x[1:-1]\n            xx = x.rfind(\":\")\n            weight *= 1.1\n            if xx > 0:\n                try:\n                    weight = float(x[xx+1:])\n                    x = x[:xx]\n                except:\n                    pass\n            out += token_weights(x, weight)\n        else:\n            out += [(x, current_weight)]\n    return out\n\ndef escape_important(text):\n    text = text.replace(\"\\\\)\", \"\\0\\1\")\n    text = text.replace(\"\\\\(\", \"\\0\\2\")\n    return text\n\ndef unescape_important(text):\n    text = text.replace(\"\\0\\1\", \")\")\n    text = text.replace(\"\\0\\2\", \"(\")\n    return text\n\ndef safe_load_embed_zip(embed_path):\n    with zipfile.ZipFile(embed_path) as myzip:\n        names = list(filter(lambda a: \"data/\" in a, myzip.namelist()))\n        names.reverse()\n        for n in names:\n            with myzip.open(n) as myfile:\n                data = myfile.read()\n                number = len(data) // 4\n                length_embed = 1024 #sd2.x\n                if number < 768:\n                    continue\n                if number % 768 == 0:\n                    length_embed = 768 #sd1.x\n                num_embeds = number // length_embed\n                embed = torch.frombuffer(data, dtype=torch.float)\n                out = embed.reshape((num_embeds, length_embed)).clone()\n                del embed\n                return out\n\ndef expand_directory_list(directories):\n    dirs = set()\n    for x in directories:\n        dirs.add(x)\n        for root, subdir, file in os.walk(x, followlinks=True):\n            dirs.add(root)\n    return list(dirs)\n\ndef load_embed(embedding_name, embedding_directory, embedding_size, embed_key=None):\n    if isinstance(embedding_directory, str):\n        embedding_directory = [embedding_directory]\n\n    embedding_directory = expand_directory_list(embedding_directory)\n\n    valid_file = None\n    for embed_dir in embedding_directory:\n        embed_path = os.path.abspath(os.path.join(embed_dir, embedding_name))\n        embed_dir = os.path.abspath(embed_dir)\n        try:\n            if os.path.commonpath((embed_dir, embed_path)) != embed_dir:\n                continue\n        except:\n            continue\n        if not os.path.isfile(embed_path):\n            extensions = ['.safetensors', '.pt', '.bin']\n            for x in extensions:\n                t = embed_path + x\n                if os.path.isfile(t):\n                    valid_file = t\n                    break\n        else:\n            valid_file = embed_path\n        if valid_file is not None:\n            break\n\n    if valid_file is None:\n        return None\n\n    embed_path = valid_file\n\n    embed_out = None\n\n    try:\n        if embed_path.lower().endswith(\".safetensors\"):\n            import safetensors.torch\n            embed = safetensors.torch.load_file(embed_path, device=\"cpu\")\n        else:\n            if 'weights_only' in torch.load.__code__.co_varnames:\n                try:\n                    embed = torch.load(embed_path, weights_only=True, map_location=\"cpu\")\n                except:\n                    embed_out = safe_load_embed_zip(embed_path)\n            else:\n                embed = torch.load(embed_path, map_location=\"cpu\")\n    except Exception as e:\n        print(traceback.format_exc())\n        print()\n        print(\"error loading embedding, skipping loading:\", embedding_name)\n        return None\n\n    if embed_out is None:\n        if 'string_to_param' in embed:\n            values = embed['string_to_param'].values()\n            embed_out = next(iter(values))\n        elif isinstance(embed, list):\n            out_list = []\n            for x in range(len(embed)):\n                for k in embed[x]:\n                    t = embed[x][k]\n                    if t.shape[-1] != embedding_size:\n                        continue\n                    out_list.append(t.reshape(-1, t.shape[-1]))\n            embed_out = torch.cat(out_list, dim=0)\n        elif embed_key is not None and embed_key in embed:\n            embed_out = embed[embed_key]\n        else:\n            values = embed.values()\n            embed_out = next(iter(values))\n    return embed_out\n\nclass SDTokenizer:\n    def __init__(self, tokenizer_path=None, max_length=77, pad_with_end=True, embedding_directory=None, embedding_size=768, embedding_key='clip_l', tokenizer_class=CLIPTokenizer, has_start_token=True, pad_to_max_length=True):\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd1_tokenizer\")\n        self.tokenizer = tokenizer_class.from_pretrained(tokenizer_path)\n        self.max_length = max_length\n\n        empty = self.tokenizer('')[\"input_ids\"]\n        if has_start_token:\n            self.tokens_start = 1\n            self.start_token = empty[0]\n            self.end_token = empty[1]\n        else:\n            self.tokens_start = 0\n            self.start_token = None\n            self.end_token = empty[0]\n        self.pad_with_end = pad_with_end\n        self.pad_to_max_length = pad_to_max_length\n\n        vocab = self.tokenizer.get_vocab()\n        self.inv_vocab = {v: k for k, v in vocab.items()}\n        self.embedding_directory = embedding_directory\n        self.max_word_length = 8\n        self.embedding_identifier = \"embedding:\"\n        self.embedding_size = embedding_size\n        self.embedding_key = embedding_key\n\n    def _try_get_embedding(self, embedding_name:str):\n        '''\n        Takes a potential embedding name and tries to retrieve it.\n        Returns a Tuple consisting of the embedding and any leftover string, embedding can be None.\n        '''\n        embed = load_embed(embedding_name, self.embedding_directory, self.embedding_size, self.embedding_key)\n        if embed is None:\n            stripped = embedding_name.strip(',')\n            if len(stripped) < len(embedding_name):\n                embed = load_embed(stripped, self.embedding_directory, self.embedding_size, self.embedding_key)\n                return (embed, embedding_name[len(stripped):])\n        return (embed, \"\")\n\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        '''\n        Takes a prompt and converts it to a list of (token, weight, word id) elements.\n        Tokens can both be integer tokens and pre computed CLIP tensors.\n        Word id values are unique per word and embedding, where the id 0 is reserved for non word tokens.\n        Returned list has the dimensions NxM where M is the input size of CLIP\n        '''\n        if self.pad_with_end:\n            pad_token = self.end_token\n        else:\n            pad_token = 0\n\n        text = escape_important(text)\n        parsed_weights = token_weights(text, 1.0)\n\n        #tokenize words\n        tokens = []\n        for weighted_segment, weight in parsed_weights:\n            to_tokenize = unescape_important(weighted_segment).replace(\"\\n\", \" \").split(' ')\n            to_tokenize = [x for x in to_tokenize if x != \"\"]\n            for word in to_tokenize:\n                #if we find an embedding, deal with the embedding\n                if word.startswith(self.embedding_identifier) and self.embedding_directory is not None:\n                    embedding_name = word[len(self.embedding_identifier):].strip('\\n')\n                    embed, leftover = self._try_get_embedding(embedding_name)\n                    if embed is None:\n                        print(f\"warning, embedding:{embedding_name} does not exist, ignoring\")\n                    else:\n                        if len(embed.shape) == 1:\n                            tokens.append([(embed, weight)])\n                        else:\n                            tokens.append([(embed[x], weight) for x in range(embed.shape[0])])\n                    #if we accidentally have leftover text, continue parsing using leftover, else move on to next word\n                    if leftover != \"\":\n                        word = leftover\n                    else:\n                        continue\n                #parse word\n                tokens.append([(t, weight) for t in self.tokenizer(word)[\"input_ids\"][self.tokens_start:-1]])\n\n        #reshape token array to CLIP input size\n        batched_tokens = []\n        batch = []\n        if self.start_token is not None:\n            batch.append((self.start_token, 1.0, 0))\n        batched_tokens.append(batch)\n        for i, t_group in enumerate(tokens):\n            #determine if we're going to try and keep the tokens in a single batch\n            is_large = len(t_group) >= self.max_word_length\n\n            while len(t_group) > 0:\n                if len(t_group) + len(batch) > self.max_length - 1:\n                    remaining_length = self.max_length - len(batch) - 1\n                    #break word in two and add end token\n                    if is_large:\n                        batch.extend([(t,w,i+1) for t,w in t_group[:remaining_length]])\n                        batch.append((self.end_token, 1.0, 0))\n                        t_group = t_group[remaining_length:]\n                    #add end token and pad\n                    else:\n                        batch.append((self.end_token, 1.0, 0))\n                        if self.pad_to_max_length:\n                            batch.extend([(pad_token, 1.0, 0)] * (remaining_length))\n                    #start new batch\n                    batch = []\n                    if self.start_token is not None:\n                        batch.append((self.start_token, 1.0, 0))\n                    batched_tokens.append(batch)\n                else:\n                    batch.extend([(t,w,i+1) for t,w in t_group])\n                    t_group = []\n\n        #fill last batch\n        batch.append((self.end_token, 1.0, 0))\n        if self.pad_to_max_length:\n            batch.extend([(pad_token, 1.0, 0)] * (self.max_length - len(batch)))\n\n        if not return_word_ids:\n            batched_tokens = [[(t, w) for t, w,_ in x] for x in batched_tokens]\n\n        return batched_tokens\n\n\n    def untokenize(self, token_weight_pair):\n        return list(map(lambda a: (a, self.inv_vocab[a[0]]), token_weight_pair))\n\n\nclass SD1Tokenizer:\n    def __init__(self, embedding_directory=None, clip_name=\"l\", tokenizer=SDTokenizer):\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, tokenizer(embedding_directory=embedding_directory))\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[self.clip_name] = getattr(self, self.clip).tokenize_with_weights(text, return_word_ids)\n        return out\n\n    def untokenize(self, token_weight_pair):\n        return getattr(self, self.clip).untokenize(token_weight_pair)\n\n\nclass SD1ClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None, clip_name=\"l\", clip_model=SDClipModel, **kwargs):\n        super().__init__()\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, clip_model(device=device, dtype=dtype, **kwargs))\n\n    def clip_layer(self, layer_idx):\n        getattr(self, self.clip).clip_layer(layer_idx)\n\n    def reset_clip_layer(self):\n        getattr(self, self.clip).reset_clip_layer()\n\n    def encode_token_weights(self, token_weight_pairs):\n        token_weight_pairs = token_weight_pairs[self.clip_name]\n        out, pooled = getattr(self, self.clip).encode_token_weights(token_weight_pairs)\n        return out, pooled\n\n    def load_sd(self, sd):\n        return getattr(self, self.clip).load_sd(sd)\n", "ldm_patched/modules/model_sampling.py": "import torch\nfrom ldm_patched.ldm.modules.diffusionmodules.util import make_beta_schedule\nimport math\nimport numpy as np\n\nclass EPS:\n    def calculate_input(self, sigma, noise):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (noise.ndim - 1))\n        return noise / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input - model_output * sigma\n\n    def noise_scaling(self, sigma, noise, latent_image, max_denoise=False):\n        if max_denoise:\n            noise = noise * torch.sqrt(1.0 + sigma ** 2.0)\n        else:\n            noise = noise * sigma\n\n        noise += latent_image\n        return noise\n\n    def inverse_noise_scaling(self, sigma, latent):\n        return latent\n\nclass V_PREDICTION(EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input * self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2) - model_output * sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\nclass EDM(V_PREDICTION):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input * self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2) + model_output * sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\n\nclass ModelSamplingDiscrete(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        beta_schedule = sampling_settings.get(\"beta_schedule\", \"linear\")\n        linear_start = sampling_settings.get(\"linear_start\", 0.00085)\n        linear_end = sampling_settings.get(\"linear_end\", 0.012)\n\n        self._register_schedule(given_betas=None, beta_schedule=beta_schedule, timesteps=1000, linear_start=linear_start, linear_end=linear_end, cosine_s=8e-3)\n        self.sigma_data = 1.0\n\n    def _register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if given_betas is not None:\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n\n        # self.register_buffer('betas', torch.tensor(betas, dtype=torch.float32))\n        # self.register_buffer('alphas_cumprod', torch.tensor(alphas_cumprod, dtype=torch.float32))\n        # self.register_buffer('alphas_cumprod_prev', torch.tensor(alphas_cumprod_prev, dtype=torch.float32))\n\n        sigmas = ((1 - alphas_cumprod) / alphas_cumprod) ** 0.5\n        alphas_cumprod = torch.tensor(np.cumprod(alphas, axis=0), dtype=torch.float32)\n        self.set_sigmas(sigmas)\n        self.set_alphas_cumprod(alphas_cumprod.float())\n\n    def set_sigmas(self, sigmas):\n        self.register_buffer('sigmas', sigmas.float())\n        self.register_buffer('log_sigmas', sigmas.log().float())\n\n    def set_alphas_cumprod(self, alphas_cumprod):\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float())\n\n    @property\n    def sigma_min(self):\n        return self.sigmas[0]\n\n    @property\n    def sigma_max(self):\n        return self.sigmas[-1]\n\n    def timestep(self, sigma):\n        log_sigma = sigma.log()\n        dists = log_sigma.to(self.log_sigmas.device) - self.log_sigmas[:, None]\n        return dists.abs().argmin(dim=0).view(sigma.shape).to(sigma.device)\n\n    def sigma(self, timestep):\n        t = torch.clamp(timestep.float().to(self.log_sigmas.device), min=0, max=(len(self.sigmas) - 1))\n        low_idx = t.floor().long()\n        high_idx = t.ceil().long()\n        w = t.frac()\n        log_sigma = (1 - w) * self.log_sigmas[low_idx] + w * self.log_sigmas[high_idx]\n        return log_sigma.exp().to(timestep.device)\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n        percent = 1.0 - percent\n        return self.sigma(torch.tensor(percent * 999.0)).item()\n\n\nclass ModelSamplingContinuousEDM(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        sigma_min = sampling_settings.get(\"sigma_min\", 0.002)\n        sigma_max = sampling_settings.get(\"sigma_max\", 120.0)\n        sigma_data = sampling_settings.get(\"sigma_data\", 1.0)\n        self.set_parameters(sigma_min, sigma_max, sigma_data)\n\n    def set_parameters(self, sigma_min, sigma_max, sigma_data):\n        self.sigma_data = sigma_data\n        sigmas = torch.linspace(math.log(sigma_min), math.log(sigma_max), 1000).exp()\n\n        self.register_buffer('sigmas', sigmas) #for compatibility with some schedulers\n        self.register_buffer('log_sigmas', sigmas.log())\n\n    @property\n    def sigma_min(self):\n        return self.sigmas[0]\n\n    @property\n    def sigma_max(self):\n        return self.sigmas[-1]\n\n    def timestep(self, sigma):\n        return 0.25 * sigma.log()\n\n    def sigma(self, timestep):\n        return (timestep / 0.25).exp()\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n        percent = 1.0 - percent\n\n        log_sigma_min = math.log(self.sigma_min)\n        return math.exp((math.log(self.sigma_max) - log_sigma_min) * percent + log_sigma_min)\n\nclass StableCascadeSampling(ModelSamplingDiscrete):\n    def __init__(self, model_config=None):\n        super().__init__()\n\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        self.set_parameters(sampling_settings.get(\"shift\", 1.0))\n\n    def set_parameters(self, shift=1.0, cosine_s=8e-3):\n        self.shift = shift\n        self.cosine_s = torch.tensor(cosine_s)\n        self._init_alpha_cumprod = torch.cos(self.cosine_s / (1 + self.cosine_s) * torch.pi * 0.5) ** 2\n\n        #This part is just for compatibility with some schedulers in the codebase\n        self.num_timesteps = 10000\n        sigmas = torch.empty((self.num_timesteps), dtype=torch.float32)\n        for x in range(self.num_timesteps):\n            t = (x + 1) / self.num_timesteps\n            sigmas[x] = self.sigma(t)\n\n        self.set_sigmas(sigmas)\n\n    def sigma(self, timestep):\n        alpha_cumprod = (torch.cos((timestep + self.cosine_s) / (1 + self.cosine_s) * torch.pi * 0.5) ** 2 / self._init_alpha_cumprod)\n\n        if self.shift != 1.0:\n            var = alpha_cumprod\n            logSNR = (var/(1-var)).log()\n            logSNR += 2 * torch.log(1.0 / torch.tensor(self.shift))\n            alpha_cumprod = logSNR.sigmoid()\n\n        alpha_cumprod = alpha_cumprod.clamp(0.0001, 0.9999)\n        return ((1 - alpha_cumprod) / alpha_cumprod) ** 0.5\n\n    def timestep(self, sigma):\n        var = 1 / ((sigma * sigma) + 1)\n        var = var.clamp(0, 1.0)\n        s, min_var = self.cosine_s.to(var.device), self._init_alpha_cumprod.to(var.device)\n        t = (((var * min_var) ** 0.5).acos() / (torch.pi * 0.5)) * (1 + s) - s\n        return t\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n\n        percent = 1.0 - percent\n        return self.sigma(torch.tensor(percent))", "ldm_patched/modules/samplers.py": "from ldm_patched.k_diffusion import sampling as k_diffusion_sampling\nfrom ldm_patched.unipc import uni_pc\nimport torch\nimport collections\nfrom ldm_patched.modules import model_management\nimport math\n\ndef get_area_and_mult(conds, x_in, timestep_in):\n    area = (x_in.shape[2], x_in.shape[3], 0, 0)\n    strength = 1.0\n\n    if 'timestep_start' in conds:\n        timestep_start = conds['timestep_start']\n        if timestep_in[0] > timestep_start:\n            return None\n    if 'timestep_end' in conds:\n        timestep_end = conds['timestep_end']\n        if timestep_in[0] < timestep_end:\n            return None\n    if 'area' in conds:\n        area = conds['area']\n    if 'strength' in conds:\n        strength = conds['strength']\n\n    input_x = x_in[:,:,area[2]:area[0] + area[2],area[3]:area[1] + area[3]]\n    if 'mask' in conds:\n        # Scale the mask to the size of the input\n        # The mask should have been resized as we began the sampling process\n        mask_strength = 1.0\n        if \"mask_strength\" in conds:\n            mask_strength = conds[\"mask_strength\"]\n        mask = conds['mask']\n        assert(mask.shape[1] == x_in.shape[2])\n        assert(mask.shape[2] == x_in.shape[3])\n        mask = mask[:,area[2]:area[0] + area[2],area[3]:area[1] + area[3]] * mask_strength\n        mask = mask.unsqueeze(1).repeat(input_x.shape[0] // mask.shape[0], input_x.shape[1], 1, 1)\n    else:\n        mask = torch.ones_like(input_x)\n    mult = mask * strength\n\n    if 'mask' not in conds:\n        rr = 8\n        if area[2] != 0:\n            for t in range(rr):\n                mult[:,:,t:1+t,:] *= ((1.0/rr) * (t + 1))\n        if (area[0] + area[2]) < x_in.shape[2]:\n            for t in range(rr):\n                mult[:,:,area[0] - 1 - t:area[0] - t,:] *= ((1.0/rr) * (t + 1))\n        if area[3] != 0:\n            for t in range(rr):\n                mult[:,:,:,t:1+t] *= ((1.0/rr) * (t + 1))\n        if (area[1] + area[3]) < x_in.shape[3]:\n            for t in range(rr):\n                mult[:,:,:,area[1] - 1 - t:area[1] - t] *= ((1.0/rr) * (t + 1))\n\n    conditioning = {}\n    model_conds = conds[\"model_conds\"]\n    for c in model_conds:\n        conditioning[c] = model_conds[c].process_cond(batch_size=x_in.shape[0], device=x_in.device, area=area)\n\n    control = conds.get('control', None)\n\n    patches = None\n    if 'gligen' in conds:\n        gligen = conds['gligen']\n        patches = {}\n        gligen_type = gligen[0]\n        gligen_model = gligen[1]\n        if gligen_type == \"position\":\n            gligen_patch = gligen_model.model.set_position(input_x.shape, gligen[2], input_x.device)\n        else:\n            gligen_patch = gligen_model.model.set_empty(input_x.shape, input_x.device)\n\n        patches['middle_patch'] = [gligen_patch]\n\n    cond_obj = collections.namedtuple('cond_obj', ['input_x', 'mult', 'conditioning', 'area', 'control', 'patches'])\n    return cond_obj(input_x, mult, conditioning, area, control, patches)\n\ndef cond_equal_size(c1, c2):\n    if c1 is c2:\n        return True\n    if c1.keys() != c2.keys():\n        return False\n    for k in c1:\n        if not c1[k].can_concat(c2[k]):\n            return False\n    return True\n\ndef can_concat_cond(c1, c2):\n    if c1.input_x.shape != c2.input_x.shape:\n        return False\n\n    def objects_concatable(obj1, obj2):\n        if (obj1 is None) != (obj2 is None):\n            return False\n        if obj1 is not None:\n            if obj1 is not obj2:\n                return False\n        return True\n\n    if not objects_concatable(c1.control, c2.control):\n        return False\n\n    if not objects_concatable(c1.patches, c2.patches):\n        return False\n\n    return cond_equal_size(c1.conditioning, c2.conditioning)\n\ndef cond_cat(c_list):\n    c_crossattn = []\n    c_concat = []\n    c_adm = []\n    crossattn_max_len = 0\n\n    temp = {}\n    for x in c_list:\n        for k in x:\n            cur = temp.get(k, [])\n            cur.append(x[k])\n            temp[k] = cur\n\n    out = {}\n    for k in temp:\n        conds = temp[k]\n        out[k] = conds[0].concat(conds[1:])\n\n    return out\n\ndef calc_cond_uncond_batch(model, cond, uncond, x_in, timestep, model_options):\n    out_cond = torch.zeros_like(x_in)\n    out_count = torch.ones_like(x_in) * 1e-37\n\n    out_uncond = torch.zeros_like(x_in)\n    out_uncond_count = torch.ones_like(x_in) * 1e-37\n\n    COND = 0\n    UNCOND = 1\n\n    to_run = []\n    for x in cond:\n        p = get_area_and_mult(x, x_in, timestep)\n        if p is None:\n            continue\n\n        to_run += [(p, COND)]\n    if uncond is not None:\n        for x in uncond:\n            p = get_area_and_mult(x, x_in, timestep)\n            if p is None:\n                continue\n\n            to_run += [(p, UNCOND)]\n\n    while len(to_run) > 0:\n        first = to_run[0]\n        first_shape = first[0][0].shape\n        to_batch_temp = []\n        for x in range(len(to_run)):\n            if can_concat_cond(to_run[x][0], first[0]):\n                to_batch_temp += [x]\n\n        to_batch_temp.reverse()\n        to_batch = to_batch_temp[:1]\n\n        free_memory = model_management.get_free_memory(x_in.device)\n        for i in range(1, len(to_batch_temp) + 1):\n            batch_amount = to_batch_temp[:len(to_batch_temp)//i]\n            input_shape = [len(batch_amount) * first_shape[0]] + list(first_shape)[1:]\n            if model.memory_required(input_shape) < free_memory:\n                to_batch = batch_amount\n                break\n\n        input_x = []\n        mult = []\n        c = []\n        cond_or_uncond = []\n        area = []\n        control = None\n        patches = None\n        for x in to_batch:\n            o = to_run.pop(x)\n            p = o[0]\n            input_x.append(p.input_x)\n            mult.append(p.mult)\n            c.append(p.conditioning)\n            area.append(p.area)\n            cond_or_uncond.append(o[1])\n            control = p.control\n            patches = p.patches\n\n        batch_chunks = len(cond_or_uncond)\n        input_x = torch.cat(input_x)\n        c = cond_cat(c)\n        timestep_ = torch.cat([timestep] * batch_chunks)\n\n        if control is not None:\n            c['control'] = control.get_control(input_x, timestep_, c, len(cond_or_uncond))\n\n        transformer_options = {}\n        if 'transformer_options' in model_options:\n            transformer_options = model_options['transformer_options'].copy()\n\n        if patches is not None:\n            if \"patches\" in transformer_options:\n                cur_patches = transformer_options[\"patches\"].copy()\n                for p in patches:\n                    if p in cur_patches:\n                        cur_patches[p] = cur_patches[p] + patches[p]\n                    else:\n                        cur_patches[p] = patches[p]\n            else:\n                transformer_options[\"patches\"] = patches\n\n        transformer_options[\"cond_or_uncond\"] = cond_or_uncond[:]\n        transformer_options[\"sigmas\"] = timestep\n\n        c['transformer_options'] = transformer_options\n\n        if 'model_function_wrapper' in model_options:\n            output = model_options['model_function_wrapper'](model.apply_model, {\"input\": input_x, \"timestep\": timestep_, \"c\": c, \"cond_or_uncond\": cond_or_uncond}).chunk(batch_chunks)\n        else:\n            output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\n        del input_x\n\n        for o in range(batch_chunks):\n            if cond_or_uncond[o] == COND:\n                out_cond[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += output[o] * mult[o]\n                out_count[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += mult[o]\n            else:\n                out_uncond[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += output[o] * mult[o]\n                out_uncond_count[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += mult[o]\n        del mult\n\n    out_cond /= out_count\n    del out_count\n    out_uncond /= out_uncond_count\n    del out_uncond_count\n    return out_cond, out_uncond\n\n#The main sampling function shared by all the samplers\n#Returns denoised\ndef sampling_function(model, x, timestep, uncond, cond, cond_scale, model_options={}, seed=None):\n        if math.isclose(cond_scale, 1.0) and model_options.get(\"disable_cfg1_optimization\", False) == False:\n            uncond_ = None\n        else:\n            uncond_ = uncond\n\n        cond_pred, uncond_pred = calc_cond_uncond_batch(model, cond, uncond_, x, timestep, model_options)\n        if \"sampler_cfg_function\" in model_options:\n            args = {\"cond\": x - cond_pred, \"uncond\": x - uncond_pred, \"cond_scale\": cond_scale, \"timestep\": timestep, \"input\": x, \"sigma\": timestep,\n                    \"cond_denoised\": cond_pred, \"uncond_denoised\": uncond_pred, \"model\": model, \"model_options\": model_options}\n            cfg_result = x - model_options[\"sampler_cfg_function\"](args)\n        else:\n            cfg_result = uncond_pred + (cond_pred - uncond_pred) * cond_scale\n\n        for fn in model_options.get(\"sampler_post_cfg_function\", []):\n            args = {\"denoised\": cfg_result, \"cond\": cond, \"uncond\": uncond, \"model\": model, \"uncond_denoised\": uncond_pred, \"cond_denoised\": cond_pred,\n                    \"sigma\": timestep, \"model_options\": model_options, \"input\": x}\n            cfg_result = fn(args)\n\n        return cfg_result\n\nclass CFGNoisePredictor(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n    def apply_model(self, x, timestep, cond, uncond, cond_scale, model_options={}, seed=None):\n        out = sampling_function(self.inner_model, x, timestep, uncond, cond, cond_scale, model_options=model_options, seed=seed)\n        return out\n    def forward(self, *args, **kwargs):\n        return self.apply_model(*args, **kwargs)\n\nclass KSamplerX0Inpaint(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n    def forward(self, x, sigma, uncond, cond, cond_scale, denoise_mask, model_options={}, seed=None):\n        if denoise_mask is not None:\n            latent_mask = 1. - denoise_mask\n            x = x * denoise_mask + (self.latent_image + self.noise * sigma.reshape([sigma.shape[0]] + [1] * (len(self.noise.shape) - 1))) * latent_mask\n        out = self.inner_model(x, sigma, cond=cond, uncond=uncond, cond_scale=cond_scale, model_options=model_options, seed=seed)\n        if denoise_mask is not None:\n            out = out * denoise_mask + self.latent_image * latent_mask\n        return out\n\ndef simple_scheduler(model, steps):\n    s = model.model_sampling\n    sigs = []\n    ss = len(s.sigmas) / steps\n    for x in range(steps):\n        sigs += [float(s.sigmas[-(1 + int(x * ss))])]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef ddim_scheduler(model, steps):\n    s = model.model_sampling\n    sigs = []\n    ss = len(s.sigmas) // steps\n    x = 1\n    while x < len(s.sigmas):\n        sigs += [float(s.sigmas[x])]\n        x += ss\n    sigs = sigs[::-1]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef normal_scheduler(model, steps, sgm=False, floor=False):\n    s = model.model_sampling\n    start = s.timestep(s.sigma_max)\n    end = s.timestep(s.sigma_min)\n\n    if sgm:\n        timesteps = torch.linspace(start, end, steps + 1)[:-1]\n    else:\n        timesteps = torch.linspace(start, end, steps)\n\n    sigs = []\n    for x in range(len(timesteps)):\n        ts = timesteps[x]\n        sigs.append(s.sigma(ts))\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef get_mask_aabb(masks):\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device, dtype=torch.int)\n\n    b = masks.shape[0]\n\n    bounding_boxes = torch.zeros((b, 4), device=masks.device, dtype=torch.int)\n    is_empty = torch.zeros((b), device=masks.device, dtype=torch.bool)\n    for i in range(b):\n        mask = masks[i]\n        if mask.numel() == 0:\n            continue\n        if torch.max(mask != 0) == False:\n            is_empty[i] = True\n            continue\n        y, x = torch.where(mask)\n        bounding_boxes[i, 0] = torch.min(x)\n        bounding_boxes[i, 1] = torch.min(y)\n        bounding_boxes[i, 2] = torch.max(x)\n        bounding_boxes[i, 3] = torch.max(y)\n\n    return bounding_boxes, is_empty\n\ndef resolve_areas_and_cond_masks(conditions, h, w, device):\n    # We need to decide on an area outside the sampling loop in order to properly generate opposite areas of equal sizes.\n    # While we're doing this, we can also resolve the mask device and scaling for performance reasons\n    for i in range(len(conditions)):\n        c = conditions[i]\n        if 'area' in c:\n            area = c['area']\n            if area[0] == \"percentage\":\n                modified = c.copy()\n                area = (max(1, round(area[1] * h)), max(1, round(area[2] * w)), round(area[3] * h), round(area[4] * w))\n                modified['area'] = area\n                c = modified\n                conditions[i] = c\n\n        if 'mask' in c:\n            mask = c['mask']\n            mask = mask.to(device=device)\n            modified = c.copy()\n            if len(mask.shape) == 2:\n                mask = mask.unsqueeze(0)\n            if mask.shape[1] != h or mask.shape[2] != w:\n                mask = torch.nn.functional.interpolate(mask.unsqueeze(1), size=(h, w), mode='bilinear', align_corners=False).squeeze(1)\n\n            if modified.get(\"set_area_to_bounds\", False):\n                bounds = torch.max(torch.abs(mask),dim=0).values.unsqueeze(0)\n                boxes, is_empty = get_mask_aabb(bounds)\n                if is_empty[0]:\n                    # Use the minimum possible size for efficiency reasons. (Since the mask is all-0, this becomes a noop anyway)\n                    modified['area'] = (8, 8, 0, 0)\n                else:\n                    box = boxes[0]\n                    H, W, Y, X = (box[3] - box[1] + 1, box[2] - box[0] + 1, box[1], box[0])\n                    H = max(8, H)\n                    W = max(8, W)\n                    area = (int(H), int(W), int(Y), int(X))\n                    modified['area'] = area\n\n            modified['mask'] = mask\n            conditions[i] = modified\n\ndef create_cond_with_same_area_if_none(conds, c):\n    if 'area' not in c:\n        return\n\n    c_area = c['area']\n    smallest = None\n    for x in conds:\n        if 'area' in x:\n            a = x['area']\n            if c_area[2] >= a[2] and c_area[3] >= a[3]:\n                if a[0] + a[2] >= c_area[0] + c_area[2]:\n                    if a[1] + a[3] >= c_area[1] + c_area[3]:\n                        if smallest is None:\n                            smallest = x\n                        elif 'area' not in smallest:\n                            smallest = x\n                        else:\n                            if smallest['area'][0] * smallest['area'][1] > a[0] * a[1]:\n                                smallest = x\n        else:\n            if smallest is None:\n                smallest = x\n    if smallest is None:\n        return\n    if 'area' in smallest:\n        if smallest['area'] == c_area:\n            return\n\n    out = c.copy()\n    out['model_conds'] = smallest['model_conds'].copy() #TODO: which fields should be copied?\n    conds += [out]\n\ndef calculate_start_end_timesteps(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n\n        timestep_start = None\n        timestep_end = None\n        if 'start_percent' in x:\n            timestep_start = s.percent_to_sigma(x['start_percent'])\n        if 'end_percent' in x:\n            timestep_end = s.percent_to_sigma(x['end_percent'])\n\n        if (timestep_start is not None) or (timestep_end is not None):\n            n = x.copy()\n            if (timestep_start is not None):\n                n['timestep_start'] = timestep_start\n            if (timestep_end is not None):\n                n['timestep_end'] = timestep_end\n            conds[t] = n\n\ndef pre_run_control(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n\n        timestep_start = None\n        timestep_end = None\n        percent_to_timestep_function = lambda a: s.percent_to_sigma(a)\n        if 'control' in x:\n            x['control'].pre_run(model, percent_to_timestep_function)\n\ndef apply_empty_x_to_equal_area(conds, uncond, name, uncond_fill_func):\n    cond_cnets = []\n    cond_other = []\n    uncond_cnets = []\n    uncond_other = []\n    for t in range(len(conds)):\n        x = conds[t]\n        if 'area' not in x:\n            if name in x and x[name] is not None:\n                cond_cnets.append(x[name])\n            else:\n                cond_other.append((x, t))\n    for t in range(len(uncond)):\n        x = uncond[t]\n        if 'area' not in x:\n            if name in x and x[name] is not None:\n                uncond_cnets.append(x[name])\n            else:\n                uncond_other.append((x, t))\n\n    if len(uncond_cnets) > 0:\n        return\n\n    for x in range(len(cond_cnets)):\n        temp = uncond_other[x % len(uncond_other)]\n        o = temp[0]\n        if name in o and o[name] is not None:\n            n = o.copy()\n            n[name] = uncond_fill_func(cond_cnets, x)\n            uncond += [n]\n        else:\n            n = o.copy()\n            n[name] = uncond_fill_func(cond_cnets, x)\n            uncond[temp[1]] = n\n\ndef encode_model_conds(model_function, conds, noise, device, prompt_type, **kwargs):\n    for t in range(len(conds)):\n        x = conds[t]\n        params = x.copy()\n        params[\"device\"] = device\n        params[\"noise\"] = noise\n        params[\"width\"] = params.get(\"width\", noise.shape[3] * 8)\n        params[\"height\"] = params.get(\"height\", noise.shape[2] * 8)\n        params[\"prompt_type\"] = params.get(\"prompt_type\", prompt_type)\n        for k in kwargs:\n            if k not in params:\n                params[k] = kwargs[k]\n\n        out = model_function(**params)\n        x = x.copy()\n        model_conds = x['model_conds'].copy()\n        for k in out:\n            model_conds[k] = out[k]\n        x['model_conds'] = model_conds\n        conds[t] = x\n    return conds\n\nclass Sampler:\n    def sample(self):\n        pass\n\n    def max_denoise(self, model_wrap, sigmas):\n        max_sigma = float(model_wrap.inner_model.model_sampling.sigma_max)\n        sigma = float(sigmas[0])\n        return math.isclose(max_sigma, sigma, rel_tol=1e-05) or sigma > max_sigma\n\nclass UNIPC(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, disable=disable_pbar)\n\nclass UNIPCBH2(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, variant='bh2', disable=disable_pbar)\n\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\", \"tcd\", \"edm_playground_v2.5\"]\n\nclass KSAMPLER(Sampler):\n    def __init__(self, sampler_function, extra_options={}, inpaint_options={}):\n        self.sampler_function = sampler_function\n        self.extra_options = extra_options\n        self.inpaint_options = inpaint_options\n\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        extra_args[\"denoise_mask\"] = denoise_mask\n        model_k = KSamplerX0Inpaint(model_wrap)\n        model_k.latent_image = latent_image\n        if self.inpaint_options.get(\"random\", False): #TODO: Should this be the default?\n            generator = torch.manual_seed(extra_args.get(\"seed\", 41) + 1)\n            model_k.noise = torch.randn(noise.shape, generator=generator, device=\"cpu\").to(noise.dtype).to(noise.device)\n        else:\n            model_k.noise = noise\n\n        if self.max_denoise(model_wrap, sigmas):\n            noise = noise * torch.sqrt(1.0 + sigmas[0] ** 2.0)\n        else:\n            noise = noise * sigmas[0]\n\n        k_callback = None\n        total_steps = len(sigmas) - 1\n        if callback is not None:\n            k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\n\n        if latent_image is not None:\n            noise += latent_image\n\n        samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\n        return samples\n\n\ndef ksampler(sampler_name, extra_options={}, inpaint_options={}):\n    if sampler_name == \"dpm_fast\":\n        def dpm_fast_function(model, noise, sigmas, extra_args, callback, disable):\n            sigma_min = sigmas[-1]\n            if sigma_min == 0:\n                sigma_min = sigmas[-2]\n            total_steps = len(sigmas) - 1\n            return k_diffusion_sampling.sample_dpm_fast(model, noise, sigma_min, sigmas[0], total_steps, extra_args=extra_args, callback=callback, disable=disable)\n        sampler_function = dpm_fast_function\n    elif sampler_name == \"dpm_adaptive\":\n        def dpm_adaptive_function(model, noise, sigmas, extra_args, callback, disable):\n            sigma_min = sigmas[-1]\n            if sigma_min == 0:\n                sigma_min = sigmas[-2]\n            return k_diffusion_sampling.sample_dpm_adaptive(model, noise, sigma_min, sigmas[0], extra_args=extra_args, callback=callback, disable=disable)\n        sampler_function = dpm_adaptive_function\n    else:\n        sampler_function = getattr(k_diffusion_sampling, \"sample_{}\".format(sampler_name))\n\n    return KSAMPLER(sampler_function, extra_options, inpaint_options)\n\ndef wrap_model(model):\n    model_denoise = CFGNoisePredictor(model)\n    return model_denoise\n\ndef sample(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    positive = positive[:]\n    negative = negative[:]\n\n    resolve_areas_and_cond_masks(positive, noise.shape[2], noise.shape[3], device)\n    resolve_areas_and_cond_masks(negative, noise.shape[2], noise.shape[3], device)\n\n    model_wrap = wrap_model(model)\n\n    calculate_start_end_timesteps(model, negative)\n    calculate_start_end_timesteps(model, positive)\n\n    if latent_image is not None:\n        latent_image = model.process_latent_in(latent_image)\n\n    if hasattr(model, 'extra_conds'):\n        positive = encode_model_conds(model.extra_conds, positive, noise, device, \"positive\", latent_image=latent_image, denoise_mask=denoise_mask, seed=seed)\n        negative = encode_model_conds(model.extra_conds, negative, noise, device, \"negative\", latent_image=latent_image, denoise_mask=denoise_mask, seed=seed)\n\n    #make sure each cond area has an opposite one with the same area\n    for c in positive:\n        create_cond_with_same_area_if_none(negative, c)\n    for c in negative:\n        create_cond_with_same_area_if_none(positive, c)\n\n    pre_run_control(model, negative + positive)\n\n    apply_empty_x_to_equal_area(list(filter(lambda c: c.get('control_apply_to_uncond', False) == True, positive)), negative, 'control', lambda cond_cnets, x: cond_cnets[x])\n    apply_empty_x_to_equal_area(positive, negative, 'gligen', lambda cond_cnets, x: cond_cnets[x])\n\n    extra_args = {\"cond\":positive, \"uncond\":negative, \"cond_scale\": cfg, \"model_options\": model_options, \"seed\":seed}\n\n    samples = sampler.sample(model_wrap, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\n    return model.process_latent_out(samples.to(torch.float32))\n\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\n\ndef calculate_sigmas_scheduler(model, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model, steps)\n    elif scheduler_name == \"simple\":\n        sigmas = simple_scheduler(model, steps)\n    elif scheduler_name == \"ddim_uniform\":\n        sigmas = ddim_scheduler(model, steps)\n    elif scheduler_name == \"sgm_uniform\":\n        sigmas = normal_scheduler(model, steps, sgm=True)\n    else:\n        print(\"error invalid scheduler\", scheduler_name)\n    return sigmas\n\ndef sampler_object(name):\n    if name == \"uni_pc\":\n        sampler = UNIPC()\n    elif name == \"uni_pc_bh2\":\n        sampler = UNIPCBH2()\n    elif name == \"ddim\":\n        sampler = ksampler(\"euler\", inpaint_options={\"random\": True})\n    else:\n        sampler = ksampler(name)\n    return sampler\n\nclass KSampler:\n    SCHEDULERS = SCHEDULER_NAMES\n    SAMPLERS = SAMPLER_NAMES\n\n    def __init__(self, model, steps, device, sampler=None, scheduler=None, denoise=None, model_options={}):\n        self.model = model\n        self.device = device\n        if scheduler not in self.SCHEDULERS:\n            scheduler = self.SCHEDULERS[0]\n        if sampler not in self.SAMPLERS:\n            sampler = self.SAMPLERS[0]\n        self.scheduler = scheduler\n        self.sampler = sampler\n        self.set_steps(steps, denoise)\n        self.denoise = denoise\n        self.model_options = model_options\n\n    def calculate_sigmas(self, steps):\n        sigmas = None\n\n        discard_penultimate_sigma = False\n        if self.sampler in ['dpm_2', 'dpm_2_ancestral', 'uni_pc', 'uni_pc_bh2']:\n            steps += 1\n            discard_penultimate_sigma = True\n\n        sigmas = calculate_sigmas_scheduler(self.model, self.scheduler, steps)\n\n        if discard_penultimate_sigma:\n            sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n        return sigmas\n\n    def set_steps(self, steps, denoise=None):\n        self.steps = steps\n        if denoise is None or denoise > 0.9999:\n            self.sigmas = self.calculate_sigmas(steps).to(self.device)\n        else:\n            new_steps = int(steps/denoise)\n            sigmas = self.calculate_sigmas(new_steps).to(self.device)\n            self.sigmas = sigmas[-(steps + 1):]\n\n    def sample(self, noise, positive, negative, cfg, latent_image=None, start_step=None, last_step=None, force_full_denoise=False, denoise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n        if sigmas is None:\n            sigmas = self.sigmas\n\n        if last_step is not None and last_step < (len(sigmas) - 1):\n            sigmas = sigmas[:last_step + 1]\n            if force_full_denoise:\n                sigmas[-1] = 0\n\n        if start_step is not None:\n            if start_step < (len(sigmas) - 1):\n                sigmas = sigmas[start_step:]\n            else:\n                if latent_image is not None:\n                    return latent_image\n                else:\n                    return torch.zeros_like(noise)\n\n        sampler = sampler_object(self.sampler)\n\n        return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n", "ldm_patched/modules/options.py": "\nargs_parsing = False\n\ndef enable_args_parsing(enable=True):\n    global args_parsing\n    args_parsing = enable\n", "ldm_patched/modules/model_patcher.py": "import torch\nimport copy\nimport inspect\n\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.model_management\n\nclass ModelPatcher:\n    def __init__(self, model, load_device, offload_device, size=0, current_device=None, weight_inplace_update=False):\n        self.size = size\n        self.model = model\n        self.patches = {}\n        self.backup = {}\n        self.object_patches = {}\n        self.object_patches_backup = {}\n        self.model_options = {\"transformer_options\":{}}\n        self.model_size()\n        self.load_device = load_device\n        self.offload_device = offload_device\n        if current_device is None:\n            self.current_device = self.offload_device\n        else:\n            self.current_device = current_device\n\n        self.weight_inplace_update = weight_inplace_update\n\n    def model_size(self):\n        if self.size > 0:\n            return self.size\n        model_sd = self.model.state_dict()\n        self.size = ldm_patched.modules.model_management.module_size(self.model)\n        self.model_keys = set(model_sd.keys())\n        return self.size\n\n    def clone(self):\n        n = ModelPatcher(self.model, self.load_device, self.offload_device, self.size, self.current_device, weight_inplace_update=self.weight_inplace_update)\n        n.patches = {}\n        for k in self.patches:\n            n.patches[k] = self.patches[k][:]\n\n        n.object_patches = self.object_patches.copy()\n        n.model_options = copy.deepcopy(self.model_options)\n        n.model_keys = self.model_keys\n        return n\n\n    def is_clone(self, other):\n        if hasattr(other, 'model') and self.model is other.model:\n            return True\n        return False\n\n    def memory_required(self, input_shape):\n        return self.model.memory_required(input_shape=input_shape)\n\n    def set_model_sampler_cfg_function(self, sampler_cfg_function, disable_cfg1_optimization=False):\n        if len(inspect.signature(sampler_cfg_function).parameters) == 3:\n            self.model_options[\"sampler_cfg_function\"] = lambda args: sampler_cfg_function(args[\"cond\"], args[\"uncond\"], args[\"cond_scale\"]) #Old way\n        else:\n            self.model_options[\"sampler_cfg_function\"] = sampler_cfg_function\n        if disable_cfg1_optimization:\n            self.model_options[\"disable_cfg1_optimization\"] = True\n\n    def set_model_sampler_post_cfg_function(self, post_cfg_function, disable_cfg1_optimization=False):\n        self.model_options[\"sampler_post_cfg_function\"] = self.model_options.get(\"sampler_post_cfg_function\", []) + [post_cfg_function]\n        if disable_cfg1_optimization:\n            self.model_options[\"disable_cfg1_optimization\"] = True\n\n    def set_model_unet_function_wrapper(self, unet_wrapper_function):\n        self.model_options[\"model_function_wrapper\"] = unet_wrapper_function\n\n    def set_model_patch(self, patch, name):\n        to = self.model_options[\"transformer_options\"]\n        if \"patches\" not in to:\n            to[\"patches\"] = {}\n        to[\"patches\"][name] = to[\"patches\"].get(name, []) + [patch]\n\n    def set_model_patch_replace(self, patch, name, block_name, number, transformer_index=None):\n        to = self.model_options[\"transformer_options\"]\n        if \"patches_replace\" not in to:\n            to[\"patches_replace\"] = {}\n        if name not in to[\"patches_replace\"]:\n            to[\"patches_replace\"][name] = {}\n        if transformer_index is not None:\n            block = (block_name, number, transformer_index)\n        else:\n            block = (block_name, number)\n        to[\"patches_replace\"][name][block] = patch\n\n    def set_model_attn1_patch(self, patch):\n        self.set_model_patch(patch, \"attn1_patch\")\n\n    def set_model_attn2_patch(self, patch):\n        self.set_model_patch(patch, \"attn2_patch\")\n\n    def set_model_attn1_replace(self, patch, block_name, number, transformer_index=None):\n        self.set_model_patch_replace(patch, \"attn1\", block_name, number, transformer_index)\n\n    def set_model_attn2_replace(self, patch, block_name, number, transformer_index=None):\n        self.set_model_patch_replace(patch, \"attn2\", block_name, number, transformer_index)\n\n    def set_model_attn1_output_patch(self, patch):\n        self.set_model_patch(patch, \"attn1_output_patch\")\n\n    def set_model_attn2_output_patch(self, patch):\n        self.set_model_patch(patch, \"attn2_output_patch\")\n\n    def set_model_input_block_patch(self, patch):\n        self.set_model_patch(patch, \"input_block_patch\")\n\n    def set_model_input_block_patch_after_skip(self, patch):\n        self.set_model_patch(patch, \"input_block_patch_after_skip\")\n\n    def set_model_output_block_patch(self, patch):\n        self.set_model_patch(patch, \"output_block_patch\")\n\n    def add_object_patch(self, name, obj):\n        self.object_patches[name] = obj\n\n    def model_patches_to(self, device):\n        to = self.model_options[\"transformer_options\"]\n        if \"patches\" in to:\n            patches = to[\"patches\"]\n            for name in patches:\n                patch_list = patches[name]\n                for i in range(len(patch_list)):\n                    if hasattr(patch_list[i], \"to\"):\n                        patch_list[i] = patch_list[i].to(device)\n        if \"patches_replace\" in to:\n            patches = to[\"patches_replace\"]\n            for name in patches:\n                patch_list = patches[name]\n                for k in patch_list:\n                    if hasattr(patch_list[k], \"to\"):\n                        patch_list[k] = patch_list[k].to(device)\n        if \"model_function_wrapper\" in self.model_options:\n            wrap_func = self.model_options[\"model_function_wrapper\"]\n            if hasattr(wrap_func, \"to\"):\n                self.model_options[\"model_function_wrapper\"] = wrap_func.to(device)\n\n    def model_dtype(self):\n        if hasattr(self.model, \"get_dtype\"):\n            return self.model.get_dtype()\n\n    def add_patches(self, patches, strength_patch=1.0, strength_model=1.0):\n        p = set()\n        for k in patches:\n            if k in self.model_keys:\n                p.add(k)\n                current_patches = self.patches.get(k, [])\n                current_patches.append((strength_patch, patches[k], strength_model))\n                self.patches[k] = current_patches\n\n        return list(p)\n\n    def get_key_patches(self, filter_prefix=None):\n        ldm_patched.modules.model_management.unload_model_clones(self)\n        model_sd = self.model_state_dict()\n        p = {}\n        for k in model_sd:\n            if filter_prefix is not None:\n                if not k.startswith(filter_prefix):\n                    continue\n            if k in self.patches:\n                p[k] = [model_sd[k]] + self.patches[k]\n            else:\n                p[k] = (model_sd[k],)\n        return p\n\n    def model_state_dict(self, filter_prefix=None):\n        sd = self.model.state_dict()\n        keys = list(sd.keys())\n        if filter_prefix is not None:\n            for k in keys:\n                if not k.startswith(filter_prefix):\n                    sd.pop(k)\n        return sd\n\n    def patch_model(self, device_to=None, patch_weights=True):\n        for k in self.object_patches:\n            old = getattr(self.model, k)\n            if k not in self.object_patches_backup:\n                self.object_patches_backup[k] = old\n            setattr(self.model, k, self.object_patches[k])\n\n        if patch_weights:\n            model_sd = self.model_state_dict()\n            for key in self.patches:\n                if key not in model_sd:\n                    print(\"could not patch. key doesn't exist in model:\", key)\n                    continue\n\n                weight = model_sd[key]\n\n                inplace_update = self.weight_inplace_update\n\n                if key not in self.backup:\n                    self.backup[key] = weight.to(device=self.offload_device, copy=inplace_update)\n\n                if device_to is not None:\n                    temp_weight = ldm_patched.modules.model_management.cast_to_device(weight, device_to, torch.float32, copy=True)\n                else:\n                    temp_weight = weight.to(torch.float32, copy=True)\n                out_weight = self.calculate_weight(self.patches[key], temp_weight, key).to(weight.dtype)\n                if inplace_update:\n                    ldm_patched.modules.utils.copy_to_param(self.model, key, out_weight)\n                else:\n                    ldm_patched.modules.utils.set_attr(self.model, key, out_weight)\n                del temp_weight\n\n            if device_to is not None:\n                self.model.to(device_to)\n                self.current_device = device_to\n\n        return self.model\n\n    def calculate_weight(self, patches, weight, key):\n        for p in patches:\n            alpha = p[0]\n            v = p[1]\n            strength_model = p[2]\n\n            if strength_model != 1.0:\n                weight *= strength_model\n\n            if isinstance(v, list):\n                v = (self.calculate_weight(v[1:], v[0].clone(), key), )\n\n            if len(v) == 1:\n                patch_type = \"diff\"\n            elif len(v) == 2:\n                patch_type = v[0]\n                v = v[1]\n\n            if patch_type == \"diff\":\n                w1 = v[0]\n                if alpha != 0.0:\n                    if w1.shape != weight.shape:\n                        print(\"WARNING SHAPE MISMATCH {} WEIGHT NOT MERGED {} != {}\".format(key, w1.shape, weight.shape))\n                    else:\n                        weight += alpha * ldm_patched.modules.model_management.cast_to_device(w1, weight.device, weight.dtype)\n            elif patch_type == \"lora\": #lora/locon\n                mat1 = ldm_patched.modules.model_management.cast_to_device(v[0], weight.device, torch.float32)\n                mat2 = ldm_patched.modules.model_management.cast_to_device(v[1], weight.device, torch.float32)\n                if v[2] is not None:\n                    alpha *= v[2] / mat2.shape[0]\n                if v[3] is not None:\n                    #locon mid weights, hopefully the math is fine because I didn't properly test it\n                    mat3 = ldm_patched.modules.model_management.cast_to_device(v[3], weight.device, torch.float32)\n                    final_shape = [mat2.shape[1], mat2.shape[0], mat3.shape[2], mat3.shape[3]]\n                    mat2 = torch.mm(mat2.transpose(0, 1).flatten(start_dim=1), mat3.transpose(0, 1).flatten(start_dim=1)).reshape(final_shape).transpose(0, 1)\n                try:\n                    weight += (alpha * torch.mm(mat1.flatten(start_dim=1), mat2.flatten(start_dim=1))).reshape(weight.shape).type(weight.dtype)\n                except Exception as e:\n                    print(\"ERROR\", key, e)\n            elif patch_type == \"lokr\":\n                w1 = v[0]\n                w2 = v[1]\n                w1_a = v[3]\n                w1_b = v[4]\n                w2_a = v[5]\n                w2_b = v[6]\n                t2 = v[7]\n                dim = None\n\n                if w1 is None:\n                    dim = w1_b.shape[0]\n                    w1 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w1_a, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w1_b, weight.device, torch.float32))\n                else:\n                    w1 = ldm_patched.modules.model_management.cast_to_device(w1, weight.device, torch.float32)\n\n                if w2 is None:\n                    dim = w2_b.shape[0]\n                    if t2 is None:\n                        w2 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w2_a, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w2_b, weight.device, torch.float32))\n                    else:\n                        w2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                          ldm_patched.modules.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                          ldm_patched.modules.model_management.cast_to_device(w2_b, weight.device, torch.float32),\n                                          ldm_patched.modules.model_management.cast_to_device(w2_a, weight.device, torch.float32))\n                else:\n                    w2 = ldm_patched.modules.model_management.cast_to_device(w2, weight.device, torch.float32)\n\n                if len(w2.shape) == 4:\n                    w1 = w1.unsqueeze(2).unsqueeze(2)\n                if v[2] is not None and dim is not None:\n                    alpha *= v[2] / dim\n\n                try:\n                    weight += alpha * torch.kron(w1, w2).reshape(weight.shape).type(weight.dtype)\n                except Exception as e:\n                    print(\"ERROR\", key, e)\n            elif patch_type == \"loha\":\n                w1a = v[0]\n                w1b = v[1]\n                if v[2] is not None:\n                    alpha *= v[2] / w1b.shape[0]\n                w2a = v[3]\n                w2b = v[4]\n                if v[5] is not None: #cp decomposition\n                    t1 = v[5]\n                    t2 = v[6]\n                    m1 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                      ldm_patched.modules.model_management.cast_to_device(t1, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w1b, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w1a, weight.device, torch.float32))\n\n                    m2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                      ldm_patched.modules.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w2b, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w2a, weight.device, torch.float32))\n                else:\n                    m1 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w1a, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w1b, weight.device, torch.float32))\n                    m2 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w2a, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w2b, weight.device, torch.float32))\n\n                try:\n                    weight += (alpha * m1 * m2).reshape(weight.shape).type(weight.dtype)\n                except Exception as e:\n                    print(\"ERROR\", key, e)\n            elif patch_type == \"glora\":\n                if v[4] is not None:\n                    alpha *= v[4] / v[0].shape[0]\n\n                a1 = ldm_patched.modules.model_management.cast_to_device(v[0].flatten(start_dim=1), weight.device, torch.float32)\n                a2 = ldm_patched.modules.model_management.cast_to_device(v[1].flatten(start_dim=1), weight.device, torch.float32)\n                b1 = ldm_patched.modules.model_management.cast_to_device(v[2].flatten(start_dim=1), weight.device, torch.float32)\n                b2 = ldm_patched.modules.model_management.cast_to_device(v[3].flatten(start_dim=1), weight.device, torch.float32)\n\n                weight += ((torch.mm(b2, b1) + torch.mm(torch.mm(weight.flatten(start_dim=1), a2), a1)) * alpha).reshape(weight.shape).type(weight.dtype)\n            else:\n                print(\"patch type not recognized\", patch_type, key)\n\n        return weight\n\n    def unpatch_model(self, device_to=None):\n        keys = list(self.backup.keys())\n\n        if self.weight_inplace_update:\n            for k in keys:\n                ldm_patched.modules.utils.copy_to_param(self.model, k, self.backup[k])\n        else:\n            for k in keys:\n                ldm_patched.modules.utils.set_attr(self.model, k, self.backup[k])\n\n        self.backup = {}\n\n        if device_to is not None:\n            self.model.to(device_to)\n            self.current_device = device_to\n\n        keys = list(self.object_patches_backup.keys())\n        for k in keys:\n            setattr(self.model, k, self.object_patches_backup[k])\n\n        self.object_patches_backup = {}\n", "ldm_patched/modules/args_parser.py": "import argparse\nimport enum\nimport ldm_patched.modules.options\n\nclass EnumAction(argparse.Action):\n    \"\"\"\n    Argparse action for handling Enums\n    \"\"\"\n    def __init__(self, **kwargs):\n        # Pop off the type value\n        enum_type = kwargs.pop(\"type\", None)\n\n        # Ensure an Enum subclass is provided\n        if enum_type is None:\n            raise ValueError(\"type must be assigned an Enum when using EnumAction\")\n        if not issubclass(enum_type, enum.Enum):\n            raise TypeError(\"type must be an Enum when using EnumAction\")\n\n        # Generate choices from the Enum\n        choices = tuple(e.value for e in enum_type)\n        kwargs.setdefault(\"choices\", choices)\n        kwargs.setdefault(\"metavar\", f\"[{','.join(list(choices))}]\")\n\n        super(EnumAction, self).__init__(**kwargs)\n\n        self._enum = enum_type\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        # Convert value back into an Enum\n        value = self._enum(values)\n        setattr(namespace, self.dest, value)\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--listen\", type=str, default=\"127.0.0.1\", metavar=\"IP\", nargs=\"?\", const=\"0.0.0.0\")\nparser.add_argument(\"--port\", type=int, default=8188)\nparser.add_argument(\"--disable-header-check\", type=str, default=None, metavar=\"ORIGIN\", nargs=\"?\", const=\"*\")\nparser.add_argument(\"--web-upload-size\", type=float, default=100)\nparser.add_argument(\"--hf-mirror\", type=str, default=None)\n\nparser.add_argument(\"--external-working-path\", type=str, default=None, metavar=\"PATH\", nargs='+', action='append')\nparser.add_argument(\"--output-path\", type=str, default=None)\nparser.add_argument(\"--temp-path\", type=str, default=None)\nparser.add_argument(\"--cache-path\", type=str, default=None)\nparser.add_argument(\"--in-browser\", action=\"store_true\")\nparser.add_argument(\"--disable-in-browser\", action=\"store_true\")\nparser.add_argument(\"--gpu-device-id\", type=int, default=None, metavar=\"DEVICE_ID\")\ncm_group = parser.add_mutually_exclusive_group()\ncm_group.add_argument(\"--async-cuda-allocation\", action=\"store_true\")\ncm_group.add_argument(\"--disable-async-cuda-allocation\", action=\"store_true\")\n\nparser.add_argument(\"--disable-attention-upcast\", action=\"store_true\")\n\nfp_group = parser.add_mutually_exclusive_group()\nfp_group.add_argument(\"--all-in-fp32\", action=\"store_true\")\nfp_group.add_argument(\"--all-in-fp16\", action=\"store_true\")\n\nfpunet_group = parser.add_mutually_exclusive_group()\nfpunet_group.add_argument(\"--unet-in-bf16\", action=\"store_true\")\nfpunet_group.add_argument(\"--unet-in-fp16\", action=\"store_true\")\nfpunet_group.add_argument(\"--unet-in-fp8-e4m3fn\", action=\"store_true\")\nfpunet_group.add_argument(\"--unet-in-fp8-e5m2\", action=\"store_true\")\n\nfpvae_group = parser.add_mutually_exclusive_group()\nfpvae_group.add_argument(\"--vae-in-fp16\", action=\"store_true\")\nfpvae_group.add_argument(\"--vae-in-fp32\", action=\"store_true\")\nfpvae_group.add_argument(\"--vae-in-bf16\", action=\"store_true\")\n\nparser.add_argument(\"--vae-in-cpu\", action=\"store_true\")\n\nfpte_group = parser.add_mutually_exclusive_group()\nfpte_group.add_argument(\"--clip-in-fp8-e4m3fn\", action=\"store_true\")\nfpte_group.add_argument(\"--clip-in-fp8-e5m2\", action=\"store_true\")\nfpte_group.add_argument(\"--clip-in-fp16\", action=\"store_true\")\nfpte_group.add_argument(\"--clip-in-fp32\", action=\"store_true\")\n\n\nparser.add_argument(\"--directml\", type=int, nargs=\"?\", metavar=\"DIRECTML_DEVICE\", const=-1)\n\nparser.add_argument(\"--disable-ipex-hijack\", action=\"store_true\")\n\nclass LatentPreviewMethod(enum.Enum):\n    NoPreviews = \"none\"\n    Auto = \"auto\"\n    Latent2RGB = \"fast\"\n    TAESD = \"taesd\"\n\nparser.add_argument(\"--preview-option\", type=LatentPreviewMethod, default=LatentPreviewMethod.NoPreviews, action=EnumAction)\n\nattn_group = parser.add_mutually_exclusive_group()\nattn_group.add_argument(\"--attention-split\", action=\"store_true\")\nattn_group.add_argument(\"--attention-quad\", action=\"store_true\")\nattn_group.add_argument(\"--attention-pytorch\", action=\"store_true\")\n\nparser.add_argument(\"--disable-xformers\", action=\"store_true\")\n\nvram_group = parser.add_mutually_exclusive_group()\nvram_group.add_argument(\"--always-gpu\", action=\"store_true\")\nvram_group.add_argument(\"--always-high-vram\", action=\"store_true\")\nvram_group.add_argument(\"--always-normal-vram\", action=\"store_true\")\nvram_group.add_argument(\"--always-low-vram\", action=\"store_true\")\nvram_group.add_argument(\"--always-no-vram\", action=\"store_true\")\nvram_group.add_argument(\"--always-cpu\", type=int, nargs=\"?\", metavar=\"CPU_NUM_THREADS\", const=-1)\n\nparser.add_argument(\"--always-offload-from-vram\", action=\"store_true\")\nparser.add_argument(\"--pytorch-deterministic\", action=\"store_true\")\n\nparser.add_argument(\"--disable-server-log\", action=\"store_true\")\nparser.add_argument(\"--debug-mode\", action=\"store_true\")\nparser.add_argument(\"--is-windows-embedded-python\", action=\"store_true\")\n\nparser.add_argument(\"--disable-server-info\", action=\"store_true\")\n\nparser.add_argument(\"--multi-user\", action=\"store_true\")\n\nif ldm_patched.modules.options.args_parsing:\n    args = parser.parse_args([])\nelse:\n    args = parser.parse_args([])\n\nif args.is_windows_embedded_python:\n    args.in_browser = True\n\nif args.disable_in_browser:\n    args.in_browser = False\n", "ldm_patched/modules/sdxl_clip.py": "from ldm_patched.modules import sd1_clip\nimport torch\nimport os\n\nclass SDXLClipG(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_config_bigg.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype,\n                         special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0}, layer_norm_hidden_state=False)\n\n    def load_sd(self, sd):\n        return super().load_sd(sd)\n\nclass SDXLClipGTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1280, embedding_key='clip_g')\n\n\nclass SDXLTokenizer:\n    def __init__(self, embedding_directory=None):\n        self.clip_l = sd1_clip.SDTokenizer(embedding_directory=embedding_directory)\n        self.clip_g = SDXLClipGTokenizer(embedding_directory=embedding_directory)\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[\"g\"] = self.clip_g.tokenize_with_weights(text, return_word_ids)\n        out[\"l\"] = self.clip_l.tokenize_with_weights(text, return_word_ids)\n        return out\n\n    def untokenize(self, token_weight_pair):\n        return self.clip_g.untokenize(token_weight_pair)\n\nclass SDXLClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__()\n        self.clip_l = sd1_clip.SDClipModel(layer=\"hidden\", layer_idx=-2, device=device, dtype=dtype, layer_norm_hidden_state=False)\n        self.clip_g = SDXLClipG(device=device, dtype=dtype)\n\n    def clip_layer(self, layer_idx):\n        self.clip_l.clip_layer(layer_idx)\n        self.clip_g.clip_layer(layer_idx)\n\n    def reset_clip_layer(self):\n        self.clip_g.reset_clip_layer()\n        self.clip_l.reset_clip_layer()\n\n    def encode_token_weights(self, token_weight_pairs):\n        token_weight_pairs_g = token_weight_pairs[\"g\"]\n        token_weight_pairs_l = token_weight_pairs[\"l\"]\n        g_out, g_pooled = self.clip_g.encode_token_weights(token_weight_pairs_g)\n        l_out, l_pooled = self.clip_l.encode_token_weights(token_weight_pairs_l)\n        return torch.cat([l_out, g_out], dim=-1), g_pooled\n\n    def load_sd(self, sd):\n        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in sd:\n            return self.clip_g.load_sd(sd)\n        else:\n            return self.clip_l.load_sd(sd)\n\nclass SDXLRefinerClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__(device=device, dtype=dtype, clip_name=\"g\", clip_model=SDXLClipG)\n", "ldm_patched/modules/gligen.py": "import torch\nfrom torch import nn\nfrom ldm_patched.ldm.modules.attention import CrossAttention\nfrom inspect import isfunction\n\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * torch.nn.functional.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass GatedCrossAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=context_dim,\n            heads=n_heads,\n            dim_head=d_head)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = nn.LayerNorm(query_dim)\n        self.norm2 = nn.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_attn) * self.attn(self.norm1(x), objs, objs)\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass GatedSelfAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = nn.Linear(context_dim, query_dim)\n\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=query_dim,\n            heads=n_heads,\n            dim_head=d_head)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = nn.LayerNorm(query_dim)\n        self.norm2 = nn.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        N_visual = x.shape[1]\n        objs = self.linear(objs)\n\n        x = x + self.scale * torch.tanh(self.alpha_attn) * self.attn(\n            self.norm1(torch.cat([x, objs], dim=1)))[:, 0:N_visual, :]\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass GatedSelfAttentionDense2(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = nn.Linear(context_dim, query_dim)\n\n        self.attn = CrossAttention(\n            query_dim=query_dim, context_dim=query_dim, dim_head=d_head)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = nn.LayerNorm(query_dim)\n        self.norm2 = nn.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        B, N_visual, _ = x.shape\n        B, N_ground, _ = objs.shape\n\n        objs = self.linear(objs)\n\n        # sanity check\n        size_v = math.sqrt(N_visual)\n        size_g = math.sqrt(N_ground)\n        assert int(size_v) == size_v, \"Visual tokens must be square rootable\"\n        assert int(size_g) == size_g, \"Grounding tokens must be square rootable\"\n        size_v = int(size_v)\n        size_g = int(size_g)\n\n        # select grounding token and resize it to visual token size as residual\n        out = self.attn(self.norm1(torch.cat([x, objs], dim=1)))[\n            :, N_visual:, :]\n        out = out.permute(0, 2, 1).reshape(B, -1, size_g, size_g)\n        out = torch.nn.functional.interpolate(\n            out, (size_v, size_v), mode='bicubic')\n        residual = out.reshape(B, -1, N_visual).permute(0, 2, 1)\n\n        # add residual to visual feature\n        x = x + self.scale * torch.tanh(self.alpha_attn) * residual\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass FourierEmbedder():\n    def __init__(self, num_freqs=64, temperature=100):\n\n        self.num_freqs = num_freqs\n        self.temperature = temperature\n        self.freq_bands = temperature ** (torch.arange(num_freqs) / num_freqs)\n\n    @torch.no_grad()\n    def __call__(self, x, cat_dim=-1):\n        \"x: arbitrary shape of tensor. dim: cat dim\"\n        out = []\n        for freq in self.freq_bands:\n            out.append(torch.sin(freq * x))\n            out.append(torch.cos(freq * x))\n        return torch.cat(out, cat_dim)\n\n\nclass PositionNet(nn.Module):\n    def __init__(self, in_dim, out_dim, fourier_freqs=8):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        self.fourier_embedder = FourierEmbedder(num_freqs=fourier_freqs)\n        self.position_dim = fourier_freqs * 2 * 4  # 2 is sin&cos, 4 is xyxy\n\n        self.linears = nn.Sequential(\n            nn.Linear(self.in_dim + self.position_dim, 512),\n            nn.SiLU(),\n            nn.Linear(512, 512),\n            nn.SiLU(),\n            nn.Linear(512, out_dim),\n        )\n\n        self.null_positive_feature = torch.nn.Parameter(\n            torch.zeros([self.in_dim]))\n        self.null_position_feature = torch.nn.Parameter(\n            torch.zeros([self.position_dim]))\n\n    def forward(self, boxes, masks, positive_embeddings):\n        B, N, _ = boxes.shape\n        dtype = self.linears[0].weight.dtype\n        masks = masks.unsqueeze(-1).to(dtype)\n        positive_embeddings = positive_embeddings.to(dtype)\n\n        # embedding position (it may includes padding as placeholder)\n        xyxy_embedding = self.fourier_embedder(boxes.to(dtype))  # B*N*4 --> B*N*C\n\n        # learnable null embedding\n        positive_null = self.null_positive_feature.view(1, 1, -1)\n        xyxy_null = self.null_position_feature.view(1, 1, -1)\n\n        # replace padding with learnable null embedding\n        positive_embeddings = positive_embeddings * \\\n            masks + (1 - masks) * positive_null\n        xyxy_embedding = xyxy_embedding * masks + (1 - masks) * xyxy_null\n\n        objs = self.linears(\n            torch.cat([positive_embeddings, xyxy_embedding], dim=-1))\n        assert objs.shape == torch.Size([B, N, self.out_dim])\n        return objs\n\n\nclass Gligen(nn.Module):\n    def __init__(self, modules, position_net, key_dim):\n        super().__init__()\n        self.module_list = nn.ModuleList(modules)\n        self.position_net = position_net\n        self.key_dim = key_dim\n        self.max_objs = 30\n        self.current_device = torch.device(\"cpu\")\n\n    def _set_position(self, boxes, masks, positive_embeddings):\n        objs = self.position_net(boxes, masks, positive_embeddings)\n        def func(x, extra_options):\n            key = extra_options[\"transformer_index\"]\n            module = self.module_list[key]\n            return module(x, objs)\n        return func\n\n    def set_position(self, latent_image_shape, position_params, device):\n        batch, c, h, w = latent_image_shape\n        masks = torch.zeros([self.max_objs], device=\"cpu\")\n        boxes = []\n        positive_embeddings = []\n        for p in position_params:\n            x1 = (p[4]) / w\n            y1 = (p[3]) / h\n            x2 = (p[4] + p[2]) / w\n            y2 = (p[3] + p[1]) / h\n            masks[len(boxes)] = 1.0\n            boxes += [torch.tensor((x1, y1, x2, y2)).unsqueeze(0)]\n            positive_embeddings += [p[0]]\n        append_boxes = []\n        append_conds = []\n        if len(boxes) < self.max_objs:\n            append_boxes = [torch.zeros(\n                [self.max_objs - len(boxes), 4], device=\"cpu\")]\n            append_conds = [torch.zeros(\n                [self.max_objs - len(boxes), self.key_dim], device=\"cpu\")]\n\n        box_out = torch.cat(\n            boxes + append_boxes).unsqueeze(0).repeat(batch, 1, 1)\n        masks = masks.unsqueeze(0).repeat(batch, 1)\n        conds = torch.cat(positive_embeddings +\n                          append_conds).unsqueeze(0).repeat(batch, 1, 1)\n        return self._set_position(\n            box_out.to(device),\n            masks.to(device),\n            conds.to(device))\n\n    def set_empty(self, latent_image_shape, device):\n        batch, c, h, w = latent_image_shape\n        masks = torch.zeros([self.max_objs], device=\"cpu\").repeat(batch, 1)\n        box_out = torch.zeros([self.max_objs, 4],\n                              device=\"cpu\").repeat(batch, 1, 1)\n        conds = torch.zeros([self.max_objs, self.key_dim],\n                            device=\"cpu\").repeat(batch, 1, 1)\n        return self._set_position(\n            box_out.to(device),\n            masks.to(device),\n            conds.to(device))\n\n\ndef load_gligen(sd):\n    sd_k = sd.keys()\n    output_list = []\n    key_dim = 768\n    for a in [\"input_blocks\", \"middle_block\", \"output_blocks\"]:\n        for b in range(20):\n            k_temp = filter(lambda k: \"{}.{}.\".format(a, b)\n                            in k and \".fuser.\" in k, sd_k)\n            k_temp = map(lambda k: (k, k.split(\".fuser.\")[-1]), k_temp)\n\n            n_sd = {}\n            for k in k_temp:\n                n_sd[k[1]] = sd[k[0]]\n            if len(n_sd) > 0:\n                query_dim = n_sd[\"linear.weight\"].shape[0]\n                key_dim = n_sd[\"linear.weight\"].shape[1]\n\n                if key_dim == 768:  # SD1.x\n                    n_heads = 8\n                    d_head = query_dim // n_heads\n                else:\n                    d_head = 64\n                    n_heads = query_dim // d_head\n\n                gated = GatedSelfAttentionDense(\n                    query_dim, key_dim, n_heads, d_head)\n                gated.load_state_dict(n_sd, strict=False)\n                output_list.append(gated)\n\n    if \"position_net.null_positive_feature\" in sd_k:\n        in_dim = sd[\"position_net.null_positive_feature\"].shape[0]\n        out_dim = sd[\"position_net.linears.4.weight\"].shape[0]\n\n        class WeightsLoader(torch.nn.Module):\n            pass\n        w = WeightsLoader()\n        w.position_net = PositionNet(in_dim, out_dim)\n        w.load_state_dict(sd, strict=False)\n\n    gligen = Gligen(output_list, w.position_net, key_dim)\n    return gligen\n", "ldm_patched/modules/checkpoint_pickle.py": "import pickle\n\nload = pickle.load\n\nclass Empty:\n    pass\n\nclass Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        #TODO: safe unpickle\n        if module.startswith(\"pytorch_lightning\"):\n            return Empty\n        return super().find_class(module, name)\n", "ldm_patched/modules/sd2_clip.py": "from ldm_patched.modules import sd1_clip\nimport torch\nimport os\n\nclass SD2ClipHModel(sd1_clip.SDClipModel):\n    def __init__(self, arch=\"ViT-H-14\", device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd2_clip_config.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype, special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0})\n\nclass SD2ClipHTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1024)\n\nclass SD2Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"h\", tokenizer=SD2ClipHTokenizer)\n\nclass SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)\n", "ldm_patched/modules/model_detection.py": "import ldm_patched.modules.supported_models\nimport ldm_patched.modules.supported_models_base\n\ndef count_blocks(state_dict_keys, prefix_string):\n    count = 0\n    while True:\n        c = False\n        for k in state_dict_keys:\n            if k.startswith(prefix_string.format(count)):\n                c = True\n                break\n        if c == False:\n            break\n        count += 1\n    return count\n\ndef calculate_transformer_depth(prefix, state_dict_keys, state_dict):\n    context_dim = None\n    use_linear_in_transformer = False\n\n    transformer_prefix = prefix + \"1.transformer_blocks.\"\n    transformer_keys = sorted(list(filter(lambda a: a.startswith(transformer_prefix), state_dict_keys)))\n    if len(transformer_keys) > 0:\n        last_transformer_depth = count_blocks(state_dict_keys, transformer_prefix + '{}')\n        context_dim = state_dict['{}0.attn2.to_k.weight'.format(transformer_prefix)].shape[1]\n        use_linear_in_transformer = len(state_dict['{}1.proj_in.weight'.format(prefix)].shape) == 2\n        time_stack = '{}1.time_stack.0.attn1.to_q.weight'.format(prefix) in state_dict or '{}1.time_mix_blocks.0.attn1.to_q.weight'.format(prefix) in state_dict\n        return last_transformer_depth, context_dim, use_linear_in_transformer, time_stack\n    return None\n\ndef detect_unet_config(state_dict, key_prefix, dtype):\n    state_dict_keys = list(state_dict.keys())\n\n    unet_config = {\n        \"use_checkpoint\": False,\n        \"image_size\": 32,\n        \"use_spatial_transformer\": True,\n        \"legacy\": False\n    }\n\n    y_input = '{}label_emb.0.0.weight'.format(key_prefix)\n    if y_input in state_dict_keys:\n        unet_config[\"num_classes\"] = \"sequential\"\n        unet_config[\"adm_in_channels\"] = state_dict[y_input].shape[1]\n    else:\n        unet_config[\"adm_in_channels\"] = None\n\n    unet_config[\"dtype\"] = dtype\n    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n    in_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[1]\n\n    out_key = '{}out.2.weight'.format(key_prefix)\n    if out_key in state_dict:\n        out_channels = state_dict[out_key].shape[0]\n    else:\n        out_channels = 4\n\n    num_res_blocks = []\n    channel_mult = []\n    attention_resolutions = []\n    transformer_depth = []\n    transformer_depth_output = []\n    context_dim = None\n    use_linear_in_transformer = False\n\n    video_model = False\n\n    current_res = 1\n    count = 0\n\n    last_res_blocks = 0\n    last_channel_mult = 0\n\n    input_block_count = count_blocks(state_dict_keys, '{}input_blocks'.format(key_prefix) + '.{}.')\n    for count in range(input_block_count):\n        prefix = '{}input_blocks.{}.'.format(key_prefix, count)\n        prefix_output = '{}output_blocks.{}.'.format(key_prefix, input_block_count - count - 1)\n\n        block_keys = sorted(list(filter(lambda a: a.startswith(prefix), state_dict_keys)))\n        if len(block_keys) == 0:\n            break\n\n        block_keys_output = sorted(list(filter(lambda a: a.startswith(prefix_output), state_dict_keys)))\n\n        if \"{}0.op.weight\".format(prefix) in block_keys: #new layer\n            num_res_blocks.append(last_res_blocks)\n            channel_mult.append(last_channel_mult)\n\n            current_res *= 2\n            last_res_blocks = 0\n            last_channel_mult = 0\n            out = calculate_transformer_depth(prefix_output, state_dict_keys, state_dict)\n            if out is not None:\n                transformer_depth_output.append(out[0])\n            else:\n                transformer_depth_output.append(0)\n        else:\n            res_block_prefix = \"{}0.in_layers.0.weight\".format(prefix)\n            if res_block_prefix in block_keys:\n                last_res_blocks += 1\n                last_channel_mult = state_dict[\"{}0.out_layers.3.weight\".format(prefix)].shape[0] // model_channels\n\n                out = calculate_transformer_depth(prefix, state_dict_keys, state_dict)\n                if out is not None:\n                    transformer_depth.append(out[0])\n                    if context_dim is None:\n                        context_dim = out[1]\n                        use_linear_in_transformer = out[2]\n                        video_model = out[3]\n                else:\n                    transformer_depth.append(0)\n\n            res_block_prefix = \"{}0.in_layers.0.weight\".format(prefix_output)\n            if res_block_prefix in block_keys_output:\n                out = calculate_transformer_depth(prefix_output, state_dict_keys, state_dict)\n                if out is not None:\n                    transformer_depth_output.append(out[0])\n                else:\n                    transformer_depth_output.append(0)\n\n\n    num_res_blocks.append(last_res_blocks)\n    channel_mult.append(last_channel_mult)\n    if \"{}middle_block.1.proj_in.weight\".format(key_prefix) in state_dict_keys:\n        transformer_depth_middle = count_blocks(state_dict_keys, '{}middle_block.1.transformer_blocks.'.format(key_prefix) + '{}')\n    else:\n        transformer_depth_middle = -1\n\n    unet_config[\"in_channels\"] = in_channels\n    unet_config[\"out_channels\"] = out_channels\n    unet_config[\"model_channels\"] = model_channels\n    unet_config[\"num_res_blocks\"] = num_res_blocks\n    unet_config[\"transformer_depth\"] = transformer_depth\n    unet_config[\"transformer_depth_output\"] = transformer_depth_output\n    unet_config[\"channel_mult\"] = channel_mult\n    unet_config[\"transformer_depth_middle\"] = transformer_depth_middle\n    unet_config['use_linear_in_transformer'] = use_linear_in_transformer\n    unet_config[\"context_dim\"] = context_dim\n\n    if video_model:\n        unet_config[\"extra_ff_mix_layer\"] = True\n        unet_config[\"use_spatial_context\"] = True\n        unet_config[\"merge_strategy\"] = \"learned_with_images\"\n        unet_config[\"merge_factor\"] = 0.0\n        unet_config[\"video_kernel_size\"] = [3, 1, 1]\n        unet_config[\"use_temporal_resblock\"] = True\n        unet_config[\"use_temporal_attention\"] = True\n    else:\n        unet_config[\"use_temporal_resblock\"] = False\n        unet_config[\"use_temporal_attention\"] = False\n\n    return unet_config\n\ndef model_config_from_unet_config(unet_config):\n    for model_config in ldm_patched.modules.supported_models.models:\n        if model_config.matches(unet_config):\n            return model_config(unet_config)\n\n    print(\"no match\", unet_config)\n    return None\n\ndef model_config_from_unet(state_dict, unet_key_prefix, dtype, use_base_if_no_match=False):\n    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n    model_config = model_config_from_unet_config(unet_config)\n    if model_config is None and use_base_if_no_match:\n        return ldm_patched.modules.supported_models_base.BASE(unet_config)\n    else:\n        return model_config\n\ndef convert_config(unet_config):\n    new_config = unet_config.copy()\n    num_res_blocks = new_config.get(\"num_res_blocks\", None)\n    channel_mult = new_config.get(\"channel_mult\", None)\n\n    if isinstance(num_res_blocks, int):\n        num_res_blocks = len(channel_mult) * [num_res_blocks]\n\n    if \"attention_resolutions\" in new_config:\n        attention_resolutions = new_config.pop(\"attention_resolutions\")\n        transformer_depth = new_config.get(\"transformer_depth\", None)\n        transformer_depth_middle = new_config.get(\"transformer_depth_middle\", None)\n\n        if isinstance(transformer_depth, int):\n            transformer_depth = len(channel_mult) * [transformer_depth]\n        if transformer_depth_middle is None:\n            transformer_depth_middle =  transformer_depth[-1]\n        t_in = []\n        t_out = []\n        s = 1\n        for i in range(len(num_res_blocks)):\n            res = num_res_blocks[i]\n            d = 0\n            if s in attention_resolutions:\n                d = transformer_depth[i]\n\n            t_in += [d] * res\n            t_out += [d] * (res + 1)\n            s *= 2\n        transformer_depth = t_in\n        transformer_depth_output = t_out\n        new_config[\"transformer_depth\"] = t_in\n        new_config[\"transformer_depth_output\"] = t_out\n        new_config[\"transformer_depth_middle\"] = transformer_depth_middle\n\n    new_config[\"num_res_blocks\"] = num_res_blocks\n    return new_config\n\n\ndef unet_config_from_diffusers_unet(state_dict, dtype):\n    match = {}\n    transformer_depth = []\n\n    attn_res = 1\n    down_blocks = count_blocks(state_dict, \"down_blocks.{}\")\n    for i in range(down_blocks):\n        attn_blocks = count_blocks(state_dict, \"down_blocks.{}.attentions.\".format(i) + '{}')\n        for ab in range(attn_blocks):\n            transformer_count = count_blocks(state_dict, \"down_blocks.{}.attentions.{}.transformer_blocks.\".format(i, ab) + '{}')\n            transformer_depth.append(transformer_count)\n            if transformer_count > 0:\n                match[\"context_dim\"] = state_dict[\"down_blocks.{}.attentions.{}.transformer_blocks.0.attn2.to_k.weight\".format(i, ab)].shape[1]\n\n        attn_res *= 2\n        if attn_blocks == 0:\n            transformer_depth.append(0)\n            transformer_depth.append(0)\n\n    match[\"transformer_depth\"] = transformer_depth\n\n    match[\"model_channels\"] = state_dict[\"conv_in.weight\"].shape[0]\n    match[\"in_channels\"] = state_dict[\"conv_in.weight\"].shape[1]\n    match[\"adm_in_channels\"] = None\n    if \"class_embedding.linear_1.weight\" in state_dict:\n        match[\"adm_in_channels\"] = state_dict[\"class_embedding.linear_1.weight\"].shape[1]\n    elif \"add_embedding.linear_1.weight\" in state_dict:\n        match[\"adm_in_channels\"] = state_dict[\"add_embedding.linear_1.weight\"].shape[1]\n\n    SDXL = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n            'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 10, 10], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 10,\n            'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 2, 2, 2, 10, 10, 10],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_refiner = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 2560, 'dtype': dtype, 'in_channels': 4, 'model_channels': 384,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [0, 0, 4, 4, 4, 4, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 4,\n                    'use_linear_in_transformer': True, 'context_dim': 1280, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21 = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'adm_in_channels': None, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [2, 2, 2, 2],\n            'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1, 'use_linear_in_transformer': True,\n            'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21_uncliph = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 2048, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1,\n                    'use_linear_in_transformer': True, 'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21_unclipl = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 1536, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1,\n                    'use_linear_in_transformer': True, 'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD15 = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False, 'adm_in_channels': None,\n            'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0],\n            'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1, 'use_linear_in_transformer': False, 'context_dim': 768, 'num_heads': 8,\n            'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_mid_cnet = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                     'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                     'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 0, 0, 1, 1], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 1,\n                     'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 0, 0, 0, 1, 1, 1],\n                     'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_small_cnet = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                       'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                       'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 0, 0, 0, 0], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 0,\n                       'use_linear_in_transformer': True, 'num_head_channels': 64, 'context_dim': 1, 'transformer_depth_output': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_diffusers_inpaint = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 9, 'model_channels': 320,\n                              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 10, 10], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 10,\n                              'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 2, 2, 2, 10, 10, 10],\n                              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SSD_1B = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 4, 4], 'transformer_depth_output': [0, 0, 0, 1, 1, 2, 10, 4, 4],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': -1, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    Segmind_Vega = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 1, 1, 2, 2], 'transformer_depth_output': [0, 0, 0, 1, 1, 1, 2, 2, 2],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': -1, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    supported_models = [SDXL, SDXL_refiner, SD21, SD15, SD21_uncliph, SD21_unclipl, SDXL_mid_cnet, SDXL_small_cnet, SDXL_diffusers_inpaint, SSD_1B, Segmind_Vega]\n\n    for unet_config in supported_models:\n        matches = True\n        for k in match:\n            if match[k] != unet_config[k]:\n                matches = False\n                break\n        if matches:\n            return convert_config(unet_config)\n    return None\n\ndef model_config_from_diffusers_unet(state_dict, dtype):\n    unet_config = unet_config_from_diffusers_unet(state_dict, dtype)\n    if unet_config is not None:\n        return model_config_from_unet_config(unet_config)\n    return None\n", "ldm_patched/modules/sample.py": "import torch\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.conds\nimport ldm_patched.modules.utils\nimport math\nimport numpy as np\n\ndef prepare_noise(latent_image, seed, noise_inds=None):\n    \"\"\"\n    creates random noise given a latent image and a seed.\n    optional arg skip can be used to skip and discard x number of noise generations for a given seed\n    \"\"\"\n    generator = torch.manual_seed(seed)\n    if noise_inds is None:\n        return torch.randn(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, generator=generator, device=\"cpu\")\n    \n    unique_inds, inverse = np.unique(noise_inds, return_inverse=True)\n    noises = []\n    for i in range(unique_inds[-1]+1):\n        noise = torch.randn([1] + list(latent_image.size())[1:], dtype=latent_image.dtype, layout=latent_image.layout, generator=generator, device=\"cpu\")\n        if i in unique_inds:\n            noises.append(noise)\n    noises = [noises[i] for i in inverse]\n    noises = torch.cat(noises, axis=0)\n    return noises\n\ndef prepare_mask(noise_mask, shape, device):\n    \"\"\"ensures noise mask is of proper dimensions\"\"\"\n    noise_mask = torch.nn.functional.interpolate(noise_mask.reshape((-1, 1, noise_mask.shape[-2], noise_mask.shape[-1])), size=(shape[2], shape[3]), mode=\"bilinear\")\n    noise_mask = torch.cat([noise_mask] * shape[1], dim=1)\n    noise_mask = ldm_patched.modules.utils.repeat_to_batch_size(noise_mask, shape[0])\n    noise_mask = noise_mask.to(device)\n    return noise_mask\n\ndef get_models_from_cond(cond, model_type):\n    models = []\n    for c in cond:\n        if model_type in c:\n            models += [c[model_type]]\n    return models\n\ndef convert_cond(cond):\n    out = []\n    for c in cond:\n        temp = c[1].copy()\n        model_conds = temp.get(\"model_conds\", {})\n        if c[0] is not None:\n            model_conds[\"c_crossattn\"] = ldm_patched.modules.conds.CONDCrossAttn(c[0]) #TODO: remove\n            temp[\"cross_attn\"] = c[0]\n        temp[\"model_conds\"] = model_conds\n        out.append(temp)\n    return out\n\ndef get_additional_models(positive, negative, dtype):\n    \"\"\"loads additional models in positive and negative conditioning\"\"\"\n    control_nets = set(get_models_from_cond(positive, \"control\") + get_models_from_cond(negative, \"control\"))\n\n    inference_memory = 0\n    control_models = []\n    for m in control_nets:\n        control_models += m.get_models()\n        inference_memory += m.inference_memory_requirements(dtype)\n\n    gligen = get_models_from_cond(positive, \"gligen\") + get_models_from_cond(negative, \"gligen\")\n    gligen = [x[1] for x in gligen]\n    models = control_models + gligen\n    return models, inference_memory\n\ndef cleanup_additional_models(models):\n    \"\"\"cleanup additional models that were loaded\"\"\"\n    for m in models:\n        if hasattr(m, 'cleanup'):\n            m.cleanup()\n\ndef prepare_sampling(model, noise_shape, positive, negative, noise_mask):\n    device = model.load_device\n    positive = convert_cond(positive)\n    negative = convert_cond(negative)\n\n    if noise_mask is not None:\n        noise_mask = prepare_mask(noise_mask, noise_shape, device)\n\n    real_model = None\n    models, inference_memory = get_additional_models(positive, negative, model.model_dtype())\n    ldm_patched.modules.model_management.load_models_gpu([model] + models, model.memory_required([noise_shape[0] * 2] + list(noise_shape[1:])) + inference_memory)\n    real_model = model.model\n\n    return real_model, positive, negative, noise_mask, models\n\n\ndef sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False, noise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n\n    noise = noise.to(model.load_device)\n    latent_image = latent_image.to(model.load_device)\n\n    sampler = ldm_patched.modules.samplers.KSampler(real_model, steps=steps, device=model.load_device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)\n\n    samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.to(ldm_patched.modules.model_management.intermediate_device())\n\n    cleanup_additional_models(models)\n    cleanup_additional_models(set(get_models_from_cond(positive_copy, \"control\") + get_models_from_cond(negative_copy, \"control\")))\n    return samples\n\ndef sample_custom(model, noise, cfg, sampler, sigmas, positive, negative, latent_image, noise_mask=None, callback=None, disable_pbar=False, seed=None):\n    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n    noise = noise.to(model.load_device)\n    latent_image = latent_image.to(model.load_device)\n    sigmas = sigmas.to(model.load_device)\n\n    samples = ldm_patched.modules.samplers.sample(real_model, noise, positive_copy, negative_copy, cfg, model.load_device, sampler, sigmas, model_options=model.model_options, latent_image=latent_image, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.to(ldm_patched.modules.model_management.intermediate_device())\n    cleanup_additional_models(models)\n    cleanup_additional_models(set(get_models_from_cond(positive_copy, \"control\") + get_models_from_cond(negative_copy, \"control\")))\n    return samples\n\n", "ldm_patched/modules/utils.py": "import torch\nimport math\nimport struct\nimport ldm_patched.modules.checkpoint_pickle\nimport safetensors.torch\nimport numpy as np\nfrom PIL import Image\n\ndef load_torch_file(ckpt, safe_load=False, device=None):\n    if device is None:\n        device = torch.device(\"cpu\")\n    if ckpt.lower().endswith(\".safetensors\"):\n        sd = safetensors.torch.load_file(ckpt, device=device.type)\n    else:\n        if safe_load:\n            if not 'weights_only' in torch.load.__code__.co_varnames:\n                print(\"Warning torch.load doesn't support weights_only on this pytorch version, loading unsafely.\")\n                safe_load = False\n        if safe_load:\n            pl_sd = torch.load(ckpt, map_location=device, weights_only=True)\n        else:\n            pl_sd = torch.load(ckpt, map_location=device, pickle_module=ldm_patched.modules.checkpoint_pickle)\n        if \"global_step\" in pl_sd:\n            print(f\"Global Step: {pl_sd['global_step']}\")\n        if \"state_dict\" in pl_sd:\n            sd = pl_sd[\"state_dict\"]\n        else:\n            sd = pl_sd\n    return sd\n\ndef save_torch_file(sd, ckpt, metadata=None):\n    if metadata is not None:\n        safetensors.torch.save_file(sd, ckpt, metadata=metadata)\n    else:\n        safetensors.torch.save_file(sd, ckpt)\n\ndef calculate_parameters(sd, prefix=\"\"):\n    params = 0\n    for k in sd.keys():\n        if k.startswith(prefix):\n            params += sd[k].nelement()\n    return params\n\ndef state_dict_key_replace(state_dict, keys_to_replace):\n    for x in keys_to_replace:\n        if x in state_dict:\n            state_dict[keys_to_replace[x]] = state_dict.pop(x)\n    return state_dict\n\ndef state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=False):\n    if filter_keys:\n        out = {}\n    else:\n        out = state_dict\n    for rp in replace_prefix:\n        replace = list(map(lambda a: (a, \"{}{}\".format(replace_prefix[rp], a[len(rp):])), filter(lambda a: a.startswith(rp), state_dict.keys())))\n        for x in replace:\n            w = state_dict.pop(x[0])\n            out[x[1]] = w\n    return out\n\n\ndef transformers_convert(sd, prefix_from, prefix_to, number):\n    keys_to_replace = {\n        \"{}positional_embedding\": \"{}embeddings.position_embedding.weight\",\n        \"{}token_embedding.weight\": \"{}embeddings.token_embedding.weight\",\n        \"{}ln_final.weight\": \"{}final_layer_norm.weight\",\n        \"{}ln_final.bias\": \"{}final_layer_norm.bias\",\n    }\n\n    for k in keys_to_replace:\n        x = k.format(prefix_from)\n        if x in sd:\n            sd[keys_to_replace[k].format(prefix_to)] = sd.pop(x)\n\n    resblock_to_replace = {\n        \"ln_1\": \"layer_norm1\",\n        \"ln_2\": \"layer_norm2\",\n        \"mlp.c_fc\": \"mlp.fc1\",\n        \"mlp.c_proj\": \"mlp.fc2\",\n        \"attn.out_proj\": \"self_attn.out_proj\",\n    }\n\n    for resblock in range(number):\n        for x in resblock_to_replace:\n            for y in [\"weight\", \"bias\"]:\n                k = \"{}transformer.resblocks.{}.{}.{}\".format(prefix_from, resblock, x, y)\n                k_to = \"{}encoder.layers.{}.{}.{}\".format(prefix_to, resblock, resblock_to_replace[x], y)\n                if k in sd:\n                    sd[k_to] = sd.pop(k)\n\n        for y in [\"weight\", \"bias\"]:\n            k_from = \"{}transformer.resblocks.{}.attn.in_proj_{}\".format(prefix_from, resblock, y)\n            if k_from in sd:\n                weights = sd.pop(k_from)\n                shape_from = weights.shape[0] // 3\n                for x in range(3):\n                    p = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"]\n                    k_to = \"{}encoder.layers.{}.{}.{}\".format(prefix_to, resblock, p[x], y)\n                    sd[k_to] = weights[shape_from*x:shape_from*(x + 1)]\n    return sd\n\nUNET_MAP_ATTENTIONS = {\n    \"proj_in.weight\",\n    \"proj_in.bias\",\n    \"proj_out.weight\",\n    \"proj_out.bias\",\n    \"norm.weight\",\n    \"norm.bias\",\n}\n\nTRANSFORMER_BLOCKS = {\n    \"norm1.weight\",\n    \"norm1.bias\",\n    \"norm2.weight\",\n    \"norm2.bias\",\n    \"norm3.weight\",\n    \"norm3.bias\",\n    \"attn1.to_q.weight\",\n    \"attn1.to_k.weight\",\n    \"attn1.to_v.weight\",\n    \"attn1.to_out.0.weight\",\n    \"attn1.to_out.0.bias\",\n    \"attn2.to_q.weight\",\n    \"attn2.to_k.weight\",\n    \"attn2.to_v.weight\",\n    \"attn2.to_out.0.weight\",\n    \"attn2.to_out.0.bias\",\n    \"ff.net.0.proj.weight\",\n    \"ff.net.0.proj.bias\",\n    \"ff.net.2.weight\",\n    \"ff.net.2.bias\",\n}\n\nUNET_MAP_RESNET = {\n    \"in_layers.2.weight\": \"conv1.weight\",\n    \"in_layers.2.bias\": \"conv1.bias\",\n    \"emb_layers.1.weight\": \"time_emb_proj.weight\",\n    \"emb_layers.1.bias\": \"time_emb_proj.bias\",\n    \"out_layers.3.weight\": \"conv2.weight\",\n    \"out_layers.3.bias\": \"conv2.bias\",\n    \"skip_connection.weight\": \"conv_shortcut.weight\",\n    \"skip_connection.bias\": \"conv_shortcut.bias\",\n    \"in_layers.0.weight\": \"norm1.weight\",\n    \"in_layers.0.bias\": \"norm1.bias\",\n    \"out_layers.0.weight\": \"norm2.weight\",\n    \"out_layers.0.bias\": \"norm2.bias\",\n}\n\nUNET_MAP_BASIC = {\n    (\"label_emb.0.0.weight\", \"class_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"class_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"class_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"class_embedding.linear_2.bias\"),\n    (\"label_emb.0.0.weight\", \"add_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"add_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"add_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"add_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),\n    (\"out.2.weight\", \"conv_out.weight\"),\n    (\"out.2.bias\", \"conv_out.bias\"),\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\")\n}\n\ndef unet_to_diffusers(unet_config):\n    num_res_blocks = unet_config[\"num_res_blocks\"]\n    channel_mult = unet_config[\"channel_mult\"]\n    transformer_depth = unet_config[\"transformer_depth\"][:]\n    transformer_depth_output = unet_config[\"transformer_depth_output\"][:]\n    num_blocks = len(channel_mult)\n\n    transformers_mid = unet_config.get(\"transformer_depth_middle\", None)\n\n    diffusers_unet_map = {}\n    for x in range(num_blocks):\n        n = 1 + (num_res_blocks[x] + 1) * x\n        for i in range(num_res_blocks[x]):\n            for b in UNET_MAP_RESNET:\n                diffusers_unet_map[\"down_blocks.{}.resnets.{}.{}\".format(x, i, UNET_MAP_RESNET[b])] = \"input_blocks.{}.0.{}\".format(n, b)\n            num_transformers = transformer_depth.pop(0)\n            if num_transformers > 0:\n                for b in UNET_MAP_ATTENTIONS:\n                    diffusers_unet_map[\"down_blocks.{}.attentions.{}.{}\".format(x, i, b)] = \"input_blocks.{}.1.{}\".format(n, b)\n                for t in range(num_transformers):\n                    for b in TRANSFORMER_BLOCKS:\n                        diffusers_unet_map[\"down_blocks.{}.attentions.{}.transformer_blocks.{}.{}\".format(x, i, t, b)] = \"input_blocks.{}.1.transformer_blocks.{}.{}\".format(n, t, b)\n            n += 1\n        for k in [\"weight\", \"bias\"]:\n            diffusers_unet_map[\"down_blocks.{}.downsamplers.0.conv.{}\".format(x, k)] = \"input_blocks.{}.0.op.{}\".format(n, k)\n\n    i = 0\n    for b in UNET_MAP_ATTENTIONS:\n        diffusers_unet_map[\"mid_block.attentions.{}.{}\".format(i, b)] = \"middle_block.1.{}\".format(b)\n    for t in range(transformers_mid):\n        for b in TRANSFORMER_BLOCKS:\n            diffusers_unet_map[\"mid_block.attentions.{}.transformer_blocks.{}.{}\".format(i, t, b)] = \"middle_block.1.transformer_blocks.{}.{}\".format(t, b)\n\n    for i, n in enumerate([0, 2]):\n        for b in UNET_MAP_RESNET:\n            diffusers_unet_map[\"mid_block.resnets.{}.{}\".format(i, UNET_MAP_RESNET[b])] = \"middle_block.{}.{}\".format(n, b)\n\n    num_res_blocks = list(reversed(num_res_blocks))\n    for x in range(num_blocks):\n        n = (num_res_blocks[x] + 1) * x\n        l = num_res_blocks[x] + 1\n        for i in range(l):\n            c = 0\n            for b in UNET_MAP_RESNET:\n                diffusers_unet_map[\"up_blocks.{}.resnets.{}.{}\".format(x, i, UNET_MAP_RESNET[b])] = \"output_blocks.{}.0.{}\".format(n, b)\n            c += 1\n            num_transformers = transformer_depth_output.pop()\n            if num_transformers > 0:\n                c += 1\n                for b in UNET_MAP_ATTENTIONS:\n                    diffusers_unet_map[\"up_blocks.{}.attentions.{}.{}\".format(x, i, b)] = \"output_blocks.{}.1.{}\".format(n, b)\n                for t in range(num_transformers):\n                    for b in TRANSFORMER_BLOCKS:\n                        diffusers_unet_map[\"up_blocks.{}.attentions.{}.transformer_blocks.{}.{}\".format(x, i, t, b)] = \"output_blocks.{}.1.transformer_blocks.{}.{}\".format(n, t, b)\n            if i == l - 1:\n                for k in [\"weight\", \"bias\"]:\n                    diffusers_unet_map[\"up_blocks.{}.upsamplers.0.conv.{}\".format(x, k)] = \"output_blocks.{}.{}.conv.{}\".format(n, c, k)\n            n += 1\n\n    for k in UNET_MAP_BASIC:\n        diffusers_unet_map[k[1]] = k[0]\n\n    return diffusers_unet_map\n\ndef repeat_to_batch_size(tensor, batch_size):\n    if tensor.shape[0] > batch_size:\n        return tensor[:batch_size]\n    elif tensor.shape[0] < batch_size:\n        return tensor.repeat([math.ceil(batch_size / tensor.shape[0])] + [1] * (len(tensor.shape) - 1))[:batch_size]\n    return tensor\n\ndef resize_to_batch_size(tensor, batch_size):\n    in_batch_size = tensor.shape[0]\n    if in_batch_size == batch_size:\n        return tensor\n\n    if batch_size <= 1:\n        return tensor[:batch_size]\n\n    output = torch.empty([batch_size] + list(tensor.shape)[1:], dtype=tensor.dtype, device=tensor.device)\n    if batch_size < in_batch_size:\n        scale = (in_batch_size - 1) / (batch_size - 1)\n        for i in range(batch_size):\n            output[i] = tensor[min(round(i * scale), in_batch_size - 1)]\n    else:\n        scale = in_batch_size / batch_size\n        for i in range(batch_size):\n            output[i] = tensor[min(math.floor((i + 0.5) * scale), in_batch_size - 1)]\n\n    return output\n\ndef convert_sd_to(state_dict, dtype):\n    keys = list(state_dict.keys())\n    for k in keys:\n        state_dict[k] = state_dict[k].to(dtype)\n    return state_dict\n\ndef safetensors_header(safetensors_path, max_size=100*1024*1024):\n    with open(safetensors_path, \"rb\") as f:\n        header = f.read(8)\n        length_of_header = struct.unpack('<Q', header)[0]\n        if length_of_header > max_size:\n            return None\n        return f.read(length_of_header)\n\ndef set_attr(obj, attr, value):\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    setattr(obj, attrs[-1], torch.nn.Parameter(value, requires_grad=False))\n    del prev\n\ndef copy_to_param(obj, attr, value):\n    # inplace update tensor instead of replacing it\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    prev.data.copy_(value)\n\ndef get_attr(obj, attr):\n    attrs = attr.split(\".\")\n    for name in attrs:\n        obj = getattr(obj, name)\n    return obj\n\ndef bislerp(samples, width, height):\n    def slerp(b1, b2, r):\n        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n        \n        c = b1.shape[-1]\n\n        #norms\n        b1_norms = torch.norm(b1, dim=-1, keepdim=True)\n        b2_norms = torch.norm(b2, dim=-1, keepdim=True)\n\n        #normalize\n        b1_normalized = b1 / b1_norms\n        b2_normalized = b2 / b2_norms\n\n        #zero when norms are zero\n        b1_normalized[b1_norms.expand(-1,c) == 0.0] = 0.0\n        b2_normalized[b2_norms.expand(-1,c) == 0.0] = 0.0\n\n        #slerp\n        dot = (b1_normalized*b2_normalized).sum(1)\n        omega = torch.acos(dot)\n        so = torch.sin(omega)\n\n        #technically not mathematically correct, but more pleasing?\n        res = (torch.sin((1.0-r.squeeze(1))*omega)/so).unsqueeze(1)*b1_normalized + (torch.sin(r.squeeze(1)*omega)/so).unsqueeze(1) * b2_normalized\n        res *= (b1_norms * (1.0-r) + b2_norms * r).expand(-1,c)\n\n        #edge cases for same or polar opposites\n        res[dot > 1 - 1e-5] = b1[dot > 1 - 1e-5] \n        res[dot < 1e-5 - 1] = (b1 * (1.0-r) + b2 * r)[dot < 1e-5 - 1]\n        return res\n    \n    def generate_bilinear_data(length_old, length_new, device):\n        coords_1 = torch.arange(length_old, dtype=torch.float32, device=device).reshape((1,1,1,-1))\n        coords_1 = torch.nn.functional.interpolate(coords_1, size=(1, length_new), mode=\"bilinear\")\n        ratios = coords_1 - coords_1.floor()\n        coords_1 = coords_1.to(torch.int64)\n        \n        coords_2 = torch.arange(length_old, dtype=torch.float32, device=device).reshape((1,1,1,-1)) + 1\n        coords_2[:,:,:,-1] -= 1\n        coords_2 = torch.nn.functional.interpolate(coords_2, size=(1, length_new), mode=\"bilinear\")\n        coords_2 = coords_2.to(torch.int64)\n        return ratios, coords_1, coords_2\n\n    orig_dtype = samples.dtype\n    samples = samples.float()\n    n,c,h,w = samples.shape\n    h_new, w_new = (height, width)\n    \n    #linear w\n    ratios, coords_1, coords_2 = generate_bilinear_data(w, w_new, samples.device)\n    coords_1 = coords_1.expand((n, c, h, -1))\n    coords_2 = coords_2.expand((n, c, h, -1))\n    ratios = ratios.expand((n, 1, h, -1))\n\n    pass_1 = samples.gather(-1,coords_1).movedim(1, -1).reshape((-1,c))\n    pass_2 = samples.gather(-1,coords_2).movedim(1, -1).reshape((-1,c))\n    ratios = ratios.movedim(1, -1).reshape((-1,1))\n\n    result = slerp(pass_1, pass_2, ratios)\n    result = result.reshape(n, h, w_new, c).movedim(-1, 1)\n\n    #linear h\n    ratios, coords_1, coords_2 = generate_bilinear_data(h, h_new, samples.device)\n    coords_1 = coords_1.reshape((1,1,-1,1)).expand((n, c, -1, w_new))\n    coords_2 = coords_2.reshape((1,1,-1,1)).expand((n, c, -1, w_new))\n    ratios = ratios.reshape((1,1,-1,1)).expand((n, 1, -1, w_new))\n\n    pass_1 = result.gather(-2,coords_1).movedim(1, -1).reshape((-1,c))\n    pass_2 = result.gather(-2,coords_2).movedim(1, -1).reshape((-1,c))\n    ratios = ratios.movedim(1, -1).reshape((-1,1))\n\n    result = slerp(pass_1, pass_2, ratios)\n    result = result.reshape(n, h_new, w_new, c).movedim(-1, 1)\n    return result.to(orig_dtype)\n\ndef lanczos(samples, width, height):\n    images = [Image.fromarray(np.clip(255. * image.movedim(0, -1).cpu().numpy(), 0, 255).astype(np.uint8)) for image in samples]\n    images = [image.resize((width, height), resample=Image.Resampling.LANCZOS) for image in images]\n    images = [torch.from_numpy(np.array(image).astype(np.float32) / 255.0).movedim(-1, 0) for image in images]\n    result = torch.stack(images)\n    return result.to(samples.device, samples.dtype)\n\ndef common_upscale(samples, width, height, upscale_method, crop):\n        if crop == \"center\":\n            old_width = samples.shape[3]\n            old_height = samples.shape[2]\n            old_aspect = old_width / old_height\n            new_aspect = width / height\n            x = 0\n            y = 0\n            if old_aspect > new_aspect:\n                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)\n            elif old_aspect < new_aspect:\n                y = round((old_height - old_height * (old_aspect / new_aspect)) / 2)\n            s = samples[:,:,y:old_height-y,x:old_width-x]\n        else:\n            s = samples\n\n        if upscale_method == \"bislerp\":\n            return bislerp(s, width, height)\n        elif upscale_method == \"lanczos\":\n            return lanczos(s, width, height)\n        else:\n            return torch.nn.functional.interpolate(s, size=(height, width), mode=upscale_method)\n\ndef get_tiled_scale_steps(width, height, tile_x, tile_y, overlap):\n    return math.ceil((height / (tile_y - overlap))) * math.ceil((width / (tile_x - overlap)))\n\n@torch.inference_mode()\ndef tiled_scale(samples, function, tile_x=64, tile_y=64, overlap = 8, upscale_amount = 4, out_channels = 3, output_device=\"cpu\", pbar = None):\n    output = torch.empty((samples.shape[0], out_channels, round(samples.shape[2] * upscale_amount), round(samples.shape[3] * upscale_amount)), device=output_device)\n    for b in range(samples.shape[0]):\n        s = samples[b:b+1]\n        out = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=output_device)\n        out_div = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=output_device)\n        for y in range(0, s.shape[2], tile_y - overlap):\n            for x in range(0, s.shape[3], tile_x - overlap):\n                s_in = s[:,:,y:y+tile_y,x:x+tile_x]\n\n                ps = function(s_in).to(output_device)\n                mask = torch.ones_like(ps)\n                feather = round(overlap * upscale_amount)\n                for t in range(feather):\n                        mask[:,:,t:1+t,:] *= ((1.0/feather) * (t + 1))\n                        mask[:,:,mask.shape[2] -1 -t: mask.shape[2]-t,:] *= ((1.0/feather) * (t + 1))\n                        mask[:,:,:,t:1+t] *= ((1.0/feather) * (t + 1))\n                        mask[:,:,:,mask.shape[3]- 1 - t: mask.shape[3]- t] *= ((1.0/feather) * (t + 1))\n                out[:,:,round(y*upscale_amount):round((y+tile_y)*upscale_amount),round(x*upscale_amount):round((x+tile_x)*upscale_amount)] += ps * mask\n                out_div[:,:,round(y*upscale_amount):round((y+tile_y)*upscale_amount),round(x*upscale_amount):round((x+tile_x)*upscale_amount)] += mask\n                if pbar is not None:\n                    pbar.update(1)\n\n        output[b:b+1] = out/out_div\n    return output\n\nPROGRESS_BAR_ENABLED = True\ndef set_progress_bar_enabled(enabled):\n    global PROGRESS_BAR_ENABLED\n    PROGRESS_BAR_ENABLED = enabled\n\nPROGRESS_BAR_HOOK = None\ndef set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\n\nclass ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK\n        self.total = total\n        self.current = 0\n        self.hook = PROGRESS_BAR_HOOK\n\n    def update_absolute(self, value, total=None, preview=None):\n        if total is not None:\n            self.total = total\n        if value > self.total:\n            value = self.total\n        self.current = value\n        if self.hook is not None:\n            self.hook(self.current, self.total, preview)\n\n    def update(self, value):\n        self.update_absolute(self.current + value)\n", "ldm_patched/modules/clip_model.py": "import torch\nfrom ldm_patched.ldm.modules.attention import optimized_attention_for_device\n\nclass CLIPAttention(torch.nn.Module):\n    def __init__(self, embed_dim, heads, dtype, device, operations):\n        super().__init__()\n\n        self.heads = heads\n        self.q_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n        self.k_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n        self.v_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n\n        self.out_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n\n    def forward(self, x, mask=None, optimized_attention=None):\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        out = optimized_attention(q, k, v, self.heads, mask)\n        return self.out_proj(out)\n\nACTIVATIONS = {\"quick_gelu\": lambda a: a * torch.sigmoid(1.702 * a),\n               \"gelu\": torch.nn.functional.gelu,\n}\n\nclass CLIPMLP(torch.nn.Module):\n    def __init__(self, embed_dim, intermediate_size, activation, dtype, device, operations):\n        super().__init__()\n        self.fc1 = operations.Linear(embed_dim, intermediate_size, bias=True, dtype=dtype, device=device)\n        self.activation = ACTIVATIONS[activation]\n        self.fc2 = operations.Linear(intermediate_size, embed_dim, bias=True, dtype=dtype, device=device)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        return x\n\nclass CLIPLayer(torch.nn.Module):\n    def __init__(self, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations):\n        super().__init__()\n        self.layer_norm1 = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n        self.self_attn = CLIPAttention(embed_dim, heads, dtype, device, operations)\n        self.layer_norm2 = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n        self.mlp = CLIPMLP(embed_dim, intermediate_size, intermediate_activation, dtype, device, operations)\n\n    def forward(self, x, mask=None, optimized_attention=None):\n        x += self.self_attn(self.layer_norm1(x), mask, optimized_attention)\n        x += self.mlp(self.layer_norm2(x))\n        return x\n\n\nclass CLIPEncoder(torch.nn.Module):\n    def __init__(self, num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations):\n        super().__init__()\n        self.layers = torch.nn.ModuleList([CLIPLayer(embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations) for i in range(num_layers)])\n\n    def forward(self, x, mask=None, intermediate_output=None):\n        optimized_attention = optimized_attention_for_device(x.device, mask=mask is not None, small_input=True)\n\n        if intermediate_output is not None:\n            if intermediate_output < 0:\n                intermediate_output = len(self.layers) + intermediate_output\n\n        intermediate = None\n        for i, l in enumerate(self.layers):\n            x = l(x, mask, optimized_attention)\n            if i == intermediate_output:\n                intermediate = x.clone()\n        return x, intermediate\n\nclass CLIPEmbeddings(torch.nn.Module):\n    def __init__(self, embed_dim, vocab_size=49408, num_positions=77, dtype=None, device=None):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim, dtype=dtype, device=device)\n        self.position_embedding = torch.nn.Embedding(num_positions, embed_dim, dtype=dtype, device=device)\n\n    def forward(self, input_tokens):\n        return self.token_embedding(input_tokens) + self.position_embedding.weight\n\n\nclass CLIPTextModel_(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        num_layers = config_dict[\"num_hidden_layers\"]\n        embed_dim = config_dict[\"hidden_size\"]\n        heads = config_dict[\"num_attention_heads\"]\n        intermediate_size = config_dict[\"intermediate_size\"]\n        intermediate_activation = config_dict[\"hidden_act\"]\n\n        super().__init__()\n        self.embeddings = CLIPEmbeddings(embed_dim, dtype=torch.float32, device=device)\n        self.encoder = CLIPEncoder(num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations)\n        self.final_layer_norm = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n\n    def forward(self, input_tokens, attention_mask=None, intermediate_output=None, final_layer_norm_intermediate=True):\n        x = self.embeddings(input_tokens)\n        mask = None\n        if attention_mask is not None:\n            mask = 1.0 - attention_mask.to(x.dtype).unsqueeze(1).unsqueeze(1).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n            mask = mask.masked_fill(mask.to(torch.bool), float(\"-inf\"))\n\n        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(float(\"-inf\")).triu_(1)\n        if mask is not None:\n            mask += causal_mask\n        else:\n            mask = causal_mask\n\n        x, i = self.encoder(x, mask=mask, intermediate_output=intermediate_output)\n        x = self.final_layer_norm(x)\n        if i is not None and final_layer_norm_intermediate:\n            i = self.final_layer_norm(i)\n\n        pooled_output = x[torch.arange(x.shape[0], device=x.device), input_tokens.to(dtype=torch.int, device=x.device).argmax(dim=-1),]\n        return x, i, pooled_output\n\nclass CLIPTextModel(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        self.num_layers = config_dict[\"num_hidden_layers\"]\n        self.text_model = CLIPTextModel_(config_dict, dtype, device, operations)\n        self.dtype = dtype\n\n    def get_input_embeddings(self):\n        return self.text_model.embeddings.token_embedding\n\n    def set_input_embeddings(self, embeddings):\n        self.text_model.embeddings.token_embedding = embeddings\n\n    def forward(self, *args, **kwargs):\n        return self.text_model(*args, **kwargs)\n\nclass CLIPVisionEmbeddings(torch.nn.Module):\n    def __init__(self, embed_dim, num_channels=3, patch_size=14, image_size=224, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.class_embedding = torch.nn.Parameter(torch.empty(embed_dim, dtype=dtype, device=device))\n\n        self.patch_embedding = operations.Conv2d(\n            in_channels=num_channels,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=False,\n            dtype=dtype,\n            device=device\n        )\n\n        num_patches = (image_size // patch_size) ** 2\n        num_positions = num_patches + 1\n        self.position_embedding = torch.nn.Embedding(num_positions, embed_dim, dtype=dtype, device=device)\n\n    def forward(self, pixel_values):\n        embeds = self.patch_embedding(pixel_values).flatten(2).transpose(1, 2)\n        return torch.cat([self.class_embedding.to(embeds.device).expand(pixel_values.shape[0], 1, -1), embeds], dim=1) + self.position_embedding.weight.to(embeds.device)\n\n\nclass CLIPVision(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        num_layers = config_dict[\"num_hidden_layers\"]\n        embed_dim = config_dict[\"hidden_size\"]\n        heads = config_dict[\"num_attention_heads\"]\n        intermediate_size = config_dict[\"intermediate_size\"]\n        intermediate_activation = config_dict[\"hidden_act\"]\n\n        self.embeddings = CLIPVisionEmbeddings(embed_dim, config_dict[\"num_channels\"], config_dict[\"patch_size\"], config_dict[\"image_size\"], dtype=torch.float32, device=device, operations=operations)\n        self.pre_layrnorm = operations.LayerNorm(embed_dim)\n        self.encoder = CLIPEncoder(num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations)\n        self.post_layernorm = operations.LayerNorm(embed_dim)\n\n    def forward(self, pixel_values, attention_mask=None, intermediate_output=None):\n        x = self.embeddings(pixel_values)\n        x = self.pre_layrnorm(x)\n        #TODO: attention_mask?\n        x, i = self.encoder(x, mask=None, intermediate_output=intermediate_output)\n        pooled_output = self.post_layernorm(x[:, 0, :])\n        return x, i, pooled_output\n\nclass CLIPVisionModelProjection(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        self.vision_model = CLIPVision(config_dict, dtype, device, operations)\n        self.visual_projection = operations.Linear(config_dict[\"hidden_size\"], config_dict[\"projection_dim\"], bias=False)\n\n    def forward(self, *args, **kwargs):\n        x = self.vision_model(*args, **kwargs)\n        out = self.visual_projection(x[2])\n        return (x[0], x[1], out)\n", "ldm_patched/modules/ops.py": "import torch\nimport ldm_patched.modules.model_management\n\ndef cast_bias_weight(s, input):\n    bias = None\n    non_blocking = ldm_patched.modules.model_management.device_supports_non_blocking(input.device)\n    if s.bias is not None:\n        bias = s.bias.to(device=input.device, dtype=input.dtype, non_blocking=non_blocking)\n    weight = s.weight.to(device=input.device, dtype=input.dtype, non_blocking=non_blocking)\n    return weight, bias\n\n\nclass disable_weight_init:\n    class Linear(torch.nn.Linear):\n        ldm_patched_cast_weights = False\n        def reset_parameters(self):\n            return None\n\n        def forward_ldm_patched_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.linear(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.ldm_patched_cast_weights:\n                return self.forward_ldm_patched_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class Conv2d(torch.nn.Conv2d):\n        ldm_patched_cast_weights = False\n        def reset_parameters(self):\n            return None\n\n        def forward_ldm_patched_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return self._conv_forward(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.ldm_patched_cast_weights:\n                return self.forward_ldm_patched_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class Conv3d(torch.nn.Conv3d):\n        ldm_patched_cast_weights = False\n        def reset_parameters(self):\n            return None\n\n        def forward_ldm_patched_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return self._conv_forward(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.ldm_patched_cast_weights:\n                return self.forward_ldm_patched_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class GroupNorm(torch.nn.GroupNorm):\n        ldm_patched_cast_weights = False\n        def reset_parameters(self):\n            return None\n\n        def forward_ldm_patched_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.group_norm(input, self.num_groups, weight, bias, self.eps)\n\n        def forward(self, *args, **kwargs):\n            if self.ldm_patched_cast_weights:\n                return self.forward_ldm_patched_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n\n    class LayerNorm(torch.nn.LayerNorm):\n        ldm_patched_cast_weights = False\n        def reset_parameters(self):\n            return None\n\n        def forward_ldm_patched_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.layer_norm(input, self.normalized_shape, weight, bias, self.eps)\n\n        def forward(self, *args, **kwargs):\n            if self.ldm_patched_cast_weights:\n                return self.forward_ldm_patched_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    @classmethod\n    def conv_nd(s, dims, *args, **kwargs):\n        if dims == 2:\n            return s.Conv2d(*args, **kwargs)\n        elif dims == 3:\n            return s.Conv3d(*args, **kwargs)\n        else:\n            raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass manual_cast(disable_weight_init):\n    class Linear(disable_weight_init.Linear):\n        ldm_patched_cast_weights = True\n\n    class Conv2d(disable_weight_init.Conv2d):\n        ldm_patched_cast_weights = True\n\n    class Conv3d(disable_weight_init.Conv3d):\n        ldm_patched_cast_weights = True\n\n    class GroupNorm(disable_weight_init.GroupNorm):\n        ldm_patched_cast_weights = True\n\n    class LayerNorm(disable_weight_init.LayerNorm):\n        ldm_patched_cast_weights = True\n", "ldm_patched/modules/clip_vision.py": "from .utils import load_torch_file, transformers_convert, state_dict_prefix_replace\nimport os\nimport torch\nimport json\n\nimport ldm_patched.modules.ops\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.clip_model\n\nclass Output:\n    def __getitem__(self, key):\n        return getattr(self, key)\n    def __setitem__(self, key, item):\n        setattr(self, key, item)\n\ndef clip_preprocess(image, size=224):\n    mean = torch.tensor([ 0.48145466,0.4578275,0.40821073], device=image.device, dtype=image.dtype)\n    std = torch.tensor([0.26862954,0.26130258,0.27577711], device=image.device, dtype=image.dtype)\n    image = image.movedim(-1, 1)\n    if not (image.shape[2] == size and image.shape[3] == size):\n        scale = (size / min(image.shape[2], image.shape[3]))\n        image = torch.nn.functional.interpolate(image, size=(round(scale * image.shape[2]), round(scale * image.shape[3])), mode=\"bicubic\", antialias=True)\n        h = (image.shape[2] - size)//2\n        w = (image.shape[3] - size)//2\n        image = image[:,:,h:h+size,w:w+size]\n    image = torch.clip((255. * image), 0, 255).round() / 255.0\n    return (image - mean.view([3,1,1])) / std.view([3,1,1])\n\nclass ClipVisionModel():\n    def __init__(self, json_config):\n        with open(json_config) as f:\n            config = json.load(f)\n\n        self.load_device = ldm_patched.modules.model_management.text_encoder_device()\n        offload_device = ldm_patched.modules.model_management.text_encoder_offload_device()\n        self.dtype = ldm_patched.modules.model_management.text_encoder_dtype(self.load_device)\n        self.model = ldm_patched.modules.clip_model.CLIPVisionModelProjection(config, self.dtype, offload_device, ldm_patched.modules.ops.manual_cast)\n        self.model.eval()\n\n        self.patcher = ldm_patched.modules.model_patcher.ModelPatcher(self.model, load_device=self.load_device, offload_device=offload_device)\n\n    def load_sd(self, sd):\n        return self.model.load_state_dict(sd, strict=False)\n\n    def get_sd(self):\n        return self.model.state_dict()\n\n    def encode_image(self, image):\n        ldm_patched.modules.model_management.load_model_gpu(self.patcher)\n        pixel_values = clip_preprocess(image.to(self.load_device)).float()\n        out = self.model(pixel_values=pixel_values, intermediate_output=-2)\n\n        outputs = Output()\n        outputs[\"last_hidden_state\"] = out[0].to(ldm_patched.modules.model_management.intermediate_device())\n        outputs[\"image_embeds\"] = out[2].to(ldm_patched.modules.model_management.intermediate_device())\n        outputs[\"penultimate_hidden_states\"] = out[1].to(ldm_patched.modules.model_management.intermediate_device())\n        return outputs\n\ndef convert_to_transformers(sd, prefix):\n    sd_k = sd.keys()\n    if \"{}transformer.resblocks.0.attn.in_proj_weight\".format(prefix) in sd_k:\n        keys_to_replace = {\n            \"{}class_embedding\".format(prefix): \"vision_model.embeddings.class_embedding\",\n            \"{}conv1.weight\".format(prefix): \"vision_model.embeddings.patch_embedding.weight\",\n            \"{}positional_embedding\".format(prefix): \"vision_model.embeddings.position_embedding.weight\",\n            \"{}ln_post.bias\".format(prefix): \"vision_model.post_layernorm.bias\",\n            \"{}ln_post.weight\".format(prefix): \"vision_model.post_layernorm.weight\",\n            \"{}ln_pre.bias\".format(prefix): \"vision_model.pre_layrnorm.bias\",\n            \"{}ln_pre.weight\".format(prefix): \"vision_model.pre_layrnorm.weight\",\n        }\n\n        for x in keys_to_replace:\n            if x in sd_k:\n                sd[keys_to_replace[x]] = sd.pop(x)\n\n        if \"{}proj\".format(prefix) in sd_k:\n            sd['visual_projection.weight'] = sd.pop(\"{}proj\".format(prefix)).transpose(0, 1)\n\n        sd = transformers_convert(sd, prefix, \"vision_model.\", 48)\n    else:\n        replace_prefix = {prefix: \"\"}\n        sd = state_dict_prefix_replace(sd, replace_prefix)\n    return sd\n\ndef load_clipvision_from_sd(sd, prefix=\"\", convert_keys=False):\n    if convert_keys:\n        sd = convert_to_transformers(sd, prefix)\n    if \"vision_model.encoder.layers.47.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_g.json\")\n    elif \"vision_model.encoder.layers.30.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_h.json\")\n    elif \"vision_model.encoder.layers.22.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_vitl.json\")\n    else:\n        return None\n\n    clip = ClipVisionModel(json_config)\n    m, u = clip.load_sd(sd)\n    if len(m) > 0:\n        print(\"extra clip vision:\", m)\n    u = set(u)\n    keys = list(sd.keys())\n    for k in keys:\n        if k not in u:\n            t = sd.pop(k)\n            del t\n    return clip\n\ndef load(ckpt_path):\n    sd = load_torch_file(ckpt_path)\n    if \"visual.transformer.resblocks.0.attn.in_proj_weight\" in sd:\n        return load_clipvision_from_sd(sd, prefix=\"visual.\", convert_keys=True)\n    else:\n        return load_clipvision_from_sd(sd)\n", "ldm_patched/modules/diffusers_convert.py": "import re\nimport torch\n\n# conversion code from https://github.com/huggingface/diffusers/blob/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n\n# =================#\n# UNet Conversion #\n# =================#\n\nunet_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),\n    (\"out.2.weight\", \"conv_out.weight\"),\n    (\"out.2.bias\", \"conv_out.bias\"),\n]\n\nunet_conversion_map_resnet = [\n    # (stable-diffusion, HF Diffusers)\n    (\"in_layers.0\", \"norm1\"),\n    (\"in_layers.2\", \"conv1\"),\n    (\"out_layers.0\", \"norm2\"),\n    (\"out_layers.3\", \"conv2\"),\n    (\"emb_layers.1\", \"time_emb_proj\"),\n    (\"skip_connection\", \"conv_shortcut\"),\n]\n\nunet_conversion_map_layer = []\n# hardcoded number of downblocks and resnets/attentions...\n# would need smarter logic for other networks.\nfor i in range(4):\n    # loop over downblocks/upblocks\n\n    for j in range(2):\n        # loop over resnets/attentions for downblocks\n        hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n        sd_down_res_prefix = f\"input_blocks.{3 * i + j + 1}.0.\"\n        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))\n\n        if i < 3:\n            # no attention layers in down_blocks.3\n            hf_down_atn_prefix = f\"down_blocks.{i}.attentions.{j}.\"\n            sd_down_atn_prefix = f\"input_blocks.{3 * i + j + 1}.1.\"\n            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))\n\n    for j in range(3):\n        # loop over resnets/attentions for upblocks\n        hf_up_res_prefix = f\"up_blocks.{i}.resnets.{j}.\"\n        sd_up_res_prefix = f\"output_blocks.{3 * i + j}.0.\"\n        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))\n\n        if i > 0:\n            # no attention layers in up_blocks.0\n            hf_up_atn_prefix = f\"up_blocks.{i}.attentions.{j}.\"\n            sd_up_atn_prefix = f\"output_blocks.{3 * i + j}.1.\"\n            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))\n\n    if i < 3:\n        # no downsample in down_blocks.3\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.conv.\"\n        sd_downsample_prefix = f\"input_blocks.{3 * (i + 1)}.0.op.\"\n        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        # no upsample in up_blocks.3\n        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n        sd_upsample_prefix = f\"output_blocks.{3 * i + 2}.{1 if i == 0 else 2}.\"\n        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))\n\nhf_mid_atn_prefix = \"mid_block.attentions.0.\"\nsd_mid_atn_prefix = \"middle_block.1.\"\nunet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\n\nfor j in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n    sd_mid_res_prefix = f\"middle_block.{2 * j}.\"\n    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\n\ndef convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    for k, v in mapping.items():\n        for sd_part, hf_part in unet_conversion_map_layer:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}\n    return new_state_dict\n\n\n# ================#\n# VAE Conversion #\n# ================#\n\nvae_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"nin_shortcut\", \"conv_shortcut\"),\n    (\"norm_out\", \"conv_norm_out\"),\n    (\"mid.attn_1.\", \"mid_block.attentions.0.\"),\n]\n\nfor i in range(4):\n    # down_blocks have two resnets\n    for j in range(2):\n        hf_down_prefix = f\"encoder.down_blocks.{i}.resnets.{j}.\"\n        sd_down_prefix = f\"encoder.down.{i}.block.{j}.\"\n        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))\n\n    if i < 3:\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.\"\n        sd_downsample_prefix = f\"down.{i}.downsample.\"\n        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n        sd_upsample_prefix = f\"up.{3 - i}.upsample.\"\n        vae_conversion_map.append((sd_upsample_prefix, hf_upsample_prefix))\n\n    # up_blocks have three resnets\n    # also, up blocks in hf are numbered in reverse from sd\n    for j in range(3):\n        hf_up_prefix = f\"decoder.up_blocks.{i}.resnets.{j}.\"\n        sd_up_prefix = f\"decoder.up.{3 - i}.block.{j}.\"\n        vae_conversion_map.append((sd_up_prefix, hf_up_prefix))\n\n# this part accounts for mid blocks in both the encoder and the decoder\nfor i in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{i}.\"\n    sd_mid_res_prefix = f\"mid.block_{i + 1}.\"\n    vae_conversion_map.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\nvae_conversion_map_attn = [\n    # (stable-diffusion, HF Diffusers)\n    (\"norm.\", \"group_norm.\"),\n    (\"q.\", \"query.\"),\n    (\"k.\", \"key.\"),\n    (\"v.\", \"value.\"),\n    (\"q.\", \"to_q.\"),\n    (\"k.\", \"to_k.\"),\n    (\"v.\", \"to_v.\"),\n    (\"proj_out.\", \"to_out.0.\"),\n    (\"proj_out.\", \"proj_attn.\"),\n]\n\n\ndef reshape_weight_for_sd(w):\n    # convert HF linear weights to SD conv2d weights\n    return w.reshape(*w.shape, 1, 1)\n\n\ndef convert_vae_state_dict(vae_state_dict):\n    mapping = {k: k for k in vae_state_dict.keys()}\n    for k, v in mapping.items():\n        for sd_part, hf_part in vae_conversion_map:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    for k, v in mapping.items():\n        if \"attentions\" in k:\n            for sd_part, hf_part in vae_conversion_map_attn:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}\n    weights_to_convert = [\"q\", \"k\", \"v\", \"proj_out\"]\n    for k, v in new_state_dict.items():\n        for weight_name in weights_to_convert:\n            if f\"mid.attn_1.{weight_name}.weight\" in k:\n                print(f\"Reshaping {k} for SD format\")\n                new_state_dict[k] = reshape_weight_for_sd(v)\n    return new_state_dict\n\n\n# =========================#\n# Text Encoder Conversion #\n# =========================#\n\n\ntextenc_conversion_lst = [\n    # (stable-diffusion, HF Diffusers)\n    (\"resblocks.\", \"text_model.encoder.layers.\"),\n    (\"ln_1\", \"layer_norm1\"),\n    (\"ln_2\", \"layer_norm2\"),\n    (\".c_fc.\", \".fc1.\"),\n    (\".c_proj.\", \".fc2.\"),\n    (\".attn\", \".self_attn\"),\n    (\"ln_final.\", \"transformer.text_model.final_layer_norm.\"),\n    (\"token_embedding.weight\", \"transformer.text_model.embeddings.token_embedding.weight\"),\n    (\"positional_embedding\", \"transformer.text_model.embeddings.position_embedding.weight\"),\n]\nprotected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}\ntextenc_pattern = re.compile(\"|\".join(protected.keys()))\n\n# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp\ncode2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\n\n\ndef convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):\n            continue\n        if (\n                k.endswith(\".self_attn.q_proj.weight\")\n                or k.endswith(\".self_attn.k_proj.weight\")\n                or k.endswith(\".self_attn.v_proj.weight\")\n        ):\n            k_pre = k[: -len(\".q_proj.weight\")]\n            k_code = k[-len(\"q_proj.weight\")]\n            if k_pre not in capture_qkv_weight:\n                capture_qkv_weight[k_pre] = [None, None, None]\n            capture_qkv_weight[k_pre][code2idx[k_code]] = v\n            continue\n\n        if (\n                k.endswith(\".self_attn.q_proj.bias\")\n                or k.endswith(\".self_attn.k_proj.bias\")\n                or k.endswith(\".self_attn.v_proj.bias\")\n        ):\n            k_pre = k[: -len(\".q_proj.bias\")]\n            k_code = k[-len(\"q_proj.bias\")]\n            if k_pre not in capture_qkv_bias:\n                capture_qkv_bias[k_pre] = [None, None, None]\n            capture_qkv_bias[k_pre][code2idx[k_code]] = v\n            continue\n\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k)\n        new_state_dict[relabelled_key] = v\n\n    for k_pre, tensors in capture_qkv_weight.items():\n        if None in tensors:\n            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n        new_state_dict[relabelled_key + \".in_proj_weight\"] = torch.cat(tensors)\n\n    for k_pre, tensors in capture_qkv_bias.items():\n        if None in tensors:\n            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n        new_state_dict[relabelled_key + \".in_proj_bias\"] = torch.cat(tensors)\n\n    return new_state_dict\n\n\ndef convert_text_enc_state_dict(text_enc_dict):\n    return text_enc_dict\n\n\n", "ldm_patched/modules/supported_models_base.py": "import torch\nfrom . import model_base\nfrom . import utils\nfrom . import latent_formats\n\nclass ClipTarget:\n    def __init__(self, tokenizer, clip):\n        self.clip = clip\n        self.tokenizer = tokenizer\n        self.params = {}\n\nclass BASE:\n    unet_config = {}\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,\n    }\n\n    clip_prefix = []\n    clip_vision_prefix = None\n    noise_aug_config = None\n    sampling_settings = {}\n    latent_format = latent_formats.LatentFormat\n\n    manual_cast_dtype = None\n\n    @classmethod\n    def matches(s, unet_config):\n        for k in s.unet_config:\n            if s.unet_config[k] != unet_config[k]:\n                return False\n        return True\n\n    def model_type(self, state_dict, prefix=\"\"):\n        return model_base.ModelType.EPS\n\n    def inpaint_model(self):\n        return self.unet_config[\"in_channels\"] > 4\n\n    def __init__(self, unet_config):\n        self.unet_config = unet_config\n        self.latent_format = self.latent_format()\n        for x in self.unet_extra_config:\n            self.unet_config[x] = self.unet_extra_config[x]\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        if self.noise_aug_config is not None:\n            out = model_base.SD21UNCLIP(self, self.noise_aug_config, model_type=self.model_type(state_dict, prefix), device=device)\n        else:\n            out = model_base.BaseModel(self, model_type=self.model_type(state_dict, prefix), device=device)\n        if self.inpaint_model():\n            out.set_inpaint()\n        return out\n\n    def process_clip_state_dict(self, state_dict):\n        return state_dict\n\n    def process_unet_state_dict(self, state_dict):\n        return state_dict\n\n    def process_vae_state_dict(self, state_dict):\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": \"cond_stage_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_clip_vision_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        if self.clip_vision_prefix is not None:\n            replace_prefix[\"\"] = self.clip_vision_prefix\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_unet_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": \"model.diffusion_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_vae_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": \"first_stage_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def set_manual_cast(self, manual_cast_dtype):\n        self.manual_cast_dtype = manual_cast_dtype\n", "ldm_patched/modules/controlnet.py": "import torch\nimport math\nimport os\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.model_detection\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.ops\n\nimport ldm_patched.controlnet.cldm\nimport ldm_patched.t2ia.adapter\n\n\ndef broadcast_image_to(tensor, target_batch_size, batched_number):\n    current_batch_size = tensor.shape[0]\n    #print(current_batch_size, target_batch_size)\n    if current_batch_size == 1:\n        return tensor\n\n    per_batch = target_batch_size // batched_number\n    tensor = tensor[:per_batch]\n\n    if per_batch > tensor.shape[0]:\n        tensor = torch.cat([tensor] * (per_batch // tensor.shape[0]) + [tensor[:(per_batch % tensor.shape[0])]], dim=0)\n\n    current_batch_size = tensor.shape[0]\n    if current_batch_size == target_batch_size:\n        return tensor\n    else:\n        return torch.cat([tensor] * batched_number, dim=0)\n\nclass ControlBase:\n    def __init__(self, device=None):\n        self.cond_hint_original = None\n        self.cond_hint = None\n        self.strength = 1.0\n        self.timestep_percent_range = (0.0, 1.0)\n        self.global_average_pooling = False\n        self.timestep_range = None\n\n        if device is None:\n            device = ldm_patched.modules.model_management.get_torch_device()\n        self.device = device\n        self.previous_controlnet = None\n\n    def set_cond_hint(self, cond_hint, strength=1.0, timestep_percent_range=(0.0, 1.0)):\n        self.cond_hint_original = cond_hint\n        self.strength = strength\n        self.timestep_percent_range = timestep_percent_range\n        return self\n\n    def pre_run(self, model, percent_to_timestep_function):\n        self.timestep_range = (percent_to_timestep_function(self.timestep_percent_range[0]), percent_to_timestep_function(self.timestep_percent_range[1]))\n        if self.previous_controlnet is not None:\n            self.previous_controlnet.pre_run(model, percent_to_timestep_function)\n\n    def set_previous_controlnet(self, controlnet):\n        self.previous_controlnet = controlnet\n        return self\n\n    def cleanup(self):\n        if self.previous_controlnet is not None:\n            self.previous_controlnet.cleanup()\n        if self.cond_hint is not None:\n            del self.cond_hint\n            self.cond_hint = None\n        self.timestep_range = None\n\n    def get_models(self):\n        out = []\n        if self.previous_controlnet is not None:\n            out += self.previous_controlnet.get_models()\n        return out\n\n    def copy_to(self, c):\n        c.cond_hint_original = self.cond_hint_original\n        c.strength = self.strength\n        c.timestep_percent_range = self.timestep_percent_range\n        c.global_average_pooling = self.global_average_pooling\n\n    def inference_memory_requirements(self, dtype):\n        if self.previous_controlnet is not None:\n            return self.previous_controlnet.inference_memory_requirements(dtype)\n        return 0\n\n    def control_merge(self, control_input, control_output, control_prev, output_dtype):\n        out = {'input':[], 'middle':[], 'output': []}\n\n        if control_input is not None:\n            for i in range(len(control_input)):\n                key = 'input'\n                x = control_input[i]\n                if x is not None:\n                    x *= self.strength\n                    if x.dtype != output_dtype:\n                        x = x.to(output_dtype)\n                out[key].insert(0, x)\n\n        if control_output is not None:\n            for i in range(len(control_output)):\n                if i == (len(control_output) - 1):\n                    key = 'middle'\n                    index = 0\n                else:\n                    key = 'output'\n                    index = i\n                x = control_output[i]\n                if x is not None:\n                    if self.global_average_pooling:\n                        x = torch.mean(x, dim=(2, 3), keepdim=True).repeat(1, 1, x.shape[2], x.shape[3])\n\n                    x *= self.strength\n                    if x.dtype != output_dtype:\n                        x = x.to(output_dtype)\n\n                out[key].append(x)\n        if control_prev is not None:\n            for x in ['input', 'middle', 'output']:\n                o = out[x]\n                for i in range(len(control_prev[x])):\n                    prev_val = control_prev[x][i]\n                    if i >= len(o):\n                        o.append(prev_val)\n                    elif prev_val is not None:\n                        if o[i] is None:\n                            o[i] = prev_val\n                        else:\n                            if o[i].shape[0] < prev_val.shape[0]:\n                                o[i] = prev_val + o[i]\n                            else:\n                                o[i] += prev_val\n        return out\n\nclass ControlNet(ControlBase):\n    def __init__(self, control_model, global_average_pooling=False, device=None, load_device=None, manual_cast_dtype=None):\n        super().__init__(device)\n        self.control_model = control_model\n        self.load_device = load_device\n        self.control_model_wrapped = ldm_patched.modules.model_patcher.ModelPatcher(self.control_model, load_device=load_device, offload_device=ldm_patched.modules.model_management.unet_offload_device())\n        self.global_average_pooling = global_average_pooling\n        self.model_sampling_current = None\n        self.manual_cast_dtype = manual_cast_dtype\n\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:\n            control_prev = self.previous_controlnet.get_control(x_noisy, t, cond, batched_number)\n\n        if self.timestep_range is not None:\n            if t[0] > self.timestep_range[0] or t[0] < self.timestep_range[1]:\n                if control_prev is not None:\n                    return control_prev\n                else:\n                    return None\n\n        dtype = self.control_model.dtype\n        if self.manual_cast_dtype is not None:\n            dtype = self.manual_cast_dtype\n\n        output_dtype = x_noisy.dtype\n        if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n            if self.cond_hint is not None:\n                del self.cond_hint\n            self.cond_hint = None\n            self.cond_hint = ldm_patched.modules.utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * 8, x_noisy.shape[2] * 8, 'nearest-exact', \"center\").to(dtype).to(self.device)\n        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)\n\n        context = cond['c_crossattn']\n        y = cond.get('y', None)\n        if y is not None:\n            y = y.to(dtype)\n        timestep = self.model_sampling_current.timestep(t)\n        x_noisy = self.model_sampling_current.calculate_input(t, x_noisy)\n\n        control = self.control_model(x=x_noisy.to(dtype), hint=self.cond_hint, timesteps=timestep.float(), context=context.to(dtype), y=y)\n        return self.control_merge(None, control, control_prev, output_dtype)\n\n    def copy(self):\n        c = ControlNet(self.control_model, global_average_pooling=self.global_average_pooling, load_device=self.load_device, manual_cast_dtype=self.manual_cast_dtype)\n        self.copy_to(c)\n        return c\n\n    def get_models(self):\n        out = super().get_models()\n        out.append(self.control_model_wrapped)\n        return out\n\n    def pre_run(self, model, percent_to_timestep_function):\n        super().pre_run(model, percent_to_timestep_function)\n        self.model_sampling_current = model.model_sampling\n\n    def cleanup(self):\n        self.model_sampling_current = None\n        super().cleanup()\n\nclass ControlLoraOps:\n    class Linear(torch.nn.Module):\n        def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                    device=None, dtype=None) -> None:\n            factory_kwargs = {'device': device, 'dtype': dtype}\n            super().__init__()\n            self.in_features = in_features\n            self.out_features = out_features\n            self.weight = None\n            self.up = None\n            self.down = None\n            self.bias = None\n\n        def forward(self, input):\n            weight, bias = ldm_patched.modules.ops.cast_bias_weight(self, input)\n            if self.up is not None:\n                return torch.nn.functional.linear(input, weight + (torch.mm(self.up.flatten(start_dim=1), self.down.flatten(start_dim=1))).reshape(self.weight.shape).type(input.dtype), bias)\n            else:\n                return torch.nn.functional.linear(input, weight, bias)\n\n    class Conv2d(torch.nn.Module):\n        def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n            padding_mode='zeros',\n            device=None,\n            dtype=None\n        ):\n            super().__init__()\n            self.in_channels = in_channels\n            self.out_channels = out_channels\n            self.kernel_size = kernel_size\n            self.stride = stride\n            self.padding = padding\n            self.dilation = dilation\n            self.transposed = False\n            self.output_padding = 0\n            self.groups = groups\n            self.padding_mode = padding_mode\n\n            self.weight = None\n            self.bias = None\n            self.up = None\n            self.down = None\n\n\n        def forward(self, input):\n            weight, bias = ldm_patched.modules.ops.cast_bias_weight(self, input)\n            if self.up is not None:\n                return torch.nn.functional.conv2d(input, weight + (torch.mm(self.up.flatten(start_dim=1), self.down.flatten(start_dim=1))).reshape(self.weight.shape).type(input.dtype), bias, self.stride, self.padding, self.dilation, self.groups)\n            else:\n                return torch.nn.functional.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass ControlLora(ControlNet):\n    def __init__(self, control_weights, global_average_pooling=False, device=None):\n        ControlBase.__init__(self, device)\n        self.control_weights = control_weights\n        self.global_average_pooling = global_average_pooling\n\n    def pre_run(self, model, percent_to_timestep_function):\n        super().pre_run(model, percent_to_timestep_function)\n        controlnet_config = model.model_config.unet_config.copy()\n        controlnet_config.pop(\"out_channels\")\n        controlnet_config[\"hint_channels\"] = self.control_weights[\"input_hint_block.0.weight\"].shape[1]\n        self.manual_cast_dtype = model.manual_cast_dtype\n        dtype = model.get_dtype()\n        if self.manual_cast_dtype is None:\n            class control_lora_ops(ControlLoraOps, ldm_patched.modules.ops.disable_weight_init):\n                pass\n        else:\n            class control_lora_ops(ControlLoraOps, ldm_patched.modules.ops.manual_cast):\n                pass\n            dtype = self.manual_cast_dtype\n\n        controlnet_config[\"operations\"] = control_lora_ops\n        controlnet_config[\"dtype\"] = dtype\n        self.control_model = ldm_patched.controlnet.cldm.ControlNet(**controlnet_config)\n        self.control_model.to(ldm_patched.modules.model_management.get_torch_device())\n        diffusion_model = model.diffusion_model\n        sd = diffusion_model.state_dict()\n        cm = self.control_model.state_dict()\n\n        for k in sd:\n            weight = sd[k]\n            try:\n                ldm_patched.modules.utils.set_attr(self.control_model, k, weight)\n            except:\n                pass\n\n        for k in self.control_weights:\n            if k not in {\"lora_controlnet\"}:\n                ldm_patched.modules.utils.set_attr(self.control_model, k, self.control_weights[k].to(dtype).to(ldm_patched.modules.model_management.get_torch_device()))\n\n    def copy(self):\n        c = ControlLora(self.control_weights, global_average_pooling=self.global_average_pooling)\n        self.copy_to(c)\n        return c\n\n    def cleanup(self):\n        del self.control_model\n        self.control_model = None\n        super().cleanup()\n\n    def get_models(self):\n        out = ControlBase.get_models(self)\n        return out\n\n    def inference_memory_requirements(self, dtype):\n        return ldm_patched.modules.utils.calculate_parameters(self.control_weights) * ldm_patched.modules.model_management.dtype_size(dtype) + ControlBase.inference_memory_requirements(self, dtype)\n\ndef load_controlnet(ckpt_path, model=None):\n    controlnet_data = ldm_patched.modules.utils.load_torch_file(ckpt_path, safe_load=True)\n    if \"lora_controlnet\" in controlnet_data:\n        return ControlLora(controlnet_data)\n\n    controlnet_config = None\n    if \"controlnet_cond_embedding.conv_in.weight\" in controlnet_data: #diffusers format\n        unet_dtype = ldm_patched.modules.model_management.unet_dtype()\n        controlnet_config = ldm_patched.modules.model_detection.unet_config_from_diffusers_unet(controlnet_data, unet_dtype)\n        diffusers_keys = ldm_patched.modules.utils.unet_to_diffusers(controlnet_config)\n        diffusers_keys[\"controlnet_mid_block.weight\"] = \"middle_block_out.0.weight\"\n        diffusers_keys[\"controlnet_mid_block.bias\"] = \"middle_block_out.0.bias\"\n\n        count = 0\n        loop = True\n        while loop:\n            suffix = [\".weight\", \".bias\"]\n            for s in suffix:\n                k_in = \"controlnet_down_blocks.{}{}\".format(count, s)\n                k_out = \"zero_convs.{}.0{}\".format(count, s)\n                if k_in not in controlnet_data:\n                    loop = False\n                    break\n                diffusers_keys[k_in] = k_out\n            count += 1\n\n        count = 0\n        loop = True\n        while loop:\n            suffix = [\".weight\", \".bias\"]\n            for s in suffix:\n                if count == 0:\n                    k_in = \"controlnet_cond_embedding.conv_in{}\".format(s)\n                else:\n                    k_in = \"controlnet_cond_embedding.blocks.{}{}\".format(count - 1, s)\n                k_out = \"input_hint_block.{}{}\".format(count * 2, s)\n                if k_in not in controlnet_data:\n                    k_in = \"controlnet_cond_embedding.conv_out{}\".format(s)\n                    loop = False\n                diffusers_keys[k_in] = k_out\n            count += 1\n\n        new_sd = {}\n        for k in diffusers_keys:\n            if k in controlnet_data:\n                new_sd[diffusers_keys[k]] = controlnet_data.pop(k)\n\n        leftover_keys = controlnet_data.keys()\n        if len(leftover_keys) > 0:\n            print(\"leftover keys:\", leftover_keys)\n        controlnet_data = new_sd\n\n    pth_key = 'control_model.zero_convs.0.0.weight'\n    pth = False\n    key = 'zero_convs.0.0.weight'\n    if pth_key in controlnet_data:\n        pth = True\n        key = pth_key\n        prefix = \"control_model.\"\n    elif key in controlnet_data:\n        prefix = \"\"\n    else:\n        net = load_t2i_adapter(controlnet_data)\n        if net is None:\n            print(\"error checkpoint does not contain controlnet or t2i adapter data\", ckpt_path)\n        return net\n\n    if controlnet_config is None:\n        unet_dtype = ldm_patched.modules.model_management.unet_dtype()\n        controlnet_config = ldm_patched.modules.model_detection.model_config_from_unet(controlnet_data, prefix, unet_dtype, True).unet_config\n    load_device = ldm_patched.modules.model_management.get_torch_device()\n    manual_cast_dtype = ldm_patched.modules.model_management.unet_manual_cast(unet_dtype, load_device)\n    if manual_cast_dtype is not None:\n        controlnet_config[\"operations\"] = ldm_patched.modules.ops.manual_cast\n    controlnet_config.pop(\"out_channels\")\n    controlnet_config[\"hint_channels\"] = controlnet_data[\"{}input_hint_block.0.weight\".format(prefix)].shape[1]\n    control_model = ldm_patched.controlnet.cldm.ControlNet(**controlnet_config)\n\n    if pth:\n        if 'difference' in controlnet_data:\n            if model is not None:\n                ldm_patched.modules.model_management.load_models_gpu([model])\n                model_sd = model.model_state_dict()\n                for x in controlnet_data:\n                    c_m = \"control_model.\"\n                    if x.startswith(c_m):\n                        sd_key = \"diffusion_model.{}\".format(x[len(c_m):])\n                        if sd_key in model_sd:\n                            cd = controlnet_data[x]\n                            cd += model_sd[sd_key].type(cd.dtype).to(cd.device)\n            else:\n                print(\"WARNING: Loaded a diff controlnet without a model. It will very likely not work.\")\n\n        class WeightsLoader(torch.nn.Module):\n            pass\n        w = WeightsLoader()\n        w.control_model = control_model\n        missing, unexpected = w.load_state_dict(controlnet_data, strict=False)\n    else:\n        missing, unexpected = control_model.load_state_dict(controlnet_data, strict=False)\n    print(missing, unexpected)\n\n    global_average_pooling = False\n    filename = os.path.splitext(ckpt_path)[0]\n    if filename.endswith(\"_shuffle\") or filename.endswith(\"_shuffle_fp16\"): #TODO: smarter way of enabling global_average_pooling\n        global_average_pooling = True\n\n    control = ControlNet(control_model, global_average_pooling=global_average_pooling, load_device=load_device, manual_cast_dtype=manual_cast_dtype)\n    return control\n\nclass T2IAdapter(ControlBase):\n    def __init__(self, t2i_model, channels_in, device=None):\n        super().__init__(device)\n        self.t2i_model = t2i_model\n        self.channels_in = channels_in\n        self.control_input = None\n\n    def scale_image_to(self, width, height):\n        unshuffle_amount = self.t2i_model.unshuffle_amount\n        width = math.ceil(width / unshuffle_amount) * unshuffle_amount\n        height = math.ceil(height / unshuffle_amount) * unshuffle_amount\n        return width, height\n\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:\n            control_prev = self.previous_controlnet.get_control(x_noisy, t, cond, batched_number)\n\n        if self.timestep_range is not None:\n            if t[0] > self.timestep_range[0] or t[0] < self.timestep_range[1]:\n                if control_prev is not None:\n                    return control_prev\n                else:\n                    return None\n\n        if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n            if self.cond_hint is not None:\n                del self.cond_hint\n            self.control_input = None\n            self.cond_hint = None\n            width, height = self.scale_image_to(x_noisy.shape[3] * 8, x_noisy.shape[2] * 8)\n            self.cond_hint = ldm_patched.modules.utils.common_upscale(self.cond_hint_original, width, height, 'nearest-exact', \"center\").float().to(self.device)\n            if self.channels_in == 1 and self.cond_hint.shape[1] > 1:\n                self.cond_hint = torch.mean(self.cond_hint, 1, keepdim=True)\n        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)\n        if self.control_input is None:\n            self.t2i_model.to(x_noisy.dtype)\n            self.t2i_model.to(self.device)\n            self.control_input = self.t2i_model(self.cond_hint.to(x_noisy.dtype))\n            self.t2i_model.cpu()\n\n        control_input = list(map(lambda a: None if a is None else a.clone(), self.control_input))\n        mid = None\n        if self.t2i_model.xl == True:\n            mid = control_input[-1:]\n            control_input = control_input[:-1]\n        return self.control_merge(control_input, mid, control_prev, x_noisy.dtype)\n\n    def copy(self):\n        c = T2IAdapter(self.t2i_model, self.channels_in)\n        self.copy_to(c)\n        return c\n\ndef load_t2i_adapter(t2i_data):\n    if 'adapter' in t2i_data:\n        t2i_data = t2i_data['adapter']\n    if 'adapter.body.0.resnets.0.block1.weight' in t2i_data: #diffusers format\n        prefix_replace = {}\n        for i in range(4):\n            for j in range(2):\n                prefix_replace[\"adapter.body.{}.resnets.{}.\".format(i, j)] = \"body.{}.\".format(i * 2 + j)\n            prefix_replace[\"adapter.body.{}.\".format(i, j)] = \"body.{}.\".format(i * 2)\n        prefix_replace[\"adapter.\"] = \"\"\n        t2i_data = ldm_patched.modules.utils.state_dict_prefix_replace(t2i_data, prefix_replace)\n    keys = t2i_data.keys()\n\n    if \"body.0.in_conv.weight\" in keys:\n        cin = t2i_data['body.0.in_conv.weight'].shape[1]\n        model_ad = ldm_patched.t2ia.adapter.Adapter_light(cin=cin, channels=[320, 640, 1280, 1280], nums_rb=4)\n    elif 'conv_in.weight' in keys:\n        cin = t2i_data['conv_in.weight'].shape[1]\n        channel = t2i_data['conv_in.weight'].shape[0]\n        ksize = t2i_data['body.0.block2.weight'].shape[2]\n        use_conv = False\n        down_opts = list(filter(lambda a: a.endswith(\"down_opt.op.weight\"), keys))\n        if len(down_opts) > 0:\n            use_conv = True\n        xl = False\n        if cin == 256 or cin == 768:\n            xl = True\n        model_ad = ldm_patched.t2ia.adapter.Adapter(cin=cin, channels=[channel, channel*2, channel*4, channel*4][:4], nums_rb=2, ksize=ksize, sk=True, use_conv=use_conv, xl=xl)\n    else:\n        return None\n    missing, unexpected = model_ad.load_state_dict(t2i_data)\n    if len(missing) > 0:\n        print(\"t2i missing\", missing)\n\n    if len(unexpected) > 0:\n        print(\"t2i unexpected\", unexpected)\n\n    return T2IAdapter(model_ad, model_ad.input_channels)\n", "ldm_patched/modules/model_base.py": "import torch\nfrom ldm_patched.ldm.modules.diffusionmodules.openaimodel import UNetModel, Timestep\nfrom ldm_patched.ldm.modules.encoders.noise_aug_modules import CLIPEmbeddingNoiseAugmentation\nfrom ldm_patched.ldm.modules.diffusionmodules.upscaling import ImageConcatWithNoiseAugmentation\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.conds\nimport ldm_patched.modules.ops\nfrom enum import Enum\nfrom . import utils\n\nclass ModelType(Enum):\n    EPS = 1\n    V_PREDICTION = 2\n    V_PREDICTION_EDM = 3\n\n\nfrom ldm_patched.modules.model_sampling import EPS, V_PREDICTION, ModelSamplingDiscrete, ModelSamplingContinuousEDM\n\n\ndef model_sampling(model_config, model_type):\n    s = ModelSamplingDiscrete\n\n    if model_type == ModelType.EPS:\n        c = EPS\n    elif model_type == ModelType.V_PREDICTION:\n        c = V_PREDICTION\n    elif model_type == ModelType.V_PREDICTION_EDM:\n        c = V_PREDICTION\n        s = ModelSamplingContinuousEDM\n\n    class ModelSampling(s, c):\n        pass\n\n    return ModelSampling(model_config)\n\n\nclass BaseModel(torch.nn.Module):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__()\n\n        unet_config = model_config.unet_config\n        self.latent_format = model_config.latent_format\n        self.model_config = model_config\n        self.manual_cast_dtype = model_config.manual_cast_dtype\n\n        if not unet_config.get(\"disable_unet_model_creation\", False):\n            if self.manual_cast_dtype is not None:\n                operations = ldm_patched.modules.ops.manual_cast\n            else:\n                operations = ldm_patched.modules.ops.disable_weight_init\n            self.diffusion_model = UNetModel(**unet_config, device=device, operations=operations)\n        self.model_type = model_type\n        self.model_sampling = model_sampling(model_config, model_type)\n\n        self.adm_channels = unet_config.get(\"adm_in_channels\", None)\n        if self.adm_channels is None:\n            self.adm_channels = 0\n        self.inpaint_model = False\n        print(\"model_type\", model_type.name)\n        print(\"UNet ADM Dimension\", self.adm_channels)\n\n    def apply_model(self, x, t, c_concat=None, c_crossattn=None, control=None, transformer_options={}, **kwargs):\n        sigma = t\n        xc = self.model_sampling.calculate_input(sigma, x)\n        if c_concat is not None:\n            xc = torch.cat([xc] + [c_concat], dim=1)\n\n        context = c_crossattn\n        dtype = self.get_dtype()\n\n        if self.manual_cast_dtype is not None:\n            dtype = self.manual_cast_dtype\n\n        xc = xc.to(dtype)\n        t = self.model_sampling.timestep(t).float()\n        context = context.to(dtype)\n        extra_conds = {}\n        for o in kwargs:\n            extra = kwargs[o]\n            if hasattr(extra, \"dtype\"):\n                if extra.dtype != torch.int and extra.dtype != torch.long:\n                    extra = extra.to(dtype)\n            extra_conds[o] = extra\n\n        model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\n        return self.model_sampling.calculate_denoised(sigma, model_output, x)\n\n    def get_dtype(self):\n        return self.diffusion_model.dtype\n\n    def is_adm(self):\n        return self.adm_channels > 0\n\n    def encode_adm(self, **kwargs):\n        return None\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        if self.inpaint_model:\n            concat_keys = (\"mask\", \"masked_image\")\n            cond_concat = []\n            denoise_mask = kwargs.get(\"concat_mask\", kwargs.get(\"denoise_mask\", None))\n            concat_latent_image = kwargs.get(\"concat_latent_image\", None)\n            if concat_latent_image is None:\n                concat_latent_image = kwargs.get(\"latent_image\", None)\n            else:\n                concat_latent_image = self.process_latent_in(concat_latent_image)\n\n            noise = kwargs.get(\"noise\", None)\n            device = kwargs[\"device\"]\n\n            if concat_latent_image.shape[1:] != noise.shape[1:]:\n                concat_latent_image = utils.common_upscale(concat_latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n            concat_latent_image = utils.resize_to_batch_size(concat_latent_image, noise.shape[0])\n\n            if len(denoise_mask.shape) == len(noise.shape):\n                denoise_mask = denoise_mask[:,:1]\n\n            denoise_mask = denoise_mask.reshape((-1, 1, denoise_mask.shape[-2], denoise_mask.shape[-1]))\n            if denoise_mask.shape[-2:] != noise.shape[-2:]:\n                denoise_mask = utils.common_upscale(denoise_mask, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n            denoise_mask = utils.resize_to_batch_size(denoise_mask.round(), noise.shape[0])\n\n            def blank_inpaint_image_like(latent_image):\n                blank_image = torch.ones_like(latent_image)\n                # these are the values for \"zero\" in pixel space translated to latent space\n                blank_image[:,0] *= 0.8223\n                blank_image[:,1] *= -0.6876\n                blank_image[:,2] *= 0.6364\n                blank_image[:,3] *= 0.1380\n                return blank_image\n\n            for ck in concat_keys:\n                if denoise_mask is not None:\n                    if ck == \"mask\":\n                        cond_concat.append(denoise_mask.to(device))\n                    elif ck == \"masked_image\":\n                        cond_concat.append(concat_latent_image.to(device)) #NOTE: the latent_image should be masked by the mask in pixel space\n                else:\n                    if ck == \"mask\":\n                        cond_concat.append(torch.ones_like(noise)[:,:1])\n                    elif ck == \"masked_image\":\n                        cond_concat.append(blank_inpaint_image_like(noise))\n            data = torch.cat(cond_concat, dim=1)\n            out['c_concat'] = ldm_patched.modules.conds.CONDNoiseShape(data)\n\n        adm = self.encode_adm(**kwargs)\n        if adm is not None:\n            out['y'] = ldm_patched.modules.conds.CONDRegular(adm)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['c_crossattn'] = ldm_patched.modules.conds.CONDCrossAttn(cross_attn)\n\n        return out\n\n    def load_model_weights(self, sd, unet_prefix=\"\"):\n        to_load = {}\n        keys = list(sd.keys())\n        for k in keys:\n            if k.startswith(unet_prefix):\n                to_load[k[len(unet_prefix):]] = sd.pop(k)\n\n        to_load = self.model_config.process_unet_state_dict(to_load)\n        m, u = self.diffusion_model.load_state_dict(to_load, strict=False)\n        if len(m) > 0:\n            print(\"unet missing:\", m)\n\n        if len(u) > 0:\n            print(\"unet unexpected:\", u)\n        del to_load\n        return self\n\n    def process_latent_in(self, latent):\n        return self.latent_format.process_in(latent)\n\n    def process_latent_out(self, latent):\n        return self.latent_format.process_out(latent)\n\n    def state_dict_for_saving(self, clip_state_dict=None, vae_state_dict=None, clip_vision_state_dict=None):\n        extra_sds = []\n        if clip_state_dict is not None:\n            extra_sds.append(self.model_config.process_clip_state_dict_for_saving(clip_state_dict))\n        if vae_state_dict is not None:\n            extra_sds.append(self.model_config.process_vae_state_dict_for_saving(vae_state_dict))\n        if clip_vision_state_dict is not None:\n            extra_sds.append(self.model_config.process_clip_vision_state_dict_for_saving(clip_vision_state_dict))\n\n        unet_state_dict = self.diffusion_model.state_dict()\n        unet_state_dict = self.model_config.process_unet_state_dict_for_saving(unet_state_dict)\n\n        if self.get_dtype() == torch.float16:\n            extra_sds = map(lambda sd: utils.convert_sd_to(sd, torch.float16), extra_sds)\n\n        if self.model_type == ModelType.V_PREDICTION:\n            unet_state_dict[\"v_pred\"] = torch.tensor([])\n\n        for sd in extra_sds:\n            unet_state_dict.update(sd)\n\n        return unet_state_dict\n\n    def set_inpaint(self):\n        self.inpaint_model = True\n\n    def memory_required(self, input_shape):\n        if ldm_patched.modules.model_management.xformers_enabled() or ldm_patched.modules.model_management.pytorch_attention_flash_attention():\n            dtype = self.get_dtype()\n            if self.manual_cast_dtype is not None:\n                dtype = self.manual_cast_dtype\n            #TODO: this needs to be tweaked\n            area = input_shape[0] * input_shape[2] * input_shape[3]\n            return (area * ldm_patched.modules.model_management.dtype_size(dtype) / 50) * (1024 * 1024)\n        else:\n            #TODO: this formula might be too aggressive since I tweaked the sub-quad and split algorithms to use less memory.\n            area = input_shape[0] * input_shape[2] * input_shape[3]\n            return (((area * 0.6) / 0.9) + 1024) * (1024 * 1024)\n\n\ndef unclip_adm(unclip_conditioning, device, noise_augmentor, noise_augment_merge=0.0, seed=None):\n    adm_inputs = []\n    weights = []\n    noise_aug = []\n    for unclip_cond in unclip_conditioning:\n        for adm_cond in unclip_cond[\"clip_vision_output\"].image_embeds:\n            weight = unclip_cond[\"strength\"]\n            noise_augment = unclip_cond[\"noise_augmentation\"]\n            noise_level = round((noise_augmentor.max_noise_level - 1) * noise_augment)\n            c_adm, noise_level_emb = noise_augmentor(adm_cond.to(device), noise_level=torch.tensor([noise_level], device=device), seed=seed)\n            adm_out = torch.cat((c_adm, noise_level_emb), 1) * weight\n            weights.append(weight)\n            noise_aug.append(noise_augment)\n            adm_inputs.append(adm_out)\n\n    if len(noise_aug) > 1:\n        adm_out = torch.stack(adm_inputs).sum(0)\n        noise_augment = noise_augment_merge\n        noise_level = round((noise_augmentor.max_noise_level - 1) * noise_augment)\n        c_adm, noise_level_emb = noise_augmentor(adm_out[:, :noise_augmentor.time_embed.dim], noise_level=torch.tensor([noise_level], device=device))\n        adm_out = torch.cat((c_adm, noise_level_emb), 1)\n\n    return adm_out\n\nclass SD21UNCLIP(BaseModel):\n    def __init__(self, model_config, noise_aug_config, model_type=ModelType.V_PREDICTION, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**noise_aug_config)\n\n    def encode_adm(self, **kwargs):\n        unclip_conditioning = kwargs.get(\"unclip_conditioning\", None)\n        device = kwargs[\"device\"]\n        if unclip_conditioning is None:\n            return torch.zeros((1, self.adm_channels))\n        else:\n            return unclip_adm(unclip_conditioning, device, self.noise_augmentor, kwargs.get(\"unclip_noise_augment_merge\", 0.05), kwargs.get(\"seed\", 0) - 10)\n\ndef sdxl_pooled(args, noise_augmentor):\n    if \"unclip_conditioning\" in args:\n        return unclip_adm(args.get(\"unclip_conditioning\", None), args[\"device\"], noise_augmentor, seed=args.get(\"seed\", 0) - 10)[:,:1280]\n    else:\n        return args[\"pooled_output\"]\n\nclass SDXLRefiner(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)\n        crop_h = kwargs.get(\"crop_h\", 0)\n\n        if kwargs.get(\"prompt_type\", \"\") == \"negative\":\n            aesthetic_score = kwargs.get(\"aesthetic_score\", 2.5)\n        else:\n            aesthetic_score = kwargs.get(\"aesthetic_score\", 6)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([height])))\n        out.append(self.embedder(torch.Tensor([width])))\n        out.append(self.embedder(torch.Tensor([crop_h])))\n        out.append(self.embedder(torch.Tensor([crop_w])))\n        out.append(self.embedder(torch.Tensor([aesthetic_score])))\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0).repeat(clip_pooled.shape[0], 1)\n        return torch.cat((clip_pooled.to(flat.device), flat), dim=1)\n\nclass SDXL(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)\n        crop_h = kwargs.get(\"crop_h\", 0)\n        target_width = kwargs.get(\"target_width\", width)\n        target_height = kwargs.get(\"target_height\", height)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([height])))\n        out.append(self.embedder(torch.Tensor([width])))\n        out.append(self.embedder(torch.Tensor([crop_h])))\n        out.append(self.embedder(torch.Tensor([crop_w])))\n        out.append(self.embedder(torch.Tensor([target_height])))\n        out.append(self.embedder(torch.Tensor([target_width])))\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0).repeat(clip_pooled.shape[0], 1)\n        return torch.cat((clip_pooled.to(flat.device), flat), dim=1)\n\nclass SVD_img2vid(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.V_PREDICTION_EDM, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n\n    def encode_adm(self, **kwargs):\n        fps_id = kwargs.get(\"fps\", 6) - 1\n        motion_bucket_id = kwargs.get(\"motion_bucket_id\", 127)\n        augmentation = kwargs.get(\"augmentation_level\", 0)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([fps_id])))\n        out.append(self.embedder(torch.Tensor([motion_bucket_id])))\n        out.append(self.embedder(torch.Tensor([augmentation])))\n\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0)\n        return flat\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        adm = self.encode_adm(**kwargs)\n        if adm is not None:\n            out['y'] = ldm_patched.modules.conds.CONDRegular(adm)\n\n        latent_image = kwargs.get(\"concat_latent_image\", None)\n        noise = kwargs.get(\"noise\", None)\n        device = kwargs[\"device\"]\n\n        if latent_image is None:\n            latent_image = torch.zeros_like(noise)\n\n        if latent_image.shape[1:] != noise.shape[1:]:\n            latent_image = utils.common_upscale(latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        latent_image = utils.resize_to_batch_size(latent_image, noise.shape[0])\n\n        out['c_concat'] = ldm_patched.modules.conds.CONDNoiseShape(latent_image)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['c_crossattn'] = ldm_patched.modules.conds.CONDCrossAttn(cross_attn)\n\n        if \"time_conditioning\" in kwargs:\n            out[\"time_context\"] = ldm_patched.modules.conds.CONDCrossAttn(kwargs[\"time_conditioning\"])\n\n        out['image_only_indicator'] = ldm_patched.modules.conds.CONDConstant(torch.zeros((1,), device=device))\n        out['num_video_frames'] = ldm_patched.modules.conds.CONDConstant(noise.shape[0])\n        return out\n\nclass Stable_Zero123(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None, cc_projection_weight=None, cc_projection_bias=None):\n        super().__init__(model_config, model_type, device=device)\n        self.cc_projection = ldm_patched.modules.ops.manual_cast.Linear(cc_projection_weight.shape[1], cc_projection_weight.shape[0], dtype=self.get_dtype(), device=device)\n        self.cc_projection.weight.copy_(cc_projection_weight)\n        self.cc_projection.bias.copy_(cc_projection_bias)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        latent_image = kwargs.get(\"concat_latent_image\", None)\n        noise = kwargs.get(\"noise\", None)\n\n        if latent_image is None:\n            latent_image = torch.zeros_like(noise)\n\n        if latent_image.shape[1:] != noise.shape[1:]:\n            latent_image = utils.common_upscale(latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        latent_image = utils.resize_to_batch_size(latent_image, noise.shape[0])\n\n        out['c_concat'] = ldm_patched.modules.conds.CONDNoiseShape(latent_image)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            if cross_attn.shape[-1] != 768:\n                cross_attn = self.cc_projection(cross_attn)\n            out['c_crossattn'] = ldm_patched.modules.conds.CONDCrossAttn(cross_attn)\n        return out\n\nclass SD_X4Upscaler(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.V_PREDICTION, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.noise_augmentor = ImageConcatWithNoiseAugmentation(noise_schedule_config={\"linear_start\": 0.0001, \"linear_end\": 0.02}, max_noise_level=350)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        image = kwargs.get(\"concat_image\", None)\n        noise = kwargs.get(\"noise\", None)\n        noise_augment = kwargs.get(\"noise_augmentation\", 0.0)\n        device = kwargs[\"device\"]\n        seed = kwargs[\"seed\"] - 10\n\n        noise_level = round((self.noise_augmentor.max_noise_level) * noise_augment)\n\n        if image is None:\n            image = torch.zeros_like(noise)[:,:3]\n\n        if image.shape[1:] != noise.shape[1:]:\n            image = utils.common_upscale(image.to(device), noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        noise_level = torch.tensor([noise_level], device=device)\n        if noise_augment > 0:\n            image, noise_level = self.noise_augmentor(image.to(device), noise_level=noise_level, seed=seed)\n\n        image = utils.resize_to_batch_size(image, noise.shape[0])\n\n        out['c_concat'] = ldm_patched.modules.conds.CONDNoiseShape(image)\n        out['y'] = ldm_patched.modules.conds.CONDRegular(noise_level)\n        return out\n", "ldm_patched/modules/conds.py": "import torch\nimport math\nimport ldm_patched.modules.utils\n\n\n\nclass CONDRegular:\n    def __init__(self, cond):\n        self.cond = cond\n\n    def _copy_with(self, cond):\n        return self.__class__(cond)\n\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(ldm_patched.modules.utils.repeat_to_batch_size(self.cond, batch_size).to(device))\n\n    def can_concat(self, other):\n        if self.cond.shape != other.cond.shape:\n            return False\n        return True\n\n    def concat(self, others):\n        conds = [self.cond]\n        for x in others:\n            conds.append(x.cond)\n        return torch.cat(conds)\n\nclass CONDNoiseShape(CONDRegular):\n    def process_cond(self, batch_size, device, area, **kwargs):\n        data = self.cond[:,:,area[2]:area[0] + area[2],area[3]:area[1] + area[3]]\n        return self._copy_with(ldm_patched.modules.utils.repeat_to_batch_size(data, batch_size).to(device))\n\n\nclass CONDCrossAttn(CONDRegular):\n    def can_concat(self, other):\n        s1 = self.cond.shape\n        s2 = other.cond.shape\n        if s1 != s2:\n            if s1[0] != s2[0] or s1[2] != s2[2]: #these 2 cases should not happen\n                return False\n\n            mult_min = math.lcm(s1[1], s2[1])\n            diff = mult_min // min(s1[1], s2[1])\n            if diff > 4: #arbitrary limit on the padding because it's probably going to impact performance negatively if it's too much\n                return False\n        return True\n\n    def concat(self, others):\n        conds = [self.cond]\n        crossattn_max_len = self.cond.shape[1]\n        for x in others:\n            c = x.cond\n            crossattn_max_len = math.lcm(crossattn_max_len, c.shape[1])\n            conds.append(c)\n\n        out = []\n        for c in conds:\n            if c.shape[1] < crossattn_max_len:\n                c = c.repeat(1, crossattn_max_len // c.shape[1], 1) #padding with repeat doesn't change result\n            out.append(c)\n        return torch.cat(out)\n\nclass CONDConstant(CONDRegular):\n    def __init__(self, cond):\n        self.cond = cond\n\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(self.cond)\n\n    def can_concat(self, other):\n        if self.cond != other.cond:\n            return False\n        return True\n\n    def concat(self, others):\n        return self.cond\n", "ldm_patched/modules/lora.py": "import ldm_patched.modules.utils\n\nLORA_CLIP_MAP = {\n    \"mlp.fc1\": \"mlp_fc1\",\n    \"mlp.fc2\": \"mlp_fc2\",\n    \"self_attn.k_proj\": \"self_attn_k_proj\",\n    \"self_attn.q_proj\": \"self_attn_q_proj\",\n    \"self_attn.v_proj\": \"self_attn_v_proj\",\n    \"self_attn.out_proj\": \"self_attn_out_proj\",\n}\n\n\ndef load_lora(lora, to_load):\n    patch_dict = {}\n    loaded_keys = set()\n    for x in to_load:\n        alpha_name = \"{}.alpha\".format(x)\n        alpha = None\n        if alpha_name in lora.keys():\n            alpha = lora[alpha_name].item()\n            loaded_keys.add(alpha_name)\n\n        regular_lora = \"{}.lora_up.weight\".format(x)\n        diffusers_lora = \"{}_lora.up.weight\".format(x)\n        transformers_lora = \"{}.lora_linear_layer.up.weight\".format(x)\n        A_name = None\n\n        if regular_lora in lora.keys():\n            A_name = regular_lora\n            B_name = \"{}.lora_down.weight\".format(x)\n            mid_name = \"{}.lora_mid.weight\".format(x)\n        elif diffusers_lora in lora.keys():\n            A_name = diffusers_lora\n            B_name = \"{}_lora.down.weight\".format(x)\n            mid_name = None\n        elif transformers_lora in lora.keys():\n            A_name = transformers_lora\n            B_name =\"{}.lora_linear_layer.down.weight\".format(x)\n            mid_name = None\n\n        if A_name is not None:\n            mid = None\n            if mid_name is not None and mid_name in lora.keys():\n                mid = lora[mid_name]\n                loaded_keys.add(mid_name)\n            patch_dict[to_load[x]] = (\"lora\", (lora[A_name], lora[B_name], alpha, mid))\n            loaded_keys.add(A_name)\n            loaded_keys.add(B_name)\n\n\n        ######## loha\n        hada_w1_a_name = \"{}.hada_w1_a\".format(x)\n        hada_w1_b_name = \"{}.hada_w1_b\".format(x)\n        hada_w2_a_name = \"{}.hada_w2_a\".format(x)\n        hada_w2_b_name = \"{}.hada_w2_b\".format(x)\n        hada_t1_name = \"{}.hada_t1\".format(x)\n        hada_t2_name = \"{}.hada_t2\".format(x)\n        if hada_w1_a_name in lora.keys():\n            hada_t1 = None\n            hada_t2 = None\n            if hada_t1_name in lora.keys():\n                hada_t1 = lora[hada_t1_name]\n                hada_t2 = lora[hada_t2_name]\n                loaded_keys.add(hada_t1_name)\n                loaded_keys.add(hada_t2_name)\n\n            patch_dict[to_load[x]] = (\"loha\", (lora[hada_w1_a_name], lora[hada_w1_b_name], alpha, lora[hada_w2_a_name], lora[hada_w2_b_name], hada_t1, hada_t2))\n            loaded_keys.add(hada_w1_a_name)\n            loaded_keys.add(hada_w1_b_name)\n            loaded_keys.add(hada_w2_a_name)\n            loaded_keys.add(hada_w2_b_name)\n\n\n        ######## lokr\n        lokr_w1_name = \"{}.lokr_w1\".format(x)\n        lokr_w2_name = \"{}.lokr_w2\".format(x)\n        lokr_w1_a_name = \"{}.lokr_w1_a\".format(x)\n        lokr_w1_b_name = \"{}.lokr_w1_b\".format(x)\n        lokr_t2_name = \"{}.lokr_t2\".format(x)\n        lokr_w2_a_name = \"{}.lokr_w2_a\".format(x)\n        lokr_w2_b_name = \"{}.lokr_w2_b\".format(x)\n\n        lokr_w1 = None\n        if lokr_w1_name in lora.keys():\n            lokr_w1 = lora[lokr_w1_name]\n            loaded_keys.add(lokr_w1_name)\n\n        lokr_w2 = None\n        if lokr_w2_name in lora.keys():\n            lokr_w2 = lora[lokr_w2_name]\n            loaded_keys.add(lokr_w2_name)\n\n        lokr_w1_a = None\n        if lokr_w1_a_name in lora.keys():\n            lokr_w1_a = lora[lokr_w1_a_name]\n            loaded_keys.add(lokr_w1_a_name)\n\n        lokr_w1_b = None\n        if lokr_w1_b_name in lora.keys():\n            lokr_w1_b = lora[lokr_w1_b_name]\n            loaded_keys.add(lokr_w1_b_name)\n\n        lokr_w2_a = None\n        if lokr_w2_a_name in lora.keys():\n            lokr_w2_a = lora[lokr_w2_a_name]\n            loaded_keys.add(lokr_w2_a_name)\n\n        lokr_w2_b = None\n        if lokr_w2_b_name in lora.keys():\n            lokr_w2_b = lora[lokr_w2_b_name]\n            loaded_keys.add(lokr_w2_b_name)\n\n        lokr_t2 = None\n        if lokr_t2_name in lora.keys():\n            lokr_t2 = lora[lokr_t2_name]\n            loaded_keys.add(lokr_t2_name)\n\n        if (lokr_w1 is not None) or (lokr_w2 is not None) or (lokr_w1_a is not None) or (lokr_w2_a is not None):\n            patch_dict[to_load[x]] = (\"lokr\", (lokr_w1, lokr_w2, alpha, lokr_w1_a, lokr_w1_b, lokr_w2_a, lokr_w2_b, lokr_t2))\n\n        #glora\n        a1_name = \"{}.a1.weight\".format(x)\n        a2_name = \"{}.a2.weight\".format(x)\n        b1_name = \"{}.b1.weight\".format(x)\n        b2_name = \"{}.b2.weight\".format(x)\n        if a1_name in lora:\n            patch_dict[to_load[x]] = (\"glora\", (lora[a1_name], lora[a2_name], lora[b1_name], lora[b2_name], alpha))\n            loaded_keys.add(a1_name)\n            loaded_keys.add(a2_name)\n            loaded_keys.add(b1_name)\n            loaded_keys.add(b2_name)\n\n        w_norm_name = \"{}.w_norm\".format(x)\n        b_norm_name = \"{}.b_norm\".format(x)\n        w_norm = lora.get(w_norm_name, None)\n        b_norm = lora.get(b_norm_name, None)\n\n        if w_norm is not None:\n            loaded_keys.add(w_norm_name)\n            patch_dict[to_load[x]] = (\"diff\", (w_norm,))\n            if b_norm is not None:\n                loaded_keys.add(b_norm_name)\n                patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (b_norm,))\n\n        diff_name = \"{}.diff\".format(x)\n        diff_weight = lora.get(diff_name, None)\n        if diff_weight is not None:\n            patch_dict[to_load[x]] = (\"diff\", (diff_weight,))\n            loaded_keys.add(diff_name)\n\n        diff_bias_name = \"{}.diff_b\".format(x)\n        diff_bias = lora.get(diff_bias_name, None)\n        if diff_bias is not None:\n            patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (diff_bias,))\n            loaded_keys.add(diff_bias_name)\n\n    for x in lora.keys():\n        if x not in loaded_keys:\n            print(\"lora key not loaded\", x)\n    return patch_dict\n\ndef model_lora_keys_clip(model, key_map={}):\n    sdk = model.state_dict().keys()\n\n    text_model_lora_key = \"lora_te_text_model_encoder_layers_{}_{}\"\n    clip_l_present = False\n    for b in range(32): #TODO: clean up\n        for c in LORA_CLIP_MAP:\n            k = \"clip_h.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                lora_key = text_model_lora_key.format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"lora_te1_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                key_map[lora_key] = k\n\n            k = \"clip_l.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                lora_key = text_model_lora_key.format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"lora_te1_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #SDXL base\n                key_map[lora_key] = k\n                clip_l_present = True\n                lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                key_map[lora_key] = k\n\n            k = \"clip_g.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                if clip_l_present:\n                    lora_key = \"lora_te2_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #SDXL base\n                    key_map[lora_key] = k\n                    lora_key = \"text_encoder_2.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                    key_map[lora_key] = k\n                else:\n                    lora_key = \"lora_te_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #TODO: test if this is correct for SDXL-Refiner\n                    key_map[lora_key] = k\n                    lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                    key_map[lora_key] = k\n\n    return key_map\n\ndef model_lora_keys_unet(model, key_map={}):\n    sdk = model.state_dict().keys()\n\n    for k in sdk:\n        if k.startswith(\"diffusion_model.\") and k.endswith(\".weight\"):\n            key_lora = k[len(\"diffusion_model.\"):-len(\".weight\")].replace(\".\", \"_\")\n            key_map[\"lora_unet_{}\".format(key_lora)] = k\n\n    diffusers_keys = ldm_patched.modules.utils.unet_to_diffusers(model.model_config.unet_config)\n    for k in diffusers_keys:\n        if k.endswith(\".weight\"):\n            unet_key = \"diffusion_model.{}\".format(diffusers_keys[k])\n            key_lora = k[:-len(\".weight\")].replace(\".\", \"_\")\n            key_map[\"lora_unet_{}\".format(key_lora)] = unet_key\n\n            diffusers_lora_prefix = [\"\", \"unet.\"]\n            for p in diffusers_lora_prefix:\n                diffusers_lora_key = \"{}{}\".format(p, k[:-len(\".weight\")].replace(\".to_\", \".processor.to_\"))\n                if diffusers_lora_key.endswith(\".to_out.0\"):\n                    diffusers_lora_key = diffusers_lora_key[:-2]\n                key_map[diffusers_lora_key] = unet_key\n    return key_map\n", "ldm_patched/modules/sd.py": "import torch\n\nfrom ldm_patched.modules import model_management\nfrom ldm_patched.ldm.models.autoencoder import AutoencoderKL, AutoencodingEngine\nimport yaml\n\nimport ldm_patched.modules.utils\n\nfrom . import clip_vision\nfrom . import gligen\nfrom . import diffusers_convert\nfrom . import model_base\nfrom . import model_detection\n\nfrom . import sd1_clip\nfrom . import sd2_clip\nfrom . import sdxl_clip\n\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.lora\nimport ldm_patched.t2ia.adapter\nimport ldm_patched.modules.supported_models_base\nimport ldm_patched.taesd.taesd\n\ndef load_model_weights(model, sd):\n    m, u = model.load_state_dict(sd, strict=False)\n    m = set(m)\n    unexpected_keys = set(u)\n\n    k = list(sd.keys())\n    for x in k:\n        if x not in unexpected_keys:\n            w = sd.pop(x)\n            del w\n    if len(m) > 0:\n        print(\"extra\", m)\n    return model\n\ndef load_clip_weights(model, sd):\n    k = list(sd.keys())\n    for x in k:\n        if x.startswith(\"cond_stage_model.transformer.\") and not x.startswith(\"cond_stage_model.transformer.text_model.\"):\n            y = x.replace(\"cond_stage_model.transformer.\", \"cond_stage_model.transformer.text_model.\")\n            sd[y] = sd.pop(x)\n\n    if 'cond_stage_model.transformer.text_model.embeddings.position_ids' in sd:\n        ids = sd['cond_stage_model.transformer.text_model.embeddings.position_ids']\n        if ids.dtype == torch.float32:\n            sd['cond_stage_model.transformer.text_model.embeddings.position_ids'] = ids.round()\n\n    sd = ldm_patched.modules.utils.transformers_convert(sd, \"cond_stage_model.model.\", \"cond_stage_model.transformer.text_model.\", 24)\n    return load_model_weights(model, sd)\n\n\ndef load_lora_for_models(model, clip, lora, strength_model, strength_clip):\n    key_map = {}\n    if model is not None:\n        key_map = ldm_patched.modules.lora.model_lora_keys_unet(model.model, key_map)\n    if clip is not None:\n        key_map = ldm_patched.modules.lora.model_lora_keys_clip(clip.cond_stage_model, key_map)\n\n    loaded = ldm_patched.modules.lora.load_lora(lora, key_map)\n    if model is not None:\n        new_modelpatcher = model.clone()\n        k = new_modelpatcher.add_patches(loaded, strength_model)\n    else:\n        k = ()\n        new_modelpatcher = None\n\n    if clip is not None:\n        new_clip = clip.clone()\n        k1 = new_clip.add_patches(loaded, strength_clip)\n    else:\n        k1 = ()\n        new_clip = None\n    k = set(k)\n    k1 = set(k1)\n    for x in loaded:\n        if (x not in k) and (x not in k1):\n            print(\"NOT LOADED\", x)\n\n    return (new_modelpatcher, new_clip)\n\n\nclass CLIP:\n    def __init__(self, target=None, embedding_directory=None, no_init=False):\n        if no_init:\n            return\n        params = target.params.copy()\n        clip = target.clip\n        tokenizer = target.tokenizer\n\n        load_device = model_management.text_encoder_device()\n        offload_device = model_management.text_encoder_offload_device()\n        params['device'] = offload_device\n        params['dtype'] = model_management.text_encoder_dtype(load_device)\n\n        self.cond_stage_model = clip(**(params))\n\n        self.tokenizer = tokenizer(embedding_directory=embedding_directory)\n        self.patcher = ldm_patched.modules.model_patcher.ModelPatcher(self.cond_stage_model, load_device=load_device, offload_device=offload_device)\n        self.layer_idx = None\n\n    def clone(self):\n        n = CLIP(no_init=True)\n        n.patcher = self.patcher.clone()\n        n.cond_stage_model = self.cond_stage_model\n        n.tokenizer = self.tokenizer\n        n.layer_idx = self.layer_idx\n        return n\n\n    def add_patches(self, patches, strength_patch=1.0, strength_model=1.0):\n        return self.patcher.add_patches(patches, strength_patch, strength_model)\n\n    def clip_layer(self, layer_idx):\n        self.layer_idx = layer_idx\n\n    def tokenize(self, text, return_word_ids=False):\n        return self.tokenizer.tokenize_with_weights(text, return_word_ids)\n\n    def encode_from_tokens(self, tokens, return_pooled=False):\n        if self.layer_idx is not None:\n            self.cond_stage_model.clip_layer(self.layer_idx)\n        else:\n            self.cond_stage_model.reset_clip_layer()\n\n        self.load_model()\n        cond, pooled = self.cond_stage_model.encode_token_weights(tokens)\n        if return_pooled:\n            return cond, pooled\n        return cond\n\n    def encode(self, text):\n        tokens = self.tokenize(text)\n        return self.encode_from_tokens(tokens)\n\n    def load_sd(self, sd):\n        return self.cond_stage_model.load_sd(sd)\n\n    def get_sd(self):\n        return self.cond_stage_model.state_dict()\n\n    def load_model(self):\n        model_management.load_model_gpu(self.patcher)\n        return self.patcher\n\n    def get_key_patches(self):\n        return self.patcher.get_key_patches()\n\nclass VAE:\n    def __init__(self, sd=None, device=None, config=None, dtype=None):\n        if 'decoder.up_blocks.0.resnets.0.norm1.weight' in sd.keys(): #diffusers format\n            sd = diffusers_convert.convert_vae_state_dict(sd)\n\n        self.memory_used_encode = lambda shape, dtype: (1767 * shape[2] * shape[3]) * model_management.dtype_size(dtype) #These are for AutoencoderKL and need tweaking (should be lower)\n        self.memory_used_decode = lambda shape, dtype: (2178 * shape[2] * shape[3] * 64) * model_management.dtype_size(dtype)\n        self.downscale_ratio = 8\n        self.latent_channels = 4\n\n        if config is None:\n            if \"decoder.mid.block_1.mix_factor\" in sd:\n                encoder_config = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n                decoder_config = encoder_config.copy()\n                decoder_config[\"video_kernel_size\"] = [3, 1, 1]\n                decoder_config[\"alpha\"] = 0.0\n                self.first_stage_model = AutoencodingEngine(regularizer_config={'target': \"ldm_patched.ldm.models.autoencoder.DiagonalGaussianRegularizer\"},\n                                                            encoder_config={'target': \"ldm_patched.ldm.modules.diffusionmodules.model.Encoder\", 'params': encoder_config},\n                                                            decoder_config={'target': \"ldm_patched.ldm.modules.temporal_ae.VideoDecoder\", 'params': decoder_config})\n            elif \"taesd_decoder.1.weight\" in sd:\n                self.first_stage_model = ldm_patched.taesd.taesd.TAESD()\n            else:\n                #default SD1.x/SD2.x VAE parameters\n                ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n\n                if 'encoder.down.2.downsample.conv.weight' not in sd: #Stable diffusion x4 upscaler VAE\n                    ddconfig['ch_mult'] = [1, 2, 4]\n                    self.downscale_ratio = 4\n\n                self.first_stage_model = AutoencoderKL(ddconfig=ddconfig, embed_dim=4)\n        else:\n            self.first_stage_model = AutoencoderKL(**(config['params']))\n        self.first_stage_model = self.first_stage_model.eval()\n\n        m, u = self.first_stage_model.load_state_dict(sd, strict=False)\n        if len(m) > 0:\n            print(\"Missing VAE keys\", m)\n\n        if len(u) > 0:\n            print(\"Leftover VAE keys\", u)\n\n        if device is None:\n            device = model_management.vae_device()\n        self.device = device\n        offload_device = model_management.vae_offload_device()\n        if dtype is None:\n            dtype = model_management.vae_dtype()\n        self.vae_dtype = dtype\n        self.first_stage_model.to(self.vae_dtype)\n        self.output_device = model_management.intermediate_device()\n\n        self.patcher = ldm_patched.modules.model_patcher.ModelPatcher(self.first_stage_model, load_device=self.device, offload_device=offload_device)\n\n    def decode_tiled_(self, samples, tile_x=64, tile_y=64, overlap = 16):\n        steps = samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x, tile_y, overlap)\n        steps += samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n        steps += samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n        pbar = ldm_patched.modules.utils.ProgressBar(steps)\n\n        decode_fn = lambda a: (self.first_stage_model.decode(a.to(self.vae_dtype).to(self.device)) + 1.0).float()\n        output = torch.clamp((\n            (ldm_patched.modules.utils.tiled_scale(samples, decode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = self.downscale_ratio, output_device=self.output_device, pbar = pbar) +\n            ldm_patched.modules.utils.tiled_scale(samples, decode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = self.downscale_ratio, output_device=self.output_device, pbar = pbar) +\n             ldm_patched.modules.utils.tiled_scale(samples, decode_fn, tile_x, tile_y, overlap, upscale_amount = self.downscale_ratio, output_device=self.output_device, pbar = pbar))\n            / 3.0) / 2.0, min=0.0, max=1.0)\n        return output\n\n    def encode_tiled_(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n        steps = pixel_samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x, tile_y, overlap)\n        steps += pixel_samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n        steps += pixel_samples.shape[0] * ldm_patched.modules.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n        pbar = ldm_patched.modules.utils.ProgressBar(steps)\n\n        encode_fn = lambda a: self.first_stage_model.encode((2. * a - 1.).to(self.vae_dtype).to(self.device)).float()\n        samples = ldm_patched.modules.utils.tiled_scale(pixel_samples, encode_fn, tile_x, tile_y, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples += ldm_patched.modules.utils.tiled_scale(pixel_samples, encode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples += ldm_patched.modules.utils.tiled_scale(pixel_samples, encode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples /= 3.0\n        return samples\n\n    def decode(self, samples_in):\n        try:\n            memory_used = self.memory_used_decode(samples_in.shape, self.vae_dtype)\n            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n            free_memory = model_management.get_free_memory(self.device)\n            batch_number = int(free_memory / memory_used)\n            batch_number = max(1, batch_number)\n\n            pixel_samples = torch.empty((samples_in.shape[0], 3, round(samples_in.shape[2] * self.downscale_ratio), round(samples_in.shape[3] * self.downscale_ratio)), device=self.output_device)\n            for x in range(0, samples_in.shape[0], batch_number):\n                samples = samples_in[x:x+batch_number].to(self.vae_dtype).to(self.device)\n                pixel_samples[x:x+batch_number] = torch.clamp((self.first_stage_model.decode(samples).to(self.output_device).float() + 1.0) / 2.0, min=0.0, max=1.0)\n        except model_management.OOM_EXCEPTION as e:\n            print(\"Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.\")\n            pixel_samples = self.decode_tiled_(samples_in)\n\n        pixel_samples = pixel_samples.to(self.output_device).movedim(1,-1)\n        return pixel_samples\n\n    def decode_tiled(self, samples, tile_x=64, tile_y=64, overlap = 16):\n        model_management.load_model_gpu(self.patcher)\n        output = self.decode_tiled_(samples, tile_x, tile_y, overlap)\n        return output.movedim(1,-1)\n\n    def encode(self, pixel_samples):\n        pixel_samples = pixel_samples.movedim(-1,1)\n        try:\n            memory_used = self.memory_used_encode(pixel_samples.shape, self.vae_dtype)\n            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n            free_memory = model_management.get_free_memory(self.device)\n            batch_number = int(free_memory / memory_used)\n            batch_number = max(1, batch_number)\n            samples = torch.empty((pixel_samples.shape[0], self.latent_channels, round(pixel_samples.shape[2] // self.downscale_ratio), round(pixel_samples.shape[3] // self.downscale_ratio)), device=self.output_device)\n            for x in range(0, pixel_samples.shape[0], batch_number):\n                pixels_in = (2. * pixel_samples[x:x+batch_number] - 1.).to(self.vae_dtype).to(self.device)\n                samples[x:x+batch_number] = self.first_stage_model.encode(pixels_in).to(self.output_device).float()\n\n        except model_management.OOM_EXCEPTION as e:\n            print(\"Warning: Ran out of memory when regular VAE encoding, retrying with tiled VAE encoding.\")\n            samples = self.encode_tiled_(pixel_samples)\n\n        return samples\n\n    def encode_tiled(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n        model_management.load_model_gpu(self.patcher)\n        pixel_samples = pixel_samples.movedim(-1,1)\n        samples = self.encode_tiled_(pixel_samples, tile_x=tile_x, tile_y=tile_y, overlap=overlap)\n        return samples\n\n    def get_sd(self):\n        return self.first_stage_model.state_dict()\n\nclass StyleModel:\n    def __init__(self, model, device=\"cpu\"):\n        self.model = model\n\n    def get_cond(self, input):\n        return self.model(input.last_hidden_state)\n\n\ndef load_style_model(ckpt_path):\n    model_data = ldm_patched.modules.utils.load_torch_file(ckpt_path, safe_load=True)\n    keys = model_data.keys()\n    if \"style_embedding\" in keys:\n        model = ldm_patched.t2ia.adapter.StyleAdapter(width=1024, context_dim=768, num_head=8, n_layes=3, num_token=8)\n    else:\n        raise Exception(\"invalid style model {}\".format(ckpt_path))\n    model.load_state_dict(model_data)\n    return StyleModel(model)\n\n\ndef load_clip(ckpt_paths, embedding_directory=None):\n    clip_data = []\n    for p in ckpt_paths:\n        clip_data.append(ldm_patched.modules.utils.load_torch_file(p, safe_load=True))\n\n    class EmptyClass:\n        pass\n\n    for i in range(len(clip_data)):\n        if \"transformer.resblocks.0.ln_1.weight\" in clip_data[i]:\n            clip_data[i] = ldm_patched.modules.utils.transformers_convert(clip_data[i], \"\", \"text_model.\", 32)\n\n    clip_target = EmptyClass()\n    clip_target.params = {}\n    if len(clip_data) == 1:\n        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in clip_data[0]:\n            clip_target.clip = sdxl_clip.SDXLRefinerClipModel\n            clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n        elif \"text_model.encoder.layers.22.mlp.fc1.weight\" in clip_data[0]:\n            clip_target.clip = sd2_clip.SD2ClipModel\n            clip_target.tokenizer = sd2_clip.SD2Tokenizer\n        else:\n            clip_target.clip = sd1_clip.SD1ClipModel\n            clip_target.tokenizer = sd1_clip.SD1Tokenizer\n    else:\n        clip_target.clip = sdxl_clip.SDXLClipModel\n        clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n\n    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n    for c in clip_data:\n        m, u = clip.load_sd(c)\n        if len(m) > 0:\n            print(\"clip missing:\", m)\n\n        if len(u) > 0:\n            print(\"clip unexpected:\", u)\n    return clip\n\ndef load_gligen(ckpt_path):\n    data = ldm_patched.modules.utils.load_torch_file(ckpt_path, safe_load=True)\n    model = gligen.load_gligen(data)\n    if model_management.should_use_fp16():\n        model = model.half()\n    return ldm_patched.modules.model_patcher.ModelPatcher(model, load_device=model_management.get_torch_device(), offload_device=model_management.unet_offload_device())\n\ndef load_checkpoint(config_path=None, ckpt_path=None, output_vae=True, output_clip=True, embedding_directory=None, state_dict=None, config=None):\n    #TODO: this function is a mess and should be removed eventually\n    if config is None:\n        with open(config_path, 'r') as stream:\n            config = yaml.safe_load(stream)\n    model_config_params = config['model']['params']\n    clip_config = model_config_params['cond_stage_config']\n    scale_factor = model_config_params['scale_factor']\n    vae_config = model_config_params['first_stage_config']\n\n    fp16 = False\n    if \"unet_config\" in model_config_params:\n        if \"params\" in model_config_params[\"unet_config\"]:\n            unet_config = model_config_params[\"unet_config\"][\"params\"]\n            if \"use_fp16\" in unet_config:\n                fp16 = unet_config.pop(\"use_fp16\")\n                if fp16:\n                    unet_config[\"dtype\"] = torch.float16\n\n    noise_aug_config = None\n    if \"noise_aug_config\" in model_config_params:\n        noise_aug_config = model_config_params[\"noise_aug_config\"]\n\n    model_type = model_base.ModelType.EPS\n\n    if \"parameterization\" in model_config_params:\n        if model_config_params[\"parameterization\"] == \"v\":\n            model_type = model_base.ModelType.V_PREDICTION\n\n    clip = None\n    vae = None\n\n    class WeightsLoader(torch.nn.Module):\n        pass\n\n    if state_dict is None:\n        state_dict = ldm_patched.modules.utils.load_torch_file(ckpt_path)\n\n    class EmptyClass:\n        pass\n\n    model_config = ldm_patched.modules.supported_models_base.BASE({})\n\n    from . import latent_formats\n    model_config.latent_format = latent_formats.SD15(scale_factor=scale_factor)\n    model_config.unet_config = model_detection.convert_config(unet_config)\n\n    if config['model'][\"target\"].endswith(\"ImageEmbeddingConditionedLatentDiffusion\"):\n        model = model_base.SD21UNCLIP(model_config, noise_aug_config[\"params\"], model_type=model_type)\n    else:\n        model = model_base.BaseModel(model_config, model_type=model_type)\n\n    if config['model'][\"target\"].endswith(\"LatentInpaintDiffusion\"):\n        model.set_inpaint()\n\n    if fp16:\n        model = model.half()\n\n    offload_device = model_management.unet_offload_device()\n    model = model.to(offload_device)\n    model.load_model_weights(state_dict, \"model.diffusion_model.\")\n\n    if output_vae:\n        vae_sd = ldm_patched.modules.utils.state_dict_prefix_replace(state_dict, {\"first_stage_model.\": \"\"}, filter_keys=True)\n        vae = VAE(sd=vae_sd, config=vae_config)\n\n    if output_clip:\n        w = WeightsLoader()\n        clip_target = EmptyClass()\n        clip_target.params = clip_config.get(\"params\", {})\n        if clip_config[\"target\"].endswith(\"FrozenOpenCLIPEmbedder\"):\n            clip_target.clip = sd2_clip.SD2ClipModel\n            clip_target.tokenizer = sd2_clip.SD2Tokenizer\n            clip = CLIP(clip_target, embedding_directory=embedding_directory)\n            w.cond_stage_model = clip.cond_stage_model.clip_h\n        elif clip_config[\"target\"].endswith(\"FrozenCLIPEmbedder\"):\n            clip_target.clip = sd1_clip.SD1ClipModel\n            clip_target.tokenizer = sd1_clip.SD1Tokenizer\n            clip = CLIP(clip_target, embedding_directory=embedding_directory)\n            w.cond_stage_model = clip.cond_stage_model.clip_l\n        load_clip_weights(w, state_dict)\n\n    return (ldm_patched.modules.model_patcher.ModelPatcher(model, load_device=model_management.get_torch_device(), offload_device=offload_device), clip, vae)\n\ndef load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=False, embedding_directory=None, output_model=True, vae_filename_param=None):\n    sd = ldm_patched.modules.utils.load_torch_file(ckpt_path)\n    sd_keys = sd.keys()\n    clip = None\n    clipvision = None\n    vae = None\n    vae_filename = None\n    model = None\n    model_patcher = None\n    clip_target = None\n\n    parameters = ldm_patched.modules.utils.calculate_parameters(sd, \"model.diffusion_model.\")\n    unet_dtype = model_management.unet_dtype(model_params=parameters)\n    load_device = model_management.get_torch_device()\n    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device)\n\n    class WeightsLoader(torch.nn.Module):\n        pass\n\n    model_config = model_detection.model_config_from_unet(sd, \"model.diffusion_model.\", unet_dtype)\n    model_config.set_manual_cast(manual_cast_dtype)\n\n    if model_config is None:\n        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(ckpt_path))\n\n    if model_config.clip_vision_prefix is not None:\n        if output_clipvision:\n            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n\n    if output_model:\n        inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n        offload_device = model_management.unet_offload_device()\n        model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n        model.load_model_weights(sd, \"model.diffusion_model.\")\n\n    if output_vae:\n        if vae_filename_param is None:\n            vae_sd = ldm_patched.modules.utils.state_dict_prefix_replace(sd, {\"first_stage_model.\": \"\"}, filter_keys=True)\n            vae_sd = model_config.process_vae_state_dict(vae_sd)\n        else:\n            vae_sd = ldm_patched.modules.utils.load_torch_file(vae_filename_param)\n            vae_filename = vae_filename_param\n        vae = VAE(sd=vae_sd)\n\n    if output_clip:\n        w = WeightsLoader()\n        clip_target = model_config.clip_target()\n        if clip_target is not None:\n            clip = CLIP(clip_target, embedding_directory=embedding_directory)\n            w.cond_stage_model = clip.cond_stage_model\n            sd = model_config.process_clip_state_dict(sd)\n            load_model_weights(w, sd)\n\n    left_over = sd.keys()\n    if len(left_over) > 0:\n        print(\"left over keys:\", left_over)\n\n    if output_model:\n        model_patcher = ldm_patched.modules.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n        if inital_load_device != torch.device(\"cpu\"):\n            print(\"loaded straight to GPU\")\n            model_management.load_model_gpu(model_patcher)\n\n    return model_patcher, clip, vae, vae_filename, clipvision\n\n\ndef load_unet_state_dict(sd): #load unet in diffusers format\n    parameters = ldm_patched.modules.utils.calculate_parameters(sd)\n    unet_dtype = model_management.unet_dtype(model_params=parameters)\n    load_device = model_management.get_torch_device()\n    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device)\n\n    if \"input_blocks.0.0.weight\" in sd: #ldm\n        model_config = model_detection.model_config_from_unet(sd, \"\", unet_dtype)\n        if model_config is None:\n            return None\n        new_sd = sd\n\n    else: #diffusers\n        model_config = model_detection.model_config_from_diffusers_unet(sd, unet_dtype)\n        if model_config is None:\n            return None\n\n        diffusers_keys = ldm_patched.modules.utils.unet_to_diffusers(model_config.unet_config)\n\n        new_sd = {}\n        for k in diffusers_keys:\n            if k in sd:\n                new_sd[diffusers_keys[k]] = sd.pop(k)\n            else:\n                print(diffusers_keys[k], k)\n    offload_device = model_management.unet_offload_device()\n    model_config.set_manual_cast(manual_cast_dtype)\n    model = model_config.get_model(new_sd, \"\")\n    model = model.to(offload_device)\n    model.load_model_weights(new_sd, \"\")\n    left_over = sd.keys()\n    if len(left_over) > 0:\n        print(\"left over keys in unet:\", left_over)\n    return ldm_patched.modules.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device)\n\ndef load_unet(unet_path):\n    sd = ldm_patched.modules.utils.load_torch_file(unet_path)\n    model = load_unet_state_dict(sd)\n    if model is None:\n        print(\"ERROR UNSUPPORTED UNET\", unet_path)\n        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n    return model\n\ndef save_checkpoint(output_path, model, clip=None, vae=None, clip_vision=None, metadata=None):\n    clip_sd = None\n    load_models = [model]\n    if clip is not None:\n        load_models.append(clip.load_model())\n        clip_sd = clip.get_sd()\n\n    model_management.load_models_gpu(load_models)\n    clip_vision_sd = clip_vision.get_sd() if clip_vision is not None else None\n    sd = model.model.state_dict_for_saving(clip_sd, vae.get_sd(), clip_vision_sd)\n    ldm_patched.modules.utils.save_torch_file(sd, output_path, metadata=metadata)\n", "ldm_patched/modules/model_management.py": "import psutil\nfrom enum import Enum\nfrom ldm_patched.modules.args_parser import args\nimport ldm_patched.modules.utils\nimport torch\nimport sys\n\nclass VRAMState(Enum):\n    DISABLED = 0    #No vram present: no need to move models to vram\n    NO_VRAM = 1     #Very low vram: enable all the options to save vram\n    LOW_VRAM = 2\n    NORMAL_VRAM = 3\n    HIGH_VRAM = 4\n    SHARED = 5      #No dedicated vram: memory shared between CPU and GPU but models still need to be moved between both.\n\nclass CPUState(Enum):\n    GPU = 0\n    CPU = 1\n    MPS = 2\n\n# Determine VRAM State\nvram_state = VRAMState.NORMAL_VRAM\nset_vram_to = VRAMState.NORMAL_VRAM\ncpu_state = CPUState.GPU\n\ntotal_vram = 0\n\nlowvram_available = True\nxpu_available = False\n\nif args.pytorch_deterministic:\n    print(\"Using deterministic algorithms for pytorch\")\n    torch.use_deterministic_algorithms(True, warn_only=True)\n\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()\n    else:\n        directml_device = torch_directml.device(device_index)\n    print(\"Using directml with device:\", torch_directml.device_name(device_index))\n    # torch_directml.disable_tiled_resources(True)\n    lowvram_available = False #TODO: need to find a way to get free memory in directml before this can be enabled by default.\n\ntry:\n    import intel_extension_for_pytorch as ipex\n    if torch.xpu.is_available():\n        xpu_available = True\nexcept:\n    pass\n\ntry:\n    if torch.backends.mps.is_available():\n        cpu_state = CPUState.MPS\n        import torch.mps\nexcept:\n    pass\n\nif args.always_cpu:\n    if args.always_cpu > 0:\n        torch.set_num_threads(args.always_cpu)\n    print(f\"Running on {torch.get_num_threads()} CPU threads\")\n    cpu_state = CPUState.CPU\n\ndef is_intel_xpu():\n    global cpu_state\n    global xpu_available\n    if cpu_state == CPUState.GPU:\n        if xpu_available:\n            return True\n    return False\n\ndef get_torch_device():\n    global directml_enabled\n    global cpu_state\n    if directml_enabled:\n        global directml_device\n        return directml_device\n    if cpu_state == CPUState.MPS:\n        return torch.device(\"mps\")\n    if cpu_state == CPUState.CPU:\n        return torch.device(\"cpu\")\n    else:\n        if is_intel_xpu():\n            return torch.device(\"xpu\")\n        else:\n            return torch.device(torch.cuda.current_device())\n\ndef get_total_memory(dev=None, torch_total_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_total = psutil.virtual_memory().total\n        mem_total_torch = mem_total\n    else:\n        if directml_enabled:\n            mem_total = 1024 * 1024 * 1024 #TODO\n            mem_total_torch = mem_total\n        elif is_intel_xpu():\n            stats = torch.xpu.memory_stats(dev)\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_total = torch.xpu.get_device_properties(dev).total_memory\n            mem_total_torch = mem_reserved\n        else:\n            stats = torch.cuda.memory_stats(dev)\n            mem_reserved = stats['reserved_bytes.all.current']\n            _, mem_total_cuda = torch.cuda.mem_get_info(dev)\n            mem_total_torch = mem_reserved\n            mem_total = mem_total_cuda\n\n    if torch_total_too:\n        return (mem_total, mem_total_torch)\n    else:\n        return mem_total\n\ntotal_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\ntotal_ram = psutil.virtual_memory().total / (1024 * 1024)\nprint(\"Total VRAM {:0.0f} MB, total RAM {:0.0f} MB\".format(total_vram, total_ram))\nif not args.always_normal_vram and not args.always_cpu:\n    if lowvram_available and total_vram <= 4096:\n        print(\"Trying to enable lowvram mode because your GPU seems to have 4GB or less. If you don't want this use: --always-normal-vram\")\n        set_vram_to = VRAMState.LOW_VRAM\n\ntry:\n    OOM_EXCEPTION = torch.cuda.OutOfMemoryError\nexcept:\n    OOM_EXCEPTION = Exception\n\nXFORMERS_VERSION = \"\"\nXFORMERS_ENABLED_VAE = True\nif args.disable_xformers:\n    XFORMERS_IS_AVAILABLE = False\nelse:\n    try:\n        import xformers\n        import xformers.ops\n        XFORMERS_IS_AVAILABLE = True\n        try:\n            XFORMERS_IS_AVAILABLE = xformers._has_cpp_library\n        except:\n            pass\n        try:\n            XFORMERS_VERSION = xformers.version.__version__\n            print(\"xformers version:\", XFORMERS_VERSION)\n            if XFORMERS_VERSION.startswith(\"0.0.18\"):\n                print()\n                print(\"WARNING: This version of xformers has a major bug where you will get black images when generating high resolution images.\")\n                print(\"Please downgrade or upgrade xformers to a different version.\")\n                print()\n                XFORMERS_ENABLED_VAE = False\n        except:\n            pass\n    except:\n        XFORMERS_IS_AVAILABLE = False\n\ndef is_nvidia():\n    global cpu_state\n    if cpu_state == CPUState.GPU:\n        if torch.version.cuda:\n            return True\n    return False\n\nENABLE_PYTORCH_ATTENTION = False\nif args.attention_pytorch:\n    ENABLE_PYTORCH_ATTENTION = True\n    XFORMERS_IS_AVAILABLE = False\n\nVAE_DTYPE = torch.float32\n\ntry:\n    if is_nvidia():\n        torch_version = torch.version.__version__\n        if int(torch_version[0]) >= 2:\n            if ENABLE_PYTORCH_ATTENTION == False and args.attention_split == False and args.attention_quad == False:\n                ENABLE_PYTORCH_ATTENTION = True\n            if torch.cuda.is_bf16_supported() and torch.cuda.get_device_properties(torch.cuda.current_device()).major >= 8:\n                VAE_DTYPE = torch.bfloat16\n    if is_intel_xpu():\n        if args.attention_split == False and args.attention_quad == False:\n            ENABLE_PYTORCH_ATTENTION = True\nexcept:\n    pass\n\nif is_intel_xpu():\n    VAE_DTYPE = torch.bfloat16\n\nif args.vae_in_cpu:\n    VAE_DTYPE = torch.float32\n\nif args.vae_in_fp16:\n    VAE_DTYPE = torch.float16\nelif args.vae_in_bf16:\n    VAE_DTYPE = torch.bfloat16\nelif args.vae_in_fp32:\n    VAE_DTYPE = torch.float32\n\n\nif ENABLE_PYTORCH_ATTENTION:\n    torch.backends.cuda.enable_math_sdp(True)\n    torch.backends.cuda.enable_flash_sdp(True)\n    torch.backends.cuda.enable_mem_efficient_sdp(True)\n\nif args.always_low_vram:\n    set_vram_to = VRAMState.LOW_VRAM\n    lowvram_available = True\nelif args.always_no_vram:\n    set_vram_to = VRAMState.NO_VRAM\nelif args.always_high_vram or args.always_gpu:\n    vram_state = VRAMState.HIGH_VRAM\n\nFORCE_FP32 = False\nFORCE_FP16 = False\nif args.all_in_fp32:\n    print(\"Forcing FP32, if this improves things please report it.\")\n    FORCE_FP32 = True\n\nif args.all_in_fp16:\n    print(\"Forcing FP16.\")\n    FORCE_FP16 = True\n\nif lowvram_available:\n    if set_vram_to in (VRAMState.LOW_VRAM, VRAMState.NO_VRAM):\n        vram_state = set_vram_to\n\n\nif cpu_state != CPUState.GPU:\n    vram_state = VRAMState.DISABLED\n\nif cpu_state == CPUState.MPS:\n    vram_state = VRAMState.SHARED\n\nprint(f\"Set vram state to: {vram_state.name}\")\n\nALWAYS_VRAM_OFFLOAD = args.always_offload_from_vram\n\nif ALWAYS_VRAM_OFFLOAD:\n    print(\"Always offload VRAM\")\n\ndef get_torch_device_name(device):\n    if hasattr(device, 'type'):\n        if device.type == \"cuda\":\n            try:\n                allocator_backend = torch.cuda.get_allocator_backend()\n            except:\n                allocator_backend = \"\"\n            return \"{} {} : {}\".format(device, torch.cuda.get_device_name(device), allocator_backend)\n        else:\n            return \"{}\".format(device.type)\n    elif is_intel_xpu():\n        return \"{} {}\".format(device, torch.xpu.get_device_name(device))\n    else:\n        return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n\ntry:\n    print(\"Device:\", get_torch_device_name(get_torch_device()))\nexcept:\n    print(\"Could not pick default device.\")\n\nprint(\"VAE dtype:\", VAE_DTYPE)\n\ncurrent_loaded_models = []\n\ndef module_size(module):\n    module_mem = 0\n    sd = module.state_dict()\n    for k in sd:\n        t = sd[k]\n        module_mem += t.nelement() * t.element_size()\n    return module_mem\n\nclass LoadedModel:\n    def __init__(self, model):\n        self.model = model\n        self.model_accelerated = False\n        self.device = model.load_device\n\n    def model_memory(self):\n        return self.model.model_size()\n\n    def model_memory_required(self, device):\n        if device == self.model.current_device:\n            return 0\n        else:\n            return self.model_memory()\n\n    def model_load(self, lowvram_model_memory=0):\n        patch_model_to = None\n        if lowvram_model_memory == 0:\n            patch_model_to = self.device\n\n        self.model.model_patches_to(self.device)\n        self.model.model_patches_to(self.model.model_dtype())\n\n        try:\n            self.real_model = self.model.patch_model(device_to=patch_model_to) #TODO: do something with loras and offloading to CPU\n        except Exception as e:\n            self.model.unpatch_model(self.model.offload_device)\n            self.model_unload()\n            raise e\n\n        if lowvram_model_memory > 0:\n            print(\"loading in lowvram mode\", lowvram_model_memory/(1024 * 1024))\n            mem_counter = 0\n            for m in self.real_model.modules():\n                if hasattr(m, \"ldm_patched_cast_weights\"):\n                    m.prev_ldm_patched_cast_weights = m.ldm_patched_cast_weights\n                    m.ldm_patched_cast_weights = True\n                    module_mem = module_size(m)\n                    if mem_counter + module_mem < lowvram_model_memory:\n                        m.to(self.device)\n                        mem_counter += module_mem\n                elif hasattr(m, \"weight\"): #only modules with ldm_patched_cast_weights can be set to lowvram mode\n                    m.to(self.device)\n                    mem_counter += module_size(m)\n                    print(\"lowvram: loaded module regularly\", m)\n\n            self.model_accelerated = True\n\n        if is_intel_xpu() and not args.disable_ipex_hijack:\n            self.real_model = torch.xpu.optimize(self.real_model.eval(), inplace=True, auto_kernel_selection=True, graph_mode=True)\n\n        return self.real_model\n\n    def model_unload(self):\n        if self.model_accelerated:\n            for m in self.real_model.modules():\n                if hasattr(m, \"prev_ldm_patched_cast_weights\"):\n                    m.ldm_patched_cast_weights = m.prev_ldm_patched_cast_weights\n                    del m.prev_ldm_patched_cast_weights\n\n            self.model_accelerated = False\n\n        self.model.unpatch_model(self.model.offload_device)\n        self.model.model_patches_to(self.model.offload_device)\n\n    def __eq__(self, other):\n        return self.model is other.model\n\ndef minimum_inference_memory():\n    return (1024 * 1024 * 1024)\n\ndef unload_model_clones(model):\n    to_unload = []\n    for i in range(len(current_loaded_models)):\n        if model.is_clone(current_loaded_models[i].model):\n            to_unload = [i] + to_unload\n\n    for i in to_unload:\n        print(\"unload clone\", i)\n        current_loaded_models.pop(i).model_unload()\n\ndef free_memory(memory_required, device, keep_loaded=[]):\n    unloaded_model = False\n    for i in range(len(current_loaded_models) -1, -1, -1):\n        if not ALWAYS_VRAM_OFFLOAD:\n            if get_free_memory(device) > memory_required:\n                break\n        shift_model = current_loaded_models[i]\n        if shift_model.device == device:\n            if shift_model not in keep_loaded:\n                m = current_loaded_models.pop(i)\n                m.model_unload()\n                del m\n                unloaded_model = True\n\n    if unloaded_model:\n        soft_empty_cache()\n    else:\n        if vram_state != VRAMState.HIGH_VRAM:\n            mem_free_total, mem_free_torch = get_free_memory(device, torch_free_too=True)\n            if mem_free_torch > mem_free_total * 0.25:\n                soft_empty_cache()\n\ndef load_models_gpu(models, memory_required=0):\n    global vram_state\n\n    inference_memory = minimum_inference_memory()\n    extra_mem = max(inference_memory, memory_required)\n\n    models_to_load = []\n    models_already_loaded = []\n    for x in models:\n        loaded_model = LoadedModel(x)\n\n        if loaded_model in current_loaded_models:\n            index = current_loaded_models.index(loaded_model)\n            current_loaded_models.insert(0, current_loaded_models.pop(index))\n            models_already_loaded.append(loaded_model)\n        else:\n            if hasattr(x, \"model\"):\n                print(f\"Requested to load {x.model.__class__.__name__}\")\n            models_to_load.append(loaded_model)\n\n    if len(models_to_load) == 0:\n        devs = set(map(lambda a: a.device, models_already_loaded))\n        for d in devs:\n            if d != torch.device(\"cpu\"):\n                free_memory(extra_mem, d, models_already_loaded)\n        return\n\n    print(f\"Loading {len(models_to_load)} new model{'s' if len(models_to_load) > 1 else ''}\")\n\n    total_memory_required = {}\n    for loaded_model in models_to_load:\n        unload_model_clones(loaded_model.model)\n        total_memory_required[loaded_model.device] = total_memory_required.get(loaded_model.device, 0) + loaded_model.model_memory_required(loaded_model.device)\n\n    for device in total_memory_required:\n        if device != torch.device(\"cpu\"):\n            free_memory(total_memory_required[device] * 1.3 + extra_mem, device, models_already_loaded)\n\n    for loaded_model in models_to_load:\n        model = loaded_model.model\n        torch_dev = model.load_device\n        if is_device_cpu(torch_dev):\n            vram_set_state = VRAMState.DISABLED\n        else:\n            vram_set_state = vram_state\n        lowvram_model_memory = 0\n        if lowvram_available and (vram_set_state == VRAMState.LOW_VRAM or vram_set_state == VRAMState.NORMAL_VRAM):\n            model_size = loaded_model.model_memory_required(torch_dev)\n            current_free_mem = get_free_memory(torch_dev)\n            lowvram_model_memory = int(max(64 * (1024 * 1024), (current_free_mem - 1024 * (1024 * 1024)) / 1.3 ))\n            if model_size > (current_free_mem - inference_memory): #only switch to lowvram if really necessary\n                vram_set_state = VRAMState.LOW_VRAM\n            else:\n                lowvram_model_memory = 0\n\n        if vram_set_state == VRAMState.NO_VRAM:\n            lowvram_model_memory = 64 * 1024 * 1024\n\n        cur_loaded_model = loaded_model.model_load(lowvram_model_memory)\n        current_loaded_models.insert(0, loaded_model)\n    return\n\n\ndef load_model_gpu(model):\n    return load_models_gpu([model])\n\ndef cleanup_models():\n    to_delete = []\n    for i in range(len(current_loaded_models)):\n        if sys.getrefcount(current_loaded_models[i].model) <= 2:\n            to_delete = [i] + to_delete\n\n    for i in to_delete:\n        x = current_loaded_models.pop(i)\n        x.model_unload()\n        del x\n\ndef dtype_size(dtype):\n    dtype_size = 4\n    if dtype == torch.float16 or dtype == torch.bfloat16:\n        dtype_size = 2\n    elif dtype == torch.float32:\n        dtype_size = 4\n    else:\n        try:\n            dtype_size = dtype.itemsize\n        except: #Old pytorch doesn't have .itemsize\n            pass\n    return dtype_size\n\ndef unet_offload_device():\n    if vram_state == VRAMState.HIGH_VRAM:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef unet_inital_load_device(parameters, dtype):\n    torch_dev = get_torch_device()\n    if vram_state == VRAMState.HIGH_VRAM:\n        return torch_dev\n\n    cpu_dev = torch.device(\"cpu\")\n    if ALWAYS_VRAM_OFFLOAD:\n        return cpu_dev\n\n    model_size = dtype_size(dtype) * parameters\n\n    mem_dev = get_free_memory(torch_dev)\n    mem_cpu = get_free_memory(cpu_dev)\n    if mem_dev > mem_cpu and model_size < mem_dev:\n        return torch_dev\n    else:\n        return cpu_dev\n\ndef unet_dtype(device=None, model_params=0):\n    if args.unet_in_bf16:\n        return torch.bfloat16\n    if args.unet_in_fp16:\n        return torch.float16\n    if args.unet_in_fp8_e4m3fn:\n        return torch.float8_e4m3fn\n    if args.unet_in_fp8_e5m2:\n        return torch.float8_e5m2\n    if should_use_fp16(device=device, model_params=model_params):\n        return torch.float16\n    return torch.float32\n\n# None means no manual cast\ndef unet_manual_cast(weight_dtype, inference_device):\n    if weight_dtype == torch.float32:\n        return None\n\n    fp16_supported = ldm_patched.modules.model_management.should_use_fp16(inference_device, prioritize_performance=False)\n    if fp16_supported and weight_dtype == torch.float16:\n        return None\n\n    if fp16_supported:\n        return torch.float16\n    else:\n        return torch.float32\n\ndef text_encoder_offload_device():\n    if args.always_gpu:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef text_encoder_device():\n    if args.always_gpu:\n        return get_torch_device()\n    elif vram_state == VRAMState.HIGH_VRAM or vram_state == VRAMState.NORMAL_VRAM:\n        if is_intel_xpu():\n            return torch.device(\"cpu\")\n        if should_use_fp16(prioritize_performance=False):\n            return get_torch_device()\n        else:\n            return torch.device(\"cpu\")\n    else:\n        return torch.device(\"cpu\")\n\ndef text_encoder_dtype(device=None):\n    if args.clip_in_fp8_e4m3fn:\n        return torch.float8_e4m3fn\n    elif args.clip_in_fp8_e5m2:\n        return torch.float8_e5m2\n    elif args.clip_in_fp16:\n        return torch.float16\n    elif args.clip_in_fp32:\n        return torch.float32\n\n    if is_device_cpu(device):\n        return torch.float16\n\n    if should_use_fp16(device, prioritize_performance=False):\n        return torch.float16\n    else:\n        return torch.float32\n\ndef intermediate_device():\n    if args.always_gpu:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef vae_device():\n    if args.vae_in_cpu:\n        return torch.device(\"cpu\")\n    return get_torch_device()\n\ndef vae_offload_device():\n    if args.always_gpu:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef vae_dtype():\n    global VAE_DTYPE\n    return VAE_DTYPE\n\ndef get_autocast_device(dev):\n    if hasattr(dev, 'type'):\n        return dev.type\n    return \"cuda\"\n\ndef supports_dtype(device, dtype): #TODO\n    if dtype == torch.float32:\n        return True\n    if is_device_cpu(device):\n        return False\n    if dtype == torch.float16:\n        return True\n    if dtype == torch.bfloat16:\n        return True\n    return False\n\ndef device_supports_non_blocking(device):\n    if is_device_mps(device):\n        return False #pytorch bug? mps doesn't support non blocking\n    return True\n\ndef cast_to_device(tensor, device, dtype, copy=False):\n    device_supports_cast = False\n    if tensor.dtype == torch.float32 or tensor.dtype == torch.float16:\n        device_supports_cast = True\n    elif tensor.dtype == torch.bfloat16:\n        if hasattr(device, 'type') and device.type.startswith(\"cuda\"):\n            device_supports_cast = True\n        elif is_intel_xpu():\n            device_supports_cast = True\n\n    non_blocking = device_supports_non_blocking(device)\n\n    if device_supports_cast:\n        if copy:\n            if tensor.device == device:\n                return tensor.to(dtype, copy=copy, non_blocking=non_blocking)\n            return tensor.to(device, copy=copy, non_blocking=non_blocking).to(dtype, non_blocking=non_blocking)\n        else:\n            return tensor.to(device, non_blocking=non_blocking).to(dtype, non_blocking=non_blocking)\n    else:\n        return tensor.to(device, dtype, copy=copy, non_blocking=non_blocking)\n\ndef xformers_enabled():\n    global directml_enabled\n    global cpu_state\n    if cpu_state != CPUState.GPU:\n        return False\n    if is_intel_xpu():\n        return False\n    if directml_enabled:\n        return False\n    return XFORMERS_IS_AVAILABLE\n\n\ndef xformers_enabled_vae():\n    enabled = xformers_enabled()\n    if not enabled:\n        return False\n\n    return XFORMERS_ENABLED_VAE\n\ndef pytorch_attention_enabled():\n    global ENABLE_PYTORCH_ATTENTION\n    return ENABLE_PYTORCH_ATTENTION\n\ndef pytorch_attention_flash_attention():\n    global ENABLE_PYTORCH_ATTENTION\n    if ENABLE_PYTORCH_ATTENTION:\n        #TODO: more reliable way of checking for flash attention?\n        if is_nvidia(): #pytorch flash attention only works on Nvidia\n            return True\n    return False\n\ndef get_free_memory(dev=None, torch_free_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_free_total = psutil.virtual_memory().available\n        mem_free_torch = mem_free_total\n    else:\n        if directml_enabled:\n            mem_free_total = 1024 * 1024 * 1024 #TODO\n            mem_free_torch = mem_free_total\n        elif is_intel_xpu():\n            stats = torch.xpu.memory_stats(dev)\n            mem_active = stats['active_bytes.all.current']\n            mem_allocated = stats['allocated_bytes.all.current']\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_free_torch = mem_reserved - mem_active\n            mem_free_total = torch.xpu.get_device_properties(dev).total_memory - mem_allocated\n        else:\n            stats = torch.cuda.memory_stats(dev)\n            mem_active = stats['active_bytes.all.current']\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_free_cuda, _ = torch.cuda.mem_get_info(dev)\n            mem_free_torch = mem_reserved - mem_active\n            mem_free_total = mem_free_cuda + mem_free_torch\n\n    if torch_free_too:\n        return (mem_free_total, mem_free_torch)\n    else:\n        return mem_free_total\n\ndef cpu_mode():\n    global cpu_state\n    return cpu_state == CPUState.CPU\n\ndef mps_mode():\n    global cpu_state\n    return cpu_state == CPUState.MPS\n\ndef is_device_cpu(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'cpu'):\n            return True\n    return False\n\ndef is_device_mps(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'mps'):\n            return True\n    return False\n\ndef should_use_fp16(device=None, model_params=0, prioritize_performance=True):\n    global directml_enabled\n\n    if device is not None:\n        if is_device_cpu(device):\n            return False\n\n    if FORCE_FP16:\n        return True\n\n    if device is not None: #TODO\n        if is_device_mps(device):\n            return False\n\n    if FORCE_FP32:\n        return False\n\n    if directml_enabled:\n        return False\n\n    if cpu_mode() or mps_mode():\n        return False #TODO ?\n\n    if is_intel_xpu():\n        return True\n\n    if torch.cuda.is_bf16_supported():\n        return True\n\n    props = torch.cuda.get_device_properties(\"cuda\")\n    if props.major < 6:\n        return False\n\n    fp16_works = False\n    #FP16 is confirmed working on a 1080 (GP104) but it's a bit slower than FP32 so it should only be enabled\n    #when the model doesn't actually fit on the card\n    #TODO: actually test if GP106 and others have the same type of behavior\n    nvidia_10_series = [\"1080\", \"1070\", \"titan x\", \"p3000\", \"p3200\", \"p4000\", \"p4200\", \"p5000\", \"p5200\", \"p6000\", \"1060\", \"1050\"]\n    for x in nvidia_10_series:\n        if x in props.name.lower():\n            fp16_works = True\n\n    if fp16_works:\n        free_model_memory = (get_free_memory() * 0.9 - minimum_inference_memory())\n        if (not prioritize_performance) or model_params * 4 > free_model_memory:\n            return True\n\n    if props.major < 7:\n        return False\n\n    #FP16 is just broken on these cards\n    nvidia_16_series = [\"1660\", \"1650\", \"1630\", \"T500\", \"T550\", \"T600\", \"MX550\", \"MX450\", \"CMP 30HX\", \"T2000\", \"T1000\", \"T1200\"]\n    for x in nvidia_16_series:\n        if x in props.name:\n            return False\n\n    return True\n\ndef soft_empty_cache(force=False):\n    global cpu_state\n    if cpu_state == CPUState.MPS:\n        torch.mps.empty_cache()\n    elif is_intel_xpu():\n        torch.xpu.empty_cache()\n    elif torch.cuda.is_available():\n        if force or is_nvidia(): #This seems to make things worse on ROCm so I only do it for cuda\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\ndef unload_all_models():\n    free_memory(1e30, get_torch_device())\n\n\ndef resolve_lowvram_weight(weight, model, key): #TODO: remove\n    return weight\n\n#TODO: might be cleaner to put this somewhere else\nimport threading\n\nclass InterruptProcessingException(Exception):\n    pass\n\ninterrupt_processing_mutex = threading.RLock()\n\ninterrupt_processing = False\ndef interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\n\ndef processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        return interrupt_processing\n\ndef throw_exception_if_processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        if interrupt_processing:\n            interrupt_processing = False\n            raise InterruptProcessingException()\n", "ldm_patched/modules/latent_formats.py": "import torch\n\nclass LatentFormat:\n    scale_factor = 1.0\n    latent_rgb_factors = None\n    taesd_decoder_name = None\n\n    def process_in(self, latent):\n        return latent * self.scale_factor\n\n    def process_out(self, latent):\n        return latent / self.scale_factor\n\nclass SD15(LatentFormat):\n    def __init__(self, scale_factor=0.18215):\n        self.scale_factor = scale_factor\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3512,  0.2297,  0.3227],\n                    [ 0.3250,  0.4974,  0.2350],\n                    [-0.2829,  0.1762,  0.2721],\n                    [-0.2120, -0.2616, -0.7177]\n                ]\n        self.taesd_decoder_name = \"taesd_decoder\"\n\nclass SDXL(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.13025\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3920,  0.4054,  0.4549],\n                    [-0.2634, -0.0196,  0.0653],\n                    [ 0.0568,  0.1687, -0.0755],\n                    [-0.3112, -0.2359, -0.2076]\n                ]\n        self.taesd_decoder_name = \"taesdxl_decoder\"\n\nclass SDXL_Playground_2_5(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.5\n        self.latents_mean = torch.tensor([-1.6574, 1.886, -1.383, 2.5155]).view(1, 4, 1, 1)\n        self.latents_std = torch.tensor([8.4927, 5.9022, 6.5498, 5.2299]).view(1, 4, 1, 1)\n\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3920,  0.4054,  0.4549],\n                    [-0.2634, -0.0196,  0.0653],\n                    [ 0.0568,  0.1687, -0.0755],\n                    [-0.3112, -0.2359, -0.2076]\n                ]\n        self.taesd_decoder_name = \"taesdxl_decoder\"\n\n    def process_in(self, latent):\n        latents_mean = self.latents_mean.to(latent.device, latent.dtype)\n        latents_std = self.latents_std.to(latent.device, latent.dtype)\n        return (latent - latents_mean) * self.scale_factor / latents_std\n\n    def process_out(self, latent):\n        latents_mean = self.latents_mean.to(latent.device, latent.dtype)\n        latents_std = self.latents_std.to(latent.device, latent.dtype)\n        return latent * latents_std / self.scale_factor + latents_mean\n\n\nclass SD_X4(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.08333\n        self.latent_rgb_factors = [\n            [-0.2340, -0.3863, -0.3257],\n            [ 0.0994,  0.0885, -0.0908],\n            [-0.2833, -0.2349, -0.3741],\n            [ 0.2523, -0.0055, -0.1651]\n        ]\n\nclass SC_Prior(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 1.0\n        self.latent_rgb_factors = [\n            [-0.0326, -0.0204, -0.0127],\n            [-0.1592, -0.0427,  0.0216],\n            [ 0.0873,  0.0638, -0.0020],\n            [-0.0602,  0.0442,  0.1304],\n            [ 0.0800, -0.0313, -0.1796],\n            [-0.0810, -0.0638, -0.1581],\n            [ 0.1791,  0.1180,  0.0967],\n            [ 0.0740,  0.1416,  0.0432],\n            [-0.1745, -0.1888, -0.1373],\n            [ 0.2412,  0.1577,  0.0928],\n            [ 0.1908,  0.0998,  0.0682],\n            [ 0.0209,  0.0365, -0.0092],\n            [ 0.0448, -0.0650, -0.1728],\n            [-0.1658, -0.1045, -0.1308],\n            [ 0.0542,  0.1545,  0.1325],\n            [-0.0352, -0.1672, -0.2541]\n        ]\n\nclass SC_B(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 1.0 / 0.43\n        self.latent_rgb_factors = [\n            [ 0.1121,  0.2006,  0.1023],\n            [-0.2093, -0.0222, -0.0195],\n            [-0.3087, -0.1535,  0.0366],\n            [ 0.0290, -0.1574, -0.4078]\n        ]", "ldm_patched/modules/diffusers_load.py": "import os\n\nimport ldm_patched.modules.sd\n\ndef first_file(path, filenames):\n    for f in filenames:\n        p = os.path.join(path, f)\n        if os.path.exists(p):\n            return p\n    return None\n\ndef load_diffusers(model_path, output_vae=True, output_clip=True, embedding_directory=None):\n    diffusion_model_names = [\"diffusion_pytorch_model.fp16.safetensors\", \"diffusion_pytorch_model.safetensors\", \"diffusion_pytorch_model.fp16.bin\", \"diffusion_pytorch_model.bin\"]\n    unet_path = first_file(os.path.join(model_path, \"unet\"), diffusion_model_names)\n    vae_path = first_file(os.path.join(model_path, \"vae\"), diffusion_model_names)\n\n    text_encoder_model_names = [\"model.fp16.safetensors\", \"model.safetensors\", \"pytorch_model.fp16.bin\", \"pytorch_model.bin\"]\n    text_encoder1_path = first_file(os.path.join(model_path, \"text_encoder\"), text_encoder_model_names)\n    text_encoder2_path = first_file(os.path.join(model_path, \"text_encoder_2\"), text_encoder_model_names)\n\n    text_encoder_paths = [text_encoder1_path]\n    if text_encoder2_path is not None:\n        text_encoder_paths.append(text_encoder2_path)\n\n    unet = ldm_patched.modules.sd.load_unet(unet_path)\n\n    clip = None\n    if output_clip:\n        clip = ldm_patched.modules.sd.load_clip(text_encoder_paths, embedding_directory=embedding_directory)\n\n    vae = None\n    if output_vae:\n        sd = ldm_patched.modules.utils.load_torch_file(vae_path)\n        vae = ldm_patched.modules.sd.VAE(sd=sd)\n\n    return (unet, clip, vae)\n", "ldm_patched/modules/supported_models.py": "import torch\nfrom . import model_base\nfrom . import utils\n\nfrom . import sd1_clip\nfrom . import sd2_clip\nfrom . import sdxl_clip\n\nfrom . import supported_models_base\nfrom . import latent_formats\n\nfrom . import diffusers_convert\n\nclass SD15(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    unet_extra_config = {\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    latent_format = latent_formats.SD15\n\n    def process_clip_state_dict(self, state_dict):\n        k = list(state_dict.keys())\n        for x in k:\n            if x.startswith(\"cond_stage_model.transformer.\") and not x.startswith(\"cond_stage_model.transformer.text_model.\"):\n                y = x.replace(\"cond_stage_model.transformer.\", \"cond_stage_model.transformer.text_model.\")\n                state_dict[y] = state_dict.pop(x)\n\n        if 'cond_stage_model.transformer.text_model.embeddings.position_ids' in state_dict:\n            ids = state_dict['cond_stage_model.transformer.text_model.embeddings.position_ids']\n            if ids.dtype == torch.float32:\n                state_dict['cond_stage_model.transformer.text_model.embeddings.position_ids'] = ids.round()\n\n        replace_prefix = {}\n        replace_prefix[\"cond_stage_model.\"] = \"cond_stage_model.clip_l.\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"clip_l.\": \"cond_stage_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def clip_target(self):\n        return supported_models_base.ClipTarget(sd1_clip.SD1Tokenizer, sd1_clip.SD1ClipModel)\n\nclass SD20(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    latent_format = latent_formats.SD15\n\n    def model_type(self, state_dict, prefix=\"\"):\n        if self.unet_config[\"in_channels\"] == 4: #SD2.0 inpainting models are not v prediction\n            k = \"{}output_blocks.11.1.transformer_blocks.0.norm1.bias\".format(prefix)\n            out = state_dict[k]\n            if torch.std(out, unbiased=False) > 0.09: # not sure how well this will actually work. I guess we will find out.\n                return model_base.ModelType.V_PREDICTION\n        return model_base.ModelType.EPS\n\n    def process_clip_state_dict(self, state_dict):\n        replace_prefix = {}\n        replace_prefix[\"conditioner.embedders.0.model.\"] = \"cond_stage_model.model.\" #SD2 in sgm format\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n        state_dict = utils.transformers_convert(state_dict, \"cond_stage_model.model.\", \"cond_stage_model.clip_h.transformer.text_model.\", 24)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        replace_prefix[\"clip_h\"] = \"cond_stage_model.model\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix)\n        state_dict = diffusers_convert.convert_text_enc_state_dict_v20(state_dict)\n        return state_dict\n\n    def clip_target(self):\n        return supported_models_base.ClipTarget(sd2_clip.SD2Tokenizer, sd2_clip.SD2ClipModel)\n\nclass SD21UnclipL(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 1536,\n        \"use_temporal_attention\": False,\n    }\n\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 768}\n\n\nclass SD21UnclipH(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 2048,\n        \"use_temporal_attention\": False,\n    }\n\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1024}\n\nclass SDXLRefiner(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 384,\n        \"use_linear_in_transformer\": True,\n        \"context_dim\": 1280,\n        \"adm_in_channels\": 2560,\n        \"transformer_depth\": [0, 0, 4, 4, 4, 4, 0, 0],\n        \"use_temporal_attention\": False,\n    }\n\n    latent_format = latent_formats.SDXL\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        return model_base.SDXLRefiner(self, device=device)\n\n    def process_clip_state_dict(self, state_dict):\n        keys_to_replace = {}\n        replace_prefix = {}\n\n        state_dict = utils.transformers_convert(state_dict, \"conditioner.embedders.0.model.\", \"cond_stage_model.clip_g.transformer.text_model.\", 32)\n        keys_to_replace[\"conditioner.embedders.0.model.text_projection\"] = \"cond_stage_model.clip_g.text_projection\"\n        keys_to_replace[\"conditioner.embedders.0.model.logit_scale\"] = \"cond_stage_model.clip_g.logit_scale\"\n\n        state_dict = utils.state_dict_key_replace(state_dict, keys_to_replace)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        state_dict_g = diffusers_convert.convert_text_enc_state_dict_v20(state_dict, \"clip_g\")\n        if \"clip_g.transformer.text_model.embeddings.position_ids\" in state_dict_g:\n            state_dict_g.pop(\"clip_g.transformer.text_model.embeddings.position_ids\")\n        replace_prefix[\"clip_g\"] = \"conditioner.embedders.0.model\"\n        state_dict_g = utils.state_dict_prefix_replace(state_dict_g, replace_prefix)\n        return state_dict_g\n\n    def clip_target(self):\n        return supported_models_base.ClipTarget(sdxl_clip.SDXLTokenizer, sdxl_clip.SDXLRefinerClipModel)\n\nclass SDXL(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 10, 10],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\n    latent_format = latent_formats.SDXL\n\n    def model_type(self, state_dict, prefix=\"\"):\n        if \"v_pred\" in state_dict:\n            return model_base.ModelType.V_PREDICTION\n        else:\n            return model_base.ModelType.EPS\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SDXL(self, model_type=self.model_type(state_dict, prefix), device=device)\n        if self.inpaint_model():\n            out.set_inpaint()\n        return out\n\n    def process_clip_state_dict(self, state_dict):\n        keys_to_replace = {}\n        replace_prefix = {}\n\n        replace_prefix[\"conditioner.embedders.0.transformer.text_model\"] = \"cond_stage_model.clip_l.transformer.text_model\"\n        state_dict = utils.transformers_convert(state_dict, \"conditioner.embedders.1.model.\", \"cond_stage_model.clip_g.transformer.text_model.\", 32)\n        keys_to_replace[\"conditioner.embedders.1.model.text_projection\"] = \"cond_stage_model.clip_g.text_projection\"\n        keys_to_replace[\"conditioner.embedders.1.model.text_projection.weight\"] = \"cond_stage_model.clip_g.text_projection\"\n        keys_to_replace[\"conditioner.embedders.1.model.logit_scale\"] = \"cond_stage_model.clip_g.logit_scale\"\n\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix)\n        state_dict = utils.state_dict_key_replace(state_dict, keys_to_replace)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        keys_to_replace = {}\n        state_dict_g = diffusers_convert.convert_text_enc_state_dict_v20(state_dict, \"clip_g\")\n        if \"clip_g.transformer.text_model.embeddings.position_ids\" in state_dict_g:\n            state_dict_g.pop(\"clip_g.transformer.text_model.embeddings.position_ids\")\n        for k in state_dict:\n            if k.startswith(\"clip_l\"):\n                state_dict_g[k] = state_dict[k]\n\n        replace_prefix[\"clip_g\"] = \"conditioner.embedders.1.model\"\n        replace_prefix[\"clip_l\"] = \"conditioner.embedders.0\"\n        state_dict_g = utils.state_dict_prefix_replace(state_dict_g, replace_prefix)\n        return state_dict_g\n\n    def clip_target(self):\n        return supported_models_base.ClipTarget(sdxl_clip.SDXLTokenizer, sdxl_clip.SDXLClipModel)\n\nclass SSD1B(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 4, 4],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass Segmind_Vega(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 1, 1, 2, 2],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass SVD_img2vid(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 320,\n        \"in_channels\": 8,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [1, 1, 1, 1, 1, 1, 0, 0],\n        \"context_dim\": 1024,\n        \"adm_in_channels\": 768,\n        \"use_temporal_attention\": True,\n        \"use_temporal_resblock\": True\n    }\n\n    clip_vision_prefix = \"conditioner.embedders.0.open_clip.model.visual.\"\n\n    latent_format = latent_formats.SD15\n\n    sampling_settings = {\"sigma_max\": 700.0, \"sigma_min\": 0.002}\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SVD_img2vid(self, device=device)\n        return out\n\n    def clip_target(self):\n        return None\n\nclass Stable_Zero123(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n        \"in_channels\": 8,\n    }\n\n    unet_extra_config = {\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    clip_vision_prefix = \"cond_stage_model.model.visual.\"\n\n    latent_format = latent_formats.SD15\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.Stable_Zero123(self, device=device, cc_projection_weight=state_dict[\"cc_projection.weight\"], cc_projection_bias=state_dict[\"cc_projection.bias\"])\n        return out\n\n    def clip_target(self):\n        return None\n\nclass SD_X4Upscaler(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 256,\n        'in_channels': 7,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    unet_extra_config = {\n        \"disable_self_attentions\": [True, True, True, False],\n        \"num_classes\": 1000,\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    latent_format = latent_formats.SD_X4\n\n    sampling_settings = {\n        \"linear_start\": 0.0001,\n        \"linear_end\": 0.02,\n    }\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SD_X4Upscaler(self, device=device)\n        return out\n\nmodels = [Stable_Zero123, SD15, SD20, SD21UnclipL, SD21UnclipH, SDXLRefiner, SDXL, SSD1B, Segmind_Vega, SD_X4Upscaler]\nmodels += [SVD_img2vid]\n", "modules/localization.py": "import json\nimport os\n\n\ncurrent_translation = {}\nlocalization_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'language')\n\n\ndef localization_js(filename):\n    global current_translation\n\n    if isinstance(filename, str):\n        full_name = os.path.abspath(os.path.join(localization_root, filename + '.json'))\n        if os.path.exists(full_name):\n            try:\n                with open(full_name, encoding='utf-8') as f:\n                    current_translation = json.load(f)\n                    assert isinstance(current_translation, dict)\n                    for k, v in current_translation.items():\n                        assert isinstance(k, str)\n                        assert isinstance(v, str)\n            except Exception as e:\n                print(str(e))\n                print(f'Failed to load localization file {full_name}')\n\n    # current_translation = {k: 'XXX' for k in current_translation.keys()}  # use this to see if all texts are covered\n\n    return f\"window.localization = {json.dumps(current_translation)}\"\n\n\ndef dump_english_config(components):\n    all_texts = []\n    for c in components:\n        label = getattr(c, 'label', None)\n        value = getattr(c, 'value', None)\n        choices = getattr(c, 'choices', None)\n        info = getattr(c, 'info', None)\n\n        if isinstance(label, str):\n            all_texts.append(label)\n        if isinstance(value, str):\n            all_texts.append(value)\n        if isinstance(info, str):\n            all_texts.append(info)\n        if isinstance(choices, list):\n            for x in choices:\n                if isinstance(x, str):\n                    all_texts.append(x)\n                if isinstance(x, tuple):\n                    for y in x:\n                        if isinstance(y, str):\n                            all_texts.append(y)\n\n    config_dict = {k: k for k in all_texts if k != \"\" and 'progress-container' not in k}\n    full_name = os.path.abspath(os.path.join(localization_root, 'en.json'))\n\n    with open(full_name, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(config_dict, json_file, indent=4)\n\n    return\n", "modules/flags.py": "from enum import IntEnum, Enum\n\ndisabled = 'Disabled'\nenabled = 'Enabled'\nsubtle_variation = 'Vary (Subtle)'\nstrong_variation = 'Vary (Strong)'\nupscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\n\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\n\nCIVITAI_NO_KARRAS = [\"euler\", \"euler_ancestral\", \"heun\", \"dpm_fast\", \"dpm_adaptive\", \"ddim\", \"uni_pc\"]\n\n# fooocus: a1111 (Civitai)\nKSAMPLER = {\n    \"euler\": \"Euler\",\n    \"euler_ancestral\": \"Euler a\",\n    \"heun\": \"Heun\",\n    \"heunpp2\": \"\",\n    \"dpm_2\": \"DPM2\",\n    \"dpm_2_ancestral\": \"DPM2 a\",\n    \"lms\": \"LMS\",\n    \"dpm_fast\": \"DPM fast\",\n    \"dpm_adaptive\": \"DPM adaptive\",\n    \"dpmpp_2s_ancestral\": \"DPM++ 2S a\",\n    \"dpmpp_sde\": \"DPM++ SDE\",\n    \"dpmpp_sde_gpu\": \"DPM++ SDE\",\n    \"dpmpp_2m\": \"DPM++ 2M\",\n    \"dpmpp_2m_sde\": \"DPM++ 2M SDE\",\n    \"dpmpp_2m_sde_gpu\": \"DPM++ 2M SDE\",\n    \"dpmpp_3m_sde\": \"\",\n    \"dpmpp_3m_sde_gpu\": \"\",\n    \"ddpm\": \"\",\n    \"lcm\": \"LCM\",\n    \"tcd\": \"TCD\"\n}\n\nSAMPLER_EXTRA = {\n    \"ddim\": \"DDIM\",\n    \"uni_pc\": \"UniPC\",\n    \"uni_pc_bh2\": \"\"\n}\n\nSAMPLERS = KSAMPLER | SAMPLER_EXTRA\n\nKSAMPLER_NAMES = list(KSAMPLER.keys())\n\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\", \"turbo\", \"align_your_steps\", \"tcd\", \"edm_playground_v2.5\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + list(SAMPLER_EXTRA.keys())\n\nsampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES\n\nclip_skip_max = 12\n\ndefault_vae = 'Default (model)'\n\nrefiner_swap_method = 'joint'\n\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\n\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\n\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\n\noutput_formats = ['png', 'jpeg', 'webp']\n\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]\n\ndesc_type_photo = 'Photograph'\ndesc_type_anime = 'Art/Anime'\n\nsdxl_aspect_ratios = [\n    '704*1408', '704*1344', '768*1344', '768*1280', '832*1216', '832*1152',\n    '896*1152', '896*1088', '960*1088', '960*1024', '1024*1024', '1024*960',\n    '1088*960', '1088*896', '1152*896', '1152*832', '1216*832', '1280*768',\n    '1344*768', '1344*704', '1408*704', '1472*704', '1536*640', '1600*640',\n    '1664*576', '1728*576'\n]\n\n\nclass MetadataScheme(Enum):\n    FOOOCUS = 'fooocus'\n    A1111 = 'a1111'\n\n\nmetadata_scheme = [\n    (f'{MetadataScheme.FOOOCUS.value} (json)', MetadataScheme.FOOOCUS.value),\n    (f'{MetadataScheme.A1111.value} (plain text)', MetadataScheme.A1111.value),\n]\n\ncontrolnet_image_count = 4\npreparation_step_count = 13\n\n\nclass OutputFormat(Enum):\n    PNG = 'png'\n    JPEG = 'jpeg'\n    WEBP = 'webp'\n\n    @classmethod\n    def list(cls) -> list:\n        return list(map(lambda c: c.value, cls))\n\n\nclass PerformanceLoRA(Enum):\n    QUALITY = None\n    SPEED = None\n    EXTREME_SPEED = 'sdxl_lcm_lora.safetensors'\n    LIGHTNING = 'sdxl_lightning_4step_lora.safetensors'\n    HYPER_SD = 'sdxl_hyper_sd_4step_lora.safetensors'\n\n\nclass Steps(IntEnum):\n    QUALITY = 60\n    SPEED = 30\n    EXTREME_SPEED = 8\n    LIGHTNING = 4\n    HYPER_SD = 4\n\n    @classmethod\n    def keys(cls) -> list:\n        return list(map(lambda c: c, Steps.__members__))\n\n\nclass StepsUOV(IntEnum):\n    QUALITY = 36\n    SPEED = 18\n    EXTREME_SPEED = 8\n    LIGHTNING = 4\n    HYPER_SD = 4\n\n\nclass Performance(Enum):\n    QUALITY = 'Quality'\n    SPEED = 'Speed'\n    EXTREME_SPEED = 'Extreme Speed'\n    LIGHTNING = 'Lightning'\n    HYPER_SD = 'Hyper-SD'\n\n    @classmethod\n    def list(cls) -> list:\n        return list(map(lambda c: c.value, cls))\n\n    @classmethod\n    def values(cls) -> list:\n        return list(map(lambda c: c.value, cls))\n\n    @classmethod\n    def by_steps(cls, steps: int | str):\n        return cls[Steps(int(steps)).name]\n\n    @classmethod\n    def has_restricted_features(cls, x) -> bool:\n        if isinstance(x, Performance):\n            x = x.value\n        return x in [cls.EXTREME_SPEED.value, cls.LIGHTNING.value, cls.HYPER_SD.value]\n\n    def steps(self) -> int | None:\n        return Steps[self.name].value if self.name in Steps.__members__ else None\n\n    def steps_uov(self) -> int | None:\n        return StepsUOV[self.name].value if self.name in StepsUOV.__members__ else None\n\n    def lora_filename(self) -> str | None:\n        return PerformanceLoRA[self.name].value if self.name in PerformanceLoRA.__members__ else None\n", "modules/async_worker.py": "import threading\nimport re\nfrom modules.patch import PatchSettings, patch_settings, patch_all\n\npatch_all()\n\n\nclass AsyncTask:\n    def __init__(self, args):\n        self.args = args\n        self.yields = []\n        self.results = []\n        self.last_stop = False\n        self.processing = False\n\n\nasync_tasks = []\n\n\ndef worker():\n    global async_tasks\n\n    import os\n    import traceback\n    import math\n    import numpy as np\n    import cv2\n    import torch\n    import time\n    import shared\n    import random\n    import copy\n    import modules.default_pipeline as pipeline\n    import modules.core as core\n    import modules.flags as flags\n    import modules.config\n    import modules.patch\n    import ldm_patched.modules.model_management\n    import extras.preprocessors as preprocessors\n    import modules.inpaint_worker as inpaint_worker\n    import modules.constants as constants\n    import extras.ip_adapter as ip_adapter\n    import extras.face_crop\n    import fooocus_version\n    import args_manager\n\n    from extras.censor import default_censor\n    from modules.sdxl_styles import apply_style, get_random_style, fooocus_expansion, apply_arrays, random_style_name\n    from modules.private_logger import log\n    from extras.expansion import safe_str\n    from modules.util import (remove_empty_str, HWC3, resize_image, get_image_shape_ceil, set_image_shape_ceil,\n                              get_shape_ceil, resample_image, erode_or_dilate, get_enabled_loras,\n                              parse_lora_references_from_prompt, apply_wildcards)\n    from modules.upscaler import perform_upscale\n    from modules.flags import Performance\n    from modules.meta_parser import get_metadata_parser, MetadataScheme\n\n    pid = os.getpid()\n    print(f'Started worker with PID {pid}')\n\n    try:\n        async_gradio_app = shared.gradio_root\n        flag = f'''App started successful. Use the app with {str(async_gradio_app.local_url)} or {str(async_gradio_app.server_name)}:{str(async_gradio_app.server_port)}'''\n        if async_gradio_app.share:\n            flag += f''' or {async_gradio_app.share_url}'''\n        print(flag)\n    except Exception as e:\n        print(e)\n\n    def progressbar(async_task, number, text):\n        print(f'[Fooocus] {text}')\n        async_task.yields.append(['preview', (number, text, None)])\n\n    def yield_result(async_task, imgs, black_out_nsfw, censor=True, do_not_show_finished_images=False,\n                     progressbar_index=flags.preparation_step_count):\n        if not isinstance(imgs, list):\n            imgs = [imgs]\n\n        if censor and (modules.config.default_black_out_nsfw or black_out_nsfw):\n            progressbar(async_task, progressbar_index, 'Checking for NSFW content ...')\n            imgs = default_censor(imgs)\n\n        async_task.results = async_task.results + imgs\n\n        if do_not_show_finished_images:\n            return\n\n        async_task.yields.append(['results', async_task.results])\n        return\n\n    def build_image_wall(async_task):\n        results = []\n\n        if len(async_task.results) < 2:\n            return\n\n        for img in async_task.results:\n            if isinstance(img, str) and os.path.exists(img):\n                img = cv2.imread(img)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if not isinstance(img, np.ndarray):\n                return\n            if img.ndim != 3:\n                return\n            results.append(img)\n\n        H, W, C = results[0].shape\n\n        for img in results:\n            Hn, Wn, Cn = img.shape\n            if H != Hn:\n                return\n            if W != Wn:\n                return\n            if C != Cn:\n                return\n\n        cols = float(len(results)) ** 0.5\n        cols = int(math.ceil(cols))\n        rows = float(len(results)) / float(cols)\n        rows = int(math.ceil(rows))\n\n        wall = np.zeros(shape=(H * rows, W * cols, C), dtype=np.uint8)\n\n        for y in range(rows):\n            for x in range(cols):\n                if y * cols + x < len(results):\n                    img = results[y * cols + x]\n                    wall[y * H:y * H + H, x * W:x * W + W, :] = img\n\n        # must use deep copy otherwise gradio is super laggy. Do not use list.append() .\n        async_task.results = async_task.results + [wall]\n        return\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def handler(async_task):\n        execution_start_time = time.perf_counter()\n        async_task.processing = True\n\n        args = async_task.args\n        args.reverse()\n\n        prompt = args.pop()\n        negative_prompt = args.pop()\n        style_selections = args.pop()\n        performance_selection = Performance(args.pop())\n        aspect_ratios_selection = args.pop()\n        image_number = args.pop()\n        output_format = args.pop()\n        image_seed = args.pop()\n        read_wildcards_in_order = args.pop()\n        sharpness = args.pop()\n        guidance_scale = args.pop()\n        base_model_name = args.pop()\n        refiner_model_name = args.pop()\n        refiner_switch = args.pop()\n        loras = get_enabled_loras([(bool(args.pop()), str(args.pop()), float(args.pop())) for _ in\n                                   range(modules.config.default_max_lora_number)])\n        input_image_checkbox = args.pop()\n        current_tab = args.pop()\n        uov_method = args.pop()\n        uov_input_image = args.pop()\n        outpaint_selections = args.pop()\n        inpaint_input_image = args.pop()\n        inpaint_additional_prompt = args.pop()\n        inpaint_mask_image_upload = args.pop()\n\n        disable_preview = args.pop()\n        disable_intermediate_results = args.pop()\n        disable_seed_increment = args.pop()\n        black_out_nsfw = args.pop()\n        adm_scaler_positive = args.pop()\n        adm_scaler_negative = args.pop()\n        adm_scaler_end = args.pop()\n        adaptive_cfg = args.pop()\n        clip_skip = args.pop()\n        sampler_name = args.pop()\n        scheduler_name = args.pop()\n        vae_name = args.pop()\n        overwrite_step = args.pop()\n        overwrite_switch = args.pop()\n        overwrite_width = args.pop()\n        overwrite_height = args.pop()\n        overwrite_vary_strength = args.pop()\n        overwrite_upscale_strength = args.pop()\n        mixing_image_prompt_and_vary_upscale = args.pop()\n        mixing_image_prompt_and_inpaint = args.pop()\n        debugging_cn_preprocessor = args.pop()\n        skipping_cn_preprocessor = args.pop()\n        canny_low_threshold = args.pop()\n        canny_high_threshold = args.pop()\n        refiner_swap_method = args.pop()\n        controlnet_softness = args.pop()\n        freeu_enabled = args.pop()\n        freeu_b1 = args.pop()\n        freeu_b2 = args.pop()\n        freeu_s1 = args.pop()\n        freeu_s2 = args.pop()\n        debugging_inpaint_preprocessor = args.pop()\n        inpaint_disable_initial_latent = args.pop()\n        inpaint_engine = args.pop()\n        inpaint_strength = args.pop()\n        inpaint_respective_field = args.pop()\n        inpaint_mask_upload_checkbox = args.pop()\n        invert_mask_checkbox = args.pop()\n        inpaint_erode_or_dilate = args.pop()\n\n        save_metadata_to_images = args.pop() if not args_manager.args.disable_metadata else False\n        metadata_scheme = MetadataScheme(\n            args.pop()) if not args_manager.args.disable_metadata else MetadataScheme.FOOOCUS\n\n        cn_tasks = {x: [] for x in flags.ip_list}\n        for _ in range(flags.controlnet_image_count):\n            cn_img = args.pop()\n            cn_stop = args.pop()\n            cn_weight = args.pop()\n            cn_type = args.pop()\n            if cn_img is not None:\n                cn_tasks[cn_type].append([cn_img, cn_stop, cn_weight])\n\n        outpaint_selections = [o.lower() for o in outpaint_selections]\n        base_model_additional_loras = []\n        raw_style_selections = copy.deepcopy(style_selections)\n        uov_method = uov_method.lower()\n\n        if fooocus_expansion in style_selections:\n            use_expansion = True\n            style_selections.remove(fooocus_expansion)\n        else:\n            use_expansion = False\n\n        use_style = len(style_selections) > 0\n\n        if base_model_name == refiner_model_name:\n            print(f'Refiner disabled because base model and refiner are same.')\n            refiner_model_name = 'None'\n\n        steps = performance_selection.steps()\n\n        performance_loras = []\n\n        if performance_selection == Performance.EXTREME_SPEED:\n            print('Enter LCM mode.')\n            progressbar(async_task, 1, 'Downloading LCM components ...')\n            performance_loras += [(modules.config.downloading_sdxl_lcm_lora(), 1.0)]\n\n            if refiner_model_name != 'None':\n                print(f'Refiner disabled in LCM mode.')\n\n            refiner_model_name = 'None'\n            sampler_name = 'lcm'\n            scheduler_name = 'lcm'\n            sharpness = 0.0\n            guidance_scale = 1.0\n            adaptive_cfg = 1.0\n            refiner_switch = 1.0\n            adm_scaler_positive = 1.0\n            adm_scaler_negative = 1.0\n            adm_scaler_end = 0.0\n\n        elif performance_selection == Performance.LIGHTNING:\n            print('Enter Lightning mode.')\n            progressbar(async_task, 1, 'Downloading Lightning components ...')\n            performance_loras += [(modules.config.downloading_sdxl_lightning_lora(), 1.0)]\n\n            if refiner_model_name != 'None':\n                print(f'Refiner disabled in Lightning mode.')\n\n            refiner_model_name = 'None'\n            sampler_name = 'euler'\n            scheduler_name = 'sgm_uniform'\n            sharpness = 0.0\n            guidance_scale = 1.0\n            adaptive_cfg = 1.0\n            refiner_switch = 1.0\n            adm_scaler_positive = 1.0\n            adm_scaler_negative = 1.0\n            adm_scaler_end = 0.0\n\n        elif performance_selection == Performance.HYPER_SD:\n            print('Enter Hyper-SD mode.')\n            progressbar(async_task, 1, 'Downloading Hyper-SD components ...')\n            performance_loras += [(modules.config.downloading_sdxl_hyper_sd_lora(), 0.8)]\n\n            if refiner_model_name != 'None':\n                print(f'Refiner disabled in Hyper-SD mode.')\n\n            refiner_model_name = 'None'\n            sampler_name = 'dpmpp_sde_gpu'\n            scheduler_name = 'karras'\n            sharpness = 0.0\n            guidance_scale = 1.0\n            adaptive_cfg = 1.0\n            refiner_switch = 1.0\n            adm_scaler_positive = 1.0\n            adm_scaler_negative = 1.0\n            adm_scaler_end = 0.0\n\n        print(f'[Parameters] Adaptive CFG = {adaptive_cfg}')\n        print(f'[Parameters] CLIP Skip = {clip_skip}')\n        print(f'[Parameters] Sharpness = {sharpness}')\n        print(f'[Parameters] ControlNet Softness = {controlnet_softness}')\n        print(f'[Parameters] ADM Scale = '\n              f'{adm_scaler_positive} : '\n              f'{adm_scaler_negative} : '\n              f'{adm_scaler_end}')\n\n        patch_settings[pid] = PatchSettings(\n            sharpness,\n            adm_scaler_end,\n            adm_scaler_positive,\n            adm_scaler_negative,\n            controlnet_softness,\n            adaptive_cfg\n        )\n\n        cfg_scale = float(guidance_scale)\n        print(f'[Parameters] CFG = {cfg_scale}')\n\n        initial_latent = None\n        denoising_strength = 1.0\n        tiled = False\n\n        width, height = aspect_ratios_selection.replace('\u00d7', ' ').split(' ')[:2]\n        width, height = int(width), int(height)\n\n        skip_prompt_processing = False\n\n        inpaint_worker.current_task = None\n        inpaint_parameterized = inpaint_engine != 'None'\n        inpaint_image = None\n        inpaint_mask = None\n        inpaint_head_model_path = None\n\n        use_synthetic_refiner = False\n\n        controlnet_canny_path = None\n        controlnet_cpds_path = None\n        clip_vision_path, ip_negative_path, ip_adapter_path, ip_adapter_face_path = None, None, None, None\n\n        seed = int(image_seed)\n        print(f'[Parameters] Seed = {seed}')\n\n        goals = []\n        tasks = []\n\n        if input_image_checkbox:\n            if (current_tab == 'uov' or (\n                    current_tab == 'ip' and mixing_image_prompt_and_vary_upscale)) \\\n                    and uov_method != flags.disabled and uov_input_image is not None:\n                uov_input_image = HWC3(uov_input_image)\n                if 'vary' in uov_method:\n                    goals.append('vary')\n                elif 'upscale' in uov_method:\n                    goals.append('upscale')\n                    if 'fast' in uov_method:\n                        skip_prompt_processing = True\n                    else:\n                        steps = performance_selection.steps_uov()\n\n                    progressbar(async_task, 1, 'Downloading upscale models ...')\n                    modules.config.downloading_upscale_model()\n            if (current_tab == 'inpaint' or (\n                    current_tab == 'ip' and mixing_image_prompt_and_inpaint)) \\\n                    and isinstance(inpaint_input_image, dict):\n                inpaint_image = inpaint_input_image['image']\n                inpaint_mask = inpaint_input_image['mask'][:, :, 0]\n\n                if inpaint_mask_upload_checkbox:\n                    if isinstance(inpaint_mask_image_upload, np.ndarray):\n                        if inpaint_mask_image_upload.ndim == 3:\n                            H, W, C = inpaint_image.shape\n                            inpaint_mask_image_upload = resample_image(inpaint_mask_image_upload, width=W, height=H)\n                            inpaint_mask_image_upload = np.mean(inpaint_mask_image_upload, axis=2)\n                            inpaint_mask_image_upload = (inpaint_mask_image_upload > 127).astype(np.uint8) * 255\n                            inpaint_mask = np.maximum(inpaint_mask, inpaint_mask_image_upload)\n\n                if int(inpaint_erode_or_dilate) != 0:\n                    inpaint_mask = erode_or_dilate(inpaint_mask, inpaint_erode_or_dilate)\n\n                if invert_mask_checkbox:\n                    inpaint_mask = 255 - inpaint_mask\n\n                inpaint_image = HWC3(inpaint_image)\n                if isinstance(inpaint_image, np.ndarray) and isinstance(inpaint_mask, np.ndarray) \\\n                        and (np.any(inpaint_mask > 127) or len(outpaint_selections) > 0):\n                    progressbar(async_task, 1, 'Downloading upscale models ...')\n                    modules.config.downloading_upscale_model()\n                    if inpaint_parameterized:\n                        progressbar(async_task, 1, 'Downloading inpainter ...')\n                        inpaint_head_model_path, inpaint_patch_model_path = modules.config.downloading_inpaint_models(\n                            inpaint_engine)\n                        base_model_additional_loras += [(inpaint_patch_model_path, 1.0)]\n                        print(f'[Inpaint] Current inpaint model is {inpaint_patch_model_path}')\n                        if refiner_model_name == 'None':\n                            use_synthetic_refiner = True\n                            refiner_switch = 0.8\n                    else:\n                        inpaint_head_model_path, inpaint_patch_model_path = None, None\n                        print(f'[Inpaint] Parameterized inpaint is disabled.')\n                    if inpaint_additional_prompt != '':\n                        if prompt == '':\n                            prompt = inpaint_additional_prompt\n                        else:\n                            prompt = inpaint_additional_prompt + '\\n' + prompt\n                    goals.append('inpaint')\n            if current_tab == 'ip' or \\\n                    mixing_image_prompt_and_vary_upscale or \\\n                    mixing_image_prompt_and_inpaint:\n                goals.append('cn')\n                progressbar(async_task, 1, 'Downloading control models ...')\n                if len(cn_tasks[flags.cn_canny]) > 0:\n                    controlnet_canny_path = modules.config.downloading_controlnet_canny()\n                if len(cn_tasks[flags.cn_cpds]) > 0:\n                    controlnet_cpds_path = modules.config.downloading_controlnet_cpds()\n                if len(cn_tasks[flags.cn_ip]) > 0:\n                    clip_vision_path, ip_negative_path, ip_adapter_path = modules.config.downloading_ip_adapters('ip')\n                if len(cn_tasks[flags.cn_ip_face]) > 0:\n                    clip_vision_path, ip_negative_path, ip_adapter_face_path = modules.config.downloading_ip_adapters(\n                        'face')\n                progressbar(async_task, 1, 'Loading control models ...')\n\n        # Load or unload CNs\n        pipeline.refresh_controlnets([controlnet_canny_path, controlnet_cpds_path])\n        ip_adapter.load_ip_adapter(clip_vision_path, ip_negative_path, ip_adapter_path)\n        ip_adapter.load_ip_adapter(clip_vision_path, ip_negative_path, ip_adapter_face_path)\n\n        if overwrite_step > 0:\n            steps = overwrite_step\n\n        switch = int(round(steps * refiner_switch))\n\n        if overwrite_switch > 0:\n            switch = overwrite_switch\n\n        if overwrite_width > 0:\n            width = overwrite_width\n\n        if overwrite_height > 0:\n            height = overwrite_height\n\n        print(f'[Parameters] Sampler = {sampler_name} - {scheduler_name}')\n        print(f'[Parameters] Steps = {steps} - {switch}')\n\n        progressbar(async_task, 1, 'Initializing ...')\n\n        if not skip_prompt_processing:\n\n            prompts = remove_empty_str([safe_str(p) for p in prompt.splitlines()], default='')\n            negative_prompts = remove_empty_str([safe_str(p) for p in negative_prompt.splitlines()], default='')\n\n            prompt = prompts[0]\n            negative_prompt = negative_prompts[0]\n\n            if prompt == '':\n                # disable expansion when empty since it is not meaningful and influences image prompt\n                use_expansion = False\n\n            extra_positive_prompts = prompts[1:] if len(prompts) > 1 else []\n            extra_negative_prompts = negative_prompts[1:] if len(negative_prompts) > 1 else []\n\n            progressbar(async_task, 2, 'Loading models ...')\n\n            lora_filenames = modules.util.remove_performance_lora(modules.config.lora_filenames, performance_selection)\n            loras, prompt = parse_lora_references_from_prompt(prompt, loras, modules.config.default_max_lora_number, lora_filenames=lora_filenames)\n            loras += performance_loras\n\n            pipeline.refresh_everything(refiner_model_name=refiner_model_name, base_model_name=base_model_name,\n                                        loras=loras, base_model_additional_loras=base_model_additional_loras,\n                                        use_synthetic_refiner=use_synthetic_refiner, vae_name=vae_name)\n\n            pipeline.set_clip_skip(clip_skip)\n\n            progressbar(async_task, 3, 'Processing prompts ...')\n            tasks = []\n\n            for i in range(image_number):\n                if disable_seed_increment:\n                    task_seed = seed % (constants.MAX_SEED + 1)\n                else:\n                    task_seed = (seed + i) % (constants.MAX_SEED + 1)  # randint is inclusive, % is not\n\n                task_rng = random.Random(task_seed)  # may bind to inpaint noise in the future\n                task_prompt = apply_wildcards(prompt, task_rng, i, read_wildcards_in_order)\n                task_prompt = apply_arrays(task_prompt, i)\n                task_negative_prompt = apply_wildcards(negative_prompt, task_rng, i, read_wildcards_in_order)\n                task_extra_positive_prompts = [apply_wildcards(pmt, task_rng, i, read_wildcards_in_order) for pmt in\n                                               extra_positive_prompts]\n                task_extra_negative_prompts = [apply_wildcards(pmt, task_rng, i, read_wildcards_in_order) for pmt in\n                                               extra_negative_prompts]\n\n                positive_basic_workloads = []\n                negative_basic_workloads = []\n\n                task_styles = style_selections.copy()\n                if use_style:\n                    for i, s in enumerate(task_styles):\n                        if s == random_style_name:\n                            s = get_random_style(task_rng)\n                            task_styles[i] = s\n                        p, n = apply_style(s, positive=task_prompt)\n                        positive_basic_workloads = positive_basic_workloads + p\n                        negative_basic_workloads = negative_basic_workloads + n\n                else:\n                    positive_basic_workloads.append(task_prompt)\n\n                negative_basic_workloads.append(task_negative_prompt)  # Always use independent workload for negative.\n\n                positive_basic_workloads = positive_basic_workloads + task_extra_positive_prompts\n                negative_basic_workloads = negative_basic_workloads + task_extra_negative_prompts\n\n                positive_basic_workloads = remove_empty_str(positive_basic_workloads, default=task_prompt)\n                negative_basic_workloads = remove_empty_str(negative_basic_workloads, default=task_negative_prompt)\n\n                tasks.append(dict(\n                    task_seed=task_seed,\n                    task_prompt=task_prompt,\n                    task_negative_prompt=task_negative_prompt,\n                    positive=positive_basic_workloads,\n                    negative=negative_basic_workloads,\n                    expansion='',\n                    c=None,\n                    uc=None,\n                    positive_top_k=len(positive_basic_workloads),\n                    negative_top_k=len(negative_basic_workloads),\n                    log_positive_prompt='\\n'.join([task_prompt] + task_extra_positive_prompts),\n                    log_negative_prompt='\\n'.join([task_negative_prompt] + task_extra_negative_prompts),\n                    styles=task_styles\n                ))\n\n            if use_expansion:\n                for i, t in enumerate(tasks):\n                    progressbar(async_task, 4, f'Preparing Fooocus text #{i + 1} ...')\n                    expansion = pipeline.final_expansion(t['task_prompt'], t['task_seed'])\n                    print(f'[Prompt Expansion] {expansion}')\n                    t['expansion'] = expansion\n                    t['positive'] = copy.deepcopy(t['positive']) + [expansion]  # Deep copy.\n\n            for i, t in enumerate(tasks):\n                progressbar(async_task, 5, f'Encoding positive #{i + 1} ...')\n                t['c'] = pipeline.clip_encode(texts=t['positive'], pool_top_k=t['positive_top_k'])\n\n            for i, t in enumerate(tasks):\n                if abs(float(cfg_scale) - 1.0) < 1e-4:\n                    t['uc'] = pipeline.clone_cond(t['c'])\n                else:\n                    progressbar(async_task, 6, f'Encoding negative #{i + 1} ...')\n                    t['uc'] = pipeline.clip_encode(texts=t['negative'], pool_top_k=t['negative_top_k'])\n\n        if len(goals) > 0:\n            progressbar(async_task, 7, 'Image processing ...')\n\n        if 'vary' in goals:\n            if 'subtle' in uov_method:\n                denoising_strength = 0.5\n            if 'strong' in uov_method:\n                denoising_strength = 0.85\n            if overwrite_vary_strength > 0:\n                denoising_strength = overwrite_vary_strength\n\n            shape_ceil = get_image_shape_ceil(uov_input_image)\n            if shape_ceil < 1024:\n                print(f'[Vary] Image is resized because it is too small.')\n                shape_ceil = 1024\n            elif shape_ceil > 2048:\n                print(f'[Vary] Image is resized because it is too big.')\n                shape_ceil = 2048\n\n            uov_input_image = set_image_shape_ceil(uov_input_image, shape_ceil)\n\n            initial_pixels = core.numpy_to_pytorch(uov_input_image)\n            progressbar(async_task, 8, 'VAE encoding ...')\n\n            candidate_vae, _ = pipeline.get_candidate_vae(\n                steps=steps,\n                switch=switch,\n                denoise=denoising_strength,\n                refiner_swap_method=refiner_swap_method\n            )\n\n            initial_latent = core.encode_vae(vae=candidate_vae, pixels=initial_pixels)\n            B, C, H, W = initial_latent['samples'].shape\n            width = W * 8\n            height = H * 8\n            print(f'Final resolution is {str((height, width))}.')\n\n        if 'upscale' in goals:\n            H, W, C = uov_input_image.shape\n            progressbar(async_task, 9, f'Upscaling image from {str((H, W))} ...')\n            uov_input_image = perform_upscale(uov_input_image)\n            print(f'Image upscaled.')\n\n            if '1.5x' in uov_method:\n                f = 1.5\n            elif '2x' in uov_method:\n                f = 2.0\n            else:\n                f = 1.0\n\n            shape_ceil = get_shape_ceil(H * f, W * f)\n\n            if shape_ceil < 1024:\n                print(f'[Upscale] Image is resized because it is too small.')\n                uov_input_image = set_image_shape_ceil(uov_input_image, 1024)\n                shape_ceil = 1024\n            else:\n                uov_input_image = resample_image(uov_input_image, width=W * f, height=H * f)\n\n            image_is_super_large = shape_ceil > 2800\n\n            if 'fast' in uov_method:\n                direct_return = True\n            elif image_is_super_large:\n                print('Image is too large. Directly returned the SR image. '\n                      'Usually directly return SR image at 4K resolution '\n                      'yields better results than SDXL diffusion.')\n                direct_return = True\n            else:\n                direct_return = False\n\n            if direct_return:\n                d = [('Upscale (Fast)', 'upscale_fast', '2x')]\n                if modules.config.default_black_out_nsfw or black_out_nsfw:\n                    progressbar(async_task, 100, 'Checking for NSFW content ...')\n                    uov_input_image = default_censor(uov_input_image)\n                progressbar(async_task, 100, 'Saving image to system ...')\n                uov_input_image_path = log(uov_input_image, d, output_format=output_format)\n                yield_result(async_task, uov_input_image_path, black_out_nsfw, False, do_not_show_finished_images=True)\n                return\n\n            tiled = True\n            denoising_strength = 0.382\n\n            if overwrite_upscale_strength > 0:\n                denoising_strength = overwrite_upscale_strength\n\n            initial_pixels = core.numpy_to_pytorch(uov_input_image)\n            progressbar(async_task, 10, 'VAE encoding ...')\n\n            candidate_vae, _ = pipeline.get_candidate_vae(\n                steps=steps,\n                switch=switch,\n                denoise=denoising_strength,\n                refiner_swap_method=refiner_swap_method\n            )\n\n            initial_latent = core.encode_vae(\n                vae=candidate_vae,\n                pixels=initial_pixels, tiled=True)\n            B, C, H, W = initial_latent['samples'].shape\n            width = W * 8\n            height = H * 8\n            print(f'Final resolution is {str((height, width))}.')\n\n        if 'inpaint' in goals:\n            if len(outpaint_selections) > 0:\n                H, W, C = inpaint_image.shape\n                if 'top' in outpaint_selections:\n                    inpaint_image = np.pad(inpaint_image, [[int(H * 0.3), 0], [0, 0], [0, 0]], mode='edge')\n                    inpaint_mask = np.pad(inpaint_mask, [[int(H * 0.3), 0], [0, 0]], mode='constant',\n                                          constant_values=255)\n                if 'bottom' in outpaint_selections:\n                    inpaint_image = np.pad(inpaint_image, [[0, int(H * 0.3)], [0, 0], [0, 0]], mode='edge')\n                    inpaint_mask = np.pad(inpaint_mask, [[0, int(H * 0.3)], [0, 0]], mode='constant',\n                                          constant_values=255)\n\n                H, W, C = inpaint_image.shape\n                if 'left' in outpaint_selections:\n                    inpaint_image = np.pad(inpaint_image, [[0, 0], [int(W * 0.3), 0], [0, 0]], mode='edge')\n                    inpaint_mask = np.pad(inpaint_mask, [[0, 0], [int(W * 0.3), 0]], mode='constant',\n                                          constant_values=255)\n                if 'right' in outpaint_selections:\n                    inpaint_image = np.pad(inpaint_image, [[0, 0], [0, int(W * 0.3)], [0, 0]], mode='edge')\n                    inpaint_mask = np.pad(inpaint_mask, [[0, 0], [0, int(W * 0.3)]], mode='constant',\n                                          constant_values=255)\n\n                inpaint_image = np.ascontiguousarray(inpaint_image.copy())\n                inpaint_mask = np.ascontiguousarray(inpaint_mask.copy())\n                inpaint_strength = 1.0\n                inpaint_respective_field = 1.0\n\n            denoising_strength = inpaint_strength\n\n            inpaint_worker.current_task = inpaint_worker.InpaintWorker(\n                image=inpaint_image,\n                mask=inpaint_mask,\n                use_fill=denoising_strength > 0.99,\n                k=inpaint_respective_field\n            )\n\n            if debugging_inpaint_preprocessor:\n                yield_result(async_task, inpaint_worker.current_task.visualize_mask_processing(), black_out_nsfw,\n                             do_not_show_finished_images=True)\n                return\n\n            progressbar(async_task, 11, 'VAE Inpaint encoding ...')\n\n            inpaint_pixel_fill = core.numpy_to_pytorch(inpaint_worker.current_task.interested_fill)\n            inpaint_pixel_image = core.numpy_to_pytorch(inpaint_worker.current_task.interested_image)\n            inpaint_pixel_mask = core.numpy_to_pytorch(inpaint_worker.current_task.interested_mask)\n\n            candidate_vae, candidate_vae_swap = pipeline.get_candidate_vae(\n                steps=steps,\n                switch=switch,\n                denoise=denoising_strength,\n                refiner_swap_method=refiner_swap_method\n            )\n\n            latent_inpaint, latent_mask = core.encode_vae_inpaint(\n                mask=inpaint_pixel_mask,\n                vae=candidate_vae,\n                pixels=inpaint_pixel_image)\n\n            latent_swap = None\n            if candidate_vae_swap is not None:\n                progressbar(async_task, 12, 'VAE SD15 encoding ...')\n                latent_swap = core.encode_vae(\n                    vae=candidate_vae_swap,\n                    pixels=inpaint_pixel_fill)['samples']\n\n            progressbar(async_task, 13, 'VAE encoding ...')\n            latent_fill = core.encode_vae(\n                vae=candidate_vae,\n                pixels=inpaint_pixel_fill)['samples']\n\n            inpaint_worker.current_task.load_latent(\n                latent_fill=latent_fill, latent_mask=latent_mask, latent_swap=latent_swap)\n\n            if inpaint_parameterized:\n                pipeline.final_unet = inpaint_worker.current_task.patch(\n                    inpaint_head_model_path=inpaint_head_model_path,\n                    inpaint_latent=latent_inpaint,\n                    inpaint_latent_mask=latent_mask,\n                    model=pipeline.final_unet\n                )\n\n            if not inpaint_disable_initial_latent:\n                initial_latent = {'samples': latent_fill}\n\n            B, C, H, W = latent_fill.shape\n            height, width = H * 8, W * 8\n            final_height, final_width = inpaint_worker.current_task.image.shape[:2]\n            print(f'Final resolution is {str((final_height, final_width))}, latent is {str((height, width))}.')\n\n        if 'cn' in goals:\n            for task in cn_tasks[flags.cn_canny]:\n                cn_img, cn_stop, cn_weight = task\n                cn_img = resize_image(HWC3(cn_img), width=width, height=height)\n\n                if not skipping_cn_preprocessor:\n                    cn_img = preprocessors.canny_pyramid(cn_img, canny_low_threshold, canny_high_threshold)\n\n                cn_img = HWC3(cn_img)\n                task[0] = core.numpy_to_pytorch(cn_img)\n                if debugging_cn_preprocessor:\n                    yield_result(async_task, cn_img, black_out_nsfw, do_not_show_finished_images=True)\n                    return\n            for task in cn_tasks[flags.cn_cpds]:\n                cn_img, cn_stop, cn_weight = task\n                cn_img = resize_image(HWC3(cn_img), width=width, height=height)\n\n                if not skipping_cn_preprocessor:\n                    cn_img = preprocessors.cpds(cn_img)\n\n                cn_img = HWC3(cn_img)\n                task[0] = core.numpy_to_pytorch(cn_img)\n                if debugging_cn_preprocessor:\n                    yield_result(async_task, cn_img, black_out_nsfw, do_not_show_finished_images=True)\n                    return\n            for task in cn_tasks[flags.cn_ip]:\n                cn_img, cn_stop, cn_weight = task\n                cn_img = HWC3(cn_img)\n\n                # https://github.com/tencent-ailab/IP-Adapter/blob/d580c50a291566bbf9fc7ac0f760506607297e6d/README.md?plain=1#L75\n                cn_img = resize_image(cn_img, width=224, height=224, resize_mode=0)\n\n                task[0] = ip_adapter.preprocess(cn_img, ip_adapter_path=ip_adapter_path)\n                if debugging_cn_preprocessor:\n                    yield_result(async_task, cn_img, black_out_nsfw, do_not_show_finished_images=True)\n                    return\n            for task in cn_tasks[flags.cn_ip_face]:\n                cn_img, cn_stop, cn_weight = task\n                cn_img = HWC3(cn_img)\n\n                if not skipping_cn_preprocessor:\n                    cn_img = extras.face_crop.crop_image(cn_img)\n\n                # https://github.com/tencent-ailab/IP-Adapter/blob/d580c50a291566bbf9fc7ac0f760506607297e6d/README.md?plain=1#L75\n                cn_img = resize_image(cn_img, width=224, height=224, resize_mode=0)\n\n                task[0] = ip_adapter.preprocess(cn_img, ip_adapter_path=ip_adapter_face_path)\n                if debugging_cn_preprocessor:\n                    yield_result(async_task, cn_img, black_out_nsfw, do_not_show_finished_images=True)\n                    return\n\n            all_ip_tasks = cn_tasks[flags.cn_ip] + cn_tasks[flags.cn_ip_face]\n\n            if len(all_ip_tasks) > 0:\n                pipeline.final_unet = ip_adapter.patch_model(pipeline.final_unet, all_ip_tasks)\n\n        if freeu_enabled:\n            print(f'FreeU is enabled!')\n            pipeline.final_unet = core.apply_freeu(\n                pipeline.final_unet,\n                freeu_b1,\n                freeu_b2,\n                freeu_s1,\n                freeu_s2\n            )\n\n        all_steps = steps * image_number\n\n        print(f'[Parameters] Denoising Strength = {denoising_strength}')\n\n        if isinstance(initial_latent, dict) and 'samples' in initial_latent:\n            log_shape = initial_latent['samples'].shape\n        else:\n            log_shape = f'Image Space {(height, width)}'\n\n        print(f'[Parameters] Initial Latent shape: {log_shape}')\n\n        preparation_time = time.perf_counter() - execution_start_time\n        print(f'Preparation time: {preparation_time:.2f} seconds')\n\n        final_sampler_name = sampler_name\n        final_scheduler_name = scheduler_name\n\n        if scheduler_name in ['lcm', 'tcd']:\n            final_scheduler_name = 'sgm_uniform'\n\n            def patch_discrete(unet):\n                return core.opModelSamplingDiscrete.patch(\n                    pipeline.final_unet,\n                    sampling=scheduler_name,\n                    zsnr=False)[0]\n\n            if pipeline.final_unet is not None:\n                pipeline.final_unet = patch_discrete(pipeline.final_unet)\n            if pipeline.final_refiner_unet is not None:\n                pipeline.final_refiner_unet = patch_discrete(pipeline.final_refiner_unet)\n            print(f'Using {scheduler_name} scheduler.')\n        elif scheduler_name == 'edm_playground_v2.5':\n            final_scheduler_name = 'karras'\n\n            def patch_edm(unet):\n                return core.opModelSamplingContinuousEDM.patch(\n                    unet,\n                    sampling=scheduler_name,\n                    sigma_max=120.0,\n                    sigma_min=0.002)[0]\n\n            if pipeline.final_unet is not None:\n                pipeline.final_unet = patch_edm(pipeline.final_unet)\n            if pipeline.final_refiner_unet is not None:\n                pipeline.final_refiner_unet = patch_edm(pipeline.final_refiner_unet)\n\n            print(f'Using {scheduler_name} scheduler.')\n\n        async_task.yields.append(['preview', (flags.preparation_step_count, 'Moving model to GPU ...', None)])\n\n        def callback(step, x0, x, total_steps, y):\n            done_steps = current_task_id * steps + step\n            async_task.yields.append(['preview', (\n                int(flags.preparation_step_count + (100 - flags.preparation_step_count) * float(done_steps) / float(all_steps)),\n                f'Sampling step {step + 1}/{total_steps}, image {current_task_id + 1}/{image_number} ...', y)])\n\n        for current_task_id, task in enumerate(tasks):\n            current_progress = int(flags.preparation_step_count + (100 - flags.preparation_step_count) * float(current_task_id * steps) / float(all_steps))\n            progressbar(async_task, current_progress, f'Preparing task {current_task_id + 1}/{image_number} ...')\n            execution_start_time = time.perf_counter()\n\n            try:\n                if async_task.last_stop is not False:\n                    ldm_patched.modules.model_management.interrupt_current_processing()\n                positive_cond, negative_cond = task['c'], task['uc']\n\n                if 'cn' in goals:\n                    for cn_flag, cn_path in [\n                        (flags.cn_canny, controlnet_canny_path),\n                        (flags.cn_cpds, controlnet_cpds_path)\n                    ]:\n                        for cn_img, cn_stop, cn_weight in cn_tasks[cn_flag]:\n                            positive_cond, negative_cond = core.apply_controlnet(\n                                positive_cond, negative_cond,\n                                pipeline.loaded_ControlNets[cn_path], cn_img, cn_weight, 0, cn_stop)\n\n                imgs = pipeline.process_diffusion(\n                    positive_cond=positive_cond,\n                    negative_cond=negative_cond,\n                    steps=steps,\n                    switch=switch,\n                    width=width,\n                    height=height,\n                    image_seed=task['task_seed'],\n                    callback=callback,\n                    sampler_name=final_sampler_name,\n                    scheduler_name=final_scheduler_name,\n                    latent=initial_latent,\n                    denoise=denoising_strength,\n                    tiled=tiled,\n                    cfg_scale=cfg_scale,\n                    refiner_swap_method=refiner_swap_method,\n                    disable_preview=disable_preview\n                )\n\n                del task['c'], task['uc'], positive_cond, negative_cond  # Save memory\n\n                if inpaint_worker.current_task is not None:\n                    imgs = [inpaint_worker.current_task.post_process(x) for x in imgs]\n\n                img_paths = []\n                current_progress = int(flags.preparation_step_count + (100 - flags.preparation_step_count) * float((current_task_id + 1) * steps) / float(all_steps))\n                if modules.config.default_black_out_nsfw or black_out_nsfw:\n                    progressbar(async_task, current_progress, 'Checking for NSFW content ...')\n                    imgs = default_censor(imgs)\n\n                progressbar(async_task, current_progress, f'Saving image {current_task_id + 1}/{image_number} to system ...')\n                for x in imgs:\n                    d = [('Prompt', 'prompt', task['log_positive_prompt']),\n                         ('Negative Prompt', 'negative_prompt', task['log_negative_prompt']),\n                         ('Fooocus V2 Expansion', 'prompt_expansion', task['expansion']),\n                         ('Styles', 'styles',\n                          str(task['styles'] if not use_expansion else [fooocus_expansion] + task['styles'])),\n                         ('Performance', 'performance', performance_selection.value)]\n\n                    if performance_selection.steps() != steps:\n                        d.append(('Steps', 'steps', steps))\n\n                    d += [('Resolution', 'resolution', str((width, height))),\n                          ('Guidance Scale', 'guidance_scale', guidance_scale),\n                          ('Sharpness', 'sharpness', sharpness),\n                          ('ADM Guidance', 'adm_guidance', str((\n                              modules.patch.patch_settings[pid].positive_adm_scale,\n                              modules.patch.patch_settings[pid].negative_adm_scale,\n                              modules.patch.patch_settings[pid].adm_scaler_end))),\n                          ('Base Model', 'base_model', base_model_name),\n                          ('Refiner Model', 'refiner_model', refiner_model_name),\n                          ('Refiner Switch', 'refiner_switch', refiner_switch)]\n\n                    if refiner_model_name != 'None':\n                        if overwrite_switch > 0:\n                            d.append(('Overwrite Switch', 'overwrite_switch', overwrite_switch))\n                        if refiner_swap_method != flags.refiner_swap_method:\n                            d.append(('Refiner Swap Method', 'refiner_swap_method', refiner_swap_method))\n                    if modules.patch.patch_settings[pid].adaptive_cfg != modules.config.default_cfg_tsnr:\n                        d.append(\n                            ('CFG Mimicking from TSNR', 'adaptive_cfg', modules.patch.patch_settings[pid].adaptive_cfg))\n\n                    if clip_skip > 1:\n                        d.append(('CLIP Skip', 'clip_skip', clip_skip))\n                    d.append(('Sampler', 'sampler', sampler_name))\n                    d.append(('Scheduler', 'scheduler', scheduler_name))\n                    d.append(('VAE', 'vae', vae_name))\n                    d.append(('Seed', 'seed', str(task['task_seed'])))\n\n                    if freeu_enabled:\n                        d.append(('FreeU', 'freeu', str((freeu_b1, freeu_b2, freeu_s1, freeu_s2))))\n\n                    for li, (n, w) in enumerate(loras):\n                        if n != 'None':\n                            d.append((f'LoRA {li + 1}', f'lora_combined_{li + 1}', f'{n} : {w}'))\n\n                    metadata_parser = None\n                    if save_metadata_to_images:\n                        metadata_parser = modules.meta_parser.get_metadata_parser(metadata_scheme)\n                        metadata_parser.set_data(task['log_positive_prompt'], task['positive'],\n                                                 task['log_negative_prompt'], task['negative'],\n                                                 steps, base_model_name, refiner_model_name, loras, vae_name)\n                    d.append(('Metadata Scheme', 'metadata_scheme',\n                              metadata_scheme.value if save_metadata_to_images else save_metadata_to_images))\n                    d.append(('Version', 'version', 'Fooocus v' + fooocus_version.version))\n                    img_paths.append(log(x, d, metadata_parser, output_format, task))\n\n                yield_result(async_task, img_paths, black_out_nsfw, False,\n                             do_not_show_finished_images=len(tasks) == 1 or disable_intermediate_results)\n            except ldm_patched.modules.model_management.InterruptProcessingException as e:\n                if async_task.last_stop == 'skip':\n                    print('User skipped')\n                    async_task.last_stop = False\n                    continue\n                else:\n                    print('User stopped')\n                    break\n\n            execution_time = time.perf_counter() - execution_start_time\n            print(f'Generating and saving time: {execution_time:.2f} seconds')\n        async_task.processing = False\n        return\n\n    while True:\n        time.sleep(0.01)\n        if len(async_tasks) > 0:\n            task = async_tasks.pop(0)\n            generate_image_grid = task.args.pop(0)\n\n            try:\n                handler(task)\n                if generate_image_grid:\n                    build_image_wall(task)\n                task.yields.append(['finish', task.results])\n                pipeline.prepare_text_encoder(async_call=True)\n            except:\n                traceback.print_exc()\n                task.yields.append(['finish', task.results])\n            finally:\n                if pid in modules.patch.patch_settings:\n                    del modules.patch.patch_settings[pid]\n    pass\n\n\nthreading.Thread(target=worker, daemon=True).start()\n", "modules/html.py": "progress_html = '''\n<div class=\"loader-container\">\n  <div class=\"loader\"></div>\n  <div class=\"progress-container\">\n    <progress value=\"*number*\" max=\"100\"></progress>\n  </div>\n  <span>*text*</span>\n</div>\n'''\n\n\ndef make_progress_html(number, text):\n    return progress_html.replace('*number*', str(number)).replace('*text*', text)\n", "modules/gradio_hijack.py": "\"\"\"gr.Image() component.\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom pathlib import Path\nfrom typing import Any, Literal\n\nimport numpy as np\nimport PIL\nimport PIL.ImageOps\nimport gradio.routes\nimport importlib\n\nfrom gradio_client import utils as client_utils\nfrom gradio_client.documentation import document, set_documentation_group\nfrom gradio_client.serializing import ImgSerializable\nfrom PIL import Image as _Image  # using _ to minimize namespace pollution\n\nfrom gradio import processing_utils, utils, Error\nfrom gradio.components.base import IOComponent, _Keywords, Block\nfrom gradio.deprecation import warn_style_method_deprecation\nfrom gradio.events import (\n    Changeable,\n    Clearable,\n    Editable,\n    EventListenerMethod,\n    Selectable,\n    Streamable,\n    Uploadable,\n)\nfrom gradio.interpretation import TokenInterpretable\n\nset_documentation_group(\"component\")\n_Image.init()  # fixes https://github.com/gradio-app/gradio/issues/2843\n\n\n@document()\nclass Image(\n    Editable,\n    Clearable,\n    Changeable,\n    Streamable,\n    Selectable,\n    Uploadable,\n    IOComponent,\n    ImgSerializable,\n    TokenInterpretable,\n):\n    \"\"\"\n    Creates an image component that can be used to upload/draw images (as an input) or display images (as an output).\n    Preprocessing: passes the uploaded image as a {numpy.array}, {PIL.Image} or {str} filepath depending on `type` -- unless `tool` is `sketch` AND source is one of `upload` or `webcam`. In these cases, a {dict} with keys `image` and `mask` is passed, and the format of the corresponding values depends on `type`.\n    Postprocessing: expects a {numpy.array}, {PIL.Image} or {str} or {pathlib.Path} filepath to an image and displays the image.\n    Examples-format: a {str} filepath to a local file that contains the image.\n    Demos: image_mod, image_mod_default_image\n    Guides: image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers, building-a-pictionary_app, create-your-own-friends-with-a-gan\n    \"\"\"\n\n    def __init__(\n        self,\n        value: str | _Image.Image | np.ndarray | None = None,\n        *,\n        shape: tuple[int, int] | None = None,\n        height: int | None = None,\n        width: int | None = None,\n        image_mode: Literal[\n            \"1\", \"L\", \"P\", \"RGB\", \"RGBA\", \"CMYK\", \"YCbCr\", \"LAB\", \"HSV\", \"I\", \"F\"\n        ] = \"RGB\",\n        invert_colors: bool = False,\n        source: Literal[\"upload\", \"webcam\", \"canvas\"] = \"upload\",\n        tool: Literal[\"editor\", \"select\", \"sketch\", \"color-sketch\"] | None = None,\n        type: Literal[\"numpy\", \"pil\", \"filepath\"] = \"numpy\",\n        label: str | None = None,\n        every: float | None = None,\n        show_label: bool | None = None,\n        show_download_button: bool = True,\n        container: bool = True,\n        scale: int | None = None,\n        min_width: int = 160,\n        interactive: bool | None = None,\n        visible: bool = True,\n        streaming: bool = False,\n        elem_id: str | None = None,\n        elem_classes: list[str] | str | None = None,\n        mirror_webcam: bool = True,\n        brush_radius: float | None = None,\n        brush_color: str = \"#000000\",\n        mask_opacity: float = 0.7,\n        show_share_button: bool | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters:\n            value: A PIL Image, numpy array, path or URL for the default value that Image component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.\n            shape: (width, height) shape to crop and resize image when passed to function. If None, matches input image size. Pass None for either width or height to only crop and resize the other.\n            height: Height of the displayed image in pixels.\n            width: Width of the displayed image in pixels.\n            image_mode: \"RGB\" if color, or \"L\" if black and white. See https://pillow.readthedocs.io/en/stable/handbook/concepts.html for other supported image modes and their meaning.\n            invert_colors: whether to invert the image as a preprocessing step.\n            source: Source of image. \"upload\" creates a box where user can drop an image file, \"webcam\" allows user to take snapshot from their webcam, \"canvas\" defaults to a white image that can be edited and drawn upon with tools.\n            tool: Tools used for editing. \"editor\" allows a full screen editor (and is the default if source is \"upload\" or \"webcam\"), \"select\" provides a cropping and zoom tool, \"sketch\" allows you to create a binary sketch (and is the default if source=\"canvas\"), and \"color-sketch\" allows you to created a sketch in different colors. \"color-sketch\" can be used with source=\"upload\" or \"webcam\" to allow sketching on an image. \"sketch\" can also be used with \"upload\" or \"webcam\" to create a mask over an image and in that case both the image and mask are passed into the function as a dictionary with keys \"image\" and \"mask\" respectively.\n            type: The format the image is converted to before being passed into the prediction function. \"numpy\" converts the image to a numpy array with shape (height, width, 3) and values from 0 to 255, \"pil\" converts the image to a PIL image object, \"filepath\" passes a str path to a temporary file containing the image.\n            label: component name in interface.\n            every: If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.\n            show_label: if True, will display label.\n            show_download_button: If True, will display button to download image.\n            container: If True, will place the component in a container - providing some extra padding around the border.\n            scale: relative width compared to adjacent Components in a Row. For example, if Component A has scale=2, and Component B has scale=1, A will be twice as wide as B. Should be an integer.\n            min_width: minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.\n            interactive: if True, will allow users to upload and edit an image; if False, can only be used to display images. If not provided, this is inferred based on whether the component is used as an input or output.\n            visible: If False, component will be hidden.\n            streaming: If True when used in a `live` interface, will automatically stream webcam feed. Only valid is source is 'webcam'.\n            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.\n            elem_classes: An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.\n            mirror_webcam: If True webcam will be mirrored. Default is True.\n            brush_radius: Size of the brush for Sketch. Default is None which chooses a sensible default\n            brush_color: Color of the brush for Sketch as hex string. Default is \"#000000\".\n            mask_opacity: Opacity of mask drawn on image, as a value between 0 and 1.\n            show_share_button: If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.\n        \"\"\"\n        self.brush_radius = brush_radius\n        self.brush_color = brush_color\n        self.mask_opacity = mask_opacity\n        self.mirror_webcam = mirror_webcam\n        valid_types = [\"numpy\", \"pil\", \"filepath\"]\n        if type not in valid_types:\n            raise ValueError(\n                f\"Invalid value for parameter `type`: {type}. Please choose from one of: {valid_types}\"\n            )\n        self.type = type\n        self.shape = shape\n        self.height = height\n        self.width = width\n        self.image_mode = image_mode\n        valid_sources = [\"upload\", \"webcam\", \"canvas\"]\n        if source not in valid_sources:\n            raise ValueError(\n                f\"Invalid value for parameter `source`: {source}. Please choose from one of: {valid_sources}\"\n            )\n        self.source = source\n        if tool is None:\n            self.tool = \"sketch\" if source == \"canvas\" else \"editor\"\n        else:\n            self.tool = tool\n        self.invert_colors = invert_colors\n        self.streaming = streaming\n        self.show_download_button = show_download_button\n        if streaming and source != \"webcam\":\n            raise ValueError(\"Image streaming only available if source is 'webcam'.\")\n        self.select: EventListenerMethod\n        \"\"\"\n        Event listener for when the user clicks on a pixel within the image.\n        Uses event data gradio.SelectData to carry `index` to refer to the [x, y] coordinates of the clicked pixel.\n        See EventData documentation on how to use this event data.\n        \"\"\"\n        self.show_share_button = (\n            (utils.get_space() is not None)\n            if show_share_button is None\n            else show_share_button\n        )\n        IOComponent.__init__(\n            self,\n            label=label,\n            every=every,\n            show_label=show_label,\n            container=container,\n            scale=scale,\n            min_width=min_width,\n            interactive=interactive,\n            visible=visible,\n            elem_id=elem_id,\n            elem_classes=elem_classes,\n            value=value,\n            **kwargs,\n        )\n        TokenInterpretable.__init__(self)\n\n    def get_config(self):\n        return {\n            \"image_mode\": self.image_mode,\n            \"shape\": self.shape,\n            \"height\": self.height,\n            \"width\": self.width,\n            \"source\": self.source,\n            \"tool\": self.tool,\n            \"value\": self.value,\n            \"streaming\": self.streaming,\n            \"mirror_webcam\": self.mirror_webcam,\n            \"brush_radius\": self.brush_radius,\n            \"brush_color\": self.brush_color,\n            \"mask_opacity\": self.mask_opacity,\n            \"selectable\": self.selectable,\n            \"show_share_button\": self.show_share_button,\n            \"show_download_button\": self.show_download_button,\n            **IOComponent.get_config(self),\n        }\n\n    @staticmethod\n    def update(\n        value: Any | Literal[_Keywords.NO_VALUE] | None = _Keywords.NO_VALUE,\n        height: int | None = None,\n        width: int | None = None,\n        label: str | None = None,\n        show_label: bool | None = None,\n        show_download_button: bool | None = None,\n        container: bool | None = None,\n        scale: int | None = None,\n        min_width: int | None = None,\n        interactive: bool | None = None,\n        visible: bool | None = None,\n        brush_radius: float | None = None,\n        brush_color: str | None = None,\n        mask_opacity: float | None = None,\n        show_share_button: bool | None = None,\n    ):\n        return {\n            \"height\": height,\n            \"width\": width,\n            \"label\": label,\n            \"show_label\": show_label,\n            \"show_download_button\": show_download_button,\n            \"container\": container,\n            \"scale\": scale,\n            \"min_width\": min_width,\n            \"interactive\": interactive,\n            \"visible\": visible,\n            \"value\": value,\n            \"brush_radius\": brush_radius,\n            \"brush_color\": brush_color,\n            \"mask_opacity\": mask_opacity,\n            \"show_share_button\": show_share_button,\n            \"__type__\": \"update\",\n        }\n\n    def _format_image(\n        self, im: _Image.Image | None\n    ) -> np.ndarray | _Image.Image | str | None:\n        \"\"\"Helper method to format an image based on self.type\"\"\"\n        if im is None:\n            return im\n        fmt = im.format\n        if self.type == \"pil\":\n            return im\n        elif self.type == \"numpy\":\n            return np.array(im)\n        elif self.type == \"filepath\":\n            path = self.pil_to_temp_file(\n                im, dir=self.DEFAULT_TEMP_DIR, format=fmt or \"png\"\n            )\n            self.temp_files.add(path)\n            return path\n        else:\n            raise ValueError(\n                \"Unknown type: \"\n                + str(self.type)\n                + \". Please choose from: 'numpy', 'pil', 'filepath'.\"\n            )\n\n    def preprocess(\n        self, x: str | dict[str, str]\n    ) -> np.ndarray | _Image.Image | str | dict | None:\n        \"\"\"\n        Parameters:\n            x: base64 url data, or (if tool == \"sketch\") a dict of image and mask base64 url data\n        Returns:\n            image in requested format, or (if tool == \"sketch\") a dict of image and mask in requested format\n        \"\"\"\n        if x is None:\n            return x\n\n        mask = None\n\n        if self.tool == \"sketch\" and self.source in [\"upload\", \"webcam\"]:\n            if isinstance(x, dict):\n                x, mask = x[\"image\"], x[\"mask\"]\n\n        assert isinstance(x, str)\n        try:\n            im = processing_utils.decode_base64_to_image(x)\n        except PIL.UnidentifiedImageError:\n            raise Error(\"Unsupported image type in input\")\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            im = im.convert(self.image_mode)\n        if self.shape is not None:\n            im = processing_utils.resize_and_crop(im, self.shape)\n        if self.invert_colors:\n            im = PIL.ImageOps.invert(im)\n        if (\n            self.source == \"webcam\"\n            and self.mirror_webcam is True\n            and self.tool != \"color-sketch\"\n        ):\n            im = PIL.ImageOps.mirror(im)\n\n        if self.tool == \"sketch\" and self.source in [\"upload\", \"webcam\"]:\n            if mask is not None:\n                mask_im = processing_utils.decode_base64_to_image(mask)\n                if mask_im.mode == \"RGBA\":  # whiten any opaque pixels in the mask\n                    alpha_data = mask_im.getchannel(\"A\").convert(\"L\")\n                    mask_im = _Image.merge(\"RGB\", [alpha_data, alpha_data, alpha_data])\n                return {\n                    \"image\": self._format_image(im),\n                    \"mask\": self._format_image(mask_im),\n                }\n            else:\n                return {\n                    \"image\": self._format_image(im),\n                    \"mask\": None,\n                }\n\n        return self._format_image(im)\n\n    def postprocess(\n        self, y: np.ndarray | _Image.Image | str | Path | None\n    ) -> str | None:\n        \"\"\"\n        Parameters:\n            y: image as a numpy array, PIL Image, string/Path filepath, or string URL\n        Returns:\n            base64 url data\n        \"\"\"\n        if y is None:\n            return None\n        if isinstance(y, np.ndarray):\n            return processing_utils.encode_array_to_base64(y)\n        elif isinstance(y, _Image.Image):\n            return processing_utils.encode_pil_to_base64(y)\n        elif isinstance(y, (str, Path)):\n            return client_utils.encode_url_or_file_to_base64(y)\n        else:\n            raise ValueError(\"Cannot process this value as an Image\")\n\n    def set_interpret_parameters(self, segments: int = 16):\n        \"\"\"\n        Calculates interpretation score of image subsections by splitting the image into subsections, then using a \"leave one out\" method to calculate the score of each subsection by whiting out the subsection and measuring the delta of the output value.\n        Parameters:\n            segments: Number of interpretation segments to split image into.\n        \"\"\"\n        self.interpretation_segments = segments\n        return self\n\n    def _segment_by_slic(self, x):\n        \"\"\"\n        Helper method that segments an image into superpixels using slic.\n        Parameters:\n            x: base64 representation of an image\n        \"\"\"\n        x = processing_utils.decode_base64_to_image(x)\n        if self.shape is not None:\n            x = processing_utils.resize_and_crop(x, self.shape)\n        resized_and_cropped_image = np.array(x)\n        try:\n            from skimage.segmentation import slic\n        except (ImportError, ModuleNotFoundError) as err:\n            raise ValueError(\n                \"Error: running this interpretation for images requires scikit-image, please install it first.\"\n            ) from err\n        try:\n            segments_slic = slic(\n                resized_and_cropped_image,\n                self.interpretation_segments,\n                compactness=10,\n                sigma=1,\n                start_label=1,\n            )\n        except TypeError:  # For skimage 0.16 and older\n            segments_slic = slic(\n                resized_and_cropped_image,\n                self.interpretation_segments,\n                compactness=10,\n                sigma=1,\n            )\n        return segments_slic, resized_and_cropped_image\n\n    def tokenize(self, x):\n        \"\"\"\n        Segments image into tokens, masks, and leave-one-out-tokens\n        Parameters:\n            x: base64 representation of an image\n        Returns:\n            tokens: list of tokens, used by the get_masked_input() method\n            leave_one_out_tokens: list of left-out tokens, used by the get_interpretation_neighbors() method\n            masks: list of masks, used by the get_interpretation_neighbors() method\n        \"\"\"\n        segments_slic, resized_and_cropped_image = self._segment_by_slic(x)\n        tokens, masks, leave_one_out_tokens = [], [], []\n        replace_color = np.mean(resized_and_cropped_image, axis=(0, 1))\n        for segment_value in np.unique(segments_slic):\n            mask = segments_slic == segment_value\n            image_screen = np.copy(resized_and_cropped_image)\n            image_screen[segments_slic == segment_value] = replace_color\n            leave_one_out_tokens.append(\n                processing_utils.encode_array_to_base64(image_screen)\n            )\n            token = np.copy(resized_and_cropped_image)\n            token[segments_slic != segment_value] = 0\n            tokens.append(token)\n            masks.append(mask)\n        return tokens, leave_one_out_tokens, masks\n\n    def get_masked_inputs(self, tokens, binary_mask_matrix):\n        masked_inputs = []\n        for binary_mask_vector in binary_mask_matrix:\n            masked_input = np.zeros_like(tokens[0], dtype=int)\n            for token, b in zip(tokens, binary_mask_vector):\n                masked_input = masked_input + token * int(b)\n            masked_inputs.append(processing_utils.encode_array_to_base64(masked_input))\n        return masked_inputs\n\n    def get_interpretation_scores(\n        self, x, neighbors, scores, masks, tokens=None, **kwargs\n    ) -> list[list[float]]:\n        \"\"\"\n        Returns:\n            A 2D array representing the interpretation score of each pixel of the image.\n        \"\"\"\n        x = processing_utils.decode_base64_to_image(x)\n        if self.shape is not None:\n            x = processing_utils.resize_and_crop(x, self.shape)\n        x = np.array(x)\n        output_scores = np.zeros((x.shape[0], x.shape[1]))\n\n        for score, mask in zip(scores, masks):\n            output_scores += score * mask\n\n        max_val, min_val = np.max(output_scores), np.min(output_scores)\n        if max_val > 0:\n            output_scores = (output_scores - min_val) / (max_val - min_val)\n        return output_scores.tolist()\n\n    def style(self, *, height: int | None = None, width: int | None = None, **kwargs):\n        \"\"\"\n        This method is deprecated. Please set these arguments in the constructor instead.\n        \"\"\"\n        warn_style_method_deprecation()\n        if height is not None:\n            self.height = height\n        if width is not None:\n            self.width = width\n        return self\n\n    def check_streamable(self):\n        if self.source != \"webcam\":\n            raise ValueError(\"Image streaming only available if source is 'webcam'.\")\n\n    def as_example(self, input_data: str | None) -> str:\n        if input_data is None:\n            return \"\"\n        elif (\n            self.root_url\n        ):  # If an externally hosted image, don't convert to absolute path\n            return input_data\n        return str(utils.abspath(input_data))\n\n\nall_components = []\n\nif not hasattr(Block, 'original__init__'):\n    Block.original_init = Block.__init__\n\n\ndef blk_ini(self, *args, **kwargs):\n    all_components.append(self)\n    return Block.original_init(self, *args, **kwargs)\n\n\nBlock.__init__ = blk_ini\n\n\ngradio.routes.asyncio = importlib.reload(gradio.routes.asyncio)\n\nif not hasattr(gradio.routes.asyncio, 'original_wait_for'):\n    gradio.routes.asyncio.original_wait_for = gradio.routes.asyncio.wait_for\n\n\ndef patched_wait_for(fut, timeout):\n    del timeout\n    return gradio.routes.asyncio.original_wait_for(fut, timeout=65535)\n\n\ngradio.routes.asyncio.wait_for = patched_wait_for\n\n", "modules/private_logger.py": "import os\nimport args_manager\nimport modules.config\nimport json\nimport urllib.parse\n\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\nfrom modules.flags import OutputFormat\nfrom modules.meta_parser import MetadataParser, get_exif\nfrom modules.util import generate_temp_filename\n\nlog_cache = {}\n\n\ndef get_current_html_path(output_format=None):\n    output_format = output_format if output_format else modules.config.default_output_format\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs,\n                                                                         extension=output_format)\n    html_name = os.path.join(os.path.dirname(local_temp_filename), 'log.html')\n    return html_name\n\n\ndef log(img, metadata, metadata_parser: MetadataParser | None = None, output_format=None, task=None) -> str:\n    path_outputs = modules.config.temp_path if args_manager.args.disable_image_log else modules.config.path_outputs\n    output_format = output_format if output_format else modules.config.default_output_format\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=path_outputs, extension=output_format)\n    os.makedirs(os.path.dirname(local_temp_filename), exist_ok=True)\n\n    parsed_parameters = metadata_parser.to_string(metadata.copy()) if metadata_parser is not None else ''\n    image = Image.fromarray(img)\n\n    if output_format == OutputFormat.PNG.value:\n        if parsed_parameters != '':\n            pnginfo = PngInfo()\n            pnginfo.add_text('parameters', parsed_parameters)\n            pnginfo.add_text('fooocus_scheme', metadata_parser.get_scheme().value)\n        else:\n            pnginfo = None\n        image.save(local_temp_filename, pnginfo=pnginfo)\n    elif output_format == OutputFormat.JPEG.value:\n        image.save(local_temp_filename, quality=95, optimize=True, progressive=True, exif=get_exif(parsed_parameters, metadata_parser.get_scheme().value) if metadata_parser else Image.Exif())\n    elif output_format == OutputFormat.WEBP.value:\n        image.save(local_temp_filename, quality=95, lossless=False, exif=get_exif(parsed_parameters, metadata_parser.get_scheme().value) if metadata_parser else Image.Exif())\n    else:\n        image.save(local_temp_filename)\n\n    if args_manager.args.disable_image_log:\n        return local_temp_filename\n\n    html_name = os.path.join(os.path.dirname(local_temp_filename), 'log.html')\n\n    css_styles = (\n        \"<style>\"\n        \"body { background-color: #121212; color: #E0E0E0; } \"\n        \"a { color: #BB86FC; } \"\n        \".metadata { border-collapse: collapse; width: 100%; } \"\n        \".metadata .label { width: 15%; } \"\n        \".metadata .value { width: 85%; font-weight: bold; } \"\n        \".metadata th, .metadata td { border: 1px solid #4d4d4d; padding: 4px; } \"\n        \".image-container img { height: auto; max-width: 512px; display: block; padding-right:10px; } \"\n        \".image-container div { text-align: center; padding: 4px; } \"\n        \"hr { border-color: gray; } \"\n        \"button { background-color: black; color: white; border: 1px solid grey; border-radius: 5px; padding: 5px 10px; text-align: center; display: inline-block; font-size: 16px; cursor: pointer; }\"\n        \"button:hover {background-color: grey; color: black;}\"\n        \"</style>\"\n    )\n\n    js = (\n        \"\"\"<script>\n        function to_clipboard(txt) { \n        txt = decodeURIComponent(txt);\n        if (navigator.clipboard && navigator.permissions) {\n            navigator.clipboard.writeText(txt)\n        } else {\n            const textArea = document.createElement('textArea')\n            textArea.value = txt\n            textArea.style.width = 0\n            textArea.style.position = 'fixed'\n            textArea.style.left = '-999px'\n            textArea.style.top = '10px'\n            textArea.setAttribute('readonly', 'readonly')\n            document.body.appendChild(textArea)\n\n            textArea.select()\n            document.execCommand('copy')\n            document.body.removeChild(textArea)\n        }\n        alert('Copied to Clipboard!\\\\nPaste to prompt area to load parameters.\\\\nCurrent clipboard content is:\\\\n\\\\n' + txt);\n        }\n        </script>\"\"\"\n    )\n\n    begin_part = f\"<!DOCTYPE html><html><head><title>Fooocus Log {date_string}</title>{css_styles}</head><body>{js}<p>Fooocus Log {date_string} (private)</p>\\n<p>Metadata is embedded if enabled in the config or developer debug mode. You can find the information for each image in line Metadata Scheme.</p><!--fooocus-log-split-->\\n\\n\"\n    end_part = f'\\n<!--fooocus-log-split--></body></html>'\n\n    middle_part = log_cache.get(html_name, \"\")\n\n    if middle_part == \"\":\n        if os.path.exists(html_name):\n            existing_split = open(html_name, 'r', encoding='utf-8').read().split('<!--fooocus-log-split-->')\n            if len(existing_split) == 3:\n                middle_part = existing_split[1]\n            else:\n                middle_part = existing_split[0]\n\n    div_name = only_name.replace('.', '_')\n    item = f\"<div id=\\\"{div_name}\\\" class=\\\"image-container\\\"><hr><table><tr>\\n\"\n    item += f\"<td><a href=\\\"{only_name}\\\" target=\\\"_blank\\\"><img src='{only_name}' onerror=\\\"this.closest('.image-container').style.display='none';\\\" loading='lazy'/></a><div>{only_name}</div></td>\"\n    item += \"<td><table class='metadata'>\"\n    for label, key, value in metadata:\n        value_txt = str(value).replace('\\n', ' </br> ')\n        item += f\"<tr><td class='label'>{label}</td><td class='value'>{value_txt}</td></tr>\\n\"\n\n    if task is not None and 'positive' in task and 'negative' in task:\n        full_prompt_details = f\"\"\"<details><summary>Positive</summary>{', '.join(task['positive'])}</details>\n        <details><summary>Negative</summary>{', '.join(task['negative'])}</details>\"\"\"\n        item += f\"<tr><td class='label'>Full raw prompt</td><td class='value'>{full_prompt_details}</td></tr>\\n\"\n\n    item += \"</table>\"\n\n    js_txt = urllib.parse.quote(json.dumps({k: v for _, k, v, in metadata}, indent=0), safe='')\n    item += f\"</br><button onclick=\\\"to_clipboard('{js_txt}')\\\">Copy to Clipboard</button>\"\n\n    item += \"</td>\"\n    item += \"</tr></table></div>\\n\\n\"\n\n    middle_part = item + middle_part\n\n    with open(html_name, 'w', encoding='utf-8') as f:\n        f.write(begin_part + middle_part + end_part)\n\n    print(f'Image generated with private log at: {html_name}')\n\n    log_cache[html_name] = middle_part\n\n    return local_temp_filename\n", "modules/config.py": "import os\nimport json\nimport math\nimport numbers\n\nimport args_manager\nimport tempfile\nimport modules.flags\nimport modules.sdxl_styles\n\nfrom modules.model_loader import load_file_from_url\nfrom modules.extra_utils import makedirs_with_log, get_files_from_folder, try_eval_env_var\nfrom modules.flags import OutputFormat, Performance, MetadataScheme\n\n\ndef get_config_path(key, default_value):\n    env = os.getenv(key)\n    if env is not None and isinstance(env, str):\n        print(f\"Environment: {key} = {env}\")\n        return env\n    else:\n        return os.path.abspath(default_value)\n\nwildcards_max_bfs_depth = 64\nconfig_path = get_config_path('config_path', \"./config.txt\")\nconfig_example_path = get_config_path('config_example_path', \"config_modification_tutorial.txt\")\nconfig_dict = {}\nalways_save_keys = []\nvisited_keys = []\n\ntry:\n    with open(os.path.abspath(f'./presets/default.json'), \"r\", encoding=\"utf-8\") as json_file:\n        config_dict.update(json.load(json_file))\nexcept Exception as e:\n    print(f'Load default preset failed.')\n    print(e)\n\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict.update(json.load(json_file))\n            always_save_keys = list(config_dict.keys())\nexcept Exception as e:\n    print(f'Failed to load config file \"{config_path}\" . The reason is: {str(e)}')\n    print('Please make sure that:')\n    print(f'1. The file \"{config_path}\" is a valid text file, and you have access to read it.')\n    print('2. Use \"\\\\\\\\\" instead of \"\\\\\" when describing paths.')\n    print('3. There is no \",\" before the last \"}\".')\n    print('4. All key/value formats are correct.')\n\n\ndef try_load_deprecated_user_path_config():\n    global config_dict\n\n    if not os.path.exists('user_path_config.txt'):\n        return\n\n    try:\n        deprecated_config_dict = json.load(open('user_path_config.txt', \"r\", encoding=\"utf-8\"))\n\n        def replace_config(old_key, new_key):\n            if old_key in deprecated_config_dict:\n                config_dict[new_key] = deprecated_config_dict[old_key]\n                del deprecated_config_dict[old_key]\n\n        replace_config('modelfile_path', 'path_checkpoints')\n        replace_config('lorafile_path', 'path_loras')\n        replace_config('embeddings_path', 'path_embeddings')\n        replace_config('vae_approx_path', 'path_vae_approx')\n        replace_config('upscale_models_path', 'path_upscale_models')\n        replace_config('inpaint_models_path', 'path_inpaint')\n        replace_config('controlnet_models_path', 'path_controlnet')\n        replace_config('clip_vision_models_path', 'path_clip_vision')\n        replace_config('fooocus_expansion_path', 'path_fooocus_expansion')\n        replace_config('temp_outputs_path', 'path_outputs')\n\n        if deprecated_config_dict.get(\"default_model\", None) == 'juggernautXL_version6Rundiffusion.safetensors':\n            os.replace('user_path_config.txt', 'user_path_config-deprecated.txt')\n            print('Config updated successfully in silence. '\n                  'A backup of previous config is written to \"user_path_config-deprecated.txt\".')\n            return\n\n        if input(\"Newer models and configs are available. \"\n                 \"Download and update files? [Y/n]:\") in ['n', 'N', 'No', 'no', 'NO']:\n            config_dict.update(deprecated_config_dict)\n            print('Loading using deprecated old models and deprecated old configs.')\n            return\n        else:\n            os.replace('user_path_config.txt', 'user_path_config-deprecated.txt')\n            print('Config updated successfully by user. '\n                  'A backup of previous config is written to \"user_path_config-deprecated.txt\".')\n            return\n    except Exception as e:\n        print('Processing deprecated config failed')\n        print(e)\n    return\n\n\ntry_load_deprecated_user_path_config()\n\n\ndef get_presets():\n    preset_folder = 'presets'\n    presets = ['initial']\n    if not os.path.exists(preset_folder):\n        print('No presets found.')\n        return presets\n\n    return presets + [f[:f.index('.json')] for f in os.listdir(preset_folder) if f.endswith('.json')]\n\n\ndef try_get_preset_content(preset):\n    if isinstance(preset, str):\n        preset_path = os.path.abspath(f'./presets/{preset}.json')\n        try:\n            if os.path.exists(preset_path):\n                with open(preset_path, \"r\", encoding=\"utf-8\") as json_file:\n                    json_content = json.load(json_file)\n                    print(f'Loaded preset: {preset_path}')\n                    return json_content\n            else:\n                raise FileNotFoundError\n        except Exception as e:\n            print(f'Load preset [{preset_path}] failed')\n            print(e)\n    return {}\n\navailable_presets = get_presets()\npreset = args_manager.args.preset\nconfig_dict.update(try_get_preset_content(preset))\n\ndef get_path_output() -> str:\n    \"\"\"\n    Checking output path argument and overriding default path.\n    \"\"\"\n    global config_dict\n    path_output = get_dir_or_set_default('path_outputs', '../outputs/', make_directory=True)\n    if args_manager.args.output_path:\n        print(f'Overriding config value path_outputs with {args_manager.args.output_path}')\n        config_dict['path_outputs'] = path_output = args_manager.args.output_path\n    return path_output\n\n\ndef get_dir_or_set_default(key, default_value, as_array=False, make_directory=False):\n    global config_dict, visited_keys, always_save_keys\n\n    if key not in visited_keys:\n        visited_keys.append(key)\n\n    if key not in always_save_keys:\n        always_save_keys.append(key)\n\n    v = os.getenv(key)\n    if v is not None:\n        print(f\"Environment: {key} = {v}\")\n        config_dict[key] = v\n    else:\n        v = config_dict.get(key, None)\n\n    if isinstance(v, str):\n        if make_directory:\n            makedirs_with_log(v)\n        if os.path.exists(v) and os.path.isdir(v):\n            return v if not as_array else [v]\n    elif isinstance(v, list):\n        if make_directory:\n            for d in v:\n                makedirs_with_log(d)\n        if all([os.path.exists(d) and os.path.isdir(d) for d in v]):\n            return v\n\n    if v is not None:\n        print(f'Failed to load config key: {json.dumps({key:v})} is invalid or does not exist; will use {json.dumps({key:default_value})} instead.')\n    if isinstance(default_value, list):\n        dp = []\n        for path in default_value:\n            abs_path = os.path.abspath(os.path.join(os.path.dirname(__file__), path))\n            dp.append(abs_path)\n            os.makedirs(abs_path, exist_ok=True)\n    else:\n        dp = os.path.abspath(os.path.join(os.path.dirname(__file__), default_value))\n        os.makedirs(dp, exist_ok=True)\n        if as_array:\n            dp = [dp]\n    config_dict[key] = dp\n    return dp\n\n\npaths_checkpoints = get_dir_or_set_default('path_checkpoints', ['../models/checkpoints/'], True)\npaths_loras = get_dir_or_set_default('path_loras', ['../models/loras/'], True)\npath_embeddings = get_dir_or_set_default('path_embeddings', '../models/embeddings/')\npath_vae_approx = get_dir_or_set_default('path_vae_approx', '../models/vae_approx/')\npath_vae = get_dir_or_set_default('path_vae', '../models/vae/')\npath_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_wildcards = get_dir_or_set_default('path_wildcards', '../wildcards/')\npath_safety_checker = get_dir_or_set_default('path_safety_checker', '../models/safety_checker/')\npath_outputs = get_path_output()\n\n\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False, expected_type=None):\n    global config_dict, visited_keys\n\n    if key not in visited_keys:\n        visited_keys.append(key)\n    \n    v = os.getenv(key)\n    if v is not None:\n        v = try_eval_env_var(v, expected_type)\n        print(f\"Environment: {key} = {v}\")\n        config_dict[key] = v\n\n    if key not in config_dict:\n        config_dict[key] = default_value\n        return default_value\n\n    v = config_dict.get(key, None)\n    if not disable_empty_as_none:\n        if v is None or v == '':\n            v = 'None'\n    if validator(v):\n        return v\n    else:\n        if v is not None:\n            print(f'Failed to load config key: {json.dumps({key:v})} is invalid; will use {json.dumps({key:default_value})} instead.')\n        config_dict[key] = default_value\n        return default_value\n\n\ndef init_temp_path(path: str | None, default_path: str) -> str:\n    if args_manager.args.temp_path:\n        path = args_manager.args.temp_path\n\n    if path != '' and path != default_path:\n        try:\n            if not os.path.isabs(path):\n                path = os.path.abspath(path)\n            os.makedirs(path, exist_ok=True)\n            print(f'Using temp path {path}')\n            return path\n        except Exception as e:\n            print(f'Could not create temp path {path}. Reason: {e}')\n            print(f'Using default temp path {default_path} instead.')\n\n    os.makedirs(default_path, exist_ok=True)\n    return default_path\n\n\ndefault_temp_path = os.path.join(tempfile.gettempdir(), 'fooocus')\ntemp_path = init_temp_path(get_config_item_or_set_default(\n    key='temp_path',\n    default_value=default_temp_path,\n    validator=lambda x: isinstance(x, str),\n    expected_type=str\n), default_temp_path)\ntemp_path_cleanup_on_launch = get_config_item_or_set_default(\n    key='temp_path_cleanup_on_launch',\n    default_value=True,\n    validator=lambda x: isinstance(x, bool),\n    expected_type=bool\n)\ndefault_base_model_name = default_model = get_config_item_or_set_default(\n    key='default_model',\n    default_value='model.safetensors',\n    validator=lambda x: isinstance(x, str),\n    expected_type=str\n)\nprevious_default_models = get_config_item_or_set_default(\n    key='previous_default_models',\n    default_value=[],\n    validator=lambda x: isinstance(x, list) and all(isinstance(k, str) for k in x),\n    expected_type=list\n)\ndefault_refiner_model_name = default_refiner = get_config_item_or_set_default(\n    key='default_refiner',\n    default_value='None',\n    validator=lambda x: isinstance(x, str),\n    expected_type=str\n)\ndefault_refiner_switch = get_config_item_or_set_default(\n    key='default_refiner_switch',\n    default_value=0.8,\n    validator=lambda x: isinstance(x, numbers.Number) and 0 <= x <= 1,\n    expected_type=numbers.Number\n)\ndefault_loras_min_weight = get_config_item_or_set_default(\n    key='default_loras_min_weight',\n    default_value=-2,\n    validator=lambda x: isinstance(x, numbers.Number) and -10 <= x <= 10,\n    expected_type=numbers.Number\n)\ndefault_loras_max_weight = get_config_item_or_set_default(\n    key='default_loras_max_weight',\n    default_value=2,\n    validator=lambda x: isinstance(x, numbers.Number) and -10 <= x <= 10,\n    expected_type=numbers.Number\n)\ndefault_loras = get_config_item_or_set_default(\n    key='default_loras',\n    default_value=[\n        [\n            True,\n            \"None\",\n            1.0\n        ],\n        [\n            True,\n            \"None\",\n            1.0\n        ],\n        [\n            True,\n            \"None\",\n            1.0\n        ],\n        [\n            True,\n            \"None\",\n            1.0\n        ],\n        [\n            True,\n            \"None\",\n            1.0\n        ]\n    ],\n    validator=lambda x: isinstance(x, list) and all(\n        len(y) == 3 and isinstance(y[0], bool) and isinstance(y[1], str) and isinstance(y[2], numbers.Number)\n        or len(y) == 2 and isinstance(y[0], str) and isinstance(y[1], numbers.Number)\n        for y in x),\n    expected_type=list\n)\ndefault_loras = [(y[0], y[1], y[2]) if len(y) == 3 else (True, y[0], y[1]) for y in default_loras]\ndefault_max_lora_number = get_config_item_or_set_default(\n    key='default_max_lora_number',\n    default_value=len(default_loras) if isinstance(default_loras, list) and len(default_loras) > 0 else 5,\n    validator=lambda x: isinstance(x, int) and x >= 1,\n    expected_type=int\n)\ndefault_cfg_scale = get_config_item_or_set_default(\n    key='default_cfg_scale',\n    default_value=7.0,\n    validator=lambda x: isinstance(x, numbers.Number),\n    expected_type=numbers.Number\n)\ndefault_sample_sharpness = get_config_item_or_set_default(\n    key='default_sample_sharpness',\n    default_value=2.0,\n    validator=lambda x: isinstance(x, numbers.Number),\n    expected_type=numbers.Number\n)\ndefault_sampler = get_config_item_or_set_default(\n    key='default_sampler',\n    default_value='dpmpp_2m_sde_gpu',\n    validator=lambda x: x in modules.flags.sampler_list,\n    expected_type=str\n)\ndefault_scheduler = get_config_item_or_set_default(\n    key='default_scheduler',\n    default_value='karras',\n    validator=lambda x: x in modules.flags.scheduler_list,\n    expected_type=str\n)\ndefault_vae = get_config_item_or_set_default(\n    key='default_vae',\n    default_value=modules.flags.default_vae,\n    validator=lambda x: isinstance(x, str),\n    expected_type=str\n)\ndefault_styles = get_config_item_or_set_default(\n    key='default_styles',\n    default_value=[\n        \"Fooocus V2\",\n        \"Fooocus Enhance\",\n        \"Fooocus Sharp\"\n    ],\n    validator=lambda x: isinstance(x, list) and all(y in modules.sdxl_styles.legal_style_names for y in x),\n    expected_type=list\n)\ndefault_prompt_negative = get_config_item_or_set_default(\n    key='default_prompt_negative',\n    default_value='',\n    validator=lambda x: isinstance(x, str),\n    disable_empty_as_none=True,\n    expected_type=str\n)\ndefault_prompt = get_config_item_or_set_default(\n    key='default_prompt',\n    default_value='',\n    validator=lambda x: isinstance(x, str),\n    disable_empty_as_none=True,\n    expected_type=str\n)\ndefault_performance = get_config_item_or_set_default(\n    key='default_performance',\n    default_value=Performance.SPEED.value,\n    validator=lambda x: x in Performance.list(),\n    expected_type=str\n)\ndefault_advanced_checkbox = get_config_item_or_set_default(\n    key='default_advanced_checkbox',\n    default_value=False,\n    validator=lambda x: isinstance(x, bool),\n    expected_type=bool\n)\ndefault_max_image_number = get_config_item_or_set_default(\n    key='default_max_image_number',\n    default_value=32,\n    validator=lambda x: isinstance(x, int) and x >= 1,\n    expected_type=int\n)\ndefault_output_format = get_config_item_or_set_default(\n    key='default_output_format',\n    default_value='png',\n    validator=lambda x: x in OutputFormat.list(),\n    expected_type=str\n)\ndefault_image_number = get_config_item_or_set_default(\n    key='default_image_number',\n    default_value=2,\n    validator=lambda x: isinstance(x, int) and 1 <= x <= default_max_image_number,\n    expected_type=int\n)\ncheckpoint_downloads = get_config_item_or_set_default(\n    key='checkpoint_downloads',\n    default_value={},\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items()),\n    expected_type=dict\n)\nlora_downloads = get_config_item_or_set_default(\n    key='lora_downloads',\n    default_value={},\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items()),\n    expected_type=dict\n)\nembeddings_downloads = get_config_item_or_set_default(\n    key='embeddings_downloads',\n    default_value={},\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items()),\n    expected_type=dict\n)\navailable_aspect_ratios = get_config_item_or_set_default(\n    key='available_aspect_ratios',\n    default_value=modules.flags.sdxl_aspect_ratios,\n    validator=lambda x: isinstance(x, list) and all('*' in v for v in x) and len(x) > 1,\n    expected_type=list\n)\ndefault_aspect_ratio = get_config_item_or_set_default(\n    key='default_aspect_ratio',\n    default_value='1152*896' if '1152*896' in available_aspect_ratios else available_aspect_ratios[0],\n    validator=lambda x: x in available_aspect_ratios,\n    expected_type=str\n)\ndefault_inpaint_engine_version = get_config_item_or_set_default(\n    key='default_inpaint_engine_version',\n    default_value='v2.6',\n    validator=lambda x: x in modules.flags.inpaint_engine_versions,\n    expected_type=str\n)\ndefault_cfg_tsnr = get_config_item_or_set_default(\n    key='default_cfg_tsnr',\n    default_value=7.0,\n    validator=lambda x: isinstance(x, numbers.Number),\n    expected_type=numbers.Number\n)\ndefault_clip_skip = get_config_item_or_set_default(\n    key='default_clip_skip',\n    default_value=2,\n    validator=lambda x: isinstance(x, int) and 1 <= x <= modules.flags.clip_skip_max,\n    expected_type=int\n)\ndefault_overwrite_step = get_config_item_or_set_default(\n    key='default_overwrite_step',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int),\n    expected_type=int\n)\ndefault_overwrite_switch = get_config_item_or_set_default(\n    key='default_overwrite_switch',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int),\n    expected_type=int\n)\nexample_inpaint_prompts = get_config_item_or_set_default(\n    key='example_inpaint_prompts',\n    default_value=[\n        'highly detailed face', 'detailed girl face', 'detailed man face', 'detailed hand', 'beautiful eyes'\n    ],\n    validator=lambda x: isinstance(x, list) and all(isinstance(v, str) for v in x),\n    expected_type=list\n)\ndefault_black_out_nsfw = get_config_item_or_set_default(\n    key='default_black_out_nsfw',\n    default_value=False,\n    validator=lambda x: isinstance(x, bool),\n    expected_type=bool\n)\ndefault_save_metadata_to_images = get_config_item_or_set_default(\n    key='default_save_metadata_to_images',\n    default_value=False,\n    validator=lambda x: isinstance(x, bool),\n    expected_type=bool\n)\ndefault_metadata_scheme = get_config_item_or_set_default(\n    key='default_metadata_scheme',\n    default_value=MetadataScheme.FOOOCUS.value,\n    validator=lambda x: x in [y[1] for y in modules.flags.metadata_scheme if y[1] == x],\n    expected_type=str\n)\nmetadata_created_by = get_config_item_or_set_default(\n    key='metadata_created_by',\n    default_value='',\n    validator=lambda x: isinstance(x, str),\n    expected_type=str\n)\n\nexample_inpaint_prompts = [[x] for x in example_inpaint_prompts]\n\nconfig_dict[\"default_loras\"] = default_loras = default_loras[:default_max_lora_number] + [[True, 'None', 1.0] for _ in range(default_max_lora_number - len(default_loras))]\n\n# mapping config to meta parameter \npossible_preset_keys = {\n    \"default_model\": \"base_model\",\n    \"default_refiner\": \"refiner_model\",\n    \"default_refiner_switch\": \"refiner_switch\",\n    \"previous_default_models\": \"previous_default_models\",\n    \"default_loras_min_weight\": \"default_loras_min_weight\",\n    \"default_loras_max_weight\": \"default_loras_max_weight\",\n    \"default_loras\": \"<processed>\",\n    \"default_cfg_scale\": \"guidance_scale\",\n    \"default_sample_sharpness\": \"sharpness\",\n    \"default_cfg_tsnr\": \"adaptive_cfg\",\n    \"default_clip_skip\": \"clip_skip\",\n    \"default_sampler\": \"sampler\",\n    \"default_scheduler\": \"scheduler\",\n    \"default_overwrite_step\": \"steps\",\n    \"default_performance\": \"performance\",\n    \"default_image_number\": \"image_number\",\n    \"default_prompt\": \"prompt\",\n    \"default_prompt_negative\": \"negative_prompt\",\n    \"default_styles\": \"styles\",\n    \"default_aspect_ratio\": \"resolution\",\n    \"default_save_metadata_to_images\": \"default_save_metadata_to_images\",\n    \"checkpoint_downloads\": \"checkpoint_downloads\",\n    \"embeddings_downloads\": \"embeddings_downloads\",\n    \"lora_downloads\": \"lora_downloads\",\n    \"default_vae\": \"vae\"\n}\n\nREWRITE_PRESET = False\n\nif REWRITE_PRESET and isinstance(args_manager.args.preset, str):\n    save_path = 'presets/' + args_manager.args.preset + '.json'\n    with open(save_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump({k: config_dict[k] for k in possible_preset_keys}, json_file, indent=4)\n    print(f'Preset saved to {save_path}. Exiting ...')\n    exit(0)\n\n\ndef add_ratio(x):\n    a, b = x.replace('*', ' ').split(' ')[:2]\n    a, b = int(a), int(b)\n    g = math.gcd(a, b)\n    return f'{a}\u00d7{b} <span style=\"color: grey;\"> \\U00002223 {a // g}:{b // g}</span>'\n\n\ndefault_aspect_ratio = add_ratio(default_aspect_ratio)\navailable_aspect_ratios_labels = [add_ratio(x) for x in available_aspect_ratios]\n\n\n# Only write config in the first launch.\nif not os.path.exists(config_path):\n    with open(config_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump({k: config_dict[k] for k in always_save_keys}, json_file, indent=4)\n\n\n# Always write tutorials.\nwith open(config_example_path, \"w\", encoding=\"utf-8\") as json_file:\n    cpa = config_path.replace(\"\\\\\", \"\\\\\\\\\")\n    json_file.write(f'You can modify your \"{cpa}\" using the below keys, formats, and examples.\\n'\n                    f'Do not modify this file. Modifications in this file will not take effect.\\n'\n                    f'This file is a tutorial and example. Please edit \"{cpa}\" to really change any settings.\\n'\n                    + 'Remember to split the paths with \"\\\\\\\\\" rather than \"\\\\\", '\n                      'and there is no \",\" before the last \"}\". \\n\\n\\n')\n    json.dump({k: config_dict[k] for k in visited_keys}, json_file, indent=4)\n\nmodel_filenames = []\nlora_filenames = []\nvae_filenames = []\nwildcard_filenames = []\n\n\ndef get_model_filenames(folder_paths, extensions=None, name_filter=None):\n    if extensions is None:\n        extensions = ['.pth', '.ckpt', '.bin', '.safetensors', '.fooocus.patch']\n    files = []\n\n    if not isinstance(folder_paths, list):\n        folder_paths = [folder_paths]\n    for folder in folder_paths:\n        files += get_files_from_folder(folder, extensions, name_filter)\n\n    return files\n\n\ndef update_files():\n    global model_filenames, lora_filenames, vae_filenames, wildcard_filenames, available_presets\n    model_filenames = get_model_filenames(paths_checkpoints)\n    lora_filenames = get_model_filenames(paths_loras)\n    vae_filenames = get_model_filenames(path_vae)\n    wildcard_filenames = get_files_from_folder(path_wildcards, ['.txt'])\n    available_presets = get_presets()\n    return\n\n\ndef downloading_inpaint_models(v):\n    assert v in modules.flags.inpaint_engine_versions\n\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/fooocus_inpaint_head.pth',\n        model_dir=path_inpaint,\n        file_name='fooocus_inpaint_head.pth'\n    )\n    head_file = os.path.join(path_inpaint, 'fooocus_inpaint_head.pth')\n    patch_file = None\n\n    if v == 'v1':\n        load_file_from_url(\n            url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint.fooocus.patch',\n            model_dir=path_inpaint,\n            file_name='inpaint.fooocus.patch'\n        )\n        patch_file = os.path.join(path_inpaint, 'inpaint.fooocus.patch')\n\n    if v == 'v2.5':\n        load_file_from_url(\n            url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint_v25.fooocus.patch',\n            model_dir=path_inpaint,\n            file_name='inpaint_v25.fooocus.patch'\n        )\n        patch_file = os.path.join(path_inpaint, 'inpaint_v25.fooocus.patch')\n\n    if v == 'v2.6':\n        load_file_from_url(\n            url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint_v26.fooocus.patch',\n            model_dir=path_inpaint,\n            file_name='inpaint_v26.fooocus.patch'\n        )\n        patch_file = os.path.join(path_inpaint, 'inpaint_v26.fooocus.patch')\n\n    return head_file, patch_file\n\n\ndef downloading_sdxl_lcm_lora():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/sdxl_lcm_lora.safetensors',\n        model_dir=paths_loras[0],\n        file_name=modules.flags.PerformanceLoRA.EXTREME_SPEED.value\n    )\n    return modules.flags.PerformanceLoRA.EXTREME_SPEED.value\n\n\ndef downloading_sdxl_lightning_lora():\n    load_file_from_url(\n        url='https://huggingface.co/mashb1t/misc/resolve/main/sdxl_lightning_4step_lora.safetensors',\n        model_dir=paths_loras[0],\n        file_name=modules.flags.PerformanceLoRA.LIGHTNING.value\n    )\n    return modules.flags.PerformanceLoRA.LIGHTNING.value\n\n\ndef downloading_sdxl_hyper_sd_lora():\n    load_file_from_url(\n        url='https://huggingface.co/mashb1t/misc/resolve/main/sdxl_hyper_sd_4step_lora.safetensors',\n        model_dir=paths_loras[0],\n        file_name=modules.flags.PerformanceLoRA.HYPER_SD.value\n    )\n    return modules.flags.PerformanceLoRA.HYPER_SD.value\n\n\ndef downloading_controlnet_canny():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/control-lora-canny-rank128.safetensors',\n        model_dir=path_controlnet,\n        file_name='control-lora-canny-rank128.safetensors'\n    )\n    return os.path.join(path_controlnet, 'control-lora-canny-rank128.safetensors')\n\n\ndef downloading_controlnet_cpds():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_xl_cpds_128.safetensors',\n        model_dir=path_controlnet,\n        file_name='fooocus_xl_cpds_128.safetensors'\n    )\n    return os.path.join(path_controlnet, 'fooocus_xl_cpds_128.safetensors')\n\n\ndef downloading_ip_adapters(v):\n    assert v in ['ip', 'face']\n\n    results = []\n\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/clip_vision_vit_h.safetensors',\n        model_dir=path_clip_vision,\n        file_name='clip_vision_vit_h.safetensors'\n    )\n    results += [os.path.join(path_clip_vision, 'clip_vision_vit_h.safetensors')]\n\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_ip_negative.safetensors',\n        model_dir=path_controlnet,\n        file_name='fooocus_ip_negative.safetensors'\n    )\n    results += [os.path.join(path_controlnet, 'fooocus_ip_negative.safetensors')]\n\n    if v == 'ip':\n        load_file_from_url(\n            url='https://huggingface.co/lllyasviel/misc/resolve/main/ip-adapter-plus_sdxl_vit-h.bin',\n            model_dir=path_controlnet,\n            file_name='ip-adapter-plus_sdxl_vit-h.bin'\n        )\n        results += [os.path.join(path_controlnet, 'ip-adapter-plus_sdxl_vit-h.bin')]\n\n    if v == 'face':\n        load_file_from_url(\n            url='https://huggingface.co/lllyasviel/misc/resolve/main/ip-adapter-plus-face_sdxl_vit-h.bin',\n            model_dir=path_controlnet,\n            file_name='ip-adapter-plus-face_sdxl_vit-h.bin'\n        )\n        results += [os.path.join(path_controlnet, 'ip-adapter-plus-face_sdxl_vit-h.bin')]\n\n    return results\n\n\ndef downloading_upscale_model():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_upscaler_s409985e5.bin',\n        model_dir=path_upscale_models,\n        file_name='fooocus_upscaler_s409985e5.bin'\n    )\n    return os.path.join(path_upscale_models, 'fooocus_upscaler_s409985e5.bin')\n\ndef downloading_safety_checker_model():\n    load_file_from_url(\n        url='https://huggingface.co/mashb1t/misc/resolve/main/stable-diffusion-safety-checker.bin',\n        model_dir=path_safety_checker,\n        file_name='stable-diffusion-safety-checker.bin'\n    )\n    return os.path.join(path_safety_checker, 'stable-diffusion-safety-checker.bin')\n\n\nupdate_files()\n", "modules/auth.py": "import json\nimport hashlib\nimport modules.constants as constants\n\nfrom os.path import exists\n\n\ndef auth_list_to_dict(auth_list):\n    auth_dict = {}\n    for auth_data in auth_list:\n        if 'user' in auth_data:\n            if 'hash' in auth_data:\n                auth_dict |= {auth_data['user']: auth_data['hash']}\n            elif 'pass' in auth_data:\n                auth_dict |= {auth_data['user']: hashlib.sha256(bytes(auth_data['pass'], encoding='utf-8')).hexdigest()}\n    return auth_dict\n\n\ndef load_auth_data(filename=None):\n    auth_dict = None\n    if filename != None and exists(filename):\n        with open(filename, encoding='utf-8') as auth_file:\n            try:\n                auth_obj = json.load(auth_file)\n                if isinstance(auth_obj, list) and len(auth_obj) > 0:\n                    auth_dict = auth_list_to_dict(auth_obj)\n            except Exception as e:\n                print('load_auth_data, e: ' + str(e))\n    return auth_dict\n\n\nauth_dict = load_auth_data(constants.AUTH_FILENAME)\n\nauth_enabled = auth_dict != None\n\n\ndef check_auth(user, password):\n    if user not in auth_dict:\n        return False\n    else:   \n        return hashlib.sha256(bytes(password, encoding='utf-8')).hexdigest() == auth_dict[user]\n", "modules/inpaint_worker.py": "import torch\nimport numpy as np\n\nfrom PIL import Image, ImageFilter\nfrom modules.util import resample_image, set_image_shape_ceil, get_image_shape_ceil\nfrom modules.upscaler import perform_upscale\nimport cv2\n\n\ninpaint_head_model = None\n\n\nclass InpaintHead(torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.head = torch.nn.Parameter(torch.empty(size=(320, 5, 3, 3), device='cpu'))\n\n    def __call__(self, x):\n        x = torch.nn.functional.pad(x, (1, 1, 1, 1), \"replicate\")\n        return torch.nn.functional.conv2d(input=x, weight=self.head)\n\n\ncurrent_task = None\n\n\ndef box_blur(x, k):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.BoxBlur(k))\n    return np.array(x)\n\n\ndef max_filter_opencv(x, ksize=3):\n    # Use OpenCV maximum filter\n    # Make sure the input type is int16\n    return cv2.dilate(x, np.ones((ksize, ksize), dtype=np.int16))\n\n\ndef morphological_open(x):\n    # Convert array to int16 type via threshold operation\n    x_int16 = np.zeros_like(x, dtype=np.int16)\n    x_int16[x > 127] = 256\n\n    for i in range(32):\n        # Use int16 type to avoid overflow\n        maxed = max_filter_opencv(x_int16, ksize=3) - 8\n        x_int16 = np.maximum(maxed, x_int16)\n\n    # Clip negative values to 0 and convert back to uint8 type\n    x_uint8 = np.clip(x_int16, 0, 255).astype(np.uint8)\n    return x_uint8\n\n\ndef up255(x, t=0):\n    y = np.zeros_like(x).astype(np.uint8)\n    y[x > t] = 255\n    return y\n\n\ndef imsave(x, path):\n    x = Image.fromarray(x)\n    x.save(path)\n\n\ndef regulate_abcd(x, a, b, c, d):\n    H, W = x.shape[:2]\n    if a < 0:\n        a = 0\n    if a > H:\n        a = H\n    if b < 0:\n        b = 0\n    if b > H:\n        b = H\n    if c < 0:\n        c = 0\n    if c > W:\n        c = W\n    if d < 0:\n        d = 0\n    if d > W:\n        d = W\n    return int(a), int(b), int(c), int(d)\n\n\ndef compute_initial_abcd(x):\n    indices = np.where(x)\n    a = np.min(indices[0])\n    b = np.max(indices[0])\n    c = np.min(indices[1])\n    d = np.max(indices[1])\n    abp = (b + a) // 2\n    abm = (b - a) // 2\n    cdp = (d + c) // 2\n    cdm = (d - c) // 2\n    l = int(max(abm, cdm) * 1.15)\n    a = abp - l\n    b = abp + l + 1\n    c = cdp - l\n    d = cdp + l + 1\n    a, b, c, d = regulate_abcd(x, a, b, c, d)\n    return a, b, c, d\n\n\ndef solve_abcd(x, a, b, c, d, k):\n    k = float(k)\n    assert 0.0 <= k <= 1.0\n\n    H, W = x.shape[:2]\n    if k == 1.0:\n        return 0, H, 0, W\n    while True:\n        if b - a >= H * k and d - c >= W * k:\n            break\n\n        add_h = (b - a) < (d - c)\n        add_w = not add_h\n\n        if b - a == H:\n            add_w = True\n\n        if d - c == W:\n            add_h = True\n\n        if add_h:\n            a -= 1\n            b += 1\n\n        if add_w:\n            c -= 1\n            d += 1\n\n        a, b, c, d = regulate_abcd(x, a, b, c, d)\n    return a, b, c, d\n\n\ndef fooocus_fill(image, mask):\n    current_image = image.copy()\n    raw_image = image.copy()\n    area = np.where(mask < 127)\n    store = raw_image[area]\n\n    for k, repeats in [(512, 2), (256, 2), (128, 4), (64, 4), (33, 8), (15, 8), (5, 16), (3, 16)]:\n        for _ in range(repeats):\n            current_image = box_blur(current_image, k)\n            current_image[area] = store\n\n    return current_image\n\n\nclass InpaintWorker:\n    def __init__(self, image, mask, use_fill=True, k=0.618):\n        a, b, c, d = compute_initial_abcd(mask > 0)\n        a, b, c, d = solve_abcd(mask, a, b, c, d, k=k)\n\n        # interested area\n        self.interested_area = (a, b, c, d)\n        self.interested_mask = mask[a:b, c:d]\n        self.interested_image = image[a:b, c:d]\n\n        # super resolution\n        if get_image_shape_ceil(self.interested_image) < 1024:\n            self.interested_image = perform_upscale(self.interested_image)\n\n        # resize to make images ready for diffusion\n        self.interested_image = set_image_shape_ceil(self.interested_image, 1024)\n        self.interested_fill = self.interested_image.copy()\n        H, W, C = self.interested_image.shape\n\n        # process mask\n        self.interested_mask = up255(resample_image(self.interested_mask, W, H), t=127)\n\n        # compute filling\n        if use_fill:\n            self.interested_fill = fooocus_fill(self.interested_image, self.interested_mask)\n\n        # soft pixels\n        self.mask = morphological_open(mask)\n        self.image = image\n\n        # ending\n        self.latent = None\n        self.latent_after_swap = None\n        self.swapped = False\n        self.latent_mask = None\n        self.inpaint_head_feature = None\n        return\n\n    def load_latent(self, latent_fill, latent_mask, latent_swap=None):\n        self.latent = latent_fill\n        self.latent_mask = latent_mask\n        self.latent_after_swap = latent_swap\n        return\n\n    def patch(self, inpaint_head_model_path, inpaint_latent, inpaint_latent_mask, model):\n        global inpaint_head_model\n\n        if inpaint_head_model is None:\n            inpaint_head_model = InpaintHead()\n            sd = torch.load(inpaint_head_model_path, map_location='cpu')\n            inpaint_head_model.load_state_dict(sd)\n\n        feed = torch.cat([\n            inpaint_latent_mask,\n            model.model.process_latent_in(inpaint_latent)\n        ], dim=1)\n\n        inpaint_head_model.to(device=feed.device, dtype=feed.dtype)\n        inpaint_head_feature = inpaint_head_model(feed)\n\n        def input_block_patch(h, transformer_options):\n            if transformer_options[\"block\"][1] == 0:\n                h = h + inpaint_head_feature.to(h)\n            return h\n\n        m = model.clone()\n        m.set_model_input_block_patch(input_block_patch)\n        return m\n\n    def swap(self):\n        if self.swapped:\n            return\n\n        if self.latent is None:\n            return\n\n        if self.latent_after_swap is None:\n            return\n\n        self.latent, self.latent_after_swap = self.latent_after_swap, self.latent\n        self.swapped = True\n        return\n\n    def unswap(self):\n        if not self.swapped:\n            return\n\n        if self.latent is None:\n            return\n\n        if self.latent_after_swap is None:\n            return\n\n        self.latent, self.latent_after_swap = self.latent_after_swap, self.latent\n        self.swapped = False\n        return\n\n    def color_correction(self, img):\n        fg = img.astype(np.float32)\n        bg = self.image.copy().astype(np.float32)\n        w = self.mask[:, :, None].astype(np.float32) / 255.0\n        y = fg * w + bg * (1 - w)\n        return y.clip(0, 255).astype(np.uint8)\n\n    def post_process(self, img):\n        a, b, c, d = self.interested_area\n        content = resample_image(img, d - c, b - a)\n        result = self.image.copy()\n        result[a:b, c:d] = content\n        result = self.color_correction(result)\n        return result\n\n    def visualize_mask_processing(self):\n        return [self.interested_fill, self.interested_mask, self.interested_image]\n\n", "modules/core.py": "import os\nimport einops\nimport torch\nimport numpy as np\n\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.model_detection\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.utils\nimport ldm_patched.modules.controlnet\nimport modules.sample_hijack\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.latent_formats\n\nfrom ldm_patched.modules.sd import load_checkpoint_guess_config\nfrom ldm_patched.contrib.external import VAEDecode, EmptyLatentImage, VAEEncode, VAEEncodeTiled, VAEDecodeTiled, \\\n    ControlNetApplyAdvanced\nfrom ldm_patched.contrib.external_freelunch import FreeU_V2\nfrom ldm_patched.modules.sample import prepare_mask\nfrom modules.lora import match_lora\nfrom modules.util import get_file_from_folder_list\nfrom ldm_patched.modules.lora import model_lora_keys_unet, model_lora_keys_clip\nfrom modules.config import path_embeddings\nfrom ldm_patched.contrib.external_model_advanced import ModelSamplingDiscrete, ModelSamplingContinuousEDM\n\nopEmptyLatentImage = EmptyLatentImage()\nopVAEDecode = VAEDecode()\nopVAEEncode = VAEEncode()\nopVAEDecodeTiled = VAEDecodeTiled()\nopVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nopModelSamplingContinuousEDM = ModelSamplingContinuousEDM()\n\n\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None, vae_filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision\n        self.filename = filename\n        self.vae_filename = vae_filename\n        self.unet_with_lora = unet\n        self.clip_with_lora = clip\n        self.visited_loras = ''\n\n        self.lora_key_map_unet = {}\n        self.lora_key_map_clip = {}\n\n        if self.unet is not None:\n            self.lora_key_map_unet = model_lora_keys_unet(self.unet.model, self.lora_key_map_unet)\n            self.lora_key_map_unet.update({x: x for x in self.unet.model.state_dict().keys()})\n\n        if self.clip is not None:\n            self.lora_key_map_clip = model_lora_keys_clip(self.clip.cond_stage_model, self.lora_key_map_clip)\n            self.lora_key_map_clip.update({x: x for x in self.clip.cond_stage_model.state_dict().keys()})\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def refresh_loras(self, loras):\n        assert isinstance(loras, list)\n\n        if self.visited_loras == str(loras):\n            return\n\n        self.visited_loras = str(loras)\n\n        if self.unet is None:\n            return\n\n        print(f'Request to load LoRAs {str(loras)} for model [{self.filename}].')\n\n        loras_to_load = []\n\n        for filename, weight in loras:\n            if filename == 'None':\n                continue\n\n            if os.path.exists(filename):\n                lora_filename = filename\n            else:\n                lora_filename = get_file_from_folder_list(filename, modules.config.paths_loras)\n\n            if not os.path.exists(lora_filename):\n                print(f'Lora file not found: {lora_filename}')\n                continue\n\n            loras_to_load.append((lora_filename, weight))\n\n        self.unet_with_lora = self.unet.clone() if self.unet is not None else None\n        self.clip_with_lora = self.clip.clone() if self.clip is not None else None\n\n        for lora_filename, weight in loras_to_load:\n            lora_unmatch = ldm_patched.modules.utils.load_torch_file(lora_filename, safe_load=False)\n            lora_unet, lora_unmatch = match_lora(lora_unmatch, self.lora_key_map_unet)\n            lora_clip, lora_unmatch = match_lora(lora_unmatch, self.lora_key_map_clip)\n\n            if len(lora_unmatch) > 12:\n                # model mismatch\n                continue\n\n            if len(lora_unmatch) > 0:\n                print(f'Loaded LoRA [{lora_filename}] for model [{self.filename}] '\n                      f'with unmatched keys {list(lora_unmatch.keys())}')\n\n            if self.unet_with_lora is not None and len(lora_unet) > 0:\n                loaded_keys = self.unet_with_lora.add_patches(lora_unet, weight)\n                print(f'Loaded LoRA [{lora_filename}] for UNet [{self.filename}] '\n                      f'with {len(loaded_keys)} keys at weight {weight}.')\n                for item in lora_unet:\n                    if item not in loaded_keys:\n                        print(\"UNet LoRA key skipped: \", item)\n\n            if self.clip_with_lora is not None and len(lora_clip) > 0:\n                loaded_keys = self.clip_with_lora.add_patches(lora_clip, weight)\n                print(f'Loaded LoRA [{lora_filename}] for CLIP [{self.filename}] '\n                      f'with {len(loaded_keys)} keys at weight {weight}.')\n                for item in lora_clip:\n                    if item not in loaded_keys:\n                        print(\"CLIP LoRA key skipped: \", item)\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef apply_freeu(model, b1, b2, s1, s2):\n    return opFreeU.patch(model=model, b1=b1, b2=b2, s1=s1, s2=s2)[0]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef load_controlnet(ckpt_filename):\n    return ldm_patched.modules.controlnet.load_controlnet(ckpt_filename)\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef apply_controlnet(positive, negative, control_net, image, strength, start_percent, end_percent):\n    return opControlNetApplyAdvanced.apply_controlnet(positive=positive, negative=negative, control_net=control_net,\n        image=image, strength=strength, start_percent=start_percent, end_percent=end_percent)\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef load_model(ckpt_filename, vae_filename=None):\n    unet, clip, vae, vae_filename, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings,\n                                                                vae_filename_param=vae_filename)\n    return StableDiffusionModel(unet=unet, clip=clip, vae=vae, clip_vision=clip_vision, filename=ckpt_filename, vae_filename=vae_filename)\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef generate_empty_latent(width=1024, height=1024, batch_size=1):\n    return opEmptyLatentImage.generate(width=width, height=height, batch_size=batch_size)[0]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef decode_vae(vae, latent_image, tiled=False):\n    if tiled:\n        return opVAEDecodeTiled.decode(samples=latent_image, vae=vae, tile_size=512)[0]\n    else:\n        return opVAEDecode.decode(samples=latent_image, vae=vae)[0]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef encode_vae(vae, pixels, tiled=False):\n    if tiled:\n        return opVAEEncodeTiled.encode(pixels=pixels, vae=vae, tile_size=512)[0]\n    else:\n        return opVAEEncode.encode(pixels=pixels, vae=vae)[0]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef encode_vae_inpaint(vae, pixels, mask):\n    assert mask.ndim == 3 and pixels.ndim == 4\n    assert mask.shape[-1] == pixels.shape[-2]\n    assert mask.shape[-2] == pixels.shape[-3]\n\n    w = mask.round()[..., None]\n    pixels = pixels * (1 - w) + 0.5 * w\n\n    latent = vae.encode(pixels)\n    B, C, H, W = latent.shape\n\n    latent_mask = mask[:, None, :, :]\n    latent_mask = torch.nn.functional.interpolate(latent_mask, size=(H * 8, W * 8), mode=\"bilinear\").round()\n    latent_mask = torch.nn.functional.max_pool2d(latent_mask, (8, 8)).round().to(latent)\n\n    return latent, latent_mask\n\n\nclass VAEApprox(torch.nn.Module):\n    def __init__(self):\n        super(VAEApprox, self).__init__()\n        self.conv1 = torch.nn.Conv2d(4, 8, (7, 7))\n        self.conv2 = torch.nn.Conv2d(8, 16, (5, 5))\n        self.conv3 = torch.nn.Conv2d(16, 32, (3, 3))\n        self.conv4 = torch.nn.Conv2d(32, 64, (3, 3))\n        self.conv5 = torch.nn.Conv2d(64, 32, (3, 3))\n        self.conv6 = torch.nn.Conv2d(32, 16, (3, 3))\n        self.conv7 = torch.nn.Conv2d(16, 8, (3, 3))\n        self.conv8 = torch.nn.Conv2d(8, 3, (3, 3))\n        self.current_type = None\n\n    def forward(self, x):\n        extra = 11\n        x = torch.nn.functional.interpolate(x, (x.shape[2] * 2, x.shape[3] * 2))\n        x = torch.nn.functional.pad(x, (extra, extra, extra, extra))\n        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6, self.conv7, self.conv8]:\n            x = layer(x)\n            x = torch.nn.functional.leaky_relu(x, 0.1)\n        return x\n\n\nVAE_approx_models = {}\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef get_previewer(model):\n    global VAE_approx_models\n\n    from modules.config import path_vae_approx\n    is_sdxl = isinstance(model.model.latent_format, ldm_patched.modules.latent_formats.SDXL)\n    vae_approx_filename = os.path.join(path_vae_approx, 'xlvaeapp.pth' if is_sdxl else 'vaeapp_sd15.pth')\n\n    if vae_approx_filename in VAE_approx_models:\n        VAE_approx_model = VAE_approx_models[vae_approx_filename]\n    else:\n        sd = torch.load(vae_approx_filename, map_location='cpu')\n        VAE_approx_model = VAEApprox()\n        VAE_approx_model.load_state_dict(sd)\n        del sd\n        VAE_approx_model.eval()\n\n        if ldm_patched.modules.model_management.should_use_fp16():\n            VAE_approx_model.half()\n            VAE_approx_model.current_type = torch.float16\n        else:\n            VAE_approx_model.float()\n            VAE_approx_model.current_type = torch.float32\n\n        VAE_approx_model.to(ldm_patched.modules.model_management.get_torch_device())\n        VAE_approx_models[vae_approx_filename] = VAE_approx_model\n\n    @torch.no_grad()\n    @torch.inference_mode()\n    def preview_function(x0, step, total_steps):\n        with torch.no_grad():\n            x_sample = x0.to(VAE_approx_model.current_type)\n            x_sample = VAE_approx_model(x_sample) * 127.5 + 127.5\n            x_sample = einops.rearrange(x_sample, 'b c h w -> b h w c')[0]\n            x_sample = x_sample.cpu().numpy().clip(0, 255).astype(np.uint8)\n            return x_sample\n\n    return preview_function\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef ksampler(model, positive, negative, latent, seed=None, steps=30, cfg=7.0, sampler_name='dpmpp_2m_sde_gpu',\n             scheduler='karras', denoise=1.0, disable_noise=False, start_step=None, last_step=None,\n             force_full_denoise=False, callback_function=None, refiner=None, refiner_switch=-1,\n             previewer_start=None, previewer_end=None, sigmas=None, noise_mean=None, disable_preview=False):\n\n    if sigmas is not None:\n        sigmas = sigmas.clone().to(ldm_patched.modules.model_management.get_torch_device())\n\n    latent_image = latent[\"samples\"]\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = ldm_patched.modules.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    if isinstance(noise_mean, torch.Tensor):\n        noise = noise + noise_mean - torch.mean(noise, dim=1, keepdim=True)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    previewer = get_previewer(model)\n\n    if previewer_start is None:\n        previewer_start = 0\n\n    if previewer_end is None:\n        previewer_end = steps\n\n    def callback(step, x0, x, total_steps):\n        ldm_patched.modules.model_management.throw_exception_if_processing_interrupted()\n        y = None\n        if previewer is not None and not disable_preview:\n            y = previewer(x0, previewer_start + step, previewer_end)\n        if callback_function is not None:\n            callback_function(previewer_start + step, x0, x, previewer_end, y)\n\n    disable_pbar = False\n    modules.sample_hijack.current_refiner = refiner\n    modules.sample_hijack.refiner_switch_step = refiner_switch\n    ldm_patched.modules.samplers.sample = modules.sample_hijack.sample_hacked\n\n    try:\n        samples = ldm_patched.modules.sample.sample(model,\n                                                    noise, steps, cfg, sampler_name, scheduler,\n                                                    positive, negative, latent_image,\n                                                    denoise=denoise, disable_noise=disable_noise,\n                                                    start_step=start_step,\n                                                    last_step=last_step,\n                                                    force_full_denoise=force_full_denoise, noise_mask=noise_mask,\n                                                    callback=callback,\n                                                    disable_pbar=disable_pbar, seed=seed, sigmas=sigmas)\n\n        out = latent.copy()\n        out[\"samples\"] = samples\n    finally:\n        modules.sample_hijack.current_refiner = None\n\n    return out\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef pytorch_to_numpy(x):\n    return [np.clip(255. * y.cpu().numpy(), 0, 255).astype(np.uint8) for y in x]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef numpy_to_pytorch(x):\n    y = x.astype(np.float32) / 255.0\n    y = y[None]\n    y = np.ascontiguousarray(y.copy())\n    y = torch.from_numpy(y).float()\n    return y\n", "modules/sample_hijack.py": "import torch\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.model_management\n\nfrom collections import namedtuple\nfrom ldm_patched.contrib.external_align_your_steps import AlignYourStepsScheduler\nfrom ldm_patched.contrib.external_custom_sampler import SDTurboScheduler\nfrom ldm_patched.k_diffusion import sampling as k_diffusion_sampling\nfrom ldm_patched.modules.samplers import normal_scheduler, simple_scheduler, ddim_scheduler\nfrom ldm_patched.modules.model_base import SDXLRefiner, SDXL\nfrom ldm_patched.modules.conds import CONDRegular\nfrom ldm_patched.modules.sample import get_additional_models, get_models_from_cond, cleanup_additional_models\nfrom ldm_patched.modules.samplers import resolve_areas_and_cond_masks, wrap_model, calculate_start_end_timesteps, \\\n    create_cond_with_same_area_if_none, pre_run_control, apply_empty_x_to_equal_area, encode_model_conds\n\n\ncurrent_refiner = None\nrefiner_switch_step = -1\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_separate_inner(c, p, target_model=None, target_clip=None):\n    if target_model is None or isinstance(target_model, SDXLRefiner):\n        c = c[..., -1280:].clone()\n    elif isinstance(target_model, SDXL):\n        c = c.clone()\n    else:\n        p = None\n        c = c[..., :768].clone()\n\n        final_layer_norm = target_clip.cond_stage_model.clip_l.transformer.text_model.final_layer_norm\n\n        final_layer_norm_origin_device = final_layer_norm.weight.device\n        final_layer_norm_origin_dtype = final_layer_norm.weight.dtype\n\n        c_origin_device = c.device\n        c_origin_dtype = c.dtype\n\n        final_layer_norm.to(device='cpu', dtype=torch.float32)\n        c = c.to(device='cpu', dtype=torch.float32)\n\n        c = torch.chunk(c, int(c.size(1)) // 77, 1)\n        c = [final_layer_norm(ci) for ci in c]\n        c = torch.cat(c, dim=1)\n\n        final_layer_norm.to(device=final_layer_norm_origin_device, dtype=final_layer_norm_origin_dtype)\n        c = c.to(device=c_origin_device, dtype=c_origin_dtype)\n    return c, p\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_separate(cond, target_model=None, target_clip=None):\n    results = []\n\n    for c, px in cond:\n        p = px.get('pooled_output', None)\n        c, p = clip_separate_inner(c, p, target_model=target_model, target_clip=target_clip)\n        p = {} if p is None else {'pooled_output': p.clone()}\n        results.append([c, p])\n\n    return results\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_separate_after_preparation(cond, target_model=None, target_clip=None):\n    results = []\n\n    for x in cond:\n        p = x.get('pooled_output', None)\n        c = x['model_conds']['c_crossattn'].cond\n\n        c, p = clip_separate_inner(c, p, target_model=target_model, target_clip=target_clip)\n\n        result = {'model_conds': {'c_crossattn': CONDRegular(c)}}\n\n        if p is not None:\n            result['pooled_output'] = p.clone()\n\n        results.append(result)\n\n    return results\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef sample_hacked(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    global current_refiner\n\n    positive = positive[:]\n    negative = negative[:]\n\n    resolve_areas_and_cond_masks(positive, noise.shape[2], noise.shape[3], device)\n    resolve_areas_and_cond_masks(negative, noise.shape[2], noise.shape[3], device)\n\n    model_wrap = wrap_model(model)\n\n    calculate_start_end_timesteps(model, negative)\n    calculate_start_end_timesteps(model, positive)\n\n    if latent_image is not None:\n        latent_image = model.process_latent_in(latent_image)\n\n    if hasattr(model, 'extra_conds'):\n        positive = encode_model_conds(model.extra_conds, positive, noise, device, \"positive\", latent_image=latent_image, denoise_mask=denoise_mask)\n        negative = encode_model_conds(model.extra_conds, negative, noise, device, \"negative\", latent_image=latent_image, denoise_mask=denoise_mask)\n\n    #make sure each cond area has an opposite one with the same area\n    for c in positive:\n        create_cond_with_same_area_if_none(negative, c)\n    for c in negative:\n        create_cond_with_same_area_if_none(positive, c)\n\n    # pre_run_control(model, negative + positive)\n    pre_run_control(model, positive)  # negative is not necessary in Fooocus, 0.5s faster.\n\n    apply_empty_x_to_equal_area(list(filter(lambda c: c.get('control_apply_to_uncond', False) == True, positive)), negative, 'control', lambda cond_cnets, x: cond_cnets[x])\n    apply_empty_x_to_equal_area(positive, negative, 'gligen', lambda cond_cnets, x: cond_cnets[x])\n\n    extra_args = {\"cond\":positive, \"uncond\":negative, \"cond_scale\": cfg, \"model_options\": model_options, \"seed\":seed}\n\n    if current_refiner is not None and hasattr(current_refiner.model, 'extra_conds'):\n        positive_refiner = clip_separate_after_preparation(positive, target_model=current_refiner.model)\n        negative_refiner = clip_separate_after_preparation(negative, target_model=current_refiner.model)\n\n        positive_refiner = encode_model_conds(current_refiner.model.extra_conds, positive_refiner, noise, device, \"positive\", latent_image=latent_image, denoise_mask=denoise_mask)\n        negative_refiner = encode_model_conds(current_refiner.model.extra_conds, negative_refiner, noise, device, \"negative\", latent_image=latent_image, denoise_mask=denoise_mask)\n\n    def refiner_switch():\n        cleanup_additional_models(set(get_models_from_cond(positive, \"control\") + get_models_from_cond(negative, \"control\")))\n\n        extra_args[\"cond\"] = positive_refiner\n        extra_args[\"uncond\"] = negative_refiner\n\n        # clear ip-adapter for refiner\n        extra_args['model_options'] = {k: {} if k == 'transformer_options' else v for k, v in extra_args['model_options'].items()}\n\n        models, inference_memory = get_additional_models(positive_refiner, negative_refiner, current_refiner.model_dtype())\n        ldm_patched.modules.model_management.load_models_gpu(\n            [current_refiner] + models,\n            model.memory_required([noise.shape[0] * 2] + list(noise.shape[1:])) + inference_memory)\n\n        model_wrap.inner_model = current_refiner.model\n        print('Refiner Swapped')\n        return\n\n    def callback_wrap(step, x0, x, total_steps):\n        if step == refiner_switch_step and current_refiner is not None:\n            refiner_switch()\n        if callback is not None:\n            # residual_noise_preview = x - x0\n            # residual_noise_preview /= residual_noise_preview.std()\n            # residual_noise_preview *= x0.std()\n            callback(step, x0, x, total_steps)\n\n    samples = sampler.sample(model_wrap, sigmas, extra_args, callback_wrap, noise, latent_image, denoise_mask, disable_pbar)\n    return model.process_latent_out(samples.to(torch.float32))\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef calculate_sigmas_scheduler_hacked(model, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model, steps)\n    elif scheduler_name == \"simple\":\n        sigmas = simple_scheduler(model, steps)\n    elif scheduler_name == \"ddim_uniform\":\n        sigmas = ddim_scheduler(model, steps)\n    elif scheduler_name == \"sgm_uniform\":\n        sigmas = normal_scheduler(model, steps, sgm=True)\n    elif scheduler_name == \"turbo\":\n        sigmas = SDTurboScheduler().get_sigmas(model=model, steps=steps, denoise=1.0)[0]\n    elif scheduler_name == \"align_your_steps\":\n        model_type = 'SDXL' if isinstance(model.latent_format, ldm_patched.modules.latent_formats.SDXL) else 'SD1'\n        sigmas = AlignYourStepsScheduler().get_sigmas(model_type=model_type, steps=steps, denoise=1.0)[0]\n    else:\n        raise TypeError(\"error invalid scheduler\")\n    return sigmas\n\n\nldm_patched.modules.samplers.calculate_sigmas_scheduler = calculate_sigmas_scheduler_hacked\nldm_patched.modules.samplers.sample = sample_hacked\n", "modules/patch_precision.py": "# Consistent with Kohya to reduce differences between model training and inference.\n\nimport torch\nimport math\nimport einops\nimport numpy as np\n\nimport ldm_patched.ldm.modules.diffusionmodules.openaimodel\nimport ldm_patched.modules.model_sampling\nimport ldm_patched.modules.sd1_clip\n\nfrom ldm_patched.ldm.modules.diffusionmodules.util import make_beta_schedule\n\n\ndef patched_timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    # Consistent with Kohya to reduce differences between model training and inference.\n\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=timesteps.device)\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = einops.repeat(timesteps, 'b -> b d', d=dim)\n    return embedding\n\n\ndef patched_register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    # Consistent with Kohya to reduce differences between model training and inference.\n\n    if given_betas is not None:\n        betas = given_betas\n    else:\n        betas = make_beta_schedule(\n            beta_schedule,\n            timesteps,\n            linear_start=linear_start,\n            linear_end=linear_end,\n            cosine_s=cosine_s)\n\n    alphas = 1. - betas\n    alphas_cumprod = np.cumprod(alphas, axis=0)\n    timesteps, = betas.shape\n    self.num_timesteps = int(timesteps)\n    self.linear_start = linear_start\n    self.linear_end = linear_end\n    sigmas = torch.tensor(((1 - alphas_cumprod) / alphas_cumprod) ** 0.5, dtype=torch.float32)\n    self.set_sigmas(sigmas)\n    alphas_cumprod = torch.tensor(alphas_cumprod, dtype=torch.float32)\n    self.set_alphas_cumprod(alphas_cumprod)\n    return\n\n\ndef patch_all_precision():\n    ldm_patched.ldm.modules.diffusionmodules.openaimodel.timestep_embedding = patched_timestep_embedding\n    ldm_patched.modules.model_sampling.ModelSamplingDiscrete._register_schedule = patched_register_schedule\n    return\n", "modules/ops.py": "import torch\nimport contextlib\n\n\n@contextlib.contextmanager\ndef use_patched_ops(operations):\n    op_names = ['Linear', 'Conv2d', 'Conv3d', 'GroupNorm', 'LayerNorm']\n    backups = {op_name: getattr(torch.nn, op_name) for op_name in op_names}\n\n    try:\n        for op_name in op_names:\n            setattr(torch.nn, op_name, getattr(operations, op_name))\n\n        yield\n\n    finally:\n        for op_name in op_names:\n            setattr(torch.nn, op_name, backups[op_name])\n    return\n", "modules/ui_gradio_extensions.py": "# based on https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/v1.6.0/modules/ui_gradio_extensions.py\n\nimport os\nimport gradio as gr\nimport args_manager\n\nfrom modules.localization import localization_js\n\n\nGradioTemplateResponseOriginal = gr.routes.templates.TemplateResponse\n\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\n\n\ndef webpath(fn):\n    if fn.startswith(script_path):\n        web_path = os.path.relpath(fn, script_path).replace('\\\\', '/')\n    else:\n        web_path = os.path.abspath(fn)\n\n    return f'file={web_path}?{os.path.getmtime(fn)}'\n\n\ndef javascript_html():\n    script_js_path = webpath('javascript/script.js')\n    context_menus_js_path = webpath('javascript/contextMenus.js')\n    localization_js_path = webpath('javascript/localization.js')\n    zoom_js_path = webpath('javascript/zoom.js')\n    edit_attention_js_path = webpath('javascript/edit-attention.js')\n    viewer_js_path = webpath('javascript/viewer.js')\n    image_viewer_js_path = webpath('javascript/imageviewer.js')\n    samples_path = webpath(os.path.abspath('./sdxl_styles/samples/fooocus_v2.jpg'))\n    head = f'<script type=\"text/javascript\">{localization_js(args_manager.args.language)}</script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{script_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{context_menus_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{localization_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{zoom_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{edit_attention_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{viewer_js_path}\"></script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{image_viewer_js_path}\"></script>\\n'\n    head += f'<meta name=\"samples-path\" content=\"{samples_path}\">\\n'\n\n    if args_manager.args.theme:\n        head += f'<script type=\"text/javascript\">set_theme(\\\"{args_manager.args.theme}\\\");</script>\\n'\n\n    return head\n\n\ndef css_html():\n    style_css_path = webpath('css/style.css')\n    head = f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{style_css_path}\">'\n    return head\n\n\ndef reload_javascript():\n    js = javascript_html()\n    css = css_html()\n\n    def template_response(*args, **kwargs):\n        res = GradioTemplateResponseOriginal(*args, **kwargs)\n        res.body = res.body.replace(b'</head>', f'{js}</head>'.encode(\"utf8\"))\n        res.body = res.body.replace(b'</body>', f'{css}</body>'.encode(\"utf8\"))\n        res.init_headers()\n        return res\n\n    gr.routes.templates.TemplateResponse = template_response\n", "modules/anisotropic.py": "import torch\n\n\nTensor = torch.Tensor\nDevice = torch.DeviceObjType\nDtype = torch.Type\npad = torch.nn.functional.pad\n\n\ndef _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    ky, kx = _unpack_2d_ks(kernel_size)\n    return (ky - 1) // 2, (kx - 1) // 2\n\n\ndef _unpack_2d_ks(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    if isinstance(kernel_size, int):\n        ky = kx = kernel_size\n    else:\n        assert len(kernel_size) == 2, '2D Kernel size should have a length of 2.'\n        ky, kx = kernel_size\n\n    ky = int(ky)\n    kx = int(kx)\n    return ky, kx\n\n\ndef gaussian(\n    window_size: int, sigma: Tensor | float, *, device: Device | None = None, dtype: Dtype | None = None\n) -> Tensor:\n\n    batch_size = sigma.shape[0]\n\n    x = (torch.arange(window_size, device=sigma.device, dtype=sigma.dtype) - window_size // 2).expand(batch_size, -1)\n\n    if window_size % 2 == 0:\n        x = x + 0.5\n\n    gauss = torch.exp(-x.pow(2.0) / (2 * sigma.pow(2.0)))\n\n    return gauss / gauss.sum(-1, keepdim=True)\n\n\ndef get_gaussian_kernel1d(\n    kernel_size: int,\n    sigma: float | Tensor,\n    force_even: bool = False,\n    *,\n    device: Device | None = None,\n    dtype: Dtype | None = None,\n) -> Tensor:\n\n    return gaussian(kernel_size, sigma, device=device, dtype=dtype)\n\n\ndef get_gaussian_kernel2d(\n    kernel_size: tuple[int, int] | int,\n    sigma: tuple[float, float] | Tensor,\n    force_even: bool = False,\n    *,\n    device: Device | None = None,\n    dtype: Dtype | None = None,\n) -> Tensor:\n\n    sigma = torch.Tensor([[sigma, sigma]]).to(device=device, dtype=dtype)\n\n    ksize_y, ksize_x = _unpack_2d_ks(kernel_size)\n    sigma_y, sigma_x = sigma[:, 0, None], sigma[:, 1, None]\n\n    kernel_y = get_gaussian_kernel1d(ksize_y, sigma_y, force_even, device=device, dtype=dtype)[..., None]\n    kernel_x = get_gaussian_kernel1d(ksize_x, sigma_x, force_even, device=device, dtype=dtype)[..., None]\n\n    return kernel_y * kernel_x.view(-1, 1, ksize_x)\n\n\ndef _bilateral_blur(\n    input: Tensor,\n    guidance: Tensor | None,\n    kernel_size: tuple[int, int] | int,\n    sigma_color: float | Tensor,\n    sigma_space: tuple[float, float] | Tensor,\n    border_type: str = 'reflect',\n    color_distance_type: str = 'l1',\n) -> Tensor:\n\n    if isinstance(sigma_color, Tensor):\n        sigma_color = sigma_color.to(device=input.device, dtype=input.dtype).view(-1, 1, 1, 1, 1)\n\n    ky, kx = _unpack_2d_ks(kernel_size)\n    pad_y, pad_x = _compute_zero_padding(kernel_size)\n\n    padded_input = pad(input, (pad_x, pad_x, pad_y, pad_y), mode=border_type)\n    unfolded_input = padded_input.unfold(2, ky, 1).unfold(3, kx, 1).flatten(-2)  # (B, C, H, W, Ky x Kx)\n\n    if guidance is None:\n        guidance = input\n        unfolded_guidance = unfolded_input\n    else:\n        padded_guidance = pad(guidance, (pad_x, pad_x, pad_y, pad_y), mode=border_type)\n        unfolded_guidance = padded_guidance.unfold(2, ky, 1).unfold(3, kx, 1).flatten(-2)  # (B, C, H, W, Ky x Kx)\n\n    diff = unfolded_guidance - guidance.unsqueeze(-1)\n    if color_distance_type == \"l1\":\n        color_distance_sq = diff.abs().sum(1, keepdim=True).square()\n    elif color_distance_type == \"l2\":\n        color_distance_sq = diff.square().sum(1, keepdim=True)\n    else:\n        raise ValueError(\"color_distance_type only acceps l1 or l2\")\n    color_kernel = (-0.5 / sigma_color**2 * color_distance_sq).exp()  # (B, 1, H, W, Ky x Kx)\n\n    space_kernel = get_gaussian_kernel2d(kernel_size, sigma_space, device=input.device, dtype=input.dtype)\n    space_kernel = space_kernel.view(-1, 1, 1, 1, kx * ky)\n\n    kernel = space_kernel * color_kernel\n    out = (unfolded_input * kernel).sum(-1) / kernel.sum(-1)\n    return out\n\n\ndef bilateral_blur(\n    input: Tensor,\n    kernel_size: tuple[int, int] | int = (13, 13),\n    sigma_color: float | Tensor = 3.0,\n    sigma_space: tuple[float, float] | Tensor = 3.0,\n    border_type: str = 'reflect',\n    color_distance_type: str = 'l1',\n) -> Tensor:\n    return _bilateral_blur(input, None, kernel_size, sigma_color, sigma_space, border_type, color_distance_type)\n\n\ndef adaptive_anisotropic_filter(x, g=None):\n    if g is None:\n        g = x\n    s, m = torch.std_mean(g, dim=(1, 2, 3), keepdim=True)\n    s = s + 1e-5\n    guidance = (g - m) / s\n    y = _bilateral_blur(x, guidance,\n                        kernel_size=(13, 13),\n                        sigma_color=3.0,\n                        sigma_space=3.0,\n                        border_type='reflect',\n                        color_distance_type='l1')\n    return y\n\n\ndef joint_bilateral_blur(\n    input: Tensor,\n    guidance: Tensor,\n    kernel_size: tuple[int, int] | int,\n    sigma_color: float | Tensor,\n    sigma_space: tuple[float, float] | Tensor,\n    border_type: str = 'reflect',\n    color_distance_type: str = 'l1',\n) -> Tensor:\n    return _bilateral_blur(input, guidance, kernel_size, sigma_color, sigma_space, border_type, color_distance_type)\n\n\nclass _BilateralBlur(torch.nn.Module):\n    def __init__(\n        self,\n        kernel_size: tuple[int, int] | int,\n        sigma_color: float | Tensor,\n        sigma_space: tuple[float, float] | Tensor,\n        border_type: str = 'reflect',\n        color_distance_type: str = \"l1\",\n    ) -> None:\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.sigma_color = sigma_color\n        self.sigma_space = sigma_space\n        self.border_type = border_type\n        self.color_distance_type = color_distance_type\n\n    def __repr__(self) -> str:\n        return (\n            f\"{self.__class__.__name__}\"\n            f\"(kernel_size={self.kernel_size}, \"\n            f\"sigma_color={self.sigma_color}, \"\n            f\"sigma_space={self.sigma_space}, \"\n            f\"border_type={self.border_type}, \"\n            f\"color_distance_type={self.color_distance_type})\"\n        )\n\n\nclass BilateralBlur(_BilateralBlur):\n    def forward(self, input: Tensor) -> Tensor:\n        return bilateral_blur(\n            input, self.kernel_size, self.sigma_color, self.sigma_space, self.border_type, self.color_distance_type\n        )\n\n\nclass JointBilateralBlur(_BilateralBlur):\n    def forward(self, input: Tensor, guidance: Tensor) -> Tensor:\n        return joint_bilateral_blur(\n            input,\n            guidance,\n            self.kernel_size,\n            self.sigma_color,\n            self.sigma_space,\n            self.border_type,\n            self.color_distance_type,\n        )\n", "modules/patch.py": "import os\nimport torch\nimport time\nimport math\nimport ldm_patched.modules.model_base\nimport ldm_patched.ldm.modules.diffusionmodules.openaimodel\nimport ldm_patched.modules.model_management\nimport modules.anisotropic as anisotropic\nimport ldm_patched.ldm.modules.attention\nimport ldm_patched.k_diffusion.sampling\nimport ldm_patched.modules.sd1_clip\nimport modules.inpaint_worker as inpaint_worker\nimport ldm_patched.ldm.modules.diffusionmodules.openaimodel\nimport ldm_patched.ldm.modules.diffusionmodules.model\nimport ldm_patched.modules.sd\nimport ldm_patched.controlnet.cldm\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.args_parser\nimport warnings\nimport safetensors.torch\nimport modules.constants as constants\n\nfrom ldm_patched.modules.samplers import calc_cond_uncond_batch\nfrom ldm_patched.k_diffusion.sampling import BatchedBrownianTree\nfrom ldm_patched.ldm.modules.diffusionmodules.openaimodel import forward_timestep_embed, apply_control\nfrom modules.patch_precision import patch_all_precision\nfrom modules.patch_clip import patch_all_clip\n\n\nclass PatchSettings:\n    def __init__(self,\n                 sharpness=2.0,\n                 adm_scaler_end=0.3,\n                 positive_adm_scale=1.5,\n                 negative_adm_scale=0.8,\n                 controlnet_softness=0.25,\n                 adaptive_cfg=7.0):\n        self.sharpness = sharpness\n        self.adm_scaler_end = adm_scaler_end\n        self.positive_adm_scale = positive_adm_scale\n        self.negative_adm_scale = negative_adm_scale\n        self.controlnet_softness = controlnet_softness\n        self.adaptive_cfg = adaptive_cfg\n        self.global_diffusion_progress = 0\n        self.eps_record = None\n\n\npatch_settings = {}\n\n\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n\n        if strength_model != 1.0:\n            weight *= strength_model\n\n        if isinstance(v, list):\n            v = (self.calculate_weight(v[1:], v[0].clone(), key),)\n\n        if len(v) == 1:\n            patch_type = \"diff\"\n        elif len(v) == 2:\n            patch_type = v[0]\n            v = v[1]\n\n        if patch_type == \"diff\":\n            w1 = v[0]\n            if alpha != 0.0:\n                if w1.shape != weight.shape:\n                    print(\"WARNING SHAPE MISMATCH {} WEIGHT NOT MERGED {} != {}\".format(key, w1.shape, weight.shape))\n                else:\n                    weight += alpha * ldm_patched.modules.model_management.cast_to_device(w1, weight.device, weight.dtype)\n        elif patch_type == \"lora\":\n            mat1 = ldm_patched.modules.model_management.cast_to_device(v[0], weight.device, torch.float32)\n            mat2 = ldm_patched.modules.model_management.cast_to_device(v[1], weight.device, torch.float32)\n            if v[2] is not None:\n                alpha *= v[2] / mat2.shape[0]\n            if v[3] is not None:\n                mat3 = ldm_patched.modules.model_management.cast_to_device(v[3], weight.device, torch.float32)\n                final_shape = [mat2.shape[1], mat2.shape[0], mat3.shape[2], mat3.shape[3]]\n                mat2 = torch.mm(mat2.transpose(0, 1).flatten(start_dim=1),\n                                mat3.transpose(0, 1).flatten(start_dim=1)).reshape(final_shape).transpose(0, 1)\n            try:\n                weight += (alpha * torch.mm(mat1.flatten(start_dim=1), mat2.flatten(start_dim=1))).reshape(\n                    weight.shape).type(weight.dtype)\n            except Exception as e:\n                print(\"ERROR\", key, e)\n        elif patch_type == \"fooocus\":\n            w1 = ldm_patched.modules.model_management.cast_to_device(v[0], weight.device, torch.float32)\n            w_min = ldm_patched.modules.model_management.cast_to_device(v[1], weight.device, torch.float32)\n            w_max = ldm_patched.modules.model_management.cast_to_device(v[2], weight.device, torch.float32)\n            w1 = (w1 / 255.0) * (w_max - w_min) + w_min\n            if alpha != 0.0:\n                if w1.shape != weight.shape:\n                    print(\"WARNING SHAPE MISMATCH {} FOOOCUS WEIGHT NOT MERGED {} != {}\".format(key, w1.shape, weight.shape))\n                else:\n                    weight += alpha * ldm_patched.modules.model_management.cast_to_device(w1, weight.device, weight.dtype)\n        elif patch_type == \"lokr\":\n            w1 = v[0]\n            w2 = v[1]\n            w1_a = v[3]\n            w1_b = v[4]\n            w2_a = v[5]\n            w2_b = v[6]\n            t2 = v[7]\n            dim = None\n\n            if w1 is None:\n                dim = w1_b.shape[0]\n                w1 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w1_a, weight.device, torch.float32),\n                              ldm_patched.modules.model_management.cast_to_device(w1_b, weight.device, torch.float32))\n            else:\n                w1 = ldm_patched.modules.model_management.cast_to_device(w1, weight.device, torch.float32)\n\n            if w2 is None:\n                dim = w2_b.shape[0]\n                if t2 is None:\n                    w2 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w2_a, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w2_b, weight.device, torch.float32))\n                else:\n                    w2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                      ldm_patched.modules.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w2_b, weight.device, torch.float32),\n                                      ldm_patched.modules.model_management.cast_to_device(w2_a, weight.device, torch.float32))\n            else:\n                w2 = ldm_patched.modules.model_management.cast_to_device(w2, weight.device, torch.float32)\n\n            if len(w2.shape) == 4:\n                w1 = w1.unsqueeze(2).unsqueeze(2)\n            if v[2] is not None and dim is not None:\n                alpha *= v[2] / dim\n\n            try:\n                weight += alpha * torch.kron(w1, w2).reshape(weight.shape).type(weight.dtype)\n            except Exception as e:\n                print(\"ERROR\", key, e)\n        elif patch_type == \"loha\":\n            w1a = v[0]\n            w1b = v[1]\n            if v[2] is not None:\n                alpha *= v[2] / w1b.shape[0]\n            w2a = v[3]\n            w2b = v[4]\n            if v[5] is not None:  # cp decomposition\n                t1 = v[5]\n                t2 = v[6]\n                m1 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                  ldm_patched.modules.model_management.cast_to_device(t1, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w1b, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w1a, weight.device, torch.float32))\n\n                m2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                  ldm_patched.modules.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w2b, weight.device, torch.float32),\n                                  ldm_patched.modules.model_management.cast_to_device(w2a, weight.device, torch.float32))\n            else:\n                m1 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w1a, weight.device, torch.float32),\n                              ldm_patched.modules.model_management.cast_to_device(w1b, weight.device, torch.float32))\n                m2 = torch.mm(ldm_patched.modules.model_management.cast_to_device(w2a, weight.device, torch.float32),\n                              ldm_patched.modules.model_management.cast_to_device(w2b, weight.device, torch.float32))\n\n            try:\n                weight += (alpha * m1 * m2).reshape(weight.shape).type(weight.dtype)\n            except Exception as e:\n                print(\"ERROR\", key, e)\n        elif patch_type == \"glora\":\n            if v[4] is not None:\n                alpha *= v[4] / v[0].shape[0]\n\n            a1 = ldm_patched.modules.model_management.cast_to_device(v[0].flatten(start_dim=1), weight.device, torch.float32)\n            a2 = ldm_patched.modules.model_management.cast_to_device(v[1].flatten(start_dim=1), weight.device, torch.float32)\n            b1 = ldm_patched.modules.model_management.cast_to_device(v[2].flatten(start_dim=1), weight.device, torch.float32)\n            b2 = ldm_patched.modules.model_management.cast_to_device(v[3].flatten(start_dim=1), weight.device, torch.float32)\n\n            weight += ((torch.mm(b2, b1) + torch.mm(torch.mm(weight.flatten(start_dim=1), a2), a1)) * alpha).reshape(weight.shape).type(weight.dtype)\n        else:\n            print(\"patch type not recognized\", patch_type, key)\n\n    return weight\n\n\nclass BrownianTreeNoiseSamplerPatched:\n    transform = None\n    tree = None\n\n    @staticmethod\n    def global_init(x, sigma_min, sigma_max, seed=None, transform=lambda x: x, cpu=False):\n        if ldm_patched.modules.model_management.directml_enabled:\n            cpu = True\n\n        t0, t1 = transform(torch.as_tensor(sigma_min)), transform(torch.as_tensor(sigma_max))\n\n        BrownianTreeNoiseSamplerPatched.transform = transform\n        BrownianTreeNoiseSamplerPatched.tree = BatchedBrownianTree(x, t0, t1, seed, cpu=cpu)\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def __call__(sigma, sigma_next):\n        transform = BrownianTreeNoiseSamplerPatched.transform\n        tree = BrownianTreeNoiseSamplerPatched.tree\n\n        t0, t1 = transform(torch.as_tensor(sigma)), transform(torch.as_tensor(sigma_next))\n        return tree(t0, t1) / (t1 - t0).abs().sqrt()\n\n\ndef compute_cfg(uncond, cond, cfg_scale, t):\n    pid = os.getpid()\n    mimic_cfg = float(patch_settings[pid].adaptive_cfg)\n    real_cfg = float(cfg_scale)\n\n    real_eps = uncond + real_cfg * (cond - uncond)\n\n    if cfg_scale > patch_settings[pid].adaptive_cfg:\n        mimicked_eps = uncond + mimic_cfg * (cond - uncond)\n        return real_eps * t + mimicked_eps * (1 - t)\n    else:\n        return real_eps\n\n\ndef patched_sampling_function(model, x, timestep, uncond, cond, cond_scale, model_options=None, seed=None):\n    pid = os.getpid()\n\n    if math.isclose(cond_scale, 1.0) and not model_options.get(\"disable_cfg1_optimization\", False):\n        final_x0 = calc_cond_uncond_batch(model, cond, None, x, timestep, model_options)[0]\n\n        if patch_settings[pid].eps_record is not None:\n            patch_settings[pid].eps_record = ((x - final_x0) / timestep).cpu()\n\n        return final_x0\n\n    positive_x0, negative_x0 = calc_cond_uncond_batch(model, cond, uncond, x, timestep, model_options)\n\n    positive_eps = x - positive_x0\n    negative_eps = x - negative_x0\n\n    alpha = 0.001 * patch_settings[pid].sharpness * patch_settings[pid].global_diffusion_progress\n\n    positive_eps_degraded = anisotropic.adaptive_anisotropic_filter(x=positive_eps, g=positive_x0)\n    positive_eps_degraded_weighted = positive_eps_degraded * alpha + positive_eps * (1.0 - alpha)\n\n    final_eps = compute_cfg(uncond=negative_eps, cond=positive_eps_degraded_weighted,\n                            cfg_scale=cond_scale, t=patch_settings[pid].global_diffusion_progress)\n\n    if patch_settings[pid].eps_record is not None:\n        patch_settings[pid].eps_record = (final_eps / timestep).cpu()\n\n    return x - final_eps\n\n\ndef round_to_64(x):\n    h = float(x)\n    h = h / 64.0\n    h = round(h)\n    h = int(h)\n    h = h * 64\n    return h\n\n\ndef sdxl_encode_adm_patched(self, **kwargs):\n    clip_pooled = ldm_patched.modules.model_base.sdxl_pooled(kwargs, self.noise_augmentor)\n    width = kwargs.get(\"width\", 1024)\n    height = kwargs.get(\"height\", 1024)\n    target_width = width\n    target_height = height\n    pid = os.getpid()\n\n    if kwargs.get(\"prompt_type\", \"\") == \"negative\":\n        width = float(width) * patch_settings[pid].negative_adm_scale\n        height = float(height) * patch_settings[pid].negative_adm_scale\n    elif kwargs.get(\"prompt_type\", \"\") == \"positive\":\n        width = float(width) * patch_settings[pid].positive_adm_scale\n        height = float(height) * patch_settings[pid].positive_adm_scale\n\n    def embedder(number_list):\n        h = self.embedder(torch.tensor(number_list, dtype=torch.float32))\n        h = torch.flatten(h).unsqueeze(dim=0).repeat(clip_pooled.shape[0], 1)\n        return h\n\n    width, height = int(width), int(height)\n    target_width, target_height = round_to_64(target_width), round_to_64(target_height)\n\n    adm_emphasized = embedder([height, width, 0, 0, target_height, target_width])\n    adm_consistent = embedder([target_height, target_width, 0, 0, target_height, target_width])\n\n    clip_pooled = clip_pooled.to(adm_emphasized)\n    final_adm = torch.cat((clip_pooled, adm_emphasized, clip_pooled, adm_consistent), dim=1)\n\n    return final_adm\n\n\ndef patched_KSamplerX0Inpaint_forward(self, x, sigma, uncond, cond, cond_scale, denoise_mask, model_options={}, seed=None):\n    if inpaint_worker.current_task is not None:\n        latent_processor = self.inner_model.inner_model.process_latent_in\n        inpaint_latent = latent_processor(inpaint_worker.current_task.latent).to(x)\n        inpaint_mask = inpaint_worker.current_task.latent_mask.to(x)\n\n        if getattr(self, 'energy_generator', None) is None:\n            # avoid bad results by using different seeds.\n            self.energy_generator = torch.Generator(device='cpu').manual_seed((seed + 1) % constants.MAX_SEED)\n\n        energy_sigma = sigma.reshape([sigma.shape[0]] + [1] * (len(x.shape) - 1))\n        current_energy = torch.randn(\n            x.size(), dtype=x.dtype, generator=self.energy_generator, device=\"cpu\").to(x) * energy_sigma\n        x = x * inpaint_mask + (inpaint_latent + current_energy) * (1.0 - inpaint_mask)\n\n        out = self.inner_model(x, sigma,\n                               cond=cond,\n                               uncond=uncond,\n                               cond_scale=cond_scale,\n                               model_options=model_options,\n                               seed=seed)\n\n        out = out * inpaint_mask + inpaint_latent * (1.0 - inpaint_mask)\n    else:\n        out = self.inner_model(x, sigma,\n                               cond=cond,\n                               uncond=uncond,\n                               cond_scale=cond_scale,\n                               model_options=model_options,\n                               seed=seed)\n    return out\n\n\ndef timed_adm(y, timesteps):\n    if isinstance(y, torch.Tensor) and int(y.dim()) == 2 and int(y.shape[1]) == 5632:\n        y_mask = (timesteps > 999.0 * (1.0 - float(patch_settings[os.getpid()].adm_scaler_end))).to(y)[..., None]\n        y_with_adm = y[..., :2816].clone()\n        y_without_adm = y[..., 2816:].clone()\n        return y_with_adm * y_mask + y_without_adm * (1.0 - y_mask)\n    return y\n\n\ndef patched_cldm_forward(self, x, hint, timesteps, context, y=None, **kwargs):\n    t_emb = ldm_patched.ldm.modules.diffusionmodules.openaimodel.timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n    emb = self.time_embed(t_emb)\n    pid = os.getpid()\n\n    guided_hint = self.input_hint_block(hint, emb, context)\n\n    y = timed_adm(y, timesteps)\n\n    outs = []\n\n    hs = []\n    if self.num_classes is not None:\n        assert y.shape[0] == x.shape[0]\n        emb = emb + self.label_emb(y)\n\n    h = x\n    for module, zero_conv in zip(self.input_blocks, self.zero_convs):\n        if guided_hint is not None:\n            h = module(h, emb, context)\n            h += guided_hint\n            guided_hint = None\n        else:\n            h = module(h, emb, context)\n        outs.append(zero_conv(h, emb, context))\n\n    h = self.middle_block(h, emb, context)\n    outs.append(self.middle_block_out(h, emb, context))\n\n    if patch_settings[pid].controlnet_softness > 0:\n        for i in range(10):\n            k = 1.0 - float(i) / 9.0\n            outs[i] = outs[i] * (1.0 - patch_settings[pid].controlnet_softness * k)\n\n    return outs\n\n\ndef patched_unet_forward(self, x, timesteps=None, context=None, y=None, control=None, transformer_options={}, **kwargs):\n    self.current_step = 1.0 - timesteps.to(x) / 999.0\n    patch_settings[os.getpid()].global_diffusion_progress = float(self.current_step.detach().cpu().numpy().tolist()[0])\n\n    y = timed_adm(y, timesteps)\n\n    transformer_options[\"original_shape\"] = list(x.shape)\n    transformer_options[\"transformer_index\"] = 0\n    transformer_patches = transformer_options.get(\"patches\", {})\n\n    num_video_frames = kwargs.get(\"num_video_frames\", self.default_num_video_frames)\n    image_only_indicator = kwargs.get(\"image_only_indicator\", self.default_image_only_indicator)\n    time_context = kwargs.get(\"time_context\", None)\n\n    assert (y is not None) == (\n            self.num_classes is not None\n    ), \"must specify y if and only if the model is class-conditional\"\n    hs = []\n    t_emb = ldm_patched.ldm.modules.diffusionmodules.openaimodel.timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n    emb = self.time_embed(t_emb)\n\n    if self.num_classes is not None:\n        assert y.shape[0] == x.shape[0]\n        emb = emb + self.label_emb(y)\n\n    h = x\n    for id, module in enumerate(self.input_blocks):\n        transformer_options[\"block\"] = (\"input\", id)\n        h = forward_timestep_embed(module, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n        h = apply_control(h, control, 'input')\n        if \"input_block_patch\" in transformer_patches:\n            patch = transformer_patches[\"input_block_patch\"]\n            for p in patch:\n                h = p(h, transformer_options)\n\n        hs.append(h)\n        if \"input_block_patch_after_skip\" in transformer_patches:\n            patch = transformer_patches[\"input_block_patch_after_skip\"]\n            for p in patch:\n                h = p(h, transformer_options)\n\n    transformer_options[\"block\"] = (\"middle\", 0)\n    h = forward_timestep_embed(self.middle_block, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n    h = apply_control(h, control, 'middle')\n\n    for id, module in enumerate(self.output_blocks):\n        transformer_options[\"block\"] = (\"output\", id)\n        hsp = hs.pop()\n        hsp = apply_control(hsp, control, 'output')\n\n        if \"output_block_patch\" in transformer_patches:\n            patch = transformer_patches[\"output_block_patch\"]\n            for p in patch:\n                h, hsp = p(h, hsp, transformer_options)\n\n        h = torch.cat([h, hsp], dim=1)\n        del hsp\n        if len(hs) > 0:\n            output_shape = hs[-1].shape\n        else:\n            output_shape = None\n        h = forward_timestep_embed(module, h, emb, context, transformer_options, output_shape, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n    h = h.type(x.dtype)\n    if self.predict_codebook_ids:\n        return self.id_predictor(h)\n    else:\n        return self.out(h)\n\n\ndef patched_load_models_gpu(*args, **kwargs):\n    execution_start_time = time.perf_counter()\n    y = ldm_patched.modules.model_management.load_models_gpu_origin(*args, **kwargs)\n    moving_time = time.perf_counter() - execution_start_time\n    if moving_time > 0.1:\n        print(f'[Fooocus Model Management] Moving model(s) has taken {moving_time:.2f} seconds')\n    return y\n\n\ndef build_loaded(module, loader_name):\n    original_loader_name = loader_name + '_origin'\n\n    if not hasattr(module, original_loader_name):\n        setattr(module, original_loader_name, getattr(module, loader_name))\n\n    original_loader = getattr(module, original_loader_name)\n\n    def loader(*args, **kwargs):\n        result = None\n        try:\n            result = original_loader(*args, **kwargs)\n        except Exception as e:\n            result = None\n            exp = str(e) + '\\n'\n            for path in list(args) + list(kwargs.values()):\n                if isinstance(path, str):\n                    if os.path.exists(path):\n                        exp += f'File corrupted: {path} \\n'\n                        corrupted_backup_file = path + '.corrupted'\n                        if os.path.exists(corrupted_backup_file):\n                            os.remove(corrupted_backup_file)\n                        os.replace(path, corrupted_backup_file)\n                        if os.path.exists(path):\n                            os.remove(path)\n                        exp += f'Fooocus has tried to move the corrupted file to {corrupted_backup_file} \\n'\n                        exp += f'You may try again now and Fooocus will download models again. \\n'\n            raise ValueError(exp)\n        return result\n\n    setattr(module, loader_name, loader)\n    return\n\n\ndef patch_all():\n    if ldm_patched.modules.model_management.directml_enabled:\n        ldm_patched.modules.model_management.lowvram_available = True\n        ldm_patched.modules.model_management.OOM_EXCEPTION = Exception\n\n    patch_all_precision()\n    patch_all_clip()\n\n    if not hasattr(ldm_patched.modules.model_management, 'load_models_gpu_origin'):\n        ldm_patched.modules.model_management.load_models_gpu_origin = ldm_patched.modules.model_management.load_models_gpu\n\n    ldm_patched.modules.model_management.load_models_gpu = patched_load_models_gpu\n    ldm_patched.modules.model_patcher.ModelPatcher.calculate_weight = calculate_weight_patched\n    ldm_patched.controlnet.cldm.ControlNet.forward = patched_cldm_forward\n    ldm_patched.ldm.modules.diffusionmodules.openaimodel.UNetModel.forward = patched_unet_forward\n    ldm_patched.modules.model_base.SDXL.encode_adm = sdxl_encode_adm_patched\n    ldm_patched.modules.samplers.KSamplerX0Inpaint.forward = patched_KSamplerX0Inpaint_forward\n    ldm_patched.k_diffusion.sampling.BrownianTreeNoiseSampler = BrownianTreeNoiseSamplerPatched\n    ldm_patched.modules.samplers.sampling_function = patched_sampling_function\n\n    warnings.filterwarnings(action='ignore', module='torchsde')\n\n    build_loaded(safetensors.torch, 'load_file')\n    build_loaded(torch, 'load')\n\n    return\n", "modules/util.py": "from pathlib import Path\n\nimport numpy as np\nimport datetime\nimport random\nimport math\nimport os\nimport cv2\nimport re\nfrom typing import List, Tuple, AnyStr, NamedTuple\n\nimport json\nimport hashlib\n\nfrom PIL import Image\n\nimport modules.config\nimport modules.sdxl_styles\nfrom modules.flags import Performance\n\nLANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n\n# Regexp compiled once. Matches entries with the following pattern:\n# <lora:some_lora:1>\n# <lora:aNotherLora:-1.6>\nLORAS_PROMPT_PATTERN = re.compile(r\"(<lora:([^:]+):([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+))>)\", re.X)\n\nHASH_SHA256_LENGTH = 10\n\n\ndef erode_or_dilate(x, k):\n    k = int(k)\n    if k > 0:\n        return cv2.dilate(x, kernel=np.ones(shape=(3, 3), dtype=np.uint8), iterations=k)\n    if k < 0:\n        return cv2.erode(x, kernel=np.ones(shape=(3, 3), dtype=np.uint8), iterations=-k)\n    return x\n\n\ndef resample_image(im, width, height):\n    im = Image.fromarray(im)\n    im = im.resize((int(width), int(height)), resample=LANCZOS)\n    return np.array(im)\n\n\ndef resize_image(im, width, height, resize_mode=1):\n    \"\"\"\n    Resizes an image with the specified resize_mode, width, and height.\n\n    Args:\n        resize_mode: The mode to use when resizing the image.\n            0: Resize the image to the specified width and height.\n            1: Resize the image to fill the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess.\n            2: Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image.\n        im: The image to resize.\n        width: The width to resize the image to.\n        height: The height to resize the image to.\n    \"\"\"\n\n    im = Image.fromarray(im)\n\n    def resize(im, w, h):\n        return im.resize((w, h), resample=LANCZOS)\n\n    if resize_mode == 0:\n        res = resize(im, width, height)\n\n    elif resize_mode == 1:\n        ratio = width / height\n        src_ratio = im.width / im.height\n\n        src_w = width if ratio > src_ratio else im.width * height // im.height\n        src_h = height if ratio <= src_ratio else im.height * width // im.width\n\n        resized = resize(im, src_w, src_h)\n        res = Image.new(\"RGB\", (width, height))\n        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n\n    else:\n        ratio = width / height\n        src_ratio = im.width / im.height\n\n        src_w = width if ratio < src_ratio else im.width * height // im.height\n        src_h = height if ratio >= src_ratio else im.height * width // im.width\n\n        resized = resize(im, src_w, src_h)\n        res = Image.new(\"RGB\", (width, height))\n        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n\n        if ratio < src_ratio:\n            fill_height = height // 2 - src_h // 2\n            if fill_height > 0:\n                res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n                res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\n        elif ratio > src_ratio:\n            fill_width = width // 2 - src_w // 2\n            if fill_width > 0:\n                res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n                res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\n\n    return np.array(res)\n\n\ndef get_shape_ceil(h, w):\n    return math.ceil(((h * w) ** 0.5) / 64.0) * 64.0\n\n\ndef get_image_shape_ceil(im):\n    H, W = im.shape[:2]\n    return get_shape_ceil(H, W)\n\n\ndef set_image_shape_ceil(im, shape_ceil):\n    shape_ceil = float(shape_ceil)\n\n    H_origin, W_origin, _ = im.shape\n    H, W = H_origin, W_origin\n    \n    for _ in range(256):\n        current_shape_ceil = get_shape_ceil(H, W)\n        if abs(current_shape_ceil - shape_ceil) < 0.1:\n            break\n        k = shape_ceil / current_shape_ceil\n        H = int(round(float(H) * k / 64.0) * 64)\n        W = int(round(float(W) * k / 64.0) * 64)\n\n    if H == H_origin and W == W_origin:\n        return im\n\n    return resample_image(im, width=W, height=H)\n\n\ndef HWC3(x):\n    assert x.dtype == np.uint8\n    if x.ndim == 2:\n        x = x[:, :, None]\n    assert x.ndim == 3\n    H, W, C = x.shape\n    assert C == 1 or C == 3 or C == 4\n    if C == 3:\n        return x\n    if C == 1:\n        return np.concatenate([x, x, x], axis=2)\n    if C == 4:\n        color = x[:, :, 0:3].astype(np.float32)\n        alpha = x[:, :, 3:4].astype(np.float32) / 255.0\n        y = color * alpha + 255.0 * (1.0 - alpha)\n        y = y.clip(0, 255).astype(np.uint8)\n        return y\n\n\ndef remove_empty_str(items, default=None):\n    items = [x for x in items if x != \"\"]\n    if len(items) == 0 and default is not None:\n        return [default]\n    return items\n\n\ndef join_prompts(*args, **kwargs):\n    prompts = [str(x) for x in args if str(x) != \"\"]\n    if len(prompts) == 0:\n        return \"\"\n    if len(prompts) == 1:\n        return prompts[0]\n    return ', '.join(prompts)\n\n\ndef generate_temp_filename(folder='./outputs/', extension='png'):\n    current_time = datetime.datetime.now()\n    date_string = current_time.strftime(\"%Y-%m-%d\")\n    time_string = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    random_number = random.randint(1000, 9999)\n    filename = f\"{time_string}_{random_number}.{extension}\"\n    result = os.path.join(folder, date_string, filename)\n    return date_string, os.path.abspath(result), filename\n\n\ndef sha256(filename, use_addnet_hash=False, length=HASH_SHA256_LENGTH):\n    print(f\"Calculating sha256 for {filename}: \", end='')\n    if use_addnet_hash:\n        with open(filename, \"rb\") as file:\n            sha256_value = addnet_hash_safetensors(file)\n    else:\n        sha256_value = calculate_sha256(filename)\n    print(f\"{sha256_value}\")\n\n    return sha256_value[:length] if length is not None else sha256_value\n\n\ndef addnet_hash_safetensors(b):\n    \"\"\"kohya-ss hash for safetensors from https://github.com/kohya-ss/sd-scripts/blob/main/library/train_util.py\"\"\"\n    hash_sha256 = hashlib.sha256()\n    blksize = 1024 * 1024\n\n    b.seek(0)\n    header = b.read(8)\n    n = int.from_bytes(header, \"little\")\n\n    offset = n + 8\n    b.seek(offset)\n    for chunk in iter(lambda: b.read(blksize), b\"\"):\n        hash_sha256.update(chunk)\n\n    return hash_sha256.hexdigest()\n\n\ndef calculate_sha256(filename) -> str:\n    hash_sha256 = hashlib.sha256()\n    blksize = 1024 * 1024\n\n    with open(filename, \"rb\") as f:\n        for chunk in iter(lambda: f.read(blksize), b\"\"):\n            hash_sha256.update(chunk)\n\n    return hash_sha256.hexdigest()\n\n\ndef quote(text):\n    if ',' not in str(text) and '\\n' not in str(text) and ':' not in str(text):\n        return text\n\n    return json.dumps(text, ensure_ascii=False)\n\n\ndef unquote(text):\n    if len(text) == 0 or text[0] != '\"' or text[-1] != '\"':\n        return text\n\n    try:\n        return json.loads(text)\n    except Exception:\n        return text\n\n\ndef unwrap_style_text_from_prompt(style_text, prompt):\n    \"\"\"\n    Checks the prompt to see if the style text is wrapped around it. If so,\n    returns True plus the prompt text without the style text. Otherwise, returns\n    False with the original prompt.\n\n    Note that the \"cleaned\" version of the style text is only used for matching\n    purposes here. It isn't returned; the original style text is not modified.\n    \"\"\"\n    stripped_prompt = prompt\n    stripped_style_text = style_text\n    if \"{prompt}\" in stripped_style_text:\n        # Work out whether the prompt is wrapped in the style text. If so, we\n        # return True and the \"inner\" prompt text that isn't part of the style.\n        try:\n            left, right = stripped_style_text.split(\"{prompt}\", 2)\n        except ValueError as e:\n            # If the style text has multple \"{prompt}\"s, we can't split it into\n            # two parts. This is an error, but we can't do anything about it.\n            print(f\"Unable to compare style text to prompt:\\n{style_text}\")\n            print(f\"Error: {e}\")\n            return False, prompt, ''\n\n        left_pos = stripped_prompt.find(left)\n        right_pos = stripped_prompt.find(right)\n        if 0 <= left_pos < right_pos:\n            real_prompt = stripped_prompt[left_pos + len(left):right_pos]\n            prompt = stripped_prompt.replace(left + real_prompt + right, '', 1)\n            if prompt.startswith(\", \"):\n                prompt = prompt[2:]\n            if prompt.endswith(\", \"):\n                prompt = prompt[:-2]\n            return True, prompt, real_prompt\n    else:\n        # Work out whether the given prompt starts with the style text. If so, we\n        # return True and the prompt text up to where the style text starts.\n        if stripped_prompt.endswith(stripped_style_text):\n            prompt = stripped_prompt[: len(stripped_prompt) - len(stripped_style_text)]\n            if prompt.endswith(\", \"):\n                prompt = prompt[:-2]\n            return True, prompt, prompt\n\n    return False, prompt, ''\n\n\ndef extract_original_prompts(style, prompt, negative_prompt):\n    \"\"\"\n    Takes a style and compares it to the prompt and negative prompt. If the style\n    matches, returns True plus the prompt and negative prompt with the style text\n    removed. Otherwise, returns False with the original prompt and negative prompt.\n    \"\"\"\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    match_positive, extracted_positive, real_prompt = unwrap_style_text_from_prompt(\n        style.prompt, prompt\n    )\n    if not match_positive:\n        return False, prompt, negative_prompt, ''\n\n    match_negative, extracted_negative, _ = unwrap_style_text_from_prompt(\n        style.negative_prompt, negative_prompt\n    )\n    if not match_negative:\n        return False, prompt, negative_prompt, ''\n\n    return True, extracted_positive, extracted_negative, real_prompt\n\n\ndef extract_styles_from_prompt(prompt, negative_prompt):\n    extracted = []\n    applicable_styles = []\n\n    for style_name, (style_prompt, style_negative_prompt) in modules.sdxl_styles.styles.items():\n        applicable_styles.append(PromptStyle(name=style_name, prompt=style_prompt, negative_prompt=style_negative_prompt))\n\n    real_prompt = ''\n\n    while True:\n        found_style = None\n\n        for style in applicable_styles:\n            is_match, new_prompt, new_neg_prompt, new_real_prompt = extract_original_prompts(\n                style, prompt, negative_prompt\n            )\n            if is_match:\n                found_style = style\n                prompt = new_prompt\n                negative_prompt = new_neg_prompt\n                if real_prompt == '' and new_real_prompt != '' and new_real_prompt != prompt:\n                    real_prompt = new_real_prompt\n                break\n\n        if not found_style:\n            break\n\n        applicable_styles.remove(found_style)\n        extracted.append(found_style.name)\n\n    # add prompt expansion if not all styles could be resolved\n    if prompt != '':\n        if real_prompt != '':\n            extracted.append(modules.sdxl_styles.fooocus_expansion)\n        else:\n            # find real_prompt when only prompt expansion is selected\n            first_word = prompt.split(', ')[0]\n            first_word_positions = [i for i in range(len(prompt)) if prompt.startswith(first_word, i)]\n            if len(first_word_positions) > 1:\n                real_prompt = prompt[:first_word_positions[-1]]\n                extracted.append(modules.sdxl_styles.fooocus_expansion)\n                if real_prompt.endswith(', '):\n                    real_prompt = real_prompt[:-2]\n\n    return list(reversed(extracted)), real_prompt, negative_prompt\n\n\nclass PromptStyle(NamedTuple):\n    name: str\n    prompt: str\n    negative_prompt: str\n\n\ndef is_json(data: str) -> bool:\n    try:\n        loaded_json = json.loads(data)\n        assert isinstance(loaded_json, dict)\n    except (ValueError, AssertionError):\n        return False\n    return True\n\n\ndef get_filname_by_stem(lora_name, filenames: List[str]) -> str | None:\n    for filename in filenames:\n        path = Path(filename)\n        if lora_name == path.stem:\n            return filename\n    return None\n\n\ndef get_file_from_folder_list(name, folders):\n    if not isinstance(folders, list):\n        folders = [folders]\n\n    for folder in folders:\n        filename = os.path.abspath(os.path.realpath(os.path.join(folder, name)))\n        if os.path.isfile(filename):\n            return filename\n\n    return os.path.abspath(os.path.realpath(os.path.join(folders[0], name)))\n\n\ndef makedirs_with_log(path):\n    try:\n        os.makedirs(path, exist_ok=True)\n    except OSError as error:\n        print(f'Directory {path} could not be created, reason: {error}')\n\n\ndef get_enabled_loras(loras: list, remove_none=True) -> list:\n    return [(lora[1], lora[2]) for lora in loras if lora[0] and (lora[1] != 'None' if remove_none else True)]\n\n\ndef parse_lora_references_from_prompt(prompt: str, loras: List[Tuple[AnyStr, float]], loras_limit: int = 5,\n                                      skip_file_check=False, prompt_cleanup=True, deduplicate_loras=True,\n                                      lora_filenames=None) -> tuple[List[Tuple[AnyStr, float]], str]:\n    if lora_filenames is None:\n        lora_filenames = []\n\n    found_loras = []\n    prompt_without_loras = ''\n    cleaned_prompt = ''\n\n    for token in prompt.split(','):\n        matches = LORAS_PROMPT_PATTERN.findall(token)\n\n        if len(matches) == 0:\n            prompt_without_loras += token + ', '\n            continue\n        for match in matches:\n            lora_name = match[1] + '.safetensors'\n            if not skip_file_check:\n                lora_name = get_filname_by_stem(match[1], lora_filenames)\n            if lora_name is not None:\n                found_loras.append((lora_name, float(match[2])))\n            token = token.replace(match[0], '')\n        prompt_without_loras += token + ', '\n\n    if prompt_without_loras != '':\n        cleaned_prompt = prompt_without_loras[:-2]\n\n    if prompt_cleanup:\n        cleaned_prompt = cleanup_prompt(prompt_without_loras)\n\n    new_loras = []\n    lora_names = [lora[0] for lora in loras]\n    for found_lora in found_loras:\n        if deduplicate_loras and (found_lora[0] in lora_names or found_lora in new_loras):\n            continue\n        new_loras.append(found_lora)\n\n    if len(new_loras) == 0:\n        return loras, cleaned_prompt\n\n    updated_loras = []\n    for lora in loras + new_loras:\n        if lora[0] != \"None\":\n            updated_loras.append(lora)\n\n    return updated_loras[:loras_limit], cleaned_prompt\n\n\ndef remove_performance_lora(filenames: list, performance: Performance | None):\n    loras_without_performance = filenames.copy()\n\n    if performance is None:\n        return loras_without_performance\n\n    performance_lora = performance.lora_filename()\n\n    for filename in filenames:\n        path = Path(filename)\n        if performance_lora == path.name:\n            loras_without_performance.remove(filename)\n\n    return loras_without_performance\n\n\ndef cleanup_prompt(prompt):\n    prompt = re.sub(' +', ' ', prompt)\n    prompt = re.sub(',+', ',', prompt)\n    cleaned_prompt = ''\n    for token in prompt.split(','):\n        token = token.strip()\n        if token == '':\n            continue\n        cleaned_prompt += token + ', '\n    return cleaned_prompt[:-2]\n\n\ndef apply_wildcards(wildcard_text, rng, i, read_wildcards_in_order) -> str:\n    for _ in range(modules.config.wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:\n            return wildcard_text\n\n        print(f'[Wildcards] processing: {wildcard_text}')\n        for placeholder in placeholders:\n            try:\n                matches = [x for x in modules.config.wildcard_filenames if os.path.splitext(os.path.basename(x))[0] == placeholder]\n                words = open(os.path.join(modules.config.path_wildcards, matches[0]), encoding='utf-8').read().splitlines()\n                words = [x for x in words if x != '']\n                assert len(words) > 0\n                if read_wildcards_in_order:\n                    wildcard_text = wildcard_text.replace(f'__{placeholder}__', words[i % len(words)], 1)\n                else:\n                    wildcard_text = wildcard_text.replace(f'__{placeholder}__', rng.choice(words), 1)\n            except:\n                print(f'[Wildcards] Warning: {placeholder}.txt missing or empty. '\n                      f'Using \"{placeholder}\" as a normal word.')\n                wildcard_text = wildcard_text.replace(f'__{placeholder}__', placeholder)\n            print(f'[Wildcards] {wildcard_text}')\n\n    print(f'[Wildcards] BFS stack overflow. Current text: {wildcard_text}')\n    return wildcard_text\n\n\ndef get_image_size_info(image: np.ndarray, aspect_ratios: list) -> str:\n    try:\n        image = Image.fromarray(np.uint8(image))\n        width, height = image.size\n        ratio = round(width / height, 2)\n        gcd = math.gcd(width, height)\n        lcm_ratio = f'{width // gcd}:{height // gcd}'\n        size_info = f'Image Size: {width} x {height}, Ratio: {ratio}, {lcm_ratio}'\n\n        closest_ratio = min(aspect_ratios, key=lambda x: abs(ratio - float(x.split('*')[0]) / float(x.split('*')[1])))\n        recommended_width, recommended_height = map(int, closest_ratio.split('*'))\n        recommended_ratio = round(recommended_width / recommended_height, 2)\n        recommended_gcd = math.gcd(recommended_width, recommended_height)\n        recommended_lcm_ratio = f'{recommended_width // recommended_gcd}:{recommended_height // recommended_gcd}'\n\n        size_info = f'{width} x {height}, {ratio}, {lcm_ratio}'\n        size_info += f'\\n{recommended_width} x {recommended_height}, {recommended_ratio}, {recommended_lcm_ratio}'\n\n        return size_info\n    except Exception as e:\n        return f'Error reading image: {e}'\n", "modules/constants.py": "# as in k-diffusion (sampling.py)\nMIN_SEED = 0\nMAX_SEED = 2**63 - 1\n\nAUTH_FILENAME = 'auth.json'\n", "modules/meta_parser.py": "import json\nimport re\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\n\nimport gradio as gr\nfrom PIL import Image\n\nimport fooocus_version\nimport modules.config\nimport modules.sdxl_styles\nfrom modules.flags import MetadataScheme, Performance, Steps\nfrom modules.flags import SAMPLERS, CIVITAI_NO_KARRAS\nfrom modules.util import quote, unquote, extract_styles_from_prompt, is_json, get_file_from_folder_list, sha256\n\nre_param_code = r'\\s*(\\w[\\w \\-/]+):\\s*(\"(?:\\\\.|[^\\\\\"])+\"|[^,]*)(?:,|$)'\nre_param = re.compile(re_param_code)\nre_imagesize = re.compile(r\"^(\\d+)x(\\d+)$\")\n\nhash_cache = {}\n\n\ndef load_parameter_button_click(raw_metadata: dict | str, is_generating: bool):\n    loaded_parameter_dict = raw_metadata\n    if isinstance(raw_metadata, str):\n        loaded_parameter_dict = json.loads(raw_metadata)\n    assert isinstance(loaded_parameter_dict, dict)\n\n    results = [len(loaded_parameter_dict) > 0]\n\n    get_image_number('image_number', 'Image Number', loaded_parameter_dict, results)\n    get_str('prompt', 'Prompt', loaded_parameter_dict, results)\n    get_str('negative_prompt', 'Negative Prompt', loaded_parameter_dict, results)\n    get_list('styles', 'Styles', loaded_parameter_dict, results)\n    performance = get_str('performance', 'Performance', loaded_parameter_dict, results)\n    get_steps('steps', 'Steps', loaded_parameter_dict, results)\n    get_number('overwrite_switch', 'Overwrite Switch', loaded_parameter_dict, results)\n    get_resolution('resolution', 'Resolution', loaded_parameter_dict, results)\n    get_number('guidance_scale', 'Guidance Scale', loaded_parameter_dict, results)\n    get_number('sharpness', 'Sharpness', loaded_parameter_dict, results)\n    get_adm_guidance('adm_guidance', 'ADM Guidance', loaded_parameter_dict, results)\n    get_str('refiner_swap_method', 'Refiner Swap Method', loaded_parameter_dict, results)\n    get_number('adaptive_cfg', 'CFG Mimicking from TSNR', loaded_parameter_dict, results)\n    get_number('clip_skip', 'CLIP Skip', loaded_parameter_dict, results, cast_type=int)\n    get_str('base_model', 'Base Model', loaded_parameter_dict, results)\n    get_str('refiner_model', 'Refiner Model', loaded_parameter_dict, results)\n    get_number('refiner_switch', 'Refiner Switch', loaded_parameter_dict, results)\n    get_str('sampler', 'Sampler', loaded_parameter_dict, results)\n    get_str('scheduler', 'Scheduler', loaded_parameter_dict, results)\n    get_str('vae', 'VAE', loaded_parameter_dict, results)\n    get_seed('seed', 'Seed', loaded_parameter_dict, results)\n\n    if is_generating:\n        results.append(gr.update())\n    else:\n        results.append(gr.update(visible=True))\n\n    results.append(gr.update(visible=False))\n\n    get_freeu('freeu', 'FreeU', loaded_parameter_dict, results)\n\n    # prevent performance LoRAs to be added twice, by performance and by lora\n    performance_filename = None\n    if performance is not None and performance in Performance.values():\n        performance = Performance(performance)\n        performance_filename = performance.lora_filename()\n\n    for i in range(modules.config.default_max_lora_number):\n        get_lora(f'lora_combined_{i + 1}', f'LoRA {i + 1}', loaded_parameter_dict, results, performance_filename)\n\n    return results\n\n\ndef get_str(key: str, fallback: str | None, source_dict: dict, results: list, default=None) -> str | None:\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        assert isinstance(h, str)\n        results.append(h)\n        return h\n    except:\n        results.append(gr.update())\n        return None\n\n\ndef get_list(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        h = eval(h)\n        assert isinstance(h, list)\n        results.append(h)\n    except:\n        results.append(gr.update())\n\n\ndef get_number(key: str, fallback: str | None, source_dict: dict, results: list, default=None, cast_type=float):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        assert h is not None\n        h = cast_type(h)\n        results.append(h)\n    except:\n        results.append(gr.update())\n\n\ndef get_image_number(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        assert h is not None\n        h = int(h)\n        h = min(h, modules.config.default_max_image_number)\n        results.append(h)\n    except:\n        results.append(1)\n\n\ndef get_steps(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        assert h is not None\n        h = int(h)\n        # if not in steps or in steps and performance is not the same\n        performance_name = source_dict.get('performance', '').replace(' ', '_').replace('-', '_').casefold()\n        performance_candidates = [key for key in Steps.keys() if key.casefold() == performance_name and Steps[key] == h]\n        if len(performance_candidates) == 0:\n            results.append(h)\n            return\n        results.append(-1)\n    except:\n        results.append(-1)\n\n\ndef get_resolution(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        width, height = eval(h)\n        formatted = modules.config.add_ratio(f'{width}*{height}')\n        if formatted in modules.config.available_aspect_ratios_labels:\n            results.append(formatted)\n            results.append(-1)\n            results.append(-1)\n        else:\n            results.append(gr.update())\n            results.append(int(width))\n            results.append(int(height))\n    except:\n        results.append(gr.update())\n        results.append(gr.update())\n        results.append(gr.update())\n\n\ndef get_seed(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        assert h is not None\n        h = int(h)\n        results.append(False)\n        results.append(h)\n    except:\n        results.append(gr.update())\n        results.append(gr.update())\n\n\ndef get_adm_guidance(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        p, n, e = eval(h)\n        results.append(float(p))\n        results.append(float(n))\n        results.append(float(e))\n    except:\n        results.append(gr.update())\n        results.append(gr.update())\n        results.append(gr.update())\n\n\ndef get_freeu(key: str, fallback: str | None, source_dict: dict, results: list, default=None):\n    try:\n        h = source_dict.get(key, source_dict.get(fallback, default))\n        b1, b2, s1, s2 = eval(h)\n        results.append(True)\n        results.append(float(b1))\n        results.append(float(b2))\n        results.append(float(s1))\n        results.append(float(s2))\n    except:\n        results.append(False)\n        results.append(gr.update())\n        results.append(gr.update())\n        results.append(gr.update())\n        results.append(gr.update())\n\n\ndef get_lora(key: str, fallback: str | None, source_dict: dict, results: list, performance_filename: str | None):\n    try:\n        split_data = source_dict.get(key, source_dict.get(fallback)).split(' : ')\n        enabled = True\n        name = split_data[0]\n        weight = split_data[1]\n\n        if len(split_data) == 3:\n            enabled = split_data[0] == 'True'\n            name = split_data[1]\n            weight = split_data[2]\n\n        if name == performance_filename:\n            raise Exception\n\n        weight = float(weight)\n        results.append(enabled)\n        results.append(name)\n        results.append(weight)\n    except:\n        results.append(True)\n        results.append('None')\n        results.append(1)\n\n\ndef get_sha256(filepath):\n    global hash_cache\n    if filepath not in hash_cache:\n        hash_cache[filepath] = sha256(filepath)\n\n    return hash_cache[filepath]\n\n\ndef parse_meta_from_preset(preset_content):\n    assert isinstance(preset_content, dict)\n    preset_prepared = {}\n    items = preset_content\n\n    for settings_key, meta_key in modules.config.possible_preset_keys.items():\n        if settings_key == \"default_loras\":\n            loras = getattr(modules.config, settings_key)\n            if settings_key in items:\n                loras = items[settings_key]\n            for index, lora in enumerate(loras[:modules.config.default_max_lora_number]):\n                preset_prepared[f'lora_combined_{index + 1}'] = ' : '.join(map(str, lora))\n        elif settings_key == \"default_aspect_ratio\":\n            if settings_key in items and items[settings_key] is not None:\n                default_aspect_ratio = items[settings_key]\n                width, height = default_aspect_ratio.split('*')\n            else:\n                default_aspect_ratio = getattr(modules.config, settings_key)\n                width, height = default_aspect_ratio.split('\u00d7')\n                height = height[:height.index(\" \")]\n            preset_prepared[meta_key] = (width, height)\n        else:\n            preset_prepared[meta_key] = items[settings_key] if settings_key in items and items[\n                settings_key] is not None else getattr(modules.config, settings_key)\n\n        if settings_key == \"default_styles\" or settings_key == \"default_aspect_ratio\":\n            preset_prepared[meta_key] = str(preset_prepared[meta_key])\n\n    return preset_prepared\n\n\nclass MetadataParser(ABC):\n    def __init__(self):\n        self.raw_prompt: str = ''\n        self.full_prompt: str = ''\n        self.raw_negative_prompt: str = ''\n        self.full_negative_prompt: str = ''\n        self.steps: int = Steps.SPEED.value\n        self.base_model_name: str = ''\n        self.base_model_hash: str = ''\n        self.refiner_model_name: str = ''\n        self.refiner_model_hash: str = ''\n        self.loras: list = []\n        self.vae_name: str = ''\n\n    @abstractmethod\n    def get_scheme(self) -> MetadataScheme:\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_json(self, metadata: dict | str) -> dict:\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_string(self, metadata: dict) -> str:\n        raise NotImplementedError\n\n    def set_data(self, raw_prompt, full_prompt, raw_negative_prompt, full_negative_prompt, steps, base_model_name,\n                 refiner_model_name, loras, vae_name):\n        self.raw_prompt = raw_prompt\n        self.full_prompt = full_prompt\n        self.raw_negative_prompt = raw_negative_prompt\n        self.full_negative_prompt = full_negative_prompt\n        self.steps = steps\n        self.base_model_name = Path(base_model_name).stem\n\n        base_model_path = get_file_from_folder_list(base_model_name, modules.config.paths_checkpoints)\n        self.base_model_hash = get_sha256(base_model_path)\n\n        if refiner_model_name not in ['', 'None']:\n            self.refiner_model_name = Path(refiner_model_name).stem\n            refiner_model_path = get_file_from_folder_list(refiner_model_name, modules.config.paths_checkpoints)\n            self.refiner_model_hash = get_sha256(refiner_model_path)\n\n        self.loras = []\n        for (lora_name, lora_weight) in loras:\n            if lora_name != 'None':\n                lora_path = get_file_from_folder_list(lora_name, modules.config.paths_loras)\n                lora_hash = get_sha256(lora_path)\n                self.loras.append((Path(lora_name).stem, lora_weight, lora_hash))\n        self.vae_name = Path(vae_name).stem\n\n\nclass A1111MetadataParser(MetadataParser):\n    def get_scheme(self) -> MetadataScheme:\n        return MetadataScheme.A1111\n\n    fooocus_to_a1111 = {\n        'raw_prompt': 'Raw prompt',\n        'raw_negative_prompt': 'Raw negative prompt',\n        'negative_prompt': 'Negative prompt',\n        'styles': 'Styles',\n        'performance': 'Performance',\n        'steps': 'Steps',\n        'sampler': 'Sampler',\n        'scheduler': 'Scheduler',\n        'vae': 'VAE',\n        'guidance_scale': 'CFG scale',\n        'seed': 'Seed',\n        'resolution': 'Size',\n        'sharpness': 'Sharpness',\n        'adm_guidance': 'ADM Guidance',\n        'refiner_swap_method': 'Refiner Swap Method',\n        'adaptive_cfg': 'Adaptive CFG',\n        'clip_skip': 'Clip skip',\n        'overwrite_switch': 'Overwrite Switch',\n        'freeu': 'FreeU',\n        'base_model': 'Model',\n        'base_model_hash': 'Model hash',\n        'refiner_model': 'Refiner',\n        'refiner_model_hash': 'Refiner hash',\n        'lora_hashes': 'Lora hashes',\n        'lora_weights': 'Lora weights',\n        'created_by': 'User',\n        'version': 'Version'\n    }\n\n    def to_json(self, metadata: str) -> dict:\n        metadata_prompt = ''\n        metadata_negative_prompt = ''\n\n        done_with_prompt = False\n\n        *lines, lastline = metadata.strip().split(\"\\n\")\n        if len(re_param.findall(lastline)) < 3:\n            lines.append(lastline)\n            lastline = ''\n\n        for line in lines:\n            line = line.strip()\n            if line.startswith(f\"{self.fooocus_to_a1111['negative_prompt']}:\"):\n                done_with_prompt = True\n                line = line[len(f\"{self.fooocus_to_a1111['negative_prompt']}:\"):].strip()\n            if done_with_prompt:\n                metadata_negative_prompt += ('' if metadata_negative_prompt == '' else \"\\n\") + line\n            else:\n                metadata_prompt += ('' if metadata_prompt == '' else \"\\n\") + line\n\n        found_styles, prompt, negative_prompt = extract_styles_from_prompt(metadata_prompt, metadata_negative_prompt)\n\n        data = {\n            'prompt': prompt,\n            'negative_prompt': negative_prompt\n        }\n\n        for k, v in re_param.findall(lastline):\n            try:\n                if v != '' and v[0] == '\"' and v[-1] == '\"':\n                    v = unquote(v)\n\n                m = re_imagesize.match(v)\n                if m is not None:\n                    data['resolution'] = str((m.group(1), m.group(2)))\n                else:\n                    data[list(self.fooocus_to_a1111.keys())[list(self.fooocus_to_a1111.values()).index(k)]] = v\n            except Exception:\n                print(f\"Error parsing \\\"{k}: {v}\\\"\")\n\n        # workaround for multiline prompts\n        if 'raw_prompt' in data:\n            data['prompt'] = data['raw_prompt']\n            raw_prompt = data['raw_prompt'].replace(\"\\n\", ', ')\n            if metadata_prompt != raw_prompt and modules.sdxl_styles.fooocus_expansion not in found_styles:\n                found_styles.append(modules.sdxl_styles.fooocus_expansion)\n\n        if 'raw_negative_prompt' in data:\n            data['negative_prompt'] = data['raw_negative_prompt']\n\n        data['styles'] = str(found_styles)\n\n        # try to load performance based on steps, fallback for direct A1111 imports\n        if 'steps' in data and 'performance' in data is None:\n            try:\n                data['performance'] = Performance.by_steps(data['steps']).value\n            except ValueError | KeyError:\n                pass\n\n        if 'sampler' in data:\n            data['sampler'] = data['sampler'].replace(' Karras', '')\n            # get key\n            for k, v in SAMPLERS.items():\n                if v == data['sampler']:\n                    data['sampler'] = k\n                    break\n\n        for key in ['base_model', 'refiner_model', 'vae']:\n            if key in data:\n                if key == 'vae':\n                    self.add_extension_to_filename(data, modules.config.vae_filenames, 'vae')\n                else:\n                    self.add_extension_to_filename(data, modules.config.model_filenames, key)\n\n        lora_data = ''\n        if 'lora_weights' in data and data['lora_weights'] != '':\n            lora_data = data['lora_weights']\n        elif 'lora_hashes' in data and data['lora_hashes'] != '' and data['lora_hashes'].split(', ')[0].count(':') == 2:\n            lora_data = data['lora_hashes']\n\n        if lora_data != '':\n            for li, lora in enumerate(lora_data.split(', ')):\n                lora_split = lora.split(': ')\n                lora_name = lora_split[0]\n                lora_weight = lora_split[2] if len(lora_split) == 3 else lora_split[1]\n                for filename in modules.config.lora_filenames:\n                    path = Path(filename)\n                    if lora_name == path.stem:\n                        data[f'lora_combined_{li + 1}'] = f'{filename} : {lora_weight}'\n                        break\n\n        return data\n\n    def to_string(self, metadata: dict) -> str:\n        data = {k: v for _, k, v in metadata}\n\n        width, height = eval(data['resolution'])\n\n        sampler = data['sampler']\n        scheduler = data['scheduler']\n\n        if sampler in SAMPLERS and SAMPLERS[sampler] != '':\n            sampler = SAMPLERS[sampler]\n            if sampler not in CIVITAI_NO_KARRAS and scheduler == 'karras':\n                sampler += f' Karras'\n\n        generation_params = {\n            self.fooocus_to_a1111['steps']: self.steps,\n            self.fooocus_to_a1111['sampler']: sampler,\n            self.fooocus_to_a1111['seed']: data['seed'],\n            self.fooocus_to_a1111['resolution']: f'{width}x{height}',\n            self.fooocus_to_a1111['guidance_scale']: data['guidance_scale'],\n            self.fooocus_to_a1111['sharpness']: data['sharpness'],\n            self.fooocus_to_a1111['adm_guidance']: data['adm_guidance'],\n            self.fooocus_to_a1111['base_model']: Path(data['base_model']).stem,\n            self.fooocus_to_a1111['base_model_hash']: self.base_model_hash,\n\n            self.fooocus_to_a1111['performance']: data['performance'],\n            self.fooocus_to_a1111['scheduler']: scheduler,\n            self.fooocus_to_a1111['vae']: Path(data['vae']).stem,\n            # workaround for multiline prompts\n            self.fooocus_to_a1111['raw_prompt']: self.raw_prompt,\n            self.fooocus_to_a1111['raw_negative_prompt']: self.raw_negative_prompt,\n        }\n\n        if self.refiner_model_name not in ['', 'None']:\n            generation_params |= {\n                self.fooocus_to_a1111['refiner_model']: self.refiner_model_name,\n                self.fooocus_to_a1111['refiner_model_hash']: self.refiner_model_hash\n            }\n\n        for key in ['adaptive_cfg', 'clip_skip', 'overwrite_switch', 'refiner_swap_method', 'freeu']:\n            if key in data:\n                generation_params[self.fooocus_to_a1111[key]] = data[key]\n\n        if len(self.loras) > 0:\n            lora_hashes = []\n            lora_weights = []\n            for index, (lora_name, lora_weight, lora_hash) in enumerate(self.loras):\n                # workaround for Fooocus not knowing LoRA name in LoRA metadata\n                lora_hashes.append(f'{lora_name}: {lora_hash}')\n                lora_weights.append(f'{lora_name}: {lora_weight}')\n            lora_hashes_string = ', '.join(lora_hashes)\n            lora_weights_string = ', '.join(lora_weights)\n            generation_params[self.fooocus_to_a1111['lora_hashes']] = lora_hashes_string\n            generation_params[self.fooocus_to_a1111['lora_weights']] = lora_weights_string\n\n        generation_params[self.fooocus_to_a1111['version']] = data['version']\n\n        if modules.config.metadata_created_by != '':\n            generation_params[self.fooocus_to_a1111['created_by']] = modules.config.metadata_created_by\n\n        generation_params_text = \", \".join(\n            [k if k == v else f'{k}: {quote(v)}' for k, v in generation_params.items() if\n             v is not None])\n        positive_prompt_resolved = ', '.join(self.full_prompt)\n        negative_prompt_resolved = ', '.join(self.full_negative_prompt)\n        negative_prompt_text = f\"\\nNegative prompt: {negative_prompt_resolved}\" if negative_prompt_resolved else \"\"\n        return f\"{positive_prompt_resolved}{negative_prompt_text}\\n{generation_params_text}\".strip()\n\n    @staticmethod\n    def add_extension_to_filename(data, filenames, key):\n        for filename in filenames:\n            path = Path(filename)\n            if data[key] == path.stem:\n                data[key] = filename\n                break\n\n\nclass FooocusMetadataParser(MetadataParser):\n    def get_scheme(self) -> MetadataScheme:\n        return MetadataScheme.FOOOCUS\n\n    def to_json(self, metadata: dict) -> dict:\n        for key, value in metadata.items():\n            if value in ['', 'None']:\n                continue\n            if key in ['base_model', 'refiner_model']:\n                metadata[key] = self.replace_value_with_filename(key, value, modules.config.model_filenames)\n            elif key.startswith('lora_combined_'):\n                metadata[key] = self.replace_value_with_filename(key, value, modules.config.lora_filenames)\n            elif key == 'vae':\n                metadata[key] = self.replace_value_with_filename(key, value, modules.config.vae_filenames)\n            else:\n                continue\n\n        return metadata\n\n    def to_string(self, metadata: list) -> str:\n        for li, (label, key, value) in enumerate(metadata):\n            # remove model folder paths from metadata\n            if key.startswith('lora_combined_'):\n                name, weight = value.split(' : ')\n                name = Path(name).stem\n                value = f'{name} : {weight}'\n                metadata[li] = (label, key, value)\n\n        res = {k: v for _, k, v in metadata}\n\n        res['full_prompt'] = self.full_prompt\n        res['full_negative_prompt'] = self.full_negative_prompt\n        res['steps'] = self.steps\n        res['base_model'] = self.base_model_name\n        res['base_model_hash'] = self.base_model_hash\n\n        if self.refiner_model_name not in ['', 'None']:\n            res['refiner_model'] = self.refiner_model_name\n            res['refiner_model_hash'] = self.refiner_model_hash\n\n        res['vae'] = self.vae_name\n        res['loras'] = self.loras\n\n        if modules.config.metadata_created_by != '':\n            res['created_by'] = modules.config.metadata_created_by\n\n        return json.dumps(dict(sorted(res.items())))\n\n    @staticmethod\n    def replace_value_with_filename(key, value, filenames):\n        for filename in filenames:\n            path = Path(filename)\n            if key.startswith('lora_combined_'):\n                name, weight = value.split(' : ')\n                if name == path.stem:\n                    return f'{filename} : {weight}'\n            elif value == path.stem:\n                return filename\n\n        return None\n\n\ndef get_metadata_parser(metadata_scheme: MetadataScheme) -> MetadataParser:\n    match metadata_scheme:\n        case MetadataScheme.FOOOCUS:\n            return FooocusMetadataParser()\n        case MetadataScheme.A1111:\n            return A1111MetadataParser()\n        case _:\n            raise NotImplementedError\n\n\ndef read_info_from_image(filepath) -> tuple[str | None, MetadataScheme | None]:\n    with Image.open(filepath) as image:\n        items = (image.info or {}).copy()\n\n    parameters = items.pop('parameters', None)\n    metadata_scheme = items.pop('fooocus_scheme', None)\n    exif = items.pop('exif', None)\n\n    if parameters is not None and is_json(parameters):\n        parameters = json.loads(parameters)\n    elif exif is not None:\n        exif = image.getexif()\n        # 0x9286 = UserComment\n        parameters = exif.get(0x9286, None)\n        # 0x927C = MakerNote\n        metadata_scheme = exif.get(0x927C, None)\n\n        if is_json(parameters):\n            parameters = json.loads(parameters)\n\n    try:\n        metadata_scheme = MetadataScheme(metadata_scheme)\n    except ValueError:\n        metadata_scheme = None\n\n        # broad fallback\n        if isinstance(parameters, dict):\n            metadata_scheme = MetadataScheme.FOOOCUS\n\n        if isinstance(parameters, str):\n            metadata_scheme = MetadataScheme.A1111\n\n    return parameters, metadata_scheme\n\n\ndef get_exif(metadata: str | None, metadata_scheme: str):\n    exif = Image.Exif()\n    # tags see see https://github.com/python-pillow/Pillow/blob/9.2.x/src/PIL/ExifTags.py\n    # 0x9286 = UserComment\n    exif[0x9286] = metadata\n    # 0x0131 = Software\n    exif[0x0131] = 'Fooocus v' + fooocus_version.version\n    # 0x927C = MakerNote\n    exif[0x927C] = metadata_scheme\n    return exif\n", "modules/style_sorter.py": "import os\nimport gradio as gr\nimport modules.localization as localization\nimport json\n\n\nall_styles = []\n\n\ndef try_load_sorted_styles(style_names, default_selected):\n    global all_styles\n\n    all_styles = style_names\n\n    try:\n        if os.path.exists('sorted_styles.json'):\n            with open('sorted_styles.json', 'rt', encoding='utf-8') as fp:\n                sorted_styles = []\n                for x in json.load(fp):\n                    if x in all_styles:\n                        sorted_styles.append(x)\n                for x in all_styles:\n                    if x not in sorted_styles:\n                        sorted_styles.append(x)\n                all_styles = sorted_styles\n    except Exception as e:\n        print('Load style sorting failed.')\n        print(e)\n\n    unselected = [y for y in all_styles if y not in default_selected]\n    all_styles = default_selected + unselected\n\n    return\n\n\ndef sort_styles(selected):\n    global all_styles\n    unselected = [y for y in all_styles if y not in selected]\n    sorted_styles = selected + unselected\n    try:\n        with open('sorted_styles.json', 'wt', encoding='utf-8') as fp:\n            json.dump(sorted_styles, fp, indent=4)\n    except Exception as e:\n        print('Write style sorting failed.')\n        print(e)\n    all_styles = sorted_styles\n    return gr.CheckboxGroup.update(choices=sorted_styles)\n\n\ndef localization_key(x):\n    return x + localization.current_translation.get(x, '')\n\n\ndef search_styles(selected, query):\n    unselected = [y for y in all_styles if y not in selected]\n    matched = [y for y in unselected if query.lower() in localization_key(y).lower()] if len(query.replace(' ', '')) > 0 else []\n    unmatched = [y for y in unselected if y not in matched]\n    sorted_styles = matched + selected + unmatched\n    return gr.CheckboxGroup.update(choices=sorted_styles)\n", "modules/lora.py": "def match_lora(lora, to_load):\n    patch_dict = {}\n    loaded_keys = set()\n    for x in to_load:\n        real_load_key = to_load[x]\n        if real_load_key in lora:\n            patch_dict[real_load_key] = ('fooocus', lora[real_load_key])\n            loaded_keys.add(real_load_key)\n            continue\n\n        alpha_name = \"{}.alpha\".format(x)\n        alpha = None\n        if alpha_name in lora.keys():\n            alpha = lora[alpha_name].item()\n            loaded_keys.add(alpha_name)\n\n        regular_lora = \"{}.lora_up.weight\".format(x)\n        diffusers_lora = \"{}_lora.up.weight\".format(x)\n        transformers_lora = \"{}.lora_linear_layer.up.weight\".format(x)\n        A_name = None\n\n        if regular_lora in lora.keys():\n            A_name = regular_lora\n            B_name = \"{}.lora_down.weight\".format(x)\n            mid_name = \"{}.lora_mid.weight\".format(x)\n        elif diffusers_lora in lora.keys():\n            A_name = diffusers_lora\n            B_name = \"{}_lora.down.weight\".format(x)\n            mid_name = None\n        elif transformers_lora in lora.keys():\n            A_name = transformers_lora\n            B_name =\"{}.lora_linear_layer.down.weight\".format(x)\n            mid_name = None\n\n        if A_name is not None:\n            mid = None\n            if mid_name is not None and mid_name in lora.keys():\n                mid = lora[mid_name]\n                loaded_keys.add(mid_name)\n            patch_dict[to_load[x]] = (\"lora\", (lora[A_name], lora[B_name], alpha, mid))\n            loaded_keys.add(A_name)\n            loaded_keys.add(B_name)\n\n\n        ######## loha\n        hada_w1_a_name = \"{}.hada_w1_a\".format(x)\n        hada_w1_b_name = \"{}.hada_w1_b\".format(x)\n        hada_w2_a_name = \"{}.hada_w2_a\".format(x)\n        hada_w2_b_name = \"{}.hada_w2_b\".format(x)\n        hada_t1_name = \"{}.hada_t1\".format(x)\n        hada_t2_name = \"{}.hada_t2\".format(x)\n        if hada_w1_a_name in lora.keys():\n            hada_t1 = None\n            hada_t2 = None\n            if hada_t1_name in lora.keys():\n                hada_t1 = lora[hada_t1_name]\n                hada_t2 = lora[hada_t2_name]\n                loaded_keys.add(hada_t1_name)\n                loaded_keys.add(hada_t2_name)\n\n            patch_dict[to_load[x]] = (\"loha\", (lora[hada_w1_a_name], lora[hada_w1_b_name], alpha, lora[hada_w2_a_name], lora[hada_w2_b_name], hada_t1, hada_t2))\n            loaded_keys.add(hada_w1_a_name)\n            loaded_keys.add(hada_w1_b_name)\n            loaded_keys.add(hada_w2_a_name)\n            loaded_keys.add(hada_w2_b_name)\n\n\n        ######## lokr\n        lokr_w1_name = \"{}.lokr_w1\".format(x)\n        lokr_w2_name = \"{}.lokr_w2\".format(x)\n        lokr_w1_a_name = \"{}.lokr_w1_a\".format(x)\n        lokr_w1_b_name = \"{}.lokr_w1_b\".format(x)\n        lokr_t2_name = \"{}.lokr_t2\".format(x)\n        lokr_w2_a_name = \"{}.lokr_w2_a\".format(x)\n        lokr_w2_b_name = \"{}.lokr_w2_b\".format(x)\n\n        lokr_w1 = None\n        if lokr_w1_name in lora.keys():\n            lokr_w1 = lora[lokr_w1_name]\n            loaded_keys.add(lokr_w1_name)\n\n        lokr_w2 = None\n        if lokr_w2_name in lora.keys():\n            lokr_w2 = lora[lokr_w2_name]\n            loaded_keys.add(lokr_w2_name)\n\n        lokr_w1_a = None\n        if lokr_w1_a_name in lora.keys():\n            lokr_w1_a = lora[lokr_w1_a_name]\n            loaded_keys.add(lokr_w1_a_name)\n\n        lokr_w1_b = None\n        if lokr_w1_b_name in lora.keys():\n            lokr_w1_b = lora[lokr_w1_b_name]\n            loaded_keys.add(lokr_w1_b_name)\n\n        lokr_w2_a = None\n        if lokr_w2_a_name in lora.keys():\n            lokr_w2_a = lora[lokr_w2_a_name]\n            loaded_keys.add(lokr_w2_a_name)\n\n        lokr_w2_b = None\n        if lokr_w2_b_name in lora.keys():\n            lokr_w2_b = lora[lokr_w2_b_name]\n            loaded_keys.add(lokr_w2_b_name)\n\n        lokr_t2 = None\n        if lokr_t2_name in lora.keys():\n            lokr_t2 = lora[lokr_t2_name]\n            loaded_keys.add(lokr_t2_name)\n\n        if (lokr_w1 is not None) or (lokr_w2 is not None) or (lokr_w1_a is not None) or (lokr_w2_a is not None):\n            patch_dict[to_load[x]] = (\"lokr\", (lokr_w1, lokr_w2, alpha, lokr_w1_a, lokr_w1_b, lokr_w2_a, lokr_w2_b, lokr_t2))\n\n        #glora\n        a1_name = \"{}.a1.weight\".format(x)\n        a2_name = \"{}.a2.weight\".format(x)\n        b1_name = \"{}.b1.weight\".format(x)\n        b2_name = \"{}.b2.weight\".format(x)\n        if a1_name in lora:\n            patch_dict[to_load[x]] = (\"glora\", (lora[a1_name], lora[a2_name], lora[b1_name], lora[b2_name], alpha))\n            loaded_keys.add(a1_name)\n            loaded_keys.add(a2_name)\n            loaded_keys.add(b1_name)\n            loaded_keys.add(b2_name)\n\n        w_norm_name = \"{}.w_norm\".format(x)\n        b_norm_name = \"{}.b_norm\".format(x)\n        w_norm = lora.get(w_norm_name, None)\n        b_norm = lora.get(b_norm_name, None)\n\n        if w_norm is not None:\n            loaded_keys.add(w_norm_name)\n            patch_dict[to_load[x]] = (\"diff\", (w_norm,))\n            if b_norm is not None:\n                loaded_keys.add(b_norm_name)\n                patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (b_norm,))\n\n        diff_name = \"{}.diff\".format(x)\n        diff_weight = lora.get(diff_name, None)\n        if diff_weight is not None:\n            patch_dict[to_load[x]] = (\"diff\", (diff_weight,))\n            loaded_keys.add(diff_name)\n\n        diff_bias_name = \"{}.diff_b\".format(x)\n        diff_bias = lora.get(diff_bias_name, None)\n        if diff_bias is not None:\n            patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (diff_bias,))\n            loaded_keys.add(diff_bias_name)\n\n    remaining_dict = {x: y for x, y in lora.items() if x not in loaded_keys}\n    return patch_dict, remaining_dict\n", "modules/extra_utils.py": "import os\nfrom ast import literal_eval\n\n\ndef makedirs_with_log(path):\n    try:\n        os.makedirs(path, exist_ok=True)\n    except OSError as error:\n        print(f'Directory {path} could not be created, reason: {error}')\n\n\ndef get_files_from_folder(folder_path, extensions=None, name_filter=None):\n    if not os.path.isdir(folder_path):\n        raise ValueError(\"Folder path is not a valid directory.\")\n\n    filenames = []\n\n    for root, _, files in os.walk(folder_path, topdown=False):\n        relative_path = os.path.relpath(root, folder_path)\n        if relative_path == \".\":\n            relative_path = \"\"\n        for filename in sorted(files, key=lambda s: s.casefold()):\n            _, file_extension = os.path.splitext(filename)\n            if (extensions is None or file_extension.lower() in extensions) and (name_filter is None or name_filter in _):\n                path = os.path.join(relative_path, filename)\n                filenames.append(path)\n\n    return filenames\n\n\ndef try_eval_env_var(value: str, expected_type=None):\n    try:\n        value_eval = value\n        if expected_type is bool:\n            value_eval = value.title()\n        value_eval = literal_eval(value_eval)\n        if expected_type is not None and not isinstance(value_eval, expected_type):\n            return value\n        return value_eval\n    except:\n        return value\n", "modules/upscaler.py": "import os\nimport torch\nimport modules.core as core\n\nfrom ldm_patched.pfn.architecture.RRDB import RRDBNet as ESRGAN\nfrom ldm_patched.contrib.external_upscale_model import ImageUpscaleWithModel\nfrom collections import OrderedDict\nfrom modules.config import path_upscale_models\n\nmodel_filename = os.path.join(path_upscale_models, 'fooocus_upscaler_s409985e5.bin')\nopImageUpscaleWithModel = ImageUpscaleWithModel()\nmodel = None\n\n\ndef perform_upscale(img):\n    global model\n\n    print(f'Upscaling image with shape {str(img.shape)} ...')\n\n    if model is None:\n        sd = torch.load(model_filename)\n        sdo = OrderedDict()\n        for k, v in sd.items():\n            sdo[k.replace('residual_block_', 'RDB')] = v\n        del sd\n        model = ESRGAN(sdo)\n        model.cpu()\n        model.eval()\n\n    img = core.numpy_to_pytorch(img)\n    img = opImageUpscaleWithModel.upscale(model, img)[0]\n    img = core.pytorch_to_numpy(img)[0]\n\n    return img\n", "modules/launch_util.py": "import os\nimport importlib\nimport importlib.util\nimport shutil\nimport subprocess\nimport sys\nimport re\nimport logging\nimport importlib.metadata\nimport packaging.version\nfrom packaging.requirements import Requirement\n\nlogging.getLogger(\"torch.distributed.nn\").setLevel(logging.ERROR)  # sshh...\nlogging.getLogger(\"xformers\").addFilter(lambda record: 'A matching Triton is not available' not in record.getMessage())\n\nre_requirement = re.compile(r\"\\s*([-\\w]+)\\s*(?:==\\s*([-+.\\w]+))?\\s*\")\n\npython = sys.executable\ndefault_command_live = (os.environ.get('LAUNCH_LIVE_OUTPUT') == \"1\")\nindex_url = os.environ.get('INDEX_URL', \"\")\n\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\n\n\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None\n\n\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:\n        print(desc)\n\n    run_kwargs = {\n        \"args\": command,\n        \"shell\": True,\n        \"env\": os.environ if custom_env is None else custom_env,\n        \"encoding\": 'utf8',\n        \"errors\": 'ignore',\n    }\n\n    if not live:\n        run_kwargs[\"stdout\"] = run_kwargs[\"stderr\"] = subprocess.PIPE\n\n    result = subprocess.run(**run_kwargs)\n\n    if result.returncode != 0:\n        error_bits = [\n            f\"{errdesc or 'Error running command'}.\",\n            f\"Command: {command}\",\n            f\"Error code: {result.returncode}\",\n        ]\n        if result.stdout:\n            error_bits.append(f\"stdout: {result.stdout}\")\n        if result.stderr:\n            error_bits.append(f\"stderr: {result.stderr}\")\n        raise RuntimeError(\"\\n\".join(error_bits))\n\n    return (result.stdout or \"\")\n\n\ndef run_pip(command, desc=None, live=default_command_live):\n    try:\n        index_url_line = f' --index-url {index_url}' if index_url != '' else ''\n        return run(f'\"{python}\" -m pip {command} --prefer-binary{index_url_line}', desc=f\"Installing {desc}\",\n                   errdesc=f\"Couldn't install {desc}\", live=live)\n    except Exception as e:\n        print(e)\n        print(f'CMD Failed {desc}: {command}')\n        return None\n\n\ndef requirements_met(requirements_file):\n    with open(requirements_file, \"r\", encoding=\"utf8\") as file:\n        for line in file:\n            line = line.strip()\n            if line == \"\" or line.startswith('#'):\n                continue\n\n            requirement = Requirement(line)\n            package = requirement.name\n\n            try:\n                version_installed = importlib.metadata.version(package)\n                installed_version = packaging.version.parse(version_installed)\n\n                # Check if the installed version satisfies the requirement\n                if installed_version not in requirement.specifier:\n                    print(f\"Version mismatch for {package}: Installed version {version_installed} does not meet requirement {requirement}\")\n                    return False\n            except Exception as e:\n                print(f\"Error checking version for {package}: {e}\")\n                return False\n\n    return True\n\n\ndef delete_folder_content(folder, prefix=None):\n    result = True\n\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'{prefix}Failed to delete {file_path}. Reason: {e}')\n            result = False\n\n    return result", "modules/patch_clip.py": "# Consistent with Kohya/A1111 to reduce differences between model training and inference.\n\nimport os\nimport torch\nimport ldm_patched.controlnet.cldm\nimport ldm_patched.k_diffusion.sampling\nimport ldm_patched.ldm.modules.attention\nimport ldm_patched.ldm.modules.diffusionmodules.model\nimport ldm_patched.ldm.modules.diffusionmodules.openaimodel\nimport ldm_patched.ldm.modules.diffusionmodules.openaimodel\nimport ldm_patched.modules.args_parser\nimport ldm_patched.modules.model_base\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.model_patcher\nimport ldm_patched.modules.samplers\nimport ldm_patched.modules.sd\nimport ldm_patched.modules.sd1_clip\nimport ldm_patched.modules.clip_vision\nimport ldm_patched.modules.ops as ops\n\nfrom modules.ops import use_patched_ops\nfrom transformers import CLIPTextModel, CLIPTextConfig, modeling_utils, CLIPVisionConfig, CLIPVisionModelWithProjection\n\n\ndef patched_encode_token_weights(self, token_weight_pairs):\n    to_encode = list()\n    max_token_len = 0\n    has_weights = False\n    for x in token_weight_pairs:\n        tokens = list(map(lambda a: a[0], x))\n        max_token_len = max(len(tokens), max_token_len)\n        has_weights = has_weights or not all(map(lambda a: a[1] == 1.0, x))\n        to_encode.append(tokens)\n\n    sections = len(to_encode)\n    if has_weights or sections == 0:\n        to_encode.append(ldm_patched.modules.sd1_clip.gen_empty_tokens(self.special_tokens, max_token_len))\n\n    out, pooled = self.encode(to_encode)\n    if pooled is not None:\n        first_pooled = pooled[0:1].to(ldm_patched.modules.model_management.intermediate_device())\n    else:\n        first_pooled = pooled\n\n    output = []\n    for k in range(0, sections):\n        z = out[k:k + 1]\n        if has_weights:\n            original_mean = z.mean()\n            z_empty = out[-1]\n            for i in range(len(z)):\n                for j in range(len(z[i])):\n                    weight = token_weight_pairs[k][j][1]\n                    if weight != 1.0:\n                        z[i][j] = (z[i][j] - z_empty[j]) * weight + z_empty[j]\n            new_mean = z.mean()\n            z = z * (original_mean / new_mean)\n        output.append(z)\n\n    if len(output) == 0:\n        return out[-1:].to(ldm_patched.modules.model_management.intermediate_device()), first_pooled\n    return torch.cat(output, dim=-2).to(ldm_patched.modules.model_management.intermediate_device()), first_pooled\n\n\ndef patched_SDClipModel__init__(self, max_length=77, freeze=True, layer=\"last\", layer_idx=None,\n                                textmodel_json_config=None, dtype=None, special_tokens=None,\n                                layer_norm_hidden_state=True, **kwargs):\n    torch.nn.Module.__init__(self)\n    assert layer in self.LAYERS\n\n    if special_tokens is None:\n        special_tokens = {\"start\": 49406, \"end\": 49407, \"pad\": 49407}\n\n    if textmodel_json_config is None:\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(ldm_patched.modules.sd1_clip.__file__)),\n                                             \"sd1_clip_config.json\")\n\n    config = CLIPTextConfig.from_json_file(textmodel_json_config)\n    self.num_layers = config.num_hidden_layers\n\n    with use_patched_ops(ops.manual_cast):\n        with modeling_utils.no_init_weights():\n            self.transformer = CLIPTextModel(config)\n\n    if dtype is not None:\n        self.transformer.to(dtype)\n\n    self.transformer.text_model.embeddings.to(torch.float32)\n\n    if freeze:\n        self.freeze()\n\n    self.max_length = max_length\n    self.layer = layer\n    self.layer_idx = None\n    self.special_tokens = special_tokens\n    self.text_projection = torch.nn.Parameter(torch.eye(self.transformer.get_input_embeddings().weight.shape[1]))\n    self.logit_scale = torch.nn.Parameter(torch.tensor(4.6055))\n    self.enable_attention_masks = False\n\n    self.layer_norm_hidden_state = layer_norm_hidden_state\n    if layer == \"hidden\":\n        assert layer_idx is not None\n        assert abs(layer_idx) < self.num_layers\n        self.clip_layer(layer_idx)\n    self.layer_default = (self.layer, self.layer_idx)\n\n\ndef patched_SDClipModel_forward(self, tokens):\n    backup_embeds = self.transformer.get_input_embeddings()\n    device = backup_embeds.weight.device\n    tokens = self.set_up_textual_embeddings(tokens, backup_embeds)\n    tokens = torch.LongTensor(tokens).to(device)\n\n    attention_mask = None\n    if self.enable_attention_masks:\n        attention_mask = torch.zeros_like(tokens)\n        max_token = self.transformer.get_input_embeddings().weight.shape[0] - 1\n        for x in range(attention_mask.shape[0]):\n            for y in range(attention_mask.shape[1]):\n                attention_mask[x, y] = 1\n                if tokens[x, y] == max_token:\n                    break\n\n    outputs = self.transformer(input_ids=tokens, attention_mask=attention_mask,\n                               output_hidden_states=self.layer == \"hidden\")\n    self.transformer.set_input_embeddings(backup_embeds)\n\n    if self.layer == \"last\":\n        z = outputs.last_hidden_state\n    elif self.layer == \"pooled\":\n        z = outputs.pooler_output[:, None, :]\n    else:\n        z = outputs.hidden_states[self.layer_idx]\n        if self.layer_norm_hidden_state:\n            z = self.transformer.text_model.final_layer_norm(z)\n\n    if hasattr(outputs, \"pooler_output\"):\n        pooled_output = outputs.pooler_output.float()\n    else:\n        pooled_output = None\n\n    if self.text_projection is not None and pooled_output is not None:\n        pooled_output = pooled_output.float().to(self.text_projection.device) @ self.text_projection.float()\n\n    return z.float(), pooled_output\n\n\ndef patched_ClipVisionModel__init__(self, json_config):\n    config = CLIPVisionConfig.from_json_file(json_config)\n\n    self.load_device = ldm_patched.modules.model_management.text_encoder_device()\n    self.offload_device = ldm_patched.modules.model_management.text_encoder_offload_device()\n\n    if ldm_patched.modules.model_management.should_use_fp16(self.load_device, prioritize_performance=False):\n        self.dtype = torch.float16\n    else:\n        self.dtype = torch.float32\n\n    with use_patched_ops(ops.manual_cast):\n        with modeling_utils.no_init_weights():\n            self.model = CLIPVisionModelWithProjection(config)\n\n    self.model.to(self.dtype)\n    self.patcher = ldm_patched.modules.model_patcher.ModelPatcher(\n        self.model,\n        load_device=self.load_device,\n        offload_device=self.offload_device\n    )\n\n\ndef patched_ClipVisionModel_encode_image(self, image):\n    ldm_patched.modules.model_management.load_model_gpu(self.patcher)\n    pixel_values = ldm_patched.modules.clip_vision.clip_preprocess(image.to(self.load_device))\n    outputs = self.model(pixel_values=pixel_values, output_hidden_states=True)\n\n    for k in outputs:\n        t = outputs[k]\n        if t is not None:\n            if k == 'hidden_states':\n                outputs[\"penultimate_hidden_states\"] = t[-2].to(ldm_patched.modules.model_management.intermediate_device())\n                outputs[\"hidden_states\"] = None\n            else:\n                outputs[k] = t.to(ldm_patched.modules.model_management.intermediate_device())\n\n    return outputs\n\n\ndef patch_all_clip():\n    ldm_patched.modules.sd1_clip.ClipTokenWeightEncoder.encode_token_weights = patched_encode_token_weights\n    ldm_patched.modules.sd1_clip.SDClipModel.__init__ = patched_SDClipModel__init__\n    ldm_patched.modules.sd1_clip.SDClipModel.forward = patched_SDClipModel_forward\n    ldm_patched.modules.clip_vision.ClipVisionModel.__init__ = patched_ClipVisionModel__init__\n    ldm_patched.modules.clip_vision.ClipVisionModel.encode_image = patched_ClipVisionModel_encode_image\n    return\n", "modules/model_loader.py": "import os\nfrom urllib.parse import urlparse\nfrom typing import Optional\n\n\ndef load_file_from_url(\n        url: str,\n        *,\n        model_dir: str,\n        progress: bool = True,\n        file_name: Optional[str] = None,\n) -> str:\n    \"\"\"Download a file from `url` into `model_dir`, using the file present if possible.\n\n    Returns the path to the downloaded file.\n    \"\"\"\n    domain = os.environ.get(\"HF_MIRROR\", \"https://huggingface.co\").rstrip('/')\n    url = str.replace(url, \"https://huggingface.co\", domain, 1)\n    os.makedirs(model_dir, exist_ok=True)\n    if not file_name:\n        parts = urlparse(url)\n        file_name = os.path.basename(parts.path)\n    cached_file = os.path.abspath(os.path.join(model_dir, file_name))\n    if not os.path.exists(cached_file):\n        print(f'Downloading: \"{url}\" to {cached_file}\\n')\n        from torch.hub import download_url_to_file\n        download_url_to_file(url, cached_file, progress=progress)\n    return cached_file\n", "modules/__init__.py": "", "modules/default_pipeline.py": "import modules.core as core\nimport os\nimport torch\nimport modules.patch\nimport modules.config\nimport modules.flags\nimport ldm_patched.modules.model_management\nimport ldm_patched.modules.latent_formats\nimport modules.inpaint_worker\nimport extras.vae_interpose as vae_interpose\nfrom extras.expansion import FooocusExpansion\n\nfrom ldm_patched.modules.model_base import SDXL, SDXLRefiner\nfrom modules.sample_hijack import clip_separate\nfrom modules.util import get_file_from_folder_list, get_enabled_loras\n\n\nmodel_base = core.StableDiffusionModel()\nmodel_refiner = core.StableDiffusionModel()\n\nfinal_expansion = None\nfinal_unet = None\nfinal_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\n\nloaded_ControlNets = {}\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:\n        if p is not None:\n            if p in loaded_ControlNets:\n                cache[p] = loaded_ControlNets[p]\n            else:\n                cache[p] = core.load_controlnet(p)\n    loaded_ControlNets = cache\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef assert_model_integrity():\n    error_message = None\n\n    if not isinstance(model_base.unet_with_lora.model, SDXL):\n        error_message = 'You have selected base model other than SDXL. This is not supported yet.'\n\n    if error_message is not None:\n        raise NotImplementedError(error_message)\n\n    return True\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_base_model(name, vae_name=None):\n    global model_base\n\n    filename = get_file_from_folder_list(name, modules.config.paths_checkpoints)\n\n    vae_filename = None\n    if vae_name is not None and vae_name != modules.flags.default_vae:\n        vae_filename = get_file_from_folder_list(vae_name, modules.config.path_vae)\n\n    if model_base.filename == filename and model_base.vae_filename == vae_filename:\n        return\n\n    model_base = core.load_model(filename, vae_filename)\n    print(f'Base model loaded: {model_base.filename}')\n    print(f'VAE loaded: {model_base.vae_filename}')\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_refiner_model(name):\n    global model_refiner\n\n    filename = get_file_from_folder_list(name, modules.config.paths_checkpoints)\n\n    if model_refiner.filename == filename:\n        return\n\n    model_refiner = core.StableDiffusionModel()\n\n    if name == 'None':\n        print(f'Refiner unloaded.')\n        return\n\n    model_refiner = core.load_model(filename)\n    print(f'Refiner model loaded: {model_refiner.filename}')\n\n    if isinstance(model_refiner.unet.model, SDXL):\n        model_refiner.clip = None\n        model_refiner.vae = None\n    elif isinstance(model_refiner.unet.model, SDXLRefiner):\n        model_refiner.clip = None\n        model_refiner.vae = None\n    else:\n        model_refiner.clip = None\n\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef synthesize_refiner_model():\n    global model_base, model_refiner\n\n    print('Synthetic Refiner Activated')\n    model_refiner = core.StableDiffusionModel(\n        unet=model_base.unet,\n        vae=model_base.vae,\n        clip=model_base.clip,\n        clip_vision=model_base.clip_vision,\n        filename=model_base.filename\n    )\n    model_refiner.vae = None\n    model_refiner.clip = None\n    model_refiner.clip_vision = None\n\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_loras(loras, base_model_additional_loras=None):\n    global model_base, model_refiner\n\n    if not isinstance(base_model_additional_loras, list):\n        base_model_additional_loras = []\n\n    model_base.refresh_loras(loras + base_model_additional_loras)\n    model_refiner.refresh_loras(loras)\n\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_encode_single(clip, text, verbose=False):\n    cached = clip.fcs_cond_cache.get(text, None)\n    if cached is not None:\n        if verbose:\n            print(f'[CLIP Cached] {text}')\n        return cached\n    tokens = clip.tokenize(text)\n    result = clip.encode_from_tokens(tokens, return_pooled=True)\n    clip.fcs_cond_cache[text] = result\n    if verbose:\n        print(f'[CLIP Encoded] {text}')\n    return result\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clone_cond(conds):\n    results = []\n\n    for c, p in conds:\n        p = p[\"pooled_output\"]\n\n        if isinstance(c, torch.Tensor):\n            c = c.clone()\n\n        if isinstance(p, torch.Tensor):\n            p = p.clone()\n\n        results.append([c, {\"pooled_output\": p}])\n\n    return results\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_encode(texts, pool_top_k=1):\n    global final_clip\n\n    if final_clip is None:\n        return None\n    if not isinstance(texts, list):\n        return None\n    if len(texts) == 0:\n        return None\n\n    cond_list = []\n    pooled_acc = 0\n\n    for i, text in enumerate(texts):\n        cond, pooled = clip_encode_single(final_clip, text)\n        cond_list.append(cond)\n        if i < pool_top_k:\n            pooled_acc += pooled\n\n    return [[torch.cat(cond_list, dim=1), {\"pooled_output\": pooled_acc}]]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef set_clip_skip(clip_skip: int):\n    global final_clip\n\n    if final_clip is None:\n        return\n\n    final_clip.clip_layer(-abs(clip_skip))\n    return\n\n@torch.no_grad()\n@torch.inference_mode()\ndef clear_all_caches():\n    final_clip.fcs_cond_cache = {}\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef prepare_text_encoder(async_call=True):\n    if async_call:\n        # TODO: make sure that this is always called in an async way so that users cannot feel it.\n        pass\n    assert_model_integrity()\n    ldm_patched.modules.model_management.load_models_gpu([final_clip.patcher, final_expansion.patcher])\n    return\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_everything(refiner_model_name, base_model_name, loras,\n                       base_model_additional_loras=None, use_synthetic_refiner=False, vae_name=None):\n    global final_unet, final_clip, final_vae, final_refiner_unet, final_refiner_vae, final_expansion\n\n    final_unet = None\n    final_clip = None\n    final_vae = None\n    final_refiner_unet = None\n    final_refiner_vae = None\n\n    if use_synthetic_refiner and refiner_model_name == 'None':\n        print('Synthetic Refiner Activated')\n        refresh_base_model(base_model_name, vae_name)\n        synthesize_refiner_model()\n    else:\n        refresh_refiner_model(refiner_model_name)\n        refresh_base_model(base_model_name, vae_name)\n\n    refresh_loras(loras, base_model_additional_loras=base_model_additional_loras)\n    assert_model_integrity()\n\n    final_unet = model_base.unet_with_lora\n    final_clip = model_base.clip_with_lora\n    final_vae = model_base.vae\n\n    final_refiner_unet = model_refiner.unet_with_lora\n    final_refiner_vae = model_refiner.vae\n\n    if final_expansion is None:\n        final_expansion = FooocusExpansion()\n\n    prepare_text_encoder(async_call=True)\n    clear_all_caches()\n    return\n\n\nrefresh_everything(\n    refiner_model_name=modules.config.default_refiner_model_name,\n    base_model_name=modules.config.default_base_model_name,\n    loras=get_enabled_loras(modules.config.default_loras),\n    vae_name=modules.config.default_vae,\n)\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef vae_parse(latent):\n    if final_refiner_vae is None:\n        return latent\n\n    result = vae_interpose.parse(latent[\"samples\"])\n    return {'samples': result}\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef calculate_sigmas_all(sampler, model, scheduler, steps):\n    from ldm_patched.modules.samplers import calculate_sigmas_scheduler\n\n    discard_penultimate_sigma = False\n    if sampler in ['dpm_2', 'dpm_2_ancestral']:\n        steps += 1\n        discard_penultimate_sigma = True\n\n    sigmas = calculate_sigmas_scheduler(model, scheduler, steps)\n\n    if discard_penultimate_sigma:\n        sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n    return sigmas\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef calculate_sigmas(sampler, model, scheduler, steps, denoise):\n    if denoise is None or denoise > 0.9999:\n        sigmas = calculate_sigmas_all(sampler, model, scheduler, steps)\n    else:\n        new_steps = int(steps / denoise)\n        sigmas = calculate_sigmas_all(sampler, model, scheduler, new_steps)\n        sigmas = sigmas[-(steps + 1):]\n    return sigmas\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef get_candidate_vae(steps, switch, denoise=1.0, refiner_swap_method='joint'):\n    assert refiner_swap_method in ['joint', 'separate', 'vae']\n\n    if final_refiner_vae is not None and final_refiner_unet is not None:\n        if denoise > 0.9:\n            return final_vae, final_refiner_vae\n        else:\n            if denoise > (float(steps - switch) / float(steps)) ** 0.834:  # karras 0.834\n                return final_vae, None\n            else:\n                return final_refiner_vae, None\n\n    return final_vae, final_refiner_vae\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef process_diffusion(positive_cond, negative_cond, steps, switch, width, height, image_seed, callback, sampler_name, scheduler_name, latent=None, denoise=1.0, tiled=False, cfg_scale=7.0, refiner_swap_method='joint', disable_preview=False):\n    target_unet, target_vae, target_refiner_unet, target_refiner_vae, target_clip \\\n        = final_unet, final_vae, final_refiner_unet, final_refiner_vae, final_clip\n\n    assert refiner_swap_method in ['joint', 'separate', 'vae']\n\n    if final_refiner_vae is not None and final_refiner_unet is not None:\n        # Refiner Use Different VAE (then it is SD15)\n        if denoise > 0.9:\n            refiner_swap_method = 'vae'\n        else:\n            refiner_swap_method = 'joint'\n            if denoise > (float(steps - switch) / float(steps)) ** 0.834:  # karras 0.834\n                target_unet, target_vae, target_refiner_unet, target_refiner_vae \\\n                    = final_unet, final_vae, None, None\n                print(f'[Sampler] only use Base because of partial denoise.')\n            else:\n                positive_cond = clip_separate(positive_cond, target_model=final_refiner_unet.model, target_clip=final_clip)\n                negative_cond = clip_separate(negative_cond, target_model=final_refiner_unet.model, target_clip=final_clip)\n                target_unet, target_vae, target_refiner_unet, target_refiner_vae \\\n                    = final_refiner_unet, final_refiner_vae, None, None\n                print(f'[Sampler] only use Refiner because of partial denoise.')\n\n    print(f'[Sampler] refiner_swap_method = {refiner_swap_method}')\n\n    if latent is None:\n        initial_latent = core.generate_empty_latent(width=width, height=height, batch_size=1)\n    else:\n        initial_latent = latent\n\n    minmax_sigmas = calculate_sigmas(sampler=sampler_name, scheduler=scheduler_name, model=final_unet.model, steps=steps, denoise=denoise)\n    sigma_min, sigma_max = minmax_sigmas[minmax_sigmas > 0].min(), minmax_sigmas.max()\n    sigma_min = float(sigma_min.cpu().numpy())\n    sigma_max = float(sigma_max.cpu().numpy())\n    print(f'[Sampler] sigma_min = {sigma_min}, sigma_max = {sigma_max}')\n\n    modules.patch.BrownianTreeNoiseSamplerPatched.global_init(\n        initial_latent['samples'].to(ldm_patched.modules.model_management.get_torch_device()),\n        sigma_min, sigma_max, seed=image_seed, cpu=False)\n\n    decoded_latent = None\n\n    if refiner_swap_method == 'joint':\n        sampled_latent = core.ksampler(\n            model=target_unet,\n            refiner=target_refiner_unet,\n            positive=positive_cond,\n            negative=negative_cond,\n            latent=initial_latent,\n            steps=steps, start_step=0, last_step=steps, disable_noise=False, force_full_denoise=True,\n            seed=image_seed,\n            denoise=denoise,\n            callback_function=callback,\n            cfg=cfg_scale,\n            sampler_name=sampler_name,\n            scheduler=scheduler_name,\n            refiner_switch=switch,\n            previewer_start=0,\n            previewer_end=steps,\n            disable_preview=disable_preview\n        )\n        decoded_latent = core.decode_vae(vae=target_vae, latent_image=sampled_latent, tiled=tiled)\n\n    if refiner_swap_method == 'separate':\n        sampled_latent = core.ksampler(\n            model=target_unet,\n            positive=positive_cond,\n            negative=negative_cond,\n            latent=initial_latent,\n            steps=steps, start_step=0, last_step=switch, disable_noise=False, force_full_denoise=False,\n            seed=image_seed,\n            denoise=denoise,\n            callback_function=callback,\n            cfg=cfg_scale,\n            sampler_name=sampler_name,\n            scheduler=scheduler_name,\n            previewer_start=0,\n            previewer_end=steps,\n            disable_preview=disable_preview\n        )\n        print('Refiner swapped by changing ksampler. Noise preserved.')\n\n        target_model = target_refiner_unet\n        if target_model is None:\n            target_model = target_unet\n            print('Use base model to refine itself - this may because of developer mode.')\n\n        sampled_latent = core.ksampler(\n            model=target_model,\n            positive=clip_separate(positive_cond, target_model=target_model.model, target_clip=target_clip),\n            negative=clip_separate(negative_cond, target_model=target_model.model, target_clip=target_clip),\n            latent=sampled_latent,\n            steps=steps, start_step=switch, last_step=steps, disable_noise=True, force_full_denoise=True,\n            seed=image_seed,\n            denoise=denoise,\n            callback_function=callback,\n            cfg=cfg_scale,\n            sampler_name=sampler_name,\n            scheduler=scheduler_name,\n            previewer_start=switch,\n            previewer_end=steps,\n            disable_preview=disable_preview\n        )\n\n        target_model = target_refiner_vae\n        if target_model is None:\n            target_model = target_vae\n        decoded_latent = core.decode_vae(vae=target_model, latent_image=sampled_latent, tiled=tiled)\n\n    if refiner_swap_method == 'vae':\n        modules.patch.patch_settings[os.getpid()].eps_record = 'vae'\n\n        if modules.inpaint_worker.current_task is not None:\n            modules.inpaint_worker.current_task.unswap()\n\n        sampled_latent = core.ksampler(\n            model=target_unet,\n            positive=positive_cond,\n            negative=negative_cond,\n            latent=initial_latent,\n            steps=steps, start_step=0, last_step=switch, disable_noise=False, force_full_denoise=True,\n            seed=image_seed,\n            denoise=denoise,\n            callback_function=callback,\n            cfg=cfg_scale,\n            sampler_name=sampler_name,\n            scheduler=scheduler_name,\n            previewer_start=0,\n            previewer_end=steps,\n            disable_preview=disable_preview\n        )\n        print('Fooocus VAE-based swap.')\n\n        target_model = target_refiner_unet\n        if target_model is None:\n            target_model = target_unet\n            print('Use base model to refine itself - this may because of developer mode.')\n\n        sampled_latent = vae_parse(sampled_latent)\n\n        k_sigmas = 1.4\n        sigmas = calculate_sigmas(sampler=sampler_name,\n                                  scheduler=scheduler_name,\n                                  model=target_model.model,\n                                  steps=steps,\n                                  denoise=denoise)[switch:] * k_sigmas\n        len_sigmas = len(sigmas) - 1\n\n        noise_mean = torch.mean(modules.patch.patch_settings[os.getpid()].eps_record, dim=1, keepdim=True)\n\n        if modules.inpaint_worker.current_task is not None:\n            modules.inpaint_worker.current_task.swap()\n\n        sampled_latent = core.ksampler(\n            model=target_model,\n            positive=clip_separate(positive_cond, target_model=target_model.model, target_clip=target_clip),\n            negative=clip_separate(negative_cond, target_model=target_model.model, target_clip=target_clip),\n            latent=sampled_latent,\n            steps=len_sigmas, start_step=0, last_step=len_sigmas, disable_noise=False, force_full_denoise=True,\n            seed=image_seed+1,\n            denoise=denoise,\n            callback_function=callback,\n            cfg=cfg_scale,\n            sampler_name=sampler_name,\n            scheduler=scheduler_name,\n            previewer_start=switch,\n            previewer_end=steps,\n            sigmas=sigmas,\n            noise_mean=noise_mean,\n            disable_preview=disable_preview\n        )\n\n        target_model = target_refiner_vae\n        if target_model is None:\n            target_model = target_vae\n        decoded_latent = core.decode_vae(vae=target_model, latent_image=sampled_latent, tiled=tiled)\n\n    images = core.pytorch_to_numpy(decoded_latent)\n    modules.patch.patch_settings[os.getpid()].eps_record = None\n    return images\n", "modules/sdxl_styles.py": "import os\nimport re\nimport json\nimport math\n\nfrom modules.extra_utils import get_files_from_folder\nfrom random import Random\n\n# cannot use modules.config - validators causing circular imports\nstyles_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../sdxl_styles/'))\n\n\ndef normalize_key(k):\n    k = k.replace('-', ' ')\n    words = k.split(' ')\n    words = [w[:1].upper() + w[1:].lower() for w in words]\n    k = ' '.join(words)\n    k = k.replace('3d', '3D')\n    k = k.replace('Sai', 'SAI')\n    k = k.replace('Mre', 'MRE')\n    k = k.replace('(s', '(S')\n    return k\n\n\nstyles = {}\nstyles_files = get_files_from_folder(styles_path, ['.json'])\n\nfor x in ['sdxl_styles_fooocus.json',\n          'sdxl_styles_sai.json',\n          'sdxl_styles_mre.json',\n          'sdxl_styles_twri.json',\n          'sdxl_styles_diva.json',\n          'sdxl_styles_marc_k3nt3l.json']:\n    if x in styles_files:\n        styles_files.remove(x)\n        styles_files.append(x)\n\nfor styles_file in styles_files:\n    try:\n        with open(os.path.join(styles_path, styles_file), encoding='utf-8') as f:\n            for entry in json.load(f):\n                name = normalize_key(entry['name'])\n                prompt = entry['prompt'] if 'prompt' in entry else ''\n                negative_prompt = entry['negative_prompt'] if 'negative_prompt' in entry else ''\n                styles[name] = (prompt, negative_prompt)\n    except Exception as e:\n        print(str(e))\n        print(f'Failed to load style file {styles_file}')\n\nstyle_keys = list(styles.keys())\nfooocus_expansion = 'Fooocus V2'\nrandom_style_name = 'Random Style'\nlegal_style_names = [fooocus_expansion, random_style_name] + style_keys\n\n\ndef get_random_style(rng: Random) -> str:\n    return rng.choice(list(styles.items()))[0]\n\n\ndef apply_style(style, positive):\n    p, n = styles[style]\n    return p.replace('{prompt}', positive).splitlines(), n.splitlines()\n\n\ndef get_words(arrays, total_mult, index):\n    if len(arrays) == 1:\n        return [arrays[0].split(',')[index]]\n    else:\n        words = arrays[0].split(',')\n        word = words[index % len(words)]\n        index -= index % len(words)\n        index /= len(words)\n        index = math.floor(index)\n        return [word] + get_words(arrays[1:], math.floor(total_mult / len(words)), index)\n\n\ndef apply_arrays(text, index):\n    arrays = re.findall(r'\\[\\[(.*?)\\]\\]', text)\n    if len(arrays) == 0:\n        return text\n\n    print(f'[Arrays] processing: {text}')\n    mult = 1\n    for arr in arrays:\n        words = arr.split(',')\n        mult *= len(words)\n    \n    index %= mult\n    chosen_words = get_words(arrays, mult, index)\n    \n    i = 0\n    for arr in arrays:\n        text = text.replace(f'[[{arr}]]', chosen_words[i], 1)   \n        i = i+1\n    \n    return text\n\n"}