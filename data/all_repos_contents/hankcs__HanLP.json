{"setup.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\nimport sys\nfrom os.path import abspath, join, dirname\nfrom setuptools import find_packages, setup\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\nversion = {}\nwith open(join(this_dir, \"hanlp\", \"version.py\")) as fp:\n    exec(fp.read(), version)\n\nFASTTEXT = 'fasttext-wheel==0.9.2'\nsys_version_info = sys.version_info\nif sys_version_info >= (3, 10):\n    TENSORFLOW = ['tensorflow>=2.8.0']\nelse:\n    TENSORFLOW = ['tensorflow==2.6.0', 'keras==2.6.0', 'protobuf<3.19']\n\nTOKENIZERS = []\nif (sys_version_info.major, sys_version_info.minor) == (3, 6) and sys.platform == 'darwin':\n    TOKENIZERS = ['tokenizers==0.10.3']\n\nextras_require = {\n    'amr': [\n        'penman==1.2.1',\n        'networkx>=2.5.1',\n        'perin-parser>=0.0.12',\n    ],\n    'fasttext': [FASTTEXT],\n    'tf': [FASTTEXT, *TENSORFLOW]\n}\nextras_require['full'] = list(set(sum(extras_require.values(), [])))\n\nsetup(\n    name='hanlp',\n    version=version['__version__'],\n    description='HanLP: Han Language Processing',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/hankcs/HanLP',\n    author='hankcs',\n    author_email='hankcshe@gmail.com',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Developers',\n        \"Development Status :: 4 - Beta\",\n        'Operating System :: OS Independent',\n        \"License :: OSI Approved :: Apache Software License\",\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        \"Topic :: Text Processing :: Linguistic\"\n    ],\n    keywords='corpus,machine-learning,NLU,NLP',\n    packages=find_packages(exclude=['docs', 'tests*']),\n    include_package_data=True,\n    install_requires=[\n        'termcolor',\n        'pynvml',\n        'toposort==1.5',\n        'transformers>=4.1.1',\n        'sentencepiece>=0.1.91',  # Essential for tokenization_bert_japanese\n        'torch>=1.6.0',\n        'hanlp-common>=0.0.19',\n        'hanlp-trie>=0.0.4',\n        'hanlp-downloader',\n        *TOKENIZERS,\n    ],\n    extras_require=extras_require,\n    python_requires='>=3.6',\n)\n", "plugins/hanlp_trie/setup.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\nfrom os.path import abspath, join, dirname\nfrom setuptools import find_packages, setup\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\n\nsetup(\n    name='hanlp_trie',\n    version='0.0.5',\n    description='HanLP: Han Language Processing',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/hankcs/HanLP',\n    author='hankcs',\n    author_email='hankcshe@gmail.com',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Developers',\n        \"Development Status :: 3 - Alpha\",\n        'Operating System :: OS Independent',\n        \"License :: OSI Approved :: Apache Software License\",\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        \"Topic :: Text Processing :: Linguistic\"\n    ],\n    keywords='corpus,machine-learning,NLU,NLP',\n    packages=find_packages(exclude=['docs', 'tests*']),\n    include_package_data=True,\n    install_requires=[\n        'hanlp_common'\n    ],\n    python_requires='>=3.6',\n)\n", "plugins/hanlp_trie/hanlp_trie/dictionary.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-29 17:53\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple, Any, Dict, Union, Sequence, Iterable, Optional\n\nfrom hanlp_common.configurable import Configurable\nfrom hanlp_common.reflection import classpath_of\nfrom hanlp_trie.trie import Trie\n\n\nclass DictInterface(ABC):\n    @abstractmethod\n    def tokenize(self, text: Union[str, Sequence[str]]) -> List[Tuple[int, int, Any]]:\n        \"\"\"Implement this method to tokenize a piece of text into a list of non-intersect spans, each span is a tuple\n        of ``(begin_offset, end_offset, label)``, where label is some properties related to this span and downstream\n        tasks have the freedom to define what kind of labels they want.\n\n        Args:\n            text: The text to be tokenized.\n\n        Returns:\n              A list of tokens.\n\n        \"\"\"\n        pass\n\n    def split(self, text: Union[str, Sequence[str]]) -> List[Tuple[int, int, Any]]:\n        \"\"\"Like the :meth:`str.split`, this method splits a piece of text into chunks by taking the keys in this\n        dictionary as delimiters. It performs longest-prefix-matching on text and split it whenever a longest key is\n        matched. Unlike the :meth:`str.split`, it inserts matched keys into the results list right after where they are\n        found. So that the text can be restored by joining chunks in the results list.\n\n        Args:\n            text: A piece of text.\n\n        Returns:\n            A list of chunks, each chunk is a span of ``(begin_offset, end_offset, label)``, where label is some\n            properties related to this span and downstream tasks.\n        \"\"\"\n        offset = 0\n        spans = []\n        for begin, end, label in self.tokenize(text):\n            if begin > offset:\n                spans.append(text[offset:begin])\n            spans.append((begin, end, label))\n            offset = end\n        if offset < len(text):\n            spans.append(text[offset:])\n        return spans\n\n\nclass TrieDict(Trie, DictInterface, Configurable):\n    def __init__(self, dictionary: Optional[Union[Dict[Iterable[str], Any], Iterable[str]]] = None) -> None:\n        r\"\"\"\n        A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with\n        a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each\n        key is assigned with a boolean value ``True``.\n\n        Args:\n            dictionary: A custom dictionary of string-value pairs.\n        \"\"\"\n        super().__init__(dictionary)\n\n    def tokenize(self, text: Union[str, Sequence[str]]) -> List[Tuple[int, int, Any]]:\n        return self.parse_longest(text)\n\n    def split_batch(self, data: List[str]) -> Tuple[List[str], List[int], List[List[Tuple[int, int, Any]]]]:\n        \"\"\" A handy method to perform longest-prefix-matching on a batch of sentences. It tokenize each sentence, record\n        the chunks being either a key in the dict or a span outside of the dict. The spans are then packed into a new\n        batch and returned along with the following information:\n\n            - which sentence a span belongs to\n            - the matched keys along with their spans and values.\n\n        This method bridges the gap between statistical models and rule-based gazetteers.\n        It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.merge_batch`.\n\n        Args:\n            data: A batch of sentences.\n\n        Returns:\n            A tuple of the new batch, the belonging information and the keys.\n        \"\"\"\n        new_data, new_data_belongs, parts = [], [], []\n        for idx, sent in enumerate(data):\n            parts.append([])\n            found = self.tokenize(sent)\n            if found:\n                pre_start = 0\n                for start, end, info in found:\n                    if start > pre_start:\n                        new_data.append(sent[pre_start:start])\n                        new_data_belongs.append(idx)\n                    pre_start = end\n                    parts[idx].append((start, end, info))\n                if pre_start != len(sent):\n                    new_data.append(sent[pre_start:])\n                    new_data_belongs.append(idx)\n            else:\n                new_data.append(sent)\n                new_data_belongs.append(idx)\n        return new_data, new_data_belongs, parts\n\n    @staticmethod\n    def merge_batch(data, new_outputs, new_data_belongs, parts):\n        \"\"\" A helper method to merge the outputs of split batch back by concatenating the output per span with the key\n        used to split it. It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.split_batch`.\n\n        Args:\n            data: Split batch.\n            new_outputs: Outputs of the split batch.\n            new_data_belongs: Belonging information.\n            parts: The keys.\n\n        Returns:\n            Merged outputs.\n        \"\"\"\n        outputs = []\n        segments = []\n        for idx in range(len(data)):\n            segments.append([])\n        for o, b in zip(new_outputs, new_data_belongs):\n            dst = segments[b]\n            dst.append(o)\n        for s, p, sent in zip(segments, parts, data):\n            s: list = s\n            if p:\n                dst = []\n                offset = 0\n                for start, end, info in p:\n                    while offset < start:\n                        head = s.pop(0)\n                        offset += sum(len(token) for token in head)\n                        dst += head\n                    if isinstance(info, list):\n                        dst += info\n                    elif isinstance(info, str):\n                        dst.append(info)\n                    else:\n                        dst.append(sent[start:end])\n                    offset = end\n                if s:\n                    assert len(s) == 1\n                    dst += s[0]\n                outputs.append(dst)\n            else:\n                outputs.append(s[0])\n        return outputs\n\n    @property\n    def config(self):\n        return {\n            'classpath': classpath_of(self),\n            'dictionary': dict(self.items())\n        }\n\n\nclass TupleTrieDict(TrieDict):\n    def __init__(self, dictionary: Optional[Union[Dict[Iterable[str], Any], Iterable[str]]] = None) -> None:\n        r\"\"\"\n        A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with\n        a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each\n        key is assigned with a boolean value ``True``. In comparison to ``TrieDict``, ``TupleTrieDict`` additionally\n        supports serializing/deserializing tuple-as-keys dict.\n\n        Args:\n            dictionary: A custom dictionary of string-value pairs.\n        \"\"\"\n        if isinstance(dictionary, list) and dictionary and isinstance(dictionary[0], (list, tuple)):\n            _d = dict()\n            for k, v in dictionary:\n                _d[tuple(k)] = v\n            dictionary = _d\n        super().__init__(dictionary)\n\n    @property\n    def config(self):\n        return {\n            'classpath': classpath_of(self),\n            'dictionary': list(self.items(prefix=()))\n        }\n\n    def parse_longest(self, text: Sequence[str]) -> List[Tuple[int, int, Any]]:\n        \"\"\"Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till\n        its tail. By definition, the matches won't overlap with each other.\n\n        Args:\n            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.\n                The trie will transit on either types properly, which means a list of str simply defines a list of\n                transition criteria while a str defines each criterion as a character.\n\n        Returns:\n            A tuple of ``(begin, end, value)``.\n\n        \"\"\"\n        found = []\n        i = 0\n        while i < len(text):\n            state = self.transit(text[i:i + 1])\n            if state:\n                to = i + 1\n                end = to\n                value = state._value\n                for to in range(i + 1, len(text)):\n                    state = state.transit(text[to:to + 1])\n                    if not state:\n                        break\n                    if state._value is not None:\n                        value = state._value\n                        end = to + 1\n                if value is not None:\n                    found.append((i, end, value))\n                    i = end - 1\n            i += 1\n        return found\n", "plugins/hanlp_trie/hanlp_trie/trie.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-04 23:46\nfrom typing import Dict, Any, List, Tuple, Sequence, Union, Iterable, Optional\n\n\nclass Node(object):\n    def __init__(self, value=None) -> None:\n        \"\"\"A node in a trie tree.\n\n        Args:\n            value: The value associated with this node.\n        \"\"\"\n        self._children = {}\n        self._value = value\n\n    def _get_or_add_child(self, char):\n        child = self._children.get(char)\n        if child is None:\n            child = Node(None)\n            self._children[char] = child\n        return child\n\n    def transit(self, key):\n        \"\"\"Transit the state of a Deterministic Finite Automata (DFA) with key.\n\n        Args:\n            key: A sequence of criterion (tokens or characters) used to transit to a new state.\n\n        Returns:\n            A new state if the transition succeeded, otherwise ``None``.\n\n        \"\"\"\n        state = self\n        for char in key:\n            state = state._children.get(char)\n            if state is None:\n                break\n        return state\n\n    def _walk(self, prefix: Union[str, tuple], ordered=False):\n        for char, child in sorted(self._children.items()) if ordered else self._children.items():\n            prefix_new = prefix + (char if isinstance(prefix, str) else (char,))\n            if child._value:\n                yield prefix_new, child._value\n            yield from child._walk(prefix_new)\n\n\nclass Trie(Node):\n    def __init__(self, tokens: Optional[Union[Dict[str, Any], Iterable[str]]] = None) -> None:\n        \"\"\"A referential implementation of the trie (:cite:`10.1145/1457838.1457895`) structure. It stores a dict by\n        assigning each key/value pair a :class:`~hanlp_trie.trie.Node` in a trie tree. It provides get/set/del/items\n        methods just like a :class:`dict` does. Additionally, it also provides longest-prefix-matching and keywords\n        lookup against a piece of text, which are very helpful in rule-based Natural Language Processing.\n\n        Args:\n            tokens: A set of keys or a dict mapping.\n        \"\"\"\n        super().__init__()\n        self._size = 0\n        if tokens:\n            if isinstance(tokens, dict):\n                for k, v in tokens.items():\n                    self[k] = v\n            else:\n                for k in tokens:\n                    self[k] = True\n\n    def __contains__(self, key):\n        return self[key] is not None\n\n    def __getitem__(self, key):\n        state = self.transit(key)\n        if state is None:\n            return None\n        return state._value\n\n    def __setitem__(self, key, value):\n        state = self\n        for char in key[:-1]:\n            state = state._get_or_add_child(char)\n\n        leaf = state._get_or_add_child(key[-1])\n        if leaf._value is None:\n            self._size += 1\n        leaf._value = value\n\n    def __delitem__(self, key):\n        state = self.transit(key)\n        if state is not None:\n            state._value = None\n            self._size -= 1\n\n    def update(self, dic: Dict[str, Any]):\n        for k, v in dic.items():\n            self[k] = v\n        return self\n\n    def parse(self, text: Sequence[str]) -> List[Tuple[int, int, Any]]:\n        \"\"\"Keywords lookup which takes a piece of text as input, and lookup all occurrences of keywords in it. These\n        occurrences can overlap with each other.\n\n        Args:\n            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.\n                The trie will transit on either types properly, which means a list of str simply defines a list of\n                transition criteria while a str defines each criterion as a character.\n\n        Returns:\n            A tuple of ``(begin, end, value)``.\n        \"\"\"\n        found = []\n        for i in range(len(text)):\n            state = self\n            for j in range(i, len(text)):\n                state = state.transit(text[j])\n                if state:\n                    if state._value is not None:\n                        found.append((i, j + 1, state._value))\n                else:\n                    break\n        return found\n\n    def parse_longest(self, text: Sequence[str]) -> List[Tuple[int, int, Any]]:\n        \"\"\"Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till\n        its tail. By definition, the matches won't overlap with each other.\n\n        Args:\n            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.\n                The trie will transit on either types properly, which means a list of str simply defines a list of\n                transition criteria while a str defines each criterion as a character.\n\n        Returns:\n            A tuple of ``(begin, end, value)``.\n\n        \"\"\"\n        found = []\n        i = 0\n        while i < len(text):\n            state = self.transit(text[i])\n            if state:\n                to = i + 1\n                end = to\n                value = state._value\n                for to in range(i + 1, len(text)):\n                    state = state.transit(text[to])\n                    if not state:\n                        break\n                    if state._value is not None:\n                        value = state._value\n                        end = to + 1\n                if value is not None:\n                    found.append((i, end, value))\n                    i = end - 1\n            i += 1\n        return found\n\n    def items(self, ordered=False, prefix=''):\n        yield from self._walk(prefix, ordered)\n\n    def __len__(self):\n        return self._size\n\n    def __bool__(self):\n        return bool(len(self))\n", "plugins/hanlp_trie/hanlp_trie/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-29 17:48\nfrom .trie import Trie\nfrom .dictionary import DictInterface, TrieDict\n", "plugins/hanlp_common/setup.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\nfrom os.path import abspath, join, dirname\nfrom setuptools import find_packages, setup\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\n\nsetup(\n    name='hanlp_common',\n    version='0.0.19',\n    description='HanLP: Han Language Processing',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/hankcs/HanLP',\n    author='hankcs',\n    author_email='hankcshe@gmail.com',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Developers',\n        \"Development Status :: 3 - Alpha\",\n        'Operating System :: OS Independent',\n        \"License :: OSI Approved :: Apache Software License\",\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        \"Topic :: Text Processing :: Linguistic\"\n    ],\n    keywords='corpus,machine-learning,NLU,NLP',\n    packages=find_packages(exclude=['docs', 'tests*']),\n    include_package_data=True,\n    install_requires=[\n        'phrasetree',\n    ],\n    extras_require={\n        # These AMR dependencies might not be necessary for most people.\n        'full': [\n            'networkx',\n            'penman==0.6.2',\n        ],\n    },\n    python_requires='>=3.6',\n)\n", "plugins/hanlp_common/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-16 22:20\n", "plugins/hanlp_common/hanlp_common/configurable.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-16 22:24\nfrom hanlp_common.reflection import str_to_type, classpath_of\n\n\nclass Configurable(object):\n    @staticmethod\n    def from_config(config: dict, **kwargs):\n        \"\"\"Build an object from config.\n\n        Args:\n          config: A ``dict`` holding parameters for its constructor. It has to contain a `classpath` key,\n                    which has a classpath str as its value. ``classpath`` will determine the type of object\n                    being deserialized.\n          kwargs: Arguments not used.\n\n        Returns: A deserialized object.\n\n        \"\"\"\n        cls = config.get('classpath', None)\n        assert cls, f'{config} doesn\\'t contain classpath field'\n        cls = str_to_type(cls)\n        deserialized_config = dict(config)\n        for k, v in config.items():\n            if isinstance(v, dict) and 'classpath' in v:\n                deserialized_config[k] = Configurable.from_config(v)\n        if cls.from_config == Configurable.from_config:\n            deserialized_config.pop('classpath')\n            return cls(**deserialized_config)\n        else:\n            return cls.from_config(deserialized_config)\n\n\nclass AutoConfigurable(Configurable):\n    @property\n    def config(self) -> dict:\n        \"\"\"\n        The config of this object, which are public properties. If any properties needs to be excluded from this config,\n        simply declare it with prefix ``_``.\n        \"\"\"\n        return dict([('classpath', classpath_of(self))] +\n                    [(k, v.config if hasattr(v, 'config') else v)\n                     for k, v in self.__dict__.items() if\n                     not k.startswith('_')])\n\n    def __repr__(self) -> str:\n        return repr(self.config)\n", "plugins/hanlp_common/hanlp_common/amr.py": "# MIT License\n#\n# Copyright (c) 2019 Sheng Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport json\nimport logging\nimport re\nimport traceback\nfrom collections import Counter, defaultdict\n\nfrom hanlp_common.io import eprint\n\ntry:\n    import networkx as nx\n    import penman\n    from penman import Triple\nexcept ModuleNotFoundError:\n    traceback.print_exc()\n    eprint('AMR support requires the full version which can be installed via:\\n'\n           'pip install hanlp_common[full]')\n    exit(1)\n\nDEFAULT_PADDING_TOKEN = \"@@PADDING@@\"\nDEFAULT_OOV_TOKEN = \"@@UNKNOWN@@\"\nlogger = logging.getLogger('amr')\n\n# Disable inverting ':mod' relation.\npenman.AMRCodec._inversions.pop('domain')\npenman.AMRCodec._deinversions.pop('mod')\n\namr_codec = penman.AMRCodec(indent=6)\n\nWORDSENSE_RE = re.compile(r'-\\d\\d$')\nQUOTED_RE = re.compile(r'^\".*\"$')\n\n\ndef is_abstract_token(token):\n    return re.search(r'^([A-Z]+_)+\\d+$', token) or re.search(r'^\\d0*$', token)\n\n\ndef is_english_punct(c):\n    return re.search(r'^[,.?!:;\"\\'-(){}\\[\\]]$', c)\n\n\ndef find_similar_token(token, tokens):\n    token = re.sub(r'-\\d\\d$', '', token)  # .lower())\n    for i, t in enumerate(tokens):\n        if token == t:\n            return tokens[i]\n        # t = t.lower()\n        # if (token == t or\n        #     (t.startswith(token) and len(token) > 3) or\n        #     token + 'd' == t or\n        #     token + 'ed' == t or\n        #     re.sub('ly$', 'le', t) == token or\n        #     re.sub('tive$', 'te', t) == token or\n        #     re.sub('tion$', 'te', t) == token or\n        #     re.sub('ied$', 'y', t) == token or\n        #     re.sub('ly$', '', t) == token\n        # ):\n        #     return tokens[i]\n    return None\n\n\nclass AMR:\n\n    def __init__(self,\n                 id=None,\n                 sentence=None,\n                 graph=None,\n                 tokens=None,\n                 lemmas=None,\n                 pos_tags=None,\n                 ner_tags=None,\n                 abstract_map=None,\n                 misc=None):\n        self.id = id\n        self.sentence = sentence\n        self.graph = graph\n        self.tokens = tokens\n        self.lemmas = lemmas\n        self.pos_tags = pos_tags\n        self.ner_tags = ner_tags\n        self.abstract_map = abstract_map\n        self.misc = misc\n\n    def is_named_entity(self, index):\n        return self.ner_tags[index] not in ('0', 'O')\n\n    def get_named_entity_span(self, index):\n        if self.ner_tags is None or not self.is_named_entity(index):\n            return []\n        span = [index]\n        tag = self.ner_tags[index]\n        prev = index - 1\n        while prev > 0 and self.ner_tags[prev] == tag:\n            span.append(prev)\n            prev -= 1\n        next = index + 1\n        while next < len(self.ner_tags) and self.ner_tags[next] == tag:\n            span.append(next)\n            next += 1\n        return span\n\n    def find_span_indexes(self, span):\n        for i, token in enumerate(self.tokens):\n            if token == span[0]:\n                _span = self.tokens[i: i + len(span)]\n                if len(_span) == len(span) and all(x == y for x, y in zip(span, _span)):\n                    return list(range(i, i + len(span)))\n        return None\n\n    def replace_span(self, indexes, new, pos=None, ner=None):\n        self.tokens = self.tokens[:indexes[0]] + new + self.tokens[indexes[-1] + 1:]\n        self.lemmas = self.lemmas[:indexes[0]] + new + self.lemmas[indexes[-1] + 1:]\n        if pos is None:\n            pos = [self.pos_tags[indexes[0]]]\n        self.pos_tags = self.pos_tags[:indexes[0]] + pos + self.pos_tags[indexes[-1] + 1:]\n        if ner is None:\n            ner = [self.ner_tags[indexes[0]]]\n        self.ner_tags = self.ner_tags[:indexes[0]] + ner + self.ner_tags[indexes[-1] + 1:]\n\n    def remove_span(self, indexes):\n        self.replace_span(indexes, [], [], [])\n\n    def __repr__(self):\n        fields = []\n        for k, v in dict(\n                id=self.id,\n                snt=self.sentence,\n                tokens=self.tokens,\n                lemmas=self.lemmas,\n                pos_tags=self.pos_tags,\n                ner_tags=self.ner_tags,\n                abstract_map=self.abstract_map,\n                misc=self.misc,\n                graph=self.graph\n        ).items():\n            if v is None:\n                continue\n            if k == 'misc':\n                fields += v\n            elif k == 'graph':\n                fields.append(str(v))\n            else:\n                if not isinstance(v, str):\n                    v = json.dumps(v)\n                fields.append('# ::{} {}'.format(k, v))\n        return '\\n'.join(fields)\n\n    def get_src_tokens(self):\n        return self.lemmas if self.lemmas else self.sentence.split()\n\n\nclass AMRNode:\n    attribute_priority = [\n        'instance', 'quant', 'mode', 'value', 'name', 'li', 'mod', 'frequency',\n        'month', 'day', 'year', 'time', 'unit', 'decade', 'poss'\n    ]\n\n    def __init__(self, identifier, attributes=None, copy_of=None):\n        self.identifier = identifier\n        if attributes is None:\n            self.attributes = []\n        else:\n            self.attributes = attributes\n            # self._sort_attributes()\n        self._num_copies = 0\n        self.copy_of = copy_of\n\n    def _sort_attributes(self):\n        def get_attr_priority(attr):\n            if attr in self.attribute_priority:\n                return self.attribute_priority.index(attr), attr\n            if not re.search(r'^(ARG|op|snt)', attr):\n                return len(self.attribute_priority), attr\n            else:\n                return len(self.attribute_priority) + 1, attr\n\n        self.attributes.sort(key=lambda x: get_attr_priority(x[0]))\n\n    def __hash__(self):\n        return hash(self.identifier)\n\n    def __eq__(self, other):\n        if not isinstance(other, AMRNode):\n            return False\n        return self.identifier == other.identifier\n\n    def __repr__(self):\n        ret = str(self.identifier)\n        for k, v in self.attributes:\n            if k == 'instance':\n                ret += ' / ' + v\n                break\n        return ret\n\n    def __str__(self):\n        ret = repr(self)\n        for key, value in self.attributes:\n            if key == 'instance':\n                continue\n            ret += '\\n\\t:{} {}'.format(key, value)\n        return ret\n\n    @property\n    def instance(self):\n        for key, value in self.attributes:\n            if key == 'instance':\n                return value\n        else:\n            return None\n\n    @property\n    def ops(self):\n        ops = []\n        for key, value in self.attributes:\n            if re.search(r'op\\d+', key):\n                ops.append((int(key[2:]), value))\n        if len(ops):\n            ops.sort(key=lambda x: x[0])\n        return [v for k, v in ops]\n\n    def copy(self):\n        attributes = None\n        if self.attributes is not None:\n            attributes = self.attributes[:]\n        self._num_copies += 1\n        copy = AMRNode(self.identifier + '_copy_{}'.format(self._num_copies), attributes, self)\n        return copy\n\n    def remove_attribute(self, attr, value):\n        self.attributes.remove((attr, value))\n\n    def add_attribute(self, attr, value):\n        self.attributes.append((attr, value))\n\n    def replace_attribute(self, attr, old, new):\n        index = self.attributes.index((attr, old))\n        self.attributes[index] = (attr, new)\n\n    def get_frame_attributes(self):\n        for k, v in self.attributes:\n            if isinstance(v, str) and re.search(r'-\\d\\d$', v):\n                yield k, v\n\n    def get_senseless_attributes(self):\n        for k, v in self.attributes:\n            if isinstance(v, str) and not re.search(r'-\\d\\d$', v):\n                yield k, v\n\n\nclass AMRGraph(penman.Graph):\n    edge_label_priority = (\n        'mod name time location degree poss domain quant manner unit purpose topic condition part-of compared-to '\n        'duration source ord beneficiary concession direction frequency consist-of example medium location-of '\n        'manner-of quant-of time-of instrument prep-in destination accompanier prep-with extent instrument-of age '\n        'path concession-of subevent-of prep-as prep-to prep-against prep-on prep-for degree-of prep-under part '\n        'condition-of prep-without topic-of season duration-of poss-of prep-from prep-at range purpose-of source-of '\n        'subevent example-of value path-of scale conj-as-if prep-into prep-by prep-on-behalf-of medium-of prep-among '\n        'calendar beneficiary-of prep-along-with extent-of age-of frequency-of dayperiod accompanier-of '\n        'destination-of prep-amid prep-toward prep-in-addition-to ord-of name-of weekday direction-of prep-out-of '\n        'timezone subset-of'.split())\n\n    def __init__(self, penman_graph):\n        super(AMRGraph, self).__init__()\n        self._triples = penman_graph._triples\n        self._top = penman_graph._top\n        self._build_extras()\n        self._src_tokens = []\n\n    def __str__(self):\n        self._triples = penman.alphanum_order(self._triples)\n        return amr_codec.encode(self)\n\n    def _build_extras(self):\n        G = nx.DiGraph()\n\n        self.variable_to_node = {}\n        for v in self.variables():\n            if type(v) is not str:\n                continue\n            attributes = [(t.relation, t.target) for t in self.attributes(source=v)]\n            node = AMRNode(v, attributes)\n            G.add_node(node)\n            self.variable_to_node[v] = node\n\n        edge_set = set()\n        for edge in self.edges():\n            if type(edge.source) is not str:\n                continue\n            source = self.variable_to_node[edge.source]\n            target = self.variable_to_node[edge.target]\n            relation = edge.relation\n\n            if relation == 'instance':\n                continue\n\n            if source == target:\n                continue\n\n            if edge.inverted:\n                source, target, relation = target, source, amr_codec.invert_relation(edge.relation)\n\n            if (source, target) in edge_set:\n                target = target.copy()\n\n            edge_set.add((source, target))\n            G.add_edge(source, target, label=relation)\n\n        self._G = G\n\n    def attributes(self, source=None, relation=None, target=None):\n        # Refine attributes because there's a bug in penman.attributes()\n        # See https://github.com/goodmami/penman/issues/29\n        attrmatch = lambda a: (\n                (source is None or source == a.source) and\n                (relation is None or relation == a.relation) and\n                (target is None or target == a.target)\n        )\n        variables = self.variables()\n        attrs = [t for t in self.triples() if t.target not in variables or t.relation == 'instance']\n        return list(filter(attrmatch, attrs))\n\n    def _update_penman_graph(self, triples):\n        self._triples = triples\n        if self._top not in self.variables():\n            self._top = None\n\n    def is_name_node(self, node):\n        edges = list(self._G.in_edges(node))\n        return any(self._G[source][target].get('label', None) == 'name' for source, target in edges)\n\n    def get_name_node_type(self, node):\n        edges = list(self._G.in_edges(node))\n        for source, target in edges:\n            if self._G[source][target].get('label', None) == 'name':\n                return source.instance\n        raise KeyError\n\n    def get_name_node_wiki(self, node):\n        edges = list(self._G.in_edges(node))\n        for source, target in edges:\n            if self._G[source][target].get('label', None) == 'name':\n                for attr, value in source.attributes:\n                    if attr == 'wiki':\n                        if value != '-':\n                            value = value[1:-1]  # remove quotes\n                        return value\n        return None\n\n    def set_name_node_wiki(self, node, wiki):\n        edges = list(self._G.in_edges(node))\n        parent = None\n        for source, target in edges:\n            if self._G[source][target].get('label', None) == 'name':\n                parent = source\n                break\n        if parent:\n            if wiki != '-':\n                wiki = '\"{}\"'.format(wiki)\n            self.add_node_attribute(parent, 'wiki', wiki)\n\n    def is_date_node(self, node):\n        return node.instance == 'date-entity'\n\n    def add_edge(self, source, target, label):\n        self._G.add_edge(source, target, label=label)\n        t = penman.Triple(source=source.identifier, relation=label, target=target.identifier)\n        triples = self._triples + [t]\n        triples = penman.alphanum_order(triples)\n        self._update_penman_graph(triples)\n\n    def remove_edge(self, x, y):\n        if isinstance(x, AMRNode) and isinstance(y, AMRNode):\n            self._G.remove_edge(x, y)\n        if isinstance(x, AMRNode):\n            x = x.identifier\n        if isinstance(y, AMRNode):\n            y = y.identifier\n        triples = [t for t in self._triples if not (t.source == x and t.target == y)]\n        self._update_penman_graph(triples)\n\n    def update_edge_label(self, x, y, old, new):\n        self._G[x][y]['label'] = new\n        triples = []\n        for t in self._triples:\n            if t.source == x.identifier and t.target == y.identifier and t.relation == old:\n                t = Triple(x.identifier, new, y.identifier)\n            triples.append(t)\n        self._update_penman_graph(triples)\n\n    def add_node(self, instance):\n        identifier = instance[0]\n        assert identifier.isalpha()\n        if identifier in self.variables():\n            i = 2\n            while identifier + str(i) in self.variables():\n                i += 1\n            identifier += str(i)\n        triples = self._triples + [Triple(identifier, 'instance', instance)]\n        self._triples = penman.alphanum_order(triples)\n\n        node = AMRNode(identifier, [('instance', instance)])\n        self._G.add_node(node)\n        return node\n\n    def remove_node(self, node):\n        self._G.remove_node(node)\n        triples = [t for t in self._triples if t.source != node.identifier]\n        self._update_penman_graph(triples)\n\n    def replace_node_attribute(self, node, attr, old, new):\n        node.replace_attribute(attr, old, new)\n        triples = []\n        found = False\n        for t in self._triples:\n            if t.source == node.identifier and t.relation == attr and t.target == old:\n                found = True\n                t = penman.Triple(source=node.identifier, relation=attr, target=new)\n            triples.append(t)\n        if not found:\n            raise KeyError\n        self._triples = penman.alphanum_order(triples)\n\n    def remove_node_attribute(self, node, attr, value):\n        node.remove_attribute(attr, value)\n        triples = [t for t in self._triples if\n                   not (t.source == node.identifier and t.relation == attr and t.target == value)]\n        self._update_penman_graph(triples)\n\n    def add_node_attribute(self, node, attr, value):\n        node.add_attribute(attr, value)\n        t = penman.Triple(source=node.identifier, relation=attr, target=value)\n        self._triples = penman.alphanum_order(self._triples + [t])\n\n    def remove_node_ops(self, node):\n        ops = []\n        for attr, value in node.attributes:\n            if re.search(r'^op\\d+$', attr):\n                ops.append((attr, value))\n        for attr, value in ops:\n            self.remove_node_attribute(node, attr, value)\n\n    def remove_subtree(self, root):\n        children = []\n        removed_nodes = set()\n        for _, child in list(self._G.edges(root)):\n            self.remove_edge(root, child)\n            children.append(child)\n        for child in children:\n            if len(list(self._G.in_edges(child))) == 0:\n                removed_nodes.update(self.remove_subtree(child))\n        if len(list(self._G.in_edges(root))) == 0:\n            self.remove_node(root)\n            removed_nodes.add(root)\n        return removed_nodes\n\n    def get_subtree(self, root, max_depth):\n        if max_depth == 0:\n            return []\n        nodes = [root]\n        children = [child for _, child in self._G.edges(root)]\n        nodes += children\n        for child in children:\n            if len(list(self._G.in_edges(child))) == 1:\n                nodes = nodes + self.get_subtree(child, max_depth - 1)\n        return nodes\n\n    def get_nodes(self):\n        return self._G.nodes\n\n    def get_edges(self):\n        return self._G.edges\n\n    def set_src_tokens(self, sentence):\n        if type(sentence) is not list:\n            sentence = sentence.split(\" \")\n        self._src_tokens = sentence\n\n    def get_src_tokens(self):\n        return self._src_tokens\n\n    def get_list_node(self, replace_copy=True):\n        visited = defaultdict(int)\n        node_list = []\n\n        def dfs(node, relation, parent):\n\n            node_list.append((\n                node if node.copy_of is None or not replace_copy else node.copy_of,\n                relation,\n                parent if parent.copy_of is None or not replace_copy else parent.copy_of))\n\n            if len(self._G[node]) > 0 and visited[node] == 0:\n                visited[node] = 1\n                for child_node, child_relation in self.sort_edges(self._G[node].items()):\n                    dfs(child_node, child_relation[\"label\"], node)\n\n        dfs(\n            self.variable_to_node[self._top],\n            'root',\n            self.variable_to_node[self._top]\n        )\n\n        return node_list\n\n    def sort_edges(self, edges):\n        return edges\n\n    def get_tgt_tokens(self):\n        node_list = self.get_list_node()\n\n        tgt_token = []\n        visited = defaultdict(int)\n\n        for node, relation, parent_node in node_list:\n            instance = [attr[1] for attr in node.attributes if attr[0] == \"instance\"]\n            assert len(instance) == 1\n            tgt_token.append(str(instance[0]))\n\n            if len(node.attributes) > 1 and visited[node] == 0:\n                for attr in node.attributes:\n                    if attr[0] != \"instance\":\n                        tgt_token.append(str(attr[1]))\n\n            visited[node] = 1\n\n        return tgt_token\n\n    def get_list_data(self, amr, bos=None, eos=None, bert_tokenizer=None, max_tgt_length=None):\n        node_list = self.get_list_node()\n\n        tgt_tokens = []\n        head_tags = []\n        head_indices = []\n\n        node_to_idx = defaultdict(list)\n        visited = defaultdict(int)\n\n        def update_info(node, relation, parent, token):\n            head_indices.append(1 + node_to_idx[parent][-1])\n            head_tags.append(relation)\n            tgt_tokens.append(str(token))\n\n        for node, relation, parent_node in node_list:\n\n            node_to_idx[node].append(len(tgt_tokens))\n\n            instance = [attr[1] for attr in node.attributes if attr[0] == \"instance\"]\n            assert len(instance) == 1\n            instance = instance[0]\n\n            update_info(node, relation, parent_node, instance)\n\n            if len(node.attributes) > 1 and visited[node] == 0:\n                for attr in node.attributes:\n                    if attr[0] != \"instance\":\n                        update_info(node, attr[0], node, attr[1])\n\n            visited[node] = 1\n\n        def trim_very_long_tgt_tokens(tgt_tokens, head_tags, head_indices, node_to_idx):\n            tgt_tokens = tgt_tokens[:max_tgt_length]\n            head_tags = head_tags[:max_tgt_length]\n            head_indices = head_indices[:max_tgt_length]\n            for node, indices in node_to_idx.items():\n                invalid_indices = [index for index in indices if index >= max_tgt_length]\n                for index in invalid_indices:\n                    indices.remove(index)\n            return tgt_tokens, head_tags, head_indices, node_to_idx\n\n        if max_tgt_length is not None:\n            tgt_tokens, head_tags, head_indices, node_to_idx = trim_very_long_tgt_tokens(\n                tgt_tokens, head_tags, head_indices, node_to_idx)\n\n        copy_offset = 0\n        if bos:\n            tgt_tokens = [bos] + tgt_tokens\n            copy_offset += 1\n        if eos:\n            tgt_tokens = tgt_tokens + [eos]\n\n        head_indices[node_to_idx[self.variable_to_node[self.top]][0]] = 0\n\n        # Target side Coreference\n        tgt_copy_indices = [i for i in range(len(tgt_tokens))]\n\n        for node, indices in node_to_idx.items():\n            if len(indices) > 1:\n                copy_idx = indices[0] + copy_offset\n                for token_idx in indices[1:]:\n                    tgt_copy_indices[token_idx + copy_offset] = copy_idx\n\n        tgt_copy_map = [(token_idx, copy_idx) for token_idx, copy_idx in enumerate(tgt_copy_indices)]\n\n        for i, copy_index in enumerate(tgt_copy_indices):\n            # Set the coreferred target to 0 if no coref is available.\n            if i == copy_index:\n                tgt_copy_indices[i] = 0\n\n        tgt_token_counter = Counter(tgt_tokens)\n        tgt_copy_mask = [0] * len(tgt_tokens)\n        for i, token in enumerate(tgt_tokens):\n            if tgt_token_counter[token] > 1:\n                tgt_copy_mask[i] = 1\n\n        def add_source_side_tags_to_target_side(_src_tokens, _src_tags):\n            assert len(_src_tags) == len(_src_tokens)\n            tag_counter = defaultdict(lambda: defaultdict(int))\n            for src_token, src_tag in zip(_src_tokens, _src_tags):\n                tag_counter[src_token][src_tag] += 1\n\n            tag_lut = {DEFAULT_OOV_TOKEN: DEFAULT_OOV_TOKEN,\n                       DEFAULT_PADDING_TOKEN: DEFAULT_OOV_TOKEN}\n            for src_token in set(_src_tokens):\n                tag = max(tag_counter[src_token].keys(), key=lambda x: tag_counter[src_token][x])\n                tag_lut[src_token] = tag\n\n            tgt_tags = []\n            for tgt_token in tgt_tokens:\n                sim_token = find_similar_token(tgt_token, _src_tokens)\n                if sim_token is not None:\n                    index = _src_tokens.index(sim_token)\n                    tag = _src_tags[index]\n                else:\n                    tag = DEFAULT_OOV_TOKEN\n                tgt_tags.append(tag)\n\n            return tgt_tags, tag_lut\n\n        # Source Copy\n        src_tokens = self.get_src_tokens()\n        src_token_ids = None\n        src_token_subword_index = None\n        src_pos_tags = amr.pos_tags\n        src_copy_vocab = SourceCopyVocabulary(src_tokens)\n        src_copy_indices = src_copy_vocab.index_sequence(tgt_tokens)\n        src_copy_map = src_copy_vocab.get_copy_map(src_tokens)\n        tgt_pos_tags, pos_tag_lut = add_source_side_tags_to_target_side(src_tokens, src_pos_tags)\n\n        if bert_tokenizer is not None:\n            src_token_ids, src_token_subword_index = bert_tokenizer.tokenize(src_tokens, True)\n\n        src_must_copy_tags = [1 if is_abstract_token(t) else 0 for t in src_tokens]\n        src_copy_invalid_ids = set(src_copy_vocab.index_sequence(\n            [t for t in src_tokens if is_english_punct(t)]))\n\n        return {\n            \"tgt_tokens\": tgt_tokens,\n            \"tgt_pos_tags\": tgt_pos_tags,\n            \"tgt_copy_indices\": tgt_copy_indices,\n            \"tgt_copy_map\": tgt_copy_map,\n            \"tgt_copy_mask\": tgt_copy_mask,\n            \"src_tokens\": src_tokens,\n            \"src_token_ids\": src_token_ids,\n            \"src_token_subword_index\": src_token_subword_index,\n            \"src_must_copy_tags\": src_must_copy_tags,\n            \"src_pos_tags\": src_pos_tags,\n            \"src_copy_vocab\": src_copy_vocab,\n            \"src_copy_indices\": src_copy_indices,\n            \"src_copy_map\": src_copy_map,\n            \"pos_tag_lut\": pos_tag_lut,\n            \"head_tags\": head_tags,\n            \"head_indices\": head_indices,\n            \"src_copy_invalid_ids\": src_copy_invalid_ids\n        }\n\n    @classmethod\n    def decode(cls, raw_graph_string):\n        _graph = amr_codec.decode(raw_graph_string)\n        return cls(_graph)\n\n    @classmethod\n    def from_lists(cls, all_list):\n        head_tags = all_list['head_tags']\n        head_indices = all_list['head_indices']\n        tgt_tokens = all_list['tokens']\n\n        tgt_copy_indices = all_list['coref']\n        variables = []\n        variables_count = defaultdict(int)\n        for i, token in enumerate(tgt_tokens):\n            if tgt_copy_indices[i] != i:\n                variables.append(variables[tgt_copy_indices[i]])\n            else:\n                if token[0] in variables_count:\n                    variables.append(token[0] + str(variables_count[token[0]]))\n                else:\n                    variables.append(token[0])\n\n                variables_count[token[0]] += 1\n\n        Triples = []\n        for variable, token in zip(variables, tgt_tokens):\n            Triples.append(Triple(variable, \"instance\", token))\n            Triples.append(\n                Triple(\n                    head_indices[variable],\n                    head_tags[variable],\n                    variable\n                )\n            )\n\n    @classmethod\n    def from_prediction(cls, prediction):\n\n        def is_attribute_value(value):\n            return re.search(r'(^\".*\"$|^[^a-zA-Z]+$)', value) is not None\n\n        def is_attribute_edge(label):\n            return label in ('instance', 'mode', 'li', 'value', 'month', 'year', 'day', 'decade', 'ARG6')\n\n        def normalize_number(text):\n            if re.search(r'^\\d+,\\d+$', text):\n                text = text.replace(',', '')\n            return text\n\n        def abstract_node(value):\n            return re.search(r'^([A-Z]+|DATE_ATTRS|SCORE_ENTITY|ORDINAL_ENTITY)_\\d+$', value)\n\n        def abstract_attribute(value):\n            return re.search(r'^_QUANTITY_\\d+$', value)\n\n        def correct_multiroot(heads):\n            for i in range(1, len(heads)):\n                if heads[i] == 0:\n                    heads[i] = 1\n            return heads\n\n        nodes = [normalize_number(n) for n in prediction['nodes']]\n        heads = correct_multiroot(prediction['heads'])\n        corefs = [int(x) for x in prediction['corefs']]\n        head_labels = prediction['head_labels']\n\n        triples = []\n        top = None\n        # Build the variable map from variable to instance.\n        variable_map = {}\n        for coref_index in corefs:\n            node = nodes[coref_index - 1]\n            head_label = head_labels[coref_index - 1]\n            if (re.search(r'[/:\\\\()]', node) or is_attribute_value(node) or\n                    is_attribute_edge(head_label) or abstract_attribute(node)):\n                continue\n            variable_map['vv{}'.format(coref_index)] = node\n        for head_index in heads:\n            if head_index == 0:\n                continue\n            node = nodes[head_index - 1]\n            coref_index = corefs[head_index - 1]\n            variable_map['vv{}'.format(coref_index)] = node\n        # Build edge triples and other attribute triples.\n        for i, head_index in enumerate(heads):\n            if head_index == 0:\n                top_variable = 'vv{}'.format(corefs[i])\n                if top_variable not in variable_map:\n                    variable_map[top_variable] = nodes[i]\n                top = top_variable\n                continue\n            head_variable = 'vv{}'.format(corefs[head_index - 1])\n            modifier = nodes[i]\n            modifier_variable = 'vv{}'.format(corefs[i])\n            label = head_labels[i]\n            assert head_variable in variable_map\n            if modifier_variable in variable_map:\n                triples.append((head_variable, label, modifier_variable))\n            else:\n                # Add quotes if there's a backslash.\n                if re.search(r'[/:\\\\()]', modifier) and not re.search(r'^\".*\"$', modifier):\n                    modifier = '\"{}\"'.format(modifier)\n                triples.append((head_variable, label, modifier))\n\n        for var, node in variable_map.items():\n            if re.search(r'^\".*\"$', node):\n                node = node[1:-1]\n            if re.search(r'[/:\\\\()]', node):\n                parts = re.split(r'[/:\\\\()]', node)\n                for part in parts[::-1]:\n                    if len(part):\n                        node = part\n                        break\n                else:\n                    node = re.sub(r'[/:\\\\()]', '_', node)\n            triples.append((var, 'instance', node))\n\n        if len(triples) == 0:\n            triples.append(('vv1', 'instance', 'string-entity'))\n            top = 'vv1'\n        triples.sort(key=lambda x: int(x[0].replace('vv', '')))\n        graph = penman.Graph()\n        graph._top = top\n        graph._triples = [penman.Triple(*t) for t in triples]\n        graph = cls(graph)\n        try:\n            GraphRepair.do(graph, nodes)\n            amr_codec.encode(graph)\n        except Exception as e:\n            graph._top = top\n            graph._triples = [penman.Triple(*t) for t in triples]\n            graph = cls(graph)\n        return graph\n\n\nclass SourceCopyVocabulary:\n    def __init__(self, sentence, pad_token=DEFAULT_PADDING_TOKEN, unk_token=DEFAULT_OOV_TOKEN):\n        if type(sentence) is not list:\n            sentence = sentence.split(\" \")\n\n        self.src_tokens = sentence\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n\n        self.token_to_idx = {self.pad_token: 0, self.unk_token: 1}\n        self.idx_to_token = {0: self.pad_token, 1: self.unk_token}\n\n        self.vocab_size = 2\n\n        for token in sentence:\n            if token not in self.token_to_idx:\n                self.token_to_idx[token] = self.vocab_size\n                self.idx_to_token[self.vocab_size] = token\n                self.vocab_size += 1\n\n    def get_token_from_idx(self, idx):\n        return self.idx_to_token[idx]\n\n    def get_token_idx(self, token):\n        return self.token_to_idx.get(token, self.token_to_idx[self.unk_token])\n\n    def index_sequence(self, list_tokens):\n        return [self.get_token_idx(token) for token in list_tokens]\n\n    def get_copy_map(self, list_tokens):\n        src_indices = [self.get_token_idx(self.unk_token)] + self.index_sequence(list_tokens)\n        return [\n            (src_idx, src_token_idx) for src_idx, src_token_idx in enumerate(src_indices)\n        ]\n\n    def get_special_tok_list(self):\n        return [self.pad_token, self.unk_token]\n\n    def __repr__(self):\n        return json.dumps(self.idx_to_token)\n\n\ndef is_similar(instances1, instances2):\n    if len(instances1) < len(instances2):\n        small = instances1\n        large = instances2\n    else:\n        small = instances2\n        large = instances1\n    coverage1 = sum(1 for x in small if x in large) / len(small)\n    coverage2 = sum(1 for x in large if x in small) / len(large)\n    return coverage1 > .8 and coverage2 > .8\n\n\nclass GraphRepair:\n\n    def __init__(self, graph, nodes):\n        self.graph = graph\n        self.nodes = nodes\n        self.repaired_items = set()\n\n    @staticmethod\n    def do(graph, nodes):\n        gr = GraphRepair(graph, nodes)\n        gr.remove_redundant_edges()\n        gr.remove_unknown_nodes()\n\n    def remove_unknown_nodes(self):\n        graph = self.graph\n        nodes = [node for node in graph.get_nodes()]\n        for node in nodes:\n            for attr, value in node.attributes:\n                if value == '@@UNKNOWN@@' and attr != 'instance':\n                    graph.remove_node_attribute(node, attr, value)\n            if node.instance == '@@UNKNOWN@@':\n                if len(list(graph._G.edges(node))) == 0:\n                    for source, target in list(graph._G.in_edges(node)):\n                        graph.remove_edge(source, target)\n                    graph.remove_node(node)\n                    self.repaired_items.add('remove-unknown-node')\n\n    def remove_redundant_edges(self):\n        \"\"\"\n        Edge labels such as ARGx, ARGx-of, and 'opx' should only appear at most once\n        in each node's outgoing edges.\n        \"\"\"\n        graph = self.graph\n        nodes = [node for node in graph.get_nodes()]\n        removed_nodes = set()\n        for node in nodes:\n            if node in removed_nodes:\n                continue\n            edges = list(graph._G.edges(node))\n            edge_counter = defaultdict(list)\n            for source, target in edges:\n                label = graph._G[source][target]['label']\n                # `name`, `ARGx`, and `ARGx-of` should only appear once.\n                if label == 'name':  # or label.startswith('ARG'):\n                    edge_counter[label].append(target)\n                # the target of `opx' should only appear once.\n                elif label.startswith('op') or label.startswith('snt'):\n                    edge_counter[str(target.instance)].append(target)\n                else:\n                    edge_counter[label + str(target.instance)].append(target)\n            for label, children in edge_counter.items():\n                if len(children) == 1:\n                    continue\n                if label == 'name':\n                    # remove redundant edges.\n                    for target in children[1:]:\n                        if len(list(graph._G.in_edges(target))) == 1 and len(list(graph._G.edges(target))) == 0:\n                            graph.remove_edge(node, target)\n                            graph.remove_node(target)\n                            removed_nodes.add(target)\n                            self.repaired_items.add('remove-redundant-edge')\n                    continue\n                visited_children = set()\n                groups = []\n                for i, target in enumerate(children):\n                    if target in visited_children:\n                        continue\n                    subtree_instances1 = [n.instance for n in graph.get_subtree(target, 5)]\n                    group = [(target, subtree_instances1)]\n                    visited_children.add(target)\n                    for _t in children[i + 1:]:\n                        if _t in visited_children or target.instance != _t.instance:\n                            continue\n                        subtree_instances2 = [n.instance for n in graph.get_subtree(_t, 5)]\n                        if is_similar(subtree_instances1, subtree_instances2):\n                            group.append((_t, subtree_instances2))\n                            visited_children.add(_t)\n                    groups.append(group)\n                for group in groups:\n                    if len(group) == 1:\n                        continue\n                    kept_target, _ = max(group, key=lambda x: len(x[1]))\n                    for target, _ in group:\n                        if target == kept_target:\n                            continue\n                        graph.remove_edge(node, target)\n                        removed_nodes.update(graph.remove_subtree(target))\n", "plugins/hanlp_common/hanlp_common/conll.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-19 20:50\nfrom typing import Union, List\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp_common.visualization import pretty_tree_horizontal, make_table, markdown_table\n\n\nclass CoNLLWord(SerializableDict):\n    def __init__(self, id, form, lemma=None, cpos=None, pos=None, feats=None, head=None, deprel=None, phead=None,\n                 pdeprel=None):\n        \"\"\"CoNLL (:cite:`buchholz-marsi-2006-conll`) format template, see http://anthology.aclweb.org/W/W06/W06-2920.pdf\n\n        Args:\n            id (int):\n                Token counter, starting at 1 for each new sentence.\n            form (str):\n                Word form or punctuation symbol.\n            lemma (str):\n                Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.\n            cpos (str):\n                Coarse-grained part-of-speech tag, where the tagset depends on the treebank.\n            pos (str):\n                Fine-grained part-of-speech tag, where the tagset depends on the treebank.\n            feats (str):\n                Unordered set of syntactic and/or morphological features (depending on the particular treebank),\n                or an underscore if not available.\n            head (Union[int, List[int]]):\n                Head of the current token, which is either a value of ID,\n                or zero (\u20190\u2019) if the token links to the virtual root node of the sentence.\n            deprel (Union[str, List[str]]):\n                Dependency relation to the HEAD.\n            phead (int):\n                Projective head of current token, which is either a value of ID or zero (\u20190\u2019),\n                or an underscore if not available.\n            pdeprel (str):\n                Dependency relation to the PHEAD, or an underscore if not available.\n        \"\"\"\n        self.id = sanitize_conll_int_value(id)\n        self.form = form\n        self.cpos = cpos\n        self.pos = pos\n        self.head = sanitize_conll_int_value(head)\n        self.deprel = deprel\n        self.lemma = lemma\n        self.feats = feats\n        self.phead = phead\n        self.pdeprel = pdeprel\n\n    def __str__(self):\n        if isinstance(self.head, list):\n            return '\\n'.join('\\t'.join(['_' if v is None else v for v in values]) for values in [\n                [str(self.id), self.form, self.lemma, self.cpos, self.pos, self.feats,\n                 None if head is None else str(head), deprel, self.phead, self.pdeprel] for head, deprel in\n                zip(self.head, self.deprel)\n            ])\n        values = [str(self.id), self.form, self.lemma, self.cpos, self.pos, self.feats,\n                  None if self.head is None else str(self.head), self.deprel, self.phead, self.pdeprel]\n        return '\\t'.join(['_' if v is None else v for v in values])\n\n    @property\n    def nonempty_fields(self):\n        \"\"\"\n        Get the values of nonempty fields as a list.\n        \"\"\"\n        return list(f for f in\n                    [self.form, self.lemma, self.cpos, self.pos, self.feats, self.head, self.deprel, self.phead,\n                     self.pdeprel] if f)\n\n    def get_pos(self):\n        \"\"\"\n        Get the precisest pos for this word.\n\n        Returns: ``self.pos`` or ``self.cpos``.\n\n        \"\"\"\n        return self.pos or self.cpos\n\n\nclass CoNLLUWord(SerializableDict):\n    def __init__(self, id: Union[int, str], form, lemma=None, upos=None, xpos=None, feats=None, head=None, deprel=None,\n                 deps=None,\n                 misc=None):\n        \"\"\"CoNLL-U format template, see https://universaldependencies.org/format.html\n\n        Args:\n\n            id (Union[int, str]):\n                Token counter, starting at 1 for each new sentence.\n            form (Union[str, None]):\n                Word form or punctuation symbol.\n            lemma (str):\n                Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.\n            upos (str):\n                Universal part-of-speech tag.\n            xpos (str):\n                Language-specific part-of-speech tag; underscore if not available.\n            feats (str):\n                List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n            head (int):\n                Head of the current token, which is either a value of ID,\n                or zero (\u20190\u2019) if the token links to the virtual root node of the sentence.\n            deprel (str):\n                Dependency relation to the HEAD.\n            deps (Union[List[Tuple[int, str], str]):\n                Projective head of current token, which is either a value of ID or zero (\u20190\u2019),\n                or an underscore if not available.\n            misc (str):\n                Dependency relation to the PHEAD, or an underscore if not available.\n        \"\"\"\n        self.id = sanitize_conll_int_value(id)\n        self.form = form\n        self.upos = upos\n        self.xpos = xpos\n        if isinstance(head, list):\n            assert deps is None, 'When head is a list, deps has to be None'\n            assert isinstance(deprel, list), 'When head is a list, deprel has to be a list'\n            assert len(deprel) == len(head), 'When head is a list, deprel has to match its length'\n            deps = list(zip(head, deprel))\n            head = None\n            deprel = None\n        self.head = sanitize_conll_int_value(head)\n        self.deprel = deprel\n        self.lemma = lemma\n        self.feats = feats\n        if deps == '_':\n            deps = None\n        if isinstance(deps, str):\n            self.deps = []\n            for pair in deps.split('|'):\n                h, r = pair.split(':')\n                h = int(h)\n                self.deps.append((h, r))\n        else:\n            self.deps = deps\n        self.misc = misc\n\n    def __str__(self):\n        deps = self.deps\n        if not deps:\n            deps = None\n        else:\n            deps = '|'.join(f'{h}:{r}' for h, r in deps)\n        values = [str(self.id), self.form, self.lemma, self.upos, self.xpos, self.feats,\n                  str(self.head) if self.head is not None else None, self.deprel, deps, self.misc]\n        return '\\t'.join(['_' if v is None else v for v in values])\n\n    @property\n    def nonempty_fields(self):\n        \"\"\"\n        Get the values of nonempty fields as a list.\n        \"\"\"\n        return list(f for f in\n                    [self.form, self.lemma, self.upos, self.xpos, self.feats, self.head, self.deprel, self.deps,\n                     self.misc] if f)\n\n    def get_pos(self):\n        \"\"\"\n        Get the precisest pos for this word.\n\n        Returns: ``self.xpos`` or ``self.upos``\n\n        \"\"\"\n        return self.xpos or self.upos\n\n\nclass CoNLLSentence(list):\n    def __init__(self, words=None):\n        \"\"\"\n        A list of :class:`~hanlp_common.conll.CoNLLWord` or :class:`~hanlp_common.conll.CoNLLUWord`. It is a sub-class\n        of :class:`list` and its words can be accessed in the same way as accessing list elements.\n\n        Args:\n            words (list[Union[CoNLLWord, CoNLLUWord]]): A list of words.\n        \"\"\"\n        super().__init__()\n        if words:\n            self.extend(words)\n\n    def __str__(self):\n        return '\\n'.join([word.__str__() for word in self])\n\n    @staticmethod\n    def from_str(conll: str, conllu=False):\n        \"\"\"Build a CoNLLSentence from CoNLL-X format str\n\n        Args:\n          conll (str): CoNLL-X or CoNLL-U format string\n          conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.\n\n        Returns:\n            A :class:`~hanlp_common.conll.CoNLLSentence`.\n        \"\"\"\n        words: List[CoNLLWord] = []\n        prev_id = None\n        for line in conll.strip().split('\\n'):\n            if line.startswith('#'):\n                continue\n            cells = line.split('\\t')\n            cells = [None if c == '_' else c for c in cells]\n            if '-' in cells[0]:\n                continue\n            cells[0] = int(cells[0])\n            cells[6] = int(cells[6])\n            if cells[0] != prev_id:\n                words.append(CoNLLUWord(*cells) if conllu else CoNLLWord(*cells))\n            else:\n                if isinstance(words[-1].head, list):\n                    words[-1].head.append(cells[6])\n                    words[-1].deprel.append(cells[7])\n                else:\n                    words[-1].head = [words[-1].head] + [cells[6]]\n                    words[-1].deprel = [words[-1].deprel] + [cells[7]]\n            prev_id = cells[0]\n        if conllu:\n            for word in words:  # type: CoNLLUWord\n                if isinstance(word.head, list):\n                    assert not word.deps\n                    word.deps = list(zip(word.head, word.deprel))\n                    word.head = None\n                    word.deprel = None\n        return CoNLLSentence(words)\n\n    @staticmethod\n    def from_file(path: str, conllu=False):\n        \"\"\"Build a CoNLLSentence from ``.conllx`` or ``.conllu`` file\n\n        Args:\n          path: Path to the file.\n          conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.\n\n        Returns:\n            A :class:`~hanlp_common.conll.CoNLLSentence`.\n        \"\"\"\n        with open(path) as src:\n            return [CoNLLSentence.from_str(x, conllu) for x in src.read().split('\\n\\n') if x.strip()]\n\n    @staticmethod\n    def from_dict(d: dict, conllu=False):\n        \"\"\"Build a CoNLLSentence from a dict.\n\n        Args:\n            d: A dict storing a list for each field, where each index corresponds to a token.\n            conllu: ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.\n\n        Returns:\n            A :class:`~hanlp_common.conll.CoNLLSentence`.\n        \"\"\"\n        if conllu:\n            headings = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']\n        else:\n            headings = ['ID', 'FORM', 'LEMMA', 'CPOS', 'POS', 'FEATS', 'HEAD', 'DEPREL', 'PHEAD', 'PDEPREL']\n        words: List[Union[CoNLLWord, CoNLLUWord]] = []\n        for cells in zip(*list(d[f] for f in headings)):\n            words.append(CoNLLUWord(*cells) if conllu else CoNLLWord(*cells))\n        return CoNLLSentence(words)\n\n    def to_markdown(self, headings: Union[str, List[str]] = 'auto') -> str:\n        r\"\"\"Convert into markdown string.\n\n        Args:\n            headings: ``auto`` to automatically detect the word type. When passed a list of string, they are treated as\n                        headings for each field.\n\n        Returns:\n            A markdown representation of this sentence.\n        \"\"\"\n        cells = [str(word).split('\\t') for word in self]\n        if headings == 'auto':\n            if isinstance(self[0], CoNLLWord):\n                headings = ['ID', 'FORM', 'LEMMA', 'CPOS', 'POS', 'FEATS', 'HEAD', 'DEPREL', 'PHEAD', 'PDEPREL']\n            else:  # conllu\n                headings = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']\n                for each in cells:\n                    # if '|' in each[8]:\n                    # each[8] = f'`{each[8]}`'\n                    each[8] = each[8].replace('|', '\u23ae')\n        alignment = [('^', '>'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '>'), ('^', '<'),\n                     ('^', '<'), ('^', '<')]\n        text = markdown_table(headings, cells, alignment=alignment)\n        return text\n\n    def to_tree(self, extras: List[str] = None) -> str:\n        \"\"\"Convert into a pretty tree string which can be printed to show the tree structure.\n\n        Args:\n            extras: Extra table to be aligned to this tree.\n\n        Returns:\n            A pretty tree string along with extra table if passed any.\n        \"\"\"\n        arrows = []\n        for word in self:  # type: Union[CoNLLWord, CoNLLUWord]\n            if word.head:\n                arrows.append({'from': word.head - 1, 'to': word.id - 1})\n        tree = pretty_tree_horizontal(arrows)\n        rows = [['Dep Tree', 'Token', 'Relation']]\n        has_lem = all(x.lemma for x in self)\n        has_pos = all(x.get_pos() for x in self)\n        if has_lem:\n            rows[0].append('Lemma')\n        if has_pos:\n            rows[0].append('PoS')\n        if extras:\n            rows[0].extend(extras[0])\n        for i, (word, arc) in enumerate(zip(self, tree)):\n            cell_per_word = [arc]\n            cell_per_word.append(word.form)\n            cell_per_word.append(word.deprel)\n            if has_lem:\n                cell_per_word.append(word.lemma)\n            if has_pos:\n                cell_per_word.append(word.get_pos())\n            if extras:\n                cell_per_word.extend(extras[i + 1])\n            rows.append(cell_per_word)\n        return make_table(rows, insert_header=True)\n\n    @property\n    def projective(self):\n        \"\"\"\n        ``True`` if this tree is projective.\n        \"\"\"\n        return isprojective([x.head for x in self])\n\n\nclass CoNLLSentenceList(list):\n\n    def __str__(self) -> str:\n        return '\\n\\n'.join(str(x) for x in self)\n\n\ndef sanitize_conll_int_value(value: Union[str, int]):\n    if value is None or isinstance(value, int):\n        return value\n    if value == '_':\n        return None\n    if isinstance(value, str):\n        return int(value)\n    return value\n\n\ndef isprojective(sequence):\n    r\"\"\"\n    Checks if a dependency tree is projective.\n    This also works for partial annotation.\n\n    Besides the obvious crossing arcs, the examples below illustrate two non-projective cases\n    which are hard to detect in the scenario of partial annotation.\n\n    Args:\n        sequence (list[int]):\n            A list of head indices.\n\n    Returns:\n        ``True`` if the tree is projective, ``False`` otherwise.\n\n    Examples:\n        >>> isprojective([2, -1, 1])  # -1 denotes un-annotated cases\n        False\n        >>> isprojective([3, -1, 2])\n        False\n    \"\"\"\n\n    pairs = [(h, d) for d, h in enumerate(sequence, 1) if h >= 0]\n    for i, (hi, di) in enumerate(pairs):\n        for hj, dj in pairs[i + 1:]:\n            (li, ri), (lj, rj) = sorted([hi, di]), sorted([hj, dj])\n            if li <= hj <= ri and hi == dj:\n                return False\n            if lj <= hi <= rj and hj == di:\n                return False\n            if (li < lj < ri or li < rj < ri) and (li - lj) * (ri - rj) > 0:\n                return False\n    return True\n", "plugins/hanlp_common/hanlp_common/util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-27 19:09\nimport math\nfrom typing import Union, Any, List, Optional, Tuple, Iterable, Dict\nimport inspect\nfrom itertools import chain, combinations\n\n\ndef powerset(iterable, descending=False):\n    \"\"\"\n    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n\n    Args:\n        iterable:\n\n    Returns:\n\n    \"\"\"\n    s = list(iterable)\n    sizes = range(len(s), -1, -1) if descending else range(len(s) + 1)\n    return chain.from_iterable(combinations(s, r) for r in sizes)\n\n\ndef isdebugging():\n    \"\"\"See Also https://stackoverflow.com/questions/333995/how-to-detect-that-python-code-is-being-executed-through-the-debugger\"\"\"\n    for frame in inspect.stack():\n        if frame[1].endswith(\"pydevd.py\"):\n            return True\n    return False\n\n\ndef list_is_list_of_lists(sent: Union[Any, List[Any]]) -> Optional[bool]:\n    if not sent:\n        return None\n    return isinstance(sent[0], list)\n\n\ndef set_tuple_with(t: Tuple, v, at=0) -> Tuple:\n    t = list(t)\n    t[at] = v\n    return tuple(t)\n\n\ndef consume_keys_from_dict(keys: Iterable, d: dict) -> dict:\n    consumed = {}\n    for k in keys:\n        if k in d:\n            consumed[k] = d.pop(k)\n    return consumed\n\n\ndef merge_dict(d: dict, overwrite=False, inplace=False, **kwargs):\n    \"\"\"Merging the provided dict with other kvs\n\n    Args:\n      d: \n      kwargs: \n      d: dict: \n      overwrite:  (Default value = False)\n      inplace:  (Default value = False)\n      **kwargs: \n\n    Returns:\n\n    \n    \"\"\"\n    nd = dict([(k, v) for k, v in d.items()] + [(k, v) for k, v in kwargs.items() if overwrite or k not in d])\n    if inplace:\n        d.update(nd)\n        return d\n    return nd\n\n\ndef merge_locals_kwargs(locals: dict, kwargs: dict = None, excludes=('self', 'kwargs', '__class__')):\n    if not kwargs:\n        kwargs = dict()\n    return merge_dict(dict((k, v) for k, v in list(locals.items())\n                           if k not in excludes), **kwargs)\n\n\ndef infer_space_after(sent: List[str]):\n    last_token = None\n    quote_count: int = 0\n    # infer whitespace after field\n    whitespace_after = [True] * len(sent)\n    for token in range(len(sent)):\n        if sent[token] == '\"':\n            quote_count += 1\n            if quote_count % 2 != 0:\n                whitespace_after[token] = False\n            elif last_token is not None:\n                whitespace_after[last_token] = False\n\n        if last_token is not None:\n\n            if sent[token] in [\".\", \":\", \",\", \";\", \")\", \"n't\", \"!\", \"?\"]:\n                whitespace_after[last_token] = False\n\n            if sent[token].startswith(\"'\"):\n                whitespace_after[last_token] = False\n\n        if sent[token] in [\"(\"]:\n            whitespace_after[token] = False\n\n        last_token = token\n    return whitespace_after\n\n\ndef collapse_json(text, indent=12):\n    \"\"\"Compacts a string of json data by collapsing whitespace after the\n    specified indent level\n    \n    NOTE: will not produce correct results when indent level is not a multiple\n    of the json indent level\n\n    Args:\n      text: \n      indent:  (Default value = 12)\n\n    Returns:\n\n    \"\"\"\n    initial = \" \" * indent\n    out = []  # final json output\n    sublevel = []  # accumulation list for sublevel entries\n    pending = None  # holder for consecutive entries at exact indent level\n    for line in text.splitlines():\n        if line.startswith(initial):\n            if line[indent] == \" \":\n                # found a line indented further than the indent level, so add\n                # it to the sublevel list\n                if pending:\n                    # the first item in the sublevel will be the pending item\n                    # that was the previous line in the json\n                    sublevel.append(pending)\n                    pending = None\n                item = line.strip()\n                sublevel.append(item)\n                if item.endswith(\",\"):\n                    sublevel.append(\" \")\n            elif sublevel:\n                # found a line at the exact indent level *and* we have sublevel\n                # items. This means the sublevel items have come to an end\n                sublevel.append(line.strip())\n                out.append(\"\".join(sublevel))\n                sublevel = []\n            else:\n                # found a line at the exact indent level but no items indented\n                # further, so possibly start a new sub-level\n                if pending:\n                    # if there is already a pending item, it means that\n                    # consecutive entries in the json had the exact same\n                    # indentation and that last pending item was not the start\n                    # of a new sublevel.\n                    out.append(pending)\n                pending = line.rstrip()\n        else:\n            if pending:\n                # it's possible that an item will be pending but not added to\n                # the output yet, so make sure it's not forgotten.\n                out.append(pending)\n                pending = None\n            if sublevel:\n                out.append(\"\".join(sublevel))\n            out.append(line)\n    return \"\\n\".join(out)\n\n\nclass DummyContext(object):\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\ndef merge_list_of_dict(samples: List[Dict]) -> dict:\n    batch = {}\n    for each in samples:\n        for k, v in each.items():\n            vs = batch.get(k, None)\n            if vs is None:\n                vs = []\n                batch[k] = vs\n            vs.append(v)\n    return batch\n\n\ndef split_dict(batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n    samples = []\n    batch = dict((k, v) for k, v in batch.items() if isinstance(v, list))\n    num_samples = len(max(batch.values(), key=len))\n    for i in range(num_samples):\n        samples.append(dict((k, v[i]) for k, v in batch.items()))\n    return samples\n\n\ndef reorder(samples: List, order: List[int]) -> List:\n    return [samples[i] for i in sorted(range(len(order)), key=lambda k: order[k])]\n\n\ndef k_fold(k, total, i):\n    trn = math.ceil(i / k * total)\n    tst = math.ceil((i + 1) / k * total)\n    return list(range(0, trn)) + list(range(tst, total)), list(range(trn, tst))\n\n\ndef dfs(graph, start):\n    seen = set()\n    path = []\n    q = [start]\n    while q:\n        v = q.pop()\n        if v not in seen:\n            seen.add(v)\n            path.append(v)\n            q.extend(graph[v])\n\n    return path\n\n\ndef topological_sort(graph, start):\n    seen = set()\n    stack = []\n    order = []\n    q = [start]\n    while q:\n        v = q.pop()\n        if v not in seen:\n            seen.add(v)\n            q.extend(graph[v])\n\n            while stack and v not in graph[stack[-1]]:\n                order.append(stack.pop())\n            stack.append(v)\n\n    return stack + order[::-1]\n\n\ndef prefix_match(target, sources: Iterable[str]):\n    if target is None:\n        return None\n    if target in sources:\n        return target\n    for each in sources:\n        if each.startswith(target):\n            return each\n", "plugins/hanlp_common/hanlp_common/constant.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-06-13 22:41\nimport os\n\nPAD = '<pad>'\n'''Padding token.'''\nUNK = '<unk>'\n'''Unknown token.'''\nCLS = '[CLS]'\nBOS = '<bos>'\nEOS = '<eos>'\nROOT = BOS\nIDX = '_idx_'\n'''Key for index.'''\nHANLP_URL = os.getenv('HANLP_URL', 'https://file.hankcs.com/hanlp/')\n'''Resource URL.'''\nHANLP_VERBOSE = os.environ.get('HANLP_VERBOSE', '1').lower() in ('1', 'true', 'yes')\n'''Enable verbose or not.'''\nNULL = '<null>'\nPRED = 'PRED'\n\nIPYTHON = os.environ.get('HANLP_IPYTHON', '1').lower() in ('1', 'true', 'yes')  # Allow the user to disable IPYTHON\nif IPYTHON:\n    try:\n        # noinspection PyUnresolvedReferences,PyStatementEffect\n        get_ipython\n    except NameError:\n        IPYTHON = False\n", "plugins/hanlp_common/hanlp_common/visualization.py": "# -*- coding:utf-8 -*-\n# Modified from https://github.com/tylerneylon/explacy\nimport io\nfrom collections import defaultdict\nfrom pprint import pprint\n\nfrom phrasetree.tree import Tree\n\n\ndef make_table(rows, insert_header=False):\n    col_widths = [max(len(s) for s in col) for col in zip(*rows[1:])]\n    rows[0] = [x[:l] for x, l in zip(rows[0], col_widths)]\n    fmt = '\\t'.join('%%-%ds' % width for width in col_widths)\n    if insert_header:\n        rows.insert(1, ['\u2500' * width for width in col_widths])\n    return '\\n'.join(fmt % tuple(row) for row in rows)\n\n\ndef _start_end(arrow):\n    start, end = arrow['from'], arrow['to']\n    mn = min(start, end)\n    mx = max(start, end)\n    return start, end, mn, mx\n\n\ndef pretty_tree_horizontal(arrows, _do_print_debug_info=False):\n    \"\"\"Print the dependency tree horizontally\n\n    Args:\n      arrows: \n      _do_print_debug_info:  (Default value = False)\n\n    Returns:\n\n    \"\"\"\n    # Set the base height; these may increase to allow room for arrowheads after this.\n    arrows_with_deps = defaultdict(set)\n    for i, arrow in enumerate(arrows):\n        arrow['underset'] = set()\n        if _do_print_debug_info:\n            print('Arrow %d: \"%s\" -> \"%s\"' % (i, arrow['from'], arrow['to']))\n        num_deps = 0\n        start, end, mn, mx = _start_end(arrow)\n        for j, other in enumerate(arrows):\n            if arrow is other:\n                continue\n            o_start, o_end, o_mn, o_mx = _start_end(other)\n            if ((start == o_start and mn <= o_end <= mx) or\n                    (start != o_start and mn <= o_start <= mx)):\n                num_deps += 1\n                if _do_print_debug_info:\n                    print('%d is over %d' % (i, j))\n                arrow['underset'].add(j)\n        arrow['num_deps_left'] = arrow['num_deps'] = num_deps\n        arrows_with_deps[num_deps].add(i)\n\n    if _do_print_debug_info:\n        print('')\n        print('arrows:')\n        pprint(arrows)\n\n        print('')\n        print('arrows_with_deps:')\n        pprint(arrows_with_deps)\n\n    # Render the arrows in characters. Some heights will be raised to make room for arrowheads.\n    sent_len = (max([max(arrow['from'], arrow['to']) for arrow in arrows]) if arrows else 0) + 1\n    lines = [[] for i in range(sent_len)]\n    num_arrows_left = len(arrows)\n    while num_arrows_left > 0:\n\n        assert len(arrows_with_deps[0])\n\n        arrow_index = arrows_with_deps[0].pop()\n        arrow = arrows[arrow_index]\n        src, dst, mn, mx = _start_end(arrow)\n\n        # Check the height needed.\n        height = 3\n        if arrow['underset']:\n            height = max(arrows[i]['height'] for i in arrow['underset']) + 1\n        height = max(height, 3, len(lines[dst]) + 3)\n        arrow['height'] = height\n\n        if _do_print_debug_info:\n            print('')\n            print('Rendering arrow %d: \"%s\" -> \"%s\"' % (arrow_index,\n                                                        arrow['from'],\n                                                        arrow['to']))\n            print('  height = %d' % height)\n\n        goes_up = src > dst\n\n        # Draw the outgoing src line.\n        if lines[src] and len(lines[src]) < height:\n            lines[src][-1].add('w')\n        while len(lines[src]) < height - 1:\n            lines[src].append(set(['e', 'w']))\n        if len(lines[src]) < height:\n            lines[src].append({'e'})\n        lines[src][height - 1].add('n' if goes_up else 's')\n\n        # Draw the incoming dst line.\n        lines[dst].append(u'\u25ba')\n        while len(lines[dst]) < height:\n            lines[dst].append(set(['e', 'w']))\n        lines[dst][-1] = set(['e', 's']) if goes_up else set(['e', 'n'])\n\n        # Draw the adjoining vertical line.\n        for i in range(mn + 1, mx):\n            while len(lines[i]) < height - 1:\n                lines[i].append(' ')\n            lines[i].append(set(['n', 's']))\n\n        # Update arrows_with_deps.\n        for arr_i, arr in enumerate(arrows):\n            if arrow_index in arr['underset']:\n                arrows_with_deps[arr['num_deps_left']].remove(arr_i)\n                arr['num_deps_left'] -= 1\n                arrows_with_deps[arr['num_deps_left']].add(arr_i)\n\n        num_arrows_left -= 1\n\n    return render_arrows(lines)\n\n\ndef render_arrows(lines):\n    arr_chars = {'ew': u'\u2500',\n                 'ns': u'\u2502',\n                 'en': u'\u2514',\n                 'es': u'\u250c',\n                 'enw': u'\u2534',\n                 'ensw': u'\u253c',\n                 'ens': u'\u251c',\n                 'esw': u'\u252c'}\n    # Convert the character lists into strings.\n    max_len = max(len(line) for line in lines)\n    for i in range(len(lines)):\n        lines[i] = [arr_chars[''.join(sorted(ch))] if type(ch) is set else ch for ch in lines[i]]\n        lines[i] = ''.join(reversed(lines[i]))\n        lines[i] = ' ' * (max_len - len(lines[i])) + lines[i]\n    return lines\n\n\ndef render_span(begin, end, unidirectional=False):\n    if end - begin == 1:\n        return ['\u2500\u2500\u2500\u25ba']\n    elif end - begin == 2:\n        return [\n            '\u2500\u2500\u2510',\n            '\u2500\u2500\u2534\u25ba',\n        ] if unidirectional else [\n            '\u25c4\u2500\u2510',\n            '\u25c4\u2500\u2534\u25ba',\n        ]\n\n    rows = []\n    for i in range(begin, end):\n        if i == (end - begin) // 2 + begin:\n            rows.append('  \u251c\u25ba')\n        elif i == begin:\n            rows.append('\u2500\u2500\u2510' if unidirectional else '\u25c4\u2500\u2510')\n        elif i == end - 1:\n            rows.append('\u2500\u2500\u2518' if unidirectional else '\u25c4\u2500\u2518')\n        else:\n            rows.append('  \u2502')\n    return rows\n\n\ndef tree_to_list(T):\n    return [T.label(), [tree_to_list(t) if isinstance(t, Tree) else t for t in T]]\n\n\ndef list_to_tree(L):\n    if isinstance(L, str):\n        return L\n    return Tree(L[0], [list_to_tree(child) for child in L[1]])\n\n\ndef render_labeled_span(b, e, spans, labels, label, offset, unidirectional=False):\n    spans.extend([''] * (b - offset))\n    spans.extend(render_span(b, e, unidirectional))\n    center = b + (e - b) // 2\n    labels.extend([''] * (center - offset))\n    labels.append(label)\n    labels.extend([''] * (e - center - 1))\n\n\ndef main():\n    # arrows = [{'from': 1, 'to': 0}, {'from': 2, 'to': 1}, {'from': 2, 'to': 4}, {'from': 2, 'to': 5},\n    #           {'from': 4, 'to': 3}]\n    # lines = pretty_tree_horizontal(arrows)\n    # print('\\n'.join(lines))\n    # print('\\n'.join([\n    #     '\u25c4\u2500\u2510',\n    #     '  \u2502',\n    #     '  \u251c\u25ba',\n    #     '  \u2502',\n    #     '\u25c4\u2500\u2518',\n    # ]))\n    print('\\n'.join(render_span(7, 12)))\n\n\nif __name__ == '__main__':\n    main()\nleft_rule = {'<': ':', '^': ':', '>': '-'}\nright_rule = {'<': '-', '^': ':', '>': ':'}\n\n\ndef evalute_field(record, field_spec):\n    \"\"\"Evalute a field of a record using the type of the field_spec as a guide.\n\n    Args:\n      record:\n      field_spec:\n\n    Returns:\n\n    \"\"\"\n    if type(field_spec) is int:\n        return str(record[field_spec])\n    elif type(field_spec) is str:\n        return str(getattr(record, field_spec))\n    else:\n        return str(field_spec(record))\n\n\ndef markdown_table(headings, records, fields=None, alignment=None, file=None):\n    \"\"\"Generate a Doxygen-flavor Markdown table from records.\n    See https://stackoverflow.com/questions/13394140/generate-markdown-tables\n\n    file -- Any object with a 'write' method that takes a single string\n        parameter.\n    records -- Iterable.  Rows will be generated from this.\n    fields -- List of fields for each row.  Each entry may be an integer,\n        string or a function.  If the entry is an integer, it is assumed to be\n        an index of each record.  If the entry is a string, it is assumed to be\n        a field of each record.  If the entry is a function, it is called with\n        the record and its return value is taken as the value of the field.\n    headings -- List of column headings.\n    alignment - List of pairs alignment characters.  The first of the pair\n        specifies the alignment of the header, (Doxygen won't respect this, but\n        it might look good, the second specifies the alignment of the cells in\n        the column.\n\n        Possible alignment characters are:\n            '<' = Left align\n            '>' = Right align (default for cells)\n            '^' = Center (default for column headings)\n\n    Args:\n      headings:\n      records:\n      fields:  (Default value = None)\n      alignment:  (Default value = None)\n      file:  (Default value = None)\n\n    Returns:\n\n    \"\"\"\n    if not file:\n        file = io.StringIO()\n    num_columns = len(headings)\n    if not fields:\n        fields = list(range(num_columns))\n    assert len(headings) == num_columns\n\n    # Compute the table cell data\n    columns = [[] for i in range(num_columns)]\n    for record in records:\n        for i, field in enumerate(fields):\n            columns[i].append(evalute_field(record, field))\n\n    # Fill out any missing alignment characters.\n    extended_align = alignment if alignment is not None else [('^', '<')]\n    if len(extended_align) > num_columns:\n        extended_align = extended_align[0:num_columns]\n    elif len(extended_align) < num_columns:\n        extended_align += [('^', '>') for i in range(num_columns - len(extended_align))]\n\n    heading_align, cell_align = [x for x in zip(*extended_align)]\n\n    field_widths = [len(max(column, key=len)) if len(column) > 0 else 0\n                    for column in columns]\n    heading_widths = [max(len(head), 2) for head in headings]\n    column_widths = [max(x) for x in zip(field_widths, heading_widths)]\n\n    _ = ' | '.join(['{:' + a + str(w) + '}'\n                    for a, w in zip(heading_align, column_widths)])\n    heading_template = '| ' + _ + ' |'\n    _ = ' | '.join(['{:' + a + str(w) + '}'\n                    for a, w in zip(cell_align, column_widths)])\n    row_template = '| ' + _ + ' |'\n\n    _ = ' | '.join([left_rule[a] + '-' * (w - 2) + right_rule[a]\n                    for a, w in zip(cell_align, column_widths)])\n    ruling = '| ' + _ + ' |'\n\n    file.write(heading_template.format(*headings).rstrip() + '\\n')\n    file.write(ruling.rstrip() + '\\n')\n    for row in zip(*columns):\n        file.write(row_template.format(*row).rstrip() + '\\n')\n    if isinstance(file, io.StringIO):\n        text = file.getvalue()\n        file.close()\n        return text\n", "plugins/hanlp_common/hanlp_common/io.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-16 22:38\nimport json\nimport os\nimport pickle\nimport sys\nfrom typing import Union\n\n\ndef save_pickle(item, path):\n    with open(path, 'wb') as f:\n        pickle.dump(item, f)\n\n\ndef load_pickle(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\ndef save_json(item: Union[dict, list, str, int, float], path: str, ensure_ascii=False, cls=None,\n              default=lambda o: repr(o), indent=2):\n    dirname = os.path.dirname(path)\n    if dirname:\n        os.makedirs(dirname, exist_ok=True)\n    with open(path, 'w', encoding='utf-8') as out:\n        json.dump(item, out, ensure_ascii=ensure_ascii, indent=indent, cls=cls, default=default)\n\n\ndef load_json(path):\n    with open(path, encoding='utf-8') as src:\n        return json.load(src)\n\n\ndef filename_is_json(filename):\n    filename, file_extension = os.path.splitext(filename)\n    return file_extension in ['.json', '.jsonl']\n\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n", "plugins/hanlp_common/hanlp_common/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-16 22:21\n", "plugins/hanlp_common/hanlp_common/reflection.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 16:41\nimport importlib\nimport inspect\n\n\ndef classpath_of(obj) -> str:\n    \"\"\"get the full class path of object\n\n    Args:\n      obj: return:\n\n    Returns:\n\n    \"\"\"\n    if inspect.isfunction(obj):\n        return module_path_of(obj)\n    return \"{0}.{1}\".format(obj.__class__.__module__, obj.__class__.__name__)\n\n\ndef module_path_of(func) -> str:\n    return inspect.getmodule(func).__name__ + '.' + func.__name__\n\n\ndef object_from_classpath(classpath, **kwargs):\n    classpath = str_to_type(classpath)\n    if inspect.isfunction(classpath):\n        return classpath\n    return classpath(**kwargs)\n\n\ndef str_to_type(classpath):\n    \"\"\"convert class path in str format to a type\n\n    Args:\n      classpath: class path\n\n    Returns:\n      type\n\n    \"\"\"\n    module_name, class_name = classpath.rsplit(\".\", 1)\n    cls = getattr(importlib.import_module(module_name), class_name)\n    return cls\n\n\ndef type_to_str(type_object) -> str:\n    \"\"\"convert a type object to class path in str format\n\n    Args:\n      type_object: type\n\n    Returns:\n      class path\n\n    \"\"\"\n    cls_name = str(type_object)\n    assert cls_name.startswith(\"<class '\"), 'illegal input'\n    cls_name = cls_name[len(\"<class '\"):]\n    assert cls_name.endswith(\"'>\"), 'illegal input'\n    cls_name = cls_name[:-len(\"'>\")]\n    return cls_name\n", "plugins/hanlp_common/hanlp_common/structure.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-19 20:56\nimport json\nfrom collections import OrderedDict\n\nfrom hanlp_common.io import filename_is_json, save_pickle, load_pickle, save_json, load_json\n\n\nclass Serializable(object):\n    \"\"\"A super class for save/load operations.\"\"\"\n\n    def save(self, path, fmt=None):\n        if not fmt:\n            if filename_is_json(path):\n                self.save_json(path)\n            else:\n                self.save_pickle(path)\n        elif fmt in ['json', 'jsonl']:\n            self.save_json(path)\n        else:\n            self.save_pickle(path)\n\n    def load(self, path, fmt=None):\n        if not fmt:\n            if filename_is_json(path):\n                self.load_json(path)\n            else:\n                self.load_pickle(path)\n        elif fmt in ['json', 'jsonl']:\n            self.load_json(path)\n        else:\n            self.load_pickle(path)\n\n    def save_pickle(self, path):\n        \"\"\"Save to path\n\n        Args:\n          path:\n\n        Returns:\n\n\n        \"\"\"\n        save_pickle(self, path)\n\n    def load_pickle(self, path):\n        \"\"\"Load from path\n\n        Args:\n          path(str): file path\n\n        Returns:\n\n\n        \"\"\"\n        item = load_pickle(path)\n        return self.copy_from(item)\n\n    def save_json(self, path):\n        save_json(self.to_dict(), path)\n\n    def load_json(self, path):\n        item = load_json(path)\n        return self.copy_from(item)\n\n    # @abstractmethod\n    def copy_from(self, item):\n        self.__dict__ = item.__dict__\n        # raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    def to_json(self, ensure_ascii=False, indent=2, sort=False) -> str:\n        d = self.to_dict()\n        if sort:\n            d = OrderedDict(sorted(d.items()))\n        return json.dumps(d, ensure_ascii=ensure_ascii, indent=indent, default=lambda o: repr(o))\n\n    def to_dict(self) -> dict:\n        return self.__dict__\n\n\nclass SerializableDict(Serializable, dict):\n\n    def save_json(self, path):\n        save_json(self, path)\n\n    def copy_from(self, item):\n        if isinstance(item, dict):\n            self.clear()\n            self.update(item)\n\n    def __getattr__(self, key):\n        if key.startswith('__'):\n            return dict.__getattr__(key)\n        return self.__getitem__(key)\n\n    def __setattr__(self, key, value):\n        return self.__setitem__(key, value)\n\n    def to_dict(self) -> dict:\n        return self", "plugins/hanlp_restful/setup.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\nfrom os.path import abspath, join, dirname\nfrom setuptools import find_packages, setup\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\n\nsetup(\n    name='hanlp_restful',\n    version='0.0.23',\n    description='HanLP: Han Language Processing',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/hankcs/HanLP',\n    author='hankcs',\n    author_email='hankcshe@gmail.com',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Developers',\n        \"Development Status :: 3 - Alpha\",\n        'Operating System :: OS Independent',\n        \"License :: OSI Approved :: Apache Software License\",\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        \"Topic :: Text Processing :: Linguistic\"\n    ],\n    keywords='corpus,machine-learning,NLU,NLP',\n    packages=find_packages(exclude=['docs', 'tests*']),\n    include_package_data=True,\n    install_requires=[\n        'hanlp_common'\n    ],\n    python_requires='>=3.6',\n)\n", "plugins/hanlp_restful/hanlp_restful/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-29 17:48\nimport json\nfrom typing import Union, List, Optional, Dict, Any, Tuple\nfrom urllib.error import HTTPError\nfrom urllib.parse import urlencode\nfrom urllib.request import Request, urlopen\nfrom hanlp_common.document import Document\n\ntry:\n    # noinspection PyUnresolvedReferences\n    import requests\n\n\n    def _post(url, form: Dict[str, Any], headers: Dict[str, Any], timeout=60, verify=True) -> str:\n        response = requests.post(url, json=form, headers=headers, timeout=timeout, verify=verify)\n        if response.status_code != 200:\n            raise HTTPError(url, response.status_code, response.text, response.headers, None)\n        return response.text\nexcept ImportError:\n    import ssl\n\n\n    def _post(url, form: Dict[str, Any], headers: Dict[str, Any], timeout=60, verify=True) -> str:\n        request = Request(url, json.dumps(form).encode())\n        for k, v in headers.items():\n            request.add_header(k, v)\n        ctx = None\n        if not verify:\n            ctx = ssl.create_default_context()\n            ctx.check_hostname = False\n            ctx.verify_mode = ssl.CERT_NONE\n        return urlopen(request, timeout=timeout, context=ctx).read().decode()\n\n\nclass HanLPClient(object):\n\n    def __init__(self, url: str, auth: str = None, language=None, timeout=60, verify=True) -> None:\n        \"\"\"\n\n        Args:\n            url (str): An API endpoint to a service provider.\n            auth (str): An auth key licenced from a service provider.\n            language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.\n                Contact the service provider for the list of languages supported.\n                Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.\n                Leave ``None`` to use the default language on server.\n            timeout (int): Maximum waiting time in seconds for a request.\n            verify (bool): ``True`` to enable SSL cert verification. You can also pass ``verify`` the path to a CA_BUNDLE\n                file or directory with certificates of trusted CAs (``requests`` required).\n        \"\"\"\n        super().__init__()\n        self._language = language\n        self._timeout = timeout\n        self._url = url\n        if auth is None:\n            import os\n            auth = os.getenv('HANLP_AUTH', None)\n        self._auth = auth\n        self._verify = verify\n\n    def parse(self,\n              text: Union[str, List[str]] = None,\n              tokens: List[List[str]] = None,\n              tasks: Optional[Union[str, List[str]]] = None,\n              skip_tasks: Optional[Union[str, List[str]]] = None,\n              language: str = None,\n              ) -> Document:\n        \"\"\"\n        Parse a piece of text.\n\n        Args:\n            text: A document (str), or a list of sentences (List[str]).\n            tokens: A list of sentences where each sentence is a list of tokens.\n            tasks: The tasks to predict. Use ``tasks=[...]`` to run selected tasks only. Dependent tasks will be\n                automatically selected.\n            skip_tasks: The tasks to skip. Use ``skip_tasks='tok/fine'`` to enable coarse tokenization for all tasks.\n                Use ``tasks=['tok/coarse', ...]`` and ``skip_tasks='tok/fine'`` to enable coarse tokenization for\n                selected tasks.\n            language: The language of input text or tokens. ``None`` to use the default language on server.\n\n        Returns:\n            A :class:`~hanlp_common.document.Document`.\n\n        Examples::\n\n            # Use tasks=[...] to run selected tasks only\n            HanLP('\u6653\u7f8e\u7130\u6765\u5230\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8', tasks=['pos', 'ner'])\n\n            # Use skip_tasks='tok/fine' to enable coarse tokenization for all tasks\n            HanLP('\u6653\u7f8e\u7130\u6765\u5230\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8', skip_tasks='tok/fine')\n\n            # Use tasks=['tok/coarse', ...] and skip_tasks='tok/fine' to enable\n            # coarse tokenization for selected tasks\n            HanLP('\u6653\u7f8e\u7130\u6765\u5230\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8', tasks=['tok/coarse','pos'],skip_tasks='tok/fine')\n\n\n        Raises:\n            HTTPError: Any errors happening on the Internet side or the server side. Refer to the ``code`` and ``msg``\n                of the exception for more details. A list of common errors :\n\n        - ``400 Bad Request`` indicates that the server cannot process the request due to a client\n          fault (e.g., text too long, language unsupported).\n        - ``401 Unauthorized`` indicates that the request lacks **valid** ``auth`` credentials for the API.\n        - ``422 Unprocessable Entity`` indicates that the content type of the request entity is not in\n          proper json format.\n        - ``429 Too Many Requests`` indicates the user has sent too many requests in a given\n          amount of time (\"rate limiting\").\n\n        \"\"\"\n        assert text or tokens, 'At least one of text or tokens has to be specified.'\n        response = self._send_post_json(self._url + '/parse', {\n            'text': text,\n            'tokens': tokens,\n            'tasks': tasks,\n            'skip_tasks': skip_tasks,\n            'language': language or self._language\n        })\n        return Document(response)\n\n    def __call__(self,\n                 text: Union[str, List[str]] = None,\n                 tokens: List[List[str]] = None,\n                 tasks: Optional[Union[str, List[str]]] = None,\n                 skip_tasks: Optional[Union[str, List[str]]] = None,\n                 language: str = None,\n                 ) -> Document:\n        \"\"\"\n        A shortcut of :meth:`~hanlp_restful.HanLPClient.parse`.\n        \"\"\"\n        return self.parse(text, tokens, tasks, skip_tasks, language)\n\n    def about(self) -> Dict[str, Any]:\n        \"\"\"Get the information about server and your client.\n\n        Returns:\n            A dict containing your rate limit and server version etc.\n\n        \"\"\"\n        info = self._send_get_json(self._url + '/about', {})\n        return Document(info)\n\n    def _send_post(self, url, form: Dict[str, Any]):\n        request = Request(url, json.dumps(form).encode())\n        self._add_headers(request)\n        return self._fire_request(request)\n\n    def _fire_request(self, request):\n        return urlopen(request, timeout=self._timeout).read().decode()\n\n    def _send_post_json(self, url, form: Dict[str, Any]):\n        headers = dict()\n        if self._auth:\n            headers['Authorization'] = f'Basic {self._auth}'\n        return json.loads(_post(url, form, headers, self._timeout, verify=self._verify))\n\n    def _send_get(self, url, form: Dict[str, Any]):\n        request = Request(url + '?' + urlencode(form))\n        self._add_headers(request)\n        return self._fire_request(request)\n\n    def _add_headers(self, request):\n        if self._auth:\n            request.add_header('Authorization', f'Basic {self._auth}')\n\n    def _send_get_json(self, url, form: Dict[str, Any]):\n        return json.loads(self._send_get(url, form))\n\n    def text_style_transfer(self, text: Union[str, List[str]], target_style: str, language: str = None) \\\n            -> Union[str, List[str]]:\n        \"\"\" Text style transfer aims to change the style of the input text to the target style while preserving its\n        content.\n\n        Args:\n            text: Source text.\n            target_style: Target style.\n            language: The language of input text. ``None`` to use the default language.\n\n        Returns:\n            Text or a list of text of the target style.\n\n        Examples::\n\n            HanLP.text_style_transfer(['\u56fd\u5bb6\u5bf9\u4e2d\u77f3\u6cb9\u62b1\u6709\u5f88\u5927\u7684\u671f\u671b.', '\u8981\u7528\u521b\u65b0\u53bb\u63a8\u52a8\u9ad8\u8d28\u91cf\u7684\u53d1\u5c55\u3002'],\n                                      target_style='gov_doc')\n            # Output:\n            [\n                '\u56fd\u5bb6\u5bf9\u4e2d\u77f3\u6cb9\u5bc4\u4e88\u539a\u671b\u3002',\n                '\u8981\u4ee5\u521b\u65b0\u9a71\u52a8\u9ad8\u8d28\u91cf\u53d1\u5c55\u3002'\n            ]\n\n            HanLP.text_style_transfer('\u6211\u770b\u5230\u4e86\u7a97\u6237\u5916\u9762\u6709\u767d\u8272\u7684\u4e91\u548c\u7eff\u8272\u7684\u68ee\u6797',\n                                      target_style='modern_poetry')\n            # Output:\n            '\u6211\u770b\u89c1\u7a97\u5916\u7684\u767d\u4e91\u7eff\u6797'\n        \"\"\"\n        response = self._send_post_json(self._url + '/text_style_transfer',\n                                        {'text': text, 'target_style': target_style,\n                                         'language': language or self._language})\n        return response\n\n    def semantic_textual_similarity(self, text: Union[Tuple[str, str], List[Tuple[str, str]]], language: str = None) \\\n            -> Union[float, List[float]]:\n        \"\"\" Semantic textual similarity deals with determining how similar two pieces of texts are.\n\n        Args:\n            text: A pair or pairs of text.\n            language: The language of input text. ``None`` to use the default language.\n\n        Returns:\n            Similarities.\n\n        Examples::\n\n            HanLP.semantic_textual_similarity([\n                ('\u770b\u56fe\u731c\u4e00\u7535\u5f71\u540d', '\u770b\u56fe\u731c\u7535\u5f71'),\n                ('\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u65e0\u7ebf\u4e0a\u7f51', '\u65e0\u7ebf\u4e0a\u7f51\u5361\u548c\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u7528'),\n                ('\u5317\u4eac\u5230\u4e0a\u6d77\u7684\u52a8\u8f66\u7968', '\u4e0a\u6d77\u5230\u5317\u4eac\u7684\u52a8\u8f66\u7968'),\n            ])\n            # Output:\n            [\n                0.9764469, # Similarity of ('\u770b\u56fe\u731c\u4e00\u7535\u5f71\u540d', '\u770b\u56fe\u731c\u7535\u5f71')\n                0.0,       # Similarity of ('\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u65e0\u7ebf\u4e0a\u7f51', '\u65e0\u7ebf\u4e0a\u7f51\u5361\u548c\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u7528')\n                0.0034587  # Similarity of ('\u5317\u4eac\u5230\u4e0a\u6d77\u7684\u52a8\u8f66\u7968', '\u4e0a\u6d77\u5230\u5317\u4eac\u7684\u52a8\u8f66\u7968')\n            ]\n        \"\"\"\n        response = self._send_post_json(self._url + '/semantic_textual_similarity',\n                                        {'text': text, 'language': language or self._language})\n        return response\n\n    def coreference_resolution(self, text: Optional[str] = None, tokens: Optional[List[List[str]]] = None,\n                               speakers: Optional[List[str]] = None, language: Optional[str] = None) -> Union[\n        Dict[str, Union[List[str], List[List[Tuple[str, int, int]]]]], List[List[Tuple[str, int, int]]]]:\n        r\"\"\" Coreference resolution is the task of clustering mentions in text that refer to the same underlying\n        real world entities.\n\n        Args:\n            text: A piece of text, usually a document without tokenization.\n            tokens: A list of sentences where each sentence is a list of tokens.\n            speakers: A list of speakers where each speaker is a ``str`` representing the speaker's ID, e.g., ``Tom``.\n            language: The language of input text. ``None`` to use the default language.\n\n        Returns:\n            When ``text`` is specified, return the clusters and tokens. Otherwise just the clusters, In this case, you need to ``sum(tokens, [])`` in order to match the span indices with tokens\n\n        Examples::\n\n            HanLP.coreference_resolution('\u6211\u59d0\u9001\u6211\u5979\u7684\u732b\u3002\u6211\u5f88\u559c\u6b22\u5b83\u3002')\n            # Output:\n            {'clusters': [\n                          [['\u6211', 0, 1], ['\u6211', 3, 4], ['\u6211', 8, 9]], # \u6307\u4ee3\u8bf4\u8bdd\u4eba\n                          [['\u6211\u59d0', 0, 2], ['\u5979', 4, 5]],             # \u6307\u4ee3\u8bf4\u8bdd\u4eba\u7684\u59d0\u59d0\n                          [['\u5979\u7684\u732b', 4, 7], ['\u5b83', 11, 12]]],        # \u6307\u4ee3\u8bf4\u8bdd\u4eba\u7684\u59d0\u59d0\u7684\u732b\n             'tokens': ['\u6211', '\u59d0', '\u9001', '\u6211', '\u5979', '\u7684', '\u732b', '\u3002',\n                        '\u6211', '\u5f88', '\u559c\u6b22', '\u5b83', '\u3002']}\n\n            HanLP.coreference_resolution(\n            tokens=[['\u6211', '\u59d0', '\u9001', '\u6211', '\u5979', '\u7684', '\u732b', '\u3002'],\n                    ['\u6211', '\u5f88', '\u559c\u6b22', '\u5b83', '\u3002']])\n            # Output:\n                         [\n                          [['\u6211', 0, 1], ['\u6211', 3, 4], ['\u6211', 8, 9]], # \u6307\u4ee3\u8bf4\u8bdd\u4eba\n                          [['\u6211\u59d0', 0, 2], ['\u5979', 4, 5]],             # \u6307\u4ee3\u8bf4\u8bdd\u4eba\u7684\u59d0\u59d0\n                          [['\u5979\u7684\u732b', 4, 7], ['\u5b83', 11, 12]]],        # \u6307\u4ee3\u8bf4\u8bdd\u4eba\u7684\u59d0\u59d0\u7684\u732b\n\n        .. image:: https://file.hankcs.com/img/coref_demo_small.png\n            :alt: Coreference resolution visualization\n        \"\"\"\n        response = self._send_post_json(self._url + '/coreference_resolution',\n                                        {'text': text, 'tokens': tokens, 'speakers': speakers,\n                                         'language': language or self._language})\n        return response\n\n    def tokenize(self, text: Union[str, List[str]], coarse: Optional[bool] = None, language=None) -> List[List[str]]:\n        \"\"\" Split a document into sentences and tokenize them. Note that it is always faster to tokenize a whole\n        document than to tokenize each sentence one by one. So avoid calling this method sentence by sentence but put\n        sentences into a ``list`` and pass them to the ``text`` argument.\n\n        Args:\n            text: A document (``str``), or a list of sentences (``List[str]``).\n            coarse: Whether to perform coarse-grained or fine-grained tokenization.\n            language: The language of input text. ``None`` to use the default language.\n\n        Returns:\n            A list of tokenized sentences.\n\n        Examples::\n\n            # Avoid tokenizing sentence by sentence, it is expensive:\n            HanLP.tokenize('\u5546\u54c1\u548c\u670d\u52a1\u3002')\n            [['\u5546\u54c1', '\u548c', '\u670d\u52a1', '\u3002']]\n            HanLP.tokenize('\u963f\u5a46\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8')\n            [['\u963f\u5a46\u4e3b', '\u6765\u5230', '\u5317\u4eac', '\u7acb\u65b9\u5ead', '\u53c2\u89c2', '\u81ea\u7136', '\u8bed\u4e49', '\u79d1\u6280', '\u516c\u53f8']]\n\n            # Instead, the following codes are much faster:\n            HanLP.tokenize('\u5546\u54c1\u548c\u670d\u52a1\u3002\u963f\u5a46\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8')\n            [['\u5546\u54c1', '\u548c', '\u670d\u52a1', '\u3002'],\n             ['\u963f\u5a46\u4e3b', '\u6765\u5230', '\u5317\u4eac', '\u7acb\u65b9\u5ead', '\u53c2\u89c2', '\u81ea\u7136', '\u8bed\u4e49', '\u79d1\u6280', '\u516c\u53f8']]\n\n            # To tokenize with coarse-grained standard:\n            HanLP.tokenize('\u5546\u54c1\u548c\u670d\u52a1\u3002\u963f\u5a46\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8', coarse=True)\n            [['\u5546\u54c1', '\u548c', '\u670d\u52a1', '\u3002'],\n             ['\u963f\u5a46\u4e3b', '\u6765\u5230', '\u5317\u4eac', '\u7acb\u65b9\u5ead', '\u53c2\u89c2', '\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8']]\n\n            # To tokenize pre-segmented sentences:\n            HanLP.tokenize(['\u5546\u54c1\u548c\u670d\u52a1\u3002', '\u5f53\u4e0b\u96e8\u5929\u5730\u9762\u79ef\u6c34\u5206\u5916\u4e25\u91cd'])\n            [['\u5546\u54c1', '\u548c', '\u670d\u52a1', '\u3002'],\n             ['\u5f53', '\u4e0b\u96e8\u5929', '\u5730\u9762', '\u79ef\u6c34', '\u5206', '\u5916', '\u4e25\u91cd']]\n\n            # Multilingual tokenization by specifying language='mul':\n            HanLP.tokenize(\n                ['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques\n                 'to production environment.',\n                 '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002',\n                 '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002'], language='mul')\n            [['In', '2021', ',', 'HanLPv2.1', 'delivers', 'state-of-the-art', 'multilingual',\n              'NLP', 'techniques', 'to', 'production', 'environment', '.'],\n             ['2021', '\u5e74', '\u3001', 'HanLPv2.1', '\u306f', '\u6b21', '\u4e16\u4ee3', '\u306e', '\u6700', '\u5148\u7aef', '\u591a',\n              '\u8a00\u8a9e', 'NLP', '\u6280\u8853', '\u3092', '\u672c\u756a', '\u74b0\u5883', '\u306b', '\u5c0e\u5165', '\u3057\u307e\u3059', '\u3002'],\n             ['2021', '\u5e74', 'HanLPv2.1', '\u4e3a', '\u751f\u4ea7', '\u73af\u5883', '\u5e26\u6765', '\u6b21\u4e16\u4ee3', '\u6700', '\u5148\u8fdb\u7684',\n              '\u591a', '\u8bed\u79cd', 'NLP', '\u6280\u672f', '\u3002']]\n        \"\"\"\n        language = language or self._language\n        if coarse and language and language != 'zh':\n            raise NotImplementedError(f'Coarse tokenization not supported for {language}. Please set language=\"zh\".')\n        doc = self.parse(text=text, tasks='tok/coarse' if coarse is True else 'tok', language=language)\n        return next(iter(doc.values()))\n\n    def abstract_meaning_representation(self,\n                                        text: Union[str, List[str]] = None,\n                                        tokens: List[List[str]] = None,\n                                        language: str = None,\n                                        visualization: str = None,\n                                        ) -> List[Dict]:\n        \"\"\"Abstract Meaning Representation (AMR) captures \u201cwho is doing what to whom\u201d in a sentence. Each sentence is\n        represented as a rooted, directed, acyclic graph consisting of nodes (concepts) and edges (relations).\n\n        Args:\n            text: A document (str), or a list of sentences (List[str]).\n            tokens: A list of sentences where each sentence is a list of tokens.\n            language: The language of input text or tokens. ``None`` to use the default language on server.\n            visualization: Set to `dot` or `svg` to obtain coresspodning visualization.\n\n        Returns:\n            Graphs in meaning represenation format.\n\n        Examples::\n\n            HanLP.abstract_meaning_representation('\u7537\u5b69\u5e0c\u671b\u5973\u5b69\u76f8\u4fe1\u4ed6\u3002')\n            HanLP.abstract_meaning_representation('The boy wants the girl to believe him.',\n                                                  language='en')\n\n        .. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=%E7%94%B7%E5%AD%A9%20%E5%B8%8C%E6%9C%9B%20%E5%A5%B3%E5%AD%A9%20%E7%9B%B8%E4%BF%A1%20%E4%BB%96%20%E3%80%82&language=zh&scale=1\n            :alt: Abstract Meaning Representation\n\n        .. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=The%20boy%20wants%20the%20girl%20to%20believe%20him%20.&language=en&scale=1\n            :alt: Abstract Meaning Representation\n\n        \"\"\"\n        assert text or tokens, 'At least one of text or tokens has to be specified.'\n        return self._send_post_json(self._url + '/abstract_meaning_representation', {\n            'text': text,\n            'tokens': tokens,\n            'language': language or self._language,\n            'visualization': visualization,\n        })\n\n    def keyphrase_extraction(\n            self,\n            text: str,\n            topk: int = 10,\n            language: str = None,\n    ) -> Dict[str, float]:\n        \"\"\" Keyphrase extraction aims to identify keywords or phrases reflecting the main topics of a document.\n\n        Args:\n            text: The text content of the document. Preferably the concatenation of the title and the content.\n            topk: The number of top-K ranked keywords or keyphrases.\n            language: The language of input text or tokens. ``None`` to use the default language on server.\n\n        Returns:\n            A dictionary containing each keyword or keyphrase and its ranking score :math:`s`, :math:`s \\in [0, 1]`.\n\n        Examples::\n\n            HanLP.keyphrase_extraction(\n                '\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u4e00\u95e8\u535a\u5927\u7cbe\u6df1\u7684\u5b66\u79d1\uff0c\u638c\u63e1\u7406\u8bba\u624d\u80fd\u53d1\u6325\u51faHanLP\u7684\u5168\u90e8\u6027\u80fd\u3002 '\n                '\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u662f\u4e00\u672c\u914d\u5957HanLP\u7684NLP\u5165\u95e8\u4e66\uff0c\u52a9\u4f60\u96f6\u8d77\u70b9\u4e0a\u624b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002', topk=3)\n            # Output:\n            {'\u81ea\u7136\u8bed\u8a00\u5904\u7406': 0.800000011920929,\n             'HanLP\u7684\u5168\u90e8\u6027\u80fd': 0.5258446335792542,\n             '\u4e00\u95e8\u535a\u5927\u7cbe\u6df1\u7684\u5b66\u79d1': 0.421421080827713}\n        \"\"\"\n        assert text, 'Text has to be specified.'\n        return self._send_post_json(self._url + '/keyphrase_extraction', {\n            'text': text,\n            'language': language or self._language,\n            'topk': topk,\n        })\n\n    def extractive_summarization(\n            self,\n            text: str,\n            topk: int = 3,\n            language: str = None,\n    ) -> Dict[str, float]:\n        \"\"\" Single document summarization is the task of selecting a subset of the sentences which best\n        represents a summary of the document, with a balance of salience and redundancy.\n\n        Args:\n            text: The text content of the document.\n            topk: The maximum number of top-K ranked sentences. Note that due to Trigram Blocking tricks, the actual\n                number of returned sentences could be less than ``topk``.\n            language: The language of input text or tokens. ``None`` to use the default language on server.\n\n        Returns:\n            A dictionary containing each sentence and its ranking score :math:`s \\in [0, 1]`.\n\n        Examples::\n\n            HanLP.extractive_summarization('''\n            \u636eDigiTimes\u62a5\u9053\uff0c\u5728\u4e0a\u6d77\u75ab\u60c5\u8d8b\u7f13\uff0c\u9632\u75ab\u7ba1\u63a7\u5f00\u59cb\u653e\u677e\u540e\uff0c\u82f9\u679c\u4f9b\u5e94\u5546\u5e7f\u8fbe\u6b63\u5728\u9010\u6b65\u6062\u590d\u5176\u4e2d\u56fd\u5de5\u5382\u7684MacBook\u4ea7\u54c1\u751f\u4ea7\u3002\n            \u636e\u4f9b\u5e94\u94fe\u6d88\u606f\u4eba\u58eb\u79f0\uff0c\u751f\u4ea7\u5382\u7684\u8ba2\u5355\u62c9\u52a8\u60c5\u51b5\u6b63\u5728\u6162\u6162\u8f6c\u5f3a\uff0c\u8fd9\u4f1a\u63d0\u9ad8MacBook Pro\u673a\u578b\u7684\u4f9b\u5e94\u91cf\uff0c\u5e76\u7f29\u77ed\u82f9\u679c\u5ba2\u6237\u5728\u8fc7\u53bb\u51e0\u5468\u6240\u7ecf\u5386\u7684\u5ef6\u957f\u4ea4\u8d27\u65f6\u95f4\u3002\n            \u4ecd\u6709\u8bb8\u591a\u82f9\u679c\u7b14\u8bb0\u672c\u7528\u6237\u5728\u7b49\u5f853\u6708\u548c4\u6708\u8ba2\u8d2d\u7684MacBook Pro\u673a\u578b\u5230\u8d27\uff0c\u7531\u4e8e\u82f9\u679c\u7684\u4f9b\u5e94\u95ee\u9898\uff0c\u4ed6\u4eec\u7684\u53d1\u8d27\u65f6\u95f4\u88ab\u5927\u5927\u63a8\u8fdf\u4e86\u3002\n            \u636e\u5206\u6790\u5e08\u90ed\u660e\u9324\u8868\u793a\uff0c\u5e7f\u8fbe\u662f\u9ad8\u7aefMacBook Pro\u7684\u552f\u4e00\u4f9b\u5e94\u5546\uff0c\u81ea\u9632\u75ab\u5c01\u63a7\u4f9d\u8d56\uff0cMacBook Pro\u5927\u90e8\u5206\u578b\u53f7\u4ea4\u8d27\u65f6\u95f4\u589e\u52a0\u4e86\u4e09\u5230\u4e94\u5468\uff0c\n            \u4e00\u4e9b\u9ad8\u7aef\u5b9a\u5236\u578b\u53f7\u7684MacBook Pro\u914d\u7f6e\u8981\u52306\u6708\u5e95\u52307\u6708\u521d\u624d\u80fd\u4ea4\u8d27\u3002\n            \u5c3d\u7ba1MacBook Pro\u7684\u751f\u4ea7\u9010\u6e10\u6062\u590d\uff0c\u4f46\u4f9b\u5e94\u95ee\u9898\u9884\u8ba1\u4f9d\u7136\u5f71\u54cd2022\u5e74\u7b2c\u4e09\u5b63\u5ea6\u7684\u4ea7\u54c1\u9500\u552e\u3002\n            \u82f9\u679c\u4e0a\u5468\u8868\u793a\uff0c\u9632\u75ab\u63aa\u65bd\u548c\u5143\u90e8\u4ef6\u77ed\u7f3a\u5c06\u7ee7\u7eed\u4f7f\u5176\u96be\u4ee5\u751f\u4ea7\u8db3\u591f\u7684\u4ea7\u54c1\u6765\u6ee1\u8db3\u6d88\u8d39\u8005\u7684\u5f3a\u52b2\u9700\u6c42\uff0c\u8fd9\u6700\u7ec8\u5c06\u5f71\u54cd\u82f9\u679c6\u6708\u4efd\u7684\u6536\u5165\u3002\n            ''')\n            # Output:\n            {'\u636eDigiTimes\u62a5\u9053\uff0c\u5728\u4e0a\u6d77\u75ab\u60c5\u8d8b\u7f13\uff0c\u9632\u75ab\u7ba1\u63a7\u5f00\u59cb\u653e\u677e\u540e\uff0c\u82f9\u679c\u4f9b\u5e94\u5546\u5e7f\u8fbe\u6b63\u5728\u9010\u6b65\u6062\u590d\u5176\u4e2d\u56fd\u5de5\u5382\u7684MacBook\u4ea7\u54c1\u751f\u4ea7\u3002': 0.9999,\n             '\u4ecd\u6709\u8bb8\u591a\u82f9\u679c\u7b14\u8bb0\u672c\u7528\u6237\u5728\u7b49\u5f853\u6708\u548c4\u6708\u8ba2\u8d2d\u7684MacBook Pro\u673a\u578b\u5230\u8d27\uff0c\u7531\u4e8e\u82f9\u679c\u7684\u4f9b\u5e94\u95ee\u9898\uff0c\u4ed6\u4eec\u7684\u53d1\u8d27\u65f6\u95f4\u88ab\u5927\u5927\u63a8\u8fdf\u4e86\u3002': 0.5800,\n             '\u5c3d\u7ba1MacBook Pro\u7684\u751f\u4ea7\u9010\u6e10\u6062\u590d\uff0c\u4f46\u4f9b\u5e94\u95ee\u9898\u9884\u8ba1\u4f9d\u7136\u5f71\u54cd2022\u5e74\u7b2c\u4e09\u5b63\u5ea6\u7684\u4ea7\u54c1\u9500\u552e\u3002': 0.5422}\n        \"\"\"\n        assert text, 'Text has to be non-empty.'\n        return self._send_post_json(self._url + '/extractive_summarization', {\n            'text': text,\n            'language': language or self._language,\n            'topk': topk,\n        })\n\n    def abstractive_summarization(\n            self,\n            text: str,\n            language: str = None,\n    ) -> str:\n        r\"\"\" Abstractive Summarization is the task of generating a short and concise summary that captures the\n        salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that\n        may not appear in the source text.\n\n        Args:\n            text: The text content of the document.\n            language: The language of input text or tokens. ``None`` to use the default language on server.\n\n        Returns:\n            Summarization.\n\n        Examples::\n\n            HanLP.abstractive_summarization('''\n            \u6bcf\u7ecfAI\u5feb\u8baf\uff0c2\u67084\u65e5\uff0c\u957f\u6c5f\u8bc1\u5238\u7814\u7a76\u6240\u91d1\u5c5e\u884c\u4e1a\u9996\u5e2d\u5206\u6790\u5e08\u738b\u9e64\u6d9b\u8868\u793a\uff0c2023\u5e74\u6d77\u5916\u7ecf\u6d4e\u8870\u9000\uff0c\u7f8e\u503a\u73b0\u5904\u4e8e\u5386\u53f2\u9ad8\u4f4d\uff0c\n            \u9ec4\u91d1\u7684\u8d8b\u52bf\u662f\u503c\u5f97\u5173\u6ce8\u7684\uff1b\u5728\u56fd\u5185\u9700\u6c42\u4fee\u590d\u7684\u8fc7\u7a0b\u4e2d\uff0c\u770b\u597d\u5927\u91d1\u5c5e\u54c1\u79cd\u4e2d\u7684\u94dc\u94dd\u94a2\u3002\n            \u6b64\u5916\uff0c\u5728\u7ec6\u5206\u7684\u5c0f\u54c1\u79cd\u91cc\uff0c\u5efa\u8bae\u5173\u6ce8\u4e24\u6761\u4e3b\u7ebf\uff0c\u4e00\u662f\u65b0\u80fd\u6e90\uff0c\u6bd4\u5982\u9502\u3001\u94b4\u3001\u954d\u3001\u7a00\u571f\uff0c\u4e8c\u662f\u4e13\u7cbe\u7279\u65b0\u4e3b\u7ebf\u3002\uff08\u592e\u89c6\u8d22\u7ecf\uff09\n            ''')\n            # Output:\n            '\u957f\u6c5f\u8bc1\u5238\uff1a\u770b\u597d\u5927\u91d1\u5c5e\u54c1\u79cd\u4e2d\u7684\u94dc\u94dd\u94a2'\n        \"\"\"\n        assert text, 'Text has to be non-empty.'\n        return self._send_post_json(self._url + '/abstractive_summarization', {\n            'text': text,\n            'language': language or self._language,\n        })\n\n    def grammatical_error_correction(self, text: Union[str, List[str]], language: str = None) \\\n            -> Union[str, List[str]]:\n        \"\"\" Grammatical Error Correction (GEC) is the task of correcting different kinds of errors in text such as\n        spelling, punctuation, grammatical, and word choice errors.\n\n        Args:\n            text: Text potentially containing different kinds of errors such as spelling, punctuation,\n                grammatical, and word choice errors.\n            language: The language of input text. ``None`` to use the default language.\n\n        Returns:\n            Corrected text.\n\n        Examples::\n\n            HanLP.grammatical_error_correction(['\u6bcf\u4e2a\u9752\u5e74\u90fd\u5e94\u5f53\u6709\u8fdc\u5927\u7684\u62a5\u590d\u3002',\n                                                '\u6709\u7684\u540c\u5b66\u5bf9\u8bed\u8a00\u5f88\u5174\u8da3\u3002'])\n            # Output:\n            [\n                '\u6bcf\u4e2a\u9752\u5e74\u90fd\u5e94\u5f53\u6709\u8fdc\u5927\u7684\u62b1\u8d1f\u3002',\n                '\u6709\u7684\u540c\u5b66\u5bf9\u8bed\u8a00\u5f88\u6709\u5174\u8da3\u3002'\n            ]\n\n        \"\"\"\n        response = self._send_post_json(self._url + '/grammatical_error_correction',\n                                        {'text': text,\n                                         'language': language or self._language})\n        return response\n\n    def text_classification(self, text: Union[str, List[str]], model, topk=False, prob=False) -> Union[\n        str, Dict[str, float], List[Union[str, Dict[str, float]]]]:\n        \"\"\"\n        Text classification is the task of assigning a sentence or document an appropriate category.\n        The categories depend on the chosen dataset and can range from topics.\n\n        Args:\n            text: A document or a list of documents.\n            model: The model to use for prediction.\n            topk: ``True`` or ``int`` to return the top-k labels.\n            prob: Return also probabilities.\n\n        Returns:\n\n            Classification results.\n        \"\"\"\n        response = self._send_post_json(self._url + '/text_classification',\n                                        {'text': text, 'model': model, 'topk': topk, 'prob': prob})\n        return response\n\n    def sentiment_analysis(self, text: Union[str, List[str]], language=None) -> Union[float, List[float]]:\n        r\"\"\"\n        Sentiment analysis is the task of classifying the polarity of a given text. For instance,\n        a text-based tweet can be categorized into either \"positive\", \"negative\", or \"neutral\".\n\n        Args:\n            text: A document or a list of documents.\n            language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.\n                Contact the service provider for the list of languages supported.\n                Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.\n                Leave ``None`` to use the default language on server.\n\n        Returns:\n\n            Sentiment polarity as a numerical value which measures how positive the sentiment is.\n\n        Examples::\n\n            HanLP.language_identification('''\u201c\u8fd9\u662f\u4e00\u90e8\u7537\u4eba\u5fc5\u770b\u7684\u7535\u5f71\u3002\u201d\u4eba\u4eba\u90fd\u8fd9\u4e48\u8bf4\u3002\u4f46\u5355\u7eaf\u4ece\u6027\u522b\u533a\u5206\uff0c\u5c31\u4f1a\u8ba9\u8fd9\u7535\u5f71\u53d8\u72ed\u9698\u3002\n            \u300a\u8096\u7533\u514b\u7684\u6551\u8d4e\u300b\u7a81\u7834\u4e86\u7537\u4eba\u7535\u5f71\u7684\u5c40\u9650\uff0c\u901a\u7bc7\u51e0\u4e4e\u5145\u6ee1\u4ee4\u4eba\u96be\u4ee5\u7f6e\u4fe1\u7684\u6e29\u99a8\u57fa\u8c03\uff0c\u800c\u7535\u5f71\u91cc\u6700\u4f1f\u5927\u7684\u4e3b\u9898\u662f\u201c\u5e0c\u671b\u201d\u3002\n            \u5f53\u6211\u4eec\u65e0\u5948\u5730\u9047\u5230\u4e86\u5982\u540c\u8096\u7533\u514b\u4e00\u822c\u56da\u7981\u4e86\u5fc3\u7075\u81ea\u7531\u7684\u90a3\u79cd\u56f9\u5704\uff0c\u6211\u4eec\u662f\u65e0\u5948\u7684\u8001\u5e03\u9c81\u514b\uff0c\u7070\u5fc3\u7684\u745e\u5fb7\uff0c\u8fd8\u662f\u667a\u6167\u7684\u5b89\u8fea\uff1f\n            \u8fd0\u7528\u667a\u6167\uff0c\u4fe1\u4efb\u5e0c\u671b\uff0c\u5e76\u4e14\u52c7\u6562\u9762\u5bf9\u6050\u60e7\u5fc3\u7406\uff0c\u53bb\u6253\u8d25\u5b83\uff1f\n            \u7ecf\u5178\u7684\u7535\u5f71\u4e4b\u6240\u4ee5\u7ecf\u5178\uff0c\u56e0\u4e3a\u4ed6\u4eec\u90fd\u5728\u505a\u540c\u4e00\u4ef6\u4e8b\u2014\u2014\u8ba9\u4f60\u4ece\u4e0d\u540c\u7684\u89d2\u5ea6\u6765\u6b23\u8d4f\u5e0c\u671b\u7684\u7f8e\u597d\u3002''')\n            0.9505730271339417\n        \"\"\"\n        response = self._send_post_json(self._url + '/sentiment_analysis',\n                                        {'text': text, 'language': language or self._language})\n        return response\n\n    def language_identification(self, text: Union[str, List[str]], topk=False, prob=False) -> Union[\n        str, Dict[str, float], List[Union[str, Dict[str, float]]]]:\n        \"\"\"\n        Identify the language of a given text.\n\n        Args:\n            text: A document or a list of documents.\n            topk: ``True`` or ``int`` to return the top-k languages.\n            prob: Return also probabilities.\n\n        Returns:\n\n            Identified language in `ISO 639-1 codes`_.\n\n        Examples::\n\n            HanLP.language_identification(\n            'In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques.')\n            'en'\n            lang, prob = HanLP.language_identification(\n            '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002', prob=True)\n            ('ja', 0.9976244568824768)\n            HanLP.language_identification(\n            '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002', topk=2)\n            ['zh', 'ja']\n            HanLP.language_identification(\n            '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002', topk=3, prob=True)\n            {'zh': 0.3952908217906952, 'en': 0.37189167737960815, 'ja': 0.056213412433862686}\n\n        .. _ISO 639-1 codes:\n           https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n        \"\"\"\n        return self.text_classification(text, 'lid', topk, prob)\n", "plugins/hanlp_demo/setup.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\nfrom os.path import abspath, join, dirname\nfrom setuptools import find_packages, setup\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\n\nsetup(\n    name='hanlp_demo',\n    version='0.0.1',\n    description='HanLP: Han Language Processing',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/hankcs/HanLP',\n    author='hankcs',\n    author_email='hankcshe@gmail.com',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Developers',\n        \"Development Status :: 3 - Alpha\",\n        'Operating System :: OS Independent',\n        \"License :: OSI Approved :: Apache Software License\",\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        \"Topic :: Text Processing :: Linguistic\"\n    ],\n    keywords='corpus,machine-learning,NLU,NLP',\n    packages=find_packages(exclude=['docs', 'tests*']),\n    include_package_data=True,\n    install_requires=[\n        'hanlp_common'\n    ],\n    python_requires='>=3.6',\n)\n", "plugins/hanlp_demo/hanlp_demo/sent_split.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 14:23\nimport hanlp\n\nsplit_sent = hanlp.load(hanlp.pretrained.eos.UD_CTB_EOS_MUL)\noutput = split_sent('3.14 is pi. \u201c\u4f60\u597d\uff01\uff01\uff01\u201d\u2014\u2014\u4ed6\u8bf4\u3002\u5287\u5834\u7248\u300cFate/stay night [HF]\u300d\u6700\u7d42\u7ae0\u516c\u958b\u30ab\u30a6\u30f3\u30c8\u30c0\u30a6\u30f3\uff01')\nprint('\\n'.join(output))\n# See also https://hanlp.hankcs.com/docs/api/hanlp/components/eos.html\n", "plugins/hanlp_demo/hanlp_demo/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-29 17:48\n\n", "plugins/hanlp_demo/hanlp_demo/block_windows.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-07-28 21:38\nfrom hanlp.utils.io_util import windows\n\nassert not windows(), 'Windows is not supported for this script. Please run it on Linux systems.'\n", "plugins/hanlp_demo/hanlp_demo/mul/demo_mtl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 13:51\nimport hanlp\nfrom hanlp_common.document import Document\n\nHanLP = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)\ndoc: Document = HanLP([\n    'In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environment.',\n    '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002',\n    '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002',\n])\nprint(doc)\ndoc.pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/mul/demo_lid.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-09-28 16:49\nimport hanlp\n\nlid = hanlp.load(hanlp.pretrained.classifiers.LID_176_FASTTEXT_BASE)\n\nprint(lid('In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.'))\nlang, prob = lid('2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002', prob=True)\nprint(f'{lang} language identified with probability {prob:.3%}')\nprint(lid('2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002', topk=2))\n\n# For a combination of languages, predict top-k languages with probabilities:\ntext = '''\n2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002\nIn 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.\n'''\n\nprint(lid(text, topk=3, prob=True))\n", "plugins/hanlp_demo/hanlp_demo/mul/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 22:25\n", "plugins/hanlp_demo/hanlp_demo/mul/demo_lid_restful.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-09-28 16:49\nfrom hanlp_restful import HanLPClient\n\nHanLP = HanLPClient('https://hanlp.hankcs.com/api', auth=None, language='mul')\n\nprint(HanLP.language_identification([\n    'In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environment.',\n    '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002',\n    '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002',\n]))\n", "plugins/hanlp_demo/hanlp_demo/mul/train/mul_base.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-03 14:24\nfrom hanlp.common.dataset import SortingSamplerBuilder\nfrom hanlp.common.transform import NormalizeToken\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization\nfrom hanlp.components.mtl.tasks.ud import UniversalDependenciesParsing\nfrom hanlp.datasets.parsing.ptb import PTB_TOKEN_MAPPING\nfrom hanlp.datasets.parsing.ud.ud210m import UD_210_MULTILINGUAL_TRAIN, UD_210_MULTILINGUAL_DEV, \\\n    UD_210_MULTILINGUAL_TEST\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding\nfrom hanlp.utils.log_util import cprint\nfrom tests import cdroot\n\n\ndef main():\n    cdroot()\n    transformer = \"nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large\"\n    tasks = {\n        'tok': TaggingTokenization(\n            'data/mtl/mul/tok/train.tsv',\n            'data/mtl/mul/tok/dev.tsv',\n            'data/mtl/mul/tok/test.tsv',\n            SortingSamplerBuilder(batch_size=128, batch_max_tokens=12800),\n            hard_constraint=True,\n            tagging_scheme='BMES',\n            delimiter='\\t',\n            max_seq_len=256,\n            char_level=True,\n            lr=1e-3,\n        ),\n        'ud': UniversalDependenciesParsing(\n            UD_210_MULTILINGUAL_TRAIN,\n            UD_210_MULTILINGUAL_DEV,\n            UD_210_MULTILINGUAL_TEST,\n            SortingSamplerBuilder(batch_size=128, batch_max_tokens=12800),\n            lr=1e-3,\n            dependencies='tok',\n            max_seq_len=256,\n        ),\n    }\n    mtl = MultiTaskLearning()\n    save_dir = 'data/model/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12'\n    cprint(f'Model will be saved in [cyan]{save_dir}[/cyan]')\n    mtl.fit(\n        ContextualWordEmbedding(\n            'token',\n            transformer,\n            average_subwords=True,\n            max_sequence_length=512,\n            word_dropout=.2,\n        ),\n        tasks,\n        save_dir,\n        30,\n        lr=1e-3,\n        encoder_lr=5e-5,\n        grad_norm=1,\n        gradient_accumulation=8,\n        eval_trn=False,\n        transform=NormalizeToken(PTB_TOKEN_MAPPING, 'token'),\n        tau=0.5,\n        cache='data/cache/ud/mtl',\n    )\n    cprint(f'Model saved in [cyan]{save_dir}[/cyan]')\n    mtl.load(save_dir)\n    mtl['tok'].dict_force = {\"'s\", \"n't\", \"'ll\", \"'m\", \"'d\", \"'ve\", \"'re\"}\n    mtl['ud'].config.tree = True\n    mtl.save_config(save_dir)\n    for k, v in mtl.tasks.items():\n        v.trn = tasks[k].trn\n        v.dev = tasks[k].dev\n        v.tst = tasks[k].tst\n    mtl.evaluate(save_dir)\n    doc = mtl(['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.',\n               '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002',\n               '2021\u5e74 HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002'])\n    doc.pretty_print()\n\n\nif __name__ == '__main__':\n    main()\n", "plugins/hanlp_demo/hanlp_demo/mul/train/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2023-02-21 19:40\n", "plugins/hanlp_demo/hanlp_demo/ja/demo_mtl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-17 22:30\nimport hanlp\nfrom hanlp_common.document import Document\n\nHanLP = hanlp.load(hanlp.pretrained.mtl.NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA)\ndoc: Document = HanLP([\n    '2021\u5e74\u3001HanLPv2.1\u306f\u6b21\u4e16\u4ee3\u306e\u6700\u5148\u7aef\u591a\u8a00\u8a9eNLP\u6280\u8853\u3092\u672c\u756a\u74b0\u5883\u306b\u5c0e\u5165\u3057\u307e\u3059\u3002',\n    '\u5948\u9808\u304d\u306e\u3053\u306f1973\u5e7411\u670828\u65e5\u306b\u5343\u8449\u770c\u5186\u7a7a\u5c71\u3067\u751f\u307e\u308c\u3001\u30b2\u30fc\u30e0\u5236\u4f5c\u4f1a\u793e\u300c\u30ce\u30fc\u30c4\u300d\u306e\u8a2d\u7acb\u8005\u3060\u3002',\n])\nprint(doc)\ndoc.pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/ja/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-17 22:30\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_mtl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 13:51\nimport hanlp\nfrom hanlp_common.document import Document\n\n# CLOSE\u662f\u81ea\u7136\u8bed\u4e49\u6807\u6ce8\u7684\u95ed\u6e90\u8bed\u6599\u5e93\uff0cBASE\u662f\u4e2d\u53f7\u6a21\u578b\uff0cZH\u4e2d\u6587\nHanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)\n# \u9ed8\u8ba4\u6267\u884c\u5168\u90e8\u4efb\u52a1\ndoc: Document = HanLP(['2021\u5e74HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002', '\u963f\u5a46\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8\u3002'])\n# \u8fd4\u56de\u7c7b\u578bDocument\u662fdict\u7684\u5b50\u7c7b\uff0c\u6253\u5370\u51fa\u6765\u517c\u5bb9JSON\nprint(doc)\n# \u5373\u65f6\u53ef\u89c6\u5316\uff0c\u9632\u6b62\u6362\u884c\u8bf7\u6700\u5927\u5316\u7a97\u53e3\uff0c\u63a8\u8350\u5728Jupyter Notebook\u91cc\u8c03\u7528\ndoc.pretty_print()\n# \u6307\u5b9a\u53ef\u89c6\u5316OntoNotes\u6807\u51c6\u7684NER\n# doc.pretty_print(ner='ner/ontonotes', pos='pku')\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_del_tasks.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-02-03 13:28\nimport hanlp\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp_common.document import Document\n\nHanLP: MultiTaskLearning = hanlp.load(hanlp.pretrained.mtl.OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\ntasks = list(HanLP.tasks.keys())\nprint(tasks)  # Pick what you need from what we have\nfor task in tasks:\n    if task not in ('tok', 'pos'):\n        del HanLP[task]\n# You can save it as a new component\n# HanLP.save('path/to/new/component')\n# HanLP.load('path/to/new/component')\nprint(HanLP.tasks.keys())\ndoc: Document = HanLP(['2021\u5e74HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002', 'up\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8\u3002'])\nprint(doc)\ndoc.pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_pos_dict.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-15 22:26\nimport hanlp\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp.components.mtl.tasks.pos import TransformerTagging\nfrom hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization\nfrom tests import cdroot\n\ncdroot()\nHanLP: MultiTaskLearning = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n\n# Demonstrates custom dict in part-of-speech tagging\npos: TransformerTagging = HanLP['pos/ctb']\n\nprint(f'\u81ea\u5b9a\u4e49\u5355\u4e2a\u8bcd\u6027:')\npos.dict_tags = {'HanLP': 'state-of-the-art-tool'}\nHanLP(\"HanLP\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002\", tasks='pos/ctb').pretty_print()\n\nprint(f'\u6839\u636e\u4e0a\u4e0b\u6587\u81ea\u5b9a\u4e49\u8bcd\u6027:')\npos.dict_tags = {('\u7684', '\u5e0c\u671b'): ('\u8865\u8bed\u6210\u5206', '\u540d\u8bcd'), '\u5e0c\u671b': '\u52a8\u8bcd'}\nHanLP(\"\u6211\u7684\u5e0c\u671b\u662f\u5e0c\u671b\u5f20\u665a\u971e\u7684\u80cc\u5f71\u88ab\u665a\u971e\u6620\u7ea2\u3002\", tasks='pos/ctb').pretty_print()\n\n# \u9700\u8981\u7b97\u6cd5\u57fa\u7840\u624d\u80fd\u7406\u89e3\uff0c\u521d\u5b66\u8005\u53ef\u53c2\u8003 http://nlp.hankcs.com/book.php\n# See also https://hanlp.hankcs.com/docs/api/hanlp/components/taggers/transformer_tagger.html\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_amr.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-04-12 22:19\nimport hanlp\n\nparser = hanlp.load(hanlp.pretrained.amr.MRP2020_AMR_ENG_ZHO_XLM_BASE)\n\n# For Chinese:\nprint(parser([\"\u7537\u5b69\", \"\u5e0c\u671b\", \"\u5973\u5b69\", \"\u76f8\u4fe1\", \"\u4ed6\", \"\u3002\"]))\nprint(parser([\"\u7537\u5b69\", \"\u5e0c\u671b\", \"\u5973\u5b69\", \"\u76f8\u4fe1\", \"\u4ed6\", \"\u3002\"], output_amr=False))\n\n# For English:\nprint(parser(['The', 'boy', 'wants', 'the', 'girl', 'to', 'believe', 'him', '.'], language='eng'))\n# It's suggested to also feed the lemma for stabler performance.\nprint(parser([('The', 'the'), ('boy', 'boy'), ('wants', 'want'), ('the', 'the'), ('girl', 'girl'), ('to', 'to'),\n              ('believe', 'believe'), ('him', 'he'), ('.', '.')], language='eng'))\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_custom_dict.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-15 22:26\nimport hanlp\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization\n\n# \u52a0\u8f7d\u591a\u4efb\u52a1\u6a21\u578b\nHanLP: MultiTaskLearning = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n# \u83b7\u53d6\u5206\u8bcd\u4efb\u52a1\uff08\u4ee5tok\u5f00\u5934\u7684\u4efb\u52a1\u90fd\u662f\u5206\u8bcd\u4efb\u52a1\uff0c\u4ee5\u7ec6\u5206\u6807\u51c6\u4e3a\u4f8b\uff09\ntok: TaggingTokenization = HanLP['tok/fine']\n\ntok.dict_force = tok.dict_combine = None\nprint(f'\u4e0d\u6302\u8bcd\u5178:\\n{HanLP(\"\u5546\u54c1\u548c\u670d\u52a1\u9879\u76ee\")[\"tok/fine\"]}')\n\ntok.dict_force = {'\u548c\u670d', '\u670d\u52a1\u9879\u76ee'}\nprint(f'\u5f3a\u5236\u6a21\u5f0f:\\n{HanLP(\"\u5546\u54c1\u548c\u670d\u52a1\u9879\u76ee\")[\"tok/fine\"]}')  # \u614e\u7528\uff0c\u8be6\u89c1\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u7b2c\u4e8c\u7ae0\n\ntok.dict_force = {'\u548c\u670d\u52a1': ['\u548c', '\u670d\u52a1']}\nprint(f'\u5f3a\u5236\u6821\u6b63:\\n{HanLP(\"\u6b63\u5411\u5339\u914d\u5546\u54c1\u548c\u670d\u52a1\u3001\u4efb\u4f55\u548c\u670d\u52a1\u5fc5\u6309\u4e0a\u8ff0\u5207\u5206\")[\"tok/fine\"]}')\n\ntok.dict_force = None\ntok.dict_combine = {'\u548c\u670d', '\u670d\u52a1\u9879\u76ee'}\nprint(f'\u5408\u5e76\u6a21\u5f0f:\\n{HanLP(\"\u5546\u54c1\u548c\u670d\u52a1\u9879\u76ee\")[\"tok/fine\"]}')\n\n# \u9700\u8981\u7b97\u6cd5\u57fa\u7840\u624d\u80fd\u7406\u89e3\uff0c\u521d\u5b66\u8005\u53ef\u53c2\u8003 http://nlp.hankcs.com/book.php\n# See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html\n\n# \u542b\u6709\u7a7a\u683c\u3001\u5236\u8868\u7b26\u7b49\uff08Transformer tokenizer\u53bb\u6389\u7684\u5b57\u7b26\uff09\u7684\u8bcd\u8bed\u9700\u8981\u7528tuple\u7684\u5f62\u5f0f\u63d0\u4f9b\ntok.dict_combine = {('iPad', 'Pro'), '2\u4e2a\u7a7a\u683c'}\nprint(f'\u7a7a\u683c\u5339\u914d\uff1a\\n{HanLP(\"\u5982\u4f55\u8bc4\u4ef7iPad Pro \uff1fiPad  Pro\u67092\u4e2a\u7a7a\u683c\", tasks=\"tok/fine\")[\"tok/fine\"]}')\n# \u806a\u660e\u7684\u7528\u6237\u8bf7\u7ee7\u7eed\u9605\u8bfb\uff1atuple\u8bcd\u5178\u4e2d\u7684\u5b57\u7b26\u4e32\u5176\u5b9e\u7b49\u4ef7\u4e8e\u8be5\u5b57\u7b26\u4e32\u7684\u6240\u6709\u53ef\u80fd\u7684\u5207\u5206\u65b9\u5f0f\nprint(f'\u8bcd\u5178\u5185\u5bb9\uff1a\\n{dict(tok.dict_combine.config[\"dictionary\"]).keys()}')\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_parse_constituency.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-18 11:09\nfrom hanlp_common.document import Document\nimport hanlp\n\ncon = hanlp.load(hanlp.pretrained.constituency.CTB9_CON_FULL_TAG_ELECTRA_SMALL)\n# To speed up, parse multiple sentences at once, and use a GPU.\nprint(con([\"2021\u5e74\", \"HanLPv2.1\", \"\u5e26\u6765\", \"\u6700\", \"\u5148\u8fdb\", \"\u7684\", \"\u591a\", \"\u8bed\u79cd\", \"NLP\", \"\u6280\u672f\", \"\u3002\"]))\n\n\n# The rest of this tutorial is written for clever users.\n# The first level of non-terminals are PoS tags. So usually a PoS model is piped.\ndef merge_pos_into_con(doc: Document):\n    flat = isinstance(doc['pos'][0], str)\n    if flat:\n        doc = Document((k, [v]) for k, v in doc.items())\n    for tree, tags in zip(doc['con'], doc['pos']):\n        offset = 0\n        for subtree in tree.subtrees(lambda t: t.height() == 2):\n            tag = subtree.label()\n            if tag == '_':\n                subtree.set_label(tags[offset])\n            offset += 1\n    if flat:\n        doc = doc.squeeze()\n    return doc\n\n\npos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)\nnlp = hanlp.pipeline() \\\n    .append(pos, input_key='tok', output_key='pos') \\\n    .append(con, input_key='tok', output_key='con') \\\n    .append(merge_pos_into_con, input_key='*')\nprint(f'The pipeline looks like this: {nlp}')\ndoc = nlp(tok=[\"2021\u5e74\", \"HanLPv2.1\", \"\u5e26\u6765\", \"\u6700\", \"\u5148\u8fdb\", \"\u7684\", \"\u591a\", \"\u8bed\u79cd\", \"NLP\", \"\u6280\u672f\", \"\u3002\"])\nprint(doc)\ndoc.pretty_print()\n\n# If you need to parse raw text, simply add a tokenizer into this pipeline.\ntok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\nnlp.insert(0, tok, output_key='tok')\nprint(f'The pipeline looks like this: {nlp}')\ndoc = nlp('2021\u5e74HanLPv2.1\u5e26\u6765\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002')\nprint(doc)\ndoc.pretty_print()\n\n# ATTENTION: Pipelines are usually slower than MTL but they are more flexible.\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_ner_dict.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-04-29 11:06\nimport hanlp\nfrom hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\nfrom hanlp.utils.io_util import get_resource\n\nHanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)\nner: TaggingNamedEntityRecognition = HanLP['ner/msra']\nner.dict_whitelist = {'\u5348\u996d\u540e': 'TIME'}\ndoc = HanLP('2021\u5e74\u6d4b\u8bd5\u9ad8\u8840\u538b\u662f138\uff0c\u65f6\u95f4\u662f\u5348\u996d\u540e2\u70b945\uff0c\u4f4e\u8840\u538b\u662f44', tasks='ner/msra')\ndoc.pretty_print()\nprint(doc['ner/msra'])\n\nner.dict_tags = {('\u540d\u5b57', '\u53eb', '\u91d1\u534e'): ('O', 'O', 'S-PERSON')}\nHanLP('\u4ed6\u5728\u6d59\u6c5f\u91d1\u534e\u51fa\u751f\uff0c\u4ed6\u7684\u540d\u5b57\u53eb\u91d1\u534e\u3002', tasks='ner/msra').pretty_print()\n\n# HanLP.save(get_resource(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH))\n\n# \u9700\u8981\u7b97\u6cd5\u57fa\u7840\u624d\u80fd\u7406\u89e3\uff0c\u521d\u5b66\u8005\u53ef\u53c2\u8003 http://nlp.hankcs.com/book.php\n# See https://hanlp.hankcs.com/docs/api/hanlp/components/mtl/tasks/ner/tag_ner.html\n", "plugins/hanlp_demo/hanlp_demo/zh/train_sota_bert_pku.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 02:47\nfrom hanlp.common.dataset import SortingSamplerBuilder\nfrom hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\nfrom hanlp.datasets.tokenization.sighan2005.pku import SIGHAN2005_PKU_TRAIN_ALL, SIGHAN2005_PKU_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTaggingTokenizer()\nsave_dir = 'data/model/cws/sighan2005_pku_bert_base_96.7'\ntokenizer.fit(\n    SIGHAN2005_PKU_TRAIN_ALL,\n    SIGHAN2005_PKU_TEST,  # Conventionally, no devset is used. See Tian et al. (2020).\n    save_dir,\n    'bert-base-chinese',\n    max_seq_len=300,\n    char_level=True,\n    hard_constraint=True,\n    sampler_builder=SortingSamplerBuilder(batch_size=32),\n    epochs=3,\n    adam_epsilon=1e-6,\n    warmup_steps=0.1,\n    weight_decay=0.01,\n    word_dropout=0.1,\n    seed=1660853059,\n)\ntokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_sts.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-24 13:15\nimport hanlp\n\nsim = hanlp.load(hanlp.pretrained.sts.STS_ELECTRA_BASE_ZH)\nprint(sim([\n    ['\u770b\u56fe\u731c\u4e00\u7535\u5f71\u540d', '\u770b\u56fe\u731c\u7535\u5f71'],\n    ['\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u65e0\u7ebf\u4e0a\u7f51', '\u65e0\u7ebf\u4e0a\u7f51\u5361\u548c\u65e0\u7ebf\u8def\u7531\u5668\u600e\u4e48\u7528'],\n    ['\u5317\u4eac\u5230\u4e0a\u6d77\u7684\u52a8\u8f66\u7968', '\u4e0a\u6d77\u5230\u5317\u4eac\u7684\u52a8\u8f66\u7968'],\n]))\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_custom_dict_stl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-15 22:26\nimport hanlp\nfrom hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\n\n# \u52a0\u8f7d\u4e00\u4e2a\u65e7\u7248\u672c\u5355\u4efb\u52a1\u6a21\u578b\u6f14\u793a\u5206\u8bcd\u9519\u8bef\uff08\u6700\u65b0\u7248\u5df2\u7ecf\u4fee\u590d\uff09\uff1a\ntok: TransformerTaggingTokenizer = hanlp.load('https://file.hankcs.com/hanlp/tok/coarse_electra_small_20220220_013548.zip')\n\ntok.dict_force = tok.dict_combine = None\nprint(f'\u4e0d\u6302\u8bcd\u5178:\\n{tok(\"\u9996\u76f8\u548c\u5ddd\u666e\u901a\u7535\u8bdd\")}')\n\ntok.dict_force = {'\u5ddd\u666e'}\nprint(f'\u5f3a\u5236\u6a21\u5f0f:\\n{tok([\"\u9996\u76f8\u548c\u5ddd\u666e\u901a\u7535\u8bdd\", \"\u94f6\u5ddd\u666e\u901a\u4eba\u4e0e\u5ddd\u666e\u901a\u7535\u8bdd\u8bb2\u56db\u5ddd\u666e\u901a\u8bdd\"])}')  # \u614e\u7528\uff0c\u8be6\u89c1\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u7b2c\u4e8c\u7ae0\n\ntok.dict_force = {'\u5ddd\u666e\u901a\u7535\u8bdd': ['\u5ddd\u666e', '\u901a', '\u7535\u8bdd']}\nprint(f'\u5f3a\u5236\u6821\u6b63:\\n{tok([\"\u9996\u76f8\u548c\u5ddd\u666e\u901a\u7535\u8bdd\", \"\u94f6\u5ddd\u666e\u901a\u4eba\u4e0e\u5ddd\u666e\u901a\u7535\u8bdd\u8bb2\u56db\u5ddd\u666e\u901a\u8bdd\"])}')\n\ntok.dict_force = None\ntok.dict_combine = {'\u7f8e\u56fd\u603b\u7edf'}\nprint(f'\u5408\u5e76\u6a21\u5f0f:\\n{tok(\"\u9996\u76f8\u548c\u5ddd\u666e\u901a\u7535\u8bdd\uff0c\u5ddd\u666e\u662f\u7f8e\u56fd\u603b\u7edf\u3002\")}')\n\n# \u9700\u8981\u7b97\u6cd5\u57fa\u7840\u624d\u80fd\u7406\u89e3\uff0c\u521d\u5b66\u8005\u53ef\u53c2\u8003 http://nlp.hankcs.com/book.php\n# See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_mlm.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-29 21:11\nfrom hanlp.components.lm.mlm import MaskedLanguageModel\n\nmlm = MaskedLanguageModel()\nmlm.load('bert-base-chinese')\nprint(mlm('\u751f\u6d3b\u7684\u771f\u8c1b\u662f[MASK]\u3002'))\n\n# Batching is always faster\nprint(mlm(['\u751f\u6d3b\u7684\u771f\u8c1b\u662f[MASK]\u3002', '\u5df4\u9ece\u662f[MASK][MASK]\u7684\u9996\u90fd\u3002']))\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_word2vec.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-12-12 18:33\nimport hanlp\nimport torch\n\nword2vec = hanlp.load(hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_PKU)\nvec = word2vec('\u5148\u8fdb')\nprint(vec)\n\nprint(torch.nn.functional.cosine_similarity(word2vec('\u5148\u8fdb'), word2vec('\u4f18\u79c0'), dim=0))\nprint(torch.nn.functional.cosine_similarity(word2vec('\u5148\u8fdb'), word2vec('\u6c34\u679c'), dim=0))\n\nprint('\u83b7\u53d6\u8bed\u4e49\u6700\u76f8\u4f3c\u7684\u8bcd\u8bed\uff1a')\nprint(word2vec.most_similar('\u4e0a\u6d77'))\n# print(word2vec.most_similar(['\u4e0a\u6d77', '\u5bd2\u51b7'])) # batching\u66f4\u5feb\n\nprint('\u975e\u5e38\u5bd2\u51b7\u662fOOV\u6240\u4ee5\u65e0\u6cd5\u83b7\u53d6\uff1a')\nprint(word2vec.most_similar('\u975e\u5e38\u5bd2\u51b7'))\nprint('\u4f46\u662f\u5728doc2vec\u6a21\u5f0f\u4e0bOOV\u4e5f\u53ef\u4ee5\u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff1a')\nprint(word2vec.most_similar('\u975e\u5e38\u5bd2\u51b7', doc2vec=True))\nprint('\u751a\u81f3\u53ef\u4ee5\u5904\u7406\u77ed\u6587\u672c\uff1a')\nprint(word2vec.most_similar('\u56fd\u5bb6\u56fe\u4e66\u9986\u63a8\u51fa2022\u5e74\u6625\u8282\u4e3b\u9898\u6d3b\u52a8', doc2vec=True))\n", "plugins/hanlp_demo/hanlp_demo/zh/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 13:51\n", "plugins/hanlp_demo/hanlp_demo/zh/demo_pipeline.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-12-28 20:47\nimport hanlp\n\n# Pipeline allows blending multiple callable functions no matter they are a rule, a TensorFlow component or a PyTorch\n# one. However, it's slower than the MTL framework.\n# pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ALBERT_BASE)  # In case both tf and torch are used, load tf first.\n\nHanLP = hanlp.pipeline() \\\n    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n    .append(hanlp.load('CTB9_TOK_ELECTRA_SMALL'), output_key='tok') \\\n    .append(hanlp.load('CTB9_POS_ELECTRA_SMALL'), output_key='pos') \\\n    .append(hanlp.load('MSRA_NER_ELECTRA_SMALL_ZH'), output_key='ner', input_key='tok') \\\n    .append(hanlp.load('CTB9_DEP_ELECTRA_SMALL', conll=False), output_key='dep', input_key='tok') \\\n    .append(hanlp.load('CTB9_CON_ELECTRA_SMALL'), output_key='con', input_key='tok')\n\ndoc = HanLP('2021\u5e74HanLPv2.1\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002\u963f\u5a46\u4e3b\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8\u3002')\nprint(doc)\ndoc.pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 21:25\nimport hanlp\n\nsyntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)\nsent = [('\u8721\u70db', 'NN'), ('\u4e24', 'CD'), ('\u5934', 'NN'), ('\u70e7', 'VV')]\ntree = syntactic_parser(sent)\nprint(tree)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_client.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-08 04:43\n# pip3 install tensorflow-serving-api-gpu\nimport grpc\nimport tensorflow as tf\nfrom tensorflow_core.python.framework import tensor_util\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\nimport hanlp\nfrom hanlp.common.keras_component import KerasComponent\n\ntagger: KerasComponent = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN, transform_only=True)\ntransform = tagger.transform\ndel tagger\n\ninputs = [['\u5546\u54c1', '\u548c', '\u670d\u52a1'],\n          ['\u6211', '\u7684', '\u5e0c\u671b', '\u662f', '\u5e0c\u671b', '\u548c\u5e73']]\n\nsamples = next(iter(transform.inputs_to_dataset(inputs)))[0]\nprint(samples)\n\nchannel = grpc.insecure_channel('{host}:{port}'.format(host='localhost', port=8500))\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'ctb5_pos_rnn_20191229_015325'\nrequest.model_spec.signature_name = 'serving_default'\nrequest.inputs['embedding_input'].CopyFrom(\n    tf.make_tensor_proto(samples, dtype=tf.float32))\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\nprint(result)\nprediction = tensor_util.MakeNdarray(result.outputs['dense'])\nprint(prediction)\n\nprint(list(transform.Y_to_outputs(prediction, inputs=inputs)))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-30 19:52\nimport hanlp\n\nrecognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\nprint(recognizer.predict([list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002'),\n                          list('\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002')]))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_classifier.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 03:52\nfrom hanlp.datasets.classification.sentiment import CHNSENTICORP_ERNIE_TEST\n\nimport hanlp\n\nclassifier = hanlp.load('CHNSENTICORP_BERT_BASE_ZH')\nprint(classifier.predict('\u524d\u53f0\u5ba2\u623f\u670d\u52a1\u6001\u5ea6\u975e\u5e38\u597d\uff01\u65e9\u9910\u5f88\u4e30\u5bcc\uff0c\u623f\u4ef7\u5f88\u5e72\u51c0\u3002\u518d\u63a5\u518d\u5389\uff01'))\n\n# predict a whole file in batch mode\noutputs = classifier.predict(classifier.transform.file_to_inputs(CHNSENTICORP_ERNIE_TEST), gold=True)\nprint(outputs[:5])\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 21:25\nimport hanlp\n\ntokenizer = hanlp.load(hanlp.pretrained.tok.LARGE_ALBERT_BASE)\nprint(tokenizer('\u5546\u54c1\u548c\u670d\u52a1'))\nprint(tokenizer(['\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002',\n                 '\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002',\n                 'HanLP\u652f\u63f4\u81fa\u7063\u6b63\u9ad4\u3001\u9999\u6e2f\u7e41\u9ad4\uff0c\u5177\u6709\u65b0\u8a5e\u8fa8\u8b58\u80fd\u529b\u7684\u4e2d\u6587\u65b7\u8a5e\u7cfb\u7d71']))\n\ntext = 'NLP\u7edf\u8ba1\u6a21\u578b\u6ca1\u6709\u52a0\u89c4\u5219\uff0c\u806a\u660e\u4eba\u77e5\u9053\u81ea\u5df1\u52a0\u3002\u82f1\u6587\u3001\u6570\u5b57\u3001\u81ea\u5b9a\u4e49\u8bcd\u5178\u7edf\u7edf\u90fd\u662f\u89c4\u5219\u3002'\nprint(tokenizer(text))\n\ndic = {'\u81ea\u5b9a\u4e49\u8bcd\u5178': 'custom_dict', '\u806a\u660e\u4eba': 'smart'}\n\n\ndef split_by_dic(text: str):\n    # We use regular expression for the sake of simplicity.\n    # However, you should use some trie trees for production\n    import re\n    p = re.compile('(' + '|'.join(dic.keys()) + ')')\n    sents, offset, words = [], 0, []\n    for m in p.finditer(text):\n        if offset < m.start():\n            sents.append(text[offset: m.start()])\n            words.append((m.group(), dic[m.group()]))\n            offset = m.end()\n    if offset < len(text):\n        sents.append(text[offset:])\n        words.append((None, None))\n    flat = []\n    for pred, (word, tag) in zip(tokenizer(sents), words):\n        flat.extend(pred)\n        if word:\n            flat.append((word, tag))\n    return flat\n\n\nprint(split_by_dic(text))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_multiprocess.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-02-15 11:30\nimport multiprocessing\nimport hanlp\n\ntokenizer = hanlp.load(hanlp.pretrained.tok.LARGE_ALBERT_BASE)\n\n\ndef worker(job):\n    print(job)\n    print(tokenizer(job))\n\n\nif __name__ == '__main__':\n    num_proc = 2\n    # Important! The python multiprocessing package defaults to just call fork when creating a child process.\n    # This cannot work when the child process calls async code (i.e TensorFlow is multithreaded).\n    # See https://github.com/tensorflow/tensorflow/issues/8220#issuecomment-302826884\n    # See https://sefiks.com/2019/03/20/tips-and-tricks-for-gpu-and-multiprocessing-in-tensorflow/\n    multiprocessing.set_start_method('spawn', force=True)  # only spawn works with TensorFlow\n    with multiprocessing.Pool(num_proc) as pool:\n        pool.map(worker, [f'\u7ed9{i}\u53f7\u8fdb\u7a0b\u7684\u4efb\u52a1' for i in range(num_proc)])\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_fasttext.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-12-12 18:33\nimport hanlp\nimport torch\n\n# fasttext is a `torch.nn.Module`. Unless you know how to code in\n# PyTorch, otherwise don't bother to use this.\nfasttext = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_WIKI_300_ZH)\n\nvec = fasttext('\u5355\u8bcd')\nprint(vec)\n\nprint(torch.nn.functional.cosine_similarity(fasttext('\u5355\u8bcd'), fasttext('\u8bcd\u8bed'), dim=0))\nprint(torch.nn.functional.cosine_similarity(fasttext('\u5355\u8bcd'), fasttext('\u4eca\u5929'), dim=0))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_serving.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-06 20:23\nimport hanlp\nfrom hanlp.common.keras_component import KerasComponent\n\ntagger: KerasComponent = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN)\nprint(tagger('\u5546\u54c1 \u548c \u670d\u52a1'.split()))\ntagger.serve()\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_cws_trie.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 21:25\nfrom hanlp_trie.trie import Trie\n\nimport hanlp\n\ntokenizer = hanlp.load('LARGE_ALBERT_BASE')\ntext = 'NLP\u7edf\u8ba1\u6a21\u578b\u6ca1\u6709\u52a0\u89c4\u5219\uff0c\u806a\u660e\u4eba\u77e5\u9053\u81ea\u5df1\u52a0\u3002\u82f1\u6587\u3001\u6570\u5b57\u3001\u81ea\u5b9a\u4e49\u8bcd\u5178\u7edf\u7edf\u90fd\u662f\u89c4\u5219\u3002'\nprint(tokenizer(text))\n\ntrie = Trie()\ntrie.update({'\u81ea\u5b9a\u4e49\u8bcd\u5178': 'custom_dict', '\u806a\u660e\u4eba': 'smart'})\n\n\ndef split_sents(text: str, trie: Trie):\n    words = trie.parse_longest(text)\n    sents = []\n    pre_start = 0\n    offsets = []\n    for start, end, value in words:\n        if pre_start != start:\n            sents.append(text[pre_start: start])\n            offsets.append(pre_start)\n        pre_start = end\n    if pre_start != len(text):\n        sents.append(text[pre_start:])\n        offsets.append(pre_start)\n    return sents, offsets, words\n\n\nprint(split_sents(text, trie))\n\n\ndef merge_parts(parts, offsets, words):\n    items = [(i, p) for (i, p) in zip(offsets, parts)]\n    items += [(start, [value]) for (start, end, value) in words]\n    return [each for x in sorted(items) for each in x[1]]\n\n\ntokenizer = hanlp.pipeline() \\\n    .append(split_sents, output_key=('parts', 'offsets', 'words'), trie=trie) \\\n    .append(tokenizer, input_key='parts', output_key='tokens') \\\n    .append(merge_parts, input_key=('tokens', 'offsets', 'words'), output_key='merged')\n\nprint(tokenizer(text))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 20:36\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_sdp.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 23:55\nimport hanlp\n\nsemantic_parser = hanlp.load('SEMEVAL16_NEWS_BIAFFINE_ZH')\nsent = [('\u8721\u70db', 'NN'), ('\u4e24', 'CD'), ('\u5934', 'NN'), ('\u70e7', 'VV')]\nprint(semantic_parser(sent))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_pipeline.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 03:24\n\nimport hanlp\n\ntokenizer = hanlp.load('LARGE_ALBERT_BASE')\ntagger = hanlp.load('CTB9_POS_ALBERT_BASE')\nsyntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')\nsemantic_parser = hanlp.load('SEMEVAL16_TEXT_BIAFFINE_ZH')\n\npipeline = hanlp.pipeline() \\\n    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n    .append(tokenizer, output_key='tokens') \\\n    .append(tagger, output_key='part_of_speech_tags') \\\n    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies', conll=False) \\\n    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies', conll=False)\nprint(pipeline)\n\ntext = '''HanLP\u662f\u4e00\u7cfb\u5217\u6a21\u578b\u4e0e\u7b97\u6cd5\u7ec4\u6210\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5305\uff0c\u76ee\u6807\u662f\u666e\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\nHanLP\u5177\u5907\u529f\u80fd\u5b8c\u5584\u3001\u6027\u80fd\u9ad8\u6548\u3001\u67b6\u6784\u6e05\u6670\u3001\u8bed\u6599\u65f6\u65b0\u3001\u53ef\u81ea\u5b9a\u4e49\u7684\u7279\u70b9\u3002\n\u5185\u90e8\u7b97\u6cd5\u7ecf\u8fc7\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u8003\u9a8c\uff0c\u914d\u5957\u4e66\u7c4d\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u5df2\u7ecf\u51fa\u7248\u3002\n'''\n\ndoc = pipeline(text)\nprint(doc)\n# By default the doc is json serializable, it holds true if your pipes output json serializable object too.\n# print(json.dumps(doc, ensure_ascii=False, indent=2))\n\n# You can save the config to disk for deploying or sharing.\npipeline.save('zh.json')\n# Then load it smoothly.\ndeployed = hanlp.load('zh.json')\nprint(deployed)\nprint(deployed(text))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/demo_pos.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 21:25\nimport hanlp\nfrom hanlp.pretrained.pos import CTB9_POS_ALBERT_BASE\n\ntagger = hanlp.load(CTB9_POS_ALBERT_BASE)\nprint(tagger.predict(['\u6211', '\u7684', '\u5e0c\u671b', '\u662f', '\u5e0c\u671b', '\u4e16\u754c', '\u548c\u5e73']))\nprint(tagger.predict([['\u652f\u6301', '\u6279\u5904\u7406', '\u5730', '\u9884\u6d4b'], ['\u901f\u5ea6', '\u66f4', '\u5feb']]))\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_biaffine_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_bert_1e-5'\nparser = BiaffineTransformerDependencyParserTF()\n# parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n#            batch_size=3000,\n#            warmup_steps_ratio=.1,\n#            token_mapping=PTB_TOKEN_MAPPING,\n#            samples_per_batch=150,\n#            transformer_dropout=.33,\n#            learning_rate=2e-3,\n#            learning_rate_transformer=1e-5,\n#            # early_stopping_patience=10,\n#            )\nparser.load(save_dir, tree='tarjan')\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_semeval15_pas.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 18:26\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineSemanticDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_6B_100D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/sdp/semeval15_biaffine_pas'\nparser = BiaffineSemanticDependencyParserTF()\nparser.fit('data/semeval15/en.pas.train.conll', 'data/semeval15/en.pas.dev.conll', save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': GLOVE_6B_100D,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)  # disable variational dropout during evaluation so as to use CudaLSTM\nsentence = [('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('chamber', 'NN'),\n            ('music', 'NN'), ('?', '.')]\nprint(parser.predict(sentence))\nparser.evaluate('data/semeval15/en.id.pas.conll', save_dir)\nparser.evaluate('data/semeval15/en.ood.pas.conll', save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_semeval16_text.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 23:20\nfrom hanlp.datasets.parsing.semeval16 import SEMEVAL2016_TEXT_TRAIN, SEMEVAL2016_TEXT_DEV, SEMEVAL2016_TEXT_TEST\nfrom hanlp.pretrained.word2vec import SEMEVAL16_EMBEDDINGS_300_TEXT_CN\nfrom hanlp.utils.tf_util import nice\n\nnice()\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineSemanticDependencyParserTF\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/sdp/semeval16-text'\nparser = BiaffineSemanticDependencyParserTF()\nparser.fit(SEMEVAL2016_TEXT_TRAIN, SEMEVAL2016_TEXT_DEV, save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': SEMEVAL16_EMBEDDINGS_300_TEXT_CN,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)\nsentence = [('\u4e2d\u56fd', 'NR'), ('\u6279\u51c6', 'VV'), ('\u8bbe\u7acb', 'VV'), ('\u5916\u5546', 'NN'), ('\u6295\u8d44', 'NN'), ('\u4f01\u4e1a', 'NN'), ('\u903e', 'VV'),\n            ('\u4e09\u5341\u4e07', 'CD'), ('\u5bb6', 'M')]\nprint(parser.predict(sentence))\nparser.evaluate(SEMEVAL2016_TEXT_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_pos_rnn_fasttext.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-25 21:34\n\nimport tensorflow as tf\n\nfrom hanlp.components.taggers.pos_tf import RNNPartOfSpeechTaggerTF\nfrom hanlp.pretrained.fasttext import FASTTEXT_CC_300_EN\nfrom tests import cdroot\n\ncdroot()\ntagger = RNNPartOfSpeechTaggerTF()\nsave_dir = 'data/model/pos/ptb_pos_rnn_fasttext'\noptimizer = tf.keras.optimizers.SGD(lr=0.015)\n# optimizer = 'adam'\ntagger.fit('data/ptb-pos/train.tsv',\n           'data/ptb-pos/dev.tsv',\n           batch_size=10,\n           save_dir=save_dir,\n           embeddings={'class_name': 'HanLP>FastTextEmbedding',\n                       'config': {'filepath': FASTTEXT_CC_300_EN}},\n           optimizer=optimizer,\n           lr_decay_per_epoch=0.05,\n           rnn_units=100,\n           rnn_input_dropout=0.5,\n           rnn_output_dropout=0.5,\n           epochs=100,\n           verbose=True)\ntagger.load(save_dir)\ntagger.evaluate('data/ptb-pos/test.tsv', save_dir=save_dir, output=False)\nprint(tagger.predict(['This' 'time', 'is', 'for', 'dinner']))\nprint(tagger.predict([['This', 'is', 'an', 'old', 'story'],\n                      ['Not', 'this', 'year', '.']]))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_chnsenticorp_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-30 21:01\nfrom hanlp.components.classifiers.transformer_classifier_tf import TransformerClassifierTF, TransformerTextTransform\nfrom hanlp.datasets.classification.sentiment import CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_TEST, \\\n    CHNSENTICORP_ERNIE_DEV\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/classification/chnsenticorp_bert_base'\nclassifier = TransformerClassifierTF(TransformerTextTransform(y_column=0))\nclassifier.fit(CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_DEV, save_dir,\n               transformer='bert-base-chinese')\nclassifier.load(save_dir)\nprint(classifier.predict('\u524d\u53f0\u5ba2\u623f\u670d\u52a1\u6001\u5ea6\u975e\u5e38\u597d\uff01\u65e9\u9910\u5f88\u4e30\u5bcc\uff0c\u623f\u4ef7\u5f88\u5e72\u51c0\u3002\u518d\u63a5\u518d\u5389\uff01'))\nclassifier.evaluate(CHNSENTICORP_ERNIE_TEST, save_dir=save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_msra_ner_rnn.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.ner.ner_tf import RNNNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom hanlp.pretrained.word2vec import RADICAL_CHAR_EMBEDDING_100\nfrom tests import cdroot\n\ncdroot()\nrecognizer = RNNNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/msra_ner_rnn'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir,\n               embeddings=RADICAL_CHAR_EMBEDDING_100,\n               embedding_trainable=True,\n               epochs=100)\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/finetune_msra_ner_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nimport hanlp\nfrom hanlp.components.ner.ner_tf import TransformerNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom tests import cdroot\n\ncdroot()\nrecognizer = TransformerNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/finetune_ner_albert_base_zh_msra'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir, transformer='albert_base_zh',\n               finetune=hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)\nrecognizer.load(save_dir)\nprint(recognizer.predict(list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002')))\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_biaffine_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_albert3'\nparser = BiaffineTransformerDependencyParserTF()\nparser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir,\n           'albert-xxlarge-v2',\n           batch_size=256,\n           warmup_steps_ratio=.1,\n           token_mapping=PTB_TOKEN_MAPPING,\n           samples_per_batch=150,\n           transformer_dropout=.33,\n           learning_rate=2e-3,\n           learning_rate_transformer=1e-5,\n           # early_stopping_patience=10,\n           )\nparser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_sa_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF, \\\n    StructuralAttentionDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_840B_300D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_sa_glove'\nparser = StructuralAttentionDependencyParserTF()\n# parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n#            batch_size=3000,\n#            warmup_steps_ratio=.1,\n#            token_mapping=PTB_TOKEN_MAPPING,\n#            samples_per_batch=150,\n#            transformer_dropout=.33,\n#            masked_lm_dropout=.33,\n#            # learning_rate=2e-3,\n#            # learning_rate_transformer=1e-5,\n#            masked_lm_embed={'class_name': 'HanLP>Word2VecEmbedding',\n#                             'config': {\n#                                 'trainable': False,\n#                                 # 'embeddings_initializer': 'zero',\n#                                 'filepath': GLOVE_840B_300D,\n#                                 'expand_vocab': False,\n#                                 'lowercase': True,\n#                                 'cpu': False\n#                             }}\n#            # alpha=1,\n#            # early_stopping_patience=10,\n#            # num_decoder_layers=2,\n#            )\nparser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_msra_ner_electra.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.ner.ner_tf import TransformerNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom tests import cdroot\n\ncdroot()\nrecognizer = TransformerNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/ner_electra_small_zh_msra_sparse_categorical_crossentropy'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir,\n               transformer='hfl/chinese-electra-small-discriminator',\n               learning_rate=5e-5,\n               metrics='accuracy')  # Use accuracy to speed up training\nrecognizer.load(save_dir, metrics='f1')\nprint(recognizer.predict(list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002')))\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_semeval15_psd.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 18:26\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineSemanticDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_6B_100D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/sdp/semeval15_biaffine_psd'\nparser = BiaffineSemanticDependencyParserTF()\nparser.fit('data/semeval15/en.psd.train.conll', 'data/semeval15/en.psd.dev.conll', save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': GLOVE_6B_100D,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)  # disable variational dropout during evaluation so as to use CudaLSTM\nsentence = [('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('chamber', 'NN'),\n            ('music', 'NN'), ('?', '.')]\nprint(parser.predict(sentence))\nparser.evaluate('data/semeval15/en.id.psd.conll', save_dir)\nparser.evaluate('data/semeval15/en.ood.psd.conll', save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_sa_albert_topk.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF, \\\n    StructuralAttentionDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_840B_300D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_sa_topk'\nparser = StructuralAttentionDependencyParserTF()\nparser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n           batch_size=3000,\n           warmup_steps_ratio=.1,\n           token_mapping=PTB_TOKEN_MAPPING,\n           samples_per_batch=150,\n           transformer_dropout=.33,\n           masked_lm_dropout=.33,\n           learning_rate=2e-3,\n           learning_rate_transformer=1e-5,\n\n           # alpha=1,\n           # early_stopping_patience=10,\n           # num_decoder_layers=2,\n           )\nparser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_msra_ner_ngram_conv.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.ner.ner_tf import NgramConvNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR, \\\n    CONVSEG_W2V_NEWS_TENSITE_WORD_MSR\nfrom tests import cdroot\n\ncdroot()\nrecognizer = NgramConvNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/msra_ner_ngram_conv'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir,\n               word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                           'config': {\n                               'trainable': True,\n                               'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                               'expand_vocab': False,\n                               'lowercase': False,\n                           }},\n               ngram_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                            'config': {\n                                'trainable': True,\n                                'filepath': CONVSEG_W2V_NEWS_TENSITE_WORD_MSR,\n                                'expand_vocab': True,\n                                'lowercase': False,\n                            }},\n               weight_norm=True)\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_conll03_ner_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-25 21:34\nfrom hanlp.components.ner.ner_tf import TransformerNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.conll03 import CONLL03_EN_TRAIN, CONLL03_EN_DEV, CONLL03_EN_TEST\nfrom tests import cdroot\n\ncdroot()\ntagger = TransformerNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/ner_conll03_bert_base_cased_en'\ntagger.fit(CONLL03_EN_TRAIN, CONLL03_EN_DEV, save_dir, transformer='bert-base-cased',\n           metrics='accuracy')\ntagger.load(save_dir, metrics='f1')\nprint(tagger.predict('West Indian all-rounder Phil Simmons eats apple .'.split()))\ntagger.evaluate(CONLL03_EN_TEST, save_dir=save_dir, output=False, batch_size=32)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ctb7_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 18:33\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineDependencyParserTF\nfrom hanlp.datasets.parsing.ctb5 import CIP_W2V_100_CN\nfrom hanlp.datasets.parsing.ctb7 import CTB7_DEP_TRAIN, CTB7_DEP_DEV, CTB7_DEP_TEST\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/biaffine_ctb7'\nparser = BiaffineDependencyParserTF()\nparser.fit(CTB7_DEP_TRAIN, CTB7_DEP_DEV, save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': CIP_W2V_100_CN,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)\nsentence = [('\u4e2d\u56fd', 'NR'), ('\u6279\u51c6', 'VV'), ('\u8bbe\u7acb', 'VV'), ('\u5916\u5546', 'NN'), ('\u6295\u8d44', 'NN'), ('\u4f01\u4e1a', 'NN'), ('\u903e', 'VV'),\n            ('\u4e09\u5341\u4e07', 'CD'), ('\u5bb6', 'M')]\nprint(parser.predict(sentence))\nparser.evaluate(CTB7_DEP_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_msra_ner_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.ner.ner_tf import TransformerNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom tests import cdroot\n\ncdroot()\nrecognizer = TransformerNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/ner_bert_base_msra_1'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir, transformer='bert-base-chinese',\n               metrics='accuracy')  # accuracy is faster\nrecognizer.load(save_dir, metrics='f1')\nprint(recognizer.predict(list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002')))\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_semeval16_news.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 23:20\nfrom hanlp.datasets.parsing.semeval16 import SEMEVAL2016_NEWS_TRAIN, SEMEVAL2016_NEWS_DEV, SEMEVAL2016_NEWS_TEST\nfrom hanlp.pretrained.word2vec import SEMEVAL16_EMBEDDINGS_300_NEWS_CN\nfrom hanlp.utils.tf_util import nice\n\nnice()\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineSemanticDependencyParserTF\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/sdp/semeval16-news'\nparser = BiaffineSemanticDependencyParserTF()\nparser.fit(SEMEVAL2016_NEWS_TRAIN, SEMEVAL2016_NEWS_DEV, save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': SEMEVAL16_EMBEDDINGS_300_NEWS_CN,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)\nsentence = [('\u4e2d\u56fd', 'NR'), ('\u6279\u51c6', 'VV'), ('\u8bbe\u7acb', 'VV'), ('\u5916\u5546', 'NN'), ('\u6295\u8d44', 'NN'), ('\u4f01\u4e1a', 'NN'), ('\u903e', 'VV'),\n            ('\u4e09\u5341\u4e07', 'CD'), ('\u5bb6', 'M')]\nprint(parser.predict(sentence))\nparser.evaluate(SEMEVAL2016_NEWS_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_biaffine_bert_positional.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_bert_positional_diff_lr'\nparser = BiaffineTransformerDependencyParserTF()\nparser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n           batch_size=3000,\n           warmup_steps_ratio=.1,\n           token_mapping=PTB_TOKEN_MAPPING,\n           samples_per_batch=150,\n           transformer_dropout=.33,\n           learning_rate=1e-4,\n           learning_rate_transformer=1e-5,\n           d_positional=128,\n           # early_stopping_patience=10,\n           )\n# parser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\n# print(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_sa_pos_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF, \\\n    StructuralAttentionDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_840B_300D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_sa_bert_joint_pos'\nparser = StructuralAttentionDependencyParserTF()\nparser.fit('data/ptb-dep/train.conllx', 'data/ptb-dep/dev.conllx', save_dir, 'bert-base-uncased',\n           batch_size=256,\n           warmup_steps_ratio=.1,\n           token_mapping=PTB_TOKEN_MAPPING,\n           samples_per_batch=150,\n           transformer_dropout=.33,\n           masked_lm_dropout=.33,\n           learning_rate=2e-3,\n           learning_rate_transformer=1e-5,\n           joint_pos=True\n           # alpha=1,\n           # early_stopping_patience=10,\n           # num_decoder_layers=2,\n           )\n# parser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate('data/ptb-dep/test.conllx', save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ctb9_pos_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF\nfrom tests import cdroot\n\ncdroot()\ntagger = TransformerTaggerTF()\nsave_dir = 'data/model/pos/ctb9_albert_base'\ntagger.fit('data/pos/ctb9/train.tsv',\n           'data/pos/ctb9/test.tsv',\n           save_dir,\n           transformer='uer/albert-base-chinese-cluecorpussmall',\n           max_seq_length=130,\n           warmup_steps_ratio=0.1,\n           epochs=20,\n           learning_rate=5e-5)\ntagger.load(save_dir)\nprint(tagger(['\u6211', '\u7684', '\u5e0c\u671b', '\u662f', '\u5e0c\u671b', '\u548c\u5e73']))\ntagger.evaluate('data/pos/ctb9/test.tsv', save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_conll03_ner_flair.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-25 21:34\n\nimport tensorflow as tf\n\nfrom hanlp.components.ner.ner_tf import RNNNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.conll03 import CONLL03_EN_TRAIN, CONLL03_EN_TEST\nfrom hanlp.pretrained.glove import GLOVE_6B_100D\nfrom hanlp.pretrained.rnnlm import FLAIR_LM_FW_WMT11_EN_TF, FLAIR_LM_BW_WMT11_EN_TF\nfrom tests import cdroot\n\ncdroot()\ntagger = RNNNamedEntityRecognizerTF()\nsave_dir = 'data/model/conll03-ner-rnn-flair'\ntagger.fit(CONLL03_EN_TRAIN, CONLL03_EN_TEST, save_dir, epochs=100,\n           optimizer=tf.keras.optimizers.Adam(learning_rate=0.1,\n                                              beta_1=0.9,\n                                              beta_2=0.999,\n                                              epsilon=1e-8),\n           loss='crf',\n           rnn_units=256,\n           embeddings=[\n               {'class_name': 'HanLP>Word2VecEmbedding',\n                'config': {\n                    'trainable': False,\n                    'embeddings_initializer': 'zero',\n                    'filepath': GLOVE_6B_100D,\n                    'expand_vocab': True,\n                    'lowercase': False\n                }},\n               {'class_name': 'HanLP>ContextualStringEmbedding',\n                'config': {\n                    'trainable': False,\n                    'forward_model_path': FLAIR_LM_FW_WMT11_EN_TF,\n                    'backward_model_path': FLAIR_LM_BW_WMT11_EN_TF\n                }}\n           ],\n           rnn_output_dropout=0.5,\n           rnn_input_dropout=0.5,\n           batch_size=32,\n           metrics='f1',\n           anneal_factor=0.5,\n           patience=2,\n           )\nprint(tagger.predict('West Indian all-rounder Phil Simmons eats apple .'.split()))\n# print(tagger.predict([['This', 'is', 'an', 'old', 'story'],\n#                       ['Not', 'this', 'year', '.']]))\n# [['DT', 'VBZ', 'DT', 'JJ', 'NN'], ['RB', 'DT', 'NN', '.']]\n# tagger.load(save_dir)\ntagger.evaluate(CONLL03_EN_TEST, save_dir=save_dir, output=False)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ctb5_pos_rnn.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:46\nfrom hanlp.components.taggers.pos_tf import RNNPartOfSpeechTaggerTF\nfrom hanlp.datasets.pos.ctb5 import CTB5_POS_TRAIN, CTB5_POS_DEV, CTB5_POS_TEST\nfrom hanlp.pretrained.fasttext import FASTTEXT_WIKI_300_ZH\nfrom tests import cdroot\n\ncdroot()\ntagger = RNNPartOfSpeechTaggerTF()\nsave_dir = 'data/model/pos/ctb5_pos_rnn_fasttext'\ntagger.fit(CTB5_POS_TRAIN, CTB5_POS_DEV, save_dir, embeddings={'class_name': 'HanLP>FastTextEmbedding',\n                                                                 'config': {'filepath': FASTTEXT_WIKI_300_ZH}}, )\ntagger.evaluate(CTB5_POS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-12-26 23:25\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ctb5_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 18:33\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineDependencyParserTF\nfrom hanlp.datasets.parsing.ctb5 import CTB5_DEP_TRAIN, CTB5_DEP_DEV, CTB5_DEP_TEST\nfrom hanlp.pretrained.word2vec import CTB5_FASTTEXT_300_CN\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/biaffine_ctb'\nparser = BiaffineDependencyParserTF()\nparser.fit(CTB5_DEP_TRAIN, CTB5_DEP_DEV, save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': CTB5_FASTTEXT_300_CN,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)\nsentence = [('\u4e2d\u56fd', 'NR'), ('\u6279\u51c6', 'VV'), ('\u8bbe\u7acb', 'VV'), ('\u5916\u5546', 'NN'), ('\u6295\u8d44', 'NN'), ('\u4f01\u4e1a', 'NN'), ('\u903e', 'VV'),\n            ('\u4e09\u5341\u4e07', 'CD'), ('\u5bb6', 'M')]\nprint(parser.predict(sentence))\nparser.evaluate(CTB5_DEP_TEST, save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_biaffine_bert_96.6.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF\nfrom tests import cdroot\nfrom hanlp.metrics.parsing import conllx_eval\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_bert_96.61'\nparser = BiaffineTransformerDependencyParserTF()\n# parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n#            batch_size=3000,\n#            warmup_steps_ratio=.1,\n#            token_mapping=PTB_TOKEN_MAPPING,\n#            samples_per_batch=150,\n#            )\nparser.load(save_dir)\noutput = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False, output=output)\nuas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\nprint(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_semeval15_dm.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 18:26\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineSemanticDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_6B_100D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/sdp/semeval15_biaffine_dm'\nparser = BiaffineSemanticDependencyParserTF()\nparser.fit('data/semeval15/en.dm.train.conll', 'data/semeval15/en.dm.dev.conll', save_dir,\n           pretrained_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                             'config': {\n                                 'trainable': False,\n                                 'embeddings_initializer': 'zero',\n                                 'filepath': GLOVE_6B_100D,\n                                 'expand_vocab': True,\n                                 'lowercase': True,\n                                 'normalize': True,\n                             }},\n           )\nparser.load(save_dir)  # disable variational dropout during evaluation so as to use CudaLSTM\nsentence = [('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('chamber', 'NN'),\n            ('music', 'NN'), ('?', '.')]\nprint(parser.predict(sentence))\nparser.evaluate('data/semeval15/en.id.dm.auto.conllu', save_dir)\nparser.evaluate('data/semeval15/en.ood.dm.auto.conllu', save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ptb_dep_sa_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-07 23:48\nfrom hanlp.metrics.parsing import conllx_eval\n\nfrom hanlp.datasets.parsing.ptb import PTB_SD330_DEV, PTB_SD330_TRAIN, PTB_SD330_TEST, PTB_TOKEN_MAPPING\nfrom hanlp.components.parsers.biaffine_parser_tf import BiaffineTransformerDependencyParserTF, \\\n    StructuralAttentionDependencyParserTF\nfrom hanlp.pretrained.glove import GLOVE_840B_300D\nfrom tests import cdroot\n\ncdroot()\nsave_dir = 'data/model/dep/ptb_sa_glove'\nparser = StructuralAttentionDependencyParserTF()\n# parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',\n#            batch_size=3000,\n#            warmup_steps_ratio=.1,\n#            token_mapping=PTB_TOKEN_MAPPING,\n#            samples_per_batch=150,\n#            transformer_dropout=.33,\n#            masked_lm_dropout=.33,\n#            # learning_rate=2e-3,\n#            # learning_rate_transformer=1e-5,\n#            masked_lm_embed={'class_name': 'HanLP>Word2VecEmbedding',\n#                             'config': {\n#                                 'trainable': False,\n#                                 # 'embeddings_initializer': 'zero',\n#                                 'filepath': GLOVE_840B_300D,\n#                                 'expand_vocab': False,\n#                                 'lowercase': True,\n#                                 'cpu': False\n#                             }}\n#            # alpha=1,\n#            # early_stopping_patience=10,\n#            # num_decoder_layers=2,\n#            )\nparser.load(save_dir)\n# output = f'{save_dir}/test.predict.conll'\nparser.evaluate(PTB_SD330_TEST, save_dir, warm_up=False)\n# uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)\n# print(f'Official UAS: {uas:.4f} LAS: {las:.4f}')\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_ctb9_pos_electra.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF\nfrom tests import cdroot\n\ncdroot()\ntagger = TransformerTaggerTF()\nsave_dir = 'data/model/pos/ctb9_electra_small_zh_epoch_20'\ntagger.fit('data/pos/ctb9/train.tsv',\n           'data/pos/ctb9/test.tsv',\n           save_dir,\n           transformer='hfl/chinese-electra-small-discriminator',\n           max_seq_length=130,\n           warmup_steps_ratio=0.1,\n           epochs=20,\n           learning_rate=5e-5)\ntagger.load(save_dir)\nprint(tagger(['\u6211', '\u7684', '\u5e0c\u671b', '\u662f', '\u5e0c\u671b', '\u548c\u5e73']))\ntagger.evaluate('data/pos/ctb9/test.tsv', save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/train_msra_ner_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 23:15\nfrom hanlp.components.ner.ner_tf import TransformerNamedEntityRecognizerTF\nfrom hanlp.datasets.ner.msra import MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST\nfrom tests import cdroot\n\ncdroot()\nrecognizer = TransformerNamedEntityRecognizerTF()\nsave_dir = 'data/model/ner/msra_ner_albert_base'\nrecognizer.fit(MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_CHAR_LEVEL_DEV, save_dir,\n               transformer='uer/albert-base-chinese-cluecorpussmall',\n               learning_rate=5e-5,\n               metrics='accuracy')  # Use accuracy to speed up training\nrecognizer.load(save_dir, metrics='f1')\nprint(recognizer.predict(list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002')))\nrecognizer.evaluate(MSRA_NER_CHAR_LEVEL_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_ctb6_cws_convseg.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import NgramConvTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_TRAIN, CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR\nfrom tests import cdroot\n\ncdroot()\ntokenizer = NgramConvTokenizerTF()\nsave_dir = 'data/model/cws/ctb6_cws'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit(CTB6_CWS_TRAIN,\n              CTB6_CWS_DEV,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              optimizer=optimizer,\n              window_size=0,\n              weight_norm=True)\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir, output=False)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_msr_cws_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\n\nfrom hanlp.components.tokenizers.tok import TransformerTokenizer\nfrom hanlp.datasets.cws.ctb import CTB6_CWS_TEST\nfrom hanlp.datasets.tokenization.sighan2005.msr import SIGHAN2005_MSR_VALID, SIGHAN2005_MSR_TRAIN\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizer()\nsave_dir = 'data/model/msr_cws_albert_base'\ntokenizer.fit(SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, save_dir,\n              transformer='albert_base_zh',\n              max_seq_length=150,\n              metrics='f1', learning_rate=5e-5, epochs=10)\ntokenizer.load(save_dir)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_large_rnn_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import RNNTokenizerTF\nfrom hanlp.datasets.cws.ctb import CTB6_CWS_TEST, CTB6_CWS_DEV\nfrom hanlp.pretrained.word2vec import RADICAL_CHAR_EMBEDDING_100\nfrom tests import cdroot\n\ncdroot()\n\ntokenizer = RNNTokenizerTF()\nsave_dir = 'data/model/cws/large_rnn_cws'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit('data/cws/large/all.txt',\n              CTB6_CWS_DEV,\n              save_dir,\n              embeddings={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': RADICAL_CHAR_EMBEDDING_100,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              early_stopping_patience=5,\n              batch_size=64,\n              max_seq_len=64,\n              metrics='accuracy'\n              )\ntokenizer.load(save_dir, metrics='f1')\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir, output=False)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_msr_cws_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.sighan2005.msr import SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_DEV, SIGHAN2005_MSR_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/cws_bert_base_msra'\ntokenizer.fit(SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_DEV, save_dir, transformer='bert-base-chinese',\n              metrics='f1')\n# tagger.load(save_dir)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_pku_conv_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nfrom hanlp.datasets.tokenization.sighan2005 import SIGHAN2005_PKU_TRAIN, SIGHAN2005_PKU_DEV, SIGHAN2005_PKU_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR\nfrom hanlp.utils.tf_util import nice\nfrom tests import cdroot\nimport tensorflow as tf\n\nnice()\ncdroot()\nfrom hanlp.components.tokenizers.tok_tf import NgramConvTokenizerTF\n\ntokenizer = NgramConvTokenizerTF()\nsave_dir = 'data/model/cws/sighan2005-pku-convseg'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit(SIGHAN2005_PKU_TRAIN,\n              SIGHAN2005_PKU_DEV,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              optimizer=optimizer,\n              window_size=0,\n              weight_norm=True)\ntokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir=save_dir, output=False)\n# print(tagger.tag(list('\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3')))\n# print(tagger.predict('\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3'))\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_ctb6_cws_bert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\n\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_TRAIN, CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/cws_bert_base_ctb6'\ntokenizer.fit(CTB6_CWS_TRAIN, CTB6_CWS_DEV, save_dir, transformer='chinese_L-12_H-768_A-12',\n              metrics='f1')\ntokenizer.load(save_dir)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_ctb6_cws_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\n\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_TRAIN, CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/cws_bert_albert_ctb6'\ntokenizer.fit(CTB6_CWS_TRAIN, CTB6_CWS_DEV, save_dir,\n              transformer='/home/ubuntu/hankcs/laser/data/transformer/albert_base_tf2',\n              metrics='f1', learning_rate=5e-5, epochs=3)\ntokenizer.load(save_dir)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_large_cws_electra.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/large_corpus_cws_electra_small'\ntokenizer.fit('data/cws/large/all.txt',\n              CTB6_CWS_DEV, save_dir,\n              transformer='hfl/chinese-electra-small-discriminator',\n              max_seq_length=128,\n              metrics='accuracy', learning_rate=5e-5, epochs=10)\ntokenizer.load(save_dir, metrics='f1')\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_large_bert_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/cws_bert_base_100million'\ntokenizer.fit('data/cws/large/all.txt', CTB6_CWS_DEV, save_dir, transformer='bert-base-chinese',\n              metrics='accuracy', batch_size=32)\ntokenizer.load(save_dir, metrics='f1')\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_msr_cws_ngram_conv.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import NgramConvTokenizerTF\nfrom hanlp.datasets.tokenization.sighan2005.msr import SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_DEV, SIGHAN2005_MSR_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR\nfrom tests import cdroot\n\ncdroot()\ntokenizer = NgramConvTokenizerTF()\nsave_dir = 'data/model/cws/convseg-msr-nocrf-noembed'\ntokenizer.fit(SIGHAN2005_MSR_TRAIN,\n              SIGHAN2005_MSR_DEV,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n                                                 epsilon=1e-8, clipnorm=5),\n              epochs=100,\n              window_size=0,\n              metrics='f1',\n              weight_norm=True)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_large_cws_albert.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 22:22\nfrom hanlp.components.tokenizers.tok_tf import TransformerTokenizerTF\nfrom hanlp.datasets.tokenization.ctb6 import CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom tests import cdroot\n\ncdroot()\ntokenizer = TransformerTokenizerTF()\nsave_dir = 'data/model/large_corpus_cws_albert_base'\ntokenizer.fit('data/cws/large/all.txt',\n              CTB6_CWS_DEV, save_dir,\n              transformer='uer/albert-base-chinese-cluecorpussmall',\n              max_seq_length=128,\n              metrics='accuracy', learning_rate=5e-5, epochs=3)\ntokenizer.load(save_dir, metrics='f1')\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 20:55", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_msr_cws_ngram_conv_embed.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok import NgramConvTokenizer\nfrom hanlp.datasets.tokenization.sighan2005.msr import SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, SIGHAN2005_MSR_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR, CONVSEG_W2V_NEWS_TENSITE_WORD_MSR\nfrom tests import cdroot\n\ncdroot()\ntokenizer = NgramConvTokenizer()\nsave_dir = 'data/model/cws/convseg-msr-nocrf-noembed'\ntokenizer.fit(SIGHAN2005_MSR_TRAIN,\n              SIGHAN2005_MSR_VALID,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              ngram_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                           'config': {\n                               'trainable': True,\n                               'filepath': CONVSEG_W2V_NEWS_TENSITE_WORD_MSR,\n                               'expand_vocab': True,\n                               'lowercase': False,\n                           }},\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n                                                 epsilon=1e-8, clipnorm=5),\n              epochs=3,\n              window_size=4,\n              metrics='f1',\n              weight_norm=True)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\ntokenizer.load(save_dir, metrics='f1')\ntokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_large_conv_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 21:58\n\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import NgramConvTokenizerTF\nfrom hanlp.datasets.cws.ctb import CTB6_CWS_TRAIN, CTB6_CWS_DEV, CTB6_CWS_TEST\nfrom hanlp.pretrained.word2vec import CONVSEG_W2V_NEWS_TENSITE_CHAR\nfrom tests import cdroot\n\ncdroot()\ntokenizer = NgramConvTokenizerTF()\nsave_dir = 'data/model/cws/ctb6_cws'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit(CTB6_CWS_TRAIN,\n              CTB6_CWS_DEV,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              optimizer=optimizer,\n              window_size=0,\n              weight_norm=True)\ntokenizer.evaluate(CTB6_CWS_TEST, save_dir=save_dir, output=False)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_pku980106_conv_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import NgramConvTokenizerTF\nfrom hanlp.pretrained.word2vec import RADICAL_CHAR_EMBEDDING_100\nfrom tests import cdroot\n\ncdroot()\n\ntokenizer = NgramConvTokenizerTF()\nsave_dir = 'data/model/cws/pku98_6m_conv_ngram'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit('data/cws/pku98/199801-06-seg.txt',\n              'data/cws/pku98/test_pku98_name_merged.txt',\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': False,\n                              'filepath': RADICAL_CHAR_EMBEDDING_100,\n                              'expand_vocab': True,\n                              'lowercase': False,\n                          }},\n              optimizer=optimizer,\n              window_size=0,\n              weight_norm=True)\ntokenizer.evaluate('data/cws/pku98/test_pku98_name_merged.txt', save_dir=save_dir, output=False)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/tf/train/cws/train_pku980106_rnn_cws.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 15:39\nimport tensorflow as tf\n\nfrom hanlp.components.tokenizers.tok_tf import RNNTokenizerTF\nfrom hanlp.pretrained.word2vec import RADICAL_CHAR_EMBEDDING_100\nfrom tests import cdroot\n\ncdroot()\n\ntokenizer = RNNTokenizerTF()\nsave_dir = 'data/model/cws/pku_6m_rnn_cws'\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n                                     epsilon=1e-8, clipnorm=5)\ntokenizer.fit('data/cws/pku98/199801-06-seg.txt',\n              'data/cws/pku98/pku98_test.txt',\n              save_dir,\n              embeddings={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': False,\n                              'filepath': RADICAL_CHAR_EMBEDDING_100,\n                              'expand_vocab': True,\n                              'lowercase': False,\n                          }}\n              )\ntokenizer.evaluate('data/cws/pku98/pku98_test.txt', save_dir=save_dir, output=False)\nprint(tokenizer.predict(['\u4e2d\u592e\u6c11\u65cf\u4e50\u56e2\u79bb\u5f00\u5317\u4eac\u524d\u5f80\u7ef4\u4e5f\u7eb3', '\u5546\u54c1\u548c\u670d\u52a1']))\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/zh/train/open_base.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-03 14:24\nfrom hanlp_demo import block_windows\nfrom hanlp.common.dataset import SortingSamplerBuilder\nfrom hanlp.common.transform import NormalizeCharacter\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing\nfrom hanlp.components.mtl.tasks.dep import BiaffineDependencyParsing\nfrom hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\nfrom hanlp.components.mtl.tasks.pos import TransformerTagging\nfrom hanlp.components.mtl.tasks.sdp import BiaffineSemanticDependencyParsing\nfrom hanlp.components.mtl.tasks.srl.bio_srl import SpanBIOSemanticRoleLabeling\nfrom hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization\nfrom hanlp.datasets.ner.msra import MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, \\\n    MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST\nfrom hanlp.datasets.parsing.ctb8 import CTB8_POS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_SD330_TEST, CTB8_SD330_DEV, \\\n    CTB8_SD330_TRAIN, CTB8_CWS_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_BRACKET_LINE_NOEC_TRAIN, \\\n    CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TEST\nfrom hanlp.datasets.parsing.semeval16 import SEMEVAL2016_TEXT_TRAIN_CONLLU, SEMEVAL2016_TEXT_TEST_CONLLU, \\\n    SEMEVAL2016_TEXT_DEV_CONLLU\nfrom hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_DEV, \\\n    ONTONOTES5_CONLL12_CHINESE_TRAIN\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding\nfrom hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder\nfrom hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON\nfrom hanlp.utils.log_util import cprint\nfrom tests import cdroot\n\ncdroot()\ntasks = {\n    'tok': TaggingTokenization(\n        CTB8_CWS_TRAIN,\n        CTB8_CWS_DEV,\n        CTB8_CWS_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        max_seq_len=510,\n        hard_constraint=True,\n        char_level=True,\n        tagging_scheme='BMES',\n        lr=1e-3,\n        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),\n    ),\n    'pos': TransformerTagging(\n        CTB8_POS_TRAIN,\n        CTB8_POS_DEV,\n        CTB8_POS_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        hard_constraint=True,\n        max_seq_len=510,\n        char_level=True,\n        dependencies='tok',\n        lr=1e-3,\n    ),\n    'ner': TaggingNamedEntityRecognition(\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        secondary_encoder=RelativeTransformerEncoder(768, k_as_x=True),\n        dependencies='tok',\n    ),\n    'srl': SpanBIOSemanticRoleLabeling(\n        ONTONOTES5_CONLL12_CHINESE_TRAIN,\n        ONTONOTES5_CONLL12_CHINESE_DEV,\n        ONTONOTES5_CONLL12_CHINESE_TEST,\n        SortingSamplerBuilder(batch_size=32, batch_max_tokens=2048),\n        lr=1e-3,\n        crf=True,\n        dependencies='tok',\n    ),\n    'dep': BiaffineDependencyParsing(\n        CTB8_SD330_TRAIN,\n        CTB8_SD330_DEV,\n        CTB8_SD330_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        tree=True,\n        punct=True,\n        dependencies='tok',\n    ),\n    'sdp': BiaffineSemanticDependencyParsing(\n        SEMEVAL2016_TEXT_TRAIN_CONLLU,\n        SEMEVAL2016_TEXT_DEV_CONLLU,\n        SEMEVAL2016_TEXT_TEST_CONLLU,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        apply_constraint=True,\n        punct=True,\n        dependencies='tok',\n    ),\n    'con': CRFConstituencyParsing(\n        CTB8_BRACKET_LINE_NOEC_TRAIN,\n        CTB8_BRACKET_LINE_NOEC_DEV,\n        CTB8_BRACKET_LINE_NOEC_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        dependencies='tok',\n    )\n}\nmtl = MultiTaskLearning()\nsave_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base'\nmtl.fit(\n    ContextualWordEmbedding('token',\n                            \"hfl/chinese-electra-180g-base-discriminator\",\n                            average_subwords=True,\n                            max_sequence_length=512,\n                            word_dropout=.1),\n    tasks,\n    save_dir,\n    30,\n    lr=1e-3,\n    encoder_lr=5e-5,\n    grad_norm=1,\n    gradient_accumulation=2,\n    eval_trn=False,\n)\ncprint(f'Model saved in [cyan]{save_dir}[/cyan]')\nmtl.evaluate(save_dir)\nmtl.load(save_dir)\nprint(mtl('\u534e\u7eb3\u97f3\u4e50\u65d7\u4e0b\u7684\u65b0\u57a3\u7ed3\u8863\u572812\u670821\u65e5\u4e8e\u65e5\u672c\u6b66\u9053\u9986\u4e3e\u529e\u6b4c\u624b\u51fa\u9053\u6d3b\u52a8'))\n", "plugins/hanlp_demo/hanlp_demo/zh/train/finetune_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2023-10-18 18:49\nimport os\n\nimport hanlp\nfrom hanlp.components.ner.transformer_ner import TransformerNamedEntityRecognizer\nfrom tests import cdroot\n\ncdroot()\n\nyour_training_corpus = 'data/ner/finetune/word_to_iobes.tsv'\nyour_development_corpus = your_training_corpus  # Use a different one in reality\nsave_dir = 'data/ner/finetune/model'\n\nif not os.path.exists(your_training_corpus):\n    os.makedirs(os.path.dirname(your_training_corpus), exist_ok=True)\n    with open(your_training_corpus, 'w') as out:\n        out.write(\n'''\u8bad\u7ec3\\tB-NLP\n\u8bed\u6599\\tE-NLP\n\u4e3a\\tO\nIOBES\\tO\n\u683c\u5f0f\\tO\n'''\n        )\n\nner = TransformerNamedEntityRecognizer()\nner.fit(\n    trn_data=your_training_corpus,\n    dev_data=your_development_corpus,\n    save_dir=save_dir,\n    epochs=50,  # Since the corpus is small, overfit it\n    finetune=hanlp.pretrained.ner.MSRA_NER_ELECTRA_SMALL_ZH,\n    # You MUST set the same parameters with the fine-tuning model:\n    average_subwords=True,\n    transformer='hfl/chinese-electra-180g-small-discriminator',\n)\n\nHanLP = hanlp.pipeline()\\\n    .append(hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH), output_key='tok')\\\n    .append(ner, output_key='ner')\nHanLP(['\u8bad\u7ec3\u8bed\u6599\u4e3aIOBES\u683c\u5f0f', '\u6653\u7f8e\u7130\u6765\u5230\u5317\u4eac\u7acb\u65b9\u5ead\u53c2\u89c2\u81ea\u7136\u8bed\u4e49\u79d1\u6280\u516c\u53f8\u3002']).pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/zh/train/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-31 20:12\n", "plugins/hanlp_demo/hanlp_demo/zh/train/open_small.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-03 14:24\nfrom hanlp_demo import block_windows\nfrom hanlp.common.dataset import SortingSamplerBuilder\nfrom hanlp.common.transform import NormalizeCharacter\nfrom hanlp.components.mtl.multi_task_learning import MultiTaskLearning\nfrom hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing\nfrom hanlp.components.mtl.tasks.dep import BiaffineDependencyParsing\nfrom hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\nfrom hanlp.components.mtl.tasks.pos import TransformerTagging\nfrom hanlp.components.mtl.tasks.sdp import BiaffineSemanticDependencyParsing\nfrom hanlp.components.mtl.tasks.srl.bio_srl import SpanBIOSemanticRoleLabeling\nfrom hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization\nfrom hanlp.datasets.ner.msra import MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, \\\n    MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN\nfrom hanlp.datasets.parsing.ctb8 import CTB8_POS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_SD330_TEST, CTB8_SD330_DEV, \\\n    CTB8_SD330_TRAIN, CTB8_CWS_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_BRACKET_LINE_NOEC_TEST, \\\n    CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TRAIN\nfrom hanlp.datasets.parsing.semeval16 import SEMEVAL2016_TEXT_TRAIN_CONLLU, SEMEVAL2016_TEXT_TEST_CONLLU, \\\n    SEMEVAL2016_TEXT_DEV_CONLLU\nfrom hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_DEV, \\\n    ONTONOTES5_CONLL12_CHINESE_TRAIN\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding\nfrom hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder\nfrom hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON\nfrom hanlp.utils.log_util import cprint\nfrom tests import cdroot\n\ncdroot()\ntasks = {\n    'tok': TaggingTokenization(\n        CTB8_CWS_TRAIN,\n        CTB8_CWS_DEV,\n        CTB8_CWS_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        max_seq_len=510,\n        hard_constraint=True,\n        char_level=True,\n        tagging_scheme='BMES',\n        lr=1e-3,\n        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),\n    ),\n    'pos': TransformerTagging(\n        CTB8_POS_TRAIN,\n        CTB8_POS_DEV,\n        CTB8_POS_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        hard_constraint=True,\n        max_seq_len=510,\n        char_level=True,\n        dependencies='tok',\n        lr=1e-3,\n    ),\n    'ner': TaggingNamedEntityRecognition(\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,\n        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        max_seq_len=510,\n        hard_constraint=True,\n        char_level=True,\n        lr=1e-3,\n        secondary_encoder=RelativeTransformerEncoder(256, k_as_x=True, feedforward_dim=128),\n        dependencies='tok',\n    ),\n    'srl': SpanBIOSemanticRoleLabeling(\n        ONTONOTES5_CONLL12_CHINESE_TRAIN,\n        ONTONOTES5_CONLL12_CHINESE_DEV,\n        ONTONOTES5_CONLL12_CHINESE_TEST,\n        SortingSamplerBuilder(batch_size=32, batch_max_tokens=1280),\n        lr=1e-3,\n        crf=True,\n        dependencies='tok',\n    ),\n    'dep': BiaffineDependencyParsing(\n        CTB8_SD330_TRAIN,\n        CTB8_SD330_DEV,\n        CTB8_SD330_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        tree=True,\n        proj=True,\n        punct=True,\n        dependencies='tok',\n    ),\n    'sdp': BiaffineSemanticDependencyParsing(\n        SEMEVAL2016_TEXT_TRAIN_CONLLU,\n        SEMEVAL2016_TEXT_DEV_CONLLU,\n        SEMEVAL2016_TEXT_TEST_CONLLU,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        apply_constraint=True,\n        punct=True,\n        dependencies='tok',\n    ),\n    'con': CRFConstituencyParsing(\n        CTB8_BRACKET_LINE_NOEC_TRAIN,\n        CTB8_BRACKET_LINE_NOEC_DEV,\n        CTB8_BRACKET_LINE_NOEC_TEST,\n        SortingSamplerBuilder(batch_size=32),\n        lr=1e-3,\n        dependencies='tok',\n    )\n}\nmtl = MultiTaskLearning()\nsave_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small'\ncprint(f'Model will be saved in [cyan]{save_dir}[/cyan]')\nmtl.fit(\n    ContextualWordEmbedding('token',\n                            \"hfl/chinese-electra-180g-small-discriminator\",\n                            average_subwords=True,\n                            max_sequence_length=512,\n                            word_dropout=.1),\n    tasks,\n    save_dir,\n    30,\n    lr=1e-3,\n    encoder_lr=5e-5,\n    grad_norm=1,\n    gradient_accumulation=1,\n    eval_trn=False,\n)\ncprint(f'Model saved in [cyan]{save_dir}[/cyan]')\nmtl.evaluate(save_dir)\nmtl.load(save_dir)\nmtl('\u534e\u7eb3\u97f3\u4e50\u65d7\u4e0b\u7684\u65b0\u57a3\u7ed3\u8863\u572812\u670821\u65e5\u4e8e\u65e5\u672c\u6b66\u9053\u9986\u4e3e\u529e\u6b4c\u624b\u51fa\u9053\u6d3b\u52a8').pretty_print()\n", "plugins/hanlp_demo/hanlp_demo/en/demo_tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-02 19:41\nfrom hanlp.utils.lang.en.english_tokenizer import tokenize_english\n\ntext = \"\"\"\\\nDon't go gentle into that good night.\n\"\"\"\nprint(tokenize_english(text))\n", "plugins/hanlp_demo/hanlp_demo/en/demo_amr.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-25 19:09\nimport hanlp\n\namr_parser = hanlp.load(hanlp.pretrained.amr.AMR3_SEQ2SEQ_BART_LARGE)\namr = amr_parser('The boy wants the girl to believe him.')\nprint(amr)\n", "plugins/hanlp_demo/hanlp_demo/en/demo_lm.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-02-11 09:14\nimport hanlp\n\nlm = hanlp.load(hanlp.pretrained.rnnlm.FLAIR_LM_FW_WMT11_EN_TF)\nprint(''.join(lm.generate_text(list('hello'))))\n", "plugins/hanlp_demo/hanlp_demo/en/demo_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 17:55\nimport hanlp\n\nsyntactic_parser = hanlp.load(hanlp.pretrained.dep.PTB_BIAFFINE_DEP_EN)\nsent = [('Is', 'VBZ'),\n        ('this', 'DT'),\n        ('the', 'DT'),\n        ('future', 'NN'),\n        ('of', 'IN'),\n        ('chamber', 'NN'),\n        ('music', 'NN'),\n        ('?', '.')]\ntree = syntactic_parser(sent)\nprint(tree)\n", "plugins/hanlp_demo/hanlp_demo/en/train_sst2_albert_base.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-10 17:41\nimport os\n\nfrom hanlp.components.classifiers.transformer_classifier_tf import TransformerClassifierTF\n\nfrom tests import cdroot\n\nfrom hanlp.datasets.glu.glue import STANFORD_SENTIMENT_TREEBANK_2_DEV, STANFORD_SENTIMENT_TREEBANK_2_TRAIN, \\\n    STANFORD_SENTIMENT_TREEBANK_2_TEST\n\ncdroot()\nsave_dir = os.path.join('data', 'model', 'sst', 'sst2_albert_base')\nclassifier = TransformerClassifierTF()\nclassifier.fit(STANFORD_SENTIMENT_TREEBANK_2_TRAIN, STANFORD_SENTIMENT_TREEBANK_2_DEV, save_dir,\n               transformer='albert-base-v2')\nclassifier.load(save_dir)\nprint(classifier('it\\' s a charming and often affecting journey'))\nclassifier.evaluate(STANFORD_SENTIMENT_TREEBANK_2_TEST, save_dir=save_dir)\nprint(f'Model saved in {save_dir}')\n", "plugins/hanlp_demo/hanlp_demo/en/demo_sentiment_analysis.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 03:52\nimport hanlp\n\nclassifier = hanlp.load('SST2_ALBERT_BASE_EN')\nprint(classifier.predict('I feel lucky'))\n", "plugins/hanlp_demo/hanlp_demo/en/demo_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-03 22:50\nimport hanlp\n\nrecognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_CASED_EN)\nprint(recognizer([\"President\", \"Obama\", \"is\", \"speaking\", \"at\", \"the\", \"White\", \"House\", \".\"]))\n", "plugins/hanlp_demo/hanlp_demo/en/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 17:55", "plugins/hanlp_demo/hanlp_demo/en/demo_sdp.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-03 15:26\nimport hanlp\nfrom hanlp_common.conll import CoNLLSentence\n\n# semeval15 offers three independent annotations over the Penn Treebank (PTB)\nsemantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_PAS_BIAFFINE_EN)\n# semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_DM_BIAFFINE_EN)\n# semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_PSD_BIAFFINE_EN)\nsent = [('Is', 'VBZ'),\n        ('this', 'DT'),\n        ('the', 'DT'),\n        ('future', 'NN'),\n        ('of', 'IN'),\n        ('chamber', 'NN'),\n        ('music', 'NN'),\n        ('?', '.')]\ntree = semantic_parser(sent)  # type:CoNLLSentence\nprint(tree)\n", "plugins/hanlp_demo/hanlp_demo/en/demo_pipeline.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-04 21:05\nimport hanlp\nfrom hanlp.utils.lang.en.english_tokenizer import tokenize_english\n\ntokenizer = tokenize_english\ntagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)\nsyntactic_parser = hanlp.load(hanlp.pretrained.dep.PTB_BIAFFINE_DEP_EN)\nsemantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_PAS_BIAFFINE_EN)\n\npipeline = hanlp.pipeline() \\\n    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n    .append(tokenizer, output_key='tokens') \\\n    .append(tagger, output_key='part_of_speech_tags') \\\n    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies',\n            conll=False) \\\n    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies',\n            conll=False)\nprint(pipeline)\n\ntext = '''Jobs and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer.\nTogether the duo gained fame and wealth a year later with the Apple II.\n'''\n\ndoc = pipeline(text)\nprint(doc)\n\n# You can save the config to disk for deploying or sharing.\npipeline.save('en.json')\n# Then load it smoothly.\ndeployed = hanlp.load('en.json')\nprint(deployed)\nprint(deployed(text))\n", "plugins/hanlp_demo/hanlp_demo/en/demo_pos.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-03 22:16\nimport hanlp\n\ntagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)\nprint(tagger([['I', 'banked', '2', 'dollars', 'in', 'a', 'bank', '.'],\n              ['Is', 'this', 'the', 'future', 'of', 'chamber', 'music', '?']]))\n", "hanlp/version.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:26\n\n__version__ = '2.1.0-beta.54'\n\"\"\"HanLP version\"\"\"\n\n\nclass NotCompatible(Exception):\n    pass\n", "hanlp/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-06-13 18:05\nimport hanlp.common\nimport hanlp.components\nimport hanlp.pretrained\nimport hanlp.utils\nfrom hanlp.version import __version__\n\nhanlp.utils.ls_resource_in_module(hanlp.pretrained)\n\n\ndef load(save_dir: str, verbose=None, **kwargs) -> hanlp.common.component.Component:\n    \"\"\"Load a pretrained component from an identifier.\n\n    Args:\n      save_dir (str): The identifier to the saved component. It could be a remote URL or a local path.\n      verbose: ``True`` to print loading progress.\n      **kwargs: Arguments passed to :func:`hanlp.common.torch_component.TorchComponent.load`, e.g.,\n        ``devices`` is a useful argument to specify which GPU devices a PyTorch component will use.\n\n    Examples::\n\n        import hanlp\n        # Load component onto the 0-th GPU.\n        hanlp.load(..., devices=0)\n        # Load component onto the 0-th and 1-st GPUs using data parallelization.\n        hanlp.load(..., devices=[0, 1])\n\n    .. Note::\n        A component can have dependencies on other components or resources, which will be recursively loaded. So it's\n        common to see multiple downloading messages per single load.\n\n    Returns:\n      hanlp.common.component.Component: A pretrained component.\n\n    \"\"\"\n    save_dir = hanlp.pretrained.ALL.get(save_dir, save_dir)\n    from hanlp.utils.component_util import load_from_meta_file\n    if verbose is None:\n        from hanlp_common.constant import HANLP_VERBOSE\n        verbose = HANLP_VERBOSE\n    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)\n\n\ndef pipeline(*pipes) -> hanlp.components.pipeline.Pipeline:\n    \"\"\"Creates a pipeline of components. It's made for bundling `KerasComponents`. For `TorchComponent`, use\n    :class:`~hanlp.components.mtl.multi_task_learning.MultiTaskLearning` instead.\n\n    Args:\n      *pipes: Components if pre-defined any.\n\n    Returns:\n      hanlp.components.pipeline.Pipeline: A pipeline, which is a list of components in order.\n\n    \"\"\"\n    return hanlp.components.pipeline.Pipeline(*pipes)\n", "hanlp/losses/sparse_categorical_crossentropy.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-20 01:29\n\nimport tensorflow as tf\n\nfrom hanlp.utils.tf_util import hanlp_register\n\n\n@hanlp_register\nclass SparseCategoricalCrossentropyOverNonzeroWeights(object):\n    def __init__(self) -> None:\n        super().__init__()\n        self.__name__ = type(self).__name__\n\n    def __call__(self, y_true, y_pred, sample_weight=None, **kwargs):\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n        if sample_weight is not None:\n            loss = loss * sample_weight\n        loss = tf.reduce_sum(loss)\n        if sample_weight is not None:\n            # This is equivalent to SUM_OVER_BATCH_SIZE\n            # loss /= tf.reduce_sum(tf.ones_like(sample_weight, dtype=loss.dtype))\n            # This one is SUM_BY_NONZERO_WEIGHTS\n            loss /= tf.reduce_sum(sample_weight)\n        return loss\n\n\n@hanlp_register\nclass SparseCategoricalCrossentropyOverBatchFirstDim(object):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.__name__ = type(self).__name__\n\n    def __call__(self, y_true, y_pred, sample_weight=None, **kwargs):\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n        if sample_weight is not None:\n            loss = loss * sample_weight\n        # could use sum of sample_weight[:,0] too\n        loss = tf.reduce_sum(loss) / tf.cast(tf.shape(y_true)[0], tf.float32)\n        return loss\n\n    def get_config(self):\n        return {}\n\n\n@hanlp_register\nclass MaskedSparseCategoricalCrossentropyOverBatchFirstDim(object):\n    def __init__(self, mask_value=0) -> None:\n        super().__init__()\n        self.mask_value = mask_value\n        self.__name__ = type(self).__name__\n\n    def __call__(self, y_true, y_pred, sample_weight=None, **kwargs):\n        assert sample_weight is None, 'the mask will be computed via y_true != mask_value, ' \\\n                                      'it might conflict with sample_weight'\n        active_loss = tf.not_equal(y_true, self.mask_value)\n        active_labels = tf.boolean_mask(y_true, active_loss)\n        active_logits = tf.boolean_mask(y_pred, active_loss)\n        loss = tf.keras.losses.sparse_categorical_crossentropy(active_labels, active_logits, from_logits=True)\n        loss = tf.reduce_sum(loss) / tf.cast(tf.shape(y_true)[0], tf.float32)\n        return loss\n", "hanlp/losses/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-20 01:28", "hanlp/utils/component_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 19:24\nimport os\nfrom hanlp_common.constant import HANLP_VERBOSE\nfrom hanlp_common.io import load_json, eprint, save_json\nfrom hanlp_common.reflection import object_from_classpath, str_to_type\nfrom hanlp import pretrained\nfrom hanlp import version\nfrom hanlp.common.component import Component\nfrom hanlp.utils.io_util import get_resource, get_latest_info_from_pypi, check_version_conflicts\nfrom hanlp_common.util import isdebugging\n\n\ndef load_from_meta_file(save_dir: str, meta_filename='meta.json', transform_only=False, verbose=HANLP_VERBOSE,\n                        **kwargs) -> Component:\n    \"\"\"\n    Load a component from a ``meta.json`` (legacy TensorFlow component) or a ``config.json`` file.\n\n    Args:\n        save_dir: The identifier.\n        meta_filename (str): The meta file of that saved component, which stores the classpath and version.\n        transform_only: Load and return only the transform.\n        **kwargs: Extra parameters passed to ``component.load()``.\n\n    Returns:\n\n        A component.\n    \"\"\"\n    identifier = save_dir\n    load_path = save_dir\n    save_dir = get_resource(save_dir)\n    if save_dir.endswith('.json'):\n        meta_filename = os.path.basename(save_dir)\n        save_dir = os.path.dirname(save_dir)\n    metapath = os.path.join(save_dir, meta_filename)\n    if not os.path.isfile(metapath):\n        tf_model = False\n        metapath = os.path.join(save_dir, 'config.json')\n    else:\n        tf_model = True\n    cls = None\n    if not os.path.isfile(metapath):\n        tips = ''\n        if save_dir.isupper():\n            from difflib import SequenceMatcher\n            similar_keys = sorted(pretrained.ALL.keys(),\n                                  key=lambda k: SequenceMatcher(None, k, identifier).ratio(),\n                                  reverse=True)[:5]\n            tips = f'Check its spelling based on the available keys:\\n' + \\\n                   f'{sorted(pretrained.ALL.keys())}\\n' + \\\n                   f'Tips: it might be one of {similar_keys}'\n        # These components are not intended to be loaded in this way, but I'm tired of explaining it again and again\n        if identifier in pretrained.word2vec.ALL.values():\n            save_dir = os.path.dirname(save_dir)\n            metapath = os.path.join(save_dir, 'config.json')\n            save_json({'classpath': 'hanlp.layers.embeddings.word2vec.Word2VecEmbeddingComponent',\n                       'embed': {'classpath': 'hanlp.layers.embeddings.word2vec.Word2VecEmbedding',\n                                 'embed': identifier, 'field': 'token', 'normalize': 'l2'},\n                       'hanlp_version': version.__version__}, metapath)\n        elif identifier in pretrained.fasttext.ALL.values():\n            save_dir = os.path.dirname(save_dir)\n            metapath = os.path.join(save_dir, 'config.json')\n            save_json({'classpath': 'hanlp.layers.embeddings.fast_text.FastTextEmbeddingComponent',\n                       'embed': {'classpath': 'hanlp.layers.embeddings.fast_text.FastTextEmbedding',\n                                 'filepath': identifier, 'src': 'token'},\n                       'hanlp_version': version.__version__}, metapath)\n        elif identifier in {pretrained.classifiers.LID_176_FASTTEXT_SMALL,\n                            pretrained.classifiers.LID_176_FASTTEXT_BASE}:\n            save_dir = os.path.dirname(save_dir)\n            metapath = os.path.join(save_dir, 'config.json')\n            save_json({'classpath': 'hanlp.components.classifiers.fasttext_classifier.FastTextClassifier',\n                       'model_path': identifier,\n                       'hanlp_version': version.__version__}, metapath)\n        else:\n            raise FileNotFoundError(f'The identifier {save_dir} resolves to a nonexistent meta file {metapath}. {tips}')\n    meta: dict = load_json(metapath)\n    cls = meta.get('classpath', cls)\n    if not cls:\n        cls = meta.get('class_path', None)  # For older version\n    if tf_model:\n        # tf models are trained with version < 2.1. To migrate them to 2.1, map their classpath to new locations\n        upgrade = {\n            'hanlp.components.tok_tf.TransformerTokenizerTF': 'hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF',\n            'hanlp.components.pos.RNNPartOfSpeechTagger': 'hanlp.components.taggers.pos_tf.RNNPartOfSpeechTaggerTF',\n            'hanlp.components.pos_tf.RNNPartOfSpeechTaggerTF': 'hanlp.components.taggers.pos_tf.RNNPartOfSpeechTaggerTF',\n            'hanlp.components.pos_tf.CNNPartOfSpeechTaggerTF': 'hanlp.components.taggers.pos_tf.CNNPartOfSpeechTaggerTF',\n            'hanlp.components.ner_tf.TransformerNamedEntityRecognizerTF': 'hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF',\n            'hanlp.components.parsers.biaffine_parser.BiaffineDependencyParser': 'hanlp.components.parsers.biaffine_parser_tf.BiaffineDependencyParserTF',\n            'hanlp.components.parsers.biaffine_parser.BiaffineSemanticDependencyParser': 'hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF',\n            'hanlp.components.tok_tf.NgramConvTokenizerTF': 'hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF',\n            'hanlp.components.classifiers.transformer_classifier.TransformerClassifier': 'hanlp.components.classifiers.transformer_classifier_tf.TransformerClassifierTF',\n            'hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger': 'hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF',\n            'hanlp.components.tok.NgramConvTokenizer': 'hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF',\n        }\n        cls = upgrade.get(cls, cls)\n    assert cls, f'{meta_filename} doesn\\'t contain classpath field'\n    try:\n        obj: Component = object_from_classpath(cls)\n        if hasattr(obj, 'load'):\n            if transform_only:\n                # noinspection PyUnresolvedReferences\n                obj.load_transform(save_dir)\n            else:\n                if os.path.isfile(os.path.join(save_dir, 'config.json')):\n                    obj.load(save_dir, verbose=verbose, **kwargs)\n                else:\n                    obj.load(metapath, **kwargs)\n            obj.config['load_path'] = load_path\n        return obj\n    except ModuleNotFoundError as e:\n        if isdebugging():\n            raise e from None\n        else:\n            raise ModuleNotFoundError(\n                f'Some modules ({e.name} etc.) required by this model are missing. Please install the full version:'\n                '\\n\\n\\tpip install hanlp[full] -U') from None\n    except ValueError as e:\n        if e.args and isinstance(e.args[0], str) and 'Internet connection' in e.args[0]:\n            raise ConnectionError(\n                'Hugging Face \ud83e\udd17 Transformers failed to download because your Internet connection is either off or bad.\\n'\n                'See https://hanlp.hankcs.com/docs/install.html#server-without-internet for solutions.') \\\n                from None\n        raise e from None\n    except Exception as e:\n        # Some users often install an incompatible tf and put the blame on HanLP. Teach them the basics.\n        try:\n            you_installed_wrong_versions, extras = check_version_conflicts(extras=('full',) if tf_model else None)\n        except Exception as check_e:\n            you_installed_wrong_versions, extras = None, None\n        if you_installed_wrong_versions:\n            raise version.NotCompatible(you_installed_wrong_versions + '\\nPlease reinstall HanLP in the proper way:' +\n                                        '\\n\\n\\tpip install --upgrade hanlp' + (\n                                            f'[{\",\".join(extras)}]' if extras else '')) from None\n        eprint(f'Failed to load {identifier}')\n        from pkg_resources import parse_version\n        model_version = meta.get(\"hanlp_version\", '2.0.0-alpha.0')\n        if model_version == '2.0.0':  # Quick fix: the first version used a wrong string\n            model_version = '2.0.0-alpha.0'\n        model_version = parse_version(model_version)\n        installed_version = parse_version(version.__version__)\n        try:\n            latest_version = get_latest_info_from_pypi()\n        except:\n            latest_version = None\n        if model_version > installed_version:\n            eprint(f'{identifier} was created with hanlp-{model_version}, '\n                   f'while you are running a lower version: {installed_version}. ')\n        if installed_version != latest_version:\n            eprint(\n                f'Please upgrade HanLP with:\\n'\n                f'\\n\\tpip install --upgrade hanlp\\n')\n        eprint(\n            'If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues\\n'\n            'When reporting an issue, make sure to paste the FULL ERROR LOG below.')\n\n        eprint(f'{\"ERROR LOG BEGINS\":=^80}')\n        import platform\n        eprint(f'OS: {platform.platform()}')\n        eprint(f'Python: {platform.python_version()}')\n        import torch\n        eprint(f'PyTorch: {torch.__version__}')\n        if tf_model:\n            try:\n                import tensorflow\n                tf_version = tensorflow.__version__\n                eprint(f'TensorFlow: {tf_version}')\n            except ModuleNotFoundError:\n                tf_version = 'not installed'\n                eprint(f'TensorFlow: {tf_version}')\n            except Exception as tf_e:\n                eprint(f'TensorFlow cannot be imported due to {tf_e.__class__.__name__}: {e}. '\n                       f'Note this is not a bug of HanLP, but rather a compatability issue caused by TensorFlow.')\n        eprint(f'HanLP: {version.__version__}')\n        import sys\n        sys.stderr.flush()\n        try:\n            if e.args and isinstance(e.args, tuple):\n                for i in range(len(e.args)):\n                    if isinstance(e.args[i], str):\n                        from hanlp_common.util import set_tuple_with\n                        e.args = set_tuple_with(e.args, e.args[i] + f'\\n{\"ERROR LOG ENDS\":=^80}', i)\n                        break\n        except:\n            pass\n        raise e from None\n\n\ndef load_from_meta(meta: dict) -> Component:\n    if 'load_path' in meta:\n        return load_from_meta_file(meta['load_path'])\n    cls = meta.get('class_path', None) or meta.get('classpath', None)\n    assert cls, f'{meta} doesn\\'t contain classpath field'\n    cls = str_to_type(cls)\n    return cls.from_config(meta)\n", "hanlp/utils/span_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-12 20:34\nimport warnings\nfrom typing import Dict, List, Tuple, Callable, Set, Optional\n\n\ndef generate_words_per_line(file_path):\n    with open(file_path, encoding='utf-8') as src:\n        for line in src:\n            cells = line.strip().split()\n            if not cells:\n                continue\n            yield cells\n\n\ndef words_to_bmes(words):\n    tags = []\n    for w in words:\n        if not w:\n            raise ValueError('{} contains None or zero-length word {}'.format(str(words), w))\n        if len(w) == 1:\n            tags.append('S')\n        else:\n            tags.extend(['B'] + ['M'] * (len(w) - 2) + ['E'])\n    return tags\n\n\ndef words_to_bi(words):\n    tags = []\n    for w in words:\n        if not w:\n            raise ValueError('{} contains None or zero-length word {}'.format(str(words), w))\n        tags.extend(['B'] + ['I'] * (len(w) - 1))\n    return tags\n\n\ndef bmes_to_words(chars, tags):\n    result = []\n    if len(chars) == 0:\n        return result\n    word = chars[0]\n\n    for c, t in zip(chars[1:], tags[1:]):\n        if t == 'B' or t == 'S':\n            result.append(word)\n            word = ''\n        word += c\n    if len(word) != 0:\n        result.append(word)\n\n    return result\n\n\ndef bmes_to_spans(tags):\n    result = []\n    offset = 0\n    pre_offset = 0\n    for t in tags[1:]:\n        offset += 1\n        if t == 'B' or t == 'S':\n            result.append((pre_offset, offset))\n            pre_offset = offset\n    if offset != len(tags):\n        result.append((pre_offset, len(tags)))\n\n    return result\n\n\ndef bmes_of(sentence, segmented):\n    if segmented:\n        chars = []\n        tags = []\n        words = sentence.split()\n        for w in words:\n            chars.extend(list(w))\n            if len(w) == 1:\n                tags.append('S')\n            else:\n                tags.extend(['B'] + ['M'] * (len(w) - 2) + ['E'])\n    else:\n        chars = list(sentence)\n        tags = ['S'] * len(chars)\n    return chars, tags\n\n\ndef iobes_to_bilou(src, dst):\n    with open(src) as src, open(dst, 'w') as out:\n        for line in src:\n            line = line.strip()\n            if not line:\n                out.write('\\n')\n                continue\n            word, tag = line.split('\\t')\n            if tag.startswith('E-'):\n                tag = 'L-' + tag[2:]\n            elif tag.startswith('S-'):\n                tag = 'U-' + tag[2:]\n            out.write(f'{word}\\t{tag}\\n')\n\n\ndef allowed_transitions(constraint_type: str, labels: Dict[int, str]) -> List[Tuple[int, int]]:\n    \"\"\"\n    Given labels and a constraint type, returns the allowed transitions. It will\n    additionally include transitions for the start and end states, which are used\n    by the conditional random field.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\".\n    labels : `Dict[int, str]`, required\n        A mapping {label_id -> label}. Most commonly this would be the value from\n        Vocabulary.get_index_to_token_vocabulary()\n\n    # Returns\n\n    `List[Tuple[int, int]]`\n        The allowed transitions (from_label_id, to_label_id).\n    \"\"\"\n    num_labels = len(labels)\n    start_tag = num_labels\n    end_tag = num_labels + 1\n    labels_with_boundaries = list(labels.items()) + [(start_tag, \"START\"), (end_tag, \"END\")]\n\n    allowed = []\n    for from_label_index, from_label in labels_with_boundaries:\n        if from_label in (\"START\", \"END\"):\n            from_tag = from_label\n            from_entity = \"\"\n        else:\n            from_tag = from_label[0]\n            from_entity = from_label[1:]\n        for to_label_index, to_label in labels_with_boundaries:\n            if to_label in (\"START\", \"END\"):\n                to_tag = to_label\n                to_entity = \"\"\n            else:\n                to_tag = to_label[0]\n                to_entity = to_label[1:]\n            if is_transition_allowed(constraint_type, from_tag, from_entity, to_tag, to_entity):\n                allowed.append((from_label_index, to_label_index))\n    return allowed\n\n\ndef is_transition_allowed(\n    constraint_type: str, from_tag: str, from_entity: str, to_tag: str, to_entity: str\n):\n    \"\"\"\n    Given a constraint type and strings `from_tag` and `to_tag` that\n    represent the origin and destination of the transition, return whether\n    the transition is allowed under the given constraint type.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\".\n    from_tag : `str`, required\n        The tag that the transition originates from. For example, if the\n        label is `I-PER`, the `from_tag` is `I`.\n    from_entity : `str`, required\n        The entity corresponding to the `from_tag`. For example, if the\n        label is `I-PER`, the `from_entity` is `PER`.\n    to_tag : `str`, required\n        The tag that the transition leads to. For example, if the\n        label is `I-PER`, the `to_tag` is `I`.\n    to_entity : `str`, required\n        The entity corresponding to the `to_tag`. For example, if the\n        label is `I-PER`, the `to_entity` is `PER`.\n\n    # Returns\n\n    `bool`\n        Whether the transition is allowed under the given `constraint_type`.\n    \"\"\"\n\n    if to_tag == \"START\" or from_tag == \"END\":\n        # Cannot transition into START or from END\n        return False\n\n    if constraint_type == \"BIOUL\":\n        if from_tag == \"START\":\n            return to_tag in (\"O\", \"B\", \"U\")\n        if to_tag == \"END\":\n            return from_tag in (\"O\", \"L\", \"U\")\n        return any(\n            [\n                # O can transition to O, B-* or U-*\n                # L-x can transition to O, B-*, or U-*\n                # U-x can transition to O, B-*, or U-*\n                from_tag in (\"O\", \"L\", \"U\") and to_tag in (\"O\", \"B\", \"U\"),\n                # B-x can only transition to I-x or L-x\n                # I-x can only transition to I-x or L-x\n                from_tag in (\"B\", \"I\") and to_tag in (\"I\", \"L\") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == \"BIO\":\n        if from_tag == \"START\":\n            return to_tag in (\"O\", \"B\")\n        if to_tag == \"END\":\n            return from_tag in (\"O\", \"B\", \"I\")\n        return any(\n            [\n                # Can always transition to O or B-x\n                to_tag in (\"O\", \"B\"),\n                # Can only transition to I-x from B-x or I-x\n                to_tag == \"I\" and from_tag in (\"B\", \"I\") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == \"IOB1\":\n        if from_tag == \"START\":\n            return to_tag in (\"O\", \"I\")\n        if to_tag == \"END\":\n            return from_tag in (\"O\", \"B\", \"I\")\n        return any(\n            [\n                # Can always transition to O or I-x\n                to_tag in (\"O\", \"I\"),\n                # Can only transition to B-x from B-x or I-x, where\n                # x is the same tag.\n                to_tag == \"B\" and from_tag in (\"B\", \"I\") and from_entity == to_entity,\n            ]\n        )\n    elif constraint_type == \"BMES\":\n        if from_tag == \"START\":\n            return to_tag in (\"B\", \"S\")\n        if to_tag == \"END\":\n            return from_tag in (\"E\", \"S\")\n        return any(\n            [\n                # Can only transition to B or S from E or S.\n                to_tag in (\"B\", \"S\") and from_tag in (\"E\", \"S\"),\n                # Can only transition to M-x from B-x, where\n                # x is the same tag.\n                to_tag == \"M\" and from_tag in (\"B\", \"M\") and from_entity == to_entity,\n                # Can only transition to E-x from B-x or M-x, where\n                # x is the same tag.\n                to_tag == \"E\" and from_tag in (\"B\", \"M\") and from_entity == to_entity,\n            ]\n        )\n    else:\n        raise ValueError(f\"Unknown constraint type: {constraint_type}\")\n\n\nTypedSpan = Tuple[int, Tuple[int, int]]\nTypedStringSpan = Tuple[str, Tuple[int, int]]\n\n\nclass InvalidTagSequence(Exception):\n    def __init__(self, tag_sequence=None):\n        super().__init__()\n        self.tag_sequence = tag_sequence\n\n    def __str__(self):\n        return \" \".join(self.tag_sequence)\n\n\nT = str\n\n\ndef enumerate_spans(\n        sentence: List[T],\n        offset: int = 0,\n        max_span_width: int = None,\n        min_span_width: int = 1,\n        filter_function: Callable[[List[T]], bool] = None,\n) -> List[Tuple[int, int]]:\n    \"\"\"\n    Given a sentence, return all token spans within the sentence. Spans are `inclusive`.\n    Additionally, you can provide a maximum and minimum span width, which will be used\n    to exclude spans outside of this range.\n\n    Finally, you can provide a function mapping `List[T] -> bool`, which will\n    be applied to every span to decide whether that span should be included. This\n    allows filtering by length, regex matches, pos tags or any Spacy `Token`\n    attributes, for example.\n\n    # Parameters\n\n    sentence : `List[T]`, required.\n        The sentence to generate spans for. The type is generic, as this function\n        can be used with strings, or Spacy `Tokens` or other sequences.\n    offset : `int`, optional (default = `0`)\n        A numeric offset to add to all span start and end indices. This is helpful\n        if the sentence is part of a larger structure, such as a document, which\n        the indices need to respect.\n    max_span_width : `int`, optional (default = `None`)\n        The maximum length of spans which should be included. Defaults to len(sentence).\n    min_span_width : `int`, optional (default = `1`)\n        The minimum length of spans which should be included. Defaults to 1.\n    filter_function : `Callable[[List[T]], bool]`, optional (default = `None`)\n        A function mapping sequences of the passed type T to a boolean value.\n        If `True`, the span is included in the returned spans from the\n        sentence, otherwise it is excluded..\n    \"\"\"\n    max_span_width = max_span_width or len(sentence)\n    filter_function = filter_function or (lambda x: True)\n    spans: List[Tuple[int, int]] = []\n\n    for start_index in range(len(sentence)):\n        last_end_index = min(start_index + max_span_width, len(sentence))\n        first_end_index = min(start_index + min_span_width - 1, len(sentence))\n        for end_index in range(first_end_index, last_end_index):\n            start = offset + start_index\n            end = offset + end_index\n            # add 1 to end index because span indices are inclusive.\n            if filter_function(sentence[slice(start_index, end_index + 1)]):\n                spans.append((start, end))\n    return spans\n\n\ndef bio_tags_to_spans(\n        tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    \"\"\"\n    Given a sequence corresponding to BIO tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans. This function works properly when\n    the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        # Actual BIO tag.\n        bio_tag = string_tag[0]\n        if bio_tag not in [\"B\", \"I\", \"O\"]:\n            raise InvalidTagSequence(tag_sequence)\n        conll_tag = string_tag[2:]\n        if bio_tag == \"O\" or conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n            # We don't care about tags we are\n            # told to ignore, so we do nothing.\n            continue\n        elif bio_tag == \"B\":\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n        elif bio_tag == \"I\" and conll_tag == active_conll_tag:\n            # We're inside a span.\n            span_end += 1\n        else:\n            # This is the case the bio label is an \"I\", but either:\n            # 1) the span hasn't started - i.e. an ill formed span.\n            # 2) The span is an I tag for a different conll annotation.\n            # We'll process the previous span if it exists, but also\n            # include this span. This is important, because otherwise,\n            # a model may get a perfect F1 score whilst still including\n            # false positive ill-formed spans.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef iob1_tags_to_spans(\n        tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    \"\"\"\n    Given a sequence corresponding to IOB1 tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded\n    by \"I-LABEL\" or \"B-LABEL\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    prev_bio_tag = None\n    prev_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        curr_bio_tag = string_tag[0]\n        curr_conll_tag = string_tag[2:]\n\n        if curr_bio_tag not in [\"B\", \"I\", \"O\"]:\n            raise InvalidTagSequence(tag_sequence)\n        if curr_bio_tag == \"O\" or curr_conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n        elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = curr_conll_tag\n            span_start = index\n            span_end = index\n        else:\n            # bio_tag == \"I\" and curr_conll_tag == active_conll_tag\n            # We're continuing a span.\n            span_end += 1\n\n        prev_bio_tag = string_tag[0]\n        prev_conll_tag = string_tag[2:]\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef _iob1_start_of_chunk(\n        prev_bio_tag: Optional[str],\n        prev_conll_tag: Optional[str],\n        curr_bio_tag: str,\n        curr_conll_tag: str,\n) -> bool:\n    if curr_bio_tag == \"B\":\n        return True\n    if curr_bio_tag == \"I\" and prev_bio_tag == \"O\":\n        return True\n    if curr_bio_tag != \"O\" and prev_conll_tag != curr_conll_tag:\n        return True\n    return False\n\n\ndef bioul_tags_to_spans(\n        tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    \"\"\"\n    Given a sequence corresponding to BIOUL tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"I\", \"O\", \"U\", and \"L\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n    \"\"\"\n    spans = []\n    classes_to_ignore = classes_to_ignore or []\n    index = 0\n    while index < len(tag_sequence):\n        label = tag_sequence[index]\n        if label[0] == \"U\":\n            spans.append((label.partition(\"-\")[2], (index, index)))\n        elif label[0] == \"B\":\n            start = index\n            while label[0] != \"L\":\n                index += 1\n                if index >= len(tag_sequence):\n                    raise InvalidTagSequence(tag_sequence)\n                label = tag_sequence[index]\n                if not (label[0] == \"I\" or label[0] == \"L\"):\n                    raise InvalidTagSequence(tag_sequence)\n            spans.append((label.partition(\"-\")[2], (start, index)))\n        else:\n            if label != \"O\":\n                raise InvalidTagSequence(tag_sequence)\n        index += 1\n    return [span for span in spans if span[0] not in classes_to_ignore]\n\n\ndef iobes_tags_to_spans(\n        tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    \"\"\"\n    Given a sequence corresponding to BIOUL tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"I\", \"O\", \"U\", and \"L\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n    \"\"\"\n    spans = []\n    classes_to_ignore = classes_to_ignore or []\n    index = 0\n    while index < len(tag_sequence):\n        label = tag_sequence[index]\n        if label[0] == \"S\":\n            spans.append((label.partition(\"-\")[2], (index, index)))\n        elif label[0] == \"B\":\n            start = index\n            while label[0] != \"E\":\n                index += 1\n                if index >= len(tag_sequence):\n                    raise InvalidTagSequence(tag_sequence)\n                label = tag_sequence[index]\n                if not (label[0] == \"I\" or label[0] == \"E\"):\n                    raise InvalidTagSequence(tag_sequence)\n            spans.append((label.partition(\"-\")[2], (start, index)))\n        else:\n            if label != \"O\":\n                raise InvalidTagSequence(tag_sequence)\n        index += 1\n    return [span for span in spans if span[0] not in classes_to_ignore]\n\n\ndef iob1_to_bioul(tag_sequence: List[str]) -> List[str]:\n    warnings.warn(\n        \"iob1_to_bioul has been replaced with 'to_bioul' to allow more encoding options.\",\n        FutureWarning,\n    )\n    return to_bioul(tag_sequence)\n\n\ndef to_bioul(tag_sequence: List[str], encoding: str = \"IOB1\") -> List[str]:\n    \"\"\"\n    Given a tag sequence encoded with IOB1 labels, recode to BIOUL.\n\n    In the IOB1 scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of span immediately following another\n    span of the same type.\n\n    In the BIO scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of a span.\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"].\n    encoding : `str`, optional, (default = `\"IOB1\"`).\n        The encoding type to convert from. Must be either \"IOB1\" or \"BIO\".\n\n    # Returns\n\n    bioul_sequence : `List[str]`\n        The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    \"\"\"\n    if encoding not in {\"IOB1\", \"BIO\"}:\n        raise ValueError(f\"Invalid encoding {encoding} passed to 'to_bioul'.\")\n\n    def replace_label(full_label, new_label):\n        # example: full_label = 'I-PER', new_label = 'U', returns 'U-PER'\n        parts = list(full_label.partition(\"-\"))\n        parts[0] = new_label\n        return \"\".join(parts)\n\n    def pop_replace_append(in_stack, out_stack, new_label):\n        # pop the last element from in_stack, replace the label, append\n        # to out_stack\n        tag = in_stack.pop()\n        new_tag = replace_label(tag, new_label)\n        out_stack.append(new_tag)\n\n    def process_stack(stack, out_stack):\n        # process a stack of labels, add them to out_stack\n        if len(stack) == 1:\n            # just a U token\n            pop_replace_append(stack, out_stack, \"U\")\n        else:\n            # need to code as BIL\n            recoded_stack = []\n            pop_replace_append(stack, recoded_stack, \"L\")\n            while len(stack) >= 2:\n                pop_replace_append(stack, recoded_stack, \"I\")\n            pop_replace_append(stack, recoded_stack, \"B\")\n            recoded_stack.reverse()\n            out_stack.extend(recoded_stack)\n\n    # Process the tag_sequence one tag at a time, adding spans to a stack,\n    # then recode them.\n    bioul_sequence = []\n    stack: List[str] = []\n\n    for label in tag_sequence:\n        # need to make a dict like\n        # token = {'token': 'Matt', \"labels\": {'conll2003': \"B-PER\"}\n        #                   'gold': 'I-PER'}\n        # where 'gold' is the raw value from the CoNLL data set\n\n        if label == \"O\" and len(stack) == 0:\n            bioul_sequence.append(label)\n        elif label == \"O\" and len(stack) > 0:\n            # need to process the entries on the stack plus this one\n            process_stack(stack, bioul_sequence)\n            bioul_sequence.append(label)\n        elif label[0] == \"I\":\n            # check if the previous type is the same as this one\n            # if it is then append to stack\n            # otherwise this start a new entity if the type\n            # is different\n            if len(stack) == 0:\n                if encoding == \"BIO\":\n                    raise InvalidTagSequence(tag_sequence)\n                stack.append(label)\n            else:\n                # check if the previous type is the same as this one\n                this_type = label.partition(\"-\")[2]\n                prev_type = stack[-1].partition(\"-\")[2]\n                if this_type == prev_type:\n                    stack.append(label)\n                else:\n                    if encoding == \"BIO\":\n                        raise InvalidTagSequence(tag_sequence)\n                    # a new entity\n                    process_stack(stack, bioul_sequence)\n                    stack.append(label)\n        elif label[0] == \"B\":\n            if len(stack) > 0:\n                process_stack(stack, bioul_sequence)\n            stack.append(label)\n        else:\n            raise InvalidTagSequence(tag_sequence)\n\n    # process the stack\n    if len(stack) > 0:\n        process_stack(stack, bioul_sequence)\n\n    return bioul_sequence\n\n\ndef bmes_tags_to_spans(\n        tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[TypedStringSpan]:\n    \"\"\"\n    Given a sequence corresponding to BMES tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"M\", \"E\" and \"S\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n\n    def extract_bmes_tag_label(text):\n        bmes_tag = text[0]\n        label = text[2:]\n        return bmes_tag, label\n\n    spans: List[Tuple[str, List[int]]] = []\n    prev_bmes_tag: Optional[str] = None\n    for index, tag in enumerate(tag_sequence):\n        bmes_tag, label = extract_bmes_tag_label(tag)\n        if bmes_tag in (\"B\", \"S\"):\n            # Regardless of tag, we start a new span when reaching B & S.\n            spans.append((label, [index, index]))\n        elif bmes_tag in (\"M\", \"E\") and prev_bmes_tag in (\"B\", \"M\") and spans[-1][0] == label:\n            # Only expand the span if\n            # 1. Valid transition: B/M -> M/E.\n            # 2. Matched label.\n            spans[-1][1][1] = index\n        else:\n            # Best effort split for invalid span.\n            spans.append((label, [index, index]))\n        # update previous BMES tag.\n        prev_bmes_tag = bmes_tag\n\n    classes_to_ignore = classes_to_ignore or []\n    return [\n        # to tuple.\n        (span[0], (span[1][0], span[1][1]))\n        for span in spans\n        if span[0] not in classes_to_ignore\n    ]", "hanlp/utils/torch_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-09 15:52\nimport os\nimport random\nimport time\nfrom typing import List, Union, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit, nvmlShutdown, nvmlDeviceGetCount\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom hanlp.utils.io_util import get_resource, replace_ext, TimingFileIterator\nfrom hanlp.utils.log_util import logger, flash\nfrom hanlp_common.constant import HANLP_VERBOSE\nfrom hanlp_common.io import load_pickle, save_pickle\n\n\ndef gpus_available() -> Dict[int, float]:\n    if not torch.cuda.is_available():\n        return dict()\n    try:\n        nvmlInit()\n        gpus = {}\n        visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None)\n        if visible_devices is None:\n            visible_devices = list(range(nvmlDeviceGetCount()))\n        else:\n            visible_devices = {int(x.strip()) for x in visible_devices.split(',')}\n        for i, real_id in enumerate(visible_devices):\n            h = nvmlDeviceGetHandleByIndex(real_id)\n            info = nvmlDeviceGetMemoryInfo(h)\n            total = info.total\n            free = info.free\n            ratio = free / total\n            gpus[i] = ratio\n            # print(f'total    : {info.total}')\n            # print(f'free     : {info.free}')\n            # print(f'used     : {info.used}')\n            # t = torch.cuda.get_device_properties(0).total_memory\n            # c = torch.cuda.memory_cached(0)\n            # a = torch.cuda.memory_allocated(0)\n            # print(t, c, a)\n        nvmlShutdown()\n        return dict(sorted(gpus.items(), key=lambda x: x[1], reverse=True))\n    except Exception as e:\n        logger.debug(f'Failed to get gpu info due to {e}')\n        return dict((i, 1.0) for i in range(torch.cuda.device_count()))\n\n\ndef cuda_devices(query=None) -> List[int]:\n    \"\"\"Decide which GPUs to use\n\n    Args:\n      query:  (Default value = None)\n\n    Returns:\n\n    \n    \"\"\"\n    if isinstance(query, list):\n        if len(query) == 0:\n            return [-1]\n        return query\n    if query is None:\n        query = gpus_available()\n        if not query:\n            return []\n        size, idx = max((v, k) for k, v in query.items())\n        # When multiple GPUs have the same size, randomly pick one to avoid conflicting\n        gpus_with_same_size = [k for k, v in query.items() if v == size]\n        query = random.choice(gpus_with_same_size)\n    if isinstance(query, float):\n        gpus = gpus_available()\n        if not query:\n            return []\n        query = [k for k, v in gpus.items() if v > query]\n    elif isinstance(query, int):\n        query = [query]\n    return query\n\n\ndef pad_lists(sequences: List[List], dtype=torch.long, padding_value=0):\n    return pad_sequence([torch.tensor(x, dtype=dtype) for x in sequences], True, padding_value)\n\n\ndef set_seed(seed=233, dont_care_speed=False):\n    \"\"\"Copied from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L76\n\n    Args:\n      seed:  (Default value = 233)\n      dont_care_speed: True may have a negative single-run performance impact, but ensures deterministic\n\n    Returns:\n\n    \n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    # ^^ safe to call this function even if cuda is not available\n    torch.cuda.manual_seed_all(seed)\n    if dont_care_speed:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\ndef batched_index_select(input, index, dim=1):\n    \"\"\"\n\n    Args:\n      input: B x * x ... x *\n      index: B x M\n      dim:  (Default value = 1)\n\n    Returns:\n\n    \n    \"\"\"\n    views = [input.shape[0]] + [1 if i != dim else -1 for i in range(1, len(input.shape))]\n    expanse = list(input.shape)\n    expanse[0] = -1\n    expanse[dim] = -1\n    index = index.view(views).expand(expanse)\n    return torch.gather(input, dim, index)\n\n\ndef truncated_normal_(tensor, mean=0, std=1):\n    size = tensor.shape\n    tmp = tensor.new_empty(size + (4,)).normal_()\n    valid = (tmp < 2) & (tmp > -2)\n    ind = valid.max(-1, keepdim=True)[1]\n    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n    tensor.data.mul_(std).add_(mean)\n    return tensor\n\n\ndef dtype_of(e: Union[int, bool, float]):\n    if isinstance(e, bool):\n        return torch.bool\n    if isinstance(e, int):\n        return torch.long\n    if isinstance(e, float):\n        return torch.float\n    raise ValueError(f'Unsupported type of {repr(e)}')\n\n\ndef mean_model(model: torch.nn.Module):\n    return float(torch.mean(torch.stack([torch.sum(p) for p in model.parameters() if p.requires_grad])))\n\n\ndef main():\n    start = time.time()\n    print(gpus_available())\n    print(time.time() - start)\n    # print(gpus_available())\n    # print(cuda_devices())\n    # print(cuda_devices(0.1))\n\n\nif __name__ == '__main__':\n    main()\n\n\ndef clip_grad_norm(model: nn.Module, grad_norm, transformer: nn.Module = None, transformer_grad_norm=None):\n    if transformer_grad_norm is None:\n        if grad_norm is not None:\n            nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_norm)\n    else:\n        is_transformer = []\n        non_transformer = []\n        transformer = set(transformer.parameters())\n        for p in model.parameters():\n            if not p.requires_grad:\n                continue\n            if p in transformer:\n                is_transformer.append(p)\n            else:\n                non_transformer.append(p)\n        nn.utils.clip_grad_norm_(non_transformer, grad_norm)\n        nn.utils.clip_grad_norm_(is_transformer, transformer_grad_norm)\n\n\ndef load_word2vec(path, delimiter=' ', cache=True) -> Tuple[Dict[str, np.ndarray], int]:\n    realpath = get_resource(path)\n    binpath = replace_ext(realpath, '.pkl')\n    if cache:\n        try:\n            flash('Loading word2vec from cache [blink][yellow]...[/yellow][/blink]')\n            word2vec, dim = load_pickle(binpath)\n            flash('')\n            return word2vec, dim\n        except IOError:\n            pass\n\n    dim = None\n    word2vec = dict()\n    f = TimingFileIterator(realpath)\n    for idx, line in enumerate(f):\n        f.log('Loading word2vec from text file [blink][yellow]...[/yellow][/blink]')\n        line = line.rstrip().split(delimiter)\n        if len(line) > 2:\n            if dim is None:\n                dim = len(line)\n            else:\n                if len(line) != dim:\n                    logger.warning('{}#{} length mismatches with {}'.format(path, idx + 1, dim))\n                    continue\n            word, vec = line[0], line[1:]\n            word2vec[word] = np.array(vec, dtype=np.float32)\n    dim -= 1\n    if cache:\n        flash('Caching word2vec [blink][yellow]...[/yellow][/blink]')\n        save_pickle((word2vec, dim), binpath)\n        flash('')\n    return word2vec, dim\n\n\ndef load_word2vec_as_vocab_tensor(path, delimiter=' ', cache=True) -> Tuple[Dict[str, int], torch.Tensor]:\n    realpath = get_resource(path)\n    vocab_path = replace_ext(realpath, '.vocab')\n    matrix_path = replace_ext(realpath, '.pt')\n    if cache:\n        try:\n            if HANLP_VERBOSE:\n                flash('Loading vocab and matrix from cache [blink][yellow]...[/yellow][/blink]')\n            vocab = load_pickle(vocab_path)\n            matrix = torch.load(matrix_path, map_location='cpu')\n            if HANLP_VERBOSE:\n                flash('')\n            return vocab, matrix\n        except IOError:\n            pass\n\n    word2vec, dim = load_word2vec(path, delimiter, cache)\n    vocab = dict((k, i) for i, k in enumerate(word2vec.keys()))\n    matrix = torch.Tensor(np.stack(list(word2vec.values())))\n    if cache:\n        flash('Caching vocab and matrix [blink][yellow]...[/yellow][/blink]')\n        save_pickle(vocab, vocab_path)\n        torch.save(matrix, matrix_path)\n        flash('')\n    return vocab, matrix\n\n\ndef save_word2vec(word2vec: dict, filepath, delimiter=' '):\n    with open(filepath, 'w', encoding='utf-8') as out:\n        for w, v in word2vec.items():\n            out.write(f'{w}{delimiter}')\n            out.write(f'{delimiter.join(str(x) for x in v)}\\n')\n\n\ndef lengths_to_mask(seq_len, max_len=None):\n    r\"\"\"\n    .. code-block::\n\n        >>> seq_len = torch.arange(2, 16)\n        >>> mask = lengths_to_mask(seq_len)\n        >>> print(mask.size())\n        torch.Size([14, 15])\n        >>> seq_len = np.arange(2, 16)\n        >>> mask = lengths_to_mask(seq_len)\n        >>> print(mask.shape)\n        (14, 15)\n        >>> seq_len = torch.arange(2, 16)\n        >>> mask = lengths_to_mask(seq_len, max_len=100)\n        >>>print(mask.size())\n        torch.Size([14, 100])\n\n    :param torch.LongTensor seq_len: (B,)\n    :param int max_len: max sequence length\u3002\n    :return:  torch.Tensor  (B, max_len)\n    \"\"\"\n    assert seq_len.dim() == 1, f\"seq_len can only have one dimension, got {seq_len.dim() == 1}.\"\n    batch_size = seq_len.size(0)\n    max_len = int(max_len) if max_len else seq_len.max().long()\n    broad_cast_seq_len = torch.arange(max_len).expand(batch_size, -1).to(seq_len)\n    mask = broad_cast_seq_len.lt(seq_len.unsqueeze(1))\n\n    return mask\n\n\ndef activation_from_name(name: str):\n    return getattr(torch.nn, name)\n\n\ndef filter_state_dict_safely(model_state: dict, load_state: dict):\n    safe_state = dict()\n    for k, v in load_state.items():\n        model_v = model_state.get(k, None)\n        if model_v is not None and model_v.shape == v.shape:\n            safe_state[k] = v\n    return safe_state\n", "hanlp/utils/tf_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-27 01:27\nimport json\nimport logging\nimport os\nimport random\nfrom typing import List\n\nimport numpy as np\n\nfrom hanlp_common.constant import PAD\n\n\ndef set_gpu(idx=0):\n    \"\"\"Restrict TensorFlow to only use the GPU of idx\n\n    Args:\n      idx:  (Default value = 0)\n\n    Returns:\n\n    \n    \"\"\"\n    gpus = get_visible_gpus()\n    if gpus:\n        try:\n            tf.config.experimental.set_visible_devices(gpus[idx], 'GPU')\n            logical_devices = tf.config.experimental.list_logical_devices('GPU')\n            assert len(logical_devices) == 1\n        except RuntimeError as e:\n            # Virtual devices must be set before GPUs have been initialized\n            # print(e)\n            raise e\n\n\ndef get_visible_gpus():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    return gpus\n\n\ndef set_gpu_memory_growth(growth=True):\n    gpus = get_visible_gpus()\n    if gpus:\n        try:\n            # Currently, memory growth needs to be the same across GPUs\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, growth)\n        except RuntimeError as e:\n            # Memory growth must be set before GPUs have been initialized\n            # print(e)\n            raise e\n\n\ndef nice_gpu():\n    \"\"\"Use GPU nicely.\"\"\"\n    set_gpu_memory_growth()\n    set_gpu()\n\n\ndef shut_up_python_logging():\n    logging.getLogger('tensorflow').setLevel(logging.ERROR)\n    import absl.logging\n    logging.root.removeHandler(absl.logging._absl_handler)\n    absl.logging._warn_preinit_stderr = False\n\n\ndef set_tf_loglevel(level=logging.ERROR):\n    if level >= logging.FATAL:\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'\n    if level >= logging.ERROR:\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n        os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2'\n    if level >= logging.WARNING:\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n        os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '1'\n    else:\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n        os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n    shut_up_python_logging()\n    logging.getLogger('tensorflow').setLevel(level)\n\n\nset_tf_loglevel()\n\nshut_up_python_logging()\nimport tensorflow as tf\n\nnice_gpu()\n\n\ndef size_of_dataset(dataset: tf.data.Dataset) -> int:\n    count = 0\n    for element in dataset.unbatch().batch(1):\n        count += 1\n    return count\n\n\ndef summary_of_model(model: tf.keras.Model):\n    \"\"\"https://stackoverflow.com/a/53668338/3730690\n\n    Args:\n      model: tf.keras.Model: \n\n    Returns:\n\n    \n    \"\"\"\n    if not model.built:\n        return 'model structure unknown until calling fit() with some data'\n    line_list = []\n    model.summary(print_fn=lambda x: line_list.append(x))\n    summary = \"\\n\".join(line_list)\n    return summary\n\n\ndef register_custom_cls(custom_cls, name=None):\n    if not name:\n        name = custom_cls.__name__\n    tf.keras.utils.get_custom_objects()[name] = custom_cls\n\n\ndef set_seed_tf(seed=233):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef nice():\n    nice_gpu()\n    set_seed_tf()\n\n\ndef hanlp_register(arg):\n    \"\"\"Registers a class with the Keras serialization framework.\n\n    Args:\n      arg: \n\n    Returns:\n\n    \"\"\"\n    class_name = arg.__name__\n    registered_name = 'HanLP' + '>' + class_name\n\n    # if tf_inspect.isclass(arg) and not hasattr(arg, 'get_config'):\n    #     raise ValueError(\n    #         'Cannot register a class that does not have a get_config() method.')\n\n    tf.keras.utils.get_custom_objects()[registered_name] = arg\n\n    return arg\n\n\ndef tensor_is_eager(tensor: tf.Tensor):\n    return hasattr(tensor, 'numpy')\n\n\ndef copy_mask(src: tf.Tensor, dst: tf.Tensor):\n    mask = getattr(src, '_keras_mask', None)\n    if mask is not None:\n        dst._keras_mask = mask\n    return mask\n\n\ndef get_callback_by_class(callbacks: List[tf.keras.callbacks.Callback], cls) -> tf.keras.callbacks.Callback:\n    for callback in callbacks:\n        if isinstance(callback, cls):\n            return callback\n\n\ndef tf_bernoulli(shape, p, dtype=None):\n    return tf.keras.backend.random_binomial(shape, p, dtype)\n\n\ndef str_tensor_to_str(str_tensor: tf.Tensor) -> str:\n    return str_tensor.numpy().decode('utf-8')\n\n\ndef str_tensor_2d_to_list(str_tensor: tf.Tensor, pad=PAD) -> List[List[str]]:\n    l = []\n    for i in str_tensor:\n        sent = []\n        for j in i:\n            j = str_tensor_to_str(j)\n            if j == pad:\n                break\n            sent.append(j)\n        l.append(sent)\n    return l\n\n\ndef str_tensor_to_list(pred):\n    return [tag.predict('utf-8') for tag in pred]\n\n\ndef format_metrics(metrics: List[tf.keras.metrics.Metric]):\n    return ' - '.join(f'{m.name}: {m.result():.4f}' for m in metrics)\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        \"\"\"Special json encoder for numpy types\n        See https://interviewbubble.com/typeerror-object-of-type-float32-is-not-json-serializable/\n\n        Args:\n            obj: Object to be json encoded.\n\n        Returns:\n            Json string.\n        \"\"\"\n        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n                            np.int16, np.int32, np.int64, np.uint8,\n                            np.uint16, np.uint32, np.uint64)):\n            return int(obj)\n        elif isinstance(obj, (np.float_, np.float16, np.float32,\n                              np.float64)):\n            return float(obj)\n        elif isinstance(obj, (np.ndarray,)):  #### This is the fix\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)", "hanlp/utils/rules.py": "import re\n\n_SEPARATOR = r'@'\n_RE_SENTENCE = re.compile(r'(\\S.+?[.!?])(?=\\s+|$)|(\\S.+?)(?=[\\n]|$)', re.UNICODE)\n_AB_SENIOR = re.compile(r'([A-Z][a-z]{1,2}\\.)\\s(\\w)', re.UNICODE)\n_AB_ACRONYM = re.compile(r'(\\.[a-zA-Z]\\.)\\s(\\w)', re.UNICODE)\n_UNDO_AB_SENIOR = re.compile(r'([A-Z][a-z]{1,2}\\.)' + _SEPARATOR + r'(\\w)', re.UNICODE)\n_UNDO_AB_ACRONYM = re.compile(r'(\\.[a-zA-Z]\\.)' + _SEPARATOR + r'(\\w)', re.UNICODE)\n\n\ndef _replace_with_separator(text, separator, regexs):\n    replacement = r\"\\1\" + separator + r\"\\2\"\n    result = text\n    for regex in regexs:\n        result = regex.sub(replacement, result)\n    return result\n\n\ndef split_sentence(text, best=True):\n    text = re.sub(r'([\u3002\uff01\uff1f?])([^\u201d\u2019])', r\"\\1\\n\\2\", text)\n    text = re.sub(r'(\\.{6})([^\u201d\u2019])', r\"\\1\\n\\2\", text)\n    text = re.sub(r'(\u2026{2})([^\u201d\u2019])', r\"\\1\\n\\2\", text)\n    text = re.sub(r'([\u3002\uff01\uff1f?][\u201d\u2019])([^\uff0c\u3002\uff01\uff1f?])', r'\\1\\n\\2', text)\n    for chunk in text.split(\"\\n\"):\n        chunk = chunk.strip()\n        if not chunk:\n            continue\n        if not best:\n            yield chunk\n            continue\n        processed = _replace_with_separator(chunk, _SEPARATOR, [_AB_SENIOR, _AB_ACRONYM])\n        sents = list(_RE_SENTENCE.finditer(processed))\n        if not sents:\n            yield chunk\n            continue\n        for sentence in sents:\n            sentence = _replace_with_separator(sentence.group(), r\" \", [_UNDO_AB_SENIOR, _UNDO_AB_ACRONYM])\n            yield sentence\n\n\n", "hanlp/utils/string_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-25 00:19\nimport unicodedata\nfrom typing import List, Dict, Tuple\n\n\ndef format_scores(results: Dict[str, float]) -> str:\n    return ' - '.join(f'{k}: {v:.4f}' for (k, v) in results.items())\n\n\ndef ispunct(token):\n    return all(unicodedata.category(char).startswith('P')\n               for char in token)\n\n\ndef split_long_sentence_into(tokens: List[str], max_seq_length, sent_delimiter=None, char_level=False,\n                             hard_constraint=False):\n    punct_offset = [i for i, x in enumerate(tokens) if\n                    ((sent_delimiter and x in sent_delimiter) or (not sent_delimiter and ispunct(x)))]\n    if not punct_offset:\n        # treat every token as punct\n        punct_offset = [i for i in range(len(tokens))]\n    punct_offset += [len(tokens)]\n    token_to_char_offset = []\n    if char_level:\n        offset = 0\n        for token in tokens:\n            token_to_char_offset.append(offset)\n            offset += len(token)\n        token_to_char_offset.append(offset)\n\n    start = 0\n    for i, offset in enumerate(punct_offset[:-1]):\n        end = punct_offset[i + 1]\n        length_at_next_punct = _len(start, end, token_to_char_offset, char_level)\n        if length_at_next_punct >= max_seq_length:\n            if hard_constraint:\n                yield from _gen_short_sent(tokens, start, offset, max_seq_length, token_to_char_offset, char_level)\n            else:\n                yield tokens[start: offset + 1]\n            start = offset + 1\n    offset = punct_offset[-1]\n    if start < offset:\n        offset -= 1\n        length_at_next_punct = _len(start, offset, token_to_char_offset, char_level)\n        if length_at_next_punct >= max_seq_length and hard_constraint:\n            yield from _gen_short_sent(tokens, start, offset, max_seq_length, token_to_char_offset, char_level)\n        else:\n            yield tokens[start:]\n\n\ndef _gen_short_sent(tokens, start, offset, max_seq_length, token_to_char_offset, char_level):\n    while start <= offset:\n        for j in range(offset + 1, start, -1):\n            if _len(start, j, token_to_char_offset, char_level) <= max_seq_length or j == start + 1:\n                yield tokens[start: j]\n                start = j\n                break\n\n\ndef _len(start, end, token_to_char_offset, char_level):\n    if char_level:\n        length_at_next_punct = token_to_char_offset[end] - token_to_char_offset[start]\n    else:\n        length_at_next_punct = end - start\n    return length_at_next_punct\n\n\ndef guess_delimiter(tokens):\n    if all(ord(c) < 128 for c in ''.join(tokens)):\n        delimiter_in_entity = ' '\n    else:\n        delimiter_in_entity = ''\n    return delimiter_in_entity\n\n\ndef split_long_sent(sent, delimiters, max_seq_length):\n    parts = []\n    offset = 0\n    for idx, char in enumerate(sent):\n        if char in delimiters:\n            parts.append(sent[offset:idx + 1])\n            offset = idx + 1\n    if not parts:\n        yield sent\n        return\n    short = []\n    for idx, part in enumerate(parts):\n        short += part\n        if idx == len(parts) - 1:\n            yield short\n        else:\n            if len(short) + len(parts[idx + 1]) > max_seq_length:\n                yield short\n                short = []\n\n\ndef possible_tokenization(text: str) -> List[Tuple[str]]:\n    \"\"\"Enumerate all possible tokenizations of a text.\n\n    Args:\n        text: A text.\n\n    Returns: All possible tokenizations.\n\n    \"\"\"\n    states = [((), ())]\n    for c in text:\n        new_states = []\n        for t, b in states:\n            # to split\n            new_states.append((t + (''.join(b + (c,)),), ()))\n            # not to split\n            new_states.append((t, b + (c,)))\n        states = new_states\n    return [t for t, b in states if not b]\n", "hanlp/utils/io_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 15:02\nimport contextlib\nimport glob\nimport gzip\nimport json\nimport logging\nimport os\nimport platform\nimport random\nimport shlex\nimport shutil\nimport sys\nimport tarfile\nimport tempfile\nimport urllib\nimport zipfile\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom subprocess import Popen, PIPE\nfrom typing import Tuple, Optional, Union, List\nfrom urllib.parse import urlparse\nfrom urllib.request import urlretrieve\n\nfrom hanlp_downloader import Downloader\nfrom hanlp_downloader.log import DownloadCallback\nfrom pkg_resources import parse_version\n\nimport hanlp\nfrom hanlp_common.constant import HANLP_URL, HANLP_VERBOSE\nfrom hanlp.utils.log_util import logger, cprint, remove_color_tag\nfrom hanlp.utils.string_util import split_long_sentence_into\nfrom hanlp.utils.time_util import now_filename, CountdownTimer\nfrom hanlp.version import __version__\nfrom hanlp_common.io import eprint\n\n\ndef load_jsonl(path, verbose=False):\n    if verbose:\n        src = TimingFileIterator(path)\n    else:\n        src = open(path, encoding='utf-8')\n    for line in src:\n        yield json.loads(line)\n    if not verbose:\n        src.close()\n\n\ndef make_debug_corpus(path, delimiter=None, percentage=0.1, max_samples=100):\n    files = []\n    if os.path.isfile(path):\n        files.append(path)\n    elif os.path.isdir(path):\n        files += [os.path.join(path, f) for f in os.listdir(path) if\n                  os.path.isfile(os.path.join(path, f)) and '.debug' not in f and not f.startswith('.')]\n    else:\n        raise FileNotFoundError(path)\n    for filepath in files:\n        filename, file_extension = os.path.splitext(filepath)\n        if not delimiter:\n            if file_extension in {'.tsv', '.conll', '.conllx', '.conllu'}:\n                delimiter = '\\n\\n'\n            else:\n                delimiter = '\\n'\n        with open(filepath, encoding='utf-8') as src, open(filename + '.debug' + file_extension, 'w',\n                                                           encoding='utf-8') as out:\n            samples = src.read().strip().split(delimiter)\n            max_samples = min(max_samples, int(len(samples) * percentage))\n            out.write(delimiter.join(samples[:max_samples]))\n\n\ndef path_join(path, *paths):\n    return os.path.join(path, *paths)\n\n\ndef makedirs(path):\n    os.makedirs(path, exist_ok=True)\n    return path\n\n\ndef tempdir(name=None):\n    path = tempfile.gettempdir()\n    if name:\n        path = makedirs(path_join(path, name))\n    return path\n\n\ndef tempdir_human():\n    return tempdir(now_filename())\n\n\ndef hanlp_home_default():\n    \"\"\"Default data directory depending on the platform and environment variables\"\"\"\n    if windows():\n        return os.path.join(os.environ.get('APPDATA'), 'hanlp')\n    else:\n        return os.path.join(os.path.expanduser(\"~\"), '.hanlp')\n\n\ndef windows():\n    system = platform.system()\n    return system == 'Windows'\n\n\ndef hanlp_home():\n    \"\"\" Home directory for HanLP resources.\n\n    Returns:\n        Data directory in the filesystem for storage, for example when downloading models.\n\n    This home directory can be customized with the following shell command or equivalent environment variable on Windows\n    systems.\n\n    .. highlight:: bash\n    .. code-block:: bash\n\n        $ export HANLP_HOME=/data/hanlp\n\n    \"\"\"\n    return os.getenv('HANLP_HOME', hanlp_home_default())\n\n\ndef file_exist(filename) -> bool:\n    return os.path.isfile(filename)\n\n\ndef remove_file(filename):\n    if file_exist(filename):\n        os.remove(filename)\n\n\ndef parent_dir(path):\n    return os.path.normpath(os.path.join(path, os.pardir))\n\n\ndef download(url, save_path=None, save_dir=hanlp_home(), prefix=HANLP_URL, append_location=True, verbose=HANLP_VERBOSE):\n    if not save_path:\n        save_path = path_from_url(url, save_dir, prefix, append_location)\n    if os.path.isfile(save_path):\n        if verbose:\n            eprint('Using local {}, ignore {}'.format(save_path, url))\n        return save_path\n    else:\n        makedirs(parent_dir(save_path))\n        if verbose:\n            eprint('Downloading {} to {}'.format(url, save_path))\n        tmp_path = '{}.downloading'.format(save_path)\n        remove_file(tmp_path)\n        try:\n            downloader = Downloader(url, tmp_path, 4, headers={\n                'User-agent': f'HanLP/{__version__} ({platform.platform()})'})\n            if verbose:\n                downloader.subscribe(DownloadCallback(show_header=False))\n            downloader.start_sync()\n        except BaseException as e:\n            remove_file(tmp_path)\n            url = url.split('#')[0]\n            try:\n                installed_version, latest_version = check_outdated()\n            except:\n                installed_version, latest_version = None, None  # No Internet\n            if installed_version != latest_version:\n                # Always prompt user to upgrade whenever a new version is available\n                hints = f'[green]Please upgrade to the latest version ({latest_version}) with:[/green]' \\\n                        f'\\n\\n\\t[yellow]pip install -U hanlp[/yellow]\\n'\n            else:  # Otherwise, prompt user to re-try\n                hints = f'[green]Please re-try or download it to {save_path} by yourself '\n                if not windows():\n                    hints += f'with:[/green]\\n\\n\\t[yellow]wget {url} -O {save_path}[/yellow]\\n\\n'\n                else:\n                    hints += 'using some decent downloading tools.[/green]\\n'\n                if not url.startswith(HANLP_URL):\n                    hints += 'For third party data, unrestricted connectivity to the global network may be required.'\n                else:\n                    hints += 'See also https://hanlp.hankcs.com/docs/install.html#install-models for instructions.'\n            message = f'Download failed due to [red]{repr(e)}[/red].\\n' \\\n                      f'{hints}'\n            if verbose:\n                cprint(message)\n            if hasattr(e, 'msg'):\n                e.msg += '\\n' + remove_color_tag(message)\n            elif hasattr(e, 'args') and e.args and isinstance(e.args, tuple) and isinstance(e.args[0], str):\n                e.args = (e.args[0] + '\\n' + remove_color_tag(message),) + e.args[1:]\n            raise e from None\n        remove_file(save_path)\n        os.rename(tmp_path, save_path)\n    return save_path\n\n\ndef parse_url_path(url):\n    parsed: urllib.parse.ParseResult = urlparse(url)\n    path = parsed.path.strip('/')\n    return parsed.netloc, path\n\n\ndef uncompress(path, dest=None, remove=True, verbose=HANLP_VERBOSE):\n    \"\"\"Uncompress a file and clean up uncompressed files once an error is triggered.\n\n    Args:\n      path: The path to a compressed file\n      dest: The dest folder.\n      remove: Remove archive file after decompression.\n      verbose: ``True`` to print log message.\n\n    Returns:\n        Destination path.\n    \n    \"\"\"\n    # assert path.endswith('.zip')\n    prefix, ext = split_if_compressed(path)\n    folder_name = os.path.basename(prefix)\n    file_is_zip = ext == '.zip'\n    root_of_folder = None\n    if ext == '.gz':\n        try:\n            with gzip.open(path, 'rb') as f_in, open(prefix, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        except Exception as e:\n            remove_file(prefix)\n            remove_file(path)\n            raise e\n    else:\n        try:\n            with zipfile.ZipFile(path, \"r\") if ext == '.zip' else tarfile.open(path, 'r:*') as archive:\n                if not dest:\n                    namelist = sorted(archive.namelist() if file_is_zip else archive.getnames())\n                    if namelist[0] == '.':\n                        namelist = namelist[1:]\n                        namelist = [p[len('./'):] if p.startswith('./') else p for p in namelist]\n                    if ext == '.tgz':\n                        roots = set(x.split('/')[0] for x in namelist)\n                        if len(roots) == 1:\n                            root_of_folder = next(iter(roots))\n                    else:\n                        # only one file, root_of_folder = ''\n                        root_of_folder = namelist[0].strip('/') if len(namelist) > 1 else ''\n                    if all(f.split('/')[0] == root_of_folder for f in namelist[1:]) or not root_of_folder:\n                        dest = os.path.dirname(path)  # only one folder, unzip to the same dir\n                    else:\n                        root_of_folder = None\n                        dest = prefix  # assume zip contains more than one file or folder\n                if verbose:\n                    eprint('Decompressing {} to {}'.format(path, dest))\n                archive.extractall(dest)\n                if root_of_folder:\n                    if root_of_folder != folder_name:\n                        # move root to match folder name\n                        os.rename(path_join(dest, root_of_folder), path_join(dest, folder_name))\n                    dest = path_join(dest, folder_name)\n                elif len(namelist) == 1:\n                    dest = path_join(dest, namelist[0])\n        except Exception as e:\n            remove_file(path)\n            if os.path.exists(prefix):\n                if os.path.isfile(prefix):\n                    os.remove(prefix)\n                elif os.path.isdir(prefix):\n                    shutil.rmtree(prefix)\n            raise e\n    if remove:\n        remove_file(path)\n    return dest\n\n\ndef split_if_compressed(path: str, compressed_ext=('.zip', '.tgz', '.gz', 'bz2', '.xz')) -> Tuple[str, Optional[str]]:\n    tar_gz = '.tar.gz'\n    if path.endswith(tar_gz):\n        root, ext = path[:-len(tar_gz)], tar_gz\n    else:\n        root, ext = os.path.splitext(path)\n    if ext in compressed_ext or ext == tar_gz:\n        return root, ext\n    return path, None\n\n\ndef get_resource(path: str, save_dir=hanlp_home(), extract=True, prefix=HANLP_URL, append_location=True,\n                 verbose=HANLP_VERBOSE):\n    \"\"\"Fetch real (local) path for a resource (model, corpus, whatever) to ``save_dir``.\n\n    Args:\n      path: A local path (which will returned as is) or a remote URL (which will be downloaded, decompressed then\n        returned).\n      save_dir: Where to store the resource (Default value = :meth:`hanlp.utils.io_util.hanlp_home`)\n      extract: Whether to unzip it if it's a zip file (Default value = True)\n      prefix: A prefix when matched with an URL (path), then that URL is considered to be official. For official\n        resources, they will not go to a folder called ``thirdparty`` under :const:`~hanlp_common.constants.HANLP_HOME`.\n      append_location: Whether to put unofficial files in a ``thirdparty`` folder.\n      verbose: Whether to print log messages.\n\n    Returns:\n      The real path to the resource.\n\n    \"\"\"\n    path = hanlp.pretrained.ALL.get(path, path)\n    anchor: str = None\n    compressed = None\n    if os.path.isdir(path):\n        return path\n    elif os.path.isfile(path):\n        pass\n    elif path.startswith('http:') or path.startswith('https:'):\n        url = path\n        if '#' in url:\n            url, anchor = url.split('#', maxsplit=1)\n        realpath = path_from_url(path, save_dir, prefix, append_location)\n        realpath, compressed = split_if_compressed(realpath)\n        # check if resource is there\n        if anchor:\n            if anchor.startswith('/'):\n                # indicates the folder name has to be polished\n                anchor = anchor.lstrip('/')\n                parts = anchor.split('/')\n                renamed_realpath = str(Path(realpath).parent.joinpath(parts[0]))\n                if os.path.isfile(realpath + compressed):\n                    os.rename(realpath + compressed, renamed_realpath + compressed)\n                realpath = renamed_realpath\n                anchor = '/'.join(parts[1:])\n            child = path_join(realpath, anchor)\n            if os.path.exists(child):\n                return child\n        elif os.path.isdir(realpath) or (os.path.isfile(realpath) and (compressed and extract)):\n            return realpath\n        else:\n            if compressed:\n                pattern = realpath + '.*'\n                files = glob.glob(pattern)\n                files = list(filter(lambda x: not x.endswith('.downloading') and not x.endswith(compressed), files))\n                if files:\n                    if len(files) > 1:\n                        logger.debug(f'Found multiple files with {pattern}, will use the first one.')\n                    return files[0]\n        # realpath is where its path after exaction\n        if compressed:\n            realpath += compressed\n        if not os.path.isfile(realpath):\n            path = download(url=path, save_path=realpath, verbose=verbose)\n        else:\n            path = realpath\n    if extract and compressed:\n        path = uncompress(path, verbose=verbose)\n        if anchor:\n            path = path_join(path, anchor)\n\n    return path\n\n\ndef path_from_url(url, save_dir=hanlp_home(), prefix=HANLP_URL, append_location=True):\n    \"\"\"Map a URL to a local path.\n\n    Args:\n        url: Remote URL.\n        save_dir: The root folder to save this file.\n        prefix: The prefix of official website. Any URLs starting with this prefix will be considered official.\n        append_location: Whether to put unofficial files in a ``thirdparty`` folder.\n\n    Returns:\n        The real path that this URL is mapped to.\n    \"\"\"\n    if not save_dir:\n        save_dir = hanlp_home()\n    domain, relative_path = parse_url_path(url)\n    if append_location:\n        if not url.startswith(prefix):\n            save_dir = os.path.join(save_dir, 'thirdparty', domain)\n        else:\n            # remove the relative path in prefix\n            middle = prefix.split(domain)[-1].lstrip('/')\n            if relative_path.startswith(middle):\n                relative_path = relative_path[len(middle):]\n        realpath = os.path.join(save_dir, relative_path)\n    else:\n        realpath = os.path.join(save_dir, os.path.basename(relative_path))\n    return realpath\n\n\ndef human_bytes(file_size: int) -> str:\n    file_size /= 1024  # KB\n    if file_size > 1024:\n        file_size /= 1024  # MB\n        if file_size > 1024:\n            file_size /= 1024  # GB\n            return '%.1f GB' % file_size\n        return '%.1f MB' % file_size\n    return '%d KB' % file_size\n\n\ndef read_cells(filepath: str, delimiter='auto', strip=True, skip_header=False):\n    filepath = get_resource(filepath)\n    if delimiter == 'auto':\n        if filepath.endswith('.tsv'):\n            delimiter = '\\t'\n        elif filepath.endswith('.csv'):\n            delimiter = ','\n        else:\n            delimiter = None\n    with open(filepath, encoding='utf-8') as src:\n        if skip_header:\n            next(src)\n        for line in src:\n            line = line.strip()\n            if not line:\n                continue\n            cells = line.split(delimiter)\n            if strip:\n                cells = [c.strip() for c in cells]\n                yield cells\n\n\ndef replace_ext(filepath, ext) -> str:\n    \"\"\" Replace the extension of filepath to ext.\n\n    Args:\n        filepath: Filepath to be replaced.\n        ext: Extension to replace.\n\n    Returns:\n        A new path.\n    \"\"\"\n    file_prefix, _ = os.path.splitext(filepath)\n    return file_prefix + ext\n\n\ndef read_tsv_as_sents(tsv_file_path, ignore_prefix=None, delimiter=None):\n    sent = []\n    tsv_file_path = get_resource(tsv_file_path)\n    with open(tsv_file_path, encoding='utf-8') as tsv_file:\n        for line in tsv_file:\n            if ignore_prefix and line.startswith(ignore_prefix):\n                continue\n            line = line.strip()\n            cells = line.split(delimiter)\n            if line and cells:\n                sent.append(cells)\n            elif sent:\n                yield sent\n                sent = []\n    if sent:\n        yield sent\n\n\ndef generate_words_tags_from_tsv(tsv_file_path, lower=False, gold=True, max_seq_length=None, sent_delimiter=None,\n                                 char_level=False, hard_constraint=False):\n    for sent in read_tsv_as_sents(tsv_file_path):\n        words = [cells[0] for cells in sent]\n        if max_seq_length:\n            offset = 0\n            # try to split the sequence to make it fit into max_seq_length\n            for shorter_words in split_long_sentence_into(words, max_seq_length, sent_delimiter, char_level,\n                                                          hard_constraint):\n                if gold:\n                    shorter_tags = [cells[1] for cells in sent[offset:offset + len(shorter_words)]]\n                    offset += len(shorter_words)\n                else:\n                    shorter_tags = None\n                if lower:\n                    shorter_words = [word.lower() for word in shorter_words]\n                yield shorter_words, shorter_tags\n        else:\n            if gold:\n                try:\n                    tags = [cells[1] for cells in sent]\n                except:\n                    raise ValueError(f'Failed to load {tsv_file_path}: {sent}')\n            else:\n                tags = None\n            if lower:\n                words = [word.lower() for word in words]\n            yield words, tags\n\n\ndef split_file(filepath, train=0.8, dev=0.1, test=0.1, names=None, shuffle=False):\n    num_samples = 0\n    if filepath.endswith('.tsv'):\n        for sent in read_tsv_as_sents(filepath):\n            num_samples += 1\n    else:\n        with open(filepath, encoding='utf-8') as src:\n            for sample in src:\n                num_samples += 1\n    splits = {'train': train, 'dev': dev, 'test': test}\n    splits = dict((k, v) for k, v in splits.items() if v)\n    splits = dict((k, v / sum(splits.values())) for k, v in splits.items())\n    accumulated = 0\n    r = []\n    for k, v in splits.items():\n        r.append(accumulated)\n        accumulated += v\n        r.append(accumulated)\n        splits[k] = accumulated\n    if names is None:\n        names = {}\n    name, ext = os.path.splitext(filepath)\n    filenames = [names.get(split, name + '.' + split + ext) for split in splits.keys()]\n    outs = [open(f, 'w', encoding='utf-8') for f in filenames]\n    if shuffle:\n        shuffle = list(range(num_samples))\n        random.shuffle(shuffle)\n    if filepath.endswith('.tsv'):\n        src = read_tsv_as_sents(filepath)\n    else:\n        src = open(filepath, encoding='utf-8')\n    for idx, sample in enumerate(src):\n        if shuffle:\n            idx = shuffle[idx]\n        ratio = idx / num_samples\n        for sid, out in enumerate(outs):\n            if r[2 * sid] <= ratio < r[2 * sid + 1]:\n                if isinstance(sample, list):\n                    sample = '\\n'.join('\\t'.join(x) for x in sample) + '\\n\\n'\n                out.write(sample)\n                break\n    if not filepath.endswith('.tsv'):\n        src.close()\n    for out in outs:\n        out.close()\n    return filenames\n\n\ndef fileno(file_or_fd):\n    try:\n        fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()\n    except:\n        return None\n    if not isinstance(fd, int):\n        raise ValueError(\"Expected a file (`.fileno()`) or a file descriptor\")\n    return fd\n\n\n@contextmanager\ndef stdout_redirected(to=os.devnull, stdout=None):\n    \"\"\"Redirect stdout to else where.\n    Copied from https://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262\n\n    Args:\n      to:  Target device.\n      stdout:  Source device.\n\n    \"\"\"\n    if windows():  # This doesn't play well with windows\n        yield None\n        return\n    if stdout is None:\n        stdout = sys.stdout\n    stdout_fd = fileno(stdout)\n    if not stdout_fd:\n        yield None\n        return\n        # copy stdout_fd before it is overwritten\n    # NOTE: `copied` is inheritable on Windows when duplicating a standard stream\n    with os.fdopen(os.dup(stdout_fd), 'wb') as copied:\n        stdout.flush()  # flush library buffers that dup2 knows nothing about\n        try:\n            os.dup2(fileno(to), stdout_fd)  # $ exec >&to\n        except ValueError:  # filename\n            with open(to, 'wb') as to_file:\n                os.dup2(to_file.fileno(), stdout_fd)  # $ exec > to\n        try:\n            yield stdout  # allow code to be run with the redirected stdout\n        finally:\n            # restore stdout to its previous value\n            # NOTE: dup2 makes stdout_fd inheritable unconditionally\n            try:\n                stdout.flush()\n                os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied\n            except:\n                # This is the best we can do\n                pass\n\n\ndef get_exitcode_stdout_stderr(cmd):\n    \"\"\"Execute the external command and get its exitcode, stdout and stderr.\n    See https://stackoverflow.com/a/21000308/3730690\n\n    Args:\n      cmd: Command.\n\n    Returns:\n        Exit code, stdout, stderr.\n    \"\"\"\n    args = shlex.split(cmd)\n    proc = Popen(args, stdout=PIPE, stderr=PIPE)\n    out, err = proc.communicate()\n    exitcode = proc.returncode\n    return exitcode, out.decode('utf-8'), err.decode('utf-8')\n\n\ndef run_cmd(cmd: str) -> str:\n    exitcode, out, err = get_exitcode_stdout_stderr(cmd)\n    if exitcode:\n        raise RuntimeError(err + '\\nThe command is:\\n' + cmd)\n    return out\n\n\n@contextlib.contextmanager\ndef pushd(new_dir):\n    previous_dir = os.getcwd()\n    os.chdir(new_dir)\n    try:\n        yield\n    finally:\n        os.chdir(previous_dir)\n\n\ndef basename_no_ext(path):\n    basename = os.path.basename(path)\n    no_ext, ext = os.path.splitext(basename)\n    return no_ext\n\n\ndef file_cache(path: str, purge=False):\n    cache_name = path + '.cache'\n    cache_time = os.path.getmtime(cache_name) if os.path.isfile(cache_name) and not purge else 0\n    file_time = os.path.getmtime(path)\n    cache_valid = cache_time > file_time\n    return cache_name, cache_valid\n\n\ndef merge_files(files: List[str], dst: str):\n    with open(dst, 'wb') as write:\n        for f in files:\n            with open(f, 'rb') as read:\n                shutil.copyfileobj(read, write)\n\n\nclass TimingFileIterator(CountdownTimer):\n\n    def __init__(self, filepath) -> None:\n        super().__init__(os.path.getsize(filepath))\n        self.filepath = filepath\n\n    def __iter__(self):\n        if not os.path.isfile(self.filepath):\n            raise FileNotFoundError(self.filepath)\n        fp = open(self.filepath, encoding='utf-8', errors='ignore')\n        line = fp.readline()\n        while line:\n            yield line\n            self.current = fp.tell()\n            line = fp.readline()\n        fp.close()\n\n    def log(self, info=None, ratio_percentage=True, ratio=True, step=0, interval=0.5, erase=True,\n            logger: Union[logging.Logger, bool] = None, newline=False, ratio_width=None):\n        assert step == 0\n        super().log(info, ratio_percentage, ratio, step, interval, erase, logger, newline, ratio_width)\n\n    @property\n    def ratio(self) -> str:\n        return f'{human_bytes(self.current)}/{human_bytes(self.total)}'\n\n    @property\n    def ratio_width(self) -> int:\n        return len(f'{human_bytes(self.total)}') * 2 + 1\n\n    def close(self):\n        pass\n\n\ndef check_outdated(package='hanlp', version=__version__, repository_url='https://pypi.python.org/pypi/%s/json'):\n    \"\"\"Given the name of a package on PyPI and a version (both strings), checks\n    if the given version is the latest version of the package available.\n    Returns a 2-tuple (installed_version, latest_version)\n    `repository_url` is a `%` style format string\n    to use a different repository PyPI repository URL,\n    e.g. test.pypi.org or a private repository.\n    The string is formatted with the package name.\n    Adopted from https://github.com/alexmojaki/outdated/blob/master/outdated/__init__.py\n\n    Args:\n        package: Package name.\n        version: Installed version string.\n        repository_url: URL on pypi.\n\n    Returns:\n        Parsed installed version and latest version.\n    \"\"\"\n    installed_version = parse_version(version)\n    latest_version = get_latest_info_from_pypi(package, repository_url)\n    return installed_version, latest_version\n\n\ndef get_latest_info_from_pypi(package='hanlp', repository_url='https://pypi.python.org/pypi/%s/json'):\n    url = repository_url % package\n    response = urllib.request.urlopen(url).read()\n    return parse_version(json.loads(response)['info']['version'])\n\n\ndef check_version_conflicts(extras=None):\n    from pkg_resources import get_distribution, Requirement, WorkingSet, VersionConflict, DistributionNotFound\n    pkg = get_distribution('hanlp')\n    if not extras:\n        extras = pkg.extras\n    if isinstance(extras, list):\n        extras = tuple(extras)\n    requirements: List[Requirement] = pkg.requires(extras=extras)\n    error = None\n    try:\n        WorkingSet().resolve(\n            requirements, extras=extras\n        )\n    except VersionConflict as e:\n        error = e.with_context('hanlp').report()\n    except DistributionNotFound as e:\n        error = str(e)\n    return error, extras\n", "hanlp/utils/time_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-27 00:01\nimport datetime\nimport logging\nimport sys\nimport time\nfrom typing import Union\n\nfrom hanlp.utils.log_util import ErasablePrinter, color_format, color_format_len\n\n\ndef human_time_delta(days, hours, minutes, seconds, delimiter=' ') -> str:\n    units = locals().copy()\n    units.pop('delimiter')\n    non_zero = False\n    result = []\n    for key, val in sorted(units.items()):\n        append = False\n        if non_zero:\n            append = True\n        elif val:\n            non_zero = True\n            append = True\n        if append:\n            result.append('{} {}'.format(val, key[0]))\n    if not non_zero:\n        return '0 s'\n    return delimiter.join(result)\n\n\ndef seconds_to_time_delta(seconds):\n    seconds = round(seconds)\n    days = seconds // 86400\n    hours = seconds // 3600 % 24\n    minutes = seconds // 60 % 60\n    seconds = seconds % 60\n    return days, hours, minutes, seconds\n\n\ndef report_time_delta(seconds, human=True):\n    days, hours, minutes, seconds = seconds_to_time_delta(seconds)\n    if human:\n        return human_time_delta(days, hours, minutes, seconds)\n    return days, hours, minutes, seconds\n\n\nclass HumanTimeDelta(object):\n\n    def __init__(self, delta_seconds) -> None:\n        super().__init__()\n        self.delta_seconds = delta_seconds\n\n    def report(self, human=True):\n        return report_time_delta(self.delta_seconds, human)\n\n    def __str__(self) -> str:\n        return self.report(human=True)\n\n    def __truediv__(self, scalar):\n        return HumanTimeDelta(self.delta_seconds / scalar)\n\n\nclass CountdownTimer(ErasablePrinter):\n\n    def __init__(self, total: int, out=sys.stdout) -> None:\n        super().__init__(out=out)\n        self.total = total\n        self.current = 0\n        self.start = time.time()\n        self.finished_in = None\n        self.last_log_time = 0\n\n    def update(self, n=1):\n        self.current += n\n        self.current = min(self.total, self.current)\n        if self.current == self.total:\n            self.finished_in = time.time() - self.start\n\n    @property\n    def ratio(self) -> str:\n        return f'{self.current}/{self.total}'\n\n    @property\n    def ratio_percentage(self) -> str:\n        return f'{self.current / self.total:.2%}'\n\n    @property\n    def eta(self) -> float:\n        elapsed = self.elapsed\n        if self.finished_in:\n            eta = 0\n        else:\n            eta = elapsed / max(self.current, 0.1) * (self.total - self.current)\n\n        return eta\n\n    @property\n    def elapsed(self) -> float:\n        if self.finished_in:\n            elapsed = self.finished_in\n        else:\n            elapsed = time.time() - self.start\n        return elapsed\n\n    @property\n    def elapsed_human(self) -> str:\n        return human_time_delta(*seconds_to_time_delta(self.elapsed))\n\n    @property\n    def elapsed_average(self) -> float:\n        return self.elapsed / self.current\n\n    @property\n    def elapsed_average_human(self) -> str:\n        return human_time_delta(*seconds_to_time_delta(self.elapsed_average))\n\n    @property\n    def eta_human(self) -> str:\n        return human_time_delta(*seconds_to_time_delta(self.eta))\n\n    @property\n    def total_time(self) -> float:\n        elapsed = self.elapsed\n        if self.finished_in:\n            t = self.finished_in\n        else:\n            t = elapsed / max(self.current, 1) * self.total\n\n        return t\n\n    @property\n    def total_time_human(self) -> str:\n        return human_time_delta(*seconds_to_time_delta(self.total_time))\n\n    def stop(self, total=None):\n        if not self.finished_in or total:\n            self.finished_in = time.time() - self.start\n            if not total:\n                self.total = self.current\n            else:\n                self.current = total\n                self.total = total\n\n    @property\n    def et_eta(self):\n        _ = self.elapsed\n        if self.finished_in:\n            return self.elapsed\n        else:\n            return self.eta\n\n    @property\n    def et_eta_human(self):\n        text = human_time_delta(*seconds_to_time_delta(self.et_eta))\n        if self.finished_in:\n            return f'ET: {text}'\n        else:\n            return f'ETA: {text}'\n\n    @property\n    def finished(self):\n        return self.total == self.current\n\n    def log(self, info=None, ratio_percentage=True, ratio=True, step=1, interval=0.5, erase=True,\n            logger: Union[logging.Logger, bool] = None, newline=False, ratio_width=None):\n        self.update(step)\n        now = time.time()\n        if now - self.last_log_time > interval or self.finished:\n            cells = []\n            if ratio_percentage:\n                cells.append(self.ratio_percentage)\n            if ratio:\n                ratio = self.ratio\n                if not ratio_width:\n                    ratio_width = self.ratio_width\n                ratio = ratio.rjust(ratio_width)\n                cells.append(ratio)\n            cells += [info, self.et_eta_human]\n            cells = [x for x in cells if x]\n            msg = f'{\" \".join(cells)}'\n            self.last_log_time = now\n            self.print(msg, newline, erase, logger)\n\n    @property\n    def ratio_width(self) -> int:\n        return len(f'{self.total}') * 2 + 1\n\n    def print(self, msg, newline=False, erase=True, logger=None):\n        self.erase()\n        msg_len = 0 if newline else len(msg)\n        if self.finished and logger:\n            sys.stdout.flush()\n            if isinstance(logger, logging.Logger):\n                logger.info(msg)\n        else:\n            msg, msg_len = color_format_len(msg)\n            sys.stdout.write(msg)\n            if newline:\n                sys.stdout.write('\\n')\n                msg_len = 0\n        self._last_print_width = msg_len\n        if self.finished and not logger:\n            if erase:\n                self.erase()\n            else:\n                sys.stdout.write(\"\\n\")\n                self._last_print_width = 0\n        sys.stdout.flush()\n\n\nclass Timer(object):\n    def __init__(self) -> None:\n        self.last = time.time()\n\n    def start(self):\n        self.last = time.time()\n\n    def stop(self) -> HumanTimeDelta:\n        now = time.time()\n        seconds = now - self.last\n        self.last = now\n        return HumanTimeDelta(seconds)\n\n\ndef now_human(year='y'):\n    now = datetime.datetime.now()\n    return now.strftime(f\"%{year}-%m-%d %H:%M:%S\")\n\n\ndef now_datetime():\n    return now_human('Y')\n\n\ndef now_filename(fmt=\"%y%m%d_%H%M%S\"):\n    \"\"\"Generate filename using current datetime, in 20180102_030405 format\n\n    Args:\n      fmt:  (Default value = \"%y%m%d_%H%M%S\")\n\n    Returns:\n\n    \n    \"\"\"\n    now = datetime.datetime.now()\n    return now.strftime(fmt)\n", "hanlp/utils/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-24 22:12\nfrom . import rules\n\n\ndef ls_resource_in_module(root) -> dict:\n    res = dict()\n    for k, v in root.__dict__.items():\n        if k.startswith('_') or v == root:\n            continue\n        if isinstance(v, str):\n            if v.startswith('http') and not v.endswith('/') and not v.endswith('#') and not v.startswith('_'):\n                res[k] = v\n        elif type(v).__name__ == 'module':\n            res.update(ls_resource_in_module(v))\n    if 'ALL' in root.__dict__ and isinstance(root.__dict__['ALL'], dict):\n        root.__dict__['ALL'].update(res)\n    return res\n", "hanlp/utils/log_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-24 22:12\nimport datetime\nimport io\nimport logging\nimport os\nimport sys\nfrom logging import LogRecord\n\nimport termcolor\n\nfrom hanlp_common.constant import IPYTHON\n\n\nclass ColoredFormatter(logging.Formatter):\n    def __init__(self, fmt=None, datefmt=None, style='%', enable=True):\n        super().__init__(fmt, datefmt, style)\n        self.enable = enable\n\n    def formatMessage(self, record: LogRecord) -> str:\n        message = super().formatMessage(record)\n        if self.enable:\n            return color_format(message)\n        else:\n            return remove_color_tag(message)\n\n\ndef init_logger(name=None, root_dir=None, level=logging.INFO, mode='w',\n                fmt=\"%(asctime)s %(levelname)s %(message)s\",\n                datefmt='%Y-%m-%d %H:%M:%S') -> logging.Logger:\n    if not name:\n        name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n    rootLogger = logging.getLogger(os.path.join(root_dir, name) if root_dir else name)\n    rootLogger.propagate = False\n\n    consoleHandler = logging.StreamHandler(sys.stdout)  # stderr will be rendered as red which is bad\n    consoleHandler.setFormatter(ColoredFormatter(fmt, datefmt=datefmt))\n    attached_to_std = False\n    for handler in rootLogger.handlers:\n        if isinstance(handler, logging.StreamHandler):\n            if handler.stream == sys.stderr or handler.stream == sys.stdout:\n                attached_to_std = True\n                break\n    if not attached_to_std:\n        rootLogger.addHandler(consoleHandler)\n    rootLogger.setLevel(level)\n    consoleHandler.setLevel(level)\n\n    if root_dir:\n        os.makedirs(root_dir, exist_ok=True)\n        log_path = \"{0}/{1}.log\".format(root_dir, name)\n        fileHandler = logging.FileHandler(log_path, mode=mode)\n        fileHandler.setFormatter(ColoredFormatter(fmt, datefmt=datefmt, enable=False))\n        rootLogger.addHandler(fileHandler)\n        fileHandler.setLevel(level)\n\n    return rootLogger\n\n\nlogger = init_logger(name='hanlp', level=os.environ.get('HANLP_LOG_LEVEL', 'INFO'))\n\n\ndef enable_debug(debug=True):\n    logger.setLevel(logging.DEBUG if debug else logging.ERROR)\n\n\nclass ErasablePrinter(object):\n    def __init__(self, out=sys.stderr):\n        self._last_print_width = 0\n        self.out = out\n\n    def erase(self):\n        if self._last_print_width:\n            if IPYTHON:\n                self.out.write(\"\\r\")\n                self.out.write(\" \" * self._last_print_width)\n            else:\n                self.out.write(\"\\b\" * self._last_print_width)\n                self.out.write(\" \" * self._last_print_width)\n                self.out.write(\"\\b\" * self._last_print_width)\n            self.out.write(\"\\r\")  # \\r is essential when multi-lines were printed\n            self._last_print_width = 0\n\n    def print(self, msg: str, color=True):\n        self.erase()\n        if color:\n            if IPYTHON:\n                msg, _len = color_format_len(msg)\n                _len = len(msg)\n            else:\n                msg, _len = color_format_len(msg)\n            self._last_print_width = _len\n        else:\n            self._last_print_width = len(msg)\n        self.out.write(msg)\n        self.out.flush()\n\n\n_printer = ErasablePrinter()\n\n\ndef flash(line: str, color=True):\n    _printer.print(line, color)\n\n\ndef color_format(msg: str):\n    for tag in termcolor.COLORS, termcolor.HIGHLIGHTS, termcolor.ATTRIBUTES:\n        for c, v in tag.items():\n            start, end = f'[{c}]', f'[/{c}]'\n            msg = msg.replace(start, '\\033[%dm' % v).replace(end, termcolor.RESET)\n    return msg\n\n\ndef remove_color_tag(msg: str):\n    for tag in termcolor.COLORS, termcolor.HIGHLIGHTS, termcolor.ATTRIBUTES:\n        for c, v in tag.items():\n            start, end = f'[{c}]', f'[/{c}]'\n            msg = msg.replace(start, '').replace(end, '')\n    return msg\n\n\ndef color_format_len(msg: str):\n    _len = len(msg)\n    for tag in termcolor.COLORS, termcolor.HIGHLIGHTS, termcolor.ATTRIBUTES:\n        for c, v in tag.items():\n            start, end = f'[{c}]', f'[/{c}]'\n            msg, delta = _replace_color_offset(msg, start, '\\033[%dm' % v)\n            _len -= delta\n            msg, delta = _replace_color_offset(msg, end, termcolor.RESET)\n            _len -= delta\n    return msg, _len\n\n\ndef _replace_color_offset(msg: str, color: str, ctrl: str):\n    chunks = msg.split(color)\n    delta = (len(chunks) - 1) * len(color)\n    return ctrl.join(chunks), delta\n\n\ndef cprint(*args, file=None, **kwargs):\n    out = io.StringIO()\n    print(*args, file=out, **kwargs)\n    text = out.getvalue()\n    out.close()\n    c_text = color_format(text)\n    print(c_text, end='', file=file)\n\n\ndef main():\n    # cprint('[blink][yellow]...[/yellow][/blink]')\n    # show_colors_and_formats()\n    show_colors()\n    # print('previous', end='')\n    # for i in range(10):\n    #     flash(f'[red]{i}[/red]')\n\n\ndef show_colors_and_formats():\n    msg = ''\n    for c in termcolor.COLORS.keys():\n        for h in termcolor.HIGHLIGHTS.keys():\n            for a in termcolor.ATTRIBUTES.keys():\n                msg += f'[{c}][{h}][{a}] {c}+{h}+{a} [/{a}][/{h}][/{c}]'\n    logger.info(msg)\n\n\ndef show_colors():\n    msg = ''\n    for c in termcolor.COLORS.keys():\n        cprint(f'[{c}]\"{c}\",[/{c}]')\n\n\n# Generates tables for Doxygen flavored Markdown.  See the Doxygen\n# documentation for details:\n#   http://www.doxygen.nl/manual/markdown.html#md_tables\n\n# Translation dictionaries for table alignment\n\n\nif __name__ == '__main__':\n    main()\n", "hanlp/utils/init_util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-27 13:25\nimport math\n\nimport torch\nfrom torch import nn\nimport functools\n\n\ndef embedding_uniform(tensor:torch.Tensor, seed=233):\n    gen = torch.Generator().manual_seed(seed)\n    with torch.no_grad():\n        fan_out = tensor.size(-1)\n        bound = math.sqrt(3.0 / fan_out)\n        return tensor.uniform_(-bound, bound, generator=gen)\n", "hanlp/utils/lang/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-09 18:46\n\n__doc__ = '''\nThis package holds misc utils for specific languages.\n'''\n", "hanlp/utils/lang/ja/bert_tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-13 13:24\nfrom typing import Union, Optional\n\nfrom transformers import BertTokenizerFast, TensorType, BatchEncoding, BertJapaneseTokenizer as _BertJapaneseTokenizer\nfrom transformers.file_utils import PaddingStrategy\nfrom transformers.tokenization_utils_base import TextInput, PreTokenizedInput, EncodedInput, TruncationStrategy\n\n\nclass BertJapaneseTokenizer(_BertJapaneseTokenizer):\n    # We may need to customize character level tokenization to handle English words and URLs\n    pass\n\n\nclass BertJapaneseTokenizerFast(BertTokenizerFast):\n    def encode_plus(\n            self,\n            text: Union[TextInput, PreTokenizedInput, EncodedInput],\n            text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n            add_special_tokens: bool = True,\n            padding: Union[bool, str, PaddingStrategy] = False,\n            truncation: Union[bool, str, TruncationStrategy] = False,\n            max_length: Optional[int] = None,\n            stride: int = 0,\n            is_split_into_words: bool = False,\n            pad_to_multiple_of: Optional[int] = None,\n            return_tensors: Optional[Union[str, TensorType]] = None,\n            return_token_type_ids: Optional[bool] = None,\n            return_attention_mask: Optional[bool] = None,\n            return_overflowing_tokens: bool = False,\n            return_special_tokens_mask: bool = False,\n            return_offsets_mapping: bool = False,\n            return_length: bool = False,\n            verbose: bool = True,\n            **kwargs\n    ) -> BatchEncoding:\n        \"\"\"\n        Tokenize and prepare for the model a sequence or a pair of sequences.\n\n        .. warning::\n            This method is deprecated, ``__call__`` should be used instead.\n\n        Args:\n            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n                method).\n            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n                ``convert_tokens_to_ids`` method).\n        \"\"\"\n        text = list(text)\n        is_split_into_words = True\n        encoding = BertJapaneseTokenizer.encode_plus(self,\n                                                     text,\n                                                     text_pair,\n                                                     add_special_tokens,\n                                                     padding,\n                                                     truncation,\n                                                     max_length,\n                                                     stride,\n                                                     is_split_into_words,\n                                                     pad_to_multiple_of,\n                                                     return_tensors,\n                                                     return_token_type_ids,\n                                                     return_attention_mask,\n                                                     return_overflowing_tokens,\n                                                     return_special_tokens_mask,\n                                                     return_offsets_mapping,\n                                                     return_length,\n                                                     verbose,\n                                                     **kwargs\n                                                     )\n        offsets = encoding.encodings[0].offsets\n        fixed_offsets = [(b + i, e + i) for i, (b, e) in enumerate(offsets)]\n        # TODO: This doesn't work with rust tokenizers\n        encoding.encodings[0].offsets.clear()\n        encoding.encodings[0].offsets.extend(fixed_offsets)\n        return encoding\n", "hanlp/utils/lang/ja/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-13 13:24\n", "hanlp/utils/lang/zh/localization.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-05 02:09\n\ntask = {\n    'dep': '\u4f9d\u5b58\u53e5\u6cd5\u6811',\n    'token': '\u5355\u8bcd',\n    'pos': '\u8bcd\u6027',\n    'ner': '\u547d\u540d\u5b9e\u4f53',\n    'srl': '\u8bed\u4e49\u89d2\u8272'\n}\n\npos = {\n    'VA': '\u8868\u8bed\u5f62\u5bb9\u8bcd', 'VC': '\u7cfb\u52a8\u8bcd', 'VE': '\u52a8\u8bcd\u6709\u65e0', 'VV': '\u5176\u4ed6\u52a8\u8bcd', 'NR': '\u4e13\u6709\u540d\u8bcd', 'NT': '\u65f6\u95f4\u540d\u8bcd', 'NN': '\u5176\u4ed6\u540d\u8bcd',\n    'LC': '\u65b9\u4f4d\u8bcd', 'PN': '\u4ee3\u8bcd', 'DT': '\u9650\u5b9a\u8bcd', 'CD': '\u6982\u6570\u8bcd', 'OD': '\u5e8f\u6570\u8bcd', 'M': '\u91cf\u8bcd', 'AD': '\u526f\u8bcd', 'P': '\u4ecb\u8bcd',\n    'CC': '\u5e76\u5217\u8fde\u63a5\u8bcd', 'CS': '\u4ece\u5c5e\u8fde\u8bcd', 'DEC': '\u8865\u8bed\u6210\u5206\u201c\u7684\u201d', 'DEG': '\u5c5e\u683c\u201c\u7684\u201d', 'DER': '\u8868\u7ed3\u679c\u7684\u201c\u5f97\u201d', 'DEV': '\u8868\u65b9\u5f0f\u7684\u201c\u5730\u201d',\n    'AS': '\u52a8\u6001\u52a9\u8bcd', 'SP': '\u53e5\u672b\u52a9\u8bcd', 'ETC': '\u8868\u793a\u7701\u7565', 'MSP': '\u5176\u4ed6\u5c0f\u54c1\u8bcd', 'IJ': '\u53e5\u9996\u611f\u53f9\u8bcd', 'ON': '\u8c61\u58f0\u8bcd',\n    'LB': '\u957f\u53e5\u5f0f\u8868\u88ab\u52a8', 'SB': '\u77ed\u53e5\u5f0f\u8868\u88ab\u52a8', 'BA': '\u628a\u5b57\u53e5', 'JJ': '\u5176\u4ed6\u540d\u8bcd\u4fee\u9970\u8bed', 'FW': '\u5916\u6765\u8bed', 'PU': '\u6807\u70b9\u7b26\u53f7',\n    'NOI': '\u566a\u58f0', 'URL': '\u7f51\u5740'\n}\n\nner = {\n    'NT': '\u673a\u6784\u56e2\u4f53', 'NS': '\u5730\u540d', 'NR': '\u4eba\u540d'\n}\n\ndep = {\n    'nn': '\u590d\u5408\u540d\u8bcd\u4fee\u9970', 'punct': '\u6807\u70b9\u7b26\u53f7', 'nsubj': '\u540d\u8bcd\u6027\u4e3b\u8bed', 'conj': '\u8fde\u63a5\u6027\u72b6\u8bed', 'dobj': '\u76f4\u63a5\u5bbe\u8bed', 'advmod': '\u540d\u8bcd\u6027\u72b6\u8bed',\n    'prep': '\u4ecb\u8bcd\u6027\u4fee\u9970\u8bed', 'nummod': '\u6570\u8bcd\u4fee\u9970\u8bed', 'amod': '\u5f62\u5bb9\u8bcd\u4fee\u9970\u8bed', 'pobj': '\u4ecb\u8bcd\u6027\u5bbe\u8bed', 'rcmod': '\u76f8\u5173\u5173\u7cfb', 'cpm': '\u8865\u8bed',\n    'assm': '\u5173\u8054\u6807\u8bb0', 'assmod': '\u5173\u8054\u4fee\u9970', 'cc': '\u5e76\u5217\u5173\u7cfb', 'elf': '\u7c7b\u522b\u4fee\u9970', 'ccomp': '\u4ece\u53e5\u8865\u5145', 'det': '\u9650\u5b9a\u8bed', 'lobj': '\u65f6\u95f4\u4ecb\u8bcd',\n    'range': '\u6570\u91cf\u8bcd\u95f4\u63a5\u5bbe\u8bed', 'asp': '\u65f6\u6001\u6807\u8bb0', 'tmod': '\u65f6\u95f4\u4fee\u9970\u8bed', 'plmod': '\u4ecb\u8bcd\u6027\u5730\u70b9\u4fee\u9970', 'attr': '\u5c5e\u6027', 'mmod': '\u60c5\u6001\u52a8\u8bcd',\n    'loc': '\u4f4d\u7f6e\u8865\u8bed', 'top': '\u4e3b\u9898', 'pccomp': '\u4ecb\u8bcd\u8865\u8bed', 'etc': '\u7701\u7565\u5173\u7cfb', 'lccomp': '\u4f4d\u7f6e\u8865\u8bed', 'ordmod': '\u91cf\u8bcd\u4fee\u9970',\n    'xsubj': '\u63a7\u5236\u4e3b\u8bed', 'neg': '\u5426\u5b9a\u4fee\u9970', 'rcomp': '\u7ed3\u679c\u8865\u8bed', 'comod': '\u5e76\u5217\u8054\u5408\u52a8\u8bcd', 'vmod': '\u52a8\u8bcd\u4fee\u9970', 'prtmod': '\u5c0f\u54c1\u8bcd',\n    'ba': '\u628a\u5b57\u5173\u7cfb', 'dvpm': '\u5730\u5b57\u4fee\u9970', 'dvpmod': '\u5730\u5b57\u52a8\u8bcd\u77ed\u8bed', 'prnmod': '\u63d2\u5165\u8bcd\u4fee\u9970', 'cop': '\u7cfb\u52a8\u8bcd', 'pass': '\u88ab\u52a8\u6807\u8bb0',\n    'nsubjpass': '\u88ab\u52a8\u540d\u8bcd\u4e3b\u8bed', 'clf': '\u7c7b\u522b\u4fee\u9970', 'dep': '\u4f9d\u8d56\u5173\u7cfb', 'root': '\u6838\u5fc3\u5173\u7cfb'\n}\n", "hanlp/utils/lang/zh/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-09 18:47", "hanlp/utils/lang/zh/char_table.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-09 19:07\nfrom typing import List\n\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp_common.io import load_json\n\nHANLP_CHAR_TABLE_TXT = 'https://file.hankcs.com/corpus/char_table.zip#CharTable.txt'\nHANLP_CHAR_TABLE_JSON = 'https://file.hankcs.com/corpus/char_table.json.zip'\n\n\nclass CharTable:\n    convert = {}\n\n    @staticmethod\n    def convert_char(c):\n        if not CharTable.convert:\n            CharTable._init()\n        return CharTable.convert.get(c, c)\n\n    @staticmethod\n    def normalize_text(text: str) -> str:\n        return ''.join(CharTable.convert_char(c) for c in text)\n\n    @staticmethod\n    def normalize_chars(chars: List[str]) -> List[str]:\n        return [CharTable.convert_char(c) for c in chars]\n\n    @staticmethod\n    def _init():\n        CharTable.convert = CharTable.load()\n\n    @staticmethod\n    def load():\n        mapper = {}\n        with open(get_resource(HANLP_CHAR_TABLE_TXT), encoding='utf-8') as src:\n            for line in src:\n                cells = line.rstrip('\\n')\n                if len(cells) != 3:\n                    continue\n                a, _, b = cells\n                mapper[a] = b\n        return mapper\n\n\nclass JsonCharTable(CharTable):\n\n    @staticmethod\n    def load():\n        return load_json(get_resource(HANLP_CHAR_TABLE_JSON))\n\n\n", "hanlp/utils/lang/en/english_tokenizer.py": "#!/usr/bin/env python\n\"\"\"\nRegex-based word tokenizers.\n\nNote that small/full/half-width character variants are *not* covered.\nIf a text were to contains such characters, normalize it first.\nA modified version of https://github.com/fnl/segtok\n\n- dropped dependency on regex\n- dropped web_tokenize\n- supported concat word\n\n\"\"\"\n\n__author__ = 'Florian Leitner <florian.leitner@gmail.com>'\nfrom re import compile, UNICODE, VERBOSE\n\nSENTENCE_TERMINALS = '.!?\\u203C\\u203D\\u2047\\u2048\\u2049\\u3002' \\\n                     '\\uFE52\\uFE57\\uFF01\\uFF0E\\uFF1F\\uFF61'\n\"The list of valid Unicode sentence terminal characters.\"\n\n# Note that Unicode the category Pd is NOT a good set for valid word-breaking hyphens,\n# because it contains many dashes that should not be considered part of a word.\nHYPHENS = '\\u00AD\\u058A\\u05BE\\u0F0C\\u1400\\u1806\\u2010-\\u2012\\u2e17\\u30A0-'\n\"Any valid word-breaking hyphen, including ASCII hyphen minus.\"\n\nAPOSTROPHES = '\\'\\u00B4\\u02B9\\u02BC\\u2019\\u2032'\n\"\"\"All apostrophe-like marks, including the ASCII \"single quote\".\"\"\"\n\nAPOSTROPHE = r\"[\\u00B4\\u02B9\\u02BC\\u2019\\u2032]\"\n\"\"\"Any apostrophe-like marks, including \"prime\" but not the ASCII \"single quote\".\"\"\"\n\nLINEBREAK = r'(?:\\r\\n|\\n|\\r|\\u2028)'\n\"\"\"Any valid linebreak sequence (Windows, Unix, Mac, or U+2028).\"\"\"\n\nLETTER = r'[^\\W\\d_]'\n\"\"\"Any Unicode letter character that can form part of a word: Ll, Lm, Lt, Lu.\"\"\"\n\nNUMBER = r'\\d'\n\"\"\"Any Unicode number character: Nd or Nl.\"\"\"\n\nPOWER = r'\\u207B?[\\u00B9\\u00B2\\u00B3]'\n\"\"\"Superscript 1, 2, and 3, optionally prefixed with a minus sign.\"\"\"\n\nSUBDIGIT = r'[\\u2080-\\u2089]'\n\"\"\"Subscript digits.\"\"\"\n\nALNUM = LETTER[:-1] + NUMBER + ']'\n\"\"\"Any alphanumeric Unicode character: letter or number.\"\"\"\n\nHYPHEN = r'[%s]' % HYPHENS\n\nSPACE = r'\\s'\n\"\"\"Any unicode space character plus the (horizontal) tab.\"\"\"\n\nAPO_MATCHER = compile(APOSTROPHE, UNICODE)\n\"\"\"Matcher for any apostrophe.\"\"\"\n\nHYPHENATED_LINEBREAK = compile(\n    r'({alnum}{hyphen}){space}*?{linebreak}{space}*?({alnum})'.format(\n        alnum=ALNUM, hyphen=HYPHEN, linebreak=LINEBREAK, space=SPACE\n    ), UNICODE\n)\n\"\"\"\nThe pattern matches any alphanumeric Unicode character, followed by a hyphen,\na single line-break surrounded by optional (non-breaking) spaces,\nand terminates with a alphanumeric character on this next line.\nThe opening char and hyphen as well as the terminating char are captured in two groups.\n\"\"\"\n\nIS_POSSESSIVE = compile(r\"{alnum}+(?:{hyphen}{alnum}+)*(?:{apo}[sS]|[sS]{apo})$\".format(\n    alnum=ALNUM, hyphen=HYPHEN, apo=\"['\" + APOSTROPHE[1:]\n), UNICODE\n)\n\"\"\"A pattern that matches English words with a possessive s terminal form.\"\"\"\n\nIS_CONTRACTION = compile(r\"{alnum}+(?:{hyphen}{alnum}+)*{apo}(?:d|ll|m|re|s|t|ve)$\".format(\n    alnum=ALNUM, hyphen=HYPHEN, apo=\"['\" + APOSTROPHE[1:]\n), UNICODE\n)\n\"\"\"A pattern that matches tokens with valid English contractions ``'(d|ll|m|re|s|t|ve)``.\"\"\"\n\nMAP_CONCAT_WORD = {'aint': [2, 4], 'arent': [3, 5], 'cant': [2, 4], 'cannot': [3, 6], 'coulda': [5, 6],\n                   'couldnt': [5, 7], 'didnt': [3, 5], 'doncha': [2, 3, 6], 'dont': [2, 4],\n                   'doesnt': [4, 6], 'dunno': [2, 3, 5], 'finna': [3, 5], 'gimme': [3, 5], 'gonna': [3, 5],\n                   'gotta': [3, 5], 'hadnt': [3, 5], 'hasnt': [3, 5], 'havent': [4, 6], 'isnt': [2, 4],\n                   'itd': [2, 3], 'itll': [2, 4], 'lemme': [3, 5], 'lets': [3, 4], 'mightnt': [5, 7],\n                   'mustnt': [4, 6], 'shant': [3, 5], 'shoulda': [6, 7], 'shouldnt': [6, 8],\n                   'thatd': [4, 5], 'thatll': [4, 6], 'thats': [4, 5], 'theyd': [4, 5], 'theyre': [4, 6],\n                   'theyve': [4, 6], 'wanna': [3, 5], 'wasnt': [3, 5], 'weve': [2, 4], 'werent': [4, 6],\n                   'whadya': [3, 4, 6], 'whatcha': [4, 7], 'whatre': [4, 6], 'whats': [4, 5],\n                   'whatve': [4, 6], 'whatz': [4, 5], 'whod': [3, 4], 'wholl': [3, 5], 'woncha': [2, 3, 6],\n                   'wont': [2, 4], 'woulda': [5, 6], 'wouldnt': [5, 7], 'youd': [3, 4], 'youll': [3, 5],\n                   'youve': [3, 5], \"'tis\": [2, 4], \"'twas\": [2, 5], \"d'ye\": [2, 4], \"don'cha\": [2, 4, 7],\n                   \"i'mma\": [1, 3, 5], \"i'mmm\": [1, 5], \"more'n\": [4, 6], '\u2019tis': [2, 4], '\u2019twas': [2, 5],\n                   'd\u2019ye': [2, 4], 'don\u2019cha': [2, 4, 7], 'i\u2019mma': [1, 3, 5], 'i\u2019mmm': [1, 5],\n                   'more\u2019n': [4, 6]}\n\nRE_APOSTROPHE = compile(r'(?i)[a-z](n[\\'\\u2019]t|[\\'\\u2019](ll|nt|re|ve|[dmstz]))(\\W|$)')\n\n\ndef split_possessive_markers(tokens):\n    \"\"\"\n    A function to split possessive markers at the end of alphanumeric (and hyphenated) tokens.\n\n    Takes the output of any of the tagger functions and produces and updated list.\n    To use it, simply wrap the tagger function, for example::\n\n    >>> my_sentence = \"This is Fred's latest book.\"\n    >>> split_possessive_markers(tokenize_english(my_sentence))\n    ['This', 'is', 'Fred', \"'s\", 'latest', 'book', '.']\n\n    :param tokens: a list of tokens\n    :returns: an updated list if a split was made or the original list otherwise\n    \"\"\"\n    idx = -1\n\n    for token in list(tokens):\n        idx += 1\n\n        if IS_POSSESSIVE.match(token) is not None:\n            if token[-1].lower() == 's' and token[-2] in APOSTROPHES:\n                tokens.insert(idx, token[:-2])\n                idx += 1\n                tokens[idx] = token[-2:]\n            elif token[-2].lower() == 's' and token[-1] in APOSTROPHES:\n                tokens.insert(idx, token[:-1])\n                idx += 1\n                tokens[idx] = token[-1:]\n\n    return tokens\n\n\ndef split_contractions(tokens):\n    \"\"\"\n    A function to split apostrophe contractions at the end of alphanumeric (and hyphenated) tokens.\n\n    Takes the output of any of the tagger functions and produces and updated list.\n\n    :param tokens: a list of tokens\n    :returns: an updated list if a split was made or the original list otherwise\n    \"\"\"\n    idx = -1\n\n    for token in list(tokens):\n        idx += 1\n\n        if IS_CONTRACTION.match(token) is not None:\n            length = len(token)\n\n            if length > 1:\n                for pos in range(length - 1, -1, -1):\n                    if token[pos] in APOSTROPHES:\n                        if 2 < length and pos + 2 == length and token[-1] == 't' and token[pos - 1] == 'n':\n                            pos -= 1\n\n                        tokens.insert(idx, token[:pos])\n                        idx += 1\n                        tokens[idx] = token[pos:]\n\n    return tokens\n\n\ndef _matches(regex):\n    \"\"\"Regular expression compiling function decorator.\"\"\"\n\n    def match_decorator(fn):\n        automaton = compile(regex, UNICODE | VERBOSE)\n        fn.split = automaton.split\n        fn.match = automaton.match\n        return fn\n\n    return match_decorator\n\n\n@_matches(r'\\s+')\ndef space_tokenizer(sentence):\n    \"\"\"\n    For a given input `sentence`, return a list of its tokens.\n\n    Split on Unicode spaces ``\\\\s+`` (i.e., any kind of **Unicode** space character).\n    The separating space characters are not included in the resulting token list.\n    \"\"\"\n    return [token for token in space_tokenizer.split(sentence) if token]\n\n\n@_matches(r'(%s+)' % ALNUM)\ndef symbol_tokenizer(sentence):\n    \"\"\"\n    The symbol tagger extends the :func:`space_tokenizer` by separating alphanumerics.\n\n    Separates alphanumeric Unicode character sequences in already space-split tokens.\n    \"\"\"\n    return [token for span in space_tokenizer(sentence) for\n            token in symbol_tokenizer.split(span) if token]\n\n\n@_matches(r\"\"\"((?:\n    # Dots, except ellipsis\n    {alnum} \\. (?!\\.\\.)\n    | # Comma, surrounded by digits (e.g., chemicals) or letters\n    {alnum} , (?={alnum})\n    | # Colon, surrounded by digits (e.g., time, references)\n    {number} : (?={number})\n    | # Hyphen, surrounded by digits (e.g., DNA endings: \"5'-ACGT-3'\") or letters\n    {alnum} {apo}? {hyphen} (?={alnum})  # incl. optional apostrophe for DNA segments\n    | # Apostophes, non-consecutive\n    {apo} (?!{apo})\n    | # ASCII single quote, surrounded by digits or letters (no dangling allowed)\n    {alnum} ' (?={alnum})\n    | # ASCII single quote after an s and at the token's end\n    s ' $\n    | # Terminal dimensions (superscript minus, 1, 2, and 3) attached to physical units\n    #  size-prefix                 unit-acronym    dimension\n    \\b [yzafpn\\u00B5mcdhkMGTPEZY]? {letter}{{1,3}} {power} $\n    | # Atom counts (subscript numbers) and ionization states (optional superscript\n    #   2 or 3 followed by a + or -) are attached to valid fragments of a chemical formula\n    \\b (?:[A-Z][a-z]?|[\\)\\]])+ {subdigit}+ (?:[\\u00B2\\u00B3]?[\\u207A\\u207B])?\n    | # Any (Unicode) letter, digit, or the underscore\n    {alnum}\n    )+)\"\"\".format(alnum=ALNUM, apo=APOSTROPHE, power=POWER, subdigit=SUBDIGIT,\n                  hyphen=HYPHEN, letter=LETTER, number=NUMBER))\ndef tokenize_english(sentence):\n    \"\"\"\n    A modified version of the segtok tagger: https://github.com/fnl/segtok\n    This tagger extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\n\n    1. Dots appearing after a letter are maintained as part of the word, except for the last word\n       in a sentence if that dot is the sentence terminal. Therefore, abbreviation marks (words\n       containing or ending in a ``.``, like \"i.e.\") remain intact and URL or ID segments remain\n       complete (\"www.ex-ample.com\", \"EC1.2.3.4.5\", etc.). The only dots that never are attached\n       are triple dots (``...``; ellipsis).\n    2. Commas surrounded by alphanumeric characters are maintained in the word, too, e.g. ``a,b``.\n       Colons surrounded by digits are maintained, e.g., 'at 12:30pm' or 'Isaiah 12:3'.\n       Commas, semi-colons, and colons dangling at the end of a token are always spliced off.\n    3. Any two alphanumeric letters that are separated by a single hyphen are joined together;\n       Those \"inner\" hyphens may optionally be followed by a linebreak surrounded by spaces;\n       The spaces will be removed, however. For example, ``Hel- \\\\r\\\\n \\t lo`` contains a (Windows)\n       linebreak and will be returned as ``Hel-lo``.\n    4. Apostrophes are always allowed in words as long as they are not repeated; The single quote\n       ASCII letter ``'`` is only allowed as a terminal apostrophe after the letter ``s``,\n       otherwise it must be surrounded by letters. To support DNA and chemicals, a apostrophe\n       (prime) may be located before the hyphen, as in the single token \"5'-ACGT-3'\" (if any\n       non-ASCII hyphens are used instead of the shown single quote).\n    5. Superscript 1, 2, and 3, optionally prefixed with a superscript minus, are attached to a\n       word if it is no longer than 3 letters (optionally 4 if the first letter is a power prefix\n       in the range from yocto, y (10^-24) to yotta, Y (10^+24)).\n    6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\n    \"\"\"\n    if not sentence:\n        return []\n    flat = not isinstance(sentence, list)\n    if flat:\n        sents = [sentence]\n    else:\n        sents = sentence\n    results = []\n    for sentence in sents:\n        pruned = HYPHENATED_LINEBREAK.sub(r'\\1\\2', sentence)\n        tokens = [token for span in space_tokenizer(pruned) for\n                  token in tokenize_english.split(span) if token]\n\n        # splice the sentence terminal off the last word/token if it has any at its borders\n        # only look for the sentence terminal in the last three tokens\n        for idx, word in enumerate(reversed(tokens[-3:]), 1):\n            if (tokenize_english.match(word) and not APO_MATCHER.match(word)) or \\\n                    any(t in word for t in SENTENCE_TERMINALS):\n                last = len(word) - 1\n\n                if 0 == last or u'...' == word:\n                    # any case of \"...\" or any single char (last == 0)\n                    pass  # leave the token as it is\n                elif any(word.rfind(t) == last for t in SENTENCE_TERMINALS):\n                    # \"stuff.\"\n                    tokens[-idx] = word[:-1]\n                    tokens.insert(len(tokens) - idx + 1, word[-1])\n                elif any(word.find(t) == 0 for t in SENTENCE_TERMINALS):\n                    # \".stuff\"\n                    tokens[-idx] = word[0]\n                    tokens.insert(len(tokens) - idx + 1, word[1:])\n\n                break\n\n        # keep splicing off any dangling commas and (semi-) colons\n        dirty = True\n        while dirty:\n            dirty = False\n\n            for idx, word in enumerate(reversed(tokens), 1):\n                while len(word) > 1 and word[-1] in u',;:':\n                    char = word[-1]  # the dangling comma/colon\n                    word = word[:-1]\n                    tokens[-idx] = word\n                    tokens.insert(len(tokens) - idx + 1, char)\n                    idx += 1\n                    dirty = True\n                if dirty:\n                    break  # restart check to avoid index errors\n\n        # split concat words\n        chunks = []\n        for token in tokens:\n            t = MAP_CONCAT_WORD.get(token.lower(), None)\n            if t:\n                i = 0\n                for j in t:\n                    chunks.append(token[i:j])\n                    i = j\n            else:\n                chunks.append(token)\n        tokens = chunks\n        # split APOSTROPHE\n        chunks = []\n        for token in tokens:\n            m = RE_APOSTROPHE.search(token)\n            if m:\n                chunks.append(token[:m.start(1)])\n                chunks.append(token[m.start(1):m.end(1)])\n                if m.end(1) < len(token):\n                    chunks.append(token[m.end(1):])\n            else:\n                chunks.append(token)\n        tokens = chunks\n        results.append(tokens)\n    return results[0] if flat else results", "hanlp/utils/lang/en/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-12-28 19:28\n", "hanlp/utils/file_read_backwards/buffer_work_space.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"BufferWorkSpace module.\"\"\"\n\nimport os\n\nnew_lines = [\"\\r\\n\", \"\\n\", \"\\r\"]\nnew_lines_bytes = [n.encode(\"ascii\") for n in new_lines]  # we only support encodings that's backward compat with ascii\n\n\nclass BufferWorkSpace:\n\n    \"\"\"It is a helper module for FileReadBackwards.\"\"\"\n\n    def __init__(self, fp, chunk_size):\n        \"\"\"Convention for the data.\n\n        When read_buffer is not None, it represents contents of the file from `read_position` onwards\n            that has not been processed/returned.\n        read_position represents the file pointer position that has been read into read_buffer\n            initialized to be just past the end of file.\n        \"\"\"\n        self.fp = fp\n        self.read_position = _get_file_size(self.fp)  # set the previously read position to the\n        self.read_buffer = None\n        self.chunk_size = chunk_size\n\n    def add_to_buffer(self, content, read_position):\n        \"\"\"Add additional bytes content as read from the read_position.\n\n        Args:\n          content(bytes): data to be added to buffer working BufferWorkSpac.\n          read_position(int): where in the file pointer the data was read from.\n\n        Returns:\n\n        \"\"\"\n        self.read_position = read_position\n        if self.read_buffer is None:\n            self.read_buffer = content\n        else:\n            self.read_buffer = content + self.read_buffer\n\n    def yieldable(self):\n        \"\"\" \"\"\"\n        if self.read_buffer is None:\n            return False\n\n        t = _remove_trailing_new_line(self.read_buffer)\n        n = _find_furthest_new_line(t)\n        if n >= 0:\n            return True\n\n        # we have read in entire file and have some unprocessed lines\n        if self.read_position == 0 and self.read_buffer is not None:\n            return True\n        return False\n\n    def return_line(self):\n        \"\"\"\n\n        Args:\n\n        Returns:\n          Precondition: self.yieldable() must be True\n\n        \"\"\"\n        assert(self.yieldable())\n\n        t = _remove_trailing_new_line(self.read_buffer)\n        i = _find_furthest_new_line(t)\n\n        if i >= 0:\n            l = i + 1\n            after_new_line = slice(l, None)\n            up_to_include_new_line = slice(0, l)\n            r = t[after_new_line]\n            self.read_buffer = t[up_to_include_new_line]\n        else:  # the case where we have read in entire file and at the \"last\" line\n            r = t\n            self.read_buffer = None\n        return r\n\n    def read_until_yieldable(self):\n        \"\"\"Read in additional chunks until it is yieldable.\"\"\"\n        while not self.yieldable():\n            read_content, read_position = _get_next_chunk(self.fp, self.read_position, self.chunk_size)\n            self.add_to_buffer(read_content, read_position)\n\n    def has_returned_every_line(self):\n        \"\"\" \"\"\"\n        if self.read_position == 0 and self.read_buffer is None:\n            return True\n        return False\n\n\ndef _get_file_size(fp):\n    return os.fstat(fp.fileno()).st_size\n\n\ndef _get_next_chunk(fp, previously_read_position, chunk_size):\n    \"\"\"Return next chunk of data that we would from the file pointer.\n\n    Args:\n      fp: file\n      previously_read_position: file pointer position that we have read from\n      chunk_size: desired read chunk_size\n\n    Returns:\n      (bytestring, int): data that has been read in, the file pointer position where the data has been read from\n\n    \"\"\"\n    seek_position, read_size = _get_what_to_read_next(fp, previously_read_position, chunk_size)\n    fp.seek(seek_position)\n    read_content = fp.read(read_size)\n    read_position = seek_position\n    return read_content, read_position\n\n\ndef _get_what_to_read_next(fp, previously_read_position, chunk_size):\n    \"\"\"Return information on which file pointer position to read from and how many bytes.\n\n    Args:\n      fp: \n      past_read_positon: int\n      chunk_size: int\n      previously_read_position: \n\n    Returns:\n      (int, int): The next seek position, how many bytes to read next\n\n    \"\"\"\n    seek_position = max(previously_read_position - chunk_size, 0)\n    read_size = chunk_size\n\n    # examples: say, our new_lines are potentially \"\\r\\n\", \"\\n\", \"\\r\"\n    # find a reading point where it is not \"\\n\", rewind further if necessary\n    # if we have \"\\r\\n\" and we read in \"\\n\",\n    # the next iteration would treat \"\\r\" as a different new line.\n    # Q: why don't I just check if it is b\"\\n\", but use a function ?\n    # A: so that we can potentially expand this into generic sets of separators, later on.\n    while seek_position > 0:\n        fp.seek(seek_position)\n        if _is_partially_read_new_line(fp.read(1)):\n            seek_position -= 1\n            read_size += 1  # as we rewind further, let's make sure we read more to compensate\n        else:\n            break\n\n    # take care of special case when we are back to the beginnin of the file\n    read_size = min(previously_read_position - seek_position, read_size)\n    return seek_position, read_size\n\n\ndef _remove_trailing_new_line(l):\n    \"\"\"Remove a single instance of new line at the end of l if it exists.\n\n    Args:\n      l: \n\n    Returns:\n      : bytestring\n\n    \"\"\"\n    # replace only 1 instance of newline\n    # match longest line first (hence the reverse=True), we want to match \"\\r\\n\" rather than \"\\n\" if we can\n    for n in sorted(new_lines_bytes, key=lambda x: len(x), reverse=True):\n        if l.endswith(n):\n            remove_new_line = slice(None, -len(n))\n            return l[remove_new_line]\n    return l\n\n\ndef _find_furthest_new_line(read_buffer):\n    \"\"\"Return -1 if read_buffer does not contain new line otherwise the position of the rightmost newline.\n\n    Args:\n      read_buffer: bytestring\n\n    Returns:\n      int: The right most position of new line character in read_buffer if found, else -1\n\n    \"\"\"\n    new_line_positions = [read_buffer.rfind(n) for n in new_lines_bytes]\n    return max(new_line_positions)\n\n\ndef _is_partially_read_new_line(b):\n    \"\"\"Return True when b is part of a new line separator found at index >= 1, False otherwise.\n\n    Args:\n      b: bytestring\n\n    Returns:\n      bool\n\n    \"\"\"\n    for n in new_lines_bytes:\n        if n.find(b) >= 1:\n            return True\n    return False\n", "hanlp/utils/file_read_backwards/file_read_backwards.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"FileReadBackwards module.\"\"\"\n\nimport io\nimport os\n\nfrom .buffer_work_space import BufferWorkSpace\n\nsupported_encodings = [\"utf-8\", \"ascii\", \"latin-1\"]  # any encodings that are backward compatible with ascii should work\n\n\nclass FileReadBackwards:\n\n    \"\"\"Class definition for `FileReadBackwards`.\n    \n    A `FileReadBackwards` will spawn a `FileReadBackwardsIterator` and keep an opened file handler.\n    \n    It can be used as a Context Manager. If done so, when exited, it will close its file handler.\n    \n    In any mode, `close()` can be called to close the file handler..\n\n    Args:\n\n    Returns:\n\n    \"\"\"\n\n    def __init__(self, path, encoding=\"utf-8\", chunk_size=io.DEFAULT_BUFFER_SIZE):\n        \"\"\"Constructor for FileReadBackwards.\n\n        Args:\n            path: Path to the file to be read\n            encoding (str): Encoding\n            chunk_size (int): How many bytes to read at a time\n        \"\"\"\n        if encoding.lower() not in supported_encodings:\n            error_message = \"{0} encoding was not supported/tested.\".format(encoding)\n            error_message += \"Supported encodings are '{0}'\".format(\",\".join(supported_encodings))\n            raise NotImplementedError(error_message)\n\n        self.path = path\n        self.encoding = encoding.lower()\n        self.chunk_size = chunk_size\n        self.iterator = FileReadBackwardsIterator(io.open(self.path, mode=\"rb\"), self.encoding, self.chunk_size)\n\n    def __iter__(self):\n        \"\"\"Return its iterator.\"\"\"\n        return self.iterator\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Closes all opened its file handler and propagates all exceptions on exit.\"\"\"\n        self.close()\n        return False\n\n    def close(self):\n        \"\"\"Closes all opened it s file handler.\"\"\"\n        self.iterator.close()\n\n    def readline(self):\n        \"\"\" \"\"\"\n\n        try:\n            r = next(self.iterator) + os.linesep\n            return r\n        except StopIteration:\n            return \"\"\n\n\nclass FileReadBackwardsIterator:\n    \"\"\"Iterator for `FileReadBackwards`.\n    \n    This will read backwards line by line a file. It holds an opened file handler.\n\n    Args:\n\n    Returns:\n\n    \"\"\"\n    def __init__(self, fp, encoding, chunk_size):\n        \"\"\"Constructor for FileReadBackwardsIterator\n\n        Args:\n            fp (File): A file that we wish to start reading backwards from\n            encoding (str): Encoding of the file\n            chunk_size (int): How many bytes to read at a time\n        \"\"\"\n        self.path = fp.name\n        self.encoding = encoding\n        self.chunk_size = chunk_size\n        self.__fp = fp\n        self.__buf = BufferWorkSpace(self.__fp, self.chunk_size)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"Returns unicode string from the last line until the beginning of file.\n        \n        Gets exhausted if::\n        \n            * already reached the beginning of the file on previous iteration\n            * the file got closed\n        \n        When it gets exhausted, it closes the file handler.\n\n        Args:\n\n        Returns:\n\n        \"\"\"\n        # Using binary mode, because some encodings such as \"utf-8\" use variable number of\n        # bytes to encode different Unicode points.\n        # Without using binary mode, we would probably need to understand each encoding more\n        # and do the seek operations to find the proper boundary before issuing read\n        if self.closed:\n            raise StopIteration\n        if self.__buf.has_returned_every_line():\n            self.close()\n            raise StopIteration\n        self.__buf.read_until_yieldable()\n        r = self.__buf.return_line()\n        return r.decode(self.encoding)\n\n    __next__ = next\n\n    @property\n    def closed(self):\n        \"\"\"The status of the file handler.\n        \n        :return: True if the file handler is still opened. False otherwise.\n\n        Args:\n\n        Returns:\n\n        \"\"\"\n        return self.__fp.closed\n\n    def close(self):\n        \"\"\"Closes the file handler.\"\"\"\n        self.__fp.close()\n", "hanlp/utils/file_read_backwards/__init__.py": "# -*- coding: utf-8 -*-\n\nfrom .file_read_backwards import FileReadBackwards  # noqa: F401\n\n__author__ = \"\"\"Robin Robin\"\"\"\n__email__ = 'robinsquare42@gmail.com'\n__version__ = '2.0.0'\n", "hanlp/components/lambda_wrapper.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 18:36\nfrom typing import Callable, Any\n\nfrom hanlp.common.component import Component\nfrom hanlp_common.reflection import classpath_of, object_from_classpath, str_to_type\n\n\nclass LambdaComponent(Component):\n    def __init__(self, function: Callable) -> None:\n        super().__init__()\n        self.config = {}\n        self.function = function\n        self.config['function'] = classpath_of(function)\n        self.config['classpath'] = classpath_of(self)\n\n    def predict(self, data: Any, **kwargs):\n        unpack = kwargs.pop('_hanlp_unpack', None)\n        if unpack:\n            return self.function(*data, **kwargs)\n        return self.function(data, **kwargs)\n\n    @staticmethod\n    def from_config(meta: dict, **kwargs):\n        cls = str_to_type(meta['classpath'])\n        function = meta['function']\n        function = object_from_classpath(function)\n        return cls(function)\n", "hanlp/components/pipeline.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 00:22\nimport types\nfrom typing import Callable, Union, Iterable, Any\nfrom hanlp.components.lambda_wrapper import LambdaComponent\nfrom hanlp.common.component import Component\nfrom hanlp_common.document import Document\nfrom hanlp.utils.component_util import load_from_meta\nfrom hanlp_common.io import save_json, load_json\nfrom hanlp_common.reflection import str_to_type, classpath_of\nimport hanlp\n\n\nclass Pipe(Component):\n\n    def __init__(self, component: Component, input_key: str = None, output_key: str = None, **kwargs) -> None:\n        super().__init__()\n        if not hasattr(self, 'config'):\n            self.config = {'classpath': classpath_of(self)}\n        self.output_key = output_key\n        self.input_key = input_key\n        self.component = component\n        self.kwargs = kwargs\n        self.config.update({\n            'component': component.config,\n            'input_key': self.input_key,\n            'output_key': self.output_key,\n            'kwargs': self.kwargs\n        })\n\n    # noinspection PyShadowingBuiltins\n    def predict(self, doc: Document, **kwargs) -> Document:\n\n        unpack = False\n        if self.input_key:\n            if isinstance(self.input_key, (tuple, list)):\n                if isinstance(self.component, LambdaComponent):  # assume functions take multiple arguments\n                    input = [doc[key] for key in self.input_key]\n                    unpack = True\n                else:\n                    input = list(list(zip(*sent)) for sent in zip(*[doc[key] for key in self.input_key]))\n            else:\n                input = doc[self.input_key]\n        else:\n            input = doc\n\n        if self.kwargs:\n            kwargs.update(self.kwargs)\n        if unpack:\n            kwargs['_hanlp_unpack'] = True\n        output = self.component(input, **kwargs)\n        if isinstance(output, types.GeneratorType):\n            output = list(output)\n        if self.output_key:\n            if not isinstance(doc, Document):\n                doc = Document()\n            if isinstance(self.output_key, tuple):\n                for key, value in zip(self.output_key, output):\n                    doc[key] = value\n            else:\n                doc[self.output_key] = output\n            return doc\n        return output\n\n    def __repr__(self):\n        name = self.component.function.__name__ if isinstance(self.component, LambdaComponent) \\\n            else self.component.__class__.__name__\n        return f'{self.input_key}->{name}->{self.output_key}'\n\n    @staticmethod\n    def from_config(meta: dict, **kwargs):\n        cls = str_to_type(meta['classpath'])\n        component = load_from_meta(meta['component'])\n        return cls(component, meta['input_key'], meta['output_key'], **meta['kwargs'])\n\n\nclass Pipeline(Component, list):\n    def __init__(self, *pipes: Pipe) -> None:\n        super().__init__()\n        if not hasattr(self, 'config'):\n            self.config = {'classpath': classpath_of(self)}\n        if pipes:\n            self.extend(pipes)\n\n    def append(self, component: Callable, input_key: Union[str, Iterable[str]] = None,\n               output_key: Union[str, Iterable[str]] = None, **kwargs):\n        \"\"\"\n        Append a pipe to the tail of this pipeline.\n\n        Args:\n            component: A callable function.\n            input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from\n                previous pipe; ``*``: use all the outputs from previous pipes wrapped in a\n                :class:`~hanlp_common.document.Document`.\n            output_key: The output key indicating where to store the outputs\n            **kwargs: Extra arguments passed to the ``Pipe`` constructor.\n\n        Returns:\n\n            Pipeline: A pipeline.\n        \"\"\"\n        self.insert(len(self), component, input_key, output_key, **kwargs)\n        return self\n\n    def insert(self, index: int, component: Callable, input_key: Union[str, Iterable[str]] = None,\n               output_key: Union[str, Iterable[str]] = None,\n               **kwargs):\n        \"\"\"\n\n        Args:\n            index: The index of the new pipe.\n            input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from\n                previous pipe; ``*``: use all the outputs from previous pipes wrapped in a\n                :class:`~hanlp_common.document.Document`.\n            output_key: The output key indicating where to store the outputs\n            **kwargs: Extra arguments passed to the ``Pipe`` constructor.\n\n        Returns:\n\n            Pipeline: A pipeline.\n        \"\"\"\n        if input_key == '*':\n            input_key = None\n        elif not input_key and len(self) and index:\n            input_key = self[index - 1].output_key\n        if not isinstance(component, Component):\n            component = LambdaComponent(component)\n        super().insert(index, Pipe(component, input_key, output_key, **kwargs))\n        return self\n\n    def __call__(self, doc: Union[Document, Any] = None, **kwargs) -> Document:\n        \"\"\"Run the pipeline as a function.\n\n        Args:\n            doc: A :class:`~hanlp_common.document.Document` or other data types.\n            **kwargs: If `doc` is set to None then create a :class:`~hanlp_common.document.Document` as the\n                input to the first pipe using all the parameters in ``kwargs``.\n\n        Returns:\n            A :class:`~hanlp_common.document.Document`.\n        \"\"\"\n        if doc is None:\n            doc = Document(**kwargs)\n        for component in self:\n            doc = component(doc)\n        return doc\n\n    @property\n    def meta(self):\n        return {\n            'classpath': classpath_of(self),\n            'hanlp_version': hanlp.version.__version__,\n            'pipes': [pipe.config for pipe in self]\n        }\n\n    @meta.setter\n    def meta(self, value):\n        pass\n\n    def save(self, filepath):\n        save_json(self.meta, filepath)\n\n    def load(self, filepath):\n        meta = load_json(filepath)\n        self.clear()\n        self.extend(Pipeline.from_config(meta))\n\n    @staticmethod\n    def from_config(meta: Union[dict, str], **kwargs):\n        if isinstance(meta, str):\n            meta = load_json(meta)\n        return Pipeline(*[load_from_meta(pipe) for pipe in meta['pipes']])\n", "hanlp/components/rnn_language_model_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-04 17:28\nfrom typing import List, Union\n\nimport tensorflow as tf\n\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp.transform.text_tf import TextTransform\n\n\nclass RNNLanguageModel(KerasComponent):\n\n    def __init__(self, transform: TextTransform = None) -> None:\n        if not transform:\n            transform = TextTransform()\n        super().__init__(transform)\n        self.transform: TextTransform = transform\n\n    def fit(self, trn_data, dev_data, save_dir,\n            forward=True,\n            embedding=100,\n            rnn_input_dropout=0.1,\n            rnn_units: int = 1024,\n            rnn_output_dropout=0.1,\n            seq_len: int = 250,\n            optimizer='sgd',\n            learning_rate=20,\n            anneal_factor: float = 0.25,\n            anneal_patience: int = 10,\n            clipnorm=0.25,\n            batch_size: int = 100, epochs=1000, run_eagerly=False, logger=None, verbose=True,\n            **kwargs):\n        return super().fit(**dict((k, v) for k, v in locals().items() if k not in ('self', 'kwargs')))\n\n    def build_model(self, embedding, rnn_input_dropout, rnn_units, rnn_output_dropout, batch_size, seq_len, training,\n                    **kwargs) -> tf.keras.Model:\n        model = tf.keras.Sequential()\n        extra_args = {}\n        if training:\n            extra_args['batch_input_shape'] = [batch_size, seq_len]\n        embedding = tf.keras.layers.Embedding(input_dim=len(self.transform.vocab), output_dim=embedding,\n                                              trainable=True, mask_zero=True, **extra_args)\n        model.add(embedding)\n        if rnn_input_dropout:\n            model.add(tf.keras.layers.Dropout(rnn_input_dropout, name='rnn_input_dropout'))\n        model.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=training, name='encoder'))\n        if rnn_output_dropout:\n            model.add(tf.keras.layers.Dropout(rnn_output_dropout, name='rnn_output_dropout'))\n        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(self.transform.vocab)), name='decoder'))\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_optimizer(self, optimizer, learning_rate, clipnorm, **kwargs):\n        if optimizer == 'sgd':\n            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=clipnorm)\n        return super().build_optimizer(optimizer, **kwargs)\n\n    def build_train_dataset(self, trn_data, batch_size):\n        trn_data = self.transform.file_to_dataset(trn_data, batch_size=batch_size, shuffle=False, repeat=-1)\n        return trn_data\n\n    def build_valid_dataset(self, dev_data, batch_size):\n        dev_data = self.transform.file_to_dataset(dev_data, batch_size=batch_size, shuffle=False, drop_remainder=True)\n        return dev_data\n\n    def generate_text(self, text: Union[str, List[str]] = '\\n', num_steps=50):\n        char_mode = False\n        if isinstance(text, str):\n            text = list(text)\n            char_mode = True\n        forward = self.config['forward']\n        # A slow implementation. Might better to let LSTM return states.\n        # But anyway, this interface is for fun so let's take it easy\n        for step in range(num_steps):\n            output = self.predict(text)\n            first_or_last_token = output[-1]\n            if forward:\n                text += first_or_last_token\n            else:\n                text = [first_or_last_token] + text\n        if char_mode:\n            text = ''.join(text)\n        return text\n", "hanlp/components/lemmatizer.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-08 18:35\nfrom typing import List\n\nfrom hanlp.common.transform import TransformList\nfrom hanlp.components.parsers.ud.lemma_edit import gen_lemma_rule, apply_lemma_rule\nfrom hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger\n\n\ndef add_lemma_rules_to_sample(sample: dict):\n    if 'tag' in sample and 'lemma' not in sample:\n        lemma_rules = [gen_lemma_rule(word, lemma)\n                       if lemma != \"_\" else \"_\"\n                       for word, lemma in zip(sample['token'], sample['tag'])]\n        sample['lemma'] = sample['tag'] = lemma_rules\n    return sample\n\n\nclass TransformerLemmatizer(TransformerTagger):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"A transition based lemmatizer using transformer as encoder.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_dataset(self, data, transform=None, **kwargs):\n        if not isinstance(transform, list):\n            transform = TransformList()\n        transform.append(add_lemma_rules_to_sample)\n        return super().build_dataset(data, transform, **kwargs)\n\n    def prediction_to_human(self, pred, vocab: List[str], batch, token=None):\n        if token is None:\n            token = batch['token']\n        rules = super().prediction_to_human(pred, vocab, batch)\n        for token_per_sent, rule_per_sent in zip(token, rules):\n            lemma_per_sent = [apply_lemma_rule(t, r) for t, r in zip(token_per_sent, rule_per_sent)]\n            yield lemma_per_sent\n", "hanlp/components/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 16:10\nfrom .pipeline import Pipeline", "hanlp/components/taggers/tagger.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 12:19\nimport logging\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom typing import List, TextIO, Any, Union, Dict, Tuple, Sequence\n\nimport torch\nfrom torch import optim, nn\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import IDX\nfrom hanlp.common.structure import History\nfrom hanlp.components.distillation.distillable_component import DistillableComponent\nfrom hanlp.components.taggers.util import guess_tagging_scheme\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.metrics.accuracy import CategoricalAccuracy\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import reorder\nfrom hanlp_trie import DictInterface, TrieDict\nfrom hanlp_trie.dictionary import TupleTrieDict\n\n\nclass Tagger(DistillableComponent, ABC):\n    def build_optimizer(self, optimizer, lr, **kwargs):\n        if optimizer == 'adam':\n            return optim.Adam(params=self.model.parameters(), lr=lr)\n        elif optimizer == 'sgd':\n            return torch.optim.SGD(self.model.parameters(), lr=lr)\n\n    def build_criterion(self, model=None, reduction='mean', decoder=None, **kwargs):\n        if self.config.get('crf', False):\n            if not model:\n                model = decoder or self.model\n            if isinstance(model, nn.DataParallel):\n                raise ValueError('DataParallel not supported when CRF is used')\n                return self.model_from_config.module.crf\n            return model.crf\n        else:\n            return nn.CrossEntropyLoss(reduction=reduction)\n\n    def build_metric(self, **kwargs):\n        return CategoricalAccuracy()\n\n    @abstractmethod\n    def feed_batch(self, batch):\n        pass\n\n    def compute_loss(self, criterion, out, y, mask):\n        if self.config.get('crf', False):\n            criterion: CRF = criterion\n            loss = -criterion.forward(out, y, mask)\n        else:\n            loss = criterion(out[mask], y[mask])\n        return loss\n\n    def decode_output(self, logits, mask, batch, model=None):\n        if self.config.get('crf', False):\n            if model is None:\n                model = self.model\n            crf: CRF = model.crf\n            return crf.decode(logits, mask)\n        else:\n            return logits.argmax(-1)\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, patience=5, teacher=None,\n                              kd_criterion=None, eval_trn=True,\n                              **kwargs):\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,\n                                eval_trn=eval_trn, **self.config)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, logger=logger, ratio_width=ratio_width)\n            timer.update()\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if dev_metric > best_metric:\n                best_epoch, best_metric = epoch, dev_metric\n                self.save_weights(save_dir)\n                report += ' [red](saved)[/red]'\n            else:\n                report += f' ({epoch - best_epoch})'\n                if epoch - best_epoch >= patience:\n                    report += ' early stop'\n            logger.info(report)\n            if epoch - best_epoch >= patience:\n                break\n        if not best_epoch:\n            self.save_weights(save_dir)\n        elif best_epoch != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric} at epoch {best_epoch}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n        return best_metric\n\n    def id_to_tags(self, ids: torch.LongTensor, lens: List[int]):\n        batch = []\n        vocab = self.vocabs['tag'].idx_to_token\n        for b, l in zip(ids, lens):\n            batch.append([])\n            for i in b[:l]:\n                batch[-1].append(vocab[i])\n        return batch\n\n    def update_metrics(self, metric, logits, y, mask, batch=None, prediction=None):\n        metric(logits, y, mask)\n\n    @torch.no_grad()\n    def evaluate_dataloader(self, data, criterion, logger=None, ratio_width=None, metric=None, output=None, **kwargs):\n        self.model.eval()\n        if isinstance(output, str):\n            output = open(output, 'w')\n\n        loss = 0\n        if not metric:\n            metric = self.build_metric()\n        else:\n            metric.reset()\n        timer = CountdownTimer(len(data))\n        for idx, batch in enumerate(data):\n            logits, mask = self.feed_batch(batch)\n            y = batch['tag_id']\n            loss += self.compute_loss(criterion, logits, y, mask).item()\n            prediction = self.decode_output(logits, mask, batch)\n            self.update_metrics(metric, logits, y, mask, batch, prediction)\n            if output:\n                self.write_prediction(prediction, batch, output)\n            timer.log(f'loss: {loss / (idx + 1):.4f} {metric}', ratio_percentage=False, logger=logger,\n                      ratio_width=ratio_width)\n        loss /= len(data)\n        if output:\n            output.close()\n        return float(loss), metric\n\n    def write_prediction(self, prediction, batch, output: TextIO):\n        for tokens, ps, gs in zip(batch[self.config.token_key], prediction, batch['tag']):\n            output.write('\\n'.join('\\t'.join([t, p, g]) for t, p, g in zip(tokens, ps, gs)))\n            output.write('\\n')\n\n    def predict(self, tokens: Any, batch_size: int = None, **kwargs):\n        if not tokens:\n            return []\n        flat = self.input_is_flat(tokens)\n        if flat:\n            tokens = [tokens]\n        outputs = self.predict_data(tokens, batch_size, **kwargs)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    def input_is_flat(self, tokens):\n        return isinstance(tokens, list) and isinstance(tokens[0], str)\n\n    def predict_data(self, data, batch_size, sampler_builder=None, **kwargs):\n        samples = self.build_samples(data, **kwargs)\n        if not batch_size:\n            batch_size = self.config.get('batch_size', 32)\n        dataloader = self.build_dataloader(samples, batch_size, False, self.device, sampler_builder=sampler_builder,\n                                           **kwargs)\n        outputs = []\n        orders = []\n        vocab = self.vocabs['tag'].idx_to_token\n        for batch in dataloader:\n            out, mask = self.feed_batch(batch)\n            pred = self.decode_output(out, mask, batch)\n            outputs.extend(self.prediction_to_human(pred, vocab, batch))\n            orders.extend(batch[IDX])\n        outputs = reorder(outputs, orders)\n        return outputs\n\n    def build_samples(self, data: List[str], **kwargs):\n        return [{self.config.token_key: sent} for sent in data]\n\n    def prediction_to_human(self, pred_ids, vocab: List[str], batch):\n        if isinstance(pred_ids, torch.Tensor):\n            pred_ids = pred_ids.tolist()\n        sents = batch[self.config.token_key]\n        dict_tags: DictInterface = self.dict_tags\n        for each, sent in zip(pred_ids, sents):\n            tags = [vocab[id] for id in each[:len(sent)]]\n            if dict_tags:\n                for begin, end, label in dict_tags.tokenize(sent):\n                    tags[begin:end] = label\n            yield tags\n\n    @property\n    def tagging_scheme(self):\n        tagging_scheme = self.config.tagging_scheme\n        if not tagging_scheme:\n            self.config.tagging_scheme = tagging_scheme = guess_tagging_scheme(self.vocabs.tag.idx_to_token)\n            if tagging_scheme == 'BIO':\n                warnings.warn(f'The tag scheme for {self.vocabs.tag.idx_to_token} might be IOB1 or IOB2 '\n                              f'but we are using IOB2 by default. Please set tagging_scheme=\"IOB1\" or tagging_scheme=\"BIO\" '\n                              f'to get rid of this warning.')\n        return tagging_scheme\n\n    @property\n    def dict_tags(self) -> DictInterface:\n        r\"\"\" A custom dictionary to override predicted tags by performing longest-prefix-matching.\n\n        Examples:\n            >>> pos.dict_tags = {'HanLP': 'state-of-the-art-tool'} # Force 'HanLP' to be 'state-of-the-art-tool'\n            >>> tagger(\"HanLP\u4e3a\u751f\u4ea7\u73af\u5883\u5e26\u6765\u6b21\u4e16\u4ee3\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cdNLP\u6280\u672f\u3002\")\n                # HanLP/state-of-the-art-tool \u4e3a/P \u751f\u4ea7/NN \u73af\u5883/NN \u5e26\u6765/VV \u6b21\u4e16\u4ee3/NN \u6700/AD \u5148\u8fdb/VA \u7684/DEC \u591a\u8bed\u79cd/NN NLP/NR \u6280\u672f/NN \u3002/PU\n            >>> pos.dict_tags = {('\u7684', '\u5e0c\u671b'): ('\u8865\u8bed\u6210\u5206', '\u540d\u8bcd'), '\u5e0c\u671b': '\u52a8\u8bcd'} # Conditional matching\n            >>> tagger(\"\u6211\u7684\u5e0c\u671b\u662f\u5e0c\u671b\u5f20\u665a\u971e\u7684\u80cc\u5f71\u88ab\u665a\u971e\u6620\u7ea2\u3002\")\n                # \u6211/PN \u7684/\u8865\u8bed\u6210\u5206 \u5e0c\u671b/\u540d\u8bcd \u662f/VC \u5e0c\u671b/\u52a8\u8bcd \u5f20\u665a\u971e/NR \u7684/DEG \u80cc\u5f71/NN \u88ab/LB \u665a\u971e/NN \u6620\u7ea2/VV \u3002/PU\n        \"\"\"\n        return self.config.get('dict_tags', None)\n\n    @dict_tags.setter\n    def dict_tags(self,\n                  dictionary: Union[DictInterface, Union[Dict[Union[str, Sequence[str]], Union[str, Sequence[str]]]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            assert isinstance(dictionary, dict), f'Expected dictionary to be `dict` but got {type(dictionary)}.'\n            _d = dict()\n            for k, v in dictionary.items():\n                if isinstance(k, str):\n                    k = (k,)\n                if isinstance(v, str):\n                    v = (v,) * len(k)\n                _d[k] = v\n            dictionary = TupleTrieDict(_d)\n        self.config.dict_tags = dictionary\n", "hanlp/components/taggers/pos_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-05 23:05\nfrom hanlp.components.taggers.cnn_tagger_tf import CNNTaggerTF\nfrom hanlp.components.taggers.rnn_tagger_tf import RNNTaggerTF\n\n\nclass CNNPartOfSpeechTaggerTF(CNNTaggerTF):\n    pass\n\n\nclass RNNPartOfSpeechTaggerTF(RNNTaggerTF):\n    pass\n", "hanlp/components/taggers/cnn_tagger_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-31 13:52\nfrom abc import ABC\nfrom typing import Union, Tuple, Any, List, Iterable\n\nimport tensorflow as tf\n\nfrom hanlp.components.taggers.tagger_tf import TaggerComponent\nfrom hanlp.transform.tsv_tf import TSVTaggingTransform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.layers.embeddings.util_tf import build_embedding\n\n\nclass WindowTokenTransform(TSVTaggingTransform):\n\n    def fit(self, trn_path: str, **kwargs):\n        self.word_vocab = VocabTF()\n        self.tag_vocab = VocabTF(pad_token=None, unk_token=None)\n        for ngrams, tags in self.file_to_samples(trn_path):\n            for words in ngrams:\n                self.word_vocab.update(words)\n            self.tag_vocab.update(tags)\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        window_radius = self.config.window_radius\n        window_size = 2 * window_radius + 1\n        types = tf.string, tf.string\n        shapes = [None, window_size], [None]\n        values = self.word_vocab.pad_token, self.tag_vocab.first_token\n        return types, shapes, values\n\n    def inputs_to_samples(self, inputs, gold=False):\n        window_radius = self.config.window_radius\n        for t in inputs:\n            if gold:\n                words, tags = t\n            else:\n                words, tags = t, [self.padding_values[-1]] * len(t)\n            ngrams = []\n            for i, word in enumerate(words):\n                features = []\n                for t in range(-window_radius, window_radius + 1):\n                    index = i + t\n                    if index < 0:\n                        feature = 'bos{}'.format(index)\n                    elif index >= len(words):\n                        feature = 'eos+{}'.format(index - len(words) + 1)\n                    else:\n                        feature = words[index]\n                    features.append(feature)\n                ngrams.append(features)\n            yield ngrams, tags\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        for xs in X:\n            words = []\n            for x in xs:\n                words.append(self.word_vocab.idx_to_token[int(x[len(x) // 2])])\n            yield words\n\n\nclass CNNTaggingModel(tf.keras.models.Model):\n    def __init__(self, filters, num_tags, embed, dropout, kernels, **kwargs):\n        super().__init__()\n        self.embed = embed\n        self.embed_dropout = tf.keras.layers.Dropout(rate=dropout)\n        self.conv2d = []\n        for k in kernels:\n            self.conv2d.append(\n                tf.keras.layers.Conv2D(filters=filters, kernel_size=k, data_format='channels_last', padding='same'))\n        self.conv2d_dropout = tf.keras.layers.Dropout(rate=dropout)\n        self.concat = tf.keras.layers.Concatenate()\n        self.dense = tf.keras.layers.Dense(units=num_tags)\n\n    def call(self, inputs, **kwargs):\n        # if inputs.shape_h[0] is None:\n        #     return tf.zeros_like()\n        #     print(inputs)\n        embeds = self.embed(inputs)\n        embeds = self.embed_dropout(embeds)\n        hs = [conv(embeds) for conv in self.conv2d]\n        h = self.concat(hs)\n        h = self.conv2d_dropout(h)\n        shape_h = tf.shape(h)\n        h = tf.reshape(h, [shape_h[0], shape_h[1], h.shape[2] * h.shape[3]])\n        o = self.dense(h)\n        if h.shape[0]:\n            mask = embeds._keras_mask[:, :, 0]\n            o._keras_mask = mask\n        return o\n\n\nclass CNNTaggerTF(TaggerComponent, ABC):\n    def __init__(self, transform: WindowTokenTransform = None) -> None:\n        if not transform:\n            transform = WindowTokenTransform()\n        super().__init__(transform)\n        self.model: CNNTaggingModel = self.model  # refine the type\n        self.transform: WindowTokenTransform = self.transform\n\n    def build_model(self, embedding, **kwargs) -> tf.keras.Model:\n        embed = build_embedding(embedding, self.transform.word_vocab, self.transform)\n        self.transform.map_x = embed.dtype != tf.string\n        model = CNNTaggingModel(num_tags=len(self.transform.tag_vocab),\n                                embed=embed,\n                                **kwargs)\n        # model.build((None, None, 3))\n        return model\n\n    # noinspection PyMethodOverriding\n    def fit(self, trn_data: Any, dev_data: Any, save_dir: str, embedding=200, window_radius=3,\n            kernels=(1, 2, 3, 4, 5), filters=200, dropout=0.3,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='accuracy', batch_size=100,\n            epochs=100,\n            logger=None, verbose=True, **kwargs):\n        kwargs.update(locals())\n        for k in 'self', 'kwargs', '__class__':\n            kwargs.pop(k)\n        super().fit(**kwargs)\n\n    @property\n    def input_shape(self) -> List:\n        return [[None, None, self.config.window_radius * 2 + 1]]\n", "hanlp/components/taggers/tagger_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-25 21:49\nimport logging\nfrom abc import ABC\n\nimport tensorflow as tf\n\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp.layers.crf.crf_layer_tf import CRF, CRFLoss, CRFWrapper\nfrom hanlp.metrics.chunking.iobes_tf import IOBES_F1_TF\n\n\nclass TaggerComponent(KerasComponent, ABC):\n\n    def build_metrics(self, metrics, logger: logging.Logger, **kwargs):\n        if metrics == 'f1':\n            assert hasattr(self.transform, 'tag_vocab'), 'Name your tag vocab tag_vocab in your transform ' \\\n                                                         'or override build_metrics'\n            if not self.config.get('run_eagerly', None):\n                logger.debug('ChunkingF1 runs only under eager mode, '\n                             'set run_eagerly=True to remove this warning')\n            self.config.run_eagerly = True\n            return IOBES_F1_TF(self.transform.tag_vocab)\n        return super().build_metrics(metrics, logger, **kwargs)\n\n    def build_loss(self, loss, **kwargs):\n        assert self.model is not None, 'should create model before build loss'\n        if loss == 'crf':\n            if isinstance(self.model, tf.keras.models.Sequential):\n                crf = CRF(len(self.transform.tag_vocab))\n                self.model.add(crf)\n                loss = CRFLoss(crf, self.model.dtype)\n            else:\n                self.model = CRFWrapper(self.model, len(self.transform.tag_vocab))\n                loss = CRFLoss(self.model.crf, self.model.dtype)\n            return loss\n        return super().build_loss(loss, **kwargs)\n", "hanlp/components/taggers/util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-01 00:31\nfrom typing import List, Tuple\nfrom hanlp.utils.span_util import allowed_transitions\n\n\ndef guess_tagging_scheme(labels: List[str]) -> str:\n    tagset = set(y.split('-')[0] for y in labels)\n    for scheme in \"BIO\", \"BIOUL\", \"BMES\", 'IOBES':\n        if tagset == set(list(scheme)):\n            return scheme\n\n\ndef guess_allowed_transitions(labels) -> List[Tuple[int, int]]:\n    scheme = guess_tagging_scheme(labels)\n    if not scheme:\n        return None\n    if scheme == 'IOBES':\n        scheme = 'BIOUL'\n        labels = [y.replace('E-', 'L-').replace('S-', 'U-') for y in labels]\n    return allowed_transitions(scheme, dict(enumerate(labels)))\n", "hanlp/components/taggers/rnn_tagger.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-20 13:12\nimport logging\n\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import PadSequenceDataLoader, SortingSampler, TransformableDataset\nfrom hanlp_common.configurable import Configurable\nfrom hanlp.common.transform import EmbeddingNamedTransform\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.taggers.rnn.rnntaggingmodel import RNNTaggingModel\nfrom hanlp.components.taggers.tagger import Tagger\nfrom hanlp.datasets.ner.loaders.tsv import TSVTaggingDataset\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.layers.embeddings.util import build_word2vec_with_vocab\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, merge_dict\n\n\nclass RNNTagger(Tagger):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"An old-school tagger using non-contextualized embeddings and RNNs as context layer.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: RNNTaggingModel = None\n\n    # noinspection PyMethodOverriding\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion,\n                              optimizer,\n                              metric,\n                              save_dir,\n                              logger,\n                              patience,\n                              **kwargs):\n        max_e, max_metric = 0, -1\n\n        criterion = self.build_criterion()\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        scheduler = self.build_scheduler(**merge_dict(self.config, optimizer=optimizer, overwrite=True))\n        if not patience:\n            patience = epochs\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, ratio_width=ratio_width)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, logger)\n            if scheduler:\n                if isinstance(scheduler, ReduceLROnPlateau):\n                    scheduler.step(dev_metric.score)\n                else:\n                    scheduler.step(epoch)\n            report_patience = f'Patience: {epoch - max_e}/{patience}'\n            # save the model if it is the best so far\n            if dev_metric > max_metric:\n                self.save_weights(save_dir)\n                max_e, max_metric = epoch, dev_metric\n                report_patience = '[red]Saved[/red] '\n            stop = epoch - max_e >= patience\n            if stop:\n                timer.stop()\n            timer.log(f'{report_patience} lr: {optimizer.param_groups[0][\"lr\"]:.4f}',\n                      ratio_percentage=False, newline=True, ratio=False)\n            if stop:\n                break\n        timer.stop()\n        if max_e != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {max_metric.score:.2%} at epoch {max_e}\")\n        logger.info(f\"{timer.elapsed_human} elapsed, average time of each epoch is {timer.elapsed_average_human}\")\n\n    def build_scheduler(self, optimizer, anneal_factor, anneal_patience, **kwargs):\n        scheduler: ReduceLROnPlateau = ReduceLROnPlateau(optimizer,\n                                                         factor=anneal_factor,\n                                                         patience=anneal_patience,\n                                                         mode='max') if anneal_factor and anneal_patience else None\n        return scheduler\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, ratio_width=None,\n                       **kwargs):\n        self.model.train()\n        timer = CountdownTimer(len(trn))\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            optimizer.zero_grad()\n            out, mask = self.feed_batch(batch)\n            y = batch['tag_id']\n            loss = self.compute_loss(criterion, out, y, mask)\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n            optimizer.step()\n            total_loss += loss.item()\n            prediction = self.decode_output(out, mask, batch)\n            self.update_metrics(metric, out, y, mask, batch, prediction)\n            timer.log(f'loss: {loss / (idx + 1):.4f} {metric}', ratio_percentage=False, logger=logger,\n                      ratio_width=ratio_width)\n            del loss\n            del out\n            del mask\n\n    def feed_batch(self, batch):\n        x = batch[f'{self.config.token_key}_id']\n        out, mask = self.model(x, **batch, batch=batch)\n        return out, mask\n\n    # noinspection PyMethodOverriding\n    def build_model(self, rnn_input, rnn_hidden, drop, crf, **kwargs) -> torch.nn.Module:\n        vocabs = self.vocabs\n        token_embed = self._convert_embed()\n        if isinstance(token_embed, EmbeddingNamedTransform):\n            token_embed = token_embed.output_dim\n        elif isinstance(token_embed, Embedding):\n            token_embed = token_embed.module(vocabs=vocabs)\n        else:\n            token_embed = build_word2vec_with_vocab(token_embed, vocabs[self.config.token_key])\n        model = RNNTaggingModel(token_embed, rnn_input, rnn_hidden, len(vocabs['tag']), drop, crf)\n        return model\n\n    def _convert_embed(self):\n        embed = self.config['embed']\n        if isinstance(embed, dict):\n            self.config['embed'] = embed = Configurable.from_config(embed)\n        return embed\n\n    def build_dataloader(self, data, batch_size, shuffle, device, logger=None, **kwargs) -> DataLoader:\n        vocabs = self.vocabs\n        token_embed = self._convert_embed()\n        dataset = data if isinstance(data, TransformableDataset) else self.build_dataset(data, transform=[vocabs])\n        if vocabs.mutable:\n            # Before building vocabs, let embeddings submit their vocabs, some embeddings will possibly opt out as their\n            # transforms are not relevant to vocabs\n            if isinstance(token_embed, Embedding):\n                transform = token_embed.transform(vocabs=vocabs)\n                if transform:\n                    dataset.transform.insert(-1, transform)\n            self.build_vocabs(dataset, logger)\n        if isinstance(token_embed, Embedding):\n            # Vocabs built, now add all transforms to the pipeline. Be careful about redundant ones.\n            transform = token_embed.transform(vocabs=vocabs)\n            if transform and transform not in dataset.transform:\n                dataset.transform.insert(-1, transform)\n        sampler = SortingSampler([len(sample[self.config.token_key]) for sample in dataset], batch_size,\n                                 shuffle=shuffle)\n        return PadSequenceDataLoader(dataset,\n                                     device=device,\n                                     batch_sampler=sampler,\n                                     vocabs=vocabs)\n\n    def build_dataset(self, data, transform):\n        return TSVTaggingDataset(data, transform)\n\n    def build_vocabs(self, dataset, logger):\n        self.vocabs.tag = Vocab(unk_token=None, pad_token=None)\n        self.vocabs[self.config.token_key] = Vocab()\n        for each in dataset:\n            pass\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n\n    def fit(self, trn_data, dev_data, save_dir,\n            batch_size=50,\n            epochs=100,\n            embed=100,\n            rnn_input=None,\n            rnn_hidden=256,\n            drop=0.5,\n            lr=0.001,\n            patience=10,\n            crf=True,\n            optimizer='adam',\n            token_key='token',\n            tagging_scheme=None,\n            anneal_factor: float = 0.5,\n            anneal_patience=2,\n            devices=None, logger=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def _id_to_tags(self, ids):\n        batch = []\n        vocab = self.vocabs['tag'].idx_to_token\n        for b in ids:\n            batch.append([])\n            for i in b:\n                batch[-1].append(vocab[i])\n        return batch\n\n    def write_output(self, yhat, y, mask, batch, prediction, output):\n        pass\n", "hanlp/components/taggers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-28 15:39", "hanlp/components/taggers/rnn_tagger_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-09-14 20:30\nfrom typing import Union, List\n\nimport tensorflow as tf\n\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.components.taggers.tagger_tf import TaggerComponent\nfrom hanlp.transform.tsv_tf import TSVTaggingTransform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.layers.embeddings.util_tf import build_embedding, embeddings_require_string_input, \\\n    embeddings_require_char_input\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass RNNTaggerTF(TaggerComponent):\n\n    def __init__(self, transform: Transform = None) -> None:\n        if not transform:\n            self.transform = transform = TSVTaggingTransform()\n        super().__init__(transform)\n\n    def fit(self, trn_data: str, dev_data: str = None, save_dir: str = None, embeddings=100, embedding_trainable=False,\n            rnn_input_dropout=0.2, rnn_units=100, rnn_output_dropout=0.2, epochs=20, lower=False, logger=None,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='accuracy',\n            batch_size=32, dev_batch_size=32, lr_decay_per_epoch=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_model(self, embeddings, embedding_trainable, rnn_input_dropout, rnn_output_dropout, rnn_units,\n                    loss,\n                    **kwargs) -> tf.keras.Model:\n        model = tf.keras.Sequential()\n        embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)\n        model.add(embeddings)\n        if rnn_input_dropout:\n            model.add(tf.keras.layers.Dropout(rnn_input_dropout, name='rnn_input_dropout'))\n        model.add(\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True), name='bilstm'))\n        if rnn_output_dropout:\n            model.add(tf.keras.layers.Dropout(rnn_output_dropout, name='rnn_output_dropout'))\n        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(self.transform.tag_vocab)), name='dense'))\n        return model\n\n    def predict(self, sents: Union[List[str], List[List[str]]], batch_size=32, **kwargs) -> Union[\n        List[str], List[List[str]]]:\n        return super().predict(sents, batch_size)\n\n    def save_weights(self, save_dir, filename='model.h5'):\n        # remove the pre-trained embedding\n        embedding_layer: tf.keras.layers.Embedding = self.model.get_layer(index=0)\n        if embedding_layer.trainable:\n            super().save_weights(save_dir, filename)\n        else:\n            truncated_model = tf.keras.Sequential(layers=self.model.layers[1:])\n            truncated_model.build(input_shape=embedding_layer.output_shape)\n            truncated_model.save_weights(save_dir)\n\n    def build_loss(self, loss, **kwargs):\n        if not loss:\n            loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM,\n                                                                 from_logits=True)\n            return loss\n        return super().build_loss(loss, **kwargs)\n\n    @property\n    def tag_vocab(self) -> VocabTF:\n        return self.transform.tag_vocab\n\n    def build_transform(self, embeddings, **kwargs):\n        if embeddings_require_string_input(embeddings):\n            self.transform.map_x = False\n            if embeddings_require_char_input(embeddings):\n                self.transform.char_vocab = VocabTF()\n        return super().build_transform(**kwargs)\n\n    @property\n    def sample_data(self):\n        if self.transform.char_vocab:\n            # You cannot build your model by calling `build` if your layers do not support float type inputs.\n            # Instead, in order to instantiate and build your model, `call` your model on real tensor data (of the\n            # correct dtype).\n            sample = tf.constant([\n                ['hello', 'world', self.transform.word_vocab.pad_token],\n                ['hello', 'this', 'world'],\n            ])\n            sample._keras_mask = tf.not_equal(sample, self.transform.word_vocab.pad_token)\n            return sample\n", "hanlp/components/taggers/rnn/rnntaggingmodel.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom typing import Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\nfrom hanlp.layers.crf.crf import CRF\n\n\nclass RNNTaggingModel(nn.Module):\n\n    def __init__(self,\n                 embed: Union[nn.Embedding, int],\n                 rnn_input,\n                 rnn_hidden,\n                 n_out,\n                 drop=0.5,\n                 crf=True,\n                 crf_constraints=None):\n        super(RNNTaggingModel, self).__init__()\n\n        # the embedding layer\n        if isinstance(embed, nn.Module):\n            self.embed = embed\n            n_embed = embed.embedding_dim\n        else:\n            self.embed = None\n            n_embed = embed\n\n        if rnn_input:\n            self.embed_to_rnn = nn.Linear(n_embed, rnn_input)\n        else:\n            self.embed_to_rnn = None\n            rnn_input = n_embed\n\n        # the word-lstm layer\n        self.word_lstm = nn.LSTM(input_size=rnn_input,\n                                 hidden_size=rnn_hidden,\n                                 batch_first=True,\n                                 bidirectional=True)\n\n        # the output layer\n        self.out = nn.Linear(rnn_hidden * 2, n_out)\n        # the CRF layer\n        self.crf = CRF(n_out, crf_constraints) if crf else None\n\n        self.drop = nn.Dropout(drop)\n        # self.drop = SharedDropout(drop)\n        # self.drop = LockedDropout(drop)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # init Linear\n        nn.init.xavier_uniform_(self.out.weight)\n\n    def forward(self,\n                x: torch.Tensor,\n                batch=None,\n                **kwargs):\n        # get the mask and lengths of given batch\n        mask = x.gt(0)\n        lens = mask.sum(dim=1)\n        # get outputs from embedding layers\n        if isinstance(self.embed, nn.Embedding):\n            x = self.embed(x[mask])\n        else:\n            x = self.embed(batch, mask=mask)\n            if x.dim() == 3:\n                x = x[mask]\n        x = self.drop(x)\n        if self.embed_to_rnn:\n            x = self.embed_to_rnn(x)\n        x = pack_sequence(torch.split(x, lens.tolist()), True)\n        x, _ = self.word_lstm(x)\n        x, _ = pad_packed_sequence(x, True)\n        x = self.drop(x)\n\n        return self.out(x), mask\n", "hanlp/components/taggers/rnn/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-19 15:41", "hanlp/components/taggers/ngram_conv/ngram_conv_tagger.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-25 00:04\n\nfrom typing import Union, Optional, Tuple, Any, Iterable, List\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.components.taggers.tagger_tf import TaggerComponent\nfrom hanlp.transform.tsv_tf import TSVTaggingTransform\nfrom hanlp.transform.txt_tf import bmes_to_words, extract_ngram_features\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.layers.embeddings.util_tf import build_embedding\nfrom hanlp.layers.weight_normalization import WeightNormalization\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass NgramTransform(TSVTaggingTransform):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, **kwargs)\n        self.ngram_vocab: Optional[VocabTF] = None\n        self.tag_vocab: Optional[VocabTF] = None\n\n    def inputs_to_samples(self, inputs, gold=False):\n        for data in inputs:\n            if gold:\n                words, tags = data\n            else:\n                words, tags = data, [self.tag_vocab.safe_pad_token] * len(data)\n            features = [words]\n            if not tags:\n                tags = [self.tag_vocab.first_token] * len(words)\n            features.extend(extract_ngram_features(words, False, self.config.window_size))\n            yield tuple(features), tags\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        ids = [self.word_vocab.lookup(x[0]) if self.config.map_word_feature else x[0]]\n        for ngram in x[1:]:\n            ids.append(self.ngram_vocab.lookup(ngram))\n        return tuple(ids)\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        return self.tag_vocab.lookup(y)\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        window_size = self.config.window_size\n        ngram_size = window_size * (window_size + 1) // 2\n        vec_dim = 2 + ngram_size\n        shapes = tuple([[None]] * (vec_dim - 1)), [None]\n        types = tuple([tf.string] * (vec_dim - 1)), tf.string\n        word_vocab, ngram_vocab, tag_vocab = self.word_vocab, self.ngram_vocab, self.tag_vocab\n        defaults = tuple([word_vocab.pad_token] + [\n            ngram_vocab.pad_token if ngram_vocab else word_vocab.pad_token] * ngram_size), (\n                       tag_vocab.pad_token if tag_vocab.pad_token else tag_vocab.first_token)\n        return types, shapes, defaults\n\n    def fit(self, trn_path: str, **kwargs):\n        word_vocab, ngram_vocab, tag_vocab = VocabTF(), VocabTF(), VocabTF(pad_token=None, unk_token=None)\n        num_samples = 0\n        for X, Y in self.file_to_samples(trn_path, gold=True):\n            num_samples += 1\n            word_vocab.update(X[0])\n            for ngram in X[1:]:\n                ngram_vocab.update(filter(lambda x: x, ngram))\n            tag_vocab.update(Y)\n        self.word_vocab, self.ngram_vocab, self.tag_vocab = word_vocab, ngram_vocab, tag_vocab\n        if self.config.window_size:\n            vocabs = word_vocab, ngram_vocab, tag_vocab\n        else:\n            vocabs = word_vocab, None, tag_vocab\n        self.word_vocab, self.ngram_vocab, self.tag_vocab = vocabs\n        return num_samples\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        yield from super().X_to_inputs(X[0])\n\n    def input_truth_output_to_str(self, input: List[str], truth: List[str], output: List[str]):\n        words = bmes_to_words(input, output)\n        return ' '.join(words)\n\n\nclass NgramConvTaggingModel(tf.keras.models.Model):\n    def __init__(self, word_embed: tf.keras.layers.Embedding, ngram_embed: tf.keras.layers.Embedding, filters,\n                 kernel_size, dropout_embed, dropout_hidden, weight_norm, num_tags, **kwargs):\n        super().__init__(**kwargs)\n        if ngram_embed is not None:\n            self.ngram_embed = ngram_embed\n        self.word_embed = word_embed\n        # self.concat = tf.keras.layers.Concatenate(axis=2)\n        self.dropout_embed = tf.keras.layers.Dropout(rate=dropout_embed)\n        self.filters_w = []\n        self.filters_v = []\n\n        def create_conv1d(filter, name):\n            conv = tf.keras.layers.Conv1D(filter, kernel_size, padding=\"same\", name=name)\n            if weight_norm:\n                conv_norm = WeightNormalization(conv, name=name + '_norm', data_init=False)\n                return conv_norm\n            return conv\n\n        for idx, filter in enumerate(filters):\n            self.filters_w.append(create_conv1d(filter, 'Conv1Dw_{}'.format(idx)))\n            self.filters_v.append(create_conv1d(filter, 'Conv1Dv_{}'.format(idx)))\n        self.dropout_hidden = tf.keras.layers.Dropout(rate=dropout_hidden)\n        self.dense = tf.keras.layers.Dense(num_tags, use_bias=False)\n\n    def call(self, inputs, **kwargs):\n        if hasattr(self, 'ngram_embed'):\n            chars, ngrams = inputs[0], inputs[1:]\n            embeds = [self.word_embed(chars)]\n            mask = embeds[0]._keras_mask\n            for ngram in ngrams:\n                embeds.append(self.ngram_embed(ngram))\n            if len(embeds) > 1:\n                embed_input = tf.concat(embeds, axis=2)\n            else:\n                embed_input = embeds[0]\n        else:\n            chars = inputs if isinstance(inputs, tf.Tensor) else inputs[0]\n            embed_input = self.word_embed(chars)\n            mask = embed_input._keras_mask\n\n        mask_float = tf.dtypes.cast(mask, tf.float32)\n        embed_input = self.dropout_embed(embed_input)\n        hidden_output = embed_input\n        for fw, fv in zip(self.filters_w.layers, self.filters_v.layers):\n            w = fw(hidden_output)\n            v = fv(hidden_output)\n            hidden_output = w * tf.nn.sigmoid(v)\n            # Mask paddings.\n            hidden_output = hidden_output * tf.expand_dims(mask_float, -1)\n            hidden_output = self.dropout_hidden(hidden_output)\n        # dirty hack\n        hidden_output._keras_mask = mask\n        logits = self.dense(hidden_output)\n        return logits\n\n\nclass NgramConvTaggerTF(TaggerComponent):\n\n    def __init__(self, transform: NgramTransform = None) -> None:\n        if not transform:\n            transform = NgramTransform()\n        super().__init__(transform)\n        self.transform: NgramTransform = transform\n\n    def build_model(self, word_embed, ngram_embed, window_size, weight_norm, filters, kernel_size, dropout_embed,\n                    dropout_hidden, **kwargs) -> tf.keras.Model:\n        word_vocab, ngram_vocab, tag_vocab = self.transform.word_vocab, self.transform.ngram_vocab, \\\n                                             self.transform.tag_vocab\n        word_embed = build_embedding(word_embed, word_vocab, self.transform)\n        if 'map_x' in self.config:\n            self.config.map_word_feature = self.config.map_x\n            del self.config.map_x\n        else:\n            self.config.map_word_feature = True\n        if window_size:\n            ngram_embed = build_embedding(ngram_embed, ngram_vocab, self.transform)\n        else:\n            ngram_embed = None\n        model = NgramConvTaggingModel(word_embed, ngram_embed, filters, kernel_size, dropout_embed, dropout_hidden,\n                                      weight_norm, len(tag_vocab))\n\n        return model\n\n    def fit(self, trn_data: Any, dev_data: Any, save_dir: str, word_embed: Union[str, int, dict] = 200,\n            ngram_embed: Union[str, int,dict] = 50, embedding_trainable=True, window_size=4, kernel_size=3,\n            filters=(200, 200, 200, 200, 200), dropout_embed=0.2, dropout_hidden=0.2, weight_norm=True,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='accuracy', batch_size=100,\n            epochs=100,\n            logger=None, verbose=True, **kwargs):\n        assert kwargs.get('run_eagerly', True), 'NgramConvTaggingModel can only run eagerly'\n        kwargs['run_eagerly'] = True\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/taggers/ngram_conv/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 22:18", "hanlp/components/taggers/transformers/transformer_tagger_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 13:55\nimport math\n\nimport tensorflow as tf\n\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.components.taggers.tagger_tf import TaggerComponent\nfrom hanlp.components.taggers.transformers.transformer_transform_tf import TransformerTransform\nfrom hanlp.layers.transformers.loader_tf import build_transformer\nfrom hanlp.layers.transformers.utils_tf import build_adamw_optimizer\nfrom hanlp.losses.sparse_categorical_crossentropy import SparseCategoricalCrossentropyOverBatchFirstDim\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass TransformerTaggingModel(tf.keras.Model):\n    def __init__(self, transformer: tf.keras.Model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.transformer = transformer\n\n    def call(self, inputs, training=None, mask=None):\n        return super().call(inputs, training, mask)\n\n\nclass TransformerTaggerTF(TaggerComponent):\n    def __init__(self, transform: TransformerTransform = None) -> None:\n        if transform is None:\n            transform = TransformerTransform()\n        super().__init__(transform)\n        self.transform: TransformerTransform = transform\n\n    def build_model(self, transformer, max_seq_length, **kwargs) -> tf.keras.Model:\n        model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)\n        self.transform.tokenizer = tokenizer\n        return model\n\n    def fit(self, trn_data, dev_data, save_dir,\n            transformer,\n            optimizer='adamw',\n            learning_rate=5e-5,\n            weight_decay_rate=0,\n            epsilon=1e-8,\n            clipnorm=1.0,\n            warmup_steps_ratio=0,\n            use_amp=False,\n            max_seq_length=128,\n            batch_size=32,\n            epochs=3,\n            metrics='accuracy',\n            run_eagerly=False,\n            logger=None,\n            verbose=True,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    # noinspection PyMethodOverriding\n    def build_optimizer(self, optimizer, learning_rate, epsilon, weight_decay_rate, clipnorm, use_amp, train_steps,\n                        warmup_steps, **kwargs):\n        if optimizer == 'adamw':\n            opt = build_adamw_optimizer(self.config, learning_rate, epsilon, clipnorm, train_steps, use_amp,\n                                        warmup_steps, weight_decay_rate)\n        else:\n            opt = super().build_optimizer(optimizer)\n        return opt\n\n    def build_vocab(self, trn_data, logger):\n        train_examples = super().build_vocab(trn_data, logger)\n        warmup_steps_per_epoch = math.ceil(train_examples * self.config.warmup_steps_ratio / self.config.batch_size)\n        self.config.warmup_steps = warmup_steps_per_epoch * self.config.epochs\n        return train_examples\n\n    def train_loop(self, trn_data, dev_data, epochs, num_examples, train_steps_per_epoch, dev_steps, model, optimizer,\n                   loss, metrics, callbacks, logger, **kwargs):\n        history = self.model.fit(trn_data, epochs=epochs, steps_per_epoch=train_steps_per_epoch,\n                                 validation_data=dev_data,\n                                 callbacks=callbacks,\n                                 validation_steps=dev_steps,\n                                 # mask out padding labels\n                                 # class_weight=dict(\n                                 #     (i, 0 if i == 0 else 1) for i in range(len(self.transform.tag_vocab)))\n                                 )  # type:tf.keras.callbacks.History\n        return history\n\n    def build_loss(self, loss, **kwargs):\n        if not loss:\n            return SparseCategoricalCrossentropyOverBatchFirstDim()\n        return super().build_loss(loss, **kwargs)\n\n    def load_transform(self, save_dir) -> Transform:\n        super().load_transform(save_dir)\n        self.transform.tokenizer = build_transformer(self.config.transformer, self.config.max_seq_length,\n                                                     len(self.transform.tag_vocab), tagging=True, tokenizer_only=True)\n        return self.transform\n", "hanlp/components/taggers/transformers/transformer_transform_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 15:14\nfrom typing import Union, Tuple, List, Iterable\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.layers.transformers.utils_tf import convert_examples_to_features\nfrom hanlp.transform.tsv_tf import TsvTaggingFormat\n\n\nclass TransformerTransform(TsvTaggingFormat, Transform):\n    def __init__(self,\n                 tokenizer=None,\n                 config: SerializableDict = None,\n                 map_x=False, map_y=False, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, **kwargs)\n        self._tokenizer = tokenizer\n        self.tag_vocab: VocabTF = None\n        self.special_token_ids = None\n        self.pad = '[PAD]'\n        self.unk = '[UNK]'\n\n    @property\n    def max_seq_length(self):\n        # -2 for special tokens [CLS] and [SEP]\n        return self.config.get('max_seq_length', 128) - 2\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, tokenizer):\n        self._tokenizer = tokenizer\n        vocab = tokenizer._vocab if hasattr(tokenizer, '_vocab') else tokenizer.vocab\n        if self.pad not in vocab:\n            # English albert use <pad> instead of [PAD]\n            self.pad = '<pad>'\n        if self.unk not in vocab:\n            self.unk = '<unk>'\n        self.special_token_ids = tf.constant([vocab[token] for token in [self.pad, '[CLS]', '[SEP]']],\n                                             dtype=tf.int32)\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        self.tag_vocab = VocabTF(unk_token=None)\n        num_samples = 0\n        for words, tags in self.file_to_inputs(trn_path, gold=True):\n            num_samples += 1\n            self.tag_vocab.update(tags)\n        return num_samples\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        max_seq_length = self.config.get('max_seq_length', 128)\n        types = (tf.int32, tf.int32, tf.int32), tf.int32\n        # (input_ids, input_mask, segment_ids), label_ids\n        shapes = ([max_seq_length], [max_seq_length], [max_seq_length]), [None]\n        values = (0, 0, 0), self.tag_vocab.pad_idx\n        return types, shapes, values\n\n    def lock_vocabs(self):\n        super().lock_vocabs()\n\n    def inputs_to_samples(self, inputs, gold=False):\n        max_seq_length = self.config.get('max_seq_length', 128)\n        tokenizer = self._tokenizer\n        xlnet = False\n        roberta = False\n        pad_token = self.pad\n        cls_token = '[CLS]'\n        sep_token = '[SEP]'\n        unk_token = self.unk\n\n        pad_label_idx = self.tag_vocab.pad_idx\n        pad_token = tokenizer.convert_tokens_to_ids([pad_token])[0]\n        for sample in inputs:\n            if gold:\n                words, tags = sample\n            else:\n                words, tags = sample, [self.tag_vocab.idx_to_token[1]] * len(sample)\n\n            input_ids, input_mask, segment_ids, label_ids = convert_examples_to_features(words,\n                                                                                         max_seq_length, tokenizer,\n                                                                                         tags,\n                                                                                         self.tag_vocab.token_to_idx,\n                                                                                         cls_token_at_end=xlnet,\n                                                                                         # xlnet has a cls token at the end\n                                                                                         cls_token=cls_token,\n                                                                                         cls_token_segment_id=2 if xlnet else 0,\n                                                                                         sep_token=sep_token,\n                                                                                         sep_token_extra=roberta,\n                                                                                         # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                                                                         pad_on_left=xlnet,\n                                                                                         # pad on the left for xlnet\n                                                                                         pad_token_id=pad_token,\n                                                                                         pad_token_segment_id=4 if xlnet else 0,\n                                                                                         pad_token_label_id=pad_label_idx,\n                                                                                         unk_token=unk_token)\n\n            if None in input_ids:\n                print(input_ids)\n            if None in input_mask:\n                print(input_mask)\n            if None in segment_ids:\n                print(input_mask)\n            yield (input_ids, input_mask, segment_ids), label_ids\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        raise NotImplementedError('transformers has its own tagger, not need to convert idx for x')\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        raise NotImplementedError('transformers has its own tagger, not need to convert idx for y')\n\n    def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:\n        return isinstance(input[0], str)\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, X=None, inputs=None, batch=None,\n                     **kwargs) -> Iterable:\n        assert batch is not None, 'Need the batch to know actual length of Y'\n        label_mask = batch[1]\n        if self.tag_vocab.pad_token:\n            Y[:, :, self.tag_vocab.pad_idx] = float('-inf')\n        Y = tf.argmax(Y, axis=-1)\n        Y = Y[label_mask > 0]\n        tags = [self.tag_vocab.idx_to_token[tid] for tid in Y]\n        offset = 0\n        for words in inputs:\n            yield tags[offset:offset + len(words)]\n            offset += len(words)\n", "hanlp/components/taggers/transformers/metrics_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-30 16:33\nimport tensorflow as tf\n\n\nclass Accuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n\n    def __init__(self, name='sparse_categorical_accuracy', dtype=None, mask_value=0):\n        super().__init__(name, dtype)\n        self.mask_value = mask_value\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        sample_weight = tf.not_equal(y_true, self.mask_value)\n        return super().update_state(y_true, y_pred, sample_weight)\n", "hanlp/components/taggers/transformers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 13:57", "hanlp/components/taggers/transformers/transformer_tagger.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-15 20:55\nimport logging\nfrom typing import Union, List\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import PadSequenceDataLoader, SamplerBuilder, TransformableDataset\nfrom hanlp.common.structure import History\nfrom hanlp.common.transform import FieldLength, TransformList\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.classifiers.transformer_classifier import TransformerComponent\nfrom hanlp.components.taggers.tagger import Tagger\nfrom hanlp.datasets.ner.loaders.tsv import TSVTaggingDataset\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.layers.embeddings.embedding import EmbeddingDim, Embedding\nfrom hanlp.layers.transformers.encoder import TransformerEncoder\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp.utils.torch_util import clip_grad_norm, lengths_to_mask, filter_state_dict_safely\nfrom hanlp_common.util import merge_locals_kwargs\n\n\n# noinspection PyAbstractClass\nclass TransformerTaggingModel(nn.Module):\n    def __init__(self,\n                 encoder: TransformerEncoder,\n                 num_labels,\n                 crf=False,\n                 secondary_encoder=None,\n                 extra_embeddings: EmbeddingDim = None) -> None:\n        \"\"\"\n        A shallow tagging model use transformer as decoder.\n        Args:\n            encoder: A pretrained transformer.\n            num_labels: Size of tagset.\n            crf: True to enable CRF.\n            extra_embeddings: Extra embeddings which will be concatenated to the encoder outputs.\n        \"\"\"\n        super().__init__()\n        self.encoder = encoder\n        self.secondary_encoder = secondary_encoder\n        self.extra_embeddings = extra_embeddings\n        # noinspection PyUnresolvedReferences\n        feature_size = encoder.transformer.config.hidden_size\n        if extra_embeddings:\n            feature_size += extra_embeddings.get_output_dim()\n        self.classifier = nn.Linear(feature_size, num_labels)\n        self.crf = CRF(num_labels) if crf else None\n\n    def forward(self, lens: torch.LongTensor, input_ids, token_span, token_type_ids=None, batch=None):\n        mask = lengths_to_mask(lens)\n        x = self.encoder(input_ids, token_span=token_span, token_type_ids=token_type_ids)\n        if self.secondary_encoder:\n            x = self.secondary_encoder(x, mask=mask)\n        if self.extra_embeddings:\n            # noinspection PyCallingNonCallable\n            embed = self.extra_embeddings(batch, mask=mask)\n            x = torch.cat([x, embed], dim=-1)\n        x = self.classifier(x)\n        return x, mask\n\n\nclass TransformerTagger(TransformerComponent, Tagger):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for\n        any tagging tasks including PoS tagging and many others.\n\n        Args:\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._tokenizer_transform = None\n        self.model: TransformerTaggingModel = None\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       history: History,\n                       gradient_accumulation=1,\n                       grad_norm=None,\n                       transformer_grad_norm=None,\n                       teacher: Tagger = None,\n                       kd_criterion=None,\n                       temperature_scheduler=None,\n                       ratio_width=None,\n                       eval_trn=True,\n                       **kwargs):\n        optimizer, scheduler = optimizer\n        if teacher:\n            scheduler, lambda_scheduler = scheduler\n        else:\n            lambda_scheduler = None\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            out, mask = self.feed_batch(batch)\n            y = batch['tag_id']\n            loss = self.compute_loss(criterion, out, y, mask)\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            if teacher:\n                with torch.no_grad():\n                    out_T, _ = teacher.feed_batch(batch)\n                # noinspection PyNoneFunctionAssignment\n                kd_loss = self.compute_distill_loss(kd_criterion, out, out_T, mask, temperature_scheduler)\n                _lambda = float(lambda_scheduler)\n                loss = _lambda * loss + (1 - _lambda) * kd_loss\n            loss.backward()\n            total_loss += loss.item()\n            if eval_trn:\n                prediction = self.decode_output(out, mask, batch)\n                self.update_metrics(metric, out, y, mask, batch, prediction)\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler, grad_norm, transformer_grad_norm, lambda_scheduler)\n                report = f'loss: {total_loss / (idx + 1):.4f} {metric if eval_trn else \"\"}'\n                timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n            del loss\n            del out\n            del mask\n\n    def _step(self, optimizer, scheduler, grad_norm, transformer_grad_norm, lambda_scheduler):\n        clip_grad_norm(self.model, grad_norm, self.model.encoder.transformer, transformer_grad_norm)\n        optimizer.step()\n        scheduler.step()\n        if lambda_scheduler:\n            lambda_scheduler.step()\n        optimizer.zero_grad()\n\n    def compute_distill_loss(self, kd_criterion, out_S, out_T, mask, temperature_scheduler):\n        logits_S = out_S[mask]\n        logits_T = out_T[mask]\n        temperature = temperature_scheduler(logits_S, logits_T)\n        return kd_criterion(logits_S, logits_T, temperature)\n\n    def build_model(self, training=True, extra_embeddings: Embedding = None, finetune=False, logger=None,\n                    **kwargs) -> torch.nn.Module:\n        model = TransformerTaggingModel(\n            self.build_transformer(training=training),\n            len(self.vocabs.tag),\n            self.config.crf,\n            self.config.get('secondary_encoder', None),\n            extra_embeddings=extra_embeddings.module(self.vocabs) if extra_embeddings else None,\n        )\n        if finetune:\n            model_state = model.state_dict()\n            load_state = self.model.state_dict()\n            safe_state = filter_state_dict_safely(model_state, load_state)\n            missing_params = model_state.keys() - safe_state.keys()\n            if missing_params:\n                logger.info(f'The following parameters were missing from the checkpoint: '\n                            f'{\", \".join(sorted(missing_params))}.')\n            model.load_state_dict(safe_state, strict=False)\n            n = self.model.classifier.bias.size(0)\n            if model.classifier.bias.size(0) != n:\n                model.classifier.weight.data[:n, :] = self.model.classifier.weight.data[:n, :]\n                model.classifier.bias.data[:n] = self.model.classifier.bias.data[:n]\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size, shuffle, device, logger: logging.Logger = None,\n                         sampler_builder: SamplerBuilder = None, gradient_accumulation=1,\n                         extra_embeddings: Embedding = None, transform=None, max_seq_len=None, **kwargs) -> DataLoader:\n        if isinstance(data, TransformableDataset):\n            dataset = data\n        else:\n            args = dict((k, self.config.get(k, None)) for k in\n                        ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'])\n            dataset = self.build_dataset(data, **args)\n        if self.config.token_key is None:\n            self.config.token_key = next(iter(dataset[0]))\n            logger.info(\n                f'Guess [bold][blue]token_key={self.config.token_key}[/blue][/bold] according to the '\n                f'training dataset: [blue]{dataset}[/blue]')\n        if transform:\n            dataset.append_transform(transform)\n        if extra_embeddings:\n            dataset.append_transform(extra_embeddings.transform(self.vocabs))\n        dataset.append_transform(self.tokenizer_transform)\n        dataset.append_transform(self.last_transform())\n        if not isinstance(data, list):\n            dataset.purge_cache()\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        if isinstance(data, str) and max_seq_len:\n            token_key = self.config.token_key\n            dataset.prune(lambda x: len(x[token_key]) > max_seq_len, logger)\n        if sampler_builder is not None:\n            sampler = sampler_builder.build([len(x[f'{self.config.token_key}_input_ids']) for x in dataset], shuffle,\n                                            gradient_accumulation=gradient_accumulation if shuffle else 1)\n        else:\n            sampler = None\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler)\n\n    def build_dataset(self, data, transform=None, **kwargs):\n        return TSVTaggingDataset(data, transform=transform, **kwargs)\n\n    def last_transform(self):\n        transforms = TransformList(self.vocabs, FieldLength(self.config.token_key))\n        return transforms\n\n    @property\n    def tokenizer_transform(self) -> TransformerSequenceTokenizer:\n        if not self._tokenizer_transform:\n            self._tokenizer_transform = TransformerSequenceTokenizer(self.transformer_tokenizer,\n                                                                     self.config.token_key,\n                                                                     ret_token_span=True)\n        return self._tokenizer_transform\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        if 'tag' not in self.vocabs:\n            self.vocabs.tag = Vocab(pad_token=None, unk_token=None)\n        timer = CountdownTimer(len(trn))\n        max_seq_len = 0\n        token_key = self.config.token_key\n        for each in trn:\n            max_seq_len = max(max_seq_len, len(each[token_key]))\n            timer.log(f'Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: {max_seq_len})')\n        self.vocabs.tag.set_unk_as_safe_unk()\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            transformer,\n            average_subwords=False,\n            word_dropout: float = 0.2,\n            hidden_dropout=None,\n            layer_dropout=0,\n            scalar_mix=None,\n            mix_embedding: int = 0,\n            grad_norm=5.0,\n            transformer_grad_norm=None,\n            lr=5e-5,\n            transformer_lr=None,\n            transformer_layers=None,\n            gradient_accumulation=1,\n            adam_epsilon=1e-6,\n            weight_decay=0,\n            warmup_steps=0.1,\n            secondary_encoder=None,\n            extra_embeddings: Embedding = None,\n            crf=False,\n            reduction='sum',\n            batch_size=32,\n            sampler_builder: SamplerBuilder = None,\n            epochs=3,\n            patience=5,\n            token_key=None,\n            max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False,\n            transform=None,\n            logger=None,\n            devices: Union[float, int, List[int]] = None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def feed_batch(self, batch: dict):\n        features = [batch[k] for k in self.tokenizer_transform.output_key]\n        if len(features) == 2:\n            input_ids, token_span = features\n        else:\n            input_ids, token_span = features[0], None\n        lens = batch[f'{self.config.token_key}_length']\n        x, mask = self.model(lens, input_ids, token_span, batch.get(f'{self.config.token_key}_token_type_ids'),\n                             batch=batch)\n        return x, mask\n\n    # noinspection PyMethodOverriding\n    def distill(self,\n                teacher: str,\n                trn_data,\n                dev_data,\n                save_dir,\n                transformer: str,\n                batch_size=None,\n                temperature_scheduler='flsw',\n                epochs=None,\n                devices=None,\n                logger=None,\n                seed=None,\n                **kwargs):\n        return super().distill(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/amr/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-20 17:35\n", "hanlp/components/amr/amrbart/bart_amr_parser.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-05 17:56\nimport logging\nimport os.path\nfrom typing import Callable, Union, List\n\nimport datetime\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.components.amr.amrbart.data_interface.dataset import AMRParsingDataSet\nfrom hanlp.common.dataset import SortingSamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.components.amr.seq2seq.dataset.dataset import AMRDataset\nfrom hanlp.components.amr.seq2seq.dataset.penman import AMRGraph\nfrom hanlp.components.amr.seq2seq.evaluation import write_predictions, compute_smatch\nfrom hanlp.layers.transformers.pt_imports import AutoConfig_\nfrom hanlp.metrics.amr.smatch_eval import smatch_eval\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import reorder\nfrom hanlp.components.amr.amrbart.model_interface.modeling_bart import BartForConditionalGeneration\nfrom hanlp.components.amr.amrbart.model_interface.tokenization_bart import AMRBartTokenizer\n\n\nclass BART_AMR_Parser(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.tokenizer: AMRBartTokenizer = None\n        self.transformer_config = None\n        self.model: BartForConditionalGeneration = None\n\n    def build_dataloader(self, data, batch_size=32, shuffle=False, device=None, logger: logging.Logger = None,\n                         sampler_builder=None,\n                         **kwargs) -> DataLoader:\n        dataset = AMRDataset(data, generate_idx=True, cache=True)\n        if isinstance(data, str):\n            dataset.append_transform(lambda x: {**x, 'text': x['amr'].metadata['snt']})\n        dataset.append_transform(\n            lambda x: AMRParsingDataSet.tokenize(x, tokenizer=self.tokenizer, text='text')\n        )\n        if not sampler_builder:\n            sampler_builder = SortingSamplerBuilder(batch_max_tokens=500)\n        sampler = sampler_builder.build([len(x['input_ids']) for x in dataset], shuffle, 1)\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler,\n                                     pad={'input_ids': self.transformer_config.pad_token_id,\n                                          'labels': self.transformer_config.pad_token_id})\n\n    def build_optimizer(self, **kwargs):\n        pass\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def build_metric(self, **kwargs):\n        pass\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        pass\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        pass\n\n    def build_model(self, training=True, transformer=None, **kwargs) -> torch.nn.Module:\n        model = BartForConditionalGeneration.from_pretrained(\n            transformer,\n            config=self.transformer_config,\n        )\n        if not training:\n            model.eval()\n        model.resize_token_embeddings(len(self.tokenizer))\n        return model\n\n    def input_is_flat(self, data):\n        return isinstance(data, str)\n\n    def predict(\n            self,\n            data: Union[str, List[str]], num_beams=5, max_length=1024, beautiful_amr_graph=True, verbose=False,\n            **kwargs\n    ):\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader([{'text': x} for x in data], **self.config, device=self.device)\n        orders = []\n        results = []\n        # inputs, logits, labels, loss = torch.load('/local/scratch/hhe43/amrbart/batch.pt')\n        if verbose:\n            timer = CountdownTimer(len(dataloader))\n        for batch in dataloader:\n            pieces = self.predict_batch(batch, num_beams, max_length)\n            results.extend(pieces)\n            orders.extend(batch[IDX])\n            if verbose:\n                # noinspection PyUnboundLocalVariable\n                timer.log()\n        results = reorder(results, orders)\n        if flat:\n            results = results[0]\n        return results\n\n    def predict_batch(self, batch, num_beams, max_length):\n        tokenizer = self.tokenizer\n        input_ids = batch['input_ids']\n        preds = self.model.generate(\n            input_ids,\n            num_beams=num_beams,\n            num_return_sequences=num_beams,\n            use_cache=True,\n            decoder_start_token_id=tokenizer.amr_bos_token_id,\n            eos_token_id=tokenizer.amr_eos_token_id,\n            no_repeat_ngram_size=0,\n            max_length=max_length,\n            min_length=0,\n            length_penalty=1.0,\n        ).tolist()\n        # tokens = batch['tgt']\n        graphs = []\n        for i in range(0, len(preds), num_beams):\n            graphs_same_source = []\n            for j in range(i, i + num_beams):\n                ith_pred = preds[j]\n                ith_pred[0] = tokenizer.bos_token_id\n                ith_pred = [\n                    tokenizer.eos_token_id if itm == tokenizer.amr_eos_token_id else itm\n                    for itm in ith_pred if itm != tokenizer.pad_token_id\n                ]\n\n                graph, status, (lin, backr) = tokenizer.decode_amr(\n                    ith_pred, restore_name_ops=False\n                )\n                graph.status = status\n                graph.nodes = lin\n                graph.backreferences = backr\n                graph.tokens = ith_pred\n                graphs_same_source.append(graph)\n            graphs_same_source[:] = \\\n                tuple(zip(*sorted(enumerate(graphs_same_source), key=lambda x: (x[1].status.value, x[0]))))[1]\n            graphs.append(graphs_same_source)\n        # assert len(graphs) == len(tokens), f\"inconsistent lengths {len(graphs)} vs {len(tokens)}\"\n        # for idx, gps, snt in zip(batch[IDX], graphs, tokens):\n        #     for gp in gps:\n        #         gp.metadata = {\"id\": str(idx), \"annotator\": \"bart-amr\",\n        #                        \"snt\": snt.replace(\"<AMR>\", '').replace(\"</AMR>\", '').strip()}\n        pieces = [AMRGraph(g.triples, g.top, g.epidata, g.metadata) for g in [gs[0] for gs in graphs]]\n        return pieces\n\n    def load_config(self, save_dir: str, filename='config.json', **kwargs):\n        if os.path.isdir(save_dir):\n            super().load_config(save_dir, filename, **kwargs)\n            transformer = self.config.transformer\n        else:\n            self.config.transformer = transformer = save_dir\n        self.transformer_config = AutoConfig_.from_pretrained(transformer)\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self.tokenizer = AMRBartTokenizer.from_pretrained(\n            self.config.transformer,\n            use_fast=True,\n        )\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n\n    @torch.no_grad()\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, ratio_width=None,\n                            logger=None, input=None, use_fast=False, num_beams=5, max_length=1024,\n                            **kwargs):\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        graphs = []\n        orders = []\n        smatch = 0\n        for idx, batch in enumerate(data):\n            graphs_per_batch = self.predict_batch(batch, num_beams, max_length)\n            # Copy meta data from gold graph\n            for gp, gg in zip(graphs_per_batch, batch['amr']):\n                metadata = gg.metadata.copy()\n                metadata['annotator'] = f'{self.transformer_config.name_or_path}-amr'\n                metadata['date'] = str(datetime.datetime.now())\n                if 'save-date' in metadata:\n                    del metadata['save-date']\n                gp.metadata = metadata\n            graphs.extend(graphs_per_batch)\n            orders.extend(batch[IDX])\n            if idx == timer.total - 1:\n                graphs = reorder(graphs, orders)\n                write_predictions(output, None, graphs)\n                try:\n                    if use_fast:\n                        smatch = compute_smatch(output, input)\n                    else:\n                        smatch = smatch_eval(output, input, use_fast=False)\n                except:\n                    pass\n                timer.log(smatch.cstr() if isinstance(smatch, MetricDict) else f'{smatch:.2%}', ratio_percentage=False,\n                          logger=logger)\n            else:\n                timer.log(ratio_percentage=False, logger=logger)\n\n        return smatch\n\n    def evaluate(self, tst_data, save_dir=None, logger: logging.Logger = None, batch_size=None, output=True, **kwargs):\n        return super().evaluate(tst_data, save_dir, logger, batch_size, output, **kwargs)\n", "hanlp/components/amr/amrbart/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-05 17:53\n", "hanlp/components/amr/amrbart/bart_amr_generation.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-05 17:56\nimport logging\nimport os.path\nfrom typing import Callable, Union, List\n\nimport penman\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.components.amr.amrbart.data_interface.dataset import AMR2TextDataSet\nfrom hanlp.common.dataset import SortingSamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.components.amr.seq2seq.dataset.dataset import AMRDataset\nfrom hanlp.layers.transformers.pt_imports import AutoConfig_\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import reorder\nfrom hanlp.components.amr.amrbart.model_interface.modeling_bart import BartForConditionalGeneration\nfrom hanlp.components.amr.amrbart.model_interface.tokenization_bart import AMRBartTokenizer\nfrom hanlp.components.amr.amrbart.preprocess.read_and_process import dfs_linearize\n\n\nclass BART_AMR_Generation(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.tokenizer: AMRBartTokenizer = None\n        self.transformer_config = None\n        self.model: BartForConditionalGeneration = None\n\n    def build_dataloader(self, data, batch_size=32, shuffle=False, device=None, logger: logging.Logger = None,\n                         sampler_builder=None,\n                         **kwargs) -> DataLoader:\n        dataset = AMRDataset(data, generate_idx=True, cache=True)\n        dataset.append_transform(lambda x: {**x, 'lamr': ' '.join(dfs_linearize(x['amr']))})\n        dataset.append_transform(\n            lambda x: AMR2TextDataSet.tokenize(x, tokenizer=self.tokenizer, text='text', amr='lamr')\n        )\n        if not sampler_builder:\n            sampler_builder = SortingSamplerBuilder(batch_max_tokens=500)\n        sampler = sampler_builder.build([len(x['input_ids']) for x in dataset], shuffle, 1)\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler,\n                                     pad={'input_ids': self.transformer_config.pad_token_id,\n                                          'labels': self.transformer_config.pad_token_id})\n\n    def build_optimizer(self, **kwargs):\n        pass\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def build_metric(self, **kwargs):\n        pass\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        pass\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        pass\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        pass\n\n    def build_model(self, training=True, transformer=None, **kwargs) -> torch.nn.Module:\n        model = BartForConditionalGeneration.from_pretrained(\n            transformer,\n            config=self.transformer_config,\n        )\n        if not training:\n            model.eval()\n        model.resize_token_embeddings(len(self.tokenizer))\n        return model\n\n    def input_is_flat(self, data):\n        return isinstance(data, (str, penman.Graph))\n\n    def predict(\n            self,\n            data: Union[str, List[str]], num_beams=5, max_length=1024, beautiful_amr_graph=True, verbose=False,\n            **kwargs\n    ):\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader([{'amr': penman.loads(x)[0] if isinstance(x, str) else x} for x in data],\n                                           **self.config, device=self.device)\n        orders = []\n        results = []\n        if verbose:\n            timer = CountdownTimer(len(dataloader))\n        for batch in dataloader:\n            pieces = self.predict_batch(batch, num_beams, max_length)\n            results.extend(pieces)\n            orders.extend(batch[IDX])\n            if verbose:\n                # noinspection PyUnboundLocalVariable\n                timer.log()\n        results = reorder(results, orders)\n        if flat:\n            results = results[0]\n        return results\n\n    def predict_batch(self, batch, num_beams, max_length):\n        tokenizer = self.tokenizer\n        input_ids = batch['input_ids']\n        preds = self.model.generate(\n            input_ids,\n            num_beams=num_beams,\n            use_cache=True,\n            decoder_start_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            no_repeat_ngram_size=0,\n            max_length=max_length,\n            min_length=0,\n            length_penalty=1.0,\n        )\n        # tokens = batch['tgt']\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        decoded_preds = [x.strip() for x in decoded_preds]\n        return decoded_preds\n\n    def load_config(self, save_dir: str, filename='config.json', **kwargs):\n        if os.path.isdir(save_dir):\n            super().load_config(save_dir, filename, **kwargs)\n            transformer = self.config.transformer\n        else:\n            self.config.transformer = transformer = save_dir\n        self.transformer_config = AutoConfig_.from_pretrained(transformer)\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self.tokenizer = AMRBartTokenizer.from_pretrained(\n            self.config.transformer,\n            use_fast=True,\n        )\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n", "hanlp/components/amr/amrbart/model_interface/modeling_bart.py": "# coding=utf-8\n# Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch BART model.\"\"\"\nimport copy\nimport math\nimport random\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n    Seq2SeqQuestionAnsweringModelOutput,\n    Seq2SeqSequenceClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_end_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.bart.configuration_bart import BartConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n_CONFIG_FOR_DOC = \"BartConfig\"\n_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n\n# Base model docstring\n_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n\n# SequenceClassification docstring\n_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n_SEQ_CLASS_EXPECTED_LOSS = 0.0\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n\n# QuestionAsnwering docstring\n_CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n_QA_EXPECTED_LOSS = 0.59\n_QA_EXPECTED_OUTPUT = \"' nice puppet'\"\n\n\nBART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/bart-large\",\n    # see all BART models at https://huggingface.co/models?filter=bart\n]\n\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\ndef _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n    mask_cond = torch.arange(mask.size(-1))\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass BartLearnedPositionalEmbedding(nn.Embedding):\n    \"\"\"\n    This module learns positional embeddings up to a fixed maximum size.\n    \"\"\"\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models don't have this hack\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n\n    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_ids_shape[:2]\n        positions = torch.arange(\n            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n        )\n        return super().forward(positions + self.offset)\n\n\nclass BartAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n\n        if (self.head_dim * num_heads) != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.view(*proj_shape)\n        value_states = value_states.view(*proj_shape)\n\n        src_len = key_states.size(1)\n        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n\n        if layer_head_mask is not None:\n            if layer_head_mask.size() != (self.num_heads,):\n                raise ValueError(\n                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n                    f\" {layer_head_mask.size()}\"\n                )\n            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if output_attentions:\n            # this operation is a bit awkward, but it's required to\n            # make sure that attn_weights keeps its gradient.\n            # In order to do so, attn_weights have to be reshaped\n            # twice and have to be reused in the following\n            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped = None\n\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn_output = torch.bmm(attn_probs, value_states)\n\n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1, 2)\n\n        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n        # partitioned aross GPUs when using tensor-parallelism.\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights_reshaped, past_key_value\n\n\nclass BartEncoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = BartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        layer_head_mask: torch.FloatTensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n        ):\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass BartDecoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n\n        self.self_attn = BartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.encoder_attn = BartAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = True,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            encoder_hidden_states (`torch.FloatTensor`):\n                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                size `(decoder_attention_heads,)`.\n            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n\n        # Self Attention\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Cross-Attention Block\n        cross_attn_present_key_value = None\n        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n                output_attentions=output_attentions,\n            )\n            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n            hidden_states = residual + hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass BartClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        inner_dim: int,\n        num_classes: int,\n        pooler_dropout: float,\n    ):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense(hidden_states)\n        hidden_states = torch.tanh(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.out_proj(hidden_states)\n        return hidden_states\n\n\nclass BartPretrainedModel(PreTrainedModel):\n    config_class = BartConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _keys_to_ignore_on_load_unexpected = [r\"encoder.version\", r\"decoder.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (BartDecoder, BartEncoder)):\n            module.gradient_checkpointing = value\n\n    @property\n    def dummy_inputs(self):\n        pad_token = self.config.pad_token_id\n        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n        dummy_inputs = {\n            \"attention_mask\": input_ids.ne(pad_token),\n            \"input_ids\": input_ids,\n        }\n        return dummy_inputs\n\n\nclass PretrainedBartModel(BartPretrainedModel):\n    def __init_subclass__(self):\n        warnings.warn(\n            \"The class `PretrainedBartModel` has been depreciated, please use `BartPretrainedModel` instead.\",\n            FutureWarning,\n        )\n\n\nBART_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`BartConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nBART_GENERATION_EXAMPLE = r\"\"\"\n    Summarization example:\n\n    ```python\n    >>> from transformers import BartTokenizer, BartForConditionalGeneration\n\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n    >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n    >>> ARTICLE_TO_SUMMARIZE = (\n    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n    ... )\n    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n\n    >>> # Generate Summary\n    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n    ```\n\n    Mask filling example:\n\n    ```python\n    >>> from transformers import BartTokenizer, BartForConditionalGeneration\n\n    >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n    >>> logits = model(input_ids).logits\n\n    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n    >>> probs = logits[0, masked_index].softmax(dim=0)\n    >>> values, predictions = probs.topk(5)\n\n    >>> tokenizer.decode(predictions).split()\n    ['not', 'good', 'healthy', 'great', 'very']\n    ```\n\"\"\"\n\nBART_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n            1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n            can choose to directly pass an embedded representation. This is useful if you want more control over how to\n            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n            input (see `past_key_values`). This is useful if you want more control over how to convert\n            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n\n            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n            of `inputs_embeds`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\nclass BartEncoder(BartPretrainedModel):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n    [`BartEncoderLayer`].\n\n    Args:\n        config: BartConfig\n        embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n        embed_dim = config.d_model\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n\n        if embed_tokens is not None:\n            self.embed_tokens = embed_tokens\n        else:\n            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            embed_dim,\n        )\n        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n        embed_pos = self.embed_positions(input_shape)\n\n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = self.layernorm_embedding(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        # expand attention_mask\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        # check if head_mask has a correct number of layers specified if desired\n        if head_mask is not None:\n            if head_mask.size()[0] != (len(self.layers)):\n                raise ValueError(\n                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n                    f\" {head_mask.size()[0]}.\"\n                )\n\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n                layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing and self.training:\n\n                    def create_custom_forward(module):\n                        def custom_forward(*inputs):\n                            return module(*inputs, output_attentions)\n\n                        return custom_forward\n\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(encoder_layer),\n                        hidden_states,\n                        attention_mask,\n                        (head_mask[idx] if head_mask is not None else None),\n                    )\n                else:\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        attention_mask,\n                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )\n\n\nclass BartDecoder(BartPretrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n\n    Args:\n        config: BartConfig\n        embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n\n        if embed_tokens is not None:\n            self.embed_tokens = embed_tokens\n        else:\n            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n        )\n        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n            ).to(inputs_embeds.device)\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                of the decoder.\n            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n                selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n                control over how to convert `input_ids` indices into associated vectors than the model's internal\n                embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, input_shape, inputs_embeds, past_key_values_length\n        )\n\n        # expand encoder attention mask\n        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n\n        # embed positions\n        positions = self.embed_positions(input_shape, past_key_values_length)\n\n        hidden_states = inputs_embeds + positions\n        hidden_states = self.layernorm_embedding(hidden_states)\n\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        next_decoder_cache = () if use_cache else None\n\n        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n            if attn_mask is not None:\n                if attn_mask.size()[0] != (len(self.layers)):\n                    raise ValueError(\n                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                        f\" {head_mask.size()[0]}.\"\n                    )\n\n        for idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability < self.layerdrop):\n                continue\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                if use_cache:\n                    logger.warning(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, use_cache)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    head_mask[idx] if head_mask is not None else None,\n                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                    None,\n                )\n            else:\n\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                    cross_attn_layer_head_mask=(\n                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                    ),\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n    BART_START_DOCSTRING,\n)\nclass BartModel(BartPretrainedModel):\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n\n        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n\n        self.encoder = BartEncoder(config, self.shared)\n        self.decoder = BartDecoder(config, self.shared)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, value):\n        self.shared = value\n        self.encoder.embed_tokens = self.shared\n        self.decoder.embed_tokens = self.shared\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_EXPECTED_OUTPUT_SHAPE,\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqModelOutput]:\n\n        # different to other models, Bart automatically creates decoder_input_ids from\n        # input_ids if no decoder_input_ids are provided\n        if decoder_input_ids is None and decoder_inputs_embeds is None:\n            if input_ids is None:\n                raise ValueError(\n                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n                )\n\n            decoder_input_ids = shift_tokens_right(\n                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n            )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n)\nclass BartForConditionalGeneration(BartPretrainedModel):\n    base_model_prefix = \"model\"\n    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head.weight\"]\n\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n        self.model = BartModel(config)\n        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_encoder(self):\n        return self.model.get_encoder()\n\n    def get_decoder(self):\n        return self.model.get_decoder()\n\n    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n        self._resize_final_logits_bias(new_num_tokens)\n        return new_embeddings\n\n    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n        old_num_tokens = self.final_logits_bias.shape[-1]\n        if new_num_tokens <= old_num_tokens:\n            new_bias = self.final_logits_bias[:, :new_num_tokens]\n        else:\n            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n        self.register_buffer(\"final_logits_bias\", new_bias)\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past=None,\n        attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs\n    ):\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"head_mask\": head_mask,\n            \"decoder_head_mask\": decoder_head_mask,\n            \"cross_attn_head_mask\": cross_attn_head_mask,\n            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n    tasks.\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForSequenceClassification(BartPretrainedModel):\n    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = BartModel(config)\n        self.classification_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classifier_dropout,\n        )\n        self.model._init_weights(self.classification_head.dense)\n        self.model._init_weights(self.classification_head.out_proj)\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n        output_type=Seq2SeqSequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        if input_ids is None and inputs_embeds is not None:\n            raise NotImplementedError(\n                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n            )\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]  # last hidden state\n\n        eos_mask = input_ids.eq(self.config.eos_token_id)\n\n        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n            :, -1, :\n        ]\n        logits = self.classification_head(sentence_representation)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.config.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.config.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return Seq2SeqSequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForQuestionAnswering(BartPretrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        config.num_labels = 2\n        self.num_labels = config.num_labels\n\n        self.model = BartModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.model._init_weights(self.qa_outputs)\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_QA,\n        output_type=Seq2SeqQuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_loss=_QA_EXPECTED_LOSS,\n        expected_output=_QA_EXPECTED_OUTPUT,\n    )\n    def forward(\n        self,\n        input_ids: torch.Tensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if start_positions is not None and end_positions is not None:\n            use_cache = False\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (\n                start_logits,\n                end_logits,\n            ) + outputs[1:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return Seq2SeqQuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n\nclass BartDecoderWrapper(BartPretrainedModel):\n    \"\"\"\n    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n    used in combination with the [`EncoderDecoderModel`] framework.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.decoder = BartDecoder(config)\n\n    def forward(self, *args, **kwargs):\n        return self.decoder(*args, **kwargs)\n\n\nclass BartForCausalLM(BartPretrainedModel):\n    def __init__(self, config):\n        config = copy.deepcopy(config)\n        config.is_decoder = True\n        config.is_encoder_decoder = False\n        super().__init__(config)\n        self.model = BartDecoderWrapper(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.decoder.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model.decoder = decoder\n\n    def get_decoder(self):\n        return self.model.decoder\n\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                if the model is configured as a decoder.\n            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import BartTokenizer, BartForCausalLM\n\n        >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n        >>> model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n        >>> outputs = model(**inputs)\n\n        >>> logits = outputs.logits\n        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n        >>> list(logits.shape) == expected_shape\n        True\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            head_mask=head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        logits = self.lm_head(outputs[0])\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, use_cache=None, **kwargs):\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_ids.shape)\n\n        if past:\n            input_ids = input_ids[:, -1:]\n        # first step, decoder_cached_states are empty\n        return {\n            \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past", "hanlp/components/amr/amrbart/model_interface/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-03 20:33\n", "hanlp/components/amr/amrbart/model_interface/tokenization_bart.py": "# coding:utf-8\n# this is a simplified version of \"https://github.com/SapienzaNLP/spring/blob/main/spring_amr/tokenization_bart.py\"\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport penman\nimport regex as re\nfrom transformers import BartTokenizer\n\nfrom hanlp.components.amr.amrbart.common import postprocessing\nfrom hanlp.components.amr.amrbart.common.constant import raw_special_tokens, recategorizations\nfrom hanlp.components.amr.amrbart.common.penman_interface import encode\n\n\nclass AMRBartTokenizer(BartTokenizer):\n    INIT = '\u0120'\n\n    def __init__(self, vocab_file, merges_file, errors=\"replace\", bos_token=\"<s>\", eos_token=\"</s>\", sep_token=\"</s>\", cls_token=\"<s>\", unk_token=\"<unk>\", pad_token=\"<pad>\", mask_token=\"<mask>\", add_prefix_space=False, **kwargs):\n        super().__init__(vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\n        self.modified = 0\n        self.recategorizations = set(recategorizations)\n        self.patterns = re.compile(r\"\"\" ?<[a-z]+:?\\d*>| ?:[^\\s]+|'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n        self.remove_pars = False\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_path, *args, **kwargs):\n        inst = super().from_pretrained(pretrained_model_path, *args, **kwargs)\n        inst.init_amr_vocabulary()\n        return inst\n\n    def init_amr_vocabulary(self):\n        self.old_enc_size = old_enc_size = len(self.encoder)\n        tokens = [t for t in raw_special_tokens if t not in self.encoder]\n\n        for i, t in enumerate(tokens, start=old_enc_size):\n            self.encoder[t] = i\n\n        self.encoder = {k: i for i, (k,v) in enumerate(sorted(self.encoder.items(), key=lambda x: x[1]))}\n        self.decoder = {v: k for k, v in sorted(self.encoder.items(), key=lambda x: x[1])}\n        self.modified = len(tokens)\n\n        self.amr_bos_token = \"<AMR>\"\n        self.amr_bos_token_id = self.encoder[self.amr_bos_token]\n        self.amr_eos_token = \"</AMR>\"\n        self.amr_eos_token_id = self.encoder[self.amr_eos_token]\n        # print(f\"Added {self.modified} AMR tokens\")\n\n    def _tokenize(self, text):\n        \"\"\" Tokenize a string. Modified in order to handle sentences with recategorization pointers\"\"\"\n        bpe_tokens = []\n        for tok_span in text.lstrip().split(' '):\n            tok_span = tok_span.strip()\n            recats = tok_span.rsplit('_', 1)\n            if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:\n                bpe_tokens.extend([self.INIT + recats[0], '_' + recats[1]])\n            else:\n                for token in re.findall(self.pat, ' ' + tok_span):\n                    token = \"\".join(\n                        self.byte_encoder[b] for b in token.encode(\"utf-8\")\n                    )   # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n                    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n\n        return bpe_tokens\n\n    def _tok_bpe(self, token):\n        tokk = []\n        tok = token.strip()\n        recats = tok.rsplit('_', 1)\n        if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:\n            tokk.extend([self.INIT + recats[0], '_' + recats[1]])\n        else:\n            for tok in self.patterns.findall(' ' + token):\n                tok = \"\".join(\n                    self.byte_encoder[b] for b in tok.encode(\"utf-8\"))\n                toks = self.bpe(tok).split(' ')\n                tokk.extend(toks)\n        return tokk\n\n    def tokenize_amr(self, amr_tokens):\n        bpe_tokens = []\n        for i, tokk in enumerate(amr_tokens):\n            is_in_enc = self.INIT + tokk in self.encoder\n            is_rel = tokk.startswith(':') and len(tokk) > 1\n            is_spc = tokk.startswith('<') and tokk.endswith('>')\n            is_of = tokk.startswith(':') and tokk.endswith('-of')\n            is_frame = re.match(r'.+-\\d\\d', tokk) is not None\n\n            if tokk.startswith('\"') and tokk.endswith('\"'):                 # dealing with examples like \"The_United_Kingdom_of_xxx\"\n                tokk = tokk[1:-1].replace('_', ' ')\n                bpe_toks = [self.INIT + \"<lit>\"]\n                bpe_toks += self._tok_bpe(tokk)\n                bpe_toks.append(self.INIT + \"</lit>\")\n\n            elif (is_rel or is_spc or is_frame or is_of):\n                if is_in_enc:\n                    bpe_toks = [self.INIT + tokk]\n                elif is_frame:\n                    bpe_toks = self._tok_bpe(tokk[:-3]) + [tokk[-3:]]\n                elif is_of:\n                    rel = tokk[:-3]\n                    if self.INIT + rel in self.encoder:\n                        bpe_toks = [self.INIT + rel, '-of']\n                    else:\n                        bpe_toks = [self.INIT + ':'] + self._tok_bpe(rel[1:]) + ['-of']\n                elif is_rel:\n                    bpe_toks = [self.INIT + ':'] + self._tok_bpe(tokk[1:])\n                else:\n                    print(\"tok:\", tokk)\n                    print(f\"is_rel:{is_rel}, is_spc:{is_spc}, is_frame:{is_frame}, is_of:{is_of}\")\n                    exit()\n                    raise\n            else:\n                if is_in_enc:\n                    bpe_toks = [self.INIT + tokk]\n                else:\n                    bpe_toks = self._tok_bpe(tokk)\n\n            bpe_tokens.append(bpe_toks)\n        bpe_tokens = [b for bb in bpe_tokens for b in bb]\n        bpe_token_ids = [self.encoder.get(b, self.unk_token_id) for b in bpe_tokens]\n        return bpe_token_ids\n\n    def decode_amr(self, tokens, restore_name_ops=None):\n        try:\n            nodes, backreferences = postprocessing.decode_into_node_and_backreferences(tokens, self)\n        except Exception as e:\n            # print('Decoding failure:', file=sys.stderr)\n            # print(e, file=sys.stderr)\n            return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)\n        try:\n            graph_ = graph = self._fix_and_make_graph(nodes)\n            # if collapse_name_ops:\n            #     graph_ = graph = postprocessing._split_name_ops(graph)\n        except Exception as e:\n            # print('Building failure:', file=sys.stderr)\n            # print(nodes, file=sys.stderr)\n            # print(backreferences, file=sys.stderr)\n            # print(e, file=sys.stderr)\n            return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)\n        try:\n            graph, status = postprocessing.connect_graph_if_not_connected(graph)\n            # if status == postprocessing.ParsedStatus.BACKOFF:\n            #     print('Reconnection 1 failure:')\n            #     print(nodes, file=sys.stderr)\n            #     print(backreferences, file=sys.stderr)\n            #     print(graph_, file=sys.stderr)\n            return graph, status, (nodes, backreferences)\n        except Exception as e:\n            # print('Reconnction 2 failure:', file=sys.stderr)\n            # print(e, file=sys.stderr)\n            # print(nodes, file=sys.stderr)\n            # print(backreferences, file=sys.stderr)\n            # print(graph_, file=sys.stderr)\n            return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (nodes, backreferences)\n\n    def _fix_and_make_graph(self, nodes):\n\n        nodes_ = []\n        for n in nodes:\n            if isinstance(n, str):\n                if n.startswith('<') and n.endswith('>') and (not n.startswith('<pointer:')):\n                    pass\n                else:\n                    nodes_.append(n)\n            else:\n                nodes_.append(n)\n        nodes = nodes_\n\n        if True:\n            i = 0\n            nodes_ = []\n            while i < len(nodes):\n                nxt = nodes[i]\n                pst = None\n                if isinstance(nxt, str) and nxt.startswith('<pointer:'):\n                    e = nxt.find('>')\n                    if e != len(nxt) -1:\n                        pst = nxt[e+1:]\n                        nxt = nxt[:e+1]\n                    nodes_.append(nxt)\n                    if pst is not None:\n                        nodes_.append(pst)\n                else:\n                    nodes_.append(nxt)\n                i += 1\n            nodes = nodes_\n\n            i = 1\n            nodes_ = [nodes[0]]\n            while i < len(nodes):\n                nxt = nodes[i]\n                if isinstance(nxt, str) and nxt.startswith('<pointer:'):\n                    nxt = 'z' + nxt[9:-1]\n                    fol = nodes[i+1]\n                    # is not expansion\n                    if isinstance(fol, str) and (fol.startswith(':') or (fol == ')')):\n                        nodes_.append(nxt)\n                    else:\n                        if self.remove_pars:\n                            nodes_.append('(')\n                        else:\n                            if nodes_[-1] != '(':\n                                nodes_.append('(')\n                                #pass\n                        nodes_.append(nxt)\n                        nodes_.append('/')\n                else:\n                    nodes_.append(nxt)\n                i += 1\n            nodes = nodes_\n\n        i = 0\n        nodes_ = []\n        while i < (len(nodes) - 1):\n            if nodes[i] == ':':\n                nodes_.append(nodes[i] + nodes[i+1])\n                i += 2\n                last = False\n            else:\n                nodes_.append(nodes[i])\n                i += 1\n                last = True\n        if last:\n            nodes_.append(nodes[-1])\n        nodes = nodes_\n\n        i = 0\n        nodes_ = []\n        while i < (len(nodes)):\n            if i < 2:\n                nodes_.append(nodes[i])\n                i += 1\n            elif nodes_[-2] == '/' and nodes[i] == '/':\n                i += 2\n            else:\n                nodes_.append(nodes[i])\n                i += 1\n        nodes = nodes_\n\n        i = 0\n        newvars = 0\n        variables = set()\n        remap = {}\n        nodes_ = []\n        while i < (len(nodes)):\n\n            next = nodes[i]\n\n            if next == '/':\n                last = nodes_[-1]\n                if last in variables:\n                    last_remap = f\"z{newvars+1000}\"\n                    newvars += 1\n                    nodes_[-1] = last_remap\n                    remap[last] = last_remap\n                variables.add(last)\n                nodes_.append(next)\n\n            elif self._classify(next) == 'VAR' and next in remap and (i < len(nodes) - 1) and nodes[i+1] != '/':\n                next = remap[next]\n                nodes_.append(next)\n\n            else:\n                nodes_.append(next)\n\n            i += 1\n\n        nodes = nodes_\n        pieces_ = []\n        open_cnt = 0\n        closed_cnt = 0\n        if nodes[0] != '(':\n            pieces_.append('(')\n            open_cnt += 1\n        for p in nodes:\n            if p == '(':\n                open_cnt += 1\n            elif p == ')':\n                closed_cnt += 1\n            pieces_.append(p)\n            if open_cnt == closed_cnt:\n                break\n        nodes = pieces_ + [')'] * (open_cnt - closed_cnt)\n\n        pieces = []\n        for piece in nodes:\n            if not pieces:\n                pieces.append('(')\n            else:\n                piece = str(piece)\n                if piece.startswith('\"') or piece.startswith('\"') or '\"' in piece.strip('\"'):\n                    piece = '\"' + piece.replace('\"', '') + '\"'\n\n                prev = self._classify(pieces[-1])\n                next = self._classify(piece)\n\n                if next == 'CONST':\n                    quote = False\n                    for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\\\', '_', '='):\n                        if char in piece:\n                            quote = True\n                            break\n                    if quote:\n                        piece = '\"' + piece.strip('\"') + '\"'\n\n                if  prev == '(':\n                    if next in ('VAR', 'I'):\n                        pieces.append(piece)\n                elif prev == ')':\n                    if next in (')', 'EDGE', 'MODE'):\n                        pieces.append(piece)\n                elif prev == 'VAR':\n                    if next in ('/', 'EDGE', 'MODE', ')'):\n                        pieces.append(piece)\n                elif prev == '/':\n                    if next in ('INST', 'I'):\n                        pieces.append(piece)\n                elif prev == 'INST':\n                    if next in (')', 'EDGE', 'MODE'):\n                        pieces.append(piece)\n                elif prev == 'I':\n                    if next in ('/', ')', 'EDGE', 'MODE'):\n                        pieces.append(piece)\n                elif prev == 'EDGE':\n                    if next in ('(', 'VAR', 'CONST', 'I'):\n                        pieces.append(piece)\n                    elif next == ')':\n                        pieces[-1] = piece\n                    elif next in ('EDGE', 'MODE'):\n                        pieces[-1] = piece\n                elif prev == 'MODE':\n                    if next == 'INST':\n                        pieces.append(piece)\n                elif prev == 'CONST':\n                    if next in (')', 'EDGE', 'MODE'):\n                        pieces.append(piece)\n\n        pieces_ = []\n        open_cnt = 0\n        closed_cnt = 0\n        if pieces[0] != '(':\n            pieces_.append('(')\n            open_cnt += 1\n        for p in pieces:\n            if p == '(':\n                open_cnt += 1\n            elif p == ')':\n                closed_cnt += 1\n            pieces_.append(p)\n            if open_cnt == closed_cnt:\n                break\n        pieces = pieces_ + [')'] * (open_cnt - closed_cnt)\n\n        linearized = re.sub(r'\\s+', ' ', ' '.join(pieces)).strip()\n\n        \"\"\"\n        line = linearized\n        # make sure parentheses match\n        # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py\n        open_count = 0\n        close_count = 0\n        for i, c in enumerate(line):\n            if c == '(':\n                open_count += 1\n            elif c == ')':\n                close_count += 1\n            if open_count == close_count and open_count > 0:\n                line = line[:i].strip()\n                break\n        old_line = line\n        while True:\n            open_count = len(re.findall(r'\\(', line))\n            close_count = len(re.findall(r'\\)', line))\n            if open_count > close_count:\n                line += ')' * (open_count - close_count)\n            elif close_count > open_count:\n                for i in range(close_count - open_count):\n                    line = line.rstrip(')')\n                    line = line.rstrip(' ')\n            if old_line == line:\n                break\n            old_line = line\n        \"\"\"\n\n        graph = penman.decode(linearized + ' ')\n        triples = []\n        newvars = 2000\n        for triple in graph.triples:\n            x, rel, y = triple\n            if x is None:\n                pass\n            elif rel == ':instance' and y is None:\n                triples.append(penman.Triple(x, rel, 'thing'))\n            elif y is None:\n                var = f'z{newvars}'\n                newvars += 1\n                triples.append(penman.Triple(x, rel, var))\n                triples.append(penman.Triple(var, ':instance', 'thing'))\n            else:\n                triples.append(triple)\n        graph = penman.Graph(triples)\n        linearized = encode(graph)\n\n        def fix_text(linearized=linearized):\n            n = 0\n            def _repl1(match):\n                nonlocal n\n                out = match.group(1) + match.group(2) + str(3000 + n) + ' / ' + match.group(2) + match.group(3)\n                n += 1\n                return out\n            linearized = re.sub(r'(\\(\\s?)([a-z])([^\\/:\\)]+[:\\)])', _repl1, linearized,\n                                flags=re.IGNORECASE | re.MULTILINE)\n\n            def _repl2(match):\n                return match.group(1)\n            linearized = re.sub(r'(\\(\\s*[a-z][\\d+]\\s*\\/\\s*[^\\s\\)\\(:\\/]+\\s*)((?:/\\s*[^\\s\\)\\(:\\/]+\\s*)+)', _repl2,\n                                linearized,\n                                flags=re.IGNORECASE | re.MULTILINE)\n\n            # adds a ':' to args w/o it\n            linearized = re.sub(r'([^:])(ARG)', r'\\1 :\\2', linearized)\n\n            # removes edges with no node\n            # linearized = re.sub(r':[^\\s\\)\\(:\\/]+?\\s*\\)', ')', linearized, flags=re.MULTILINE)\n\n            return linearized\n\n        linearized = fix_text(linearized)\n        g = penman.decode(linearized)\n        return g\n\n    def _classify(self, node):\n        if not isinstance(node, str):\n            return \"CONST\"\n        elif node == 'i':\n            return \"I\"\n        elif re.match(r'^[a-z]\\d*$', node) is not None:\n            return \"VAR\"\n        elif node[0].isdigit():\n            return \"CONST\"\n        elif node.startswith('\"') and node.endswith('\"'):\n            return \"CONST\"\n        elif node in ('+', '-'):\n            return \"CONST\"\n        elif node == ':mode':\n            return 'MODE'\n        elif node.startswith(':'):\n            return \"EDGE\"\n        elif node in ['/', '(', ')']:\n            return node\n        elif node[0].isalpha():\n            for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\\\'):\n                if char in node:\n                    return \"CONST\"\n            return \"INST\"\n        else:\n            return 'CONST'", "hanlp/components/amr/amrbart/preprocess/penman_interface.py": "# coding:utf-8\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nfrom penman import load as load_, Graph, Triple\nfrom penman import loads as loads_\nfrom penman import encode as encode_\nfrom penman.model import Model\nfrom penman.models.noop import NoOpModel\nfrom penman.models import amr\n\nop_model = Model()\nnoop_model = NoOpModel()\namr_model = amr.model\nDEFAULT = op_model\n\n\ndef _get_model(dereify):\n    if dereify is None:\n        return DEFAULT\n\n    elif dereify:\n        return op_model\n\n    else:\n        return noop_model\n\n\ndef _remove_wiki(graph):\n    metadata = graph.metadata\n    triples = []\n    for t in graph.triples:\n        v1, rel, v2 = t\n        if rel == \":wiki\":\n            t = Triple(v1, rel, \"+\")\n        triples.append(t)\n    graph = Graph(triples)\n    graph.metadata = metadata\n    return graph\n\n\ndef load(source, dereify=None, remove_wiki=False):\n    model = _get_model(dereify)\n    out = load_(source=source, model=model)\n    if remove_wiki:\n        for i in range(len(out)):\n            out[i] = _remove_wiki(out[i])\n    return out\n\n\ndef loads(string, dereify=None, remove_wiki=False):\n    model = _get_model(dereify)\n    out = loads_(string=string, model=model)\n    if remove_wiki:\n        for i in range(len(out)):\n            out[i] = _remove_wiki(out[i])\n    return out\n\n\ndef encode(g, top=None, indent=-1, compact=False):\n    model = amr_model\n    return encode_(g=g, top=top, indent=indent, compact=compact, model=model)\n", "hanlp/components/amr/amrbart/preprocess/read_and_process.py": "# coding:utf-8\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport re\nimport copy\nimport json\nimport yaml\nimport penman\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom hanlp.components.amr.amrbart.preprocess.amr_io import read_raw_amr_data\n\n\ndef _tokenize_encoded_graph(encoded):\n    linearized = re.sub(r\"(\\\".+?\\\")\", r\" \\1 \", encoded)\n    pieces = []\n    for piece in linearized.split():\n        if piece.startswith('\"') and piece.endswith('\"'):\n            pieces.append(piece)\n        else:\n            piece = piece.replace(\"(\", \" ( \")\n            piece = piece.replace(\")\", \" ) \")\n            piece = piece.replace(\":\", \" :\")\n            piece = piece.replace(\"/\", \" / \")\n            piece = piece.strip()\n            pieces.append(piece)\n    linearized = re.sub(r\"\\s+\", \" \", \" \".join(pieces)).strip()\n    return linearized.split(\" \")\n\n\ndef dfs_linearize(graph, remove_pars=False, use_pointer_tokens=True):\n    graph_ = copy.deepcopy(graph)\n    graph_.metadata = {}\n    linearized = penman.encode(graph_)\n    linearized_nodes = _tokenize_encoded_graph(linearized)\n\n    if use_pointer_tokens:\n        remap = {}\n        for i in range(1, len(linearized_nodes)):\n            nxt = linearized_nodes[i]\n            lst = linearized_nodes[i - 1]\n            if nxt == \"/\":\n                remap[lst] = f\"<pointer:{len(remap)}>\"\n        i = 1\n        linearized_nodes_ = [linearized_nodes[0]]\n        while i < (len(linearized_nodes)):\n            nxt = linearized_nodes[i]\n            lst = linearized_nodes_[-1]\n            if nxt in remap:\n                if lst == \"(\" and linearized_nodes[i + 1] == \"/\":\n                    nxt = remap[nxt]\n                    i += 1\n                elif lst.startswith(\":\"):\n                    nxt = remap[nxt]\n            linearized_nodes_.append(nxt)\n            i += 1\n        linearized_nodes = linearized_nodes_\n        if remove_pars:\n            linearized_nodes = [n for n in linearized_nodes if n != \"(\"]\n    return linearized_nodes\n\n\ndef main():\n    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n    parser = ArgumentParser(\n        description=\"AMR processing script\",\n        formatter_class=ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('--config', type=Path, default='default.yaml',\n                        help='Use the following config for hparams.')\n    parser.add_argument('--input_file', type=str,\n                        help='The input AMR file.')\n    parser.add_argument('--output_prefix', type=str,\n                        help='The output_prefix.')\n\n    args, unknown = parser.parse_known_args()\n\n    with args.config.open() as y:\n        config = yaml.load(y, Loader=yaml.FullLoader)\n\n    remove_pars = False\n    use_pointer_tokens = True\n    graphs = read_raw_amr_data(\n        [args.input_file],\n        use_recategorization=config[\"use_recategorization\"],\n        remove_wiki=config[\"remove_wiki\"],\n        dereify=config[\"dereify\"],\n    )\n\n    line_amr, sentences = [], []\n\n    for g in tqdm(graphs):\n        lin_tokens = dfs_linearize(g)\n        sentences.append(g.metadata[\"snt\"])\n        # line_amr.append(\" \".join(lin_tokens[1:-1]))\n        line_amr.append(\" \".join(lin_tokens))\n\n    print(f\"all {len(line_amr)} AMRs processed\")\n\n    with open(args.output_prefix + \".amr\", \"w\", encoding=\"utf-8\") as fout:\n        fout.write(\"\\n\".join(line_amr) + \"\\n\")\n\n    with open(args.output_prefix + \".txt\", \"w\", encoding=\"utf-8\") as fout:\n        fout.write(\"\\n\".join(sentences) + \"\\n\")\n\n    res_out = [json.dumps({\"sent\": sent, \"amr\": lamr}) for lamr, sent in zip(line_amr, sentences)]\n\n    with open(args.output_prefix + \".jsonl\", \"w\", encoding=\"utf-8\") as fout:\n        fout.write(\"\\n\".join(res_out) + \"\\n\")\n\n\nif __name__ == '__main__':\n    main()\n", "hanlp/components/amr/amrbart/preprocess/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-03 20:33\n", "hanlp/components/amr/amrbart/preprocess/amr_io.py": "# coding:utf-8\n# the code is migrated from https://github.com/SapienzaNLP/spring \n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport glob\nfrom pathlib import Path\nfrom typing import List, Union, Iterable\nfrom hanlp.components.amr.amrbart.preprocess.penman_interface import load as pm_load\n\n\ndef read_raw_amr_data(\n        paths: List[Union[str, Path]], use_recategorization=False, dereify=True, remove_wiki=False,\n):\n    \"\"\" code for loading AMR from a set of files\n        - use_recategorization: use graph recategorization trick\n        - dereify: Dereify edges in g that have reifications in model.\n        - remove_wiki: remove wiki links\n    \"\"\"\n    assert paths\n    if not isinstance(paths, Iterable):\n        paths = [paths]\n\n    graphs = []\n    for path_ in paths:\n        for path in glob.glob(str(path_)):\n            path = Path(path)\n            graphs.extend(pm_load(path, dereify=dereify, remove_wiki=remove_wiki))\n\n    assert graphs\n\n    if use_recategorization:\n        for g in graphs:\n            metadata = g.metadata\n            metadata[\"snt_orig\"] = metadata[\"snt\"]\n            tokens = eval(metadata[\"tokens\"])\n            metadata[\"snt\"] = \" \".join(\n                [\n                    t\n                    for t in tokens\n                    if not ((t.startswith(\"-L\") or t.startswith(\"-R\")) and t.endswith(\"-\"))\n                ]\n            )\n\n    return graphs\n", "hanlp/components/amr/amrbart/common/penman_interface.py": "# coding:utf-8\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nfrom penman import load as load_, Graph, Triple\nfrom penman import loads as loads_\nfrom penman import encode as encode_\nfrom penman.model import Model\nfrom penman.models.noop import NoOpModel\nfrom penman.models import amr\n\nop_model = Model()\nnoop_model = NoOpModel()\namr_model = amr.model\nDEFAULT = op_model\n\n\ndef _get_model(dereify):\n    if dereify is None:\n        return DEFAULT\n\n    elif dereify:\n        return op_model\n\n    else:\n        return noop_model\n\n\ndef _remove_wiki(graph):\n    metadata = graph.metadata\n    triples = []\n    for t in graph.triples:\n        v1, rel, v2 = t\n        if rel == \":wiki\":\n            t = Triple(v1, rel, \"+\")\n        triples.append(t)\n    graph = Graph(triples)\n    graph.metadata = metadata\n    return graph\n\n\ndef load(source, dereify=None, remove_wiki=False):\n    model = _get_model(dereify)\n    out = load_(source=source, model=model)\n    if remove_wiki:\n        for i in range(len(out)):\n            out[i] = _remove_wiki(out[i])\n    return out\n\n\ndef loads(string, dereify=None, remove_wiki=False):\n    model = _get_model(dereify)\n    out = loads_(string=string, model=model)\n    if remove_wiki:\n        for i in range(len(out)):\n            out[i] = _remove_wiki(out[i])\n    return out\n\n\ndef encode(g, top=None, indent=-1, compact=False):\n    model = amr_model\n    return encode_(g=g, top=top, indent=indent, compact=compact, model=model)\n", "hanlp/components/amr/amrbart/common/postprocessing.py": "# coding:utf-8\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport re\nimport enum\nimport penman\nimport networkx as nx\nfrom hanlp.components.amr.amrbart.common.penman_interface import encode\nfrom collections import defaultdict, Counter\n\nBACKOFF = penman.Graph(\n    [\n        penman.Triple(\"d2\", \":instance\", \"dog\"),\n        penman.Triple(\"b1\", \":instance\", \"bark-01\"),\n        penman.Triple(\"b1\", \":ARG0\", \"d2\"),\n    ]\n)\n\n\ndef token_processing(tok):\n    if tok is None:\n        return None\n    elif tok.isdigit():\n        try:\n            return eval(tok)\n        except:\n            return tok\n    elif tok.startswith('\"') and (not tok.endswith('\"')):\n        return tok + '\"'\n    elif tok.endswith('\"') and (not tok.startswith('\"')):\n        return '\"' + tok\n    else:\n        return tok\n\n\ndef decode_into_node_and_backreferences(subtoken_ids, tokenizer):\n    rex_arg = re.compile(f\"^{tokenizer.INIT}(op|snt|conj|prep)\")\n    rex_spc = re.compile(r\"<(s|/s|lit|/lit|stop|unk|pad|mask)>\")\n    \n    # subtoken_ids.insert(1,36)           # add \"(\" id\n    # subtoken_ids.insert(-1, 4839)       # add \")\" id\n\n    # get strings\n    subtokens = [tokenizer.decoder.get(t) for t in subtoken_ids]\n    # print(\"subtokens:\", subtokens)\n    # fix backreferences\n    \n    subtoken_backreferences = [max(t - len(tokenizer.encoder), -1) for t in subtoken_ids]\n    # strip padding\n    subtokens, subtoken_backreferences = zip(\n        *[\n            (s, b)\n            for s, b in zip(subtokens, subtoken_backreferences)\n            if s != (\"<pad>\")\n        ]\n    )\n\n    # subword collapse\n    tokens = []\n    backreferences = []\n    subword_to_token_map = {}\n    current_token_i = 0\n    for subw_i, (subw_backr, subtok) in enumerate(zip(subtoken_backreferences, subtokens)):\n        subword_to_token_map[subw_i] = current_token_i\n\n        # if empty you cannot do anything but add a new word\n        if not tokens:\n            tokens.append(subtok.lstrip(tokenizer.INIT))\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # backref can't be splitted\n        elif subw_backr > -1:\n            tokens.append(None)\n            backreferences.append(subword_to_token_map[subw_backr])\n            current_token_i += 1\n\n        # after a special token release\n        elif isinstance(tokens[-1], str) and rex_spc.match(tokens[-1]):\n            tokens.append(subtok.lstrip(tokenizer.INIT))\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # after a subtoken ':' (which should be followed by the rest of the edge) ignore tokenizer.INIT\n        # TODO: this is an ugly patch due to the fact that BART tokenizer splits after ':'\n        elif (tokens[-1] == \":\") and rex_arg.match(subtok):\n            tokens[-1] = tokens[-1] + subtok[1:]\n\n        # leading tokenizer.INIT\n        elif subtok.startswith(tokenizer.INIT):\n            tokens.append(subtok.lstrip(tokenizer.INIT))\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # very ugly patch for some cases in which tokenizer.INIT is not in the following token to the edge\n        elif (\n            isinstance(tokens[-1], str)\n            and tokens[-1].startswith(\":\")\n            and tokens[-1][-1].isdigit()\n            and (subtok != \"-of\")\n        ):\n            tokens.append(subtok.lstrip(tokenizer.INIT))\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # in any other case attach to the previous\n        else:\n            tokens[-1] = tokens[-1] + subtok\n\n    # strip INIT and fix byte-level\n    tokens = [\n        tokenizer.convert_tokens_to_string(list(t)).lstrip() if isinstance(t, str) else t\n        for t in tokens\n    ]\n    # tokens = [t.replace(tokenizer.INIT, '') if isinstance(t, str) else t for t in tokens]\n\n    # unks are substituted with thing\n    tokens = [t if t != \"<unk>\" else \"thing\" for t in tokens]\n\n    old_tokens = tokens\n    old_backreferences = backreferences\n\n    # <lit> Barack Obama </lit> -> \"Barack Obama\"\n    tokens = []\n    backreferences = []\n    token_to_token_map = {}\n    start_search = 0\n    removed = 0\n    while True:\n        try:\n\n            lit_start = old_tokens.index(\"<lit>\", start_search)\n            token_addition = old_tokens[start_search:lit_start]\n            for i, t in enumerate(token_addition, start=start_search):\n                token_to_token_map[i] = i - removed\n            tokens += token_addition\n\n            backreferences_addition = [\n                token_to_token_map[b] if b > -1 else -1\n                for b in old_backreferences[start_search:lit_start]\n            ]\n            backreferences += backreferences_addition\n\n            lit_end = min(lit_start + 2, len(old_tokens) - 1)\n\n            while lit_end < len(old_tokens):\n                old_tok = old_tokens[lit_end]\n\n                if isinstance(old_tok, str) and (\n                    (old_tok.startswith(\":\") and len(old_tok) > 3) or (old_tok == \"<stop>\")\n                ):\n                    res_tok = old_tokens[lit_start + 1 : lit_end]\n                    for i in range(lit_start, lit_end):\n                        token_to_token_map[i] = len(tokens)\n\n                    # Remove possible wrong None\n                    res = old_tokens[lit_start + 1 : lit_end]\n                    res = [str(r) for r in res if r is not None]\n                    res = '\"' + \"_\".join(res) + '\"'\n\n                    removed += len(res_tok)\n                    start_search = lit_end\n                    tokens += [res, old_tok]\n                    backreferences += [-1, -1]\n                    break\n\n                elif old_tok == \"</lit>\":\n                    res_tok = old_tokens[lit_start + 1 : lit_end]\n                    for i in range(lit_start, lit_end + 1):\n                        token_to_token_map[i] = len(tokens)\n\n                    # Remove possible wrong None\n                    res = old_tokens[lit_start + 1 : lit_end]\n                    res = [str(r) for r in res if r is not None]\n                    res = '\"' + \"_\".join(res) + '\"'\n\n                    removed += len(res_tok) + 1\n                    start_search = lit_end + 1\n                    tokens.append(res)\n                    backreferences.append(-1)\n                    break\n\n                else:\n                    lit_end += 1\n                    start_search = lit_end\n\n        except ValueError:\n            token_addition = old_tokens[start_search:]\n            for i, t in enumerate(token_addition, start=start_search):\n                token_to_token_map[i] = i - removed\n            backreferences_addition = [\n                token_to_token_map[b] if b > -1 else b for b in old_backreferences[start_search:]\n            ]\n            tokens += token_addition\n            backreferences += backreferences_addition\n            break\n\n    tokens = [token_processing(t) for t in tokens]\n\n    shift = 1\n    if tokens[1] == \"<s>\":\n        shift = 2\n\n    tokens = tokens[shift:]\n    backreferences = [b if b == -1 else b - shift for b in backreferences[shift:]]\n\n    if tokens[-1] == \"</s>\":\n        tokens.pop()\n        backreferences.pop()\n\n    return tokens, backreferences\n\n\ndef index_of(element, iterable, default=None, start=None, end=None):\n    if not callable(element):\n\n        def check(x):\n            return element == x\n\n    else:\n        check = element\n    if start is None:\n        start = 0\n    if end is None:\n        end = len(iterable)\n    item = start\n    while item < end:\n        if check(iterable[item]):\n            return item\n        item += 1\n    return default\n\n\ndef separate_edges_nodes(edges_nodes_slice, *other):\n    is_arg = lambda x: isinstance(x, str) and x.startswith(\":\")\n    start = 0\n    edges = []\n    nodes = []\n    l = len(edges_nodes_slice)\n    while start < l:\n        edge_index = index_of(is_arg, edges_nodes_slice, start=start)\n        if edge_index is None or edge_index == (l - 1):\n            break\n        if is_arg(edges_nodes_slice[edge_index + 1]):\n            start = edge_index + 1\n            continue\n        edges.append(edge_index)\n        nodes.append(edge_index + 1)\n        start = edge_index + 2\n    ret = []\n    for oth in other:\n        edges_oth = [oth[i] for i in edges]\n        nodes_oth = [oth[i] for i in nodes]\n        ret.append((edges_oth, nodes_oth))\n    return ret\n\n\ndef _split_name_ops(graph):\n    # identify name triples\n    name_vars = {}\n    for i, (v1, rel, v2) in enumerate(graph.triples):\n        if rel == \":instance\" and v2 == \"name\":\n            name_vars[v1] = 1\n\n    # check if they have ops\n    name_vars_to_ops = defaultdict(list)\n    for i, (v1, rel, v2) in enumerate(graph.triples):\n        if v1 in name_vars and rel.startswith(\":op\"):\n            name_vars_to_ops[v1].append((i, rel, v2.strip('\"')))\n\n    triples = graph.triples.copy()\n    for nv, ops in name_vars_to_ops.items():\n        ops = sorted(ops, key=lambda x: int(x[1][3:]))\n        idx, _, lits = zip(*ops)\n        for i in idx:\n            triples[i] = None\n\n        lits = ['\"' + l + '\"' for lit in lits for l in lit.split(\"_\")]\n\n        tt = []\n        for i, l in enumerate(lits, start=1):\n            rel = \":op\" + str(i)\n            tt.append(penman.Triple(nv, rel, l))\n\n        triples[min(idx)] = tt\n\n    triples = [t if isinstance(t, list) else [t] for t in triples if t is not None]\n    triples = [t for tt in triples for t in tt]\n\n    graph_ = penman.Graph(triples)\n    graph_.metadata = graph.metadata\n    return graph_\n\n\ndef _reconstruct_graph_from_nodes(nodes, backreferences):\n    triples = []\n    triples_added = set()\n\n    variable2index = {}\n    index2variable = {}\n    start_index = 0\n\n    cnt = defaultdict(Counter)\n\n    while start_index < len(nodes):\n        stop_index = index_of(\"<stop>\", nodes, default=len(nodes) + 1, start=start_index)\n        old_start_index = start_index\n        start_index = stop_index + 1\n\n        src_node, src_backr = nodes[old_start_index], backreferences[old_start_index]\n\n        if src_node == \"<stop>\":\n            continue\n\n        trg_nodes_edges = nodes[old_start_index:stop_index]\n        trg_nodes_edges_backr = backreferences[old_start_index:stop_index]\n        trg_nodes_edges_indices = list(range(old_start_index, stop_index))\n\n        if isinstance(src_node, str):\n            if src_node in (\"<s>\", \"</s>\", \"<stop>\"):\n                continue\n            elif (\"/\" in src_node) or (\":\" in src_node) or (\"(\" in src_node) or (\")\" in src_node):\n                src_node = \"thing\"\n\n        if src_node is not None:\n            src_node = str(src_node)\n            src_var = src_node[0].lower()\n            if not src_var not in \"abcdefghijklmnopqrstuvwxyz\":\n                src_var = \"x\"\n            # src_var = f'{src_var}_{len(variable2index)}'\n            src_var = f\"{src_var}{len(variable2index)}\"\n            src_var_i = old_start_index\n            variable2index[src_var] = src_var_i\n            index2variable[src_var_i] = src_var\n            triple = penman.Triple(src_var, \":instance\", src_node)\n            if triple not in triples_added:\n                triples.append(triple)\n                triples_added.add(triple)\n        else:\n            if src_backr in index2variable:\n                src_var = index2variable[src_backr]\n        # more resilient logic here\n        (trg_edges, trg_nodes), (_, trg_nodes_backr), (_, trg_nodes_indices) = separate_edges_nodes(\n            trg_nodes_edges, trg_nodes_edges, trg_nodes_edges_backr, trg_nodes_edges_indices\n        )\n\n        for n, e, nb, ni in zip(trg_nodes, trg_edges, trg_nodes_backr, trg_nodes_indices):\n\n            if isinstance(n, str) and n.startswith(\":\"):\n                continue\n            if isinstance(n, str) and n.startswith(\"<\") and n.endswith(\">\"):\n                continue\n            if e == \":li\":\n                pass\n            elif len(e) < 4 or (not e.startswith(\":\")):\n                continue\n\n            # same edge more than once\n            num = cnt[src_var][e]\n            # num = 0\n            if num:\n\n                if e.startswith(\":op\") or e.startswith(\":snt\"):\n                    continue\n                # elif e.startswith(':ARG'):\n                #    continue\n                elif num > 3:\n                    continue\n\n            if n is None:\n                if nb not in index2variable:\n                    continue\n                trg_var = index2variable[nb]\n                trg = trg_var\n            elif e == \":mode\":\n                trg = n\n            elif (\n                (not isinstance(n, str))\n                or re.match(r\"^[+-]?\\d+\\.?\\d*$\", n)\n                or (n == \"-\")\n                or (n == \"+\")\n            ):\n                trg = str(n)\n            elif n.startswith('\"') and n.endswith('\"') and len(n) > 2:\n                trg = '\"' + n.replace('\"', \"\") + '\"'\n            elif (\"/\" in n) or (\":\" in n) or (\"(\" in n) or (\")\" in n) or (\"=\" in n):\n                trg = f'\"{n}\"'\n            elif n == '\"':\n                continue\n            elif (\n                (n.startswith('\"') and (not n.endswith('\"')))\n                or (not n.startswith('\"') and (n.endswith('\"')))\n                or ('\"' in n)\n            ):\n                trg = '\"' + n.replace('\"', \"\") + '\"'\n            else:\n                trg_var = n[0].lower()\n                if trg_var not in \"abcdefghijklmnopqrstuvwxyz\":\n                    trg_var = \"x\"\n                # trg_var = f'{trg_var}_{len(variable2index)}'\n                trg_var = f\"{trg_var}{len(variable2index)}\"\n                trg_var_i = ni\n                variable2index[trg_var] = trg_var_i\n                index2variable[trg_var_i] = trg_var\n                triple = penman.Triple(trg_var, \":instance\", n)\n                if triple not in triples_added:\n                    triples.append(triple)\n                    triples_added.add(triple)\n                trg = trg_var\n\n            triple = penman.Triple(src_var, e, trg)\n            if triple not in triples_added:\n                triples.append(triple)\n                triples_added.add(triple)\n\n            cnt[src_var][e] += 1\n\n    return penman.Graph(triples)\n\n\ndef build_graph(nodes, backreferences, restore_name_ops=False):\n    graph = _reconstruct_graph_from_nodes(nodes, backreferences)\n    if restore_name_ops:\n        graph = _split_name_ops(graph)\n    return graph\n\n\nclass ParsedStatus(enum.Enum):\n    OK = 0\n    FIXED = 1\n    BACKOFF = 2\n\n\ndef connect_graph_if_not_connected(graph):\n\n    try:\n        encoded = encode(graph)\n        return graph, ParsedStatus.OK\n    except:\n        pass\n\n    nxgraph = nx.MultiGraph()\n    variables = graph.variables()\n    for v1, _, v2 in graph.triples:\n        if v1 in variables and v2 in variables:\n            nxgraph.add_edge(v1, v2)\n        elif v1 in variables:\n            nxgraph.add_edge(v1, v1)\n\n    triples = graph.triples.copy()\n    new_triples = []\n    addition = f\"a{len(variables) + 1}\"\n    triples.append(penman.Triple(addition, \":instance\", \"and\"))\n    for i, conn_set in enumerate(nx.connected_components(nxgraph), start=1):\n        edge = f\":op{i}\"\n        conn_set = sorted(conn_set, key=lambda x: int(x[1:]))\n        conn_set = [c for c in conn_set if c in variables]\n        node = conn_set[0]\n        new_triples.append(penman.Triple(addition, edge, node))\n    triples = new_triples + triples\n    metadata = graph.metadata\n    graph = penman.Graph(triples)\n    graph.metadata.update(metadata)\n    encode(graph)\n\n    return graph, ParsedStatus.FIXED\n\n\ndef restore_backreferences_from_pointers(nodes):\n    new_nodes, new_backreferences = [], []\n    prev_pointer = None\n    pointer2i = {}\n    for n in nodes:\n        is_pointer = isinstance(n, str) and n.startswith(\"<pointer:\") and n.endswith(\">\")\n\n        if not is_pointer:\n            if prev_pointer is not None:\n                if prev_pointer in pointer2i:\n                    new_nodes.append(None)\n                    new_backreferences.append(pointer2i[prev_pointer])\n                    new_nodes.append(n)\n                    new_backreferences.append(-1)\n\n                else:\n                    pointer2i[prev_pointer] = len(new_nodes)\n                    new_nodes.append(n)\n                    new_backreferences.append(-1)\n            else:\n                new_nodes.append(n)\n                new_backreferences.append(-1)\n\n            prev_pointer = None\n        else:\n            prev_pointer = n\n    return new_nodes, new_backreferences\n", "hanlp/components/amr/amrbart/common/constant.py": "# coding:utf-8\n# MIT License\n#\n# Copyright (c) 2022 xfbai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    BartTokenizer,\n    BartForConditionalGeneration,\n    T5Tokenizer,\n    T5Model,\n    T5ForConditionalGeneration,\n)\nfrom transformers.optimization import (\n    get_cosine_schedule_with_warmup,\n    get_cosine_with_hard_restarts_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    get_polynomial_decay_schedule_with_warmup,\n    get_constant_schedule_with_warmup,\n)\n\nraw_special_tokens = ['\u0120cause-01', '\u0120possible-01', '\u0120contrast-01', '\u0120say-01', '\u0120have-03', '\u0120govern-01', '\u0120state-01',\n                      '\u0120think-01', '\u0120do-02', '\u0120want-01', '\u0120know-01', '\u0120recommend-01', '\u0120see-01', '\u0120resemble-01',\n                      '\u0120mean-01', '\u0120obligate-01', '\u0120use-01', '\u0120good-02', '\u0120need-01', '\u0120work-01', '\u0120pay-01', '\u0120get-01',\n                      '\u0120attack-01', '\u0120real-04', '\u0120believe-01', '\u0120support-01', '\u0120report-01', '\u0120try-01', '\u0120same-01',\n                      '\u0120tax-01', '\u0120oppose-01', '\u0120live-01', '\u0120tell-01', '\u0120make-02', '\u0120die-01', '\u0120kill-01', '\u0120new-01',\n                      '\u0120give-01', '\u0120increase-01', '\u0120agree-01', '\u0120actual-02', '\u0120go-02', '\u0120right-05', '\u0120vote-01',\n                      '\u0120make-01', '\u0120take-01', '\u0120seem-01', '\u0120talk-01', '\u0120issue-02', '\u0120become-01', '\u0120post-01', '\u0120help-01',\n                      '\u0120start-01', '\u0120end-01', '\u0120develop-02', '\u0120decide-01', '\u0120find-01', '\u0120claim-01', '\u0120defend-01',\n                      '\u0120lead-02', '\u0120high-02', '\u0120control-01', '\u0120free-04', '\u0120traffic-01', '\u0120long-03', '\u0120provide-01',\n                      '\u0120come-01', '\u0120plan-01', '\u0120produce-01', '\u0120change-01', '\u0120differ-02', '\u0120marry-01', '\u0120employ-01',\n                      '\u0120choose-01', '\u0120fight-01', '\u0120meet-03', '\u0120call-01', '\u0120read-01', '\u0120understand-01', '\u0120sure-02',\n                      '\u0120capable-01', '\u0120allow-01', '\u0120crime-02', '\u0120include-01', '\u0120sell-01', '\u0120infer-01', '\u0120show-01',\n                      '\u0120feel-01', '\u0120war-01', '\u0120question-01', '\u0120look-01', '\u0120opine-01', '\u0120legal-02', '\u0120lose-02',\n                      '\u0120stop-01', '\u0120create-01', '\u0120cost-01', '\u0120continue-01', '\u0120bad-07', '\u0120act-02', '\u0120care-03', '\u0120win-01',\n                      '\u0120discuss-01', '\u0120destroy-01', '\u0120policy-01', '\u0120elect-01', '\u0120go-01', '\u0120true-01', '\u0120lie-08',\n                      '\u0120base-02', '\u0120insure-02', '\u0120invest-01', '\u0120fund-01', '\u0120liberal-02', '\u0120trade-01', '\u0120speak-01',\n                      '\u0120involve-01', '\u0120fail-01', '\u0120hear-01', '\u0120let-01', '\u0120hope-01', '\u0120interest-01', '\u0120threaten-01',\n                      '\u0120grow-01', '\u0120deal-01', '\u0120spend-01', '\u0120exist-01', '\u0120begin-01', '\u0120depend-01', '\u0120arrest-01',\n                      '\u0120prove-01', '\u0120buy-01', '\u0120put-01', '\u0120get-05', '\u0120activity-06', '\u0120offer-01', '\u0120personal-02',\n                      '\u0120protect-01', '\u0120quote-01', '\u0120write-01', '\u0120own-01', '\u0120build-01', '\u0120benefit-01', '\u0120relation-03',\n                      '\u0120equal-01', '\u0120surrender-01', '\u0120expect-01', '\u0120like-01', '\u0120cooperate-01', '\u0120move-01', '\u0120except-01',\n                      '\u0120realize-01', '\u0120strong-02', '\u0120hate-01', '\u0120argue-01', '\u0120ask-01', '\u0120answer-01', '\u0120low-04',\n                      '\u0120case-03', '\u0120result-01', '\u0120easy-05', '\u0120hard-02', '\u0120concern-01', '\u0120suspect-01', '\u0120bear-02',\n                      '\u0120serve-01', '\u0120accept-01', '\u0120clear-06', '\u0120love-01', '\u0120demand-01', '\u0120launch-01', '\u0120explain-01',\n                      '\u0120wrong-04', '\u0120right-06', '\u0120require-01', '\u0120affect-01', '\u0120effort-01', '\u0120force-01', '\u0120look-02',\n                      '\u0120watch-01', '\u0120out-06', '\u0120operate-01', '\u0120attempt-01', '\u0120ban-01', '\u0120study-01', '\u0120suggest-01',\n                      '\u0120likely-01', '\u0120concern-02', '\u0120thank-01', '\u0120public-02', '\u0120work-09', '\u0120exemplify-01', '\u0120intend-01',\n                      '\u0120price-01', '\u0120respond-01', '\u0120propose-01', '\u0120visit-01', '\u0120complete-02', '\u0120transfer-01',\n                      '\u0120accuse-01', '\u0120counter-01', '\u0120cut-02', '\u0120simple-02', '\u0120care-01', '\u0120charge-05', '\u0120represent-01',\n                      '\u0120succeed-01', '\u0120local-02', '\u0120murder-01', '\u0120remember-01', '\u0120send-01', '\u0120evidence-01',\n                      '\u0120research-01', '\u0120major-02', '\u0120wait-01', '\u0120establish-01', '\u0120remain-01', '\u0120test-01', '\u0120keep-02',\n                      '\u0120export-01', '\u0120announce-01', '\u0120bomb-01', '\u0120favor-01', '\u0120deny-01', '\u0120run-01', '\u0120experience-01',\n                      '\u0120expert-01', '\u0120prevent-01', '\u0120fair-01', '\u0120know-02', '\u0120general-02', '\u0120approve-01', '\u0120white-02',\n                      '\u0120describe-01', '\u0120share-01', '\u0120consider-01', '\u0120case-04', '\u0120receive-01', '\u0120ignore-01', '\u0120link-01',\n                      '\u0120keep-01', '\u0120comment-01', '\u0120sex-01', '\u0120laugh-01', '\u0120investigate-01', '\u0120view-02',\n                      '\u0120proliferate-01', '\u0120refuse-01', '\u0120fear-01', '\u0120get-03', '\u0120will-02', '\u0120rape-01', '\u0120allege-01',\n                      '\u0120get-04', '\u0120stay-01', '\u0120rise-01', '\u0120supply-01', '\u0120direct-02', '\u0120honest-01', '\u0120debate-01',\n                      '\u0120obvious-01', '\u0120appear-02', '\u0120campaign-01', '\u0120black-05', '\u0120reduce-01', '\u0120ask-02',\n                      '\u0120criticize-01', '\u0120guess-01', '\u0120learn-01', '\u0120seek-01', '\u0120access-01', '\u0120safe-01', '\u0120wish-01',\n                      '\u0120wrong-02', '\u0120educate-01', '\u0120conflict-01', '\u0120respect-01', '\u0120reach-01', '\u0120age-01', '\u0120mention-01',\n                      '\u0120execute-01', '\u0120find-02', '\u0120judge-01', '\u0120bring-01', '\u0120blame-01', '\u0120head-01', '\u0120well-09',\n                      '\u0120ensure-01', '\u0120arm-01', '\u0120cover-01', '\u0120serious-02', '\u0120treat-01', '\u0120teach-01', '\u0120doubt-01',\n                      '\u0120immigrate-01', '\u0120invade-01', '\u0120smuggle-01', '\u0120lack-01', '\u0120earn-01', '\u0120hold-01', '\u0120limit-01',\n                      '\u0120participate-01', '\u0120sentence-01', '\u0120damage-01', '\u0120consider-02', '\u0120name-01', '\u0120sorry-01',\n                      '\u0120relate-01', '\u0120criminal-03', '\u0120left-19', '\u0120admit-01', '\u0120administrate-01', '\u0120target-01',\n                      '\u0120run-02', '\u0120go-06', '\u0120improve-01', '\u0120construct-01', '\u0120moral-02', '\u0120follow-01', '\u0120correct-02',\n                      '\u0120protest-01', '\u0120leave-11', '\u0120aid-01', '\u0120value-01', '\u0120sense-02', '\u0120drop-01', '\u0120face-01',\n                      '\u0120serious-01', '\u0120seize-01', '\u0120train-01', '\u0120warn-01', '\u0120avoid-01', '\u0120effective-04', '\u0120deserve-01',\n                      '\u0120play-01', '\u0120enter-01', '\u0120regulate-01', '\u0120near-02', '\u0120border-01', '\u0120solve-01', '\u0120prefer-01',\n                      '\u0120violate-01', '\u0120release-01', '\u0120cite-01', '\u0120focus-01', '\u0120advise-01', '\u0120sound-01', '\u0120risk-01',\n                      '\u0120return-01', '\u0120list-01', '\u0120significant-02', '\u0120hire-01', '\u0120surprise-01', '\u0120open-01', '\u0120nice-01',\n                      '\u0120raise-01', '\u0120maintain-01', '\u0120private-03', '\u0120implement-01', '\u0120assist-01', '\u0120call-02',\n                      '\u0120compare-01', '\u0120profit-01', '\u0120contribute-01', '\u0120have-to-do-with-04', '\u0120corrupt-01', '\u0120close-10',\n                      '\u0120suffer-01', '\u0120expand-01', '\u0120wonder-01', '\u0120responsible-01', '\u0120total-01', '\u0120specific-02',\n                      '\u0120pass-01', '\u0120happy-01', '\u0120assume-02', '\u0120chance-02', '\u0120remove-01', '\u0120add-02', '\u0120manufacture-01',\n                      '\u0120express-01', '\u0120inspect-01', '\u0120walk-01', '\u0120good-03', '\u0120rule-01', '\u0120manage-01', '\u0120hold-04',\n                      '\u0120special-02', '\u0120influence-01', '\u0120exchange-01', '\u0120take-10', '\u0120convict-01', '\u0120process-02',\n                      '\u0120travel-01', '\u0120carry-01', '\u0120define-01', '\u0120disagree-01', '\u0120save-02', '\u0120permit-01', '\u0120estimate-01',\n                      '\u0120rate-01', '\u0120call-03', '\u0120single-02', '\u0120abuse-01', '\u0120sign-01', '\u0120rule-03', '\u0120act-01',\n                      '\u0120achieve-01', '\u0120intervene-01', '\u0120fall-01', '\u0120attend-02', '\u0120feel-02', '\u0120adopt-01', '\u0120follow-02',\n                      '\u0120go-on-15', '\u0120loan-01', '\u0120negotiate-01', '\u0120hit-01', '\u0120condition-01', '\u0120short-07', '\u0120promise-01',\n                      '\u0120rebel-01', '\u0120promote-02', '\u0120strengthen-01', '\u0120sanction-02', '\u0120warm-01', '\u0120behave-01',\n                      '\u0120have-06', '\u0120suffice-01', '\u0120lead-03', '\u0120try-02', '\u0120like-02', '\u0120fire-01', '\u0120drive-01', '\u0120fly-01',\n                      '\u0120gain-02', '\u0120afford-01', '\u0120explode-01', '\u0120point-out-02', '\u0120consume-01', '\u0120measure-02',\n                      '\u0120reform-01', '\u0120enjoy-01', '\u0120sit-01', '\u0120available-02', '\u0120strike-01', '\u0120sign-02', '\u0120come-03',\n                      '\u0120natural-03', '\u0120organize-01', '\u0120prepare-02', '\u0120replace-01', '\u0120hanging-07', '\u0120leave-15',\n                      '\u0120retire-01', '\u0120import-01', '\u0120range-01', '\u0120okay-04', '\u0120cover-03', '\u0120imagine-01', '\u0120key-02',\n                      '\u0120survive-01', '\u0120free-03', '\u0120base-01', '\u0120complain-01', '\u0120normal-02', '\u0120complete-01', '\u0120reveal-01',\n                      '\u0120enforce-01', '\u0120determine-01', '\u0120victimize-01', '\u0120repeat-01', '\u0120interview-01', '\u0120make-05',\n                      '\u0120donate-01', '\u0120steal-01', '\u0120quick-02', '\u0120attract-01', '\u0120analyze-01', '\u0120ally-01', '\u0120suppose-01',\n                      '\u0120responsible-03', '\u0120close-01', '\u0120combat-01', '\u0120identify-01', '\u0120suppose-02', '\u0120record-01',\n                      '\u0120nominate-01', '\u0120rely-01', '\u0120turn-02', '\u0120handle-01', '\u0120process-01', '\u0120predict-01', '\u0120deploy-01',\n                      '\u0120fortunate-01', '\u0120eat-01', '\u0120justify-01', '\u0120expend-01', '\u0120bullshit-01', '\u0120discover-01',\n                      '\u0120enrich-01', '\u0120commit-02', '\u0120shoot-02', '\u0120cheap-02', '\u0120reject-01', '\u0120weak-02', '\u0120powerful-02',\n                      '\u0120dispute-01', '\u0120legislate-01', '\u0120issue-01', '\u0120arrive-01', '\u0120join-01', '\u0120apply-02',\n                      '\u0120indicate-01', '\u0120engage-01', '\u0120innocent-01', '\u0120fast-02', '\u0120pressure-01', '\u0120publish-01',\n                      '\u0120obtain-01', '\u0120sad-02', '\u0120confirm-01', '\u0120treat-03', '\u0120lead-01', '\u0120listen-01', '\u0120offend-01',\n                      '\u0120address-02', '\u0120word-01', '\u0120right-08', '\u0120note-01', '\u0120contain-01', '\u0120purchase-01', '\u0120request-01',\n                      '\u0120good-04', '\u0120design-01', '\u0120notice-01', '\u0120present-01', '\u0120shock-01', '\u0120right-02', '\u0120transport-01',\n                      '\u0120deliver-01', '\u0120burn-01', '\u0120fault-01', '\u0120matter-01', '\u0120abort-01', '\u0120stick-01', '\u0120connect-01',\n                      '\u0120conclude-01', '\u0120contract-02', '\u0120possess-01', '\u0120end-up-03', '\u0120search-01', '\u0120get-02',\n                      '\u0120qualify-02', '\u0120react-01', '\u0120confuse-01', '\u0120anger-01', '\u0120pursue-01', '\u0120reside-01',\n                      '\u0120relevant-01', '\u0120occupy-01', '\u0120withdraw-01', '\u0120okay-01', '\u0120conform-01', '\u0120demonstrate-01',\n                      '\u0120wear-01', '\u0120have-04', '\u0120decrease-01', '\u0120punish-01', '\u0120practice-01', '\u0120capture-01', '\u0120go-03',\n                      '\u0120poll-01', '\u0120show-04', '\u0120refer-01', '\u0120commit-01', '\u0120disarm-01', '\u0120belong-01', '\u0120divide-02',\n                      '\u0120drink-01', '\u0120desire-01', '\u0120save-01', '\u0120ignorant-02', '\u0120perfect-02', '\u0120position-02', '\u0120crap-01',\n                      '\u0120insult-01', '\u0120private-02', '\u0120waste-01', '\u0120guilty-01', '\u0120eliminate-01', '\u0120mortgage-01',\n                      '\u0120worth-01', '\u0120inherit-01', '\u0120throw-01', '\u0120tour-01', '\u0120suspend-01', '\u0120harm-01', '\u0120impose-01',\n                      '\u0120imprison-01', '\u0120recognize-01', '\u0120prosecute-01', '\u0120view-01', '\u0120forget-01', '\u0120found-01',\n                      '\u0120challenge-01', '\u0120trouble-01', '\u0120secure-02', '\u0120order-01', '\u0120partner-01', '\u0120spend-02',\n                      '\u0120progressive-02', '\u0120account-01', '\u0120block-01', '\u0120guarantee-01', '\u0120convince-01', '\u0120worry-02',\n                      '\u0120endanger-01', '\u0120movement-07', '\u0120fuck-01', '\u0120extend-01', '\u0120separate-02', '\u0120balance-01',\n                      '\u0120lose-03', '\u0120power-01', '\u0120sue-02', '\u0120urge-01', '\u0120check-01', '\u0120point-01', '\u0120turn-01',\n                      '\u0120progress-01', '\u0120recover-01', '\u0120ridiculous-02', '\u0120accompany-01', '\u0120appear-01', '\u0120worry-01',\n                      '\u0120place-01', '\u0120attend-01', '\u0120sleep-01', '\u0120break-01', '\u0120find-out-03', '\u0120bias-01', '\u0120accord-03',\n                      '\u0120wide-02', '\u0120enable-01', '\u0120affair-02', '\u0120hide-01', '\u0120hold-02', '\u0120recognize-02', '\u0120back-01',\n                      '\u0120bet-01', '\u0120hack-04', '\u0120acquire-01', '\u0120take-04', '\u0120penalize-01', '\u0120message-01', '\u0120ready-02',\n                      '\u0120cease-01', '\u0120crazy-03', '\u0120bad-04', '\u0120compete-02', '\u0120contact-01', '\u0120source-01', '\u0120set-up-03',\n                      '\u0120restrict-01', '\u0120regard-01', '\u0120witness-01', '\u0120labor-01', '\u0120smoke-02', '\u0120kick-01', '\u0120compete-01',\n                      '\u0120house-01', '\u0120hurt-01', '\u0120improvise-01', '\u0120finance-01', '\u0120insist-01', '\u0120farm-01', '\u0120apply-01',\n                      '\u0120step-01', '\u0120deep-02', '\u0120pride-01', '\u0120bill-01', '\u0120pretend-01', '\u0120fill-01', '\u0120fine-04',\n                      '\u0120stop-03', '\u0120offend-03', '\u0120advertise-01', '\u0120stand-01', '\u0120aim-02', '\u0120impact-01', '\u0120feed-01',\n                      '\u0120grant-01', '\u0120last-01', '\u0120form-01', '\u0120drive-02', '\u0120engineer-01', '\u0120injure-01', '\u0120develop-01',\n                      '\u0120present-02', '\u0120subsidize-01', '\u0120bring-up-02', '\u0120intelligent-01', '\u0120welcome-01', '\u0120take-away-05',\n                      '\u0120resolve-01', '\u0120appropriate-02', '\u0120encourage-01', '\u0120perform-02', '\u0120go-back-19', '\u0120declare-02',\n                      '\u0120full-09', '\u0120hopeful-03', '\u0120conduct-01', '\u0120surgery-01', '\u0120detain-01', '\u0120relative-05',\n                      '\u0120count-01', '\u0120glad-02', '\u0120rare-02', '\u0120come-out-09', '\u0120approach-02', '\u0120race-02', '\u0120battle-01',\n                      '\u0120cross-02', '\u0120move-02', '\u0120question-03', '\u0120administer-01', '\u0120grow-03', '\u0120meet-02', '\u0120down-03',\n                      '\u0120meet-01', '\u0120condemn-01', '\u0120reason-01', '\u0120carry-out-03', '\u0120worth-02', '\u0120inform-01', '\u0120stable-03',\n                      '\u0120stand-11', '\u0120utilize-01', '\u0120perpetrate-01', '\u0120associate-01', '\u0120apologize-01', '\u0120credit-01',\n                      '\u0120disgust-01', '\u0120spread-03', '\u0120command-02', '\u0120sense-01', '\u0120detail-01', '\u0120defeat-01',\n                      '\u0120distribute-01', '\u0120give-up-07', '\u0120pain-01', '\u0120ship-01', '\u0120keep-04', '\u0120addict-01',\n                      '\u0120compromise-01', '\u0120legitimate-02', '\u0120regular-02', '\u0120pick-01', '\u0120source-02', '\u0120raid-01',\n                      '\u0120hard-04', '\u0120rain-01', '\u0120communicate-01', '\u0120market-01', '\u0120lower-05', '\u0120ill-01', '\u0120defraud-01',\n                      '\u0120position-01', '\u0120terrible-01', '\u0120divorce-01', '\u0120amaze-01', '\u0120edit-01', '\u0120spread-02',\n                      '\u0120clarify-10', '\u0120argue-02', '\u0120push-01', '\u0120miss-01', '\u0120imply-01', '\u0120discriminate-02', '\u0120light-06',\n                      '\u0120appoint-01', '\u0120delay-01', '\u0120gross-03', '\u0120put-03', '\u0120introduce-02', '\u0120standard-02', '\u0120pull-01',\n                      '\u0120draw-02', '\u0120go-08', '\u0120aim-01', '\u0120modern-02', '\u0120dare-01', '\u0120neighbor-01', '\u0120confront-01',\n                      '\u0120superior-01', '\u0120reasonable-02', '\u0120schedule-01', '\u0120add-01', '\u0120new-02', '\u0120lend-01', '\u0120double-01',\n                      '\u0120finish-01', '\u0120raise-03', '\u0120excuse-02', '\u0120monitor-01', '\u0120observe-01', '\u0120popular-02',\n                      '\u0120charge-01', '\u0120budget-01', '\u0120negative-03', '\u0120direct-01', '\u0120rid-01', '\u0120make-18', '\u0120mean-02',\n                      '\u0120fame-01', '\u0120joke-01', '\u0120beautiful-02', '\u0120tend-02', '\u0120rob-01', '\u0120riot-01', '\u0120sponsor-01',\n                      '\u0120entitle-01', '\u0120lobby-01', '\u0120bad-02', '\u0120collapse-01', '\u0120expose-01', '\u0120emphasize-01',\n                      '\u0120friendly-01', '\u0120play-02', '\u0120initiate-01', '\u0120appreciate-02', '\u0120remind-01', '\u0120black-04',\n                      '\u0120efficient-01', '\u0120converse-01', '\u0120responsible-02', '\u0120measure-01', '\u0120come-04', '\u0120effect-03',\n                      '\u0120subject-01', '\u0120mistake-02', '\u0120pass-03', '\u0120signal-07', '\u0120guard-01', '\u0120open-04', '\u0120set-02',\n                      '\u0120fun-01', '\u0120come-up-11', '\u0120flee-05', '\u0120label-01', '\u0120size-01', '\u0120confident-01', '\u0120smart-06',\n                      '\u0120host-01', '\u0120tough-02', '\u0120recall-02', '\u0120scare-01', '\u0120dream-01', '\u0120assault-01', '\u0120freeze-02',\n                      '\u0120take-over-12', '\u0120recession-02', '\u0120function-01', '\u0120whine-01', '\u0120short-06', '\u0120prosper-01',\n                      '\u0120advanced-02', '\u0120value-02', '\u0120bother-01', '\u0120comply-01', '\u0120right-04', '\u0120revolution-03',\n                      '\u0120accomplish-01', '\u0120go-out-17', '\u0120figure-out-05', '\u0120slow-05', '\u0120accountable-02', '\u0120cool-01',\n                      '\u0120document-01', '\u0120authorize-01', '\u0120embargo-01', '\u0120volunteer-01', '\u0120register-02', '\u0120frequent-02',\n                      '\u0120rank-01', '\u0120resist-01', '\u0120break-up-08', '\u0120red-02', '\u0120comfortable-02', '\u0120examine-01',\n                      '\u0120adjust-01', '\u0120originate-01', '\u0120reply-01', '\u0120break-18', '\u0120shoot-01', '\u0120miss-02', '\u0120dismiss-01',\n                      '\u0120collect-01', '\u0120draft-01', '\u0120submit-01', '\u0120relieve-01', '\u0120embarrass-01', '\u0120return-02',\n                      '\u0120voluntary-02', '\u0120pure-02', '\u0120beat-01', '\u0120bear-01', '\u0120vary-01', '\u0120sick-05', '\u0120affair-01',\n                      '\u0120typical-02', '\u0120negative-02', '\u0120serve-02', '\u0120eradicate-01', '\u0120realize-02', '\u0120perceive-01',\n                      '\u0120leave-14', '\u0120give-16', '\u0120back-up-04', '\u0120generate-01', '\u0120bail-out-02', '\u0120touch-01',\n                      '\u0120cultivate-01', '\u0120convert-01', '\u0120dismantle-01', '\u0120service-05', '\u0120straight-04', '\u0120bad-05',\n                      '\u0120force-04', '\u0120advocate-01', '\u0120pray-01', '\u0120decline-01', '\u0120infect-01', '\u0120title-01',\n                      '\u0120desperate-02', '\u0120upset-01', '\u0120tolerate-01', '\u0120prohibit-01', '\u0120mind-05', '\u0120beat-03', '\u0120veto-01',\n                      '\u0120crash-01', '\u0120side-01', '\u0120combine-01', '\u0120close-13', '\u0120go-10', '\u0120equip-01', '\u0120rant-01',\n                      '\u0120jail-01', '\u0120copy-01', '\u0120drop-05', '\u0120consistent-02', '\u0120spend-04', '\u0120send-03', '\u0120critical-02',\n                      '\u0120carry-on-02', '\u0120raise-02', '\u0120motivate-01', '\u0120guide-01', '\u0120wonderful-03', '\u0120trust-01',\n                      '\u0120reverse-01', '\u0120just-02', '\u0120claim-02', '\u0120survey-01', '\u0120spy-01', '\u0120get-22', '\u0120have-05',\n                      '\u0120cool-04', '\u0120picture-01', '\u0120union-02', '\u0120manage-02', '\u0120instruct-01', '\u0120blow-03', '\u0120sacrifice-01',\n                      '\u0120owe-01', '\u0120appeal-01', '\u0120exceed-01', '\u0120radiate-01', '\u0120honor-01', '\u0120separate-01', '\u0120arrange-01',\n                      '\u0120dominate-01', '\u0120transact-01', '\u0120grow-up-04', '\u0120verify-01', '\u0120go-05', '\u0120familiarize-01',\n                      '\u0120renew-01', '\u0120fire-02', '\u0120take-out-11', '\u0120interpret-01', '\u0120valid-02', '\u0120show-up-02',\n                      '\u0120confiscate-01', '\u0120shut-down-05', '\u0120cheat-03', '\u0120harass-01', '\u0120tie-01', '\u0120abuse-02',\n                      '\u0120assess-01', '\u0120compensate-01', '\u0120sensitive-03', '\u0120settle-02', '\u0120encounter-01', '\u0120match-01',\n                      '\u0120recover-02', '\u0120trust-02', '\u0120perform-01', '\u0120borrow-01', '\u0120select-01', '\u0120betray-01', '\u0120ride-01',\n                      '\u0120useful-05', '\u0120split-01', '\u0120shift-01', '\u0120annoy-01', '\u0120mind-01', '\u0120fair-04', '\u0120oppress-01',\n                      '\u0120interfere-01', '\u0120credit-02', '\u0120launder-01', '\u0120amount-01', '\u0120leave-13', '\u0120rescue-01',\n                      '\u0120staff-01', '\u0120play-11', '\u0120kind-01', '\u0120author-01', '\u0120sympathize-01', '\u0120upgrade-02',\n                      '\u0120suppress-01', '\u0120wake-up-02', '\u0120invite-01', '\u0120come-12', '\u0120deter-01', '\u0120brainwash-01', '\u0120shit-01',\n                      '\u0120fix-02', '\u0120white-03', '\u0120group-01', '\u0120absent-01', '\u0120armor-01', '\u0120up-03', '\u0120praise-01',\n                      '\u0120review-01', '\u0120dry-02', '\u0120intercept-01', '\u0120broadcast-01', '\u0120worship-01', '\u0120term-01',\n                      '\u0120object-01', '\u0120pledge-01', '\u0120prepare-01', '\u0120open-up-03', '\u0120lay-01', '\u0120file-01', '\u0120check-out-05',\n                      '\u0120attach-01', '\u0120satisfy-01', '\u0120depart-01', '\u0120opposite-01', '\u0120worsen-01', '\u0120award-01',\n                      '\u0120pollute-01', '\u0120retaliate-01', '\u0120disrupt-01', '\u0120return-05', '\u0120populate-01', '\u0120envision-01',\n                      '\u0120please-01', '\u0120repair-01', '\u0120slaughter-01', '\u0120sin-01', '\u0120constitute-01', '\u0120shop-01',\n                      '\u0120translate-01', '\u0120assure-01', '\u0120pay-off-02', '\u0120stimulate-01', '\u0120damn-01', '\u0120switch-01',\n                      '\u0120disappear-01', '\u0120reelect-01', '\u0120spin-03', '\u0120testify-01', '\u0120legalize-01', '\u0120print-01',\n                      '\u0120average-01', '\u0120right-03', '\u0120fix-03', '\u0120undermine-01', '\u0120come-on-25', '\u0120license-01',\n                      '\u0120indict-01', '\u0120transit-01', '\u0120wash-01', '\u0120breathe-01', '\u0120broad-02', '\u0120leave-17', '\u0120order-02',\n                      '\u0120head-02', '\u0120sing-01', '\u0120entertain-01', '\u0120complicate-01', '\u0120push-02', '\u0120realistic-03',\n                      '\u0120disappoint-01', '\u0120bother-02', '\u0120tough-03', '\u0120display-01', '\u0120flow-01', '\u0120differ-01', '\u0120lie-07',\n                      '\u0120premise-01', '\u0120relocate-01', '\u0120correct-01', '\u0120coordinate-01', '\u0120abandon-01', '\u0120dictate-01',\n                      '\u0120play-08', '\u0120rebuild-01', '\u0120clean-04', '\u0120work-out-02', '\u0120run-13', '\u0120curious-01', '\u0120promote-01',\n                      '\u0120specialize-01', '\u0120starve-01', '\u0120shame-02', '\u0120fit-06', '\u0120flaw-01', '\u0120figure-01', '\u0120hunt-01',\n                      '\u0120experiment-01', '\u0120mix-01', '\u0120regular-03', '\u0120free-01', '\u0120declare-01', '\u0120escape-01', '\u0120put-02',\n                      '\u0120obsess-01', '\u0120build-up-05', '\u0120shut-up-06', '\u0120rally-01', '\u0120dissent-01', '\u0120program-01',\n                      '\u0120amend-01', '\u0120invent-01', '\u0120leak-01', '\u0120trigger-01', '\u0120distinguish-01', '\u0120symbolize-01',\n                      '\u0120excellent-02', '\u0120look-04', '\u0120cry-02', '\u0120assign-01', '\u0120recruit-01', '\u0120cope-01', '\u0120migrate-01',\n                      '\u0120take-on-09', '\u0120bless-01', '\u0120sharp-02', '\u0120use-02', '\u0120disturb-01', '\u0120consult-01', '\u0120lay-off-02',\n                      '\u0120bid-01', '\u0120accord-02', '\u0120busy-01', '\u0120provoke-01', '\u0120isolate-01', '\u0120dirty-02', '\u0120blind-02',\n                      '\u0120stage-01', '\u0120boost-01', '\u0120outrage-01', '\u0120track-01', '\u0120retard-01', '\u0120exclude-01', '\u0120patent-01',\n                      '\u0120blog-01', '\u0120torture-01', '\u0120plot-01', '\u0120cut-01', '\u0120hunger-01', '\u0120overwhelm-01', '\u0120exploit-01',\n                      '\u0120land-01', '\u0120reserve-01', '\u0120better-01', '\u0120up-02', '\u0120remark-01', '\u0120piss-03', '\u0120excuse-01',\n                      '\u0120paralyze-01', '\u0120summarize-01', '\u0120load-01', '\u0120devote-01', '\u0120bury-01', '\u0120surround-01',\n                      '\u0120dance-01', '\u0120distort-01', '\u0120retain-01', '\u0120overthrow-01', '\u0120rival-01', '\u0120ready-01', '\u0120evolve-01',\n                      '\u0120impoverish-01', '\u0120alarm-01', '\u0120unify-01', '\u0120repay-01', '\u0120assume-01', '\u0120close-06', '\u0120admire-01',\n                      '\u0120vow-01', '\u0120average-04', '\u0120sight-01', '\u0120inflate-01', '\u0120reference-04', '\u0120look-up-05',\n                      '\u0120civilize-01', '\u0120suitable-04', '\u0120detect-01', '\u0120piss-off-02', '\u0120assassinate-01', '\u0120open-05',\n                      '\u0120shave-01', '\u0120email-01', '\u0120fuel-01', '\u0120incentivize-01', '\u0120mark-01', '\u0120sustain-01',\n                      '\u0120speculate-01', '\u0120surveil-01', '\u0120swim-01', '\u0120conquer-01', '\u0120genocide-01', '\u0120hoax-01',\n                      '\u0120notice-03', '\u0120be-done-08', '\u0120opt-01', '\u0120bait-01', '\u0120compile-01', '\u0120innovate-01', '\u0120allocate-01',\n                      '\u0120shelter-01', '\u0120contrary-01', '\u0120burden-01', '\u0120freeze-01', '\u0120inspire-01', '\u0120graduate-01',\n                      '\u0120wipe-out-02', '\u0120fall-05', '\u0120cover-up-04', '\u0120repute-01', '\u0120enhance-01', '\u0120classify-01',\n                      '\u0120green-03', '\u0120score-01', '\u0120modify-01', '\u0120reflect-01', '\u0120force-02', '\u0120equate-01',\n                      '\u0120merchandise-01', '\u0120regret-01', '\u0120overcome-01', '\u0120procure-01', '\u0120scam-01', '\u0120quit-01',\n                      '\u0120drill-01', '\u0120disable-01', '\u0120grasp-01', '\u0120orbit-01', '\u0120laughable-03', '\u0120consent-01',\n                      '\u0120endorse-01', '\u0120catch-02', '\u0120leave-02', '\u0120weigh-01', '\u0120roll-01', '\u0120restore-01', '\u0120shape-01',\n                      '\u0120comprehend-01', '\u0120trip-03', '\u0120get-away-08', '\u0120single-03', '\u0120phone-01', '\u0120intimidate-01',\n                      '\u0120install-01', '\u0120suck-03', '\u0120back-02', '\u0120deem-01', '\u0120make-up-10', '\u0120plant-01', '\u0120hand-out-03',\n                      '\u0120go-off-16', '\u0120speed-01', '\u0120refute-01', '\u0120implicate-01', '\u0120dock-01', '\u0120crack-down-06',\n                      '\u0120forecast-01', '\u0120rush-01', '\u0120generous-01', '\u0120unite-01', '\u0120grab-01', '\u0120competent-01',\n                      '\u0120ground-02', '\u0120evaluate-01', '\u0120advance-01', '\u0120mainstream-02', '\u0120diagnose-01', '\u0120pass-05',\n                      '\u0120uphold-01', '\u0120halt-01', '\u0120hinder-01', '\u0120befriend-01', '\u0120convene-01', '\u0120awe-01', '\u0120applaud-01',\n                      '\u0120modernize-01', '\u0120integrate-01', '\u0120execute-02', '\u0120wound-01', '\u0120prostitute-01', '\u0120exercise-01',\n                      '\u0120bind-01', '\u0120photograph-01', '\u0120fascinate-01', '\u0120reward-01', '\u0120clean-up-02', '\u0120repeal-01',\n                      '\u0120twist-01', '\u0120model-01', '\u0120mandate-01', '\u0120conspire-01', '\u0120tear-01', '\u0120brutal-02', '\u0120charge-08',\n                      '\u0120dry-08', '\u0120wow-01', '\u0120bank-01', '\u0120fuck-up-02', '\u0120stand-up-07', '\u0120portray-01', '\u0120nationalize-01',\n                      '\u0120liberate-01', '\u0120exempt-01', '\u0120defy-01', '\u0120shout-01', '\u0120devastate-01', '\u0120hijack-01',\n                      '\u0120acknowledge-01', '\u0120compromise-02', '\u0120consist-01', '\u0120coach-01', '\u0120intense-02', '\u0120drag-01',\n                      '\u0120minor-01', '\u0120fulfill-01', '\u0120clear-01', '\u0120deceive-01', '\u0120shake-01', '\u0120cold-01', '\u0120align-01',\n                      '\u0120supervise-01', '\u0120internal-02', '\u0120gift-01', '\u0120struggle-01', '\u0120cast-01', '\u0120feature-01',\n                      '\u0120harsh-02', '\u0120emerge-01', '\u0120follow-04', '\u0120cut-off-04', '\u0120mistake-01', '\u0120locate-01', '\u0120slow-01',\n                      '\u0120accelerate-01', '\u0120cover-02', '\u0120soft-02', '\u0120identical-01', '\u0120sail-01', '\u0120jump-03',\n                      '\u0120facilitate-01', '\u0120excessive-02', '\u0120alter-01', '\u0120escalate-01', '\u0120mad-04', '\u0120kid-01', '\u0120float-01',\n                      '\u0120mess-up-02', '\u0120kidnap-01', '\u0120bore-02', '\u0120clean-01', '\u0120forgive-01', '\u0120go-through-20', '\u0120care-04',\n                      '\u0120meet-up-04', '\u0120moisturize-01', '\u0120highlight-01', '\u0120dislike-01', '\u0120boom-02', '\u0120blow-up-06',\n                      '\u0120appeal-02', '\u0120adhere-02', '\u0120contradict-01', '\u0120leave-12', '\u0120dialogue-01', '\u0120push-04',\n                      '\u0120contaminate-01', '\u0120finalize-01', '\u0120tape-02', '\u0120patrol-01', '\u0120incite-01', '\u0120renounce-01',\n                      '\u0120hallucinate-01', '\u0120undertake-01', '\u0120average-03', '\u0120compel-01', '\u0120struggle-02', '\u0120go-12',\n                      '\u0120trap-01', '\u0120quiet-04', '\u0120convey-01', '\u0120open-02', '\u0120clothe-01', '\u0120exclusive-02', '\u0120gather-03',\n                      '\u0120extensive-03', '\u0120approach-01', '\u0120manipulate-02', '\u0120infringe-01', '\u0120ruin-01', '\u0120strive-01',\n                      '\u0120productive-03', '\u0120explore-01', '\u0120inhabit-01', '\u0120press-01', '\u0120forbid-01', '\u0120hit-02',\n                      '\u0120abolish-01', '\u0120impress-01', '\u0120prospect-02', '\u0120google-01', '\u0120sink-01', '\u0120resign-01',\n                      '\u0120pull-out-02', '\u0120station-01', '\u0120center-02', '\u0120industrialize-01', '\u0120counsel-01', '\u0120propel-01',\n                      '\u0120smell-01', '\u0120moderate-03', '\u0120presume-01', '\u0120run-09', '\u0120keep-up-10', '\u0120deal-03', '\u0120apprehend-01',\n                      '\u0120sick-02', '\u0120smell-02', '\u0120have-11', '\u0120frustrate-01', '\u0120catch-01', '\u0120impression-03',\n                      '\u0120specify-01', '\u0120employ-02', '\u0120thankful-02', '\u0120man-01', '\u0120prioritize-01', '\u0120attribute-01',\n                      '\u0120project-01', '\u0120parrot-01', '\u0120bitch-01', '\u0120stand-04', '\u0120voice-01', '\u0120preserve-01',\n                      '\u0120publicize-01', '\u0120exhibit-01', '\u0120undergo-28', '\u0120help-02', '\u0120bankrupt-01', '\u0120flood-01',\n                      '\u0120precede-01', '\u0120reinforce-01', '\u0120task-01', '\u0120type-03', '\u0120transform-01', '\u0120despair-01',\n                      '\u0120chase-01', '\u0120spread-01', '\u0120appall-01', '\u0120restrain-01', '\u0120terrify-01', '\u0120fool-01', '\u0120aspire-01',\n                      '\u0120warm-07', '\u0120bring-up-08', '\u0120bleed-01', '\u0120depress-01', '\u0120care-02', '\u0120alert-01', '\u0120wonder-02',\n                      '\u0120drop-out-04', '\u0120spoil-01', '\u0120stink-01', '\u0120drug-01', '\u0120overturn-01', '\u0120heat-01', '\u0120merge-01',\n                      '\u0120peak-01', '\u0120set-01', '\u0120solid-02', '\u0120interact-01', '\u0120throw-out-06', '\u0120holiday-01', '\u0120refine-01',\n                      '\u0120allow-02', '\u0120sign-up-03', '\u0120bribe-01', '\u0120appease-01', '\u0120stress-02', '\u0120fine-01', '\u0120minor-02',\n                      '\u0120mine-01', '\u0120love-02', '\u0120network-01', '\u0120deposit-01', '\u0120store-01', '\u0120extract-01',\n                      '\u0120interrogate-01', '\u0120turn-out-11', '\u0120impregnate-01', '\u0120fake-02', '\u0120whore-01', '\u0120conceal-01',\n                      '\u0120fire-03', '\u0120lean-01', '\u0120harmful-02', '\u0120out-05', '\u0120fall-07', '\u0120dodge-01', '\u0120orient-01',\n                      '\u0120brand-01', '\u0120social-03', '\u0120cut-03', '\u0120cap-01', '\u0120overpay-01', '\u0120bridge-01', '\u0120collaborate-01',\n                      '\u0120address-03', '\u0120divert-01', '\u0120pull-09', '\u0120revise-01', '\u0120molest-01', '\u0120extradite-01',\n                      '\u0120dismiss-02', '\u0120reprocess-01', '\u0120accumulate-01', '\u0120occasion-02', '\u0120obstruct-01',\n                      '\u0120break-down-12', '\u0120rumor-01', '\u0120firm-03', '\u0120settle-03', '\u0120order-03', '\u0120stipulate-01',\n                      '\u0120audit-01', '\u0120enact-01', '\u0120celebrate-02', '\u0120bargain-01', '\u0120succeed-03', '\u0120inject-01',\n                      '\u0120excite-01', '\u0120greet-01', '\u0120black-07', '\u0120terminate-01', '\u0120descend-01', '\u0120emerge-02', '\u0120wreck-01',\n                      '\u0120absorb-01', '\u0120blow-01', '\u0120fine-03', '\u0120circulate-01', '\u0120tight-05', '\u0120offense-02', '\u0120activate-01',\n                      '\u0120secure-01', '\u0120pass-by-17', '\u0120bash-01', '\u0120prop-up-01', '\u0120count-04', '\u0120slap-01', '\u0120bring-down-03',\n                      '\u0120amuse-01', '\u0120film-01', '\u0120introduce-01', '\u0120designate-01', '\u0120hang-01', '\u0120wave-04',\n                      '\u0120privilege-01', '\u0120take-02', '\u0120cycle-02', '\u0120cancel-01', '\u0120buy-05', '\u0120sweep-01', '\u0120help-out-03',\n                      '\u0120left-20', '\u0120suit-01', '\u0120enslave-01', '\u0120rest-01', '\u0120ambush-01', '\u0120mean-04', '\u0120distract-01',\n                      '\u0120match-03', '\u0120warrant-01', '\u0120disguise-01', '\u0120make-up-07', '\u0120party-01', '\u0120close-11', '\u0120fall-10',\n                      '\u0120pump-01', '\u0120resort-01', '\u0120get-back-10', '\u0120regain-01', '\u0120lose-01', '\u0120err-01', '\u0120run-out-05',\n                      '\u0120that-is-it-00', '\u0120aggravate-01', '\u0120loot-01', '\u0120happen-02', '\u0120screw-02', '\u0120make-it-14',\n                      '\u0120pick-up-04', '\u0120refer-02', '\u0120break-13', '\u0120update-01', '\u0120shine-01', '\u0120congratulate-01',\n                      '\u0120pilot-01', '\u0120disgrace-01', '\u0120fabricate-01', '\u0120sicken-01', '\u0120criticism-04', '\u0120preach-01',\n                      '\u0120deport-01', '\u0120deal-02', '\u0120inflict-01', '\u0120gain-01', '\u0120resume-01', '\u0120outlaw-01', '\u0120shoot-down-05',\n                      '\u0120partition-01', '\u0120address-01', '\u0120envy-01', '\u0120break-02', '\u0120speak-out-03', '\u0120broaden-01',\n                      '\u0120stress-01', '\u0120infiltrate-01', '\u0120flat-06', '\u0120impeach-01', '\u0120transgress-01', '\u0120pardon-01',\n                      '\u0120uncover-01', '\u0120comprise-01', '\u0120reconstruct-01', '\u0120libel-01', '\u0120hand-01', '\u0120hint-01',\n                      '\u0120encourage-02', '\u0120prevail-02', '\u0120brave-02', '\u0120foresee-01', '\u0120concede-01', '\u0120deteriorate-01',\n                      '\u0120topple-01', '\u0120mobile-02', '\u0120panic-01', '\u0120misunderstand-01', '\u0120tire-01', '\u0120enthusiastic-03',\n                      '\u0120exercise-02', '\u0120persist-01', '\u0120inferior-01', '\u0120brilliant-01', '\u0120build-02', '\u0120scream-01',\n                      '\u0120anticipate-01', '\u0120out-03', '\u0120ration-01', '\u0120count-02', '\u0120consistent-01', '\u0120await-01',\n                      '\u0120school-01', '\u0120rent-01', '\u0120arise-02', '\u0120appeal-03', '\u0120helpful-04', '\u0120see-03', '\u0120lock-01',\n                      '\u0120stereotype-01', '\u0120join-in-05', '\u0120screw-up-01', '\u0120withhold-01', '\u0120moderate-01', '\u0120affiliate-01',\n                      '\u0120waive-01', '\u0120suck-01', '\u0120golf-01', '\u0120turn-out-17', '\u0120put-up-11', '\u0120keep-up-05', '\u0120straight-05',\n                      '\u0120dress-01', '\u0120dig-01', '\u0120plead-02', '\u0120lecture-01', '\u0120go-09', '\u0120pervert-01', '\u0120cry-01',\n                      '\u0120mitigate-01', '\u0120substitute-01', '\u0120send-02', '\u0120down-01', '\u0120westernize-01', '\u0120color-01',\n                      '\u0120refer-03', '\u0120persecute-01', '\u0120scheme-01', '\u0120reactionary-02', '\u0120subscribe-01', '\u0120shield-01',\n                      '\u0120exile-01', '\u0120detonate-01', '\u0120stall-01', '\u0120broker-01', '\u0120calculate-01', '\u0120narrow-02',\n                      '\u0120stock-01', '\u0120turn-down-05', '\u0120parole-01', '\u0120join-04', '\u0120institute-01', '\u0120disprove-01',\n                      '\u0120pass-20', '\u0120spew-01', '\u0120bid-03', '\u0120wage-01', '\u0120sample-01', '\u0120retail-01', '\u0120ratify-01',\n                      '\u0120spank-01', '\u0120dispatch-01', '\u0120harvest-01', '\u0120rot-01', '\u0120delude-01', '\u0120climb-01', '\u0120frighten-01',\n                      '\u0120yell-01', '\u0120coerce-01', '\u0120scary-03', '\u0120stretch-01', '\u0120destabilize-01', '\u0120blood-02',\n                      '\u0120confine-01', '\u0120outrageous-02', '\u0120beg-01', '\u0120wield-01', '\u0120scrap-01', '\u0120privatize-01', '\u0120cure-01',\n                      '\u0120mature-02', '\u0120coexist-01', '\u0120assert-02', '\u0120get-along-18', '\u0120reunify-01', '\u0120look-forward-03',\n                      '\u0120number-01', '\u0120trash-01', '\u0120run-04', '\u0120give-up-08', '\u0120bright-02', '\u0120out-01', '\u0120heal-01',\n                      '\u0120massacre-01', '\u0120tackle-01', '\u0120stake-01', '\u0120open-09', '\u0120know-04', '\u0120correspond-02',\n                      '\u0120disregard-01', '\u0120alienate-01', '\u0120insure-01', '\u0120disapprove-01', '\u0120drain-01', '\u0120deflect-01',\n                      '\u0120exit-01', '\u0120vacation-01', '\u0120cook-01', '\u0120adapt-01', '\u0120dissolve-01', '\u0120lift-01', '\u0120close-down-04',\n                      '\u0120come-down-23', '\u0120bully-01', '\u0120denounce-01', '\u0120stab-01', '\u0120expel-01', '\u0120abstain-01',\n                      '\u0120cut-out-06', '\u0120swallow-01', '\u0120come-in-07', '\u0120step-in-02', '\u0120seek-out-02', '\u0120pace-01', '\u0120wed-01',\n                      '\u0120go-on-25', '\u0120save-03', '\u0120come-up-13', '\u0120sort-out-02', '\u0120tattoo-01', '\u0120leave-out-03', '\u0120kiss-01',\n                      '\u0120chance-01', '\u0120prolong-01', '\u0120troll-01', '\u0120concentrate-01', '\u0120channel-01', '\u0120recreation-02',\n                      '\u0120center-01', '\u0120weaponize-01', '\u0120explicit-03', '\u0120draft-02', '\u0120pose-02', '\u0120crush-01',\n                      '\u0120discredit-01', '\u0120further-01', '\u0120dedicate-01', '\u0120sit-down-02', '\u0120leave-10', '\u0120forge-02',\n                      '\u0120censor-01', '\u0120parade-02', '\u0120paint-02', '\u0120catch-03', '\u0120remortgage-01', '\u0120slow-down-03',\n                      '\u0120admit-02', '\u0120break-19', '\u0120counterfeit-01', '\u0120run-10', '\u0120upgrade-01', '\u0120deduct-01',\n                      '\u0120confess-01', '\u0120decline-02', '\u0120bar-01', '\u0120brief-01', '\u0120conduct-02', '\u0120lynch-01', '\u0120acquit-01',\n                      '\u0120hyperlink-01', '\u0120light-04', '\u0120concrete-02', '\u0120reach-02', '\u0120march-01', '\u0120purport-01',\n                      '\u0120call-on-05', '\u0120paddle-01', '\u0120filter-02', '\u0120strip-01', '\u0120compose-01', '\u0120erupt-01', '\u0120wipe-01',\n                      '\u0120trace-02', '\u0120despise-01', '\u0120minimize-01', '\u0120neglect-01', '\u0120loyal-01', '\u0120slip-01', '\u0120revive-01',\n                      '\u0120work-07', '\u0120beat-up-05', '\u0120determined-02', '\u0120pass-07', '\u0120prescribe-02', '\u0120fuss-01',\n                      '\u0120demolish-01', '\u0120avail-01', '\u0120put-in-05', '\u0120lease-01', '\u0120embrace-01', '\u0120merit-01',\n                      '\u0120intensify-01', '\u0120hearing-02', '\u0120weaken-01', '\u0120colonize-01', '\u0120offset-01', '\u0120gather-01',\n                      '\u0120take-off-07', '\u0120bright-03', '\u0120extend-02', '\u0120get-30', '\u0120preexist-01', '\u0120snow-01', '\u0120strike-02',\n                      '\u0120gross-06', '\u0120diminish-01', '\u0120prejudice-01', '\u0120rage-02', '\u0120notify-01', '\u0120contest-02', '\u0120hype-01',\n                      '\u0120revisit-01', '\u0120dark-02', '\u0120stand-08', '\u0120certify-01', '\u0120oversee-01', '\u0120name-02', '\u0120lock-up-03',\n                      '\u0120know-03', '\u0120minimal-02', '\u0120tell-02', '\u0120rotate-01', '\u0120operate-02', '\u0120fat-03', '\u0120indulge-01',\n                      '\u0120feel-06', '\u0120set-08', '\u0120surpass-01', '\u0120pull-06', '\u0120get-06', '\u0120camp-02', '\u0120gut-01', '\u0120chair-01',\n                      '\u0120qualify-01', '\u0120spare-01', '\u0120blunt-02', '\u0120proceed-01', '\u0120dump-01', '\u0120reckon-01', '\u0120pierce-01',\n                      '\u0120melt-01', '\u0120feel-05', '\u0120stand-03', '\u0120elaborate-01', '\u0120reach-03', '\u0120spark-01', '\u0120coincide-01',\n                      '\u0120slander-01', '\u0120join-up-02', '\u0120shame-01', '\u0120board-01', '\u0120rule-out-02', '\u0120blockade-01',\n                      '\u0120incinerate-01', '\u0120derive-01', '\u0120get-by-17', '\u0120characterize-01', '\u0120stockpile-01', '\u0120persuade-01',\n                      '\u0120decapitate-01', '\u0120run-08', '\u0120pack-01', '\u0120bust-01', '\u0120police-01', '\u0120trick-01', '\u0120blast-05',\n                      '\u0120treat-04', '\u0120run-off-24', '\u0120apprentice-01', '\u0120dispose-01', '\u0120inhibit-01', '\u0120wire-01', '\u0120top-01',\n                      '\u0120hand-over-02', '\u0120know-06', '\u0120abet-01', '\u0120catch-up-04', '\u0120sleep-02', '\u0120slam-02', '\u0120breed-01',\n                      '\u0120contend-02', '\u0120perjure-01', '\u0120manipulate-01', '\u0120probe-01', '\u0120trend-01', '\u0120tighten-01',\n                      '\u0120boycott-01', '\u0120table-01', '\u0120indoctrinate-01', '\u0120safeguard-01', '\u0120evacuate-01', '\u0120interdict-01',\n                      '\u0120petition-01', '\u0120formulate-01', '\u0120partake-01', '\u0120pass-04', '\u0120override-01', '\u0120emit-01',\n                      '\u0120characteristic-02', '\u0120timely-03', '\u0120stun-01', '\u0120crumble-01', '\u0120maximize-01', '\u0120pass-away-16',\n                      '\u0120run-07', '\u0120smile-01', '\u0120inquire-01', '\u0120lag-01', '\u0120live-up-04', '\u0120distance-01', '\u0120cold-02',\n                      '\u0120deep-03', '\u0120relax-01', '\u0120ill-02', '\u0120signify-01', '\u0120hold-back-07', '\u0120transplant-01', '\u0120smoke-01',\n                      '\u0120curb-01', '\u0120delegate-01', '\u0120seal-01', '\u0120lure-01', '\u0120intimate-02', '\u0120fresh-04', '\u0120seat-01',\n                      '\u0120move-03', '\u0120keep-03', '\u0120outweigh-01', '\u0120revere-01', '\u0120clone-01', '\u0120enlist-01', '\u0120click-01',\n                      '\u0120empty-02', '\u0120fire-04', '\u0120contend-01', '\u0120abide-01', '\u0120craft-01', '\u0120tip-05', '\u0120wrap-01',\n                      '\u0120bite-01', '\u0120toss-01', '\u0120polite-01', '\u0120desirable-02', '\u0120defuse-01', '\u0120thrill-01', '\u0120produce-02',\n                      '\u0120oblige-02', '\u0120date-02', '\u0120alternate-01', '\u0120get-on-21', '\u0120ramble-02', '\u0120hurt-02', '\u0120distant-02',\n                      '\u0120hot-05', '\u0120pale-03', '\u0120proclaim-01', '\u0120class-01', '\u0120come-across-21', '\u0120sneak-01', '\u0120erode-01',\n                      '\u0120champion-01', '\u0120neutral-02', '\u0120alien-01', '\u0120grieve-01', '\u0120swear-01', '\u0120go-21',\n                      '\u0120underestimate-01', '\u0120addictive-02', '\u0120propagate-01', '\u0120last-04', '\u0120commence-01', '\u0120air-01',\n                      '\u0120mark-02', '\u0120accommodate-01', '\u0120demonize-01', '\u0120mock-01', '\u0120nuke-01', '\u0120swell-01', '\u0120brag-01',\n                      '\u0120assert-03', '\u0120disrespect-01', '\u0120work-12', '\u0120remarkable-02', '\u0120pool-01', '\u0120paint-03', '\u0120pour-01',\n                      '\u0120decommission-01', '\u0120amplify-01', '\u0120mad-02', '\u0120correlate-01', '\u0120automate-01', '\u0120money-01',\n                      '\u0120content-02', '\u0120storm-01', '\u0120thrive-01', '\u0120liable-01', '\u0120hopeful-02', '\u0120expire-01', '\u0120work-06',\n                      '\u0120disperse-01', '\u0120lay-04', '\u0120fall-apart-09', '\u0120terror-02', '\u0120philander-01', '\u0120scrutinize-01',\n                      '\u0120fathom-01', '\u0120make-up-08', '\u0120humiliate-01', '\u0120charge-06', '\u0120natural-02', '\u0120follow-up-03',\n                      '\u0120bend-01', '\u0120grade-01', '\u0120enter-02', '\u0120pend-01', '\u0120prey-01', '\u0120mediate-01', '\u0120conclude-02',\n                      '\u0120mask-01', '\u0120reactivate-01', '\u0120evolve-02', '\u0120restart-01', '\u0120encrypt-01', '\u0120get-through-12',\n                      '\u0120grow-02', '\u0120bestow-01', '\u0120put-out-10', '\u0120displace-01', '\u0120count-03', '\u0120stabilize-01',\n                      '\u0120embezzle-01', '\u0120pass-on-09', '\u0120form-02', '\u0120root-02', '\u0120trample-01', '\u0120make-out-23',\n                      '\u0120fit-in-02', '\u0120hospitalize-01', '\u0120cut-down-11', '\u0120constrain-01', '\u0120clash-01', '\u0120consolidate-01',\n                      '\u0120meddle-01', '\u0120reproduce-01', '\u0120clever-01', '\u0120diversify-01', '\u0120postpone-01', '\u0120structure-01',\n                      '\u0120narrow-01', '\u0120incur-01', '\u0120draw-up-03', '\u0120drive-04', '\u0120pin-01', '\u0120delight-01', '\u0120put-on-08',\n                      '\u0120coverage-06', '\u0120bring-about-05', '\u0120stir-up-04', '\u0120let-down-04', '\u0120sigh-02', '\u0120space-01',\n                      '\u0120cheat-02', '\u0120lessen-01', '\u0120render-02', '\u0120render-01', '\u0120menace-01', '\u0120prevail-01', '\u0120reclaim-01',\n                      '\u0120puzzle-01', '\u0120hesitate-01', '\u0120go-23', '\u0120charm-01', '\u0120turn-over-12', '\u0120wander-01',\n                      '\u0120renovate-01', '\u0120package-01', '\u0120headquarter-01', '\u0120line-01', '\u0120straight-06', '\u0120park-01',\n                      '\u0120turn-on-13', '\u0120arbitrary-02', '\u0120conceive-01', '\u0120exert-01', '\u0120spell-01', '\u0120dye-01', '\u0120tune-01',\n                      '\u0120rip-01', '\u0120garner-01', '\u0120sick-04', '\u0120shove-01', '\u0120wave-01', '\u0120rust-01', '\u0120kneel-01',\n                      '\u0120celebrate-01', '\u0120misrepresent-01', '\u0120incarcerate-01', '\u0120awake-03', '\u0120up-01', '\u0120slip-02',\n                      '\u0120concentrate-02', '\u0120round-05', '\u0120loose-04', '\u0120cripple-01', '\u0120part-01', '\u0120hoard-01', '\u0120chain-01',\n                      '\u0120tricky-02', '\u0120hook-up-02', '\u0120type-01', '\u0120glance-01', '\u0120prize-01', '\u0120transmit-01', '\u0120hold-03',\n                      '\u0120surge-01', '\u0120headline-01', '\u0120vote-02', '\u0120draw-01', '\u0120text-01', '\u0120shower-01', '\u0120calm-down-02',\n                      '\u0120feed-up-03', '\u0120slide-01', '\u0120go-down-27', '\u0120forward-01', '\u0120project-02', '\u0120empower-01',\n                      '\u0120mind-04', '\u0120pass-02', '\u0120neutralize-01', '\u0120repress-01', '\u0120serve-04', '\u0120eye-01',\n                      '\u0120discriminate-01', '\u0120overlook-01', '\u0120top-02', '\u0120mobilize-01', '\u0120start-out-05', '\u0120punishable-02',\n                      '\u0120underlie-01', '\u0120penetrate-01', '\u0120grind-01', '\u0120jump-01', '\u0120pertain-01', '\u0120incline-01',\n                      '\u0120humble-01', '\u0120moderate-02', '\u0120meaningful-05', '\u0120mislead-01', '\u0120finish-07', '\u0120disgruntle-01',\n                      '\u0120turn-up-15', '\u0120knock-01', '\u0120take-03', '\u0120lunch-01', '\u0120add-03', '\u0120commend-01', '\u0120patient-01',\n                      '\u0120attain-01', '\u0120hike-02', '\u0120lurk-01', '\u0120be-02', '\u0120blackmail-01', '\u0120dubious-02', '\u0120entrench-01',\n                      '\u0120get-off-23', '\u0120flame-01', '\u0120stand-02', '\u0120survive-02', '\u0120afford-02', '\u0120live-02', '\u0120moan-01',\n                      '\u0120portion-01', '\u0120slash-02', '\u0120break-through-22', '\u0120plague-01', '\u0120blunt-01', '\u0120abominable-02',\n                      '\u0120honorable-03', '\u0120related-04', '\u0120deprive-01', '\u0120decay-01', '\u0120distress-01', '\u0120redistribute-01',\n                      '\u0120foreclose-01', '\u0120warm-06', '\u0120jealous-02', '\u0120cohere-01', '\u0120paste-01', '\u0120prompt-01',\n                      '\u0120curtail-01', '\u0120track-down-02', '\u0120pity-01', '\u0120ticket-02', '\u0120transition-01', '\u0120burst-02',\n                      '\u0120broke-23', '\u0120rewrite-01', '\u0120deliberate-01', '\u0120disclose-01', '\u0120situate-01', '\u0120reiterate-01',\n                      '\u0120profess-01', '\u0120babble-01', '\u0120lift-02', '\u0120declassify-01', '\u0120remand-01', '\u0120reconcile-01',\n                      '\u0120assemble-01', '\u0120extort-01', '\u0120corroborate-01', '\u0120snip-01', '\u0120normalize-01', '\u0120close-03',\n                      '\u0120remit-01', '\u0120sweep-06', '\u0120breach-01', '\u0120behead-01', '\u0120simulate-01', '\u0120astonish-01',\n                      '\u0120deviate-01', '\u0120smear-02', '\u0120give-away-02', '\u0120differentiate-01', '\u0120intersect-01', '\u0120rectify-01',\n                      '\u0120lose-out-06', '\u0120telephone-01', '\u0120revolutionary-04', '\u0120blow-14', '\u0120exaggerate-01', '\u0120soar-01',\n                      '\u0120content-01', '\u0120preside-01', '\u0120check-07', '\u0120refrain-01', '\u0120crack-02', '\u0120disintegrate-01',\n                      '\u0120exterminate-01', '\u0120ridicule-01', '\u0120obey-01', '\u0120bundle-01', '\u0120compound-01', '\u0120wine-01',\n                      '\u0120dine-01', '\u0120resent-01', '\u0120jeopardize-01', '\u0120usher-in-01', '\u0120crowd-01', '\u0120elevate-01',\n                      '\u0120tear-down-05', '\u0120resolve-02', '\u0120earnest-01', '\u0120irritate-01', '\u0120green-02', '\u0120heed-01',\n                      '\u0120play-10', '\u0120spread-out-04', '\u0120cruise-01', '\u0120cater-01', '\u0120stay-on-02', '\u0120stick-around-03',\n                      '\u0120call-13', '\u0120bicker-01', '\u0120curse-02', '\u0120open-07', '\u0120run-up-19', '\u0120trump-01', '\u0120happy-02',\n                      '\u0120redeem-01', '\u0120strike-04', '\u0120bring-on-06', '\u0120enlighten-01', '\u0120gray-02', '\u0120note-02', '\u0120shred-01',\n                      '\u0120gas-03', '\u0120levy-01', '\u0120turn-18', '\u0120level-04', '\u0120bow-01', '\u0120turn-14', '\u0120rehabilitate-01',\n                      '\u0120couple-01', '\u0120dent-01', '\u0120cautious-02', '\u0120bust-02', '\u0120shut-01', '\u0120flip-01', '\u0120validate-01',\n                      '\u0120kill-03', '\u0120hot-04', '\u0120chat-01', '\u0120curious-02', '\u0120lump-01', '\u0120exacerbate-01', '\u0120sneaky-03',\n                      '\u0120conviction-02', '\u0120proceeding-02', '\u0120reorganize-01', '\u0120fit-05', '\u0120see-05', '\u0120acquaint-01',\n                      '\u0120vile-02', '\u0120zap-01', '\u0120uniform-01', '\u0120replicate-01', '\u0120intent-02', '\u0120grip-01', '\u0120swear-02',\n                      '\u0120decry-01', '\u0120segregate-01', '\u0120spur-01', '\u0120storm-02', '\u0120cap-02', '\u0120slant-01', '\u0120span-01',\n                      '\u0120cut-back-05', '\u0120fledge-01', '\u0120foster-01', '\u0120gripe-01', '\u0120quest-01', '\u0120punch-01',\n                      '\u0120deregulate-01', '\u0120loathe-01', '\u0120imitate-01', '\u0120hang-out-06', '\u0120baffle-01', '\u0120suck-up-04',\n                      '\u0120tempt-01', '\u0120condone-01', '\u0120assemble-02', '\u0120oust-01', '\u0120vent-01', '\u0120spout-01', '\u0120sound-02',\n                      '\u0120evade-01', '\u0120endure-01', '\u0120invoke-01', '\u0120devalue-01', '\u0120pose-01', '\u0120bear-06', '\u0120hypothesize-01',\n                      '\u0120spot-01', '\u0120discount-02', '\u0120rail-01', '\u0120haul-01', '\u0120gauge-01', '\u0120copyright-01', '\u0120give-in-09',\n                      '\u0120impede-01', '\u0120blast-01', '\u0120true-02', '\u0120beware-01', '\u0120restore-02', '\u0120negative-05', '\u0120steady-01',\n                      '\u0120fluctuate-01', '\u0120date-01', '\u0120bathe-01', '\u0120go-22', '\u0120restructure-01', '\u0120pile-01', '\u0120spin-01',\n                      '\u0120take-down-22', '\u0120bake-01', '\u0120triple-01', '\u0120downgrade-02', '\u0120ordain-01', '\u0120multiply-01',\n                      '\u0120skip-01', '\u0120incorporate-02', '\u0120settle-01', '\u0120pass-on-14', '\u0120creepy-04', '\u0120stuff-01',\n                      '\u0120line-up-02', '\u0120immune-02', '\u0120lust-01', '\u0120notable-04', '\u0120buy-into-04', '\u0120impair-01',\n                      '\u0120figure-04', '\u0120piss-01', '\u0120give-back-03', '\u0120boast-01', '\u0120lay-off-06', '\u0120dive-01', '\u0120commute-02',\n                      '\u0120racket-02', '\u0120dip-01', '\u0120rotate-02', '\u0120demagogue-01', '\u0120change-02', '\u0120barter-01', '\u0120alike-05',\n                      '\u0120bind-03', '\u0120whip-up-03', '\u0120manifest-01', '\u0120check-03', '\u0120yield-01', '\u0120slay-01', '\u0120tally-01',\n                      '\u0120get-through-13', '\u0120break-through-26', '\u0120relinquish-01', '\u0120reopen-01', '\u0120defame-01',\n                      '\u0120interrupt-01', '\u0120cast-03', '\u0120pattern-01', '\u0120dose-01', '\u0120reenter-01', '\u0120motivate-02',\n                      '\u0120standardize-01', '\u0120date-entity', '\u0120government-organization', '\u0120temporal-quantity',\n                      '\u0120amr-unknown', '\u0120multi-sentence', '\u0120political-party', '\u0120:compared-to', '\u0120monetary-quantity',\n                      '\u0120ordinal-entity', '\u0120religious-group', '\u0120percentage-entity', '\u0120world-region', '\u0120:consist',\n                      '\u0120url-entity', '\u0120political-movement', '\u0120et-cetera', '\u0120at-least', '\u0120mass-quantity',\n                      '\u0120have-org-role-91', '\u0120have-rel-role-91', '\u0120include-91', '\u0120have-concession-91',\n                      '\u0120have-condition-91', '\u0120be-located-at-91', '\u0120rate-entity-91', '\u0120instead-of-91', '\u0120hyperlink-91',\n                      '\u0120request-confirmation-91', '\u0120have-purpose-91', '\u0120be-temporally-at-91', '\u0120regardless-91',\n                      '\u0120have-polarity-91', '\u0120byline-91', '\u0120have-manner-91', '\u0120have-part-91', '\u0120have-quant-91',\n                      '\u0120publication-91', '\u0120be-from-91', '\u0120have-mod-91', '\u0120have-frequency-91', '\u0120score-on-scale-91',\n                      '\u0120have-li-91', '\u0120be-compared-to-91', '\u0120be-destined-for-91', '\u0120course-91', '\u0120have-subevent-91',\n                      '\u0120street-address-91', '\u0120have-extent-91', '\u0120statistical-test-91', '\u0120have-instrument-91',\n                      '\u0120have-name-91', '\u0120be-polite-91', '-00', '-01', '-02', '-03', '-04', '-05', '-06', '-07', '-08',\n                      '-09', '-10', '-11', '-12', '-13', '-14', '-15', '-16', '-17', '-18', '-19', '-20', '-21', '-22',\n                      '-23', '-24', '-25', '-26', '-27', '-28', '-29', '-20', '-31', '-32', '-33', '-34', '-35', '-36',\n                      '-37', '-38', '-39', '-40', '-41', '-42', '-43', '-44', '-45', '-46', '-47', '-48', '-49', '-50',\n                      '-51', '-52', '-53', '-54', '-55', '-56', '-57', '-58', '-59', '-60', '-61', '-62', '-63', '-64',\n                      '-65', '-66', '-67', '-68', '-69', '-70', '-71', '-72', '-73', '-74', '-75', '-76', '-77', '-78',\n                      '-79', '-80', '-81', '-82', '-83', '-84', '-85', '-86', '-87', '-88', '-89', '-90', '-91', '-92',\n                      '-93', '-94', '-95', '-96', '-97', '-98', '-of', '\u0120:op1', '\u0120:op2', '\u0120:op3', '\u0120:op4', '\u0120:op5',\n                      '\u0120:ARG0', '\u0120:ARG1', '\u0120:ARG2', '\u0120:ARG3', '\u0120:ARG4', '\u0120:ARG5', '\u0120:ARG6', '\u0120:ARG7', '\u0120:ARG8',\n                      '\u0120:ARG9', '\u0120:ARG10', '\u0120:ARG11', '\u0120:ARG12', '\u0120:ARG13', '\u0120:ARG14', '\u0120:ARG15', '\u0120:ARG16', '\u0120:ARG17',\n                      '\u0120:ARG18', '\u0120:ARG19', '\u0120:ARG20', '\u0120:accompanier', '\u0120:age', '\u0120:beneficiary', '\u0120:calendar',\n                      '\u0120:cause', '\u0120:century', '\u0120:concession', '\u0120:condition', '\u0120:conj-as-if', '\u0120:consist-of', '\u0120:cost',\n                      '\u0120:day', '\u0120:dayperiod', '\u0120:decade', '\u0120:degree', '\u0120:destination', '\u0120:direction', '\u0120:domain',\n                      '\u0120:duration', '\u0120:employed-by', '\u0120:era', '\u0120:example', '\u0120:extent', '\u0120:frequency', '\u0120:instrument',\n                      '\u0120:li', '\u0120:location', '\u0120:manner', '\u0120:meaning', '\u0120:medium', '\u0120:mod', '\u0120:mode', '\u0120:month', '\u0120:name',\n                      '\u0120:ord', '\u0120:part', '\u0120:path', '\u0120:polarity', '\u0120:polite', '\u0120:poss', '\u0120:purpose', '\u0120:quant',\n                      '\u0120:quarter', '\u0120:range', '\u0120:relation', '\u0120:role', '\u0120:scale', '\u0120:season', '\u0120:source', '\u0120:subevent',\n                      '\u0120:subset', '\u0120:superset', '\u0120:time', '\u0120:timezone', '\u0120:topic', '\u0120:unit', '\u0120:value', '\u0120:weekday',\n                      '\u0120:wiki', '\u0120:year', '\u0120:year2', '\u0120:snt0', '\u0120:snt1', '\u0120:snt2', '\u0120:snt3', '\u0120:snt4', '\u0120:snt5',\n                      '\u0120COUNTRY', '\u0120QUANTITY', '\u0120ORGANIZATION', '\u0120DATE_ATTRS', '\u0120NATIONALITY', '\u0120LOCATION', '\u0120ENTITY',\n                      '\u0120MISC', '\u0120ORDINAL_ENTITY', '\u0120IDEOLOGY', '\u0120RELIGION', '\u0120STATE_OR_PROVINCE', '\u0120CAUSE_OF_DEATH',\n                      '\u0120TITLE', '\u0120DATE', '\u0120NUMBER', '\u0120HANDLE', '\u0120SCORE_ENTITY', '\u0120DURATION', '\u0120ORDINAL', '\u0120MONEY',\n                      '\u0120CRIMINAL_CHARGE', '_1', '_2', '_3', '_4', '_2', '_5', '_6', '_7', '_8', '_9', '_10', '_11',\n                      '_12', '_13', '_14', '_15', '\u0120<pointer:0>', '\u0120<pointer:1>', '\u0120<pointer:2>', '\u0120<pointer:3>',\n                      '\u0120<pointer:4>', '\u0120<pointer:5>', '\u0120<pointer:6>', '\u0120<pointer:7>', '\u0120<pointer:8>', '\u0120<pointer:9>',\n                      '\u0120<pointer:10>', '\u0120<pointer:11>', '\u0120<pointer:12>', '\u0120<pointer:13>', '\u0120<pointer:14>',\n                      '\u0120<pointer:15>', '\u0120<pointer:16>', '\u0120<pointer:17>', '\u0120<pointer:18>', '\u0120<pointer:19>',\n                      '\u0120<pointer:20>', '\u0120<pointer:21>', '\u0120<pointer:22>', '\u0120<pointer:23>', '\u0120<pointer:24>',\n                      '\u0120<pointer:25>', '\u0120<pointer:26>', '\u0120<pointer:27>', '\u0120<pointer:28>', '\u0120<pointer:29>',\n                      '\u0120<pointer:30>', '\u0120<pointer:31>', '\u0120<pointer:32>', '\u0120<pointer:33>', '\u0120<pointer:34>',\n                      '\u0120<pointer:35>', '\u0120<pointer:36>', '\u0120<pointer:37>', '\u0120<pointer:38>', '\u0120<pointer:39>',\n                      '\u0120<pointer:40>', '\u0120<pointer:41>', '\u0120<pointer:42>', '\u0120<pointer:43>', '\u0120<pointer:44>',\n                      '\u0120<pointer:45>', '\u0120<pointer:46>', '\u0120<pointer:47>', '\u0120<pointer:48>', '\u0120<pointer:49>',\n                      '\u0120<pointer:50>', '\u0120<pointer:51>', '\u0120<pointer:52>', '\u0120<pointer:53>', '\u0120<pointer:54>',\n                      '\u0120<pointer:55>', '\u0120<pointer:56>', '\u0120<pointer:57>', '\u0120<pointer:58>', '\u0120<pointer:59>',\n                      '\u0120<pointer:60>', '\u0120<pointer:61>', '\u0120<pointer:62>', '\u0120<pointer:63>', '\u0120<pointer:64>',\n                      '\u0120<pointer:65>', '\u0120<pointer:66>', '\u0120<pointer:67>', '\u0120<pointer:68>', '\u0120<pointer:69>',\n                      '\u0120<pointer:70>', '\u0120<pointer:71>', '\u0120<pointer:72>', '\u0120<pointer:73>', '\u0120<pointer:74>',\n                      '\u0120<pointer:75>', '\u0120<pointer:76>', '\u0120<pointer:77>', '\u0120<pointer:78>', '\u0120<pointer:79>',\n                      '\u0120<pointer:80>', '\u0120<pointer:81>', '\u0120<pointer:82>', '\u0120<pointer:83>', '\u0120<pointer:84>',\n                      '\u0120<pointer:85>', '\u0120<pointer:86>', '\u0120<pointer:87>', '\u0120<pointer:88>', '\u0120<pointer:89>',\n                      '\u0120<pointer:90>', '\u0120<pointer:91>', '\u0120<pointer:92>', '\u0120<pointer:93>', '\u0120<pointer:94>',\n                      '\u0120<pointer:95>', '\u0120<pointer:96>', '\u0120<pointer:97>', '\u0120<pointer:98>', '\u0120<pointer:99>',\n                      '\u0120<pointer:100>', '\u0120<pointer:101>', '\u0120<pointer:102>', '\u0120<pointer:103>', '\u0120<pointer:104>',\n                      '\u0120<pointer:105>', '\u0120<pointer:106>', '\u0120<pointer:107>', '\u0120<pointer:108>', '\u0120<pointer:109>',\n                      '\u0120<pointer:110>', '\u0120<pointer:111>', '\u0120<pointer:112>', '\u0120<pointer:113>', '\u0120<pointer:114>',\n                      '\u0120<pointer:115>', '\u0120<pointer:116>', '\u0120<pointer:117>', '\u0120<pointer:118>', '\u0120<pointer:119>',\n                      '\u0120<pointer:120>', '\u0120<pointer:121>', '\u0120<pointer:122>', '\u0120<pointer:123>', '\u0120<pointer:124>',\n                      '\u0120<pointer:125>', '\u0120<pointer:126>', '\u0120<pointer:127>', '\u0120<pointer:128>', '\u0120<pointer:129>',\n                      '\u0120<pointer:130>', '\u0120<pointer:131>', '\u0120<pointer:132>', '\u0120<pointer:133>', '\u0120<pointer:134>',\n                      '\u0120<pointer:135>', '\u0120<pointer:136>', '\u0120<pointer:137>', '\u0120<pointer:138>', '\u0120<pointer:139>',\n                      '\u0120<pointer:140>', '\u0120<pointer:141>', '\u0120<pointer:142>', '\u0120<pointer:143>', '\u0120<pointer:144>',\n                      '\u0120<pointer:145>', '\u0120<pointer:146>', '\u0120<pointer:147>', '\u0120<pointer:148>', '\u0120<pointer:149>',\n                      '\u0120<pointer:150>', '\u0120<pointer>', '\u0120<stop>', '\u0120<lit>', '\u0120</lit>', '\u0120<backr:src:XXX>',\n                      '\u0120<backr:trg:XXX>', '<AMR>', '</AMR>']\nspecial_tokens = [itm.lstrip(\"\u0120\") for itm in raw_special_tokens]\n\nrecategorizations = [\n    \"\\u0120COUNTRY\",\n    \"\\u0120QUANTITY\",\n    \"\\u0120ORGANIZATION\",\n    \"\\u0120DATE_ATTRS\",\n    \"\\u0120NATIONALITY\",\n    \"\\u0120LOCATION\",\n    \"\\u0120ENTITY\",\n    \"\\u0120MISC\",\n    \"\\u0120ORDINAL_ENTITY\",\n    \"\\u0120IDEOLOGY\",\n    \"\\u0120RELIGION\",\n    \"\\u0120STATE_OR_PROVINCE\",\n    \"\\u0120CAUSE_OF_DEATH\",\n    \"\\u0120TITLE\",\n    \"\\u0120DATE\",\n    \"\\u0120NUMBER\",\n    \"\\u0120HANDLE\",\n    \"\\u0120SCORE_ENTITY\",\n    \"\\u0120DURATION\",\n    \"\\u0120ORDINAL\",\n    \"\\u0120MONEY\",\n    \"\\u0120CRIMINAL_CHARGE\",\n]\n\n# special_tokens = [\"<AMR>\", \"</AMR>\"]\n\narg_to_scheduler = {\n    \"linear\": get_linear_schedule_with_warmup,\n    \"cosine\": get_cosine_schedule_with_warmup,\n    \"cosine_w_restarts\": get_cosine_with_hard_restarts_schedule_with_warmup,\n    \"polynomial\": get_polynomial_decay_schedule_with_warmup,\n    \"constant\": get_constant_schedule_with_warmup,\n}\narg_to_scheduler_choices = sorted(arg_to_scheduler.keys())\narg_to_scheduler_metavar = \"{\" + \", \".join(arg_to_scheduler_choices) + \"}\"\n\nROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\narg_to_tokenizer = {\n    \"AutoTokenizer\": AutoTokenizer,\n    \"BartTokenizer\": BartTokenizer,\n    \"T5Tokenizer\": T5Tokenizer,\n}\narg_to_plm_model = {\n    \"AutoModelForSeq2SeqLM\": AutoModelForSeq2SeqLM,\n    \"BartForConditionalGeneration\": BartForConditionalGeneration,\n    \"T5Model\": T5Model,\n    \"T5ForConditionalGeneration\": T5ForConditionalGeneration,\n}\n", "hanlp/components/amr/amrbart/common/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-05 17:53\n", "hanlp/components/amr/seq2seq/optim.py": "# taken from\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                        buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt(\n                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n                                    N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n", "hanlp/components/amr/seq2seq/seq2seq_amr_parser.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-04-28 17:33\nimport datetime\nimport functools\nimport logging\nimport os\nfrom typing import Union, List, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import get_constant_schedule_with_warmup, T5ForConditionalGeneration\nfrom transformers.models.bart.modeling_bart import BartForConditionalGeneration\n\nfrom hanlp.common.dataset import SamplerBuilder, SortingSamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.amr.seq2seq.dataset.dataset import AMRDataset, dfs_linearize_tokenize\nfrom hanlp.components.amr.seq2seq.dataset.penman import AMRGraph\nfrom hanlp.components.amr.seq2seq.dataset.tokenization_bart import PENMANBartTokenizer\nfrom hanlp.components.amr.seq2seq.dataset.tokenization_t5 import PENMANT5Tokenizer\nfrom hanlp.components.amr.seq2seq.evaluation import write_predictions, compute_smatch\nfrom hanlp.components.amr.seq2seq.optim import RAdam\nfrom hanlp.layers.transformers.pt_imports import PretrainedConfig, AutoConfig_\nfrom hanlp.layers.transformers.resource import get_model_mirror, get_tokenizer_mirror\nfrom hanlp.metrics.amr.smatch_eval import smatch_eval\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import merge_locals_kwargs, reorder\n\n\nclass Seq2seq_AMR_Parser(TorchComponent):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._transformer_config: PretrainedConfig = None\n        self._tokenizer: PENMANBartTokenizer = None\n        self.model: BartForConditionalGeneration = None\n\n    def build_dataloader(self, data, batch_size,\n                         gradient_accumulation=1,\n                         shuffle=False,\n                         sampler_builder: SamplerBuilder = None,\n                         device=None,\n                         logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        dataset = self.build_dataset(data, not shuffle)\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        self.finalize_dataset(dataset, logger)\n        if isinstance(data, str):\n            dataset.purge_cache()\n            timer = CountdownTimer(len(dataset))\n            max_num_tokens = 0\n            # lc = Counter()\n            for each in dataset:\n                max_num_tokens = max(max_num_tokens, len(each['text_token_ids']))\n                # lc[len(each['text_token_ids'])] += 1\n                timer.log(f'Preprocessing and caching samples (longest sequence {max_num_tokens})'\n                          f'[blink][yellow]...[/yellow][/blink]')\n            # print(lc.most_common())\n            if self.vocabs.mutable:\n                self.vocabs.lock()\n                self.vocabs.summary(logger)\n\n        if not sampler_builder:\n            sampler_builder = SortingSamplerBuilder(batch_max_tokens=500)\n        sampler = sampler_builder.build([len(x['text_token_ids']) for x in dataset], shuffle,\n                                        gradient_accumulation if dataset.cache else 1)\n        return self._create_dataloader(dataset, batch_size, device, sampler, shuffle)\n\n    def _create_dataloader(self, dataset, batch_size, device, sampler, shuffle):\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler,\n                                     pad=self._get_pad_dict())\n\n    def _get_pad_dict(self):\n        return {'text_token_ids': self._transformer_config.pad_token_id,\n                'graph_token_ids': self._transformer_config.pad_token_id}\n\n    def finalize_dataset(self, dataset, logger: logging.Logger = None):\n        dataset.append_transform(functools.partial(dfs_linearize_tokenize, tokenizer=self._tokenizer,\n                                                   remove_space='chinese' in self.config.transformer))\n\n    def build_dataset(self, data, generate_idx):\n        dataset = AMRDataset(data, generate_idx=generate_idx)\n        return dataset\n\n    def collect_additional_tokens(self, additional_tokens, dataset):\n        pred_min = self.config.pred_min\n        frames = dataset.get_frames()\n        for token, freq in frames.items():\n            if freq >= pred_min:\n                additional_tokens.add(token)\n        for token, freq in dataset.get_roles().items():\n            additional_tokens.add(token)\n        additional_tokens.update(self.config.additional_tokens)\n\n    def build_tokenizer(self, additional_tokens) -> PENMANBartTokenizer:\n        transformer = self.config.transformer\n        if 't5-' in transformer:\n            cls = PENMANT5Tokenizer\n        elif 'bart-' in transformer:\n            cls = PENMANBartTokenizer\n        else:\n            raise NotImplemented(f'Unsupported transformer {transformer}')\n        transformer = get_tokenizer_mirror(transformer)\n        self._tokenizer = cls.from_pretrained(\n            transformer,\n            collapse_name_ops=self.config.collapse_name_ops,\n            use_pointer_tokens=self.config.use_pointer_tokens,\n            raw_graph=self.config.raw_graph,\n            additional_tokens=additional_tokens,\n            recategorization_tokens=self.config.recategorization_tokens,\n            config=self._transformer_config,\n        )\n        return self._tokenizer\n\n    def build_optimizer(self, trn, lr, epochs, gradient_accumulation, warmup_steps, weight_decay, **kwargs):\n        num_training_steps = len(trn) * epochs // gradient_accumulation\n        if isinstance(warmup_steps, float):\n            warmup_steps = int(num_training_steps * warmup_steps)\n        optimizer = RAdam(\n            self.model.parameters(),\n            lr=lr,\n            weight_decay=weight_decay)\n        scheduler = get_constant_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=warmup_steps)\n        return optimizer, scheduler\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def build_metric(self, **kwargs):\n        pass\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, dev_data=None, eval_after=None,\n                              **kwargs):\n        best_epoch, best_metric = 0, -1\n        if isinstance(eval_after, float):\n            eval_after = int(epochs * eval_after)\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,\n                                **self.config)\n            if epoch > eval_after:\n                dev_metric = self.evaluate_dataloader(dev, criterion, logger=logger, ratio_width=ratio_width,\n                                                      output=os.path.join(save_dir, 'dev.pred.txt'),\n                                                      input=dev_data, use_fast=True)\n            timer.update()\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if epoch > eval_after:\n                if dev_metric > best_metric:\n                    best_epoch, best_metric = epoch, dev_metric\n                    self.save_weights(save_dir)\n                    report += ' [red](saved)[/red]'\n                else:\n                    report += f' ({epoch - best_epoch})'\n                # if epoch - best_epoch >= patience:\n                #     report += ' early stop'\n            logger.info(report)\n            # if epoch - best_epoch >= patience:\n            #     break\n        if not best_epoch:\n            self.save_weights(save_dir)\n        elif best_epoch != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric} at epoch {best_epoch}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n        return best_metric\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger,\n                       history: History = None, gradient_accumulation=1, ratio_percentage=None, **kwargs):\n        optimizer, scheduler = optimizer\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        for batch in trn:\n            output_dict = self.feed_batch(batch)\n            loss = output_dict['loss']\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler)\n                timer.log(self.report_metrics(total_loss / (timer.current + 1)),\n                          ratio_percentage=ratio_percentage, logger=logger)\n            del loss\n            del output_dict\n        return total_loss / max(timer.total, 1)\n\n    def _step(self, optimizer, scheduler):\n        if self.config.grad_norm:\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm)\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n        optimizer.zero_grad()\n\n    def report_metrics(self, loss):\n        return f'loss: {loss:.4f}'\n\n    def feed_batch(self, batch):\n        input_ids, labels = batch['text_token_ids'], batch.get('graph_token_ids')\n        attention_mask = input_ids.ne(self.model.config.pad_token_id).to(torch.long)\n        if labels is not None:\n            decoder_input_ids = labels[:, :-1]\n            labels = labels[:, 1:].contiguous()\n        else:\n            decoder_input_ids = None\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids,\n                          labels=labels)\n\n    @torch.no_grad()\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, ratio_width=None,\n                            logger=None, input=None, use_fast=False,\n                            **kwargs):\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        graphs = []\n        orders = []\n        smatch = 0\n        for idx, batch in enumerate(data):\n            graphs_per_batch = self.predict_amrs(batch)\n            graphs_per_batch = [x[0] for x in graphs_per_batch]\n            # Copy meta data from gold graph\n            for gp, gg in zip(graphs_per_batch, batch['amr']):\n                metadata = gg.metadata.copy()\n                metadata['annotator'] = f'{self.config.transformer}-amr'\n                metadata['date'] = str(datetime.datetime.now())\n                if 'save-date' in metadata:\n                    del metadata['save-date']\n                gp.metadata = metadata\n            graphs.extend(graphs_per_batch)\n            orders.extend(batch[IDX])\n            if idx == timer.total - 1:\n                graphs = reorder(graphs, orders)\n                write_predictions(output, self._tokenizer, graphs)\n                try:\n                    if use_fast:\n                        smatch = compute_smatch(output, input)\n                    else:\n                        smatch = smatch_eval(output, input, use_fast=False)\n                except:\n                    pass\n                timer.log(smatch.cstr() if isinstance(smatch, MetricDict) else f'{smatch:.2%}', ratio_percentage=False,\n                          logger=logger)\n            else:\n                timer.log(ratio_percentage=False, logger=logger)\n\n        return smatch\n\n    def predict_amrs(self, batch, beam_size=1):\n        out = self._model_generate(batch, beam_size)\n        tokens = []\n        for i1 in range(0, out.size(0), beam_size):\n            tokens_same_source = []\n            tokens.append(tokens_same_source)\n            for i2 in range(i1, i1 + beam_size):\n                tokk = out[i2].tolist()\n                tokens_same_source.append(tokk)\n        tokens = [t for tt in tokens for t in tt]\n        graphs = []\n        tokenizer = self._tokenizer\n        for i1 in range(0, len(tokens), beam_size):\n            graphs_same_source = []\n            graphs.append(graphs_same_source)\n            for i2 in range(i1, i1 + beam_size):\n                tokk = tokens[i2]\n                graph, status, (lin, backr) = tokenizer.decode_amr(tokk, restore_name_ops=False)\n                graph.status = status\n                graph.nodes = lin\n                graph.backreferences = backr\n                graph.tokens = tokk\n                graphs_same_source.append(graph)\n            graphs_same_source[:] = \\\n                tuple(zip(*sorted(enumerate(graphs_same_source), key=lambda x: (x[1].status.value, x[0]))))[1]\n\n        return graphs\n\n    def _model_generate(self, batch, beam_size):\n        input_ids = batch['text_token_ids']\n        attention_mask = input_ids.ne(self.model.config.pad_token_id).to(torch.long)\n        out = self.model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=1024,\n            decoder_start_token_id=0,\n            num_beams=beam_size,\n            num_return_sequences=beam_size)\n        return out\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        # noinspection PyTypeChecker\n        transformer = self.config.transformer\n        cls = self._get_model_cls(transformer)\n        transformer = get_model_mirror(self.config.transformer)\n        model: cls = cls.from_pretrained(\n            transformer,\n            config=self._transformer_config) if training else cls(self._transformer_config)\n        if not training:\n            self.build_tokenizer(self.vocabs['additional_tokens'])\n        tokenizer = self._tokenizer\n        model.resize_token_embeddings(len(tokenizer.encoder))\n        if training:\n            self._init_new_embeddings(model if cls == T5ForConditionalGeneration else model.model, tokenizer)\n        return model\n\n    def _get_model_cls(self, transformer: str):\n        if 't5-' in transformer:\n            cls = T5ForConditionalGeneration\n        elif 'bart-' in transformer:\n            cls = BartForConditionalGeneration\n        else:\n            raise NotImplemented(f'Unsupported transformer {transformer}')\n        return cls\n\n    @staticmethod\n    def _init_new_embeddings(model, tokenizer):\n        modified = 0\n        encoder = tokenizer.encoder\n        for tok, idx in encoder.items():\n            tok = tok.lstrip(tokenizer.INIT)\n\n            if idx < tokenizer.old_enc_size:\n                continue\n\n            elif tok.startswith('<pointer:') and tok.endswith('>'):\n                tok_split = ['pointer', str(tok.split(':')[1].strip('>'))]\n\n            elif tok.startswith('<'):\n                continue\n\n            elif tok.startswith(':'):\n                if tok.startswith(':op'):\n                    tok_split = ['relation', 'operator', str(int(tok[3:]))]\n\n                elif tok.startswith(':snt'):\n                    tok_split = ['relation', 'sentence', str(int(tok[4:]))]\n\n                elif tok.startswith(':ARG'):\n                    tok_split = ['relation', 'argument', str(int(tok[4:]))]\n                else:\n                    tok_split = ['relation'] + tok.lstrip(':').split('-')\n            else:\n                tok_split = tok.split('-')\n\n            tok_split_ = tok_split\n            tok_split = []\n            for s in tok_split_:\n                s_ = s + tokenizer.INIT\n                if s_ in encoder:\n                    tok_split.append(s_)\n                else:\n                    tok_split.extend(tokenizer._tok_bpe(s))\n\n            vecs = []\n            for s in tok_split:\n                idx_split = encoder.get(s, -1)\n                if idx_split > -1:\n                    vec_split = model.encoder.embed_tokens.weight.data[idx_split].clone()\n                    vecs.append(vec_split)\n\n            if vecs:\n                vec = torch.stack(vecs, 0).mean(0)\n                noise = torch.empty_like(vec)\n                noise.uniform_(-0.1, +0.1)\n                model.encoder.embed_tokens.weight.data[idx] = vec + noise\n                modified += 1\n\n    def input_is_flat(self, data):\n        return isinstance(data, str)\n\n    def predict(self, data: Union[str, List[str]], beautiful_amr_graph=True, **kwargs):\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader([{'text': x} for x in data], **self.config, device=self.device)\n        orders = []\n        results = []\n        for batch in dataloader:\n            graphs = self.predict_amrs(batch)\n            graphs = [x[0] for x in graphs]\n            if beautiful_amr_graph:\n                graphs = [AMRGraph(x.triples, x.top, x.epidata, x.metadata) for x in graphs]\n            results.extend(graphs)\n            orders.extend(batch[IDX])\n        results = reorder(results, orders)\n        if flat:\n            results = results[0]\n        return results\n\n    def fit(self, trn_data, dev_data, save_dir, batch_size=32, epochs=30,\n            transformer='facebook/bart-base',\n            lr=5e-05,\n            grad_norm=2.5,\n            weight_decay=0.004,\n            warmup_steps=1,\n            dropout=0.25,\n            attention_dropout=0.0,\n            pred_min=5,\n            eval_after=0.5,\n            collapse_name_ops=False,\n            use_pointer_tokens=True,\n            raw_graph=False,\n            gradient_accumulation=1,\n            recategorization_tokens=(\n                    'PERSON', 'COUNTRY', 'QUANTITY', 'ORGANIZATION', 'DATE_ATTRS', 'NATIONALITY', 'LOCATION', 'ENTITY',\n                    'CITY',\n                    'MISC', 'ORDINAL_ENTITY', 'IDEOLOGY', 'RELIGION', 'STATE_OR_PROVINCE', 'URL', 'CAUSE_OF_DEATH', 'O',\n                    'TITLE', 'DATE', 'NUMBER', 'HANDLE', 'SCORE_ENTITY', 'DURATION', 'ORDINAL', 'MONEY', 'SET',\n                    'CRIMINAL_CHARGE', '_1', '_2', '_3', '_4', '_2', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12',\n                    '_13',\n                    '_14', '_15'),\n            additional_tokens=(\n                    'date-entity', 'government-organization', 'temporal-quantity', 'amr-unknown', 'multi-sentence',\n                    'political-party', 'monetary-quantity', 'ordinal-entity', 'religious-group', 'percentage-entity',\n                    'world-region', 'url-entity', 'political-movement', 'et-cetera', 'at-least', 'mass-quantity',\n                    'have-org-role-91', 'have-rel-role-91', 'include-91', 'have-concession-91', 'have-condition-91',\n                    'be-located-at-91', 'rate-entity-91', 'instead-of-91', 'hyperlink-91', 'request-confirmation-91',\n                    'have-purpose-91', 'be-temporally-at-91', 'regardless-91', 'have-polarity-91', 'byline-91',\n                    'have-manner-91', 'have-part-91', 'have-quant-91', 'publication-91', 'be-from-91', 'have-mod-91',\n                    'have-frequency-91', 'score-on-scale-91', 'have-li-91', 'be-compared-to-91', 'be-destined-for-91',\n                    'course-91', 'have-subevent-91', 'street-address-91', 'have-extent-91', 'statistical-test-91',\n                    'have-instrument-91', 'have-name-91', 'be-polite-91', '-00', '-01', '-02', '-03', '-04', '-05',\n                    '-06',\n                    '-07', '-08', '-09', '-10', '-11', '-12', '-13', '-14', '-15', '-16', '-17', '-18', '-19', '-20',\n                    '-21',\n                    '-22', '-23', '-24', '-25', '-26', '-27', '-28', '-29', '-20', '-31', '-32', '-33', '-34', '-35',\n                    '-36',\n                    '-37', '-38', '-39', '-40', '-41', '-42', '-43', '-44', '-45', '-46', '-47', '-48', '-49', '-50',\n                    '-51',\n                    '-52', '-53', '-54', '-55', '-56', '-57', '-58', '-59', '-60', '-61', '-62', '-63', '-64', '-65',\n                    '-66',\n                    '-67', '-68', '-69', '-70', '-71', '-72', '-73', '-74', '-75', '-76', '-77', '-78', '-79', '-80',\n                    '-81',\n                    '-82', '-83', '-84', '-85', '-86', '-87', '-88', '-89', '-90', '-91', '-92', '-93', '-94', '-95',\n                    '-96',\n                    '-97', '-98', '-of'),\n            devices=None,\n            logger=None,\n            seed=None,\n            finetune: Union[bool, str] = False,\n            eval_trn=True,\n            _device_placeholder=False,\n            **kwargs):\n        \"\"\"\n\n        Args:\n            trn_data:\n            dev_data:\n            save_dir:\n            batch_size:\n            epochs:\n            transformer:\n            lr:\n            grad_norm:\n            weight_decay:\n            warmup_steps:\n            dropout:\n            attention_dropout:\n            pred_min:\n            eval_after:\n            collapse_name_ops: ``True`` to merge name ops.\n            use_pointer_tokens: ``True`` to use pointer tokens to represent variables.\n            raw_graph: ``True`` to use the raw graph as input and skip all pre/post-processing steps.\n            gradient_accumulation:\n            recategorization_tokens: Tokens used in re-categorization. They will be added to tokenizer too but do not\n            put them into ``additional_tokens``.\n            additional_tokens: Tokens to be added to the tokenizer vocab.\n            devices:\n            logger:\n            seed:\n            finetune:\n            eval_trn:\n            _device_placeholder:\n            **kwargs:\n\n        Returns:\n\n        \"\"\"\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def on_config_ready(self, **kwargs):\n        super().on_config_ready(**kwargs)\n        config = AutoConfig_.from_pretrained(self.config.transformer)\n        config.output_past = False\n        config.no_repeat_ngram_size = 0\n        config.prefix = \" \"\n        # config.output_attentions = True\n        config.dropout = self.config.dropout\n        config.attention_dropout = self.config.attention_dropout\n        self._transformer_config = config\n\n    def evaluate(self, tst_data, save_dir=None, logger: logging.Logger = None, batch_size=None, output=True,\n                 cache=None, ret_speed=False, **kwargs):\n        return super().evaluate(tst_data, save_dir, logger, batch_size, output, cache, ret_speed, **kwargs)\n\n    def build_vocabs(self, trn: torch.utils.data.Dataset, logger: logging.Logger):\n        additional_tokens = set()\n        self.collect_additional_tokens(additional_tokens, trn)\n        additional_tokens = sorted(additional_tokens)\n        self.build_tokenizer(additional_tokens)\n        self.vocabs['additional_tokens'] = Vocab(idx_to_token=list(additional_tokens))\n", "hanlp/components/amr/seq2seq/evaluation.py": "from pathlib import Path\n\nimport penman\n\n\ndef write_predictions(predictions_path, tokenizer, graphs):\n    pieces = [penman.encode(g) for g in graphs]\n    text = '\\n\\n'.join(pieces)\n    if tokenizer:\n        text = text.replace(tokenizer.INIT, '')\n    Path(predictions_path).write_text(text)\n    return predictions_path\n\n\ndef compute_smatch(pred, gold):\n    from perin_parser.thirdparty.mtool import smatch\n    with Path(pred).open() as p, Path(gold).open() as g:\n        score = next(smatch.score_amr_pairs(p, g))\n    return score[2]\n\n\ndef compute_bleu(gold_sentences, pred_sentences):\n    from sacrebleu import corpus_bleu\n    return corpus_bleu(pred_sentences, [gold_sentences])\n", "hanlp/components/amr/seq2seq/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-04-27 19:24\n", "hanlp/components/eos/ngram.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-26 20:19\nimport logging\nfrom collections import Counter\nfrom typing import Union, List, Callable\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.datasets.eos.eos import SentenceBoundaryDetectionDataset\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass NgramSentenceBoundaryDetectionModel(nn.Module):\n\n    def __init__(self,\n                 char_vocab_size,\n                 embedding_size=128,\n                 rnn_type: str = 'LSTM',\n                 rnn_size=256,\n                 rnn_layers=1,\n                 rnn_bidirectional=False,\n                 dropout=0.2,\n                 **kwargs\n                 ):\n        super(NgramSentenceBoundaryDetectionModel, self).__init__()\n        self.embed = nn.Embedding(num_embeddings=char_vocab_size,\n                                  embedding_dim=embedding_size)\n        rnn_type = rnn_type.lower()\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(input_size=embedding_size,\n                               hidden_size=rnn_size,\n                               num_layers=rnn_layers,\n                               dropout=self.dropout if rnn_layers > 1 else 0.0,\n                               bidirectional=rnn_bidirectional,\n                               batch_first=True)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(input_size=self.embdding_size,\n                              hidden_size=rnn_size,\n                              num_layers=rnn_layers,\n                              dropout=self.dropout if rnn_layers > 1 else 0.0,\n                              bidirectional=rnn_bidirectional,\n                              batch_first=True)\n        else:\n            raise NotImplementedError(f\"'{rnn_type}' has to be one of [LSTM, GRU]\")\n        self.dropout = nn.Dropout(p=dropout) if dropout else None\n        self.dense = nn.Linear(in_features=rnn_size * (2 if rnn_bidirectional else 1),\n                               out_features=1)\n\n    def forward(self, x: torch.Tensor):\n        output = self.embed(x)\n        self.rnn.flatten_parameters()\n        output, _ = self.rnn(output)\n        if self.dropout:\n            output = self.dropout(output[:, -1, :])\n        output = output.squeeze(1)\n        output = self.dense(output).squeeze(-1)\n        return output\n\n\nclass NgramSentenceBoundaryDetector(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"A sentence boundary detector using ngram as features and LSTM as encoder (:cite:`Schweter:Ahmed:2019`).\n        It predicts whether a punctuation marks an ``EOS``.\n\n        .. Note::\n            This component won't work on text without the punctuations defined in its config. It's always\n            recommended to understand how it works before using it. The predefined punctuations can be listed by the\n            following codes.\n\n            >>> print(eos.config.eos_chars)\n\n        Args:\n            **kwargs: Passed to config.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_optimizer(self, **kwargs):\n        optimizer = optim.Adam(self.model.parameters(), lr=self.config.lr)\n        return optimizer\n\n    def build_criterion(self, **kwargs):\n        return BCEWithLogitsLoss()\n\n    def build_metric(self, **kwargs):\n        return F1()\n\n    def execute_training_loop(self,\n                              trn: DataLoader,\n                              dev: DataLoader,\n                              epochs,\n                              criterion,\n                              optimizer,\n                              metric,\n                              save_dir,\n                              logger: logging.Logger,\n                              devices,\n                              **kwargs):\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger)\n            if dev:\n                self.evaluate_dataloader(dev, criterion, metric, logger, ratio_width=ratio_width)\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            dev_score = metric.score\n            if dev_score > best_metric:\n                self.save_weights(save_dir)\n                best_metric = dev_score\n                report += ' [red]saved[/red]'\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       **kwargs):\n        self.model.train()\n        timer = CountdownTimer(len(trn))\n        total_loss = 0\n        self.reset_metrics(metric)\n        for batch in trn:\n            optimizer.zero_grad()\n            prediction = self.feed_batch(batch)\n            loss = self.compute_loss(prediction, batch, criterion)\n            self.update_metrics(batch, prediction, metric)\n            loss.backward()\n            if self.config.grad_norm:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm)\n            optimizer.step()\n            total_loss += loss.item()\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger)\n            del loss\n        return total_loss / timer.total\n\n    def compute_loss(self, prediction, batch, criterion):\n        loss = criterion(prediction, batch['label_id'])\n        return loss\n\n    # noinspection PyMethodOverriding\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric,\n                            logger,\n                            ratio_width=None,\n                            output=False,\n                            **kwargs):\n        self.model.eval()\n        self.reset_metrics(metric)\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        for batch in data:\n            prediction = self.feed_batch(batch)\n            self.update_metrics(batch, prediction, metric)\n            loss = self.compute_loss(prediction, batch, criterion)\n            total_loss += loss.item()\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger,\n                      ratio_width=ratio_width)\n            del loss\n        return total_loss / timer.total, metric\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        model = NgramSentenceBoundaryDetectionModel(**self.config, char_vocab_size=len(self.vocabs.char))\n        return model\n\n    def build_dataloader(self, data, batch_size, shuffle, device, logger: logging.Logger, **kwargs) -> DataLoader:\n        dataset = SentenceBoundaryDetectionDataset(data, **self.config, transform=[self.vocabs])\n        if isinstance(data, str):\n            dataset.purge_cache()\n        if not self.vocabs:\n            self.build_vocabs(dataset, logger)\n        return PadSequenceDataLoader(dataset, batch_size=batch_size, shuffle=shuffle, device=device,\n                                     pad={'label_id': .0})\n\n    def predict(self, data: Union[str, List[str]], batch_size: int = None, strip=True, **kwargs):\n        \"\"\"Sentence split.\n\n        Args:\n            data: A paragraph or a list of paragraphs.\n            batch_size: Number of samples per batch.\n            strip: Strip out blank characters at the head and tail of each sentence.\n\n        Returns:\n            A list of sentences or a list of lists of sentences.\n        \"\"\"\n        if not data:\n            return []\n        self.model.eval()\n        flat = isinstance(data, str)\n        if flat:\n            data = [data]\n        samples = []\n        eos_chars = self.config.eos_chars\n        window_size = self.config.window_size\n        for doc_id_, corpus in enumerate(data):\n            corpus = list(corpus)\n            for i, c in enumerate(corpus):\n                if c in eos_chars:\n                    window = corpus[max(0, i - window_size): i + window_size + 1]\n                    samples.append({'char': window, 'offset_': i, 'doc_id_': doc_id_})\n        eos_prediction = [[] for _ in range(len(data))]\n        if samples:\n            dataloader = self.build_dataloader(samples, **self.config, device=self.device, shuffle=False, logger=None)\n            for batch in dataloader:\n                logits = self.feed_batch(batch)\n                prediction = (logits > 0).tolist()\n                for doc_id_, offset_, eos in zip(batch['doc_id_'], batch['offset_'], prediction):\n                    if eos:\n                        eos_prediction[doc_id_].append(offset_)\n        outputs = []\n        for corpus, output in zip(data, eos_prediction):\n            sents_per_document = []\n            prev_offset = 0\n            for offset in output:\n                offset += 1\n                sents_per_document.append(corpus[prev_offset:offset])\n                prev_offset = offset\n            if prev_offset != len(corpus):\n                sents_per_document.append(corpus[prev_offset:])\n            if strip:\n                sents_per_document = [x.strip() for x in sents_per_document]\n            sents_per_document = [x for x in sents_per_document if x]\n            outputs.append(sents_per_document)\n        if flat:\n            outputs = outputs[0]\n        return outputs\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            epochs=5,\n            append_after_sentence=None,\n            eos_chars=None,\n            eos_char_min_freq=200,\n            eos_char_is_punct=True,\n            char_min_freq=None,\n            window_size=5,\n            batch_size=32,\n            lr=0.001,\n            grad_norm=None,\n            loss_reduction='sum',\n            embedding_size=128,\n            rnn_type: str = 'LSTM',\n            rnn_size=256,\n            rnn_layers=1,\n            rnn_bidirectional=False,\n            dropout=0.2,\n            devices=None,\n            logger=None,\n            seed=None,\n            **kwargs\n            ):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, dataset: SentenceBoundaryDetectionDataset, logger, **kwargs):\n        char_min_freq = self.config.char_min_freq\n        if char_min_freq:\n            has_cache = dataset.cache is not None\n            char_counter = Counter()\n            for each in dataset:\n                for c in each['char']:\n                    char_counter[c] += 1\n            self.vocabs.char = vocab = Vocab()\n            for c, f in char_counter.items():\n                if f >= char_min_freq:\n                    vocab.add(c)\n            if has_cache:\n                dataset.purge_cache()\n                for each in dataset:\n                    pass\n        else:\n            self.vocabs.char = Vocab()\n            for each in dataset:\n                pass\n        self.config.eos_chars = dataset.eos_chars\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n\n    def reset_metrics(self, metrics):\n        metrics.reset()\n\n    def report_metrics(self, loss, metrics):\n        return f'loss: {loss:.4f} {metrics}'\n\n    def update_metrics(self, batch: dict, prediction: torch.FloatTensor, metrics):\n        def nonzero_offsets(y):\n            return set(y.nonzero().squeeze(-1).tolist())\n\n        metrics(nonzero_offsets(prediction > 0), nonzero_offsets(batch['label_id']))\n\n    def feed_batch(self, batch):\n        prediction = self.model(batch['char_id'])\n        return prediction\n", "hanlp/components/eos/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-26 20:19", "hanlp/components/tokenizers/multi_criteria_cws_transformer.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-10-21 19:55\nfrom typing import List, Union\n\nfrom hanlp.common.dataset import SamplerBuilder\nfrom hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger\nfrom hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\nfrom hanlp.datasets.tokenization.loaders.multi_criteria_cws.mcws_dataset import MultiCriteriaTextTokenizingDataset, append_criteria_token\nimport functools\n\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass MultiCriteriaTransformerTaggingTokenizer(TransformerTaggingTokenizer):\n    def __init__(self, **kwargs) -> None:\n        r\"\"\"Transformer based implementation of \"Effective Neural Solution for Multi-Criteria Word Segmentation\"\n        (:cite:`he2019effective`). It uses an artificial token ``[unused_i]`` instead of ``[SEP]`` in the input_ids to\n        mark the i-th segmentation criteria.\n\n        Args:\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_dataset(self, data, **kwargs):\n        return MultiCriteriaTextTokenizingDataset(data, **kwargs)\n\n    def on_config_ready(self, **kwargs):\n        super().on_config_ready(**kwargs)\n        # noinspection PyAttributeOutsideInit\n        if 'criteria_token_map' not in self.config:\n            unused_tokens = [f'[unused{i}]' for i in range(1, 100)]\n            ids = self.transformer_tokenizer.convert_tokens_to_ids(unused_tokens)\n            self.config.unused_tokens = dict((x, ids[i]) for i, x in enumerate(unused_tokens) if\n                                             ids[i] != self.transformer_tokenizer.unk_token_id)\n            self.config.criteria_token_map = dict()\n\n    def last_transform(self):\n        transforms = super().last_transform()\n        transforms.append(functools.partial(append_criteria_token,\n                                            criteria_tokens=self.config.unused_tokens,\n                                            criteria_token_map=self.config.criteria_token_map))\n        return transforms\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        super().build_vocabs(trn, logger, **kwargs)\n        logger.info(f'criteria[{len(self.config.criteria_token_map)}] = {list(self.config.criteria_token_map)}')\n\n    def feed_batch(self, batch: dict):\n        x, mask = TransformerTagger.feed_batch(self, batch)\n        # strip [CLS], [SEP] and [unused_i]\n        return x[:, 1:-2, :], mask\n\n    def build_samples(self, data: List[str], criteria=None, **kwargs):\n        if not criteria:\n            criteria = next(iter(self.config.criteria_token_map.keys()))\n        else:\n            assert criteria in self.config.criteria_token_map, \\\n                f'Unsupported criteria {criteria}. Choose one from {list(self.config.criteria_token_map.keys())}'\n        samples = super().build_samples(data, **kwargs)\n        for sample in samples:\n            sample['criteria'] = criteria\n        return samples\n\n    def build_metric(self, **kwargs):\n        metrics = MetricDict()\n        for criteria in self.config.criteria_token_map:\n            metrics[criteria] = F1()\n        return metrics\n\n    def update_metrics(self, metric, logits, y, mask, batch, prediction):\n        for p, g, c in zip(prediction, self.tag_to_span(batch['tag']), batch['criteria']):\n            pred = set(p)\n            gold = set(g)\n            metric[c](pred, gold)\n\n    def fit(self, trn_data, dev_data, save_dir, transformer, average_subwords=False, word_dropout: float = 0.2,\n            hidden_dropout=None, layer_dropout=0, scalar_mix=None, mix_embedding: int = 0, grad_norm=5.0,\n            transformer_grad_norm=None, lr=5e-5,\n            transformer_lr=None, transformer_layers=None, gradient_accumulation=1,\n            adam_epsilon=1e-8, weight_decay=0, warmup_steps=0.1, crf=False, reduction='sum',\n            batch_size=32, sampler_builder: SamplerBuilder = None, epochs=30, patience=5, token_key=None,\n            tagging_scheme='BMES', delimiter=None,\n            max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False, transform=None, logger=None,\n            devices: Union[float, int, List[int]] = None, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/tokenizers/tok_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-27 14:30\nimport logging\nfrom typing import Union, Any, List, Tuple, Iterable\n\nimport tensorflow as tf\n\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp.components.taggers.ngram_conv.ngram_conv_tagger import NgramTransform, NgramConvTaggerTF\nfrom hanlp.components.taggers.rnn_tagger_tf import RNNTaggerTF\nfrom hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF\nfrom hanlp.components.taggers.transformers.transformer_transform_tf import TransformerTransform\nfrom hanlp.losses.sparse_categorical_crossentropy import SparseCategoricalCrossentropyOverBatchFirstDim\nfrom hanlp.metrics.chunking.bmes_tf import BMES_F1_TF\nfrom hanlp.transform.tsv_tf import TSVTaggingTransform\nfrom hanlp.transform.txt_tf import TxtFormat, TxtBMESFormat, extract_ngram_features_and_tags, bmes_to_words\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BMESTokenizerTF(KerasComponent):\n\n    def build_metrics(self, metrics, logger: logging.Logger, **kwargs):\n        if metrics == 'f1':\n            self.config.run_eagerly = True\n            return BMES_F1_TF(self.transform.tag_vocab)\n        return super().build_metrics(metrics, logger, **kwargs)\n\n\nclass NgramConvTokenizerTransform(TxtFormat, NgramTransform):\n\n    def inputs_to_samples(self, inputs, gold=False):\n        if self.input_is_single_sample(inputs):\n            inputs = [inputs]\n        for sent in inputs:\n            # bigram_only = false\n            yield extract_ngram_features_and_tags(sent, False, self.config.window_size, gold)\n\n    def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:\n        if not input:\n            return True\n        return isinstance(input, str)\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None,\n                     **kwargs) -> Iterable:\n        yield from TxtBMESFormat.Y_to_tokens(self, self.tag_vocab, Y, gold, inputs)\n\n\nclass NgramConvTokenizerTF(BMESTokenizerTF, NgramConvTaggerTF):\n\n    def __init__(self) -> None:\n        super().__init__(NgramConvTokenizerTransform())\n\n    def fit(self, trn_data: Any, dev_data: Any, save_dir: str, word_embed: Union[str, int, dict] = 200,\n            ngram_embed: Union[str, int, dict] = 50, embedding_trainable=True, window_size=4, kernel_size=3,\n            filters=(200, 200, 200, 200, 200), dropout_embed=0.2, dropout_hidden=0.2, weight_norm=True,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='f1', batch_size=100,\n            epochs=100, logger=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def evaluate_output_to_file(self, batch, outputs, out):\n        for x, y_pred in zip(self.transform.X_to_inputs(batch[0]),\n                             self.transform.Y_to_outputs(outputs, gold=False)):\n            out.write(self.transform.input_truth_output_to_str(x, None, y_pred))\n            out.write('\\n')\n\n    def build_loss(self, loss, **kwargs):\n        if loss is None:\n            return SparseCategoricalCrossentropyOverBatchFirstDim()\n        return super().build_loss(loss, **kwargs)\n\n\nclass TransformerTokenizerTransform(TxtBMESFormat, TransformerTransform):\n\n    def inputs_to_samples(self, inputs, gold=False):\n        yield from TransformerTransform.inputs_to_samples(self, TxtBMESFormat.inputs_to_samples(self, inputs, gold),\n                                                          True)\n\n    def Y_to_tokens(self, tag_vocab, Y, gold, inputs):\n        if not gold:\n            Y = tf.argmax(Y, axis=2)\n        for text, ys in zip(inputs, Y):\n            tags = [tag_vocab.idx_to_token[int(y)] for y in ys[1:len(text) + 1]]\n            yield bmes_to_words(list(text), tags)\n\n\nclass TransformerTokenizerTF(BMESTokenizerTF, TransformerTaggerTF):\n    def __init__(self, transform: TransformerTokenizerTransform = None) -> None:\n        if transform is None:\n            transform = TransformerTokenizerTransform()\n        super().__init__(transform)\n\n\nclass RNNTokenizerTransform(TxtBMESFormat, TSVTaggingTransform):\n    pass\n\n\nclass RNNTokenizerTF(BMESTokenizerTF, RNNTaggerTF):\n    def __init__(self, transform: RNNTokenizerTransform = None) -> None:\n        if not transform:\n            transform = RNNTokenizerTransform()\n        super().__init__(transform)\n\n    def fit(self, trn_data: str, dev_data: str = None, save_dir: str = None, embeddings=100, embedding_trainable=False,\n            rnn_input_dropout=0.2, rnn_units=100, rnn_output_dropout=0.2, epochs=20, lower=False, max_seq_len=50,\n            logger=None, loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='f1', batch_size=32,\n            dev_batch_size=32, lr_decay_per_epoch=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/tokenizers/transformer.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 02:48\nimport functools\nfrom typing import TextIO, Union, List, Dict, Any, Set\n\nimport torch\nfrom hanlp.common.dataset import SamplerBuilder\nfrom hanlp.common.transform import TransformList\nfrom hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger\nfrom hanlp.datasets.tokenization.loaders.txt import TextTokenizingDataset, generate_tags_for_subtokens\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp.utils.span_util import bmes_to_spans\nfrom hanlp.utils.string_util import possible_tokenization\nfrom hanlp_common.util import merge_locals_kwargs\nfrom hanlp_trie import DictInterface, TrieDict\nfrom hanlp_trie.dictionary import TupleTrieDict\n\n\nclass TransformerTaggingTokenizer(TransformerTagger):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\" A tokenizer using transformer tagger for span prediction. It features with 2 high performance dictionaries\n        to handle edge cases in real application.\n\n        - ``dict_force``: High priority dictionary performs longest-prefix-matching on input text which takes higher\n          priority over model predictions.\n        - ``dict_combine``: Low priority dictionary performs longest-prefix-matching on model predictions then\n          combines them.\n\n        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can\n            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.\n\n        It also supports outputting the span of each token by setting ``config.output_spans = True``.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @property\n    def dict_force(self) -> DictInterface:\n        r\"\"\" The high priority dictionary which perform longest-prefix-matching on inputs to split them into two subsets:\n\n        1. spans containing no keywords, which are then fed into tokenizer for further tokenization.\n        2. keywords, which will be outputed without furthur tokenization.\n\n        .. Caution::\n            Longest-prefix-matching **NEVER** guarantee the presence of any keywords. Abuse of\n            ``dict_force`` can lead to low quality results. For more details, refer to\n            `this book <http://nlp.hankcs.com/book.php>`_.\n\n        Examples:\n            >>> tok.dict_force = {'\u548c\u670d', '\u670d\u52a1\u884c\u4e1a'} # Force '\u548c\u670d' and '\u670d\u52a1\u884c\u4e1a' by longest-prefix-matching\n            >>> tok(\"\u5546\u54c1\u548c\u670d\u52a1\u884c\u4e1a\")\n                ['\u5546\u54c1', '\u548c\u670d', '\u52a1\u884c\u4e1a']\n            >>> tok.dict_force = {'\u548c\u670d\u52a1': ['\u548c', '\u670d\u52a1']} # Force '\u548c\u670d\u52a1' to be tokenized as ['\u548c', '\u670d\u52a1']\n            >>> tok(\"\u5546\u54c1\u548c\u670d\u52a1\u884c\u4e1a\")\n                ['\u5546\u54c1', '\u548c', '\u670d\u52a1', '\u884c\u4e1a']\n        \"\"\"\n        return self.config.get('dict_force', None)\n\n    @dict_force.setter\n    def dict_force(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            dictionary = TrieDict(dictionary)\n        self.config.dict_force = dictionary\n        self.tokenizer_transform.dict = dictionary\n\n    @property\n    def dict_combine(self) -> DictInterface:\n        \"\"\" The low priority dictionary which perform longest-prefix-matching on model predictions and combing them.\n\n        Examples:\n            >>> tok.dict_combine = {'\u548c\u670d', '\u670d\u52a1\u884c\u4e1a'}\n            >>> tok(\"\u5546\u54c1\u548c\u670d\u52a1\u884c\u4e1a\") # '\u548c\u670d' is not in the original results ['\u5546\u54c1', '\u548c', '\u670d\u52a1']. '\u670d\u52a1', '\u884c\u4e1a' are combined to '\u670d\u52a1\u884c\u4e1a'\n                ['\u5546\u54c1', '\u548c', '\u670d\u52a1\u884c\u4e1a']\n\n        \"\"\"\n        return self.config.get('dict_combine', None)\n\n    @dict_combine.setter\n    def dict_combine(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            if all(isinstance(k, str) for k in dictionary):\n                dictionary = TrieDict(dictionary)\n            else:\n                _d = set()\n                for k in dictionary:\n                    if isinstance(k, str):\n                        _d.update(possible_tokenization(k))\n                    else:\n                        _d.add(k)\n                dictionary = TupleTrieDict(_d)\n        self.config.dict_combine = dictionary\n\n    def build_metric(self, **kwargs):\n        return F1()\n\n    # noinspection PyMethodOverriding\n    def update_metrics(self, metric, logits, y, mask, batch, prediction):\n        for p, g in zip(prediction, self.tag_to_span(batch['tag'], batch)):\n            pred = set(p)\n            gold = set(g)\n            metric(pred, gold)\n\n    def decode_output(self, logits, mask, batch, model=None):\n        output = super().decode_output(logits, mask, batch, model)\n        if isinstance(output, torch.Tensor):\n            output = output.tolist()\n        prediction = self.id_to_tags(output, [len(x) for x in batch['token']])\n        return self.tag_to_span(prediction, batch)\n\n    def tag_to_span(self, batch_tags, batch: dict):\n        spans = []\n        if 'custom_words' in batch:\n            if self.config.tagging_scheme == 'BMES':\n                S = 'S'\n                M = 'M'\n                E = 'E'\n            else:\n                S = 'B'\n                M = 'I'\n                E = 'I'\n            for tags, custom_words in zip(batch_tags, batch['custom_words']):\n                # [batch['raw_token'][0][x[0]:x[1]] for x in subwords]\n                if custom_words:\n                    for start, end, label in custom_words:\n                        if end - start == 1:\n                            tags[start] = S\n                        else:\n                            tags[start] = 'B'\n                            tags[end - 1] = E\n                            for i in range(start + 1, end - 1):\n                                tags[i] = M\n                        if end < len(tags):\n                            tags[end] = 'B'\n        if 'token_subtoken_offsets_group' not in batch:  # only check prediction on raw text for now\n            # Check cases that a single char gets split into multiple subtokens, e.g., \u2025 -> . + .\n            for tags, subtoken_offsets in zip(batch_tags, batch['token_subtoken_offsets']):\n                offset = -1  # BERT produces '\u1112', '##\u1161', '##\u11ab' for '\ud55c' and they share the same span\n                prev_tag = None\n                for i, (tag, (b, e)) in enumerate(zip(tags, subtoken_offsets)):\n                    if b < offset:\n                        if prev_tag == 'S':\n                            tags[i - 1] = 'B'\n                        elif prev_tag == 'E':\n                            tags[i - 1] = 'M'\n                        tags[i] = 'M'\n                    offset = e\n                    prev_tag = tag\n        for tags in batch_tags:\n            spans.append(bmes_to_spans(tags))\n        return spans\n\n    def write_prediction(self, prediction, batch, output: TextIO):\n        batch_tokens = self.spans_to_tokens(prediction, batch)\n        for tokens in batch_tokens:\n            output.write(' '.join(tokens))\n            output.write('\\n')\n\n    @property\n    def tokenizer_transform(self):\n        if not self._tokenizer_transform:\n            self._tokenizer_transform = TransformerSequenceTokenizer(self.transformer_tokenizer,\n                                                                     self.config.token_key,\n                                                                     ret_subtokens=True,\n                                                                     ret_subtokens_group=True,\n                                                                     ret_token_span=False,\n                                                                     dict_force=self.dict_force)\n        return self._tokenizer_transform\n\n    def spans_to_tokens(self, spans, batch, rebuild_span=False):\n        batch_tokens = []\n        dict_combine = self.dict_combine\n        raw_text = batch.get('token_', None)  # Use raw text to rebuild the token according to its offset\n        for b, (spans_per_sent, sub_tokens) in enumerate(zip(spans, batch[self.config.token_key])):\n            if raw_text:  # This will restore iPhone X as a whole\n                text = raw_text[b]\n                offsets = batch['token_subtoken_offsets'][b]\n                tokens = [text[offsets[b][0]:offsets[e - 1][-1]] for b, e in spans_per_sent]\n            else:  # This will merge iPhone X into iPhoneX\n                tokens = [''.join(sub_tokens[span[0]:span[1]]) for span in spans_per_sent]\n            if dict_combine:\n                buffer = []\n                offset = 0\n                delta = 0\n                for start, end, label in dict_combine.tokenize(tokens):\n                    if offset < start:\n                        buffer.extend(tokens[offset:start])\n                    if raw_text:\n                        # noinspection PyUnboundLocalVariable\n                        combined = text[offsets[spans_per_sent[start - delta][0]][0]:\n                                        offsets[spans_per_sent[end - delta - 1][1] - 1][1]]\n                    else:\n                        combined = ''.join(tokens[start:end])\n                    buffer.append(combined)\n                    offset = end\n                    if rebuild_span:\n                        start -= delta\n                        end -= delta\n                        combined_span = (spans_per_sent[start][0], spans_per_sent[end - 1][1])\n                        del spans_per_sent[start:end]\n                        delta += end - start - 1\n                        spans_per_sent.insert(start, combined_span)\n                if offset < len(tokens):\n                    buffer.extend(tokens[offset:])\n                tokens = buffer\n            batch_tokens.append(tokens)\n        return batch_tokens\n\n    def generate_prediction_filename(self, tst_data, save_dir):\n        return super().generate_prediction_filename(tst_data.replace('.tsv', '.txt'), save_dir)\n\n    def prediction_to_human(self, pred, vocab, batch, rebuild_span=False):\n        output_spans = self.config.get('output_spans', None)\n        tokens = self.spans_to_tokens(pred, batch, rebuild_span or output_spans)\n        if output_spans:\n            subtoken_spans = batch['token_subtoken_offsets']\n            results = []\n            for toks, offs, subs in zip(tokens, pred, subtoken_spans):\n                r = []\n                results.append(r)\n                for t, (b, e) in zip(toks, offs):\n                    r.append([t, subs[b][0], subs[e - 1][-1]])\n            return results\n        return tokens\n\n    def input_is_flat(self, tokens):\n        return isinstance(tokens, str)\n\n    def build_dataset(self, data, **kwargs):\n        return TextTokenizingDataset(data, **kwargs)\n\n    def last_transform(self):\n        return TransformList(functools.partial(generate_tags_for_subtokens, tagging_scheme=self.config.tagging_scheme),\n                             super().last_transform())\n\n    def fit(self, trn_data, dev_data, save_dir, transformer, average_subwords=False, word_dropout: float = 0.2,\n            hidden_dropout=None, layer_dropout=0, scalar_mix=None, grad_norm=5.0,\n            transformer_grad_norm=None, lr=5e-5, eval_trn=True,\n            transformer_lr=None, transformer_layers=None, gradient_accumulation=1,\n            adam_epsilon=1e-8, weight_decay=0, warmup_steps=0.1, crf=False, reduction='sum',\n            batch_size=32, sampler_builder: SamplerBuilder = None, epochs=30, patience=5, token_key=None,\n            tagging_scheme='BMES', delimiter=None,\n            max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False, transform=None, logger=None,\n            devices: Union[float, int, List[int]] = None, **kwargs):\n        \"\"\"\n\n        Args:\n            trn_data: Training set.\n            dev_data: Development set.\n            save_dir: The directory to save trained component.\n            transformer: An identifier of a pre-trained transformer.\n            average_subwords: ``True`` to average subword representations.\n            word_dropout: Dropout rate to randomly replace a subword with MASK.\n            hidden_dropout: Dropout rate applied to hidden states.\n            layer_dropout: Randomly zero out hidden states of a transformer layer.\n            scalar_mix: Layer attention.\n            grad_norm: Gradient norm for clipping.\n            transformer_grad_norm: Gradient norm for clipping transformer gradient.\n            lr: Learning rate for decoder.\n            transformer_lr: Learning for encoder.\n            transformer_layers: The number of bottom layers to use.\n            gradient_accumulation: Number of batches per update.\n            adam_epsilon: The epsilon to use in Adam.\n            weight_decay: The weight decay to use.\n            warmup_steps: The number of warmup steps.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            reduction: The loss reduction used in aggregating losses.\n            batch_size: The number of samples in a batch.\n            sampler_builder: The builder to build sampler, which will override batch_size.\n            epochs: The number of epochs to train.\n            patience: The number of patience epochs before early stopping.\n            token_key: The key to tokens in dataset.\n            tagging_scheme: Either ``BMES`` or ``BI``.\n            delimiter: Delimiter between tokens used to split a line in the corpus.\n            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            transform: An optional transform to be applied to samples. Usually a character normalization transform is\n                passed in.\n            devices: Devices this component will live on.\n            logger: Any :class:`logging.Logger` instance.\n            seed: Random seed to reproduce this training.\n            **kwargs: Not used.\n\n        Returns:\n            Best metrics on dev set.\n        \"\"\"\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def feed_batch(self, batch: dict):\n        x, mask = super().feed_batch(batch)\n        return x[:, 1:-1, :], mask\n", "hanlp/components/tokenizers/tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-12 13:08\nfrom typing import Any, Callable\n\nfrom hanlp.components.taggers.rnn_tagger import RNNTagger\nfrom hanlp.datasets.tokenization.loaders.chunking_dataset import ChunkingDataset\nfrom hanlp.metrics.chunking.chunking_f1 import ChunkingF1\nfrom hanlp.utils.span_util import bmes_to_words\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass RNNTokenizer(RNNTagger):\n\n    def predict(self, sentence: Any, batch_size: int = None, **kwargs):\n        flat = isinstance(sentence, str)\n        if flat:\n            sentence = [sentence]\n        for i, s in enumerate(sentence):\n            sentence[i] = list(s)\n        outputs = RNNTagger.predict(self, sentence, batch_size, **kwargs)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    def predict_data(self, data, batch_size, **kwargs):\n        tags = RNNTagger.predict_data(self, data, batch_size, **kwargs)\n        words = [bmes_to_words(c, t) for c, t in zip(data, tags)]\n        return words\n\n    def build_dataset(self, data, transform=None):\n        dataset = ChunkingDataset(data)\n        if 'transform' in self.config:\n            dataset.append_transform(self.config.transform)\n        if transform:\n            dataset.append_transform(transform)\n        return dataset\n\n    def build_metric(self, **kwargs):\n        return ChunkingF1()\n\n    def update_metrics(self, metric, logits, y, mask, batch):\n        pred = self.decode_output(logits, mask, batch)\n        pred = self._id_to_tags(pred)\n        gold = batch['tag']\n        metric(pred, gold)\n\n    def fit(self, trn_data, dev_data, save_dir, batch_size=50, epochs=100, embed=100, rnn_input=None, rnn_hidden=256,\n            drop=0.5, lr=0.001, patience=10, crf=True, optimizer='adam', token_key='char', tagging_scheme=None,\n            anneal_factor: float = 0.5, anneal_patience=2, devices=None, logger=None,\n            verbose=True, transform: Callable = None, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n\n", "hanlp/components/tokenizers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 02:48", "hanlp/components/lm/mlm.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-29 21:07\nimport logging\nimport math\nfrom typing import Callable, Union, List\n\nimport torch\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import reorder\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForMaskedLM\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nfrom hanlp.common.dataset import TransformableDataset, PadSequenceDataLoader, SortingSampler\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.layers.transformers.pt_imports import AutoTokenizer_\nfrom hanlp.transform.transformer_tokenizer import TransformerTextTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\n\n\nclass MaskedLanguageModelDataset(TransformableDataset):\n\n    def load_file(self, filepath: str):\n        raise NotImplementedError()\n\n\nclass MaskedLanguageModel(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.tokenizer: PreTrainedTokenizer = None\n\n    def build_dataloader(self, data, batch_size, shuffle=False, device=None, logger: logging.Logger = None,\n                         verbose=False, **kwargs) -> DataLoader:\n        dataset = MaskedLanguageModelDataset([{'token': x} for x in data], generate_idx=True,\n                                             transform=TransformerTextTokenizer(self.tokenizer, text_a_key='token'))\n        if verbose:\n            verbose = CountdownTimer(len(dataset))\n        lens = []\n        for each in dataset:\n            lens.append(len(each['token_input_ids']))\n            if verbose:\n                verbose.log('Preprocessing and caching samples [blink][yellow]...[/yellow][/blink]')\n        dataloader = PadSequenceDataLoader(dataset, batch_sampler=SortingSampler(lens, batch_size=batch_size),\n                                           device=device)\n        return dataloader\n\n    def build_optimizer(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_criterion(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_metric(self, **kwargs):\n        raise NotImplementedError()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        raise NotImplementedError()\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        raise NotImplementedError()\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        raise NotImplementedError()\n\n    def build_model(self, training=True, transformer=None, **kwargs) -> torch.nn.Module:\n        return AutoModelForMaskedLM.from_pretrained(transformer)\n\n    def input_is_flat(self, masked_sents):\n        return isinstance(masked_sents, str)\n\n    def predict(self, masked_sents: Union[str, List[str]], batch_size=32, topk=10, **kwargs):\n        flat = self.input_is_flat(masked_sents)\n        if flat:\n            masked_sents = [masked_sents]\n        dataloader = self.build_dataloader(masked_sents, **self.config, device=self.device, batch_size=batch_size)\n        orders = []\n        results = []\n        for batch in dataloader:\n            input_ids = batch['token_input_ids']\n            outputs = self.model(input_ids=input_ids, attention_mask=batch['token_attention_mask'])\n            mask = input_ids == self.tokenizer.mask_token_id\n            if mask.any():\n                num_masks = mask.sum(dim=-1).tolist()\n                masked_logits = outputs.logits[mask]\n                masked_logits[:, self.tokenizer.all_special_ids] = -math.inf\n                probs, indices = torch.nn.functional.softmax(masked_logits, dim=-1).topk(topk)\n                br = []\n                for p, index in zip(probs.tolist(), indices.tolist()):\n                    br.append(dict(zip(self.tokenizer.convert_ids_to_tokens(index), p)))\n                offset = 0\n                for n in num_masks:\n                    results.append(br[offset:offset + n])\n                    offset += n\n            else:\n                results.extend([[]] * input_ids.size(0))\n            orders.extend(batch[IDX])\n        results = reorder(results, orders)\n        if flat:\n            results = results[0]\n        return results\n\n    def load_config(self, save_dir, filename='config.json', **kwargs):\n        self.config.transformer = save_dir\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self.tokenizer = AutoTokenizer_.from_pretrained(self.config.transformer)\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n", "hanlp/components/lm/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-29 21:07\n", "hanlp/components/parsers/chu_liu_edmonds.py": "# Adopted from https://github.com/allenai/allennlp under Apache Licence 2.0.\n# Changed the packaging.\n\nfrom typing import List, Set, Tuple, Dict\nimport numpy\n\n\ndef decode_mst(\n        energy: numpy.ndarray, length: int, has_labels: bool = True\n) -> Tuple[numpy.ndarray, numpy.ndarray]:\n    \"\"\"Note: Counter to typical intuition, this function decodes the _maximum_\n    spanning tree.\n    \n    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for\n    maximum spanning arborescences on graphs.\n    \n    Adopted from https://github.com/allenai/allennlp/blob/master/allennlp/nn/chu_liu_edmonds.py\n    which is licensed under the Apache License 2.0\n    \n    # Parameters\n    \n    energy : `numpy.ndarray`, required.\n        A tensor with shape (num_labels, timesteps, timesteps)\n        containing the energy of each edge. If has_labels is `False`,\n        the tensor should have shape (timesteps, timesteps) instead.\n    length : `int`, required.\n        The length of this sequence, as the energy may have come\n        from a padded batch.\n    has_labels : `bool`, optional, (default = True)\n        Whether the graph has labels or not.\n\n    Args:\n      energy: numpy.ndarray: \n      length: int: \n      has_labels: bool:  (Default value = True)\n\n    Returns:\n\n    \"\"\"\n    if has_labels and energy.ndim != 3:\n        raise ValueError(\"The dimension of the energy array is not equal to 3.\")\n    elif not has_labels and energy.ndim != 2:\n        raise ValueError(\"The dimension of the energy array is not equal to 2.\")\n    input_shape = energy.shape\n    max_length = input_shape[-1]\n\n    # Our energy matrix might have been batched -\n    # here we clip it to contain only non padded tokens.\n    if has_labels:\n        energy = energy[:, :length, :length]\n        # get best label for each edge.\n        label_id_matrix = energy.argmax(axis=0)\n        energy = energy.max(axis=0)\n    else:\n        energy = energy[:length, :length]\n        label_id_matrix = None\n    # get original score matrix\n    original_score_matrix = energy\n    # initialize score matrix to original score matrix\n    score_matrix = numpy.array(original_score_matrix, copy=True)\n\n    old_input = numpy.zeros([length, length], dtype=numpy.int32)\n    old_output = numpy.zeros([length, length], dtype=numpy.int32)\n    current_nodes = [True for _ in range(length)]\n    representatives: List[Set[int]] = []\n\n    for node1 in range(length):\n        original_score_matrix[node1, node1] = 0.0\n        score_matrix[node1, node1] = 0.0\n        representatives.append({node1})\n\n        for node2 in range(node1 + 1, length):\n            old_input[node1, node2] = node1\n            old_output[node1, node2] = node2\n\n            old_input[node2, node1] = node2\n            old_output[node2, node1] = node1\n\n    final_edges: Dict[int, int] = {}\n\n    # The main algorithm operates inplace.\n    chu_liu_edmonds(\n        length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives\n    )\n\n    heads = numpy.zeros([max_length], numpy.int32)\n    if has_labels:\n        head_type = numpy.ones([max_length], numpy.int32)\n    else:\n        head_type = None\n\n    for child, parent in final_edges.items():\n        heads[child] = parent\n        if has_labels:\n            head_type[child] = label_id_matrix[parent, child]\n\n    return heads, head_type\n\n\ndef chu_liu_edmonds(\n        length: int,\n        score_matrix: numpy.ndarray,\n        current_nodes: List[bool],\n        final_edges: Dict[int, int],\n        old_input: numpy.ndarray,\n        old_output: numpy.ndarray,\n        representatives: List[Set[int]],\n):\n    \"\"\"Applies the chu-liu-edmonds algorithm recursively\n    to a graph with edge weights defined by score_matrix.\n    \n    Note that this function operates in place, so variables\n    will be modified.\n    \n    # Parameters\n    \n    length : `int`, required.\n        The number of nodes.\n    score_matrix : `numpy.ndarray`, required.\n        The score matrix representing the scores for pairs\n        of nodes.\n    current_nodes : `List[bool]`, required.\n        The nodes which are representatives in the graph.\n        A representative at it's most basic represents a node,\n        but as the algorithm progresses, individual nodes will\n        represent collapsed cycles in the graph.\n    final_edges : `Dict[int, int]`, required.\n        An empty dictionary which will be populated with the\n        nodes which are connected in the maximum spanning tree.\n    old_input : `numpy.ndarray`, required.\n    old_output : `numpy.ndarray`, required.\n    representatives : `List[Set[int]]`, required.\n        A list containing the nodes that a particular node\n        is representing at this iteration in the graph.\n    \n    # Returns\n    \n    Nothing - all variables are modified in place.\n\n    Args:\n      length: int: \n      score_matrix: numpy.ndarray: \n      current_nodes: List[bool]: \n      final_edges: Dict[int: \n      int]: \n      old_input: numpy.ndarray: \n      old_output: numpy.ndarray: \n      representatives: List[Set[int]]: \n\n    Returns:\n\n    \"\"\"\n    # Set the initial graph to be the greedy best one.\n    parents = [-1]\n    for node1 in range(1, length):\n        parents.append(0)\n        if current_nodes[node1]:\n            max_score = score_matrix[0, node1]\n            for node2 in range(1, length):\n                if node2 == node1 or not current_nodes[node2]:\n                    continue\n\n                new_score = score_matrix[node2, node1]\n                if new_score > max_score:\n                    max_score = new_score\n                    parents[node1] = node2\n\n    # Check if this solution has a cycle.\n    has_cycle, cycle = _find_cycle(parents, length, current_nodes)\n    # If there are no cycles, find all edges and return.\n    if not has_cycle:\n        final_edges[0] = -1\n        for node in range(1, length):\n            if not current_nodes[node]:\n                continue\n\n            parent = old_input[parents[node], node]\n            child = old_output[parents[node], node]\n            final_edges[child] = parent\n        return\n\n    # Otherwise, we have a cycle so we need to remove an edge.\n    # From here until the recursive call is the contraction stage of the algorithm.\n    cycle_weight = 0.0\n    # Find the weight of the cycle.\n    index = 0\n    for node in cycle:\n        index += 1\n        cycle_weight += score_matrix[parents[node], node]\n\n    # For each node in the graph, find the maximum weight incoming\n    # and outgoing edge into the cycle.\n    cycle_representative = cycle[0]\n    for node in range(length):\n        if not current_nodes[node] or node in cycle:\n            continue\n\n        in_edge_weight = float(\"-inf\")\n        in_edge = -1\n        out_edge_weight = float(\"-inf\")\n        out_edge = -1\n\n        for node_in_cycle in cycle:\n            if score_matrix[node_in_cycle, node] > in_edge_weight:\n                in_edge_weight = score_matrix[node_in_cycle, node]\n                in_edge = node_in_cycle\n\n            # Add the new edge score to the cycle weight\n            # and subtract the edge we're considering removing.\n            score = (\n                    cycle_weight\n                    + score_matrix[node, node_in_cycle]\n                    - score_matrix[parents[node_in_cycle], node_in_cycle]\n            )\n\n            if score > out_edge_weight:\n                out_edge_weight = score\n                out_edge = node_in_cycle\n\n        score_matrix[cycle_representative, node] = in_edge_weight\n        old_input[cycle_representative, node] = old_input[in_edge, node]\n        old_output[cycle_representative, node] = old_output[in_edge, node]\n\n        score_matrix[node, cycle_representative] = out_edge_weight\n        old_output[node, cycle_representative] = old_output[node, out_edge]\n        old_input[node, cycle_representative] = old_input[node, out_edge]\n\n    # For the next recursive iteration, we want to consider the cycle as a\n    # single node. Here we collapse the cycle into the first node in the\n    # cycle (first node is arbitrary), set all the other nodes not be\n    # considered in the next iteration. We also keep track of which\n    # representatives we are considering this iteration because we need\n    # them below to check if we're done.\n    considered_representatives: List[Set[int]] = []\n    for i, node_in_cycle in enumerate(cycle):\n        considered_representatives.append(set())\n        if i > 0:\n            # We need to consider at least one\n            # node in the cycle, arbitrarily choose\n            # the first.\n            current_nodes[node_in_cycle] = False\n\n        for node in representatives[node_in_cycle]:\n            considered_representatives[i].add(node)\n            if i > 0:\n                representatives[cycle_representative].add(node)\n\n    chu_liu_edmonds(\n        length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives\n    )\n\n    # Expansion stage.\n    # check each node in cycle, if one of its representatives\n    # is a key in the final_edges, it is the one we need.\n    found = False\n    key_node = -1\n    for i, node in enumerate(cycle):\n        for cycle_rep in considered_representatives[i]:\n            if cycle_rep in final_edges:\n                key_node = node\n                found = True\n                break\n        if found:\n            break\n\n    previous = parents[key_node]\n    while previous != key_node:\n        child = old_output[parents[previous], previous]\n        parent = old_input[parents[previous], previous]\n        final_edges[child] = parent\n        previous = parents[previous]\n\n\ndef _find_cycle(\n        parents: List[int], length: int, current_nodes: List[bool]\n) -> Tuple[bool, List[int]]:\n    added = [False for _ in range(length)]\n    added[0] = True\n    cycle = set()\n    has_cycle = False\n    for i in range(1, length):\n        if has_cycle:\n            break\n        # don't redo nodes we've already\n        # visited or aren't considering.\n        if added[i] or not current_nodes[i]:\n            continue\n        # Initialize a new possible cycle.\n        this_cycle = set()\n        this_cycle.add(i)\n        added[i] = True\n        has_cycle = True\n        next_node = i\n        while parents[next_node] not in this_cycle:\n            next_node = parents[next_node]\n            # If we see a node we've already processed,\n            # we can stop, because the node we are\n            # processing would have been in that cycle.\n            if added[next_node]:\n                has_cycle = False\n                break\n            added[next_node] = True\n            this_cycle.add(next_node)\n\n        if has_cycle:\n            original = next_node\n            cycle.add(original)\n            next_node = parents[original]\n            while next_node != original:\n                cycle.add(next_node)\n                next_node = parents[next_node]\n            break\n\n    return has_cycle, list(cycle)\n", "hanlp/components/parsers/alg_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 19:49\n# Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser\nfrom typing import List\nimport numpy as np\nimport tensorflow as tf\nfrom collections import defaultdict\n\n\ndef nonzero(t: tf.Tensor) -> tf.Tensor:\n    return tf.where(t > 0)\n\n\ndef view(t: tf.Tensor, *dims) -> tf.Tensor:\n    return tf.reshape(t, dims)\n\n\ndef arange(n: int) -> tf.Tensor:\n    return tf.range(n)\n\n\ndef randperm(n: int) -> tf.Tensor:\n    return tf.random.shuffle(arange(n))\n\n\ndef tolist(t: tf.Tensor) -> List:\n    if isinstance(t, tf.Tensor):\n        t = t.numpy()\n    return t.tolist()\n\n\ndef kmeans(x, k, seed=None):\n    \"\"\"See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7\n\n    Args:\n      x(list): Lengths of sentences\n      k(int): \n      seed:  (Default value = None)\n\n    Returns:\n\n    \n    \"\"\"\n    x = tf.constant(x, dtype=tf.float32)\n    # count the frequency of each datapoint\n    d, indices, f = tf.unique_with_counts(x, tf.int32)\n    f = tf.cast(f, tf.float32)\n    # calculate the sum of the values of the same datapoints\n    total = d * f\n    # initialize k centroids randomly\n    c, old = tf.random.shuffle(d, seed)[:k], None\n    # assign labels to each datapoint based on centroids\n    dists = tf.abs(tf.expand_dims(d, -1) - c)\n    y = tf.argmin(dists, axis=-1, output_type=tf.int32)\n    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))\n    # make sure number of datapoints is greater than that of clusters\n    assert len(d) >= k, f\"unable to assign {len(d)} datapoints to {k} clusters\"\n\n    while old is None or not tf.reduce_all(c == old):\n        # if an empty cluster is encountered,\n        # choose the farthest datapoint from the biggest cluster\n        # and move that the empty one\n        for i in range(k):\n            if not tf.reduce_any(y == i):\n                mask = tf.cast(y == tf.expand_dims(tf.range(k, dtype=tf.int32), -1), tf.float32)\n                lens = tf.reduce_sum(mask, axis=-1)\n                biggest = view(nonzero(mask[tf.argmax(lens)]), -1)\n                farthest = tf.argmax(tf.gather(dists, biggest))\n                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], -1), -1), [i])\n        mask = tf.cast(y == tf.expand_dims(tf.range(k, dtype=tf.int32), -1), tf.float32)\n        # update the centroids\n        c, old = tf.cast(tf.reduce_sum(total * mask, axis=-1), tf.float32) / tf.cast(tf.reduce_sum(f * mask, axis=-1),\n                                                                                     tf.float32), c\n        # re-assign all datapoints to clusters\n        dists = tf.abs(tf.expand_dims(d, -1) - c)\n        y = tf.argmin(dists, axis=-1, output_type=tf.int32)\n        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))\n    # assign all datapoints to the new-generated clusters\n    # without considering the empty ones\n    y, (assigned, _) = tf.gather(y, indices), tf.unique(y)\n    # get the centroids of the assigned clusters\n    centroids = tf.gather(c, assigned).numpy().tolist()\n    # map all values of datapoints to buckets\n    clusters = [tf.squeeze(tf.where(y == i), axis=-1).numpy().tolist() for i in assigned]\n\n    return centroids, clusters\n\n\n# ***************************************************************\nclass Tarjan:\n    \"\"\"Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph\"\"\"\n\n    def __init__(self, prediction, tokens):\n        \"\"\"\n\n        Parameters\n        ----------\n        prediction : numpy.ndarray\n            a predicted dependency tree where prediction[dep_idx] = head_idx\n        tokens : numpy.ndarray\n            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)\n        \"\"\"\n        self._edges = defaultdict(set)\n        self._vertices = set((0,))\n        for dep, head in enumerate(prediction[tokens]):\n            self._vertices.add(dep + 1)\n            self._edges[head].add(dep + 1)\n        self._indices = {}\n        self._lowlinks = {}\n        self._onstack = defaultdict(lambda: False)\n        self._SCCs = []\n\n        index = 0\n        stack = []\n        for v in self.vertices:\n            if v not in self.indices:\n                self.strongconnect(v, index, stack)\n\n    # =============================================================\n    def strongconnect(self, v, index, stack):\n        \"\"\"\n\n        Args:\n          v: \n          index: \n          stack: \n\n        Returns:\n\n        \"\"\"\n\n        self._indices[v] = index\n        self._lowlinks[v] = index\n        index += 1\n        stack.append(v)\n        self._onstack[v] = True\n        for w in self.edges[v]:\n            if w not in self.indices:\n                self.strongconnect(w, index, stack)\n                self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])\n            elif self._onstack[w]:\n                self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])\n\n        if self._lowlinks[v] == self._indices[v]:\n            self._SCCs.append(set())\n            while stack[-1] != v:\n                w = stack.pop()\n                self._onstack[w] = False\n                self._SCCs[-1].add(w)\n            w = stack.pop()\n            self._onstack[w] = False\n            self._SCCs[-1].add(w)\n        return\n\n    # ======================\n    @property\n    def edges(self):\n        return self._edges\n\n    @property\n    def vertices(self):\n        return self._vertices\n\n    @property\n    def indices(self):\n        return self._indices\n\n    @property\n    def SCCs(self):\n        return self._SCCs\n\n\ndef tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):\n    \"\"\"Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py\n\n    Args:\n      parse_probs(NDArray): seq_len x seq_len, the probability of arcs\n      length(NDArray): sentence length including ROOT\n      tokens_to_keep(NDArray): mask matrix\n      ensure_tree:  (Default value = True)\n\n    Returns:\n\n    \n    \"\"\"\n    if ensure_tree:\n        I = np.eye(len(tokens_to_keep))\n        # block loops and pad heads\n        parse_probs = parse_probs * tokens_to_keep * (1 - I)\n        parse_preds = np.argmax(parse_probs, axis=1)\n        tokens = np.arange(1, length)\n        roots = np.where(parse_preds[tokens] == 0)[0] + 1\n        # ensure at least one root\n        if len(roots) < 1:\n            # The current root probabilities\n            root_probs = parse_probs[tokens, 0]\n            # The current head probabilities\n            old_head_probs = parse_probs[tokens, parse_preds[tokens]]\n            # Get new potential root probabilities\n            new_root_probs = root_probs / old_head_probs\n            # Select the most probable root\n            new_root = tokens[np.argmax(new_root_probs)]\n            # Make the change\n            parse_preds[new_root] = 0\n        # ensure at most one root\n        elif len(roots) > 1:\n            # The probabilities of the current heads\n            root_probs = parse_probs[roots, 0]\n            # Set the probability of depending on the root zero\n            parse_probs[roots, 0] = 0\n            # Get new potential heads and their probabilities\n            new_heads = np.argmax(parse_probs[roots][:, tokens], axis=1) + 1\n            new_head_probs = parse_probs[roots, new_heads] / root_probs\n            # Select the most probable root\n            new_root = roots[np.argmin(new_head_probs)]\n            # Make the change\n            parse_preds[roots] = new_heads\n            parse_preds[new_root] = 0\n        # remove cycles\n        tarjan = Tarjan(parse_preds, tokens)\n        for SCC in tarjan.SCCs:\n            if len(SCC) > 1:\n                dependents = set()\n                to_visit = set(SCC)\n                while len(to_visit) > 0:\n                    node = to_visit.pop()\n                    if not node in dependents:\n                        dependents.add(node)\n                        to_visit.update(tarjan.edges[node])\n                # The indices of the nodes that participate in the cycle\n                cycle = np.array(list(SCC))\n                # The probabilities of the current heads\n                old_heads = parse_preds[cycle]\n                old_head_probs = parse_probs[cycle, old_heads]\n                # Set the probability of depending on a non-head to zero\n                non_heads = np.array(list(dependents))\n                parse_probs[np.repeat(cycle, len(non_heads)), np.repeat([non_heads], len(cycle), axis=0).flatten()] = 0\n                # Get new potential heads and their probabilities\n                new_heads = np.argmax(parse_probs[cycle][:, tokens], axis=1) + 1\n                new_head_probs = parse_probs[cycle, new_heads] / old_head_probs\n                # Select the most probable change\n                change = np.argmax(new_head_probs)\n                changed_cycle = cycle[change]\n                old_head = old_heads[change]\n                new_head = new_heads[change]\n                # Make the change\n                parse_preds[changed_cycle] = new_head\n                tarjan.edges[new_head].add(changed_cycle)\n                tarjan.edges[old_head].remove(changed_cycle)\n        return parse_preds\n    else:\n        # block and pad heads\n        parse_probs = parse_probs * tokens_to_keep\n        parse_preds = np.argmax(parse_probs, axis=1)\n        return parse_preds\n\n\ndef rel_argmax(rel_probs, length, root, ensure_tree=True):\n    \"\"\"Fix the relation prediction by heuristic rules\n\n    Args:\n      rel_probs(NDArray): seq_len x rel_size\n      length: real sentence length\n      ensure_tree:  (Default value = True)\n      root: \n\n    Returns:\n\n    \n    \"\"\"\n    if ensure_tree:\n        tokens = np.arange(1, length)\n        rel_preds = np.argmax(rel_probs, axis=1)\n        roots = np.where(rel_preds[tokens] == root)[0] + 1\n        if len(roots) < 1:\n            rel_preds[1 + np.argmax(rel_probs[tokens, root])] = root\n        elif len(roots) > 1:\n            root_probs = rel_probs[roots, root]\n            rel_probs[roots, root] = 0\n            new_rel_preds = np.argmax(rel_probs[roots], axis=1)\n            new_rel_probs = rel_probs[roots, new_rel_preds] / root_probs\n            new_root = roots[np.argmin(new_rel_probs)]\n            rel_preds[roots] = new_rel_preds\n            rel_preds[new_root] = root\n        return rel_preds\n    else:\n        rel_preds = np.argmax(rel_probs, axis=1)\n        return rel_preds\n", "hanlp/components/parsers/conll.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 15:37\nfrom typing import Union\n\nfrom hanlp.utils.io_util import get_resource, TimingFileIterator\nfrom hanlp.utils.log_util import logger\n\n\ndef collapse_enhanced_empty_nodes(sent: list):\n    collapsed = []\n    for cells in sent:\n        if isinstance(cells[0], float):\n            id = cells[0]\n            head, deprel = cells[8].split(':', 1)\n            for x in sent:\n                arrows = [s.split(':', 1) for s in x[8].split('|')]\n                arrows = [(head, f'{head}:{deprel}>{r}') if h == str(id) else (h, r) for h, r in arrows]\n                arrows = sorted(arrows)\n                x[8] = '|'.join(f'{h}:{r}' for h, r in arrows)\n            sent[head][7] += f'>{cells[7]}'\n        else:\n            collapsed.append(cells)\n    return collapsed\n\n\ndef read_conll(filepath: Union[str, TimingFileIterator], underline_to_none=False, enhanced_collapse_empty_nodes=False):\n    sent = []\n    if isinstance(filepath, str):\n        filepath: str = get_resource(filepath)\n        if filepath.endswith('.conllu') and enhanced_collapse_empty_nodes is None:\n            enhanced_collapse_empty_nodes = True\n        src = open(filepath, encoding='utf-8')\n    else:\n        src = filepath\n    for idx, line in enumerate(src):\n        if line.startswith('#'):\n            continue\n        line = line.strip()\n        cells = line.split('\\t')\n        if line and cells:\n            if enhanced_collapse_empty_nodes and '.' in cells[0]:\n                cells[0] = float(cells[0])\n                cells[6] = None\n            else:\n                if '-' in cells[0] or '.' in cells[0]:\n                    # sent[-1][1] += cells[1]\n                    continue\n                cells[0] = int(cells[0])\n                if cells[6] != '_':\n                    try:\n                        cells[6] = int(cells[6])\n                    except ValueError:\n                        cells[6] = 0\n                        logger.exception(f'Wrong CoNLL format {filepath}:{idx + 1}\\n{line}')\n            if underline_to_none:\n                for i, x in enumerate(cells):\n                    if x == '_':\n                        cells[i] = None\n            sent.append(cells)\n        else:\n            if enhanced_collapse_empty_nodes:\n                sent = collapse_enhanced_empty_nodes(sent)\n            yield sent\n            sent = []\n\n    if sent:\n        if enhanced_collapse_empty_nodes:\n            sent = collapse_enhanced_empty_nodes(sent)\n        yield sent\n\n    src.close()\n\n", "hanlp/components/parsers/parse_alg.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-04-02 23:20\nfrom collections import defaultdict\n\nimport hanlp.utils.span_util\nfrom hanlp.components.parsers.chu_liu_edmonds import decode_mst\nimport numpy as np\n\n\nclass Tarjan:\n    \"\"\"Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph\"\"\"\n\n    def __init__(self, prediction, tokens):\n        \"\"\"\n\n        Parameters\n        ----------\n        prediction : numpy.ndarray\n            a predicted dependency tree where prediction[dep_idx] = head_idx\n        tokens : numpy.ndarray\n            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)\n        \"\"\"\n        self._edges = defaultdict(set)\n        self._vertices = set((0,))\n        for dep, head in enumerate(prediction[tokens]):\n            self._vertices.add(dep + 1)\n            self._edges[head].add(dep + 1)\n        self._indices = {}\n        self._lowlinks = {}\n        self._onstack = defaultdict(lambda: False)\n        self._SCCs = []\n\n        index = 0\n        stack = []\n        for v in self.vertices:\n            if v not in self.indices:\n                self.strongconnect(v, index, stack)\n\n    # =============================================================\n    def strongconnect(self, v, index, stack):\n        \"\"\"\n\n        Args:\n          v: \n          index: \n          stack: \n\n        Returns:\n\n        \"\"\"\n\n        self._indices[v] = index\n        self._lowlinks[v] = index\n        index += 1\n        stack.append(v)\n        self._onstack[v] = True\n        for w in self.edges[v]:\n            if w not in self.indices:\n                self.strongconnect(w, index, stack)\n                self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])\n            elif self._onstack[w]:\n                self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])\n\n        if self._lowlinks[v] == self._indices[v]:\n            self._SCCs.append(set())\n            while stack[-1] != v:\n                w = stack.pop()\n                self._onstack[w] = False\n                self._SCCs[-1].add(w)\n            w = stack.pop()\n            self._onstack[w] = False\n            self._SCCs[-1].add(w)\n        return\n\n    # ======================\n    @property\n    def edges(self):\n        return self._edges\n\n    @property\n    def vertices(self):\n        return self._vertices\n\n    @property\n    def indices(self):\n        return self._indices\n\n    @property\n    def SCCs(self):\n        return self._SCCs\n\n\nclass UnionFind(object):\n\n    def __init__(self, n) -> None:\n        super().__init__()\n        self.parent = [x for x in range(n)]\n        self.height = [0] * n\n\n    def find(self, x):\n        if self.parent[x] == x:\n            return x\n        self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n\n    def unite(self, x, y):\n        x = self.find(x)\n        y = self.find(y)\n        if x == y:\n            return\n        if self.height[x] < self.height[y]:\n            self.parent[x] = y\n        else:\n            self.parent[y] = x\n            if self.height[x] == self.height[y]:\n                self.height[x] += 1\n\n    def same(self, x, y):\n        return self.find(x) == self.find(y)\n\n\ndef tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):\n    \"\"\"Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py\n\n    Args:\n      parse_probs(NDArray): seq_len x seq_len, the probability of arcs\n      length(NDArray): sentence length including ROOT\n      tokens_to_keep(NDArray): mask matrix\n      ensure_tree:  (Default value = True)\n\n    Returns:\n\n    \n    \"\"\"\n    if ensure_tree:\n        parse_preds, parse_probs, tokens = unique_root(parse_probs, tokens_to_keep, length)\n        # remove cycles\n        tarjan = Tarjan(parse_preds, tokens)\n        for SCC in tarjan.SCCs:\n            if len(SCC) > 1:\n                dependents = set()\n                to_visit = set(SCC)\n                while len(to_visit) > 0:\n                    node = to_visit.pop()\n                    if not node in dependents:\n                        dependents.add(node)\n                        to_visit.update(tarjan.edges[node])\n                # The indices of the nodes that participate in the cycle\n                cycle = np.array(list(SCC))\n                # The probabilities of the current heads\n                old_heads = parse_preds[cycle]\n                old_head_probs = parse_probs[cycle, old_heads]\n                # Set the probability of depending on a non-head to zero\n                non_heads = np.array(list(dependents))\n                parse_probs[np.repeat(cycle, len(non_heads)), np.repeat([non_heads], len(cycle), axis=0).flatten()] = 0\n                # Get new potential heads and their probabilities\n                new_heads = np.argmax(parse_probs[cycle][:, tokens], axis=1) + 1\n                new_head_probs = parse_probs[cycle, new_heads] / old_head_probs\n                # Select the most probable change\n                change = np.argmax(new_head_probs)\n                changed_cycle = cycle[change]\n                old_head = old_heads[change]\n                new_head = new_heads[change]\n                # Make the change\n                parse_preds[changed_cycle] = new_head\n                tarjan.edges[new_head].add(changed_cycle)\n                tarjan.edges[old_head].remove(changed_cycle)\n        return parse_preds\n    else:\n        # block and pad heads\n        parse_probs = parse_probs * tokens_to_keep\n        parse_preds = np.argmax(parse_probs, axis=1)\n        return parse_preds\n\n\ndef chu_liu_edmonds(parse_probs, length):\n    tree = decode_mst(hanlp.utils.span_util.T, length, False)[0]\n    tree[0] = 0\n    return tree\n\n\ndef unique_root(parse_probs, tokens_to_keep: np.ndarray, length):\n    I = np.eye(len(tokens_to_keep))\n    # block loops and pad heads\n    if tokens_to_keep.ndim == 1:\n        tokens_to_keep = np.expand_dims(tokens_to_keep, -1)\n    parse_probs = parse_probs * tokens_to_keep * (1 - I)\n    parse_preds = np.argmax(parse_probs, axis=1)\n    tokens = np.arange(1, length)\n    roots = np.where(parse_preds[tokens] == 0)[0] + 1\n    # ensure at least one root\n    if len(roots) < 1:\n        # The current root probabilities\n        root_probs = parse_probs[tokens, 0]\n        # The current head probabilities\n        old_head_probs = parse_probs[tokens, parse_preds[tokens]]\n        # Get new potential root probabilities\n        new_root_probs = root_probs / old_head_probs\n        # Select the most probable root\n        new_root = tokens[np.argmax(new_root_probs)]\n        # Make the change\n        parse_preds[new_root] = 0\n    # ensure at most one root\n    elif len(roots) > 1:\n        # The probabilities of the current heads\n        root_probs = parse_probs[roots, 0]\n        # Set the probability of depending on the root zero\n        parse_probs[roots, 0] = 0\n        # Get new potential heads and their probabilities\n        new_heads = np.argmax(parse_probs[roots][:, tokens], axis=1) + 1\n        new_head_probs = parse_probs[roots, new_heads] / root_probs\n        # Select the most probable root\n        new_root = roots[np.argmin(new_head_probs)]\n        # Make the change\n        parse_preds[roots] = new_heads\n        parse_preds[new_root] = 0\n    return parse_preds, parse_probs, tokens\n\n\ndef dfs(graph, start, end):\n    fringe = [(start, [])]\n    while fringe:\n        state, path = fringe.pop()\n        if path and state == end:\n            yield path\n            continue\n        for next_state in graph[state]:\n            if next_state in path:\n                continue\n            fringe.append((next_state, path + [next_state]))\n\n\ndef mst_then_greedy(arc_scores, rel_scores, mask, root_rel_idx, rel_idx=None):\n    from scipy.special import softmax\n    from scipy.special import expit as sigmoid\n    length = sum(mask) + 1\n    mask = mask[:length]\n    arc_scores = arc_scores[:length, :length]\n    arc_pred = arc_scores > 0\n    arc_probs = sigmoid(arc_scores)\n    rel_scores = rel_scores[:length, :length, :]\n    rel_probs = softmax(rel_scores, -1)\n    if not any(arc_pred[:, 0][1:]):  # no root\n        root = np.argmax(rel_probs[1:, 0, root_rel_idx]) + 1\n        arc_probs[root, 0] = 1\n    parse_preds, parse_probs, tokens = unique_root(arc_probs, mask, length)\n    root = adjust_root_score(arc_scores, parse_preds, root_rel_idx, rel_scores)\n    tree = chu_liu_edmonds(arc_scores, length)\n    if rel_idx is not None:  # Unknown DEPREL label: 'ref'\n        rel_scores[np.arange(len(tree)), tree, rel_idx] = -float('inf')\n    return tree, add_secondary_arcs_by_scores(arc_scores, rel_scores, tree, root_rel_idx)\n\n\ndef adjust_root_score(arc_scores, parse_preds, root_rel_idx, rel_scores=None):\n    root = np.where(parse_preds[1:] == 0)[0] + 1\n    arc_scores[:, 0] = min(np.min(arc_scores), -1000)\n    arc_scores[root, 0] = max(np.max(arc_scores), 1000)\n    if rel_scores is not None:\n        rel_scores[:, :, root_rel_idx] = -float('inf')\n        rel_scores[root, 0, root_rel_idx] = float('inf')\n    return root\n\n\ndef add_secondary_arcs_by_scores(arc_scores, rel_scores, tree, root_rel_idx, arc_preds=None):\n    if not isinstance(tree, np.ndarray):\n        tree = np.array(tree)\n    if arc_preds is None:\n        arc_preds = arc_scores > 0\n    rel_pred = np.argmax(rel_scores, axis=-1)\n\n    return add_secondary_arcs_by_preds(arc_scores, arc_preds, rel_pred, tree, root_rel_idx)\n\n\ndef add_secondary_arcs_by_preds(arc_scores, arc_preds, rel_preds, tree, root_rel_idx=None):\n    dh = np.argwhere(arc_preds)\n    sdh = sorted([(arc_scores[x[0], x[1]], list(x)) for x in dh], reverse=True)\n    graph = [[] for _ in range(len(tree))]\n    for d, h in enumerate(tree):\n        if d:\n            graph[h].append(d)\n    for s, (d, h) in sdh:\n        if not d or not h or d in graph[h]:\n            continue\n        try:\n            path = next(dfs(graph, d, h))\n        except StopIteration:\n            # no path from d to h\n            graph[h].append(d)\n    parse_graph = [[] for _ in range(len(tree))]\n    num_root = 0\n    for h in range(len(tree)):\n        for d in graph[h]:\n            rel = rel_preds[d, h]\n            if h == 0 and root_rel_idx is not None:\n                rel = root_rel_idx\n                assert num_root == 0\n                num_root += 1\n            parse_graph[d].append((h, rel))\n        parse_graph[d] = sorted(parse_graph[d])\n    return parse_graph\n\n\ndef adjust_root_score_then_add_secondary_arcs(arc_scores, rel_scores, tree, root_rel_idx):\n    if len(arc_scores) != tree:\n        arc_scores = arc_scores[:len(tree), :len(tree)]\n        rel_scores = rel_scores[:len(tree), :len(tree), :]\n    parse_preds = arc_scores > 0\n    # adjust_root_score(arc_scores, parse_preds, rel_scores)\n    parse_preds[:, 0] = False  # set heads to False\n    rel_scores[:, :, root_rel_idx] = -float('inf')\n    return add_secondary_arcs_by_scores(arc_scores, rel_scores, tree, root_rel_idx, parse_preds)\n", "hanlp/components/parsers/alg.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport torch\n\nfrom hanlp_common.conll import isprojective\n\n\ndef kmeans(x, k, max_it=32):\n    r\"\"\"\n    KMeans algorithm for clustering the sentences by length.\n\n    Args:\n        x (list[int]):\n            The list of sentence lengths.\n        k (int):\n            The number of clusters.\n            This is an approximate value. The final number of clusters can be less or equal to `k`.\n        max_it (int):\n            Maximum number of iterations.\n            If centroids does not converge after several iterations, the algorithm will be early stopped.\n\n    Returns:\n        list[float], list[list[int]]:\n            The first list contains average lengths of sentences in each cluster.\n            The second is the list of clusters holding indices of data points.\n\n    Examples:\n        >>> x = torch.randint(10,20,(10,)).tolist()\n        >>> x\n        [15, 10, 17, 11, 18, 13, 17, 19, 18, 14]\n        >>> centroids, clusters = kmeans(x, 3)\n        >>> centroids\n        [10.5, 14.0, 17.799999237060547]\n        >>> clusters\n        [[1, 3], [0, 5, 9], [2, 4, 6, 7, 8]]\n    \"\"\"\n\n    # the number of clusters must not be greater than the number of datapoints\n    x, k = torch.tensor(x, dtype=torch.float), min(len(x), k)\n    # collect unique datapoints\n    d = x.unique()\n    # initialize k centroids randomly\n    c = d[torch.randperm(len(d))[:k]]\n    # assign each datapoint to the cluster with the closest centroid\n    dists, y = torch.abs_(x.unsqueeze(-1) - c).min(-1)\n\n    for _ in range(max_it):\n        # if an empty cluster is encountered,\n        # choose the farthest datapoint from the biggest cluster and move that the empty one\n        mask = torch.arange(k).unsqueeze(-1).eq(y)\n        none = torch.where(~mask.any(-1))[0].tolist()\n        while len(none) > 0:\n            for i in none:\n                # the biggest cluster\n                b = torch.where(mask[mask.sum(-1).argmax()])[0]\n                # the datapoint farthest from the centroid of cluster b\n                f = dists[b].argmax()\n                # update the assigned cluster of f\n                y[b[f]] = i\n                # re-calculate the mask\n                mask = torch.arange(k).unsqueeze(-1).eq(y)\n            none = torch.where(~mask.any(-1))[0].tolist()\n        # update the centroids\n        c, old = (x * mask).sum(-1) / mask.sum(-1), c\n        # re-assign all datapoints to clusters\n        dists, y = torch.abs_(x.unsqueeze(-1) - c).min(-1)\n        # stop iteration early if the centroids converge\n        if c.equal(old):\n            break\n    # assign all datapoints to the new-generated clusters\n    # the empty ones are discarded\n    assigned = y.unique().tolist()\n    # get the centroids of the assigned clusters\n    centroids = c[assigned].tolist()\n    # map all values of datapoints to buckets\n    clusters = [torch.where(y.eq(i))[0].tolist() for i in assigned]\n\n    return centroids, clusters\n\n\ndef eisner(scores, mask):\n    r\"\"\"\n    First-order Eisner algorithm for projective decoding.\n\n    References:\n        - Ryan McDonald, Koby Crammer and Fernando Pereira. 2005.\n          `Online Large-Margin Training of Dependency Parsers`_.\n\n    Args:\n        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n            Scores of all dependent-head pairs.\n        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n            The mask to avoid parsing over padding tokens.\n            The first column serving as pseudo words for roots should be ``False``.\n\n    Returns:\n        ~torch.Tensor:\n            A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.\n\n    Examples:\n        >>> scores = torch.tensor([[[-13.5026, -18.3700, -13.0033, -16.6809],\n                                    [-36.5235, -28.6344, -28.4696, -31.6750],\n                                    [ -2.9084,  -7.4825,  -1.4861,  -6.8709],\n                                    [-29.4880, -27.6905, -26.1498, -27.0233]]])\n        >>> mask = torch.tensor([[False,  True,  True,  True]])\n        >>> eisner(scores, mask)\n        tensor([[0, 2, 0, 2]])\n\n    .. _Online Large-Margin Training of Dependency Parsers:\n        https://www.aclweb.org/anthology/P05-1012/\n    \"\"\"\n\n    lens = mask.sum(1)\n    batch_size, seq_len, _ = scores.shape\n    scores = scores.permute(2, 1, 0)\n    s_i = torch.full_like(scores, float('-inf'))\n    s_c = torch.full_like(scores, float('-inf'))\n    p_i = scores.new_zeros(seq_len, seq_len, batch_size).long()\n    p_c = scores.new_zeros(seq_len, seq_len, batch_size).long()\n    s_c.diagonal().fill_(0)\n\n    for w in range(1, seq_len):\n        n = seq_len - w\n        starts = p_i.new_tensor(range(n)).unsqueeze(0)\n        # ilr = C(i->r) + C(j->r+1)\n        ilr = stripe(s_c, n, w) + stripe(s_c, n, w, (w, 1))\n        # [batch_size, n, w]\n        il = ir = ilr.permute(2, 0, 1)\n        # I(j->i) = max(C(i->r) + C(j->r+1) + s(j->i)), i <= r < j\n        il_span, il_path = il.max(-1)\n        s_i.diagonal(-w).copy_(il_span + scores.diagonal(-w))\n        p_i.diagonal(-w).copy_(il_path + starts)\n        # I(i->j) = max(C(i->r) + C(j->r+1) + s(i->j)), i <= r < j\n        ir_span, ir_path = ir.max(-1)\n        s_i.diagonal(w).copy_(ir_span + scores.diagonal(w))\n        p_i.diagonal(w).copy_(ir_path + starts)\n\n        # C(j->i) = max(C(r->i) + I(j->r)), i <= r < j\n        cl = stripe(s_c, n, w, (0, 0), 0) + stripe(s_i, n, w, (w, 0))\n        cl_span, cl_path = cl.permute(2, 0, 1).max(-1)\n        s_c.diagonal(-w).copy_(cl_span)\n        p_c.diagonal(-w).copy_(cl_path + starts)\n        # C(i->j) = max(I(i->r) + C(r->j)), i < r <= j\n        cr = stripe(s_i, n, w, (0, 1)) + stripe(s_c, n, w, (1, w), 0)\n        cr_span, cr_path = cr.permute(2, 0, 1).max(-1)\n        s_c.diagonal(w).copy_(cr_span)\n        s_c[0, w][lens.ne(w)] = float('-inf')\n        p_c.diagonal(w).copy_(cr_path + starts + 1)\n\n    def backtrack(p_i, p_c, heads, i, j, complete):\n        if i == j:\n            return\n        if complete:\n            r = p_c[i, j]\n            backtrack(p_i, p_c, heads, i, r, False)\n            backtrack(p_i, p_c, heads, r, j, True)\n        else:\n            r, heads[j] = p_i[i, j], i\n            i, j = sorted((i, j))\n            backtrack(p_i, p_c, heads, i, r, True)\n            backtrack(p_i, p_c, heads, j, r + 1, True)\n\n    preds = []\n    p_c = p_c.permute(2, 0, 1).cpu()\n    p_i = p_i.permute(2, 0, 1).cpu()\n    for i, length in enumerate(lens.tolist()):\n        heads = p_c.new_zeros(length + 1, dtype=torch.long)\n        backtrack(p_i[i], p_c[i], heads, 0, length, True)\n        preds.append(heads.to(mask.device))\n\n    return pad(preds, total_length=seq_len).to(mask.device)\n\n\ndef backtrack(p_i, p_c, heads, i, j, complete):\n    if i == j:\n        return\n    if complete:\n        r = p_c[i, j]\n        backtrack(p_i, p_c, heads, i, r, False)\n        backtrack(p_i, p_c, heads, r, j, True)\n    else:\n        r, heads[j] = p_i[i, j], i\n        i, j = sorted((i, j))\n        backtrack(p_i, p_c, heads, i, r, True)\n        backtrack(p_i, p_c, heads, j, r + 1, True)\n\n\ndef stripe(x, n, w, offset=(0, 0), dim=1):\n    \"\"\"r'''Returns a diagonal stripe of the tensor.\n\n    Args:\n      x: Tensor\n      n: int\n      w: int\n      offset: tuple (Default value = (0)\n      dim: int (Default value = 1)\n      Example: \n      0): \n\n    Returns:\n\n    >>> x = torch.arange(25).view(5, 5)\n    >>> x\n    tensor([[ 0,  1,  2,  3,  4],\n            [ 5,  6,  7,  8,  9],\n            [10, 11, 12, 13, 14],\n            [15, 16, 17, 18, 19],\n            [20, 21, 22, 23, 24]])\n    >>> stripe(x, 2, 3, (1, 1))\n    tensor([[ 6,  7,  8],\n            [12, 13, 14]])\n    >>> stripe(x, 2, 3, dim=0)\n    tensor([[ 0,  5, 10],\n            [ 6, 11, 16]])\n    \"\"\"\n    x, seq_len = x.contiguous(), x.size(1)\n    stride, numel = list(x.stride()), x[0, 0].numel()\n    stride[0] = (seq_len + 1) * numel\n    stride[1] = (1 if dim == 1 else seq_len) * numel\n    return x.as_strided(size=(n, w, *x.shape[2:]),\n                        stride=stride,\n                        storage_offset=(offset[0] * seq_len + offset[1]) * numel)\n\n\ndef cky(scores, mask):\n    r\"\"\"\n    The implementation of `Cocke-Kasami-Younger`_ (CKY) algorithm to parse constituency trees.\n\n    References:\n        - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.\n          `Fast and Accurate Neural CRF Constituency Parsing`_.\n\n    Args:\n        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n            Scores of all candidate constituents.\n        mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.\n            The mask to avoid parsing over padding tokens.\n            For each square matrix in a batch, the positions except upper triangular part should be masked out.\n\n    Returns:\n        Sequences of factorized predicted bracketed trees that are traversed in pre-order.\n\n    Examples:\n        >>> scores = torch.tensor([[[ 2.5659,  1.4253, -2.5272,  3.3011],\n                                    [ 1.3687, -0.5869,  1.0011,  3.3020],\n                                    [ 1.2297,  0.4862,  1.1975,  2.5387],\n                                    [-0.0511, -1.2541, -0.7577,  0.2659]]])\n        >>> mask = torch.tensor([[[False,  True,  True,  True],\n                                  [False, False,  True,  True],\n                                  [False, False, False,  True],\n                                  [False, False, False, False]]])\n        >>> cky(scores, mask)\n        [[(0, 3), (0, 1), (1, 3), (1, 2), (2, 3)]]\n\n    .. _Cocke-Kasami-Younger:\n        https://en.wikipedia.org/wiki/CYK_algorithm\n    .. _Fast and Accurate Neural CRF Constituency Parsing:\n        https://www.ijcai.org/Proceedings/2020/560/\n    \"\"\"\n\n    lens = mask[:, 0].sum(-1)\n    scores = scores.permute(1, 2, 0)\n    seq_len, seq_len, batch_size = scores.shape\n    s = scores.new_zeros(seq_len, seq_len, batch_size)\n    p = scores.new_zeros(seq_len, seq_len, batch_size).long()\n\n    for w in range(1, seq_len):\n        n = seq_len - w\n        starts = p.new_tensor(range(n)).unsqueeze(0)\n\n        if w == 1:\n            s.diagonal(w).copy_(scores.diagonal(w))\n            continue\n        # [n, w, batch_size]\n        s_span = stripe(s, n, w - 1, (0, 1)) + stripe(s, n, w - 1, (1, w), 0)\n        # [batch_size, n, w]\n        s_span = s_span.permute(2, 0, 1)\n        # [batch_size, n]\n        s_span, p_span = s_span.max(-1)\n        s.diagonal(w).copy_(s_span + scores.diagonal(w))\n        p.diagonal(w).copy_(p_span + starts + 1)\n\n    def backtrack(p, i, j):\n        if j == i + 1:\n            return [(i, j)]\n        split = p[i][j]\n        ltree = backtrack(p, i, split)\n        rtree = backtrack(p, split, j)\n        return [(i, j)] + ltree + rtree\n\n    p = p.permute(2, 0, 1).tolist()\n    trees = [backtrack(p[i], 0, length) if length else [] for i, length in enumerate(lens.tolist())]\n\n    return trees\n\n\ndef istree(sequence, proj=False, multiroot=False):\n    r\"\"\"\n    Checks if the arcs form an valid dependency tree.\n\n    Args:\n        sequence (list[int]):\n            A list of head indices.\n        proj (bool):\n            If ``True``, requires the tree to be projective. Default: ``False``.\n        multiroot (bool):\n            If ``False``, requires the tree to contain only a single root. Default: ``True``.\n\n    Returns:\n        ``True`` if the arcs form an valid tree, ``False`` otherwise.\n\n    Examples:\n        >>> istree([3, 0, 0, 3], multiroot=True)\n        True\n        >>> istree([3, 0, 0, 3], proj=True)\n        False\n    \"\"\"\n\n    if proj and not isprojective(sequence):\n        return False\n    n_roots = sum(head == 0 for head in sequence)\n    if n_roots == 0:\n        return False\n    if not multiroot and n_roots > 1:\n        return False\n    if any(i == head for i, head in enumerate(sequence, 1)):\n        return False\n    return next(tarjan(sequence), None) is None\n\n\ndef tarjan(sequence):\n    r\"\"\"\n    Tarjan algorithm for finding Strongly Connected Components (SCCs) of a graph.\n\n    Args:\n        sequence (list):\n            List of head indices.\n\n    Yields:\n        A list of indices that make up a SCC. All self-loops are ignored.\n\n    Examples:\n        >>> next(tarjan([2, 5, 0, 3, 1]))  # (1 -> 5 -> 2 -> 1) is a cycle\n        [2, 5, 1]\n    \"\"\"\n\n    sequence = [-1] + sequence\n    # record the search order, i.e., the timestep\n    dfn = [-1] * len(sequence)\n    # record the the smallest timestep in a SCC\n    low = [-1] * len(sequence)\n    # push the visited into the stack\n    stack, onstack = [], [False] * len(sequence)\n\n    def connect(i, timestep):\n        dfn[i] = low[i] = timestep[0]\n        timestep[0] += 1\n        stack.append(i)\n        onstack[i] = True\n\n        for j, head in enumerate(sequence):\n            if head != i:\n                continue\n            if dfn[j] == -1:\n                yield from connect(j, timestep)\n                low[i] = min(low[i], low[j])\n            elif onstack[j]:\n                low[i] = min(low[i], dfn[j])\n\n        # a SCC is completed\n        if low[i] == dfn[i]:\n            cycle = [stack.pop()]\n            while cycle[-1] != i:\n                onstack[cycle[-1]] = False\n                cycle.append(stack.pop())\n            onstack[i] = False\n            # ignore the self-loop\n            if len(cycle) > 1:\n                yield cycle\n\n    timestep = [0]\n    for i in range(len(sequence)):\n        if dfn[i] == -1:\n            yield from connect(i, timestep)\n\n\ndef chuliu_edmonds(s):\n    r\"\"\"\n    ChuLiu/Edmonds algorithm for non-projective decoding.\n\n    Some code is borrowed from `tdozat's implementation`_.\n    Descriptions of notations and formulas can be found in\n    `Non-projective Dependency Parsing using Spanning Tree Algorithms`_.\n\n    Notes:\n        The algorithm does not guarantee to parse a single-root tree.\n\n    References:\n        - Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan Hajic. 2005.\n          `Non-projective Dependency Parsing using Spanning Tree Algorithms`_.\n\n    Args:\n        s (~torch.Tensor): ``[seq_len, seq_len]``.\n            Scores of all dependent-head pairs.\n\n    Returns:\n        ~torch.Tensor:\n            A tensor with shape ``[seq_len]`` for the resulting non-projective parse tree.\n\n    .. _tdozat's implementation:\n        https://github.com/tdozat/Parser-v3\n    .. _Non-projective Dependency Parsing using Spanning Tree Algorithms:\n        https://www.aclweb.org/anthology/H05-1066/\n    \"\"\"\n\n    s[0, 1:] = float('-inf')\n    # prevent self-loops\n    s.diagonal()[1:].fill_(float('-inf'))\n    # select heads with highest scores\n    tree = s.argmax(-1)\n    # return the cycle finded by tarjan algorithm lazily\n    cycle = next(tarjan(tree.tolist()[1:]), None)\n    # if the tree has no cycles, then it is a MST\n    if not cycle:\n        return tree\n    # indices of cycle in the original tree\n    cycle = torch.tensor(cycle)\n    # indices of noncycle in the original tree\n    noncycle = torch.ones(len(s)).index_fill_(0, cycle, 0)\n    noncycle = torch.where(noncycle.gt(0))[0]\n\n    def contract(s):\n        # heads of cycle in original tree\n        cycle_heads = tree[cycle]\n        # scores of cycle in original tree\n        s_cycle = s[cycle, cycle_heads]\n\n        # calculate the scores of cycle's potential dependents\n        # s(c->x) = max(s(x'->x)), x in noncycle and x' in cycle\n        s_dep = s[noncycle][:, cycle]\n        # find the best cycle head for each noncycle dependent\n        deps = s_dep.argmax(1)\n        # calculate the scores of cycle's potential heads\n        # s(x->c) = max(s(x'->x) - s(a(x')->x') + s(cycle)), x in noncycle and x' in cycle\n        #                                                    a(v) is the predecessor of v in cycle\n        #                                                    s(cycle) = sum(s(a(v)->v))\n        s_head = s[cycle][:, noncycle] - s_cycle.view(-1, 1) + s_cycle.sum()\n        # find the best noncycle head for each cycle dependent\n        heads = s_head.argmax(0)\n\n        contracted = torch.cat((noncycle, torch.tensor([-1])))\n        # calculate the scores of contracted graph\n        s = s[contracted][:, contracted]\n        # set the contracted graph scores of cycle's potential dependents\n        s[:-1, -1] = s_dep[range(len(deps)), deps]\n        # set the contracted graph scores of cycle's potential heads\n        s[-1, :-1] = s_head[heads, range(len(heads))]\n\n        return s, heads, deps\n\n    # keep track of the endpoints of the edges into and out of cycle for reconstruction later\n    s, heads, deps = contract(s)\n\n    # y is the contracted tree\n    y = chuliu_edmonds(s)\n    # exclude head of cycle from y\n    y, cycle_head = y[:-1], y[-1]\n\n    # fix the subtree with no heads coming from the cycle\n    # len(y) denotes heads coming from the cycle\n    subtree = y < len(y)\n    # add the nodes to the new tree\n    tree[noncycle[subtree]] = noncycle[y[subtree]]\n    # fix the subtree with heads coming from the cycle\n    subtree = ~subtree\n    # add the nodes to the tree\n    tree[noncycle[subtree]] = cycle[deps[subtree]]\n    # fix the root of the cycle\n    cycle_root = heads[cycle_head]\n    # break the cycle and add the root of the cycle to the tree\n    tree[cycle[cycle_root]] = noncycle[cycle_head]\n\n    return tree\n\n\ndef mst(scores, mask, multiroot=False):\n    r\"\"\"\n    MST algorithm for decoding non-pojective trees.\n    This is a wrapper for ChuLiu/Edmonds algorithm.\n\n    The algorithm first runs ChuLiu/Edmonds to parse a tree and then have a check of multi-roots,\n    If ``multiroot=True`` and there indeed exist multi-roots, the algorithm seeks to find\n    best single-root trees by iterating all possible single-root trees parsed by ChuLiu/Edmonds.\n    Otherwise the resulting trees are directly taken as the final outputs.\n\n    Args:\n        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n            Scores of all dependent-head pairs.\n        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n            The mask to avoid parsing over padding tokens.\n            The first column serving as pseudo words for roots should be ``False``.\n        muliroot (bool):\n            Ensures to parse a single-root tree If ``False``.\n\n    Returns:\n        ~torch.Tensor:\n            A tensor with shape ``[batch_size, seq_len]`` for the resulting non-projective parse trees.\n\n    Examples:\n        >>> scores = torch.tensor([[[-11.9436, -13.1464,  -6.4789, -13.8917],\n                                    [-60.6957, -60.2866, -48.6457, -63.8125],\n                                    [-38.1747, -49.9296, -45.2733, -49.5571],\n                                    [-19.7504, -23.9066,  -9.9139, -16.2088]]])\n        >>> scores[:, 0, 1:] = float('-inf')\n        >>> scores.diagonal(0, 1, 2)[1:].fill_(float('-inf'))\n        >>> mask = torch.tensor([[False,  True,  True,  True]])\n        >>> mst(scores, mask)\n        tensor([[0, 2, 0, 2]])\n    \"\"\"\n\n    batch_size, seq_len, _ = scores.shape\n    scores = scores.detach().cpu().unbind()\n\n    preds = []\n    for i, length in enumerate(mask.sum(1).tolist()):\n        s = scores[i][:length + 1, :length + 1]\n        tree = chuliu_edmonds(s)\n        roots = torch.where(tree[1:].eq(0))[0] + 1\n        if not multiroot and len(roots) > 1:\n            s_root = s[:, 0]\n            s_best = float('-inf')\n            s = s.index_fill(1, torch.tensor(0), float('-inf'))\n            for root in roots:\n                s[:, 0] = float('-inf')\n                s[root, 0] = s_root[root]\n                t = chuliu_edmonds(s)\n                s_tree = s[1:].gather(1, t[1:].unsqueeze(-1)).sum()\n                if s_tree > s_best:\n                    s_best, tree = s_tree, t\n        preds.append(tree)\n\n    return pad(preds, total_length=seq_len).to(mask.device)\n\n\ndef eisner2o(scores, mask):\n    r\"\"\"\n    Second-order Eisner algorithm for projective decoding.\n    This is an extension of the first-order one that further incorporates sibling scores into tree scoring.\n\n    References:\n        - Ryan McDonald and Fernando Pereira. 2006.\n          `Online Learning of Approximate Dependency Parsing Algorithms`_.\n\n    Args:\n        scores (~torch.Tensor, ~torch.Tensor):\n            A tuple of two tensors representing the first-order and second-order scores repectively.\n            The first (``[batch_size, seq_len, seq_len]``) holds scores of all dependent-head pairs.\n            The second (``[batch_size, seq_len, seq_len, seq_len]``) holds scores of all dependent-head-sibling triples.\n        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n            The mask to avoid parsing over padding tokens.\n            The first column serving as pseudo words for roots should be ``False``.\n\n    Returns:\n        ~torch.Tensor:\n            A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.\n\n    Examples:\n        >>> s_arc = torch.tensor([[[ -2.8092,  -7.9104,  -0.9414,  -5.4360],\n                                   [-10.3494,  -7.9298,  -3.6929,  -7.3985],\n                                   [  1.1815,  -3.8291,   2.3166,  -2.7183],\n                                   [ -3.9776,  -3.9063,  -1.6762,  -3.1861]]])\n        >>> s_sib = torch.tensor([[[[ 0.4719,  0.4154,  1.1333,  0.6946],\n                                    [ 1.1252,  1.3043,  2.1128,  1.4621],\n                                    [ 0.5974,  0.5635,  1.0115,  0.7550],\n                                    [ 1.1174,  1.3794,  2.2567,  1.4043]],\n                                   [[-2.1480, -4.1830, -2.5519, -1.8020],\n                                    [-1.2496, -1.7859, -0.0665, -0.4938],\n                                    [-2.6171, -4.0142, -2.9428, -2.2121],\n                                    [-0.5166, -1.0925,  0.5190,  0.1371]],\n                                   [[ 0.5827, -1.2499, -0.0648, -0.0497],\n                                    [ 1.4695,  0.3522,  1.5614,  1.0236],\n                                    [ 0.4647, -0.7996, -0.3801,  0.0046],\n                                    [ 1.5611,  0.3875,  1.8285,  1.0766]],\n                                   [[-1.3053, -2.9423, -1.5779, -1.2142],\n                                    [-0.1908, -0.9699,  0.3085,  0.1061],\n                                    [-1.6783, -2.8199, -1.8853, -1.5653],\n                                    [ 0.3629, -0.3488,  0.9011,  0.5674]]]])\n        >>> mask = torch.tensor([[False,  True,  True,  True]])\n        >>> eisner2o((s_arc, s_sib), mask)\n        tensor([[0, 2, 0, 2]])\n\n    .. _Online Learning of Approximate Dependency Parsing Algorithms:\n        https://www.aclweb.org/anthology/E06-1011/\n    \"\"\"\n\n    # the end position of each sentence in a batch\n    lens = mask.sum(1)\n    s_arc, s_sib = scores\n    batch_size, seq_len, _ = s_arc.shape\n    # [seq_len, seq_len, batch_size]\n    s_arc = s_arc.permute(2, 1, 0)\n    # [seq_len, seq_len, seq_len, batch_size]\n    s_sib = s_sib.permute(2, 1, 3, 0)\n    s_i = torch.full_like(s_arc, float('-inf'))\n    s_s = torch.full_like(s_arc, float('-inf'))\n    s_c = torch.full_like(s_arc, float('-inf'))\n    p_i = s_arc.new_zeros(seq_len, seq_len, batch_size).long()\n    p_s = s_arc.new_zeros(seq_len, seq_len, batch_size).long()\n    p_c = s_arc.new_zeros(seq_len, seq_len, batch_size).long()\n    s_c.diagonal().fill_(0)\n\n    for w in range(1, seq_len):\n        # n denotes the number of spans to iterate,\n        # from span (0, w) to span (n, n+w) given width w\n        n = seq_len - w\n        starts = p_i.new_tensor(range(n)).unsqueeze(0)\n        # I(j->i) = max(I(j->r) + S(j->r, i)), i < r < j |\n        #               C(j->j) + C(i->j-1))\n        #           + s(j->i)\n        # [n, w, batch_size]\n        il = stripe(s_i, n, w, (w, 1)) + stripe(s_s, n, w, (1, 0), 0)\n        il += stripe(s_sib[range(w, n + w), range(n)], n, w, (0, 1))\n        # [n, 1, batch_size]\n        il0 = stripe(s_c, n, 1, (w, w)) + stripe(s_c, n, 1, (0, w - 1))\n        # il0[0] are set to zeros since the scores of the complete spans starting from 0 are always -inf\n        il[:, -1] = il0.index_fill_(0, lens.new_tensor(0), 0).squeeze(1)\n        il_span, il_path = il.permute(2, 0, 1).max(-1)\n        s_i.diagonal(-w).copy_(il_span + s_arc.diagonal(-w))\n        p_i.diagonal(-w).copy_(il_path + starts + 1)\n        # I(i->j) = max(I(i->r) + S(i->r, j), i < r < j |\n        #               C(i->i) + C(j->i+1))\n        #           + s(i->j)\n        # [n, w, batch_size]\n        ir = stripe(s_i, n, w) + stripe(s_s, n, w, (0, w), 0)\n        ir += stripe(s_sib[range(n), range(w, n + w)], n, w)\n        ir[0] = float('-inf')\n        # [n, 1, batch_size]\n        ir0 = stripe(s_c, n, 1) + stripe(s_c, n, 1, (w, 1))\n        ir[:, 0] = ir0.squeeze(1)\n        ir_span, ir_path = ir.permute(2, 0, 1).max(-1)\n        s_i.diagonal(w).copy_(ir_span + s_arc.diagonal(w))\n        p_i.diagonal(w).copy_(ir_path + starts)\n\n        # [n, w, batch_size]\n        slr = stripe(s_c, n, w) + stripe(s_c, n, w, (w, 1))\n        slr_span, slr_path = slr.permute(2, 0, 1).max(-1)\n        # S(j, i) = max(C(i->r) + C(j->r+1)), i <= r < j\n        s_s.diagonal(-w).copy_(slr_span)\n        p_s.diagonal(-w).copy_(slr_path + starts)\n        # S(i, j) = max(C(i->r) + C(j->r+1)), i <= r < j\n        s_s.diagonal(w).copy_(slr_span)\n        p_s.diagonal(w).copy_(slr_path + starts)\n\n        # C(j->i) = max(C(r->i) + I(j->r)), i <= r < j\n        cl = stripe(s_c, n, w, (0, 0), 0) + stripe(s_i, n, w, (w, 0))\n        cl_span, cl_path = cl.permute(2, 0, 1).max(-1)\n        s_c.diagonal(-w).copy_(cl_span)\n        p_c.diagonal(-w).copy_(cl_path + starts)\n        # C(i->j) = max(I(i->r) + C(r->j)), i < r <= j\n        cr = stripe(s_i, n, w, (0, 1)) + stripe(s_c, n, w, (1, w), 0)\n        cr_span, cr_path = cr.permute(2, 0, 1).max(-1)\n        s_c.diagonal(w).copy_(cr_span)\n        # disable multi words to modify the root\n        s_c[0, w][lens.ne(w)] = float('-inf')\n        p_c.diagonal(w).copy_(cr_path + starts + 1)\n\n    def backtrack(p_i, p_s, p_c, heads, i, j, flag):\n        if i == j:\n            return\n        if flag == 'c':\n            r = p_c[i, j]\n            backtrack(p_i, p_s, p_c, heads, i, r, 'i')\n            backtrack(p_i, p_s, p_c, heads, r, j, 'c')\n        elif flag == 's':\n            r = p_s[i, j]\n            i, j = sorted((i, j))\n            backtrack(p_i, p_s, p_c, heads, i, r, 'c')\n            backtrack(p_i, p_s, p_c, heads, j, r + 1, 'c')\n        elif flag == 'i':\n            r, heads[j] = p_i[i, j], i\n            if r == i:\n                r = i + 1 if i < j else i - 1\n                backtrack(p_i, p_s, p_c, heads, j, r, 'c')\n            else:\n                backtrack(p_i, p_s, p_c, heads, i, r, 'i')\n                backtrack(p_i, p_s, p_c, heads, r, j, 's')\n\n    preds = []\n    p_i = p_i.permute(2, 0, 1).cpu()\n    p_s = p_s.permute(2, 0, 1).cpu()\n    p_c = p_c.permute(2, 0, 1).cpu()\n    for i, length in enumerate(lens.tolist()):\n        heads = p_c.new_zeros(length + 1, dtype=torch.long)\n        backtrack(p_i[i], p_s[i], p_c[i], heads, 0, length, 'c')\n        preds.append(heads.to(mask.device))\n\n    return pad(preds, total_length=seq_len).to(mask.device)\n\n\ndef pad(tensors, padding_value=0, total_length=None):\n    size = [len(tensors)] + [max(tensor.size(i) for tensor in tensors)\n                             for i in range(len(tensors[0].size()))]\n    if total_length is not None:\n        assert total_length >= size[1]\n        size[1] = total_length\n    out_tensor = tensors[0].data.new(*size).fill_(padding_value)\n    for i, tensor in enumerate(tensors):\n        out_tensor[i][[slice(0, i) for i in tensor.size()]] = tensor\n    return out_tensor\n\n\ndef decode_dep(s_arc, mask, tree=False, proj=False):\n    r\"\"\"\n    Args:\n        s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n            Scores of all possible arcs.\n        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n            The mask for covering the unpadded tokens.\n        tree (bool):\n            If ``True``, ensures to output well-formed trees. Default: ``False``.\n        proj (bool):\n            If ``True``, ensures to output projective trees. Default: ``False``.\n\n    Returns:\n        ~torch.Tensor, ~torch.Tensor:\n            Predicted arcs and labels of shape ``[batch_size, seq_len]``.\n    \"\"\"\n\n    lens = mask.sum(1)\n    arc_preds = s_arc.argmax(-1)\n    bad = [not istree(seq[1:i + 1], proj) for i, seq in zip(lens.tolist(), arc_preds.tolist())]\n    if tree and any(bad):\n        if proj:\n            alg = eisner\n        else:\n            alg = mst\n            s_arc.diagonal(0, 1, 2)[1:].fill_(float('-inf'))\n        arc_preds[bad] = alg(s_arc[bad], mask[bad])\n\n    return arc_preds\n", "hanlp/components/parsers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-22 12:46", "hanlp/components/parsers/biaffine_parser_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-22 12:47\nimport logging\nimport math\nimport os\nfrom typing import List\nimport numpy as np\nimport tensorflow as tf\n\nfrom hanlp.components.parsers.parse_alg import unique_root, adjust_root_score, chu_liu_edmonds\nfrom hanlp.layers.transformers.loader_tf import build_transformer\n\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp.components.parsers.alg_tf import tarjan\nfrom hanlp.components.parsers.biaffine_tf.model import BiaffineModelTF\nfrom hanlp.transform.conll_tf import CoNLL_DEP_Transform, CoNLL_Transformer_Transform, CoNLL_SDP_Transform\nfrom hanlp.layers.embeddings.util_tf import build_embedding\nfrom hanlp.layers.transformers.tf_imports import PreTrainedTokenizer, TFAutoModel, TFPreTrainedModel, AutoTokenizer, \\\n    TFAutoModelWithLMHead, BertTokenizerFast, AlbertConfig, BertTokenizer, TFBertModel\nfrom hanlp.layers.transformers.utils_tf import build_adamw_optimizer\nfrom hanlp.metrics.parsing.labeled_f1_tf import LabeledF1TF\nfrom hanlp.metrics.parsing.labeled_score import LabeledScore\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BiaffineDependencyParserTF(KerasComponent):\n    def __init__(self, transform: CoNLL_DEP_Transform = None) -> None:\n        if not transform:\n            transform = CoNLL_DEP_Transform()\n        super().__init__(transform)\n        self.transform: CoNLL_DEP_Transform = transform\n        self.model: BiaffineModelTF = None\n\n    def build_model(self, pretrained_embed, n_embed, training, **kwargs) -> tf.keras.Model:\n        if training:\n            self.config.n_words = len(self.transform.form_vocab)\n        else:\n            self.config.lstm_dropout = 0.  # keras will use cuda lstm when config.lstm_dropout is 0\n        self.config.n_feats = len(self.transform.cpos_vocab)\n        self._init_config()\n        pretrained: tf.keras.layers.Embedding = build_embedding(pretrained_embed, self.transform.form_vocab,\n                                                                self.transform) if pretrained_embed else None\n        if pretrained_embed:\n            self.config.n_embed = pretrained.output_dim\n        model = BiaffineModelTF(self.config, pretrained)\n        return model\n\n    def _init_config(self):\n        self.config.n_rels = len(self.transform.rel_vocab)\n        self.config.pad_index = self.transform.form_vocab.pad_idx\n        self.config.unk_index = self.transform.form_vocab.unk_idx\n        self.config.bos_index = 2\n\n    def load_weights(self, save_dir, filename='model.h5', functional=False, **kwargs):\n        super().load_weights(save_dir, filename)\n        if functional:\n            self.model = self.model.to_functional()\n\n    def fit(self, trn_data, dev_data, save_dir,\n            n_embed=100,\n            pretrained_embed=None,\n            embed_dropout=.33,\n            n_lstm_hidden=400,\n            n_lstm_layers=3,\n            lstm_dropout=.33,\n            n_mlp_arc=500,\n            n_mlp_rel=100,\n            mlp_dropout=.33,\n            optimizer='adam',\n            lr=2e-3,\n            mu=.9,\n            nu=.9,\n            epsilon=1e-12,\n            clip=5.0,\n            decay=.75,\n            decay_steps=5000,\n            patience=100,\n            arc_loss='sparse_categorical_crossentropy',\n            rel_loss='sparse_categorical_crossentropy',\n            metrics=('UAS', 'LAS'),\n            n_buckets=32,\n            batch_size=5000,\n            epochs=50000,\n            early_stopping_patience=100,\n            tree=False,\n            punct=False,\n            min_freq=2,\n            run_eagerly=False, logger=None, verbose=True,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    # noinspection PyMethodOverriding\n    def train_loop(self, trn_data, dev_data, epochs, num_examples,\n                   train_steps_per_epoch, dev_steps, model, optimizer, loss, metrics,\n                   callbacks, logger: logging.Logger, arc_loss, rel_loss,\n                   **kwargs):\n        arc_loss, rel_loss = loss\n        # because we are customizing batching\n        train_steps_per_epoch = len(list(iter(trn_data)))\n        # progbar: tf.keras.callbacks.ProgbarLogger = callbacks[-1]\n        c: tf.keras.callbacks.Callback = None\n        metric = self._build_metrics()\n        for c in callbacks:\n            if not hasattr(c, 'params'):\n                c.params = dict()\n            c.params['epochs'] = epochs\n            c.params['trn_data'] = trn_data\n            c.params['metrics'] = ['loss'] + self.config.metrics\n            c.params['metrics'] = c.params['metrics'] + [f'val_{k}' for k in c.params['metrics']]\n            c.on_train_begin()\n        for epoch in range(epochs):\n            metric.reset_states()\n            for c in callbacks:\n                c.params['steps'] = train_steps_per_epoch\n                c.on_epoch_begin(epoch)\n            for idx, ((words, feats), (arcs, rels)) in enumerate(iter(trn_data)):\n                logs = {}\n                for c in callbacks:\n                    c.on_batch_begin(idx, logs)\n                mask = tf.not_equal(words, self.config.pad_index) & tf.not_equal(words, self.config.bos_index)\n                loss, arc_scores, rel_scores = self.train_batch(words, feats, arcs, rels, mask,\n                                                                optimizer, arc_loss, rel_loss)\n                self.run_metrics(arcs, rels, arc_scores, rel_scores, words, mask, metric)\n                logs['loss'] = loss\n                logs.update(metric.to_dict())\n                if epoch == epochs - 1:\n                    self.model.stop_training = True\n                for c in callbacks:\n                    c.on_batch_end(idx, logs)\n            # evaluate on dev\n            metric.reset_states()\n            logs = {}\n            for idx, ((words, feats), (arcs, rels)) in enumerate(iter(dev_data)):\n                arc_scores, rel_scores, loss, mask, arc_preds, rel_preds = self.evaluate_batch(words, feats, arcs, rels,\n                                                                                               arc_loss, rel_loss,\n                                                                                               metric)\n                logs['val_loss'] = loss\n                logs.update((f'val_{k}', v) for k, v in metric.to_dict().items())\n\n            for c in callbacks:\n                c.on_epoch_end(epoch, logs)\n            if getattr(self.model, 'stop_training', None):\n                break\n\n        for c in callbacks:\n            c.on_train_end()\n\n    def evaluate(self, input_path: str, save_dir=None, output=False, batch_size=None, logger: logging.Logger = None,\n                 callbacks: List[tf.keras.callbacks.Callback] = None, warm_up=False, verbose=True, **kwargs):\n        if batch_size is None:\n            batch_size = self.config.batch_size\n        return super().evaluate(input_path, save_dir, output, batch_size, logger, callbacks, warm_up, verbose, **kwargs)\n\n    def evaluate_batch(self, words, feats, arcs, rels, arc_loss, rel_loss, metric):\n        mask = tf.not_equal(words, self.config.pad_index) & tf.not_equal(words, self.config.bos_index)\n        arc_scores, rel_scores = self.model((words, feats))\n        loss = self.get_loss(arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss)\n        arc_preds, rel_preds = self.run_metrics(arcs, rels, arc_scores, rel_scores, words, mask, metric)\n        return arc_scores, rel_scores, loss, mask, arc_preds, rel_preds\n\n    def _build_metrics(self):\n        if isinstance(self.config.metrics, tuple):\n            self.config.metrics = list(self.config.metrics)\n        if self.config.metrics == ['UAS', 'LAS']:\n            metric = LabeledScore()\n        else:\n            metric = LabeledF1TF()\n        return metric\n\n    def run_metrics(self, arcs, rels, arc_scores, rel_scores, words, mask, metric):\n        arc_preds, rel_preds = self.decode(arc_scores, rel_scores, mask)\n        # ignore all punctuation if not specified\n        if not self.config.punct:\n            mask &= tf.reduce_all(tf.not_equal(tf.expand_dims(words, axis=-1), self.transform.puncts), axis=-1)\n        metric(arc_preds, rel_preds, arcs, rels, mask)\n        return arc_preds, rel_preds\n\n    def train_batch(self, words, feats, arcs, rels, mask, optimizer, arc_loss, rel_loss):\n        with tf.GradientTape() as tape:\n            arc_scores, rel_scores = self.model((words, feats), training=True)\n            loss = self.get_loss(arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss)\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n        return loss, arc_scores, rel_scores\n\n    def get_loss(self, arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss):\n        arc_scores, arcs = arc_scores[mask], arcs[mask]\n        rel_scores, rels = rel_scores[mask], rels[mask]\n        rel_scores = tf.gather_nd(rel_scores, tf.stack([tf.range(len(arcs), dtype=tf.int64), arcs], axis=1))\n        arc_loss = arc_loss(arcs, arc_scores)\n        rel_loss = rel_loss(rels, rel_scores)\n        loss = arc_loss + rel_loss\n\n        return loss\n\n    def build_optimizer(self, optimizer='adam', lr=2e-3, mu=.9, nu=.9, epsilon=1e-12, clip=5.0, decay=.75,\n                        decay_steps=5000, **kwargs):\n        if optimizer == 'adam':\n            scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr,\n                                                                       decay_steps=decay_steps,\n                                                                       decay_rate=decay)\n            optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler,\n                                                 beta_1=mu,\n                                                 beta_2=nu,\n                                                 epsilon=epsilon,\n                                                 clipnorm=clip)\n            return optimizer\n        return super().build_optimizer(optimizer, **kwargs)\n\n    # noinspection PyMethodOverriding\n    def build_loss(self, arc_loss, rel_loss, **kwargs):\n        if arc_loss == 'binary_crossentropy':\n            arc_loss = tf.losses.BinaryCrossentropy(from_logits=True)\n        else:\n            arc_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n                from_logits=True) if arc_loss == 'sparse_categorical_crossentropy' else super().build_loss(arc_loss)\n        rel_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True) if rel_loss == 'sparse_categorical_crossentropy' else super().build_loss(rel_loss)\n        return arc_loss, rel_loss\n\n    @property\n    def sample_data(self):\n        return tf.constant([[2, 3, 4], [2, 5, 0]], dtype=tf.int64), tf.constant([[1, 2, 3], [4, 5, 0]], dtype=tf.int64)\n\n    def num_samples_in(self, dataset):\n        return sum(len(x[0][0]) for x in iter(dataset))\n\n    def build_train_dataset(self, trn_data, batch_size, num_examples):\n        trn_data = self.transform.file_to_dataset(trn_data, batch_size=batch_size,\n                                                  shuffle=True,\n                                                  repeat=None)\n        return trn_data\n\n    # noinspection PyMethodOverriding\n    def build_callbacks(self, save_dir, logger, metrics, **kwargs):\n        callbacks = super().build_callbacks(save_dir, logger, metrics=metrics, **kwargs)\n        if isinstance(metrics, tuple):\n            metrics = list(metrics)\n        callbacks.append(self.build_progbar(metrics))\n        params = {'verbose': 1, 'epochs': 1}\n        for c in callbacks:\n            c.set_params(params)\n            c.set_model(self.model)\n        return callbacks\n\n    def build_progbar(self, metrics, training=True):\n        return tf.keras.callbacks.ProgbarLogger(count_mode='steps',\n                                                stateful_metrics=metrics + [f'val_{k}' for k in metrics] if training\n                                                else [])\n\n    def decode(self, arc_scores, rel_scores, mask):\n        if self.config.tree:\n            root_rel_idx = self.transform.root_rel_idx\n            root_rel_onehot = np.eye(len(self.transform.rel_vocab))[root_rel_idx]\n            arc_preds = np.zeros_like(mask, dtype=np.int64)\n            rel_preds = np.zeros_like(mask, dtype=np.int64)\n            for arc, rel, m, arc_pred, rel_pred in zip(arc_scores, rel_scores, mask, arc_preds, rel_preds):\n                length = int(tf.math.count_nonzero(m)) + 1\n                arc = arc[:length, :length]\n                arc_probs = tf.nn.softmax(arc).numpy()\n                m = np.expand_dims(m.numpy()[:length], -1)\n                if self.config.tree == 'tarjan':\n                    heads = tarjan(arc_probs, length, m)\n                elif self.config.tree == 'mst':\n                    heads, head_probs, tokens = unique_root(arc_probs, m, length)\n                    arc = arc.numpy()\n                    adjust_root_score(arc, heads, root_rel_idx)\n                    heads = chu_liu_edmonds(arc, length)\n                else:\n                    raise ValueError(f'Unknown tree algorithm {self.config.tree}')\n                arc_pred[:length] = heads\n                root = np.where(heads[np.arange(1, length)] == 0)[0] + 1\n                rel_prob = tf.nn.softmax(rel[:length, :length, :]).numpy()\n                rel_prob = rel_prob[np.arange(length), heads]\n                rel_prob[root] = root_rel_onehot\n                rel_prob[np.arange(length) != root, np.arange(len(self.transform.rel_vocab)) == root_rel_idx] = 0\n                # rels = rel_argmax(rel_prob, length, root_rel_idx)\n                rels = np.argmax(rel_prob, axis=1)\n                rel_pred[:length] = rels\n            arc_preds = tf.constant(arc_preds)\n            rel_preds = tf.constant(rel_preds)\n        else:\n            arc_preds = tf.argmax(arc_scores, -1)\n            rel_preds = tf.argmax(rel_scores, -1)\n            rel_preds = tf.squeeze(tf.gather(rel_preds, tf.expand_dims(arc_preds, -1), batch_dims=2), axis=-1)\n\n        return arc_preds, rel_preds\n\n    def evaluate_dataset(self, tst_data, callbacks, output, num_batches, ret_scores=None, **kwargs):\n        if 'mask_p' in self.config:\n            self.config['mask_p'] = None\n        arc_loss, rel_loss = self.build_loss(**self.config)\n        callbacks = [self.build_progbar(self.config['metrics'])]\n        steps_per_epoch = len(list(iter(tst_data)))\n        metric = self._build_metrics()\n        params = {'verbose': 1, 'epochs': 1, 'metrics': ['loss'] + self.config.metrics, 'steps': steps_per_epoch}\n        for c in callbacks:\n            c.set_params(params)\n            c.on_test_begin()\n            c.on_epoch_end(0)\n        logs = {}\n        if ret_scores:\n            scores = []\n        if output:\n            ext = os.path.splitext(output)[-1]\n            output = open(output, 'w', encoding='utf-8')\n        for idx, ((words, feats), Y) in enumerate(iter(tst_data)):\n            arcs, rels = Y[0], Y[1]\n            for c in callbacks:\n                c.on_test_batch_begin(idx, logs)\n            arc_scores, rel_scores, loss, mask, arc_preds, rel_preds = self.evaluate_batch(words, feats, arcs, rels,\n                                                                                           arc_loss, rel_loss, metric)\n            if ret_scores:\n                scores.append((arc_scores.numpy(), rel_scores.numpy(), mask.numpy()))\n            if output:\n                for sent in self.transform.XY_to_inputs_outputs((words, feats, mask), (arc_preds, rel_preds),\n                                                                conll=ext, arc_scores=arc_scores,\n                                                                rel_scores=rel_scores):\n                    output.write(str(sent))\n                    output.write('\\n\\n')\n            logs['loss'] = loss\n            logs.update(metric.to_dict())\n            for c in callbacks:\n                c.on_test_batch_end(idx, logs)\n        for c in callbacks:\n            c.on_epoch_end(0)\n            c.on_test_end()\n        if output:\n            output.close()\n        loss = float(c.progbar._values['loss'][0] / c.progbar._values['loss'][1])\n        outputs = loss, metric.to_dict(), False\n        if ret_scores:\n            outputs += (scores,)\n        return outputs\n\n    def predict_batch(self, batch, inputs=None, conll=True, **kwargs):\n        ((words, feats), (arcs, rels)) = batch\n        mask = tf.not_equal(words, self.config.pad_index) & tf.not_equal(words, self.config.bos_index)\n        arc_scores, rel_scores = self.model((words, feats))\n        arc_preds, rel_preds = self.decode(arc_scores, rel_scores, mask)\n        for sent in self.transform.XY_to_inputs_outputs((words, feats, mask), (arc_preds, rel_preds), gold=False,\n                                                        inputs=inputs, conll=conll):\n            yield sent\n\n    def compile_model(self, optimizer, loss, metrics):\n        super().compile_model(optimizer, loss, metrics)\n\n\nclass BiaffineSemanticDependencyParserTF(BiaffineDependencyParserTF):\n    def __init__(self, transform: CoNLL_SDP_Transform = None) -> None:\n        if not transform:\n            transform = CoNLL_SDP_Transform()\n        # noinspection PyTypeChecker\n        super().__init__(transform)\n        self.transform: CoNLL_SDP_Transform = transform\n\n    def fit(self, trn_data, dev_data, save_dir, n_embed=100, pretrained_embed=None, embed_dropout=.33,\n            n_lstm_hidden=400, n_lstm_layers=3, lstm_dropout=.33, n_mlp_arc=500, n_mlp_rel=100, mlp_dropout=.33,\n            optimizer='adam', lr=2e-3, mu=.9, nu=.9, epsilon=1e-12, clip=5.0, decay=.75, decay_steps=5000, patience=100,\n            arc_loss='binary_crossentropy', rel_loss='sparse_categorical_crossentropy',\n            metrics=('UF', 'LF'), n_buckets=32, batch_size=5000, epochs=50000, early_stopping_patience=100,\n            tree=False, punct=False, min_freq=2, run_eagerly=False, logger=None, verbose=True, **kwargs):\n        return super().fit(trn_data, dev_data, save_dir, n_embed, pretrained_embed, embed_dropout, n_lstm_hidden,\n                           n_lstm_layers, lstm_dropout, n_mlp_arc, n_mlp_rel, mlp_dropout, optimizer, lr, mu, nu,\n                           epsilon, clip, decay, decay_steps, patience, arc_loss, rel_loss, metrics, n_buckets,\n                           batch_size, epochs, early_stopping_patience, tree, punct, min_freq, run_eagerly, logger,\n                           verbose, **kwargs)\n\n    def get_loss(self, arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss):\n        mask = tf.tile(tf.expand_dims(mask, -1), [1, 1, tf.shape(mask)[-1]])\n        mask &= tf.transpose(mask, [0, 2, 1])\n        arc_scores, arcs = arc_scores[mask], arcs[mask]\n        rel_scores, rels = rel_scores[mask], rels[mask]\n        rel_scores, rels = rel_scores[arcs], rels[arcs]\n        arc_loss = arc_loss(arcs, arc_scores)\n        rel_loss = rel_loss(rels, rel_scores)\n        loss = arc_loss + rel_loss\n\n        return loss\n\n    def decode(self, arc_scores, rel_scores, mask):\n        arc_preds = arc_scores > 0\n        rel_preds = tf.argmax(rel_scores, -1)\n\n        return arc_preds, rel_preds\n\n\nclass BiaffineTransformerDependencyParserTF(BiaffineDependencyParserTF, tf.keras.callbacks.Callback):\n    def __init__(self, transform: CoNLL_Transformer_Transform = None) -> None:\n        if not transform:\n            transform = CoNLL_Transformer_Transform()\n        super().__init__(transform)\n        self.transform: CoNLL_Transformer_Transform = transform\n\n    def build_model(self, transformer, training, **kwargs) -> tf.keras.Model:\n        transformer = self.build_transformer(training, transformer)\n        model = BiaffineModelTF(self.config, transformer=transformer)\n        return model\n\n    def build_transformer(self, training, transformer):\n        if training:\n            self.config.n_words = len(self.transform.form_vocab)\n        self._init_config()\n        if isinstance(transformer, str):\n            if 'albert_chinese' in transformer:\n                tokenizer = BertTokenizerFast.from_pretrained(transformer, add_special_tokens=False)\n                transformer: TFPreTrainedModel = TFAutoModel.from_pretrained(transformer, name=transformer,\n                                                                             from_pt=True)\n            elif transformer.startswith('albert') and transformer.endswith('zh'):\n                transformer, tokenizer, path = build_transformer(transformer)\n                transformer.config = AlbertConfig.from_json_file(os.path.join(path, \"albert_config.json\"))\n                tokenizer = BertTokenizer.from_pretrained(os.path.join(path, \"vocab_chinese.txt\"),\n                                                          add_special_tokens=False)\n            elif 'chinese-roberta' in transformer:\n                tokenizer = BertTokenizer.from_pretrained(transformer)\n                transformer = TFBertModel.from_pretrained(transformer, name=transformer, from_pt=True)\n            else:\n                tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(transformer)\n                try:\n                    transformer: TFPreTrainedModel = TFAutoModel.from_pretrained(transformer, name=transformer)\n                except (TypeError, OSError):\n                    transformer: TFPreTrainedModel = TFAutoModel.from_pretrained(transformer, name=transformer,\n                                                                                 from_pt=True)\n        elif transformer[0] == 'AutoModelWithLMHead':\n            tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(transformer[1])\n            transformer: TFAutoModelWithLMHead = TFAutoModelWithLMHead.from_pretrained(transformer[1])\n        else:\n            raise ValueError(f'Unknown identifier {transformer}')\n        self.transform.tokenizer = tokenizer\n        if self.config.get('fp16', None) or self.config.get('use_amp', None):\n            policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n            tf.keras.mixed_precision.experimental.set_policy(policy)\n            # tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n            transformer.set_weights([w.astype('float16') for w in transformer.get_weights()])\n        self.transform.transformer_config = transformer.config\n        return transformer\n\n    # noinspection PyMethodOverriding\n    def fit(self, trn_data, dev_data, save_dir, transformer, max_seq_length=256, transformer_dropout=.33,\n            d_positional=None,\n            n_mlp_arc=500, n_mlp_rel=100, mlp_dropout=.33,\n            optimizer='adamw',\n            learning_rate=5e-5,\n            learning_rate_transformer=None,\n            weight_decay_rate=0,\n            epsilon=1e-8,\n            clipnorm=None,\n            fp16=False,\n            warmup_steps_ratio=0,\n            arc_loss='sparse_categorical_crossentropy', rel_loss='sparse_categorical_crossentropy',\n            metrics=('UAS', 'LAS'),\n            batch_size=3000,\n            samples_per_batch=150,\n            max_samples_per_batch=None,\n            epochs=100,\n            tree=False, punct=False, token_mapping=None, run_eagerly=False, logger=None, verbose=True, **kwargs):\n        self.set_params({})\n        return KerasComponent.fit(self, **merge_locals_kwargs(locals(), kwargs))\n\n    @property\n    def sample_data(self):\n        dataset = self.transform.inputs_to_dataset(\n            [[('Hello', 'NN'), ('world', 'NN')], [('HanLP', 'NN'), ('is', 'NN'), ('good', 'NN')]] if self.config.get(\n                'use_pos', None) else\n            [['Hello', 'world'], ['HanLP', 'is', 'good']])\n        return next(iter(dataset))[0]\n\n    # noinspection PyMethodOverriding\n    def build_optimizer(self, optimizer, learning_rate, epsilon, weight_decay_rate, clipnorm, fp16, train_steps,\n                        **kwargs):\n        if optimizer == 'adamw':\n            epochs = self.config['epochs']\n            learning_rate_transformer = kwargs.get('learning_rate_transformer', None)\n            train_steps = math.ceil(self.config.train_examples * epochs / self.config.samples_per_batch)\n            warmup_steps = math.ceil(train_steps * self.config['warmup_steps_ratio'])\n            if learning_rate_transformer is not None:\n                if learning_rate_transformer > 0:\n                    self.params['optimizer_transformer'] = build_adamw_optimizer(self.config, learning_rate_transformer,\n                                                                                 epsilon,\n                                                                                 clipnorm, train_steps, fp16,\n                                                                                 math.ceil(warmup_steps),\n                                                                                 weight_decay_rate)\n                else:\n                    self.model.transformer.trainable = False\n                return super().build_optimizer(lr=learning_rate)  # use a normal adam for biaffine\n            else:\n                return build_adamw_optimizer(self.config, learning_rate, epsilon, clipnorm, train_steps, fp16,\n                                             math.ceil(warmup_steps), weight_decay_rate)\n        return super().build_optimizer(optimizer, **kwargs)\n\n    def build_vocab(self, trn_data, logger):\n        self.config.train_examples = train_examples = super().build_vocab(trn_data, logger)\n        return train_examples\n\n    def build_callbacks(self, save_dir, logger, metrics, **kwargs):\n        callbacks = super().build_callbacks(save_dir, logger, metrics=metrics, **kwargs)\n        callbacks.append(self)\n        if not self.params:\n            self.set_params({})\n        return callbacks\n\n    def on_train_begin(self):\n        self.params['accum_grads'] = [tf.Variable(tf.zeros_like(tv.read_value()), trainable=False) for tv in\n                                      self.model.trainable_variables]\n        self.params['trained_samples'] = 0\n        self.params['transformer_variable_names'] = {x.name for x in self.model.transformer.trainable_variables}\n\n    def train_batch(self, words, feats, arcs, rels, mask, optimizer, arc_loss, rel_loss):\n        with tf.GradientTape() as tape:\n            arc_scores, rel_scores = self.model((words, feats), training=True)\n            loss = self.get_loss(arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss)\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        accum_grads = self.params['accum_grads']\n        for i, grad in enumerate(grads):\n            if grad is not None:\n                accum_grads[i].assign_add(grad)\n        self.params['trained_samples'] += tf.shape(words)[0]\n        if self.params['trained_samples'] >= self.config.samples_per_batch:\n            self._apply_grads(accum_grads)\n        return loss, arc_scores, rel_scores\n\n    def _apply_grads(self, accum_grads):\n        optimizer_transformer = self.params.get('optimizer_transformer', None)\n        if optimizer_transformer:\n            transformer = self.params['transformer_variable_names']\n            trainable_variables = self.model.trainable_variables\n            optimizer_transformer.apply_gradients(\n                (g, w) for g, w in zip(accum_grads, trainable_variables) if w.name in transformer)\n            self.model.optimizer.apply_gradients(\n                (g, w) for g, w in zip(accum_grads, trainable_variables) if w.name not in transformer)\n        else:\n            self.model.optimizer.apply_gradients(zip(accum_grads, self.model.trainable_variables))\n        for tv in accum_grads:\n            tv.assign(tf.zeros_like(tv))\n        # print('Apply grads after', self.params['trained_samples'], 'samples')\n        self.params['trained_samples'] = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.params['trained_samples']:\n            self._apply_grads(self.params['accum_grads'])\n\n\nclass BiaffineTransformerSemanticDependencyParser(BiaffineTransformerDependencyParserTF):\n\n    def __init__(self, transform: CoNLL_Transformer_Transform = None) -> None:\n        if not transform:\n            transform = CoNLL_Transformer_Transform(graph=True)\n        super().__init__(transform)\n\n    def get_loss(self, arc_scores, rel_scores, arcs, rels, mask, arc_loss, rel_loss):\n        return BiaffineSemanticDependencyParserTF.get_loss(self, arc_scores, rel_scores, arcs, rels, mask, arc_loss,\n                                                           rel_loss)\n\n    def fit(self, trn_data, dev_data, save_dir, transformer, max_seq_length=256, transformer_dropout=.33,\n            d_positional=None, n_mlp_arc=500, n_mlp_rel=100, mlp_dropout=.33, optimizer='adamw', learning_rate=5e-5,\n            learning_rate_transformer=None, weight_decay_rate=0, epsilon=1e-8, clipnorm=None, fp16=False,\n            warmup_steps_ratio=0, arc_loss='binary_crossentropy',\n            rel_loss='sparse_categorical_crossentropy', metrics=('UF', 'LF'), batch_size=3000, samples_per_batch=150,\n            max_samples_per_batch=None, epochs=100, tree=False, punct=False, token_mapping=None, enhanced_only=False,\n            run_eagerly=False,\n            logger=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def decode(self, arc_scores, rel_scores, mask):\n        return BiaffineSemanticDependencyParserTF.decode(self, arc_scores, rel_scores, mask)\n", "hanlp/components/parsers/constituency/crf_constituency_model.py": "# -*- coding:utf-8 -*-\n# Adopted from https://github.com/yzhangcs/parser\n# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport torch\nfrom torch import nn\nfrom hanlp.components.parsers.constituency.treecrf import CRFConstituency\nfrom hanlp.components.parsers.alg import cky\nfrom hanlp.components.parsers.biaffine.biaffine import Biaffine\nfrom hanlp.components.parsers.biaffine.mlp import MLP\n\n\nclass CRFConstituencyDecoder(nn.Module):\n    r\"\"\"\n    The implementation of CRF Constituency Parser,\n    also called FANCY (abbr. of Fast and Accurate Neural Crf constituencY) Parser.\n\n    References:\n        - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.\n          `Fast and Accurate Neural CRF Constituency Parsing`_.\n\n    Args:\n        n_words (int):\n            The size of the word vocabulary.\n        n_feats (int):\n            The size of the feat vocabulary.\n        n_labels (int):\n            The number of labels.\n        feat (str):\n            Specifies which type of additional feature to use: ``'char'`` | ``'bert'`` | ``'tag'``.\n            ``'char'``: Character-level representations extracted by CharLSTM.\n            ``'bert'``: BERT representations, other pretrained langugae models like XLNet are also feasible.\n            ``'tag'``: POS tag embeddings.\n            Default: 'char'.\n        n_embed (int):\n            The size of word embeddings. Default: 100.\n        n_feat_embed (int):\n            The size of feature representations. Default: 100.\n        n_char_embed (int):\n            The size of character embeddings serving as inputs of CharLSTM, required if ``feat='char'``. Default: 50.\n        bert (str):\n            Specifies which kind of language model to use, e.g., ``'bert-base-cased'`` and ``'xlnet-base-cased'``.\n            This is required if ``feat='bert'``. The full list can be found in `transformers`.\n            Default: ``None``.\n        n_bert_layers (int):\n            Specifies how many last layers to use. Required if ``feat='bert'``.\n            The final outputs would be the weight sum of the hidden states of these layers.\n            Default: 4.\n        mix_dropout (float):\n            The dropout ratio of BERT layers. Required if ``feat='bert'``. Default: .0.\n        embed_dropout (float):\n            The dropout ratio of input embeddings. Default: .33.\n        n_hidden (int):\n            The size of LSTM hidden states. Default: 400.\n        n_lstm_layers (int):\n            The number of LSTM layers. Default: 3.\n        lstm_dropout (float):\n            The dropout ratio of LSTM. Default: .33.\n        n_mlp_span (int):\n            Span MLP size. Default: 500.\n        n_mlp_label  (int):\n            Label MLP size. Default: 100.\n        mlp_dropout (float):\n            The dropout ratio of MLP layers. Default: .33.\n        feat_pad_index (int):\n            The index of the padding token in the feat vocabulary. Default: 0.\n        pad_index (int):\n            The index of the padding token in the word vocabulary. Default: 0.\n        unk_index (int):\n            The index of the unknown token in the word vocabulary. Default: 1.\n\n    .. _Fast and Accurate Neural CRF Constituency Parsing:\n        https://www.ijcai.org/Proceedings/2020/560/\n    .. _transformers:\n        https://github.com/huggingface/transformers\n    \"\"\"\n\n    def __init__(self,\n                 n_labels,\n                 n_hidden=400,\n                 n_mlp_span=500,\n                 n_mlp_label=100,\n                 mlp_dropout=.33,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        # the MLP layers\n        self.mlp_span_l = MLP(n_in=n_hidden, n_out=n_mlp_span, dropout=mlp_dropout)\n        self.mlp_span_r = MLP(n_in=n_hidden, n_out=n_mlp_span, dropout=mlp_dropout)\n        self.mlp_label_l = MLP(n_in=n_hidden, n_out=n_mlp_label, dropout=mlp_dropout)\n        self.mlp_label_r = MLP(n_in=n_hidden, n_out=n_mlp_label, dropout=mlp_dropout)\n\n        # the Biaffine layers\n        self.span_attn = Biaffine(n_in=n_mlp_span, bias_x=True, bias_y=False)\n        self.label_attn = Biaffine(n_in=n_mlp_label, n_out=n_labels, bias_x=True, bias_y=True)\n        self.crf = CRFConstituency()\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, x, **kwargs):\n        r\"\"\"\n        Args:\n            x (~torch.FloatTensor): ``[batch_size, seq_len, hidden_dim]``.\n                Hidden states from encoder.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.\n                The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds\n                scores of all possible labels on each span.\n        \"\"\"\n\n        x_f, x_b = x.chunk(2, -1)\n        x = torch.cat((x_f[:, :-1], x_b[:, 1:]), -1)\n        # apply MLPs to the BiLSTM output states\n        span_l = self.mlp_span_l(x)\n        span_r = self.mlp_span_r(x)\n        label_l = self.mlp_label_l(x)\n        label_r = self.mlp_label_r(x)\n\n        # [batch_size, seq_len, seq_len]\n        s_span = self.span_attn(span_l, span_r)\n        # [batch_size, seq_len, seq_len, n_labels]\n        s_label = self.label_attn(label_l, label_r).permute(0, 2, 3, 1)\n\n        return s_span, s_label\n\n    def loss(self, s_span, s_label, charts, mask, mbr=True):\n        r\"\"\"\n        Args:\n            s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n                Scores of all spans\n            s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.\n                Scores of all labels on each span.\n            charts (~torch.LongTensor): ``[batch_size, seq_len, seq_len]``.\n                The tensor of gold-standard labels, in which positions without labels are filled with -1.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.\n                The mask for covering the unpadded tokens in each chart.\n            mbr (bool):\n                If ``True``, returns marginals for MBR decoding. Default: ``True``.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The training loss and\n                original span scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise.\n        \"\"\"\n\n        span_mask = charts.ge(0) & mask\n        span_loss, span_probs = self.crf(s_span, mask, span_mask, mbr)\n        label_loss = self.criterion(s_label[span_mask], charts[span_mask])\n        loss = span_loss + label_loss\n\n        return loss, span_probs\n\n    def decode(self, s_span, s_label, mask):\n        r\"\"\"\n        Args:\n            s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n                Scores of all spans.\n            s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.\n                Scores of all labels on each span.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.\n                The mask for covering the unpadded tokens in each chart.\n\n        Returns:\n            list[list[tuple]]:\n                Sequences of factorized labeled trees traversed in pre-order.\n        \"\"\"\n\n        span_preds = cky(s_span, mask)\n        label_preds = s_label.argmax(-1).tolist()\n        return [[(i, j, labels[i][j]) for i, j in spans] for spans, labels in zip(span_preds, label_preds)]\n\n\nclass CRFConstituencyModel(nn.Module):\n\n    def __init__(self, encoder, decoder: CRFConstituencyDecoder) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, batch):\n        r\"\"\"\n        Args:\n            batch (~dict):\n                Batch of input data.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.\n                The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds\n                scores of all possible labels on each span.\n        \"\"\"\n        x = self.encoder(batch)\n        return self.decoder(x)\n", "hanlp/components/parsers/constituency/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-28 19:26\n", "hanlp/components/parsers/constituency/treecrf.py": "# -*- coding:utf-8 -*-\n# Adopted from https://github.com/yzhangcs/parser\n# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\n\nfrom hanlp.components.parsers.alg import stripe, istree, eisner, mst, eisner2o\n\n\nclass CRFConstituency(nn.Module):\n    r\"\"\"\n    TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for constituency trees.\n\n    References:\n        - Yu Zhang, houquan Zhou and Zhenghua Li. 2020.\n          `Fast and Accurate Neural CRF Constituency Parsing`_.\n\n    .. _Fast and Accurate Neural CRF Constituency Parsing:\n        https://www.ijcai.org/Proceedings/2020/560/\n    \"\"\"\n\n    @torch.enable_grad()\n    def forward(self, scores, mask, target=None, mbr=False):\n        r\"\"\"\n        Args:\n            scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n                Scores of all possible constituents.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.\n                The mask to avoid parsing over padding tokens.\n                For each square matrix in a batch, the positions except upper triangular part should be masked out.\n            target (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.\n                The tensor of gold-standard constituents. ``True`` if a constituent exists. Default: ``None``.\n            mbr (bool):\n                If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.\n                The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,\n                or original scores otherwise.\n        \"\"\"\n\n        training = scores.requires_grad\n        # always enable the gradient computation of scores in order for the computation of marginals\n        logZ = self.inside(scores.requires_grad_(), mask)\n        # marginals are used for decoding, and can be computed by combining the inside pass and autograd mechanism\n        probs = scores\n        if mbr:\n            probs, = autograd.grad(logZ, scores, retain_graph=training)\n        if target is None:\n            return probs\n        loss = (logZ - scores[mask & target].sum()) / mask[:, 0].sum()\n\n        return loss, probs\n\n    def inside(self, scores, mask):\n        lens = mask[:, 0].sum(-1)\n        batch_size, seq_len, _ = scores.shape\n        # [seq_len, seq_len, batch_size]\n        scores, mask = scores.permute(1, 2, 0), mask.permute(1, 2, 0)\n        s = torch.full_like(scores, float('-inf'))\n\n        for w in range(1, seq_len):\n            # n denotes the number of spans to iterate,\n            # from span (0, w) to span (n, n+w) given width w\n            n = seq_len - w\n\n            if w == 1:\n                s.diagonal(w).copy_(scores.diagonal(w))\n                continue\n            # [n, w, batch_size]\n            s_s = stripe(s, n, w - 1, (0, 1)) + stripe(s, n, w - 1, (1, w), 0)\n            # [batch_size, n, w]\n            s_s = s_s.permute(2, 0, 1)\n            if s_s.requires_grad:\n                s_s.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            s_s = s_s.logsumexp(-1)\n            s.diagonal(w).copy_(s_s + scores.diagonal(w))\n\n        return s[0].gather(0, lens.unsqueeze(0)).sum()\n\n\nclass CRF2oDependency(nn.Module):\n    r\"\"\"\n    Second-order TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for projective dependency trees.\n\n    References:\n        - Yu Zhang, Zhenghua Li and Min Zhang. 2020.\n          `Efficient Second-Order TreeCRF for Neural Dependency Parsing`_.\n\n    .. _Efficient Second-Order TreeCRF for Neural Dependency Parsing:\n        https://www.aclweb.org/anthology/2020.acl-main.302/\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss()\n\n    @torch.enable_grad()\n    def forward(self, scores, mask, target=None, mbr=True, partial=False):\n        r\"\"\"\n        Args:\n            scores (~torch.Tensor, ~torch.Tensor):\n                Tuple of two tensors `s_arc` and `s_sib`.\n                `s_arc` (``[batch_size, seq_len, seq_len]``) holds Scores of all possible dependent-head pairs.\n                `s_sib` (``[batch_size, seq_len, seq_len, seq_len]``) holds the scores of dependent-head-sibling triples.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n                The mask to avoid aggregation on padding tokens.\n                The first column serving as pseudo words for roots should be ``False``.\n            target (~torch.LongTensor): ``[batch_size, seq_len]``.\n                Tensors of gold-standard dependent-head pairs and dependent-head-sibling triples.\n                If partially annotated, the unannotated positions should be filled with -1.\n                Default: ``None``.\n            mbr (bool):\n                If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.\n            partial (bool):\n                ``True`` indicates that the trees are partially annotated. Default: ``False``.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.\n                The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,\n                or original scores otherwise.\n        \"\"\"\n\n        s_arc, s_sib = scores\n        training = s_arc.requires_grad\n        batch_size, seq_len, _ = s_arc.shape\n        # always enable the gradient computation of scores in order for the computation of marginals\n        logZ = self.inside((s.requires_grad_() for s in scores), mask)\n        # marginals are used for decoding, and can be computed by combining the inside pass and autograd mechanism\n        probs = s_arc\n        if mbr:\n            probs, = autograd.grad(logZ, s_arc, retain_graph=training)\n\n        if target is None:\n            return probs\n        arcs, sibs = target\n        # the second inside process is needed if use partial annotation\n        if partial:\n            score = self.inside(scores, mask, arcs)\n        else:\n            arc_seq, sib_seq = arcs[mask], sibs[mask]\n            arc_mask, sib_mask = mask, sib_seq.gt(0)\n            sib_seq = sib_seq[sib_mask]\n            s_sib = s_sib[mask][torch.arange(len(arc_seq)), arc_seq]\n            s_arc = s_arc[arc_mask].gather(-1, arc_seq.unsqueeze(-1))\n            s_sib = s_sib[sib_mask].gather(-1, sib_seq.unsqueeze(-1))\n            score = s_arc.sum() + s_sib.sum()\n        loss = (logZ - score) / mask.sum()\n\n        return loss, probs\n\n    def inside(self, scores, mask, cands=None):\n        # the end position of each sentence in a batch\n        lens = mask.sum(1)\n        s_arc, s_sib = scores\n        batch_size, seq_len, _ = s_arc.shape\n        # [seq_len, seq_len, batch_size]\n        s_arc = s_arc.permute(2, 1, 0)\n        # [seq_len, seq_len, seq_len, batch_size]\n        s_sib = s_sib.permute(2, 1, 3, 0)\n        s_i = torch.full_like(s_arc, float('-inf'))\n        s_s = torch.full_like(s_arc, float('-inf'))\n        s_c = torch.full_like(s_arc, float('-inf'))\n        s_c.diagonal().fill_(0)\n\n        # set the scores of arcs excluded by cands to -inf\n        if cands is not None:\n            mask = mask.index_fill(1, lens.new_tensor(0), 1)\n            mask = (mask.unsqueeze(1) & mask.unsqueeze(-1)).permute(2, 1, 0)\n            cands = cands.unsqueeze(-1).index_fill(1, lens.new_tensor(0), -1)\n            cands = cands.eq(lens.new_tensor(range(seq_len))) | cands.lt(0)\n            cands = cands.permute(2, 1, 0) & mask\n            s_arc = s_arc.masked_fill(~cands, float('-inf'))\n\n        for w in range(1, seq_len):\n            # n denotes the number of spans to iterate,\n            # from span (0, w) to span (n, n+w) given width w\n            n = seq_len - w\n            # I(j->i) = logsum(exp(I(j->r) + S(j->r, i)) +, i < r < j\n            #                  exp(C(j->j) + C(i->j-1)))\n            #           + s(j->i)\n            # [n, w, batch_size]\n            il = stripe(s_i, n, w, (w, 1)) + stripe(s_s, n, w, (1, 0), 0)\n            il += stripe(s_sib[range(w, n + w), range(n)], n, w, (0, 1))\n            # [n, 1, batch_size]\n            il0 = stripe(s_c, n, 1, (w, w)) + stripe(s_c, n, 1, (0, w - 1))\n            # il0[0] are set to zeros since the scores of the complete spans starting from 0 are always -inf\n            il[:, -1] = il0.index_fill_(0, lens.new_tensor(0), 0).squeeze(1)\n            if il.requires_grad:\n                il.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            il = il.permute(2, 0, 1).logsumexp(-1)\n            s_i.diagonal(-w).copy_(il + s_arc.diagonal(-w))\n            # I(i->j) = logsum(exp(I(i->r) + S(i->r, j)) +, i < r < j\n            #                  exp(C(i->i) + C(j->i+1)))\n            #           + s(i->j)\n            # [n, w, batch_size]\n            ir = stripe(s_i, n, w) + stripe(s_s, n, w, (0, w), 0)\n            ir += stripe(s_sib[range(n), range(w, n + w)], n, w)\n            ir[0] = float('-inf')\n            # [n, 1, batch_size]\n            ir0 = stripe(s_c, n, 1) + stripe(s_c, n, 1, (w, 1))\n            ir[:, 0] = ir0.squeeze(1)\n            if ir.requires_grad:\n                ir.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            ir = ir.permute(2, 0, 1).logsumexp(-1)\n            s_i.diagonal(w).copy_(ir + s_arc.diagonal(w))\n\n            # [n, w, batch_size]\n            slr = stripe(s_c, n, w) + stripe(s_c, n, w, (w, 1))\n            if slr.requires_grad:\n                slr.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            slr = slr.permute(2, 0, 1).logsumexp(-1)\n            # S(j, i) = logsumexp(C(i->r) + C(j->r+1)), i <= r < j\n            s_s.diagonal(-w).copy_(slr)\n            # S(i, j) = logsumexp(C(i->r) + C(j->r+1)), i <= r < j\n            s_s.diagonal(w).copy_(slr)\n\n            # C(j->i) = logsumexp(C(r->i) + I(j->r)), i <= r < j\n            cl = stripe(s_c, n, w, (0, 0), 0) + stripe(s_i, n, w, (w, 0))\n            cl.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            s_c.diagonal(-w).copy_(cl.permute(2, 0, 1).logsumexp(-1))\n            # C(i->j) = logsumexp(I(i->r) + C(r->j)), i < r <= j\n            cr = stripe(s_i, n, w, (0, 1)) + stripe(s_c, n, w, (1, w), 0)\n            cr.register_hook(lambda x: x.masked_fill_(torch.isnan(x), 0))\n            s_c.diagonal(w).copy_(cr.permute(2, 0, 1).logsumexp(-1))\n            # disable multi words to modify the root\n            s_c[0, w][lens.ne(w)] = float('-inf')\n\n        return s_c[0].gather(0, lens.unsqueeze(0)).sum()\n\n    def loss(self, s_arc, s_sib, s_rel, arcs, sibs, rels, mask, mbr=True, partial=False):\n        r\"\"\"\n        Args:\n            s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n                Scores of all possible arcs.\n            s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.\n                Scores of all possible dependent-head-sibling triples.\n            s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.\n                Scores of all possible labels on each arc.\n            arcs (~torch.LongTensor): ``[batch_size, seq_len]``.\n                The tensor of gold-standard arcs.\n            sibs (~torch.LongTensor): ``[batch_size, seq_len]``.\n                The tensor of gold-standard siblings.\n            rels (~torch.LongTensor): ``[batch_size, seq_len]``.\n                The tensor of gold-standard labels.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n                The mask for covering the unpadded tokens.\n            mbr (bool):\n                If ``True``, returns marginals for MBR decoding. Default: ``True``.\n            partial (bool):\n                ``True`` denotes the trees are partially annotated. Default: ``False``.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                The training loss and\n                original arc scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise.\n        \"\"\"\n\n        scores, target = (s_arc, s_sib), (arcs, sibs)\n        arc_loss, arc_probs = self.forward(scores, mask, target, mbr, partial)\n        # -1 denotes un-annotated arcs\n        if partial:\n            mask = mask & arcs.ge(0)\n        s_rel, rels = s_rel[mask], rels[mask]\n        s_rel = s_rel[torch.arange(len(rels)), arcs[mask]]\n        rel_loss = self.criterion(s_rel, rels)\n        loss = arc_loss + rel_loss\n        return loss, arc_probs\n\n    # def decode(self, s_arc, s_rel, mask, tree=False, proj=False, alg=None):\n    #     r\"\"\"\n    #     Args:\n    #         s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n    #             Scores of all possible arcs.\n    #         s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.\n    #             Scores of all possible labels on each arc.\n    #         mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n    #             The mask for covering the unpadded tokens.\n    #         tree (bool):\n    #             If ``True``, ensures to output well-formed trees. Default: ``False``.\n    #         proj (bool):\n    #             If ``True``, ensures to output projective trees. Default: ``False``.\n    # \n    #     Returns:\n    #         ~torch.Tensor, ~torch.Tensor:\n    #             Predicted arcs and labels of shape ``[batch_size, seq_len]``.\n    #     \"\"\"\n    # \n    #     lens = mask.sum(1)\n    #     arc_preds = s_arc.argmax(-1)\n    #     if tree and not alg:\n    #         bad = [not istree(seq[1:i + 1], proj)\n    #                for i, seq in zip(lens.tolist(), arc_preds.tolist())]\n    #         if any(bad):\n    #             alg = eisner if proj else mst\n    #             arc_preds[bad] = alg(s_arc[bad], mask[bad])\n    #     rel_preds = s_rel.argmax(-1).gather(-1, arc_preds.unsqueeze(-1)).squeeze(-1)\n    # \n    #     return arc_preds, rel_preds\n    def decode(self, s_arc, s_sib, s_rel, mask, tree=False, mbr=True, proj=False):\n        r\"\"\"\n        Args:\n            s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.\n                Scores of all possible arcs.\n            s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.\n                Scores of all possible dependent-head-sibling triples.\n            s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.\n                Scores of all possible labels on each arc.\n            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.\n                The mask for covering the unpadded tokens.\n            tree (bool):\n                If ``True``, ensures to output well-formed trees. Default: ``False``.\n            mbr (bool):\n                If ``True``, performs MBR decoding. Default: ``True``.\n            proj (bool):\n                If ``True``, ensures to output projective trees. Default: ``False``.\n\n        Returns:\n            ~torch.Tensor, ~torch.Tensor:\n                Predicted arcs and labels of shape ``[batch_size, seq_len]``.\n        \"\"\"\n\n        lens = mask.sum(1)\n        arc_preds = s_arc.argmax(-1)\n        if tree:\n            bad = [not istree(seq[1:i + 1], proj)\n                   for i, seq in zip(lens.tolist(), arc_preds.tolist())]\n            if any(bad):\n                if proj and not mbr:\n                    arc_preds = eisner2o((s_arc, s_sib), mask)\n                else:\n                    alg = eisner if proj else mst\n                    arc_preds[bad] = alg(s_arc[bad], mask[bad])\n        rel_preds = s_rel.argmax(-1).gather(-1, arc_preds.unsqueeze(-1)).squeeze(-1)\n\n        return arc_preds, rel_preds\n", "hanlp/components/parsers/constituency/crf_constituency_parser.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-28 21:24\nimport logging\nfrom typing import Union, List\n\nimport torch\nfrom phrasetree.tree import Tree\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import BOS, EOS, IDX\nfrom hanlp.common.dataset import TransformableDataset, SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength, TransformList\nfrom hanlp.common.vocab import VocabWithNone\nfrom hanlp.components.classifiers.transformer_classifier import TransformerComponent\nfrom hanlp.datasets.parsing.loaders.constituency_dataset import ConstituencyDataset, unpack_tree_to_features, \\\n    build_tree, factorize, remove_subcategory\nfrom hanlp.components.parsers.constituency.crf_constituency_model import CRFConstituencyDecoder, CRFConstituencyModel\nfrom hanlp.metrics.parsing.span import SpanMetric\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp.utils.torch_util import clip_grad_norm\nfrom hanlp_common.util import merge_locals_kwargs, merge_dict, reorder\n\n\nclass CRFConstituencyParser(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        \"\"\"Two-stage CRF Parsing (:cite:`ijcai2020-560`).\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: CRFConstituencyModel = self.model\n\n    def build_optimizer(self, trn, **kwargs):\n        # noinspection PyCallByClass,PyTypeChecker\n        return TransformerComponent.build_optimizer(self, trn, **kwargs)\n\n    def build_criterion(self, decoder=None, **kwargs):\n        return decoder\n\n    def build_metric(self, **kwargs):\n        return SpanMetric()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, patience=0.5, eval_trn=True, **kwargs):\n        if isinstance(patience, float):\n            patience = int(patience * epochs)\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,\n                                eval_trn=eval_trn, **self.config)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, logger=logger, ratio_width=ratio_width)\n            timer.update()\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if dev_metric > best_metric:\n                best_epoch, best_metric = epoch, dev_metric\n                self.save_weights(save_dir)\n                report += ' [red](saved)[/red]'\n            else:\n                report += f' ({epoch - best_epoch})'\n                if epoch - best_epoch >= patience:\n                    report += ' early stop'\n            logger.info(report)\n            if epoch - best_epoch >= patience:\n                break\n        if not best_epoch:\n            self.save_weights(save_dir)\n        elif best_epoch != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric} at epoch {best_epoch}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric: SpanMetric,\n                       logger: logging.Logger,\n                       history: History,\n                       gradient_accumulation=1,\n                       grad_norm=None,\n                       ratio_width=None,\n                       eval_trn=True,\n                       **kwargs):\n        optimizer, scheduler = optimizer\n        metric.reset()\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            out, mask = self.feed_batch(batch)\n            y = batch['chart_id']\n            loss, span_probs = self.compute_loss(out, y, mask)\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if eval_trn:\n                prediction = self.decode_output(out, mask, batch, span_probs)\n                self.update_metrics(metric, batch, prediction)\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler, grad_norm)\n                report = f'loss: {total_loss / (idx + 1):.4f} {metric}' if eval_trn \\\n                    else f'loss: {total_loss / (idx + 1):.4f}'\n                timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n            del loss\n            del out\n            del mask\n\n    def decode_output(self, out, mask, batch, span_probs=None, decoder=None, tokens=None):\n        s_span, s_label = out\n        if not decoder:\n            decoder = self.model.decoder\n        if mask.any().item():\n            if span_probs is None:\n                if self.config.mbr:\n                    s_span = decoder.crf(s_span, mask, mbr=True)\n            else:\n                s_span = span_probs\n            chart_preds = decoder.decode(s_span, s_label, mask)\n        else:\n            chart_preds = [[]] * len(tokens)\n        idx_to_token = self.vocabs.chart.idx_to_token\n        if tokens is None:\n            tokens = batch.get('token_', None)  # Use the original tokens if any\n            if tokens is None:\n                tokens = batch['token']\n            tokens = [x[1:-1] for x in tokens]\n        trees = [build_tree(token, [(i, j, idx_to_token[label]) for i, j, label in chart]) for token, chart in\n                 zip(tokens, chart_preds)]\n        # probs = [prob[:i - 1, 1:i].cpu() for i, prob in zip(lens, s_span.unbind())]\n        return trees\n\n    def update_metrics(self, metric, batch, prediction):\n        # Add pre-terminals (pos tags) back to prediction for safe factorization (deletion based on pos)\n        for pred, gold in zip(prediction, batch['constituency']):\n            pred: Tree = pred\n            gold: Tree = gold\n            for p, g in zip(pred.subtrees(lambda t: t.height() == 2), gold.pos()):\n                token, pos = g\n                p: Tree = p\n                assert p.label() == '_'\n                p.set_label(pos)\n        metric([factorize(tree, self.config.delete, self.config.equal) for tree in prediction],\n               [factorize(tree, self.config.delete, self.config.equal) for tree in batch['constituency']])\n        return metric\n\n    def feed_batch(self, batch: dict):\n        mask = self.compute_mask(batch)\n        s_span, s_label = self.model(batch)\n        return (s_span, s_label), mask\n\n    def compute_mask(self, batch, offset=1):\n        lens = batch['token_length'] - offset\n        seq_len = lens.max()\n        mask = lens.new_tensor(range(seq_len)) < lens.view(-1, 1, 1)\n        mask = mask & mask.new_ones(seq_len, seq_len).triu_(1)\n        return mask\n\n    def compute_loss(self, out, y, mask, crf_decoder=None):\n        if not crf_decoder:\n            crf_decoder = self.model.decoder\n        loss, span_probs = crf_decoder.loss(out[0], out[1], y, mask, self.config.mbr)\n        if loss < 0:  # wired negative loss\n            loss *= 0\n        return loss, span_probs\n\n    def _step(self, optimizer, scheduler, grad_norm):\n        clip_grad_norm(self.model, grad_norm)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    @torch.no_grad()\n    def evaluate_dataloader(self, data, criterion, logger=None, ratio_width=None, metric=None, output=None, **kwargs):\n        self.model.eval()\n        total_loss = 0\n        if not metric:\n            metric = self.build_metric()\n        else:\n            metric.reset()\n        timer = CountdownTimer(len(data))\n        for idx, batch in enumerate(data):\n            out, mask = self.feed_batch(batch)\n            y = batch['chart_id']\n            loss, span_probs = self.compute_loss(out, y, mask)\n            total_loss += loss.item()\n            prediction = self.decode_output(out, mask, batch, span_probs)\n            self.update_metrics(metric, batch, prediction)\n            timer.log(f'loss: {total_loss / (idx + 1):.4f} {metric}', ratio_percentage=False, logger=logger,\n                      ratio_width=ratio_width)\n        total_loss /= len(data)\n        if output:\n            output.close()\n        return total_loss, metric\n\n    # noinspection PyMethodOverriding\n    def build_model(self, encoder, training=True, **kwargs) -> torch.nn.Module:\n        decoder = CRFConstituencyDecoder(n_labels=len(self.vocabs.chart), n_hidden=encoder.get_output_dim(), **kwargs)\n        encoder = encoder.module(vocabs=self.vocabs, training=training)\n        return CRFConstituencyModel(encoder, decoder)\n\n    def build_dataloader(self,\n                         data,\n                         batch_size,\n                         sampler_builder: SamplerBuilder = None,\n                         gradient_accumulation=1,\n                         shuffle=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        if isinstance(data, TransformableDataset):\n            dataset = data\n        else:\n            transform = self.config.encoder.transform()\n            if self.config.get('transform', None):\n                transform = TransformList(self.config.transform, transform)\n            dataset = self.build_dataset(data, transform, logger)\n        if self.vocabs.mutable:\n            # noinspection PyTypeChecker\n            self.build_vocabs(dataset, logger)\n        lens = [len(x['token_input_ids']) for x in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, gradient_accumulation)\n        else:\n            sampler = None\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler)\n\n    def predict(self, data: Union[str, List[str]], **kwargs):\n        if not data:\n            return []\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        samples = self.build_samples(data)\n        dataloader = self.build_dataloader(samples, device=self.device, **kwargs)\n        outputs = []\n        orders = []\n        for idx, batch in enumerate(dataloader):\n            out, mask = self.feed_batch(batch)\n            prediction = self.decode_output(out, mask, batch, span_probs=None)\n            # prediction = [x[0] for x in prediction]\n            outputs.extend(prediction)\n            orders.extend(batch[IDX])\n        outputs = reorder(outputs, orders)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    def input_is_flat(self, data):\n        return isinstance(data[0], str)\n\n    def build_samples(self, data):\n        return [{'token': [BOS] + token + [EOS]} for token in data]\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            encoder,\n            lr=5e-5,\n            transformer_lr=None,\n            adam_epsilon=1e-8,\n            weight_decay=0,\n            warmup_steps=0.1,\n            grad_norm=1.0,\n            n_mlp_span=500,\n            n_mlp_label=100,\n            mlp_dropout=.33,\n            batch_size=None,\n            batch_max_tokens=5000,\n            gradient_accumulation=1,\n            epochs=30,\n            patience=0.5,\n            mbr=True,\n            sampler_builder=None,\n            delete=('', ':', '``', \"''\", '.', '?', '!', '-NONE-', 'TOP', ',', 'S1'),\n            equal=(('ADVP', 'PRT'),),\n            no_subcategory=True,\n            eval_trn=True,\n            transform=None,\n            devices=None,\n            logger=None,\n            seed=None,\n            **kwargs):\n        if isinstance(equal, tuple):\n            equal = dict(equal)\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_dataset(self, data, transform, logger=None):\n        _transform = [\n            unpack_tree_to_features,\n            self.vocabs,\n            FieldLength('token'),\n            transform\n        ]\n        if self.config.get('no_subcategory', True):\n            _transform.insert(0, remove_subcategory)\n        dataset = ConstituencyDataset(data,\n                                      transform=_transform,\n                                      cache=isinstance(data, str))\n        return dataset\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        self.vocabs.chart = VocabWithNone(pad_token=None, unk_token=None)\n        timer = CountdownTimer(len(trn))\n        max_seq_len = 0\n        for each in trn:\n            max_seq_len = max(max_seq_len, len(each['token_input_ids']))\n            timer.log(f'Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: {max_seq_len})')\n        self.vocabs.chart.set_unk_as_safe_unk()\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n", "hanlp/components/parsers/biaffine_tf/model.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 23:04\nimport tensorflow as tf\nfrom hanlp.layers.transformers.tf_imports import TFPreTrainedModel\nfrom hanlp.components.parsers.biaffine_tf.layers import IndependentDropout, SharedDropout, Biaffine, MLP\n\n\nclass BiaffineModelTF(tf.keras.Model):\n\n    def __init__(self, config, embed=None, transformer: TFPreTrainedModel = None):\n        \"\"\"An implementation of T. Dozat and C. D. Manning, \u201cDeep Biaffine Attention for Neural Dependency Parsing.,\u201d ICLR, 2017.\n            Although I have my MXNet implementation, I found zysite's PyTorch implementation is cleaner so I port it to TensorFlow\n\n        Args:\n          config: param embed:\n\n        Returns:\n\n        \"\"\"\n        super(BiaffineModelTF, self).__init__()\n        assert not (embed and transformer), 'Either pre-trained word embed and transformer is supported, but not both'\n        normal = tf.keras.initializers.RandomNormal(stddev=1.)\n        if not transformer:\n            # the embedding layer\n            self.word_embed = tf.keras.layers.Embedding(input_dim=config.n_words,\n                                                        output_dim=config.n_embed,\n                                                        embeddings_initializer=tf.keras.initializers.zeros() if embed\n                                                        else normal,\n                                                        name='word_embed')\n            self.feat_embed = tf.keras.layers.Embedding(input_dim=config.n_feats,\n                                                        output_dim=config.n_embed,\n                                                        embeddings_initializer=tf.keras.initializers.zeros() if embed\n                                                        else normal,\n                                                        name='feat_embed')\n            self.embed_dropout = IndependentDropout(p=config.embed_dropout, name='embed_dropout')\n\n            # the word-lstm layer\n            self.lstm = tf.keras.models.Sequential(name='lstm')\n            for _ in range(config.n_lstm_layers):\n                self.lstm.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n                    units=config.n_lstm_hidden,\n                    dropout=config.lstm_dropout,\n                    recurrent_dropout=config.lstm_dropout,\n                    return_sequences=True,\n                    kernel_initializer='orthogonal',\n                    unit_forget_bias=False,  # turns out to hinder performance\n                )))\n            self.lstm_dropout = SharedDropout(p=config.lstm_dropout, name='lstm_dropout')\n        else:\n            self.transformer = transformer\n            transformer_dropout = config.get('transformer_dropout', None)\n            if transformer_dropout:\n                self.transformer_dropout = SharedDropout(p=config.transformer_dropout, name='transformer_dropout')\n            d_positional = config.get('d_positional', None)\n            if d_positional:\n                max_seq_length = config.get('max_seq_length', 256)\n                self.position_table = self.add_weight(shape=(max_seq_length, d_positional),\n                                                      initializer='random_normal',\n                                                      trainable=True)\n        # the MLP layers\n        self.mlp_arc_h = MLP(n_hidden=config.n_mlp_arc,\n                             dropout=config.mlp_dropout, name='mlp_arc_h')\n        self.mlp_arc_d = MLP(n_hidden=config.n_mlp_arc,\n                             dropout=config.mlp_dropout, name='mlp_arc_d')\n        self.mlp_rel_h = MLP(n_hidden=config.n_mlp_rel,\n                             dropout=config.mlp_dropout, name='mlp_rel_h')\n        self.mlp_rel_d = MLP(n_hidden=config.n_mlp_rel,\n                             dropout=config.mlp_dropout, name='mlp_rel_d')\n\n        # the Biaffine layers\n        self.arc_attn = Biaffine(n_in=config.n_mlp_arc,\n                                 bias_x=True,\n                                 bias_y=False, name='arc_attn')\n        self.rel_attn = Biaffine(n_in=config.n_mlp_rel,\n                                 n_out=config.n_rels,\n                                 bias_x=True,\n                                 bias_y=True, name='rel_attn')\n        if embed is not None:\n            self.pretrained = embed\n        self.pad_index = tf.constant(config.pad_index, dtype=tf.int64)\n        self.unk_index = tf.constant(config.unk_index, dtype=tf.int64)\n\n    # noinspection PyMethodOverriding\n    def call(self, inputs, mask_inf=True, **kwargs):\n        # batch_size, seq_len = words.shape\n        # get the mask and lengths of given batch\n        # mask = words.ne(self.pad_index)\n        if hasattr(self, 'lstm'):\n            words, feats = inputs\n            mask = tf.not_equal(words, self.pad_index)\n            # set the indices larger than num_embeddings to unk_index\n            # ext_mask = words.ge(self.word_embed.num_embeddings)\n            ext_mask = tf.greater_equal(words, self.word_embed.input_dim)\n            ext_words = tf.where(ext_mask, self.unk_index, words)\n\n            # get outputs from embedding layers\n            word_embed = self.word_embed(ext_words)\n            if hasattr(self, 'pretrained'):\n                word_embed += self.pretrained(words)\n            feat_embed = self.feat_embed(feats)\n            word_embed, feat_embed = self.embed_dropout([word_embed, feat_embed])\n            # concatenate the word and feat representations\n            embed = tf.concat((word_embed, feat_embed), axis=-1)\n\n            x = self.lstm(embed, mask=mask)\n            x = self.lstm_dropout(x)\n        else:\n            words, (input_ids, input_mask, prefix_offset) = inputs\n            mask = tf.not_equal(words, self.pad_index)\n            x = self.run_transformer(input_ids, input_mask, prefix_offset)\n\n        # apply MLPs to the BiLSTM output states\n        arc_h = self.mlp_arc_h(x)\n        arc_d = self.mlp_arc_d(x)\n        rel_h = self.mlp_rel_h(x)\n        rel_d = self.mlp_rel_d(x)\n\n        # get arc and rel scores from the bilinear attention\n        # [batch_size, seq_len, seq_len]\n        s_arc = self.arc_attn(arc_d, arc_h)\n        # [batch_size, seq_len, seq_len, n_rels]\n        s_rel = tf.transpose(self.rel_attn(rel_d, rel_h), [0, 2, 3, 1])\n        # set the scores that exceed the length of each sentence to -inf\n        if mask_inf:\n            s_arc = tf.where(tf.expand_dims(mask, 1), s_arc, float('-inf'))\n\n        return s_arc, s_rel\n\n    def run_transformer(self, input_ids, input_mask, prefix_offset):\n        if isinstance(self.transformer, TFPreTrainedModel):\n            sequence_output = self.transformer([input_ids, input_mask])\n            sequence_output = sequence_output[0]\n        else:\n            sequence_output = self.transformer([input_ids, tf.zeros_like(input_ids)], mask=input_mask)\n        x = tf.gather(sequence_output, prefix_offset, batch_dims=1)\n        if hasattr(self, 'transformer_dropout'):\n            x = self.transformer_dropout(x)\n        if hasattr(self, 'position_table'):\n            batch_size, seq_length = tf.shape(x)[:2]\n            timing_signal = tf.broadcast_to(self.position_table[:seq_length],\n                                            [batch_size, seq_length, self.position_table.shape[-1]])\n            x = tf.concat([x, timing_signal], axis=-1)\n        return x\n\n    def to_functional(self):\n        words = tf.keras.Input(shape=[None], dtype=tf.int64, name='words')\n        feats = tf.keras.Input(shape=[None], dtype=tf.int64, name='feats')\n        s_arc, s_rel = self.call([words, feats], mask_inf=False)\n        return tf.keras.Model(inputs=[words, feats], outputs=[s_arc, s_rel])\n", "hanlp/components/parsers/biaffine_tf/alg.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 19:49\n# Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser\nfrom typing import List\nimport numpy as np\nimport tensorflow as tf\nfrom collections import defaultdict\n\n\ndef nonzero(t: tf.Tensor) -> tf.Tensor:\n    return tf.where(t > 0)\n\n\ndef view(t: tf.Tensor, *dims) -> tf.Tensor:\n    return tf.reshape(t, dims)\n\n\ndef arange(n: int) -> tf.Tensor:\n    return tf.range(n)\n\n\ndef randperm(n: int) -> tf.Tensor:\n    return tf.random.shuffle(arange(n))\n\n\ndef tolist(t: tf.Tensor) -> List:\n    if isinstance(t, tf.Tensor):\n        t = t.numpy()\n    return t.tolist()\n\n\ndef kmeans(x, k, seed=None):\n    \"\"\"See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7\n\n    Args:\n      x(list): Lengths of sentences\n      k(int): \n      seed:  (Default value = None)\n\n    Returns:\n\n    \n    \"\"\"\n    x = tf.constant(x, dtype=tf.float32)\n    # count the frequency of each datapoint\n    d, indices, f = tf.unique_with_counts(x, tf.int32)\n    f = tf.cast(f, tf.float32)\n    # calculate the sum of the values of the same datapoints\n    total = d * f\n    # initialize k centroids randomly\n    c, old = tf.random.shuffle(d, seed)[:k], None\n    # assign labels to each datapoint based on centroids\n    dists = tf.abs(tf.expand_dims(d, -1) - c)\n    y = tf.argmin(dists, axis=-1, output_type=tf.int32)\n    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))\n    # make sure number of datapoints is greater than that of clusters\n    assert len(d) >= k, f\"unable to assign {len(d)} datapoints to {k} clusters\"\n\n    while old is None or not tf.reduce_all(c == old):\n        # if an empty cluster is encountered,\n        # choose the farthest datapoint from the biggest cluster\n        # and move that the empty one\n        for i in range(k):\n            if not tf.reduce_any(y == i):\n                mask = tf.cast(y == tf.expand_dims(tf.range(k, dtype=tf.int32), -1), tf.float32)\n                lens = tf.reduce_sum(mask, axis=-1)\n                biggest = view(nonzero(mask[tf.argmax(lens)]), -1)\n                farthest = tf.argmax(tf.gather(dists, biggest))\n                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], -1), -1), [i])\n        mask = tf.cast(y == tf.expand_dims(tf.range(k, dtype=tf.int32), -1), tf.float32)\n        # update the centroids\n        c, old = tf.cast(tf.reduce_sum(total * mask, axis=-1), tf.float32) / tf.cast(tf.reduce_sum(f * mask, axis=-1),\n                                                                                     tf.float32), c\n        # re-assign all datapoints to clusters\n        dists = tf.abs(tf.expand_dims(d, -1) - c)\n        y = tf.argmin(dists, axis=-1, output_type=tf.int32)\n        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))\n    # assign all datapoints to the new-generated clusters\n    # without considering the empty ones\n    y, (assigned, _) = tf.gather(y, indices), tf.unique(y)\n    # get the centroids of the assigned clusters\n    centroids = tf.gather(c, assigned).numpy().tolist()\n    # map all values of datapoints to buckets\n    clusters = [tf.squeeze(tf.where(y == i), axis=-1).numpy().tolist() for i in assigned]\n\n    return centroids, clusters\n\n\n# ***************************************************************\nclass Tarjan:\n    \"\"\"Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph\"\"\"\n\n    def __init__(self, prediction, tokens):\n        \"\"\"\n\n        Parameters\n        ----------\n        prediction : numpy.ndarray\n            a predicted dependency tree where prediction[dep_idx] = head_idx\n        tokens : numpy.ndarray\n            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)\n        \"\"\"\n        self._edges = defaultdict(set)\n        self._vertices = set((0,))\n        for dep, head in enumerate(prediction[tokens]):\n            self._vertices.add(dep + 1)\n            self._edges[head].add(dep + 1)\n        self._indices = {}\n        self._lowlinks = {}\n        self._onstack = defaultdict(lambda: False)\n        self._SCCs = []\n\n        index = 0\n        stack = []\n        for v in self.vertices:\n            if v not in self.indices:\n                self.strongconnect(v, index, stack)\n\n    # =============================================================\n    def strongconnect(self, v, index, stack):\n        \"\"\"\n\n        Args:\n          v: \n          index: \n          stack: \n\n        Returns:\n\n        \"\"\"\n\n        self._indices[v] = index\n        self._lowlinks[v] = index\n        index += 1\n        stack.append(v)\n        self._onstack[v] = True\n        for w in self.edges[v]:\n            if w not in self.indices:\n                self.strongconnect(w, index, stack)\n                self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])\n            elif self._onstack[w]:\n                self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])\n\n        if self._lowlinks[v] == self._indices[v]:\n            self._SCCs.append(set())\n            while stack[-1] != v:\n                w = stack.pop()\n                self._onstack[w] = False\n                self._SCCs[-1].add(w)\n            w = stack.pop()\n            self._onstack[w] = False\n            self._SCCs[-1].add(w)\n        return\n\n    # ======================\n    @property\n    def edges(self):\n        return self._edges\n\n    @property\n    def vertices(self):\n        return self._vertices\n\n    @property\n    def indices(self):\n        return self._indices\n\n    @property\n    def SCCs(self):\n        return self._SCCs\n\n\ndef tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):\n    \"\"\"Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py\n\n    Args:\n      parse_probs(NDArray): seq_len x seq_len, the probability of arcs\n      length(NDArray): sentence length including ROOT\n      tokens_to_keep(NDArray): mask matrix\n      ensure_tree:  (Default value = True)\n\n    Returns:\n\n    \n    \"\"\"\n    if ensure_tree:\n        I = np.eye(len(tokens_to_keep))\n        # block loops and pad heads\n        parse_probs = parse_probs * tokens_to_keep * (1 - I)\n        parse_preds = np.argmax(parse_probs, axis=1)\n        tokens = np.arange(1, length)\n        roots = np.where(parse_preds[tokens] == 0)[0] + 1\n        # ensure at least one root\n        if len(roots) < 1:\n            # The current root probabilities\n            root_probs = parse_probs[tokens, 0]\n            # The current head probabilities\n            old_head_probs = parse_probs[tokens, parse_preds[tokens]]\n            # Get new potential root probabilities\n            new_root_probs = root_probs / old_head_probs\n            # Select the most probable root\n            new_root = tokens[np.argmax(new_root_probs)]\n            # Make the change\n            parse_preds[new_root] = 0\n        # ensure at most one root\n        elif len(roots) > 1:\n            # The probabilities of the current heads\n            root_probs = parse_probs[roots, 0]\n            # Set the probability of depending on the root zero\n            parse_probs[roots, 0] = 0\n            # Get new potential heads and their probabilities\n            new_heads = np.argmax(parse_probs[roots][:, tokens], axis=1) + 1\n            new_head_probs = parse_probs[roots, new_heads] / root_probs\n            # Select the most probable root\n            new_root = roots[np.argmin(new_head_probs)]\n            # Make the change\n            parse_preds[roots] = new_heads\n            parse_preds[new_root] = 0\n        # remove cycles\n        tarjan = Tarjan(parse_preds, tokens)\n        for SCC in tarjan.SCCs:\n            if len(SCC) > 1:\n                dependents = set()\n                to_visit = set(SCC)\n                while len(to_visit) > 0:\n                    node = to_visit.pop()\n                    if not node in dependents:\n                        dependents.add(node)\n                        to_visit.update(tarjan.edges[node])\n                # The indices of the nodes that participate in the cycle\n                cycle = np.array(list(SCC))\n                # The probabilities of the current heads\n                old_heads = parse_preds[cycle]\n                old_head_probs = parse_probs[cycle, old_heads]\n                # Set the probability of depending on a non-head to zero\n                non_heads = np.array(list(dependents))\n                parse_probs[np.repeat(cycle, len(non_heads)), np.repeat([non_heads], len(cycle), axis=0).flatten()] = 0\n                # Get new potential heads and their probabilities\n                new_heads = np.argmax(parse_probs[cycle][:, tokens], axis=1) + 1\n                new_head_probs = parse_probs[cycle, new_heads] / old_head_probs\n                # Select the most probable change\n                change = np.argmax(new_head_probs)\n                changed_cycle = cycle[change]\n                old_head = old_heads[change]\n                new_head = new_heads[change]\n                # Make the change\n                parse_preds[changed_cycle] = new_head\n                tarjan.edges[new_head].add(changed_cycle)\n                tarjan.edges[old_head].remove(changed_cycle)\n        return parse_preds\n    else:\n        # block and pad heads\n        parse_probs = parse_probs * tokens_to_keep\n        parse_preds = np.argmax(parse_probs, axis=1)\n        return parse_preds\n\n\ndef rel_argmax(rel_probs, length, root, ensure_tree=True):\n    \"\"\"Fix the relation prediction by heuristic rules\n\n    Args:\n      rel_probs(NDArray): seq_len x rel_size\n      length: real sentence length\n      ensure_tree:  (Default value = True)\n      root: \n\n    Returns:\n\n    \n    \"\"\"\n    if ensure_tree:\n        tokens = np.arange(1, length)\n        rel_preds = np.argmax(rel_probs, axis=1)\n        roots = np.where(rel_preds[tokens] == root)[0] + 1\n        if len(roots) < 1:\n            rel_preds[1 + np.argmax(rel_probs[tokens, root])] = root\n        elif len(roots) > 1:\n            root_probs = rel_probs[roots, root]\n            rel_probs[roots, root] = 0\n            new_rel_preds = np.argmax(rel_probs[roots], axis=1)\n            new_rel_probs = rel_probs[roots, new_rel_preds] / root_probs\n            new_root = roots[np.argmin(new_rel_probs)]\n            rel_preds[roots] = new_rel_preds\n            rel_preds[new_root] = root\n        return rel_preds\n    else:\n        rel_preds = np.argmax(rel_probs, axis=1)\n        return rel_preds\n", "hanlp/components/parsers/biaffine_tf/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 23:03", "hanlp/components/parsers/biaffine_tf/layers.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-26 23:05\n# Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser\nimport tensorflow as tf\nfrom hanlp.utils.tf_util import tf_bernoulli\n\n\nclass Biaffine(tf.keras.layers.Layer):\n    def __init__(self, n_in, n_out=1, bias_x=True, bias_y=True, trainable=True, name=None, dtype=None, dynamic=False,\n                 **kwargs):\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.n_in = n_in\n        self.n_out = n_out\n        self.bias_x = bias_x\n        self.bias_y = bias_y\n        self.weight = None\n\n    def build(self, input_shape):\n        self.weight = self.add_weight(name='kernel',\n                                      shape=(self.n_out,\n                                             self.n_in + self.bias_x,\n                                             self.n_in + self.bias_y),\n                                      initializer='zero')\n\n    def extra_repr(self):\n        s = f\"n_in={self.n_in}, n_out={self.n_out}\"\n        if self.bias_x:\n            s += f\", bias_x={self.bias_x}\"\n        if self.bias_y:\n            s += f\", bias_y={self.bias_y}\"\n\n        return s\n\n    # noinspection PyMethodOverriding\n    def call(self, x, y, **kwargs):\n        if self.bias_x:\n            x = tf.concat((x, tf.ones_like(x[..., :1])), -1)\n        if self.bias_y:\n            y = tf.concat((y, tf.ones_like(y[..., :1])), -1)\n        # [batch_size, n_out, seq_len, seq_len]\n        s = tf.einsum('bxi,oij,byj->boxy', x, self.weight, y)\n        # remove dim 1 if n_out == 1\n        if self.n_out == 1:\n            s = tf.squeeze(s, axis=1)\n\n        return s\n\n\nclass MLP(tf.keras.layers.Layer):\n    def __init__(self, n_hidden, dropout=0, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.linear = tf.keras.layers.Dense(n_hidden, kernel_initializer='orthogonal')\n        self.activation = tf.keras.layers.LeakyReLU(0.1)\n        self.dropout = SharedDropout(p=dropout)\n\n    def call(self, x, **kwargs):\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass SharedDropout(tf.keras.layers.Layer):\n\n    def __init__(self, p=0.5, batch_first=True, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n        \"\"\"Dropout on timesteps with bernoulli distribution\"\"\"\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.p = p\n        self.batch_first = batch_first\n\n    def extra_repr(self):\n        s = f\"p={self.p}\"\n        if self.batch_first:\n            s += f\", batch_first={self.batch_first}\"\n\n        return s\n\n    def call(self, x, training=None, **kwargs):\n        if training and self.p > 0:\n            if self.batch_first:\n                mask = self.get_mask(x[:, 0], self.p)\n            else:\n                mask = self.get_mask(x[0], self.p)\n            x *= tf.expand_dims(mask, axis=1) if self.batch_first else mask\n\n        return x\n\n    @staticmethod\n    def get_mask(x, p):\n        mask = tf_bernoulli(tf.shape(x), 1 - p, x.dtype)\n        mask = mask / (1 - p)\n\n        return mask\n\n\nclass IndependentDropout(tf.keras.layers.Layer):\n\n    def __init__(self, p=0.5, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n        \"\"\"Dropout on the first two dimensions\"\"\"\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.p = p\n\n    def extra_repr(self):\n        return f\"p={self.p}\"\n\n    def call(self, inputs, training=None, **kwargs):\n        if training and self.p > 0:\n            masks = [tf_bernoulli(tf.shape(x)[:2], 1 - self.p)\n                     for x in inputs]\n            total = sum(masks)\n            scale = len(inputs) / tf.reduce_max(tf.ones_like(total))\n            masks = [mask * scale for mask in masks]\n            inputs = [item * tf.expand_dims(mask, axis=-1)\n                      for item, mask in zip(inputs, masks)]\n\n        return inputs\n\n\n", "hanlp/components/parsers/ud/tag_decoder.py": "# This file is modified from udify, which is licensed under the MIT license:\n# MIT License\n#\n# Copyright (c) 2019 Dan Kondratyuk\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"\nDecodes sequences of tags, e.g., POS tags, given a list of contextualized word embeddings\n\"\"\"\n\nfrom typing import Dict\n\nimport numpy\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.adaptive import AdaptiveLogSoftmaxWithLoss\nfrom torch.nn.modules.linear import Linear\n\nfrom hanlp.components.parsers.ud.lemma_edit import apply_lemma_rule\nfrom hanlp.components.parsers.ud.udify_util import sequence_cross_entropy, sequence_cross_entropy_with_logits\n\n\nclass TagDecoder(torch.nn.Module):\n    \"\"\"A basic sequence tagger that decodes from inputs of word embeddings\"\"\"\n\n    def __init__(self,\n                 input_dim,\n                 num_classes,\n                 label_smoothing: float = 0.03,\n                 adaptive: bool = False) -> None:\n        super(TagDecoder, self).__init__()\n\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.adaptive = adaptive\n\n        if self.adaptive:\n            adaptive_cutoffs = [round(self.num_classes / 15), 3 * round(self.num_classes / 15)]\n            self.task_output = AdaptiveLogSoftmaxWithLoss(input_dim,\n                                                          self.num_classes,\n                                                          cutoffs=adaptive_cutoffs,\n                                                          div_value=4.0)\n        else:\n            self.task_output = Linear(self.output_dim, self.num_classes)\n\n    def forward(self,\n                encoded_text: torch.FloatTensor,\n                mask: torch.LongTensor,\n                gold_tags: torch.LongTensor,\n                ) -> Dict[str, torch.Tensor]:\n        hidden = encoded_text\n\n        batch_size, sequence_length, _ = hidden.size()\n        output_dim = [batch_size, sequence_length, self.num_classes]\n\n        loss_fn = self._adaptive_loss if self.adaptive else self._loss\n\n        output_dict = loss_fn(hidden, mask, gold_tags, output_dim)\n\n        return output_dict\n\n    def _adaptive_loss(self, hidden, mask, gold_tags, output_dim):\n        logits = hidden\n        reshaped_log_probs = logits.reshape(-1, logits.size(2))\n\n        class_probabilities = self.task_output.log_prob(reshaped_log_probs).view(output_dim)\n\n        output_dict = {\"logits\": logits, \"class_probabilities\": class_probabilities}\n\n        if gold_tags is not None:\n            output_dict[\"loss\"] = sequence_cross_entropy(class_probabilities,\n                                                         gold_tags,\n                                                         mask,\n                                                         label_smoothing=self.label_smoothing)\n\n        return output_dict\n\n    def _loss(self, hidden, mask, gold_tags, output_dim):\n        logits = self.task_output(hidden)\n        reshaped_log_probs = logits.view(-1, self.num_classes)\n        class_probabilities = F.softmax(reshaped_log_probs, dim=-1).view(output_dim)\n\n        output_dict = {\"logits\": logits, \"class_probabilities\": class_probabilities}\n\n        if gold_tags is not None:\n            output_dict[\"loss\"] = sequence_cross_entropy_with_logits(logits,\n                                                                     gold_tags,\n                                                                     mask,\n                                                                     label_smoothing=self.label_smoothing)\n        return output_dict\n\n    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        all_words = output_dict[\"words\"]\n\n        all_predictions = output_dict[\"class_probabilities\"][self.task].cpu().data.numpy()\n        if all_predictions.ndim == 3:\n            predictions_list = [all_predictions[i] for i in range(all_predictions.shape[0])]\n        else:\n            predictions_list = [all_predictions]\n        all_tags = []\n        for predictions, words in zip(predictions_list, all_words):\n            argmax_indices = numpy.argmax(predictions, axis=-1)\n            tags = [self.vocab.get_token_from_index(x, namespace=self.task)\n                    for x in argmax_indices]\n\n            if self.task == \"lemmas\":\n                def decode_lemma(word, rule):\n                    if rule == \"_\":\n                        return \"_\"\n                    if rule == \"@@UNKNOWN@@\":\n                        return word\n                    return apply_lemma_rule(word, rule)\n\n                tags = [decode_lemma(word, rule) for word, rule in zip(words, tags)]\n\n            all_tags.append(tags)\n        output_dict[self.task] = all_tags\n\n        return output_dict\n", "hanlp/components/parsers/ud/udify_util.py": "# This file is modified from udify and allennlp, which are licensed under the MIT license:\n# MIT License\n#\n# Copyright (c) 2019 Dan Kondratyuk and allennlp\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nfrom typing import List, Dict, Tuple, Union\n\nimport numpy\nimport torch\n\n\ndef get_ud_treebank_files(dataset_dir: str, treebanks: List[str] = None) -> Dict[str, Tuple[str, str, str]]:\n    \"\"\"Retrieves all treebank data paths in the given directory.\n    Adopted from https://github.com/Hyperparticle/udify\n    MIT Licence\n\n    Args:\n      dataset_dir: \n      treebanks: \n      dataset_dir: str: \n      treebanks: List[str]:  (Default value = None)\n\n    Returns:\n\n    \n    \"\"\"\n    datasets = {}\n    treebanks = os.listdir(dataset_dir) if not treebanks else treebanks\n    for treebank in treebanks:\n        treebank_path = os.path.join(dataset_dir, treebank)\n        conllu_files = [file for file in sorted(os.listdir(treebank_path)) if file.endswith(\".conllu\")]\n\n        train_file = [file for file in conllu_files if file.endswith(\"train.conllu\")]\n        dev_file = [file for file in conllu_files if file.endswith(\"dev.conllu\")]\n        test_file = [file for file in conllu_files if file.endswith(\"test.conllu\")]\n\n        train_file = os.path.join(treebank_path, train_file[0]) if train_file else None\n        dev_file = os.path.join(treebank_path, dev_file[0]) if dev_file else None\n        test_file = os.path.join(treebank_path, test_file[0]) if test_file else None\n\n        datasets[treebank] = (train_file, dev_file, test_file)\n    return datasets\n\n\ndef sequence_cross_entropy(log_probs: torch.FloatTensor,\n                           targets: torch.LongTensor,\n                           weights: torch.FloatTensor,\n                           average: str = \"batch\",\n                           label_smoothing: float = None) -> torch.FloatTensor:\n    if average not in {None, \"token\", \"batch\"}:\n        raise ValueError(\"Got average f{average}, expected one of \"\n                         \"None, 'token', or 'batch'\")\n    # shape : (batch * sequence_length, num_classes)\n    log_probs_flat = log_probs.view(-1, log_probs.size(2))\n    # shape : (batch * max_len, 1)\n    targets_flat = targets.view(-1, 1).long()\n\n    if label_smoothing is not None and label_smoothing > 0.0:\n        num_classes = log_probs.size(-1)\n        smoothing_value = label_smoothing / num_classes\n        # Fill all the correct indices with 1 - smoothing value.\n        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(-1, targets_flat, 1.0 - label_smoothing)\n        smoothed_targets = one_hot_targets + smoothing_value\n        negative_log_likelihood_flat = - log_probs_flat * smoothed_targets\n        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n    else:\n        # Contribution to the negative log likelihood only comes from the exact indices\n        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n        # to extract the indices of the num_classes dimension which contribute to the loss.\n        # shape : (batch * sequence_length, 1)\n        negative_log_likelihood_flat = - torch.gather(log_probs_flat, dim=1, index=targets_flat)\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood * weights.float()\n\n    if average == \"batch\":\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        num_non_empty_sequences = ((weights.sum(1) > 0).float().sum() + 1e-13)\n        return per_batch_loss.sum() / num_non_empty_sequences\n    elif average == \"token\":\n        return negative_log_likelihood.sum() / (weights.sum().float() + 1e-13)\n    else:\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        return per_batch_loss\n\n\ndef sequence_cross_entropy_with_logits(\n        logits: torch.FloatTensor,\n        targets: torch.LongTensor,\n        weights: Union[torch.FloatTensor, torch.BoolTensor],\n        average: str = \"batch\",\n        label_smoothing: float = None,\n        gamma: float = None,\n        alpha: Union[float, List[float], torch.FloatTensor] = None,\n) -> torch.FloatTensor:\n    \"\"\"Computes the cross entropy loss of a sequence, weighted with respect to\n    some user provided weights. Note that the weighting here is not the same as\n    in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting\n    classes; here we are weighting the loss contribution from particular elements\n    in the sequence. This allows loss computations for models which use padding.\n    \n    # Parameters\n    \n    logits : `torch.FloatTensor`, required.\n        A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)\n        which contains the unnormalized probability for each class.\n    targets : `torch.LongTensor`, required.\n        A `torch.LongTensor` of size (batch, sequence_length) which contains the\n        index of the true class for each corresponding step.\n    weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.\n        A `torch.FloatTensor` of size (batch, sequence_length)\n    average: `str`, optional (default = `\"batch\"`)\n        If \"batch\", average the loss across the batches. If \"token\", average\n        the loss across each item in the input. If `None`, return a vector\n        of losses per batch element.\n    label_smoothing : `float`, optional (default = `None`)\n        Whether or not to apply label smoothing to the cross-entropy loss.\n        For example, with a label smoothing value of 0.2, a 4 class classification\n        target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was\n        the correct label.\n    gamma : `float`, optional (default = `None`)\n        Focal loss[*] focusing parameter `gamma` to reduces the relative loss for\n        well-classified examples and put more focus on hard. The greater value\n        `gamma` is, the more focus on hard examples.\n    alpha : `Union[float, List[float]]`, optional (default = `None`)\n        Focal loss[*] weighting factor `alpha` to balance between classes. Can be\n        used independently with `gamma`. If a single `float` is provided, it\n        is assumed binary case using `alpha` and `1 - alpha` for positive and\n        negative respectively. If a list of `float` is provided, with the same\n        length as the number of classes, the weights will match the classes.\n        [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r, \"Focal Loss for\n        Dense Object Detection,\" 2017 IEEE International Conference on Computer\n        Vision (ICCV), Venice, 2017, pp. 2999-3007.\n    \n    # Returns\n    \n    `torch.FloatTensor`\n        A torch.FloatTensor representing the cross entropy loss.\n        If `average==\"batch\"` or `average==\"token\"`, the returned loss is a scalar.\n        If `average is None`, the returned loss is a vector of shape (batch_size,).\n\n    Args:\n      logits: torch.FloatTensor: \n      targets: torch.LongTensor: \n      weights: Union[torch.FloatTensor: \n      torch.BoolTensor]: \n      average: str:  (Default value = \"batch\")\n      label_smoothing: float:  (Default value = None)\n      gamma: float:  (Default value = None)\n      alpha: Union[float: \n      List[float]: \n      torch.FloatTensor]:  (Default value = None)\n\n    Returns:\n\n    \"\"\"\n    if average not in {None, \"token\", \"batch\"}:\n        raise ValueError(\"Got average f{average}, expected one of None, 'token', or 'batch'\")\n\n    # make sure weights are float\n    weights = weights.to(logits.dtype)\n    # sum all dim except batch\n    non_batch_dims = tuple(range(1, len(weights.shape)))\n    # shape : (batch_size,)\n    weights_batch_sum = weights.sum(dim=non_batch_dims)\n    # shape : (batch * sequence_length, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # shape : (batch * sequence_length, num_classes)\n    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n    # shape : (batch * max_len, 1)\n    targets_flat = targets.view(-1, 1).long()\n    # focal loss coefficient\n    if gamma:\n        # shape : (batch * sequence_length, num_classes)\n        probs_flat = log_probs_flat.exp()\n        # shape : (batch * sequence_length,)\n        probs_flat = torch.gather(probs_flat, dim=1, index=targets_flat)\n        # shape : (batch * sequence_length,)\n        focal_factor = (1.0 - probs_flat) ** gamma\n        # shape : (batch, sequence_length)\n        focal_factor = focal_factor.view(*targets.size())\n        weights = weights * focal_factor\n\n    if alpha is not None:\n        # shape : () / (num_classes,)\n        if isinstance(alpha, (float, int)):\n\n            # shape : (2,)\n            alpha_factor = torch.tensor(\n                [1.0 - float(alpha), float(alpha)], dtype=weights.dtype, device=weights.device\n            )\n\n        elif isinstance(alpha, (list, numpy.ndarray, torch.Tensor)):\n\n            # shape : (c,)\n            alpha_factor = torch.tensor(alpha, dtype=weights.dtype, device=weights.device)\n\n            if not alpha_factor.size():\n                # shape : (1,)\n                alpha_factor = alpha_factor.view(1)\n                # shape : (2,)\n                alpha_factor = torch.cat([1 - alpha_factor, alpha_factor])\n        else:\n            raise TypeError(\n                (\"alpha must be float, list of float, or torch.FloatTensor, {} provided.\").format(\n                    type(alpha)\n                )\n            )\n        # shape : (batch, max_len)\n        alpha_factor = torch.gather(alpha_factor, dim=0, index=targets_flat.view(-1)).view(\n            *targets.size()\n        )\n        weights = weights * alpha_factor\n\n    if label_smoothing is not None and label_smoothing > 0.0:\n        num_classes = logits.size(-1)\n        smoothing_value = label_smoothing / num_classes\n        # Fill all the correct indices with 1 - smoothing value.\n        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(\n            -1, targets_flat, 1.0 - label_smoothing\n        )\n        smoothed_targets = one_hot_targets + smoothing_value\n        negative_log_likelihood_flat = -log_probs_flat * smoothed_targets\n        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n    else:\n        # Contribution to the negative log likelihood only comes from the exact indices\n        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n        # to extract the indices of the num_classes dimension which contribute to the loss.\n        # shape : (batch * sequence_length, 1)\n        negative_log_likelihood_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood * weights\n\n    if average == \"batch\":\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (\n                weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n        num_non_empty_sequences = (weights_batch_sum > 0).sum() + tiny_value_of_dtype(\n            negative_log_likelihood.dtype\n        )\n        return per_batch_loss.sum() / num_non_empty_sequences\n    elif average == \"token\":\n        return negative_log_likelihood.sum() / (\n                weights_batch_sum.sum() + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n    else:\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (\n                weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)\n        )\n        return per_batch_loss\n\n\ndef tiny_value_of_dtype(dtype: torch.dtype):\n    \"\"\"Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical\n    issues such as division by zero.\n    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.\n    Only supports floating point dtypes.\n\n    Args:\n      dtype: torch.dtype: \n\n    Returns:\n\n    \"\"\"\n    if not dtype.is_floating_point:\n        raise TypeError(\"Only supports floating point dtypes.\")\n    if dtype == torch.float or dtype == torch.double:\n        return 1e-13\n    elif dtype == torch.half:\n        return 1e-4\n    else:\n        raise TypeError(\"Does not support dtype \" + str(dtype))\n\n\ndef combine_initial_dims_to_1d_or_2d(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"Given a (possibly higher order) tensor of ids with shape\n    (d1, ..., dn, sequence_length)\n\n    Args:\n      tensor: torch.Tensor: \n\n    Returns:\n      If original tensor is 1-d or 2-d, return it as is.\n\n    \"\"\"\n    if tensor.dim() <= 2:\n        return tensor\n    else:\n        return tensor.view(-1, tensor.size(-1))\n\n\ndef uncombine_initial_dims(tensor: torch.Tensor, original_size: torch.Size) -> torch.Tensor:\n    \"\"\"Given a tensor of embeddings with shape\n    (d1 * ... * dn, sequence_length, embedding_dim)\n    and the original shape\n    (d1, ..., dn, sequence_length),\n\n    Args:\n      tensor: torch.Tensor: \n      original_size: torch.Size: \n\n    Returns:\n      (d1, ..., dn, sequence_length, embedding_dim).\n      If original size is 1-d or 2-d, return it as is.\n\n    \"\"\"\n    if len(original_size) <= 2:\n        return tensor\n    else:\n        view_args = list(original_size) + [tensor.size(-1)]\n        return tensor.view(*view_args)\n\n\ndef get_range_vector(size: int, device: int) -> torch.Tensor:\n    \"\"\"Returns a range vector with the desired size, starting at 0. The CUDA implementation\n    is meant to avoid copy data from CPU to GPU.\n\n    Args:\n      size: int: \n      device: int: \n\n    Returns:\n\n    \"\"\"\n    if device > -1:\n        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n    else:\n        return torch.arange(0, size, dtype=torch.long)\n\n\ndef get_device_of(tensor: torch.Tensor) -> int:\n    \"\"\"Returns the device of the tensor.\n\n    Args:\n      tensor: torch.Tensor: \n\n    Returns:\n\n    \"\"\"\n    if not tensor.is_cuda:\n        return -1\n    else:\n        return tensor.get_device()\n", "hanlp/components/parsers/ud/lemma_edit.py": "\"\"\"\nUtilities for processing lemmas\n\nAdopted from UDPipe Future\nhttps://github.com/CoNLL-UD-2018/UDPipe-Future\n\"\"\"\n\n\ndef min_edit_script(source, target, allow_copy=False):\n    \"\"\"Finds the minimum edit script to transform the source to the target\n\n    Args:\n      source: \n      target: \n      allow_copy:  (Default value = False)\n\n    Returns:\n\n    \"\"\"\n    a = [[(len(source) + len(target) + 1, None)] * (len(target) + 1) for _ in range(len(source) + 1)]\n    for i in range(0, len(source) + 1):\n        for j in range(0, len(target) + 1):\n            if i == 0 and j == 0:\n                a[i][j] = (0, \"\")\n            else:\n                if allow_copy and i and j and source[i - 1] == target[j - 1] and a[i - 1][j - 1][0] < a[i][j][0]:\n                    a[i][j] = (a[i - 1][j - 1][0], a[i - 1][j - 1][1] + \"\u2192\")\n                if i and a[i - 1][j][0] < a[i][j][0]:\n                    a[i][j] = (a[i - 1][j][0] + 1, a[i - 1][j][1] + \"-\")\n                if j and a[i][j - 1][0] < a[i][j][0]:\n                    a[i][j] = (a[i][j - 1][0] + 1, a[i][j - 1][1] + \"+\" + target[j - 1])\n    return a[-1][-1][1]\n\n\ndef gen_lemma_rule(form, lemma, allow_copy=False):\n    \"\"\"Generates a lemma rule to transform the source to the target\n\n    Args:\n      form: \n      lemma: \n      allow_copy:  (Default value = False)\n\n    Returns:\n\n    \"\"\"\n    form = form.lower()\n\n    previous_case = -1\n    lemma_casing = \"\"\n    for i, c in enumerate(lemma):\n        case = \"\u2191\" if c.lower() != c else \"\u2193\"\n        if case != previous_case:\n            lemma_casing += \"{}{}{}\".format(\"\u00a6\" if lemma_casing else \"\", case,\n                                            i if i <= len(lemma) // 2 else i - len(lemma))\n        previous_case = case\n    lemma = lemma.lower()\n\n    best, best_form, best_lemma = 0, 0, 0\n    for l in range(len(lemma)):\n        for f in range(len(form)):\n            cpl = 0\n            while f + cpl < len(form) and l + cpl < len(lemma) and form[f + cpl] == lemma[l + cpl]: cpl += 1\n            if cpl > best:\n                best = cpl\n                best_form = f\n                best_lemma = l\n\n    rule = lemma_casing + \";\"\n    if not best:\n        rule += \"a\" + lemma\n    else:\n        rule += \"d{}\u00a6{}\".format(\n            min_edit_script(form[:best_form], lemma[:best_lemma], allow_copy),\n            min_edit_script(form[best_form + best:], lemma[best_lemma + best:], allow_copy),\n        )\n    return rule\n\n\ndef apply_lemma_rule(form, lemma_rule):\n    \"\"\"Applies the lemma rule to the form to generate the lemma\n\n    Args:\n      form: \n      lemma_rule: \n\n    Returns:\n\n    \"\"\"\n    cells = lemma_rule.split(\";\", 1)\n    if len(cells) == 1:  # Some predicted lemma rules are _, which might be due to partial annotation\n        return form.lower()\n    casing, rule = cells\n    if rule.startswith(\"a\"):\n        lemma = rule[1:]\n    else:\n        form = form.lower()\n        rules, rule_sources = rule[1:].split(\"\u00a6\"), []\n        assert len(rules) == 2\n        for rule in rules:\n            source, i = 0, 0\n            while i < len(rule):\n                if rule[i] == \"\u2192\" or rule[i] == \"-\":\n                    source += 1\n                else:\n                    assert rule[i] == \"+\"\n                    i += 1\n                i += 1\n            rule_sources.append(source)\n\n        try:\n            lemma, form_offset = \"\", 0\n            for i in range(2):\n                j, offset = 0, (0 if i == 0 else len(form) - rule_sources[1])\n                while j < len(rules[i]):\n                    if rules[i][j] == \"\u2192\":\n                        lemma += form[offset]\n                        offset += 1\n                    elif rules[i][j] == \"-\":\n                        offset += 1\n                    else:\n                        assert (rules[i][j] == \"+\")\n                        lemma += rules[i][j + 1]\n                        j += 1\n                    j += 1\n                if i == 0:\n                    lemma += form[rule_sources[0]: len(form) - rule_sources[1]]\n        except:\n            lemma = form\n\n    for rule in casing.split(\"\u00a6\"):\n        if rule == \"\u21930\": continue  # The lemma is lowercased initially\n        case, offset = rule[0], int(rule[1:])\n        lemma = lemma[:offset] + (lemma[offset:].upper() if case == \"\u2191\" else lemma[offset:].lower())\n\n    return lemma\n", "hanlp/components/parsers/ud/ud_parser.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-14 20:34\nimport logging\nfrom copy import deepcopy\nfrom typing import Union, List, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import IDX\nfrom hanlp.common.dataset import PadSequenceDataLoader, SortingSamplerBuilder\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength, PunctuationMask\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.classifiers.transformer_classifier import TransformerComponent\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp_common.conll import CoNLLUWord, CoNLLSentence\nfrom hanlp.components.parsers.ud.ud_model import UniversalDependenciesModel\nfrom hanlp.components.parsers.ud.util import generate_lemma_rule, append_bos, sample_form_missing\nfrom hanlp.components.parsers.ud.lemma_edit import apply_lemma_rule\nfrom hanlp.datasets.parsing.loaders.conll_dataset import CoNLLParsingDataset\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding\nfrom hanlp.metrics.accuracy import CategoricalAccuracy\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.metrics.parsing.attachmentscore import AttachmentScore\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp.utils.torch_util import clip_grad_norm, lengths_to_mask\nfrom hanlp_common.util import merge_locals_kwargs, merge_dict, reorder\n\n\nclass UniversalDependenciesParser(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation\n        of \"75 Languages, 1 Model: Parsing Universal Dependencies Universally\" (:cite:`kondratyuk-straka-2019-75`).\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: UniversalDependenciesModel = self.model\n\n    def build_dataloader(self,\n                         data,\n                         batch_size,\n                         shuffle=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         sampler_builder=None,\n                         gradient_accumulation=1,\n                         transformer: ContextualWordEmbedding = None,\n                         **kwargs) -> DataLoader:\n        transform = [generate_lemma_rule, append_bos, self.vocabs, transformer.transform(), FieldLength('token')]\n        if not self.config.punct:\n            transform.append(PunctuationMask('token', 'punct_mask'))\n        dataset = self.build_dataset(data, transform)\n        if self.vocabs.mutable:\n            # noinspection PyTypeChecker\n            self.build_vocabs(dataset, logger)\n        lens = [len(x['token_input_ids']) for x in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, gradient_accumulation)\n        else:\n            sampler = SortingSamplerBuilder(batch_size).build(lens, shuffle, gradient_accumulation)\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler,\n                                     pad={'arc': 0}, )\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        self.vocabs.pos = Vocab(unk_token=None, pad_token=None)\n        self.vocabs.rel = Vocab(unk_token=None, pad_token=None)\n        self.vocabs.lemma = Vocab(unk_token=None, pad_token=None)\n        self.vocabs.feat = Vocab(unk_token=None, pad_token=None)\n        timer = CountdownTimer(len(trn))\n        max_seq_len = 0\n        for each in trn:\n            max_seq_len = max(max_seq_len, len(each['token']))\n            timer.log(f'Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: {max_seq_len})')\n        for v in self.vocabs.values():\n            v.set_unk_as_safe_unk()\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n\n    def build_dataset(self, data, transform):\n        dataset = CoNLLParsingDataset(data, transform=transform, prune=sample_form_missing, cache=isinstance(data, str))\n        return dataset\n\n    def build_optimizer(self, trn, **kwargs):\n        # noinspection PyCallByClass,PyTypeChecker\n        return TransformerComponent.build_optimizer(self, trn, **kwargs)\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def build_metric(self, **kwargs):\n        return MetricDict({\n            'lemmas': CategoricalAccuracy(),\n            'upos': CategoricalAccuracy(),\n            'deps': AttachmentScore(),\n            'feats': CategoricalAccuracy(),\n        })\n\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric: MetricDict = None,\n                            output=False,\n                            logger=None,\n                            ratio_width=None,\n                            **kwargs):\n\n        metric.reset()\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        for idx, batch in enumerate(data):\n            out, mask = self.feed_batch(batch)\n            loss = out['loss']\n            total_loss += loss.item()\n            self.decode_output(out, mask, batch)\n            self.update_metrics(metric, batch, out, mask)\n            report = f'loss: {total_loss / (idx + 1):.4f} {metric.cstr()}'\n            timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n            del loss\n            del out\n            del mask\n        return total_loss / len(data), metric\n\n    # noinspection PyMethodOverriding\n    def build_model(self,\n                    transformer: ContextualWordEmbedding,\n                    n_mlp_arc,\n                    n_mlp_rel,\n                    mlp_dropout,\n                    mix_embedding,\n                    layer_dropout,\n                    training=True,\n                    **kwargs) -> torch.nn.Module:\n        assert bool(transformer.scalar_mix) == bool(mix_embedding), 'transformer.scalar_mix has to be 1 ' \\\n                                                                    'when mix_embedding is non-zero.'\n        # noinspection PyTypeChecker\n        return UniversalDependenciesModel(transformer.module(training=training),\n                                          n_mlp_arc,\n                                          n_mlp_rel,\n                                          mlp_dropout,\n                                          len(self.vocabs.rel),\n                                          len(self.vocabs.lemma),\n                                          len(self.vocabs.pos),\n                                          len(self.vocabs.feat),\n                                          mix_embedding,\n                                          layer_dropout)\n\n    def predict(self, data: Union[List[str], List[List[str]]], batch_size: int = None, **kwargs):\n        if not data:\n            return []\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        samples = self.build_samples(data)\n        if not batch_size:\n            batch_size = self.config.batch_size\n        dataloader = self.build_dataloader(samples,\n                                           device=self.devices[0], shuffle=False,\n                                           **merge_dict(self.config,\n                                                        batch_size=batch_size,\n                                                        overwrite=True,\n                                                        **kwargs))\n        order = []\n        outputs = []\n        for batch in dataloader:\n            out, mask = self.feed_batch(batch)\n            self.decode_output(out, mask, batch)\n            outputs.extend(self.prediction_to_human(out, batch))\n            order.extend(batch[IDX])\n        outputs = reorder(outputs, order)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    def build_samples(self, data: List[List[str]]):\n        return [{'FORM': x} for x in data]\n\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            transformer: ContextualWordEmbedding,\n            sampler_builder=None,\n            mix_embedding: int = 13,\n            layer_dropout: int = 0.1,\n            n_mlp_arc=768,\n            n_mlp_rel=256,\n            mlp_dropout=.33,\n            lr=1e-3,\n            transformer_lr=2.5e-5,\n            patience=0.1,\n            batch_size=32,\n            epochs=30,\n            gradient_accumulation=1,\n            adam_epsilon=1e-8,\n            weight_decay=0,\n            warmup_steps=0.1,\n            grad_norm=1.0,\n            tree=False,\n            proj=False,\n            punct=False,\n            logger=None,\n            verbose=True,\n            devices: Union[float, int, List[int]] = None, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, patience=0.5, eval_trn=True, **kwargs):\n        if isinstance(patience, float):\n            patience = int(patience * epochs)\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,\n                                eval_trn=eval_trn, **self.config)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, metric, logger=logger, ratio_width=ratio_width)\n            timer.update()\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if dev_metric > best_metric:\n                best_epoch, best_metric = epoch, deepcopy(dev_metric)\n                self.save_weights(save_dir)\n                report += ' [red](saved)[/red]'\n            else:\n                report += f' ({epoch - best_epoch})'\n                if epoch - best_epoch >= patience:\n                    report += ' early stop'\n            logger.info(report)\n            if epoch - best_epoch >= patience:\n                break\n        if not best_epoch:\n            self.save_weights(save_dir)\n        elif best_epoch != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric.cstr()} at epoch {best_epoch}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric: MetricDict,\n                       logger: logging.Logger,\n                       history: History,\n                       gradient_accumulation=1,\n                       grad_norm=None,\n                       ratio_width=None,\n                       eval_trn=True,\n                       **kwargs):\n        optimizer, scheduler = optimizer\n        metric.reset()\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            out, mask = self.feed_batch(batch)\n            loss = out['loss']\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if eval_trn:\n                self.decode_output(out, mask, batch)\n                self.update_metrics(metric, batch, out, mask)\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler, grad_norm)\n                report = f'loss: {total_loss / (idx + 1):.4f} {metric.cstr()}' if eval_trn \\\n                    else f'loss: {total_loss / (idx + 1):.4f}'\n                timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n            del loss\n            del out\n            del mask\n\n    def decode_output(self, outputs, mask, batch):\n        arc_scores, rel_scores = outputs['class_probabilities']['deps']['s_arc'], \\\n                                 outputs['class_probabilities']['deps']['s_rel']\n        arc_preds, rel_preds = BiaffineDependencyParser.decode(self, arc_scores, rel_scores, mask, batch)\n        outputs['arc_preds'], outputs['rel_preds'] = arc_preds, rel_preds\n        return outputs\n\n    def update_metrics(self, metrics, batch, outputs, mask):\n        arc_preds, rel_preds, puncts = outputs['arc_preds'], outputs['rel_preds'], batch.get('punct_mask', None)\n        BiaffineDependencyParser.update_metric(self, arc_preds, rel_preds, batch['arc'], batch['rel_id'], mask, puncts,\n                                               metrics['deps'], batch)\n        for task, key in zip(['lemmas', 'upos', 'feats'], ['lemma_id', 'pos_id', 'feat_id']):\n            metric: Metric = metrics[task]\n            pred = outputs['class_probabilities'][task]\n            gold = batch[key]\n            metric(pred.detach(), gold, mask=mask)\n        return metrics\n\n    def feed_batch(self, batch: dict):\n        mask = self.compute_mask(batch)\n        output_dict = self.model(batch, mask)\n        if self.model.training:\n            mask = mask.clone()\n        mask[:, 0] = 0\n        return output_dict, mask\n\n    def compute_mask(self, batch):\n        lens = batch['token_length']\n        mask = lengths_to_mask(lens)\n        return mask\n\n    def _step(self, optimizer, scheduler, grad_norm):\n        clip_grad_norm(self.model, grad_norm)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    def input_is_flat(self, data):\n        # noinspection PyCallByClass,PyTypeChecker\n        return BiaffineDependencyParser.input_is_flat(self, data, False)\n\n    def prediction_to_human(self, outputs: dict, batch):\n        arcs, rels = outputs['arc_preds'], outputs['rel_preds']\n        upos = outputs['class_probabilities']['upos'][:, 1:, :].argmax(-1).tolist()\n        feats = outputs['class_probabilities']['feats'][:, 1:, :].argmax(-1).tolist()\n        lemmas = outputs['class_probabilities']['lemmas'][:, 1:, :].argmax(-1).tolist()\n        lem_vocab = self.vocabs['lemma'].idx_to_token\n        pos_vocab = self.vocabs['pos'].idx_to_token\n        feat_vocab = self.vocabs['feat'].idx_to_token\n        # noinspection PyCallByClass,PyTypeChecker\n        for tree, form, lemma, pos, feat in zip(BiaffineDependencyParser.prediction_to_head_rel(\n                self, arcs, rels, batch), batch['token'], lemmas, upos, feats):\n            form = form[1:]\n            assert len(form) == len(tree)\n            lemma = [apply_lemma_rule(t, lem_vocab[r]) for t, r in zip(form, lemma)]\n            pos = [pos_vocab[x] for x in pos]\n            feat = [feat_vocab[x] for x in feat]\n            yield CoNLLSentence(\n                [CoNLLUWord(id=i + 1, form=fo, lemma=l, upos=p, feats=fe, head=a, deprel=r) for\n                 i, (fo, (a, r), l, p, fe) in enumerate(zip(form, tree, lemma, pos, feat))])\n\n    def __call__(self, data, batch_size=None, **kwargs) -> Union[CoNLLSentence, List[CoNLLSentence]]:\n        return super().__call__(data, batch_size, **kwargs)\n", "hanlp/components/parsers/ud/util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-14 20:44\nfrom hanlp_common.constant import ROOT\nfrom hanlp.components.parsers.ud.lemma_edit import gen_lemma_rule\n\n\ndef generate_lemma_rule(sample: dict):\n    if 'LEMMA' in sample:\n        sample['lemma'] = [gen_lemma_rule(word, lemma) if lemma != \"_\" else \"_\" for word, lemma in\n                           zip(sample['FORM'], sample['LEMMA'])]\n    return sample\n\n\ndef append_bos(sample: dict):\n    if 'FORM' in sample:\n        sample['token'] = [ROOT] + sample['FORM']\n    if 'UPOS' in sample:\n        sample['pos'] = sample['UPOS'][:1] + sample['UPOS']\n        sample['arc'] = [0] + sample['HEAD']\n        sample['rel'] = sample['DEPREL'][:1] + sample['DEPREL']\n        sample['lemma'] = sample['lemma'][:1] + sample['lemma']\n        sample['feat'] = sample['FEATS'][:1] + sample['FEATS']\n    return sample\n\n\ndef sample_form_missing(sample: dict):\n    return all(t == '_' for t in sample['FORM'])\n", "hanlp/components/parsers/ud/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-14 20:34\n", "hanlp/components/parsers/ud/ud_model.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-15 14:21\n\nfrom typing import Dict, Any\n\nimport torch\n\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDecoder\nfrom hanlp.components.parsers.ud.tag_decoder import TagDecoder\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbeddingModule\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropout\n\n\nclass UniversalDependenciesModel(torch.nn.Module):\n    def __init__(self,\n                 encoder: ContextualWordEmbeddingModule,\n                 n_mlp_arc,\n                 n_mlp_rel,\n                 mlp_dropout,\n                 num_rels,\n                 num_lemmas,\n                 num_upos,\n                 num_feats,\n                 mix_embedding: int = 13,\n                 layer_dropout: int = 0.0):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = UniversalDependenciesDecoder(\n            encoder.get_output_dim(),\n            n_mlp_arc,\n            n_mlp_rel,\n            mlp_dropout,\n            num_rels,\n            num_lemmas,\n            num_upos,\n            num_feats,\n            mix_embedding,\n            layer_dropout\n        )\n\n    def forward(self,\n                batch: Dict[str, torch.Tensor],\n                mask,\n                ):\n        hidden = self.encoder(batch)\n        return self.decoder(hidden, batch=batch, mask=mask)\n\n\nclass UniversalDependenciesDecoder(torch.nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 n_mlp_arc,\n                 n_mlp_rel,\n                 mlp_dropout,\n                 num_rels,\n                 num_lemmas,\n                 num_upos,\n                 num_feats,\n                 mix_embedding: int = 13,\n                 layer_dropout: int = 0.0,\n                 ) -> None:\n        super(UniversalDependenciesDecoder, self).__init__()\n\n        # decoders\n        self.decoders = torch.nn.ModuleDict({\n            'lemmas': TagDecoder(hidden_size, num_lemmas, label_smoothing=0.03, adaptive=True),\n            'upos': TagDecoder(hidden_size, num_upos, label_smoothing=0.03, adaptive=True),\n            'deps': BiaffineDecoder(hidden_size, n_mlp_arc, n_mlp_rel, mlp_dropout, num_rels),\n            'feats': TagDecoder(hidden_size, num_feats, label_smoothing=0.03, adaptive=True),\n        })\n        self.gold_keys = {\n            'lemmas': 'lemma_id',\n            'upos': 'pos_id',\n            'feats': 'feat_id',\n        }\n\n        if mix_embedding:\n            self.scalar_mix = torch.nn.ModuleDict({\n                task: ScalarMixWithDropout((1, mix_embedding),\n                                           do_layer_norm=False,\n                                           dropout=layer_dropout)\n                for task in self.decoders\n            })\n        else:\n            self.scalar_mix = None\n\n    def forward(self,\n                hidden,\n                batch: Dict[str, torch.Tensor],\n                mask) -> Dict[str, Any]:\n        mask_without_root = mask.clone()\n        mask_without_root[:, 0] = False\n\n        logits = {}\n        class_probabilities = {}\n        output_dict = {\"logits\": logits,\n                       \"class_probabilities\": class_probabilities}\n        loss = 0\n\n        arc = batch.get('arc', None)\n        # Run through each of the tasks on the shared encoder and save predictions\n        for task in self.decoders:\n            if self.scalar_mix:\n                decoder_input = self.scalar_mix[task](hidden, mask)\n            else:\n                decoder_input = hidden\n\n            if task == \"deps\":\n                s_arc, s_rel = self.decoders[task](decoder_input, mask)\n                pred_output = {'class_probabilities': {'s_arc': s_arc, 's_rel': s_rel}}\n                if arc is not None:\n                    # noinspection PyTypeChecker\n                    pred_output['loss'] = BiaffineDependencyParser.compute_loss(None, s_arc, s_rel, arc,\n                                                                                batch['rel_id'],\n                                                                                mask_without_root,\n                                                                                torch.nn.functional.cross_entropy)\n            else:\n                pred_output = self.decoders[task](decoder_input, mask_without_root,\n                                                  batch.get(self.gold_keys[task], None))\n            if 'logits' in pred_output:\n                logits[task] = pred_output[\"logits\"]\n            if 'class_probabilities' in pred_output:\n                class_probabilities[task] = pred_output[\"class_probabilities\"]\n            if 'loss' in pred_output:\n                # Keep track of the loss if we have the gold tags available\n                loss += pred_output[\"loss\"]\n\n        if arc is not None:\n            output_dict[\"loss\"] = loss\n\n        return output_dict\n\n    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        for task in self.tasks:\n            self.decoders[task].decode(output_dict)\n\n        return output_dict\n", "hanlp/components/parsers/biaffine/biaffine_model.py": "# -*- coding: utf-8 -*-\nfrom typing import Any, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence,\n                                pad_sequence)\n\nfrom hanlp.components.parsers.biaffine.biaffine import Biaffine\nfrom hanlp.components.parsers.biaffine.mlp import MLP\nfrom hanlp.components.parsers.biaffine.variationalbilstm import VariationalLSTM\nfrom hanlp.layers.dropout import IndependentDropout, SharedDropout, WordDropout\nfrom hanlp.layers.transformers.encoder import TransformerEncoder\nfrom hanlp.layers.transformers.pt_imports import PreTrainedModel, PreTrainedTokenizer\nfrom hanlp.layers.transformers.utils import transformer_encode\n\n\nclass EncoderWithContextualLayer(nn.Module):\n    def __init__(self,\n                 config,\n                 pretrained_embed: torch.Tensor = None,\n                 transformer: PreTrainedModel = None,\n                 transformer_tokenizer: PreTrainedTokenizer = None,\n                 ):\n        super(EncoderWithContextualLayer, self).__init__()\n\n        self.secondary_encoder = config.get('secondary_encoder', None)\n        self.config = config\n\n        if not transformer:\n            self.pad_index = config.pad_index\n            self.unk_index = config.unk_index\n            if config.word_dropout:\n                oov = self.unk_index\n                excludes = [self.pad_index]\n                self.word_dropout = WordDropout(p=config.word_dropout, oov_token=oov, exclude_tokens=excludes)\n            else:\n                self.word_dropout = None\n        if transformer:\n            input_size = 0\n            if self.config.transformer_lr:\n                hidden_size = transformer.config.hidden_size\n            else:\n                input_size = transformer.config.hidden_size\n                hidden_size = config.n_lstm_hidden * 2\n            if config.feat == 'pos':\n                self.feat_embed = nn.Embedding(num_embeddings=config.n_feats,\n                                               embedding_dim=config.n_embed)\n                self.embed_dropout = IndependentDropout(p=config.embed_dropout)\n                if self.config.transformer_lr:\n                    hidden_size += config.n_embed\n                else:\n                    input_size += config.n_embed\n            if not self.config.transformer_lr:\n                self.lstm = VariationalLSTM(input_size=input_size,\n                                            hidden_size=config.n_lstm_hidden,\n                                            num_layers=config.n_lstm_layers,\n                                            dropout=config.hidden_dropout, bidirectional=True)\n        else:\n            # the embedding layer\n            input_size = config.n_embed\n            self.word_embed = nn.Embedding(num_embeddings=config.n_words,\n                                           embedding_dim=config.n_embed)\n            if pretrained_embed is not None:\n                if not isinstance(pretrained_embed, torch.Tensor):\n                    pretrained_embed = torch.Tensor(pretrained_embed)\n                self.pretrained = nn.Embedding.from_pretrained(pretrained_embed)\n                nn.init.zeros_(self.word_embed.weight)\n            if config.feat == 'pos':\n                self.feat_embed = nn.Embedding(num_embeddings=config.n_feats,\n                                               embedding_dim=config.n_embed)\n                self.embed_dropout = IndependentDropout(p=config.embed_dropout)\n                input_size += config.n_embed\n\n            # the word-lstm layer\n            hidden_size = config.n_lstm_hidden * 2\n            self.lstm = VariationalLSTM(input_size=input_size,\n                                        hidden_size=config.n_lstm_hidden,\n                                        num_layers=config.n_lstm_layers,\n                                        dropout=config.hidden_dropout, bidirectional=True)\n        self.hidden_size = hidden_size\n        self.hidden_dropout = SharedDropout(p=config.hidden_dropout)\n        if transformer:\n            transformer = TransformerEncoder(transformer, transformer_tokenizer, config.average_subwords,\n                                             word_dropout=config.word_dropout,\n                                             max_sequence_length=config.max_sequence_length)\n        self.transformer = transformer\n\n    def forward(self, words, feats, input_ids, token_span, mask, lens):\n        if mask is None:\n            # get the mask and lengths of given batch\n            mask = words.ne(self.pad_index)\n        if lens is None:\n            lens = mask.sum(dim=1)\n        batch_size, seq_len = mask.shape\n        if self.config.transformer:\n            # trans_embed = self.run_transformer(input_ids, token_span=token_span)\n            trans_embed = self.transformer.forward(input_ids, token_span=token_span)\n            if hasattr(self, 'feat_embed'):\n                feat_embed = self.feat_embed(feats)\n                trans_embed, feat_embed = self.embed_dropout(trans_embed, feat_embed)\n                embed = torch.cat((trans_embed, feat_embed), dim=-1)\n            else:\n                embed = trans_embed\n            if hasattr(self, 'lstm'):\n                x = self.run_rnn(embed, lens, seq_len)\n            else:\n                x = embed\n            if self.secondary_encoder:\n                x = self.secondary_encoder(x, mask)\n            x = self.hidden_dropout(x)\n        else:\n            if self.word_dropout:\n                words = self.word_dropout(words)\n            # set the indices larger than num_embeddings to unk_index\n            ext_mask = words.ge(self.word_embed.num_embeddings)\n            ext_words = words.masked_fill(ext_mask, self.unk_index)\n\n            # get outputs from embedding layers\n            word_embed = self.word_embed(ext_words)\n            if hasattr(self, 'pretrained'):\n                word_embed += self.pretrained(words)\n            if self.config.feat == 'char':\n                feat_embed = self.feat_embed(feats[mask])\n                feat_embed = pad_sequence(feat_embed.split(lens.tolist()), True)\n            elif self.config.feat == 'bert':\n                feat_embed = self.feat_embed(*feats)\n            elif hasattr(self, 'feat_embed'):\n                feat_embed = self.feat_embed(feats)\n            else:\n                feat_embed = None\n            if feat_embed is not None:\n                word_embed, feat_embed = self.embed_dropout(word_embed, feat_embed)\n                # concatenate the word and feat representations\n                embed = torch.cat((word_embed, feat_embed), dim=-1)\n            else:\n                embed = word_embed\n\n            x = self.run_rnn(embed, lens, seq_len)\n            x = self.hidden_dropout(x)\n        return x, mask\n\n    def run_rnn(self, embed, lens, seq_len):\n        x = pack_padded_sequence(embed, lens, True, False)\n        x, _ = self.lstm(x)\n        x, _ = pad_packed_sequence(x, True, total_length=seq_len)\n        return x\n\n    def run_transformer(self, input_ids, token_span):\n        return transformer_encode(self.transformer, input_ids, None, None, token_span,\n                                  average_subwords=self.config.average_subwords)\n\n\nclass BiaffineDecoder(nn.Module):\n    def __init__(self, hidden_size, n_mlp_arc, n_mlp_rel, mlp_dropout, n_rels, arc_dropout=None,\n                 rel_dropout=None) -> None:\n        super().__init__()\n        # the MLP layers\n        self.mlp_arc_h = MLP(hidden_size,\n                             n_mlp_arc,\n                             dropout=arc_dropout or mlp_dropout)\n        self.mlp_arc_d = MLP(hidden_size,\n                             n_mlp_arc,\n                             dropout=arc_dropout or mlp_dropout)\n        self.mlp_rel_h = MLP(hidden_size,\n                             n_mlp_rel,\n                             dropout=rel_dropout or mlp_dropout)\n        self.mlp_rel_d = MLP(hidden_size,\n                             n_mlp_rel,\n                             dropout=rel_dropout or mlp_dropout)\n\n        # the Biaffine layers\n        self.arc_attn = Biaffine(n_in=n_mlp_arc,\n                                 bias_x=True,\n                                 bias_y=False)\n        self.rel_attn = Biaffine(n_in=n_mlp_rel,\n                                 n_out=n_rels,\n                                 bias_x=True,\n                                 bias_y=True)\n\n    def forward(self, x, mask=None, **kwargs: Any) -> Tuple[torch.Tensor, torch.Tensor]:\n        arc_d, arc_h, rel_d, rel_h = self.apply_mlps(x)\n\n        s_arc, s_rel = self.decode(arc_d, arc_h, rel_d, rel_h, mask, self.arc_attn, self.rel_attn)\n\n        return s_arc, s_rel\n\n    @staticmethod\n    def decode(arc_d, arc_h, rel_d, rel_h, mask, arc_attn, rel_attn):\n        # get arc and rel scores from the bilinear attention\n        # [batch_size, seq_len, seq_len]\n        s_arc = arc_attn(arc_d, arc_h)\n        # [batch_size, seq_len, seq_len, n_rels]\n        s_rel = rel_attn(rel_d, rel_h).permute(0, 2, 3, 1)\n        if mask is not None:\n            # set the scores that exceed the length of each sentence to -inf\n            s_arc.masked_fill_(~mask.unsqueeze(1), float('-inf'))\n        return s_arc, s_rel\n\n    def apply_mlps(self, x):\n        # apply MLPs to the hidden states\n        arc_d = self.mlp_arc_d(x)\n        arc_h = self.mlp_arc_h(x)\n        rel_d = self.mlp_rel_d(x)\n        rel_h = self.mlp_rel_h(x)\n        return arc_d, arc_h, rel_d, rel_h\n\n\nclass BiaffineDependencyModel(nn.Module):\n\n    def __init__(self, config, pretrained_embed: torch.Tensor = None, transformer: PreTrainedModel = None,\n                 transformer_tokenizer: PreTrainedTokenizer = None):\n        super().__init__()\n        self.encoder = EncoderWithContextualLayer(config, pretrained_embed, transformer, transformer_tokenizer)\n        self.biaffine_decoder = BiaffineDecoder(self.encoder.hidden_size,\n                                                config.n_mlp_arc,\n                                                config.n_mlp_rel,\n                                                config.mlp_dropout,\n                                                config.n_rels)\n\n    def forward(self,\n                words=None,\n                feats=None,\n                input_ids=None,\n                token_span=None,\n                mask=None, lens=None, **kwargs):\n        x, mask = self.encoder(words, feats, input_ids, token_span, mask, lens)\n        s_arc, s_rel = self.biaffine_decoder(x, mask)\n\n        return s_arc, s_rel\n", "hanlp/components/parsers/biaffine/structual_attention.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-26 10:40\nfrom typing import Union, List\n\nimport torch\nimport torch.nn.functional as F\nfrom hanlp.utils.torch_util import lengths_to_mask\nfrom torch import nn\n\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDecoder\nfrom hanlp.layers.transformers.encoder import TransformerEncoder\nfrom hanlp.layers.transformers.pt_imports import PreTrainedModel, PreTrainedTokenizer\nfrom hanlp.metrics.accuracy import CategoricalAccuracy\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass StructuralAttentionLayer(nn.Module):\n\n    def __init__(self, hidden_size, n_mlp_arc, n_mlp_rel, mlp_dropout, n_rels, projeciton=None) -> None:\n        super().__init__()\n        self.biaffine = BiaffineDecoder(hidden_size, n_mlp_arc, n_mlp_rel, mlp_dropout, n_rels)\n        if projeciton:\n            self.projection = nn.Linear(hidden_size, projeciton)\n            hidden_size = projeciton\n        else:\n            self.projection = None\n        self.head_WV = nn.Parameter(torch.randn(n_rels, hidden_size, hidden_size))\n        self.dense = nn.Linear(hidden_size * n_rels, hidden_size)\n        self.activation = nn.GELU()\n\n    def forward(self, x, mask):\n        s_arc, s_rel = self.biaffine(x, mask)\n        p_arc = F.softmax(s_arc, dim=-1) * mask.unsqueeze(-1)\n        p_rel = F.softmax(s_rel, -1)\n        A = p_arc.unsqueeze(-1) * p_rel\n        if self.projection:\n            x = self.projection(x)\n        Ax = torch.einsum('bijk,bih->bihk', A, x)\n        AxW = torch.einsum('bihk,khm->bihk', Ax, self.head_WV)\n        AxW = AxW.flatten(2)\n        x = self.dense(AxW)\n        x = self.activation(x)\n        return s_arc, s_rel, x\n\n\nclass StructuralAttentionModel(nn.Module):\n    def __init__(self,\n                 config,\n                 transformer: PreTrainedModel = None,\n                 transformer_tokenizer: PreTrainedTokenizer = None\n                 ) -> None:\n        super().__init__()\n        self.encoder = TransformerEncoder(transformer,\n                                          transformer_tokenizer,\n                                          config.average_subwords,\n                                          config.scalar_mix,\n                                          None,  # No word_dropout since SA is predicting masked tokens\n                                          config.transformer_hidden_dropout,\n                                          config.layer_dropout,\n                                          config.max_sequence_length)\n        hidden_size = transformer.config.hidden_size\n        self.sa = StructuralAttentionLayer(hidden_size,\n                                           config.n_mlp_arc,\n                                           config.n_mlp_rel,\n                                           config.mlp_dropout,\n                                           config.n_rels,\n                                           config.projection\n                                           )\n        if config.projection:\n            hidden_size = config.projection\n        self.mlm = nn.Linear(hidden_size, transformer_tokenizer.vocab_size)\n\n    def forward(self,\n                input_ids: torch.LongTensor,\n                attention_mask=None,\n                token_type_ids=None,\n                token_span=None,\n                mask=None,\n                batch=None,\n                **kwargs):\n        h = self.encoder(input_ids, attention_mask, token_type_ids, token_span)\n        s_arc, s_rel, h = self.sa(h, mask)\n        x = self.mlm(h)\n        return s_arc, s_rel, x\n\n\nclass MaskedTokenGenerator(object):\n\n    def __init__(self, transformer_tokenizer: PreTrainedTokenizer, mask_prob=0.15) -> None:\n        super().__init__()\n        self.mask_prob = mask_prob\n        self.transformer_tokenizer = transformer_tokenizer\n        self.oov = transformer_tokenizer.mask_token_id\n        self.pad = transformer_tokenizer.pad_token_id\n        self.cls = transformer_tokenizer.cls_token_id\n        self.sep = transformer_tokenizer.sep_token_id\n        self.excludes = [self.pad, self.cls, self.sep]\n\n    def __call__(self, tokens: torch.LongTensor, prefix_mask: torch.LongTensor):\n        padding_mask = tokens.new_ones(tokens.size(), dtype=torch.bool)\n        for pad in self.excludes:\n            padding_mask &= (tokens != pad)\n        padding_mask &= prefix_mask  # Only mask prefixes since the others won't be attended\n        # Create a uniformly random mask selecting either the original words or OOV tokens\n        dropout_mask = (tokens.new_empty(tokens.size(), dtype=torch.float).uniform_() < self.mask_prob)\n        oov_mask = dropout_mask & padding_mask\n\n        oov_fill = tokens.new_empty(tokens.size(), dtype=torch.long).fill_(self.oov)\n\n        result = torch.where(oov_mask, oov_fill, tokens)\n        return result, oov_mask\n\n\nclass StructuralAttentionParser(BiaffineDependencyParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.model: StructuralAttentionModel = None\n        self.mlm_generator: MaskedTokenGenerator = None\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        transformer = TransformerEncoder.build_transformer(config=self.config, training=training)\n        model = StructuralAttentionModel(self.config, transformer, self.transformer_tokenizer)\n        return model\n\n    def fit(self, trn_data, dev_data, save_dir,\n            transformer=None,\n            mask_prob=0.15,\n            projection=None,\n            average_subwords=False,\n            transformer_hidden_dropout=None,\n            layer_dropout=0,\n            mix_embedding: int = None,\n            embed_dropout=.33,\n            n_mlp_arc=500,\n            n_mlp_rel=100,\n            mlp_dropout=.33,\n            lr=2e-3,\n            transformer_lr=5e-5,\n            mu=.9,\n            nu=.9,\n            epsilon=1e-12,\n            clip=5.0,\n            decay=.75,\n            decay_steps=5000,\n            patience=100,\n            sampler='kmeans',\n            n_buckets=32,\n            batch_max_tokens=5000,\n            batch_size=None,\n            epochs=50000,\n            tree=False,\n            punct=False,\n            logger=None,\n            verbose=True,\n            max_sequence_length=512,\n            devices: Union[float, int, List[int]] = None,\n            transform=None,\n            **kwargs):\n        return TorchComponent.fit(self, **merge_locals_kwargs(locals(), kwargs))\n\n    def feed_batch(self, batch):\n        if self.model.training:\n            input_ids = batch['input_ids']\n            prefix_mask = batch['prefix_mask']\n            batch['gold_input_ids'] = input_ids\n            batch['input_ids'], batch['input_ids_mask'] = self.mlm_generator(input_ids, prefix_mask)\n        words, feats, lens, puncts = batch.get('token_id', None), batch.get('pos_id', None), batch['sent_length'], \\\n                                     batch.get('punct_mask', None)\n        mask = lengths_to_mask(lens)\n        arc_scores, rel_scores, pred_input_ids = self.model(words=words, feats=feats, mask=mask, batch=batch, **batch)\n        batch['pred_input_ids'] = pred_input_ids\n        # ignore the first token of each sentence\n        # RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n        if self.model.training:\n            mask = mask.clone()\n        mask[:, 0] = 0\n        return arc_scores, rel_scores, mask, puncts\n\n    def on_config_ready(self, **kwargs):\n        super().on_config_ready(**kwargs)\n        self.mlm_generator = MaskedTokenGenerator(self.transformer_tokenizer, self.config.mask_prob)\n\n    def compute_loss(self, arc_scores, rel_scores, arcs, rels, mask, criterion, batch=None):\n        parse_loss = BiaffineDependencyParser.compute_loss(self, arc_scores, rel_scores, arcs, rels, mask, criterion, batch)\n        if self.model.training:\n            gold_input_ids = batch['gold_input_ids']\n            pred_input_ids = batch['pred_input_ids']\n            input_ids_mask = batch['input_ids_mask']\n            token_span = batch['token_span']\n            gold_input_ids = batch['gold_input_ids'] = gold_input_ids.gather(1, token_span[:, :, 0])\n            input_ids_mask = batch['input_ids_mask'] = input_ids_mask.gather(1, token_span[:, :, 0])\n            mlm_loss = F.cross_entropy(pred_input_ids[input_ids_mask], gold_input_ids[input_ids_mask])\n            loss = parse_loss + mlm_loss\n            return loss\n        return parse_loss\n\n    def build_tokenizer_transform(self):\n        return TransformerSequenceTokenizer(self.transformer_tokenizer, 'token', '', ret_prefix_mask=True,\n                                            ret_token_span=True, cls_is_bos=True,\n                                            max_seq_length=self.config.get('max_sequence_length',\n                                                                           512),\n                                            truncate_long_sequences=False)\n\n    def build_metric(self, training=None, **kwargs):\n        parse_metric = super().build_metric(**kwargs)\n        if training:\n            mlm_metric = CategoricalAccuracy()\n            return parse_metric, mlm_metric\n        return parse_metric\n\n    def update_metric(self, arc_scores, rel_scores, arcs, rels, mask, puncts, metric, batch=None):\n        if isinstance(metric, tuple):\n            parse_metric, mlm_metric = metric\n            super().update_metric(arc_scores, rel_scores, arcs, rels, mask, puncts, parse_metric)\n            gold_input_ids = batch['gold_input_ids']\n            input_ids_mask = batch['input_ids_mask']\n            pred_input_ids = batch['pred_input_ids']\n            pred_input_ids = pred_input_ids[input_ids_mask]\n            gold_input_ids = gold_input_ids[input_ids_mask]\n            if len(pred_input_ids):\n                mlm_metric(pred_input_ids, gold_input_ids)\n        else:\n            super().update_metric(arc_scores, rel_scores, arcs, rels, mask, puncts, metric)\n\n    def _report(self, loss, metric):\n        if isinstance(metric, tuple):\n            parse_metric, mlm_metric = metric\n            return super()._report(loss, parse_metric) + f' {mlm_metric}'\n        else:\n            return super()._report(loss, metric)\n", "hanlp/components/parsers/biaffine/mlp.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\n\nimport torch.nn as nn\n\nfrom hanlp.layers.dropout import SharedDropout\n\n\nclass MLP(nn.Module):\n    r\"\"\"\n    Applies a linear transformation together with a non-linear activation to the incoming tensor:\n    :math:`y = \\mathrm{Activation}(x A^T + b)`\n\n    Args:\n        n_in (~torch.Tensor):\n            The size of each input feature.\n        n_out (~torch.Tensor):\n            The size of each output feature.\n        dropout (float):\n            If non-zero, introduce a :class:`SharedDropout` layer on the output with this dropout ratio. Default: 0.\n        activation (bool):\n            Whether to use activations. Default: True.\n    \"\"\"\n\n    def __init__(self, n_in, n_out, dropout=0, activation=True):\n        super().__init__()\n\n        self.n_in = n_in\n        self.n_out = n_out\n        self.linear = nn.Linear(n_in, n_out)\n        self.activation = nn.LeakyReLU(negative_slope=0.1) if activation else nn.Identity()\n        self.dropout = SharedDropout(p=dropout)\n\n        self.reset_parameters()\n\n    def __repr__(self):\n        s = f\"n_in={self.n_in}, n_out={self.n_out}\"\n        if self.dropout.p > 0:\n            s += f\", dropout={self.dropout.p}\"\n\n        return f\"{self.__class__.__name__}({s})\"\n\n    def reset_parameters(self):\n        nn.init.orthogonal_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        r\"\"\"\n        Args:\n            x (~torch.Tensor):\n                The size of each input feature is `n_in`.\n\n        Returns:\n            A tensor with the size of each output feature `n_out`.\n        \"\"\"\n\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n\n        return x\n\n", "hanlp/components/parsers/biaffine/biaffine_sdp.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-28 15:12\nimport functools\nfrom collections import Counter\nfrom typing import Union, List\n\nimport torch\nfrom torch import nn\n\nfrom hanlp_common.constant import UNK\nfrom hanlp.common.transform import TransformList\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp_common.conll import CoNLLUWord, CoNLLSentence\nfrom hanlp.datasets.parsing.semeval15 import unpack_deps_to_head_deprel, append_bos_to_form_pos\nfrom hanlp.metrics.parsing.labeled_f1 import LabeledF1\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BiaffineSemanticDependencyParser(BiaffineDependencyParser):\n    def __init__(self) -> None:\n        r\"\"\"Implementation of \"Stanford's graph-based neural dependency parser at\n        the conll 2017 shared task\" (:cite:`dozat2017stanford`) and \"Establishing Strong Baselines for the New Decade\"\n        (:cite:`he-choi-2019`).\n        \"\"\"\n        super().__init__()\n\n    def get_pad_dict(self):\n        return {'arc': False}\n\n    def build_metric(self, **kwargs):\n        return LabeledF1()\n\n    # noinspection PyMethodOverriding\n    def build_dataset(self, data, transform=None):\n        transforms = TransformList(functools.partial(append_bos_to_form_pos, pos_key='UPOS'),\n                                   functools.partial(unpack_deps_to_head_deprel, pad_rel=self.config.pad_rel))\n        if transform:\n            transforms.append(transform)\n        return super(BiaffineSemanticDependencyParser, self).build_dataset(data, transforms)\n\n    def build_criterion(self, **kwargs):\n        return nn.BCEWithLogitsLoss(), nn.CrossEntropyLoss()\n\n    def feed_batch(self, batch):\n        arc_scores, rel_scores, mask, puncts = super().feed_batch(batch)\n        mask = self.convert_to_3d_mask(arc_scores, mask)\n        puncts = self.convert_to_3d_puncts(puncts, mask)\n        return arc_scores, rel_scores, mask, puncts\n\n    @staticmethod\n    def convert_to_3d_puncts(puncts, mask):\n        if puncts is not None:\n            puncts = puncts.unsqueeze(-1).expand_as(mask)\n        return puncts\n\n    @staticmethod\n    def convert_to_3d_mask(arc_scores, mask):\n        # 3d masks\n        mask = mask.unsqueeze(-1).expand_as(arc_scores).clone()\n        mask[:, :, 1:] = mask[:, :, 1:] & mask.transpose(1, 2)[:, :, 1:]  # Keep the 1st colum because it predicts root\n        return mask\n\n    def compute_loss(self, arc_scores, rel_scores, arcs, rels, mask: torch.BoolTensor, criterion, batch=None):\n        bce, ce = criterion\n        arc_scores, arcs = arc_scores[mask], arcs[mask]\n        rel_scores, rels = rel_scores[mask], rels[mask]\n        rel_scores, rels = rel_scores[arcs], rels[arcs]\n        arc_loss = bce(arc_scores, arcs.to(torch.float))\n        arc_loss_interpolation = self.config.get('arc_loss_interpolation', None)\n        loss = arc_loss * arc_loss_interpolation if arc_loss_interpolation else arc_loss\n        if len(rels):\n            rel_loss = ce(rel_scores, rels)\n            loss += (rel_loss * (1 - arc_loss_interpolation)) if arc_loss_interpolation else rel_loss\n        if arc_loss_interpolation:\n            loss *= 2\n        return loss\n\n    def cache_dataset(self, dataset, timer, training=False, logger=None):\n        if not self.config.apply_constraint:\n            return super(BiaffineSemanticDependencyParser, self).cache_dataset(dataset, timer, training)\n        num_roots = Counter()\n        no_zero_head = True\n        root_rels = Counter()\n        for each in dataset:\n            if training:\n                num_roots[sum([x[0] for x in each['arc']])] += 1\n                no_zero_head &= all([x != '_' for x in each['DEPS']])\n                head_is_root = [i for i in range(len(each['arc'])) if each['arc'][i][0]]\n                if head_is_root:\n                    for i in head_is_root:\n                        root_rels[each['rel'][i][0]] += 1\n            timer.log('Preprocessing and caching samples [blink][yellow]...[/yellow][/blink]')\n        if training:\n            if self.config.single_root is None:\n                self.config.single_root = len(num_roots) == 1 and num_roots.most_common()[0][0] == 1\n            if self.config.no_zero_head is None:\n                self.config.no_zero_head = no_zero_head\n            root_rel = root_rels.most_common()[0][0]\n            self.config.root_rel_id = self.vocabs['rel'].get_idx(root_rel)\n            if logger:\n                logger.info(f'Training set properties: [blue]single_root = {self.config.single_root}[/blue], '\n                            f'[blue]no_zero_head = {no_zero_head}[/blue], '\n                            f'[blue]root_rel = {root_rel}[/blue]')\n\n    def decode(self, arc_scores, rel_scores, mask, batch=None):\n        eye = torch.arange(0, arc_scores.size(1), device=arc_scores.device).view(1, 1, -1).expand(\n            arc_scores.size(0), -1, -1)\n        inf = float('inf')\n        arc_scores.scatter_(dim=1, index=eye, value=-inf)\n\n        if self.config.apply_constraint:\n            if self.config.get('single_root', False):\n                arc_scores[~mask] = -inf  # the biaffine decoder doesn't apply 3d mask for now\n                root_mask = arc_scores[:, :, 0].argmax(dim=-1).unsqueeze_(-1).expand_as(arc_scores[:, :, 0])\n                arc_scores[:, :, 0] = -inf\n                arc_scores[:, :, 0].scatter_(dim=-1, index=root_mask, value=inf)\n\n            root_rel_id = self.config.root_rel_id\n            rel_scores[:, :, 0, root_rel_id] = inf\n            rel_scores[:, :, 1:, root_rel_id] = -inf\n\n            arc_scores_T = arc_scores.transpose(-1, -2)\n            arc = ((arc_scores > 0) & (arc_scores_T < arc_scores))\n            if self.config.get('no_zero_head', False):\n                arc_scores_T[arc] = -inf  # avoid cycle between a pair of nodes\n                arc_scores_fix = arc_scores_T.argmax(dim=-2).unsqueeze_(-1).expand_as(arc_scores)\n                arc.scatter_(dim=-1, index=arc_scores_fix, value=True)\n        else:\n            arc = arc_scores > 0\n        rel = rel_scores.argmax(dim=-1)\n        return arc, rel\n\n    def collect_outputs_extend(self, predictions, arc_preds, rel_preds, lens, mask):\n        predictions.extend(zip(arc_preds.tolist(), rel_preds.tolist(), mask.tolist()))\n        # all_arcs.extend(seq.tolist() for seq in arc_preds[mask].split([x * x for x in lens]))\n        # all_rels.extend(seq.tolist() for seq in rel_preds[mask].split([x * x for x in lens]))\n\n    def predictions_to_human(self, predictions, outputs, data, use_pos, conll=True):\n        for d, (arcs, rels, masks) in zip(data, predictions):\n            sent = CoNLLSentence()\n            for idx, (cell, a, r) in enumerate(zip(d, arcs[1:], rels[1:])):\n                if use_pos:\n                    token, pos = cell\n                else:\n                    token, pos = cell, None\n                heads = [i for i in range(len(d) + 1) if a[i]]\n                deprels = [self.vocabs['rel'][r[i]] for i in range(len(d) + 1) if a[i]]\n                sent.append(\n                    CoNLLUWord(idx + 1, token, upos=pos, head=None, deprel=None, deps=list(zip(heads, deprels))))\n            outputs.append(sent)\n\n    def fit(self, trn_data, dev_data, save_dir,\n            feat=None,\n            n_embed=100,\n            pretrained_embed=None,\n            transformer=None,\n            average_subwords=False,\n            word_dropout: float = 0.2,\n            transformer_hidden_dropout=None,\n            layer_dropout=0,\n            mix_embedding: int = None,\n            embed_dropout=.33,\n            n_lstm_hidden=400,\n            n_lstm_layers=3,\n            hidden_dropout=.33,\n            n_mlp_arc=500,\n            n_mlp_rel=100,\n            mlp_dropout=.33,\n            arc_dropout=None,\n            rel_dropout=None,\n            arc_loss_interpolation=0.4,\n            lr=2e-3,\n            transformer_lr=5e-5,\n            mu=.9,\n            nu=.9,\n            epsilon=1e-12,\n            clip=5.0,\n            decay=.75,\n            decay_steps=5000,\n            weight_decay=0,\n            warmup_steps=0.1,\n            separate_optimizer=True,\n            patience=100,\n            batch_size=None,\n            sampler_builder=None,\n            lowercase=False,\n            epochs=50000,\n            apply_constraint=False,\n            single_root=None,\n            no_zero_head=None,\n            punct=False,\n            min_freq=2,\n            logger=None,\n            verbose=True,\n            unk=UNK,\n            pad_rel=None,\n            max_sequence_length=512,\n            gradient_accumulation=1,\n            devices: Union[float, int, List[int]] = None,\n            transform=None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/parsers/biaffine/biaffine_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 20:51\nimport os\nfrom collections import Counter\nfrom typing import Union, Any, List\n\nfrom hanlp.layers.transformers.pt_imports import AutoTokenizer, PreTrainedTokenizer, AutoModel_\nimport torch\nfrom hanlp.utils.torch_util import lengths_to_mask\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader\nfrom hanlp_common.constant import ROOT, UNK, IDX\nfrom hanlp.common.dataset import PadSequenceDataLoader\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import LowerCase, FieldLength, PunctuationMask\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.parsers.alg import decode_dep\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDependencyModel\nfrom hanlp_common.conll import CoNLLWord, CoNLLSentence\nfrom hanlp.datasets.parsing.loaders.conll_dataset import CoNLLParsingDataset, append_bos\nfrom hanlp.layers.embeddings.util import index_word2vec_with_vocab\nfrom hanlp.layers.transformers.utils import build_optimizer_scheduler_with_transformer\nfrom hanlp.metrics.parsing.attachmentscore import AttachmentScore\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import isdebugging, merge_locals_kwargs, merge_dict, reorder\n\n\nclass BiaffineDependencyParser(TorchComponent):\n    def __init__(self) -> None:\n        \"\"\"Biaffine dependency parsing (:cite:`dozat:17a`).\n        \"\"\"\n        super().__init__()\n        self.model: BiaffineDependencyModel = None\n        self.transformer_tokenizer: PreTrainedTokenizer = None\n\n    def predict(self, data: Any, batch_size=None, batch_max_tokens=None, conll=True, **kwargs):\n        if not data:\n            return []\n        use_pos = self.use_pos\n        flat = self.input_is_flat(data, use_pos)\n        if flat:\n            data = [data]\n        samples = self.build_samples(data, use_pos)\n        if not batch_max_tokens:\n            batch_max_tokens = self.config.get('batch_max_tokens', None)\n        if not batch_size:\n            batch_size = self.config.batch_size\n        dataloader = self.build_dataloader(samples,\n                                           device=self.devices[0], shuffle=False,\n                                           **merge_dict(self.config,\n                                                        batch_size=batch_size,\n                                                        batch_max_tokens=batch_max_tokens,\n                                                        overwrite=True,\n                                                        **kwargs))\n        predictions, build_data, data, order = self.before_outputs(data)\n        for batch in dataloader:\n            arc_scores, rel_scores, mask, puncts = self.feed_batch(batch)\n            self.collect_outputs(arc_scores, rel_scores, mask, batch, predictions, order, data, use_pos,\n                                 build_data)\n        outputs = self.post_outputs(predictions, data, order, use_pos, build_data, conll=conll)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    def build_samples(self, data, use_pos=None):\n        samples = []\n        pos_key = 'CPOS' if 'CPOS' in self.vocabs else 'UPOS'\n        for idx, each in enumerate(data):\n            sample = {IDX: idx}\n            if use_pos:\n                token, pos = zip(*each)\n                sample.update({'FORM': list(token), pos_key: list(pos)})\n            else:\n                token = each\n                sample.update({'FORM': list(token)})\n            samples.append(sample)\n        return samples\n\n    def input_is_flat(self, data, use_pos=None):\n        if use_pos is None:\n            use_pos = 'CPOS' in self.vocabs\n        if use_pos:\n            flat = isinstance(data[0], (list, tuple)) and isinstance(data[0][0], str)\n        else:\n            flat = isinstance(data[0], str)\n        return flat\n\n    def before_outputs(self, data):\n        predictions, order = [], []\n        build_data = data is None\n        if build_data:\n            data = []\n        return predictions, build_data, data, order\n\n    def post_outputs(self, predictions, data, order, use_pos, build_data, conll=True):\n        predictions = reorder(predictions, order)\n        if build_data:\n            data = reorder(data, order)\n        outputs = []\n        self.predictions_to_human(predictions, outputs, data, use_pos, conll=conll)\n        return outputs\n\n    def predictions_to_human(self, predictions, outputs, data, use_pos, conll=True):\n        if conll:\n            for d, (arcs, rels) in zip(data, predictions):\n                sent = CoNLLSentence()\n                for idx, (cell, a, r) in enumerate(zip(d, arcs, rels)):\n                    if use_pos:\n                        token, pos = cell\n                    else:\n                        token, pos = cell, None\n                    sent.append(CoNLLWord(idx + 1, token, cpos=pos, head=a, deprel=self.vocabs['rel'][r]))\n                outputs.append(sent)\n        else:\n            for d, (arcs, rels) in zip(data, predictions):\n                sent = []\n                for idx, (a, r) in enumerate(zip(arcs, rels)):\n                    sent.append((a, self.vocabs['rel'][r]))\n                outputs.append(sent)\n\n    def collect_outputs(self, arc_scores, rel_scores, mask, batch, predictions, order, data, use_pos,\n                        build_data):\n        lens = [len(token) - 1 for token in batch['token']]\n        arc_preds, rel_preds = self.decode(arc_scores, rel_scores, mask, batch)\n        self.collect_outputs_extend(predictions, arc_preds, rel_preds, lens, mask)\n        order.extend(batch[IDX])\n        if build_data:\n            if use_pos:\n                data.extend(zip(batch['FORM'], batch['CPOS']))\n            else:\n                data.extend(batch['FORM'])\n\n    def collect_outputs_extend(self, predictions: list, arc_preds, rel_preds, lens, mask):\n        predictions.extend(zip([seq.tolist() for seq in arc_preds[mask].split(lens)],\n                               [seq.tolist() for seq in rel_preds[mask].split(lens)]))\n\n    @property\n    def use_pos(self):\n        return self.config.get('feat', None) == 'pos'\n\n    def fit(self, trn_data, dev_data, save_dir,\n            feat=None,\n            n_embed=100,\n            pretrained_embed=None,\n            transformer=None,\n            average_subwords=False,\n            word_dropout=0.2,\n            transformer_hidden_dropout=None,\n            layer_dropout=0,\n            scalar_mix: int = None,\n            embed_dropout=.33,\n            n_lstm_hidden=400,\n            n_lstm_layers=3,\n            hidden_dropout=.33,\n            n_mlp_arc=500,\n            n_mlp_rel=100,\n            mlp_dropout=.33,\n            lr=2e-3,\n            transformer_lr=5e-5,\n            mu=.9,\n            nu=.9,\n            epsilon=1e-12,\n            grad_norm=5.0,\n            decay=.75,\n            decay_steps=5000,\n            weight_decay=0,\n            warmup_steps=0.1,\n            separate_optimizer=False,\n            patience=100,\n            lowercase=False,\n            epochs=50000,\n            tree=False,\n            proj=False,\n            punct=False,\n            min_freq=2,\n            logger=None,\n            verbose=True,\n            unk=UNK,\n            max_sequence_length=512,\n            batch_size=None,\n            sampler_builder=None,\n            gradient_accumulation=1,\n            devices: Union[float, int, List[int]] = None,\n            transform=None,\n            secondary_encoder=None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def execute_training_loop(self, trn, dev, devices, epochs, logger, patience, save_dir, optimizer,\n                              gradient_accumulation, **kwargs):\n        optimizer, scheduler, transformer_optimizer, transformer_scheduler = optimizer\n        criterion = self.build_criterion()\n        best_e, best_metric = 0, self.build_metric()\n        timer = CountdownTimer(epochs)\n        history = History()\n        ratio_width = len(f'{len(trn) // gradient_accumulation}/{len(trn) // gradient_accumulation}')\n        for epoch in range(1, epochs + 1):\n            # train one epoch and update the parameters\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, optimizer, scheduler, criterion, epoch, logger, history,\n                                transformer_optimizer, transformer_scheduler,\n                                gradient_accumulation=gradient_accumulation)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, ratio_width=ratio_width, logger=logger)\n            timer.update()\n            # logger.info(f\"{'Dev' + ' ' * ratio_width} loss: {loss:.4f} {dev_metric}\")\n            # save the model if it is the best so far\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if dev_metric > best_metric:\n                best_e, best_metric = epoch, dev_metric\n                self.save_weights(save_dir)\n                report += ' ([red]saved[/red])'\n            else:\n                if patience != epochs:\n                    report += f' ({epoch - best_e}/{patience})'\n                else:\n                    report += f' ({epoch - best_e})'\n            logger.info(report)\n            if patience is not None and epoch - best_e >= patience:\n                logger.info(f'LAS has stopped improving for {patience} epochs, early stop.')\n                break\n        timer.stop()\n        if not best_e:\n            self.save_weights(save_dir)\n        elif best_e != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric.score:.2%} at epoch {best_e}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n\n    def build_optimizer(self, epochs, trn, gradient_accumulation, **kwargs):\n        config = self.config\n        model = self.model\n        if isinstance(model, nn.DataParallel):\n            model = model.module\n        if self.config.transformer:\n            transformer = model.encoder.transformer\n            optimizer = Adam(set(model.parameters()) - set(transformer.parameters()),\n                             config.lr,\n                             (config.mu, config.nu),\n                             config.epsilon)\n            if self.config.transformer_lr:\n                num_training_steps = len(trn) * epochs // gradient_accumulation\n                if self.config.separate_optimizer:\n                    transformer_optimizer, transformer_scheduler = \\\n                        build_optimizer_scheduler_with_transformer(transformer,\n                                                                   transformer,\n                                                                   config.transformer_lr,\n                                                                   config.transformer_lr,\n                                                                   num_training_steps,\n                                                                   config.warmup_steps,\n                                                                   config.weight_decay,\n                                                                   adam_epsilon=1e-8)\n                else:\n                    optimizer, scheduler = build_optimizer_scheduler_with_transformer(model,\n                                                                                      transformer,\n                                                                                      config.lr,\n                                                                                      config.transformer_lr,\n                                                                                      num_training_steps,\n                                                                                      config.warmup_steps,\n                                                                                      config.weight_decay,\n                                                                                      adam_epsilon=1e-8)\n                    transformer_optimizer, transformer_scheduler = None, None\n            else:\n                transformer.requires_grad_(False)\n                transformer_optimizer, transformer_scheduler = None, None\n        else:\n            optimizer = Adam(model.parameters(),\n                             config.lr,\n                             (config.mu, config.nu),\n                             config.epsilon)\n            transformer_optimizer, transformer_scheduler = None, None\n        if self.config.separate_optimizer:\n            scheduler = ExponentialLR(optimizer, config.decay ** (1 / config.decay_steps))\n        # noinspection PyUnboundLocalVariable\n        return optimizer, scheduler, transformer_optimizer, transformer_scheduler\n\n    def build_transformer_tokenizer(self):\n        transformer = self.config.transformer\n        if transformer:\n            transformer_tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(transformer, use_fast=True)\n        else:\n            transformer_tokenizer = None\n        self.transformer_tokenizer = transformer_tokenizer\n        return transformer_tokenizer\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self,\n                         data,\n                         shuffle,\n                         device,\n                         training=False,\n                         logger=None,\n                         gradient_accumulation=1,\n                         sampler_builder=None,\n                         batch_size=None,\n                         **kwargs) -> DataLoader:\n        dataset = self.build_dataset(data)\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger, self.config.transformer)\n        transformer_tokenizer = self.transformer_tokenizer\n        if transformer_tokenizer:\n            dataset.transform.append(self.build_tokenizer_transform())\n        dataset.append_transform(FieldLength('token', 'sent_length'))\n        if isinstance(data, str):\n            dataset.purge_cache()\n        if len(dataset) > 1000 and isinstance(data, str):\n            timer = CountdownTimer(len(dataset))\n            self.cache_dataset(dataset, timer, training, logger)\n        if self.config.transformer:\n            lens = [len(sample['input_ids']) for sample in dataset]\n        else:\n            lens = [sample['sent_length'] for sample in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, gradient_accumulation)\n        else:\n            sampler = None\n        loader = PadSequenceDataLoader(dataset=dataset,\n                                       batch_sampler=sampler,\n                                       batch_size=batch_size,\n                                       pad=self.get_pad_dict(),\n                                       device=device,\n                                       vocabs=self.vocabs)\n        return loader\n\n    def cache_dataset(self, dataset, timer, training=False, logger=None):\n        for each in dataset:\n            timer.log('Preprocessing and caching samples [blink][yellow]...[/yellow][/blink]')\n\n    def get_pad_dict(self):\n        return {'arc': 0}\n\n    def build_dataset(self, data, bos_transform=None):\n        if not bos_transform:\n            bos_transform = append_bos\n        transform = [bos_transform]\n        if self.config.get('transform', None):\n            transform.append(self.config.transform)\n        if self.config.get('lowercase', False):\n            transform.append(LowerCase('token'))\n        transform.append(self.vocabs)\n        if not self.config.punct:\n            transform.append(PunctuationMask('token', 'punct_mask'))\n        return CoNLLParsingDataset(data, transform=transform)\n\n    def build_tokenizer_transform(self):\n        return TransformerSequenceTokenizer(self.transformer_tokenizer, 'token', '',\n                                            ret_token_span=True, cls_is_bos=True,\n                                            max_seq_length=self.config.get('max_sequence_length',\n                                                                           512),\n                                            truncate_long_sequences=False)\n\n    def build_vocabs(self, dataset, logger=None, transformer=None):\n        rel_vocab = self.vocabs.get('rel', None)\n        if rel_vocab is None:\n            rel_vocab = Vocab(unk_token=None, pad_token=self.config.get('pad_rel', None))\n            self.vocabs.put(rel=rel_vocab)\n        if self.config.get('feat', None) == 'pos' or self.config.get('use_pos', False):\n            self.vocabs['pos'] = Vocab(unk_token=None, pad_token=None)\n\n        timer = CountdownTimer(len(dataset))\n        if transformer:\n            token_vocab = None\n        else:\n            token_vocab = Vocab()\n            self.vocabs.token = token_vocab\n            unk = self.config.get('unk', None)\n            if unk is not None:\n                token_vocab.unk_token = unk\n        if token_vocab and self.config.get('min_freq', None):\n            counter = Counter()\n            for sample in dataset:\n                for form in sample['token']:\n                    counter[form] += 1\n            reserved_token = [token_vocab.pad_token, token_vocab.unk_token]\n            if ROOT in token_vocab:\n                reserved_token.append(ROOT)\n            freq_words = reserved_token + [token for token, freq in counter.items() if\n                                           freq >= self.config.min_freq]\n            token_vocab.token_to_idx.clear()\n            for word in freq_words:\n                token_vocab(word)\n        else:\n            for i, sample in enumerate(dataset):\n                timer.log('vocab building [blink][yellow]...[/yellow][/blink]', ratio_percentage=True)\n        rel_vocab.set_unk_as_safe_unk()  # Some relation in dev set is OOV\n        self.vocabs.lock()\n        self.vocabs.summary(logger=logger)\n        if token_vocab:\n            self.config.n_words = len(self.vocabs['token'])\n        if 'pos' in self.vocabs:\n            self.config.n_feats = len(self.vocabs['pos'])\n            self.vocabs['pos'].set_unk_as_safe_unk()\n        self.config.n_rels = len(self.vocabs['rel'])\n        if token_vocab:\n            self.config.pad_index = self.vocabs['token'].pad_idx\n            self.config.unk_index = self.vocabs['token'].unk_idx\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        pretrained_embed, transformer = self.build_embeddings(training=training)\n        if pretrained_embed is not None:\n            self.config.n_embed = pretrained_embed.size(-1)\n        model = self.create_model(pretrained_embed, transformer)\n        return model\n\n    def create_model(self, pretrained_embed, transformer):\n        return BiaffineDependencyModel(self.config,\n                                       pretrained_embed,\n                                       transformer,\n                                       self.transformer_tokenizer)\n\n    def build_embeddings(self, training=True):\n        pretrained_embed = None\n        if self.config.get('pretrained_embed', None):\n            pretrained_embed = index_word2vec_with_vocab(self.config.pretrained_embed, self.vocabs['token'],\n                                                         init='zeros', normalize=True)\n        transformer = self.config.transformer\n        if transformer:\n            transformer = AutoModel_.from_pretrained(transformer, training=training)\n        return pretrained_embed, transformer\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn,\n                       optimizer,\n                       scheduler,\n                       criterion,\n                       epoch,\n                       logger,\n                       history: History,\n                       transformer_optimizer=None,\n                       transformer_scheduler=None,\n                       gradient_accumulation=1,\n                       **kwargs):\n        self.model.train()\n\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation))\n        metric = self.build_metric(training=True)\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            arc_scores, rel_scores, mask, puncts = self.feed_batch(batch)\n            arcs, rels = batch['arc'], batch['rel_id']\n            loss = self.compute_loss(arc_scores, rel_scores, arcs, rels, mask, criterion, batch)\n            if gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            arc_preds, rel_preds = self.decode(arc_scores, rel_scores, mask, batch)\n            self.update_metric(arc_preds, rel_preds, arcs, rels, mask, puncts, metric, batch)\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler, transformer_optimizer, transformer_scheduler)\n                report = self._report(total_loss / (timer.current + 1), metric)\n                timer.log(report, ratio_percentage=False, logger=logger)\n            del loss\n\n    def _step(self, optimizer, scheduler, transformer_optimizer, transformer_scheduler):\n        if self.config.get('grad_norm', None):\n            nn.utils.clip_grad_norm_(self.model.parameters(),\n                                     self.config.grad_norm)\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step()\n        if self.config.transformer and self.config.transformer_lr and transformer_optimizer:\n            transformer_optimizer.step()\n            transformer_optimizer.zero_grad()\n            transformer_scheduler.step()\n\n    def feed_batch(self, batch):\n        words, feats, lens, puncts = batch.get('token_id', None), batch.get('pos_id', None), batch['sent_length'], \\\n                                     batch.get('punct_mask', None)\n        mask = lengths_to_mask(lens)\n        arc_scores, rel_scores = self.model(words=words, feats=feats, mask=mask, batch=batch, **batch)\n        # ignore the first token of each sentence\n        # RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n        if self.model.training:\n            mask = mask.clone()\n        mask[:, 0] = 0\n        return arc_scores, rel_scores, mask, puncts\n\n    def _report(self, loss, metric: AttachmentScore):\n        return f'loss: {loss:.4f} {metric}'\n\n    def compute_loss(self, arc_scores, rel_scores, arcs, rels, mask, criterion, batch=None):\n        arc_scores, arcs = arc_scores[mask], arcs[mask]\n        rel_scores, rels = rel_scores[mask], rels[mask]\n        rel_scores = rel_scores[torch.arange(len(arcs)), arcs]\n        arc_loss = criterion(arc_scores, arcs)\n        rel_loss = criterion(rel_scores, rels)\n        loss = arc_loss + rel_loss\n\n        return loss\n\n    # noinspection PyUnboundLocalVariable\n    @torch.no_grad()\n    def evaluate_dataloader(self, loader: PadSequenceDataLoader, criterion, logger=None, filename=None, output=False,\n                            ratio_width=None,\n                            metric=None,\n                            **kwargs):\n        self.model.eval()\n\n        loss = 0\n        if not metric:\n            metric = self.build_metric()\n        if output:\n            fp = open(output, 'w')\n            predictions, build_data, data, order = self.before_outputs(None)\n\n        timer = CountdownTimer(len(loader))\n        use_pos = self.use_pos\n        for batch in loader:\n            arc_scores, rel_scores, mask, puncts = self.feed_batch(batch)\n            if output:\n                self.collect_outputs(arc_scores, rel_scores, mask, batch, predictions, order, data, use_pos,\n                                     build_data)\n            arcs, rels = batch['arc'], batch['rel_id']\n            loss += self.compute_loss(arc_scores, rel_scores, arcs, rels, mask, criterion, batch).item()\n            arc_preds, rel_preds = self.decode(arc_scores, rel_scores, mask, batch)\n            self.update_metric(arc_preds, rel_preds, arcs, rels, mask, puncts, metric, batch)\n            report = self._report(loss / (timer.current + 1), metric)\n            if filename:\n                report = f'{os.path.basename(filename)} ' + report\n            timer.log(report, ratio_percentage=False, logger=logger, ratio_width=ratio_width)\n        loss /= len(loader)\n        if output:\n            outputs = self.post_outputs(predictions, data, order, use_pos, build_data)\n            for each in outputs:\n                fp.write(f'{each}\\n\\n')\n            fp.close()\n            logger.info(f'Predictions saved in [underline][yellow]{output}[/yellow][/underline]')\n\n        return loss, metric\n\n    def update_metric(self, arc_preds, rel_preds, arcs, rels, mask, puncts, metric, batch=None):\n        # ignore all punctuation if not specified\n        if not self.config.punct:\n            mask &= puncts\n        metric(arc_preds, rel_preds, arcs, rels, mask)\n\n    def decode(self, arc_scores, rel_scores, mask, batch=None):\n        tree, proj = self.config.tree, self.config.get('proj', False)\n        if tree:\n            arc_preds = decode_dep(arc_scores, mask, tree, proj)\n        else:\n            arc_preds = arc_scores.argmax(-1)\n        rel_preds = rel_scores.argmax(-1)\n        rel_preds = rel_preds.gather(-1, arc_preds.unsqueeze(-1)).squeeze(-1)\n\n        return arc_preds, rel_preds\n\n    def build_criterion(self, **kwargs):\n        criterion = nn.CrossEntropyLoss()\n        return criterion\n\n    def build_metric(self, **kwargs):\n        return AttachmentScore()\n\n    def on_config_ready(self, **kwargs):\n        self.build_transformer_tokenizer()  # We have to build tokenizer before building the dataloader and model\n        self.config.patience = min(self.config.patience, self.config.epochs)\n\n    def prediction_to_head_rel(self, arcs: torch.LongTensor, rels: torch.LongTensor, batch: dict):\n        arcs = arcs[:, 1:]  # Skip the ROOT\n        rels = rels[:, 1:]\n        arcs = arcs.tolist()\n        rels = rels.tolist()\n        vocab = self.vocabs['rel'].idx_to_token\n        for arcs_per_sent, rels_per_sent, tokens in zip(arcs, rels, batch['token']):\n            tokens = tokens[1:]\n            sent_len = len(tokens)\n            result = list(zip(arcs_per_sent[:sent_len], [vocab[r] for r in rels_per_sent[:sent_len]]))\n            yield result\n", "hanlp/components/parsers/biaffine/variationalbilstm.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.rnn import apply_permutation\nfrom torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n\nfrom hanlp.common.structure import ConfigTracker\nfrom hanlp.layers.dropout import SharedDropout\n\n\nclass VariationalLSTM(nn.Module):\n    r\"\"\"\n    LSTM is an variant of the vanilla bidirectional LSTM adopted by Biaffine Parser\n    with the only difference of the dropout strategy.\n    It drops nodes in the LSTM layers (input and recurrent connections)\n    and applies the same dropout mask at every recurrent timesteps.\n\n    APIs are roughly the same as :class:`~torch.nn.LSTM` except that we only allows\n    :class:`~torch.nn.utils.rnn.PackedSequence` as input.\n\n    References:\n        - Timothy Dozat and Christopher D. Manning. 2017.\n          `Deep Biaffine Attention for Neural Dependency Parsing`_.\n\n    Args:\n        input_size (int):\n            The number of expected features in the input.\n        hidden_size (int):\n            The number of features in the hidden state `h`.\n        num_layers (int):\n            The number of recurrent layers. Default: 1.\n        bidirectional (bool):\n            If ``True``, becomes a bidirectional LSTM. Default: ``False``\n        dropout (float):\n            If non-zero, introduces a :class:`SharedDropout` layer on the outputs of each LSTM layer except the last layer.\n            Default: 0.\n\n    .. _Deep Biaffine Attention for Neural Dependency Parsing:\n        https://openreview.net/forum?id=Hk95PK9le\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, dropout=0):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n        self.num_directions = 1 + self.bidirectional\n\n        self.f_cells = nn.ModuleList()\n        if bidirectional:\n            self.b_cells = nn.ModuleList()\n        for _ in range(self.num_layers):\n            self.f_cells.append(nn.LSTMCell(input_size=input_size, hidden_size=hidden_size))\n            if bidirectional:\n                self.b_cells.append(nn.LSTMCell(input_size=input_size, hidden_size=hidden_size))\n            input_size = hidden_size * self.num_directions\n\n        self.reset_parameters()\n\n    def __repr__(self):\n        s = f\"{self.input_size}, {self.hidden_size}\"\n        if self.num_layers > 1:\n            s += f\", num_layers={self.num_layers}\"\n        if self.bidirectional:\n            s += f\", bidirectional={self.bidirectional}\"\n        if self.dropout > 0:\n            s += f\", dropout={self.dropout}\"\n\n        return f\"{self.__class__.__name__}({s})\"\n\n    def reset_parameters(self):\n        for param in self.parameters():\n            # apply orthogonal_ to weight\n            if len(param.shape) > 1:\n                nn.init.orthogonal_(param)\n            # apply zeros_ to bias\n            else:\n                nn.init.zeros_(param)\n\n    def permute_hidden(self, hx, permutation):\n        if permutation is None:\n            return hx\n        h = apply_permutation(hx[0], permutation)\n        c = apply_permutation(hx[1], permutation)\n\n        return h, c\n\n    def layer_forward(self, x, hx, cell, batch_sizes, reverse=False):\n        hx_0 = hx_i = hx\n        hx_n, output = [], []\n        steps = reversed(range(len(x))) if reverse else range(len(x))\n        if self.training:\n            hid_mask = SharedDropout.get_mask(hx_0[0], self.dropout)\n\n        for t in steps:\n            last_batch_size, batch_size = len(hx_i[0]), batch_sizes[t]\n            if last_batch_size < batch_size:\n                hx_i = [torch.cat((h, ih[last_batch_size:batch_size])) for h, ih in zip(hx_i, hx_0)]\n            else:\n                hx_n.append([h[batch_size:] for h in hx_i])\n                hx_i = [h[:batch_size] for h in hx_i]\n            hx_i = [h for h in cell(x[t], hx_i)]\n            output.append(hx_i[0])\n            if self.training:\n                hx_i[0] = hx_i[0] * hid_mask[:batch_size]\n        if reverse:\n            hx_n = hx_i\n            output.reverse()\n        else:\n            hx_n.append(hx_i)\n            hx_n = [torch.cat(h) for h in zip(*reversed(hx_n))]\n        output = torch.cat(output)\n\n        return output, hx_n\n\n    def forward(self, sequence, hx=None):\n        r\"\"\"\n        Args:\n            sequence (~torch.nn.utils.rnn.PackedSequence):\n                A packed variable length sequence.\n            hx (~torch.Tensor, ~torch.Tensor):\n                A tuple composed of two tensors `h` and `c`.\n                `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial hidden state\n                for each element in the batch.\n                `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial cell state\n                for each element in the batch.\n                If `hx` is not provided, both `h` and `c` default to zero.\n                Default: ``None``.\n\n        Returns:\n            ~torch.nn.utils.rnn.PackedSequence, (~torch.Tensor, ~torch.Tensor):\n                The first is a packed variable length sequence.\n                The second is a tuple of tensors `h` and `c`.\n                `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the hidden state for `t=seq_len`.\n                Like output, the layers can be separated using ``h.view(num_layers, num_directions, batch_size, hidden_size)``\n                and similarly for c.\n                `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the cell state for `t=seq_len`.\n        \"\"\"\n        x, batch_sizes = sequence.data, sequence.batch_sizes.tolist()\n        batch_size = batch_sizes[0]\n        h_n, c_n = [], []\n\n        if hx is None:\n            ih = x.new_zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n            h, c = ih, ih\n        else:\n            h, c = self.permute_hidden(hx, sequence.sorted_indices)\n        h = h.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)\n        c = c.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)\n\n        for i in range(self.num_layers):\n            x = torch.split(x, batch_sizes)\n            if self.training:\n                mask = SharedDropout.get_mask(x[0], self.dropout)\n                x = [i * mask[:len(i)] for i in x]\n            x_i, (h_i, c_i) = self.layer_forward(x=x,\n                                                 hx=(h[i, 0], c[i, 0]),\n                                                 cell=self.f_cells[i],\n                                                 batch_sizes=batch_sizes)\n            if self.bidirectional:\n                x_b, (h_b, c_b) = self.layer_forward(x=x,\n                                                     hx=(h[i, 1], c[i, 1]),\n                                                     cell=self.b_cells[i],\n                                                     batch_sizes=batch_sizes,\n                                                     reverse=True)\n                x_i = torch.cat((x_i, x_b), -1)\n                h_i = torch.stack((h_i, h_b))\n                c_i = torch.stack((c_i, c_b))\n            x = x_i\n            h_n.append(h_i)\n            c_n.append(h_i)\n\n        x = PackedSequence(x,\n                           sequence.batch_sizes,\n                           sequence.sorted_indices,\n                           sequence.unsorted_indices)\n        hx = torch.cat(h_n, 0), torch.cat(c_n, 0)\n        hx = self.permute_hidden(hx, sequence.unsorted_indices)\n\n        return x, hx\n\n\nclass VariationalLSTMEncoder(VariationalLSTM, ConfigTracker):\n    def __init__(self,\n                 input_size,\n                 hidden_size,\n                 num_layers=1,\n                 bidirectional=False,\n                 variational_dropout=0,\n                 word_dropout=0,\n                 ):\n        super().__init__(input_size, hidden_size, num_layers, bidirectional, variational_dropout)\n        ConfigTracker.__init__(self, locals())\n        self.lstm_dropout = SharedDropout(p=word_dropout)\n\n    # noinspection PyMethodOverriding\n    def forward(self, embed, mask):\n        batch_size, seq_len = mask.shape\n        x = pack_padded_sequence(embed, mask.sum(1), True, False)\n        x, _ = super().forward(x)\n        x, _ = pad_packed_sequence(x, True, total_length=seq_len)\n        x = self.lstm_dropout(x)\n        return x\n\n    def get_output_dim(self):\n        return self.hidden_size * self.num_directions\n", "hanlp/components/parsers/biaffine/biaffine_2nd_dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-06 13:57\nimport functools\nfrom typing import Union, List, Any\n\nimport torch\nfrom hanlp_common.constant import UNK\nfrom hanlp.common.transform import TransformList\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.parsers.biaffine.biaffine import Biaffine\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDecoder, \\\n    EncoderWithContextualLayer\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp.components.parsers.biaffine.biaffine_sdp import BiaffineSemanticDependencyParser\nfrom hanlp_common.conll import CoNLLUWord, CoNLLSentence\nfrom hanlp.components.parsers.parse_alg import add_secondary_arcs_by_preds\nfrom hanlp.datasets.parsing.loaders.conll_dataset import append_bos\nfrom hanlp.datasets.parsing.semeval15 import unpack_deps_to_head_deprel, merge_head_deprel_with_2nd\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\n\n\nclass BiaffineSeparateDecoder(torch.nn.Module):\n\n    def __init__(self, hidden_size, config) -> None:\n        super().__init__()\n        self.biaffine_decoder = BiaffineDecoder(hidden_size,\n                                                config.n_mlp_arc,\n                                                config.n_mlp_rel,\n                                                config.mlp_dropout,\n                                                config.n_rels)\n        self.biaffine_decoder_2nd = BiaffineDecoder(hidden_size,\n                                                    config.n_mlp_arc,\n                                                    config.n_mlp_rel,\n                                                    config.mlp_dropout,\n                                                    config.n_rels_2nd)\n\n    def forward(self, x, mask):\n        return tuple(zip(self.biaffine_decoder(x, mask), self.biaffine_decoder_2nd(x, mask)))\n\n\nclass BiaffineJointDecoder(BiaffineDecoder):\n    def __init__(self, hidden_size, config) -> None:\n        super().__init__(hidden_size, config.n_mlp_arc, config.n_mlp_rel, config.mlp_dropout, config.n_rels)\n        # the Biaffine layers for secondary dep\n        self.arc_attn_2nd = Biaffine(n_in=config.n_mlp_arc,\n                                     bias_x=True,\n                                     bias_y=False)\n        self.rel_attn_2nd = Biaffine(n_in=config.n_mlp_rel,\n                                     n_out=config.n_rels,\n                                     bias_x=True,\n                                     bias_y=True)\n\n    def forward(self, x, mask=None, **kwargs: Any):\n        arc_d, arc_h, rel_d, rel_h = self.apply_mlps(x)\n        s_arc, s_rel = self.decode(arc_d, arc_h, rel_d, rel_h, mask, self.arc_attn, self.rel_attn)\n        s_arc_2nd, s_rel_2nd = self.decode(arc_d, arc_h, rel_d, rel_h, mask, self.arc_attn_2nd, self.rel_attn_2nd)\n        return (s_arc, s_arc_2nd), (s_rel, s_rel_2nd)\n\n\nclass BiaffineSecondaryModel(torch.nn.Module):\n\n    def __init__(self, config, pretrained_embed: torch.Tensor = None, transformer: PreTrainedModel = None,\n                 transformer_tokenizer: PreTrainedTokenizer = None):\n        super().__init__()\n        self.encoder = EncoderWithContextualLayer(config, pretrained_embed, transformer, transformer_tokenizer)\n        self.decoder = BiaffineJointDecoder(self.encoder.hidden_size, config) if config.joint \\\n            else BiaffineSeparateDecoder(self.encoder.hidden_size, config)\n\n    def forward(self,\n                words=None,\n                feats=None,\n                input_ids=None,\n                token_span=None,\n                mask=None, lens=None, **kwargs):\n        x, mask = self.encoder(words, feats, input_ids, token_span, mask, lens)\n        return self.decoder(x, mask)\n\n\nclass BiaffineSecondaryParser(BiaffineDependencyParser):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.model: BiaffineSecondaryModel = None\n\n    def build_dataset(self, data, bos_transform=None):\n        transform = TransformList(functools.partial(append_bos, pos_key='UPOS'),\n                                  functools.partial(unpack_deps_to_head_deprel, pad_rel=self.config.pad_rel,\n                                                    arc_key='arc_2nd',\n                                                    rel_key='rel_2nd'))\n        if self.config.joint:\n            transform.append(merge_head_deprel_with_2nd)\n        if bos_transform:\n            transform.append(bos_transform)\n        return super().build_dataset(data, transform)\n\n    def build_criterion(self, **kwargs):\n        # noinspection PyCallByClass\n        return super().build_criterion(**kwargs), (BiaffineSemanticDependencyParser.build_criterion(self, **kwargs))\n\n    def fit(self, trn_data, dev_data, save_dir, feat=None, n_embed=100, pretrained_embed=None, transformer=None,\n            average_subwords=False, word_dropout: float = 0.2, transformer_hidden_dropout=None, layer_dropout=0,\n            scalar_mix: int = None, embed_dropout=.33, n_lstm_hidden=400, n_lstm_layers=3, hidden_dropout=.33,\n            n_mlp_arc=500, n_mlp_rel=100, mlp_dropout=.33, lr=2e-3, transformer_lr=5e-5, mu=.9, nu=.9, epsilon=1e-12,\n            clip=5.0, decay=.75, decay_steps=5000, patience=100, batch_size=None, sampler_builder=None,\n            lowercase=False, epochs=50000, tree=False, punct=False, min_freq=2,\n            apply_constraint=True, joint=False, no_cycle=False, root=None,\n            logger=None,\n            verbose=True, unk=UNK, pad_rel=None, max_sequence_length=512, devices: Union[float, int, List[int]] = None,\n            transform=None, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, dataset, logger=None, transformer=None):\n        self.vocabs['rel_2nd'] = rel_2nd = Vocab(pad_token=self.config.pad_rel, unk_token=self.config.pad_rel)\n        if self.config.joint:\n            self.vocabs['rel'] = rel_2nd\n        super().build_vocabs(dataset, logger, transformer)\n        self.config.n_rels_2nd = len(rel_2nd)\n\n    def create_model(self, pretrained_embed, transformer):\n        return BiaffineSecondaryModel(self.config, pretrained_embed, transformer, self.transformer_tokenizer)\n\n    def compute_loss(self, arc_scores, rel_scores, arcs, rels, mask, criterion, batch=None):\n        arc_scores_1st, arc_scores_2nd, rel_scores_1st, rel_scores_2nd = self.unpack_scores(arc_scores, rel_scores)\n        loss_1st = super().compute_loss(arc_scores_1st, rel_scores_1st, arcs, rels, mask, criterion[0], batch)\n        mask = self.compute_mask(arc_scores_2nd, batch, mask)\n        # noinspection PyCallByClass\n        loss_2st = BiaffineSemanticDependencyParser.compute_loss(self, arc_scores_2nd, rel_scores_2nd,\n                                                                 batch['arc_2nd'], batch['rel_2nd_id'], mask,\n                                                                 criterion[1], batch)\n        return loss_1st + loss_2st\n\n    @staticmethod\n    def compute_mask(arc_scores_2nd, batch, mask_1st):\n        mask = batch.get('mask_2nd', None)\n        if mask is None:\n            batch['mask_2nd'] = mask = BiaffineSemanticDependencyParser.convert_to_3d_mask(arc_scores_2nd, mask_1st)\n        return mask\n\n    def unpack_scores(self, arc_scores, rel_scores):\n        arc_scores_1st, arc_scores_2nd = arc_scores\n        rel_scores_1st, rel_scores_2nd = rel_scores\n        return arc_scores_1st, arc_scores_2nd, rel_scores_1st, rel_scores_2nd\n\n    def get_pad_dict(self):\n        d = super(BiaffineSecondaryParser, self).get_pad_dict()\n        d.update({'arc_2nd': False})\n        return d\n\n    def decode(self, arc_scores, rel_scores, mask, batch=None, predicting=None):\n        output_1st, output_2nd = batch.get('outputs', (None, None))\n        if output_1st is None:\n            arc_scores_1st, arc_scores_2nd, rel_scores_1st, rel_scores_2nd = self.unpack_scores(arc_scores, rel_scores)\n            output_1st = super().decode(arc_scores_1st, rel_scores_1st, mask)\n            mask = self.compute_mask(arc_scores_2nd, batch, mask)\n            # noinspection PyCallByClass\n            output_2nd = BiaffineSemanticDependencyParser.decode(self, arc_scores_2nd, rel_scores_2nd, mask, batch)\n            if self.config.get('no_cycle'):\n                assert predicting, 'No cycle constraint for evaluation is not implemented yet. If you are ' \\\n                                   'interested, welcome to submit a pull request.'\n                root_rel_idx = self.vocabs['rel'].token_to_idx.get(self.config.get('root', None), None)\n                arc_pred_1st, rel_pred_1st, arc_pred_2nd, rel_pred_2nd = *output_1st, *output_2nd\n                arc_scores_2nd = arc_scores_2nd.transpose(1, 2).cpu().detach().numpy()\n                arc_pred_2nd = arc_pred_2nd.cpu().detach().numpy()\n                rel_pred_2nd = rel_pred_2nd.cpu().detach().numpy()\n                trees = arc_pred_1st.cpu().detach().numpy()\n                graphs = []\n                for i, (arc_scores, arc_preds, rel_preds, tree, tokens) in enumerate(\n                        zip(arc_scores_2nd, arc_pred_2nd, rel_pred_2nd, trees, batch['token'])):\n                    sent_len = len(tokens)\n                    graph = add_secondary_arcs_by_preds(arc_scores, arc_preds[:sent_len, :sent_len], rel_preds,\n                                                        tree[:sent_len], root_rel_idx)\n                    graphs.append(graph[1:])  # Remove root\n                    # if not predicting:\n                    #     # Write back to torch Tensor\n                    #     for d, hr in zip(graph):\n                    #         pass\n                output_2nd = None, graphs\n\n        return tuple(zip(output_1st, output_2nd))\n\n    def update_metric(self, arc_preds, rel_preds, arcs, rels, mask, puncts, metric, batch=None):\n        super().update_metric(arc_preds[0], rel_preds[0], arcs, rels, mask, puncts, metric['1st'], batch)\n        puncts = BiaffineSemanticDependencyParser.convert_to_3d_puncts(puncts, batch['mask_2nd'])\n        # noinspection PyCallByClass\n        BiaffineSemanticDependencyParser.update_metric(self, arc_preds[1], rel_preds[1], batch['arc_2nd'],\n                                                       batch['rel_2nd_id'], batch['mask_2nd'], puncts, metric['2nd'],\n                                                       batch)\n\n    def build_metric(self, **kwargs):\n        # noinspection PyCallByClass\n        return MetricDict({'1st': super().build_metric(**kwargs),\n                           '2nd': BiaffineSemanticDependencyParser.build_metric(self, **kwargs)})\n\n    def collect_outputs_extend(self, predictions: list, arc_preds, rel_preds, lens, mask):\n        predictions.extend(rel_preds[1])\n\n    def predictions_to_human(self, predictions, outputs, data, use_pos, conll=True):\n        rel_vocab = self.vocabs['rel'].idx_to_token\n        for d, graph in zip(data, predictions):\n            sent = CoNLLSentence()\n            for idx, (cell, hrs) in enumerate(zip(d, graph)):\n                if use_pos:\n                    token, pos = cell\n                else:\n                    token, pos = cell, None\n                head = hrs[0][0]\n                deprel = rel_vocab[hrs[0][1]]\n                deps = [(h, rel_vocab[r]) for h, r in hrs[1:]]\n                sent.append(CoNLLUWord(idx + 1, token, upos=pos, head=head, deprel=deprel, deps=deps))\n            outputs.append(sent)\n", "hanlp/components/parsers/biaffine/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 20:43\n", "hanlp/components/parsers/biaffine/biaffine.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport torch\nimport torch.nn as nn\n\n\nclass Biaffine(nn.Module):\n    r\"\"\"\n    Biaffine layer for first-order scoring.\n\n    This function has a tensor of weights :math:`W` and bias terms if needed.\n    The score :math:`s(x, y)` of the vector pair :math:`(x, y)` is computed as :math:`x^T W y`,\n    in which :math:`x` and :math:`y` can be concatenated with bias terms.\n\n    References:\n        - Timothy Dozat and Christopher D. Manning. 2017.\n          `Deep Biaffine Attention for Neural Dependency Parsing`_.\n\n    Args:\n        n_in (int):\n            The size of the input feature.\n        n_out (int):\n            The number of output channels.\n        bias_x (bool):\n            If ``True``, adds a bias term for tensor :math:`x`. Default: ``True``.\n        bias_y (bool):\n            If ``True``, adds a bias term for tensor :math:`y`. Default: ``True``.\n\n    .. _Deep Biaffine Attention for Neural Dependency Parsing:\n        https://openreview.net/forum?id=Hk95PK9le\n    \"\"\"\n\n    def __init__(self, n_in, n_out=1, bias_x=True, bias_y=True):\n        super().__init__()\n\n        self.n_in = n_in\n        self.n_out = n_out\n        self.bias_x = bias_x\n        self.bias_y = bias_y\n        self.weight = nn.Parameter(torch.Tensor(n_out, n_in + bias_x, n_in + bias_y))\n\n        self.reset_parameters()\n\n    def __repr__(self):\n        s = f\"n_in={self.n_in}, n_out={self.n_out}\"\n        if self.bias_x:\n            s += f\", bias_x={self.bias_x}\"\n        if self.bias_y:\n            s += f\", bias_y={self.bias_y}\"\n\n        return f\"{self.__class__.__name__}({s})\"\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.weight)\n\n    def forward(self, x, y):\n        r\"\"\"\n        Args:\n            x (torch.Tensor): ``[batch_size, seq_len, n_in]``.\n            y (torch.Tensor): ``[batch_size, seq_len, n_in]``.\n\n        Returns:\n            ~torch.Tensor:\n                A scoring tensor of shape ``[batch_size, n_out, seq_len, seq_len]``.\n                If ``n_out=1``, the dimension for ``n_out`` will be squeezed automatically.\n        \"\"\"\n\n        if self.bias_x:\n            x = torch.cat((x, torch.ones_like(x[..., :1])), -1)\n        if self.bias_y:\n            y = torch.cat((y, torch.ones_like(y[..., :1])), -1)\n        # [batch_size, n_out, seq_len, seq_len]\n        s = torch.einsum('bxi,oij,byj->boxy', x, self.weight, y)\n        # remove dim 1 if n_out == 1\n        s = s.squeeze(1)\n\n        return s\n", "hanlp/components/distillation/losses.py": "# Adopted from https://github.com/airaria/TextBrewer\n# Apache License Version 2.0\n\nimport torch\nimport torch.nn.functional as F\n\nfrom hanlp_common.configurable import AutoConfigurable\n\n\ndef kd_mse_loss(logits_S, logits_T, temperature=1):\n    '''\n    Calculate the mse loss between logits_S and logits_T\n\n    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n    '''\n    if isinstance(temperature, torch.Tensor) and temperature.dim() > 0:\n        temperature = temperature.unsqueeze(-1)\n    beta_logits_T = logits_T / temperature\n    beta_logits_S = logits_S / temperature\n    loss = F.mse_loss(beta_logits_S, beta_logits_T)\n    return loss\n\n\ndef kd_ce_loss(logits_S, logits_T, temperature=1):\n    '''\n    Calculate the cross entropy between logits_S and logits_T\n\n    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n    '''\n    if isinstance(temperature, torch.Tensor) and temperature.dim() > 0:\n        temperature = temperature.unsqueeze(-1)\n    beta_logits_T = logits_T / temperature\n    beta_logits_S = logits_S / temperature\n    p_T = F.softmax(beta_logits_T, dim=-1)\n    loss = -(p_T * F.log_softmax(beta_logits_S, dim=-1)).sum(dim=-1).mean()\n    return loss\n\n\ndef att_mse_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the mse loss between `attention_S` and `attention_T`.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor mask: tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is None:\n        attention_S_select = torch.where(attention_S <= -1e-3, torch.zeros_like(attention_S), attention_S)\n        attention_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), attention_T)\n        loss = F.mse_loss(attention_S_select, attention_T_select)\n    else:\n        mask = mask.to(attention_S).unsqueeze(1).expand(-1, attention_S.size(1), -1)  # (bs, num_of_heads, len)\n        valid_count = torch.pow(mask.sum(dim=2), 2).sum()\n        loss = (F.mse_loss(attention_S, attention_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(\n            2)).sum() / valid_count\n    return loss\n\n\ndef att_mse_sum_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the mse loss between `attention_S` and `attention_T`. \n    * If the the shape is (*batch_size*, *num_heads*, *length*, *length*), sums along the `num_heads` dimension and then calcuates the mse loss between the two matrices.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    if len(attention_S.size()) == 4:\n        attention_T = attention_T.sum(dim=1)\n        attention_S = attention_S.sum(dim=1)\n    if mask is None:\n        attention_S_select = torch.where(attention_S <= -1e-3, torch.zeros_like(attention_S), attention_S)\n        attention_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), attention_T)\n        loss = F.mse_loss(attention_S_select, attention_T_select)\n    else:\n        mask = mask.to(attention_S)\n        valid_count = torch.pow(mask.sum(dim=1), 2).sum()\n        loss = (F.mse_loss(attention_S, attention_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(\n            1)).sum() / valid_count\n    return loss\n\n\ndef att_ce_loss(attention_S, attention_T, mask=None):\n    '''\n\n    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    \n    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)\n    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    probs_T = F.softmax(attention_T, dim=-1)\n    if mask is None:\n        probs_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), probs_T)\n        loss = -((probs_T_select * F.log_softmax(attention_S, dim=-1)).sum(dim=-1)).mean()\n    else:\n        mask = mask.to(attention_S).unsqueeze(1).expand(-1, attention_S.size(1), -1)  # (bs, num_of_heads, len)\n        loss = -((probs_T * F.log_softmax(attention_S, dim=-1) * mask.unsqueeze(2)).sum(\n            dim=-1) * mask).sum() / mask.sum()\n    return loss\n\n\ndef att_ce_mean_loss(attention_S, attention_T, mask=None):\n    '''\n    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.\n    * If the shape is (*batch_size*, *num_heads*, *length*, *length*), averages over dimension `num_heads` and then computes cross-entropy loss between the two matrics.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    \n    :param torch.tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)\n    :param torch.tensor mask:     tensor of shape  (*batch_size*, *length*)\n    '''\n    if len(attention_S.size()) == 4:\n        attention_S = attention_S.mean(dim=1)  # (bs, len, len)\n        attention_T = attention_T.mean(dim=1)\n    probs_T = F.softmax(attention_T, dim=-1)\n    if mask is None:\n        probs_T_select = torch.where(attention_T <= -1e-3, torch.zeros_like(attention_T), probs_T)\n        loss = -((probs_T_select * F.log_softmax(attention_S, dim=-1)).sum(dim=-1)).mean()\n    else:\n        mask = mask.to(attention_S)\n        loss = -((probs_T * F.log_softmax(attention_S, dim=-1) * mask.unsqueeze(1)).sum(\n            dim=-1) * mask).sum() / mask.sum()\n    return loss\n\n\ndef hid_mse_loss(state_S, state_T, mask=None):\n    '''\n    * Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is None:\n        loss = F.mse_loss(state_S, state_T)\n    else:\n        mask = mask.to(state_S)\n        valid_count = mask.sum() * state_S.size(-1)\n        loss = (F.mse_loss(state_S, state_T, reduction='none') * mask.unsqueeze(-1)).sum() / valid_count\n    return loss\n\n\ndef cos_loss(state_S, state_T, mask=None):\n    '''\n    * Computes the cosine similarity loss between the inputs. This is the loss used in DistilBERT, see `DistilBERT <https://arxiv.org/abs/1910.01108>`_\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)\n    '''\n    if mask is None:\n        state_S = state_S.view(-1, state_S.size(-1))\n        state_T = state_T.view(-1, state_T.size(-1))\n    else:\n        mask = mask.to(state_S).unsqueeze(-1).expand_as(state_S)  # (bs,len,dim)\n        state_S = torch.masked_select(state_S, mask).view(-1, mask.size(-1))  # (bs * select, dim)\n        state_T = torch.masked_select(state_T, mask).view(-1, mask.size(-1))  # (bs * select, dim)\n\n    target = state_S.new(state_S.size(0)).fill_(1)\n    loss = F.cosine_embedding_loss(state_S, state_T, target, reduction='mean')\n    return loss\n\n\ndef pkd_loss(state_S, state_T, mask=None):\n    '''\n    * Computes normalized vector mse loss at position 0 along `length` dimension. This is the loss used in BERT-PKD, see `Patient Knowledge Distillation for BERT Model Compression <https://arxiv.org/abs/1908.09355>`_.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)\n    :param mask: not used.\n    '''\n\n    cls_T = state_T[:, 0]  # (batch_size, hidden_dim)\n    cls_S = state_S[:, 0]  # (batch_size, hidden_dim)\n    normed_cls_T = cls_T / torch.norm(cls_T, dim=1, keepdim=True)\n    normed_cls_S = cls_S / torch.norm(cls_S, dim=1, keepdim=True)\n    loss = (normed_cls_S - normed_cls_T).pow(2).sum(dim=-1).mean()\n    return loss\n\n\ndef fsp_loss(state_S, state_T, mask=None):\n    r'''\n    * Takes in two lists of matrics `state_S` and `state_T`. Each list contains two matrices of the shape (*batch_size*, *length*, *hidden_size*). Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ) and the ones in B ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ), then computes the mse loss between the similarity matrices:\n\n    .. math::\n\n        loss = mean((S_{1}^T \\cdot S_{2} - T_{1}^T \\cdot T_{2})^2)\n\n    * It is a Variant of FSP loss in `A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning <http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf>`_.\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.\n\n    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)\n\n    Example in `intermediate_matches`::\n\n        intermediate_matches = [\n        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'fsp', 'weight' : 1, 'proj':['linear',384,768]},\n        ...]\n    '''\n    if mask is None:\n        state_S_0 = state_S[0]  # (batch_size , length, hidden_dim)\n        state_S_1 = state_S[1]  # (batch_size,  length, hidden_dim)\n        state_T_0 = state_T[0]\n        state_T_1 = state_T[1]\n        gram_S = torch.bmm(state_S_0.transpose(1, 2), state_S_1) / state_S_1.size(\n            1)  # (batch_size, hidden_dim, hidden_dim)\n        gram_T = torch.bmm(state_T_0.transpose(1, 2), state_T_1) / state_T_1.size(1)\n    else:\n        mask = mask.to(state_S[0]).unsqueeze(-1)\n        lengths = mask.sum(dim=1, keepdim=True)\n        state_S_0 = state_S[0] * mask\n        state_S_1 = state_S[1] * mask\n        state_T_0 = state_T[0] * mask\n        state_T_1 = state_T[1] * mask\n        gram_S = torch.bmm(state_S_0.transpose(1, 2), state_S_1) / lengths\n        gram_T = torch.bmm(state_T_0.transpose(1, 2), state_T_1) / lengths\n    loss = F.mse_loss(gram_S, gram_T)\n    return loss\n\n\ndef mmd_loss(state_S, state_T, mask=None):\n    r'''\n    * Takes in two lists of matrices `state_S` and `state_T`. Each list contains 2 matrices of the shape (*batch_size*, *length*, *hidden_size*). `hidden_size` of matrices in `State_S` doesn't need to be the same as that of `state_T`. Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *length*, *length*) ) and the ones in B ( with the resulting shape (*batch_size*, *length*, *length*) ), then computes the mse loss between the similarity matrices:\n    \n    .. math::\n\n            loss = mean((S_{1} \\cdot S_{2}^T - T_{1} \\cdot T_{2}^T)^2)\n\n    * It is a Variant of the NST loss in `Like What You Like: Knowledge Distill via Neuron Selectivity Transfer <https://arxiv.org/abs/1707.01219>`_\n    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.\n\n    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)\n    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)\n\n    Example in `intermediate_matches`::\n\n        intermediate_matches = [\n        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'nst', 'weight' : 1},\n        ...]\n    '''\n    state_S_0 = state_S[0]  # (batch_size , length, hidden_dim_S)\n    state_S_1 = state_S[1]  # (batch_size , length, hidden_dim_S)\n    state_T_0 = state_T[0]  # (batch_size , length, hidden_dim_T)\n    state_T_1 = state_T[1]  # (batch_size , length, hidden_dim_T)\n    if mask is None:\n        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)\n        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)\n        loss = F.mse_loss(gram_S, gram_T)\n    else:\n        mask = mask.to(state_S[0])\n        valid_count = torch.pow(mask.sum(dim=1), 2).sum()\n        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(1)  # (batch_size, length, length)\n        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(1)\n        loss = (F.mse_loss(gram_S, gram_T, reduction='none') * mask.unsqueeze(-1) * mask.unsqueeze(\n            1)).sum() / valid_count\n    return loss\n\n\nclass KnowledgeDistillationLoss(AutoConfigurable):\n    def __init__(self, name) -> None:\n        super().__init__()\n        self.name = name\n        import sys\n        thismodule = sys.modules[__name__]\n        self._loss = getattr(thismodule, name)\n\n    def __call__(self, *args, **kwargs):\n        return self._loss(*args, **kwargs)\n", "hanlp/components/distillation/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-10-17 20:29\n", "hanlp/components/distillation/distillable_component.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-10-17 20:30\nfrom abc import ABC\nfrom copy import copy\n\nimport hanlp\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.components.distillation.losses import KnowledgeDistillationLoss\nfrom hanlp.components.distillation.schedulers import TemperatureScheduler\nfrom hanlp.utils.torch_util import cuda_devices\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass DistillableComponent(TorchComponent, ABC):\n\n    # noinspection PyMethodMayBeStatic,PyTypeChecker\n    def build_teacher(self, teacher: str, devices) -> TorchComponent:\n        return hanlp.load(teacher, load_kwargs={'devices': devices})\n\n    def distill(self,\n                teacher: str,\n                trn_data,\n                dev_data,\n                save_dir,\n                batch_size=None,\n                epochs=None,\n                kd_criterion='kd_ce_loss',\n                temperature_scheduler='flsw',\n                devices=None,\n                logger=None,\n                seed=None,\n                **kwargs):\n        devices = devices or cuda_devices()\n        if isinstance(kd_criterion, str):\n            kd_criterion = KnowledgeDistillationLoss(kd_criterion)\n        if isinstance(temperature_scheduler, str):\n            temperature_scheduler = TemperatureScheduler.from_name(temperature_scheduler)\n        teacher = self.build_teacher(teacher, devices=devices)\n        self.vocabs = teacher.vocabs\n        config = copy(teacher.config)\n        batch_size = batch_size or config.get('batch_size', None)\n        epochs = epochs or config.get('epochs', None)\n        config.update(kwargs)\n        return super().fit(**merge_locals_kwargs(locals(),\n                                                 config,\n                                                 excludes=('self', 'kwargs', '__class__', 'config')))\n\n    @property\n    def _savable_config(self):\n        config = super(DistillableComponent, self)._savable_config\n        if 'teacher' in config:\n            config.teacher = config.teacher.load_path\n        return config\n", "hanlp/components/distillation/schedulers.py": "# Adopted from https://github.com/airaria/TextBrewer\n# Apache License Version 2.0\nfrom abc import ABC, abstractmethod\n\nimport torch\n\n# x is between 0 and 1\nfrom hanlp_common.configurable import AutoConfigurable\n\n\ndef linear_growth_weight_scheduler(x):\n    return x\n\n\ndef linear_decay_weight_scheduler(x):\n    return 1 - x\n\n\ndef constant_temperature_scheduler(logits_S, logits_T, base_temperature):\n    '''\n    Remember to detach logits_S \n    '''\n    return base_temperature\n\n\ndef flsw_temperature_scheduler_builder(beta, gamma, eps=1e-4, *args):\n    '''\n    adapted from arXiv:1911.07471\n    '''\n\n    def flsw_temperature_scheduler(logits_S, logits_T, base_temperature):\n        v = logits_S.detach()\n        t = logits_T.detach()\n        with torch.no_grad():\n            v = v / (torch.norm(v, dim=-1, keepdim=True) + eps)\n            t = t / (torch.norm(t, dim=-1, keepdim=True) + eps)\n            w = torch.pow((1 - (v * t).sum(dim=-1)), gamma)\n            tau = base_temperature + (w.mean() - w) * beta\n        return tau\n\n    return flsw_temperature_scheduler\n\n\ndef cwsm_temperature_scheduler_builder(beta, *args):\n    '''\n    adapted from arXiv:1911.07471\n    '''\n\n    def cwsm_temperature_scheduler(logits_S, logits_T, base_temperature):\n        v = logits_S.detach()\n        with torch.no_grad():\n            v = torch.softmax(v, dim=-1)\n            v_max = v.max(dim=-1)[0]\n            w = 1 / (v_max + 1e-3)\n            tau = base_temperature + (w.mean() - w) * beta\n        return tau\n\n    return cwsm_temperature_scheduler\n\n\nclass LinearTeacherAnnealingScheduler(object):\n    def __init__(self, num_training_steps: int) -> None:\n        super().__init__()\n        self._num_training_steps = num_training_steps\n        self._current_training_steps = 0\n\n    def step(self):\n        self._current_training_steps += 1\n\n    def __float__(self):\n        return self._current_training_steps / self._num_training_steps\n\n\nclass TemperatureScheduler(ABC, AutoConfigurable):\n\n    def __init__(self, base_temperature) -> None:\n        super().__init__()\n        self.base_temperature = base_temperature\n\n    def __call__(self, logits_S, logits_T):\n        return self.forward(logits_S, logits_T)\n\n    @abstractmethod\n    def forward(self, logits_S, logits_T):\n        raise NotImplementedError()\n\n    @staticmethod\n    def from_name(name):\n        classes = {\n            'constant': ConstantScheduler,\n            'flsw': FlswScheduler,\n            'cwsm': CwsmScheduler,\n        }\n        assert name in classes, f'Unsupported temperature scheduler {name}. Expect one from {list(classes.keys())}.'\n        return classes[name]()\n\n\nclass FunctionalScheduler(TemperatureScheduler):\n\n    def __init__(self, scheduler_func, base_temperature) -> None:\n        super().__init__(base_temperature)\n        self._scheduler_func = scheduler_func\n\n    def forward(self, logits_S, logits_T):\n        return self._scheduler_func(logits_S, logits_T, self.base_temperature)\n\n\nclass ConstantScheduler(TemperatureScheduler):\n    def forward(self, logits_S, logits_T):\n        return self.base_temperature\n\n\nclass FlswScheduler(FunctionalScheduler):\n    def __init__(self, beta=1, gamma=1, eps=1e-4, base_temperature=8):\n        super().__init__(flsw_temperature_scheduler_builder(beta, gamma, eps), base_temperature)\n        self.beta = beta\n        self.gamma = gamma\n        self.eps = eps\n\n\nclass CwsmScheduler(FunctionalScheduler):\n    def __init__(self, beta=1, base_temperature=8):\n        super().__init__(cwsm_temperature_scheduler_builder(beta), base_temperature)\n        self.beta = beta\n", "hanlp/components/mtl/multi_task_learning.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-20 19:55\nimport functools\nimport itertools\nimport logging\nimport os\nfrom collections import defaultdict\nfrom copy import copy\nfrom itertools import chain\nfrom typing import Union, List, Callable, Dict, Optional, Any, Iterable, Tuple\n\nimport numpy as np\nimport torch\nfrom hanlp_common.constant import IDX, BOS, EOS\nfrom hanlp_common.document import Document\nfrom hanlp_common.util import merge_locals_kwargs, topological_sort, reorder, prefix_match\nfrom hanlp_common.visualization import markdown_table\nfrom toposort import toposort\nfrom torch.utils.data import DataLoader\n\nimport hanlp.utils.torch_util\nfrom hanlp.common.dataset import PadSequenceDataLoader, PrefetchDataLoader, CachedDataLoader\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding, ContextualWordEmbeddingModule\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.layers.transformers.utils import pick_tensor_for_each_token\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp.utils.torch_util import clip_grad_norm\n\n\nclass MultiTaskModel(torch.nn.Module):\n\n    def __init__(self,\n                 encoder: torch.nn.Module,\n                 scalar_mixes: torch.nn.ModuleDict,\n                 decoders: torch.nn.ModuleDict,\n                 use_raw_hidden_states: dict) -> None:\n        super().__init__()\n        self.use_raw_hidden_states = use_raw_hidden_states\n        self.encoder: ContextualWordEmbeddingModule = encoder\n        self.scalar_mixes = scalar_mixes\n        self.decoders = decoders\n\n\nclass MultiTaskDataLoader(DataLoader):\n\n    def __init__(self, training=True, tau: float = 0.8, **dataloaders) -> None:\n        # noinspection PyTypeChecker\n        super().__init__(None)\n        self.tau = tau\n        self.training = training\n        self.dataloaders: Dict[str, DataLoader] = dataloaders if dataloaders else {}\n        # self.iterators = dict((k, iter(v)) for k, v in dataloaders.items())\n\n    def __len__(self) -> int:\n        if self.dataloaders:\n            return sum(len(x) for x in self.dataloaders.values())\n        return 0\n\n    def __iter__(self):\n        if self.training:\n            sampling_weights, total_size = self.sampling_weights\n            task_names = list(self.dataloaders.keys())\n            iterators = dict((k, itertools.cycle(v)) for k, v in self.dataloaders.items())\n            for i in range(total_size):\n                task_name = np.random.choice(task_names, p=sampling_weights)\n                yield task_name, next(iterators[task_name])\n        else:\n            for task_name, dataloader in self.dataloaders.items():\n                for batch in dataloader:\n                    yield task_name, batch\n\n    @property\n    def sampling_weights(self):\n        sampling_weights = self.sizes\n        total_size = sum(sampling_weights)\n        Z = sum(pow(v, self.tau) for v in sampling_weights)\n        sampling_weights = [pow(v, self.tau) / Z for v in sampling_weights]\n        return sampling_weights, total_size\n\n    @property\n    def sizes(self):\n        return [len(v) for v in self.dataloaders.values()]\n\n\nclass MultiTaskLearning(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\" A multi-task learning (MTL) framework. It shares the same encoder across multiple decoders. These decoders\n        can have dependencies on each other which will be properly handled during decoding. To integrate a component\n        into this MTL framework, a component needs to implement the :class:`~hanlp.components.mtl.tasks.Task` interface.\n\n        This framework mostly follows the architecture of :cite:`clark-etal-2019-bam` and :cite:`he-choi-2021-stem`, with additional scalar mix\n        tricks (:cite:`kondratyuk-straka-2019-75`) allowing each task to attend to any subset of layers. We also\n        experimented with knowledge distillation on single tasks, the performance gain was nonsignificant on a large\n        dataset. In the near future, we have no plan to invest more efforts in distillation, since most datasets HanLP\n        uses are relatively large, and our hardware is relatively powerful.\n\n        Args:\n            **kwargs: Arguments passed to config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: Optional[MultiTaskModel] = None\n        self.tasks: Dict[str, Task] = None\n        self.vocabs = None\n\n    def build_dataloader(self,\n                         data,\n                         batch_size,\n                         shuffle=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         gradient_accumulation=1,\n                         tau: float = 0.8,\n                         prune=None,\n                         prefetch=None,\n                         tasks_need_custom_eval=None,\n                         cache=False,\n                         debug=False,\n                         **kwargs) -> DataLoader:\n        # This method is only called during training or evaluation but not prediction\n        dataloader = MultiTaskDataLoader(training=shuffle, tau=tau)\n        for i, (task_name, task) in enumerate(self.tasks.items()):\n            encoder_transform, transform = self.build_transform(task)\n            training = None\n            if data == 'trn':\n                if debug:\n                    _data = task.dev\n                else:\n                    _data = task.trn\n                training = True\n            elif data == 'dev':\n                _data = task.dev\n                training = False\n            elif data == 'tst':\n                _data = task.tst\n                training = False\n            else:\n                _data = data\n            if isinstance(data, str):\n                logger.info(f'[yellow]{i + 1} / {len(self.tasks)}[/yellow] Building [blue]{data}[/blue] dataset for '\n                            f'[cyan]{task_name}[/cyan] ...')\n            # Adjust Tokenizer according to task config\n            config = copy(task.config)\n            config.pop('transform', None)\n            task_dataloader: DataLoader = task.build_dataloader(_data, transform, training, device, logger,\n                                                                tokenizer=encoder_transform.tokenizer,\n                                                                gradient_accumulation=gradient_accumulation,\n                                                                cache=isinstance(data, str), **config)\n            # if prune:\n            #     # noinspection PyTypeChecker\n            #     task_dataset: TransformDataset = task_dataloader.dataset\n            #     size_before = len(task_dataset)\n            #     task_dataset.prune(prune)\n            #     size_after = len(task_dataset)\n            #     num_pruned = size_before - size_after\n            #     logger.info(f'Pruned [yellow]{num_pruned} ({num_pruned / size_before:.1%})[/yellow] '\n            #                 f'samples out of {size_before}.')\n            if cache and data in ('trn', 'dev'):\n                task_dataloader: CachedDataLoader = CachedDataLoader(\n                    task_dataloader,\n                    f'{cache}/{os.getpid()}-{data}-{task_name.replace(\"/\", \"-\")}-cache.pt' if isinstance(cache,\n                                                                                                         str) else None\n                )\n            dataloader.dataloaders[task_name] = task_dataloader\n        if data == 'trn':\n            sampling_weights, total_size = dataloader.sampling_weights\n            headings = ['task', '#batches', '%batches', '#scaled', '%scaled', '#epoch']\n            matrix = []\n            min_epochs = []\n            for (task_name, dataset), weight in zip(dataloader.dataloaders.items(), sampling_weights):\n                epochs = len(dataset) / weight / total_size\n                matrix.append(\n                    [f'{task_name}', len(dataset), f'{len(dataset) / total_size:.2%}', int(total_size * weight),\n                     f'{weight:.2%}', f'{epochs:.2f}'])\n                min_epochs.append(epochs)\n            longest = int(torch.argmax(torch.tensor(min_epochs)))\n            table = markdown_table(headings, matrix)\n            rows = table.splitlines()\n            cells = rows[longest + 2].split('|')\n            cells[-2] = cells[-2].replace(f'{min_epochs[longest]:.2f}',\n                                          f'[bold][red]{min_epochs[longest]:.2f}[/red][/bold]')\n            rows[longest + 2] = '|'.join(cells)\n            logger.info(f'[bold][yellow]{\"Samples Distribution\": ^{len(rows[0])}}[/yellow][/bold]')\n            logger.info('\\n'.join(rows))\n        if prefetch and (data == 'trn' or not tasks_need_custom_eval):\n            dataloader = PrefetchDataLoader(dataloader, prefetch=prefetch)\n\n        return dataloader\n\n    def build_transform(self, task: Task) -> Tuple[TransformerSequenceTokenizer, TransformList]:\n        encoder: ContextualWordEmbedding = self.config.encoder\n        encoder_transform: TransformerSequenceTokenizer = task.build_tokenizer(encoder.transform())\n        length_transform = FieldLength('token', 'token_length')\n        transform = TransformList(encoder_transform, length_transform)\n        extra_transform = self.config.get('transform', None)\n        if extra_transform:\n            transform.insert(0, extra_transform)\n        return encoder_transform, transform\n\n    def build_optimizer(self,\n                        trn,\n                        epochs,\n                        adam_epsilon,\n                        weight_decay,\n                        warmup_steps,\n                        lr,\n                        encoder_lr,\n                        **kwargs):\n        model = self.model_\n        encoder = model.encoder\n        num_training_steps = len(trn) * epochs // self.config.get('gradient_accumulation', 1)\n        encoder_parameters = list(encoder.parameters())\n        parameter_groups: List[Dict[str, Any]] = []\n\n        decoders = model.decoders\n        decoder_optimizers = dict()\n        for k, task in self.tasks.items():\n            decoder: torch.nn.Module = decoders[k]\n            decoder_parameters = list(decoder.parameters())\n            if task.separate_optimizer:\n                decoder_optimizers[k] = task.build_optimizer(decoder=decoder, **kwargs)\n            else:\n                task_lr = task.lr or lr\n                parameter_groups.append({\"params\": decoder_parameters, 'lr': task_lr})\n        parameter_groups.append({\"params\": encoder_parameters, 'lr': encoder_lr})\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        no_decay_parameters = set()\n        for n, p in model.named_parameters():\n            if any(nd in n for nd in no_decay):\n                no_decay_parameters.add(p)\n        no_decay_by_lr = defaultdict(list)\n        for group in parameter_groups:\n            _lr = group['lr']\n            ps = group['params']\n            group['params'] = decay_parameters = []\n            group['weight_decay'] = weight_decay\n            for p in ps:\n                if p in no_decay_parameters:\n                    no_decay_by_lr[_lr].append(p)\n                else:\n                    decay_parameters.append(p)\n        for _lr, ps in no_decay_by_lr.items():\n            parameter_groups.append({\"params\": ps, 'lr': _lr, 'weight_decay': 0.0})\n        # noinspection PyTypeChecker\n        from transformers import optimization\n        encoder_optimizer = optimization.AdamW(\n            parameter_groups,\n            lr=lr,\n            weight_decay=weight_decay,\n            eps=adam_epsilon,\n        )\n        encoder_scheduler = optimization.get_linear_schedule_with_warmup(encoder_optimizer,\n                                                                         num_training_steps * warmup_steps,\n                                                                         num_training_steps)\n        return encoder_optimizer, encoder_scheduler, decoder_optimizers\n\n    def build_criterion(self, **kwargs):\n        return dict((k, v.build_criterion(decoder=self.model_.decoders[k], **kwargs)) for k, v in self.tasks.items())\n\n    def build_metric(self, **kwargs):\n        metrics = MetricDict()\n        for key, task in self.tasks.items():\n            metric = task.build_metric(**kwargs)\n            assert metric, f'Please implement `build_metric` of {type(task)} to return a metric.'\n            metrics[key] = metric\n        return metrics\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, patience=0.5, **kwargs):\n        if isinstance(patience, float):\n            patience = int(patience * epochs)\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        epoch = 0\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history, ratio_width=ratio_width,\n                                **self.config)\n            if dev:\n                self.evaluate_dataloader(dev, criterion, metric, logger, ratio_width=ratio_width, input='dev')\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            dev_score = metric.score\n            if dev_score > best_metric:\n                self.save_weights(save_dir)\n                best_metric = dev_score\n                best_epoch = epoch\n                report += ' [red]saved[/red]'\n            else:\n                report += f' ({epoch - best_epoch})'\n                if epoch - best_epoch >= patience:\n                    report += ' early stop'\n                    break\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n        for d in [trn, dev]:\n            self._close_dataloader(d)\n        if best_epoch != epoch:\n            logger.info(f'Restoring best model saved [red]{epoch - best_epoch}[/red] epochs ago')\n            self.load_weights(save_dir)\n        return best_metric\n\n    def _close_dataloader(self, d):\n        if isinstance(d, PrefetchDataLoader):\n            d.close()\n            if hasattr(d.dataset, 'close'):\n                self._close_dataloader(d.dataset)\n        elif isinstance(d, CachedDataLoader):\n            d.close()\n        elif isinstance(d, MultiTaskDataLoader):\n            for d in d.dataloaders.values():\n                self._close_dataloader(d)\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       history: History,\n                       ratio_width=None,\n                       gradient_accumulation=1,\n                       encoder_grad_norm=None,\n                       decoder_grad_norm=None,\n                       patience=0.5,\n                       eval_trn=False,\n                       **kwargs):\n        self.model.train()\n        encoder_optimizer, encoder_scheduler, decoder_optimizers = optimizer\n        timer = CountdownTimer(len(trn))\n        total_loss = 0\n        self.reset_metrics(metric)\n        model = self.model_\n        encoder_parameters = model.encoder.parameters()\n        decoder_parameters = model.decoders.parameters()\n        for idx, (task_name, batch) in enumerate(trn):\n            decoder_optimizer = decoder_optimizers.get(task_name, None)\n            output_dict, _ = self.feed_batch(batch, task_name)\n            loss = self.compute_loss(batch, output_dict[task_name]['output'], criterion[task_name],\n                                     self.tasks[task_name])\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += float(loss.item())\n            if history.step(gradient_accumulation):\n                if self.config.get('grad_norm', None):\n                    clip_grad_norm(model, self.config.grad_norm)\n                if encoder_grad_norm:\n                    torch.nn.utils.clip_grad_norm_(encoder_parameters, encoder_grad_norm)\n                if decoder_grad_norm:\n                    torch.nn.utils.clip_grad_norm_(decoder_parameters, decoder_grad_norm)\n                encoder_optimizer.step()\n                encoder_optimizer.zero_grad()\n                encoder_scheduler.step()\n                if decoder_optimizer:\n                    if isinstance(decoder_optimizer, tuple):\n                        decoder_optimizer, decoder_scheduler = decoder_optimizer\n                    else:\n                        decoder_scheduler = None\n                    decoder_optimizer.step()\n                    decoder_optimizer.zero_grad()\n                    if decoder_scheduler:\n                        decoder_scheduler.step()\n            if eval_trn:\n                self.decode_output(output_dict, batch, task_name)\n                self.update_metrics(batch, output_dict, metric, task_name)\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric if eval_trn else None),\n                      ratio_percentage=None,\n                      ratio_width=ratio_width,\n                      logger=logger)\n            del loss\n            del output_dict\n        return total_loss / timer.total\n\n    def report_metrics(self, loss, metrics: MetricDict):\n        return f'loss: {loss:.4f} {metrics.cstr()}' if metrics else f'loss: {loss:.4f}'\n\n    # noinspection PyMethodOverriding\n    @torch.no_grad()\n    def evaluate_dataloader(self,\n                            data: MultiTaskDataLoader,\n                            criterion,\n                            metric: MetricDict,\n                            logger,\n                            ratio_width=None,\n                            input: str = None,\n                            **kwargs):\n        self.model.eval()\n        self.reset_metrics(metric)\n        tasks_need_custom_eval = self.config.get('tasks_need_custom_eval', None)\n        tasks_need_custom_eval = tasks_need_custom_eval or {}\n        tasks_need_custom_eval = dict((k, None) for k in tasks_need_custom_eval)\n        for each in tasks_need_custom_eval:\n            tasks_need_custom_eval[each] = data.dataloaders.pop(each)\n        timer = CountdownTimer(len(data) + len(tasks_need_custom_eval))\n        total_loss = 0\n        for idx, (task_name, batch) in enumerate(data):\n            output_dict, _ = self.feed_batch(batch, task_name)\n            loss = self.compute_loss(batch, output_dict[task_name]['output'], criterion[task_name],\n                                     self.tasks[task_name])\n            total_loss += loss.item()\n            self.decode_output(output_dict, batch, task_name)\n            self.update_metrics(batch, output_dict, metric, task_name)\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger,\n                      ratio_width=ratio_width)\n            del loss\n            del output_dict\n\n        for task_name, dataset in tasks_need_custom_eval.items():\n            task = self.tasks[task_name]\n            decoder = self.model_.decoders[task_name]\n            task.evaluate_dataloader(\n                dataset, task.build_criterion(decoder=decoder),\n                metric=metric[task_name],\n                input=task.dev if input == 'dev' else task.tst,\n                split=input,\n                decoder=decoder,\n                h=functools.partial(self._encode, task_name=task_name,\n                                    cls_is_bos=task.cls_is_bos, sep_is_eos=task.sep_is_eos)\n            )\n            data.dataloaders[task_name] = dataset\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger,\n                      ratio_width=ratio_width)\n\n        return total_loss / timer.total, metric, data\n\n    def build_model(self, training=False, **kwargs) -> torch.nn.Module:\n        tasks = self.tasks\n        encoder: ContextualWordEmbedding = self.config.encoder\n        transformer_module = encoder.module(training=training)\n        encoder_size = transformer_module.get_output_dim()\n        scalar_mixes = torch.nn.ModuleDict()\n        decoders = torch.nn.ModuleDict()\n        use_raw_hidden_states = dict()\n        for task_name, task in tasks.items():\n            decoder = task.build_model(encoder_size, training=training, **task.config)\n            assert decoder, f'Please implement `build_model` of {type(task)} to return a decoder.'\n            decoders[task_name] = decoder\n            if task.scalar_mix:\n                scalar_mix = task.scalar_mix.build()\n                scalar_mixes[task_name] = scalar_mix\n                # Activate scalar mix starting from 0-th layer\n                encoder.scalar_mix = 0\n            use_raw_hidden_states[task_name] = task.use_raw_hidden_states\n        encoder.ret_raw_hidden_states = any(use_raw_hidden_states.values())\n        return MultiTaskModel(transformer_module, scalar_mixes, decoders, use_raw_hidden_states)\n\n    def predict(self,\n                data: Union[str, List[str]],\n                tasks: Optional[Union[str, List[str]]] = None,\n                skip_tasks: Optional[Union[str, List[str]]] = None,\n                resolved_tasks=None,\n                **kwargs) -> Document:\n        \"\"\"Predict on data.\n\n        Args:\n            data: A sentence or a list of sentences.\n            tasks: The tasks to predict.\n            skip_tasks: The tasks to skip.\n            resolved_tasks: The resolved tasks to override ``tasks`` and ``skip_tasks``.\n            **kwargs: Not used.\n\n        Returns:\n            A :class:`~hanlp_common.document.Document`.\n        \"\"\"\n        doc = Document()\n        target_tasks = resolved_tasks or self.resolve_tasks(tasks, skip_tasks)\n        if data == []:\n            for group in target_tasks:\n                for task_name in group:\n                    doc[task_name] = []\n            return doc\n        flatten_target_tasks = [self.tasks[t] for group in target_tasks for t in group]\n        cls_is_bos = any([x.cls_is_bos for x in flatten_target_tasks])\n        sep_is_eos = any([x.sep_is_eos for x in flatten_target_tasks])\n        # Now build the dataloaders and execute tasks\n        first_task_name: str = list(target_tasks[0])[0]\n        first_task: Task = self.tasks[first_task_name]\n        encoder_transform, transform = self.build_transform(first_task)\n        # Override the tokenizer config of the 1st task\n        encoder_transform.sep_is_eos = sep_is_eos\n        encoder_transform.cls_is_bos = cls_is_bos\n        average_subwords = self.model.encoder.average_subwords\n        flat = first_task.input_is_flat(data)\n        if flat:\n            data = [data]\n        device = self.device\n        samples = first_task.build_samples(data, cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)\n        dataloader = first_task.build_dataloader(samples, transform=transform, device=device)\n        results = defaultdict(list)\n        order = []\n        for batch in dataloader:\n            order.extend(batch[IDX])\n            # Run the first task, let it make the initial batch for the successors\n            output_dict = self.predict_task(first_task, first_task_name, batch, results, run_transform=True,\n                                            cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)\n            # Run each task group in order\n            for group_id, group in enumerate(target_tasks):\n                # We could parallelize this in the future\n                for task_name in group:\n                    if task_name == first_task_name:\n                        continue\n                    output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,\n                                                    run_transform=True, cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)\n                if group_id == 0:\n                    # We are kind of hard coding here. If the first task is a tokenizer,\n                    # we need to convert the hidden and mask to token level\n                    if first_task_name.startswith('tok'):\n                        spans = []\n                        tokens = []\n                        output_spans = first_task.config.get('output_spans', None)\n                        for span_per_sent, token_per_sent in zip(output_dict[first_task_name]['prediction'],\n                                                                 results[first_task_name][-len(batch[IDX]):]):\n                            if output_spans:\n                                token_per_sent = [x[0] for x in token_per_sent]\n                            if cls_is_bos:\n                                span_per_sent = [(-1, 0)] + span_per_sent\n                                token_per_sent = [BOS] + token_per_sent\n                            if sep_is_eos:\n                                span_per_sent = span_per_sent + [(span_per_sent[-1][0] + 1, span_per_sent[-1][1] + 1)]\n                                token_per_sent = token_per_sent + [EOS]\n                            # The offsets start with 0 while [CLS] is zero\n                            if average_subwords:\n                                span_per_sent = [list(range(x[0] + 1, x[1] + 1)) for x in span_per_sent]\n                            else:\n                                span_per_sent = [x[0] + 1 for x in span_per_sent]\n                            spans.append(span_per_sent)\n                            tokens.append(token_per_sent)\n                        spans = PadSequenceDataLoader.pad_data(spans, 0, torch.long, device=device)\n                        output_dict['hidden'] = pick_tensor_for_each_token(output_dict['hidden'], spans,\n                                                                           average_subwords)\n                        batch['token_token_span'] = spans\n                        batch['token'] = tokens\n                        # noinspection PyTypeChecker\n                        batch['token_length'] = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=device)\n                        batch.pop('mask', None)\n        # Put results into doc in the order of tasks\n        for k in self.config.task_names:\n            v = results.get(k, None)\n            if v is None:\n                continue\n            doc[k] = reorder(v, order)\n        # Allow task to perform finalization on document\n        for group in target_tasks:\n            for task_name in group:\n                task = self.tasks[task_name]\n                task.finalize_document(doc, task_name)\n        # If no tok in doc, use raw input as tok\n        if not any(k.startswith('tok') for k in doc):\n            doc['tok'] = data\n        if flat:\n            for k, v in list(doc.items()):\n                doc[k] = v[0]\n        # If there is only one field, don't bother to wrap it\n        # if len(doc) == 1:\n        #     return list(doc.values())[0]\n        return doc\n\n    def resolve_tasks(self, tasks, skip_tasks) -> List[Iterable[str]]:\n        # Now we decide which tasks to perform and their orders\n        tasks_in_topological_order = self._tasks_in_topological_order\n        task_topological_order = self._task_topological_order\n        computation_graph = self._computation_graph\n        target_tasks = self._resolve_task_name(tasks)\n        if not target_tasks:\n            target_tasks = tasks_in_topological_order\n        else:\n            target_topological_order = defaultdict(set)\n            for task_name in target_tasks:\n                for dependency in topological_sort(computation_graph, task_name):\n                    target_topological_order[task_topological_order[dependency]].add(dependency)\n            target_tasks = [item[1] for item in sorted(target_topological_order.items())]\n        if skip_tasks:\n            skip_tasks = self._resolve_task_name(skip_tasks)\n            target_tasks = [x - skip_tasks for x in target_tasks]\n            target_tasks = [x for x in target_tasks if x]\n        assert target_tasks, f'No task to perform due to `tasks = {tasks}`.'\n        # Sort target tasks within the same group in a defined order\n        target_tasks = [sorted(x, key=lambda _x: self.config.task_names.index(_x)) for x in target_tasks]\n        return target_tasks\n\n    def predict_task(self, task: Task, output_key, batch, results, output_dict=None, run_transform=True,\n                     cls_is_bos=True, sep_is_eos=True):\n        output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,\n                                             results)\n        self.decode_output(output_dict, batch, output_key)\n        results[output_key].extend(task.prediction_to_result(output_dict[output_key]['prediction'], batch))\n        return output_dict\n\n    def _resolve_task_name(self, dependencies):\n        resolved_dependencies = set()\n        if isinstance(dependencies, str):\n            if dependencies in self.tasks:\n                resolved_dependencies.add(dependencies)\n            elif dependencies.endswith('*'):\n                resolved_dependencies.update(x for x in self.tasks if x.startswith(dependencies[:-1]))\n            else:\n                prefix_matched = prefix_match(dependencies, self.config.task_names)\n                assert prefix_matched, f'No prefix matching for {dependencies}. ' \\\n                                       f'Check your dependencies definition: {list(self.tasks.values())}'\n                resolved_dependencies.add(prefix_matched)\n        elif isinstance(dependencies, Iterable):\n            resolved_dependencies.update(set(chain.from_iterable(self._resolve_task_name(x) for x in dependencies)))\n        return resolved_dependencies\n\n    def fit(self,\n            encoder: Embedding,\n            tasks: Dict[str, Task],\n            save_dir,\n            epochs,\n            patience=0.5,\n            lr=1e-3,\n            encoder_lr=5e-5,\n            adam_epsilon=1e-8,\n            weight_decay=0.0,\n            warmup_steps=0.1,\n            gradient_accumulation=1,\n            grad_norm=5.0,\n            encoder_grad_norm=None,\n            decoder_grad_norm=None,\n            tau: float = 0.8,\n            transform=None,\n            # prune: Callable = None,\n            eval_trn=True,\n            prefetch=None,\n            tasks_need_custom_eval=None,\n            _device_placeholder=False,\n            cache=False,\n            devices=None,\n            logger=None,\n            seed=None,\n            **kwargs):\n        trn_data, dev_data, batch_size = 'trn', 'dev', None\n        task_names = list(tasks.keys())\n        return super().fit(**merge_locals_kwargs(locals(), kwargs, excludes=('self', 'kwargs', '__class__', 'tasks')),\n                           **tasks)\n\n    # noinspection PyAttributeOutsideInit\n    def on_config_ready(self, **kwargs):\n        self.tasks = dict((key, task) for key, task in self.config.items() if isinstance(task, Task))\n        computation_graph = dict()\n        for task_name, task in self.tasks.items():\n            dependencies = task.dependencies\n            resolved_dependencies = self._resolve_task_name(dependencies)\n            computation_graph[task_name] = resolved_dependencies\n\n        # We can cache this order\n        tasks_in_topological_order = list(toposort(computation_graph))\n        task_topological_order = dict()\n        for i, group in enumerate(tasks_in_topological_order):\n            for task_name in group:\n                task_topological_order[task_name] = i\n        self._tasks_in_topological_order = tasks_in_topological_order\n        self._task_topological_order = task_topological_order\n        self._computation_graph = computation_graph\n\n    @staticmethod\n    def reset_metrics(metrics: Dict[str, Metric]):\n        for metric in metrics.values():\n            metric.reset()\n\n    def feed_batch(self,\n                   batch: Dict[str, Any],\n                   task_name,\n                   output_dict=None,\n                   run_transform=False,\n                   cls_is_bos=False,\n                   sep_is_eos=False,\n                   results=None) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        h, output_dict = self._encode(batch, task_name, output_dict, cls_is_bos, sep_is_eos)\n        task = self.tasks[task_name]\n        if run_transform:\n            batch = task.transform_batch(batch, results=results, cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)\n        batch['mask'] = mask = hanlp.utils.torch_util.lengths_to_mask(batch['token_length'])\n        output_dict[task_name] = {\n            'output': task.feed_batch(h,\n                                      batch=batch,\n                                      mask=mask,\n                                      decoder=self.model.decoders[task_name]),\n            'mask': mask\n        }\n        return output_dict, batch\n\n    def _encode(self, batch, task_name, output_dict=None, cls_is_bos=False, sep_is_eos=False):\n        model = self.model\n        if output_dict:\n            hidden, raw_hidden = output_dict['hidden'], output_dict['raw_hidden']\n        else:\n            hidden = model.encoder(batch)\n            if isinstance(hidden, tuple):\n                hidden, raw_hidden = hidden\n            else:\n                raw_hidden = None\n            output_dict = {'hidden': hidden, 'raw_hidden': raw_hidden}\n        hidden_states = raw_hidden if model.use_raw_hidden_states[task_name] else hidden\n        if task_name in model.scalar_mixes:\n            scalar_mix = model.scalar_mixes[task_name]\n            h = scalar_mix(hidden_states)\n        else:\n            if model.scalar_mixes:  # If any task enables scalar_mix, hidden_states will be a 4d tensor\n                hidden_states = hidden_states[-1, :, :, :]\n            h = hidden_states\n        # If the task doesn't need cls while h has cls, remove cls\n        task = self.tasks[task_name]\n        if cls_is_bos and not task.cls_is_bos:\n            h = h[:, 1:, :]\n        if sep_is_eos and not task.sep_is_eos:\n            h = h[:, :-1, :]\n        return h, output_dict\n\n    def decode_output(self, output_dict, batch, task_name=None):\n        if not task_name:\n            for task_name, task in self.tasks.items():\n                output_per_task = output_dict.get(task_name, None)\n                if output_per_task is not None:\n                    output_per_task['prediction'] = task.decode_output(\n                        output_per_task['output'],\n                        output_per_task['mask'],\n                        batch, self.model.decoders[task_name])\n        else:\n            output_per_task = output_dict[task_name]\n            output_per_task['prediction'] = self.tasks[task_name].decode_output(\n                output_per_task['output'],\n                output_per_task['mask'],\n                batch,\n                self.model.decoders[task_name])\n\n    def update_metrics(self, batch: Dict[str, Any], output_dict: Dict[str, Any], metrics: MetricDict, task_name):\n        task = self.tasks[task_name]\n        output_per_task = output_dict.get(task_name, None)\n        if output_per_task:\n            output = output_per_task['output']\n            prediction = output_per_task['prediction']\n            metric = metrics.get(task_name, None)\n            task.update_metrics(batch, output, prediction, metric)\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion: Callable,\n                     task: Task) -> torch.FloatTensor:\n        return task.compute_loss(batch, output, criterion)\n\n    def evaluate(self, save_dir=None, logger: logging.Logger = None, batch_size=None, output=False, **kwargs):\n        rets = super().evaluate('tst', save_dir, logger, batch_size, output, **kwargs)\n        tst = rets[-1]\n        self._close_dataloader(tst)\n        return rets\n\n    def save_vocabs(self, save_dir, filename='vocabs.json'):\n        for task_name, task in self.tasks.items():\n            task.save_vocabs(save_dir, f'{task_name}_{filename}')\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        for task_name, task in self.tasks.items():\n            task.load_vocabs(save_dir, f'{task_name}_{filename}')\n\n    def parallelize(self, devices: List[Union[int, torch.device]]):\n        raise NotImplementedError('Parallelization is not implemented yet.')\n\n    def __call__(self, data, **kwargs) -> Document:\n        return super().__call__(data, **kwargs)\n\n    def __getitem__(self, task_name: str) -> Task:\n        return self.tasks[task_name]\n\n    def __delitem__(self, task_name: str):\n        \"\"\"Delete a task (and every resource it owns) from this component.\n\n        Args:\n            task_name: The name of the task to be deleted.\n\n        Examples:\n            >>> del mtl['dep']  # Delete dep from MTL\n\n        \"\"\"\n        del self.config[task_name]\n        self.config.task_names.remove(task_name)\n        del self.tasks[task_name]\n        del self.model.decoders[task_name]\n        del self._computation_graph[task_name]\n        self._task_topological_order.pop(task_name)\n        for group in self._tasks_in_topological_order:\n            group: set = group\n            group.discard(task_name)\n\n    def __repr__(self):\n        return repr(self.config)\n\n    def items(self):\n        yield from self.tasks.items()\n\n    def __setattr__(self, key: str, value):\n        if key and key.startswith('dict') and not hasattr(self, key):\n            please_read_the_doc_ok = f'This MTL component has no {key}.'\n            matched_children = []\n            for name in self.config.task_names:\n                if hasattr(self[name], key):\n                    matched_children.append(name)\n            if matched_children:\n                please_read_the_doc_ok += f' Maybe you are looking for one of its tasks: {matched_children}. ' \\\n                                          f'For example, HanLP[\"{matched_children[0]}\"].{key} = ...'\n            raise TypeError(please_read_the_doc_ok)\n        object.__setattr__(self, key, value)\n", "hanlp/components/mtl/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-20 19:54", "hanlp/components/mtl/tasks/dep_2nd.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-07 14:14\nimport logging\nfrom typing import Dict, Any, Union, Iterable, Callable, List\n\nimport torch\nfrom hanlp_common.util import merge_locals_kwargs\nfrom torch.utils.data import DataLoader\n\nimport hanlp.utils.torch_util\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.parsers.biaffine.biaffine_2nd_dep import BiaffineSecondaryParser, BiaffineJointDecoder, \\\n    BiaffineSeparateDecoder\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\n\n\nclass BiaffineSecondaryDependencyDecoder(torch.nn.Module):\n    def __init__(self, hidden_size, config) -> None:\n        super().__init__()\n        self.decoder = BiaffineJointDecoder(hidden_size, config) if config.joint \\\n            else BiaffineSeparateDecoder(hidden_size, config)\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        if mask is None:\n            mask = hanlp.utils.torch_util.lengths_to_mask(batch['token_length'])\n        else:\n            mask = mask.clone()\n        scores = self.decoder(contextualized_embeddings, mask)\n        mask[:, 0] = 0\n        return scores, mask\n\n\nclass BiaffineSecondaryDependencyParsing(Task, BiaffineSecondaryParser):\n\n    def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None, scalar_mix: ScalarMixWithDropoutBuilder = None, use_raw_hidden_states=False,\n                 lr=2e-3, separate_optimizer=False,\n                 punct=False,\n                 tree=False,\n                 apply_constraint=True,\n                 n_mlp_arc=500,\n                 n_mlp_rel=100,\n                 mlp_dropout=.33,\n                 pad_rel=None,\n                 joint=True,\n                 mu=.9,\n                 nu=.9,\n                 epsilon=1e-12,\n                 cls_is_bos=True,\n                 **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def build_dataloader(self, data, transform: Callable = None, training=False, device=None,\n                         logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:\n        dataset = BiaffineSecondaryParser.build_dataset(self, data, transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            BiaffineSecondaryParser.build_vocabs(self, dataset, logger, transformer=True)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,\n                                                     gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset,\n            pad={'arc': 0, 'arc_2nd': False})\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n\n        BiaffineSecondaryParser.update_metric(self, *prediction, batch['arc'], batch['rel_id'], output[1],\n                                              batch['punct_mask'], metric, batch)\n\n    def decode_output(self, output: Dict[str, Any], batch: Dict[str, Any], decoder, **kwargs) \\\n            -> Union[Dict[str, Any], Any]:\n        return BiaffineSecondaryParser.decode(self, *output[0], output[1], batch)\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return BiaffineSecondaryParser.compute_loss(self, *output[0], batch['arc'], batch['rel_id'], output[1],\n                                                    criterion, batch)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return BiaffineSecondaryDependencyDecoder(encoder_size, self.config)\n\n    def build_metric(self, **kwargs):\n        return BiaffineSecondaryParser.build_metric(self, **kwargs)\n\n    def build_criterion(self, **kwargs):\n        return BiaffineSecondaryParser.build_criterion(self, **kwargs)\n\n    def build_optimizer(self, decoder: torch.nn.Module, **kwargs):\n        config = self.config\n        optimizer = torch.optim.Adam(decoder.parameters(),\n                                     config.lr,\n                                     (config.mu, config.nu),\n                                     config.epsilon)\n        return optimizer\n\n    def input_is_flat(self, data) -> bool:\n        return BiaffineSecondaryParser.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        outputs = []\n        return BiaffineSecondaryParser.predictions_to_human(self, prediction, outputs, batch['token'], use_pos=False)\n", "hanlp/components/mtl/tasks/amr.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-12 16:05\nimport logging\nfrom typing import Dict, Any, List, Union, Iterable, Callable\n\nimport torch\nfrom stog.data.dataset_readers.amr_parsing.amr import AMRGraph\nfrom stog.data.dataset_readers.amr_parsing.node_utils import NodeUtilities\nfrom stog.data.dataset_readers.amr_parsing.postprocess.node_restore import NodeRestore\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import CLS\nfrom hanlp.common.dataset import PrefetchDataLoader, SamplerBuilder\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.amr.amr_parser.graph_amr_decoder import GraphAbstractMeaningRepresentationDecoder\nfrom hanlp.components.amr.amr_parser.graph_parser import GraphAbstractMeaningRepresentationParser\nfrom hanlp.components.amr.amr_parser.postprocess import PostProcessor\nfrom hanlp.components.amr.amr_parser.work import parse_batch\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.datasets.parsing.amr import batchify, get_concepts\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.amr.smatch_eval import SmatchScores, get_amr_utils\nfrom hanlp.metrics.f1 import F1_\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp_common.util import merge_list_of_dict, merge_locals_kwargs\n\n\nclass GraphAbstractMeaningRepresentationParsing(Task, GraphAbstractMeaningRepresentationParser):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=1e-3,\n                 separate_optimizer=False,\n                 cls_is_bos=True,\n                 sep_is_eos=False,\n                 char2concept_dim=128,\n                 cnn_filters=((3, 256),),\n                 concept_char_dim=32,\n                 concept_dim=300,\n                 dropout=0.2,\n                 embed_dim=512,\n                 eval_every=20,\n                 ff_embed_dim=1024,\n                 graph_layers=2,\n                 inference_layers=4,\n                 num_heads=8,\n                 rel_dim=100,\n                 snt_layers=4,\n                 unk_rate=0.33,\n                 vocab_min_freq=5,\n                 beam_size=8,\n                 alpha=0.6,\n                 max_time_step=100,\n                 amr_version='2.0',\n                 **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n        utils_dir = get_resource(get_amr_utils(amr_version))\n        self.sense_restore = NodeRestore(NodeUtilities.from_json(utils_dir))\n\n    def build_dataloader(self,\n                         data,\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        if isinstance(data, list):\n            data = GraphAbstractMeaningRepresentationParser.build_samples(self, data)\n        dataset, lens = GraphAbstractMeaningRepresentationParser.build_dataset(self, data, logger=logger,\n                                                                               transform=transform, training=training)\n        if self.vocabs.mutable:\n            GraphAbstractMeaningRepresentationParser.build_vocabs(self, dataset, logger)\n        dataloader = PrefetchDataLoader(\n            DataLoader(batch_sampler=self.sampler_builder.build(lens, shuffle=training,\n                                                                gradient_accumulation=gradient_accumulation),\n                       dataset=dataset,\n                       collate_fn=merge_list_of_dict,\n                       num_workers=0), batchify=self.build_batchify(device, training),\n            prefetch=None)\n        return dataloader\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        concept_loss, arc_loss, rel_loss, graph_arc_loss = output\n        concept_loss, concept_correct, concept_total = concept_loss\n        rel_loss, rel_correct, rel_total = rel_loss\n        loss = concept_loss + arc_loss + rel_loss\n        return loss\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder: torch.nn.Module, **kwargs) -> Union[Dict[str, Any], Any]:\n        return output\n\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any],\n                       metric: Union[MetricDict, Metric]):\n        pass\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return GraphAbstractMeaningRepresentationDecoder(vocabs=self.vocabs, encoder_size=encoder_size, **self.config)\n\n    def build_metric(self, **kwargs):\n        return SmatchScores({'Smatch': F1_(0, 0, 0)})\n\n    def input_is_flat(self, data) -> bool:\n        return GraphAbstractMeaningRepresentationParser.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        pp = PostProcessor(self.vocabs['rel'])\n        for concept, relation, score in zip(prediction['concept'], prediction['relation'], prediction['score']):\n            amr = pp.to_amr(concept, relation)\n            amr_graph = AMRGraph(amr)\n            self.sense_restore.restore_graph(amr_graph)\n            yield amr_graph\n\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric=None,\n                            output=False,\n                            input=None,\n                            decoder=None,\n                            h=None,\n                            split=None,\n                            **kwargs):\n        # noinspection PyTypeChecker\n        GraphAbstractMeaningRepresentationParser.evaluate_dataloader(self, data, logger=None, metric=metric,\n                                                                     input=input, model=decoder, h=lambda x: h(x)[0],\n                                                                     use_fast=True)\n\n    def feed_batch(self,\n                   h: torch.FloatTensor,\n                   batch: Dict[str, torch.Tensor],\n                   mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        if decoder.training:\n            return super().feed_batch(h, batch, mask, decoder)\n        beam_size = self.config.get('beam_size', 8)\n        alpha = self.config.get('alpha', 0.6)\n        max_time_step = self.config.get('max_time_step', 100)\n        res = parse_batch(decoder, batch, beam_size, alpha, max_time_step, h=h)\n        return res\n\n    def transform_batch(self, batch: Dict[str, Any], results: Dict[str, Any] = None, cls_is_bos=False,\n                        sep_is_eos=False) -> Dict[str, Any]:\n        batch = super().transform_batch(batch, results, cls_is_bos, sep_is_eos)\n        batch['lemma'] = [[CLS] + x for x in results['lem']]\n        copy_seq = merge_list_of_dict(\n            [get_concepts({'token': t[1:], 'lemma': l[1:]}, self.vocabs.predictable_concept) for t, l in\n             zip(batch['token'], batch['lemma'])])\n        copy_seq.pop('token')\n        copy_seq.pop('lemma')\n        batch.update(copy_seq)\n        ret = batchify(batch, self.vocabs, device=batch['token_input_ids'].device)\n        return ret\n", "hanlp/components/mtl/tasks/sdp.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-13 21:39\nimport logging\nfrom typing import Dict, Any, Union, Iterable, List\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDecoder\nfrom hanlp.components.parsers.biaffine.biaffine_sdp import BiaffineSemanticDependencyParser\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BiaffineSemanticDependencyParsing(Task, BiaffineSemanticDependencyParser):\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=2e-3, separate_optimizer=False,\n                 punct=False,\n                 tree=True,\n                 pad_rel=None,\n                 apply_constraint=False,\n                 single_root=True,\n                 no_zero_head=None,\n                 n_mlp_arc=500,\n                 n_mlp_rel=100,\n                 mlp_dropout=.33,\n                 mu=.9,\n                 nu=.9,\n                 epsilon=1e-12,\n                 decay=.75,\n                 decay_steps=5000,\n                 cls_is_bos=True,\n                 use_pos=False,\n                 **kwargs) -> None:\n        r\"\"\"Implementation of \"Stanford's graph-based neural dependency parser at\n        the conll 2017 shared task\" (:cite:`dozat2017stanford`) and \"Establishing Strong Baselines for the New Decade\"\n        (:cite:`he-choi-2019`).\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            punct: ``True`` to include punctuations in evaluation.\n            pad_rel: Padding token for relations.\n            apply_constraint: Enforce constraints (see following parameters).\n            single_root: Force single root.\n            no_zero_head: Every token has at least one head.\n            n_mlp_arc: Number of features for arc representation.\n            n_mlp_rel: Number of features for rel representation.\n            mlp_dropout: Dropout applied to MLPs.\n            mu: First coefficient used for computing running averages of gradient and its square in Adam.\n            nu: Second coefficient used for computing running averages of gradient and its square in Adam.\n            epsilon: Term added to the denominator to improve numerical stability\n            decay: Decay rate for exceptional lr scheduler.\n            decay_steps: Decay every ``decay_steps`` steps.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            use_pos: Use pos feature.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        BiaffineSemanticDependencyParser.update_metric(self, *prediction, batch['arc'], batch['rel_id'], output[1],\n                                                       output[-1], metric, batch)\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder, **kwargs) -> Union[Dict[str, Any], Any]:\n        (arc_scores, rel_scores), mask, punct_mask = output\n        return BiaffineSemanticDependencyParser.decode(self, arc_scores, rel_scores, mask, batch)\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        (arc_scores, rel_scores), mask, punct_mask = output\n        return BiaffineSemanticDependencyParser.compute_loss(self, arc_scores, rel_scores, batch['arc'],\n                                                             batch['rel_id'], mask, criterion,\n                                                             batch)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return BiaffineDecoder(encoder_size, self.config.n_mlp_arc, self.config.n_mlp_rel, self.config.mlp_dropout,\n                               len(self.vocabs.rel))\n\n    def build_metric(self, **kwargs):\n        return BiaffineSemanticDependencyParser.build_metric(self, **kwargs)\n\n    def build_dataloader(self, data, transform: TransformList = None, training=False, device=None,\n                         logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:\n        dataset = BiaffineSemanticDependencyParser.build_dataset(self, data, transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            BiaffineSemanticDependencyParser.build_vocabs(self, dataset, logger, transformer=True)\n        if isinstance(data, str):\n            timer = CountdownTimer(len(dataset))\n            BiaffineSemanticDependencyParser.cache_dataset(self, dataset, timer, training, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset,\n            pad=self.get_pad_dict())\n\n    def feed_batch(self, h: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        logits = super().feed_batch(h, batch, mask, decoder)\n        arc_scores = logits[0]\n        mask = mask.clone()\n        mask[:, 0] = 0\n        mask = self.convert_to_3d_mask(arc_scores, mask)\n        punct_mask = self.convert_to_3d_puncts(batch.get('punct_mask', None), mask)\n        return logits, mask, punct_mask\n\n    def build_optimizer(self, decoder: torch.nn.Module, **kwargs):\n        config = self.config\n        optimizer = Adam(decoder.parameters(),\n                         config.lr,\n                         (config.mu, config.nu),\n                         config.epsilon)\n        scheduler = ExponentialLR(optimizer, config.decay ** (1 / config.decay_steps))\n        return optimizer, scheduler\n\n    def input_is_flat(self, data) -> bool:\n        return BiaffineSemanticDependencyParser.input_is_flat(self, data, self.config.use_pos)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        arcs, rels = prediction\n        arcs = arcs[:, 1:, :]  # Skip the ROOT\n        rels = rels[:, 1:, :]\n        arcs = arcs.tolist()\n        rels = rels.tolist()\n        vocab = self.vocabs['rel'].idx_to_token\n        for arcs_per_sent, rels_per_sent, tokens in zip(arcs, rels, batch['token']):\n            tokens = tokens[1:]\n            sent_len = len(tokens)\n            result = []\n            for a, r in zip(arcs_per_sent[:sent_len], rels_per_sent[:sent_len]):\n                heads = [i for i in range(sent_len + 1) if a[i]]\n                deprels = [vocab[r[i]] for i in range(sent_len + 1) if a[i]]\n                result.append(list(zip(heads, deprels)))\n            yield result\n\n    def build_samples(self, inputs, cls_is_bos=False, sep_is_eos=False):\n        return BiaffineSemanticDependencyParser.build_samples(self, inputs, self.config.use_pos)\n", "hanlp/components/mtl/tasks/pos.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-10-19 18:56\nimport logging\nfrom typing import Dict, Any, Union, Iterable, Callable, List, Tuple, Sequence\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\nfrom hanlp_trie import DictInterface, TrieDict\n\n\nclass LinearCRFDecoder(torch.nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 num_labels,\n                 crf=False) -> None:\n        \"\"\"A linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer on top of it.\n\n        Args:\n            hidden_size: Size of hidden states.\n            num_labels: Size of tag set.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n        \"\"\"\n        super().__init__()\n        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n        self.crf = CRF(num_labels) if crf else None\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        \"\"\"\n\n        Args:\n            contextualized_embeddings: Hidden states for contextual layer.\n            batch: A dict of a batch.\n            mask: Mask for tokens.\n\n        Returns:\n            Logits. Users are expected to call ``CRF.decode`` on these emissions during decoding and ``CRF.forward``\n            during training.\n\n        \"\"\"\n        return self.classifier(contextualized_embeddings)\n\n\nclass TransformerTagging(Task, TransformerTagger):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=1e-3,\n                 separate_optimizer=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 max_seq_len=None,\n                 sent_delimiter=None,\n                 char_level=False,\n                 hard_constraint=False,\n                 crf=False,\n                 token_key='token',\n                 dict_tags: Union[\n                     DictInterface, Union[Dict[Union[str, Sequence[str]], Union[str, Sequence[str]]]]] = None,\n                 **kwargs) -> None:\n        \"\"\"A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for\n        any tagging tasks including PoS tagging and many others. It also features with a custom dictionary ``dict_tags``\n        to perform ``longest-prefix-matching`` which replaces matched tokens with given tags.\n\n\n        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can\n            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level, which is never the case for\n                lemmatization.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.\n            dict_tags: A custom dictionary to override predicted tags by performing longest-prefix-matching.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n        self.dict_tags = dict_tags\n\n    def build_dataloader(self,\n                         data,\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        args = dict((k, self.config[k]) for k in\n                    ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)\n        dataset = self.build_dataset(data, cache=True, transform=transform, **args)\n        dataset.append_transform(self.vocabs)\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return TransformerTagger.compute_loss(self, criterion, output, batch['tag_id'], batch['mask'])\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder,\n                      **kwargs) -> Union[Dict[str, Any], Any]:\n        return TransformerTagger.decode_output(self, output, mask, batch, decoder)\n\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any],\n                       metric: Union[MetricDict, Metric]):\n        return TransformerTagger.update_metrics(self, metric, output, batch['tag_id'], batch['mask'])\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return LinearCRFDecoder(encoder_size, len(self.vocabs['tag']), self.config.crf)\n\n    def build_metric(self, **kwargs):\n        return TransformerTagger.build_metric(self, **kwargs)\n\n    def input_is_flat(self, data) -> bool:\n        return TransformerTagger.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> Union[List, Dict]:\n        return TransformerTagger.prediction_to_human(self, prediction, self.vocabs['tag'].idx_to_token, batch)\n", "hanlp/components/mtl/tasks/constituency.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-11-29 16:52\nimport logging\nfrom typing import Dict, Any, List, Union, Iterable, Callable\n\nimport torch\nfrom phrasetree.tree import Tree\n\nfrom hanlp_common.constant import BOS, EOS\nfrom hanlp_common.document import Document\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.parsers.constituency.crf_constituency_model import CRFConstituencyDecoder\nfrom hanlp.components.parsers.constituency.crf_constituency_parser import CRFConstituencyParser\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, prefix_match\n\n\nclass CRFConstituencyParsing(Task, CRFConstituencyParser):\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=None,\n                 separate_optimizer=False,\n                 cls_is_bos=True,\n                 sep_is_eos=True,\n                 delete=('', ':', '``', \"''\", '.', '?', '!', '-NONE-', 'TOP', ',', 'S1'),\n                 equal=(('ADVP', 'PRT'),),\n                 mbr=True,\n                 n_mlp_span=500,\n                 n_mlp_label=100,\n                 mlp_dropout=.33,\n                 no_subcategory=True,\n                 **kwargs\n                 ) -> None:\n        r\"\"\"Two-stage CRF Parsing (:cite:`ijcai2020-560`).\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            delete: Constituencies to be deleted from training and evaluation.\n            equal: Constituencies that are regarded as equal during evaluation.\n            mbr: ``True`` to enable Minimum Bayes Risk (MBR) decoding (:cite:`smith-smith-2007-probabilistic`).\n            n_mlp_span: Number of features for span decoder.\n            n_mlp_label: Number of features for label decoder.\n            mlp_dropout: Dropout applied to MLPs.\n            no_subcategory: Strip out subcategories.\n            **kwargs: Not used.\n        \"\"\"\n        if isinstance(equal, tuple):\n            equal = dict(equal)\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    # noinspection DuplicatedCode\n    def build_dataloader(self,\n                         data,\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        dataset = CRFConstituencyParsing.build_dataset(self, data, transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            CRFConstituencyParsing.build_vocabs(self, dataset, logger)\n        if isinstance(data, str):\n            timer = CountdownTimer(len(dataset))\n            # noinspection PyCallByClass\n            BiaffineDependencyParser.cache_dataset(self, dataset, timer, training, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,\n                                                     gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def feed_batch(self,\n                   h: torch.FloatTensor,\n                   batch: Dict[str, torch.Tensor],\n                   mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        return {\n            'output': decoder(h),\n            'mask': CRFConstituencyParser.compute_mask(\n                self, batch, offset=1 if 'constituency' in batch or batch['token'][0][-1] == EOS else -1)\n        }\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        out, mask = output['output'], output['mask']\n        loss, span_probs = CRFConstituencyParser.compute_loss(self, out, batch['chart_id'], mask, crf_decoder=criterion)\n        output['span_probs'] = span_probs\n        return loss\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder: torch.nn.Module, **kwargs) -> Union[Dict[str, Any], Any]:\n        out, mask = output['output'], output['mask']\n        tokens = []\n        for sent in batch['token']:\n            if sent[0] == BOS:\n                sent = sent[1:]\n            if sent[-1] == EOS:\n                sent = sent[:-1]\n            tokens.append(sent)\n        return CRFConstituencyParser.decode_output(self, out, mask, batch, output.get('span_probs', None),\n                                                   decoder=decoder, tokens=tokens)\n\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        return CRFConstituencyParser.update_metrics(self, metric, batch, prediction)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return CRFConstituencyDecoder(n_labels=len(self.vocabs.chart), n_hidden=encoder_size)\n\n    def build_metric(self, **kwargs):\n        return CRFConstituencyParser.build_metric(self)\n\n    def input_is_flat(self, data) -> bool:\n        return CRFConstituencyParser.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: List, batch: Dict[str, Any]) -> List:\n        return prediction\n\n    def finalize_document(self, doc: Document, task_name: str):\n        pos_key = prefix_match('pos', doc)\n        pos: List[List[str]] = doc.get(pos_key, None)\n        if pos:\n            for tree, pos_per_sent in zip(doc[task_name], pos):\n                tree: Tree = tree\n                offset = 0\n                for subtree in tree.subtrees(lambda t: t.height() == 2):\n                    tag = subtree.label()\n                    if tag == '_':\n                        subtree.set_label(pos_per_sent[offset])\n                    offset += 1\n\n    def build_samples(self, inputs, cls_is_bos=False, sep_is_eos=False):\n        return CRFConstituencyParser.build_samples(self, inputs)\n", "hanlp/components/mtl/tasks/dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-13 21:39\nimport logging\nfrom typing import Dict, Any, Union, Iterable, List\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.parsers.biaffine.biaffine_dep import BiaffineDependencyParser\nfrom hanlp.components.parsers.biaffine.biaffine_model import BiaffineDecoder\nfrom hanlp.datasets.parsing.loaders.conll_dataset import append_bos\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.constant import EOS\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BiaffineDependencyParsing(Task, BiaffineDependencyParser):\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=2e-3, separate_optimizer=False,\n                 cls_is_bos=True,\n                 sep_is_eos=False,\n                 punct=False,\n                 tree=False,\n                 proj=False,\n                 n_mlp_arc=500,\n                 n_mlp_rel=100,\n                 mlp_dropout=.33,\n                 mu=.9,\n                 nu=.9,\n                 epsilon=1e-12,\n                 decay=.75,\n                 decay_steps=5000,\n                 use_pos=False,\n                 max_seq_len=None,\n                 **kwargs) -> None:\n        \"\"\"Biaffine dependency parsing (:cite:`dozat:17a`).\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            punct: ``True`` to include punctuations in evaluation.\n            tree: ``True`` to enforce tree constraint.\n            proj: ``True`` for projective parsing.\n            n_mlp_arc: Number of features for arc representation.\n            n_mlp_rel: Number of features for rel representation.\n            mlp_dropout: Dropout applied to MLPs.\n            mu: First coefficient used for computing running averages of gradient and its square in Adam.\n            nu: Second coefficient used for computing running averages of gradient and its square in Adam.\n            epsilon: Term added to the denominator to improve numerical stability\n            decay: Decay rate for exceptional lr scheduler.\n            decay_steps: Decay every ``decay_steps`` steps.\n            use_pos: Use pos feature.\n            max_seq_len: Prune samples longer than this length.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        BiaffineDependencyParser.update_metric(self, *prediction, batch['arc'], batch['rel_id'], output[1],\n                                               batch.get('punct_mask', None), metric, batch)\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder, **kwargs) -> Union[Dict[str, Any], Any]:\n        (arc_scores, rel_scores), mask = output\n        return BiaffineDependencyParser.decode(self, arc_scores, rel_scores, mask, batch)\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        (arc_scores, rel_scores), mask = output\n        return BiaffineDependencyParser.compute_loss(self, arc_scores, rel_scores, batch['arc'], batch['rel_id'], mask,\n                                                     criterion,\n                                                     batch)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return BiaffineDecoder(encoder_size, self.config.n_mlp_arc, self.config.n_mlp_rel, self.config.mlp_dropout,\n                               len(self.vocabs.rel))\n\n    def build_metric(self, **kwargs):\n        return BiaffineDependencyParser.build_metric(self, **kwargs)\n\n    def build_dataloader(self, data, transform: TransformList = None, training=False, device=None,\n                         logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:\n        transform.insert(0, append_bos)\n        dataset = BiaffineDependencyParser.build_dataset(self, data, transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            BiaffineDependencyParser.build_vocabs(self, dataset, logger, transformer=True)\n        if isinstance(data, str):\n            timer = CountdownTimer(len(dataset))\n            BiaffineDependencyParser.cache_dataset(self, dataset, timer, training, logger)\n        max_seq_len = self.config.get('max_seq_len', None)\n        if max_seq_len and isinstance(data, str):\n            dataset.prune(lambda x: len(x['token_input_ids']) > max_seq_len, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset,\n            pad=self.get_pad_dict())\n\n    def feed_batch(self, h: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        logits = super().feed_batch(h, batch, mask, decoder)\n        mask = mask.clone()\n        mask[:, 0] = 0\n        return logits, mask\n\n    def build_optimizer(self, decoder: torch.nn.Module, **kwargs):\n        config = self.config\n        optimizer = Adam(decoder.parameters(),\n                         config.lr,\n                         (config.mu, config.nu),\n                         config.epsilon)\n        scheduler = ExponentialLR(optimizer, config.decay ** (1 / config.decay_steps))\n        return optimizer, scheduler\n\n    def input_is_flat(self, data) -> bool:\n        return BiaffineDependencyParser.input_is_flat(self, data, self.config.use_pos)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        arcs, rels = prediction\n        arcs = arcs[:, 1:]  # Skip the ROOT\n        rels = rels[:, 1:]\n        arcs = arcs.tolist()\n        rels = rels.tolist()\n        vocab = self.vocabs['rel'].idx_to_token\n        for arcs_per_sent, rels_per_sent, tokens in zip(arcs, rels, batch['token']):\n            tokens = tokens[1:]\n            sent_len = len(tokens)\n            result = list(zip(arcs_per_sent[:sent_len], [vocab[r] for r in rels_per_sent[:sent_len]]))\n            yield result\n\n    def build_samples(self, inputs, cls_is_bos=False, sep_is_eos=False):\n        return [{'FORM': token + ([EOS] if sep_is_eos else [])} for token in inputs]\n", "hanlp/components/mtl/tasks/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-02 16:51\nimport logging\nimport os\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom copy import copy\nfrom typing import Callable, Dict, Any, Union, Iterable, List\n\nimport torch\nfrom hanlp_common.util import merge_locals_kwargs\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import BOS, EOS\nfrom hanlp.common.dataset import SamplerBuilder, SortingSamplerBuilder, TransformableDataset, KMeansSamplerBuilder\nfrom hanlp_common.document import Document\nfrom hanlp.common.structure import ConfigTracker\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\n\n\nclass Task(ConfigTracker, TorchComponent, ABC):\n    # noinspection PyMissingConstructor\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=None,\n                 separate_optimizer=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 **kwargs) -> None:\n        \"\"\"\n        A task in the multi-task learning framework\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            **kwargs: Additional config.\n        \"\"\"\n        ConfigTracker.__init__(self, merge_locals_kwargs(locals(), kwargs))\n        for f, n in zip([trn, dev, tst], ['trn', 'dev', 'tst']):\n            if f and os.path.isfile(f):  # anonymize local file names\n                self.config.pop(n)\n        self.separate_optimizer = separate_optimizer\n        self.lr = lr\n        self.use_raw_hidden_states = use_raw_hidden_states\n        if sampler_builder is None:\n            sampler_builder = SortingSamplerBuilder(batch_size=32)\n        self.sampler_builder: Union[SortingSamplerBuilder, KMeansSamplerBuilder] = sampler_builder\n        self.dependencies = dependencies\n        self.tst = tst\n        self.dev = dev\n        self.trn = trn\n        self.scalar_mix = scalar_mix\n        self.cls_is_bos = cls_is_bos\n        self.sep_is_eos = sep_is_eos\n\n    @abstractmethod\n    def build_dataloader(self,\n                         data,\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        \"\"\"\n        Build a dataloader for training or evaluation.\n\n        Args:\n            data: Either a path or a list of samples.\n            transform: The transform from MTL, which is usually [TransformerSequenceTokenizer, FieldLength('token')]\n            training: Whether this method is called on training set.\n            device: The device dataloader is intended to work with.\n            logger: Logger for printing message indicating progress.\n            cache: Whether the dataloader should be cached.\n            gradient_accumulation: Gradient accumulation to be passed to sampler builder.\n            **kwargs: Additional experimental arguments.\n        \"\"\"\n        pass\n\n    def build_optimizer(self, decoder: torch.nn.Module, **kwargs):\n        pass\n\n    def build_batch_wise_scheduler(self, decoder: torch.nn.Module, **kwargs):\n        pass\n\n    @abstractmethod\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion,\n                     ) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        pass\n\n    @abstractmethod\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any], decoder: torch.nn.Module, **kwargs) -> Union[Dict[str, Any], Any]:\n        pass\n\n    @abstractmethod\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any],\n                       metric: Union[MetricDict, Metric]):\n        pass\n\n    # noinspection PyMethodOverriding\n    @abstractmethod\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        pass\n\n    @abstractmethod\n    def build_metric(self, **kwargs):\n        pass\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        pass\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, output=False, **kwargs):\n        pass\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, **kwargs):\n        pass\n\n    # noinspection PyMethodMayBeStatic\n    def compute_lens(self, data: Union[List[Dict[str, Any]], str], dataset: TransformableDataset,\n                     input_ids='token_input_ids'):\n        \"\"\"\n\n        Args:\n            data: Samples to be measured or path to dataset during training time.\n            dataset: During training time, use this dataset to measure the length of each sample inside.\n            input_ids: Field name corresponds to input ids.\n\n        Returns:\n\n            Length list of this samples\n\n        \"\"\"\n        if dataset.cache is None:\n            warnings.warn(f'Caching for the dataset is not enabled, '\n                          f'try `dataset.purge_cache()` if possible. The dataset is {dataset}.')\n        if isinstance(data, str):\n            timer = CountdownTimer(len(dataset))\n            for each in dataset:\n                timer.log('Preprocessing and caching samples [blink][yellow]...[/yellow][/blink]')\n            timer.erase()\n        return [len(x[input_ids]) for x in dataset]\n\n    def feed_batch(self,\n                   h: torch.FloatTensor,\n                   batch: Dict[str, torch.Tensor],\n                   mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        return decoder(h, batch=batch, mask=mask)\n\n    def input_is_flat(self, data) -> bool:\n        \"\"\"\n        Check whether the data is flat (meaning that it's only a single sample, not even batched).\n\n        Returns:\n            bool: ``True`` to indicate the input data is flat.\n        \"\"\"\n        raise NotImplementedError(\n            '`input_is_flat()` needs to be implemented for the task component to accept raw input from user.'\n        )\n\n    @abstractmethod\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        raise NotImplementedError()\n\n    # noinspection PyMethodMayBeStatic\n    def transform_batch(self,\n                        batch: Dict[str, Any],\n                        # inputs: List[List[str]],\n                        results: Dict[str, Any] = None,\n                        cls_is_bos=False,\n                        sep_is_eos=False) -> Dict[str, Any]:\n        \"\"\"\n        Let the task transform the batch before feeding the batch into its decoder. The default behavior is to\n        adjust the head and tail of tokens, according to ``cls_is_bos``, ``sep_is_eos`` passed in and the two\n        settings of the task itself.\n\n        Args:\n            batch: A batch of samples.\n            results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task\n                uses both token and pos as features, then it will need both tok and pos results to make a batch.\n            cls_is_bos: First token in this batch is BOS.\n            sep_is_eos: Last token in this batch is EOS.\n\n        Returns:\n            A batch.\n\n        \"\"\"\n        if cls_is_bos != self.cls_is_bos or sep_is_eos != self.sep_is_eos:\n            batch = copy(batch)\n            tokens = self._adjust_token(batch, cls_is_bos, sep_is_eos, 'token')\n            delta = len(tokens[0]) - len(batch['token'][0])\n            batch['token_length'] = batch['token_length'] + delta\n            batch['token'] = tokens\n            if 'token_' in batch:\n                if isinstance(batch['token_'][0], list):\n                    batch['token_'] = self._adjust_token(batch, cls_is_bos, sep_is_eos, 'token_')\n                else:\n                    batch['token_'] = tokens\n        return batch\n\n    def _adjust_token(self, batch, cls_is_bos, sep_is_eos, token_key):\n        tokens = []\n        for sent in batch[token_key]:\n            if cls_is_bos:\n                if not self.cls_is_bos:\n                    sent = sent[1:]\n            elif self.cls_is_bos:\n                sent = [BOS] + sent\n            if sep_is_eos:\n                if not self.sep_is_eos:\n                    sent = sent[:-1]\n            elif self.sep_is_eos:\n                sent = sent + [EOS]\n            tokens.append(sent)\n        return tokens\n\n    # noinspection PyMethodMayBeStatic\n    def build_samples(self, inputs, cls_is_bos=False, sep_is_eos=False):\n        \"\"\"\n        Build samples for this task. Called when this task is the first task. Default behaviour is to take inputs as\n        list of tokens and put these tokens into a dict per sample.\n\n        Args:\n            inputs: Inputs from users, usually a list of lists of tokens.\n            cls_is_bos: Insert BOS to the head of each sentence.\n            sep_is_eos: Append EOS to the tail of each sentence.\n\n        Returns:\n            List of samples.\n\n        \"\"\"\n        if cls_is_bos:\n            inputs = [[BOS] + x for x in inputs]\n        if sep_is_eos:\n            inputs = [x + [EOS] for x in inputs]\n        return [{'token': token} for token in inputs]\n\n    def build_tokenizer(self, tokenizer: TransformerSequenceTokenizer):\n        \"\"\"Build a transformer tokenizer for this task.\n\n        Args:\n            tokenizer: A tokenizer which is shared but can be adjusted to provide per-task settings.\n\n        Returns:\n            A TransformerSequenceTokenizer.\n\n        \"\"\"\n        if tokenizer.cls_is_bos != self.cls_is_bos or tokenizer.sep_is_eos != self.sep_is_eos:\n            tokenizer = copy(tokenizer)\n            tokenizer.cls_is_bos = self.cls_is_bos\n            tokenizer.sep_is_eos = self.sep_is_eos\n        return tokenizer\n\n    # noinspection PyMethodMayBeStatic\n    def finalize_document(self, doc: Document, task_name: str):\n        pass\n", "hanlp/components/mtl/tasks/ud.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-17 21:54\nimport logging\nfrom typing import Dict, Any, List, Union, Iterable, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp_common.document import Document\nfrom hanlp.common.transform import VocabDict, PunctuationMask\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp_common.conll import CoNLLUWord\nfrom hanlp.components.parsers.ud.ud_model import UniversalDependenciesDecoder\nfrom hanlp.components.parsers.ud.ud_parser import UniversalDependenciesParser\nfrom hanlp.components.parsers.ud.util import generate_lemma_rule, append_bos\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass UniversalDependenciesParsing(Task, UniversalDependenciesParser):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=None,\n                 separate_optimizer=False,\n                 cls_is_bos=True,\n                 sep_is_eos=False,\n                 n_mlp_arc=768,\n                 n_mlp_rel=256,\n                 mlp_dropout=.33,\n                 tree=False,\n                 proj=False,\n                 punct=False,\n                 max_seq_len=None,\n                 **kwargs) -> None:\n        r\"\"\"Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation\n        of \"75 Languages, 1 Model: Parsing Universal Dependencies Universally\" (:cite:`kondratyuk-straka-2019-75`).\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            n_mlp_arc: Number of features for arc representation.\n            n_mlp_rel: Number of features for rel representation.\n            mlp_dropout: Dropout applied to MLPs.\n            tree: ``True`` to enforce tree constraint.\n            proj: ``True`` for projective parsing.\n            punct: ``True`` to include punctuations in evaluation.\n            max_seq_len: Prune samples longer than this length. Useful for reducing GPU consumption.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def build_dataloader(self, data, transform: Callable = None, training=False, device=None,\n                         logger: logging.Logger = None, cache=False, gradient_accumulation=1, **kwargs) -> DataLoader:\n        _transform = [generate_lemma_rule, append_bos, self.vocabs, transform]\n        if isinstance(data, str) and not self.config.punct:\n            _transform.append(PunctuationMask('token', 'punct_mask'))\n        dataset = UniversalDependenciesParser.build_dataset(self, data, _transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            UniversalDependenciesParser.build_vocabs(self, dataset, logger, transformer=True)\n        max_seq_len = self.config.get('max_seq_len', None)\n        if max_seq_len and isinstance(data, str):\n            dataset.prune(lambda x: len(x['token_input_ids']) > max_seq_len, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset,\n            pad={'arc': 0})\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return output[0]['loss'] / 4  # we have 4 tasks\n\n    def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor, batch: Dict[str, Any], decoder: torch.nn.Module, **kwargs) -> Union[\n        Dict[str, Any], Any]:\n        return UniversalDependenciesParser.decode_output(self, *output, batch)\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        UniversalDependenciesParser.update_metrics(self, metric, batch, *output)\n\n    # noinspection PyMethodOverriding\n    def build_model(self,\n                    encoder_size,\n                    n_mlp_arc,\n                    n_mlp_rel,\n                    mlp_dropout,\n                    training=True,\n                    **kwargs) -> torch.nn.Module:\n        return UniversalDependenciesDecoder(\n            encoder_size,\n            n_mlp_arc,\n            n_mlp_rel,\n            mlp_dropout,\n            len(self.vocabs.rel),\n            len(self.vocabs.lemma),\n            len(self.vocabs.pos),\n            len(self.vocabs.feat),\n            0,\n            0\n        )\n\n    def build_metric(self, **kwargs):\n        return UniversalDependenciesParser.build_metric(self)\n\n    def input_is_flat(self, data) -> bool:\n        return UniversalDependenciesParser.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        yield from UniversalDependenciesParser.prediction_to_human(self, prediction, batch)\n\n    def feed_batch(self, h: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        mask = self.compute_mask(batch)\n        output_dict = decoder(h, batch, mask)\n        if decoder.training:\n            mask = mask.clone()\n        mask[:, 0] = 0\n        return output_dict, mask\n\n    def finalize_document(self, doc: Document, task_name: str):\n        lem = []\n        pos = []\n        feat = []\n        dep = []\n        for sent in doc[task_name]:\n            sent: List[CoNLLUWord] = sent\n            lem.append([x.lemma for x in sent])\n            pos.append([x.upos for x in sent])\n            feat.append([x.feats for x in sent])\n            dep.append([(x.head, x.deprel) for x in sent])\n        promoted = 0\n        if 'lem' not in doc:\n            doc['lem'] = lem\n            promoted += 1\n        if 'pos' not in doc:\n            doc['pos'] = pos\n            promoted += 1\n        if 'feat' not in doc:\n            doc['fea'] = feat\n            promoted += 1\n        if 'dep' not in doc:\n            doc['dep'] = dep\n            promoted += 1\n        if promoted == 4:\n            doc.pop(task_name)\n", "hanlp/components/mtl/tasks/lem.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-09 16:37\nimport logging\nfrom typing import Dict, Any, Union, Iterable, Callable, List\n\nimport torch\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.lemmatizer import TransformerLemmatizer\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\nfrom torch.utils.data import DataLoader\n\n\nclass LinearDecoder(torch.nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 num_labels) -> None:\n        super().__init__()\n        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        return self.classifier(contextualized_embeddings)\n\n\nclass TransformerLemmatization(Task, TransformerLemmatizer):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=1e-3,\n                 separate_optimizer=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 max_seq_len=None,\n                 sent_delimiter=None,\n                 char_level=False,\n                 hard_constraint=False,\n                 token_key='token', **kwargs) -> None:\n        \"\"\" Transition based lemmatization (:cite:`kondratyuk-straka-2019-75`).\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level, which is never the case for\n                lemmatization.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def build_dataloader(self,\n                         data: List[List[str]],\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        args = dict((k, self.config[k]) for k in\n                    ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)\n        dataset = self.build_dataset(data, cache=True, transform=transform, **args)\n        dataset.append_transform(self.vocabs)\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return TransformerLemmatizer.compute_loss(self, criterion, output, batch['tag_id'], batch['mask'])\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder,\n                      **kwargs) -> Union[Dict[str, Any], Any]:\n        return TransformerLemmatizer.decode_output(self, output, mask, batch, decoder)\n\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any],\n                       metric: Union[MetricDict, Metric]):\n        return TransformerLemmatizer.update_metrics(self, metric, output, batch['tag_id'], batch['mask'])\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return LinearDecoder(encoder_size, len(self.vocabs['tag']))\n\n    def build_metric(self, **kwargs):\n        return TransformerLemmatizer.build_metric(self, **kwargs)\n\n    def input_is_flat(self, data) -> bool:\n        return TransformerLemmatizer.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> Union[List, Dict]:\n        return TransformerLemmatizer.prediction_to_human(self, prediction, self.vocabs['tag'].idx_to_token, batch,\n                                                         token=batch['token'])\n", "hanlp/components/mtl/tasks/tok/reg_tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-02 16:51\nimport logging\nfrom typing import Union, List, Dict, Any, Iterable, Tuple\n\nimport torch\nfrom hanlp_common.util import merge_locals_kwargs\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\n\nimport hanlp.utils.torch_util\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import FieldLength, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.datasets.tokenization.loaders.txt import TextTokenizingDataset\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.layers.transformers.pt_imports import PreTrainedTokenizer\nfrom hanlp.metrics.chunking.binary_chunking_f1 import BinaryChunkingF1\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\n\n\ndef generate_token_span_tuple(sample: dict):\n    prefix_mask = sample.get('text_prefix_mask', None)\n    if prefix_mask:\n        sample['span_tuple'] = spans = []\n        previous_prefix = 0\n        prefix_mask_ = prefix_mask[1:-1]\n        for i, mask in enumerate(prefix_mask_):\n            if i and mask:\n                spans.append((previous_prefix, i))\n                previous_prefix = i\n        spans.append((previous_prefix, len(prefix_mask_)))\n    return sample\n\n\nclass RegressionTokenizingDecoder(torch.nn.Linear):\n\n    def __init__(self, in_features: int, out_features: int = 1, bias: bool = ...) -> None:\n        super().__init__(in_features, out_features, bias)\n\n    # noinspection PyMethodOverriding\n    def forward(self, input: Tensor, **kwargs) -> Tensor:\n        return super().forward(input[:, 1:-1, :]).squeeze_(-1)\n\n\nclass RegressionTokenization(Task):\n\n    def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None, scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=True, lr=1e-3, separate_optimizer=False, delimiter=None,\n                 max_seq_len=None, sent_delimiter=None) -> None:\n        super().__init__(**merge_locals_kwargs(locals()))\n\n    def build_criterion(self, **kwargs):\n        return torch.nn.BCEWithLogitsLoss(reduction='mean')\n\n    def build_metric(self, **kwargs):\n        return BinaryChunkingF1()\n\n    # noinspection PyMethodOverriding\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return RegressionTokenizingDecoder(encoder_size)\n\n    def predict(self, data: Union[str, List[str]], batch_size: int = None, **kwargs):\n        pass\n\n    def build_dataloader(self,\n                         data,\n                         transform: TransformList = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         tokenizer: PreTrainedTokenizer = None,\n                         **kwargs) -> DataLoader:\n        assert tokenizer\n        dataset = TextTokenizingDataset(data, cache=True, delimiter=self.config.sent_delimiter,\n                                        generate_idx=isinstance(data, list),\n                                        max_seq_len=self.config.max_seq_len,\n                                        sent_delimiter=self.config.sent_delimiter,\n                                        transform=[\n                                            TransformerSequenceTokenizer(tokenizer,\n                                                                         'text',\n                                                                         ret_prefix_mask=True,\n                                                                         ret_subtokens=True,\n                                                                         ),\n                                            FieldLength('text_input_ids', 'text_input_ids_length', delta=-2),\n                                            generate_token_span_tuple])\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset, 'text_input_ids'),\n                                                     shuffle=training),\n            device=device,\n            dataset=dataset)\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      batch: Dict[str, Any], **kwargs) -> List[Tuple[int, int]]:\n        spans = BinaryChunkingF1.decode_spans(output > 0, batch['text_input_ids_length'])\n        return spans\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: List[Tuple[int, int]], metric: BinaryChunkingF1):\n        metric.update(prediction, batch['span_tuple'])\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion):\n        mask = hanlp.utils.torch_util.lengths_to_mask(batch['text_input_ids_length'])\n        return criterion(output[mask], batch['text_prefix_mask'][:, 1:-1][mask].to(torch.float))\n", "hanlp/components/mtl/tasks/tok/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 16:34", "hanlp/components/mtl/tasks/tok/tag_tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-11 16:35\nimport logging\nfrom typing import Dict, Any, Union, Iterable, List, Set\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\nfrom hanlp_common.util import merge_locals_kwargs\nfrom hanlp_trie import DictInterface, TrieDict\n\n\nclass LinearCRFDecoder(torch.nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 num_labels,\n                 crf=False) -> None:\n        super().__init__()\n        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n        self.crf = CRF(num_labels) if crf else None\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        return self.classifier(contextualized_embeddings[:, 1:-1, :])\n\n\nclass TaggingTokenization(Task, TransformerTaggingTokenizer):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=1e-3, separate_optimizer=False,\n                 cls_is_bos=True,\n                 sep_is_eos=True,\n                 delimiter=None,\n                 max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False,\n                 transform=None,\n                 tagging_scheme='BMES',\n                 crf=False,\n                 token_key='token',\n                 dict_force: Union[DictInterface, Union[Dict[str, Any], Set[str]]] = None,\n                 dict_combine: Union[DictInterface, Union[Dict[str, Any], Set[str]]] = None,\n                 **kwargs) -> None:\n        \"\"\"Tokenization which casts a chunking problem into a tagging problem.\n        This task has to create batch of tokens containing both [CLS] and [SEP] since it's usually the first task\n        and later tasks might need them.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            delimiter: Delimiter used to split a line in the corpus.\n            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            transform: An optional transform to be applied to samples. Usually a character normalization transform is\n                passed in.\n            tagging_scheme: Either ``BMES`` or ``BI``.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs, excludes=(\n            'self', 'kwargs', '__class__', 'dict_force', 'dict_combine')))  # avoid to config\n        self.transform = transform\n        self.vocabs = VocabDict()\n        self.dict_force = dict_force\n        self.dict_combine = dict_combine\n\n    def build_dataloader(self, data, transform: TransformList = None, training=False, device=None,\n                         logger: logging.Logger = None, cache=False, gradient_accumulation=1, **kwargs) -> DataLoader:\n        args = dict((k, self.config[k]) for k in\n                    ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)\n        # We only need those transforms before TransformerTokenizer\n        transformer_index = transform.index_by_type(TransformerSequenceTokenizer)\n        assert transformer_index is not None\n        transform = transform[:transformer_index + 1]\n        if self.transform:\n            transform.insert(0, self.transform)\n        transform.append(self.last_transform())\n        dataset = self.build_dataset(data, cache=cache, transform=transform, **args)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset),\n                                                     shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return TransformerTaggingTokenizer.compute_loss(self, criterion, output, batch['tag_id'], batch['mask'])\n\n    def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor, batch: Dict[str, Any], decoder, **kwargs) -> Union[Dict[str, Any], Any]:\n        return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        TransformerTaggingTokenizer.update_metrics(self, metric, output, batch['tag_id'], None, batch, prediction)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return LinearCRFDecoder(encoder_size, len(self.vocabs['tag']), self.config.crf)\n\n    def build_metric(self, **kwargs):\n        return TransformerTaggingTokenizer.build_metric(self)\n\n    def build_criterion(self, model=None, **kwargs):\n        return TransformerTaggingTokenizer.build_criterion(self, model=model, reduction='mean')\n\n    def input_is_flat(self, data) -> bool:\n        return TransformerTaggingTokenizer.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> Union[List, Dict]:\n        return TransformerTaggingTokenizer.prediction_to_human(self, prediction, None, batch, rebuild_span=True)\n\n    def build_tokenizer(self, tokenizer: TransformerSequenceTokenizer):\n        # The transform for tokenizer needs very special settings, ensure these settings are set properly.\n        return TransformerSequenceTokenizer(\n            tokenizer.tokenizer,\n            tokenizer.input_key,\n            tokenizer.output_key,\n            tokenizer.max_seq_length,\n            tokenizer.truncate_long_sequences,\n            ret_subtokens=True,\n            ret_subtokens_group=True,\n            ret_token_span=True,\n            cls_is_bos=True,\n            sep_is_eos=True,\n            use_fast=tokenizer.tokenizer.is_fast,\n            dict_force=self.dict_force,\n            strip_cls_sep=False,\n        )\n\n    def build_samples(self, inputs, cls_is_bos=False, sep_is_eos=False):\n        return [{self.config.token_key: sent} for sent in inputs]\n\n    @property\n    def dict_force(self) -> DictInterface:\n        return TransformerTaggingTokenizer.dict_force.fget(self)\n\n    @dict_force.setter\n    def dict_force(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            dictionary = TrieDict(dictionary)\n        self.config.dict_force = dictionary\n\n    @property\n    def dict_combine(self) -> DictInterface:\n        return TransformerTaggingTokenizer.dict_combine.fget(self)\n\n    @dict_combine.setter\n    def dict_combine(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        # noinspection PyArgumentList\n        TransformerTaggingTokenizer.dict_combine.fset(self, dictionary)\n\n    def transform_batch(self, batch: Dict[str, Any], results: Dict[str, Any] = None, cls_is_bos=False,\n                        sep_is_eos=False) -> Dict[str, Any]:\n        \"\"\"\n        This method is overrode to honor the zero indexed token used in custom dict. Although for a tokenizer,\n        cls_is_bos = sep_is_eos = True, its tokens don't contain [CLS] or [SEP]. This behaviour is adopted from the\n        early versions and it is better kept to avoid migration efforts.\n\n\n        Args:\n            batch: A batch of samples.\n            results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task\n                uses both token and pos as features, then it will need both tok and pos results to make a batch.\n            cls_is_bos: First token in this batch is BOS.\n            sep_is_eos: Last token in this batch is EOS.\n\n        Returns:\n            A batch.\n\n        \"\"\"\n        return batch\n", "hanlp/components/mtl/tasks/srl/rank_srl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-05 15:43\nimport logging\nfrom typing import Union, List, Dict, Any, Iterable, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.srl.span_rank.span_rank import SpanRankingSemanticRoleLabeler\nfrom hanlp.components.srl.span_rank.span_ranking_srl_model import SpanRankingSRLDecoder\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass SpanRankingSemanticRoleLabeling(Task, SpanRankingSemanticRoleLabeler):\n\n    def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None, scalar_mix: ScalarMixWithDropoutBuilder = None, use_raw_hidden_states=False,\n                 lr=1e-3, separate_optimizer=False,\n                 lexical_dropout=0.5,\n                 dropout=0.2,\n                 span_width_feature_size=20,\n                 ffnn_size=150,\n                 ffnn_depth=2,\n                 argument_ratio=0.8,\n                 predicate_ratio=0.4,\n                 max_arg_width=30,\n                 mlp_label_size=100,\n                 enforce_srl_constraint=False,\n                 use_gold_predicates=False,\n                 doc_level_offset=True,\n                 use_biaffine=False,\n                 loss_reduction='mean',\n                 with_argument=' ',\n                 **kwargs) -> None:\n        r\"\"\" An implementation of \"Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling\"\n        (:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            lexical_dropout: Dropout applied to hidden states of encoder.\n            dropout: Dropout used for other layers except the encoder.\n            span_width_feature_size: Span width feature size.\n            ffnn_size: Feedforward size.\n            ffnn_depth: Number of layers of feedforward MLPs.\n            argument_ratio: Ratio of candidate arguments over number of tokens.\n            predicate_ratio: Ratio of candidate predicates over number of tokens.\n            max_arg_width: Maximum argument width.\n            mlp_label_size: Feature size for label representation.\n            enforce_srl_constraint: Enforce SRL constraints (number of core ARGs etc.).\n            use_gold_predicates: Use gold predicates instead of predicting them.\n            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.\n            use_biaffine: ``True`` to use biaffine (:cite:`dozat:17a`) instead of lineary layer for label prediction.\n            loss_reduction: The loss reduction used in aggregating losses.\n            with_argument: The delimiter between tokens in arguments to be used for joining tokens for outputs.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def build_dataloader(self, data, transform: Callable = None, training=False, device=None,\n                         logger: logging.Logger = None, gradient_accumulation=1, **kwargs) -> DataLoader:\n        dataset = self.build_dataset(data, isinstance(data, list), logger, transform)\n        dataset.purge_cache()\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,\n                                                     gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        return SpanRankingSemanticRoleLabeler.update_metrics(self, batch, {'prediction': prediction},\n                                                             tuple(metric.values()))\n\n    def decode_output(self,\n                      output: Dict[str, Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder, **kwargs) -> Union[Dict[str, Any], Any]:\n        return SpanRankingSemanticRoleLabeler.decode_output(self, output, batch)\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return output['loss']\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return SpanRankingSRLDecoder(encoder_size, len(self.vocabs.srl_label), self.config)\n\n    def build_metric(self, **kwargs):\n        predicate_f1, end_to_end_f1 = SpanRankingSemanticRoleLabeler.build_metric(self, **kwargs)\n        return MetricDict({'predicate': predicate_f1, 'e2e': end_to_end_f1})\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def input_is_flat(self, data) -> bool:\n        return SpanRankingSemanticRoleLabeler.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        return SpanRankingSemanticRoleLabeler.format_dict_to_results(batch['token'], prediction, exclusive_offset=True,\n                                                                     with_predicate=True,\n                                                                     with_argument=self.config.get('with_argument',\n                                                                                                   ' '),\n                                                                     label_first=True)\n", "hanlp/components/mtl/tasks/srl/bio_srl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-04 16:50\nimport logging\nfrom typing import Dict, Any, List, Union, Iterable, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import PadSequenceDataLoader, SamplerBuilder\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.srl.span_bio.baffine_tagging import BiaffineTaggingDecoder\nfrom hanlp.components.srl.span_bio.span_bio import SpanBIOSemanticRoleLabeler\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\nimport torch.nn.functional as F\n\n\nclass SpanBIOSemanticRoleLabeling(Task, SpanBIOSemanticRoleLabeler):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=None,\n                 separate_optimizer=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 crf=False,\n                 n_mlp_rel=300,\n                 mlp_dropout=0.2,\n                 loss_reduction='mean',\n                 doc_level_offset=True,\n                 **kwargs) -> None:\n        \"\"\"A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a\n        predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            cls_is_bos: ``True`` to treat the first token as ``BOS``.\n            sep_is_eos: ``True`` to treat the last token as ``EOS``.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            n_mlp_rel: Output size of MLPs for representing predicate and tokens.\n            mlp_dropout: Dropout applied to MLPs.\n            loss_reduction: Loss reduction for aggregating losses.\n            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def build_dataloader(self, data, transform: Callable = None, training=False, device=None,\n                         logger: logging.Logger = None, cache=False, gradient_accumulation=1, **kwargs) -> DataLoader:\n        dataset = self.build_dataset(data, transform=[transform, self.vocabs])\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            SpanBIOSemanticRoleLabeler.build_vocabs(self, dataset, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,\n                                                     gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        pred, mask = output\n        return SpanBIOSemanticRoleLabeler.compute_loss(self, criterion, pred, batch['srl_id'], mask)\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder: torch.nn.Module, **kwargs) -> Union[Dict[str, Any], Any]:\n        pred, mask = output\n        return SpanBIOSemanticRoleLabeler.decode_output(self, pred, mask, batch, decoder)\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        return SpanBIOSemanticRoleLabeler.update_metrics(self, metric, prediction, batch)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return BiaffineTaggingDecoder(\n            len(self.vocabs['srl']),\n            encoder_size,\n            self.config.n_mlp_rel,\n            self.config.mlp_dropout,\n            self.config.crf,\n        )\n\n    def feed_batch(self, h: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask: torch.BoolTensor,\n                   decoder: torch.nn.Module):\n        if not h.numel():  # No tokens, don't bother to run the decoder\n            return [], None\n        pred = decoder(h)\n        mask3d = self.compute_mask(mask)\n        if self.config.crf:\n            token_index = mask3d[0]\n            pred = pred.flatten(end_dim=1)[token_index]\n            pred = F.log_softmax(pred, dim=-1)\n        return pred, mask3d\n\n    def build_metric(self, **kwargs):\n        return SpanBIOSemanticRoleLabeler.build_metric(self)\n\n    def input_is_flat(self, data) -> bool:\n        return SpanBIOSemanticRoleLabeler.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: List, batch: Dict[str, Any]) -> List:\n        yield from SpanBIOSemanticRoleLabeler.prediction_to_result(self, prediction, batch)\n", "hanlp/components/mtl/tasks/srl/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-04 16:49\n", "hanlp/components/mtl/tasks/ner/tag_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-03 14:35\nimport logging\nfrom typing import Union, List, Dict, Any, Iterable, Callable, Set, Sequence\n\nimport torch\nfrom hanlp_trie import DictInterface\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.ner.transformer_ner import TransformerNamedEntityRecognizer\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass LinearCRFDecoder(torch.nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 num_labels,\n                 secondary_encoder=None,\n                 crf=False) -> None:\n        super().__init__()\n        self.secondary_encoder = secondary_encoder\n        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n        self.crf = CRF(num_labels) if crf else None\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        if self.secondary_encoder:\n            contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)\n        return self.classifier(contextualized_embeddings)\n\n\nclass TaggingNamedEntityRecognition(Task, TransformerNamedEntityRecognizer):\n\n    def __init__(self,\n                 trn: str = None,\n                 dev: str = None,\n                 tst: str = None,\n                 sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None,\n                 scalar_mix: ScalarMixWithDropoutBuilder = None,\n                 use_raw_hidden_states=False,\n                 lr=1e-3,\n                 separate_optimizer=False,\n                 max_seq_len=None,\n                 sent_delimiter=None,\n                 char_level=False,\n                 hard_constraint=False,\n                 tagging_scheme=None,\n                 crf=False,\n                 delimiter_in_entity=None,\n                 merge_types: List[str] = None,\n                 secondary_encoder=None,\n                 token_key='token',\n                 dict_whitelist: Union[DictInterface, Union[Dict[str, Any], Set[str]]] = None,\n                 dict_blacklist: Union[DictInterface, Union[Dict[str, Any], Set[str]]] = None,\n                 dict_tags: Union[\n                     DictInterface, Union[Dict[Union[str, Sequence[str]], Union[str, Sequence[str]]]]] = None,\n                 **kwargs) -> None:\n        r\"\"\"A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for\n        NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.\n        During decoding, it performs longest-prefix-matching of these words to override the prediction from\n        underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.\n\n        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can\n            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level, which is never the case for\n                lemmatization.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining\n                tokens during decoding.\n            merge_types: The types of consecutive entities to be merged.\n            secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden\n                states from the main encoder as input.\n            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.\n            dict_whitelist: A :class:`dict` or a :class:`~hanlp_trie.dictionary.DictInterface` of gazetteers to be\n                included into the final results.\n            dict_blacklist: A :class:`set` or a :class:`~hanlp_trie.dictionary.DictInterface` of badcases to be\n                excluded from the final results.\n            **kwargs:\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n        self.secondary_encoder = secondary_encoder\n        self.dict_whitelist = dict_whitelist\n        self.dict_blacklist = dict_blacklist\n        self.dict_tags = dict_tags\n\n    def build_dataloader(self,\n                         data,\n                         transform: Callable = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         cache=False,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        args = dict((k, self.config[k]) for k in\n                    ['delimiter', 'max_seq_len', 'sent_delimiter', 'char_level', 'hard_constraint'] if k in self.config)\n        dataset = self.build_dataset(data, cache=cache, transform=transform, **args)\n        dataset.append_transform(self.vocabs)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(\n                self.compute_lens(data, dataset),\n                shuffle=training, gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def compute_loss(self,\n                     batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                     criterion) -> Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return TransformerNamedEntityRecognizer.compute_loss(self, criterion, output, batch['tag_id'], batch['mask'])\n\n    def decode_output(self,\n                      output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder,\n                      **kwargs) -> Union[Dict[str, Any], Any]:\n        return TransformerNamedEntityRecognizer.decode_output(self, output, batch['mask'], batch, decoder)\n\n    def update_metrics(self,\n                       batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any],\n                       metric: Union[MetricDict, Metric]):\n        return TransformerNamedEntityRecognizer.update_metrics(self, metric, output, batch['tag_id'], batch['mask'],\n                                                               batch, prediction)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return LinearCRFDecoder(encoder_size, len(self.vocabs['tag']), self.secondary_encoder, self.config.crf)\n\n    def build_metric(self, **kwargs):\n        return TransformerNamedEntityRecognizer.build_metric(self, **kwargs)\n\n    def input_is_flat(self, data) -> bool:\n        return TransformerNamedEntityRecognizer.input_is_flat(self, data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> Union[List, Dict]:\n        return TransformerNamedEntityRecognizer.prediction_to_human(self, prediction, self.vocabs['tag'].idx_to_token,\n                                                                    batch)\n", "hanlp/components/mtl/tasks/ner/biaffine_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-05 01:49\nimport logging\nfrom copy import copy\nfrom typing import Dict, Any, Union, Iterable, List\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import SamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.transform import VocabDict, TransformList\nfrom hanlp.components.mtl.tasks import Task\nfrom hanlp.components.ner.biaffine_ner.biaffine_ner import BiaffineNamedEntityRecognizer\nfrom hanlp.components.ner.biaffine_ner.biaffine_ner_model import BiaffineNamedEntityRecognitionDecoder\nfrom hanlp.datasets.ner.loaders.json_ner import unpack_ner\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.metrics.metric import Metric\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass BiaffineNamedEntityRecognition(Task, BiaffineNamedEntityRecognizer):\n\n    def __init__(self, trn: str = None, dev: str = None, tst: str = None, sampler_builder: SamplerBuilder = None,\n                 dependencies: str = None, scalar_mix: ScalarMixWithDropoutBuilder = None, use_raw_hidden_states=False,\n                 lr=None, separate_optimizer=False,\n                 doc_level_offset=True, is_flat_ner=True, tagset=None, ret_tokens=' ',\n                 ffnn_size=150, loss_reduction='mean', **kwargs) -> None:\n        \"\"\"An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats\n        every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL\n        label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no\n        assumption about the spans, it naturally supports flat NER and nested NER.\n\n        Args:\n            trn: Path to training set.\n            dev: Path to dev set.\n            tst: Path to test set.\n            sampler_builder: A builder which builds a sampler.\n            dependencies: Its dependencies on other tasks.\n            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.\n            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.\n            lr: Learning rate for this task.\n            separate_optimizer: Use customized separate optimizer for this task.\n            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.\n            is_flat_ner: ``True`` for flat NER, otherwise nested NER.\n            tagset: Optional tagset to prune entities outside of this tagset from datasets.\n            ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.\n            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.\n            loss_reduction: The loss reduction used in aggregating losses.\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.vocabs = VocabDict()\n\n    def update_metrics(self, batch: Dict[str, Any],\n                       output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],\n                       prediction: Dict[str, Any], metric: Union[MetricDict, Metric]):\n        BiaffineNamedEntityRecognizer.update_metrics(self, batch, prediction, metric)\n\n    def decode_output(self,\n                      output: Dict[str, Any],\n                      mask: torch.BoolTensor,\n                      batch: Dict[str, Any],\n                      decoder,\n                      **kwargs) -> Union[Dict[str, Any], Any]:\n        return self.get_pred_ner(batch['token'], output['candidate_ner_scores'])\n\n    def compute_loss(self, batch: Dict[str, Any],\n                     output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any], criterion) -> \\\n            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]:\n        return output['loss']\n\n    def build_dataloader(self, data,\n                         transform: TransformList = None,\n                         training=False,\n                         device=None,\n                         logger: logging.Logger = None,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        transform = copy(transform)\n        transform.append(unpack_ner)\n        dataset = BiaffineNamedEntityRecognizer.build_dataset(self, data, self.vocabs, transform)\n        dataset.purge_cache()\n        if self.vocabs.mutable:\n            BiaffineNamedEntityRecognizer.build_vocabs(self, dataset, logger, self.vocabs)\n        return PadSequenceDataLoader(\n            batch_sampler=self.sampler_builder.build(self.compute_lens(data, dataset), shuffle=training,\n                                                     gradient_accumulation=gradient_accumulation),\n            device=device,\n            dataset=dataset)\n\n    def build_model(self, encoder_size, training=True, **kwargs) -> torch.nn.Module:\n        return BiaffineNamedEntityRecognitionDecoder(encoder_size, self.config.ffnn_size, len(self.vocabs.label),\n                                                     self.config.loss_reduction)\n\n    def build_metric(self, **kwargs):\n        return BiaffineNamedEntityRecognizer.build_metric(self, **kwargs)\n\n    def input_is_flat(self, data) -> bool:\n        return BiaffineNamedEntityRecognizer.input_is_flat(data)\n\n    def prediction_to_result(self, prediction: Dict[str, Any], batch: Dict[str, Any]) -> List:\n        results = []\n        BiaffineNamedEntityRecognizer.prediction_to_result(batch['token'], prediction, results,\n                                                           ret_tokens=self.config.get('ret_tokens', ' '))\n        return results\n", "hanlp/components/mtl/tasks/ner/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-03 14:34\n", "hanlp/components/srl/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-22 20:50", "hanlp/components/srl/span_rank/srl_eval_utils.py": "# Evaluation util functions for PropBank SRL.\n\nimport codecs\nimport collections\nimport operator\nimport tempfile\nfrom collections import Counter\n\nfrom hanlp.metrics.srl.srlconll import official_conll_05_evaluate\n\n_SRL_CONLL_EVAL_SCRIPT = \"../run_eval.sh\"\n\n\ndef split_example_for_eval(example):\n    \"\"\"Split document-based samples into sentence-based samples for evaluation.\n\n    Args:\n      example: \n\n    Returns:\n\n    \n    \"\"\"\n    sentences = example[\"sentences\"]\n    num_words = sum(len(s) for s in sentences)\n    word_offset = 0\n    samples = []\n    # assert len(sentences) == 1\n    for i, sentence in enumerate(sentences):\n        # assert i == 0  # For CoNLL-2005, there are always document == sentence.\n        srl_rels = {}\n        ner_spans = []  # Unused.\n        for r in example[\"srl\"][i]:\n            pred_id = r[0] - word_offset\n            if pred_id not in srl_rels:\n                srl_rels[pred_id] = []\n            srl_rels[pred_id].append((r[1] - word_offset, r[2] - word_offset, r[3]))\n        samples.append((sentence, srl_rels, ner_spans))\n        word_offset += len(sentence)\n    return samples\n\n\ndef evaluate_retrieval(span_starts, span_ends, span_scores, pred_starts, pred_ends, gold_spans,\n                       text_length, evaluators, debugging=False):\n    \"\"\"Evaluation for unlabeled retrieval.\n\n    Args:\n      gold_spans: Set of tuples of (start, end).\n      span_starts: \n      span_ends: \n      span_scores: \n      pred_starts: \n      pred_ends: \n      text_length: \n      evaluators: \n      debugging: (Default value = False)\n\n    Returns:\n\n    \n    \"\"\"\n    if len(span_starts) > 0:\n        sorted_starts, sorted_ends, sorted_scores = list(zip(*sorted(\n            zip(span_starts, span_ends, span_scores),\n            key=operator.itemgetter(2), reverse=True)))\n    else:\n        sorted_starts = []\n        sorted_ends = []\n    for k, evaluator in list(evaluators.items()):\n        if k == -3:\n            predicted_spans = set(zip(span_starts, span_ends)) & gold_spans\n        else:\n            if k == -2:\n                predicted_starts = pred_starts\n                predicted_ends = pred_ends\n                if debugging:\n                    print(\"Predicted\", list(zip(sorted_starts, sorted_ends, sorted_scores))[:len(gold_spans)])\n                    print(\"Gold\", gold_spans)\n            # FIXME: scalar index error\n            elif k == 0:\n                is_predicted = span_scores > 0\n                predicted_starts = span_starts[is_predicted]\n                predicted_ends = span_ends[is_predicted]\n            else:\n                if k == -1:\n                    num_predictions = len(gold_spans)\n                else:\n                    num_predictions = (k * text_length) / 100\n                predicted_starts = sorted_starts[:num_predictions]\n                predicted_ends = sorted_ends[:num_predictions]\n            predicted_spans = set(zip(predicted_starts, predicted_ends))\n        evaluator.update(gold_set=gold_spans, predicted_set=predicted_spans)\n\n\ndef _calc_f1(total_gold, total_predicted, total_matched, message=None):\n    precision = total_matched / total_predicted if total_predicted > 0 else 0\n    recall = total_matched / total_gold if total_gold > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    if message:\n        print((\"{}: Precision: {:.2%} Recall: {:.2%} F1: {:.2%}\".format(message, precision, recall, f1)))\n    return precision, recall, f1\n\n\ndef compute_span_f1(gold_data, predictions, task_name):\n    assert len(gold_data) == len(predictions)\n    total_gold = 0\n    total_predicted = 0\n    total_matched = 0\n    total_unlabeled_matched = 0\n    label_confusions = Counter()  # Counter of (gold, pred) label pairs.\n\n    for i in range(len(gold_data)):\n        gold = gold_data[i]\n        pred = predictions[i]\n        total_gold += len(gold)\n        total_predicted += len(pred)\n        for a0 in gold:\n            for a1 in pred:\n                if a0[0] == a1[0] and a0[1] == a1[1]:\n                    total_unlabeled_matched += 1\n                    label_confusions.update([(a0[2], a1[2]), ])\n                    if a0[2] == a1[2]:\n                        total_matched += 1\n    prec, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched, task_name)\n    ul_prec, ul_recall, ul_f1 = _calc_f1(total_gold, total_predicted, total_unlabeled_matched,\n                                         \"Unlabeled \" + task_name)\n    return prec, recall, f1, ul_prec, ul_recall, ul_f1, label_confusions\n\n\ndef compute_unlabeled_span_f1(gold_data, predictions, task_name):\n    assert len(gold_data) == len(predictions)\n    total_gold = 0\n    total_predicted = 0\n    total_matched = 0\n    total_unlabeled_matched = 0\n    label_confusions = Counter()  # Counter of (gold, pred) label pairs.\n\n    for i in range(len(gold_data)):\n        gold = gold_data[i]\n        pred = predictions[i]\n        total_gold += len(gold)\n        total_predicted += len(pred)\n        for a0 in gold:\n            for a1 in pred:\n                if a0[0] == a1[0] and a0[1] == a1[1]:\n                    total_unlabeled_matched += 1\n                    label_confusions.update([(a0[2], a1[2]), ])\n                    if a0[2] == a1[2]:\n                        total_matched += 1\n    prec, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched, task_name)\n    ul_prec, ul_recall, ul_f1 = _calc_f1(total_gold, total_predicted, total_unlabeled_matched,\n                                         \"Unlabeled \" + task_name)\n    return prec, recall, f1, ul_prec, ul_recall, ul_f1, label_confusions\n\n\nSRLScores = collections.namedtuple('SRLScores',\n                                   ['unlabeled_precision', 'unlabeled_recall', 'unlabeled_f1', 'precision', 'recall',\n                                    'f1', 'conll_precision', 'conll_recall', 'conll_f1', 'label_confusions',\n                                    'num_sents'])\n\n\ndef compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:\n    assert len(gold_srl) == len(predictions)\n    total_gold = 0\n    total_predicted = 0\n    total_matched = 0\n    total_unlabeled_matched = 0\n    num_sents = 0\n    label_confusions = Counter()\n\n    # Compute unofficial F1 of SRL relations.\n    for gold, prediction in zip(gold_srl, predictions):\n        gold_rels = 0\n        pred_rels = 0\n        matched = 0\n        for pred_id, gold_args in gold.items():\n            filtered_gold_args = [a for a in gold_args if a[2] not in [\"V\", \"C-V\"]]\n            total_gold += len(filtered_gold_args)\n            gold_rels += len(filtered_gold_args)\n            if pred_id not in prediction:\n                continue\n            for a0 in filtered_gold_args:\n                for a1 in prediction[pred_id]:\n                    if a0[0] == a1[0] and a0[1] == a1[1]:\n                        total_unlabeled_matched += 1\n                        label_confusions.update([(a0[2], a1[2]), ])\n                        if a0[2] == a1[2]:\n                            total_matched += 1\n                            matched += 1\n        for pred_id, args in prediction.items():\n            filtered_args = [a for a in args if a[2] not in [\"V\"]]  # \"C-V\"]]\n            total_predicted += len(filtered_args)\n            pred_rels += len(filtered_args)\n\n        if gold_rels == matched and pred_rels == matched:\n            num_sents += 1\n\n    precision, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched,\n                                     # \"SRL (unofficial)\"\n                                     )\n    unlabeled_precision, unlabeled_recall, unlabeled_f1 = _calc_f1(total_gold, total_predicted,\n                                                                   total_unlabeled_matched,\n                                                                   # \"Unlabeled SRL (unofficial)\"\n                                                                   )\n\n    # Prepare to compute official F1.\n    if not gold_path:\n        # print(\"No gold conll_eval data provided. Recreating ...\")\n        gold_path = tempfile.NamedTemporaryFile().name\n        print_to_conll(sentences, gold_srl, gold_path, None)\n        gold_predicates = None\n    else:\n        gold_predicates = read_gold_predicates(gold_path)\n\n    temp_output = tempfile.NamedTemporaryFile().name\n    # print((\"Output temp outoput {}\".format(temp_output)))\n    print_to_conll(sentences, predictions, temp_output, gold_predicates)\n\n    # Evaluate twice with official script.\n    conll_precision, conll_recall, conll_f1 = official_conll_05_evaluate(temp_output, gold_path)\n    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision,\n                     conll_recall, conll_f1, label_confusions, num_sents)\n\n\ndef print_sentence_to_conll(fout, tokens, labels):\n    \"\"\"Print a labeled sentence into CoNLL format.\n\n    Args:\n      fout: \n      tokens: \n      labels: \n\n    Returns:\n\n    \n    \"\"\"\n    for label_column in labels:\n        assert len(label_column) == len(tokens)\n    for i in range(len(tokens)):\n        fout.write(tokens[i].ljust(15))\n        for label_column in labels:\n            fout.write(label_column[i].rjust(15))\n        fout.write(\"\\n\")\n    fout.write(\"\\n\")\n\n\ndef read_gold_predicates(gold_path):\n    print(\"gold path\", gold_path)\n    fin = codecs.open(gold_path, \"r\", \"utf-8\")\n    gold_predicates = [[], ]\n    for line in fin:\n        line = line.strip()\n        if not line:\n            gold_predicates.append([])\n        else:\n            info = line.split()\n            gold_predicates[-1].append(info[0])\n    fin.close()\n    return gold_predicates\n\n\ndef print_to_conll(sentences, srl_labels, output_filename, gold_predicates=None):\n    fout = codecs.open(output_filename, \"w\", \"utf-8\")\n    for sent_id, words in enumerate(sentences):\n        if gold_predicates:\n            assert len(gold_predicates[sent_id]) == len(words)\n        pred_to_args = srl_labels[sent_id]\n        props = [\"-\" for _ in words]\n        col_labels = [[\"*\" for _ in words] for _ in range(len(pred_to_args))]\n        for i, pred_id in enumerate(sorted(pred_to_args.keys())):\n            # To make sure CoNLL-eval script count matching predicates as correct.\n            if gold_predicates and gold_predicates[sent_id][pred_id] != \"-\":\n                props[pred_id] = gold_predicates[sent_id][pred_id]\n            else:\n                props[pred_id] = \"P\" + words[pred_id]\n            flags = [False for _ in words]\n            for start, end, label in pred_to_args[pred_id]:\n                if not max(flags[start:end + 1]):\n                    col_labels[i][start] = \"(\" + label + col_labels[i][start]\n                    col_labels[i][end] = col_labels[i][end] + \")\"\n                    for j in range(start, end + 1):\n                        flags[j] = True\n            # Add unpredicted verb (for predicted SRL).\n            if not flags[pred_id]:  # if the predicate id is False\n                col_labels[i][pred_id] = \"(V*)\"\n        print_sentence_to_conll(fout, props, col_labels)\n    fout.close()\n", "hanlp/components/srl/span_rank/span_ranking_srl_model.py": "from typing import Dict\n\nimport hanlp.utils.torch_util\nfrom hanlp.layers.feedforward import FeedForward\nfrom hanlp.layers.time_distributed import TimeDistributed\n\nfrom .highway_variational_lstm import *\nimport torch\n\nfrom ...parsers.biaffine.biaffine import Biaffine\n\n\ndef initializer_1d(input_tensor, initializer):\n    assert len(input_tensor.size()) == 1\n    input_tensor = input_tensor.view(-1, 1)\n    input_tensor = initializer(input_tensor)\n    return input_tensor.view(-1)\n\n\nclass SpanRankingSRLDecoder(nn.Module):\n\n    def __init__(self, context_layer_output_dim, label_space_size, config) -> None:\n        super().__init__()\n        self.config = config\n        self.label_space_size = label_space_size\n        self.dropout = float(config.dropout)\n        self.use_gold_predicates = config.use_gold_predicates\n        # span width feature embedding\n        self.span_width_embedding = nn.Embedding(self.config.max_arg_width, self.config.span_width_feature_size)\n        # self.context_projective_layer = nn.Linear(2 * self.lstm_hidden_size, self.config.num_attention_heads)\n        # span scores\n        self.span_emb_size = 3 * context_layer_output_dim + self.config.span_width_feature_size\n        self.arg_unary_score_layers = nn.ModuleList([nn.Linear(self.span_emb_size, self.config.ffnn_size) if i == 0\n                                                     else nn.Linear(self.config.ffnn_size, self.config.ffnn_size) for i\n                                                     in range(self.config.ffnn_depth)])  # [,150]\n        self.arg_dropout_layers = nn.ModuleList([nn.Dropout(self.dropout) for _ in range(self.config.ffnn_depth)])\n        self.arg_unary_score_projection = nn.Linear(self.config.ffnn_size, 1)\n        # predicate scores\n        self.pred_unary_score_layers = nn.ModuleList(\n            [nn.Linear(context_layer_output_dim, self.config.ffnn_size) if i == 0\n             else nn.Linear(self.config.ffnn_size, self.config.ffnn_size) for i\n             in range(self.config.ffnn_depth)])  # [,150]\n        self.pred_dropout_layers = nn.ModuleList([nn.Dropout(self.dropout) for _ in range(self.config.ffnn_depth)])\n        self.pred_unary_score_projection = nn.Linear(self.config.ffnn_size, 1)\n        # srl scores\n        self.srl_unary_score_input_size = self.span_emb_size + context_layer_output_dim\n        self.srl_unary_score_layers = nn.ModuleList([nn.Linear(self.srl_unary_score_input_size, self.config.ffnn_size)\n                                                     if i == 0 else nn.Linear(self.config.ffnn_size,\n                                                                              self.config.ffnn_size)\n                                                     for i in range(self.config.ffnn_depth)])\n        self.srl_dropout_layers = nn.ModuleList([nn.Dropout(self.dropout) for _ in range(self.config.ffnn_depth)])\n        self.srl_unary_score_projection = nn.Linear(self.config.ffnn_size, self.label_space_size - 1)\n        if config.use_biaffine:\n            self.predicate_scale = TimeDistributed(FeedForward(context_layer_output_dim, 1, self.span_emb_size, 'ReLU'))\n            self.biaffine = Biaffine(self.span_emb_size, self.label_space_size - 1)\n        self.loss_reduction = config.loss_reduction\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.xavier_uniform_(self.span_width_embedding.weight)\n        # init.xavier_uniform_(self.context_projective_layer.weight)\n        # initializer_1d(self.context_projective_layer.bias, init.xavier_uniform_)\n\n        for layer in self.arg_unary_score_layers:\n            init.xavier_uniform_(layer.weight)\n            initializer_1d(layer.bias, init.xavier_uniform_)\n        init.xavier_uniform_(self.arg_unary_score_projection.weight)\n        initializer_1d(self.arg_unary_score_projection.bias, init.xavier_uniform_)\n\n        for layer in self.pred_unary_score_layers:\n            init.xavier_uniform_(layer.weight)\n            initializer_1d(layer.bias, init.xavier_uniform_)\n        init.xavier_uniform_(self.pred_unary_score_projection.weight)\n        initializer_1d(self.pred_unary_score_projection.bias, init.xavier_uniform_)\n\n        for layer in self.srl_unary_score_layers:\n            init.xavier_uniform_(layer.weight)\n            initializer_1d(layer.bias, init.xavier_uniform_)\n        init.xavier_uniform_(self.srl_unary_score_projection.weight)\n        initializer_1d(self.srl_unary_score_projection.bias, init.xavier_uniform_)\n        return None\n\n    def forward(self, hidden_states, batch, mask=None):\n        gold_arg_ends, gold_arg_labels, gold_arg_starts, gold_predicates, masks, sent_lengths = SpanRankingSRLModel.unpack(\n            batch, mask=mask, training=self.training)\n        return self.decode(hidden_states, sent_lengths, masks, gold_arg_starts, gold_arg_ends, gold_arg_labels,\n                           gold_predicates)\n\n    @staticmethod\n    def get_candidate_spans(sent_lengths: torch.Tensor, max_sent_length, max_arg_width):\n        num_sentences = len(sent_lengths)\n        device = sent_lengths.device\n        candidate_starts = torch.arange(0, max_sent_length, device=device).expand(num_sentences, max_arg_width, -1)\n        candidate_width = torch.arange(0, max_arg_width, device=device).view(1, -1, 1)\n        candidate_ends = candidate_starts + candidate_width\n\n        candidate_starts = candidate_starts.contiguous().view(num_sentences, max_sent_length * max_arg_width)\n        candidate_ends = candidate_ends.contiguous().view(num_sentences, max_sent_length * max_arg_width)\n        actual_sent_lengths = sent_lengths.view(-1, 1).expand(-1, max_sent_length * max_arg_width)\n        candidate_mask = candidate_ends < actual_sent_lengths\n\n        candidate_starts = candidate_starts * candidate_mask\n        candidate_ends = candidate_ends * candidate_mask\n        return candidate_starts, candidate_ends, candidate_mask\n\n    @staticmethod\n    def exclusive_cumsum(input: torch.Tensor, exclusive=True):\n        \"\"\"\n\n        Args:\n          input: input is the sentence lengths tensor.\n          exclusive: exclude the last sentence length (Default value = True)\n          input(torch.Tensor :): \n          input: torch.Tensor: \n\n        Returns:\n\n        \n        \"\"\"\n        assert exclusive is True\n        if exclusive is True:\n            exclusive_sent_lengths = input.new_zeros(1, dtype=torch.long)\n            result = torch.cumsum(torch.cat([exclusive_sent_lengths, input], 0)[:-1], 0).view(-1, 1)\n        else:\n            result = torch.cumsum(input, 0).view(-1, 1)\n        return result\n\n    def flatten_emb(self, emb):\n        num_sentences, max_sentence_length = emb.size()[0], emb.size()[1]\n        assert len(emb.size()) == 3\n        flatted_emb = emb.contiguous().view(num_sentences * max_sentence_length, -1)\n        return flatted_emb\n\n    def flatten_emb_in_sentence(self, emb, batch_sentences_mask):\n        num_sentences, max_sentence_length = emb.size()[0], emb.size()[1]\n        flatted_emb = self.flatten_emb(emb)\n        return flatted_emb[batch_sentences_mask.reshape(num_sentences * max_sentence_length)]\n\n    def get_span_emb(self, flatted_context_emb, flatted_candidate_starts, flatted_candidate_ends,\n                     config, dropout=0.0):\n        batch_word_num = flatted_context_emb.size()[0]\n        # gather slices from embeddings according to indices\n        span_start_emb = flatted_context_emb[flatted_candidate_starts]\n        span_end_emb = flatted_context_emb[flatted_candidate_ends]\n        span_emb_feature_list = [span_start_emb, span_end_emb]  # store the span vector representations for span rep.\n\n        span_width = 1 + flatted_candidate_ends - flatted_candidate_starts  # [num_spans], generate the span width\n        max_arg_width = config.max_arg_width\n\n        # get the span width feature emb\n        span_width_index = span_width - 1\n        span_width_emb = self.span_width_embedding(span_width_index)\n        span_width_emb = F.dropout(span_width_emb, dropout, self.training)\n        span_emb_feature_list.append(span_width_emb)\n\n        \"\"\"head features\"\"\"\n        cpu_flatted_candidte_starts = flatted_candidate_starts\n        span_indices = torch.arange(0, max_arg_width, device=flatted_context_emb.device).view(1, -1) + \\\n                       cpu_flatted_candidte_starts.view(-1, 1)  # For all the i, where i in [begin, ..i, end] for span\n        # reset the position index to the batch_word_num index with index - 1\n        span_indices = torch.clamp(span_indices, max=batch_word_num - 1)\n        num_spans, spans_width = span_indices.size()[0], span_indices.size()[1]\n        flatted_span_indices = span_indices.view(-1)  # so Huge!!!, column is the span?\n        # if torch.cuda.is_available():\n        flatted_span_indices = flatted_span_indices\n        span_text_emb = flatted_context_emb.index_select(0, flatted_span_indices).view(num_spans, spans_width, -1)\n        span_indices_mask = hanlp.utils.torch_util.lengths_to_mask(span_width, max_len=max_arg_width)\n        # project context output to num head\n        # head_scores = self.context_projective_layer.forward(flatted_context_emb)\n        # get span attention\n        # span_attention = head_scores.index_select(0, flatted_span_indices).view(num_spans, spans_width)\n        # span_attention = torch.add(span_attention, expanded_span_indices_log_mask).unsqueeze(2)  # control the span len\n        # span_attention = F.softmax(span_attention, dim=1)\n        span_text_emb = span_text_emb * span_indices_mask.unsqueeze(2).expand(-1, -1, span_text_emb.size()[-1])\n        span_head_emb = torch.mean(span_text_emb, 1)\n        span_emb_feature_list.append(span_head_emb)\n\n        span_emb = torch.cat(span_emb_feature_list, 1)\n        return span_emb, None, span_text_emb, span_indices, span_indices_mask\n\n    def get_arg_unary_scores(self, span_emb):\n        \"\"\"Compute span score with FFNN(span embedding)\n\n        Args:\n          span_emb: tensor of [num_sentences, num_spans, emb_size]\n          config: param dropout:\n          num_labels: param name:\n\n        Returns:\n\n        \n        \"\"\"\n        input = span_emb\n        for i, ffnn in enumerate(self.arg_unary_score_layers):\n            input = F.relu(ffnn.forward(input))\n            input = self.arg_dropout_layers[i].forward(input)\n        output = self.arg_unary_score_projection.forward(input)\n        return output\n\n    def get_pred_unary_scores(self, span_emb):\n        input = span_emb\n        for i, ffnn in enumerate(self.pred_unary_score_layers):\n            input = F.relu(ffnn.forward(input))\n            input = self.pred_dropout_layers[i].forward(input)\n        output = self.pred_unary_score_projection.forward(input)\n        return output\n\n    def extract_spans(self, candidate_scores, candidate_starts, candidate_ends, topk, max_sentence_length,\n                      sort_spans, enforce_non_crossing):\n        \"\"\"extract the topk span indices\n\n        Args:\n          candidate_scores: param candidate_starts:\n          candidate_ends: param topk: [num_sentences]\n          max_sentence_length: param sort_spans:\n          enforce_non_crossing: return: indices [num_sentences, max_num_predictions]\n          candidate_starts: \n          topk: \n          sort_spans: \n\n        Returns:\n\n        \n        \"\"\"\n        # num_sentences = candidate_scores.size()[0]\n        # num_input_spans = candidate_scores.size()[1]\n        max_num_output_spans = int(torch.max(topk))\n        indices = [score.topk(k)[1] for score, k in zip(candidate_scores, topk)]\n        output_span_indices_tensor = [F.pad(item, [0, max_num_output_spans - item.size()[0]], value=item[-1])\n                                      for item in indices]\n        output_span_indices_tensor = torch.stack(output_span_indices_tensor)\n        return output_span_indices_tensor\n\n    def batch_index_select(self, emb, indices):\n        num_sentences = emb.size()[0]\n        max_sent_length = emb.size()[1]\n        flatten_emb = self.flatten_emb(emb)\n        offset = (torch.arange(0, num_sentences, device=emb.device) * max_sent_length).unsqueeze(1)\n        return torch.index_select(flatten_emb, 0, (indices + offset).view(-1)) \\\n            .view(indices.size()[0], indices.size()[1], emb.size(-1))\n\n    def get_batch_topk(self, candidate_starts: torch.Tensor, candidate_ends, candidate_scores, topk_ratio, text_len,\n                       max_sentence_length, sort_spans=False, enforce_non_crossing=True):\n        num_sentences = candidate_starts.size()[0]\n        max_sentence_length = candidate_starts.size()[1]\n\n        topk = torch.floor(text_len.to(torch.float) * topk_ratio).to(torch.long)\n        topk = torch.max(topk, torch.ones(num_sentences, device=candidate_starts.device, dtype=torch.long))\n\n        # this part should be implemented with C++\n        predicted_indices = self.extract_spans(candidate_scores, candidate_starts, candidate_ends, topk,\n                                               max_sentence_length, sort_spans, enforce_non_crossing)\n        predicted_starts = torch.gather(candidate_starts, 1, predicted_indices)\n        predicted_ends = torch.gather(candidate_ends, 1, predicted_indices)\n        predicted_scores = torch.gather(candidate_scores, 1, predicted_indices)\n        return predicted_starts, predicted_ends, predicted_scores, topk, predicted_indices\n\n    def get_dense_span_labels(self, span_starts, span_ends, span_labels, max_sentence_length,\n                              span_parents=None):\n        num_sentences = span_starts.size()[0]\n        max_spans_num = span_starts.size()[1]\n\n        # span_starts = span_starts + 1 - (span_labels > 0).to(torch.long)\n        span_starts[(span_labels == 0) & (span_starts < max_sentence_length - 1)] += 1  # make start > end\n        sentence_indices = torch.arange(0, num_sentences, device=span_starts.device).unsqueeze(1).expand(-1,\n                                                                                                         max_spans_num)\n\n        sparse_indices = torch.cat([sentence_indices.unsqueeze(2), span_starts.unsqueeze(2), span_ends.unsqueeze(2)],\n                                   dim=2)\n        if span_parents is not None:  # semantic span predicate offset\n            sparse_indices = torch.cat([sparse_indices, span_parents.unsqueeze(2)], 2)\n\n        rank = 3 if span_parents is None else 4\n        dense_labels = torch.sparse.LongTensor(sparse_indices.view(num_sentences * max_spans_num, rank).t(),\n                                               span_labels.view(-1),\n                                               torch.Size([num_sentences] + [max_sentence_length] * (rank - 1))) \\\n            .to_dense()\n        return dense_labels\n\n    @staticmethod\n    def gather_4d(params, indices):\n        assert len(params.size()) == 4 and len(indices) == 4\n        indices_a, indices_b, indices_c, indices_d = indices\n        result = params[indices_a, indices_b, indices_c, indices_d]\n        return result\n\n    def get_srl_labels(self,\n                       arg_starts,\n                       arg_ends,\n                       predicates,\n                       gold_predicates,\n                       gold_arg_starts,\n                       gold_arg_ends,\n                       gold_arg_labels,\n                       max_sentence_length\n                       ):\n        num_sentences = arg_starts.size()[0]\n        max_arg_num = arg_starts.size()[1]\n        max_pred_num = predicates.size()[1]\n\n        sentence_indices_2d = torch.arange(0, num_sentences, device=arg_starts.device).unsqueeze(1).unsqueeze(2).expand(\n            -1, max_arg_num, max_pred_num)\n        expanded_arg_starts = arg_starts.unsqueeze(2).expand(-1, -1, max_pred_num)\n        expanded_arg_ends = arg_ends.unsqueeze(2).expand(-1, -1, max_pred_num)\n        expanded_predicates = predicates.unsqueeze(1).expand(-1, max_arg_num, -1)\n\n        dense_srl_labels = self.get_dense_span_labels(gold_arg_starts,\n                                                      gold_arg_ends,\n                                                      gold_arg_labels,\n                                                      max_sentence_length, span_parents=gold_predicates)  # ans\n        srl_labels = self.gather_4d(dense_srl_labels,\n                                    [sentence_indices_2d, expanded_arg_starts, expanded_arg_ends, expanded_predicates])\n        return srl_labels\n\n    def get_srl_unary_scores(self, span_emb):\n        input = span_emb\n        for i, ffnn in enumerate(self.srl_unary_score_layers):\n            input = F.relu(ffnn.forward(input))\n            input = self.srl_dropout_layers[i].forward(input)\n        output = self.srl_unary_score_projection.forward(input)\n        return output\n\n    def get_srl_scores(self, arg_emb, pred_emb, arg_scores, pred_scores, num_labels, config, dropout):\n        num_sentences = arg_emb.size()[0]\n        num_args = arg_emb.size()[1]  # [batch_size, max_arg_num, arg_emb_size]\n        num_preds = pred_emb.size()[1]  # [batch_size, max_pred_num, pred_emb_size]\n\n        unsqueezed_arg_emb = arg_emb.unsqueeze(2)\n        unsqueezed_pred_emb = pred_emb.unsqueeze(1)\n        expanded_arg_emb = unsqueezed_arg_emb.expand(-1, -1, num_preds, -1)\n        expanded_pred_emb = unsqueezed_pred_emb.expand(-1, num_args, -1, -1)\n        pair_emb_list = [expanded_arg_emb, expanded_pred_emb]\n        pair_emb = torch.cat(pair_emb_list, 3)  # concatenate the argument emb and pre emb\n        pair_emb_size = pair_emb.size()[3]\n        flat_pair_emb = pair_emb.view(num_sentences * num_args * num_preds, pair_emb_size)\n        # get unary scores\n        flat_srl_scores = self.get_srl_unary_scores(flat_pair_emb)\n        srl_scores = flat_srl_scores.view(num_sentences, num_args, num_preds, flat_srl_scores.size(-1))\n        if self.config.use_biaffine:\n            srl_scores += self.biaffine(arg_emb, self.predicate_scale(pred_emb)).permute([0, 2, 3, 1])\n        unsqueezed_arg_scores, unsqueezed_pred_scores = \\\n            arg_scores.unsqueeze(2).unsqueeze(3), pred_scores.unsqueeze(1).unsqueeze(3)\n        srl_scores = srl_scores + unsqueezed_arg_scores + unsqueezed_pred_scores\n        dummy_scores = torch.zeros([num_sentences, num_args, num_preds, 1], device=arg_emb.device)\n        srl_scores = torch.cat([dummy_scores, srl_scores], 3)\n        return srl_scores\n\n    def get_srl_softmax_loss(self, srl_scores, srl_labels, num_predicted_args, num_predicted_preds):\n        srl_loss_mask = self.get_srl_loss_mask(srl_scores, num_predicted_args, num_predicted_preds)\n\n        loss = torch.nn.functional.cross_entropy(srl_scores[srl_loss_mask], srl_labels[srl_loss_mask],\n                                                 reduction=self.loss_reduction)\n        return loss, srl_loss_mask\n\n    def get_srl_loss_mask(self, srl_scores, num_predicted_args, num_predicted_preds):\n        max_num_arg = srl_scores.size()[1]\n        max_num_pred = srl_scores.size()[2]\n        # num_predicted_args, 1D tensor; max_num_arg: a int variable means the gold ans's max arg number\n        args_mask = hanlp.utils.torch_util.lengths_to_mask(num_predicted_args, max_num_arg)\n        pred_mask = hanlp.utils.torch_util.lengths_to_mask(num_predicted_preds, max_num_pred)\n        srl_loss_mask = args_mask.unsqueeze(2) & pred_mask.unsqueeze(1)\n        return srl_loss_mask\n\n    def decode(self, contextualized_embeddings, sent_lengths, masks, gold_arg_starts, gold_arg_ends, gold_arg_labels,\n               gold_predicates):\n        num_sentences, max_sent_length = masks.size()\n        device = sent_lengths.device\n        \"\"\"generate candidate spans with argument pruning\"\"\"\n        # candidate_starts [num_sentences, max_sent_length * max_arg_width]\n        candidate_starts, candidate_ends, candidate_mask = self.get_candidate_spans(\n            sent_lengths, max_sent_length, self.config.max_arg_width)\n        flatted_candidate_mask = candidate_mask.view(-1)\n        batch_word_offset = self.exclusive_cumsum(sent_lengths)  # get the word offset in a batch\n        # choose the flatted_candidate_starts with the actual existing positions, i.e. exclude the illegal starts\n        flatted_candidate_starts = candidate_starts + batch_word_offset\n        flatted_candidate_starts = flatted_candidate_starts.view(-1)[flatted_candidate_mask].to(torch.long)\n        flatted_candidate_ends = candidate_ends + batch_word_offset\n        flatted_candidate_ends = flatted_candidate_ends.view(-1)[flatted_candidate_mask].to(torch.long)\n        # flatten the lstm output according to the sentence mask, i.e. exclude the illegal (padding) lstm output\n        flatted_context_output = self.flatten_emb_in_sentence(contextualized_embeddings, masks)\n        \"\"\"generate the span embedding\"\"\"\n        candidate_span_emb, head_scores, span_head_emb, head_indices, head_indices_log_mask = self.get_span_emb(\n            flatted_context_output, flatted_candidate_starts, flatted_candidate_ends,\n            self.config, dropout=self.dropout)\n        \"\"\"Get the span ids\"\"\"\n        candidate_span_number = candidate_span_emb.size()[0]\n        max_candidate_spans_num_per_sentence = candidate_mask.size()[1]\n        sparse_indices = candidate_mask.nonzero(as_tuple=False)\n        sparse_values = torch.arange(0, candidate_span_number, device=device)\n        candidate_span_ids = torch.sparse.FloatTensor(sparse_indices.t(), sparse_values,\n                                                      torch.Size([num_sentences,\n                                                                  max_candidate_spans_num_per_sentence])).to_dense()\n        spans_log_mask = torch.log(candidate_mask.to(torch.float))\n        predict_dict = {\"candidate_starts\": candidate_starts, \"candidate_ends\": candidate_ends,\n                        \"head_scores\": head_scores}\n        \"\"\"Get unary scores and topk of candidate argument spans.\"\"\"\n        flatted_candidate_arg_scores = self.get_arg_unary_scores(candidate_span_emb)\n        candidate_arg_scores = flatted_candidate_arg_scores.index_select(0, candidate_span_ids.view(-1)) \\\n            .view(candidate_span_ids.size()[0], candidate_span_ids.size()[1])\n        candidate_arg_scores = candidate_arg_scores + spans_log_mask\n        arg_starts, arg_ends, arg_scores, num_args, top_arg_indices = \\\n            self.get_batch_topk(candidate_starts, candidate_ends, candidate_arg_scores,\n                                self.config.argument_ratio, sent_lengths, max_sent_length,\n                                sort_spans=False, enforce_non_crossing=False)\n        \"\"\"Get the candidate predicate\"\"\"\n        candidate_pred_ids = torch.arange(0, max_sent_length, device=device).unsqueeze(0).expand(num_sentences, -1)\n        candidate_pred_emb = contextualized_embeddings\n        candidate_pred_scores = self.get_pred_unary_scores(candidate_pred_emb)\n        candidate_pred_scores = candidate_pred_scores + torch.log(masks.to(torch.float).unsqueeze(2))\n        candidate_pred_scores = candidate_pred_scores.squeeze(2)\n        if self.use_gold_predicates is True:\n            predicates = gold_predicates\n            num_preds = (gold_arg_labels > 0).sum(dim=-1)\n            pred_scores = torch.zeros_like(predicates)\n            top_pred_indices = predicates\n        else:\n            predicates, _, pred_scores, num_preds, top_pred_indices = self.get_batch_topk(\n                candidate_pred_ids, candidate_pred_ids, candidate_pred_scores, self.config.predicate_ratio,\n                sent_lengths, max_sent_length,\n                sort_spans=False, enforce_non_crossing=False)\n        \"\"\"Get top arg embeddings\"\"\"\n        arg_span_indices = torch.gather(candidate_span_ids, 1, top_arg_indices)  # [num_sentences, max_num_args]\n        arg_emb = candidate_span_emb.index_select(0, arg_span_indices.view(-1)).view(\n            arg_span_indices.size()[0], arg_span_indices.size()[1], -1\n        )  # [num_sentences, max_num_args, emb]\n        \"\"\"Get top predicate embeddings\"\"\"\n        pred_emb = self.batch_index_select(candidate_pred_emb,\n                                           top_pred_indices)  # [num_sentences, max_num_preds, emb]\n        \"\"\"Get the srl scores according to the arg emb and pre emb.\"\"\"\n        srl_scores = self.get_srl_scores(arg_emb, pred_emb, arg_scores, pred_scores, self.label_space_size, self.config,\n                                         self.dropout)  # [num_sentences, max_num_args, max_num_preds, num_labels]\n        if gold_arg_labels is not None:\n            \"\"\"Get the answers according to the labels\"\"\"\n            srl_labels = self.get_srl_labels(arg_starts, arg_ends, predicates, gold_predicates, gold_arg_starts,\n                                             gold_arg_ends, gold_arg_labels, max_sent_length)\n\n            \"\"\"Compute the srl loss\"\"\"\n            srl_loss, srl_mask = self.get_srl_softmax_loss(srl_scores, srl_labels, num_args, num_preds)\n            predict_dict.update({\n                'srl_mask': srl_mask,\n                'loss': srl_loss\n            })\n        else:\n            predict_dict['srl_mask'] = self.get_srl_loss_mask(srl_scores, num_args, num_preds)\n        predict_dict.update({\n            \"candidate_arg_scores\": candidate_arg_scores,\n            \"candidate_pred_scores\": candidate_pred_scores,\n            \"predicates\": predicates,\n            \"arg_starts\": arg_starts,\n            \"arg_ends\": arg_ends,\n            \"arg_scores\": arg_scores,\n            \"pred_scores\": pred_scores,\n            \"num_args\": num_args,\n            \"num_preds\": num_preds,\n            # [num_sentences, num_args, num_preds] avoid max on empty tensor\n            # \"arg_labels\": torch.max(srl_scores, 1)[1] if srl_scores.numel() else srl_scores[:, :, :, 0],\n            \"srl_scores\": srl_scores,\n        })\n        return predict_dict\n\n\nclass SpanRankingSRLModel(nn.Module):\n\n    def __init__(self, config, embed: torch.nn.Module, context_layer: torch.nn.Module, label_space_size):\n        super(SpanRankingSRLModel, self).__init__()\n        self.config = config\n        self.dropout = float(config.dropout)\n        self.lexical_dropout = float(self.config.lexical_dropout)\n        self.label_space_size = label_space_size\n\n        # Initialize layers and parameters\n        self.word_embedding_dim = embed.get_output_dim()  # get the embedding dim\n        self.embed = embed\n        # Initialize context layer\n        self.context_layer = context_layer\n        context_layer_output_dim = context_layer.get_output_dim() if context_layer else self.word_embedding_dim\n        self.decoder = SpanRankingSRLDecoder(context_layer_output_dim, label_space_size, config)\n\n    def forward(self,\n                batch: Dict[str, torch.Tensor]\n                ):\n        gold_arg_ends, gold_arg_labels, gold_arg_starts, gold_predicates, masks, sent_lengths = \\\n            self.unpack(batch, training=self.training)\n\n        context_embeddings = self.embed(batch)\n        context_embeddings = F.dropout(context_embeddings, self.lexical_dropout, self.training)\n        if self.context_layer:\n            context_embeddings = self.context_layer(context_embeddings, masks)\n\n        return self.decoder.decode(context_embeddings, sent_lengths, masks, gold_arg_starts, gold_arg_ends,\n                                   gold_arg_labels, gold_predicates)\n\n    @staticmethod\n    def unpack(batch, mask=None, training=False):\n        keys = 'token_length', 'predicate_offset', 'argument_begin_offset', 'argument_end_offset', 'srl_label_id'\n        sent_lengths, gold_predicates, gold_arg_starts, gold_arg_ends, gold_arg_labels = [batch.get(k, None) for k in\n                                                                                          keys]\n        if mask is None:\n            mask = hanlp.utils.torch_util.lengths_to_mask(sent_lengths)\n        # elif not training:\n        #     sent_lengths = mask.sum(dim=1)\n        return gold_arg_ends, gold_arg_labels, gold_arg_starts, gold_predicates, mask, sent_lengths\n", "hanlp/components/srl/span_rank/span_rank.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-09 18:13\nimport logging\nfrom bisect import bisect\nfrom typing import Union, List, Callable, Tuple, Dict, Any\n\nfrom hanlp_common.constant import IDX\nfrom hanlp.layers.transformers.utils import build_optimizer_scheduler_with_transformer\nimport torch\nfrom torch.utils.data import DataLoader\nfrom hanlp.common.dataset import PadSequenceDataLoader, SortingSampler\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.srl.span_rank.inference_utils import srl_decode\nfrom hanlp.components.srl.span_rank.span_ranking_srl_model import SpanRankingSRLModel\nfrom hanlp.components.srl.span_rank.srl_eval_utils import compute_srl_f1\nfrom hanlp.datasets.srl.loaders.conll2012 import CoNLL2012SRLDataset, filter_v_args, unpack_srl, \\\n    group_pa_by_p\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.metrics.f1 import F1\nfrom hanlp_common.visualization import markdown_table\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, reorder\n\n\nclass SpanRankingSemanticRoleLabeler(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        \"\"\"An implementation of \"Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling\"\n        (:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: SpanRankingSRLModel = None\n\n    def build_optimizer(self,\n                        trn,\n                        epochs,\n                        lr,\n                        adam_epsilon,\n                        weight_decay,\n                        warmup_steps,\n                        transformer_lr,\n                        **kwargs):\n        # noinspection PyProtectedMember\n        transformer = self._get_transformer()\n        if transformer:\n            num_training_steps = len(trn) * epochs // self.config.get('gradient_accumulation', 1)\n            optimizer, scheduler = build_optimizer_scheduler_with_transformer(self.model,\n                                                                              transformer,\n                                                                              lr, transformer_lr,\n                                                                              num_training_steps, warmup_steps,\n                                                                              weight_decay, adam_epsilon)\n        else:\n            optimizer = torch.optim.Adam(self.model.parameters(), self.config.lr)\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer=optimizer,\n                mode='max',\n                factor=0.5,\n                patience=2,\n                verbose=True,\n            )\n        return optimizer, scheduler\n\n    def _get_transformer(self):\n        return getattr(self.model_.embed, 'transformer', None)\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    # noinspection PyProtectedMember\n    def build_metric(self, **kwargs) -> Tuple[F1, F1]:\n        predicate_f1 = F1()\n        end_to_end_f1 = F1()\n        return predicate_f1, end_to_end_f1\n\n    def execute_training_loop(self,\n                              trn: DataLoader,\n                              dev: DataLoader,\n                              epochs,\n                              criterion,\n                              optimizer,\n                              metric,\n                              save_dir,\n                              logger: logging.Logger,\n                              devices,\n                              **kwargs):\n        best_epoch, best_metric = 0, -1\n        predicate, end_to_end = metric\n        optimizer, scheduler = optimizer\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger,\n                                linear_scheduler=scheduler if self._get_transformer() else None)\n            if dev:\n                self.evaluate_dataloader(dev, criterion, metric, logger, ratio_width=ratio_width)\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            dev_score = end_to_end.score\n            if not self._get_transformer():\n                scheduler.step(dev_score)\n            if dev_score > best_metric:\n                self.save_weights(save_dir)\n                best_metric = dev_score\n                report += ' [red]saved[/red]'\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       linear_scheduler=None,\n                       gradient_accumulation=1,\n                       **kwargs):\n        self.model.train()\n        timer = CountdownTimer(len(trn) // gradient_accumulation)\n        total_loss = 0\n        self.reset_metrics(metric)\n        for idx, batch in enumerate(trn):\n            output_dict = self.feed_batch(batch)\n            self.update_metrics(batch, output_dict, metric)\n            loss = output_dict['loss']\n            loss = loss.sum()  # For data parallel\n            if torch.isnan(loss):  # w/ gold pred, some batches do not have PAs at all, resulting in empty scores\n                loss = torch.zeros((1,), device=loss.device)\n            else:\n                loss.backward()\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            if self.config.grad_norm:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm)\n            if (idx + 1) % gradient_accumulation == 0:\n                self._step(optimizer, linear_scheduler)\n                timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                          logger=logger)\n            total_loss += loss.item()\n            del loss\n        if len(trn) % gradient_accumulation:\n            self._step(optimizer, linear_scheduler)\n        return total_loss / timer.total\n\n    def _step(self, optimizer, linear_scheduler):\n        optimizer.step()\n        optimizer.zero_grad()\n        if linear_scheduler:\n            linear_scheduler.step()\n\n    # noinspection PyMethodOverriding\n    @torch.no_grad()\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric,\n                            logger,\n                            ratio_width=None,\n                            output=False,\n                            official=False,\n                            confusion_matrix=False,\n                            **kwargs):\n        self.model.eval()\n        self.reset_metrics(metric)\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        if official:\n            sentences = []\n            gold = []\n            pred = []\n        for batch in data:\n            output_dict = self.feed_batch(batch)\n            if official:\n                sentences += batch['token']\n                gold += batch['srl']\n                pred += output_dict['prediction']\n            self.update_metrics(batch, output_dict, metric)\n            loss = output_dict['loss']\n            total_loss += loss.item()\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger,\n                      ratio_width=ratio_width)\n            del loss\n        if official:\n            scores = compute_srl_f1(sentences, gold, pred)\n            if logger:\n                if confusion_matrix:\n                    labels = sorted(set(y for x in scores.label_confusions.keys() for y in x))\n                    headings = ['GOLD\u2193PRED\u2192'] + labels\n                    matrix = []\n                    for i, gold in enumerate(labels):\n                        row = [gold]\n                        matrix.append(row)\n                        for j, pred in enumerate(labels):\n                            row.append(scores.label_confusions.get((gold, pred), 0))\n                    matrix = markdown_table(headings, matrix)\n                    logger.info(f'{\"Confusion Matrix\": ^{len(matrix.splitlines()[0])}}')\n                    logger.info(matrix)\n                headings = ['Settings', 'Precision', 'Recall', 'F1']\n                data = []\n                for h, (p, r, f) in zip(['Unlabeled', 'Labeled', 'Official'], [\n                    [scores.unlabeled_precision, scores.unlabeled_recall, scores.unlabeled_f1],\n                    [scores.precision, scores.recall, scores.f1],\n                    [scores.conll_precision, scores.conll_recall, scores.conll_f1],\n                ]):\n                    data.append([h] + [f'{x:.2%}' for x in [p, r, f]])\n                table = markdown_table(headings, data)\n                logger.info(f'{\"Scores\": ^{len(table.splitlines()[0])}}')\n                logger.info(table)\n        else:\n            scores = metric\n        return total_loss / timer.total, scores\n\n    def build_model(self,\n                    training=True,\n                    **kwargs) -> torch.nn.Module:\n        # noinspection PyTypeChecker\n        # embed: torch.nn.Embedding = self.config.embed.module(vocabs=self.vocabs)[0].embed\n        model = SpanRankingSRLModel(self.config,\n                                    self.config.embed.module(vocabs=self.vocabs, training=training),\n                                    self.config.context_layer,\n                                    len(self.vocabs.srl_label))\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size, shuffle, device, logger: logging.Logger,\n                         generate_idx=False, transform=None, **kwargs) -> DataLoader:\n        batch_max_tokens = self.config.batch_max_tokens\n        gradient_accumulation = self.config.get('gradient_accumulation', 1)\n        if batch_size:\n            batch_size //= gradient_accumulation\n        if batch_max_tokens:\n            batch_max_tokens //= gradient_accumulation\n        dataset = self.build_dataset(data, generate_idx, logger, transform)\n\n        sampler = SortingSampler([x['token_length'] for x in dataset],\n                                 batch_size=batch_size,\n                                 batch_max_tokens=batch_max_tokens,\n                                 shuffle=shuffle)\n        return PadSequenceDataLoader(batch_sampler=sampler,\n                                     device=device,\n                                     dataset=dataset)\n\n    def build_dataset(self, data, generate_idx, logger, transform=None):\n        dataset = CoNLL2012SRLDataset(data, transform=[filter_v_args, unpack_srl, group_pa_by_p],\n                                      doc_level_offset=self.config.doc_level_offset, generate_idx=generate_idx)\n        if transform:\n            dataset.append_transform(transform)\n        if isinstance(self.config.get('embed', None), Embedding):\n            transform = self.config.embed.transform(vocabs=self.vocabs)\n            if transform:\n                dataset.append_transform(transform)\n        dataset.append_transform(self.vocabs)\n        dataset.append_transform(FieldLength('token'))\n        if isinstance(data, str):\n            dataset.purge_cache()  # Enable cache\n        if self.vocabs.mutable:\n            self.build_vocabs(dataset, logger)\n        return dataset\n\n    def predict(self, data: Union[str, List[str]], batch_size: int = None, fmt='dict', **kwargs):\n        if not data:\n            return []\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        samples = []\n        for token in data:\n            sample = dict()\n            sample['token'] = token\n            samples.append(sample)\n        batch_size = batch_size or self.config.batch_size\n        dataloader = self.build_dataloader(samples, batch_size, False, self.device, None, generate_idx=True)\n        outputs = []\n        order = []\n        for batch in dataloader:\n            output_dict = self.feed_batch(batch)\n            outputs.extend(output_dict['prediction'])\n            order.extend(batch[IDX])\n        outputs = reorder(outputs, order)\n        if fmt == 'list':\n            outputs = self.format_dict_to_results(data, outputs)\n        if flat:\n            return outputs[0]\n        return outputs\n\n    @staticmethod\n    def format_dict_to_results(data, outputs, exclusive_offset=False, with_predicate=False, with_argument=False,\n                               label_first=False):\n        results = []\n        for i in range(len(outputs)):\n            tokens = data[i]\n            output = []\n            for p, a in outputs[i].items():\n                # a: [(0, 0, 'ARG0')]\n                if with_predicate:\n                    a.insert(bisect([x[0] for x in a], p), (p, p, 'PRED'))\n                if with_argument is not False:\n                    a = [x + (tokens[x[0]:x[1] + 1],) for x in a]\n                    if isinstance(with_argument, str):\n                        a = [x[:-1] + (with_argument.join(x[-1]),) for x in a]\n                if exclusive_offset:\n                    a = [(x[0], x[1] + 1) + x[2:] for x in a]\n                if label_first:\n                    a = [tuple(reversed(x[2:])) + x[:2] for x in a]\n                output.append(a)\n            results.append(output)\n        return results\n\n    def input_is_flat(self, data):\n        return isinstance(data[0], str)\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            embed,\n            context_layer,\n            batch_size=40,\n            batch_max_tokens=700,\n            lexical_dropout=0.5,\n            dropout=0.2,\n            span_width_feature_size=20,\n            ffnn_size=150,\n            ffnn_depth=2,\n            argument_ratio=0.8,\n            predicate_ratio=0.4,\n            max_arg_width=30,\n            mlp_label_size=100,\n            enforce_srl_constraint=False,\n            use_gold_predicates=False,\n            doc_level_offset=True,\n            use_biaffine=False,\n            lr=1e-3,\n            transformer_lr=1e-5,\n            adam_epsilon=1e-6,\n            weight_decay=0.01,\n            warmup_steps=0.1,\n            grad_norm=5.0,\n            gradient_accumulation=1,\n            loss_reduction='sum',\n            transform=None,\n            devices=None,\n            logger=None,\n            seed=None,\n            **kwargs\n            ):\n\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, dataset, logger, **kwargs):\n        self.vocabs.srl_label = Vocab(pad_token=None, unk_token=None)\n        # Use null to indicate no relationship\n        self.vocabs.srl_label.add('<null>')\n        timer = CountdownTimer(len(dataset))\n        max_seq_len = 0\n        for each in dataset:\n            max_seq_len = max(max_seq_len, len(each['token_input_ids']))\n            timer.log(f'Building vocabs (max sequence length {max_seq_len}) [blink][yellow]...[/yellow][/blink]')\n            pass\n        timer.stop()\n        timer.erase()\n        self.vocabs['srl_label'].set_unk_as_safe_unk()\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n\n    def reset_metrics(self, metrics):\n        for each in metrics:\n            each.reset()\n\n    def report_metrics(self, loss, metrics):\n        predicate, end_to_end = metrics\n        return f'loss: {loss:.4f} predicate: {predicate.score:.2%} end_to_end: {end_to_end.score:.2%}'\n\n    def feed_batch(self, batch) -> Dict[str, Any]:\n        output_dict = self.model(batch)\n        prediction = self.decode_output(output_dict, batch, self.model.training)\n        output_dict['prediction'] = prediction\n        return output_dict\n\n    def decode_output(self, output_dict, batch, training=False):\n        idx_to_label = self.vocabs['srl_label'].idx_to_token\n        if training:\n            # Use fast decoding during training,\n            prediction = []\n            top_predicate_indices = output_dict['predicates'].tolist()\n            top_spans = torch.stack([output_dict['arg_starts'], output_dict['arg_ends']], dim=-1).tolist()\n            srl_mask = output_dict['srl_mask'].tolist()\n            srl_scores = output_dict['srl_scores']\n            pal_list = srl_scores.argmax(-1).tolist() if srl_scores.numel() else []\n            for n, (pal, predicate_indices, argument_spans) in enumerate(\n                    zip(pal_list, top_predicate_indices, top_spans)):\n                srl_per_sentence = {}\n                for p, (al, predicate_index) in enumerate(zip(pal, predicate_indices)):\n                    for a, (l, argument_span) in enumerate(zip(al, argument_spans)):\n                        if l and srl_mask[n][p][a]:\n                            args = srl_per_sentence.get(p, None)\n                            if args is None:\n                                args = srl_per_sentence[p] = []\n                            args.append((*argument_span, idx_to_label[l]))\n                prediction.append(srl_per_sentence)\n        else:\n            prediction = srl_decode(batch['token_length'], output_dict, idx_to_label, self.config)\n        return prediction\n\n    def update_metrics(self, batch: dict, output_dict: dict, metrics):\n        def unpack(y: dict):\n            return set((p, bel) for p, a in y.items() for bel in a)\n\n        predicate, end_to_end = metrics\n        for pred, gold in zip(output_dict['prediction'], batch['srl']):\n            predicate(pred.keys(), gold.keys())\n            end_to_end(unpack(pred), unpack(gold))\n", "hanlp/components/srl/span_rank/util.py": "# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL\nimport torch\n\n\ndef block_orth_normal_initializer(input_size, output_size):\n    weight = []\n    for o in output_size:\n        for i in input_size:\n            param = torch.FloatTensor(o, i)\n            torch.nn.init.orthogonal_(param)\n            weight.append(param)\n    return torch.cat(weight)\n", "hanlp/components/srl/span_rank/inference_utils.py": "# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL\n\n# Inference functions for the SRL model.\nimport numpy as np\n\n\ndef decode_spans(span_starts, span_ends, span_scores, labels_inv):\n    \"\"\"\n\n    Args:\n      span_starts: [num_candidates,]\n      span_scores: [num_candidates, num_labels]\n      span_ends: \n      labels_inv: \n\n    Returns:\n\n    \n    \"\"\"\n    pred_spans = []\n    span_labels = np.argmax(span_scores, axis=1)  # [num_candidates]\n    spans_list = list(zip(span_starts, span_ends, span_labels, span_scores))\n    spans_list = sorted(spans_list, key=lambda x: x[3][x[2]], reverse=True)\n    predicted_spans = {}\n    for start, end, label, _ in spans_list:\n        # Skip invalid span.\n        if label == 0 or (start, end) in predicted_spans:\n            continue\n        pred_spans.append((start, end, labels_inv[label]))\n        predicted_spans[(start, end)] = label\n    return pred_spans\n\n\ndef greedy_decode(predict_dict, srl_labels_inv):\n    \"\"\"Greedy decoding for SRL predicate-argument structures.\n\n    Args:\n      predict_dict: Dictionary of name to numpy arrays.\n      srl_labels_inv: SRL label id to string name.\n      suppress_overlap: Whether to greedily suppress overlapping arguments for the same predicate.\n\n    Returns:\n\n    \n    \"\"\"\n    arg_starts = predict_dict[\"arg_starts\"]\n    arg_ends = predict_dict[\"arg_ends\"]\n    predicates = predict_dict[\"predicates\"]\n    arg_labels = predict_dict[\"arg_labels\"]\n    scores = predict_dict[\"srl_scores\"]\n\n    num_suppressed_args = 0\n\n    # Map from predicates to a list of labeled spans.\n    pred_to_args = {}\n    if len(arg_ends) > 0 and len(predicates) > 0:\n        max_len = max(np.max(arg_ends), np.max(predicates)) + 1\n    else:\n        max_len = 1\n\n    for j, pred_id in enumerate(predicates):\n        args_list = []\n        for i, (arg_start, arg_end) in enumerate(zip(arg_starts, arg_ends)):\n            # If label is not null.\n            if arg_labels[i][j] == 0:\n                continue\n            label = srl_labels_inv[arg_labels[i][j]]\n            # if label not in [\"V\", \"C-V\"]:\n            args_list.append((arg_start, arg_end, label, scores[i][j][arg_labels[i][j]]))\n\n        # Sort arguments by highest score first.\n        args_list = sorted(args_list, key=lambda x: x[3], reverse=True)\n        new_args_list = []\n\n        flags = [False for _ in range(max_len)]\n        # Predicate will not overlap with arguments either.\n        flags[pred_id] = True\n\n        for (arg_start, arg_end, label, score) in args_list:\n            # If none of the tokens has been covered:\n            if not max(flags[arg_start:arg_end + 1]):\n                new_args_list.append((arg_start, arg_end, label))\n                for k in range(arg_start, arg_end + 1):\n                    flags[k] = True\n\n        # Only add predicate if it has any argument.\n        if new_args_list:\n            pred_to_args[pred_id] = new_args_list\n\n        num_suppressed_args += len(args_list) - len(new_args_list)\n\n    return pred_to_args, num_suppressed_args\n\n\n_CORE_ARGS = {\"ARG0\": 1, \"ARG1\": 2, \"ARG2\": 4, \"ARG3\": 8, \"ARG4\": 16, \"ARG5\": 32, \"ARGA\": 64,\n              \"A0\": 1, \"A1\": 2, \"A2\": 4, \"A3\": 8, \"A4\": 16, \"A5\": 32, \"AA\": 64}\n\n\ndef get_predicted_clusters(top_span_starts, top_span_ends, predicted_antecedents):\n    mention_to_predicted = {}\n    predicted_clusters = []\n    for i, predicted_index in enumerate(predicted_antecedents):\n        if predicted_index < 0:\n            continue\n        assert i > predicted_index\n        predicted_antecedent = (int(top_span_starts[predicted_index]), int(top_span_ends[predicted_index]))\n        if predicted_antecedent in mention_to_predicted:\n            predicted_cluster = mention_to_predicted[predicted_antecedent]\n        else:\n            predicted_cluster = len(predicted_clusters)\n            predicted_clusters.append([predicted_antecedent])\n            mention_to_predicted[predicted_antecedent] = predicted_cluster\n\n        mention = (int(top_span_starts[i]), int(top_span_ends[i]))\n        predicted_clusters[predicted_cluster].append(mention)\n        mention_to_predicted[mention] = predicted_cluster\n\n    predicted_clusters = [tuple(pc) for pc in predicted_clusters]\n    mention_to_predicted = {m: predicted_clusters[i] for m, i in list(mention_to_predicted.items())}\n\n    return predicted_clusters, mention_to_predicted\n\n\ndef _decode_non_overlapping_spans(starts, ends, scores, max_len, labels_inv, pred_id):\n    labels = np.argmax(scores, axis=1)\n    spans = []\n    for i, (start, end, label) in enumerate(zip(starts, ends, labels)):\n        if label <= 0:\n            continue\n        label_str = labels_inv[label]\n        if pred_id is not None and label_str == \"V\":\n            continue\n        spans.append((start, end, label_str, scores[i][label]))\n    spans = sorted(spans, key=lambda x: x[3], reverse=True)\n    flags = np.zeros([max_len], dtype=bool)\n    if pred_id is not None:\n        flags[pred_id] = True\n    new_spans = []\n    for start, end, label_str, score in spans:\n        if not max(flags[start:end + 1]):\n            new_spans.append((start, end, label_str))  # , score))\n            for k in range(start, end + 1):\n                flags[k] = True\n    return new_spans\n\n\ndef _dp_decode_non_overlapping_spans(starts, ends, scores, max_len, labels_inv, pred_id, u_constraint=False):\n    num_roles = scores.shape[1]  # [num_arg, num_roles]\n    labels = np.argmax(scores, axis=1).astype(np.int64)\n    spans = list(zip(starts, ends, list(range(len(starts)))))\n    spans = sorted(spans, key=lambda x: (x[0], x[1]))  # sort according to the span start index\n\n    if u_constraint:\n        f = np.zeros([max_len + 1, 128], dtype=float) - 0.1\n    else:  # This one\n        f = np.zeros([max_len + 1, 1], dtype=float) - 0.1\n\n    f[0, 0] = 0\n    states = {0: set([0])}  # A dictionary from id to list of binary core-arg states.\n    pointers = {}  # A dictionary from states to (arg_id, role, prev_t, prev_rs)\n    best_state = [(0, 0)]\n\n    def _update_state(t0, rs0, t1, rs1, delta, arg_id, role):\n        if f[t0][rs0] + delta > f[t1][rs1]:\n            f[t1][rs1] = f[t0][rs0] + delta\n            if t1 not in states:\n                states[t1] = set()\n            states[t1].update([rs1])\n            pointers[(t1, rs1)] = (arg_id, role, t0, rs0)  # the pointers store\n            if f[t1][rs1] > f[best_state[0][0]][best_state[0][1]]:\n                best_state[0] = (t1, rs1)\n\n    for start, end, i in spans:  # [arg_start, arg_end, arg_span_id]\n        assert scores[i][0] == 0  # dummy score\n        # The extra dummy score should be same for all states, so we can safely skip arguments overlap\n        # with the predicate.\n        if pred_id is not None and start <= pred_id and pred_id <= end:  # skip the span contains the predicate\n            continue\n        r0 = labels[i]  # Locally best role assignment.\n        # Strictly better to incorporate a dummy span if it has the highest local score.\n        if r0 == 0:  # labels_inv[r0] == \"O\"\n            continue\n        r0_str = labels_inv[r0]\n        # Enumerate explored states.\n        t_states = [t for t in list(states.keys()) if t <= start]  # collect the state which is before the current span\n        for t in t_states:  # for each state\n            role_states = states[t]\n            # Update states if best role is not a core arg.\n            if not u_constraint or r0_str not in _CORE_ARGS:  # True; this one\n                for rs in role_states:  # the set type in the value in the state dict\n                    _update_state(t, rs, end + 1, rs, scores[i][r0], i, r0)  # update the state\n            else:\n                for rs in role_states:\n                    for r in range(1, num_roles):\n                        if scores[i][r] > 0:\n                            r_str = labels_inv[r]\n                            core_state = _CORE_ARGS.get(r_str, 0)\n                            # print start, end, i, r_str, core_state, rs\n                            if core_state & rs == 0:\n                                _update_state(t, rs, end + 1, rs | core_state, scores[i][r], i, r)\n    # Backtrack to decode.\n    new_spans = []\n    t, rs = best_state[0]\n    while (t, rs) in pointers:\n        i, r, t0, rs0 = pointers[(t, rs)]\n        new_spans.append((int(starts[i]), int(ends[i]), labels_inv[r]))\n        t = t0\n        rs = rs0\n    return new_spans[::-1]\n\n\ndef srl_decode(sentence_lengths, predict_dict, srl_labels_inv, config):  # decode the predictions.\n    # Decode sentence-level tasks.\n    num_sentences = len(sentence_lengths)\n    predictions = [{} for _ in range(num_sentences)]\n    # Sentence-level predictions.\n    for i in range(num_sentences):  # for each sentences\n        # if predict_dict[\"No_arg\"] is True:\n        #     predictions[\"srl\"][i][predict_dict[\"predicates\"][i]] = []\n        #     continue\n        predict_dict_num_args_ = predict_dict[\"num_args\"].cpu().numpy()\n        predict_dict_num_preds_ = predict_dict[\"num_preds\"].cpu().numpy()\n        predict_dict_predicates_ = predict_dict[\"predicates\"].cpu().numpy()\n        predict_dict_arg_starts_ = predict_dict[\"arg_starts\"].cpu().numpy()\n        predict_dict_arg_ends_ = predict_dict[\"arg_ends\"].cpu().numpy()\n        predict_dict_srl_scores_ = predict_dict[\"srl_scores\"].detach().cpu().numpy()\n        num_args = predict_dict_num_args_[i]  # the number of the candidate argument spans\n        num_preds = predict_dict_num_preds_[i]  # the number of the candidate predicates\n        # for each predicate id, exec the decode process\n        for j, pred_id in enumerate(predict_dict_predicates_[i][:num_preds]):\n            # sorted arg_starts and arg_ends and srl_scores ? should be??? enforce_srl_constraint = False\n            arg_spans = _dp_decode_non_overlapping_spans(\n                predict_dict_arg_starts_[i][:num_args],\n                predict_dict_arg_ends_[i][:num_args],\n                predict_dict_srl_scores_[i, :num_args, j, :],\n                sentence_lengths[i], srl_labels_inv, pred_id, config.enforce_srl_constraint)\n            # To avoid warnings in the eval script.\n            if config.use_gold_predicates:  # false\n                arg_spans.append((pred_id, pred_id, \"V\"))\n            if arg_spans:\n                predictions[i][int(pred_id)] = sorted(arg_spans, key=lambda x: (x[0], x[1]))\n\n    return predictions\n", "hanlp/components/srl/span_rank/highway_variational_lstm.py": "# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\nfrom .layer import DropoutLayer, HighwayLSTMCell, VariationalLSTMCell\n\n\ndef initializer_1d(input_tensor, initializer):\n    assert len(input_tensor.size()) == 1\n    input_tensor = input_tensor.view(-1, 1)\n    input_tensor = initializer(input_tensor)\n    return input_tensor.view(-1)\n\n\nclass HighwayBiLSTM(nn.Module):\n    \"\"\"A module that runs multiple steps of HighwayBiLSTM.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=False, bidirectional=False, dropout_in=0,\n                 dropout_out=0):\n        super(HighwayBiLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.num_directions = 2 if bidirectional else 1\n\n        self.fcells, self.f_dropout, self.f_hidden_dropout = [], [], []\n        self.bcells, self.b_dropout, self.b_hidden_dropout = [], [], []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n            self.fcells.append(HighwayLSTMCell(input_size=layer_input_size, hidden_size=hidden_size))\n            self.f_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            self.f_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            if self.bidirectional:\n                self.bcells.append(HighwayLSTMCell(input_size=hidden_size, hidden_size=hidden_size))\n                self.b_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n                self.b_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n        self.fcells, self.bcells = nn.ModuleList(self.fcells), nn.ModuleList(self.bcells)\n        self.f_dropout, self.b_dropout = nn.ModuleList(self.f_dropout), nn.ModuleList(self.b_dropout)\n\n    def reset_dropout_layer(self, batch_size):\n        for layer in range(self.num_layers):\n            self.f_dropout[layer].reset_dropout_mask(batch_size)\n            if self.bidirectional:\n                self.b_dropout[layer].reset_dropout_mask(batch_size)\n\n    @staticmethod\n    def _forward_rnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in range(max_time):\n            h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)\n            hx = (h_next, c_next)\n            output.append(h_next)\n        output = torch.stack(output, 0)\n        return output, hx\n\n    @staticmethod\n    def _forward_brnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in reversed(list(range(max_time))):\n            h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)\n            hx = (h_next, c_next)\n            output.append(h_next)\n        output.reverse()\n        output = torch.stack(output, 0)\n        return output, hx\n\n    def forward(self, input, masks, initial=None):\n        if self.batch_first:\n            input = input.transpose(0, 1)  # transpose: return the transpose matrix\n            masks = torch.unsqueeze(masks.transpose(0, 1), dim=2)\n        max_time, batch_size, _ = input.size()\n\n        self.reset_dropout_layer(batch_size)  # reset the dropout each batch forward\n\n        masks = masks.expand(-1, -1, self.hidden_size)  # expand: -1 means not expand that dimension\n        if initial is None:\n            initial = Variable(input.data.new(batch_size, self.hidden_size).zero_())\n            initial = (initial, initial)  # h0, c0\n\n        h_n, c_n = [], []\n        for layer in range(self.num_layers):\n            # hidden_mask, hidden_drop = None, None\n            hidden_mask, hidden_drop = self.f_dropout[layer], self.f_hidden_dropout[layer]\n            layer_output, (layer_h_n, layer_c_n) = HighwayBiLSTM._forward_rnn(cell=self.fcells[layer], \\\n                                                                              gate=None, input=input, masks=masks,\n                                                                              initial=initial, \\\n                                                                              drop_masks=hidden_mask,\n                                                                              hidden_drop=hidden_drop)\n            h_n.append(layer_h_n)\n            c_n.append(layer_c_n)\n            if self.bidirectional:\n                hidden_mask, hidden_drop = self.b_dropout[layer], self.b_hidden_dropout[layer]\n                blayer_output, (blayer_h_n, blayer_c_n) = HighwayBiLSTM._forward_brnn(cell=self.bcells[layer], \\\n                                                                                      gate=None, input=layer_output,\n                                                                                      masks=masks, initial=initial, \\\n                                                                                      drop_masks=hidden_mask,\n                                                                                      hidden_drop=hidden_drop)\n                h_n.append(blayer_h_n)\n                c_n.append(blayer_c_n)\n\n            input = blayer_output if self.bidirectional else layer_output\n\n        h_n, c_n = torch.stack(h_n, 0), torch.stack(c_n, 0)\n        if self.batch_first:\n            input = input.transpose(1, 0)  # transpose: return the transpose matrix\n        return input, (h_n, c_n)\n\n\nclass StackedHighwayBiLSTM(nn.Module):\n    \"\"\"A module that runs multiple steps of HighwayBiLSTM.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=False, \\\n                 bidirectional=False, dropout_in=0, dropout_out=0):\n        super(StackedHighwayBiLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.num_directions = 2 if bidirectional else 1\n\n        self.fcells, self.f_dropout, self.f_hidden_dropout = [], [], []\n        self.bcells, self.b_dropout, self.b_hidden_dropout = [], [], []\n        self.f_initial, self.b_initial = [], []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else 2 * hidden_size if self.bidirectional else hidden_size\n            self.fcells.append(VariationalLSTMCell(input_size=layer_input_size, hidden_size=hidden_size))\n            self.f_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            self.f_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            self.f_initial.append(nn.Parameter(torch.Tensor(2, self.hidden_size)))\n            assert self.bidirectional is True\n            self.bcells.append(VariationalLSTMCell(input_size=layer_input_size, hidden_size=hidden_size))\n            self.b_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            self.b_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))\n            self.b_initial.append(nn.Parameter(torch.Tensor(2, self.hidden_size)))\n        self.lstm_project_layer = nn.ModuleList([nn.Linear(2 * self.hidden_size, 2 * self.hidden_size)\n                                                 for _ in range(num_layers - 1)])\n        self.fcells, self.bcells = nn.ModuleList(self.fcells), nn.ModuleList(self.bcells)\n        self.f_dropout, self.b_dropout = nn.ModuleList(self.f_dropout), nn.ModuleList(self.b_dropout)\n        self.f_hidden_dropout, self.b_hidden_dropout = \\\n            nn.ModuleList(self.f_hidden_dropout), nn.ModuleList(self.b_hidden_dropout)\n        self.f_initial, self.b_initial = nn.ParameterList(self.f_initial), nn.ParameterList(self.b_initial)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer_initial in [self.f_initial, self.b_initial]:\n            for initial in layer_initial:\n                init.xavier_uniform_(initial)\n        for layer in self.lstm_project_layer:\n            init.xavier_uniform_(layer.weight)\n            initializer_1d(layer.bias, init.xavier_uniform_)\n\n    def reset_dropout_layer(self, batch_size):\n        for layer in range(self.num_layers):\n            self.f_dropout[layer].reset_dropout_mask(batch_size)\n            self.f_hidden_dropout[layer].reset_dropout_mask(batch_size)\n            if self.bidirectional:\n                self.b_dropout[layer].reset_dropout_mask(batch_size)\n                self.b_hidden_dropout[layer].reset_dropout_mask(batch_size)\n\n    def reset_state(self, batch_size):\n        f_states, b_states = [], []\n        for f_layer_initial, b_layer_initial in zip(self.f_initial, self.b_initial):\n            f_states.append([f_layer_initial[0].expand(batch_size, -1), f_layer_initial[1].expand(batch_size, -1)])\n            b_states.append([b_layer_initial[0].expand(batch_size, -1), b_layer_initial[1].expand(batch_size, -1)])\n        return f_states, b_states\n\n    @staticmethod\n    def _forward_rnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in range(max_time):\n            h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)\n            hx = (h_next, c_next)\n            output.append(h_next)\n        output = torch.stack(output, 0)\n        return output, hx\n\n    @staticmethod\n    def _forward_brnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in reversed(list(range(max_time))):\n            h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)\n            hx = (h_next, c_next)\n            output.append(h_next)\n        output.reverse()\n        output = torch.stack(output, 0)\n        return output, hx\n\n    def forward(self, input, masks, initial=None):\n        if self.batch_first:\n            input = input.transpose(0, 1)  # transpose: return the transpose matrix\n            masks = torch.unsqueeze(masks.transpose(0, 1), dim=2)\n        max_time, batch_size, _ = input.size()\n\n        self.reset_dropout_layer(batch_size)  # reset the dropout each batch forward\n        f_states, b_states = self.reset_state(batch_size)\n\n        masks = masks.expand(-1, -1, self.hidden_size)  # expand: -1 means not expand that dimension\n\n        h_n, c_n = [], []\n        outputs = []\n        for layer in range(self.num_layers):\n            hidden_mask, hidden_drop = self.f_dropout[layer], self.f_hidden_dropout[layer]\n            layer_output, (layer_h_n, layer_c_n) = \\\n                StackedHighwayBiLSTM._forward_rnn(cell=self.fcells[layer],\n                                                  gate=None, input=input, masks=masks, initial=f_states[layer],\n                                                  drop_masks=hidden_mask, hidden_drop=hidden_drop)\n            h_n.append(layer_h_n)\n            c_n.append(layer_c_n)\n            assert self.bidirectional is True\n            hidden_mask, hidden_drop = self.b_dropout[layer], self.b_hidden_dropout[layer]\n            blayer_output, (blayer_h_n, blayer_c_n) = \\\n                StackedHighwayBiLSTM._forward_brnn(cell=self.bcells[layer],\n                                                   gate=None, input=input, masks=masks, initial=b_states[layer],\n                                                   drop_masks=hidden_mask, hidden_drop=hidden_drop)\n            h_n.append(blayer_h_n)\n            c_n.append(blayer_c_n)\n\n            output = torch.cat([layer_output, blayer_output], 2) if self.bidirectional else layer_output\n            output = F.dropout(output, self.dropout_out, self.training)\n            if layer > 0:  # Highway\n                highway_gates = torch.sigmoid(self.lstm_project_layer[layer - 1].forward(output))\n                output = highway_gates * output + (1 - highway_gates) * input\n            if self.batch_first:\n                outputs.append(output.transpose(1, 0))\n            else:\n                outputs.append(output)\n            input = output\n\n        h_n, c_n = torch.stack(h_n, 0), torch.stack(c_n, 0)\n        if self.batch_first:\n            output = output.transpose(1, 0)  # transpose: return the transpose matrix\n        return output, (h_n, c_n), outputs\n", "hanlp/components/srl/span_rank/layer.py": "# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom hanlp.components.srl.span_rank.util import block_orth_normal_initializer\n\n\ndef get_tensor_np(t):\n    return t.data.cpu().numpy()\n\n\ndef orthonormal_initializer(output_size, input_size):\n    \"\"\"adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/linalg.py\n\n    Args:\n      output_size: \n      input_size: \n\n    Returns:\n\n    \n    \"\"\"\n    print((output_size, input_size))\n    I = np.eye(output_size)\n    lr = .1\n    eps = .05 / (output_size + input_size)\n    success = False\n    tries = 0\n    while not success and tries < 10:\n        Q = np.random.randn(input_size, output_size) / np.sqrt(output_size)\n        for i in range(100):\n            QTQmI = Q.T.dot(Q) - I\n            loss = np.sum(QTQmI ** 2 / 2)\n            Q2 = Q ** 2\n            Q -= lr * Q.dot(QTQmI) / (\n                    np.abs(Q2 + Q2.sum(axis=0, keepdims=True) + Q2.sum(axis=1, keepdims=True) - 1) + eps)\n            if np.max(Q) > 1e6 or loss > 1e6 or not np.isfinite(loss):\n                tries += 1\n                lr /= 2\n                break\n        success = True\n    if success:\n        print(('Orthogonal pretrainer loss: %.2e' % loss))\n    else:\n        print('Orthogonal pretrainer failed, using non-orthogonal random matrix')\n        Q = np.random.randn(input_size, output_size) / np.sqrt(output_size)\n    return np.transpose(Q.astype(np.float32))\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-8):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\nclass DropoutLayer3D(nn.Module):\n    def __init__(self, input_size, dropout_rate=0.0):\n        super(DropoutLayer3D, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.input_size = input_size\n        self.drop_mask = torch.FloatTensor(self.input_size).fill_(1 - self.dropout_rate)\n        self.drop_mask = Variable(torch.bernoulli(self.drop_mask), requires_grad=False)\n        if torch.cuda.is_available():\n            self.drop_mask = self.drop_mask.cuda()\n\n    def reset_dropout_mask(self, batch_size, length):\n        self.drop_mask = torch.FloatTensor(batch_size, length, self.input_size).fill_(1 - self.dropout_rate)\n        self.drop_mask = Variable(torch.bernoulli(self.drop_mask), requires_grad=False)\n        if torch.cuda.is_available():\n            self.drop_mask = self.drop_mask.cuda()\n\n    def forward(self, x):\n        if self.training:\n            return torch.mul(x, self.drop_mask)\n        else:  # eval\n            return x * (1.0 - self.dropout_rate)\n\n\nclass DropoutLayer(nn.Module):\n    def __init__(self, input_size, dropout_rate=0.0):\n        super(DropoutLayer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.input_size = input_size\n        self.drop_mask = torch.Tensor(self.input_size).fill_(1 - self.dropout_rate)\n        self.drop_mask = torch.bernoulli(self.drop_mask)\n\n    def reset_dropout_mask(self, batch_size):\n        self.drop_mask = torch.Tensor(batch_size, self.input_size).fill_(1 - self.dropout_rate)\n        self.drop_mask = torch.bernoulli(self.drop_mask)\n\n    def forward(self, x):\n        if self.training:\n            return torch.mul(x, self.drop_mask.to(x.device))\n        else:  # eval\n            return x * (1.0 - self.dropout_rate)\n\n\nclass NonLinear(nn.Module):\n    def __init__(self, input_size, hidden_size, activation=None):\n        super(NonLinear, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(in_features=input_size, out_features=hidden_size)\n        if activation is None:\n            self._activate = lambda x: x\n        else:\n            if not callable(activation):\n                raise ValueError(\"activation must be callable: type={}\".format(type(activation)))\n            self._activate = activation\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        y = self.linear(x)\n        return self._activate(y)\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n\nclass Biaffine(nn.Module):\n    def __init__(self, in1_features, in2_features, out_features,\n                 bias=(True, True)):\n        super(Biaffine, self).__init__()\n        self.in1_features = in1_features\n        self.in2_features = in2_features\n        self.out_features = out_features\n        self.bias = bias\n        self.linear_input_size = in1_features + int(bias[0])\n        self.linear_output_size = out_features * (in2_features + int(bias[1]))\n        self.linear = nn.Linear(in_features=self.linear_input_size,\n                                out_features=self.linear_output_size,\n                                bias=False)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self.linear.weight)\n\n    def forward(self, input1, input2):\n        batch_size, len1, dim1 = input1.size()\n        batch_size, len2, dim2 = input2.size()\n        if self.bias[0]:\n            ones = input1.data.new(batch_size, len1, 1).zero_().fill_(1)  # this kind of implementation is too tedious\n            input1 = torch.cat((input1, Variable(ones)), dim=2)\n            dim1 += 1\n        if self.bias[1]:\n            ones = input2.data.new(batch_size, len2, 1).zero_().fill_(1)\n            input2 = torch.cat((input2, Variable(ones)), dim=2)\n            dim2 += 1\n\n        affine = self.linear(input1)\n\n        affine = affine.view(batch_size, len1 * self.out_features, dim2)\n        input2 = torch.transpose(input2, 1, 2)\n        # torch.bmm: Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.\n        biaffine = torch.transpose(torch.bmm(affine, input2), 1, 2)\n        # view: Returns a new tensor with the same data as the self tensor but of a different size.\n        biaffine = biaffine.contiguous().view(batch_size, len2, len1, self.out_features)\n\n        return biaffine\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + 'in1_features=' + str(self.in1_features) \\\n               + ', in2_features=' + str(self.in2_features) \\\n               + ', out_features=' + str(self.out_features) + ')'\n\n\nclass HighwayLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(HighwayLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear_ih = nn.Linear(in_features=input_size,\n                                   out_features=6 * hidden_size)\n        self.linear_hh = nn.Linear(in_features=hidden_size,\n                                   out_features=5 * hidden_size,\n                                   bias=False)\n        self.reset_parameters()  # reset all the param in the MyLSTMCell\n\n    def reset_parameters(self):\n        weight_ih = block_orth_normal_initializer([self.input_size, ], [self.hidden_size] * 6)\n        self.linear_ih.weight.data.copy_(weight_ih)\n\n        weight_hh = block_orth_normal_initializer([self.hidden_size, ], [self.hidden_size] * 5)\n        self.linear_hh.weight.data.copy_(weight_hh)\n        # nn.init.constant(self.linear_hh.weight, 1.0)\n        # nn.init.constant(self.linear_ih.weight, 1.0)\n\n        nn.init.constant(self.linear_ih.bias, 0.0)\n\n    def forward(self, x, mask=None, hx=None, dropout=None):\n        assert mask is not None and hx is not None\n        _h, _c = hx\n        _x = self.linear_ih(x)  # compute the x\n        preact = self.linear_hh(_h) + _x[:, :self.hidden_size * 5]\n\n        i, f, o, t, j = preact.chunk(chunks=5, dim=1)\n        i, f, o, t, j = F.sigmoid(i), F.sigmoid(f + 1.0), F.sigmoid(o), F.sigmoid(t), F.tanh(j)\n        k = _x[:, self.hidden_size * 5:]\n\n        c = f * _c + i * j\n        c = mask * c + (1.0 - mask) * _c\n\n        h = t * o * F.tanh(c) + (1.0 - t) * k\n        if dropout is not None:\n            h = dropout(h)\n        h = mask * h + (1.0 - mask) * _h\n        return h, c\n\n\nclass VariationalLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(VariationalLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(in_features=input_size + self.hidden_size, out_features=3 * hidden_size)\n        self.reset_parameters()  # reset all the param in the MyLSTMCell\n\n    def reset_parameters(self):\n        weight = block_orth_normal_initializer([self.input_size + self.hidden_size, ], [self.hidden_size] * 3)\n        self.linear.weight.data.copy_(weight)\n        nn.init.constant_(self.linear.bias, 0.0)\n\n    def forward(self, x, mask=None, hx=None, dropout=None):\n        assert mask is not None and hx is not None\n        _h, _c = hx\n        _h = dropout(_h)\n        _x = self.linear(torch.cat([x, _h], 1))  # compute the x\n        i, j, o = _x.chunk(3, dim=1)\n        i = torch.sigmoid(i)\n        c = (1.0 - i) * _c + i * torch.tanh(j)\n        c = mask * c  # + (1.0 - mask) * _c\n        h = torch.tanh(c) * torch.sigmoid(o)\n        h = mask * h  # + (1.0 - mask) * _h\n\n        return h, c\n\n\nclass VariationalLSTM(nn.Module):\n    \"\"\"A module that runs multiple steps of LSTM.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=False, \\\n                 bidirectional=False, dropout_in=0, dropout_out=0):\n        super(VariationalLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.num_directions = 2 if bidirectional else 1\n\n        self.fcells = []\n        self.bcells = []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions\n            self.fcells.append(nn.LSTMCell(input_size=layer_input_size, hidden_size=hidden_size))\n            if self.bidirectional:\n                self.bcells.append(nn.LSTMCell(input_size=layer_input_size, hidden_size=hidden_size))\n\n        self._all_weights = []\n        for layer in range(num_layers):\n            layer_params = (self.fcells[layer].weight_ih, self.fcells[layer].weight_hh, \\\n                            self.fcells[layer].bias_ih, self.fcells[layer].bias_hh)\n            suffix = ''\n            param_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']\n            param_names += ['bias_ih_l{}{}', 'bias_hh_l{}{}']\n            param_names = [x.format(layer, suffix) for x in param_names]\n            for name, param in zip(param_names, layer_params):\n                setattr(self, name, param)\n            self._all_weights.append(param_names)\n\n            if self.bidirectional:\n                layer_params = (self.bcells[layer].weight_ih, self.bcells[layer].weight_hh, \\\n                                self.bcells[layer].bias_ih, self.bcells[layer].bias_hh)\n                suffix = '_reverse'\n                param_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']\n                param_names += ['bias_ih_l{}{}', 'bias_hh_l{}{}']\n                param_names = [x.format(layer, suffix) for x in param_names]\n                for name, param in zip(param_names, layer_params):\n                    setattr(self, name, param)\n                self._all_weights.append(param_names)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):  # modified by kiro\n        for name, param in self.named_parameters():\n            print(name)\n            if \"weight\" in name:\n                # for i in range(4):\n                # nn.init.orthogonal(self.__getattr__(name)[self.hidden_size*i:self.hidden_size*(i+1),:])\n                nn.init.orthogonal(self.__getattr__(name))\n            if \"bias\" in name:\n                nn.init.normal(self.__getattr__(name), 0.0, 0.01)\n                # nn.init.constant(self.__getattr__(name), 1.0)  # different from zhang's 0\n\n    @staticmethod\n    def _forward_rnn(cell, input, masks, initial, drop_masks):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in range(max_time):\n            h_next, c_next = cell(input=input[time], hx=hx)\n            h_next = h_next * masks[time] + initial[0] * (1 - masks[time])\n            c_next = c_next * masks[time] + initial[1] * (1 - masks[time])\n            output.append(h_next)\n            if drop_masks is not None: h_next = h_next * drop_masks\n            hx = (h_next, c_next)\n        output = torch.stack(output, 0)\n        return output, hx\n\n    @staticmethod\n    def _forward_brnn(cell, input, masks, initial, drop_masks):\n        max_time = input.size(0)\n        output = []\n        hx = initial\n        for time in reversed(list(range(max_time))):\n            h_next, c_next = cell(input=input[time], hx=hx)\n            h_next = h_next * masks[time] + initial[0] * (1 - masks[time])\n            c_next = c_next * masks[time] + initial[1] * (1 - masks[time])\n            output.append(h_next)\n            if drop_masks is not None: h_next = h_next * drop_masks\n            hx = (h_next, c_next)\n        output.reverse()\n        output = torch.stack(output, 0)\n        return output, hx\n\n    def forward(self, input, masks, initial=None):\n        if self.batch_first:\n            input = input.transpose(0, 1)  # transpose: return the transpose matrix\n            masks = torch.unsqueeze(masks.transpose(0, 1), dim=2)\n        max_time, batch_size, _ = input.size()\n        masks = masks.expand(-1, -1, self.hidden_size)  # expand: -1 means not expand that dimension\n        if initial is None:\n            initial = Variable(input.data.new(batch_size, self.hidden_size).zero_())\n            initial = (initial, initial)  # h0, c0\n        h_n = []\n        c_n = []\n\n        for layer in range(self.num_layers):\n            max_time, batch_size, input_size = input.size()\n            input_mask, hidden_mask = None, None\n            if self.training:  # when training, use the dropout\n                input_mask = input.data.new(batch_size, input_size).fill_(1 - self.dropout_in)\n                input_mask = Variable(torch.bernoulli(input_mask), requires_grad=False)\n                input_mask = input_mask / (1 - self.dropout_in)\n                # permute: exchange the dimension\n                input_mask = torch.unsqueeze(input_mask, dim=2).expand(-1, -1, max_time).permute(2, 0, 1)\n                input = input * input_mask\n\n                hidden_mask = input.data.new(batch_size, self.hidden_size).fill_(1 - self.dropout_out)\n                hidden_mask = Variable(torch.bernoulli(hidden_mask), requires_grad=False)\n                hidden_mask = hidden_mask / (1 - self.dropout_out)\n\n            layer_output, (layer_h_n, layer_c_n) = VariationalLSTM._forward_rnn(cell=self.fcells[layer], \\\n                                                                                input=input, masks=masks,\n                                                                                initial=initial,\n                                                                                drop_masks=hidden_mask)\n            if self.bidirectional:\n                blayer_output, (blayer_h_n, blayer_c_n) = VariationalLSTM._forward_brnn(cell=self.bcells[layer], \\\n                                                                                        input=input, masks=masks,\n                                                                                        initial=initial,\n                                                                                        drop_masks=hidden_mask)\n\n            h_n.append(torch.cat([layer_h_n, blayer_h_n], 1) if self.bidirectional else layer_h_n)\n            c_n.append(torch.cat([layer_c_n, blayer_c_n], 1) if self.bidirectional else layer_c_n)\n            input = torch.cat([layer_output, blayer_output], 2) if self.bidirectional else layer_output\n\n        h_n = torch.stack(h_n, 0)\n        c_n = torch.stack(c_n, 0)\n        if self.batch_first:\n            input = input.transpose(1, 0)  # transpose: return the transpose matrix\n        return input, (h_n, c_n)\n", "hanlp/components/srl/span_rank/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-19 22:22", "hanlp/components/srl/span_bio/baffine_tagging.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-04 13:59\nimport math\n\nimport torch\nfrom torch import nn\n\nfrom hanlp.components.parsers.biaffine.biaffine import Biaffine\nfrom hanlp.components.parsers.biaffine.mlp import MLP\nfrom hanlp.layers.crf.crf import CRF\n\n\nclass BiaffineTaggingDecoder(nn.Module):\n\n    def __init__(self,\n                 n_rels,\n                 hidden_size,\n                 n_mlp_rel=300,\n                 mlp_dropout=0.2,\n                 crf=False) -> None:\n        super().__init__()\n        self.mlp_rel_h = MLP(n_in=hidden_size,\n                             n_out=n_mlp_rel,\n                             dropout=mlp_dropout)\n        self.mlp_rel_d = MLP(n_in=hidden_size,\n                             n_out=n_mlp_rel,\n                             dropout=mlp_dropout)\n        self.rel_attn = Biaffine(n_in=n_mlp_rel,\n                                 n_out=n_rels,\n                                 bias_x=True,\n                                 bias_y=True)\n        bias = 1 / math.sqrt(self.rel_attn.weight.size(1))\n        nn.init.uniform_(self.rel_attn.weight, -bias, bias)\n        self.crf = CRF(n_rels) if crf else None\n\n    # noinspection PyUnusedLocal\n    def forward(self, x: torch.Tensor, **kwargs):\n        rel_h = self.mlp_rel_h(x)\n        rel_d = self.mlp_rel_d(x)\n\n        # get arc and rel scores from the bilinear attention\n        # [batch_size, seq_len, seq_len, n_rels]\n        s_rel = self.rel_attn(rel_d, rel_h).permute(0, 2, 3, 1)\n        return s_rel\n\n\nclass SpanBIOSemanticRoleLabelingModel(nn.Module):\n\n    def __init__(self,\n                 embed,\n                 encoder,\n                 num_labels: int,\n                 n_mlp_rel,\n                 mlp_dropout,\n                 crf=False,\n                 ) -> None:\n        super().__init__()\n        self.embed = embed\n        self.encoder = encoder\n        hidden_size = encoder.get_output_dim() if encoder else embed.get_output_dim()\n        self.decoder = BiaffineTaggingDecoder(\n            num_labels,\n            hidden_size,\n            n_mlp_rel,\n            mlp_dropout,\n            crf,\n        )\n\n    def forward(self, batch, mask):\n        x = self.embed(batch)\n        if self.encoder:\n            x = self.encoder(x, mask=mask)\n        x = self.decoder(x)\n        return x\n", "hanlp/components/srl/span_bio/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-04 13:59\n", "hanlp/components/srl/span_bio/span_bio.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-22 20:54\nimport logging\nfrom copy import copy\nfrom typing import Union, List, Callable, Dict, Any\nfrom bisect import bisect\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import IDX, PRED\nfrom hanlp.common.dataset import PadSequenceDataLoader, SamplerBuilder, TransformableDataset\nfrom hanlp.common.structure import History\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.srl.span_bio.baffine_tagging import SpanBIOSemanticRoleLabelingModel\nfrom hanlp.datasets.srl.loaders.conll2012 import CoNLL2012SRLBIODataset\nfrom hanlp.layers.crf.crf import CRF\nfrom hanlp.layers.embeddings.contextual_word_embedding import find_transformer\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.layers.transformers.utils import build_optimizer_scheduler_with_transformer\nfrom hanlp.metrics.chunking.sequence_labeling import get_entities\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.utils.string_util import guess_delimiter\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp.utils.torch_util import clip_grad_norm, lengths_to_mask\nfrom hanlp_common.util import merge_locals_kwargs, reorder\n\n\nclass SpanBIOSemanticRoleLabeler(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a\n        predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: SpanBIOSemanticRoleLabelingModel = None\n\n    def build_optimizer(self,\n                        trn,\n                        epochs,\n                        lr,\n                        adam_epsilon,\n                        weight_decay,\n                        warmup_steps,\n                        transformer_lr=None,\n                        gradient_accumulation=1,\n                        **kwargs):\n        num_training_steps = len(trn) * epochs // gradient_accumulation\n        if transformer_lr is None:\n            transformer_lr = lr\n        transformer = find_transformer(self.model.embed)\n        optimizer, scheduler = build_optimizer_scheduler_with_transformer(self.model, transformer,\n                                                                          lr, transformer_lr,\n                                                                          num_training_steps, warmup_steps,\n                                                                          weight_decay, adam_epsilon)\n        return optimizer, scheduler\n\n    def build_criterion(self, decoder=None, **kwargs):\n        if self.config.crf:\n            if not decoder:\n                decoder = self.model.decoder\n            if isinstance(decoder, torch.nn.DataParallel):\n                decoder = decoder.module\n            return decoder.crf\n        else:\n            return nn.CrossEntropyLoss(reduction=self.config.loss_reduction)\n\n    def build_metric(self, **kwargs):\n        return F1()\n\n    def execute_training_loop(self,\n                              trn: DataLoader,\n                              dev: DataLoader,\n                              epochs,\n                              criterion,\n                              optimizer,\n                              metric,\n                              save_dir,\n                              logger: logging.Logger,\n                              devices,\n                              ratio_width=None,\n                              patience=0.5,\n                              **kwargs):\n        if isinstance(patience, float):\n            patience = int(patience * epochs)\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,\n                                **self.config)\n            loss, dev_metric = self.evaluate_dataloader(dev, criterion, metric, logger=logger, ratio_width=ratio_width)\n            timer.update()\n            report = f\"{timer.elapsed_human} / {timer.total_time_human} ETA: {timer.eta_human}\"\n            if dev_metric > best_metric:\n                best_epoch, best_metric = epoch, copy(dev_metric)\n                self.save_weights(save_dir)\n                report += ' [red](saved)[/red]'\n            else:\n                report += f' ({epoch - best_epoch})'\n                if epoch - best_epoch >= patience:\n                    report += ' early stop'\n            logger.info(report)\n            if epoch - best_epoch >= patience:\n                break\n        if not best_epoch:\n            self.save_weights(save_dir)\n        elif best_epoch != epoch:\n            self.load_weights(save_dir)\n        logger.info(f\"Max score of dev is {best_metric} at epoch {best_epoch}\")\n        logger.info(f\"Average time of each epoch is {timer.elapsed_average_human}\")\n        logger.info(f\"{timer.elapsed_human} elapsed\")\n\n    # noinspection PyMethodOverriding\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       history: History,\n                       gradient_accumulation=1,\n                       grad_norm=None,\n                       ratio_width=None,\n                       eval_trn=False,\n                       **kwargs):\n        optimizer, scheduler = optimizer\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        for idx, batch in enumerate(trn):\n            pred, mask = self.feed_batch(batch)\n            loss = self.compute_loss(criterion, pred, batch['srl_id'], mask)\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if eval_trn:\n                prediction = self.decode_output(pred, mask, batch)\n                self.update_metrics(metric, prediction, batch)\n            if history.step(gradient_accumulation):\n                self._step(optimizer, scheduler, grad_norm)\n                report = f'loss: {total_loss / (idx + 1):.4f} {metric}' if eval_trn else f'loss: {total_loss / (idx + 1):.4f}'\n                timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n            del loss\n            del pred\n            del mask\n\n    def naive_decode(self, pred, mask, batch, decoder=None):\n        vocab = self.vocabs['srl'].idx_to_token\n        results = []\n        for sent, matrix in zip(batch['token'], pred.argmax(-1).tolist()):\n            results.append([])\n            for token, tags_per_token in zip(sent, matrix):\n                tags_per_token = [vocab[x] for x in tags_per_token][:len(sent)]\n                srl_per_token = get_entities(tags_per_token)\n                results[-1].append(srl_per_token)\n        return results\n\n    def decode_output(self, pred, mask, batch, decoder=None):\n        # naive = self.naive_decode(pred, mask, batch, decoder)\n        vocab = self.vocabs['srl'].idx_to_token\n        if mask is not None:\n            if self.config.crf:\n                if not decoder:\n                    decoder = self.model.decoder\n                crf: CRF = decoder.crf\n                token_index, mask = mask\n                pred = crf.decode(pred, mask)\n                pred = sum(pred, [])\n            else:\n                pred = pred[mask].argmax(-1)\n                pred = pred.tolist()\n        pred = [vocab[x] for x in pred]\n        results = []\n        offset = 0\n        for sent in batch['token']:\n            results.append([])\n            for token in sent:\n                tags_per_token = pred[offset:offset + len(sent)]\n                srl_per_token = get_entities(tags_per_token)\n                results[-1].append(srl_per_token)\n                offset += len(sent)\n        assert offset == len(pred)\n        # assert results == naive\n        return results\n\n    def update_metrics(self, metric, prediction, batch):\n        for p, g in zip(prediction, batch['srl_set']):\n            srl = set()\n            for i, args in enumerate(p):\n                srl.update((i, start, end, label) for (label, start, end) in args)\n            metric(srl, g)\n        return metric\n\n    def feed_batch(self, batch: dict):\n        lens = batch['token_length']\n        mask2d = lengths_to_mask(lens)\n        pred = self.model(batch, mask=mask2d)\n        mask3d = self.compute_mask(mask2d)\n        if self.config.crf:\n            token_index = mask3d[0]\n            pred = pred.flatten(end_dim=1)[token_index]\n            pred = F.log_softmax(pred, dim=-1)\n        return pred, mask3d\n\n    def compute_mask(self, mask2d):\n        mask3d = mask2d.unsqueeze_(-1).expand(-1, -1, mask2d.size(1))\n        mask3d = mask3d & mask3d.transpose(1, 2)\n        if self.config.crf:\n            mask3d = mask3d.flatten(end_dim=1)\n            token_index = mask3d[:, 0]\n            mask3d = mask3d[token_index]\n            return token_index, mask3d\n        else:\n            return mask3d\n\n    def _step(self, optimizer, scheduler, grad_norm):\n        clip_grad_norm(self.model, grad_norm)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    # noinspection PyMethodOverriding\n    def build_model(self, embed: Embedding, encoder, training, **kwargs) -> torch.nn.Module:\n        # noinspection PyCallByClass\n        model = SpanBIOSemanticRoleLabelingModel(\n            embed.module(training=training, vocabs=self.vocabs),\n            encoder,\n            len(self.vocabs.srl),\n            self.config.n_mlp_rel,\n            self.config.mlp_dropout,\n            self.config.crf,\n        )\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size,\n                         sampler_builder: SamplerBuilder = None,\n                         gradient_accumulation=1,\n                         shuffle=False, device=None, logger: logging.Logger = None,\n                         transform=None,\n                         **kwargs) -> DataLoader:\n        if isinstance(data, TransformableDataset):\n            dataset = data\n        else:\n            transforms = [self.config.embed.transform(vocabs=self.vocabs), self.vocabs, FieldLength('token')]\n            if transform:\n                transforms.insert(0, transform)\n            dataset = self.build_dataset(data, transforms)\n        if self.vocabs.mutable:\n            # noinspection PyTypeChecker\n            self.build_vocabs(dataset, logger)\n        lens = [len(x['token_input_ids']) for x in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, gradient_accumulation)\n        else:\n            sampler = None\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, device=device, batch_sampler=sampler)\n\n    def build_dataset(self, data, transform):\n        dataset = CoNLL2012SRLBIODataset(data,\n                                         transform=transform,\n                                         doc_level_offset=self.config.get('doc_level_offset', True),\n                                         cache=isinstance(data, str))\n        return dataset\n\n    def build_vocabs(self, dataset, logger, **kwargs):\n        self.vocabs.srl = Vocab(pad_token=None, unk_token=None)\n        timer = CountdownTimer(len(dataset))\n        max_seq_len = 0\n        for sample in dataset:\n            max_seq_len = max(max_seq_len, len(sample['token_input_ids']))\n            timer.log(f'Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: {max_seq_len})')\n        self.vocabs['srl'].set_unk_as_safe_unk()  # C-ARGM-FRQ appears only in test set\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n        if self.config.get('delimiter') is None:\n            tokens = dataset[0]['token']\n            self.config.delimiter = guess_delimiter(tokens)\n            logger.info(f'Guess the delimiter between tokens could be [blue]\"{self.config.delimiter}\"[/blue]. '\n                        f'If not, specify `delimiter` in `fit()`')\n\n    def predict(self, data: Union[str, List[str]], batch_size: int = None, **kwargs):\n        if not data:\n            return []\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader(self.build_samples(data), batch_size, device=self.device, **kwargs)\n        results = []\n        order = []\n        for batch in dataloader:\n            pred, mask = self.feed_batch(batch)\n            prediction = self.decode_output(pred, mask, batch)\n            results.extend(self.prediction_to_result(prediction, batch))\n            order.extend(batch[IDX])\n        results = reorder(results, order)\n        if flat:\n            return results[0]\n        return results\n\n    def build_samples(self, data):\n        return [{'token': token} for token in data]\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            embed,\n            encoder=None,\n            lr=1e-3,\n            transformer_lr=1e-4,\n            adam_epsilon=1e-8,\n            warmup_steps=0.1,\n            weight_decay=0,\n            crf=False,\n            n_mlp_rel=300,\n            mlp_dropout=0.2,\n            batch_size=32,\n            gradient_accumulation=1,\n            grad_norm=1,\n            loss_reduction='mean',\n            epochs=30,\n            delimiter=None,\n            doc_level_offset=True,\n            eval_trn=False,\n            logger=None,\n            devices: Union[float, int, List[int]] = None,\n            transform=None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def compute_loss(self, criterion, pred, srl, mask):\n        if self.config.crf:\n            token_index, mask = mask\n            criterion: CRF = criterion\n            loss = -criterion.forward(pred, srl.flatten(end_dim=1)[token_index], mask,\n                                      reduction=self.config.loss_reduction)\n        else:\n            loss = criterion(pred[mask], srl[mask])\n        return loss\n\n    # noinspection PyMethodOverriding\n    @torch.no_grad()\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric, logger, ratio_width=None,\n                            filename=None, **kwargs):\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        metric.reset()\n        for idx, batch in enumerate(data):\n            pred, mask = self.feed_batch(batch)\n            loss = self.compute_loss(criterion, pred, batch['srl_id'], mask)\n            total_loss += loss.item()\n            prediction = self.decode_output(pred, mask, batch)\n            self.update_metrics(metric, prediction, batch)\n            report = f'loss: {total_loss / (idx + 1):.4f} {metric}'\n            timer.log(report, logger=logger, ratio_percentage=False, ratio_width=ratio_width)\n        return total_loss / timer.total, metric\n\n    def input_is_flat(self, data) -> bool:\n        return isinstance(data[0], str)\n\n    def prediction_to_result(self, prediction: List, batch: Dict[str, Any], delimiter=None) -> List:\n        if delimiter is None:\n            delimiter = self.config.delimiter\n        for matrix, tokens in zip(prediction, batch['token']):\n            result = []\n            for i, arguments in enumerate(matrix):\n                if arguments:\n                    pas = [(delimiter.join(tokens[x[1]:x[2]]),) + x for x in arguments]\n                    pas.insert(bisect([a[1] for a in arguments], i), (tokens[i], PRED, i, i + 1))\n                    result.append(pas)\n            yield result\n", "hanlp/components/classifiers/transformer_classifier_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-10 13:19\n\nimport math\nfrom typing import Union, Tuple, Any, Iterable\nimport tensorflow as tf\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.layers.transformers.loader_tf import build_transformer\nfrom hanlp.optimizers.adamw import create_optimizer\nfrom hanlp.transform.table_tf import TableTransform\nfrom hanlp.utils.log_util import logger\nfrom hanlp_common.util import merge_locals_kwargs\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nclass TransformerTextTransform(TableTransform):\n\n    def __init__(self, config: SerializableDict = None, map_x=False, map_y=True, x_columns=None,\n                 y_column=-1, skip_header=True, delimiter='auto', multi_label=False, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, x_columns, y_column, multi_label, skip_header, delimiter, **kwargs)\n        self.tokenizer: PreTrainedTokenizer = None\n\n    def inputs_to_samples(self, inputs, gold=False):\n        tokenizer = self.tokenizer\n        max_length = self.config.max_length\n        num_features = None\n        pad_token = None if self.label_vocab.mutable else tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n        for (X, Y) in super().inputs_to_samples(inputs, gold):\n            if self.label_vocab.mutable:\n                yield None, Y\n                continue\n            if isinstance(X, str):\n                X = (X,)\n            if num_features is None:\n                num_features = self.config.num_features\n            assert num_features == len(X), f'Numbers of features {num_features} ' \\\n                                           f'inconsistent with current {len(X)}={X}'\n            text_a = X[0]\n            text_b = X[1] if len(X) > 1 else None\n            tokens_a = self.tokenizer.tokenize(text_a)\n            tokens_b = self.tokenizer.tokenize(text_b) if text_b else None\n            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n            segment_ids = [0] * len(tokens)\n            if tokens_b:\n                tokens += tokens_b\n                segment_ids += [1] * len(tokens_b)\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            attention_mask = [1] * len(token_ids)\n            diff = max_length - len(token_ids)\n            if diff < 0:\n                # logger.warning(\n                #     f'Input tokens {tokens} exceed the max sequence length of {max_length - 2}. '\n                #     f'The exceeded part will be truncated and ignored. '\n                #     f'You are recommended to split your long text into several sentences within '\n                #     f'{max_length - 2} tokens beforehand.')\n                token_ids = token_ids[:max_length]\n                attention_mask = attention_mask[:max_length]\n                segment_ids = segment_ids[:max_length]\n            elif diff > 0:\n                token_ids += [pad_token] * diff\n                attention_mask += [0] * diff\n                segment_ids += [0] * diff\n\n            assert len(token_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_ids), max_length)\n            assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask),\n                                                                                                max_length)\n            assert len(segment_ids) == max_length, \"Error with input length {} vs {}\".format(len(segment_ids),\n                                                                                             max_length)\n\n            label = Y\n            yield (token_ids, attention_mask, segment_ids), label\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        max_length = self.config.max_length\n        types = (tf.int32, tf.int32, tf.int32), tf.string\n        shapes = ([max_length], [max_length], [max_length]), [None, ] if self.config.get('multi_label', None) else []\n        values = (0, 0, 0), self.label_vocab.safe_pad_token\n        return types, shapes, values\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        logger.fatal('map_x should always be set to True')\n        exit(1)\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        if self.config.get('multi_label', None):\n            # need to change index to binary vector\n            mapped = tf.map_fn(fn=lambda x: tf.cast(self.label_vocab.lookup(x), tf.int32), elems=y,\n                               fn_output_signature=tf.TensorSpec(dtype=tf.dtypes.int32, shape=[None, ]))\n            one_hots = tf.one_hot(mapped, len(self.label_vocab))\n            idx = tf.reduce_sum(one_hots, -2)\n        else:\n            idx = self.label_vocab.lookup(y)\n        return idx\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None,\n                     batch=None) -> Iterable:\n        # Prediction to be Y > 0:\n        if self.config.get('multi_label', None):\n            preds = Y\n        else:\n            preds = tf.argmax(Y, axis=-1)\n        for y in preds:\n            yield self.label_vocab.idx_to_token[y]\n\n    def input_is_single_sample(self, input: Any) -> bool:\n        return isinstance(input, (str, tuple))\n\n\nclass TransformerClassifierTF(KerasComponent):\n\n    def __init__(self, bert_text_transform=None) -> None:\n        if not bert_text_transform:\n            bert_text_transform = TransformerTextTransform()\n        super().__init__(bert_text_transform)\n        self.model: tf.keras.Model\n        self.transform: TransformerTextTransform = bert_text_transform\n\n    # noinspection PyMethodOverriding\n    def fit(self, trn_data: Any, dev_data: Any, save_dir: str, transformer: str, max_length: int = 128,\n            optimizer='adamw', warmup_steps_ratio=0.1, use_amp=False, batch_size=32,\n            epochs=3, logger=None, verbose=1, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def evaluate_output(self, tst_data, out, num_batches, metric):\n        out.write('sentence\\tpred\\tgold\\n')\n        total, correct, score = 0, 0, 0\n        for idx, batch in enumerate(tst_data):\n            outputs = self.model.predict_on_batch(batch[0])\n            outputs = tf.argmax(outputs, axis=1)\n            for X, Y_pred, Y_gold, in zip(batch[0][0], outputs, batch[1]):\n                feature = ' '.join(self.transform.tokenizer.convert_ids_to_tokens(X.numpy()))\n                feature = feature.replace(' ##', '')  # fix sub-word generated by BERT tagger\n                out.write('{}\\t{}\\t{}\\n'.format(feature,\n                                                self._y_id_to_str(Y_pred),\n                                                self._y_id_to_str(Y_gold)))\n                total += 1\n                correct += int(tf.equal(Y_pred, Y_gold).numpy())\n            score = correct / total\n            print('\\r{}/{} {}: {:.2f}'.format(idx + 1, num_batches, metric, score * 100), end='')\n        print()\n        return score\n\n    def _y_id_to_str(self, Y_pred) -> str:\n        return self.transform.label_vocab.idx_to_token[Y_pred.numpy()]\n\n    def build_loss(self, loss, **kwargs):\n        if loss:\n            assert isinstance(loss, tf.keras.losses.loss), 'Must specify loss as an instance in tf.keras.losses'\n            return loss\n        elif self.config.get('multi_label', None):\n            # Loss to be BinaryCrossentropy for multi-label:\n            loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        else:\n            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        return loss\n\n    # noinspection PyMethodOverriding\n    def build_optimizer(self, optimizer, use_amp, train_steps, warmup_steps, **kwargs):\n        if optimizer == 'adamw':\n            opt = create_optimizer(init_lr=5e-5, num_train_steps=train_steps, num_warmup_steps=warmup_steps)\n            # opt = tfa.optimizers.AdamW(learning_rate=3e-5, epsilon=1e-08, weight_decay=0.01)\n            # opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n            self.config.optimizer = tf.keras.utils.serialize_keras_object(opt)\n            lr_config = self.config.optimizer['config']['learning_rate']['config']\n            if hasattr(lr_config['decay_schedule_fn'], 'get_config'):\n                lr_config['decay_schedule_fn'] = dict(\n                    (k, v) for k, v in lr_config['decay_schedule_fn'].config().items() if not k.startswith('_'))\n        else:\n            opt = super().build_optimizer(optimizer)\n        if use_amp:\n            # loss scaling is currently required when using mixed precision\n            opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n        return opt\n\n    # noinspection PyMethodOverriding\n    def build_model(self, transformer, max_length, **kwargs):\n        model, self.transform.tokenizer = build_transformer(transformer, max_length, len(self.transform.label_vocab),\n                                                            tagging=False)\n        return model\n\n    def build_vocab(self, trn_data, logger):\n        train_examples = super().build_vocab(trn_data, logger)\n        warmup_steps_per_epoch = math.ceil(train_examples * self.config.warmup_steps_ratio / self.config.batch_size)\n        self.config.warmup_steps = warmup_steps_per_epoch * self.config.epochs\n        return train_examples\n\n    def build_metrics(self, metrics, logger, **kwargs):\n        if self.config.get('multi_label', None):\n            metric = tf.keras.metrics.BinaryAccuracy('binary_accuracy')\n        else:\n            metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n        return [metric]\n", "hanlp/components/classifiers/fasttext_classifier.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-09-28 13:31\nimport os\nimport sys\nfrom typing import List, Union\n\nimport fasttext\nfrom fasttext.FastText import _FastText\n\nimport hanlp\nfrom hanlp.common.component import Component\nfrom hanlp.utils.io_util import get_resource, stdout_redirected\nfrom hanlp_common.io import load_json\nfrom hanlp_common.reflection import classpath_of\nfrom hanlp_common.structure import SerializableDict\n\n\nclass FastTextClassifier(Component):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._model: _FastText = None\n        self.config = SerializableDict({\n            'classpath': classpath_of(self),\n            'hanlp_version': hanlp.__version__,\n        })\n\n    def load(self, save_dir, model_path=None, **kwargs):\n        config_path = os.path.join(save_dir, 'config.json')\n        if os.path.isfile(config_path):\n            self.config: dict = load_json(config_path)\n            model_path = self.config.get('model_path', model_path)\n        else:\n            model_path = model_path or save_dir\n            self.config['model_path'] = model_path\n        filepath = get_resource(model_path)\n        with stdout_redirected(to=os.devnull, stdout=sys.stderr):\n            self._model = fasttext.load_model(filepath)\n\n    def predict(self, text: Union[str, List[str]], topk=False, prob=False, max_len=None, **kwargs):\n        \"\"\"\n        Classify text.\n\n        Args:\n            text: A document or a list of documents.\n            topk: ``True`` or ``int`` to return the top-k labels.\n            prob: Return also probabilities.\n            max_len: Strip long document into ``max_len`` characters for faster prediction.\n            **kwargs: Not used\n\n        Returns:\n            Classification results.\n        \"\"\"\n        num_labels = len(self._model.get_labels())\n        flat = isinstance(text, str)\n        if flat:\n            text = [text]\n        if not isinstance(topk, list):\n            topk = [topk] * len(text)\n        if not isinstance(prob, list):\n            prob = [prob] * len(text)\n        if max_len:\n            text = [x[:max_len] for x in text]\n        text = [x.replace('\\n', ' ') for x in text]\n        batch_labels, batch_probs = self._model.predict(text, k=num_labels)\n        results = []\n        for labels, probs, k, p in zip(batch_labels, batch_probs, topk, prob):\n            labels = [self._strip_prefix(x) for x in labels]\n            if k is False:\n                labels = labels[0]\n            elif k is True:\n                pass\n            elif k:\n                labels = labels[:k]\n            if p:\n                probs = probs.tolist()\n                if k is False:\n                    result = labels, probs[0]\n                else:\n                    result = dict(zip(labels, probs))\n            else:\n                result = labels\n            results.append(result)\n        if flat:\n            results = results[0]\n        return results\n\n    @property\n    def labels(self):\n        return [self._strip_prefix(x) for x in self._model.get_labels()]\n\n    @staticmethod\n    def _strip_prefix(label: str):\n        return label[len('__label__'):]\n", "hanlp/components/classifiers/transformer_regression_hf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2023-02-17 17:54\nimport logging\nfrom typing import List, Union, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, PreTrainedTokenizer, AutoTokenizer\n\nfrom hanlp.common.dataset import TableDataset, PadSequenceDataLoader, SortingSamplerBuilder\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import split_dict, reorder\n\n\nclass TransformerRegressionHF(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self._tokenizer: PreTrainedTokenizer = None\n\n    def build_dataloader(self, data, sampler_builder=None, shuffle=False, device=None,\n                         logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        dataset = TableDataset(data)\n        lens = [len(sample['input_ids']) for sample in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, 1)\n        else:\n            sampler = SortingSamplerBuilder(batch_size=32).build(lens, shuffle, 1)\n        loader = PadSequenceDataLoader(dataset=dataset,\n                                       batch_sampler=sampler,\n                                       pad={'input_ids': self._tokenizer.pad_token_id},\n                                       device=device,\n                                       vocabs=self.vocabs)\n        return loader\n\n    def build_optimizer(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_criterion(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_metric(self, **kwargs):\n        raise NotImplementedError()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        raise NotImplementedError()\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        raise NotImplementedError()\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        raise NotImplementedError()\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self._tokenizer = AutoTokenizer.from_pretrained(save_dir)\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n\n    def build_model(self, training=True, save_dir=None, **kwargs) -> torch.nn.Module:\n        return AutoModelForSequenceClassification.from_pretrained(save_dir)\n\n    def predict(self, text: Union[str, List[str]], **kwargs):\n        \"\"\"\n        Classify text.\n\n        Args:\n            text: A document or a list of documents.\n            topk: ``True`` or ``int`` to return the top-k labels.\n            prob: Return also probabilities.\n            max_len: Strip long document into ``max_len`` characters for faster prediction.\n            **kwargs: Not used\n\n        Returns:\n            Classification results.\n        \"\"\"\n        flat = isinstance(text, str)\n        if flat:\n            text = [text]\n        # noinspection PyTypeChecker\n        dataloader = self.build_dataloader(\n            split_dict(self._tokenizer(text, max_length=self.model.config.max_position_embeddings, truncation=True,\n                                       return_token_type_ids=False, return_attention_mask=False)),\n            device=self.device)\n        results = []\n        order = []\n        for batch in dataloader:\n            logits = self.model(input_ids=batch['input_ids']).logits\n            logits = logits.squeeze(-1).clip(-1, 1)\n            logits = logits.tolist()\n            results.extend(logits)\n            order.extend(batch[IDX])\n        results = reorder(results, order)\n        if flat:\n            results = results[0]\n        return results\n", "hanlp/components/classifiers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-10 13:18", "hanlp/components/classifiers/transformer_classifier.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-08 16:31\nimport logging\nfrom abc import ABC\nfrom typing import Callable, Union\nfrom typing import List\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom hanlp_common.constant import IDX\nfrom hanlp.common.dataset import TableDataset, SortingSampler, PadSequenceDataLoader, TransformableDataset\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.components.distillation.schedulers import LinearTeacherAnnealingScheduler\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.layers.transformers.encoder import TransformerEncoder\nfrom hanlp.layers.transformers.pt_imports import PreTrainedModel, AutoTokenizer, BertTokenizer, AutoTokenizer_\nfrom hanlp.layers.transformers.utils import transformer_sliding_window, build_optimizer_scheduler_with_transformer\nfrom hanlp.metrics.accuracy import CategoricalAccuracy\nfrom hanlp.transform.transformer_tokenizer import TransformerTextTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, merge_dict, isdebugging\n\n\nclass TransformerClassificationModel(nn.Module):\n\n    def __init__(self,\n                 transformer: PreTrainedModel,\n                 num_labels: int,\n                 max_seq_length=512) -> None:\n        super().__init__()\n        self.max_seq_length = max_seq_length\n        self.transformer = transformer\n        self.dropout = nn.Dropout(transformer.config.hidden_dropout_prob)\n        self.classifier = nn.Linear(transformer.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        seq_length = input_ids.size(-1)\n        if seq_length > self.max_seq_length:\n            sequence_output = transformer_sliding_window(self.transformer, input_ids,\n                                                         max_pieces=self.max_seq_length, ret_cls='max')\n        else:\n            sequence_output = self.transformer(input_ids, attention_mask, token_type_ids)[0][:, 0, :]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n        return logits\n\n\nclass TransformerComponent(TorchComponent, ABC):\n    def __init__(self, **kwargs) -> None:\n        \"\"\" The base class for transorfmer based components. If offers methods to build transformer tokenizers\n        , optimizers and models.\n\n        Args:\n            **kwargs: Passed to config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.transformer_tokenizer = None\n\n    def build_optimizer(self,\n                        trn,\n                        epochs,\n                        lr,\n                        adam_epsilon,\n                        weight_decay,\n                        warmup_steps,\n                        transformer_lr=None,\n                        teacher=None,\n                        **kwargs):\n        num_training_steps = len(trn) * epochs // self.config.get('gradient_accumulation', 1)\n        if transformer_lr is None:\n            transformer_lr = lr\n        transformer = self.model.encoder.transformer\n        optimizer, scheduler = build_optimizer_scheduler_with_transformer(self.model, transformer,\n                                                                          lr, transformer_lr,\n                                                                          num_training_steps, warmup_steps,\n                                                                          weight_decay, adam_epsilon)\n        if teacher:\n            lambda_scheduler = LinearTeacherAnnealingScheduler(num_training_steps)\n            scheduler = (scheduler, lambda_scheduler)\n        return optimizer, scheduler\n\n    def fit(self, trn_data, dev_data, save_dir,\n            transformer=None,\n            lr=5e-5,\n            transformer_lr=None,\n            adam_epsilon=1e-8,\n            weight_decay=0,\n            warmup_steps=0.1,\n            batch_size=32,\n            gradient_accumulation=1,\n            grad_norm=5.0,\n            transformer_grad_norm=None,\n            average_subwords=False,\n            scalar_mix: Union[ScalarMixWithDropoutBuilder, int] = None,\n            word_dropout=None,\n            hidden_dropout=None,\n            max_seq_len=None,\n            ret_raw_hidden_states=False,\n            batch_max_tokens=None,\n            epochs=3,\n            logger=None,\n            devices: Union[float, int, List[int]] = None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def on_config_ready(self, **kwargs):\n        super().on_config_ready(**kwargs)\n        if 'albert_chinese' in self.config.transformer:\n            self.transformer_tokenizer = BertTokenizer.from_pretrained(self.config.transformer, use_fast=True)\n        else:\n            self.transformer_tokenizer = AutoTokenizer_.from_pretrained(self.config.transformer, use_fast=True)\n\n    def build_transformer(self, training=True):\n        transformer = TransformerEncoder(self.config.transformer, self.transformer_tokenizer,\n                                         self.config.average_subwords,\n                                         self.config.scalar_mix, self.config.word_dropout,\n                                         ret_raw_hidden_states=self.config.ret_raw_hidden_states,\n                                         training=training)\n        transformer_layers = self.config.get('transformer_layers', None)\n        if transformer_layers:\n            transformer.transformer.encoder.layer = transformer.transformer.encoder.layer[:transformer_layers]\n        return transformer\n\n\nclass TransformerClassifier(TransformerComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"A classifier using transformer as encoder.\n\n        Args:\n            **kwargs: Passed to config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: TransformerClassificationModel = None\n\n    def build_criterion(self, **kwargs):\n        criterion = nn.CrossEntropyLoss()\n        return criterion\n\n    def build_metric(self, **kwargs):\n        return CategoricalAccuracy()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, **kwargs):\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger)\n            if dev:\n                self.evaluate_dataloader(dev, criterion, metric, logger, ratio_width=ratio_width)\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            dev_score = metric.get_metric()\n            if dev_score > best_metric:\n                self.save_weights(save_dir)\n                best_metric = dev_score\n                report += ' [red]saved[/red]'\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n\n    @property\n    def label_vocab(self):\n        return self.vocabs[self.config.label_key]\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        self.model.train()\n        timer = CountdownTimer(len(trn))\n        optimizer, scheduler = optimizer\n        total_loss = 0\n        metric.reset()\n        for batch in trn:\n            optimizer.zero_grad()\n            logits = self.feed_batch(batch)\n            target = batch['label_id']\n            loss = self.compute_loss(criterion, logits, target, batch)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n            self.update_metric(metric, logits, target)\n            timer.log(f'loss: {total_loss / (timer.current + 1):.4f} acc: {metric.get_metric():.2%}',\n                      ratio_percentage=None,\n                      logger=logger)\n            del loss\n        return total_loss / timer.total\n\n    def update_metric(self, metric, logits: torch.Tensor, target, output=None):\n        metric(logits, target)\n        if output:\n            label_ids = logits.argmax(-1)\n            return label_ids\n\n    def compute_loss(self, criterion, logits, target, batch):\n        loss = criterion(logits, target)\n        return loss\n\n    def feed_batch(self, batch) -> torch.LongTensor:\n        logits = self.model(*[batch[key] for key in ['input_ids', 'attention_mask', 'token_type_ids']])\n        return logits\n\n    # noinspection PyMethodOverriding\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric,\n                            logger,\n                            ratio_width=None,\n                            filename=None,\n                            output=None,\n                            **kwargs):\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        metric.reset()\n        num_samples = 0\n        if output:\n            output = open(output, 'w')\n        for batch in data:\n            logits = self.feed_batch(batch)\n            target = batch['label_id']\n            loss = self.compute_loss(criterion, logits, target, batch)\n            total_loss += loss.item()\n            label_ids = self.update_metric(metric, logits, target, output)\n            if output:\n                labels = [self.vocabs[self.config.label_key].idx_to_token[i] for i in label_ids.tolist()]\n                for i, label in enumerate(labels):\n                    # text_a text_b pred gold\n                    columns = [batch[self.config.text_a_key][i]]\n                    if self.config.text_b_key:\n                        columns.append(batch[self.config.text_b_key][i])\n                    columns.append(label)\n                    columns.append(batch[self.config.label_key][i])\n                    output.write('\\t'.join(columns))\n                    output.write('\\n')\n            num_samples += len(target)\n            report = f'loss: {total_loss / (timer.current + 1):.4f} acc: {metric.get_metric():.2%}'\n            if filename:\n                report = f'{filename} {report} {num_samples / timer.elapsed:.0f} samples/sec'\n            timer.log(report, ratio_percentage=None, logger=logger, ratio_width=ratio_width)\n        if output:\n            output.close()\n        return total_loss / timer.total\n\n    # noinspection PyMethodOverriding\n    def build_model(self, transformer, training=True, **kwargs) -> torch.nn.Module:\n        # config: PretrainedConfig = AutoConfig.from_pretrained(transformer)\n        # config.num_labels = len(self.vocabs.label)\n        # config.hidden_dropout_prob = self.config.hidden_dropout_prob\n        transformer = self.build_transformer(training=training).transformer\n        model = TransformerClassificationModel(transformer, len(self.vocabs.label))\n        # truncated_normal_(model.classifier.weight, mean=0.02, std=0.05)\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size, shuffle, device, text_a_key, text_b_key,\n                         label_key,\n                         logger: logging.Logger = None,\n                         sorting=True,\n                         **kwargs) -> DataLoader:\n        if not batch_size:\n            batch_size = self.config.batch_size\n        dataset = self.build_dataset(data)\n        dataset.append_transform(self.vocabs)\n        if self.vocabs.mutable:\n            if not any([text_a_key, text_b_key]):\n                if len(dataset.headers) == 2:\n                    self.config.text_a_key = dataset.headers[0]\n                    self.config.label_key = dataset.headers[1]\n                elif len(dataset.headers) >= 3:\n                    self.config.text_a_key, self.config.text_b_key, self.config.label_key = dataset.headers[0], \\\n                                                                                            dataset.headers[1], \\\n                                                                                            dataset.headers[-1]\n                else:\n                    raise ValueError('Wrong dataset format')\n                report = {'text_a_key', 'text_b_key', 'label_key'}\n                report = dict((k, self.config[k]) for k in report)\n                report = [f'{k}={v}' for k, v in report.items() if v]\n                report = ', '.join(report)\n                logger.info(f'Guess [bold][blue]{report}[/blue][/bold] according to the headers of training dataset: '\n                            f'[blue]{dataset}[/blue]')\n            self.build_vocabs(dataset, logger)\n            dataset.purge_cache()\n        # if self.config.transform:\n        #     dataset.append_transform(self.config.transform)\n        dataset.append_transform(TransformerTextTokenizer(tokenizer=self.transformer_tokenizer,\n                                                          text_a_key=self.config.text_a_key,\n                                                          text_b_key=self.config.text_b_key,\n                                                          max_seq_length=self.config.max_seq_length,\n                                                          truncate_long_sequences=self.config.truncate_long_sequences,\n                                                          output_key=''))\n        batch_sampler = None\n        if sorting and not isdebugging():\n            if dataset.cache and len(dataset) > 1000:\n                timer = CountdownTimer(len(dataset))\n                lens = []\n                for idx, sample in enumerate(dataset):\n                    lens.append(len(sample['input_ids']))\n                    timer.log('Pre-processing and caching dataset [blink][yellow]...[/yellow][/blink]',\n                              ratio_percentage=None)\n            else:\n                lens = [len(sample['input_ids']) for sample in dataset]\n            batch_sampler = SortingSampler(lens, batch_size=batch_size, shuffle=shuffle,\n                                           batch_max_tokens=self.config.batch_max_tokens)\n        return PadSequenceDataLoader(dataset, batch_size, shuffle, batch_sampler=batch_sampler, device=device)\n\n    def build_dataset(self, data) -> TransformableDataset:\n        if isinstance(data, str):\n            dataset = TableDataset(data, cache=True)\n        elif isinstance(data, TableDataset):\n            dataset = data\n        elif isinstance(data, list):\n            dataset = TableDataset(data)\n        else:\n            raise ValueError(f'Unsupported data {data}')\n        return dataset\n\n    def predict(self, data: Union[str, List[str]], batch_size: int = None, **kwargs):\n        if not data:\n            return []\n        flat = isinstance(data, str) or isinstance(data, tuple)\n        if flat:\n            data = [data]\n        samples = []\n        for idx, d in enumerate(data):\n            sample = {IDX: idx}\n            if self.config.text_b_key:\n                sample[self.config.text_a_key] = d[0]\n                sample[self.config.text_b_key] = d[1]\n            else:\n                sample[self.config.text_a_key] = d\n            samples.append(sample)\n        dataloader = self.build_dataloader(samples,\n                                           sorting=False,\n                                           **merge_dict(self.config,\n                                                        batch_size=batch_size,\n                                                        shuffle=False,\n                                                        device=self.device,\n                                                        overwrite=True)\n                                           )\n        labels = [None] * len(data)\n        vocab = self.vocabs.label\n        for batch in dataloader:\n            logits = self.feed_batch(batch)\n            pred = logits.argmax(-1)\n            pred = pred.tolist()\n            for idx, tag in zip(batch[IDX], pred):\n                labels[idx] = vocab.idx_to_token[tag]\n        if flat:\n            return labels[0]\n        return labels\n\n    def fit(self, trn_data, dev_data, save_dir,\n            text_a_key=None,\n            text_b_key=None,\n            label_key=None,\n            transformer=None,\n            max_seq_len=512,\n            truncate_long_sequences=True,\n            # hidden_dropout_prob=0.0,\n            lr=5e-5,\n            transformer_lr=None,\n            adam_epsilon=1e-6,\n            weight_decay=0,\n            warmup_steps=0.1,\n            batch_size=32,\n            batch_max_tokens=None,\n            epochs=3,\n            logger=None,\n            # transform=None,\n            devices: Union[float, int, List[int]] = None,\n            **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        self.vocabs.label = Vocab(pad_token=None, unk_token=None)\n        for each in trn:\n            pass\n        self.vocabs.lock()\n        self.vocabs.summary(logger)\n", "hanlp/components/classifiers/transformer_classifier_hf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2023-02-17 17:54\nimport logging\nfrom typing import List, Union, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, PreTrainedTokenizer, AutoTokenizer\n\nfrom hanlp.common.dataset import TableDataset, PadSequenceDataLoader, SortingSamplerBuilder\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp_common.constant import IDX\nfrom hanlp_common.util import split_dict, reorder\n\n\nclass TransformerClassifierHF(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self._tokenizer: PreTrainedTokenizer = None\n\n    def build_dataloader(self, data, sampler_builder=None, shuffle=False, device=None,\n                         logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        dataset = TableDataset(data)\n        lens = [len(sample['input_ids']) for sample in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, 1)\n        else:\n            sampler = SortingSamplerBuilder(batch_size=32).build(lens, shuffle, 1)\n        loader = PadSequenceDataLoader(dataset=dataset,\n                                       batch_sampler=sampler,\n                                       pad={'input_ids': self._tokenizer.pad_token_id},\n                                       device=device,\n                                       vocabs=self.vocabs)\n        return loader\n\n    def build_optimizer(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_criterion(self, **kwargs):\n        raise NotImplementedError()\n\n    def build_metric(self, **kwargs):\n        raise NotImplementedError()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        raise NotImplementedError()\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        raise NotImplementedError()\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        raise NotImplementedError()\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self._tokenizer = AutoTokenizer.from_pretrained(save_dir)\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n\n    def build_model(self, training=True, save_dir=None, **kwargs) -> torch.nn.Module:\n        return AutoModelForSequenceClassification.from_pretrained(save_dir)\n\n    def predict(self, text: Union[str, List[str]], topk=False, prob=False, **kwargs):\n        \"\"\"\n        Classify text.\n\n        Args:\n            text: A document or a list of documents.\n            topk: ``True`` or ``int`` to return the top-k labels.\n            prob: Return also probabilities.\n            max_len: Strip long document into ``max_len`` characters for faster prediction.\n            **kwargs: Not used\n\n        Returns:\n            Classification results.\n        \"\"\"\n        flat = isinstance(text, str)\n        if flat:\n            text = [text]\n        if not isinstance(topk, list):\n            topk = [topk] * len(text)\n        if not isinstance(prob, list):\n            prob = [prob] * len(text)\n        # noinspection PyTypeChecker\n        dataloader = self.build_dataloader(\n            split_dict(self._tokenizer(text, max_length=self.model.config.max_position_embeddings, truncation=True,\n                                       return_token_type_ids=False, return_attention_mask=False)),\n            device=self.device)\n        results = []\n        order = []\n        id2label = self.model.config.id2label\n        for batch in dataloader:\n            logits = self.model(input_ids=batch['input_ids']).logits\n            logits, batch_labels = logits.sort(descending=True)\n            batch_labels = [[id2label[l] for l in ls] for ls in batch_labels.tolist()]\n            batch_probs = logits.softmax(dim=-1).tolist()\n            for labels, probs, i in zip(batch_labels, batch_probs, batch[IDX]):\n                k = topk[i]\n                p = prob[i]\n                if k is False:\n                    labels = labels[0]\n                elif k is True:\n                    pass\n                elif k:\n                    labels = labels[:k]\n                if p:\n                    if k is False:\n                        result = labels, probs[0]\n                    else:\n                        result = dict(zip(labels, probs))\n                else:\n                    result = labels\n                results.append(result)\n            order.extend(batch[IDX])\n        results = reorder(results, order)\n        if flat:\n            results = results[0]\n        return results\n\n    @property\n    def labels(self):\n        return [x[1] for x in sorted(self.model.config.id2label.items())]\n", "hanlp/components/sts/transformer_sts.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-20 17:03\nimport logging\nfrom typing import Union, List\nimport torch\nfrom torch.utils.data import DataLoader\nfrom hanlp.common.structure import History\nfrom hanlp.layers.transformers.pt_imports import AutoConfig_, AutoTokenizer_\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom hanlp.common.dataset import SortingSamplerBuilder, PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.datasets.sts.stsb import SemanticTextualSimilarityDataset\nfrom hanlp.layers.transformers.utils import build_optimizer_scheduler_with_transformer\nfrom hanlp.metrics.spearman_correlation import SpearmanCorrelation\nfrom hanlp.transform.transformer_tokenizer import TransformerTextTokenizer\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, reorder\nfrom hanlp_common.constant import IDX\n\n\nclass TransformerSemanticTextualSimilarity(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"\n        A simple Semantic Textual Similarity (STS) baseline which fine-tunes a transformer with a regression layer on\n        top of it.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._tokenizer = None\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size, sent_a_col=None,\n                         sent_b_col=None,\n                         similarity_col=None,\n                         delimiter='auto',\n                         gradient_accumulation=1,\n                         sampler_builder=None,\n                         shuffle=False, device=None, logger: logging.Logger = None,\n                         split=None,\n                         **kwargs) -> DataLoader:\n        dataset = SemanticTextualSimilarityDataset(data,\n                                                   sent_a_col,\n                                                   sent_b_col,\n                                                   similarity_col,\n                                                   delimiter=delimiter,\n                                                   transform=self._tokenizer,\n                                                   cache=isinstance(data, str))\n        if split == 'trn':\n            scores = [x['similarity'] for x in dataset]\n            self.config.max_score = max(scores)\n            self.config.min_score = min(scores)\n        if not sampler_builder:\n            sampler_builder = SortingSamplerBuilder(batch_size=batch_size)\n        lens = [len(x['input_ids']) for x in dataset]\n        return PadSequenceDataLoader(dataset, batch_sampler=sampler_builder.build(lens, shuffle, gradient_accumulation),\n                                     device=device,\n                                     pad={'similarity': 0.0, 'input_ids': self._tokenizer.tokenizer.pad_token_id})\n\n    def build_optimizer(self, trn, epochs, gradient_accumulation=1, lr=1e-3, transformer_lr=5e-5, adam_epsilon=1e-8,\n                        weight_decay=0.0, warmup_steps=0.1, **kwargs):\n        num_training_steps = len(trn) * epochs // gradient_accumulation\n        optimizer, scheduler = build_optimizer_scheduler_with_transformer(self.model,\n                                                                          self.model.base_model,\n                                                                          lr, transformer_lr,\n                                                                          num_training_steps, warmup_steps,\n                                                                          weight_decay, adam_epsilon)\n        return optimizer, scheduler\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    def build_metric(self, **kwargs):\n        return SpearmanCorrelation()\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, gradient_accumulation=1, **kwargs):\n        best_epoch, best_metric = 0, -1\n        timer = CountdownTimer(epochs)\n        history = History()\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, ratio_width=ratio_width,\n                                gradient_accumulation=gradient_accumulation, history=history, save_dir=save_dir)\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            self.evaluate_dataloader(dev, logger, ratio_width=ratio_width, save_dir=save_dir, metric=metric)\n            if metric > best_metric:\n                self.save_weights(save_dir)\n                best_metric = float(metric)\n                best_epoch = epoch\n                report += ' [red]saved[/red]'\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n        if best_epoch and best_epoch != epochs:\n            logger.info(f'Restored the best model with {best_metric} saved {epochs - best_epoch} epochs ago')\n            self.load_weights(save_dir)\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric: SpearmanCorrelation, logger: logging.Logger,\n                       history=None, gradient_accumulation=1, **kwargs):\n        self.model.train()\n        optimizer, scheduler = optimizer\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        metric.reset()\n        for batch in trn:\n            output = self.feed_batch(batch)\n            prediction = self.decode(output)\n            metric(prediction, batch['similarity'])\n            loss = output['loss']\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if history.step(gradient_accumulation):\n                if self.config.grad_norm:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm)\n                optimizer.step()\n                if scheduler:\n                    scheduler.step()\n                optimizer.zero_grad()\n                timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                          logger=logger)\n            del loss\n        return total_loss / timer.total\n\n    @torch.no_grad()\n    def evaluate_dataloader(self, data: DataLoader, logger: logging.Logger, metric=None, output=False, **kwargs):\n        self.model.eval()\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        metric.reset()\n        if output:\n            predictions = []\n            orders = []\n            samples = []\n        for batch in data:\n            output_dict = self.feed_batch(batch)\n            prediction = self.decode(output_dict)\n            metric(prediction, batch['similarity'])\n            if output:\n                predictions.extend(prediction.tolist())\n                orders.extend(batch[IDX])\n                samples.extend(list(zip(batch['sent_a'], batch['sent_b'])))\n            loss = output_dict['loss']\n            total_loss += loss.item()\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger)\n            del loss\n        if output:\n            predictions = reorder(predictions, orders)\n            samples = reorder(samples, orders)\n            with open(output, 'w') as out:\n                for s, p in zip(samples, predictions):\n                    out.write('\\t'.join(s + (str(p),)))\n                    out.write('\\n')\n        return total_loss / timer.total\n\n    # noinspection PyMethodOverriding\n    def build_model(self, transformer, training=True, **kwargs) -> torch.nn.Module:\n        config = AutoConfig_.from_pretrained(transformer, num_labels=1)\n        if training:\n            model = AutoModelForSequenceClassification.from_pretrained(transformer, config=config)\n        else:\n            model = AutoModelForSequenceClassification.from_config(config)\n        return model\n\n    def predict(self, data: Union[List[str], List[List[str]]], batch_size: int = None, **kwargs) -> Union[\n        float, List[float]]:\n        \"\"\" Predict the similarity between sentence pairs.\n\n        Args:\n            data: Sentence pairs.\n            batch_size: The number of samples in a batch.\n            **kwargs: Not used.\n\n        Returns:\n            Similarities between sentences.\n        \"\"\"\n        if not data:\n            return []\n        flat = isinstance(data[0], str)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader([{'sent_a': x[0], 'sent_b': x[1]} for x in data],\n                                           batch_size=batch_size or self.config.batch_size,\n                                           device=self.device)\n        orders = []\n        predictions = []\n        for batch in dataloader:\n            output_dict = self.feed_batch(batch)\n            prediction = self.decode(output_dict)\n            predictions.extend(prediction.tolist())\n            orders.extend(batch[IDX])\n        predictions = reorder(predictions, orders)\n        if flat:\n            return predictions[0]\n        return predictions\n\n    # noinspection PyMethodOverriding\n    def fit(self, trn_data, dev_data, save_dir,\n            transformer,\n            sent_a_col,\n            sent_b_col,\n            similarity_col,\n            delimiter='auto',\n            batch_size=32,\n            max_seq_len=128,\n            epochs=3,\n            lr=1e-3,\n            transformer_lr=5e-5,\n            adam_epsilon=1e-8,\n            weight_decay=0.0,\n            warmup_steps=0.1,\n            gradient_accumulation=1,\n            grad_norm=1.0,\n            sampler_builder=None,\n            devices=None,\n            logger=None,\n            seed=None,\n            finetune: Union[bool, str] = False, eval_trn=True, _device_placeholder=False, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def on_config_ready(self, transformer, max_seq_len, **kwargs):\n        super().on_config_ready(**kwargs)\n        self._tokenizer = TransformerTextTokenizer(AutoTokenizer_.from_pretrained(transformer),\n                                                   text_a_key='sent_a',\n                                                   text_b_key='sent_b',\n                                                   output_key='',\n                                                   max_seq_length=max_seq_len)\n\n    def feed_batch(self, batch) -> SequenceClassifierOutput:\n        return self.model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'],\n                          token_type_ids=batch['token_type_ids'], labels=batch.get('similarity', None))\n\n    def decode(self, output: SequenceClassifierOutput):\n        return output.logits.squeeze(-1).detach().clip(self.config.min_score, self.config.max_score)\n\n    def report_metrics(self, loss, metric):\n        return f'loss: {loss:.4f} {metric}'\n", "hanlp/components/sts/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-20 17:02\n", "hanlp/components/ner/ner_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-09-14 20:33\nfrom abc import ABC\nfrom typing import Union, Any, Tuple, Iterable\n\nimport tensorflow as tf\nfrom hanlp.components.taggers.transformers.transformer_transform_tf import TransformerTransform\n\nfrom hanlp.common.transform_tf import Transform\n\nfrom hanlp.common.keras_component import KerasComponent\nfrom hanlp.components.taggers.ngram_conv.ngram_conv_tagger import NgramConvTaggerTF\nfrom hanlp.components.taggers.rnn_tagger_tf import RNNTaggerTF\nfrom hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF\nfrom hanlp.metrics.chunking.sequence_labeling import iobes_to_span\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass IOBES_NamedEntityRecognizer(KerasComponent, ABC):\n\n    def predict_batch(self, batch, inputs=None):\n        for words, tags in zip(inputs, super().predict_batch(batch, inputs)):\n            yield from iobes_to_span(words, tags)\n\n\nclass IOBES_Transform(Transform):\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None,\n                     batch=None) -> Iterable:\n        for words, tags in zip(inputs, super().Y_to_outputs(Y, gold, inputs=inputs, X=X, batch=batch)):\n            yield from iobes_to_span(words, tags)\n\n\nclass RNNNamedEntityRecognizerTF(RNNTaggerTF, IOBES_NamedEntityRecognizer):\n\n    def fit(self, trn_data: str, dev_data: str = None, save_dir: str = None, embeddings=100, embedding_trainable=False,\n            rnn_input_dropout=0.2, rnn_units=100, rnn_output_dropout=0.2, epochs=20, logger=None,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='f1', batch_size=32,\n            dev_batch_size=32, lr_decay_per_epoch=None,\n            run_eagerly=False,\n            verbose=True, **kwargs):\n        # assert kwargs.get('run_eagerly', True), 'This component can only run eagerly'\n        # kwargs['run_eagerly'] = True\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_loss(self, loss, **kwargs):\n        if not loss:\n            loss = tf.keras.losses.SparseCategoricalCrossentropy(\n                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n                from_logits=True)\n        return super().build_loss(loss, **kwargs)\n\n\nclass NgramConvNamedEntityRecognizerTF(NgramConvTaggerTF, IOBES_NamedEntityRecognizer):\n\n    def fit(self, trn_data: Any, dev_data: Any, save_dir: str, word_embed: Union[str, int, dict] = 200,\n            ngram_embed: Union[str, int, dict] = 50, embedding_trainable=True, window_size=4, kernel_size=3,\n            filters=(200, 200, 200, 200, 200), dropout_embed=0.2, dropout_hidden=0.2, weight_norm=True,\n            loss: Union[tf.keras.losses.Loss, str] = None,\n            optimizer: Union[str, tf.keras.optimizers.Optimizer] = 'adam', metrics='f1', batch_size=100,\n            epochs=100, logger=None, verbose=True, **kwargs):\n        return super().fit(trn_data, dev_data, save_dir, word_embed, ngram_embed, embedding_trainable, window_size,\n                           kernel_size, filters, dropout_embed, dropout_hidden, weight_norm, loss, optimizer, metrics,\n                           batch_size, epochs, logger, verbose, **kwargs)\n\n\nclass IOBES_TransformerTransform(IOBES_Transform, TransformerTransform):\n    pass\n\n\nclass TransformerNamedEntityRecognizerTF(TransformerTaggerTF):\n\n    def __init__(self, transform: TransformerTransform = None) -> None:\n        if not transform:\n            transform = IOBES_TransformerTransform()\n        super().__init__(transform)\n\n    def fit(self, trn_data, dev_data, save_dir, transformer, optimizer='adamw', learning_rate=5e-5, weight_decay_rate=0,\n            epsilon=1e-8, clipnorm=1.0, warmup_steps_ratio=0, use_amp=False, max_seq_length=128, batch_size=32,\n            epochs=3, metrics='f1', run_eagerly=False, logger=None, verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n", "hanlp/components/ner/transformer_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-10-07 11:08\nimport functools\nfrom typing import Union, List, Dict, Any, Set\n\nfrom hanlp_trie import DictInterface, TrieDict\n\nfrom hanlp.common.dataset import SamplerBuilder\nfrom hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger\nfrom hanlp.metrics.chunking.sequence_labeling import get_entities\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.datasets.ner.loaders.json_ner import prune_ner_tagset\nfrom hanlp.utils.string_util import guess_delimiter\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass TransformerNamedEntityRecognizer(TransformerTagger):\n\n    def __init__(self, **kwargs) -> None:\n        r\"\"\"A simple tagger using transformers and a linear layer with an optional CRF\n        (:cite:`lafferty2001conditional`) layer for\n        NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.\n        During decoding, it performs longest-prefix-matching of these words to override the prediction from\n        underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.\n\n        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can\n            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.\n\n        Args:\n            **kwargs: Not used.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_metric(self, **kwargs):\n        return F1()\n\n    # noinspection PyMethodOverriding\n    def update_metrics(self, metric, logits, y, mask, batch, prediction):\n        for p, g in zip(prediction, self.tag_to_span(batch['tag'], batch)):\n            pred = set(p)\n            gold = set(g)\n            metric(pred, gold)\n\n    # noinspection PyMethodOverriding\n    def decode_output(self, logits, mask, batch, model=None):\n        output = super().decode_output(logits, mask, batch, model)\n        prediction = super().prediction_to_human(output, self.vocabs['tag'].idx_to_token, batch)\n        return self.tag_to_span(prediction, batch)\n\n    def tag_to_span(self, batch_tags, batch):\n        spans = []\n        sents = batch[self.config.token_key]\n        dict_whitelist = self.dict_whitelist\n        dict_blacklist = self.dict_blacklist\n        merge_types = self.config.get('merge_types', None)\n        for tags, tokens in zip(batch_tags, sents):\n            entities = get_entities(tags)\n            if dict_whitelist:\n                matches = dict_whitelist.tokenize(tokens)\n                if matches:\n                    # Fix O E-LOC O like predictions\n                    entities = get_entities(tags)\n                    for label, start, end in entities:\n                        if end - start == 1:\n                            tags[start] = 'S-' + label\n                        else:\n                            tags[start] = 'B-' + label\n                            for i in range(start + 1, end - 1):\n                                tags[i] = 'I-' + label\n                            tags[end - 1] = 'E-' + label\n                    for start, end, label in matches:\n                        if (not tags[start][0] in 'ME') and (not tags[end - 1][0] in 'BM'):\n                            if end - start == 1:\n                                tags[start] = 'S-' + label\n                            else:\n                                tags[start] = 'B-' + label\n                                for i in range(start + 1, end - 1):\n                                    tags[i] = 'I-' + label\n                                tags[end - 1] = 'E-' + label\n                    entities = get_entities(tags)\n            if merge_types and len(entities) > 1:\n                merged_entities = []\n                begin = 0\n                for i in range(1, len(entities)):\n                    if entities[begin][0] != entities[i][0] or entities[i - 1][2] != entities[i][1] \\\n                            or entities[i][0] not in merge_types:\n                        merged_entities.append((entities[begin][0], entities[begin][1], entities[i - 1][2]))\n                        begin = i\n                merged_entities.append((entities[begin][0], entities[begin][1], entities[-1][2]))\n                entities = merged_entities\n\n            if dict_blacklist:\n                pruned = []\n                delimiter_in_entity = self.config.get('delimiter_in_entity', ' ')\n                for label, start, end in entities:\n                    entity = delimiter_in_entity.join(tokens[start:end])\n                    if entity not in dict_blacklist:\n                        pruned.append((label, start, end))\n                entities = pruned\n            spans.append(entities)\n        return spans\n\n    def decorate_spans(self, spans, batch):\n        batch_ner = []\n        delimiter_in_entity = self.config.get('delimiter_in_entity', ' ')\n        for spans_per_sent, tokens in zip(spans, batch.get(f'{self.config.token_key}_', batch[self.config.token_key])):\n            ner_per_sent = []\n            for label, start, end in spans_per_sent:\n                ner_per_sent.append((delimiter_in_entity.join(tokens[start:end]), label, start, end))\n            batch_ner.append(ner_per_sent)\n        return batch_ner\n\n    def generate_prediction_filename(self, tst_data, save_dir):\n        return super().generate_prediction_filename(tst_data.replace('.tsv', '.txt'), save_dir)\n\n    def prediction_to_human(self, pred, vocab, batch):\n        return self.decorate_spans(pred, batch)\n\n    def input_is_flat(self, tokens):\n        return tokens and isinstance(tokens, list) and isinstance(tokens[0], str)\n\n    def fit(self, trn_data, dev_data, save_dir, transformer,\n            delimiter_in_entity=None,\n            merge_types: List[str] = None,\n            average_subwords=False,\n            word_dropout: float = 0.2,\n            hidden_dropout=None,\n            layer_dropout=0,\n            scalar_mix=None,\n            grad_norm=5.0,\n            lr=5e-5,\n            transformer_lr=None,\n            adam_epsilon=1e-8,\n            weight_decay=0,\n            warmup_steps=0.1,\n            crf=False,\n            secondary_encoder=None,\n            reduction='sum',\n            batch_size=32,\n            sampler_builder: SamplerBuilder = None,\n            epochs=3,\n            tagset=None,\n            token_key='token',\n            max_seq_len=None,\n            sent_delimiter=None,\n            char_level=False,\n            hard_constraint=False,\n            transform=None,\n            logger=None,\n            seed=None,\n            devices: Union[float, int, List[int]] = None,\n            **kwargs):\n        \"\"\"Fit component to training set.\n\n        Args:\n            trn_data: Training set.\n            dev_data: Development set.\n            save_dir: The directory to save trained component.\n            transformer: An identifier of a pre-trained transformer.\n            delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining\n                tokens during decoding.\n            merge_types: The types of consecutive entities to be merged.\n            average_subwords: ``True`` to average subword representations.\n            word_dropout: Dropout rate to randomly replace a subword with MASK.\n            hidden_dropout: Dropout rate applied to hidden states.\n            layer_dropout: Randomly zero out hidden states of a transformer layer.\n            scalar_mix: Layer attention.\n            grad_norm: Gradient norm for clipping.\n            lr: Learning rate for decoder.\n            transformer_lr: Learning for encoder.\n            adam_epsilon: The epsilon to use in Adam.\n            weight_decay: The weight decay to use.\n            warmup_steps: The number of warmup steps.\n            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).\n            secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden\n                states from the main encoder as input.\n            reduction: The loss reduction used in aggregating losses.\n            batch_size: The number of samples in a batch.\n            sampler_builder: The builder to build sampler, which will override batch_size.\n            epochs: The number of epochs to train.\n            tagset: Optional tagset to prune entities outside of this tagset from datasets.\n            token_key: The key to tokens in dataset.\n            max_seq_len: The maximum sequence length. Sequence longer than this will be handled by sliding\n                window.\n            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can\n                be split here.\n            char_level: Whether the sequence length is measured at char level, which is never the case for\n                lemmatization.\n            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``\n                in a sentence, it will be split at a token anyway.\n            transform: An optional transform to be applied to samples. Usually a character normalization transform is\n                passed in.\n            devices: Devices this component will live on.\n            logger: Any :class:`logging.Logger` instance.\n            seed: Random seed to reproduce this training.\n            **kwargs: Not used.\n\n        Returns:\n            The best metrics on training set.\n        \"\"\"\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, trn, logger, **kwargs):\n        super().build_vocabs(trn, logger, **kwargs)\n        if self.config.get('delimiter_in_entity', None) is None:\n            # Check the first sample to guess the delimiter between tokens in a NE\n            tokens = trn[0][self.config.token_key]\n            delimiter_in_entity = guess_delimiter(tokens)\n            logger.info(f'Guess the delimiter between tokens in named entity could be [blue]\"{delimiter_in_entity}'\n                        f'\"[/blue]. If not, specify `delimiter_in_entity` in `fit()`')\n            self.config.delimiter_in_entity = delimiter_in_entity\n\n    def build_dataset(self, data, transform=None, **kwargs):\n        dataset = super().build_dataset(data, transform, **kwargs)\n        if isinstance(data, str):\n            tagset = self.config.get('tagset', None)\n            if tagset:\n                dataset.append_transform(functools.partial(prune_ner_tagset, tagset=tagset))\n        return dataset\n\n    @property\n    def dict_whitelist(self) -> DictInterface:\n        return self.config.get('dict_whitelist', None)\n\n    @dict_whitelist.setter\n    def dict_whitelist(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            dictionary = TrieDict(dictionary)\n        self.config.dict_whitelist = dictionary\n\n    @property\n    def dict_blacklist(self) -> DictInterface:\n        return self.config.get('dict_blacklist', None)\n\n    @dict_blacklist.setter\n    def dict_blacklist(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):\n        if dictionary is not None and not isinstance(dictionary, DictInterface):\n            dictionary = TrieDict(dictionary)\n        self.config.dict_blacklist = dictionary\n", "hanlp/components/ner/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-21 17:22", "hanlp/components/ner/rnn_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-12 18:00\nfrom typing import Any\n\nimport torch\nfrom hanlp_common.util import merge_locals_kwargs\n\nimport hanlp.utils.span_util\nfrom hanlp.components.taggers.rnn_tagger import RNNTagger\nfrom hanlp.metrics.chunking.conlleval import SpanF1\n\n\nclass RNNNamedEntityRecognizer(RNNTagger):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"An old-school RNN tagger using word2vec or fasttext embeddings.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_metric(self, **kwargs):\n        return SpanF1(self.tagging_scheme)\n\n    def evaluate_dataloader(self, data, criterion, logger=None, ratio_width=None, **kwargs):\n        loss, metric = super().evaluate_dataloader(data, criterion, logger, ratio_width, **kwargs)\n        if logger:\n            logger.info(metric.result(True, False)[-1])\n        return loss, metric\n\n    def fit(self, trn_data, dev_data, save_dir, batch_size=50, epochs=100, embed=100, rnn_input=None, rnn_hidden=256,\n            drop=0.5, lr=0.001, patience=10, crf=True, optimizer='adam', token_key='token', tagging_scheme=None,\n            anneal_factor: float = 0.5, delimiter=None, anneal_patience=2, devices=None,\n            token_delimiter=None,\n            logger=None,\n            verbose=True, **kwargs):\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def update_metrics(self, metric, logits, y, mask, batch, prediction):\n        logits = self.decode_output(logits, mask, batch)\n        if isinstance(logits, torch.Tensor):\n            logits = logits.tolist()\n        metric(self._id_to_tags(logits), batch['tag'])\n\n    def predict(self, tokens: Any, batch_size: int = None, **kwargs):\n        return super().predict(tokens, batch_size, **kwargs)\n\n    def predict_data(self, data, batch_size, **kwargs):\n        outputs = super().predict_data(data, batch_size)\n        tagging_scheme = self.tagging_scheme\n        if tagging_scheme == 'IOBES':\n            entities = [hanlp.utils.span_util.iobes_tags_to_spans(y) for y in outputs]\n        elif tagging_scheme == 'BIO':\n            entities = [hanlp.utils.span_util.bio_tags_to_spans(y) for y in outputs]\n        elif tagging_scheme == 'BIOUL':\n            entities = [hanlp.utils.span_util.bioul_tags_to_spans(y) for y in outputs]\n        else:\n            raise ValueError(f'Unrecognized tag scheme {tagging_scheme}')\n        for i, (tokens, es) in enumerate(zip(data, entities)):\n            outputs[i] = [(self.config.token_delimiter.join(tokens[b:e + 1]), t, b, e + 1) for t, (b, e) in es]\n        return outputs\n\n    def save_config(self, save_dir, filename='config.json'):\n        if self.config.token_delimiter is None:\n            self.config.token_delimiter = '' if all(\n                [len(x) == 1 for x in self.vocabs[self.config.token_key].idx_to_token[-100:]]) else ' '\n        super().save_config(save_dir, filename)\n", "hanlp/components/ner/biaffine_ner/biaffine_ner_model.py": "from typing import Dict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nimport hanlp.utils.torch_util\nfrom hanlp.layers.time_distributed import TimeDistributed\nfrom ...parsers.biaffine.biaffine import Biaffine\n\n\ndef initializer_1d(input_tensor, initializer):\n    assert len(input_tensor.size()) == 1\n    input_tensor = input_tensor.view(-1, 1)\n    input_tensor = initializer(input_tensor)\n    return input_tensor.view(-1)\n\n\nclass BiaffineNamedEntityRecognitionModel(nn.Module):\n\n    def __init__(self, config, embed: torch.nn.Module, context_layer: torch.nn.Module, label_space_size):\n        super(BiaffineNamedEntityRecognitionModel, self).__init__()\n        self.config = config\n        self.lexical_dropout = float(self.config.lexical_dropout)\n        self.label_space_size = label_space_size\n\n        # Initialize layers and parameters\n        self.word_embedding_dim = embed.get_output_dim()  # get the embedding dim\n        self.embed = embed\n        # Initialize context layer\n        self.context_layer = context_layer\n        context_layer_output_dim = context_layer.get_output_dim()\n\n        self.decoder = BiaffineNamedEntityRecognitionDecoder(context_layer_output_dim, config.ffnn_size,\n                                                             label_space_size, config.loss_reduction)\n\n    def forward(self,\n                batch: Dict[str, torch.Tensor]\n                ):\n        keys = 'token_length', 'begin_offset', 'end_offset', 'label_id'\n        sent_lengths, gold_starts, gold_ends, gold_labels = [batch.get(k, None) for k in keys]\n        masks = hanlp.utils.torch_util.lengths_to_mask(sent_lengths)\n        num_sentences, max_sent_length = masks.size()\n        raw_embeddings = self.embed(batch, mask=masks)\n\n        raw_embeddings = F.dropout(raw_embeddings, self.lexical_dropout, self.training)\n\n        contextualized_embeddings = self.context_layer(raw_embeddings, masks)\n        return self.decoder.decode(contextualized_embeddings, gold_starts, gold_ends, gold_labels, masks,\n                                   max_sent_length,\n                                   num_sentences, sent_lengths)\n\n\nclass BiaffineNamedEntityRecognitionDecoder(nn.Module):\n    def __init__(self, hidden_size, ffnn_size, label_space_size, loss_reduction='sum') -> None:\n        \"\"\"An implementation of the biaffine decoder in \"Named Entity Recognition as Dependency Parsing\"\n        (:cite:`yu-etal-2020-named`).\n\n        Args:\n            hidden_size: Size of hidden states.\n            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.\n            label_space_size: Size of tag set.\n            loss_reduction: The loss reduction used in aggregating losses.\n        \"\"\"\n        super().__init__()\n        self.loss_reduction = loss_reduction\n\n        # MLPs\n        def new_mlp():\n            return TimeDistributed(nn.Linear(hidden_size, ffnn_size))\n\n        self.start_mlp = new_mlp()\n        self.end_mlp = new_mlp()\n        self.biaffine = Biaffine(ffnn_size, label_space_size)\n\n    def forward(self, contextualized_embeddings: torch.FloatTensor, batch: Dict[str, torch.Tensor], mask=None):\n        keys = 'token_length', 'begin_offset', 'end_offset', 'label_id'\n        sent_lengths, gold_starts, gold_ends, gold_labels = [batch.get(k, None) for k in keys]\n        if mask is None:\n            mask = hanlp.utils.torch_util.lengths_to_mask(sent_lengths)\n        num_sentences, max_sent_length = mask.size()\n        return self.decode(contextualized_embeddings, gold_starts, gold_ends, gold_labels, mask,\n                           max_sent_length,\n                           num_sentences, sent_lengths)\n\n    def get_dense_span_labels(self, span_starts, span_ends, span_labels, max_sentence_length):\n        num_sentences, max_spans_num = span_starts.size()\n\n        sentence_indices = torch.arange(0, num_sentences, device=span_starts.device).unsqueeze(1).expand(-1,\n                                                                                                         max_spans_num)\n\n        sparse_indices = torch.cat([sentence_indices.unsqueeze(2), span_starts.unsqueeze(2), span_ends.unsqueeze(2)],\n                                   dim=2)\n        rank = 3\n        dense_labels = torch.sparse.LongTensor(sparse_indices.view(num_sentences * max_spans_num, rank).t(),\n                                               span_labels.view(-1),\n                                               torch.Size([num_sentences] + [max_sentence_length] * (rank - 1))) \\\n            .to_dense()\n        return dense_labels\n\n    def decode(self, contextualized_embeddings, gold_starts, gold_ends, gold_labels, masks, max_sent_length,\n               num_sentences, sent_lengths):\n        # Apply MLPs to starts and ends, [num_sentences, max_sentences_length,emb]\n        candidate_starts_emb = self.start_mlp(contextualized_embeddings)\n        candidate_ends_emb = self.end_mlp(contextualized_embeddings)\n        candidate_ner_scores = self.biaffine(candidate_starts_emb, candidate_ends_emb).permute([0, 2, 3, 1])\n\n        \"\"\"generate candidate spans with argument pruning\"\"\"\n        # Generate masks\n        candidate_scores_mask = masks.unsqueeze(1) & masks.unsqueeze(2)\n        device = sent_lengths.device\n        sentence_ends_leq_starts = (\n            ~hanlp.utils.torch_util.lengths_to_mask(torch.arange(max_sent_length, device=device), max_sent_length)) \\\n            .unsqueeze_(0).expand(num_sentences, -1, -1)\n        candidate_scores_mask &= sentence_ends_leq_starts\n        candidate_ner_scores = candidate_ner_scores[candidate_scores_mask]\n        predict_dict = {\n            \"candidate_ner_scores\": candidate_ner_scores,\n\n        }\n        if gold_starts is not None:\n            gold_ner_labels = self.get_dense_span_labels(gold_starts, gold_ends, gold_labels, max_sent_length)\n            loss = torch.nn.functional.cross_entropy(candidate_ner_scores,\n                                                     gold_ner_labels[candidate_scores_mask],\n                                                     reduction=self.loss_reduction)\n            predict_dict['loss'] = loss\n        return predict_dict\n", "hanlp/components/ner/biaffine_ner/biaffine_ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-09 18:13\nimport logging\nfrom typing import Union, List, Callable, Dict, Any\n\nfrom hanlp_common.constant import IDX\nfrom hanlp.common.structure import History\nfrom hanlp.components.ner.biaffine_ner.biaffine_ner_model import BiaffineNamedEntityRecognitionModel\nfrom hanlp.datasets.ner.loaders.json_ner import JsonNERDataset, unpack_ner\nfrom hanlp.layers.transformers.utils import build_optimizer_scheduler_with_transformer\nimport torch\nfrom torch.utils.data import DataLoader\nfrom hanlp.common.dataset import PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import FieldLength, TransformList\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.utils.time_util import CountdownTimer\nfrom hanlp_common.util import merge_locals_kwargs, reorder\n\n\nclass BiaffineNamedEntityRecognizer(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\"An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats\n        every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL\n        label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no\n        assumption about the spans, it naturally supports flat NER and nested NER.\n\n        Args:\n            **kwargs: Predefined config.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model: BiaffineNamedEntityRecognitionModel = None\n\n    def build_optimizer(self,\n                        trn,\n                        epochs,\n                        lr,\n                        adam_epsilon,\n                        weight_decay,\n                        warmup_steps,\n                        transformer_lr,\n                        **kwargs):\n        # noinspection PyProtectedMember\n        if self.use_transformer:\n            num_training_steps = len(trn) * epochs // self.config.get('gradient_accumulation', 1)\n            optimizer, scheduler = build_optimizer_scheduler_with_transformer(self.model,\n                                                                              self._get_transformer(),\n                                                                              lr, transformer_lr,\n                                                                              num_training_steps, warmup_steps,\n                                                                              weight_decay, adam_epsilon)\n        else:\n            optimizer = torch.optim.Adam(self.model.parameters(), self.config.lr)\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer=optimizer,\n                mode='max',\n                factor=0.5,\n                patience=2,\n                verbose=True,\n            )\n        return optimizer, scheduler\n\n    @property\n    def use_transformer(self):\n        return 'token' not in self.vocabs\n\n    def _get_transformer(self):\n        return getattr(self.model_.embed, 'transformer', None)\n\n    def build_criterion(self, **kwargs):\n        pass\n\n    # noinspection PyProtectedMember\n    def build_metric(self, **kwargs) -> F1:\n        return F1()\n\n    def execute_training_loop(self,\n                              trn: DataLoader,\n                              dev: DataLoader,\n                              epochs,\n                              criterion,\n                              optimizer,\n                              metric,\n                              save_dir,\n                              logger: logging.Logger,\n                              devices,\n                              gradient_accumulation=1,\n                              **kwargs):\n        best_epoch, best_metric = 0, -1\n        optimizer, scheduler = optimizer\n        history = History()\n        timer = CountdownTimer(epochs)\n        ratio_width = len(f'{len(trn)}/{len(trn)}')\n        for epoch in range(1, epochs + 1):\n            logger.info(f\"[yellow]Epoch {epoch} / {epochs}:[/yellow]\")\n            self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history,\n                                gradient_accumulation=gradient_accumulation,\n                                linear_scheduler=scheduler if self._get_transformer() else None)\n            if dev:\n                self.evaluate_dataloader(dev, criterion, metric, logger, ratio_width=ratio_width)\n            report = f'{timer.elapsed_human}/{timer.total_time_human}'\n            dev_score = metric.score\n            if not self._get_transformer():\n                scheduler.step(dev_score)\n            if dev_score > best_metric:\n                self.save_weights(save_dir)\n                best_metric = dev_score\n                report += ' [red]saved[/red]'\n            timer.log(report, ratio_percentage=False, newline=True, ratio=False)\n        return best_metric\n\n    def fit_dataloader(self,\n                       trn: DataLoader,\n                       criterion,\n                       optimizer,\n                       metric,\n                       logger: logging.Logger,\n                       linear_scheduler=None,\n                       history: History = None,\n                       gradient_accumulation=1,\n                       **kwargs):\n        self.model.train()\n        timer = CountdownTimer(history.num_training_steps(len(trn), gradient_accumulation=gradient_accumulation))\n        total_loss = 0\n        self.reset_metrics(metric)\n        for batch in trn:\n            optimizer.zero_grad()\n            output_dict = self.feed_batch(batch)\n            self.update_metrics(batch, output_dict, metric)\n            loss = output_dict['loss']\n            if gradient_accumulation and gradient_accumulation > 1:\n                loss /= gradient_accumulation\n            loss.backward()\n            total_loss += loss.item()\n            if history.step(gradient_accumulation):\n                if self.config.grad_norm:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm)\n                optimizer.step()\n                if linear_scheduler:\n                    linear_scheduler.step()\n                timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                          logger=logger)\n            del loss\n        return total_loss / timer.total\n\n    # noinspection PyMethodOverriding\n    @torch.no_grad()\n    def evaluate_dataloader(self,\n                            data: DataLoader,\n                            criterion: Callable,\n                            metric,\n                            logger,\n                            ratio_width=None,\n                            output=False,\n                            **kwargs):\n        self.model.eval()\n        self.reset_metrics(metric)\n        timer = CountdownTimer(len(data))\n        total_loss = 0\n        if output:\n            fp = open(output, 'w')\n        for batch in data:\n            output_dict = self.feed_batch(batch)\n            if output:\n                for sent, pred, gold in zip(batch['token'], output_dict['prediction'], batch['ner']):\n                    fp.write('Tokens\\t' + ' '.join(sent) + '\\n')\n                    fp.write('Pred\\t' + '\\t'.join(\n                        ['[' + ' '.join(sent[x:y + 1]) + f']/{label}' for x, y, label in pred]) + '\\n')\n                    fp.write('Gold\\t' + '\\t'.join(\n                        ['[' + ' '.join(sent[x:y + 1]) + f']/{label}' for x, y, label in gold]) + '\\n')\n                    fp.write('\\n')\n            self.update_metrics(batch, output_dict, metric)\n            loss = output_dict['loss']\n            total_loss += loss.item()\n            timer.log(self.report_metrics(total_loss / (timer.current + 1), metric), ratio_percentage=None,\n                      logger=logger,\n                      ratio_width=ratio_width)\n            del loss\n        if output:\n            fp.close()\n        return total_loss / timer.total, metric\n\n    def build_model(self,\n                    training=True,\n                    **kwargs) -> torch.nn.Module:\n        # noinspection PyTypeChecker\n        # embed: torch.nn.Embedding = self.config.embed.module(vocabs=self.vocabs)[0].embed\n        model = BiaffineNamedEntityRecognitionModel(self.config,\n                                                    self.config.embed.module(vocabs=self.vocabs),\n                                                    self.config.context_layer,\n                                                    len(self.vocabs.label))\n        return model\n\n    # noinspection PyMethodOverriding\n    def build_dataloader(self, data, batch_size, shuffle, device, logger: logging.Logger = None, vocabs=None,\n                         sampler_builder=None,\n                         gradient_accumulation=1,\n                         **kwargs) -> DataLoader:\n        if vocabs is None:\n            vocabs = self.vocabs\n        transform = TransformList(unpack_ner, FieldLength('token'))\n        if isinstance(self.config.embed, Embedding):\n            transform.append(self.config.embed.transform(vocabs=vocabs))\n        transform.append(self.vocabs)\n        dataset = self.build_dataset(data, vocabs, transform)\n        if vocabs.mutable:\n            self.build_vocabs(dataset, logger, vocabs)\n        if 'token' in vocabs:\n            lens = [x['token'] for x in dataset]\n        else:\n            lens = [len(x['token_input_ids']) for x in dataset]\n        if sampler_builder:\n            sampler = sampler_builder.build(lens, shuffle, gradient_accumulation)\n        else:\n            sampler = None\n        return PadSequenceDataLoader(batch_sampler=sampler,\n                                     device=device,\n                                     dataset=dataset)\n\n    def build_dataset(self, data, vocabs, transform):\n        dataset = JsonNERDataset(data, transform=transform,\n                                 doc_level_offset=self.config.get('doc_level_offset', True),\n                                 tagset=self.config.get('tagset', None))\n        dataset.append_transform(vocabs)\n        if isinstance(data, str):\n            dataset.purge_cache()  # Enable cache\n        return dataset\n\n    def predict(self, data: Union[List[str], List[List[str]]], batch_size: int = None, ret_tokens=True, **kwargs):\n        if not data:\n            return []\n        flat = self.input_is_flat(data)\n        if flat:\n            data = [data]\n        dataloader = self.build_dataloader([{'token': x} for x in data], batch_size, False, self.device)\n        predictions = []\n        orders = []\n        for batch in dataloader:\n            output_dict = self.feed_batch(batch)\n            token = batch['token']\n            prediction = output_dict['prediction']\n            self.prediction_to_result(token, prediction, predictions, ret_tokens)\n            orders.extend(batch[IDX])\n        predictions = reorder(predictions, orders)\n        if flat:\n            return predictions[0]\n        return predictions\n\n    @staticmethod\n    def prediction_to_result(token, prediction, predictions: List, ret_tokens: Union[bool, str]):\n        for tokens, ner in zip(token, prediction):\n            prediction_per_sent = []\n            for i, (b, e, l) in enumerate(ner):\n                if ret_tokens is not None:\n                    entity = tokens[b: e + 1]\n                    if isinstance(ret_tokens, str):\n                        entity = ret_tokens.join(entity)\n                    prediction_per_sent.append((entity, l, b, e + 1))\n                else:\n                    prediction_per_sent.append((b, e + 1, l))\n            predictions.append(prediction_per_sent)\n\n    @staticmethod\n    def input_is_flat(data):\n        return isinstance(data[0], str)\n\n    # noinspection PyMethodOverriding\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            embed: Embedding,\n            context_layer,\n            sampler='sorting',\n            n_buckets=32,\n            batch_size=50,\n            lexical_dropout=0.5,\n            ffnn_size=150,\n            is_flat_ner=True,\n            doc_level_offset=True,\n            lr=1e-3,\n            transformer_lr=1e-5,\n            adam_epsilon=1e-6,\n            weight_decay=0.01,\n            warmup_steps=0.1,\n            grad_norm=5.0,\n            epochs=50,\n            loss_reduction='sum',\n            gradient_accumulation=1,\n            ret_tokens=True,\n            tagset=None,\n            sampler_builder=None,\n            devices=None,\n            logger=None,\n            seed=None,\n            **kwargs\n            ):\n        \"\"\"\n\n        Args:\n            trn_data: Path to training set.\n            dev_data: Path to dev set.\n            save_dir: The directory to save trained component.\n            embed: Embeddings to use.\n            context_layer: A contextualization layer (transformer or RNN).\n            sampler: Sampler to use.\n            n_buckets: Number of buckets to use in KMeans sampler.\n            batch_size: The number of samples in a batch.\n            lexical_dropout: Dropout applied to hidden states of context layer.\n            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.\n            is_flat_ner: ``True`` for flat NER, otherwise nested NER.\n            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.\n            lr: Learning rate for decoder.\n            transformer_lr: Learning rate for encoder.\n            adam_epsilon: The epsilon to use in Adam.\n            weight_decay: The weight decay to use.\n            warmup_steps: The number of warmup steps.\n            grad_norm: Gradient norm for clipping.\n            epochs: The number of epochs to train.\n            loss_reduction: The loss reduction used in aggregating losses.\n            gradient_accumulation: Number of mini-batches per update step.\n            ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.\n            tagset: Optional tagset to prune entities outside of this tagset from datasets.\n            sampler_builder: The builder to build sampler, which will override batch_size.\n            devices: Devices this component will live on.\n            logger: Any :class:`logging.Logger` instance.\n            seed: Random seed to reproduce this training.\n            **kwargs: Not used.\n\n        Returns:\n            The best metrics on training set.\n        \"\"\"\n        return super().fit(**merge_locals_kwargs(locals(), kwargs))\n\n    def build_vocabs(self, dataset, logger, vocabs, lock=True, label_vocab_name='label', **kwargs):\n        vocabs[label_vocab_name] = label_vocab = Vocab(pad_token=None, unk_token=None)\n        # Use null to indicate no relationship\n        label_vocab.add('<null>')\n        timer = CountdownTimer(len(dataset))\n        for each in dataset:\n            timer.log('Building NER vocab [blink][yellow]...[/yellow][/blink]')\n        label_vocab.set_unk_as_safe_unk()\n        if lock:\n            vocabs.lock()\n            vocabs.summary(logger)\n\n    def reset_metrics(self, metrics):\n        metrics.reset()\n\n    def report_metrics(self, loss, metrics):\n        return f'loss: {loss:.4f} {metrics}'\n\n    def feed_batch(self, batch) -> Dict[str, Any]:\n        output_dict = self.model(batch)\n        output_dict['prediction'] = self.get_pred_ner(batch['token'], output_dict['candidate_ner_scores'])\n        return output_dict\n\n    def update_metrics(self, batch: dict, prediction: Union[Dict, List], metrics):\n        if isinstance(prediction, dict):\n            prediction = prediction['prediction']\n        assert len(prediction) == len(batch['ner'])\n        for pred, gold in zip(prediction, batch['ner']):\n            metrics(set(pred), set(gold))\n\n    def get_pred_ner(self, sentences, span_scores):\n        is_flat_ner = self.config.is_flat_ner\n        candidates = []\n        for sid, sent in enumerate(sentences):\n            for s in range(len(sent)):\n                for e in range(s, len(sent)):\n                    candidates.append((sid, s, e))\n\n        top_spans = [[] for _ in range(len(sentences))]\n        span_scores_cpu = span_scores.tolist()\n        for i, type in enumerate(torch.argmax(span_scores, dim=-1).tolist()):\n            if type > 0:\n                sid, s, e = candidates[i]\n                top_spans[sid].append((s, e, type, span_scores_cpu[i][type]))\n\n        top_spans = [sorted(top_span, reverse=True, key=lambda x: x[3]) for top_span in top_spans]\n        sent_pred_mentions = [[] for _ in range(len(sentences))]\n        for sid, top_span in enumerate(top_spans):\n            for ns, ne, t, _ in top_span:\n                for ts, te, _ in sent_pred_mentions[sid]:\n                    if ns < ts <= ne < te or ts < ns <= te < ne:\n                        # for both nested and flat ner no clash is allowed\n                        break\n                    if is_flat_ner and (ns <= ts <= te <= ne or ts <= ns <= ne <= te):\n                        # for flat ner nested mentions are not allowed\n                        break\n                else:\n                    sent_pred_mentions[sid].append((ns, ne, t))\n        pred_mentions = set((sid, s, e, t) for sid, spr in enumerate(sent_pred_mentions) for s, e, t in spr)\n        prediction = [[] for _ in range(len(sentences))]\n        idx_to_label = self.vocabs['label'].idx_to_token\n        for sid, s, e, t in sorted(pred_mentions):\n            prediction[sid].append((s, e, idx_to_label[t]))\n        return prediction\n", "hanlp/components/ner/biaffine_ner/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-21 18:41", "hanlp/layers/dropout.py": "# -*- coding:utf-8 -*-\n# Date: 2020-06-05 17:47\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\n\n\nclass WordDropout(nn.Module):\n    def __init__(self, p: float, oov_token: int, exclude_tokens: List[int] = None) -> None:\n        super().__init__()\n        self.oov_token = oov_token\n        self.p = p\n        if not exclude_tokens:\n            exclude_tokens = [0]\n        self.exclude = exclude_tokens\n\n    @staticmethod\n    def token_dropout(tokens: torch.LongTensor,\n                      oov_token: int,\n                      exclude_tokens: List[int],\n                      p: float = 0.2,\n                      training: float = True) -> torch.LongTensor:\n        \"\"\"During training, randomly replaces some of the non-padding tokens to a mask token with probability ``p``\n        \n        Adopted from https://github.com/Hyperparticle/udify\n\n        Args:\n          tokens: The current batch of padded sentences with word ids\n          oov_token: The mask token\n          exclude_tokens: The tokens for padding the input batch\n          p: The probability a word gets mapped to the unknown token\n          training: Applies the dropout if set to ``True``\n          tokens: torch.LongTensor: \n          oov_token: int: \n          exclude_tokens: List[int]: \n          p: float:  (Default value = 0.2)\n          training: float:  (Default value = True)\n\n        Returns:\n          A copy of the input batch with token dropout applied\n\n        \"\"\"\n        if training and p > 0:\n            # This creates a mask that only considers unpadded tokens for mapping to oov\n            padding_mask = tokens.new_ones(tokens.size(), dtype=torch.bool)\n            for pad in exclude_tokens:\n                padding_mask &= (tokens != pad)\n\n            # Create a uniformly random mask selecting either the original words or OOV tokens\n            dropout_mask = (tokens.new_empty(tokens.size(), dtype=torch.float).uniform_() < p)\n            oov_mask = dropout_mask & padding_mask\n\n            oov_fill = tokens.new_empty(tokens.size(), dtype=torch.long).fill_(oov_token)\n\n            result = torch.where(oov_mask, oov_fill, tokens)\n\n            return result\n        else:\n            return tokens\n\n    def forward(self, tokens: torch.LongTensor) -> torch.LongTensor:\n        return self.token_dropout(tokens, self.oov_token, self.exclude, self.p, self.training)\n\n\nclass SharedDropout(nn.Module):\n\n    def __init__(self, p=0.5, batch_first=True):\n        super(SharedDropout, self).__init__()\n\n        self.p = p\n        self.batch_first = batch_first\n\n    def extra_repr(self):\n        s = f\"p={self.p}\"\n        if self.batch_first:\n            s += f\", batch_first={self.batch_first}\"\n\n        return s\n\n    def forward(self, x):\n        if self.training:\n            if self.batch_first:\n                mask = self.get_mask(x[:, 0], self.p)\n            else:\n                mask = self.get_mask(x[0], self.p)\n            x *= mask.unsqueeze(1) if self.batch_first else mask\n\n        return x\n\n    @staticmethod\n    def get_mask(x, p):\n        mask = x.new_empty(x.shape).bernoulli_(1 - p)\n        mask = mask / (1 - p)\n\n        return mask\n\n\nclass IndependentDropout(nn.Module):\n\n    def __init__(self, p=0.5):\n        r\"\"\"\n        For :math:`N` tensors, they use different dropout masks respectively.\n        When :math:`N-M` of them are dropped, the remaining :math:`M` ones are scaled by a factor of :math:`N/M` to compensate,\n        and when all of them are dropped together, zeros are returned.\n        Copied from https://github.com/yzhangcs/parser/master/supar/modules/dropout.py.\n\n        Args:\n            p (float):\n                The probability of an element to be zeroed. Default: 0.5.\n\n        Examples:\n            >>> x, y = torch.ones(1, 3, 5), torch.ones(1, 3, 5)\n            >>> x, y = IndependentDropout()(x, y)\n            >>> x\n            tensor([[[1., 1., 1., 1., 1.],\n                     [0., 0., 0., 0., 0.],\n                     [2., 2., 2., 2., 2.]]])\n            >>> y\n            tensor([[[1., 1., 1., 1., 1.],\n                     [2., 2., 2., 2., 2.],\n                     [0., 0., 0., 0., 0.]]])\n        \"\"\"\n        super(IndependentDropout, self).__init__()\n        self.p = p\n\n    def extra_repr(self):\n        return f\"p={self.p}\"\n\n    def forward(self, *items):\n        if self.training:\n            masks = [x.new_empty(x.shape[:2]).bernoulli_(1 - self.p)\n                     for x in items]\n            total = sum(masks)\n            scale = len(items) / total.max(torch.ones_like(total))\n            masks = [mask * scale for mask in masks]\n            items = [item * mask.unsqueeze(dim=-1)\n                     for item, mask in zip(items, masks)]\n\n        return items\n\n\nclass LockedDropout(nn.Module):\n    def __init__(self, dropout_rate=0.5):\n        super(LockedDropout, self).__init__()\n        self.dropout_rate = dropout_rate\n\n    def forward(self, x):\n        if not self.training or not self.dropout_rate:\n            return x\n\n        if x.dim() == 3:\n            mask = x.new(x.size(0), 1, x.size(2)).bernoulli_(1 - self.dropout_rate) / (1 - self.dropout_rate)\n            mask = mask.expand_as(x)\n        elif x.dim() == 2:\n            mask = torch.empty_like(x).bernoulli_(1 - self.dropout_rate) / (1 - self.dropout_rate)\n        else:\n            raise ValueError(f'Unsupported dim: {x.dim()}. Only 2d (T,C) or 3d (B,T,C) is supported')\n        return mask * x\n", "hanlp/layers/time_distributed.py": "\"\"\"\nA wrapper that unrolls the second (time) dimension of a tensor\ninto the first (batch) dimension, applies some other `Module`,\nand then rolls the time dimension back up.\n\"\"\"\n\nfrom typing import List\n\n\nimport torch\n\n\nclass TimeDistributed(torch.nn.Module):\n    \"\"\"\n    Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes\n    inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be\n    `(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.\n\n    Note that while the above gives shapes with `batch_size` first, this `Module` also works if\n    `batch_size` is second - we always just combine the first two dimensions, then split them.\n\n    It also reshapes keyword arguments unless they are not tensors or their name is specified in\n    the optional `pass_through` iterable.\n    \"\"\"\n\n    def __init__(self, module):\n        super().__init__()\n        self._module = module\n\n\n    def forward(self, *inputs, pass_through: List[str] = None, **kwargs):\n\n        pass_through = pass_through or []\n\n        reshaped_inputs = [self._reshape_tensor(input_tensor) for input_tensor in inputs]\n\n        # Need some input to then get the batch_size and time_steps.\n        some_input = None\n        if inputs:\n            some_input = inputs[-1]\n\n        reshaped_kwargs = {}\n        for key, value in kwargs.items():\n            if isinstance(value, torch.Tensor) and key not in pass_through:\n                if some_input is None:\n                    some_input = value\n\n                value = self._reshape_tensor(value)\n\n            reshaped_kwargs[key] = value\n\n        reshaped_outputs = self._module(*reshaped_inputs, **reshaped_kwargs)\n\n        if some_input is None:\n            raise RuntimeError(\"No input tensor to time-distribute\")\n\n        # Now get the output back into the right shape.\n        # (batch_size, time_steps, **output_size)\n        new_size = some_input.size()[:2] + reshaped_outputs.size()[1:]\n        outputs = reshaped_outputs.contiguous().view(new_size)\n\n        return outputs\n\n    @staticmethod\n    def _reshape_tensor(input_tensor):\n        input_size = input_tensor.size()\n        if len(input_size) <= 2:\n            raise RuntimeError(f\"No dimension to distribute: {input_size}\")\n        # Squash batch_size and time_steps into a single axis; result has shape\n        # (batch_size * time_steps, **input_size).\n        squashed_shape = [-1] + list(input_size[2:])\n        return input_tensor.contiguous().view(*squashed_shape)\n", "hanlp/layers/cnn_encoder.py": "from typing import Optional, Tuple\n\nimport torch\nfrom torch.nn import Conv1d, Linear\n\n\nclass CnnEncoder(torch.nn.Module):\n    \"\"\"\n    A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a\n    [`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,\n    input_dim)`, and the output is of shape `(batch_size, output_dim)`.\n\n    The CNN has one convolution layer for each ngram filter size. Each convolution operation gives\n    out a vector of size num_filters. The number of times a convolution layer will be used\n    is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these\n    outputs from the convolution layer and outputs the max.\n\n    This operation is repeated for every ngram size passed, and consequently the dimensionality of\n    the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets\n    (optionally) projected down to a lower dimensional output, specified by `output_dim`.\n\n    We then use a fully connected layer to project in back to the desired output_dim.  For more\n    details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural\n    Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1.\n\n    Registered as a `Seq2VecEncoder` with name \"cnn\".\n\n    # Parameters\n\n    embedding_dim : `int`, required\n        This is the input dimension to the encoder.  We need this because we can't do shape\n        inference in pytorch, and we need to know what size filters to construct in the CNN.\n    num_filters : `int`, required\n        This is the output dim for each convolutional layer, which is the number of \"filters\"\n        learned by that layer.\n    ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)\n        This specifies both the number of convolutional layers we will create and their sizes.  The\n        default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding\n        ngrams of size 2 to 5 with some number of filters.\n    conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)\n        Activation to use after the convolution layers.\n    output_dim : `Optional[int]`, optional (default=`None`)\n        After doing convolutions and pooling, we'll project the collected features into a vector of\n        this size.  If this value is `None`, we will just return the result of the max pooling,\n        giving an output of shape `len(ngram_filter_sizes) * num_filters`.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_filters: int,\n        ngram_filter_sizes: Tuple[int, ...] = (2, 3, 4, 5),\n        conv_layer_activation: str = 'ReLU',\n        output_dim: Optional[int] = None,\n    ) -> None:\n        super().__init__()\n        self._embedding_dim = embedding_dim\n        self._num_filters = num_filters\n        self._ngram_filter_sizes = ngram_filter_sizes\n        self._activation = getattr(torch.nn, conv_layer_activation)()\n        self._output_dim = output_dim\n\n        self._convolution_layers = [\n            Conv1d(\n                in_channels=self._embedding_dim,\n                out_channels=self._num_filters,\n                kernel_size=ngram_size,\n            )\n            for ngram_size in self._ngram_filter_sizes\n        ]\n        for i, conv_layer in enumerate(self._convolution_layers):\n            self.add_module(\"conv_layer_%d\" % i, conv_layer)\n\n        maxpool_output_dim = self._num_filters * len(self._ngram_filter_sizes)\n        if self._output_dim:\n            self.projection_layer = Linear(maxpool_output_dim, self._output_dim)\n        else:\n            self.projection_layer = None\n            self._output_dim = maxpool_output_dim\n\n    def get_input_dim(self) -> int:\n        return self._embedding_dim\n\n    def get_output_dim(self) -> int:\n        return self._output_dim\n\n    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):\n        if mask is not None:\n            tokens = tokens * mask.unsqueeze(-1)\n\n        # Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The\n        # convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,\n        # where the conv layer `in_channels` is our `embedding_dim`.  We thus need to transpose the\n        # tensor first.\n        tokens = torch.transpose(tokens, 1, 2)\n        # Each convolution layer returns output of size `(batch_size, num_filters, pool_length)`,\n        # where `pool_length = num_tokens - ngram_size + 1`.  We then do an activation function,\n        # then do max pooling over each filter for the whole input sequence.  Because our max\n        # pooling is simple, we just use `torch.max`.  The resultant tensor of has shape\n        # `(batch_size, num_conv_layers * num_filters)`, which then gets projected using the\n        # projection layer, if requested.\n\n        filter_outputs = []\n        for i in range(len(self._convolution_layers)):\n            convolution_layer = getattr(self, \"conv_layer_{}\".format(i))\n            filter_outputs.append(self._activation(convolution_layer(tokens)).max(dim=2)[0])\n\n        # Now we have a list of `num_conv_layers` tensors of shape `(batch_size, num_filters)`.\n        # Concatenating them gives us a tensor of shape `(batch_size, num_filters * num_conv_layers)`.\n        maxpool_output = (\n            torch.cat(filter_outputs, dim=1) if len(filter_outputs) > 1 else filter_outputs[0]\n        )\n\n        if self.projection_layer:\n            result = self.projection_layer(maxpool_output)\n        else:\n            result = maxpool_output\n        return result\n", "hanlp/layers/weight_normalization.py": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom hanlp.utils.tf_util import hanlp_register\n\n\n@hanlp_register\nclass WeightNormalization(tf.keras.layers.Wrapper):\n    \"\"\"This wrapper reparameterizes a layer by decoupling the weight's\n    magnitude and direction.\n    \n    This speeds up convergence by improving the\n    conditioning of the optimization problem.\n    Weight Normalization: A Simple Reparameterization to Accelerate\n    Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868\n    Tim Salimans, Diederik P. Kingma (2016)\n    WeightNormalization wrapper works for keras and tf layers.\n    ```python\n      net = WeightNormalization(\n          tf.keras.layers.Conv2D(2, 2, activation='relu'),\n          input_shape=(32, 32, 3),\n          data_init=True)(x)\n      net = WeightNormalization(\n          tf.keras.layers.Conv2D(16, 5, activation='relu'),\n          data_init=True)(net)\n      net = WeightNormalization(\n          tf.keras.layers.Dense(120, activation='relu'),\n          data_init=True)(net)\n      net = WeightNormalization(\n          tf.keras.layers.Dense(n_classes),\n          data_init=True)(net)\n    ```\n\n    Args:\n      layer: a layer instance\n      data_init: If\n\n    Returns:\n\n    Raises:\n      ValueError: If not initialized with a\n      ValueError: If\n      NotImplementedError: If\n\n    \"\"\"\n\n    def __init__(self, layer, data_init=True, **kwargs):\n        super(WeightNormalization, self).__init__(layer, **kwargs)\n        self.data_init = data_init\n        self._track_trackable(layer, name='layer')\n        self._init_critical_section = tf.CriticalSection(name='init_mutex')\n        self.is_rnn = isinstance(self.layer, tf.keras.layers.RNN)\n\n    def build(self, input_shape):\n        \"\"\"Build `Layer`\n\n        Args:\n          input_shape: \n\n        Returns:\n\n        \"\"\"\n        input_shape = tf.TensorShape(input_shape)\n        self.input_spec = tf.keras.layers.InputSpec(\n            shape=[None] + input_shape[1:])\n\n        if not self.layer.built:\n            self.layer.build(input_shape)\n\n        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n\n        if not hasattr(kernel_layer, 'kernel'):\n            raise ValueError('`WeightNormalization` must wrap a layer that'\n                             ' contains a `kernel` for weights')\n\n        # The kernel's filter or unit dimension is -1\n        self.layer_depth = int(kernel_layer.kernel.shape[-1])\n        self.kernel_norm_axes = list(range(kernel_layer.kernel.shape.rank - 1))\n\n        self.g = self.add_weight(\n            name='g',\n            shape=(self.layer_depth,),\n            initializer='ones',\n            dtype=kernel_layer.kernel.dtype,\n            trainable=True)\n        self.v = kernel_layer.kernel\n\n        self._initialized = self.add_weight(\n            name='initialized',\n            shape=None,\n            initializer='zeros',\n            dtype=tf.dtypes.bool,\n            trainable=False)\n\n        if self.data_init:\n            # Used for data initialization in self._data_dep_init.\n            with tf.name_scope('data_dep_init'):\n                layer_config = tf.keras.layers.serialize(self.layer)\n                layer_config['config']['trainable'] = False\n                self._naked_clone_layer = tf.keras.layers.deserialize(\n                    layer_config)\n                self._naked_clone_layer.build(input_shape)\n                self._naked_clone_layer.set_weights(self.layer.get_weights())\n                if self.is_rnn:\n                    self._naked_clone_layer.cell.activation = None\n                else:\n                    self._naked_clone_layer.activation = None\n\n        self.built = True\n\n    def call(self, inputs):\n        \"\"\"Call `Layer`\n\n        Args:\n          inputs: \n\n        Returns:\n\n        \"\"\"\n\n        def _do_nothing():\n            return tf.identity(self.g)\n\n        def _update_weights():\n            # Ensure we read `self.g` after _update_weights.\n            with tf.control_dependencies(self._initialize_weights(inputs)):\n                return tf.identity(self.g)\n\n        g = self._init_critical_section.execute(lambda: tf.cond(\n            self._initialized, _do_nothing, _update_weights))\n\n        with tf.name_scope('compute_weights'):\n            # Replace kernel by normalized weight variable.\n            self.layer.kernel = tf.nn.l2_normalize(\n                self.v, axis=self.kernel_norm_axes) * g\n\n            # Ensure we calculate result after updating kernel.\n            update_kernel = tf.identity(self.layer.kernel)\n            with tf.control_dependencies([update_kernel]):\n                outputs = self.layer(inputs)\n                return outputs\n\n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(\n            self.layer.compute_output_shape(input_shape).as_list())\n\n    def _initialize_weights(self, inputs):\n        \"\"\"Initialize weight g.\n        \n        The initial value of g could either from the initial value in v,\n        or by the input value if self.data_init is True.\n\n        Args:\n          inputs: \n\n        Returns:\n\n        \"\"\"\n        with tf.control_dependencies([\n            tf.debugging.assert_equal(  # pylint: disable=bad-continuation\n                self._initialized,\n                False,\n                message='The layer has been initialized.')\n        ]):\n            if self.data_init:\n                assign_tensors = self._data_dep_init(inputs)\n            else:\n                assign_tensors = self._init_norm()\n            assign_tensors.append(self._initialized.assign(True))\n            return assign_tensors\n\n    def _init_norm(self):\n        \"\"\"Set the weight g with the norm of the weight vector.\"\"\"\n        with tf.name_scope('init_norm'):\n            v_flat = tf.reshape(self.v, [-1, self.layer_depth])\n            v_norm = tf.linalg.norm(v_flat, axis=0)\n            g_tensor = self.g.assign(tf.reshape(v_norm, (self.layer_depth,)))\n            return [g_tensor]\n\n    def _data_dep_init(self, inputs):\n        \"\"\"Data dependent initialization.\n\n        Args:\n          inputs: \n\n        Returns:\n\n        \"\"\"\n        with tf.name_scope('data_dep_init'):\n            # Generate data dependent init values\n            x_init = self._naked_clone_layer(inputs)\n            data_norm_axes = list(range(x_init.shape.rank - 1))\n            m_init, v_init = tf.nn.moments(x_init, data_norm_axes)\n            scale_init = 1. / tf.math.sqrt(v_init + 1e-10)\n\n            # Assign data dependent init values\n            g_tensor = self.g.assign(self.g * scale_init)\n            if hasattr(self.layer, 'bias') and self.layer.bias is not None:\n                bias_tensor = self.layer.bias.assign(-m_init * scale_init)\n                return [g_tensor, bias_tensor]\n            else:\n                return [g_tensor]\n\n    def get_config(self):\n        config = {'data_init': self.data_init}\n        base_config = super(WeightNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n", "hanlp/layers/feedforward.py": "\"\"\"\nA feed-forward neural network.\n\"\"\"\nfrom typing import List, Union\n\nimport torch\nfrom hanlp.utils.torch_util import activation_from_name\n\n\nclass FeedForward(torch.nn.Module):\n    \"\"\"\n    This `Module` is a feed-forward neural network, just a sequence of `Linear` layers with\n    activation functions in between.\n\n    # Parameters\n\n    input_dim : `int`, required\n        The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.\n    num_layers : `int`, required\n        The number of `Linear` layers to apply to the input.\n    hidden_dims : `Union[int, List[int]]`, required\n        The output dimension of each of the `Linear` layers.  If this is a single `int`, we use\n        it for all `Linear` layers.  If it is a `List[int]`, `len(hidden_dims)` must be\n        `num_layers`.\n    activations : `Union[Activation, List[Activation]]`, required\n        The activation function to use after each `Linear` layer.  If this is a single function,\n        we use it after all `Linear` layers.  If it is a `List[Activation]`,\n        `len(activations)` must be `num_layers`. Activation must have torch.nn.Module type.\n    dropout : `Union[float, List[float]]`, optional (default = `0.0`)\n        If given, we will apply this amount of dropout after each layer.  Semantics of `float`\n        versus `List[float]` is the same as with other parameters.\n\n    # Examples\n\n    ```python\n    FeedForward(124, 2, [64, 32], torch.nn.ReLU(), 0.2)\n    #> FeedForward(\n    #>   (_activations): ModuleList(\n    #>     (0): ReLU()\n    #>     (1): ReLU()\n    #>   )\n    #>   (_linear_layers): ModuleList(\n    #>     (0): Linear(in_features=124, out_features=64, bias=True)\n    #>     (1): Linear(in_features=64, out_features=32, bias=True)\n    #>   )\n    #>   (_dropout): ModuleList(\n    #>     (0): Dropout(p=0.2, inplace=False)\n    #>     (1): Dropout(p=0.2, inplace=False)\n    #>   )\n    #> )\n    ```\n    \"\"\"\n\n    def __init__(\n            self,\n            input_dim: int,\n            num_layers: int,\n            hidden_dims: Union[int, List[int]],\n            activations: Union[str, List[str]],\n            dropout: Union[float, List[float]] = 0.0,\n    ) -> None:\n\n        super().__init__()\n        if not isinstance(hidden_dims, list):\n            hidden_dims = [hidden_dims] * num_layers  # type: ignore\n        if not isinstance(activations, list):\n            activations = [activations] * num_layers  # type: ignore\n        activations = [activation_from_name(a)() for a in activations]\n        if not isinstance(dropout, list):\n            dropout = [dropout] * num_layers  # type: ignore\n        if len(hidden_dims) != num_layers:\n            raise ValueError(\n                \"len(hidden_dims) (%d) != num_layers (%d)\" % (len(hidden_dims), num_layers)\n            )\n        if len(activations) != num_layers:\n            raise ValueError(\n                \"len(activations) (%d) != num_layers (%d)\" % (len(activations), num_layers)\n            )\n        if len(dropout) != num_layers:\n            raise ValueError(\n                \"len(dropout) (%d) != num_layers (%d)\" % (len(dropout), num_layers)\n            )\n        self._activations = torch.nn.ModuleList(activations)\n        input_dims = [input_dim] + hidden_dims[:-1]\n        linear_layers = []\n        for layer_input_dim, layer_output_dim in zip(input_dims, hidden_dims):\n            linear_layers.append(torch.nn.Linear(layer_input_dim, layer_output_dim))\n        self._linear_layers = torch.nn.ModuleList(linear_layers)\n        dropout_layers = [torch.nn.Dropout(p=value) for value in dropout]\n        self._dropout = torch.nn.ModuleList(dropout_layers)\n        self._output_dim = hidden_dims[-1]\n        self.input_dim = input_dim\n\n    def get_output_dim(self):\n        return self._output_dim\n\n    def get_input_dim(self):\n        return self.input_dim\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n\n        output = inputs\n        for layer, activation, dropout in zip(\n                self._linear_layers, self._activations, self._dropout\n        ):\n            output = dropout(activation(layer(output)))\n        return output\n", "hanlp/layers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-26 00:50", "hanlp/layers/scalar_mix.py": "# This file is modified from udify, which is licensed under the MIT license:\n# MIT License\n#\n# Copyright (c) 2019 Dan Kondratyuk\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nThe dot-product \"Layer Attention\" that is applied to the layers of BERT, along with layer dropout to reduce overfitting\n\"\"\"\n\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.nn import ParameterList, Parameter\n\nfrom hanlp.common.structure import ConfigTracker\n\n\nclass ScalarMixWithDropout(torch.nn.Module):\n    \"\"\"Computes a parameterised scalar mixture of N tensors, ``mixture = gamma * sum(s_k * tensor_k)``\n    where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.\n    \n    If ``do_layer_norm=True`` then apply layer normalization to each tensor before weighting.\n    \n    If ``dropout > 0``, then for each scalar weight, adjust its softmax weight mass to 0 with\n    the dropout probability (i.e., setting the unnormalized weight to -inf). This effectively\n    should redistribute dropped probability mass to all other weights.\n\n    Args:\n\n    Returns:\n\n    \"\"\"\n\n    def __init__(self,\n                 mixture_range: Tuple[int, int],\n                 do_layer_norm: bool = False,\n                 initial_scalar_parameters: List[float] = None,\n                 trainable: bool = True,\n                 dropout: float = None,\n                 dropout_value: float = -1e20,\n                 **kwargs) -> None:\n        super(ScalarMixWithDropout, self).__init__()\n        self.mixture_range = mixture_range\n        mixture_size = mixture_range[1] - mixture_range[0]\n        self.mixture_size = mixture_size\n        self.do_layer_norm = do_layer_norm\n        self.dropout = dropout\n\n        if initial_scalar_parameters is None:\n            initial_scalar_parameters = [0.0] * mixture_size\n        elif len(initial_scalar_parameters) != mixture_size:\n            raise ValueError(\"Length of initial_scalar_parameters {} differs \"\n                             \"from mixture_size {}\".format(\n                initial_scalar_parameters, mixture_size))\n\n        # self.scalar_parameters = ParameterList(\n        #     [Parameter(torch.FloatTensor([initial_scalar_parameters[i]]),\n        #                requires_grad=trainable) for i\n        #      in range(mixture_size)])\n        self.scalar_parameters = Parameter(torch.FloatTensor(initial_scalar_parameters), requires_grad=True)\n        self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)\n\n        if self.dropout:\n            dropout_mask = torch.zeros(len(self.scalar_parameters))\n            dropout_fill = torch.empty(len(self.scalar_parameters)).fill_(dropout_value)\n            self.register_buffer(\"dropout_mask\", dropout_mask)\n            self.register_buffer(\"dropout_fill\", dropout_fill)\n\n    def forward(self, tensors: List[torch.Tensor],  # pylint: disable=arguments-differ\n                mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Compute a weighted average of the ``tensors``.  The input tensors an be any shape\n        with at least two dimensions, but must all be the same shape.\n        \n        When ``do_layer_norm=True``, the ``mask`` is required input.  If the ``tensors`` are\n        dimensioned  ``(dim_0, ..., dim_{n-1}, dim_n)``, then the ``mask`` is dimensioned\n        ``(dim_0, ..., dim_{n-1})``, as in the typical case with ``tensors`` of shape\n        ``(batch_size, timesteps, dim)`` and ``mask`` of shape ``(batch_size, timesteps)``.\n        \n        When ``do_layer_norm=False`` the ``mask`` is ignored.\n\n        Args:\n          tensors: List[torch.Tensor]: \n          # pylint: disable:  (Default value = arguments-differmask: torch.Tensor = None)\n\n        Returns:\n\n        \"\"\"\n        if len(tensors) != self.mixture_size:\n            tensors = tensors[self.mixture_range[0]:self.mixture_range[1]]\n        if len(tensors) != self.mixture_size:\n            raise ValueError(\"{} tensors were passed, but the module was initialized to \"\n                             \"mix {} tensors.\".format(len(tensors), self.mixture_size))\n\n        def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):\n            tensor_masked = tensor * broadcast_mask\n            mean = torch.sum(tensor_masked) / num_elements_not_masked\n            variance = torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked\n            return (tensor - mean) / torch.sqrt(variance + 1E-12)\n\n        weights = self.scalar_parameters\n\n        if self.dropout:\n            weights = torch.where(self.dropout_mask.uniform_() > self.dropout, weights, self.dropout_fill)\n\n        normed_weights = torch.nn.functional.softmax(weights, dim=0)\n\n        if not self.do_layer_norm:\n            return self.gamma * torch.einsum('i,ijkl->jkl', normed_weights, tensors)\n            # pieces = []\n            # for weight, tensor in zip(normed_weights, tensors):\n            #     pieces.append(weight * tensor)\n            # return self.gamma * sum(pieces)\n        else:\n            normed_weights = torch.split(normed_weights, split_size_or_sections=1)\n            mask_float = mask.float()\n            broadcast_mask = mask_float.unsqueeze(-1)\n            input_dim = tensors[0].size(-1)\n            num_elements_not_masked = torch.sum(mask_float) * input_dim\n\n            pieces = []\n            for weight, tensor in zip(normed_weights, tensors):\n                pieces.append(weight * _do_layer_norm(tensor,\n                                                      broadcast_mask, num_elements_not_masked))\n            return self.gamma * sum(pieces)\n\n\nclass ScalarMixWithDropoutBuilder(ConfigTracker, ScalarMixWithDropout):\n\n    def __init__(self,\n                 mixture_range: Tuple[int, int],\n                 do_layer_norm: bool = False,\n                 initial_scalar_parameters: List[float] = None,\n                 trainable: bool = True,\n                 dropout: float = None,\n                 dropout_value: float = -1e20) -> None:\n        super().__init__(locals())\n\n    def build(self):\n        return ScalarMixWithDropout(**self.config)\n", "hanlp/layers/feed_forward.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-06 14:37\nfrom typing import Union, List\n\nfrom hanlp.layers import feedforward\n\nfrom hanlp.common.structure import ConfigTracker\n\n\nclass FeedForward(feedforward.FeedForward, ConfigTracker):\n    def __init__(self, input_dim: int, num_layers: int, hidden_dims: Union[int, List[int]],\n                 activations: Union[str, List[str]], dropout: Union[float, List[float]] = 0.0) -> None:\n        super().__init__(input_dim, num_layers, hidden_dims, activations, dropout)\n        ConfigTracker.__init__(self, locals())\n", "hanlp/layers/crf/crf_tf.py": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n# TODO: Wrap functions in @tf.function once\n# https://github.com/tensorflow/tensorflow/issues/29075 is resolved\n\n\ndef crf_sequence_score(inputs, tag_indices, sequence_lengths,\n                       transition_params):\n    \"\"\"Computes the unnormalized score for a tag sequence.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n    to use as input to the CRF layer.\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which\n    we compute the unnormalized score.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: \n\n    Returns:\n      sequence_scores: A [batch_size] vector of unnormalized sequence scores.\n\n    \"\"\"\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    # If max_seq_len is 1, we skip the score calculation and simply gather the\n    # unary potentials of the single tag.\n    def _single_seq_fn():\n        batch_size = tf.shape(inputs, out_type=tag_indices.dtype)[0]\n\n        example_inds = tf.reshape(\n            tf.range(batch_size, dtype=tag_indices.dtype), [-1, 1])\n        sequence_scores = tf.gather_nd(\n            tf.squeeze(inputs, [1]),\n            tf.concat([example_inds, tag_indices], axis=1))\n        sequence_scores = tf.where(\n            tf.less_equal(sequence_lengths, 0), tf.zeros_like(sequence_scores),\n            sequence_scores)\n        return sequence_scores\n\n    def _multi_seq_fn():\n        # Compute the scores of the given tag sequence.\n        unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)\n        binary_scores = crf_binary_score(tag_indices, sequence_lengths,\n                                         transition_params)\n        sequence_scores = unary_scores + binary_scores\n        return sequence_scores\n\n    if inputs.shape[1] == 1:\n        return _single_seq_fn()\n    else:\n        return _multi_seq_fn()\n\n\ndef crf_multitag_sequence_score(inputs, tag_bitmap, sequence_lengths,\n                                transition_params):\n    \"\"\"Computes the unnormalized score of all tag sequences matching\n    tag_bitmap.\n    \n    tag_bitmap enables more than one tag to be considered correct at each time\n    step. This is useful when an observed output at a given time step is\n    consistent with more than one tag, and thus the log likelihood of that\n    observation must take into account all possible consistent tags.\n    \n    Using one-hot vectors in tag_bitmap gives results identical to\n    crf_sequence_score.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n    to use as input to the CRF layer.\n      tag_bitmap: A [batch_size, max_seq_len, num_tags] boolean tensor\n    representing all active tags at each index for which to calculate the\n    unnormalized score.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: \n\n    Returns:\n      sequence_scores: A [batch_size] vector of unnormalized sequence scores.\n\n    \"\"\"\n    tag_bitmap = tf.cast(tag_bitmap, dtype=tf.bool)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    filtered_inputs = tf.where(tag_bitmap, inputs,\n                               tf.fill(tf.shape(inputs), float(\"-inf\")))\n\n    # If max_seq_len is 1, we skip the score calculation and simply gather the\n    # unary potentials of all active tags.\n    def _single_seq_fn():\n        return tf.reduce_logsumexp(\n            filtered_inputs, axis=[1, 2], keepdims=False)\n\n    def _multi_seq_fn():\n        # Compute the logsumexp of all scores of sequences matching the given tags.\n        return crf_log_norm(\n            inputs=filtered_inputs,\n            sequence_lengths=sequence_lengths,\n            transition_params=transition_params)\n\n    if inputs.shape[1] == 1:\n        return _single_seq_fn()\n    else:\n        return _multi_seq_fn()\n\n\ndef crf_log_norm(inputs, sequence_lengths, transition_params):\n    \"\"\"Computes the normalization for a CRF.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n    to use as input to the CRF layer.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: \n\n    Returns:\n      log_norm: A [batch_size] vector of normalizers for a CRF.\n\n    \"\"\"\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    # Split up the first and rest of the inputs in preparation for the forward\n    # algorithm.\n    first_input = tf.slice(inputs, [0, 0, 0], [-1, 1, -1])\n    first_input = tf.squeeze(first_input, [1])\n\n    # If max_seq_len is 1, we skip the algorithm and simply reduce_logsumexp over\n    # the \"initial state\" (the unary potentials).\n    def _single_seq_fn():\n        log_norm = tf.reduce_logsumexp(first_input, [1])\n        # Mask `log_norm` of the sequences with length <= zero.\n        log_norm = tf.where(\n            tf.less_equal(sequence_lengths, 0), tf.zeros_like(log_norm),\n            log_norm)\n        return log_norm\n\n    def _multi_seq_fn():\n        \"\"\"Forward computation of alpha values.\"\"\"\n        rest_of_input = tf.slice(inputs, [0, 1, 0], [-1, -1, -1])\n        # Compute the alpha values in the forward algorithm in order to get the\n        # partition function.\n\n        alphas = crf_forward(rest_of_input, first_input, transition_params,\n                             sequence_lengths)\n        log_norm = tf.reduce_logsumexp(alphas, [1])\n        # Mask `log_norm` of the sequences with length <= zero.\n        log_norm = tf.where(\n            tf.less_equal(sequence_lengths, 0), tf.zeros_like(log_norm),\n            log_norm)\n        return log_norm\n\n    if inputs.shape[1] == 1:\n        return _single_seq_fn()\n    else:\n        return _multi_seq_fn()\n\n\ndef crf_log_likelihood(inputs,\n                       tag_indices,\n                       sequence_lengths,\n                       transition_params=None):\n    \"\"\"Computes the log-likelihood of tag sequences in a CRF.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n    to use as input to the CRF layer.\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which\n    we compute the log-likelihood.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] transition matrix, (Default value = None)\n\n    Returns:\n      log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of\n      each example, given the sequence of tag indices.\n      transition_params: A [num_tags, num_tags] transition matrix. This is\n      either provided by the caller or created in this function.\n\n    \"\"\"\n    num_tags = inputs.shape[2]\n\n    # cast type to handle different types\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    if transition_params is None:\n        initializer = tf.keras.initializers.GlorotUniform()\n        transition_params = tf.Variable(\n            initializer([num_tags, num_tags]), \"transitions\")\n\n    sequence_scores = crf_sequence_score(inputs, tag_indices, sequence_lengths,\n                                         transition_params)\n    log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)\n\n    # Normalize the scores to get the log-likelihood per example.\n    log_likelihood = sequence_scores - log_norm\n    return log_likelihood, transition_params\n\n\ndef crf_unary_score(tag_indices, sequence_lengths, inputs):\n    \"\"\"Computes the unary scores of tag sequences.\n\n    Args:\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      inputs: \n\n    Returns:\n      unary_scores: A [batch_size] vector of unary scores.\n\n    \"\"\"\n    assert len(tag_indices.shape) == 2, 'tag_indices: A [batch_size, max_seq_len] matrix of tag indices.'\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    batch_size = tf.shape(inputs)[0]\n    max_seq_len = tf.shape(inputs)[1]\n    num_tags = tf.shape(inputs)[2]\n\n    flattened_inputs = tf.reshape(inputs, [-1])\n\n    offsets = tf.expand_dims(tf.range(batch_size) * max_seq_len * num_tags, 1)\n    offsets += tf.expand_dims(tf.range(max_seq_len) * num_tags, 0)\n    # Use int32 or int64 based on tag_indices' dtype.\n    if tag_indices.dtype == tf.int64:\n        offsets = tf.cast(offsets, tf.int64)\n    flattened_tag_indices = tf.reshape(offsets + tag_indices, [-1])\n\n    unary_scores = tf.reshape(\n        tf.gather(flattened_inputs, flattened_tag_indices),\n        [batch_size, max_seq_len])\n\n    masks = tf.sequence_mask(\n        sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32)\n\n    unary_scores = tf.reduce_sum(unary_scores * masks, 1)\n    return unary_scores\n\n\ndef crf_binary_score(tag_indices, sequence_lengths, transition_params):\n    \"\"\"Computes the binary scores of tag sequences.\n\n    Args:\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: \n\n    Returns:\n      binary_scores: A [batch_size] vector of binary scores.\n\n    \"\"\"\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    num_tags = tf.shape(transition_params)[0]\n    num_transitions = tf.shape(tag_indices)[1] - 1\n\n    # Truncate by one on each side of the sequence to get the start and end\n    # indices of each transition.\n    start_tag_indices = tf.slice(tag_indices, [0, 0], [-1, num_transitions])\n    end_tag_indices = tf.slice(tag_indices, [0, 1], [-1, num_transitions])\n\n    # Encode the indices in a flattened representation.\n    flattened_transition_indices = start_tag_indices * \\\n        num_tags + end_tag_indices\n    flattened_transition_params = tf.reshape(transition_params, [-1])\n\n    # Get the binary scores based on the flattened representation.\n    binary_scores = tf.gather(flattened_transition_params,\n                              flattened_transition_indices)\n\n    masks = tf.sequence_mask(\n        sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32)\n    truncated_masks = tf.slice(masks, [0, 1], [-1, -1])\n    binary_scores = tf.reduce_sum(binary_scores * truncated_masks, 1)\n    return binary_scores\n\n\ndef crf_forward(inputs, state, transition_params, sequence_lengths):\n    \"\"\"Computes the alpha values in a linear-chain CRF.\n    \n    See http://www.cs.columbia.edu/~mcollins/fb.pdf for reference.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of unary potentials.\n      state: A [batch_size, num_tags] matrix containing the previous alpha\n    values.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n    This matrix is expanded into a [1, num_tags, num_tags] in preparation\n    for the broadcast summation occurring within the cell.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      new_alphas: A [batch_size, num_tags] matrix containing the\n      new alpha values.\n\n    \"\"\"\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    sequence_lengths = tf.maximum(\n        tf.constant(0, dtype=sequence_lengths.dtype), sequence_lengths - 2)\n    inputs = tf.transpose(inputs, [1, 0, 2])\n    transition_params = tf.expand_dims(transition_params, 0)\n\n    def _scan_fn(state, inputs):\n        state = tf.expand_dims(state, 2)\n        transition_scores = state + transition_params\n        new_alphas = inputs + tf.reduce_logsumexp(transition_scores, [1])\n        return new_alphas\n\n    all_alphas = tf.transpose(tf.scan(_scan_fn, inputs, state), [1, 0, 2])\n    idxs = tf.stack(\n        [tf.range(tf.shape(sequence_lengths)[0]), sequence_lengths], axis=1)\n    return tf.gather_nd(all_alphas, idxs)\n\n\ndef viterbi_decode(score, transition_params):\n    \"\"\"Decode the highest scoring sequence of tags outside of TensorFlow.\n    \n    This should only be used at test time.\n\n    Args:\n      score: A [seq_len, num_tags] matrix of unary potentials.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n\n    Returns:\n      viterbi: A [seq_len] list of integers containing the highest scoring tag\n      indices.\n      viterbi_score: A float containing the score for the Viterbi sequence.\n\n    \"\"\"\n    trellis = np.zeros_like(score)\n    backpointers = np.zeros_like(score, dtype=np.int32)\n    trellis[0] = score[0]\n\n    for t in range(1, score.shape[0]):\n        v = np.expand_dims(trellis[t - 1], 1) + transition_params\n        trellis[t] = score[t] + np.max(v, 0)\n        backpointers[t] = np.argmax(v, 0)\n\n    viterbi = [np.argmax(trellis[-1])]\n    for bp in reversed(backpointers[1:]):\n        viterbi.append(bp[viterbi[-1]])\n    viterbi.reverse()\n\n    viterbi_score = np.max(trellis[-1])\n    return viterbi, viterbi_score\n\n\nclass CrfDecodeForwardRnnCell(tf.keras.layers.AbstractRNNCell):\n    \"\"\"Computes the forward decoding in a linear-chain CRF.\"\"\"\n\n    def __init__(self, transition_params, **kwargs):\n        \"\"\"Initialize the CrfDecodeForwardRnnCell.\n\n        Args:\n          transition_params: A [num_tags, num_tags] matrix of binary\n            potentials. This matrix is expanded into a\n            [1, num_tags, num_tags] in preparation for the broadcast\n            summation occurring within the cell.\n        \"\"\"\n        super(CrfDecodeForwardRnnCell, self).__init__(**kwargs)\n        self._transition_params = tf.expand_dims(transition_params, 0)\n        self._num_tags = transition_params.shape[0]\n\n    @property\n    def state_size(self):\n        return self._num_tags\n\n    @property\n    def output_size(self):\n        return self._num_tags\n\n    def build(self, input_shape):\n        super(CrfDecodeForwardRnnCell, self).build(input_shape)\n\n    def call(self, inputs, state):\n        \"\"\"Build the CrfDecodeForwardRnnCell.\n\n        Args:\n          inputs: A [batch_size, num_tags] matrix of unary potentials.\n          state: A [batch_size, num_tags] matrix containing the previous step's\n        score values.\n\n        Returns:\n          backpointers: A [batch_size, num_tags] matrix of backpointers.\n          new_state: A [batch_size, num_tags] matrix of new score values.\n\n        \"\"\"\n        state = tf.expand_dims(state[0], 2)\n        transition_scores = state + self._transition_params\n        new_state = inputs + tf.reduce_max(transition_scores, [1])\n        backpointers = tf.argmax(transition_scores, 1)\n        backpointers = tf.cast(backpointers, dtype=tf.int32)\n        return backpointers, new_state\n\n\ndef crf_decode_forward(inputs, state, transition_params, sequence_lengths):\n    \"\"\"Computes forward decoding in a linear-chain CRF.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of unary potentials.\n      state: A [batch_size, num_tags] matrix containing the previous step's\n    score values.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      backpointers: A [batch_size, num_tags] matrix of backpointers.\n      new_state: A [batch_size, num_tags] matrix of new score values.\n\n    \"\"\"\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    mask = tf.sequence_mask(sequence_lengths, tf.shape(inputs)[1])\n    crf_fwd_cell = CrfDecodeForwardRnnCell(transition_params)\n    crf_fwd_layer = tf.keras.layers.RNN(\n        crf_fwd_cell, return_sequences=True, return_state=True)\n    return crf_fwd_layer(inputs, state, mask=mask)\n\n\ndef crf_decode_backward(inputs, state):\n    \"\"\"Computes backward decoding in a linear-chain CRF.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of\n    backpointer of next step (in time order).\n      state: A [batch_size, 1] matrix of tag index of next step.\n\n    Returns:\n      new_tags: A [batch_size, num_tags]\n      tensor containing the new tag indices.\n\n    \"\"\"\n    inputs = tf.transpose(inputs, [1, 0, 2])\n\n    def _scan_fn(state, inputs):\n        state = tf.squeeze(state, axis=[1])\n        idxs = tf.stack([tf.range(tf.shape(inputs)[0]), state], axis=1)\n        new_tags = tf.expand_dims(tf.gather_nd(inputs, idxs), axis=-1)\n        return new_tags\n\n    return tf.transpose(tf.scan(_scan_fn, inputs, state), [1, 0, 2])\n\n\ndef crf_decode(potentials, transition_params, sequence_length):\n    \"\"\"Decode the highest scoring sequence of tags in TensorFlow.\n    \n    This is a function for tensor.\n\n    Args:\n      potentials: A [batch_size, max_seq_len, num_tags] tensor of\n    unary potentials.\n      transition_params: A [num_tags, num_tags] matrix of\n    binary potentials.\n      sequence_length: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.\n      Contains the highest scoring tag indices.\n      best_score: A [batch_size] vector, containing the score of `decode_tags`.\n\n    \"\"\"\n    sequence_length = tf.cast(sequence_length, dtype=tf.int32)\n\n    # If max_seq_len is 1, we skip the algorithm and simply return the argmax tag\n    # and the max activation.\n    def _single_seq_fn():\n        squeezed_potentials = tf.squeeze(potentials, [1])\n        decode_tags = tf.expand_dims(tf.argmax(squeezed_potentials, axis=1), 1)\n        best_score = tf.reduce_max(squeezed_potentials, axis=1)\n        return tf.cast(decode_tags, dtype=tf.int32), best_score\n\n    def _multi_seq_fn():\n        \"\"\"Decoding of highest scoring sequence.\"\"\"\n        # Computes forward decoding. Get last score and backpointers.\n        initial_state = tf.slice(potentials, [0, 0, 0], [-1, 1, -1])\n        initial_state = tf.squeeze(initial_state, axis=[1])\n        inputs = tf.slice(potentials, [0, 1, 0], [-1, -1, -1])\n\n        sequence_length_less_one = tf.maximum(\n            tf.constant(0, dtype=sequence_length.dtype), sequence_length - 1)\n\n        backpointers, last_score = crf_decode_forward(\n            inputs, initial_state, transition_params, sequence_length_less_one)\n\n        backpointers = tf.reverse_sequence(\n            backpointers, sequence_length_less_one, seq_axis=1)\n\n        initial_state = tf.cast(tf.argmax(last_score, axis=1), dtype=tf.int32)\n        initial_state = tf.expand_dims(initial_state, axis=-1)\n\n        decode_tags = crf_decode_backward(backpointers, initial_state)\n        decode_tags = tf.squeeze(decode_tags, axis=[2])\n        decode_tags = tf.concat([initial_state, decode_tags], axis=1)\n        decode_tags = tf.reverse_sequence(\n            decode_tags, sequence_length, seq_axis=1)\n\n        best_score = tf.reduce_max(last_score, axis=1)\n        return decode_tags, best_score\n\n    if potentials.shape[1] == 1:\n        return _single_seq_fn()\n    else:\n        return _multi_seq_fn()\n", "hanlp/layers/crf/crf_layer_tf.py": "# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport tensorflow as tf\n\nfrom hanlp.layers.crf.crf_tf import crf_decode, crf_log_likelihood\n\n\nclass CRF(tf.keras.layers.Layer):\n    \"\"\"Conditional Random Field layer (tf.keras)\n    `CRF` can be used as the last layer in a network (as a classifier). Input shape (features)\n    must be equal to the number of classes the CRF can predict (a linear layer is recommended).\n    \n    Note: the loss and accuracy functions of networks using `CRF` must\n    use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)\n    as the classification of sequences are used with the layers internal weights.\n    \n    Copyright: this is a modified version of\n    https://github.com/NervanaSystems/nlp-architect/blob/master/nlp_architect/nn/tensorflow/python/keras/layers/crf.py\n\n    Args:\n      num_labels(int): the number of labels to tag each temporal input.\n    Input shape:\n      num_labels(int): the number of labels to tag each temporal input.\n    Input shape:\n    nD tensor with shape `(batch_size, sentence length, num_classes)`.\n    Output shape:\n      nD tensor with shape: `(batch_size, sentence length, num_classes)`.\n\n    Returns:\n\n    \"\"\"\n\n    def __init__(self, num_classes, **kwargs):\n        self.transitions = None\n        super(CRF, self).__init__(**kwargs)\n        # num of output labels\n        self.output_dim = int(num_classes)\n        self.input_spec = tf.keras.layers.InputSpec(min_ndim=3)\n        self.supports_masking = False\n        sequence_lengths = None\n\n    def get_config(self):\n        config = {\n            'output_dim': self.output_dim,\n            'supports_masking': self.supports_masking,\n            'transitions': tf.keras.backend.eval(self.transitions)\n        }\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        f_shape = tf.TensorShape(input_shape)\n        input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n\n        if f_shape[-1] is None:\n            raise ValueError('The last dimension of the inputs to `CRF` '\n                             'should be defined. Found `None`.')\n        if f_shape[-1] != self.output_dim:\n            raise ValueError('The last dimension of the input shape must be equal to output'\n                             ' shape. Use a linear layer if needed.')\n        self.input_spec = input_spec\n        self.transitions = self.add_weight(name='transitions',\n                                           shape=[self.output_dim, self.output_dim],\n                                           initializer='glorot_uniform',\n                                           trainable=True)\n        self.built = True\n\n    def compute_mask(self, inputs, mask=None):\n        # Just pass the received mask from previous layer, to the next layer or\n        # manipulate it if this layer changes the shape of the input\n        return mask\n\n    # pylint: disable=arguments-differ\n    def call(self, inputs, sequence_lengths=None, mask=None, training=None, **kwargs):\n        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n        if sequence_lengths is not None:\n            assert len(sequence_lengths.shape) == 2\n            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n            assert seq_len_shape[1] == 1\n            sequence_lengths = tf.keras.backend.flatten(sequence_lengths)\n        else:\n            sequence_lengths = tf.math.count_nonzero(mask, axis=1)\n\n        viterbi_sequence, _ = crf_decode(sequences, self.transitions,\n                                         sequence_lengths)\n        output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n        return tf.keras.backend.in_train_phase(sequences, output)\n\n    # def loss(self, y_true, y_pred):\n    #     y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n    #     log_likelihood, self.transitions = \\\n    #         crf_log_likelihood(y_pred,\n    #                            tf.cast(y_true, dtype=tf.int32),\n    #                            sequence_lengths,\n    #                            transition_params=self.transitions)\n    #     return tf.reduce_mean(-log_likelihood)\n\n    def compute_output_shape(self, input_shape):\n        tf.TensorShape(input_shape).assert_has_rank(3)\n        return input_shape[:2] + (self.output_dim,)\n\n    @property\n    def viterbi_accuracy(self):\n        def accuracy(y_true, y_pred):\n            shape = tf.shape(y_pred)\n            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n            viterbi_sequence, _ = crf_decode(y_pred, self.transitions,\n                                             sequence_lengths)\n            output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n            return tf.keras.metrics.categorical_accuracy(y_true, output)\n\n        accuracy.func_name = 'viterbi_accuracy'\n        return accuracy\n\n\nclass CRFLoss(object):\n\n    def __init__(self, crf: CRF, dtype) -> None:\n        super().__init__()\n        self.crf = crf\n        self.dtype = dtype\n        self.__name__ = type(self).__name__\n\n    def __call__(self, y_true, y_pred, sample_weight=None, **kwargs):\n        assert sample_weight is not None, 'your model has to support masking'\n        if len(y_true.shape) == 3:\n            y_true = tf.argmax(y_true, axis=-1)\n        sequence_lengths = tf.math.count_nonzero(sample_weight, axis=1)\n        y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n        log_likelihood, self.crf.transitions = \\\n            crf_log_likelihood(y_pred,\n                               tf.cast(y_true, dtype=tf.int32),\n                               sequence_lengths,\n                               transition_params=self.crf.transitions)\n        return tf.reduce_mean(-log_likelihood)\n\n\nclass CRFWrapper(tf.keras.Model):\n    def __init__(self, model: tf.keras.Model, num_classes=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n        self.crf = CRF(model.output.shape[-1] if not num_classes else num_classes)\n\n    def call(self, inputs, training=None, mask=None):\n        output = self.model(inputs, training=training, mask=mask)\n        viterbi_output = self.crf(output)\n        return viterbi_output\n\n    def compute_output_shape(self, input_shape):\n        return self.model.compute_output_shape(input_shape)\n", "hanlp/layers/crf/crf.py": "# Copied from https://github.com/kmkurn/pytorch-crf\n# Copyright 2017 Kemal Kurniawan <kemal@kkurniawan.com>\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n__version__ = '0.7.2'\n\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\n\n\nclass CRF(nn.Module):\n    \"\"\"Conditional random field.\n\n    This module implements a conditional random field [LMP01]_. The forward computation\n    of this class computes the log likelihood of the given sequence of tags and\n    emission score tensor. This class also has `~CRF.decode` method which finds\n    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n\n    Args:\n        num_tags: Number of tags.\n        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n\n    Attributes:\n        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n            ``(num_tags,)``.\n        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n            ``(num_tags,)``.\n        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n            ``(num_tags, num_tags)``.\n\n\n    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n       \"Conditional random fields: Probabilistic models for segmenting and\n       labeling sequence data\". *Proc. 18th International Conf. on Machine\n       Learning*. Morgan Kaufmann. pp. 282\u2013289.\n\n    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n    \"\"\"\n\n    def __init__(self, num_tags: int, batch_first: bool = True) -> None:\n        if num_tags <= 0:\n            raise ValueError(f'invalid number of tags: {num_tags}')\n        super().__init__()\n        self.num_tags = num_tags\n        self.batch_first = batch_first\n        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"Initialize the transition parameters.\n\n        The parameters will be initialized randomly from a uniform distribution\n        between -0.1 and 0.1.\n        \"\"\"\n        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.transitions, -0.1, 0.1)\n\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n\n    def forward(\n            self,\n            emissions: torch.Tensor,\n            tags: torch.LongTensor,\n            mask: Optional[torch.ByteTensor] = None,\n            reduction: str = 'sum',\n    ) -> torch.Tensor:\n        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n            reduction: Specifies  the reduction to apply to the output:\n                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n                ``sum``: the output will be summed over batches. ``mean``: the output will be\n                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n\n        Returns:\n            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n            reduction is ``none``, ``()`` otherwise.\n        \"\"\"\n        self._validate(emissions, tags=tags, mask=mask)\n        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n            raise ValueError(f'invalid reduction: {reduction}')\n        if mask is None:\n            mask = torch.ones_like(tags, dtype=torch.uint8)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            tags = tags.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        # shape: (batch_size,)\n        numerator = self._compute_score(emissions, tags, mask)\n        # shape: (batch_size,)\n        denominator = self._compute_normalizer(emissions, mask)\n        # shape: (batch_size,)\n        llh = numerator - denominator\n\n        if reduction == 'none':\n            return llh\n        if reduction == 'sum':\n            return llh.sum()\n        if reduction == 'mean':\n            return llh.mean()\n        assert reduction == 'token_mean'\n        return llh.sum() / mask.type_as(emissions).sum()\n\n    def decode(self, emissions: torch.Tensor,\n               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n\n        Returns:\n            List of list containing the best tag sequence for each batch.\n        \"\"\"\n        self._validate(emissions, mask=mask)\n        if mask is None:\n            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        return self._viterbi_decode(emissions, mask)\n\n    def _validate(\n            self,\n            emissions: torch.Tensor,\n            tags: Optional[torch.LongTensor] = None,\n            mask: Optional[torch.ByteTensor] = None) -> None:\n        if emissions.dim() != 3:\n            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n        if emissions.size(2) != self.num_tags:\n            raise ValueError(\n                f'expected last dimension of emissions is {self.num_tags}, '\n                f'got {emissions.size(2)}')\n\n        if tags is not None:\n            if emissions.shape[:2] != tags.shape:\n                raise ValueError(\n                    'the first two dimensions of emissions and tags must match, '\n                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n\n        if mask is not None:\n            if emissions.shape[:2] != mask.shape:\n                raise ValueError(\n                    'the first two dimensions of emissions and mask must match, '\n                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n            no_empty_seq = not self.batch_first and mask[0].all()\n            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n            if not no_empty_seq and not no_empty_seq_bf:\n                raise ValueError('mask of the first timestep must all be on')\n\n    def _compute_score(\n            self, emissions: torch.Tensor, tags: torch.LongTensor,\n            mask: torch.ByteTensor) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # tags: (seq_length, batch_size)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and tags.dim() == 2\n        assert emissions.shape[:2] == tags.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask.shape == tags.shape\n        assert mask[0].all()\n\n        seq_length, batch_size = tags.shape\n        mask = mask.type_as(emissions)\n\n        # Start transition score and first emission\n        # shape: (batch_size,)\n        score = self.start_transitions[tags[0]]\n        score += emissions[0, torch.arange(batch_size), tags[0]]\n\n        for i in range(1, seq_length):\n            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n\n            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n\n        # End transition score\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n        # shape: (batch_size,)\n        last_tags = tags[seq_ends, torch.arange(batch_size)]\n        # shape: (batch_size,)\n        score += self.end_transitions[last_tags]\n\n        return score\n\n    def _compute_normalizer(\n            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and mask.dim() == 2\n        assert emissions.shape[:2] == mask.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask[0].all()\n\n        seq_length = emissions.size(0)\n\n        # Start transition score and first emission; score has size of\n        # (batch_size, num_tags) where for each batch, the j-th column stores\n        # the score that the first timestep has tag j\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n\n        for i in range(1, seq_length):\n            # Broadcast score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emissions = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the sum of scores of all\n            # possible tag sequences so far that end with transitioning from tag i to tag j\n            # and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emissions\n\n            # Sum over all possible current tags, but we're in score space, so a sum\n            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n            # all possible tag sequences so far, that end in tag i\n            # shape: (batch_size, num_tags)\n            next_score = torch.logsumexp(next_score, dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        score += self.end_transitions\n\n        # Sum (log-sum-exp) over all possible tags\n        # shape: (batch_size,)\n        return torch.logsumexp(score, dim=1)\n\n    def _viterbi_decode(self, emissions: torch.FloatTensor,\n                        mask: torch.ByteTensor) -> List[List[int]]:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and mask.dim() == 2\n        assert emissions.shape[:2] == mask.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask[0].all()\n\n        seq_length, batch_size = mask.shape\n\n        # Start transition and first emission\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n        history = []\n\n        # score is a tensor of size (batch_size, num_tags) where for every batch,\n        # value at column j stores the score of the best tag sequence so far that ends\n        # with tag j\n        # history saves where the best tags candidate transitioned from; this is used\n        # when we trace back the best tag sequence\n\n        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n        # for every possible next tag\n        for i in range(1, seq_length):\n            # Broadcast viterbi score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emission = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the score of the best\n            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emission\n\n            # Find the maximum score over all possible current tag\n            # shape: (batch_size, num_tags)\n            next_score, indices = next_score.max(dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # and save the index that produces the next score\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n            history.append(indices)\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        score += self.end_transitions\n\n        # Now, compute the best path for each sample\n\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n        best_tags_list = []\n\n        for idx in range(batch_size):\n            # Find the tag which maximizes the score at the last timestep; this is our best tag\n            # for the last timestep\n            _, best_last_tag = score[idx].max(dim=0)\n            best_tags = [best_last_tag.item()]\n\n            # We trace back where the best last tag comes from, append that to our best tag\n            # sequence, and trace it back again, and so on\n            for hist in reversed(history[:seq_ends[idx]]):\n                best_last_tag = hist[idx][best_tags[-1]]\n                best_tags.append(best_last_tag.item())\n\n            # Reverse the order because we start from the last timestep\n            best_tags.reverse()\n            best_tags_list.append(best_tags)\n\n        return best_tags_list\n", "hanlp/layers/crf/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-18 22:55", "hanlp/layers/embeddings/contextual_word_embedding.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-05 13:50\nfrom typing import Optional, Union, List, Any, Dict, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom hanlp_common.configurable import AutoConfigurable\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropoutBuilder\nfrom hanlp.layers.transformers.encoder import TransformerEncoder\nfrom hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, AutoConfig_, AutoTokenizer_\nfrom hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer\n\n\nclass ContextualWordEmbeddingModule(TransformerEncoder):\n    def __init__(self,\n                 field: str,\n                 transformer: str,\n                 transformer_tokenizer: PreTrainedTokenizer,\n                 average_subwords=False,\n                 scalar_mix: Union[ScalarMixWithDropoutBuilder, int] = None,\n                 word_dropout=None,\n                 max_sequence_length=None,\n                 ret_raw_hidden_states=False,\n                 transformer_args: Dict[str, Any] = None,\n                 trainable=True,\n                 training=True) -> None:\n        \"\"\"A contextualized word embedding module.\n\n        Args:\n            field: The field to work on. Usually some token fields.\n            transformer:  An identifier of a ``PreTrainedModel``.\n            transformer_tokenizer:\n            average_subwords: ``True`` to average subword representations.\n            scalar_mix: Layer attention.\n            word_dropout: Dropout rate of randomly replacing a subword with MASK.\n            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding\n                window.\n            ret_raw_hidden_states: ``True`` to return hidden states of each layer.\n            transformer_args: Extra arguments passed to the transformer.\n            trainable: ``False`` to use static embeddings.\n            training: ``False`` to skip loading weights from pre-trained transformers.\n        \"\"\"\n        super().__init__(transformer, transformer_tokenizer, average_subwords, scalar_mix, word_dropout,\n                         max_sequence_length, ret_raw_hidden_states, transformer_args, trainable,\n                         training)\n        self.field = field\n\n    # noinspection PyMethodOverriding\n    # noinspection PyTypeChecker\n    def forward(self, batch: dict, mask=None, **kwargs):\n        input_ids: torch.LongTensor = batch[f'{self.field}_input_ids']\n        token_span: torch.LongTensor = batch.get(f'{self.field}_token_span', None)\n        # input_device = input_ids.device\n        # this_device = self.get_device()\n        # if input_device != this_device:\n        #     input_ids = input_ids.to(this_device)\n        #     token_span = token_span.to(this_device)\n        # We might want to apply mask here\n        output: Union[torch.Tensor, List[torch.Tensor]] = super().forward(input_ids, token_span=token_span, **kwargs)\n        # if input_device != this_device:\n        #     if isinstance(output, torch.Tensor):\n        #         output = output.to(input_device)\n        #     else:\n        #         output = [x.to(input_device) for x in output]\n        return output\n\n    def get_output_dim(self):\n        return self.transformer.config.hidden_size\n\n    def get_device(self):\n        device: torch.device = next(self.parameters()).device\n        return device\n\n\nclass ContextualWordEmbedding(Embedding, AutoConfigurable):\n    def __init__(self, field: str,\n                 transformer: str,\n                 average_subwords=False,\n                 scalar_mix: Union[ScalarMixWithDropoutBuilder, int] = None,\n                 word_dropout: Optional[Union[float, Tuple[float, str]]] = None,\n                 max_sequence_length=None,\n                 truncate_long_sequences=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 ret_token_span=True,\n                 ret_subtokens=False,\n                 ret_subtokens_group=False,\n                 ret_prefix_mask=False,\n                 ret_raw_hidden_states=False,\n                 transformer_args: Dict[str, Any] = None,\n                 use_fast=True,\n                 do_basic_tokenize=True,\n                 trainable=True) -> None:\n        \"\"\"A contextual word embedding builder which builds a\n        :class:`~hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule` and a\n        :class:`~hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer`.\n\n        Args:\n            field: The field to work on. Usually some token fields.\n            transformer:  An identifier of a ``PreTrainedModel``.\n            average_subwords: ``True`` to average subword representations.\n            scalar_mix: Layer attention.\n            word_dropout: Dropout rate of randomly replacing a subword with MASK.\n            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding\n                window.\n            truncate_long_sequences: ``True`` to return hidden states of each layer.\n            cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.\n                        ``False`` (default) means the first token is not [CLS], it will have its own embedding other than\n                        the embedding of [CLS].\n            sep_is_eos: ``True`` means the last token of input is [SEP].\n                        ``False`` means it's not but [SEP] will be appended,\n                        ``None`` means it dependents on `input[-1] == [EOS]`.\n            ret_token_span: ``True`` to return span of each token measured by subtoken offsets.\n            ret_subtokens: ``True`` to return list of subtokens belonging to each token.\n            ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.\n            ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.\n            ret_raw_hidden_states: ``True`` to return hidden states of each layer.\n            transformer_args: Extra arguments passed to the transformer.\n            use_fast: Whether or not to try to load the fast version of the tokenizer.\n            do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n            trainable: ``False`` to use static embeddings.\n        \"\"\"\n        super().__init__()\n        self.truncate_long_sequences = truncate_long_sequences\n        self.transformer_args = transformer_args\n        self.trainable = trainable\n        self.ret_subtokens_group = ret_subtokens_group\n        self.ret_subtokens = ret_subtokens\n        self.ret_raw_hidden_states = ret_raw_hidden_states\n        self.sep_is_eos = sep_is_eos\n        self.cls_is_bos = cls_is_bos\n        self.max_sequence_length = max_sequence_length\n        self.word_dropout = word_dropout\n        self.scalar_mix = scalar_mix\n        self.average_subwords = average_subwords\n        self.transformer = transformer\n        self.field = field\n        self._transformer_tokenizer = AutoTokenizer_.from_pretrained(self.transformer,\n                                                                     use_fast=use_fast,\n                                                                     do_basic_tokenize=do_basic_tokenize)\n        self._tokenizer_transform = TransformerSequenceTokenizer(self._transformer_tokenizer,\n                                                                 field,\n                                                                 truncate_long_sequences=truncate_long_sequences,\n                                                                 ret_prefix_mask=ret_prefix_mask,\n                                                                 ret_token_span=ret_token_span,\n                                                                 cls_is_bos=cls_is_bos,\n                                                                 sep_is_eos=sep_is_eos,\n                                                                 ret_subtokens=ret_subtokens,\n                                                                 ret_subtokens_group=ret_subtokens_group,\n                                                                 max_seq_length=self.max_sequence_length\n                                                                 )\n\n    def transform(self, **kwargs) -> TransformerSequenceTokenizer:\n        return self._tokenizer_transform\n\n    def module(self, training=True, **kwargs) -> Optional[nn.Module]:\n        return ContextualWordEmbeddingModule(self.field,\n                                             self.transformer,\n                                             self._transformer_tokenizer,\n                                             self.average_subwords,\n                                             self.scalar_mix,\n                                             self.word_dropout,\n                                             self.max_sequence_length,\n                                             self.ret_raw_hidden_states,\n                                             self.transformer_args,\n                                             self.trainable,\n                                             training=training)\n\n    def get_output_dim(self):\n        config = AutoConfig_.from_pretrained(self.transformer)\n        return config.hidden_size\n\n    def get_tokenizer(self):\n        return self._transformer_tokenizer\n\n\ndef find_transformer(embed: nn.Module):\n    if isinstance(embed, ContextualWordEmbeddingModule):\n        return embed\n    if isinstance(embed, nn.ModuleList):\n        for child in embed:\n            found = find_transformer(child)\n            if found:\n                return found\n", "hanlp/layers/embeddings/fast_text_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-29 13:14\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.utils import tf_utils\n\nfrom hanlp_common.constant import PAD\nfrom hanlp.utils.io_util import get_resource, stdout_redirected\nfrom hanlp.utils.log_util import logger\nfrom hanlp.utils.tf_util import hanlp_register\n\n\n@hanlp_register\nclass FastTextEmbeddingTF(tf.keras.layers.Embedding):\n\n    def __init__(self, filepath: str, padding=PAD, name=None, **kwargs):\n        import fasttext\n        self.padding = padding.encode('utf-8')\n        self.filepath = filepath\n        filepath = get_resource(filepath)\n        assert os.path.isfile(filepath), f'Resolved path {filepath} is not a file'\n        logger.debug('Loading fasttext model from [{}].'.format(filepath))\n        # fasttext print a blank line here\n        with stdout_redirected(to=os.devnull, stdout=sys.stderr):\n            self.model = fasttext.load_model(filepath)\n        kwargs.pop('input_dim', None)\n        kwargs.pop('output_dim', None)\n        kwargs.pop('mask_zero', None)\n        if not name:\n            name = os.path.splitext(os.path.basename(filepath))[0]\n        super().__init__(input_dim=len(self.model.words), output_dim=self.model['king'].size,\n                         mask_zero=padding is not None, trainable=False, dtype=tf.string, name=name, **kwargs)\n        embed_fn = np.frompyfunc(self.embed, 1, 1)\n        # vf = np.vectorize(self.embed, otypes=[np.ndarray])\n        self._embed_np = embed_fn\n\n    def embed(self, word):\n        return self.model[word]\n\n    def embed_np(self, words: np.ndarray):\n        output = self._embed_np(words)\n        if self.mask_zero:\n            mask = words != self.padding\n            output *= mask\n            output = np.stack(output.reshape(-1)).reshape(list(words.shape) + [self.output_dim])\n            return output, tf.constant(mask)\n        else:\n            output = np.stack(output.reshape(-1)).reshape(list(words.shape) + [self.output_dim])\n            return output\n\n    @tf_utils.shape_type_conversion\n    def build(self, input_shape):\n        self.built = True\n\n    @tf_utils.shape_type_conversion\n    def compute_output_shape(self, input_shape):\n        return input_shape + (self.output_dim,)\n\n    def call(self, inputs: tf.Tensor):\n        if isinstance(inputs, list):\n            inputs = inputs[0]\n        if not hasattr(inputs, 'numpy'):  # placeholder tensor\n            inputs = tf.expand_dims(inputs, axis=-1)\n            inputs = tf.tile(inputs, [1] * (len(inputs.shape) - 1) + [self.output_dim])\n            inputs = tf.zeros_like(inputs, dtype=tf.float32)\n            return inputs\n            # seq_len = inputs.shape[-1]\n            # if not seq_len:\n            #     seq_len = 1\n            # return tf.zeros([1, seq_len, self.output_dim])\n        if self.mask_zero:\n            outputs, masks = self.embed_np(inputs.numpy())\n            outputs = tf.constant(outputs)\n            outputs._keras_mask = masks\n        else:\n            outputs = self.embed_np(inputs.numpy())\n            outputs = tf.constant(outputs)\n        return outputs\n\n    def compute_mask(self, inputs, mask=None):\n        if not self.mask_zero:\n            return None\n        return tf.not_equal(inputs, self.padding)\n\n    def get_config(self):\n        config = {\n            'filepath': self.filepath,\n            'padding': self.padding.decode('utf-8')\n        }\n        base_config = super(FastTextEmbeddingTF, self).get_config()\n        for junk in 'embeddings_initializer' \\\n                , 'batch_input_shape' \\\n                , 'embeddings_regularizer' \\\n                , 'embeddings_constraint' \\\n                , 'activity_regularizer' \\\n                , 'trainable' \\\n                , 'input_length' \\\n                :\n            base_config.pop(junk)\n        return dict(list(base_config.items()) + list(config.items()))\n", "hanlp/layers/embeddings/word2vec.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-09 13:38\nimport logging\nimport math\nimport os.path\nfrom typing import Optional, Callable, Union, List, Dict\n\nimport torch\nfrom hanlp_common.configurable import AutoConfigurable\nfrom hanlp_common.constant import HANLP_VERBOSE\nfrom hanlp_trie.trie import Trie\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import TransformableDataset, PadSequenceDataLoader\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.dropout import WordDropout\nfrom hanlp.layers.embeddings.embedding import Embedding, EmbeddingDim\nfrom hanlp.layers.embeddings.util import build_word2vec_with_vocab\nfrom hanlp.utils.log_util import flash\nfrom hanlp.utils.torch_util import load_word2vec_as_vocab_tensor\n\n\nclass Word2VecEmbeddingModule(nn.Module, EmbeddingDim):\n    def __init__(self, field: str, embed: nn.Embedding, word_dropout: WordDropout = None, cpu=False,\n                 second_channel=False, num_tokens_in_trn=None, unk_idx=1) -> None:\n        \"\"\"A word2vec style embedding module which maps a token to its embedding through looking up a pre-defined table.\n\n        Args:\n            field: The field to work on. Usually some token fields.\n            embed: An ``Embedding`` layer.\n            word_dropout: The probability of randomly replacing a token with ``UNK``.\n            cpu: Reside on CPU instead of GPU.\n            second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.\n            num_tokens_in_trn: The number of tokens in training set.\n            unk_idx: The index of ``UNK``.\n        \"\"\"\n        super().__init__()\n        self.cpu = cpu\n        self.field = field\n        self.embed = embed\n        self.word_dropout = word_dropout\n        self.num_tokens_in_trn = num_tokens_in_trn\n        self.unk_idx = unk_idx\n        if second_channel:\n            n_words, n_embed = embed.weight.size()\n            if num_tokens_in_trn:\n                n_words = num_tokens_in_trn\n            second_channel = nn.Embedding(num_embeddings=n_words,\n                                          embedding_dim=n_embed)\n            nn.init.zeros_(second_channel.weight)\n        self.second_channel = second_channel\n\n    def forward(self, batch: dict, **kwargs):\n        x: torch.Tensor = batch[f'{self.field}_id']\n        if self.cpu:\n            device = x.device\n            x = x.cpu()\n        if self.word_dropout:\n            x = self.word_dropout(x)\n        if self.second_channel:\n            ext_mask = x.ge(self.second_channel.num_embeddings)\n            ext_words = x.masked_fill(ext_mask, self.unk_idx)\n            x = self.embed(x) + self.second_channel(ext_words)\n        else:\n            x = self.embed(x)\n        if self.cpu:\n            # noinspection PyUnboundLocalVariable\n            x = x.to(device)\n        return x\n\n    @property\n    def embedding_dim(self) -> int:\n        return self.embed.embedding_dim\n\n    # noinspection PyMethodOverriding\n    # def to(self, device, **kwargs):\n    #     print(self.cpu)\n    #     exit(1)\n    #     if self.cpu:\n    #         return super(Word2VecEmbeddingModule, self).to(-1, **kwargs)\n    #     return super(Word2VecEmbeddingModule, self).to(device, **kwargs)\n\n    def _apply(self, fn):\n\n        if not self.cpu:  # This might block all fn not limiting to moving between devices.\n            return super(Word2VecEmbeddingModule, self)._apply(fn)\n\n\nclass Word2VecEmbedding(Embedding, AutoConfigurable):\n    def __init__(self,\n                 field,\n                 embed: Union[int, str],\n                 extend_vocab=True,\n                 pad=None,\n                 unk=None,\n                 lowercase=False,\n                 trainable=False,\n                 second_channel=False,\n                 word_dropout: float = 0,\n                 normalize=False,\n                 cpu=False,\n                 init='zeros') -> None:\n        \"\"\"A word2vec style embedding builder which maps a token to its embedding through looking up a pre-defined\n        table.\n\n        Args:\n            field: The field to work on. Usually some token fields.\n            embed: A path to pre-trained embedding file or an integer defining the size of randomly initialized\n                embedding.\n            extend_vocab: Unlock vocabulary of training set to add those tokens in pre-trained embedding file.\n            pad: The padding token.\n            unk: The unknown token.\n            lowercase: Convert words in pretrained embeddings into lowercase.\n            trainable: ``False`` to use static embeddings.\n            second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.\n            word_dropout: The probability of randomly replacing a token with ``UNK``.\n            normalize: ``l2`` or ``std`` to normalize the embedding matrix.\n            cpu: Reside on CPU instead of GPU.\n            init: Indicate which initialization to use for oov tokens.\n        \"\"\"\n        super().__init__()\n        self.pad = pad\n        self.second_channel = second_channel\n        self.cpu = cpu\n        self.normalize = normalize\n        self.word_dropout = word_dropout\n        self.init = init\n        self.lowercase = lowercase\n        self.unk = unk\n        self.extend_vocab = extend_vocab\n        self.trainable = trainable\n        self.embed = embed\n        self.field = field\n\n    def module(self, vocabs: VocabDict, **kwargs) -> Optional[nn.Module]:\n        vocab = vocabs[self.field]\n        num_tokens_in_trn = len(vocab)\n        embed = build_word2vec_with_vocab(self.embed,\n                                          vocab,\n                                          self.extend_vocab,\n                                          self.unk,\n                                          self.lowercase,\n                                          self.trainable,\n                                          normalize=self.normalize)\n        if self.word_dropout:\n            assert vocab.unk_token, f'unk_token of vocab {self.field} has to be set in order to ' \\\n                                    f'make use of word_dropout'\n            padding = []\n            if vocab.pad_token:\n                padding.append(vocab.pad_idx)\n            word_dropout = WordDropout(self.word_dropout, vocab.unk_idx, exclude_tokens=padding)\n        else:\n            word_dropout = None\n        return Word2VecEmbeddingModule(self.field, embed, word_dropout=word_dropout, cpu=self.cpu,\n                                       second_channel=self.second_channel, num_tokens_in_trn=num_tokens_in_trn,\n                                       unk_idx=vocab.unk_idx)\n\n    def transform(self, vocabs: VocabDict = None, **kwargs) -> Optional[Callable]:\n        assert vocabs is not None\n        if self.field not in vocabs:\n            vocabs[self.field] = Vocab(pad_token=self.pad, unk_token=self.unk)\n        return super().transform(**kwargs)\n\n\nclass Word2VecDataset(TransformableDataset):\n\n    def load_file(self, filepath: str):\n        raise NotImplementedError('Not supported.')\n\n\nclass Word2VecEmbeddingComponent(TorchComponent):\n\n    def __init__(self, **kwargs) -> None:\n        \"\"\" Toy example of Word2VecEmbedding. It simply returns the embedding of a given word\n\n        Args:\n            **kwargs:\n        \"\"\"\n        super().__init__(**kwargs)\n        self._tokenizer: Trie = None\n\n    def build_dataloader(self, data: List[str], shuffle=False, device=None, logger: logging.Logger = None,\n                         doc2vec=False, batch_size=32, **kwargs) -> DataLoader:\n        dataset = Word2VecDataset([{'token': x} for x in data], transform=self._tokenize if doc2vec else self.vocabs)\n        return PadSequenceDataLoader(dataset, device=device, batch_size=batch_size)\n\n    def build_optimizer(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def build_criterion(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def build_metric(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        self.vocabs['token'] = Vocab()\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        self._tokenizer = None\n        embed: Word2VecEmbedding = self.config.embed\n        model = embed.module(self.vocabs)\n        return model\n\n    def predict(self, word: str, doc2vec=False, **kwargs):\n        dataloader = self.build_dataloader([word], device=self.device, doc2vec=doc2vec)\n        for batch in dataloader:  # It's a toy so doesn't really do batching\n            embeddings = self.model(batch)[0]\n            if doc2vec:\n                embeddings = embeddings[0].mean(dim=0)\n            return embeddings\n\n    @torch.no_grad()\n    def most_similar(self, words: Union[str, List[str]], topk=10, doc2vec=False, similarity_less_than=None,\n                     batch_size=32) -> Union[Dict[str, float], List[Dict[str, float]]]:\n        \"\"\"Find the `topk` most similar words of a given word or phrase.\n\n        Args:\n            words: A word or phrase or multiple words/phrases.\n            topk: Number of top similar words.\n            doc2vec: Enable doc2vec model for processing OOV and phrases.\n            similarity_less_than: Only return words with a similarity less than this value.\n            batch_size: Number of words or phrases per batch.\n\n        Returns:\n            Similar words and similarities stored in a dict.\n        \"\"\"\n        flat = isinstance(words, str)\n        if flat:\n            words = [words]\n        dataloader = self.build_dataloader(words, device=self.device, doc2vec=doc2vec, batch_size=batch_size)\n        results = []\n        vocab = self.vocabs['token']\n        for batch in dataloader:\n            embeddings = self.model(batch)\n            token_id = batch['token_id']\n            if doc2vec:\n                lens = token_id.count_nonzero(dim=1)\n                embeddings = embeddings.sum(1)\n                embeddings = embeddings / lens.unsqueeze(1)\n                block_word_id = batch['block_word_id']\n                token_is_unk = (lens == 1) & (token_id[:, 0] == vocab.unk_idx)\n            else:\n                block_word_id = token_id\n                token_is_unk = token_id == vocab.unk_idx\n            similarities = torch.nn.functional.cosine_similarity(embeddings.unsqueeze(1), self.model.embed.weight,\n                                                                 dim=-1)\n            if similarity_less_than is not None:\n                similarities[similarities > similarity_less_than] = -math.inf\n            similarities[torch.arange(similarities.size(0), device=self.device), block_word_id] = -math.inf\n            scores, indices = similarities.topk(topk)\n\n            for sc, idx, unk in zip(scores.tolist(), indices.tolist(), token_is_unk.tolist()):\n                results.append(dict() if unk else dict(zip([vocab.idx_to_token[i] for i in idx], sc)))\n        if flat:\n            results = results[0]\n        return results\n\n    def _tokenize(self, sample: dict) -> dict:\n        tokens = sample['token']\n        ids = [idx for b, e, idx in self.tokenizer.parse_longest(tokens)]\n        vocab = self.vocabs['token']\n        if not ids:\n            ids = [vocab.unk_idx]\n        sample['token_id'] = ids\n        sample['block_word_id'] = ids[0] if len(ids) == 1 else vocab.pad_idx\n        return sample\n\n    @property\n    def tokenizer(self):\n        if not self._tokenizer:\n            if HANLP_VERBOSE:\n                flash('Building Trie-based tokenizer for Doc2Vec [blink][yellow]...[/yellow][/blink]')\n            self._tokenizer = Trie(self.vocabs['token'].token_to_idx)\n            if HANLP_VERBOSE:\n                flash('')\n        return self._tokenizer\n\n    def load_config(self, save_dir, filename='config.json', **kwargs):\n        if os.path.isfile(save_dir):\n            self.config.update({'classpath': 'hanlp.layers.embeddings.word2vec.Word2VecEmbeddingComponent',\n                                'embed': Word2VecEmbedding(field='token', embed=save_dir, normalize='l2')})\n            return\n        super().load_config(save_dir, filename, **kwargs)\n\n\nclass GazetterTransform(object):\n    def __init__(self, field, words: dict) -> None:\n        super().__init__()\n        self.field = field\n        self.trie = Trie()\n        for word, idx in words.items():\n            self.trie[word] = idx\n\n    def __call__(self, sample: dict) -> dict:\n        tokens = sample[self.field]\n        lexicons = self.trie.parse(tokens)\n        skips_l2r = [[] for _ in range(len(tokens))]\n        skips_r2l = [[] for _ in range(len(tokens))]\n        for w, i, s, e in lexicons:\n            e = e - 1\n            skips_l2r[e].append((s, w, i))\n            skips_r2l[s].append((e, w, i))\n        for direction, value in zip(['skips_l2r', 'skips_r2l'], [skips_l2r, skips_r2l]):\n            sample[f'{self.field}_{direction}_offset'] = [list(map(lambda x: x[0], p)) for p in value]\n            sample[f'{self.field}_{direction}_id'] = [list(map(lambda x: x[-1], p)) for p in value]\n            sample[f'{self.field}_{direction}_count'] = list(map(len, value))\n        return sample\n\n\nclass GazetteerEmbedding(Embedding, AutoConfigurable):\n    def __init__(self, embed: str, field='char', trainable=False) -> None:\n        self.trainable = trainable\n        self.embed = embed\n        self.field = field\n        vocab, matrix = load_word2vec_as_vocab_tensor(self.embed)\n        ids = []\n        _vocab = {}\n        for word, idx in vocab.items():\n            if len(word) > 1:\n                ids.append(idx)\n                _vocab[word] = len(_vocab)\n        ids = torch.tensor(ids)\n        _matrix = matrix.index_select(0, ids)\n        self._vocab = _vocab\n        self._matrix = _matrix\n\n    def transform(self, **kwargs) -> Optional[Callable]:\n        return GazetterTransform(self.field, self._vocab)\n\n    def module(self, **kwargs) -> Optional[nn.Module]:\n        embed = nn.Embedding.from_pretrained(self._matrix, freeze=not self.trainable)\n        return embed\n\n    @staticmethod\n    def _remove_short_tokens(word2vec):\n        word2vec = dict((w, v) for w, v in word2vec.items() if len(w) > 1)\n        return word2vec\n", "hanlp/layers/embeddings/char_cnn_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-20 21:15\nfrom functools import reduce\n\nimport tensorflow as tf\n\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.tf_util import hanlp_register\n\n\n@hanlp_register\nclass CharCNNEmbeddingTF(tf.keras.layers.Layer):\n    def __init__(self, word_vocab: VocabTF, char_vocab: VocabTF,\n                 char_embedding=100,\n                 kernel_size=3,\n                 filters=50,\n                 dropout=0.5,\n                 trainable=True, name=None, dtype=None, dynamic=False,\n                 **kwargs):\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.char_embedding = char_embedding\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.char_vocab = char_vocab\n        self.word_vocab = word_vocab\n        self.embedding = tf.keras.layers.Embedding(input_dim=len(self.char_vocab), output_dim=char_embedding,\n                                                   trainable=True, mask_zero=True)\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.cnn = tf.keras.layers.Conv1D(filters, kernel_size, padding='same')\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        mask = tf.not_equal(inputs, self.word_vocab.pad_token)\n        inputs = tf.ragged.boolean_mask(inputs, mask)\n        chars = tf.strings.unicode_split(inputs, input_encoding='UTF-8')\n        chars = chars.to_tensor(default_value=self.char_vocab.pad_token)\n        chars = self.char_vocab.lookup(chars)\n        embed = self.embedding(chars)\n        weights = embed._keras_mask\n        embed = self.dropout(embed)\n        features = masked_conv1d_and_max(embed, weights, self.cnn)\n        features._keras_mask = mask\n        return features\n\n    def compute_output_shape(self, input_shape):\n        return super().compute_output_shape(input_shape)\n\n    def get_config(self):\n        config = {\n            'char_embedding': self.char_embedding,\n            'kernel_size': self.kernel_size,\n            'filters': self.filters,\n            'dropout': self.dropout.rate,\n        }\n        base_config = super(CharCNNEmbeddingTF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ndef masked_conv1d_and_max(t, weights, conv1d):\n    \"\"\"Applies 1d convolution and a masked max-pooling\n    \n    https://github.com/guillaumegenthial/tf_ner/blob/master/models/chars_conv_lstm_crf/masked_conv.py\n\n    Args:\n      t(tf.Tensor): A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]\n      weights(tf.Tensor of tf.bool): A Tensor of shape [d1, d2, dn-1]\n      filters(int): number of filters\n      kernel_size(int): kernel size for the temporal convolution\n      conv1d: \n\n    Returns:\n\n    \n    \"\"\"\n    # Get shape and parameters\n    shape = tf.shape(t)\n    ndims = t.shape.ndims\n    dim1 = reduce(lambda x, y: x * y, [shape[i] for i in range(ndims - 2)])\n    dim2 = shape[-2]\n    dim3 = t.shape[-1]\n\n    # Reshape weights\n    weights = tf.reshape(weights, shape=[dim1, dim2, 1])\n    weights = tf.cast(weights, tf.float32)\n\n    # Reshape input and apply weights\n    flat_shape = [dim1, dim2, dim3]\n    t = tf.reshape(t, shape=flat_shape)\n    t *= weights\n\n    # Apply convolution\n    t_conv = conv1d(t)\n    t_conv *= weights\n\n    # Reduce max -- set to zero if all padded\n    t_conv += (1. - weights) * tf.reduce_min(t_conv, axis=-2, keepdims=True)\n    t_max = tf.reduce_max(t_conv, axis=-2)\n\n    # Reshape the output\n    final_shape = [shape[i] for i in range(ndims - 2)] + [conv1d.filters]\n    t_max = tf.reshape(t_max, shape=final_shape)\n\n    return t_max\n", "hanlp/layers/embeddings/contextual_string_embedding.py": "# Most codes of this file is adopted from flair, which is licenced under:\n#\n# The MIT License (MIT)\n#\n# Flair is licensed under the following MIT License (MIT) Copyright \u00a9 2018 Zalando SE, https://tech.zalando.com\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nfrom typing import List, Dict, Callable\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom hanlp_common.configurable import Configurable\nfrom hanlp.common.transform import TransformList, FieldToIndex\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.embeddings.embedding import Embedding, EmbeddingDim\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp.utils.torch_util import pad_lists, batched_index_select\nfrom tests import cdroot\n\n\nclass RNNLanguageModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self,\n                 n_tokens,\n                 is_forward_lm: bool,\n                 hidden_size: int,\n                 embedding_size: int = 100):\n        super(RNNLanguageModel, self).__init__()\n\n        self.is_forward_lm: bool = is_forward_lm\n        self.n_tokens = n_tokens\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n\n        self.encoder = nn.Embedding(n_tokens, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n\n    def forward(self, ids: torch.LongTensor, lens: torch.LongTensor):\n        emb = self.encoder(ids)\n        x = pack_padded_sequence(emb, lens, True, False)\n        x, _ = self.rnn(x)\n        x, _ = pad_packed_sequence(x, True)\n        return x\n\n    @classmethod\n    def load_language_model(cls, model_file):\n        model_file = get_resource(model_file)\n        state = torch.load(model_file)\n        model = RNNLanguageModel(state['n_tokens'],\n                                 state['is_forward_lm'],\n                                 state['hidden_size'],\n                                 state['embedding_size'])\n        model.load_state_dict(state['state_dict'], strict=False)\n        return model\n\n    def save(self, file):\n        model_state = {\n            'state_dict': self.state_dict(),\n            'n_tokens': self.n_tokens,\n            'is_forward_lm': self.is_forward_lm,\n            'hidden_size': self.hidden_size,\n            'embedding_size': self.embedding_size,\n        }\n        torch.save(model_state, file, pickle_protocol=4)\n\n\nclass ContextualStringEmbeddingModule(nn.Module, EmbeddingDim):\n\n    def __init__(self, field: str, path: str, trainable=False) -> None:\n        super().__init__()\n        self.field = field\n        path = get_resource(path)\n        f = os.path.join(path, 'forward.pt')\n        b = os.path.join(path, 'backward.pt')\n        self.f: RNNLanguageModel = RNNLanguageModel.load_language_model(f)\n        self.b: RNNLanguageModel = RNNLanguageModel.load_language_model(b)\n        if not trainable:\n            for p in self.parameters():\n                p.requires_grad_(False)\n\n    def __call__(self, batch: dict, **kwargs):\n        args = ['f_char_id', 'f_offset', 'b_char_id', 'b_offset']\n        keys = [f'{self.field}_{key}' for key in args]\n        args = [batch[key] for key in keys]\n        return super().__call__(*args, **kwargs)\n\n    @property\n    def embedding_dim(self):\n        return self.f.rnn.hidden_size + self.b.rnn.hidden_size\n\n    def run_lm(self, lm, ids: torch.Tensor, offsets: torch.LongTensor):\n        lens = offsets.max(-1)[0] + 1\n        rnn_output = lm(ids, lens)\n        return batched_index_select(rnn_output, offsets)\n\n    def forward(self,\n                f_chars_id: torch.Tensor,\n                f_offset: torch.LongTensor,\n                b_chars_id: torch.Tensor,\n                b_offset: torch.LongTensor, **kwargs):\n        f = self.run_lm(self.f, f_chars_id, f_offset)\n        b = self.run_lm(self.b, b_chars_id, b_offset)\n        return torch.cat([f, b], dim=-1)\n\n    def embed(self, sents: List[List[str]], vocab: Dict[str, int]):\n        f_chars, f_offsets = [], []\n        b_chars, b_offsets = [], []\n\n        transform = ContextualStringEmbeddingTransform('token')\n        for tokens in sents:\n            sample = transform({'token': tokens})\n            for each, name in zip([f_chars, b_chars, f_offsets, b_offsets],\n                                  'f_chars, b_chars, f_offsets, b_offsets'.split(', ')):\n                each.append(sample[f'token_{name}'])\n        f_ids = []\n        for cs in f_chars:\n            f_ids.append([vocab[c] for c in cs])\n        f_ids = pad_lists(f_ids)\n        f_offsets = pad_lists(f_offsets)\n\n        b_ids = []\n        for cs in b_chars:\n            b_ids.append([vocab[c] for c in cs])\n        b_ids = pad_lists(b_ids)\n        b_offsets = pad_lists(b_offsets)\n        return self.forward(f_ids, f_offsets, b_ids, b_offsets)\n\n\nclass ContextualStringEmbeddingTransform(Configurable):\n\n    def __init__(self, src: str) -> None:\n        self.src = src\n\n    def __call__(self, sample: dict):\n        tokens = sample[self.src]\n        f_o = []\n        b_o = []\n        sentence_text = ' '.join(tokens)\n        end_marker = ' '\n        extra_offset = 1\n        # f\n        input_text = '\\n' + sentence_text + end_marker\n        f_chars = list(input_text)\n        # b\n        sentence_text = sentence_text[::-1]\n        input_text = '\\n' + sentence_text + end_marker\n        b_chars = list(input_text)\n        offset_forward: int = extra_offset\n        offset_backward: int = len(sentence_text) + extra_offset\n        for token in tokens:\n            offset_forward += len(token)\n\n            f_o.append(offset_forward)\n            b_o.append(offset_backward)\n\n            # This language model is tokenized\n            offset_forward += 1\n            offset_backward -= 1\n\n            offset_backward -= len(token)\n        sample[f'{self.src}_f_char'] = f_chars\n        sample[f'{self.src}_b_char'] = b_chars\n        sample[f'{self.src}_f_offset'] = f_o\n        sample[f'{self.src}_b_offset'] = b_o\n        return sample\n\n\nclass ContextualStringEmbedding(Embedding):\n    def __init__(self, field, path, trainable=False) -> None:\n        super().__init__()\n        self.trainable = trainable\n        self.path = path\n        self.field = field\n\n    def transform(self, **kwargs) -> Callable:\n        vocab = Vocab()\n        vocab.load(os.path.join(get_resource(self.path), 'vocab.json'))\n        return TransformList(ContextualStringEmbeddingTransform(self.field),\n                             FieldToIndex(f'{self.field}_f_char', vocab),\n                             FieldToIndex(f'{self.field}_b_char', vocab))\n\n    def module(self, **kwargs) -> nn.Module:\n        return ContextualStringEmbeddingModule(self.field, self.path, self.trainable)\n\n\ndef main():\n    # _validate()\n    flair = ContextualStringEmbedding('token', 'FASTTEXT_DEBUG_EMBEDDING_EN')\n    print(flair.config)\n\n\ndef _validate():\n    cdroot()\n    flair = ContextualStringEmbeddingModule('token', 'FLAIR_LM_WMT11_EN')\n    vocab = torch.load('/home/hhe43/flair/item2idx.pt')\n    vocab = dict((x.decode(), y) for x, y in vocab.items())\n    # vocab = Vocab(token_to_idx=vocab, pad_token='<unk>')\n    # vocab.lock()\n    # vocab.summary()\n    # vocab.save('vocab.json')\n    tokens = 'I love Berlin .'.split()\n    sent = ' '.join(tokens)\n    embed = flair.embed([tokens, tokens], vocab)\n    gold = torch.load('/home/hhe43/flair/gold.pt')\n    print(torch.allclose(embed[1, :, :2048], gold, atol=1e-6))\n    # print(torch.all(torch.eq(embed[1, :, :], gold)))\n\n\nif __name__ == '__main__':\n    main()\n", "hanlp/layers/embeddings/concat_embedding.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-20 17:08\nimport tensorflow as tf\n\nfrom hanlp.utils.tf_util import hanlp_register, copy_mask\n\n\n@hanlp_register\nclass ConcatEmbedding(tf.keras.layers.Layer):\n    def __init__(self, *embeddings, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n        self.embeddings = []\n        for embed in embeddings:\n            embed: tf.keras.layers.Layer = tf.keras.utils.deserialize_keras_object(embed) if isinstance(embed,\n                                                                                                        dict) else embed\n            self.embeddings.append(embed)\n            if embed.trainable:\n                trainable = True\n            if embed.dynamic:\n                dynamic = True\n            if embed.supports_masking:\n                self.supports_masking = True\n\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n\n    def build(self, input_shape):\n        for embed in self.embeddings:\n            embed.build(input_shape)\n        super().build(input_shape)\n\n    def compute_mask(self, inputs, mask=None):\n        for embed in self.embeddings:\n            mask = embed.compute_mask(inputs, mask)\n            if mask is not None:\n                return mask\n        return mask\n\n    def call(self, inputs, **kwargs):\n        embeds = [embed.call(inputs) for embed in self.embeddings]\n        feature = tf.concat(embeds, axis=-1)\n\n        for embed in embeds:\n            mask = copy_mask(embed, feature)\n            if mask is not None:\n                break\n        return feature\n\n    def get_config(self):\n        config = {\n            'embeddings': [embed.get_config() for embed in self.embeddings],\n        }\n        base_config = super(ConcatEmbedding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        dim = 0\n        for embed in self.embeddings:\n            dim += embed.compute_output_shape(input_shape)[-1]\n\n        return input_shape + dim\n", "hanlp/layers/embeddings/util.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-09 15:45\nfrom typing import Union\n\nimport torch\nfrom torch import nn\n\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.utils.init_util import embedding_uniform\nfrom hanlp.utils.torch_util import load_word2vec, load_word2vec_as_vocab_tensor\n\n\ndef index_word2vec_with_vocab(filepath: str,\n                              vocab: Vocab,\n                              extend_vocab=True,\n                              unk=None,\n                              lowercase=False,\n                              init='uniform',\n                              normalize=None) -> torch.Tensor:\n    \"\"\"\n\n    Args:\n        filepath: The path to pretrained embedding.\n        vocab: The vocabulary from training set.\n        extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.\n        unk: UNK token.\n        lowercase: Convert words in pretrained embeddings into lowercase.\n        init: Indicate which initialization to use for oov tokens.\n        normalize: ``True`` or a method to normalize the embedding matrix.\n\n    Returns:\n        An embedding matrix.\n\n    \"\"\"\n    pret_vocab, pret_matrix = load_word2vec_as_vocab_tensor(filepath)\n    if unk and unk in pret_vocab:\n        pret_vocab[vocab.safe_unk_token] = pret_vocab.pop(unk)\n    if extend_vocab:\n        vocab.unlock()\n        for word in pret_vocab:\n            vocab.get_idx(word.lower() if lowercase else word)\n    vocab.lock()\n    ids = []\n\n    unk_id_offset = 0\n    for word, idx in vocab.token_to_idx.items():\n        word_id = pret_vocab.get(word, None)\n        # Retry lower case\n        if word_id is None:\n            word_id = pret_vocab.get(word.lower(), None)\n        if word_id is None:\n            word_id = len(pret_vocab) + unk_id_offset\n            unk_id_offset += 1\n        ids.append(word_id)\n    if unk_id_offset:\n        unk_embeds = torch.zeros(unk_id_offset, pret_matrix.size(1))\n        if init and init != 'zeros':\n            if init == 'uniform':\n                init = embedding_uniform\n            else:\n                raise ValueError(f'Unsupported init {init}')\n            unk_embeds = init(unk_embeds)\n        pret_matrix = torch.cat([pret_matrix, unk_embeds])\n    ids = torch.LongTensor(ids)\n    embedding = pret_matrix.index_select(0, ids)\n    if normalize == 'norm':\n        embedding /= (torch.norm(embedding, dim=1, keepdim=True) + 1e-12)\n    elif normalize == 'l2':\n        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n    elif normalize == 'std':\n        embedding /= torch.std(embedding)\n    else:\n        raise ValueError(f'Unsupported normalization method {normalize}')\n    return embedding\n\n\ndef build_word2vec_with_vocab(embed: Union[str, int],\n                              vocab: Vocab,\n                              extend_vocab=True,\n                              unk=None,\n                              lowercase=False,\n                              trainable=False,\n                              init='zeros',\n                              normalize=None) -> nn.Embedding:\n    \"\"\"Build word2vec embedding and a vocab.\n\n    Args:\n        embed:\n        vocab: The vocabulary from training set.\n        extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.\n        unk: UNK token.\n        lowercase: Convert words in pretrained embeddings into lowercase.\n        trainable: ``False`` to use static embeddings.\n        init: Indicate which initialization to use for oov tokens.\n        normalize: ``True`` or a method to normalize the embedding matrix.\n\n    Returns:\n        An embedding matrix.\n\n    \"\"\"\n    if isinstance(embed, str):\n        embed = index_word2vec_with_vocab(embed, vocab, extend_vocab, unk, lowercase, init, normalize)\n        embed = nn.Embedding.from_pretrained(embed, freeze=not trainable, padding_idx=vocab.pad_idx)\n        return embed\n    elif isinstance(embed, int):\n        embed = nn.Embedding(len(vocab), embed, padding_idx=vocab.pad_idx)\n        return embed\n    else:\n        raise ValueError(f'Unsupported parameter type: {embed}')\n", "hanlp/layers/embeddings/char_cnn.py": "# Adopted from https://github.com/allenai/allennlp under Apache Licence 2.0.\n# Changed the packaging and created a subclass CharCNNEmbedding\n\nfrom typing import Union, Tuple, Optional, Callable\nimport torch\nfrom torch import nn\nfrom hanlp.layers.cnn_encoder import CnnEncoder\nfrom hanlp.layers.time_distributed import TimeDistributed\nfrom hanlp_common.configurable import AutoConfigurable\nfrom hanlp.common.transform import VocabDict, ToChar\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.embeddings.embedding import EmbeddingDim, Embedding\n\n\nclass CharCNN(nn.Module):\n    def __init__(self,\n                 field: str,\n                 embed: Union[int, Embedding], num_filters: int,\n                 ngram_filter_sizes: Tuple[int, ...] = (2, 3, 4, 5),\n                 conv_layer_activation: str = 'ReLU',\n                 output_dim: Optional[int] = None,\n                 vocab_size=None) -> None:\n        \"\"\"A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.\n        The input to this module is of shape `(batch_size, num_tokens,\n        input_dim)`, and the output is of shape `(batch_size, output_dim)`.\n\n        The CNN has one convolution layer for each ngram filter size. Each convolution operation gives\n        out a vector of size num_filters. The number of times a convolution layer will be used\n        is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these\n        outputs from the convolution layer and outputs the max.\n\n        This operation is repeated for every ngram size passed, and consequently the dimensionality of\n        the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets\n        (optionally) projected down to a lower dimensional output, specified by `output_dim`.\n\n        We then use a fully connected layer to project in back to the desired output_dim.  For more\n        details, refer to \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural\n        Networks for Sentence Classification\", Zhang and Wallace 2016, particularly Figure 1.\n\n        See allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder, Apache 2.0\n\n        Args:\n            field: The field in samples this encoder will work on.\n            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.\n            num_filters: This is the output dim for each convolutional layer, which is the number of \"filters\"\n                learned by that layer.\n            ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The\n                default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding\n                ngrams of size 2 to 5 with some number of filters.\n            conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)\n                Activation to use after the convolution layers.\n            output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of\n                this size.  If this value is `None`, we will just return the result of the max pooling,\n                giving an output of shape `len(ngram_filter_sizes) * num_filters`.\n            vocab_size: The size of character vocab.\n\n        Returns:\n            A tensor of shape `(batch_size, output_dim)`.\n        \"\"\"\n        super().__init__()\n        EmbeddingDim.__init__(self)\n        # the embedding layer\n        if isinstance(embed, int):\n            embed = nn.Embedding(num_embeddings=vocab_size,\n                                 embedding_dim=embed)\n        else:\n            raise ValueError(f'Unrecognized type for {embed}')\n        self.field = field\n        self.embed = TimeDistributed(embed)\n        self.encoder = TimeDistributed(\n            CnnEncoder(embed.embedding_dim, num_filters, ngram_filter_sizes, conv_layer_activation, output_dim))\n        self.embedding_dim = output_dim or num_filters * len(ngram_filter_sizes)\n\n    def forward(self, batch: dict, **kwargs):\n        tokens: torch.Tensor = batch[f'{self.field}_char_id']\n        mask = tokens.ge(0)\n        x = self.embed(tokens)\n        return self.encoder(x, mask)\n\n    def get_output_dim(self) -> int:\n        return self.embedding_dim\n\n\nclass CharCNNEmbedding(Embedding, AutoConfigurable):\n    def __init__(self,\n                 field,\n                 embed: Union[int, Embedding],\n                 num_filters: int,\n                 ngram_filter_sizes: Tuple[int, ...] = (2, 3, 4, 5),\n                 conv_layer_activation: str = 'ReLU',\n                 output_dim: Optional[int] = None,\n                 min_word_length=None\n                 ) -> None:\n        \"\"\"\n\n        Args:\n            field: The character field in samples this encoder will work on.\n            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.\n            num_filters: This is the output dim for each convolutional layer, which is the number of \"filters\"\n                learned by that layer.\n            ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The\n                default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding\n                ngrams of size 2 to 5 with some number of filters.\n            conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)\n                Activation to use after the convolution layers.\n            output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of\n                this size.  If this value is `None`, we will just return the result of the max pooling,\n                giving an output of shape `len(ngram_filter_sizes) * num_filters`.\n            min_word_length: For ngram filter with max size, the input (chars) is required to have at least max size\n                chars.\n        \"\"\"\n        super().__init__()\n        if min_word_length is None:\n            min_word_length = max(ngram_filter_sizes)\n        self.min_word_length = min_word_length\n        self.output_dim = output_dim\n        self.conv_layer_activation = conv_layer_activation\n        self.ngram_filter_sizes = ngram_filter_sizes\n        self.num_filters = num_filters\n        self.embed = embed\n        self.field = field\n\n    def transform(self, vocabs: VocabDict, **kwargs) -> Optional[Callable]:\n        if isinstance(self.embed, Embedding):\n            self.embed.transform(vocabs=vocabs)\n        vocab_name = self.vocab_name\n        if vocab_name not in vocabs:\n            vocabs[vocab_name] = Vocab()\n        return ToChar(self.field, vocab_name, min_word_length=self.min_word_length,\n                      pad=vocabs[vocab_name].safe_pad_token)\n\n    @property\n    def vocab_name(self):\n        vocab_name = f'{self.field}_char'\n        return vocab_name\n\n    def module(self, vocabs: VocabDict, **kwargs) -> Optional[nn.Module]:\n        embed = self.embed\n        if isinstance(embed, Embedding):\n            embed = embed.module(vocabs=vocabs)\n        return CharCNN(self.field,\n                       embed,\n                       self.num_filters,\n                       self.ngram_filter_sizes,\n                       self.conv_layer_activation,\n                       self.output_dim,\n                       vocab_size=len(vocabs[self.vocab_name]))\n", "hanlp/layers/embeddings/contextual_string_embedding_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-19 03:24\nfrom typing import List\n\nimport tensorflow as tf\nimport numpy as np\nfrom hanlp.components.rnn_language_model_tf import RNNLanguageModel\nfrom hanlp_common.constant import PAD\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp.utils.tf_util import copy_mask, hanlp_register, str_tensor_2d_to_list\nfrom hanlp_common.util import infer_space_after\n\n\n@hanlp_register\nclass ContextualStringEmbeddingTF(tf.keras.layers.Layer):\n\n    def __init__(self, forward_model_path=None, backward_model_path=None, max_word_len=10,\n                 trainable=False, name=None, dtype=None,\n                 dynamic=True, **kwargs):\n        assert dynamic, 'ContextualStringEmbedding works only in eager mode'\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        assert any([forward_model_path, backward_model_path]), 'At least one model is required'\n        self.forward_model_path = forward_model_path\n        self.backward_model_path = backward_model_path\n        self.forward_model = self._load_lm(forward_model_path) if forward_model_path else None\n        self.backward_model = self._load_lm(backward_model_path) if backward_model_path else None\n        if trainable:\n            self._fw = self.forward_model.model\n            self._bw = self.backward_model.model\n            for m in self._fw, self._bw:\n                m.trainable = True\n        self.supports_masking = True\n        self.max_word_len = max_word_len\n\n    def call(self, inputs, **kwargs):\n        str_inputs = str_tensor_2d_to_list(inputs)\n        outputs = self.embed(str_inputs)\n        copy_mask(inputs, outputs)\n        return outputs\n\n    def _load_lm(self, filepath):\n        filepath = get_resource(filepath)\n        lm = RNNLanguageModel()\n        lm.load(filepath)\n        model: tf.keras.Sequential = lm.model\n        for idx, layer in enumerate(model.layers):\n            if isinstance(layer, tf.keras.layers.LSTM):\n                lm.model = tf.keras.Sequential(model.layers[:idx + 1])  # discard dense layer\n                return lm\n\n    def embed(self, texts: List[List[str]]):\n        \"\"\"Embedding sentences (list of words) with contextualized string embedding\n\n        Args:\n          texts: List of words, not chars\n          texts: List[List[str]]: \n\n        Returns:\n\n        \n        \"\"\"\n        fw = None\n        if self.forward_model:\n            fw = self._run_rnn(texts, model=self.forward_model)\n        bw = None\n        if self.backward_model:\n            bw = self._run_rnn(texts, model=self.backward_model)\n        if not all(x is not None for x in [fw, bw]):\n            return fw if fw is not None else bw\n        else:\n            return tf.concat([fw, bw], axis=-1)\n\n    def _run_rnn(self, texts, model):\n        embeddings = []\n        inputs = []\n        offsets = []\n        tokenizer = model.transform.tokenize_func()\n        backward = not model.config['forward']\n        for sent in texts:\n            raw, off = self._get_raw_string(sent, tokenizer)\n            inputs.append(raw)\n            offsets.append(off)\n        outputs = model.model_from_config.predict(model.transform.inputs_to_dataset(inputs))\n        if backward:\n            outputs = tf.reverse(outputs, axis=[1])\n        maxlen = len(max(texts, key=len))\n        for hidden, off, sent in zip(outputs, offsets, texts):\n            embed = []\n            for (start, end), word in zip(off, sent):\n                embed.append(hidden[end - 1, :])\n            if len(embed) < maxlen:\n                embed += [np.zeros_like(embed[-1])] * (maxlen - len(embed))\n            embeddings.append(np.stack(embed))\n        return tf.stack(embeddings)\n\n    def _get_raw_string(self, sent: List[str], tokenizer):\n        raw_string = []\n        offsets = []\n        whitespace_after = infer_space_after(sent)\n        start = 0\n        for word, space in zip(sent, whitespace_after):\n            chars = tokenizer(word)\n            chars = chars[:self.max_word_len]\n            if space:\n                chars += [' ']\n            end = start + len(chars)\n            offsets.append((start, end))\n            start = end\n            raw_string += chars\n        return raw_string, offsets\n\n    def get_config(self):\n        config = {\n            'forward_model_path': self.forward_model_path,\n            'backward_model_path': self.backward_model_path,\n            'max_word_len': self.max_word_len,\n        }\n        base_config = super(ContextualStringEmbeddingTF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @property\n    def output_dim(self):\n        dim = 0\n        for model in self.forward_model, self.backward_model:\n            if model:\n                dim += model.config['rnn_units']\n        return dim\n\n    def compute_output_shape(self, input_shape):\n        return input_shape + self.output_dim\n\n    def compute_mask(self, inputs, mask=None):\n\n        return tf.not_equal(inputs, PAD)\n", "hanlp/layers/embeddings/fast_text.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-27 15:06\nimport logging\nimport os\nimport sys\nfrom typing import Optional, Callable\n\nimport fasttext\nimport torch\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom hanlp_common.configurable import AutoConfigurable\nfrom torch.utils.data import DataLoader\n\nfrom hanlp.common.dataset import PadSequenceDataLoader, TransformableDataset\nfrom hanlp.common.torch_component import TorchComponent\nfrom hanlp.common.transform import EmbeddingNamedTransform\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.embeddings.embedding import Embedding\nfrom hanlp.utils.io_util import get_resource, stdout_redirected\nfrom hanlp.utils.log_util import flash\n\n\nclass FastTextTransform(EmbeddingNamedTransform):\n    def __init__(self, filepath: str, src, dst=None, **kwargs) -> None:\n        if not dst:\n            dst = src + '_fasttext'\n        self.filepath = filepath\n        flash(f'Loading fasttext model {filepath} [blink][yellow]...[/yellow][/blink]')\n        filepath = get_resource(filepath)\n        with stdout_redirected(to=os.devnull, stdout=sys.stderr):\n            self._model = fasttext.load_model(filepath)\n        flash('')\n        output_dim = self._model['king'].size\n        super().__init__(output_dim, src, dst)\n\n    def __call__(self, sample: dict):\n        word = sample[self.src]\n        if isinstance(word, str):\n            vector = self.embed(word)\n        else:\n            vector = torch.stack([self.embed(each) for each in word])\n        sample[self.dst] = vector\n        return sample\n\n    def embed(self, word: str):\n        return torch.tensor(self._model[word])\n\n\nclass SelectFromBatchModule(torch.nn.Module):\n    def __init__(self, key) -> None:\n        super().__init__()\n        self.key = key\n\n    def __call__(self, batch: dict, mask=None, **kwargs):\n        return batch[self.key]\n\n\nclass FastTextEmbeddingModule(SelectFromBatchModule):\n\n    def __init__(self, key, embedding_dim: int) -> None:\n        \"\"\"An embedding layer for fastText (:cite:`bojanowski2017enriching`).\n\n        Args:\n            key: Field name.\n            embedding_dim: Size of this embedding layer\n        \"\"\"\n        super().__init__(key)\n        self.embedding_dim = embedding_dim\n\n    def __call__(self, batch: dict, mask=None, **kwargs):\n        outputs = super().__call__(batch, **kwargs)\n        outputs = pad_sequence(outputs, True, 0)\n        if mask is not None:\n            outputs = outputs.to(mask.device)\n        return outputs\n\n    def __repr__(self):\n        s = self.__class__.__name__ + '('\n        s += f'key={self.key}, embedding_dim={self.embedding_dim}'\n        s += ')'\n        return s\n\n    def get_output_dim(self):\n        return self.embedding_dim\n\n\nclass FastTextEmbedding(Embedding, AutoConfigurable):\n    def __init__(self, src: str, filepath: str) -> None:\n        \"\"\"An embedding layer builder for fastText (:cite:`bojanowski2017enriching`).\n\n        Args:\n            src: Field name.\n            filepath: Filepath to pretrained fastText embeddings.\n        \"\"\"\n        super().__init__()\n        self.src = src\n        self.filepath = filepath\n        self._fasttext = FastTextTransform(self.filepath, self.src)\n\n    def transform(self, **kwargs) -> Optional[Callable]:\n        return self._fasttext\n\n    def module(self, **kwargs) -> Optional[nn.Module]:\n        return FastTextEmbeddingModule(self._fasttext.dst, self._fasttext.output_dim)\n\n\nclass FastTextDataset(TransformableDataset):\n\n    def load_file(self, filepath: str):\n        raise NotImplementedError('Not supported.')\n\n\nclass FastTextEmbeddingComponent(TorchComponent):\n    def __init__(self, **kwargs) -> None:\n        \"\"\" Toy example of Word2VecEmbedding. It simply returns the embedding of a given word\n\n        Args:\n            **kwargs:\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def build_dataloader(self, data, shuffle=False, device=None, logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        embed: FastTextEmbedding = self.config.embed\n        dataset = FastTextDataset([{'token': data}], transform=embed.transform())\n        return PadSequenceDataLoader(dataset, device=device)\n\n    def build_optimizer(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def build_criterion(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def build_metric(self, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        raise NotImplementedError('Not supported.')\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        pass\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        pass\n\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        embed: FastTextEmbedding = self.config.embed\n        return embed.module()\n\n    def predict(self, data: str, **kwargs):\n        dataloader = self.build_dataloader(data, device=self.device)\n        for batch in dataloader:  # It's a toy so doesn't really do batching\n            return self.model(batch)[0]\n\n    @property\n    def devices(self):\n        return [torch.device('cpu')]\n", "hanlp/layers/embeddings/embedding.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-02 13:04\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Optional, Iterable\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Module\n\nfrom hanlp_common.configurable import AutoConfigurable\nfrom hanlp.common.transform import TransformList\nfrom hanlp.layers.dropout import IndependentDropout\n\n\nclass EmbeddingDim(ABC):\n    @property\n    @abstractmethod\n    def embedding_dim(self) -> int:\n        return -1\n\n    def get_output_dim(self) -> int:\n        return self.embedding_dim\n\n\nclass Embedding(AutoConfigurable, ABC):\n\n    def __init__(self) -> None:\n        \"\"\"\n        Base class for embedding builders.\n        \"\"\"\n        super().__init__()\n\n    def transform(self, **kwargs) -> Optional[Callable]:\n        \"\"\"Build a transform function for this embedding.\n\n        Args:\n            **kwargs: Containing vocabs, training etc. Not finalized for now.\n\n        Returns:\n            A transform function.\n        \"\"\"\n        return None\n\n    def module(self, **kwargs) -> Optional[nn.Module]:\n        \"\"\"Build a module for this embedding.\n\n        Args:\n            **kwargs: Containing vocabs, training etc. Not finalized for now.\n\n        Returns:\n            A module.\n        \"\"\"\n        return None\n\n\nclass ConcatModuleList(nn.ModuleList, EmbeddingDim):\n\n    def __init__(self, *modules: Optional[Iterable[Module]], dropout=None) -> None:\n        \"\"\"A ``nn.ModuleList`` to bundle several embeddings modules.\n\n        Args:\n            *modules: Embedding layers.\n            dropout: Dropout applied on the concatenated embedding.\n        \"\"\"\n        super().__init__(*modules)\n        if dropout:\n            dropout = IndependentDropout(p=dropout)\n        self.dropout = dropout\n\n    @property\n    def embedding_dim(self) -> int:\n        return sum(embed.embedding_dim for embed in self)\n\n    def get_output_dim(self) -> int:\n        return sum(embed.get_output_dim() for embed in self)\n\n    # noinspection PyMethodOverriding\n    def forward(self, batch: dict, **kwargs):\n        embeds = [embed(batch, **kwargs) for embed in self.embeddings]\n        if self.dropout:\n            embeds = self.dropout(*embeds)\n        return torch.cat(embeds, -1)\n\n    @property\n    def embeddings(self):\n        embeddings = [x for x in self]\n        if self.dropout:\n            embeddings.remove(self.dropout)\n        return embeddings\n\n\nclass EmbeddingList(Embedding):\n    def __init__(self, *embeddings_, embeddings: dict = None, dropout=None) -> None:\n        \"\"\"An embedding builder to bundle several embedding builders.\n\n        Args:\n            *embeddings_: A list of embedding builders.\n            embeddings: Deserialization for a dict of embedding builders.\n            dropout: Dropout applied on the concatenated embedding.\n        \"\"\"\n        # noinspection PyTypeChecker\n        self.dropout = dropout\n        self._embeddings: List[Embedding] = list(embeddings_)\n        if embeddings:\n            for each in embeddings:\n                if isinstance(each, dict):\n                    each = AutoConfigurable.from_config(each)\n                self._embeddings.append(each)\n        self.embeddings = [e.config for e in self._embeddings]\n\n    def transform(self, **kwargs):\n        transforms = [e.transform(**kwargs) for e in self._embeddings]\n        transforms = [t for t in transforms if t]\n        return TransformList(*transforms)\n\n    def module(self, **kwargs):\n        modules = [e.module(**kwargs) for e in self._embeddings]\n        modules = [m for m in modules if m]\n        return ConcatModuleList(modules, dropout=self.dropout)\n\n    def to_list(self):\n        return self._embeddings\n\n\ndef find_embedding_by_class(embed: Embedding, cls):\n    if isinstance(embed, cls):\n        return embed\n    if isinstance(embed, EmbeddingList):\n        for child in embed.to_list():\n            found = find_embedding_by_class(child, cls)\n            if found:\n                return found\n", "hanlp/layers/embeddings/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-24 21:48\n", "hanlp/layers/embeddings/word2vec_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-24 21:49\nimport os\nfrom typing import Tuple, Union, List\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\n\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp.utils.torch_util import load_word2vec\nfrom hanlp.utils.tf_util import hanlp_register\nfrom hanlp_common.util import DummyContext\n\n\nclass Word2VecEmbeddingV1(tf.keras.layers.Layer):\n    def __init__(self, path: str = None, vocab: VocabTF = None, normalize: bool = False, load_all=True, mask_zero=True,\n                 trainable=False, name=None, dtype=None, dynamic=False, **kwargs):\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        if load_all and vocab and vocab.locked:\n            vocab.unlock()\n        self.vocab, self.array_np = self._load(path, vocab, normalize)\n        self.vocab.lock()\n        self.array_ks = tf.keras.layers.Embedding(input_dim=len(self.vocab), output_dim=self.dim, trainable=trainable,\n                                                  embeddings_initializer=tf.keras.initializers.Constant(self.array_np),\n                                                  mask_zero=mask_zero)\n        self.mask_zero = mask_zero\n        self.supports_masking = mask_zero\n\n    def compute_mask(self, inputs, mask=None):\n        if not self.mask_zero:\n            return None\n\n        return math_ops.not_equal(inputs, self.vocab.pad_idx)\n\n    def call(self, inputs, **kwargs):\n        return self.array_ks(inputs, **kwargs)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.dim\n\n    @staticmethod\n    def _load(path, vocab, normalize=False) -> Tuple[VocabTF, Union[np.ndarray, None]]:\n        if not vocab:\n            vocab = VocabTF()\n        if not path:\n            return vocab, None\n        assert vocab.unk_idx is not None\n\n        word2vec, dim = load_word2vec(path)\n        for word in word2vec:\n            vocab.get_idx(word)\n\n        pret_embs = np.zeros(shape=(len(vocab), dim), dtype=np.float32)\n        state = np.random.get_state()\n        np.random.seed(0)\n        bias = np.random.uniform(low=-0.001, high=0.001, size=dim).astype(dtype=np.float32)\n        scale = np.sqrt(3.0 / dim)\n        for word, idx in vocab.token_to_idx.items():\n            vec = word2vec.get(word, None)\n            if vec is None:\n                vec = word2vec.get(word.lower(), None)\n                # if vec is not None:\n                #     vec += bias\n            if vec is None:\n                # vec = np.random.uniform(-scale, scale, [dim])\n                vec = np.zeros([dim], dtype=np.float32)\n            pret_embs[idx] = vec\n        # noinspection PyTypeChecker\n        np.random.set_state(state)\n        return vocab, pret_embs\n\n    @property\n    def size(self):\n        if self.array_np is not None:\n            return self.array_np.shape[0]\n\n    @property\n    def dim(self):\n        if self.array_np is not None:\n            return self.array_np.shape[1]\n\n    @property\n    def shape(self):\n        if self.array_np is None:\n            return None\n        return self.array_np.shape\n\n    def get_vector(self, word: str) -> np.ndarray:\n        assert self.array_np is not None\n        return self.array_np[self.vocab.get_idx_without_add(word)]\n\n    def __getitem__(self, word: Union[str, List, tf.Tensor]) -> np.ndarray:\n        if isinstance(word, str):\n            return self.get_vector(word)\n        elif isinstance(word, list):\n            vectors = np.zeros(shape=(len(word), self.dim))\n            for idx, token in enumerate(word):\n                vectors[idx] = self.get_vector(token)\n            return vectors\n        elif isinstance(word, tf.Tensor):\n            if word.dtype == tf.string:\n                word_ids = self.vocab.token_to_idx_table.lookup(word)\n                return tf.nn.embedding_lookup(self.array_tf, word_ids)\n            elif word.dtype == tf.int32 or word.dtype == tf.int64:\n                return tf.nn.embedding_lookup(self.array_tf, word)\n\n\n@hanlp_register\nclass Word2VecEmbeddingTF(tf.keras.layers.Embedding):\n\n    def __init__(self, filepath: str = None, vocab: VocabTF = None, expand_vocab=True, lowercase=True,\n                 input_dim=None, output_dim=None, unk=None, normalize=False,\n                 embeddings_initializer='VarianceScaling',\n                 embeddings_regularizer=None,\n                 activity_regularizer=None, embeddings_constraint=None, mask_zero=True, input_length=None,\n                 name=None, cpu=True, **kwargs):\n        filepath = get_resource(filepath)\n        word2vec, _output_dim = load_word2vec(filepath)\n        if output_dim:\n            assert output_dim == _output_dim, f'output_dim = {output_dim} does not match {filepath}'\n        output_dim = _output_dim\n        # if the `unk` token exists in the pretrained,\n        # then replace it with a self-defined one, usually the one in word vocab\n        if unk and unk in word2vec:\n            word2vec[vocab.safe_unk_token] = word2vec.pop(unk)\n        if vocab is None:\n            vocab = VocabTF()\n            vocab.update(word2vec.keys())\n        if expand_vocab and vocab.mutable:\n            for word in word2vec:\n                vocab.get_idx(word.lower() if lowercase else word)\n        if input_dim:\n            assert input_dim == len(vocab), f'input_dim = {input_dim} does not match {filepath}'\n        input_dim = len(vocab)\n        # init matrix\n        self._embeddings_initializer = embeddings_initializer\n        embeddings_initializer = tf.keras.initializers.get(embeddings_initializer)\n        with tf.device('cpu:0') if cpu else DummyContext():\n            pret_embs = embeddings_initializer(shape=[input_dim, output_dim]).numpy()\n        # insert to pret_embs\n        for word, idx in vocab.token_to_idx.items():\n            vec = word2vec.get(word, None)\n            # Retry lower case\n            if vec is None and lowercase:\n                vec = word2vec.get(word.lower(), None)\n            if vec is not None:\n                pret_embs[idx] = vec\n        if normalize:\n            pret_embs /= np.std(pret_embs)\n        if not name:\n            name = os.path.splitext(os.path.basename(filepath))[0]\n        super().__init__(input_dim, output_dim, tf.keras.initializers.Constant(pret_embs), embeddings_regularizer,\n                         activity_regularizer, embeddings_constraint, mask_zero, input_length, name=name, **kwargs)\n        self.filepath = filepath\n        self.expand_vocab = expand_vocab\n        self.lowercase = lowercase\n\n    def get_config(self):\n        config = {\n            'filepath': self.filepath,\n            'expand_vocab': self.expand_vocab,\n            'lowercase': self.lowercase,\n        }\n        base_config = super(Word2VecEmbeddingTF, self).get_config()\n        base_config['embeddings_initializer'] = self._embeddings_initializer\n        return dict(list(base_config.items()) + list(config.items()))\n\n\n@hanlp_register\nclass StringWord2VecEmbeddingTF(Word2VecEmbeddingTF):\n\n    def __init__(self, filepath: str = None, vocab: VocabTF = None, expand_vocab=True, lowercase=False, input_dim=None,\n                 output_dim=None, unk=None, normalize=False, embeddings_initializer='VarianceScaling',\n                 embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=True,\n                 input_length=None, name=None, **kwargs):\n        if vocab is None:\n            vocab = VocabTF()\n        self.vocab = vocab\n        super().__init__(filepath, vocab, expand_vocab, lowercase, input_dim, output_dim, unk, normalize,\n                         embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint,\n                         mask_zero, input_length, name, **kwargs)\n\n    def call(self, inputs):\n        assert inputs.dtype == tf.string, \\\n            f'Expect tf.string but got tf.{inputs.dtype.name}. {inputs}' \\\n            f'Please pass tf.{inputs.dtype.name} in.'\n        inputs = self.vocab.lookup(inputs)\n        # inputs._keras_mask = tf.not_equal(inputs, self.vocab.pad_idx)\n        return super().call(inputs)\n\n    def compute_mask(self, inputs, mask=None):\n        if not self.mask_zero:\n            return None\n        return tf.not_equal(inputs, self.vocab.pad_token)\n", "hanlp/layers/embeddings/char_rnn.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-02 23:49\nfrom typing import Optional, Callable, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom hanlp_common.configurable import AutoConfigurable\nfrom hanlp.common.transform import VocabDict, ToChar\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.layers.embeddings.embedding import Embedding, EmbeddingDim\n\n\nclass CharRNN(nn.Module, EmbeddingDim):\n    def __init__(self,\n                 field,\n                 vocab_size,\n                 embed: Union[int, nn.Embedding],\n                 hidden_size):\n        \"\"\"Character level RNN embedding module.\n\n        Args:\n            field: The field in samples this encoder will work on.\n            vocab_size: The size of character vocab.\n            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.\n            hidden_size: The hidden size of RNNs.\n        \"\"\"\n        super(CharRNN, self).__init__()\n        self.field = field\n        # the embedding layer\n        if isinstance(embed, int):\n            self.embed = nn.Embedding(num_embeddings=vocab_size,\n                                      embedding_dim=embed)\n        elif isinstance(embed, nn.Module):\n            self.embed = embed\n            embed = embed.embedding_dim\n        else:\n            raise ValueError(f'Unrecognized type for {embed}')\n        # the lstm layer\n        self.lstm = nn.LSTM(input_size=embed,\n                            hidden_size=hidden_size,\n                            batch_first=True,\n                            bidirectional=True)\n\n    def forward(self, batch, mask, **kwargs):\n        x = batch[f'{self.field}_char_id']\n        # [batch_size, seq_len, fix_len]\n        mask = x.ne(0)\n        # [batch_size, seq_len]\n        lens = mask.sum(-1)\n        char_mask = lens.gt(0)\n\n        # [n, fix_len, n_embed]\n        x = self.embed(batch) if isinstance(self.embed, EmbeddingDim) else self.embed(x[char_mask])\n        x = pack_padded_sequence(x[char_mask], lens[char_mask].cpu(), True, False)\n        x, (h, _) = self.lstm(x)\n        # [n, fix_len, n_out]\n        h = torch.cat(torch.unbind(h), -1)\n        # [batch_size, seq_len, n_out]\n        embed = h.new_zeros(*lens.shape, h.size(-1))\n        embed = embed.masked_scatter_(char_mask.unsqueeze(-1), h)\n\n        return embed\n\n    @property\n    def embedding_dim(self) -> int:\n        return self.lstm.hidden_size * 2\n\n\nclass CharRNNEmbedding(Embedding, AutoConfigurable):\n    def __init__(self,\n                 field,\n                 embed,\n                 hidden_size,\n                 max_word_length=None) -> None:\n        \"\"\"Character level RNN embedding module builder.\n\n        Args:\n            field: The field in samples this encoder will work on.\n            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.\n            hidden_size: The hidden size of RNNs.\n            max_word_length: Character sequence longer than ``max_word_length`` will be truncated.\n        \"\"\"\n        super().__init__()\n        self.field = field\n        self.hidden_size = hidden_size\n        self.embed = embed\n        self.max_word_length = max_word_length\n\n    def transform(self, vocabs: VocabDict, **kwargs) -> Optional[Callable]:\n        if isinstance(self.embed, Embedding):\n            self.embed.transform(vocabs=vocabs)\n        vocab_name = self.vocab_name\n        if vocab_name not in vocabs:\n            vocabs[vocab_name] = Vocab()\n        return ToChar(self.field, vocab_name, max_word_length=self.max_word_length)\n\n    @property\n    def vocab_name(self):\n        vocab_name = f'{self.field}_char'\n        return vocab_name\n\n    def module(self, vocabs: VocabDict, **kwargs) -> Optional[nn.Module]:\n        embed = self.embed\n        if isinstance(self.embed, Embedding):\n            embed = self.embed.module(vocabs=vocabs)\n        return CharRNN(self.field, len(vocabs[self.vocab_name]), embed, self.hidden_size)\n", "hanlp/layers/embeddings/util_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-09 15:46\nfrom typing import Union\n\nimport tensorflow as tf\n\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.layers.embeddings.char_cnn_tf import CharCNNEmbeddingTF\nfrom hanlp.layers.embeddings.char_rnn_tf import CharRNNEmbeddingTF\nfrom hanlp.layers.embeddings.concat_embedding import ConcatEmbedding\nfrom hanlp.layers.embeddings.contextual_string_embedding_tf import ContextualStringEmbeddingTF\nfrom hanlp.layers.embeddings.fast_text_tf import FastTextEmbeddingTF\nfrom hanlp.layers.embeddings.word2vec_tf import Word2VecEmbeddingTF, StringWord2VecEmbeddingTF, Word2VecEmbeddingV1\n\n_upgrade = tf.keras.utils.get_custom_objects()\nfor k, v in list(_upgrade.items()):\n    if k.startswith('HanLP>') and k.endswith('TF'):\n        _upgrade[k[:-2]] = v\n\n\ndef build_embedding(embeddings: Union[str, int, dict], word_vocab: VocabTF, transform: Transform):\n    if not embeddings:\n        return None\n    config = transform.config\n    if isinstance(embeddings, int):\n        embeddings = tf.keras.layers.Embedding(input_dim=len(word_vocab), output_dim=embeddings,\n                                               trainable=True, mask_zero=True)\n        config.embedding_trainable = True\n    elif isinstance(embeddings, dict):\n        # Upgrade to 2.1\n        embed_name = embeddings['class_name'].split('>')[-1]\n        if embeddings['class_name'].startswith('HanLP>') and not embeddings['class_name'].endswith('TF'):\n            embed_name += 'TF'\n        # Embeddings need vocab\n        if embed_name in (Word2VecEmbeddingTF.__name__, StringWord2VecEmbeddingTF.__name__):\n            # Vocab won't present in the dict\n            embeddings['config']['vocab'] = word_vocab\n        elif embed_name in (CharRNNEmbeddingTF.__name__, CharCNNEmbeddingTF.__name__):\n            embeddings['config']['word_vocab'] = word_vocab\n            embeddings['config']['char_vocab'] = transform.char_vocab\n            transform.map_x = False\n        layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)\n        # Embedding specific configuration\n        if layer.__class__.__name__ in ('FastTextEmbedding', 'FastTextEmbeddingTF'):\n            config.run_eagerly = True  # fasttext can only run in eager mode\n            config.embedding_trainable = False\n            transform.map_x = False  # fasttext accept string instead of int\n        return layer\n    elif isinstance(embeddings, list):\n        if embeddings_require_string_input(embeddings):\n            # those embeddings require string as input\n            transform.map_x = False\n            # use the string version of Word2VecEmbedding instead\n            for embed in embeddings:\n                if embed['class_name'].split('>')[-1] == Word2VecEmbeddingTF.__name__:\n                    embed['class_name'] = 'HanLP>' + StringWord2VecEmbeddingTF.__name__\n        return ConcatEmbedding(*[build_embedding(embed, word_vocab, transform) for embed in embeddings])\n    else:\n        assert isinstance(embeddings, str), 'embedding should be str or int or dict'\n        # word_vocab.unlock()\n        embeddings = Word2VecEmbeddingV1(path=embeddings, vocab=word_vocab,\n                                         trainable=config.get('embedding_trainable', False))\n        embeddings = embeddings.array_ks\n    return embeddings\n\n\ndef any_embedding_in(embeddings, *cls):\n    names = set(x.__name__ for x in cls)\n    names.update(list(x[:-2] for x in names if x.endswith('TF')))\n    for embed in embeddings:\n        if isinstance(embed, dict) and embed['class_name'].split('>')[-1] in names:\n            return True\n    return False\n\n\ndef embeddings_require_string_input(embeddings):\n    if not isinstance(embeddings, list):\n        embeddings = [embeddings]\n    return any_embedding_in(embeddings, CharRNNEmbeddingTF, CharCNNEmbeddingTF, FastTextEmbeddingTF,\n                            ContextualStringEmbeddingTF)\n\n\ndef embeddings_require_char_input(embeddings):\n    if not isinstance(embeddings, list):\n        embeddings = [embeddings]\n    return any_embedding_in(embeddings, CharRNNEmbeddingTF, CharCNNEmbeddingTF, ContextualStringEmbeddingTF)\n", "hanlp/layers/embeddings/char_rnn_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-20 17:02\nimport tensorflow as tf\n\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.tf_util import hanlp_register\n\n\n@hanlp_register\nclass CharRNNEmbeddingTF(tf.keras.layers.Layer):\n    def __init__(self, word_vocab: VocabTF, char_vocab: VocabTF,\n                 char_embedding=100,\n                 char_rnn_units=25,\n                 dropout=0.5,\n                 trainable=True, name=None, dtype=None, dynamic=False,\n                 **kwargs):\n        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n        self.char_embedding = char_embedding\n        self.char_rnn_units = char_rnn_units\n        self.char_vocab = char_vocab\n        self.word_vocab = word_vocab\n        self.embedding = tf.keras.layers.Embedding(input_dim=len(self.char_vocab), output_dim=char_embedding,\n                                                   trainable=True, mask_zero=True)\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=char_rnn_units,\n                                                                      return_state=True), name='bilstm')\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        mask = tf.not_equal(inputs, self.word_vocab.pad_token)\n        inputs = tf.ragged.boolean_mask(inputs, mask)\n        chars = tf.strings.unicode_split(inputs, input_encoding='UTF-8')\n        chars = chars.to_tensor(default_value=self.char_vocab.pad_token)\n        chars = self.char_vocab.lookup(chars)\n        embed = self.embedding(chars)\n        char_mask = embed._keras_mask\n        embed = self.dropout(embed)\n        embed_shape = tf.shape(embed)\n        embed = tf.reshape(embed, [-1, embed_shape[2], embed_shape[3]])\n        char_mask = tf.reshape(char_mask, [-1, embed_shape[2]])\n        all_zeros = tf.reduce_sum(tf.cast(char_mask, tf.int32), axis=1) == 0\n        char_mask_shape = tf.shape(char_mask)\n        hole = tf.zeros(shape=(char_mask_shape[0], char_mask_shape[1] - 1), dtype=tf.bool)\n        all_zeros = tf.expand_dims(all_zeros, -1)\n        non_all_zeros = tf.concat([all_zeros, hole], axis=1)\n        char_mask = tf.logical_or(char_mask, non_all_zeros)\n        output, h_fw, c_fw, h_bw, c_bw = self.rnn(embed, mask=char_mask)\n        hidden = tf.concat([h_fw, h_bw], axis=-1)\n        # hidden = output\n        hidden = tf.reshape(hidden, [embed_shape[0], embed_shape[1], -1])\n        hidden._keras_mask = mask\n        return hidden\n\n    def get_config(self):\n        config = {\n            'char_embedding': self.char_embedding,\n            'char_rnn_units': self.char_rnn_units,\n            'dropout': self.dropout.rate,\n        }\n        base_config = super(CharRNNEmbeddingTF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n", "hanlp/layers/transformers/utils_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 15:32\nimport tensorflow as tf\nfrom hanlp.optimizers.adamw import create_optimizer\nfrom hanlp.utils.log_util import logger\n\n\ndef config_is(config, model='bert'):\n    return model in type(config).__name__.lower()\n\n\ndef convert_examples_to_features(\n        words,\n        max_seq_length,\n        tokenizer,\n        labels=None,\n        label_map=None,\n        cls_token_at_end=False,\n        cls_token=\"[CLS]\",\n        cls_token_segment_id=1,\n        sep_token=\"[SEP]\",\n        sep_token_extra=False,\n        pad_on_left=False,\n        pad_token_id=0,\n        pad_token_segment_id=0,\n        pad_token_label_id=0,\n        sequence_a_segment_id=0,\n        mask_padding_with_zero=True,\n        unk_token='[UNK]',\n        do_padding=True\n):\n    \"\"\"Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n\n    Args:\n      words: \n      max_seq_length: \n      tokenizer: \n      labels:  (Default value = None)\n      label_map:  (Default value = None)\n      cls_token_at_end:  (Default value = False)\n      cls_token:  (Default value = \"[CLS]\")\n      cls_token_segment_id:  (Default value = 1)\n      sep_token:  (Default value = \"[SEP]\")\n      sep_token_extra:  (Default value = False)\n      pad_on_left:  (Default value = False)\n      pad_token_id:  (Default value = 0)\n      pad_token_segment_id:  (Default value = 0)\n      pad_token_label_id:  (Default value = 0)\n      sequence_a_segment_id:  (Default value = 0)\n      mask_padding_with_zero:  (Default value = True)\n      unk_token:  (Default value = '[UNK]')\n      do_padding:  (Default value = True)\n\n    Returns:\n\n    \"\"\"\n    args = locals()\n    if not labels:\n        labels = words\n        pad_token_label_id = False\n\n    tokens = []\n    label_ids = []\n    for word, label in zip(words, labels):\n        word_tokens = tokenizer.tokenize(word)\n        if not word_tokens:\n            # some wired chars cause the tagger to return empty list\n            word_tokens = [unk_token] * len(word)\n        tokens.extend(word_tokens)\n        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n        label_ids.extend([label_map[label] if label_map else True] + [pad_token_label_id] * (len(word_tokens) - 1))\n\n    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n    special_tokens_count = 3 if sep_token_extra else 2\n    if len(tokens) > max_seq_length - special_tokens_count:\n        logger.warning(\n            f'Input tokens {words} exceed the max sequence length of {max_seq_length - special_tokens_count}. '\n            f'The exceeded part will be truncated and ignored. '\n            f'You are recommended to split your long text into several sentences within '\n            f'{max_seq_length - special_tokens_count} tokens beforehand.')\n        tokens = tokens[: (max_seq_length - special_tokens_count)]\n        label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  token_type_ids:   0   0   0   0  0     0   0\n    #\n    # Where \"token_type_ids\" are used to indicate whether this is the first\n    # sequence or the second sequence. The embedding vectors for `type=0` and\n    # `type=1` were learned during pre-training and are added to the wordpiece\n    # embedding vector (and position vector). This is not *strictly* necessary\n    # since the [SEP] token unambiguously separates the sequences, but it makes\n    # it easier for the model to learn the concept of sequences.\n    #\n    # For classification tasks, the first vector (corresponding to [CLS]) is\n    # used as as the \"sentence vector\". Note that this only makes sense because\n    # the entire model is fine-tuned.\n    tokens += [sep_token]\n    label_ids += [pad_token_label_id]\n    if sep_token_extra:\n        # roberta uses an extra separator b/w pairs of sentences\n        tokens += [sep_token]\n        label_ids += [pad_token_label_id]\n    segment_ids = [sequence_a_segment_id] * len(tokens)\n\n    if cls_token_at_end:\n        tokens += [cls_token]\n        label_ids += [pad_token_label_id]\n        segment_ids += [cls_token_segment_id]\n    else:\n        tokens = [cls_token] + tokens\n        label_ids = [pad_token_label_id] + label_ids\n        segment_ids = [cls_token_segment_id] + segment_ids\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n    if do_padding:\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token_id] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n        else:\n            input_ids += [pad_token_id] * padding_length\n            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n            segment_ids += [pad_token_segment_id] * padding_length\n            label_ids += [pad_token_label_id] * padding_length\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(label_ids) == max_seq_length, f'failed for:\\n {args}'\n    else:\n        assert len(set(len(x) for x in [input_ids, input_mask, segment_ids, label_ids])) == 1\n    return input_ids, input_mask, segment_ids, label_ids\n\n\ndef build_adamw_optimizer(config, learning_rate, epsilon, clipnorm, train_steps, use_amp, warmup_steps,\n                          weight_decay_rate):\n    opt = create_optimizer(init_lr=learning_rate,\n                           epsilon=epsilon,\n                           weight_decay_rate=weight_decay_rate,\n                           clipnorm=clipnorm,\n                           num_train_steps=train_steps, num_warmup_steps=warmup_steps)\n    # opt = tfa.optimizers.AdamW(learning_rate=3e-5, epsilon=1e-08, weight_decay=0.01)\n    # opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n    config.optimizer = tf.keras.utils.serialize_keras_object(opt)\n    lr_config = config.optimizer['config']['learning_rate']['config']\n    if 'decay_schedule_fn' in lr_config:\n        lr_config['decay_schedule_fn'] = dict(\n            (k, v) for k, v in lr_config['decay_schedule_fn'].items() if not k.startswith('_'))\n    if use_amp:\n        # loss scaling is currently required when using mixed precision\n        opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n    return opt\n\n\ndef adjust_tokens_for_transformers(sentence):\n    \"\"\"Adjust tokens for BERT\n    See https://github.com/DoodleJZ/HPSG-Neural-Parser/blob/master/src_joint/Zparser.py#L1204\n\n    Args:\n      sentence: \n\n    Returns:\n\n    \n    \"\"\"\n    cleaned_words = []\n    for word in sentence:\n        # word = BERT_TOKEN_MAPPING.get(word, word)\n        if word == \"n't\" and cleaned_words:\n            cleaned_words[-1] = cleaned_words[-1] + \"n\"\n            word = \"'t\"\n        cleaned_words.append(word)\n    return cleaned_words\n", "hanlp/layers/transformers/utils.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-15 21:22\nfrom collections import defaultdict\nfrom typing import Tuple, Union\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom hanlp.components.parsers.ud import udify_util as util\nfrom hanlp.layers.transformers.pt_imports import PreTrainedModel\n\n\ndef transformer_encode(transformer: PreTrainedModel,\n                       input_ids,\n                       attention_mask=None,\n                       token_type_ids=None,\n                       token_span=None,\n                       layer_range: Union[int, Tuple[int, int]] = 0,\n                       max_sequence_length=None,\n                       average_subwords=False,\n                       ret_raw_hidden_states=False):\n    \"\"\"Run transformer and pool its outputs.\n\n    Args:\n        transformer: A transformer model.\n        input_ids: Indices of subwords.\n        attention_mask: Mask for these subwords.\n        token_type_ids: Type ids for each subword.\n        token_span: The spans of tokens.\n        layer_range: The range of layers to use. Note that the 0-th layer means embedding layer, so the last 3 layers\n                    of a 12-layer BERT will be (10, 13).\n        max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding\n                    window.\n         average_subwords: ``True`` to average subword representations.\n        ret_raw_hidden_states: ``True`` to return hidden states of each layer.\n\n    Returns:\n        Pooled outputs.\n\n    \"\"\"\n    if max_sequence_length and input_ids.size(-1) > max_sequence_length:\n        # TODO: split token type ids in transformer_sliding_window if token type ids are not always 1\n        outputs = transformer_sliding_window(transformer, input_ids, max_pieces=max_sequence_length)\n    else:\n        if attention_mask is None:\n            attention_mask = input_ids.ne(0)\n        if transformer.config.output_hidden_states:\n            outputs = transformer(input_ids, attention_mask, token_type_ids)[-1]\n        else:\n            outputs = transformer(input_ids, attention_mask, token_type_ids)[0]\n    if transformer.config.output_hidden_states:\n        if isinstance(layer_range, int):\n            outputs = outputs[layer_range:]\n        else:\n            outputs = outputs[layer_range[0], layer_range[1]]\n        # Slow pick\n        # hs = []\n        # for h in outputs:\n        #     hs.append(pick_tensor_for_each_token(h, token_span, average_subwords))\n        # Fast pick\n        if not isinstance(outputs, torch.Tensor):\n            x = torch.stack(outputs)\n        else:\n            x = outputs\n        L, B, T, F = x.size()\n        x = x.flatten(end_dim=1)\n        # tile token_span as x\n        if token_span is not None:\n            token_span = token_span.repeat(L, 1, 1)\n        hs = pick_tensor_for_each_token(x, token_span, average_subwords).view(L, B, -1, F)\n        if ret_raw_hidden_states:\n            return hs, outputs\n        return hs\n    else:\n        if ret_raw_hidden_states:\n            return pick_tensor_for_each_token(outputs, token_span, average_subwords), outputs\n        return pick_tensor_for_each_token(outputs, token_span, average_subwords)\n\n\ndef pick_tensor_for_each_token(h, token_span, average_subwords):\n    if token_span is None:\n        return h\n    if average_subwords and token_span.size(-1) > 1:\n        batch_size = h.size(0)\n        h_span = h.gather(1, token_span.view(batch_size, -1).unsqueeze(-1).expand(-1, -1, h.shape[-1]))\n        h_span = h_span.view(batch_size, *token_span.shape[1:], -1)\n        n_sub_tokens = token_span.ne(0)\n        n_sub_tokens[:, 0, 0] = True\n        h_span = (h_span * n_sub_tokens.unsqueeze(-1)).sum(2)\n        n_sub_tokens = n_sub_tokens.sum(-1).unsqueeze(-1)\n        zero_mask = n_sub_tokens == 0\n        if torch.any(zero_mask):\n            n_sub_tokens[zero_mask] = 1  # avoid dividing by zero\n        embed = h_span / n_sub_tokens\n    else:\n        embed = h.gather(1, token_span[:, :, 0].unsqueeze(-1).expand(-1, -1, h.size(-1)))\n    return embed\n\n\ndef transformer_sliding_window(transformer: PreTrainedModel,\n                               input_ids: torch.LongTensor,\n                               input_mask=None,\n                               offsets: torch.LongTensor = None,\n                               token_type_ids: torch.LongTensor = None,\n                               max_pieces=512,\n                               start_tokens: int = 1,\n                               end_tokens: int = 1,\n                               ret_cls=None,\n                               ) -> torch.Tensor:\n    \"\"\"\n\n    Args:\n      transformer:\n      input_ids: torch.LongTensor: \n      input_mask:  (Default value = None)\n      offsets: torch.LongTensor:  (Default value = None)\n      token_type_ids: torch.LongTensor:  (Default value = None)\n      max_pieces:  (Default value = 512)\n      start_tokens: int:  (Default value = 1)\n      end_tokens: int:  (Default value = 1)\n      ret_cls:  (Default value = None)\n\n    Returns:\n\n    \n    \"\"\"\n    # pylint: disable=arguments-differ\n    batch_size, full_seq_len = input_ids.size(0), input_ids.size(-1)\n    initial_dims = list(input_ids.shape[:-1])\n\n    # The embedder may receive an input tensor that has a sequence length longer than can\n    # be fit. In that case, we should expect the wordpiece indexer to create padded windows\n    # of length `max_pieces` for us, and have them concatenated into one long sequence.\n    # E.g., \"[CLS] I went to the [SEP] [CLS] to the store to [SEP] ...\"\n    # We can then split the sequence into sub-sequences of that length, and concatenate them\n    # along the batch dimension so we effectively have one huge batch of partial sentences.\n    # This can then be fed into BERT without any sentence length issues. Keep in mind\n    # that the memory consumption can dramatically increase for large batches with extremely\n    # long sentences.\n    needs_split = full_seq_len > max_pieces\n    if needs_split:\n        input_ids = split_to_sliding_window(input_ids, max_pieces)\n\n    # if token_type_ids is None:\n    #     token_type_ids = torch.zeros_like(input_ids)\n    if input_mask is None:\n        input_mask = (input_ids != 0).long()\n\n    # input_ids may have extra dimensions, so we reshape down to 2-d\n    # before calling the BERT model and then reshape back at the end.\n    outputs = transformer(input_ids=util.combine_initial_dims_to_1d_or_2d(input_ids),\n                          # token_type_ids=util.combine_initial_dims_to_1d_or_2d(token_type_ids),\n                          attention_mask=util.combine_initial_dims_to_1d_or_2d(input_mask)).to_tuple()\n    if len(outputs) == 3:\n        all_encoder_layers = outputs.hidden_states\n        all_encoder_layers = torch.stack(all_encoder_layers)\n    elif len(outputs) == 2:\n        all_encoder_layers, _ = outputs[:2]\n    else:\n        all_encoder_layers = outputs[0]\n\n    if needs_split:\n        if ret_cls is not None:\n            cls_mask = input_ids[:, 0] == input_ids[0][0]\n            cls_hidden = all_encoder_layers[:, 0, :]\n            if ret_cls == 'max':\n                cls_hidden[~cls_mask] = -1e20\n            else:\n                cls_hidden[~cls_mask] = 0\n            cls_mask = cls_mask.view(-1, batch_size).transpose(0, 1)\n            cls_hidden = cls_hidden.reshape(cls_mask.size(1), batch_size, -1).transpose(0, 1)\n            if ret_cls == 'max':\n                cls_hidden = cls_hidden.max(1)[0]\n            elif ret_cls == 'raw':\n                return cls_hidden, cls_mask\n            else:\n                cls_hidden = torch.sum(cls_hidden, dim=1)\n                cls_hidden /= torch.sum(cls_mask, dim=1, keepdim=True)\n            return cls_hidden\n        else:\n            recombined_embeddings, select_indices = restore_from_sliding_window(all_encoder_layers, batch_size,\n                                                                                max_pieces, full_seq_len, start_tokens,\n                                                                                end_tokens)\n\n            initial_dims.append(len(select_indices))\n    else:\n        recombined_embeddings = all_encoder_layers\n\n    # Recombine the outputs of all layers\n    # (layers, batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n    # recombined = torch.cat(combined, dim=2)\n    # input_mask = (recombined_embeddings != 0).long()\n\n    # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n\n    if offsets is None:\n        # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)\n        dims = initial_dims if needs_split else input_ids.size()\n        layers = util.uncombine_initial_dims(recombined_embeddings, dims)\n    else:\n        # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n        offsets2d = util.combine_initial_dims_to_1d_or_2d(offsets)\n        # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n        range_vector = util.get_range_vector(offsets2d.size(0),\n                                             device=util.get_device_of(recombined_embeddings)).unsqueeze(1)\n        # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n        selected_embeddings = recombined_embeddings[:, range_vector, offsets2d]\n\n        layers = util.uncombine_initial_dims(selected_embeddings, offsets.size())\n\n    return layers\n\n\ndef split_to_sliding_window(input_ids, max_pieces):\n    # Split the flattened list by the window size, `max_pieces`\n    split_input_ids = list(input_ids.split(max_pieces, dim=-1))\n    # We want all sequences to be the same length, so pad the last sequence\n    last_window_size = split_input_ids[-1].size(-1)\n    padding_amount = max_pieces - last_window_size\n    split_input_ids[-1] = F.pad(split_input_ids[-1], pad=[0, padding_amount], value=0)\n    # Now combine the sequences along the batch dimension\n    input_ids = torch.cat(split_input_ids, dim=0)\n    return input_ids\n\n\ndef restore_from_sliding_window(all_encoder_layers, batch_size, max_pieces, full_seq_len, start_tokens, end_tokens):\n    # First, unpack the output embeddings into one long sequence again\n    unpacked_embeddings = torch.split(all_encoder_layers, batch_size, dim=-3)\n    unpacked_embeddings = torch.cat(unpacked_embeddings, dim=-2)\n    # Next, select indices of the sequence such that it will result in embeddings representing the original\n    # sentence. To capture maximal context, the indices will be the middle part of each embedded window\n    # sub-sequence (plus any leftover start and final edge windows), e.g.,\n    #  0     1 2    3  4   5    6    7     8     9   10   11   12    13 14  15\n    # \"[CLS] I went to the very fine [SEP] [CLS] the very fine store to eat [SEP]\"\n    # with max_pieces = 8 should produce max context indices [2, 3, 4, 10, 11, 12] with additional start\n    # and final windows with indices [0, 1] and [14, 15] respectively.\n    # Find the stride as half the max pieces, ignoring the special start and end tokens\n    # Calculate an offset to extract the centermost embeddings of each window\n    stride = (max_pieces - start_tokens - end_tokens) // 2\n    stride_offset = stride // 2 + start_tokens\n    first_window = list(range(stride_offset))\n    max_context_windows = [i for i in range(full_seq_len)\n                           if stride_offset - 1 < i % max_pieces < stride_offset + stride]\n    final_window_start = max_context_windows[-1] + 1\n    final_window = list(range(final_window_start, full_seq_len))\n    select_indices = first_window + max_context_windows + final_window\n    select_indices = torch.LongTensor(select_indices).to(unpacked_embeddings.device)\n    recombined_embeddings = unpacked_embeddings.index_select(-2, select_indices)\n    return recombined_embeddings, select_indices\n\n\ndef build_optimizer_for_pretrained(model: torch.nn.Module,\n                                   pretrained: torch.nn.Module,\n                                   lr=1e-5,\n                                   weight_decay=0.01,\n                                   eps=1e-8,\n                                   transformer_lr=None,\n                                   transformer_weight_decay=None,\n                                   no_decay=('bias', 'LayerNorm.bias', 'LayerNorm.weight'),\n                                   **kwargs):\n    if transformer_lr is None:\n        transformer_lr = lr\n    if transformer_weight_decay is None:\n        transformer_weight_decay = weight_decay\n    params = defaultdict(lambda: defaultdict(list))\n    pretrained = set(pretrained.parameters())\n    if isinstance(no_decay, tuple):\n        def no_decay_fn(name):\n            return any(nd in name for nd in no_decay)\n    else:\n        assert callable(no_decay), 'no_decay has to be callable or a tuple of str'\n        no_decay_fn = no_decay\n    for n, p in model.named_parameters():\n        is_pretrained = 'pretrained' if p in pretrained else 'non_pretrained'\n        is_no_decay = 'no_decay' if no_decay_fn(n) else 'decay'\n        params[is_pretrained][is_no_decay].append(p)\n\n    grouped_parameters = [\n        {'params': params['pretrained']['decay'], 'weight_decay': transformer_weight_decay, 'lr': transformer_lr},\n        {'params': params['pretrained']['no_decay'], 'weight_decay': 0.0, 'lr': transformer_lr},\n        {'params': params['non_pretrained']['decay'], 'weight_decay': weight_decay, 'lr': lr},\n        {'params': params['non_pretrained']['no_decay'], 'weight_decay': 0.0, 'lr': lr},\n    ]\n\n    from transformers import optimization\n    return optimization.AdamW(\n        grouped_parameters,\n        lr=lr,\n        weight_decay=weight_decay,\n        eps=eps,\n        no_deprecation_warning=True,  # For backwards compatability\n        **kwargs)\n\n\ndef build_optimizer_scheduler_with_transformer(model: torch.nn.Module,\n                                               transformer: torch.nn.Module,\n                                               lr: float,\n                                               transformer_lr: float,\n                                               num_training_steps: int,\n                                               warmup_steps: Union[float, int],\n                                               weight_decay: float,\n                                               adam_epsilon: float,\n                                               no_decay=('bias', 'LayerNorm.bias', 'LayerNorm.weight')):\n    optimizer = build_optimizer_for_pretrained(model,\n                                               transformer,\n                                               lr,\n                                               weight_decay,\n                                               eps=adam_epsilon,\n                                               transformer_lr=transformer_lr,\n                                               no_decay=no_decay)\n    if isinstance(warmup_steps, float):\n        assert 0 < warmup_steps < 1, 'warmup_steps has to fall in range (0, 1) when it is float.'\n        warmup_steps = num_training_steps * warmup_steps\n    from transformers import optimization\n    scheduler = optimization.get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps)\n    return optimizer, scheduler\n\n\ndef get_optimizers(\n        model: torch.nn.Module,\n        num_training_steps: int,\n        learning_rate=5e-5,\n        adam_epsilon=1e-8,\n        weight_decay=0.0,\n        warmup_steps=0.1,\n) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n    \"\"\"\n    Modified from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L232\n    Setup the optimizer and the learning rate scheduler.\n\n    We provide a reasonable default that works well.\n    \"\"\"\n    if isinstance(warmup_steps, float):\n        assert 0 < warmup_steps < 1\n        warmup_steps = int(num_training_steps * warmup_steps)\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    from transformers import AdamW, get_linear_schedule_with_warmup\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n    )\n    return optimizer, scheduler\n\n\ndef collect_decay_params(model, weight_decay):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return optimizer_grouped_parameters\n", "hanlp/layers/transformers/pt_imports.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-09 11:25\nimport os\nimport warnings\n\nfrom hanlp.layers.transformers.resource import get_tokenizer_mirror, get_model_mirror\n\nif os.environ.get('TOKENIZERS_PARALLELISM', None) is None:\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom transformers import BertTokenizer, BertConfig, PretrainedConfig, AutoConfig, AutoTokenizer, PreTrainedTokenizer, \\\n    BertTokenizerFast, AlbertConfig, BertModel, AutoModel, PreTrainedModel, AutoModelForSequenceClassification, \\\n    AutoModelForTokenClassification, BartModel\n\n\nclass AutoModel_(AutoModel):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, training=True, **kwargs):\n        pretrained_model_name_or_path = get_model_mirror(pretrained_model_name_or_path)\n        if training:\n            return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        else:\n            if isinstance(pretrained_model_name_or_path, str):\n                pretrained_model_name_or_path = get_tokenizer_mirror(pretrained_model_name_or_path)\n                return super().from_config(AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs))\n            else:\n                assert not kwargs\n                return super().from_config(pretrained_model_name_or_path)\n\n\nclass AutoConfig_(AutoConfig):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        pretrained_model_name_or_path = get_tokenizer_mirror(pretrained_model_name_or_path)\n        return super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n\n\nclass AutoTokenizer_(AutoTokenizer):\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, use_fast=True,\n                        do_basic_tokenize=True) -> PreTrainedTokenizer:\n        if isinstance(pretrained_model_name_or_path, str):\n            transformer = pretrained_model_name_or_path\n        else:\n            transformer = pretrained_model_name_or_path.transformer\n        additional_config = dict()\n        if transformer.startswith('voidful/albert_chinese_') or transformer.startswith('uer/albert'):\n            cls = BertTokenizer\n        elif transformer == 'cl-tohoku/bert-base-japanese-char':\n            # Since it's char level model, it's OK to use char level tok instead of fugashi\n            # from hanlp.utils.lang.ja.bert_tok import BertJapaneseTokenizerFast\n            # cls = BertJapaneseTokenizerFast\n            from transformers import BertJapaneseTokenizer\n            cls = BertJapaneseTokenizer\n            # from transformers import BertTokenizerFast\n            # cls = BertTokenizerFast\n            additional_config['word_tokenizer_type'] = 'basic'\n        elif transformer == \"Langboat/mengzi-bert-base\":\n            cls = BertTokenizerFast if use_fast else BertTokenizer\n        else:\n            cls = AutoTokenizer\n        if use_fast and not do_basic_tokenize:\n            warnings.warn('`do_basic_tokenize=False` might not work when `use_fast=True`')\n        tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,\n                                        do_basic_tokenize=do_basic_tokenize,\n                                        **additional_config)\n        tokenizer.name_or_path = transformer\n        return tokenizer\n", "hanlp/layers/transformers/loader_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-04 06:05\nimport tensorflow as tf\nfrom transformers import TFAutoModel\n\nfrom hanlp.layers.transformers.pt_imports import AutoTokenizer_, AutoModel_\n\n\ndef build_transformer(transformer, max_seq_length, num_labels, tagging=True, tokenizer_only=False):\n    tokenizer = AutoTokenizer_.from_pretrained(transformer)\n    if tokenizer_only:\n        return tokenizer\n    l_bert = TFAutoModel.from_pretrained(transformer)\n    l_input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=\"input_ids\")\n    l_mask_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=\"mask_ids\")\n    l_token_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=\"token_type_ids\")\n    output = l_bert(input_ids=l_input_ids, token_type_ids=l_token_type_ids, attention_mask=l_mask_ids).last_hidden_state\n    if not tagging:\n        output = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n    logits = tf.keras.layers.Dense(num_labels)(output)\n    model = tf.keras.Model(inputs=[l_input_ids, l_mask_ids, l_token_type_ids], outputs=logits)\n    model.build(input_shape=(None, max_seq_length))\n    return model, tokenizer\n", "hanlp/layers/transformers/relative_transformer.py": "# A modified version of the implementation from the following paper:\n# TENER: Adapting Transformer Encoder for Named Entity Recognition\n# Hang Yan, Bocao Deng, Xiaonan Li, Xipeng Qiu\n\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nfrom hanlp.common.structure import ConfigTracker\n\n\nclass RelativeSinusoidalPositionalEmbedding(nn.Module):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\n    Padding symbols are ignored.\n\n    Args:\n        embedding_dim: embedding size of each position\n        padding_idx:\n    Returns:\n\n    \"\"\"\n\n    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        assert init_size % 2 == 0\n        weights = self.get_embedding(\n            init_size + 1,\n            embedding_dim,\n            padding_idx,\n        )\n        self.register_buffer('weights', weights)\n\n    def get_embedding(self, num_embeddings, embedding_dim, padding_idx=None):\n        \"\"\"Build sinusoidal embeddings.\n        This matches the implementation in tensor2tensor, but differs slightly\n        from the description in Section 3.5 of \"Attention Is All You Need\".\n\n        Args:\n          num_embeddings:\n          embedding_dim:\n          padding_idx:  (Default value = None)\n\n        Returns:\n\n        \"\"\"\n        half_dim = embedding_dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n        emb = torch.arange(-num_embeddings // 2, num_embeddings // 2, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n        if embedding_dim % 2 == 1:\n            # zero pad\n            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n        if padding_idx is not None:\n            emb[padding_idx, :] = 0\n        self.origin_shift = num_embeddings // 2 + 1\n        return emb\n\n    def forward(self, inputs: Tensor):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\n\n        Args:\n          inputs: Tensor:\n\n        Returns:\n\n        \"\"\"\n        bsz, seq_len = inputs.size()\n        max_pos = self.padding_idx + seq_len\n        if max_pos > self.origin_shift:\n            # recompute/expand embeddings if needed\n            weights = self.get_embedding(\n                max_pos * 2,\n                self.embedding_dim,\n                self.padding_idx,\n            )\n            weights = weights.to(self.weights.device)\n            del self.weights\n            self.origin_shift = weights.size(0) // 2\n            self.register_buffer('weights', weights)\n\n        positions = torch.arange(-seq_len, seq_len).to(inputs.device).long() + self.origin_shift  # 2*seq_len\n        embed = self.weights.index_select(0, positions.long()).detach()\n        return embed\n\n\nclass RelativeMultiHeadAttn(nn.Module):\n    def __init__(self, in_features, num_heads, dropout, r_w_bias=None, r_r_bias=None, init_seq_length=1024,\n                 k_as_x=True):\n        \"\"\"\n        Args:\n            in_features:\n            num_heads:\n            dropout:\n            r_w_bias: n_head x head_dim or None\n            r_r_bias: n_head x head_dim or None\n            init_seq_length:\n            k_as_x:\n        \"\"\"\n        super().__init__()\n        self.k_as_x = k_as_x\n        if k_as_x:\n            self.qv_linear = nn.Linear(in_features, in_features * 2, bias=False)\n        else:\n            self.qkv_linear = nn.Linear(in_features, in_features * 3, bias=False)\n        self.n_head = num_heads\n        self.head_dim = in_features // num_heads\n        self.dropout_layer = nn.Dropout(dropout)\n        self.pos_embed = RelativeSinusoidalPositionalEmbedding(self.head_dim, 0, init_seq_length)\n        if r_r_bias is None or r_w_bias is None:  # Biases are not shared\n            self.r_r_bias = nn.Parameter(nn.init.xavier_normal_(torch.zeros(num_heads, in_features // num_heads)))\n            self.r_w_bias = nn.Parameter(nn.init.xavier_normal_(torch.zeros(num_heads, in_features // num_heads)))\n        else:\n            self.r_r_bias = r_r_bias  # r_r_bias\u5c31\u662fv\n            self.r_w_bias = r_w_bias  # r_w_bias\u5c31\u662fu\n\n    def forward(self, x, mask):\n        \"\"\"\n\n        Args:\n          x: batch_size x max_len x d_model\n          mask: batch_size x max_len\n\n        Returns:\n\n        \"\"\"\n\n        batch_size, max_len, d_model = x.size()\n        pos_embed = self.pos_embed(mask)  # l x head_dim\n\n        if self.k_as_x:\n            qv = self.qv_linear(x)  # batch_size x max_len x d_model2\n            q, v = torch.chunk(qv, chunks=2, dim=-1)\n            k = x.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n        else:\n            qkv = self.qkv_linear(x)  # batch_size x max_len x d_model3\n            q, k, v = torch.chunk(qkv, chunks=3, dim=-1)\n            k = k.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n\n        q = q.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n        v = v.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)  # b x n x l x d\n\n        rw_head_q = q + self.r_r_bias[:, None]\n        AC = torch.einsum('bnqd,bnkd->bnqk', [rw_head_q, k])  # b x n x l x d, n\u662fhead\n\n        D_ = torch.einsum('nd,ld->nl', self.r_w_bias, pos_embed)[None, :, None]  # head x 2max_len, \u6bcf\u4e2ahead\u5bf9\u4f4d\u7f6e\u7684bias\n        B_ = torch.einsum('bnqd,ld->bnql', q, pos_embed)  # bsz x head  x max_len x 2max_len\uff0c\u6bcf\u4e2aquery\u5bf9\u6bcf\u4e2ashift\u7684\u504f\u79fb\n        E_ = torch.einsum('bnqd,ld->bnql', k, pos_embed)  # bsz x head x max_len x 2max_len, key\u5bf9relative\u7684bias\n        BD = B_ + D_  # bsz x head x max_len x 2max_len, \u8981\u8f6c\u6362\u4e3absz x head x max_len x max_len\n        if self.k_as_x:\n            BD = self._shift(BD)\n            attn = AC + BD\n        else:\n            BDE = self._shift(BD) + self._transpose_shift(E_)\n            attn = AC + BDE\n\n        attn = attn.masked_fill(mask[:, None, None, :].eq(0), float('-inf'))\n\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout_layer(attn)\n        v = torch.matmul(attn, v).transpose(1, 2).reshape(batch_size, max_len, d_model)  # b x n x l x d\n\n        return v\n\n    def _shift(self, BD):\n        \"\"\"\u7c7b\u4f3c\n        -3 -2 -1 0 1 2\n        -3 -2 -1 0 1 2\n        -3 -2 -1 0 1 2\n        \u8f6c\u6362\u4e3a\n        0   1  2\n        -1  0  1\n        -2 -1  0\n\n        Args:\n          BD: batch_size x n_head x max_len x 2max_len\n\n        Returns:\n          batch_size x n_head x max_len x max_len\n\n        \"\"\"\n        bsz, n_head, max_len, _ = BD.size()\n        zero_pad = BD.new_zeros(bsz, n_head, max_len, 1)\n        BD = torch.cat([BD, zero_pad], dim=-1).view(bsz, n_head, -1, max_len)  # bsz x n_head x (2max_len+1) x max_len\n        BD = BD.narrow(dim=2, start=0, length=2 * max_len) \\\n            .view(bsz, n_head, max_len, -1)  # bsz x n_head x 2max_len x max_len\n        BD = BD.narrow(dim=-1, start=max_len, length=max_len)\n        return BD\n\n    def _transpose_shift(self, E):\n        \"\"\"\u7c7b\u4f3c\n          -3   -2   -1   0   1   2\n         -30  -20  -10  00  10  20\n        -300 -200 -100 000 100 200\n\n        \u8f6c\u6362\u4e3a\n          0  -10   -200\n          1   00   -100\n          2   10    000\n\n        Args:\n          E: batch_size x n_head x max_len x 2max_len\n\n        Returns:\n          batch_size x n_head x max_len x max_len\n\n        \"\"\"\n        bsz, n_head, max_len, _ = E.size()\n        zero_pad = E.new_zeros(bsz, n_head, max_len, 1)\n        # bsz x n_head x -1 x (max_len+1)\n        E = torch.cat([E, zero_pad], dim=-1).view(bsz, n_head, -1, max_len)\n        indice = (torch.arange(max_len) * 2 + 1).to(E.device)\n        E = E.index_select(index=indice, dim=-2).transpose(-1, -2)  # bsz x n_head x max_len x max_len\n\n        return E\n\n\nclass RelativeTransformerLayer(nn.Module):\n    def __init__(self,\n                 in_features,\n                 num_heads=4,\n                 feedforward_dim=256,\n                 dropout=0.2,\n                 dropout_attn=None,\n                 after_norm=True,\n                 k_as_x=True,\n                 init_seq_length=1024):\n        super().__init__()\n        if dropout_attn is None:\n            dropout_attn = dropout\n        self.after_norm = after_norm\n        self.norm1 = nn.LayerNorm(in_features)\n        self.norm2 = nn.LayerNorm(in_features)\n        self.self_attn = RelativeMultiHeadAttn(in_features,\n                                               num_heads,\n                                               dropout=dropout_attn,\n                                               init_seq_length=init_seq_length,\n                                               k_as_x=k_as_x)\n        self.ffn = nn.Sequential(nn.Linear(in_features, feedforward_dim),\n                                 nn.LeakyReLU(),\n                                 nn.Dropout(dropout, inplace=True),\n                                 nn.Linear(feedforward_dim, in_features),\n                                 nn.Dropout(dropout, inplace=True))\n\n    def forward(self, x, mask):\n        \"\"\"\n\n        Args:\n          x: batch_size x max_len x hidden_size\n          mask: batch_size x max_len, \u4e3a0\u7684\u5730\u65b9\u4e3apad\n\n        Returns:\n          batch_size x max_len x hidden_size\n\n        \"\"\"\n        residual = x\n        if not self.after_norm:\n            x = self.norm1(x)\n\n        x = self.self_attn(x, mask)\n        x = x + residual\n        if self.after_norm:\n            x = self.norm1(x)\n        residual = x\n        if not self.after_norm:\n            x = self.norm2(x)\n        x = self.ffn(x)\n        x = residual + x\n        if self.after_norm:\n            x = self.norm2(x)\n        return x\n\n\nclass RelativeTransformer(nn.Module):\n    def __init__(self,\n                 in_features,\n                 num_layers,\n                 feedforward_dim,\n                 num_heads,\n                 dropout,\n                 dropout_attn=None,\n                 after_norm=True,\n                 init_seq_length=1024,\n                 k_as_x=True):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            RelativeTransformerLayer(in_features, feedforward_dim, num_heads, dropout, dropout_attn, after_norm,\n                                     init_seq_length=init_seq_length, k_as_x=k_as_x)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x: Tensor, mask: Tensor):\n        \"\"\"\n\n        Args:\n          x: batch_size x max_len\n          mask: batch_size x max_len. \u6709value\u7684\u5730\u65b9\u4e3a1\n          x: Tensor: \n          mask: Tensor: \n\n        Returns:\n\n        \"\"\"\n        if not x.numel():\n            return x\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\n\nclass RelativeTransformerEncoder(RelativeTransformer, ConfigTracker):\n    def __init__(self,\n                 in_features,\n                 num_layers=2,\n                 num_heads=4,\n                 feedforward_dim=256,\n                 dropout=0.1,\n                 dropout_attn=0.1,\n                 after_norm=True,\n                 k_as_x=True,\n                 ):\n        super().__init__(in_features, num_layers, num_heads, feedforward_dim, dropout, dropout_attn, after_norm)\n        ConfigTracker.__init__(self, locals())\n\n    def get_output_dim(self):\n        return self.config['in_features']\n", "hanlp/layers/transformers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 15:17\n# mute transformers\nimport logging\n\nlogging.getLogger('transformers.file_utils').setLevel(logging.ERROR)\nlogging.getLogger('transformers.filelock').setLevel(logging.ERROR)\nlogging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)\nlogging.getLogger('transformers.configuration_utils').setLevel(logging.ERROR)\nlogging.getLogger('transformers.modeling_tf_utils').setLevel(logging.ERROR)\nlogging.getLogger('transformers.modeling_utils').setLevel(logging.ERROR)\nlogging.getLogger('transformers.tokenization_utils_base').setLevel(logging.ERROR)\n", "hanlp/layers/transformers/encoder.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-22 21:06\nimport warnings\nfrom typing import Union, Dict, Any, Sequence, Tuple, Optional\n\nimport torch\nfrom torch import nn\nfrom hanlp.layers.dropout import WordDropout\nfrom hanlp.layers.scalar_mix import ScalarMixWithDropout, ScalarMixWithDropoutBuilder\nfrom hanlp.layers.transformers.resource import get_tokenizer_mirror\nfrom hanlp.layers.transformers.pt_imports import PreTrainedModel, PreTrainedTokenizer, AutoTokenizer, AutoModel_, \\\n    BertTokenizer, AutoTokenizer_\nfrom hanlp.layers.transformers.utils import transformer_encode\n\n\n# noinspection PyAbstractClass\nclass TransformerEncoder(nn.Module):\n    def __init__(self,\n                 transformer: Union[PreTrainedModel, str],\n                 transformer_tokenizer: PreTrainedTokenizer,\n                 average_subwords=False,\n                 scalar_mix: Union[ScalarMixWithDropoutBuilder, int] = None,\n                 word_dropout=None,\n                 max_sequence_length=None,\n                 ret_raw_hidden_states=False,\n                 transformer_args: Dict[str, Any] = None,\n                 trainable=Union[bool, Optional[Tuple[int, int]]],\n                 training=True) -> None:\n        \"\"\"A pre-trained transformer encoder.\n\n        Args:\n            transformer: A ``PreTrainedModel`` or an identifier of a ``PreTrainedModel``.\n            transformer_tokenizer: A ``PreTrainedTokenizer``.\n            average_subwords: ``True`` to average subword representations.\n            scalar_mix: Layer attention.\n            word_dropout: Dropout rate of randomly replacing a subword with MASK.\n            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding\n                window. If ``None``, then the ``max_position_embeddings`` of the transformer will be used.\n            ret_raw_hidden_states: ``True`` to return hidden states of each layer.\n            transformer_args: Extra arguments passed to the transformer.\n            trainable: ``False`` to use static embeddings.\n            training: ``False`` to skip loading weights from pre-trained transformers.\n        \"\"\"\n        super().__init__()\n        self.ret_raw_hidden_states = ret_raw_hidden_states\n        self.average_subwords = average_subwords\n        if word_dropout:\n            oov = transformer_tokenizer.mask_token_id\n            if isinstance(word_dropout, Sequence):\n                word_dropout, replacement = word_dropout\n                if replacement == 'unk':\n                    # Electra English has to use unk\n                    oov = transformer_tokenizer.unk_token_id\n                elif replacement == 'mask':\n                    # UDify uses [MASK]\n                    oov = transformer_tokenizer.mask_token_id\n                else:\n                    oov = replacement\n            pad = transformer_tokenizer.pad_token_id\n            cls = transformer_tokenizer.cls_token_id\n            sep = transformer_tokenizer.sep_token_id\n            excludes = [pad, cls, sep]\n            self.word_dropout = WordDropout(p=word_dropout, oov_token=oov, exclude_tokens=excludes)\n        else:\n            self.word_dropout = None\n        if isinstance(transformer, str):\n            output_hidden_states = scalar_mix is not None\n            if transformer_args is None:\n                transformer_args = dict()\n            transformer_args['output_hidden_states'] = output_hidden_states\n            transformer = AutoModel_.from_pretrained(transformer, training=training or not trainable,\n                                                     **transformer_args)\n            if max_sequence_length is None:\n                max_sequence_length = transformer.config.max_position_embeddings\n        self.max_sequence_length = max_sequence_length\n        if hasattr(transformer, 'encoder') and hasattr(transformer, 'decoder'):\n            # For seq2seq model, use its encoder\n            transformer = transformer.encoder\n        self.transformer = transformer\n        if not trainable:\n            transformer.requires_grad_(False)\n        elif isinstance(trainable, tuple):\n            layers = []\n            if hasattr(transformer, 'embeddings'):\n                layers.append(transformer.embeddings)\n            layers.extend(transformer.encoder.layer)\n            for i, layer in enumerate(layers):\n                if i < trainable[0] or i >= trainable[1]:\n                    layer.requires_grad_(False)\n\n        if isinstance(scalar_mix, ScalarMixWithDropoutBuilder):\n            self.scalar_mix: ScalarMixWithDropout = scalar_mix.build()\n        else:\n            self.scalar_mix = None\n\n    def forward(self, input_ids: torch.LongTensor, attention_mask=None, token_type_ids=None, token_span=None, **kwargs):\n        if self.word_dropout:\n            input_ids = self.word_dropout(input_ids)\n\n        x = transformer_encode(self.transformer,\n                               input_ids,\n                               attention_mask,\n                               token_type_ids,\n                               token_span,\n                               layer_range=self.scalar_mix.mixture_range if self.scalar_mix else 0,\n                               max_sequence_length=self.max_sequence_length,\n                               average_subwords=self.average_subwords,\n                               ret_raw_hidden_states=self.ret_raw_hidden_states)\n        if self.ret_raw_hidden_states:\n            x, raw_hidden_states = x\n        if self.scalar_mix:\n            x = self.scalar_mix(x)\n        if self.ret_raw_hidden_states:\n            # noinspection PyUnboundLocalVariable\n            return x, raw_hidden_states\n        return x\n\n    @staticmethod\n    def build_transformer(config, training=True) -> PreTrainedModel:\n        kwargs = {}\n        if config.scalar_mix and config.scalar_mix > 0:\n            kwargs['output_hidden_states'] = True\n        transformer = AutoModel_.from_pretrained(config.transformer, training=training, **kwargs)\n        return transformer\n\n    @staticmethod\n    def build_transformer_tokenizer(config_or_str, use_fast=True, do_basic_tokenize=True) -> PreTrainedTokenizer:\n        return AutoTokenizer_.from_pretrained(config_or_str, use_fast, do_basic_tokenize)\n", "hanlp/layers/transformers/tf_imports.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 21:57\nfrom transformers import BertTokenizer, BertConfig, PretrainedConfig, TFAutoModel, \\\n    AutoConfig, AutoTokenizer, PreTrainedTokenizer, TFPreTrainedModel, TFAlbertModel, TFAutoModelWithLMHead, \\\n    BertTokenizerFast, TFAlbertForMaskedLM, AlbertConfig, TFBertModel\n", "hanlp/layers/transformers/resource.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-20 12:43\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp_common.constant import HANLP_URL\n\ntokenizer_mirrors = {\n    'hfl/chinese-electra-180g-base-discriminator': HANLP_URL + 'transformers/electra_zh_base_20210706_125233.zip',\n    'hfl/chinese-electra-180g-small-discriminator': HANLP_URL + 'transformers/electra_zh_small_20210706_125427.zip',\n    'xlm-roberta-base': HANLP_URL + 'transformers/xlm-roberta-base_20210706_125502.zip',\n    'cl-tohoku/bert-base-japanese-char': HANLP_URL + 'transformers/bert-base-japanese-char_20210602_215445.zip',\n    'bart5-chinese-small': HANLP_URL + 'transformers/bart5-chinese-small_tok_20210723_180743.zip',\n    'ernie-gram': HANLP_URL + 'transformers/ernie-gram_20220207_103518.zip',\n    'xlm-roberta-base-no-space': HANLP_URL + 'transformers/xlm-roberta-base-no-space-tokenizer_20220610_204241.zip',\n    'mMiniLMv2L6-no-space': HANLP_URL + 'transformers/mMiniLMv2L6-no-space-tokenizer_20220616_094859.zip',\n    'mMiniLMv2L12-no-space': HANLP_URL + 'transformers/mMiniLMv2L12-no-space-tokenizer_20220616_095900.zip',\n}\n\nmodel_mirrors = {\n    'bart5-chinese-small': HANLP_URL + 'transformers/bart5-chinese-small_20210723_203923.zip',\n    'xlm-roberta-base-no-space': HANLP_URL + 'transformers/xlm-roberta-base-no-space_20220610_203944.zip',\n    'mMiniLMv2L6-no-space': HANLP_URL + 'transformers/mMiniLMv2L6-no-space_20220616_094949.zip',\n    'mMiniLMv2L12-no-space': HANLP_URL + 'transformers/mMiniLMv2L12-no-space_20220616_095924.zip',\n}\n\n\ndef get_tokenizer_mirror(transformer: str) -> str:\n    m = tokenizer_mirrors.get(transformer, None)\n    if m:\n        return get_resource(m)\n    return transformer\n\n\ndef get_model_mirror(transformer: str) -> str:\n    m = model_mirrors.get(transformer, None)\n    if m:\n        return get_resource(m)\n    return transformer\n", "hanlp/pretrained/classifiers.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-01-01 03:51\nfrom hanlp_common.constant import HANLP_URL\n\nCHNSENTICORP_BERT_BASE_ZH = HANLP_URL + 'classification/chnsenticorp_bert_base_20211228_163210.zip'\nSST2_ALBERT_BASE_EN = HANLP_URL + 'classification/sst2_albert_base_20211228_164917.zip'\n\nLID_176_FASTTEXT_BASE = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin'\n'''\n126MB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.\n'''\nLID_176_FASTTEXT_SMALL = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz'\n'''\n917kB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.\n'''\n\nALL = {}\n", "hanlp/pretrained/glove.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-27 20:42\n\n_GLOVE_6B_ROOT = 'http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'\n\nGLOVE_6B_50D = _GLOVE_6B_ROOT + '#' + 'glove.6B.50d.txt'\n'Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 50d trained on 6B tokens.'\nGLOVE_6B_100D = _GLOVE_6B_ROOT + '#' + 'glove.6B.100d.txt'\n'Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 100d trained on 6B tokens.'\nGLOVE_6B_200D = _GLOVE_6B_ROOT + '#' + 'glove.6B.200d.txt'\n'Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 200d trained on 6B tokens.'\nGLOVE_6B_300D = _GLOVE_6B_ROOT + '#' + 'glove.6B.300d.txt'\n'Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 300d trained on 6B tokens.'\n\nGLOVE_840B_300D = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n'Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 300d trained on 840B tokens.'\n\nALL = {}\n", "hanlp/pretrained/amr.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-01-25 11:47\nfrom hanlp_common.constant import HANLP_URL\n\nAMR3_SEQ2SEQ_BART_LARGE = HANLP_URL + 'amr/amr3_seq2seq_bart_large_83.30_20220125_114450.zip'\n'''A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract \nMeaning Representation 3.0 (:cite:`knight2014abstract`). Its performance is\n\n =================== ========= ========= ========= \n  Metric              P         R         F1       \n =================== ========= ========= ========= \n  Smatch              84.00     82.60     83.30    \n  Unlabeled           86.40     84.90     85.70    \n  No WSD              84.50     83.10     83.80    \n  Non_sense_frames    91.90     91.30     91.60    \n  Wikification        81.70     80.80     81.20    \n  Named Ent.          89.20     87.00     88.10    \n  Negations           71.70     70.90     71.30    \n  IgnoreVars          73.80     73.10     73.50    \n  Concepts            90.70     89.60     90.10    \n  Frames              88.50     87.90     88.20    \n  Reentrancies        70.40     71.80     71.10    \n  SRL                 79.00     79.60     79.30    \n =================== ========= ========= ========= \n    \nNote this parser does NOT perform wikification.\n'''\n\nAMR3_GRAPH_PRETRAIN_PARSER = HANLP_URL + 'amr/amr3_graph_pretrain_parser_20221207_153759.zip'\n'''A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract \nMeaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`). \nIts performance is ``84.3`` according to their official repository. Using ``amr-evaluation-enhanced``, the performance is\nslightly lower:\n\n =================== ========= ========= ========= \n  Metric              P         R         F1       \n =================== ========= ========= ========= \n  Smatch             84.4       83.6        84.0       \n  Unlabeled          86.7       85.8        86.2       \n  No WSD             84.9       84.1        84.5       \n  Non_sense_frames   91.8       91.6        91.7       \n  Wikification       83.6       81.7        82.6       \n  Named Ent.         89.3       87.4        88.4       \n  Negations          71.6       72.2        71.9       \n  IgnoreVars         74.6       74.2        74.4       \n  Concepts           90.7       90.0        90.3       \n  Frames             88.8       88.5        88.7       \n  Reentrancies       72.1       72.9        72.5       \n  SRL                80.1       80.7        80.4      \n =================== ========= ========= ========= \n    \nNote this parser does NOT perform wikification.\n'''\n\nMRP2020_AMR_ENG_ZHO_XLM_BASE = 'http://download.hanlp.com/amr/extra/amr-eng-zho-xlm-roberta-base_20220412_223756.zip'\n'''A wrapper for the Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020 English \nand Chinese AMR corpus. It was ranked the top in the MRP2020 competition, while this release is a base version. \nSee the original paper for the detailed performance. Note this model requires tokens and lemmas (for English) to be \nprovided as inputs. \n'''\n\nMRP2020_AMR_ZHO_MENGZI_BASE = 'http://download.hanlp.com/amr/extra/amr-zho-mengzi-base_20220415_101941.zip'\n'''A Chinese Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020  \nChinese AMR corpus using Mengzi BERT base (:cite:`zhang2021mengzi`). Its performance on dev set is \n``{amr-zho [tops F1: 85.43%][anchors F1: 93.41%][labels F1: 87.68%][properties F1: 82.02%][edges F1: 73.17%]\n[attributes F1: 0.00%][all F1: 84.11%]}``. Test set performance is unknown since the test set is not released to the \npublic. \n'''\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/sdp.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-31 23:54\nfrom hanlp_common.constant import HANLP_URL\n\nSEMEVAL16_NEWS_BIAFFINE_ZH = HANLP_URL + 'sdp/semeval16-news-biaffine_20191231_235407.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 news data.'\nSEMEVAL16_TEXT_BIAFFINE_ZH = HANLP_URL + 'sdp/semeval16-text-biaffine_20200101_002257.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 text data.'\n\nSEMEVAL16_ALL_ELECTRA_SMALL_ZH = HANLP_URL + 'sdp/semeval16_sdp_electra_small_20220719_171433.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 text and news data. Performance: ``UF: 83.03% LF: 72.58%``'\n\nSEMEVAL15_PAS_BIAFFINE_EN = HANLP_URL + 'sdp/semeval15_biaffine_pas_20200103_152405.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 PAS data.'\nSEMEVAL15_PSD_BIAFFINE_EN = HANLP_URL + 'sdp/semeval15_biaffine_psd_20200106_123009.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 PSD data.'\nSEMEVAL15_DM_BIAFFINE_EN = HANLP_URL + 'sdp/semeval15_biaffine_dm_20200106_122808.zip'\n'Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 DM data.'\n\nALL = {}\n", "hanlp/pretrained/amr2text.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2022-12-07 15:19\nfrom hanlp_common.constant import HANLP_URL\n\nAMR3_GRAPH_PRETRAIN_GENERATION = HANLP_URL + 'amr2text/amr3_graph_pretrain_generation_20221207_153535.zip'\n'''A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large AMR2Text generator trained on \nAbstract Meaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`). \nIts Sacre-BLEU is ``50.38`` according to their official repository.\n'''\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/word2vec.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 18:25\nfrom hanlp_common.constant import HANLP_URL\n\nCONVSEG_W2V_NEWS_TENSITE = HANLP_URL + 'embeddings/convseg_embeddings.zip'\nCONVSEG_W2V_NEWS_TENSITE_WORD_PKU = CONVSEG_W2V_NEWS_TENSITE + '#news_tensite.pku.words.w2v50'\nCONVSEG_W2V_NEWS_TENSITE_WORD_MSR = CONVSEG_W2V_NEWS_TENSITE + '#news_tensite.msr.words.w2v50'\nCONVSEG_W2V_NEWS_TENSITE_CHAR = CONVSEG_W2V_NEWS_TENSITE + '#news_tensite.w2v200'\n\nSEMEVAL16_EMBEDDINGS_CN = HANLP_URL + 'embeddings/semeval16_embeddings.zip'\nSEMEVAL16_EMBEDDINGS_300_NEWS_CN = SEMEVAL16_EMBEDDINGS_CN + '#news.fasttext.300.txt'\nSEMEVAL16_EMBEDDINGS_300_TEXT_CN = SEMEVAL16_EMBEDDINGS_CN + '#text.fasttext.300.txt'\n\nCTB5_FASTTEXT_300_CN = HANLP_URL + 'embeddings/ctb.fasttext.300.txt.zip'\n\nTENCENT_AILAB_EMBEDDING_SMALL_200 = 'https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d200-v0.2.0-s.tar.gz#tencent-ailab-embedding-zh-d200-v0.2.0-s.txt'\n'Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with small vocabulary size and 200 dimension provided by Tencent AI lab.'\nTENCENT_AILAB_EMBEDDING_LARGE_200 = 'https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d200-v0.2.0.tar.gz#tencent-ailab-embedding-zh-d200-v0.2.0.txt'\n'Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with large vocabulary size and 200 dimension provided by Tencent AI lab.'\nTENCENT_AILAB_EMBEDDING_SMALL_100 = 'https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d100-v0.2.0-s.tar.gz#tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n'Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with small vocabulary size and 100 dimension provided by Tencent AI lab.'\nTENCENT_AILAB_EMBEDDING_LARGE_100 = 'https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d100-v0.2.0.tar.gz#tencent-ailab-embedding-zh-d100-v0.2.0.txt'\n'Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with large vocabulary size and 100 dimension provided by Tencent AI lab.'\n\nMERGE_SGNS_BIGRAM_CHAR_300_ZH = 'http://download.hanlp.com/embeddings/extra/merge_sgns_bigram_char300_20220130_214613.txt.zip'\n'Chinese word embeddings trained with context features (word, ngram, character, and more) using Skip-Gram with Negative Sampling (SGNS) (:cite:`li-etal-2018-analogical`).'\n\nRADICAL_CHAR_EMBEDDING_100 = HANLP_URL + 'embeddings/radical_char_vec_20191229_013849.zip#character.vec.txt'\n'Chinese character embedding enhanced with rich radical information (:cite:`he2018dual`).'\n\n_SUBWORD_ENCODING_CWS = 'http://download.hanlp.com/embeddings/extra/subword_encoding_cws_20200524_190636.zip'\nSUBWORD_ENCODING_CWS_ZH_WIKI_BPE_50 = _SUBWORD_ENCODING_CWS + '#zh.wiki.bpe.vs200000.d50.w2v.txt'\nSUBWORD_ENCODING_CWS_GIGAWORD_UNI = _SUBWORD_ENCODING_CWS + '#gigaword_chn.all.a2b.uni.ite50.vec'\nSUBWORD_ENCODING_CWS_GIGAWORD_BI = _SUBWORD_ENCODING_CWS + '#gigaword_chn.all.a2b.bi.ite50.vec'\nSUBWORD_ENCODING_CWS_CTB_GAZETTEER_50 = _SUBWORD_ENCODING_CWS + '#ctb.50d.vec'\n\nALL = {}\n", "hanlp/pretrained/pos.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 01:57\nfrom hanlp_common.constant import HANLP_URL\n\nCTB5_POS_RNN = HANLP_URL + 'pos/ctb5_pos_rnn_20200113_235925.zip'\n'An old school BiLSTM tagging model trained on CTB5.'\nCTB5_POS_RNN_FASTTEXT_ZH = HANLP_URL + 'pos/ctb5_pos_rnn_fasttext_20191230_202639.zip'\n'An old school BiLSTM tagging model with FastText (:cite:`bojanowski2017enriching`) embeddings trained on CTB5.'\nCTB9_POS_ALBERT_BASE = HANLP_URL + 'pos/ctb9_albert_base_20211228_163935.zip'\n'ALBERT model (:cite:`Lan2020ALBERT:`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). This is a TF component.'\nCTB9_POS_ELECTRA_SMALL_TF = HANLP_URL + 'pos/pos_ctb_electra_small_20211227_121341.zip'\n'Electra small model (:cite:`clark2020electra`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.75`. This is a TF component.'\nCTB9_POS_ELECTRA_SMALL = HANLP_URL + 'pos/pos_ctb_electra_small_20220215_111944.zip'\n'Electra small model (:cite:`clark2020electra`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.26`.'\nCTB9_POS_RADICAL_ELECTRA_SMALL = HANLP_URL + 'pos/pos_ctb_radical_electra_small_20220215_111932.zip'\n'Electra small model (:cite:`clark2020electra`) with radical embeddings (:cite:`he2018dual`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.14`.'\nC863_POS_ELECTRA_SMALL = HANLP_URL + 'pos/pos_863_electra_small_20220217_101958.zip'\n'Electra small model (:cite:`clark2020electra`) trained on Chinese 863 corpus. Accuracy = `95.19`.'\nPKU_POS_ELECTRA_SMALL = HANLP_URL + 'pos/pos_pku_electra_small_20220217_142436.zip'\n'Electra small model (:cite:`clark2020electra`) trained on Chinese PKU corpus. Accuracy = `97.55`.'\nPTB_POS_RNN_FASTTEXT_EN = HANLP_URL + 'pos/ptb_pos_rnn_fasttext_20220418_101708.zip'\n'An old school BiLSTM tagging model with FastText (:cite:`bojanowski2017enriching`) embeddings trained on PTB.'\n\nALL = {}\n", "hanlp/pretrained/sts.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-24 12:51\nfrom hanlp_common.constant import HANLP_URL\n\nSTS_ELECTRA_BASE_ZH = HANLP_URL + 'sts/sts_electra_base_zh_20210530_200109.zip'\n'A naive regression model trained on concatenated STS corpora.'\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/mtl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-22 13:16\nfrom hanlp_common.constant import HANLP_URL\n\nOPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH = HANLP_URL + 'mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip'\n\"Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep, sdp and con model trained on open-source Chinese corpus.\"\nOPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH = HANLP_URL + 'mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip'\n\"Electra (:cite:`clark2020electra`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on open-source Chinese corpus.\"\nCLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH = HANLP_URL + 'mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip'\n\"Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep (SD Standard), sdp and con model trained on close-source Chinese corpus.\"\nCLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH = HANLP_URL + 'mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip'\n'''\nElectra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep (UD Standard), sdp and con model trained on close-source Chinese corpus.\nPerformance: ``{con UCM: 39.33% LCM: 35.69% UP: 90.24% UR: 90.28% UF: 90.26% LP: 87.55% LR: 87.59% LF: 87.57%}{dep UAS: 86.80% LAS: 82.82%}{ner/msra P: 95.45% R: 96.65% F1: 96.05%}{ner/ontonotes P: 75.98% R: 79.09% F1: 77.50%}{ner/pku P: 95.77% R: 96.75% F1: 96.26%}{pos/863 Accuracy:94.83%}{pos/ctb Accuracy:96.57%}{pos/pku Accuracy:97.54%}{sdp UF: 85.55% LF: 73.67%}{srl P: 75.71% R: 74.25% F1: 74.97%}{tok/coarse P: 97.77% R: 97.70% F1: 97.74%}{tok/fine P: 97.44% R: 97.32% F1: 97.38%}``.\n'''\nCLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH = HANLP_URL + 'mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip'\n\"Electra (:cite:`clark2020electra`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on close-source Chinese corpus.\"\nCLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH = HANLP_URL + 'mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip'\n\"ERNIE (:cite:`xiao-etal-2021-ernie`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on close-source Chinese corpus.\"\n\nUD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6 = HANLP_URL + 'mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip'\n'''\nmMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.\nThe following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurin\u00e3, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajaj\u00e1ra, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\\'iche\\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makur\u00e1p, Maltese, Manx, Marathi, Mby\u00e1Guaran\u00ed, Modern Greek (1453-), Moksha, Munduruk\u00fa, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinamb\u00e1, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urub\u00fa-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.\nPerformance: ``{con UCM: 15.48% LCM: 11.45% UP: 68.92% UR: 66.88% UF: 67.88% LP: 61.19% LR: 59.38% LF: 60.27%}{ner P: 76.06% R: 77.83% F1: 76.93%}{sdp/dm UF: 91.84% LF: 91.00%}{sdp/pas UF: 95.46% LF: 93.90%}{sdp/psd UF: 91.94% LF: 81.26%}{srl [predicate P: 91.71% R: 74.51% F1: 82.22%][e2e P: 77.48% R: 55.28% F1: 64.52%]}{tok P: 93.17% R: 93.53% F1: 93.35%}{ud [lemmas Accuracy:81.74%][upos Accuracy:85.94%][deps UAS: 80.60% LAS: 71.21%][feats Accuracy:77.17%]}``.\n'''\nUD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12 = HANLP_URL + 'mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip'\n'''\nmMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.\nThe following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurin\u00e3, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajaj\u00e1ra, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\\'iche\\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makur\u00e1p, Maltese, Manx, Marathi, Mby\u00e1Guaran\u00ed, Modern Greek (1453-), Moksha, Munduruk\u00fa, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinamb\u00e1, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urub\u00fa-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.\nPerformance: ``{con UCM: 17.32% LCM: 13.28% UP: 70.53% UR: 68.73% UF: 69.62% LP: 63.03% LR: 61.42% LF: 62.22%}{ner P: 76.91% R: 78.72% F1: 77.80%}{sdp/dm UF: 92.78% LF: 92.02%}{sdp/pas UF: 96.43% LF: 95.02%}{sdp/psd UF: 92.75% LF: 81.86%}{srl [predicate P: 91.82% R: 77.57% F1: 84.10%][e2e P: 78.33% R: 59.14% F1: 67.40%]}{tok P: 93.69% R: 94.34% F1: 94.02%}{ud [lemmas Accuracy:82.48%][upos Accuracy:87.09%][deps UAS: 82.41% LAS: 73.69%][feats Accuracy:78.58%]}``.\n'''\nUD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE = HANLP_URL + 'mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip'\n'''\nXLM-R (:cite:`conneau-etal-2020-unsupervised`) base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.\nThe following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurin\u00e3, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajaj\u00e1ra, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\\'iche\\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makur\u00e1p, Maltese, Manx, Marathi, Mby\u00e1Guaran\u00ed, Modern Greek (1453-), Moksha, Munduruk\u00fa, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinamb\u00e1, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urub\u00fa-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.\nPerformance: ``{con UCM: 20.31% LCM: 16.82% UP: 77.50% UR: 76.63% UF: 77.06% LP: 71.25% LR: 70.46% LF: 70.85%}{ner P: 79.93% R: 80.76% F1: 80.34%}{sdp/dm UF: 93.71% LF: 93.00%}{sdp/pas UF: 97.63% LF: 96.37%}{sdp/psd UF: 93.08% LF: 80.95%}{srl [predicate P: 90.95% R: 84.25% F1: 87.47%][e2e P: 78.89% R: 67.32% F1: 72.65%]}{tok P: 98.50% R: 98.70% F1: 98.60%}{ud [lemmas Accuracy:85.95%][upos Accuracy:89.95%][deps UAS: 85.78% LAS: 78.51%][feats Accuracy:82.18%]}``.\n'''\n\nNPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA = HANLP_URL + 'mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip'\n'BERT (:cite:`devlin-etal-2019-bert`) base char encoder trained on NPCMJ/UD/Kyoto corpora with decoders including tok, pos, ner, dep, con, srl.'\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/rnnlm.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-19 03:47\nfrom hanlp_common.constant import HANLP_URL\n\nFLAIR_LM_FW_WMT11_EN_TF = HANLP_URL + 'lm/flair_lm_wmt11_en_20200211_091932.zip#flair_lm_fw_wmt11_en'\n'The forward LSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`).'\nFLAIR_LM_BW_WMT11_EN_TF = HANLP_URL + 'lm/flair_lm_wmt11_en_20200211_091932.zip#flair_lm_bw_wmt11_en'\n'The backward LSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`).'\nFLAIR_LM_WMT11_EN = HANLP_URL + 'lm/flair_lm_wmt11_en_20200601_205350.zip'\n'The BiLSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`).'\n\nALL = {}\n", "hanlp/pretrained/constituency.py": "# -*- coding:utf-8 -*-\n# Author=hankcs\n# Date=2022-01-18 10:34\nfrom hanlp_common.constant import HANLP_URL\n\nCTB9_CON_ELECTRA_SMALL = HANLP_URL + 'constituency/ctb9_con_electra_small_20220215_230116.zip'\n'Electra (:cite:`clark2020electra`) small tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with major categories. ' \\\n'Its performance is UCM=39.06% LCM=34.99% UP=90.05% UR=90.01% UF=90.03% LP=87.02% LR=86.98% LF=87.00%.'\n\nCTB9_CON_FULL_TAG_ELECTRA_SMALL = HANLP_URL + 'constituency/ctb9_full_tag_con_electra_small_20220118_103119.zip'\n'Electra (:cite:`clark2020electra`) small tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with full subcategories. ' \\\n'Its performance is UCM=38.29% LCM=28.95% UP=90.16% UR=90.13% UF=90.15% LP=83.46% LR=83.43% LF=83.45%.'\n\nCTB9_CON_FULL_TAG_ERNIE_GRAM = 'http://download.hanlp.com/constituency/extra/ctb9_full_tag_con_ernie_20220331_121430.zip'\n'ERNIE-GRAM (:cite:`xiao-etal-2021-ernie`) base tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with full subcategories. ' \\\n'Its performance is UCM=42.04% LCM=31.72% UP=91.33% UR=91.53% UF=91.43% LP=85.31% LR=85.49% LF=85.40%.'\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/eos.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-12-22 13:22\nfrom hanlp_common.constant import HANLP_URL\n\nUD_CTB_EOS_MUL = HANLP_URL + 'eos/eos_ud_ctb_mul_20201222_133543.zip'\n'EOS model (:cite:`Schweter:Ahmed:2019`) trained on concatenated UD2.3 and CTB9.'\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/dep.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 02:55\nfrom hanlp_common.constant import HANLP_URL\n\nCTB5_BIAFFINE_DEP_ZH = HANLP_URL + 'dep/biaffine_ctb5_20191229_025833.zip'\n'Biaffine LSTM model (:cite:`dozat:17a`) trained on CTB5.'\nCTB7_BIAFFINE_DEP_ZH = HANLP_URL + 'dep/biaffine_ctb7_20200109_022431.zip'\n'Biaffine LSTM model (:cite:`dozat:17a`) trained on CTB7.'\nCTB9_DEP_ELECTRA_SMALL = HANLP_URL + 'dep/ctb9_dep_electra_small_20220216_100306.zip'\n'Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on CTB9-SD330. ' \\\n'Performance is UAS=87.68% LAS=83.54%.'\nPMT1_DEP_ELECTRA_SMALL = HANLP_URL + 'dep/pmt_dep_electra_small_20220218_134518.zip'\n'Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on PKU ' \\\n'Multi-view Chinese Treebank (PMT) 1.0 (:cite:`qiu-etal-2014-multi`). Performance is UAS=91.21% LAS=88.65%.'\nCTB9_UDC_ELECTRA_SMALL = HANLP_URL + 'dep/udc_dep_electra_small_20220218_095452.zip'\n'Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on CTB9-UD420. ' \\\n'Performance is UAS=85.92% LAS=81.13% .'\n\nPTB_BIAFFINE_DEP_EN = HANLP_URL + 'dep/ptb_dep_biaffine_20200101_174624.zip'\n'Biaffine LSTM model (:cite:`dozat:17a`) trained on PTB.'\n\nALL = {}\n", "hanlp/pretrained/tok.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 21:12\nfrom hanlp_common.constant import HANLP_URL\n\nSIGHAN2005_PKU_CONVSEG = HANLP_URL + 'tok/sighan2005-pku-convseg_20200110_153722.zip'\n'Conv model (:cite:`wang-xu-2017-convolutional`) trained on sighan2005 pku dataset.'\nSIGHAN2005_MSR_CONVSEG = HANLP_URL + 'tok/convseg-msr-nocrf-noembed_20200110_153524.zip'\n'Conv model (:cite:`wang-xu-2017-convolutional`) trained on sighan2005 msr dataset.'\nCTB6_CONVSEG = HANLP_URL + 'tok/ctb6_convseg_nowe_nocrf_20200110_004046.zip'\n'Conv model (:cite:`wang-xu-2017-convolutional`) trained on CTB6 dataset.'\nPKU_NAME_MERGED_SIX_MONTHS_CONVSEG = HANLP_URL + 'tok/pku98_6m_conv_ngram_20200110_134736.zip'\n'Conv model (:cite:`wang-xu-2017-convolutional`) trained on pku98 six months dataset with familiy name and given name merged into one unit.'\nLARGE_ALBERT_BASE = HANLP_URL + 'tok/large_corpus_cws_albert_base_20211228_160926.zip'\n'ALBERT model (:cite:`Lan2020ALBERT:`) trained on the largest CWS dataset in the world.'\nSIGHAN2005_PKU_BERT_BASE_ZH = HANLP_URL + 'tok/sighan2005_pku_bert_base_zh_20201231_141130.zip'\n'BERT model (:cite:`devlin-etal-2019-bert`) trained on sighan2005 pku dataset.'\nCOARSE_ELECTRA_SMALL_ZH = HANLP_URL + 'tok/coarse_electra_small_20220616_012050.zip'\n'Electra (:cite:`clark2020electra`) small model trained on coarse-grained CWS corpora. Its performance is ``P: 98.34% R: 98.38% F1: 98.36%`` which is ' \\\n'much higher than that of MTL model '\nFINE_ELECTRA_SMALL_ZH = HANLP_URL + 'tok/fine_electra_small_20220615_231803.zip'\n'Electra (:cite:`clark2020electra`) small model trained on fine-grained CWS corpora. Its performance is ``P: 98.14% R: 98.07% F1: 98.11%`` which is ' \\\n'much higher than that of MTL model '\nCTB9_TOK_ELECTRA_SMALL = HANLP_URL + 'tok/ctb9_electra_small_20220215_205427.zip'\n'Electra (:cite:`clark2020electra`) small model trained on CTB9. Its performance is P=97.15% R=97.36% F1=97.26% which is ' \\\n'much higher than that of MTL model '\nCTB9_TOK_ELECTRA_BASE = 'http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_20220426_111949.zip'\n'Electra (:cite:`clark2020electra`) base model trained on CTB9. Its performance is ``P: 97.62% R: 97.67% F1: 97.65%`` ' \\\n'which is much higher than that of MTL model '\nCTB9_TOK_ELECTRA_BASE_CRF = 'http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_crf_20220426_161255.zip'\n'Electra (:cite:`clark2020electra`) base model trained on CTB9. Its performance is ``P: 97.68% R: 97.71% F1: 97.69%`` ' \\\n'which is much higher than that of MTL model '\nMSR_TOK_ELECTRA_BASE_CRF = 'http://download.hanlp.com/tok/extra/msra_crf_electra_base_20220507_113936.zip'\n'Electra (:cite:`clark2020electra`) base model trained on MSR CWS dataset. Its performance is ``P: 98.71% R: 98.64% F1: 98.68%`` ' \\\n'which is much higher than that of MTL model '\n\nUD_TOK_MMINILMV2L6 = HANLP_URL + 'tok/ud_tok_mMiniLMv2L6_no_space_mul_20220619_091824.zip'\n'''\nmMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 based tokenizer trained on UD 2.10.\nThe following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurin\u00e3, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajaj\u00e1ra, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\\'iche\\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makur\u00e1p, Maltese, Manx, Marathi, Mby\u00e1Guaran\u00ed, Modern Greek (1453-), Moksha, Munduruk\u00fa, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinamb\u00e1, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urub\u00fa-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.\nPerformance: ``P: 94.99% R: 94.74% F1: 94.86%``.\n'''\nUD_TOK_MMINILMV2L12 = HANLP_URL + 'tok/ud_tok_mMiniLMv2L12_no_space_mul_20220619_091159.zip'\n'''\nmMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L12xH384 based tokenizer trained on UD 2.10.\nThe following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurin\u00e3, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajaj\u00e1ra, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\\'iche\\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makur\u00e1p, Maltese, Manx, Marathi, Mby\u00e1Guaran\u00ed, Modern Greek (1453-), Moksha, Munduruk\u00fa, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinamb\u00e1, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urub\u00fa-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.\nPerformance: ``P: 95.41% R: 95.25% F1: 95.33%``.\n'''\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-28 19:10\nfrom hanlp.pretrained import tok\nfrom hanlp.pretrained import dep\nfrom hanlp.pretrained import sdp\nfrom hanlp.pretrained import glove\nfrom hanlp.pretrained import pos\nfrom hanlp.pretrained import rnnlm\nfrom hanlp.pretrained import word2vec\nfrom hanlp.pretrained import ner\nfrom hanlp.pretrained import classifiers\nfrom hanlp.pretrained import fasttext\nfrom hanlp.pretrained import mtl\nfrom hanlp.pretrained import eos\nfrom hanlp.pretrained import sts\nfrom hanlp.pretrained import constituency\nfrom hanlp.pretrained import amr\nfrom hanlp.pretrained import amr2text\nfrom hanlp.pretrained import srl\n\n# Will be filled up during runtime\nALL = {}\n", "hanlp/pretrained/ner.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-30 20:07\nfrom hanlp_common.constant import HANLP_URL\n\nMSRA_NER_BERT_BASE_ZH = HANLP_URL + 'ner/ner_bert_base_msra_20211227_114712.zip'\n'BERT model (:cite:`devlin-etal-2019-bert`) trained on MSRA with 3 entity types.'\nMSRA_NER_ALBERT_BASE_ZH = HANLP_URL + 'ner/msra_ner_albert_base_20211228_173323.zip'\n'ALBERT model (:cite:`Lan2020ALBERT:`) trained on MSRA with 3 entity types.'\nMSRA_NER_ELECTRA_SMALL_ZH = HANLP_URL + 'ner/msra_ner_electra_small_20220215_205503.zip'\n'Electra small model (:cite:`clark2020electra`) trained on MSRA with 26 entity types. F1 = `95.16`'\nCONLL03_NER_BERT_BASE_CASED_EN = HANLP_URL + 'ner/ner_conll03_bert_base_cased_en_20211227_121443.zip'\n'BERT model (:cite:`devlin-etal-2019-bert`) trained on CoNLL03.'\n\nALL = {}\n", "hanlp/pretrained/fasttext.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-30 18:57\nFASTTEXT_DEBUG_EMBEDDING_EN = 'https://elit-models.s3-us-west-2.amazonaws.com/fasttext.debug.bin.zip'\nFASTTEXT_CC_300_EN = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n'FastText (:cite:`bojanowski2017enriching`) embeddings trained on Common Crawl.'\nFASTTEXT_WIKI_NYT_AMAZON_FRIENDS_200_EN \\\n    = 'https://elit-models.s3-us-west-2.amazonaws.com/fasttext-200-wikipedia-nytimes-amazon-friends-20191107.bin'\n'FastText (:cite:`bojanowski2017enriching`) embeddings trained on wikipedia, nytimes and friends.'\n\nFASTTEXT_WIKI_300_ZH = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin'\n'FastText (:cite:`bojanowski2017enriching`) embeddings trained on Chinese Wikipedia.'\nFASTTEXT_WIKI_300_ZH_CLASSICAL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh_classical.zip#wiki.zh_classical.bin'\n'FastText (:cite:`bojanowski2017enriching`) embeddings trained on traditional Chinese wikipedia.'\n\nALL = {}\n", "hanlp/pretrained/srl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-08-07 19:07\nfrom hanlp_common.constant import HANLP_URL\n\nCPB3_SRL_ELECTRA_SMALL = HANLP_URL + 'srl/cpb3_electra_small_crf_has_transform_20220218_135910.zip'\n'Electra small model (:cite:`clark2020electra`) trained on CPB3. P=75.87% R=76.24% F1=76.05%.'\n\nALL = {}\n", "hanlp/optimizers/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-11 18:44", "hanlp/optimizers/adamw/optimization.py": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\n\nclass WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"Applys a warmup schedule on a given learning rate decay schedule.\"\"\"\n\n    def __init__(\n            self,\n            initial_learning_rate,\n            decay_schedule_fn,\n            warmup_steps,\n            power=1.0,\n            name=None):\n        super(WarmUp, self).__init__()\n        self.initial_learning_rate = initial_learning_rate\n        self.warmup_steps = warmup_steps\n        self.power = power\n        self.decay_schedule_fn = decay_schedule_fn\n        self.name = name\n\n    def __call__(self, step):\n        with tf.name_scope(self.name or 'WarmUp') as name:\n            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n            global_step_float = tf.cast(step, tf.float32)\n            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n            warmup_percent_done = global_step_float / warmup_steps_float\n            warmup_learning_rate = (\n                    self.initial_learning_rate *\n                    tf.math.pow(warmup_percent_done, self.power))\n            return tf.cond(global_step_float < warmup_steps_float,\n                           lambda: warmup_learning_rate,\n                           lambda: self.decay_schedule_fn(step),\n                           name=name)\n\n    def get_config(self):\n        return {\n            'initial_learning_rate': self.initial_learning_rate,\n            'decay_schedule_fn': self.decay_schedule_fn,\n            'warmup_steps': self.warmup_steps,\n            'power': self.power,\n            'name': self.name\n        }\n\n\ndef create_optimizer(init_lr, num_train_steps, num_warmup_steps):\n    \"\"\"Creates an optimizer with learning rate schedule.\n\n    Args:\n      init_lr: \n      num_train_steps: \n      num_warmup_steps: \n\n    Returns:\n\n    \"\"\"\n    # Implements linear decay of the learning rate.\n    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=init_lr,\n        decay_steps=num_train_steps,\n        end_learning_rate=0.0)\n    if num_warmup_steps:\n        learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n                                  decay_schedule_fn=learning_rate_fn,\n                                  warmup_steps=num_warmup_steps)\n    optimizer = AdamWeightDecay(\n        learning_rate=learning_rate_fn,\n        weight_decay_rate=0.01,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-6,\n        exclude_from_weight_decay=['layer_norm', 'bias'])\n    return optimizer\n\n\nclass AdamWeightDecay(tf.keras.optimizers.Adam):\n    \"\"\"Adam enables L2 weight decay and clip_by_global_norm on gradients.\n    \n      Just adding the square of the weights to the loss function is *not* the\n      correct way of using L2 regularization/weight decay with Adam, since that will\n      interact with the m and v parameters in strange ways.\n    \n      Instead we want to decay the weights in a manner that doesn't interact with\n      the m/v parameters. This is equivalent to adding the square of the weights to\n      the loss with plain (non-momentum) SGD.\n\n    Args:\n\n    Returns:\n\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 amsgrad=False,\n                 weight_decay_rate=0.0,\n                 include_in_weight_decay=None,\n                 exclude_from_weight_decay=None,\n                 name='AdamWeightDecay',\n                 **kwargs):\n        super(AdamWeightDecay, self).__init__(\n            learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)\n        self.weight_decay_rate = weight_decay_rate\n        self._include_in_weight_decay = include_in_weight_decay\n        self._exclude_from_weight_decay = exclude_from_weight_decay\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates an optimizer from its config with WarmUp custom object.\n\n        Args:\n          config:\n\n        Returns:\n\n        \"\"\"\n        custom_objects = {'WarmUp': WarmUp}\n        return super(AdamWeightDecay, cls).from_config(\n            config, custom_objects=custom_objects)\n\n    def _prepare_local(self, var_device, var_dtype, apply_state):\n        super(AdamWeightDecay, self)._prepare_local(var_device, var_dtype,\n                                                    apply_state)\n        apply_state['weight_decay_rate'] = tf.constant(\n            self.weight_decay_rate, name='adam_weight_decay_rate')\n\n    def _decay_weights_op(self, var, learning_rate, apply_state):\n        do_decay = self._do_use_weight_decay(var.name)\n        if do_decay:\n            return var.assign_sub(\n                learning_rate * var *\n                apply_state['weight_decay_rate'],\n                use_locking=self._use_locking)\n        return tf.no_op()\n\n    def apply_gradients(self, grads_and_vars, name=None):\n        grads, tvars = list(zip(*grads_and_vars))\n        (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars))\n\n    def _get_lr(self, var_device, var_dtype, apply_state):\n        \"\"\"Retrieves the learning rate with the given state.\n\n        Args:\n          var_device:\n          var_dtype:\n          apply_state:\n\n        Returns:\n\n        \"\"\"\n        if apply_state is None:\n            return self._decayed_lr_t[var_dtype], {}\n\n        apply_state = apply_state or {}\n        coefficients = apply_state.get((var_device, var_dtype))\n        if coefficients is None:\n            coefficients = self._fallback_apply_state(var_device, var_dtype)\n            apply_state[(var_device, var_dtype)] = coefficients\n\n        return coefficients['lr_t'], dict(apply_state=apply_state)\n\n    def _resource_apply_dense(self, grad, var, apply_state=None):\n        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n        decay = self._decay_weights_op(var, lr_t, apply_state)\n        with tf.control_dependencies([decay]):\n            return super(AdamWeightDecay, self)._resource_apply_dense(\n                grad, var, **kwargs)\n\n    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n        lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n        decay = self._decay_weights_op(var, lr_t, apply_state)\n        with tf.control_dependencies([decay]):\n            return super(AdamWeightDecay, self)._resource_apply_sparse(\n                grad, var, indices, **kwargs)\n\n    def get_config(self):\n        config = super(AdamWeightDecay, self).get_config()\n        config.update({\n            'weight_decay_rate': self.weight_decay_rate,\n        })\n        return config\n\n    def _do_use_weight_decay(self, param_name):\n        \"\"\"Whether to use L2 weight decay for `param_name`.\n\n        Args:\n          param_name:\n\n        Returns:\n\n        \"\"\"\n        if self.weight_decay_rate == 0:\n            return False\n\n        if self._include_in_weight_decay:\n            for r in self._include_in_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return True\n\n        if self._exclude_from_weight_decay:\n            for r in self._exclude_from_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def apply_gradients(self, grads_and_vars, name=None, **kwargs):\n        grads, tvars = list(zip(*grads_and_vars))\n        return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)\n", "hanlp/optimizers/adamw/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-11 18:44\nimport tensorflow as tf\nfrom hanlp.optimizers.adamw.optimization import WarmUp, AdamWeightDecay\n\n\n# from hanlp.optimization.adamw.optimizers_v2 import AdamW\n# from hanlp.optimization.adamw.utils import get_weight_decays\n\n\n# def create_optimizer(model, init_lr, num_train_steps, num_warmup_steps):\n#     \"\"\"Creates an optimizer with learning rate schedule.\"\"\"\n#     wd_dict = get_weight_decays(model)\n#\n#     # Implements linear decay of the learning rate.\n#     learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n#         initial_learning_rate=init_lr,\n#         decay_steps=num_train_steps,\n#         end_learning_rate=0.0)\n#     if num_warmup_steps:\n#         learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n#                                   decay_schedule_fn=learning_rate_fn,\n#                                   warmup_steps=num_warmup_steps)\n#     optimizer = AdamW(\n#         learning_rate=learning_rate_fn,\n#         weight_decay_rate=0.01,\n#         beta_1=0.9,\n#         beta_2=0.999,\n#         epsilon=1e-6,\n#         exclude_from_weight_decay=['layer_norm', 'bias'])\n#     return optimizer\n\n\ndef create_optimizer(init_lr, num_train_steps, num_warmup_steps, weight_decay_rate=0.01, epsilon=1e-6, clipnorm=None):\n    \"\"\"Creates an optimizer with learning rate schedule.\n\n    Args:\n      init_lr: \n      num_train_steps: \n      num_warmup_steps: \n      weight_decay_rate:  (Default value = 0.01)\n      epsilon:  (Default value = 1e-6)\n      clipnorm:  (Default value = None)\n\n    Returns:\n\n    \"\"\"\n    # Implements linear decay of the learning rate.\n    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=init_lr,\n        decay_steps=num_train_steps,\n        end_learning_rate=0.0)\n    if num_warmup_steps:\n        learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n                                  decay_schedule_fn=learning_rate_fn,\n                                  warmup_steps=num_warmup_steps)\n    additional_args = {}\n    if clipnorm:\n        additional_args['clipnorm'] = clipnorm\n    optimizer = AdamWeightDecay(\n        learning_rate=learning_rate_fn,\n        weight_decay_rate=weight_decay_rate,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=epsilon,\n        exclude_from_weight_decay=['LayerNorm', 'bias'],\n        **additional_args\n    )\n    # {'LayerNorm/gamma:0', 'LayerNorm/beta:0'}\n    return optimizer\n", "hanlp/callbacks/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-05 02:10", "hanlp/callbacks/fine_csv_logger.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-05 02:12\nimport copy\nfrom io import TextIOWrapper\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass StreamTableFormatter(object):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.col_widths = None\n\n    def format_row(self, cells) -> List[str]:\n        if not isinstance(cells, list):\n            cells = list(cells)\n        if not self.col_widths:\n            self.col_widths = [0] * len([_ for _ in cells])\n        for i, c in enumerate(cells):\n            self.col_widths[i] = max(self.col_widths[i], len(self.format_cell(c, self.col_widths[i])))\n        return list(self.format_cell(cell, width) for cell, width in zip(cells, self.col_widths))\n\n    def format_cell(self, cell: str, min_width) -> str:\n        if isinstance(cell, (np.float32, np.float)):\n            return '{:>{}.4f}'.format(cell, min_width)\n        return '{:>{}}'.format(cell, min_width)\n\n\nclass FineCSVLogger(tf.keras.callbacks.History):\n\n    def __init__(self, filename, separator=',', append=False):\n        super().__init__()\n        self.append = append\n        self.separator = separator\n        self.filename = filename\n        self.out: TextIOWrapper = None\n        self.keys = []\n        self.formatter = StreamTableFormatter()\n\n    def on_train_begin(self, logs=None):\n        super().on_train_begin(logs)\n        self.out = open(self.filename, 'a' if self.append else 'w')\n\n    def on_train_end(self, logs=None):\n        self.out.close()\n\n    def on_epoch_end(self, epoch, logs=None):\n        super().on_epoch_end(epoch, logs)\n        if not self.keys:\n            self.keys = sorted(logs.keys())\n\n            if getattr(self.model, 'stop_training', None):\n                # We set NA so that csv parsers do not fail for this last epoch.\n                logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n\n            # feed them twice to decide the actual width\n            values = self.formatter.format_row([epoch + 1] + [logs.get(k, 'NA') for k in self.keys])\n            headers = self.formatter.format_row(['epoch'] + self.keys)\n            # print headers and bars\n            self.out.write(self.separator.join(headers) + '\\n')\n            # bars for markdown style\n            bars = [''.join(['-'] * width) for width in self.formatter.col_widths]\n            self.out.write(self.separator.join(bars) + '\\n')\n\n        values = self.formatter.format_row([epoch + 1] + [logs.get(k, 'NA') for k in self.keys])\n        self.out.write(self.separator.join(values) + '\\n')\n        self.out.flush()\n", "hanlp/transform/table_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-11-10 21:00\nfrom abc import ABC\nfrom typing import Tuple, Union\nimport numpy as np\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp_common.constant import PAD\nfrom hanlp.common.vocab_tf import create_label_vocab\nfrom hanlp.utils.io_util import read_cells\nfrom hanlp.utils.log_util import logger\n\n\nclass TableTransform(Transform, ABC):\n    def __init__(self, config: SerializableDict = None, map_x=False, map_y=True, x_columns=None,\n                 y_column=-1, multi_label=False,\n                 skip_header=True, delimiter='auto', **kwargs) -> None:\n        super().__init__(config, map_x, map_y, x_columns=x_columns, y_column=y_column, multi_label=multi_label,\n                         skip_header=skip_header,\n                         delimiter=delimiter, **kwargs)\n        self.label_vocab = create_label_vocab()\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        x_columns = self.config.x_columns\n        y_column = self.config.y_column\n        num_features = self.config.get('num_features', None)\n        for cells in read_cells(filepath, skip_header=self.config.skip_header, delimiter=self.config.delimiter):\n            #multi-label: Dataset in .tsv format: x_columns: at most 2 columns being a sentence pair while in most\n            # cases just one column being the doc content. y_column being the single label, which shall be modified\n            # to load a list of labels.\n            if x_columns:\n                inputs = tuple(c for i, c in enumerate(cells) if i in x_columns), cells[y_column]\n            else:\n                if y_column != -1:\n                    cells[-1], cells[y_column] = cells[y_column], cells[-1]\n                inputs = tuple(cells[:-1]), cells[-1]\n            if num_features is None:\n                num_features = len(inputs[0])\n                self.config.num_features = num_features\n            # multi-label support\n            if self.config.get('multi_label', None):\n                assert type(inputs[1]) is str, 'Y value has to be string'\n                if inputs[1][0] == '[':\n                    # multi-label is in literal form of a list\n                    labels = eval(inputs[1])\n                else:\n                    labels = inputs[1].strip().split(',')\n                inputs = inputs[0], labels\n            else:\n                assert num_features == len(inputs[0]), f'Numbers of columns {num_features} ' \\\n                                                       f'inconsistent with current {len(inputs[0])}'\n            yield inputs\n\n    def inputs_to_samples(self, inputs, gold=False):\n        pad = self.label_vocab.safe_pad_token\n        for cells in inputs:\n            if gold:\n                yield cells\n            else:\n                yield cells, pad\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        return self.label_vocab.lookup(y)\n\n    def fit(self, trn_path: str, **kwargs):\n        samples = 0\n        for t in self.file_to_samples(trn_path, gold=True):\n            if self.config.get('multi_label', None):\n                for l in t[1]:\n                    self.label_vocab.add(l)\n            else:\n                self.label_vocab.add(t[1])  # the second one regardless of t is pair or triple\n            samples += 1\n        return samples\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        num_features = self.config.num_features\n        # It's crucial to use tuple instead of list for all the three\n        types = tuple([tf.string] * num_features), tf.string\n        shapes = tuple([[]] * num_features), []\n        values = tuple([PAD] * num_features), self.label_vocab.safe_pad_token\n        return types, shapes, values\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        logger.warning('TableTransform can not map x to idx. Please override x_to_idx')\n        return x\n", "hanlp/transform/txt_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-24 15:07\nimport functools\nfrom abc import ABC\nfrom typing import Tuple, Union, List, Iterable\n\nimport tensorflow as tf\n\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp.utils.lang.zh.char_table import CharTable\nfrom hanlp.utils.span_util import bmes_of, bmes_to_words\nfrom hanlp.utils.string_util import split_long_sent\n\n\ndef generate_words_per_line(file_path):\n    with open(file_path, encoding='utf-8') as src:\n        for line in src:\n            cells = line.strip().split()\n            if not cells:\n                continue\n            yield cells\n\n\ndef words_to_bmes(words):\n    tags = []\n    for w in words:\n        if not w:\n            raise ValueError('{} contains None or zero-length word {}'.format(str(words), w))\n        if len(w) == 1:\n            tags.append('S')\n        else:\n            tags.extend(['B'] + ['M'] * (len(w) - 2) + ['E'])\n    return tags\n\n\ndef extract_ngram_features_and_tags(sentence, bigram_only=False, window_size=4, segmented=True):\n    \"\"\"\n    Feature extraction for windowed approaches\n    See Also https://github.com/chqiwang/convseg/\n    Parameters\n    ----------\n    sentence\n    bigram_only\n    window_size\n    segmented\n\n    Returns\n    -------\n\n    \"\"\"\n    chars, tags = bmes_of(sentence, segmented)\n    chars = CharTable.normalize_chars(chars)\n    ret = []\n    ret.append(chars)\n    # TODO: optimize ngram generation using https://www.tensorflow.org/api_docs/python/tf/strings/ngrams\n    ret.extend(extract_ngram_features(chars, bigram_only, window_size))\n    ret.append(tags)\n    return tuple(ret[:-1]), ret[-1]  # x, y\n\n\ndef extract_ngram_features(chars, bigram_only, window_size):\n    ret = []\n    if bigram_only:\n        chars = ['', ''] + chars + ['', '']\n        ret.append([a + b if a and b else '' for a, b in zip(chars[:-4], chars[1:])])\n        ret.append([a + b if a and b else '' for a, b in zip(chars[1:-3], chars[2:])])\n        ret.append([a + b if a and b else '' for a, b in zip(chars[2:-2], chars[3:])])\n        ret.append([a + b if a and b else '' for a, b in zip(chars[3:-1], chars[4:])])\n    elif window_size > 0:\n        chars = ['', '', ''] + chars + ['', '', '']\n        # single char\n        if window_size >= 1:\n            ret.append(chars[3:-3])\n        if window_size >= 2:\n            # bi chars\n            ret.append([a + b if a and b else '' for a, b in zip(chars[2:], chars[3:-3])])\n            ret.append([a + b if a and b else '' for a, b in zip(chars[3:-3], chars[4:])])\n        if window_size >= 3:\n            # tri chars\n            ret.append(\n                [a + b + c if a and b and c else '' for a, b, c in zip(chars[1:], chars[2:], chars[3:-3])])\n            ret.append(\n                [a + b + c if a and b and c else '' for a, b, c in zip(chars[2:], chars[3:-3], chars[4:])])\n            ret.append(\n                [a + b + c if a and b and c else '' for a, b, c in zip(chars[3:-3], chars[4:], chars[5:])])\n        if window_size >= 4:\n            # four chars\n            ret.append([a + b + c + d if a and b and c and d else '' for a, b, c, d in\n                        zip(chars[0:], chars[1:], chars[2:], chars[3:-3])])\n            ret.append([a + b + c + d if a and b and c and d else '' for a, b, c, d in\n                        zip(chars[1:], chars[2:], chars[3:-3], chars[4:])])\n            ret.append([a + b + c + d if a and b and c and d else '' for a, b, c, d in\n                        zip(chars[2:], chars[3:-3], chars[4:], chars[5:])])\n            ret.append([a + b + c + d if a and b and c and d else '' for a, b, c, d in\n                        zip(chars[3:-3], chars[4:], chars[5:], chars[6:])])\n    return ret\n\n\ndef generate_ngram_bmes(file_path, bigram_only=False, window_size=4, gold=True):\n    with open(file_path, encoding='utf-8') as src:\n        for line in src:\n            sentence = line.strip()\n            if not sentence:\n                continue\n            yield extract_ngram_features_and_tags(sentence, bigram_only, window_size, gold)\n\n\ndef vocab_from_txt(txt_file_path, bigram_only=False, window_size=4, **kwargs) -> Tuple[VocabTF, VocabTF, VocabTF]:\n    char_vocab, ngram_vocab, tag_vocab = VocabTF(), VocabTF(), VocabTF(pad_token=None, unk_token=None)\n    for X, Y in generate_ngram_bmes(txt_file_path, bigram_only, window_size, gold=True):\n        char_vocab.update(X[0])\n        for ngram in X[1:]:\n            ngram_vocab.update(filter(lambda x: x, ngram))\n        tag_vocab.update(Y)\n    return char_vocab, ngram_vocab, tag_vocab\n\n\ndef dataset_from_txt(txt_file_path: str, char_vocab: VocabTF, ngram_vocab: VocabTF, tag_vocab: VocabTF,\n                     bigram_only=False,\n                     window_size=4, segmented=True, batch_size=32, shuffle=None, repeat=None, prefetch=1):\n    generator = functools.partial(generate_ngram_bmes, txt_file_path, bigram_only, window_size, segmented)\n    return dataset_from_generator(generator, char_vocab, ngram_vocab, tag_vocab, bigram_only, window_size, batch_size,\n                                  shuffle, repeat, prefetch)\n\n\ndef dataset_from_generator(generator, char_vocab, ngram_vocab, tag_vocab, bigram_only=False, window_size=4,\n                           batch_size=32, shuffle=None, repeat=None, prefetch=1):\n    if bigram_only:\n        ngram_size = 4\n    else:\n        ngram_size = window_size * (window_size + 1) // 2\n    vec_dim = 2 + ngram_size\n    shapes = tuple([[None]] * (vec_dim - 1)), [None]\n    types = tuple([tf.string] * (vec_dim - 1)), tf.string\n    defaults = tuple([char_vocab.pad_token] + [\n        ngram_vocab.pad_token if ngram_vocab else char_vocab.pad_token] * ngram_size), (\n                   tag_vocab.pad_token if tag_vocab.pad_token else tag_vocab.first_token)\n    dataset = tf.data.Dataset.from_generator(generator, output_shapes=shapes, output_types=types)\n    if shuffle:\n        if isinstance(shuffle, bool):\n            shuffle = 1024\n        dataset = dataset.shuffle(shuffle)\n    if repeat:\n        dataset = dataset.repeat(repeat)\n    dataset = dataset.padded_batch(batch_size, shapes, defaults).prefetch(prefetch)\n    return dataset\n\n\nclass TxtFormat(Transform, ABC):\n    def file_to_inputs(self, filepath: str, gold=True):\n        filepath = get_resource(filepath)\n        with open(filepath, encoding='utf-8') as src:\n            for line in src:\n                sentence = line.strip()\n                if not sentence:\n                    continue\n                yield sentence\n\n\nclass TxtBMESFormat(TxtFormat, ABC):\n    def file_to_inputs(self, filepath: str, gold=True):\n        max_seq_length = self.config.get('max_seq_length', False)\n        if max_seq_length:\n            if 'transformer' in self.config:\n                max_seq_length -= 2  # allow for [CLS] and [SEP]\n            delimiter = set()\n            delimiter.update('\u3002\uff01\uff1f\uff1a\uff1b\u3001\uff0c,;!?\u3001,')\n        for text in super().file_to_inputs(filepath, gold):\n            chars, tags = bmes_of(text, gold)\n            if max_seq_length:\n                start = 0\n                for short_chars in split_long_sent(chars, delimiter, max_seq_length):\n                    end = start + len(short_chars)\n                    yield short_chars, tags[start:end]\n                    start = end\n            else:\n                yield chars, tags\n\n    def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:\n        return isinstance(input, str)\n\n    def inputs_to_samples(self, inputs, gold=False):\n        for chars, tags in (inputs if gold else zip(inputs, [None] * len(inputs))):\n            if not gold:\n                tags = [self.tag_vocab.safe_pad_token] * len(chars)\n            chars = CharTable.normalize_chars(chars)\n            yield chars, tags\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None,\n                     batch=None) -> Iterable:\n        yield from self.Y_to_tokens(self.tag_vocab, Y, gold, inputs)\n\n    def Y_to_tokens(self, tag_vocab, Y, gold, inputs):\n        if not gold:\n            Y = tf.argmax(Y, axis=2)\n        for text, ys in zip(inputs, Y):\n            tags = [tag_vocab.idx_to_token[int(y)] for y in ys[:len(text)]]\n            yield bmes_to_words(list(text), tags)\n", "hanlp/transform/text_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-04 11:46\nfrom typing import Union, Tuple, Iterable, Any\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.metrics.chunking.sequence_labeling import get_entities\nfrom hanlp.utils.file_read_backwards import FileReadBackwards\nfrom hanlp.utils.io_util import read_tsv_as_sents\n\n\nclass TextTransform(Transform):\n\n    def __init__(self,\n                 forward=True,\n                 seq_len=10,\n                 tokenizer='char',\n                 config: SerializableDict = None, map_x=True, map_y=True, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, seq_len=seq_len, tokenizer=tokenizer, forward=forward, **kwargs)\n        self.vocab: VocabTF = None\n\n    def tokenize_func(self):\n        if self.config.tokenizer == 'char':\n            return list\n        elif self.config.tokenizer == 'whitespace':\n            return lambda x: x.split()\n        else:\n            return lambda x: x.split(self.config.tokenizer)\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        self.vocab = VocabTF()\n        num_samples = 0\n        for x, y in self.file_to_inputs(trn_path):\n            self.vocab.update(x)\n            num_samples += 1\n        return num_samples\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        types = tf.string, tf.string\n        shapes = [None], [None]\n        defaults = self.vocab.pad_token, self.vocab.pad_token\n        return types, shapes, defaults\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        forward = self.config.forward\n        seq_len = self.config.seq_len\n        buffer = []\n        tokenizer = self.tokenize_func()\n        with open(filepath, encoding='utf-8') if forward else FileReadBackwards(filepath, encoding=\"utf-8\") as src:\n            for line in src:\n                tokens = tokenizer(line)\n                buffer += tokens\n                while len(buffer) > seq_len:\n                    yield buffer[:seq_len], buffer[1:1 + seq_len]\n                    buffer.pop(0)\n\n    def inputs_to_samples(self, inputs, gold=False):\n        forward = self.config.forward\n        for t in inputs:\n            if gold:\n                x, y = t\n            else:\n                x, y = t, t\n            if not forward:\n                x = list(reversed(x))\n                y = list(reversed(y))\n            yield x, y\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        return self.vocab.lookup(x)\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        return self.x_to_idx(y)\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, **kwargs) -> Iterable:\n        pred = tf.argmax(Y, axis=-1)\n        for ys, ms in zip(pred, inputs):\n            ret = []\n            for y in ys:\n                ret.append(self.vocab.idx_to_token[int(y)])\n            yield ret\n\n    def input_is_single_sample(self, input: Any) -> bool:\n        return isinstance(input[0], str)\n\n\ndef bmes_to_flat(inpath, outpath):\n    with open(outpath, 'w', encoding='utf-8') as out:\n        for sent in read_tsv_as_sents(inpath):\n            chunks = get_entities([cells[1] for cells in sent])\n            chars = [cells[0] for cells in sent]\n            words = []\n            for tag, start, end in chunks:\n                word = ''.join(chars[start: end])\n                words.append(word)\n            out.write(' '.join(f'{word}/{tag}' for word, (tag, _, _) in zip(words, chunks)))\n            out.write('\\n')", "hanlp/transform/tacred_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-14 17:06\nfrom typing import Union, Tuple\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp_common.io import load_json\nfrom hanlp_common.util import merge_locals_kwargs\n\n\ndef get_positions(start_idx, end_idx, length):\n    \"\"\"Get subj/obj position sequence.\n\n    Args:\n      start_idx: \n      end_idx: \n      length: \n\n    Returns:\n\n    \"\"\"\n    return list(range(-start_idx, 0)) + [0] * (end_idx - start_idx + 1) + \\\n           list(range(1, length - end_idx))\n\n\nclass TACREDTransform(Transform):\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, lower=False, **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.token_vocab = VocabTF()\n        self.pos_vocab = VocabTF(pad_token=None, unk_token=None)\n        self.ner_vocab = VocabTF(pad_token=None)\n        self.deprel_vocab = VocabTF(pad_token=None, unk_token=None)\n        self.rel_vocab = VocabTF(pad_token=None, unk_token=None)\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        count = 0\n        for (tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type,\n             obj_type), relation in self.file_to_samples(\n            trn_path, gold=True):\n            count += 1\n            self.token_vocab.update(tokens)\n            self.pos_vocab.update(pos)\n            self.ner_vocab.update(ner)\n            self.deprel_vocab.update(deprel)\n            self.rel_vocab.add(relation)\n        return count\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        data = load_json(filepath)\n        for d in data:\n            tokens = list(d['token'])\n            ss, se = d['subj_start'], d['subj_end']\n            os, oe = d['obj_start'], d['obj_end']\n            pos = d['stanford_pos']\n            ner = d['stanford_ner']\n            deprel = d['stanford_deprel']\n            head = [int(x) for x in d['stanford_head']]\n            assert any([x == 0 for x in head])\n            relation = d['relation']\n            yield (tokens, pos, ner, head, deprel, ss, se, os, oe), relation\n\n    def inputs_to_samples(self, inputs, gold=False):\n        for input in inputs:\n            if gold:\n                (tokens, pos, ner, head, deprel, ss, se, os, oe), relation = input\n            else:\n                tokens, pos, ner, head, deprel, ss, se, os, oe = input\n                relation = self.rel_vocab.safe_pad_token\n            l = len(tokens)\n            subj_positions = get_positions(ss, se, l)\n            obj_positions = get_positions(os, oe, l)\n            subj_type = ner[ss]\n            obj_type = ner[os]\n            # anonymize tokens\n            tokens[ss:se + 1] = ['SUBJ-' + subj_type] * (se - ss + 1)\n            tokens[os:oe + 1] = ['OBJ-' + obj_type] * (oe - os + 1)\n            # min head is 0, but root is not included in tokens, so take 1 off from each head\n            head = [h - 1 for h in head]\n            yield (tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type, obj_type), relation\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        # (tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type, obj_type), relation\n        types = (tf.string, tf.string, tf.string, tf.int32, tf.string, tf.int32, tf.int32, tf.string,\n                 tf.string), tf.string\n        shapes = ([None], [None], [None], [None], [None], [None], [None], [], []), []\n        pads = (self.token_vocab.safe_pad_token, self.pos_vocab.safe_pad_token, self.ner_vocab.safe_pad_token, 0,\n                self.deprel_vocab.safe_pad_token,\n                0, 0, self.ner_vocab.safe_pad_token, self.ner_vocab.safe_pad_token), self.rel_vocab.safe_pad_token\n        return types, shapes, pads\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type, obj_type = x\n        tokens = self.token_vocab.lookup(tokens)\n        pos = self.pos_vocab.lookup(pos)\n        ner = self.ner_vocab.lookup(ner)\n        deprel = self.deprel_vocab.lookup(deprel)\n        subj_type = self.ner_vocab.lookup(subj_type)\n        obj_type = self.ner_vocab.lookup(obj_type)\n        return tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type, obj_type\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        return self.rel_vocab.lookup(y)\n", "hanlp/transform/transformer_tokenizer.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-03 16:23\nimport warnings\nfrom typing import Union, Optional\n\nfrom hanlp_common.constant import BOS, EOS\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, PretrainedConfig, AutoTokenizer_\nfrom hanlp_trie import DictInterface\n\n\nclass TransformerTokenizer(object):\n\n    def __init__(self, max_seq_length=512, truncate_long_sequences=True) -> None:\n        self.truncate_long_sequences = truncate_long_sequences\n        self.max_seq_length = max_seq_length\n\n    def sliding_window(self, flat_wordpiece_ids, same_tail=True):\n        if same_tail:\n            start_piece_ids, flat_wordpiece_ids, end_piece_ids = flat_wordpiece_ids[:1], \\\n                                                                 flat_wordpiece_ids[1:-1], flat_wordpiece_ids[-1:]\n        else:\n            start_piece_ids, flat_wordpiece_ids, end_piece_ids = flat_wordpiece_ids[:1], \\\n                                                                 flat_wordpiece_ids[1:], []\n        window_length = self.max_seq_length - len(start_piece_ids) - len(end_piece_ids)\n        stride = window_length // 2\n        wordpiece_windows = [start_piece_ids + flat_wordpiece_ids[i:i + window_length] + end_piece_ids\n                             for i in range(0, len(flat_wordpiece_ids), stride)]\n\n        # Check for overlap in the last window. Throw it away if it is redundant.\n        last_window = wordpiece_windows[-1][1:]\n        penultimate_window = wordpiece_windows[-2]\n        if last_window == penultimate_window[-len(last_window):]:\n            wordpiece_windows = wordpiece_windows[:-1]\n\n        wordpiece_ids = [wordpiece for sequence in wordpiece_windows for wordpiece in sequence]\n        return wordpiece_ids\n\n\nclass TransformerTextTokenizer(TransformerTokenizer):\n    _KEY = ['input_ids', 'attention_mask', 'token_type_ids']\n\n    def __init__(self,\n                 tokenizer: Union[PreTrainedTokenizer, str],\n                 text_a_key: str,\n                 text_b_key: str = None,\n                 output_key=None,\n                 max_seq_length=512, truncate_long_sequences=True) -> None:\n        super().__init__(max_seq_length, truncate_long_sequences)\n        self.text_b = text_b_key\n        self.text_a = text_a_key\n        if output_key is None:\n            output_key = self.text_a\n            if text_b_key:\n                output_key += '_' + text_b_key\n        if output_key == '':\n            output_key = self._KEY\n        else:\n            output_key = [f'{output_key}_{key}' for key in self._KEY]\n        self.output_key = output_key\n        if isinstance(tokenizer, str):\n            tokenizer = AutoTokenizer_.from_pretrained(tokenizer)\n        self.tokenizer = tokenizer\n\n    def __call__(self, sample: dict):\n        text_a = sample[self.text_a]\n        text_b = sample[self.text_b] if self.text_b else None\n        max_seq_length = self.max_seq_length if self.truncate_long_sequences else None\n        encoding = self.tokenizer.encode_plus(text_a, text_b, max_length=max_seq_length)\n        results = dict((k, encoding.data.get(k, None)) for k in self._KEY)\n        if not self.truncate_long_sequences and len(results['input_ids']) > self.max_seq_length:\n            # TODO: other fields should be properly handled too\n            results['input_ids'] = self.sliding_window(results['input_ids'])\n        if not results['token_type_ids']:\n            results['token_type_ids'] = encoding[0].type_ids\n        for k, v in zip(self.output_key, [results[_] for _ in self._KEY]):\n            sample[k] = v\n        return sample\n\n\nclass TransformerSequenceTokenizer(TransformerTokenizer):\n\n    def __init__(self,\n                 tokenizer: Union[PreTrainedTokenizer, str],\n                 input_key,\n                 output_key=None,\n                 max_seq_length=512,\n                 truncate_long_sequences=False,\n                 config: PretrainedConfig = None,\n                 cls_token_at_end=False,\n                 cls_token_segment_id=0,\n                 pad_token_segment_id=0,\n                 pad_on_left=False,\n                 do_padding=False,\n                 sep_token_extra=False,\n                 ret_mask_and_type=False,\n                 ret_prefix_mask=False,\n                 ret_token_span=True,\n                 ret_subtokens=False,\n                 ret_subtokens_group=False,\n                 cls_is_bos=False,\n                 sep_is_eos=False,\n                 do_basic_tokenize=True,\n                 use_fast=True,\n                 dict_force=None,\n                 strip_cls_sep=True,\n                 check_space_before=None,\n                 ) -> None:\n        \"\"\"A transformer tokenizer for token-level tasks. It honors the boundary of tokens and tokenize each token into\n        several subtokens then merge them. The information about each subtoken belongs to which token are kept and\n        returned as a new field in the sample. It also provides out-of-box sliding window trick on long sequences.\n\n        Args:\n            tokenizer: The identifier of a pre-trained tokenizer or a ``PreTrainedTokenizer``.\n            input_key: The token key in samples.\n            output_key: The output keys to store results.\n                max_seq_length: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.\n            truncate_long_sequences: ``True`` to truncate exceeded parts of long sequences. ``False`` to  enable\n                sliding window.\n            config: The ``PretrainedConfig`` to determine the model structure of the transformer, so that special\n                tokenization can be applied.\n            cls_token_at_end: ``True`` to put ``[CLS]`` at the end of input tokens.\n            cls_token_segment_id: The id of ``[CLS]``.\n            pad_token_segment_id: The id of ``[SEP]``.\n            pad_on_left: ``True`` to put ``[PAD]`` at the left side of input tokens.\n            do_padding: ``True`` to pad sequence to the left.\n            sep_token_extra: ``True`` to have two ``[SEP]``.\n            ret_mask_and_type: ``True`` to return masks and type ids.\n            ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.\n            ret_token_span: ``True`` to return span of each token measured by subtoken offsets.\n            ret_subtokens: ``True`` to return list of subtokens belonging to each token for tokenization purpose.\n                When enabled, the prefix mask for each subtoken is set to True as each subtoken is a token unit in\n                tokenization task. Similarity, the token span for each token will be a continuous integer sequence.\n            ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.\n            cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.\n                        ``False`` (default) means the first token is not [CLS], it will have its own embedding other than\n                        the embedding of [CLS].\n            sep_is_eos: ``True`` means the last token of input is [SEP].\n                        ``False`` means it's not but [SEP] will be appended,\n                        ``None`` means it dependents on `input[-1] == [EOS]`.\n            do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n            use_fast: Whether or not to try to load the fast version of the tokenizer.\n            dict_force: A dictionary doing longest-prefix-match on input text so that the head and tail of each keyword\n                won't be concatenated to other tokens by transformer tokenizers.\n            strip_cls_sep: ``True`` to strip [CLS] and [SEP] off the input tokens.\n            check_space_before: ``True`` to detect the space before each token to handle underline in sentence piece\n                tokenization.\n\n        Examples:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            transform = TransformerSequenceTokenizer('bert-base-uncased', 'token')\n            sample = {'token': 'HanLP good'.split()}\n            print(transform(sample))\n\n        \"\"\"\n        super().__init__(max_seq_length, truncate_long_sequences)\n        tokenizer_name = tokenizer if isinstance(tokenizer, str) else tokenizer.name_or_path\n        if check_space_before is None:\n            # These tokenizer is BPE-based which appends a space before each token and tokenizes loving into\n            # ['\u2581lo', 'ving'], tokenize \u5546\u54c1 into ['\u2581', '\u5546\u54c1']. For the later case, the prefix '\u2581' has to be removed\n            # as there is no space between some languages like Chinese\n            check_space_before = tokenizer_name in ('xlm-roberta-base', 'xlm-roberta-large', 'google/mt5-small',\n                                                    'google/mt5-base', 'xlm-roberta-base-no-space',\n                                                    'mMiniLMv2L6-no-space', 'mMiniLMv2L12-no-space')\n        self.check_space_before = check_space_before\n        self.ret_subtokens_group = ret_subtokens_group\n        self.ret_subtokens = ret_subtokens\n        self.sep_is_eos = sep_is_eos\n        self.ret_prefix_mask = ret_prefix_mask\n        self.ret_mask_and_type = ret_mask_and_type\n        self.cls_is_bos = cls_is_bos\n        self.ret_token_span = ret_token_span\n        if not output_key or isinstance(output_key, str):\n            suffixes = ['input_ids']\n            if ret_mask_and_type:\n                suffixes += 'attention_mask', 'token_type_ids'\n            if ret_prefix_mask:\n                suffixes += ['prefix_mask']\n            if ret_token_span:\n                suffixes.append('token_span')\n            if output_key is None:\n                output_key = [f'{input_key}_{key}' for key in suffixes]\n            elif output_key == '':\n                output_key = suffixes\n            else:\n                output_key = [f'{output_key}_{key}' for key in suffixes]\n\n        self.input_key = input_key\n        self.output_key = output_key\n        if config:\n            xlnet = config_is(config, 'xlnet')\n            pad_token_segment_id = 4 if xlnet else 0\n            cls_token_segment_id = 2 if xlnet else 0\n            cls_token_at_end = xlnet\n            pad_on_left = xlnet\n        if isinstance(tokenizer, str):\n            tokenizer = AutoTokenizer_.from_pretrained(tokenizer, use_fast=use_fast,\n                                                       do_basic_tokenize=do_basic_tokenize)\n        if use_fast:\n            # Dirty fix upstream bug: https://github.com/hankcs/HanLP/issues/1602\n            if hasattr(tokenizer, '_tokenizer') and hasattr(tokenizer._tokenizer, 'no_truncation'):\n                _t = tokenizer._tokenizer\n                _t.no_truncation()\n                _t.no_padding()\n                _t.no_truncation = _t.no_padding = lambda: None\n        pad_token = tokenizer.pad_token\n        self.pad_token_id = tokenizer.convert_tokens_to_ids([pad_token])[0]\n        self.pad_token_segment_id = pad_token_segment_id\n        if tokenizer_name in ('google/mt5-small', 'google/mt5-base'):\n            # mt5 doesn't have cls or sep, but we can use something similar\n            self.has_cls = False\n            self.cls_token = '\u2581'\n            self.cls_token_id = tokenizer.convert_tokens_to_ids(self.cls_token)\n            self.sep_token = tokenizer.eos_token\n            self.sep_token_id = tokenizer.eos_token_id\n        else:\n            self.has_cls = True\n            self.cls_token = tokenizer.cls_token\n            self.sep_token = tokenizer.sep_token\n            self.cls_token_segment_id = cls_token_segment_id\n            self.cls_token_id = tokenizer.cls_token_id\n            self.sep_token_id = tokenizer.sep_token_id\n\n        self.sep_token_extra = sep_token_extra\n        self.cls_token_at_end = cls_token_at_end\n        self.tokenizer = tokenizer\n        self.pad_on_left = pad_on_left\n        self.do_padding = do_padding\n        if self.ret_token_span or not self.truncate_long_sequences:\n            assert not self.cls_token_at_end\n            assert not self.pad_on_left\n        # if self.ret_subtokens:\n        #     if not use_fast:\n        #         raise NotImplementedError(\n        #             'ret_subtokens is not available when using Python tokenizers. '\n        #             'To use this feature, set use_fast = True.')\n        self.dict: Optional[DictInterface] = dict_force  # For tokenization of raw text\n        self.strip_cls_sep = strip_cls_sep\n\n    def __call__(self, sample: dict):\n        input_tokens = sample[self.input_key]\n        input_is_str = isinstance(input_tokens, str)\n        tokenizer = self.tokenizer\n        ret_token_span = self.ret_token_span\n        if input_is_str:  # This happens in a tokenizer component where the raw sentence is fed.\n\n            # noinspection PyShadowingNames\n            def tokenize_str(input_str, add_special_tokens=True):\n                if tokenizer.is_fast:\n                    encoding = tokenizer.encode_plus(input_str,\n                                                     return_offsets_mapping=True,\n                                                     add_special_tokens=add_special_tokens).encodings[0]\n                    subtoken_offsets = encoding.offsets\n                    input_tokens = encoding.tokens\n                    input_ids = encoding.ids\n\n                    # Fill up missing non-blank characters swallowed by HF tokenizer\n                    offset = 0\n                    fixed_offsets = []\n                    fixed_tokens = []\n                    fixed_ids = []\n                    for token, id, (b, e) in zip(input_tokens, input_ids, subtoken_offsets):\n                        if b > offset:\n                            missing_token = input_str[offset: b]\n                            if not missing_token.isspace():  # In the future, we may want space back\n                                fixed_tokens.append(missing_token)\n                                fixed_ids.append(tokenizer.unk_token_id)\n                                fixed_offsets.append((offset, b))\n                        if e == offset:  # LI\u2122 -> LIT + M\n                            if fixed_offsets and fixed_offsets[-1][0] < b:\n                                fixed_offsets[-1] = (fixed_offsets[-1][0], b)\n\n                        fixed_tokens.append(token)\n                        fixed_ids.append(id)\n                        fixed_offsets.append((b, e))\n                        offset = e\n                    subtoken_offsets = fixed_offsets\n                    input_tokens = fixed_tokens\n                    input_ids = fixed_ids\n\n                    if add_special_tokens:\n                        subtoken_offsets = subtoken_offsets[1 if self.has_cls else 0:-1]\n\n                    # Edge case that the input_str is swallowed in whole\n                    if input_str and not subtoken_offsets and not input_str.isspace():\n                        __index = 1 if add_special_tokens and self.has_cls else 0\n                        input_tokens.insert(__index, input_str)\n                        input_ids.insert(__index, tokenizer.unk_token_id)\n                        subtoken_offsets.append((0, len(input_str)))\n\n                    if not self.has_cls:\n                        input_tokens = [self.cls_token] + input_tokens\n                        input_ids = [self.cls_token_id] + input_ids\n                else:\n                    input_tokens = tokenizer.tokenize(input_str)\n                    subtoken_offsets = []\n                    _o = 0\n                    for each in input_tokens:\n                        subtoken_offsets.append((_o, _o + len(each)))\n                        _o += len(each)\n                    if add_special_tokens:\n                        input_tokens = [self.cls_token] + input_tokens + [self.sep_token]\n                    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n                if self.check_space_before:\n                    non_blank_offsets = [i for i in range(len(input_tokens)) if input_tokens[i] != '\u2581']\n                    if add_special_tokens and not self.has_cls:\n                        non_blank_offsets.insert(0, 0)\n                    input_tokens = [input_tokens[i] for i in non_blank_offsets]\n                    input_ids = [input_ids[i] for i in non_blank_offsets]\n                    if add_special_tokens:\n                        non_blank_offsets = non_blank_offsets[1:-1]\n                        subtoken_offsets = [subtoken_offsets[i - 1] for i in non_blank_offsets]\n                    else:\n                        subtoken_offsets = [subtoken_offsets[i] for i in non_blank_offsets]\n                    # MT5 generates tokens like \u2581of, which is bad for the tokenizer. So we want to remove the prefix.\n                    for i, token in enumerate(input_tokens[1:-1] if add_special_tokens else input_tokens):\n                        if input_str[subtoken_offsets[i][0]] == ' ':\n                            subtoken_offsets[i] = (subtoken_offsets[i][0] + 1, subtoken_offsets[i][1])\n                # The following block will tokenize each empty string (space) into an unk token\n                # if add_special_tokens:\n                #     if len(input_tokens) == 2:  # bos and eos, meaning that the text contains only some spaces\n                #         input_tokens.insert(1, input_str)\n                #         input_ids.insert(1, tokenizer.unk_token_id)\n                #         subtoken_offsets.append((0, len(input_str)))\n                # else:\n                #     if not input_ids:  # This chunk might be some control chars getting removed by tokenizer\n                #         input_tokens = [input_str]\n                #         input_ids = [tokenizer.unk_token_id]\n                #         subtoken_offsets = [(0, len(input_str))]\n                return input_tokens, input_ids, subtoken_offsets\n\n            if self.dict:\n                chunks = self.dict.split(sample.get(f'{self.input_key}_', input_tokens))  # Match original text directly\n                _input_tokens, _input_ids, _subtoken_offsets = [self.cls_token], [self.cls_token_id], []\n                _offset = 0\n                custom_words = sample['custom_words'] = []\n                char_offset = 0\n                for chunk in chunks:\n                    if isinstance(chunk, str):  # Use transformed text as it's what models are trained on\n                        chunk = input_tokens[char_offset:char_offset + len(chunk)]\n                        tokens, ids, offsets = tokenize_str(chunk, add_special_tokens=False)\n                        char_offset += len(chunk)\n                    else:\n                        begin, end, label = chunk\n                        _offset = begin\n                        # chunk offset is on char level, at this moment, there is no concept of tokens, just subtokens\n                        if isinstance(label, list):\n                            tokens, ids, offsets, delta = [], [], [], 0\n                            for token in label:\n                                _tokens, _ids, _offsets = tokenize_str(token, add_special_tokens=False)\n                                tokens.extend(_tokens)\n                                # track the subword offset of this chunk, -1 for [CLS]\n                                custom_words.append(\n                                    (len(_input_ids) + len(ids) - 1, len(_input_ids) + len(ids) - 1 + len(_ids), token))\n                                ids.extend(_ids)\n                                offsets.extend((x[0] + delta, x[1] + delta) for x in _offsets)\n                                delta = offsets[-1][-1]\n                        else:\n                            tokens, ids, offsets = tokenize_str(input_tokens[begin:end], add_special_tokens=False)\n                            # offsets = [(offsets[0][0], offsets[-1][-1])]\n                            custom_words.append((len(_input_ids) - 1, len(_input_ids) + len(ids) - 1, label))\n                        char_offset = end\n                    _input_tokens.extend(tokens)\n                    _input_ids.extend(ids)\n                    _subtoken_offsets.extend((x[0] + _offset, x[1] + _offset) for x in offsets)\n                    _offset = _subtoken_offsets[-1][-1]\n                subtoken_offsets = _subtoken_offsets\n                input_tokens = _input_tokens + [self.sep_token]\n                input_ids = _input_ids + [self.sep_token_id]\n            else:\n                input_tokens, input_ids, subtoken_offsets = tokenize_str(input_tokens, add_special_tokens=True)\n\n            if self.ret_subtokens:\n                sample[f'{self.input_key}_subtoken_offsets'] = subtoken_offsets\n\n        cls_is_bos = self.cls_is_bos\n        if cls_is_bos is None:\n            cls_is_bos = input_tokens[0] == BOS\n        sep_is_eos = self.sep_is_eos\n        if sep_is_eos is None:\n            sep_is_eos = input_tokens[-1] == EOS\n        if self.strip_cls_sep:\n            if cls_is_bos:\n                input_tokens = input_tokens[1:]\n            if sep_is_eos:\n                input_tokens = input_tokens[:-1]\n        if not self.ret_mask_and_type:  # only need input_ids and token_span, use a light version\n            if input_is_str:\n                prefix_mask = self._init_prefix_mask(input_ids)\n            else:\n                if input_tokens:\n                    return_offsets_mapping = tokenizer.is_fast and self.ret_subtokens\n                    encodings = tokenizer.batch_encode_plus(\n                        input_tokens,\n                        return_offsets_mapping=return_offsets_mapping,  # Many tokenizers do not offer fast version\n                        add_special_tokens=False\n                    )\n                    subtoken_ids_per_token = encodings.data['input_ids']\n                    if return_offsets_mapping:\n                        offsets_mapping = [encoding.offsets for encoding in encodings.encodings]\n                    else:\n                        offsets_mapping = []\n                        for token, subtoken_ids in zip(input_tokens, subtoken_ids_per_token):\n                            if len(subtoken_ids) > len(token):  # \u2026 --> ...\n                                del subtoken_ids[len(token):]\n                            if not subtoken_ids:\n                                subtoken_ids = [tokenizer.unk_token_id]\n                            # Since non-fast tok generates no mapping, we have to guess\n                            char_per_subtoken = max(len(token) // len(subtoken_ids), 1)\n                            bes = [(b, b + char_per_subtoken) for b in range(0, len(token), char_per_subtoken)]\n                            if not bes:  # the token is an empty string\n                                bes = [(0, 0)]\n                            if len(bes) != len(subtoken_ids):\n                                bes[len(subtoken_ids) - 1] = (bes[len(subtoken_ids) - 1][0], len(token))\n                                del bes[len(subtoken_ids):]\n                            offsets_mapping.append(bes)\n                else:\n                    encodings = SerializableDict()\n                    subtoken_ids_per_token = []\n                    encodings.data = {'input_ids': subtoken_ids_per_token}\n                if self.check_space_before:\n                    # noinspection PyUnboundLocalVariable\n                    for token, subtokens, mapping, encoding in zip(input_tokens, subtoken_ids_per_token,\n                                                                   offsets_mapping, encodings.encodings):\n                        # Remove \u2581 generated by spm for 2 reasons:\n                        # 1. During decoding, mostly no \u2581 will be created unless blanks are placed between tokens (which\n                        # is true for English but in English it will likely be concatenated to the token following it)\n                        # 2. For T5, '\u2581' is used as CLS\n                        if len(subtokens) > 1 and encoding.tokens[0] == '\u2581':\n                            subtokens.pop(0)\n                            if mapping:\n                                mapping.pop(0)\n                # Some tokens get stripped out\n                subtoken_ids_per_token = [ids if ids else [tokenizer.unk_token_id] for ids in subtoken_ids_per_token]\n                input_ids = sum(subtoken_ids_per_token, [self.cls_token_id])\n                if self.sep_is_eos is None:\n                    # None means to check whether sep is at the tail or between tokens\n                    if sep_is_eos:\n                        input_ids += [self.sep_token_id]\n                    elif self.sep_token_id not in input_ids:\n                        input_ids += [self.sep_token_id]\n                else:\n                    input_ids += [self.sep_token_id]\n                # else self.sep_is_eos == False means sep is between tokens and don't bother to check\n\n                if self.ret_subtokens:\n                    prefix_mask = self._init_prefix_mask(input_ids)\n                    # if self.check_space_before:\n                    #     if offsets_mapping[0] and not input_tokens[0].startswith(' '):\n                    #         prefix_mask[1] = False\n                else:\n                    prefix_mask = [False] * len(input_ids)\n                    offset = 1\n                    for _subtokens in subtoken_ids_per_token:\n                        prefix_mask[offset] = True\n                        offset += len(_subtokens)\n                if self.ret_subtokens:\n                    subtoken_offsets = []\n                    for token, offsets in zip(input_tokens, offsets_mapping):\n                        if offsets:\n                            subtoken_offsets.append(offsets)\n                        else:\n                            subtoken_offsets.append([(0, len(token))])\n                    if self.ret_subtokens_group:\n                        sample[f'{self.input_key}_subtoken_offsets_group'] = subtoken_offsets\n                    else:\n                        sample[f'{self.input_key}_subtoken_offsets'] = sum(subtoken_offsets, [])\n        else:\n            input_ids, attention_mask, token_type_ids, prefix_mask = \\\n                convert_examples_to_features(input_tokens,\n                                             None,\n                                             tokenizer,\n                                             cls_token_at_end=self.cls_token_at_end,\n                                             # xlnet has a cls token at the end\n                                             cls_token=tokenizer.cls_token,\n                                             cls_token_segment_id=self.cls_token_segment_id,\n                                             sep_token=self.sep_token,\n                                             sep_token_extra=self.sep_token_extra,\n                                             # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                             pad_on_left=self.pad_on_left,\n                                             # pad on the left for xlnet\n                                             pad_token_id=self.pad_token_id,\n                                             pad_token_segment_id=self.pad_token_segment_id,\n                                             pad_token_label_id=0,\n                                             do_padding=self.do_padding)\n        if len(input_ids) > self.max_seq_length:\n            if self.truncate_long_sequences:\n                # raise SequenceTooLong(\n                #     f'Input tokens {input_tokens} exceed the max sequence length of {self.max_seq_length - 2}. '\n                #     f'For sequence tasks, truncate_long_sequences = True is not supported.'\n                #     f'You are recommended to split your long text into several sentences within '\n                #     f'{self.max_seq_length - 2} tokens beforehand. '\n                #     f'Or simply set truncate_long_sequences = False to enable sliding window.')\n                input_ids = input_ids[:self.max_seq_length]\n                prefix_mask = prefix_mask[:self.max_seq_length]\n                warnings.warn(\n                    f'Input tokens {input_tokens} exceed the max sequence length of {self.max_seq_length - 2}. '\n                    f'The exceeded part will be truncated and ignored. '\n                    f'You are recommended to split your long text into several sentences within '\n                    f'{self.max_seq_length - 2} tokens beforehand.'\n                    f'Or simply set truncate_long_sequences = False to enable sliding window.'\n                )\n            else:\n                input_ids = self.sliding_window(input_ids, input_ids[-1] == self.sep_token_id)\n        if prefix_mask:\n            if cls_is_bos:\n                prefix_mask[0] = True\n            if sep_is_eos:\n                prefix_mask[-1] = True\n        outputs = [input_ids]\n        if self.ret_mask_and_type:\n            # noinspection PyUnboundLocalVariable\n            outputs += [attention_mask, token_type_ids]\n        if self.ret_prefix_mask:\n            outputs += [prefix_mask]\n        if ret_token_span and prefix_mask:\n            if cls_is_bos:\n                token_span = [[0]]\n            else:\n                token_span = []\n            offset = 1\n            span = []\n            for mask in prefix_mask[1:len(prefix_mask) if sep_is_eos is None else -1]:  # skip [CLS] and [SEP]\n                if mask and span:\n                    token_span.append(span)\n                    span = []\n                span.append(offset)\n                offset += 1\n            if span:\n                token_span.append(span)\n            if sep_is_eos:\n                assert offset == len(prefix_mask) - 1\n                token_span.append([offset])\n            outputs.append(token_span)\n        for k, v in zip(self.output_key, outputs):\n            sample[k] = v\n        return sample\n\n    def _init_prefix_mask(self, input_ids):\n        prefix_mask = [True] * len(input_ids)\n        if not self.cls_is_bos:\n            prefix_mask[0] = False\n        if not self.sep_is_eos:\n            prefix_mask[-1] = False\n        return prefix_mask\n\n\ndef config_is(config, model='bert'):\n    return model in type(config).__name__.lower()\n\n\ndef convert_examples_to_features(\n        words,\n        max_seq_length: Optional[int],\n        tokenizer,\n        labels=None,\n        label_map=None,\n        cls_token_at_end=False,\n        cls_token=\"[CLS]\",\n        cls_token_segment_id=1,\n        sep_token=\"[SEP]\",\n        sep_token_extra=False,\n        pad_on_left=False,\n        pad_token_id=0,\n        pad_token_segment_id=0,\n        pad_token_label_id=0,\n        sequence_a_segment_id=0,\n        mask_padding_with_zero=True,\n        unk_token='[UNK]',\n        do_padding=True\n):\n    \"\"\"Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n\n    Args:\n      words: \n      max_seq_length: \n      tokenizer: \n      labels:  (Default value = None)\n      label_map:  (Default value = None)\n      cls_token_at_end:  (Default value = False)\n      cls_token:  (Default value = \"[CLS]\")\n      cls_token_segment_id:  (Default value = 1)\n      sep_token:  (Default value = \"[SEP]\")\n      sep_token_extra:  (Default value = False)\n      pad_on_left:  (Default value = False)\n      pad_token_id:  (Default value = 0)\n      pad_token_segment_id:  (Default value = 0)\n      pad_token_label_id:  (Default value = 0)\n      sequence_a_segment_id:  (Default value = 0)\n      mask_padding_with_zero:  (Default value = True)\n      unk_token:  (Default value = '[UNK]')\n      do_padding:  (Default value = True)\n\n    Returns:\n\n    \"\"\"\n    args = locals()\n    if not labels:\n        labels = words\n        pad_token_label_id = False\n\n    tokens = []\n    label_ids = []\n    for word, label in zip(words, labels):\n        word_tokens = tokenizer.tokenize(word)\n        if not word_tokens:\n            # some wired chars cause the tagger to return empty list\n            word_tokens = [unk_token] * len(word)\n        tokens.extend(word_tokens)\n        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n        label_ids.extend([label_map[label] if label_map else True] + [pad_token_label_id] * (len(word_tokens) - 1))\n\n    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n    special_tokens_count = 3 if sep_token_extra else 2\n    if max_seq_length and len(tokens) > max_seq_length - special_tokens_count:\n        warnings.warn(\n            f'Input tokens {words} exceed the max sequence length of {max_seq_length - special_tokens_count}. '\n            f'The exceeded part will be truncated and ignored. '\n            f'You are recommended to split your long text into several sentences within '\n            f'{max_seq_length - special_tokens_count} tokens beforehand.')\n        tokens = tokens[: (max_seq_length - special_tokens_count)]\n        label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  token_type_ids:   0   0   0   0  0     0   0\n    #\n    # Where \"token_type_ids\" are used to indicate whether this is the first\n    # sequence or the second sequence. The embedding vectors for `type=0` and\n    # `type=1` were learned during pre-training and are added to the wordpiece\n    # embedding vector (and position vector). This is not *strictly* necessary\n    # since the [SEP] token unambiguously separates the sequences, but it makes\n    # it easier for the model to learn the concept of sequences.\n    #\n    # For classification tasks, the first vector (corresponding to [CLS]) is\n    # used as as the \"sentence vector\". Note that this only makes sense because\n    # the entire model is fine-tuned.\n    tokens += [sep_token]\n    label_ids += [pad_token_label_id]\n    if sep_token_extra:\n        # roberta uses an extra separator b/w pairs of sentences\n        tokens += [sep_token]\n        label_ids += [pad_token_label_id]\n    segment_ids = [sequence_a_segment_id] * len(tokens)\n\n    if cls_token_at_end:\n        tokens += [cls_token]\n        label_ids += [pad_token_label_id]\n        segment_ids += [cls_token_segment_id]\n    else:\n        tokens = [cls_token] + tokens\n        label_ids = [pad_token_label_id] + label_ids\n        segment_ids = [cls_token_segment_id] + segment_ids\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n    if do_padding:\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token_id] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n        else:\n            input_ids += [pad_token_id] * padding_length\n            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n            segment_ids += [pad_token_segment_id] * padding_length\n            label_ids += [pad_token_label_id] * padding_length\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(label_ids) == max_seq_length, f'failed for:\\n {args}'\n    else:\n        assert len(set(len(x) for x in [input_ids, input_mask, segment_ids, label_ids])) == 1\n    return input_ids, input_mask, segment_ids, label_ids\n\n\ndef main():\n    transformer = 'bert-base-uncased'\n    tokenizer: PreTrainedTokenizer = AutoTokenizer_.from_pretrained(transformer)\n    # _test_text_transform(tokenizer)\n    _test_sequence_transform(tokenizer)\n\n\ndef _test_text_transform(tokenizer):\n    transform = TransformerTextTokenizer(tokenizer, 'text')\n    sample = {'text': 'HanLP good'}\n    print(transform(sample))\n\n\ndef _test_sequence_transform(tokenizer):\n    transform = TransformerSequenceTokenizer(tokenizer, 'token')\n    sample = {'token': 'HanLP good'.split()}\n    print(transform(sample))\n\n\nif __name__ == '__main__':\n    main()\n", "hanlp/transform/conll_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 15:30\nfrom abc import abstractmethod\nfrom collections import Counter\nfrom typing import Union, Tuple, Iterable, Any, Generator\n\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import PreTrainedTokenizer, PretrainedConfig\n\nfrom hanlp_common.constant import ROOT\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.components.parsers.alg_tf import tolist, kmeans, randperm, arange\nfrom hanlp.components.parsers.conll import read_conll\nfrom hanlp_common.conll import CoNLLWord, CoNLLUWord, CoNLLSentence\nfrom hanlp.layers.transformers.utils_tf import config_is, adjust_tokens_for_transformers, convert_examples_to_features\nfrom hanlp.utils.log_util import logger\nfrom hanlp.utils.string_util import ispunct\nfrom hanlp_common.util import merge_locals_kwargs\n\n\nclass CoNLLTransform(Transform):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, lower=True, n_buckets=32, min_freq=2,\n                 use_pos=True, **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.form_vocab: VocabTF = None\n        if use_pos:\n            self.cpos_vocab: VocabTF = None\n        self.rel_vocab: VocabTF = None\n        self.puncts: tf.Tensor = None\n\n    @property\n    def use_pos(self):\n        return self.config.get('use_pos', True)\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        form, cpos = x\n        return self.form_vocab.token_to_idx_table.lookup(form), self.cpos_vocab.token_to_idx_table.lookup(cpos)\n\n    def y_to_idx(self, y):\n        head, rel = y\n        return head, self.rel_vocab.token_to_idx_table.lookup(rel)\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        if len(X) == 2:\n            form_batch, cposes_batch = X\n            mask = tf.not_equal(form_batch, 0)\n        elif len(X) == 3:\n            form_batch, cposes_batch, mask = X\n        else:\n            raise ValueError(f'Expect X to be 2 or 3 elements but got {repr(X)}')\n        sents = []\n\n        for form_sent, cposes_sent, length in zip(form_batch, cposes_batch,\n                                                  tf.math.count_nonzero(mask, axis=-1)):\n            forms = tolist(form_sent)[1:length + 1]\n            cposes = tolist(cposes_sent)[1:length + 1]\n            sents.append([(self.form_vocab.idx_to_token[f],\n                           self.cpos_vocab.idx_to_token[c]) for f, c in zip(forms, cposes)])\n\n        return sents\n\n    def lock_vocabs(self):\n        super().lock_vocabs()\n        self.puncts = tf.constant([i for s, i in self.form_vocab.token_to_idx.items()\n                                   if ispunct(s)], dtype=tf.int64)\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        assert gold, 'only support gold file for now'\n        use_pos = self.use_pos\n        conllu = filepath.endswith('.conllu')\n        for sent in read_conll(filepath):\n            for i, cell in enumerate(sent):\n                form = cell[1]\n                cpos = cell[3]\n                head = cell[6]\n                deprel = cell[7]\n                # if conllu:\n                #     deps = cell[8]\n                #     deps = [x.split(':', 1) for x in deps.split('|')]\n                #     heads = [int(x[0]) for x in deps if '_' not in x[0] and '.' not in x[0]]\n                #     rels = [x[1] for x in deps if '_' not in x[0] and '.' not in x[0]]\n                #     if head in heads:\n                #         offset = heads.index(head)\n                #         if not self.rel_vocab or rels[offset] in self.rel_vocab:\n                #             deprel = rels[offset]\n                sent[i] = [form, cpos, head, deprel] if use_pos else [form, head, deprel]\n            yield sent\n\n    @property\n    def bos(self):\n        if self.form_vocab.idx_to_token is None:\n            return ROOT\n        return self.form_vocab.idx_to_token[2]\n\n    def input_is_single_sample(self, input: Any) -> bool:\n        if self.use_pos:\n            return isinstance(input[0][0], str) if len(input[0]) else False\n        else:\n            return isinstance(input[0], str) if len(input[0]) else False\n\n    @abstractmethod\n    def batched_inputs_to_batches(self, corpus, indices, shuffle):\n        pass\n\n    def len_of_sent(self, sent):\n        return 1 + len(sent)  # take ROOT into account\n\n    def samples_to_dataset(self, samples: Generator, map_x=None, map_y=None, batch_size=5000, shuffle=None, repeat=None,\n                           drop_remainder=False, prefetch=1, cache=True) -> tf.data.Dataset:\n        if shuffle:\n            def generator():\n                # custom bucketing, load corpus into memory\n                corpus = list(x for x in (samples() if callable(samples) else samples))\n                lengths = [self.len_of_sent(i) for i in corpus]\n                if len(corpus) < 32:\n                    n_buckets = 1\n                else:\n                    n_buckets = min(self.config.n_buckets, len(corpus))\n                buckets = dict(zip(*kmeans(lengths, n_buckets)))\n                sizes, buckets = zip(*[\n                    (size, bucket) for size, bucket in buckets.items()\n                ])\n                # the number of chunks in each bucket, which is clipped by\n                # range [1, len(bucket)]\n                chunks = [min(len(bucket), max(round(size * len(bucket) / batch_size), 1)) for size, bucket in\n                          zip(sizes, buckets)]\n                range_fn = randperm if shuffle else arange\n                max_samples_per_batch = self.config.get('max_samples_per_batch', None)\n                for i in tolist(range_fn(len(buckets))):\n                    split_sizes = [(len(buckets[i]) - j - 1) // chunks[i] + 1\n                                   for j in range(chunks[i])]  # how many sentences in each batch\n                    for batch_indices in tf.split(range_fn(len(buckets[i])), split_sizes):\n                        indices = [buckets[i][j] for j in tolist(batch_indices)]\n                        if max_samples_per_batch:\n                            for j in range(0, len(indices), max_samples_per_batch):\n                                yield from self.batched_inputs_to_batches(corpus, indices[j:j + max_samples_per_batch],\n                                                                          shuffle)\n                        else:\n                            yield from self.batched_inputs_to_batches(corpus, indices, shuffle)\n\n        else:\n            def generator():\n                # custom bucketing, load corpus into memory\n                corpus = list(x for x in (samples() if callable(samples) else samples))\n                n_tokens = 0\n                batch = []\n                for idx, sent in enumerate(corpus):\n                    sent_len = self.len_of_sent(sent)\n                    if n_tokens + sent_len > batch_size and batch:\n                        yield from self.batched_inputs_to_batches(corpus, batch, shuffle)\n                        n_tokens = 0\n                        batch = []\n                    n_tokens += sent_len\n                    batch.append(idx)\n                if batch:\n                    yield from self.batched_inputs_to_batches(corpus, batch, shuffle)\n\n        # next(generator())\n        return Transform.samples_to_dataset(self, generator, False, False, 0, False, repeat, drop_remainder, prefetch,\n                                            cache)\n\n\nclass CoNLL_DEP_Transform(CoNLLTransform):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, lower=True, n_buckets=32,\n                 min_freq=2, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, lower, n_buckets, min_freq, **kwargs)\n\n    def batched_inputs_to_batches(self, corpus, indices, shuffle):\n        \"\"\"Convert batched inputs to batches of samples\n\n        Args:\n          corpus(list): A list of inputs\n          indices(list): A list of indices, each list belongs to a batch\n          shuffle:\n\n        Returns:\n\n\n        \"\"\"\n        raw_batch = [[], [], [], []]\n        for idx in indices:\n            for b in raw_batch:\n                b.append([])\n            for cells in corpus[idx]:\n                for b, c, v in zip(raw_batch, cells,\n                                   [self.form_vocab, self.cpos_vocab, None, self.rel_vocab]):\n                    b[-1].append(v.get_idx_without_add(c) if v else c)\n        batch = []\n        for b, v in zip(raw_batch, [self.form_vocab, self.cpos_vocab, None, self.rel_vocab]):\n            b = tf.keras.preprocessing.sequence.pad_sequences(b, padding='post',\n                                                              value=v.safe_pad_token_idx if v else 0,\n                                                              dtype='int64')\n            batch.append(b)\n        assert len(batch) == 4\n        yield (batch[0], batch[1]), (batch[2], batch[3])\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        types = (tf.int64, tf.int64), (tf.int64, tf.int64)\n        shapes = ([None, None], [None, None]), ([None, None], [None, None])\n        values = (self.form_vocab.safe_pad_token_idx, self.cpos_vocab.safe_pad_token_idx), (\n            0, self.rel_vocab.safe_pad_token_idx)\n        return types, shapes, values\n\n    def inputs_to_samples(self, inputs, gold=False):\n        token_mapping: dict = self.config.get('token_mapping', None)\n        use_pos = self.config.get('use_pos', True)\n        for sent in inputs:\n            sample = []\n            for i, cell in enumerate(sent):\n                if isinstance(cell, tuple):\n                    cell = list(cell)\n                elif isinstance(cell, str):\n                    cell = [cell]\n                if token_mapping:\n                    cell[0] = token_mapping.get(cell[0], cell[0])\n                if self.config['lower']:\n                    cell[0] = cell[0].lower()\n                if not gold:\n                    cell += [0, self.rel_vocab.safe_pad_token]\n                sample.append(cell)\n            # insert root word with arbitrary fields, anyway it will be masked\n            # form, cpos, head, deprel = sample[0]\n            sample.insert(0, [self.bos, self.bos, 0, self.bos] if use_pos else [self.bos, 0, self.bos])\n            yield sample\n\n    def XY_to_inputs_outputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]], Y: Union[tf.Tensor, Tuple[tf.Tensor]],\n                             gold=False, inputs=None, conll=True, arc_scores=None, rel_scores=None) -> Iterable:\n        (words, feats, mask), (arc_preds, rel_preds) = X, Y\n        if inputs is None:\n            inputs = self.X_to_inputs(X)\n        ys = self.Y_to_outputs((arc_preds, rel_preds, mask), inputs=inputs)\n        sents = []\n        for x, y in zip(inputs, ys):\n            sent = CoNLLSentence()\n            for idx, (cell, (head, deprel)) in enumerate(zip(x, y)):\n                if self.use_pos and not self.config.get('joint_pos', None):\n                    form, cpos = cell\n                else:\n                    form, cpos = cell, None\n                if conll:\n                    sent.append(\n                        CoNLLWord(id=idx + 1, form=form, cpos=cpos, head=head, deprel=deprel) if conll == '.conll'\n                        else CoNLLUWord(id=idx + 1, form=form, upos=cpos, head=head, deprel=deprel))\n                else:\n                    sent.append([head, deprel])\n            sents.append(sent)\n        return sents\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        use_pos = self.config.use_pos\n        self.form_vocab = VocabTF()\n        self.form_vocab.add(ROOT)  # make root the 2ed elements while 0th is pad, 1st is unk\n        if self.use_pos:\n            self.cpos_vocab = VocabTF(pad_token=None, unk_token=None)\n        self.rel_vocab = VocabTF(pad_token=None, unk_token=None)\n        num_samples = 0\n        counter = Counter()\n        for sent in self.file_to_samples(trn_path, gold=True):\n            num_samples += 1\n            for idx, cell in enumerate(sent):\n                if use_pos:\n                    form, cpos, head, deprel = cell\n                else:\n                    form, head, deprel = cell\n                if idx == 0:\n                    root = form\n                else:\n                    counter[form] += 1\n                if use_pos:\n                    self.cpos_vocab.add(cpos)\n                self.rel_vocab.add(deprel)\n\n        for token in [token for token, freq in counter.items() if freq >= self.config.min_freq]:\n            self.form_vocab.add(token)\n        return num_samples\n\n    @property\n    def root_rel_idx(self):\n        root_rel_idx = self.config.get('root_rel_idx', None)\n        if root_rel_idx is None:\n            for idx, rel in enumerate(self.rel_vocab.idx_to_token):\n                if 'root' in rel.lower() and rel != self.bos:\n                    self.config['root_rel_idx'] = root_rel_idx = idx\n                    break\n        return root_rel_idx\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None) -> Iterable:\n        arc_preds, rel_preds, mask = Y\n        sents = []\n\n        for arc_sent, rel_sent, length in zip(arc_preds, rel_preds,\n                                              tf.math.count_nonzero(mask, axis=-1)):\n            arcs = tolist(arc_sent)[1:length + 1]\n            rels = tolist(rel_sent)[1:length + 1]\n            sents.append([(a, self.rel_vocab.idx_to_token[r]) for a, r in zip(arcs, rels)])\n\n        return sents\n\n\nclass CoNLL_Transformer_Transform(CoNLL_DEP_Transform):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True,\n                 lower=True, n_buckets=32, min_freq=0, max_seq_length=256, use_pos=False,\n                 mask_p=None, graph=False, topk=None,\n                 **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.tokenizer: PreTrainedTokenizer = None\n        self.transformer_config: PretrainedConfig = None\n        if graph:\n            self.orphan_relation = ROOT\n\n    def lock_vocabs(self):\n        super().lock_vocabs()\n        if self.graph:\n            CoNLL_SDP_Transform._find_orphan_relation(self)\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        if self.config.get('joint_pos', None):\n            self.config.use_pos = True\n        if self.graph:\n            # noinspection PyCallByClass\n            num = CoNLL_SDP_Transform.fit(self, trn_path, **kwargs)\n        else:\n            num = super().fit(trn_path, **kwargs)\n        if self.config.get('topk', None):\n            counter = Counter()\n            for sent in self.file_to_samples(trn_path, gold=True):\n                for idx, cell in enumerate(sent):\n                    form, head, deprel = cell\n                    counter[form] += 1\n            self.topk_vocab = VocabTF()\n            for k, v in counter.most_common(self.config.topk):\n                self.topk_vocab.add(k)\n        return num\n\n    def inputs_to_samples(self, inputs, gold=False):\n        if self.graph:\n            yield from CoNLL_SDP_Transform.inputs_to_samples(self, inputs, gold)\n        else:\n            yield from super().inputs_to_samples(inputs, gold)\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        if self.graph:\n            yield from CoNLL_SDP_Transform.file_to_inputs(self, filepath, gold)\n        else:\n            yield from super().file_to_inputs(filepath, gold)\n\n    @property\n    def mask_p(self) -> float:\n        return self.config.get('mask_p', None)\n\n    @property\n    def graph(self):\n        return self.config.get('graph', None)\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        mask_p = self.mask_p\n        types = (tf.int64, (tf.int64, tf.int64, tf.int64)), (tf.bool if self.graph else tf.int64, tf.int64, tf.int64) if mask_p else (\n            tf.bool if self.graph else tf.int64, tf.int64)\n        if self.graph:\n            shapes = ([None, None], ([None, None], [None, None], [None, None])), (\n                [None, None, None], [None, None, None], [None, None]) if mask_p else (\n                [None, None, None], [None, None, None])\n        else:\n            shapes = ([None, None], ([None, None], [None, None], [None, None])), (\n                [None, None], [None, None], [None, None]) if mask_p else ([None, None], [None, None])\n\n        values = (self.form_vocab.safe_pad_token_idx, (0, 0, 0)), \\\n                 (0, self.rel_vocab.safe_pad_token_idx, 0) if mask_p else (0, self.rel_vocab.safe_pad_token_idx)\n        types_shapes_values = types, shapes, values\n        if self.use_pos:\n            types_shapes_values = [((shapes[0][0], shapes[0][1] + (shapes[0][0],)), shapes[1]) for shapes in\n                                   types_shapes_values]\n        return types_shapes_values\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        form_batch, feat, prefix_mask = X\n        sents = []\n\n        for form_sent, length in zip(form_batch, tf.math.count_nonzero(prefix_mask, axis=-1)):\n            forms = tolist(form_sent)[1:length + 1]\n            sents.append([self.form_vocab.idx_to_token[f] for f in forms])\n\n        return sents\n\n    def batched_inputs_to_batches(self, corpus, indices, shuffle):\n        use_pos = self.use_pos\n        if use_pos:\n            raw_batch = [[], [], [], []]\n        else:\n            raw_batch = [[], [], []]\n        if self.graph:\n            max_len = len(max([corpus[i] for i in indices], key=len))\n            for idx in indices:\n                arc = np.zeros((max_len, max_len), dtype=np.bool)\n                rel = np.zeros((max_len, max_len), dtype=np.int64)\n                for b in raw_batch[:2 if use_pos else 1]:\n                    b.append([])\n                for m, cells in enumerate(corpus[idx]):\n                    if use_pos:\n                        for b, c, v in zip(raw_batch, cells, [None, self.cpos_vocab]):\n                            b[-1].append(v.get_idx_without_add(c) if v else c)\n                    else:\n                        for b, c, v in zip(raw_batch, cells, [None]):\n                            b[-1].append(c)\n                    for n, r in zip(cells[-2], cells[-1]):\n                        arc[m, n] = True\n                        rid = self.rel_vocab.get_idx_without_add(r)\n                        if rid is None:\n                            logger.warning(f'Relation OOV: {r} not exists in train')\n                            continue\n                        rel[m, n] = rid\n                raw_batch[-2].append(arc)\n                raw_batch[-1].append(rel)\n        else:\n            for idx in indices:\n                for s in raw_batch:\n                    s.append([])\n                for cells in corpus[idx]:\n                    if use_pos:\n                        for s, c, v in zip(raw_batch, cells, [None, self.cpos_vocab, None, self.rel_vocab]):\n                            s[-1].append(v.get_idx_without_add(c) if v else c)\n                    else:\n                        for s, c, v in zip(raw_batch, cells, [None, None, self.rel_vocab]):\n                            s[-1].append(v.get_idx_without_add(c) if v else c)\n\n        # Transformer tokenizing\n        config = self.transformer_config\n        tokenizer = self.tokenizer\n        xlnet = config_is(config, 'xlnet')\n        roberta = config_is(config, 'roberta')\n        pad_token = tokenizer.pad_token\n        pad_token_id = tokenizer.convert_tokens_to_ids([pad_token])[0]\n        cls_token = tokenizer.cls_token\n        sep_token = tokenizer.sep_token\n        max_seq_length = self.config.max_seq_length\n        batch_forms = []\n        batch_input_ids = []\n        batch_input_mask = []\n        batch_prefix_offset = []\n        mask_p = self.mask_p\n        if mask_p:\n            batch_masked_offsets = []\n            mask_token_id = tokenizer.mask_token_id\n        for sent_idx, sent in enumerate(raw_batch[0]):\n            batch_forms.append([self.form_vocab.get_idx_without_add(token) for token in sent])\n            sent = adjust_tokens_for_transformers(sent)\n            sent = sent[1:]  # remove <root> use [CLS] instead\n            pad_label_idx = self.form_vocab.pad_idx\n            input_ids, input_mask, segment_ids, prefix_mask = \\\n                convert_examples_to_features(sent,\n                                             max_seq_length,\n                                             tokenizer,\n                                             cls_token_at_end=xlnet,\n                                             # xlnet has a cls token at the end\n                                             cls_token=cls_token,\n                                             cls_token_segment_id=2 if xlnet else 0,\n                                             sep_token=sep_token,\n                                             sep_token_extra=roberta,\n                                             # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                             pad_on_left=xlnet,\n                                             # pad on the left for xlnet\n                                             pad_token_id=pad_token_id,\n                                             pad_token_segment_id=4 if xlnet else 0,\n                                             pad_token_label_id=pad_label_idx,\n                                             do_padding=False)\n            num_masks = sum(prefix_mask)\n            # assert len(sent) == num_masks  # each token has a True subtoken\n            if num_masks < len(sent):  # long sent gets truncated, +1 for root\n                batch_forms[-1] = batch_forms[-1][:num_masks + 1]  # form\n                raw_batch[-1][sent_idx] = raw_batch[-1][sent_idx][:num_masks + 1]  # head\n                raw_batch[-2][sent_idx] = raw_batch[-2][sent_idx][:num_masks + 1]  # rel\n                raw_batch[-3][sent_idx] = raw_batch[-3][sent_idx][:num_masks + 1]  # pos\n            prefix_mask[0] = True  # <root> is now [CLS]\n            prefix_offset = [idx for idx, m in enumerate(prefix_mask) if m]\n            batch_input_ids.append(input_ids)\n            batch_input_mask.append(input_mask)\n            batch_prefix_offset.append(prefix_offset)\n            if mask_p:\n                if shuffle:\n                    size = int(np.ceil(mask_p * len(prefix_offset[1:])))  # never mask [CLS]\n                    mask_offsets = np.random.choice(np.arange(1, len(prefix_offset)), size, replace=False)\n                    for offset in sorted(mask_offsets):\n                        assert 0 < offset < len(input_ids)\n                        # mask_word = raw_batch[0][sent_idx][offset]\n                        # mask_prefix = tokenizer.convert_ids_to_tokens([input_ids[prefix_offset[offset]]])[0]\n                        # assert mask_word.startswith(mask_prefix) or mask_prefix.startswith(\n                        #     mask_word) or mask_prefix == \"'\", \\\n                        #     f'word {mask_word} prefix {mask_prefix} not match'  # could vs couldn\n                        # mask_offsets.append(input_ids[offset]) # subword token\n                        # mask_offsets.append(offset)  # form token\n                        input_ids[prefix_offset[offset]] = mask_token_id  # mask prefix\n                        # whole word masking, mask the rest of the word\n                        for i in range(prefix_offset[offset] + 1, len(input_ids) - 1):\n                            if prefix_mask[i]:\n                                break\n                            input_ids[i] = mask_token_id\n\n                    batch_masked_offsets.append(sorted(mask_offsets))\n                else:\n                    batch_masked_offsets.append([0])  # No masking in prediction\n\n        batch_forms = tf.keras.preprocessing.sequence.pad_sequences(batch_forms, padding='post',\n                                                                    value=self.form_vocab.safe_pad_token_idx,\n                                                                    dtype='int64')\n        batch_input_ids = tf.keras.preprocessing.sequence.pad_sequences(batch_input_ids, padding='post',\n                                                                        value=pad_token_id,\n                                                                        dtype='int64')\n        batch_input_mask = tf.keras.preprocessing.sequence.pad_sequences(batch_input_mask, padding='post',\n                                                                         value=0,\n                                                                         dtype='int64')\n        batch_prefix_offset = tf.keras.preprocessing.sequence.pad_sequences(batch_prefix_offset, padding='post',\n                                                                            value=0,\n                                                                            dtype='int64')\n        batch_heads = tf.keras.preprocessing.sequence.pad_sequences(raw_batch[-2], padding='post',\n                                                                    value=0,\n                                                                    dtype='int64')\n        batch_rels = tf.keras.preprocessing.sequence.pad_sequences(raw_batch[-1], padding='post',\n                                                                   value=self.rel_vocab.safe_pad_token_idx,\n                                                                   dtype='int64')\n        if mask_p:\n            batch_masked_offsets = tf.keras.preprocessing.sequence.pad_sequences(batch_masked_offsets, padding='post',\n                                                                                 value=pad_token_id,\n                                                                                 dtype='int64')\n        feats = (tf.constant(batch_input_ids, dtype='int64'), tf.constant(batch_input_mask, dtype='int64'),\n                 tf.constant(batch_prefix_offset))\n        if use_pos:\n            batch_pos = tf.keras.preprocessing.sequence.pad_sequences(raw_batch[1], padding='post',\n                                                                      value=self.cpos_vocab.safe_pad_token_idx,\n                                                                      dtype='int64')\n            feats += (batch_pos,)\n        yield (batch_forms, feats), \\\n              (batch_heads, batch_rels, batch_masked_offsets) if mask_p else (batch_heads, batch_rels)\n\n    def len_of_sent(self, sent):\n        # Transformer tokenizing\n        config = self.transformer_config\n        tokenizer = self.tokenizer\n        xlnet = config_is(config, 'xlnet')\n        roberta = config_is(config, 'roberta')\n        pad_token = tokenizer.pad_token\n        pad_token_id = tokenizer.convert_tokens_to_ids([pad_token])[0]\n        cls_token = tokenizer.cls_token\n        sep_token = tokenizer.sep_token\n        max_seq_length = self.config.max_seq_length\n        sent = sent[1:]  # remove <root> use [CLS] instead\n        pad_label_idx = self.form_vocab.pad_idx\n        sent = [x[0] for x in sent]\n        sent = adjust_tokens_for_transformers(sent)\n        input_ids, input_mask, segment_ids, prefix_mask = \\\n            convert_examples_to_features(sent,\n                                         max_seq_length,\n                                         tokenizer,\n                                         cls_token_at_end=xlnet,\n                                         # xlnet has a cls token at the end\n                                         cls_token=cls_token,\n                                         cls_token_segment_id=2 if xlnet else 0,\n                                         sep_token=sep_token,\n                                         sep_token_extra=roberta,\n                                         # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n                                         pad_on_left=xlnet,\n                                         # pad on the left for xlnet\n                                         pad_token_id=pad_token_id,\n                                         pad_token_segment_id=4 if xlnet else 0,\n                                         pad_token_label_id=pad_label_idx,\n                                         do_padding=False)\n        return len(input_ids)\n\n    def samples_to_dataset(self, samples: Generator, map_x=None, map_y=None, batch_size=5000, shuffle=None, repeat=None,\n                           drop_remainder=False, prefetch=1, cache=True) -> tf.data.Dataset:\n        if shuffle:\n            return CoNLL_DEP_Transform.samples_to_dataset(self, samples, map_x, map_y, batch_size, shuffle, repeat,\n                                                          drop_remainder, prefetch, cache)\n\n        def generator():\n            # custom bucketing, load corpus into memory\n            corpus = list(x for x in (samples() if callable(samples) else samples))\n            n_tokens = 0\n            batch = []\n            for idx, sent in enumerate(corpus):\n                sent_len = self.len_of_sent(sent)\n                if n_tokens + sent_len > batch_size and batch:\n                    yield from self.batched_inputs_to_batches(corpus, batch, shuffle)\n                    n_tokens = 0\n                    batch = []\n                n_tokens += sent_len\n                batch.append(idx)\n            if batch:\n                yield from self.batched_inputs_to_batches(corpus, batch, shuffle)\n\n        # debug for transformer\n        # next(generator())\n        return Transform.samples_to_dataset(self, generator, False, False, 0, False, repeat, drop_remainder, prefetch,\n                                            cache)\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None) -> Iterable:\n        if self.graph:\n            ys = CoNLL_SDP_Transform.Y_to_outputs(self, Y, gold, inputs, X)\n            ys = [[([t[0] for t in l], [t[1] for t in l]) for l in y] for y in ys]\n            return ys\n        return super().Y_to_outputs(Y, gold, inputs, X)\n\n\nclass CoNLL_SDP_Transform(CoNLLTransform):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, lower=True, n_buckets=32, min_freq=2,\n                 use_pos=True, **kwargs) -> None:\n        super().__init__(config, map_x, map_y, lower, n_buckets, min_freq, use_pos, **kwargs)\n        self.orphan_relation = ROOT\n\n    def lock_vocabs(self):\n        super().lock_vocabs()\n        # heuristic to find the orphan relation\n        self._find_orphan_relation()\n\n    def _find_orphan_relation(self):\n        for rel in self.rel_vocab.idx_to_token:\n            if 'root' in rel.lower():\n                self.orphan_relation = rel\n                break\n\n    def file_to_inputs(self, filepath: str, gold=True):\n        assert gold, 'only support gold file for now'\n        use_pos = self.use_pos\n        conllu = filepath.endswith('.conllu')\n        enhanced_only = self.config.get('enhanced_only', None)\n        for i, sent in enumerate(read_conll(filepath)):\n            parsed_sent = []\n            if conllu:\n                for cell in sent:\n                    ID = cell[0]\n                    form = cell[1]\n                    cpos = cell[3]\n                    head = cell[6]\n                    deprel = cell[7]\n                    deps = cell[8]\n                    deps = [x.split(':', 1) for x in deps.split('|')]\n                    heads = [int(x[0]) for x in deps if x[0].isdigit()]\n                    rels = [x[1] for x in deps if x[0].isdigit()]\n                    if enhanced_only:\n                        if head in heads:\n                            offset = heads.index(head)\n                            heads.pop(offset)\n                            rels.pop(offset)\n                    else:\n                        if head not in heads:\n                            heads.append(head)\n                            rels.append(deprel)\n                    parsed_sent.append([form, cpos, heads, rels] if use_pos else [form, heads, rels])\n            else:\n                prev_cells = None\n                heads = []\n                rels = []\n                for j, cell in enumerate(sent):\n                    ID = cell[0]\n                    form = cell[1]\n                    cpos = cell[3]\n                    head = cell[6]\n                    deprel = cell[7]\n                    if prev_cells and ID != prev_cells[0]:  # found end of token\n                        parsed_sent.append(\n                            [prev_cells[1], prev_cells[2], heads, rels] if use_pos else [prev_cells[1], heads, rels])\n                        heads = []\n                        rels = []\n                    heads.append(head)\n                    rels.append(deprel)\n                    prev_cells = [ID, form, cpos, head, deprel] if use_pos else [ID, form, head, deprel]\n                parsed_sent.append(\n                    [prev_cells[1], prev_cells[2], heads, rels] if use_pos else [prev_cells[1], heads, rels])\n            yield parsed_sent\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        self.form_vocab = VocabTF()\n        self.form_vocab.add(ROOT)  # make root the 2ed elements while 0th is pad, 1st is unk\n        if self.use_pos:\n            self.cpos_vocab = VocabTF(pad_token=None, unk_token=None)\n        self.rel_vocab = VocabTF(pad_token=None, unk_token=None)\n        num_samples = 0\n        counter = Counter()\n        for sent in self.file_to_samples(trn_path, gold=True):\n            num_samples += 1\n            for idx, cell in enumerate(sent):\n                if len(cell) == 4:\n                    form, cpos, head, deprel = cell\n                elif len(cell) == 3:\n                    if self.use_pos:\n                        form, cpos = cell[0]\n                    else:\n                        form = cell[0]\n                    head, deprel = cell[1:]\n                else:\n                    raise ValueError('Unknown data arrangement')\n                if idx == 0:\n                    root = form\n                else:\n                    counter[form] += 1\n                if self.use_pos:\n                    self.cpos_vocab.add(cpos)\n                self.rel_vocab.update(deprel)\n\n        for token in [token for token, freq in counter.items() if freq >= self.config.min_freq]:\n            self.form_vocab.add(token)\n        return num_samples\n\n    def inputs_to_samples(self, inputs, gold=False):\n        use_pos = self.use_pos\n        for sent in inputs:\n            sample = []\n            for i, cell in enumerate(sent):\n                if isinstance(cell, tuple):\n                    cell = list(cell)\n                elif isinstance(cell, str):\n                    cell = [cell]\n                if self.config['lower']:\n                    cell[0] = cell[0].lower()\n                if not gold:\n                    cell += [[0], [self.rel_vocab.safe_pad_token]]\n                sample.append(cell)\n            # insert root word with arbitrary fields, anyway it will be masked\n            if use_pos:\n                form, cpos, head, deprel = sample[0]\n                sample.insert(0, [self.bos, self.bos, [0], deprel])\n            else:\n                form, head, deprel = sample[0]\n                sample.insert(0, [self.bos, [0], deprel])\n            yield sample\n\n    def batched_inputs_to_batches(self, corpus, indices, shuffle):\n        use_pos = self.use_pos\n        raw_batch = [[], [], [], []] if use_pos else [[], [], []]\n        max_len = len(max([corpus[i] for i in indices], key=len))\n        for idx in indices:\n            arc = np.zeros((max_len, max_len), dtype=np.bool)\n            rel = np.zeros((max_len, max_len), dtype=np.int64)\n            for b in raw_batch[:2]:\n                b.append([])\n            for m, cells in enumerate(corpus[idx]):\n                if use_pos:\n                    for b, c, v in zip(raw_batch, cells,\n                                       [self.form_vocab, self.cpos_vocab]):\n                        b[-1].append(v.get_idx_without_add(c))\n                else:\n                    for b, c, v in zip(raw_batch, cells,\n                                       [self.form_vocab]):\n                        b[-1].append(v.get_idx_without_add(c))\n                for n, r in zip(cells[-2], cells[-1]):\n                    arc[m, n] = True\n                    rid = self.rel_vocab.get_idx_without_add(r)\n                    if rid is None:\n                        logger.warning(f'Relation OOV: {r} not exists in train')\n                        continue\n                    rel[m, n] = rid\n            raw_batch[-2].append(arc)\n            raw_batch[-1].append(rel)\n        batch = []\n        for b, v in zip(raw_batch, [self.form_vocab, self.cpos_vocab]):\n            b = tf.keras.preprocessing.sequence.pad_sequences(b, padding='post',\n                                                              value=v.safe_pad_token_idx,\n                                                              dtype='int64')\n            batch.append(b)\n        batch += raw_batch[2:]\n        assert len(batch) == 4\n        yield (batch[0], batch[1]), (batch[2], batch[3])\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        types = (tf.int64, tf.int64), (tf.bool, tf.int64)\n        shapes = ([None, None], [None, None]), ([None, None, None], [None, None, None])\n        values = (self.form_vocab.safe_pad_token_idx, self.cpos_vocab.safe_pad_token_idx), (\n            False, self.rel_vocab.safe_pad_token_idx)\n        return types, shapes, values\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None) -> Iterable:\n        arc_preds, rel_preds, mask = Y\n        sents = []\n\n        for arc_sent, rel_sent, length in zip(arc_preds, rel_preds,\n                                              tf.math.count_nonzero(mask, axis=-1)):\n            sent = []\n            for arc, rel in zip(tolist(arc_sent[1:, 1:]), tolist(rel_sent[1:, 1:])):\n                ar = []\n                for idx, (a, r) in enumerate(zip(arc, rel)):\n                    if a:\n                        ar.append((idx + 1, self.rel_vocab.idx_to_token[r]))\n                if not ar:\n                    # orphan\n                    ar.append((0, self.orphan_relation))\n                sent.append(ar)\n            sents.append(sent)\n\n        return sents\n\n    def XY_to_inputs_outputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]], Y: Union[tf.Tensor, Tuple[tf.Tensor]],\n                             gold=False, inputs=None, conll=True) -> Iterable:\n        (words, feats, mask), (arc_preds, rel_preds) = X, Y\n        xs = inputs\n        ys = self.Y_to_outputs((arc_preds, rel_preds, mask))\n        sents = []\n        for x, y in zip(xs, ys):\n            sent = CoNLLSentence()\n            for idx, ((form, cpos), pred) in enumerate(zip(x, y)):\n                head = [p[0] for p in pred]\n                deprel = [p[1] for p in pred]\n                if conll:\n                    sent.append(CoNLLWord(id=idx + 1, form=form, cpos=cpos, head=head, deprel=deprel))\n                else:\n                    sent.append([head, deprel])\n            sents.append(sent)\n        return sents\n", "hanlp/transform/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 22:24", "hanlp/transform/tsv_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-06-13 21:15\nimport functools\nfrom abc import ABC\nfrom typing import Tuple, Union, Optional, Iterable, List\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\n\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.io_util import generate_words_tags_from_tsv\nfrom hanlp.utils.tf_util import str_tensor_to_str\nfrom hanlp_common.util import merge_locals_kwargs\n\n\ndef dataset_from_tsv(tsv_file_path, word_vocab: VocabTF, char_vocab: VocabTF, tag_vocab: VocabTF, batch_size=32,\n                     shuffle=None, repeat=None, prefetch=1, lower=False, **kwargs):\n    generator = functools.partial(generate_words_tags_from_tsv, tsv_file_path, word_vocab, char_vocab, tag_vocab, lower)\n    return dataset_from_generator(generator, word_vocab, tag_vocab, batch_size, shuffle, repeat, prefetch,\n                                  **kwargs)\n\n\ndef dataset_from_generator(generator, word_vocab, tag_vocab, batch_size=32, shuffle=None, repeat=None, prefetch=1,\n                           **kwargs):\n    shapes = [None], [None]\n    types = tf.string, tf.string\n    defaults = word_vocab.pad_token, tag_vocab.pad_token if tag_vocab.pad_token else tag_vocab.first_token\n    dataset = tf.data.Dataset.from_generator(generator, output_shapes=shapes, output_types=types)\n    if shuffle:\n        if isinstance(shuffle, bool):\n            shuffle = 1024\n        dataset = dataset.shuffle(shuffle)\n    if repeat:\n        dataset = dataset.repeat(repeat)\n    dataset = dataset.padded_batch(batch_size, shapes, defaults).prefetch(prefetch)\n    return dataset\n\n\ndef vocab_from_tsv(tsv_file_path, lower=False, lock_word_vocab=False, lock_char_vocab=True, lock_tag_vocab=True) \\\n        -> Tuple[VocabTF, VocabTF, VocabTF]:\n    word_vocab = VocabTF()\n    char_vocab = VocabTF()\n    tag_vocab = VocabTF(unk_token=None)\n    with open(tsv_file_path, encoding='utf-8') as tsv_file:\n        for line in tsv_file:\n            cells = line.strip().split()\n            if cells:\n                word, tag = cells\n                if lower:\n                    word_vocab.add(word.lower())\n                else:\n                    word_vocab.add(word)\n                char_vocab.update(list(word))\n                tag_vocab.add(tag)\n    if lock_word_vocab:\n        word_vocab.lock()\n    if lock_char_vocab:\n        char_vocab.lock()\n    if lock_tag_vocab:\n        tag_vocab.lock()\n    return word_vocab, char_vocab, tag_vocab\n\n\nclass TsvTaggingFormat(Transform, ABC):\n    def file_to_inputs(self, filepath: str, gold=True):\n        assert gold, 'TsvTaggingFormat does not support reading non-gold files'\n        yield from generate_words_tags_from_tsv(filepath, gold=gold, lower=self.config.get('lower', False),\n                                                max_seq_length=self.max_seq_length)\n\n    @property\n    def max_seq_length(self):\n        return self.config.get('max_seq_length', None)\n\n\nclass TSVTaggingTransform(TsvTaggingFormat, Transform):\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, use_char=False, **kwargs) -> None:\n        super().__init__(**merge_locals_kwargs(locals(), kwargs))\n        self.word_vocab: Optional[VocabTF] = None\n        self.tag_vocab: Optional[VocabTF] = None\n        self.char_vocab: Optional[VocabTF] = None\n\n    def fit(self, trn_path: str, **kwargs) -> int:\n        self.word_vocab = VocabTF()\n        self.tag_vocab = VocabTF(pad_token=None, unk_token=None)\n        num_samples = 0\n        for words, tags in self.file_to_inputs(trn_path, True):\n            self.word_vocab.update(words)\n            self.tag_vocab.update(tags)\n            num_samples += 1\n        if self.char_vocab:\n            self.char_vocab = VocabTF()\n            for word in self.word_vocab.token_to_idx.keys():\n                if word in (self.word_vocab.pad_token, self.word_vocab.unk_token):\n                    continue\n                self.char_vocab.update(list(word))\n        return num_samples\n\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        types = tf.string, tf.string\n        shapes = [None], [None]\n        values = self.word_vocab.pad_token, self.tag_vocab.first_token\n        return types, shapes, values\n\n    def inputs_to_samples(self, inputs, gold=False):\n        lower = self.config.get('lower', False)\n        if gold:\n            if lower:\n                for x, y in inputs:\n                    yield x.lower(), y\n            else:\n                yield from inputs\n        else:\n            for x in inputs:\n                yield x.lower() if lower else x, [self.padding_values[-1]] * len(x)\n\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        return self.word_vocab.lookup(x)\n\n    def y_to_idx(self, y) -> tf.Tensor:\n        return self.tag_vocab.lookup(y)\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        for xs in X:\n            words = []\n            for x in xs:\n                words.append(str_tensor_to_str(x) if self.char_vocab else self.word_vocab.idx_to_token[int(x)])\n            yield words\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False,\n                     inputs=None, X=None, **kwargs) -> Iterable:\n        if not gold:\n            Y = tf.argmax(Y, axis=2)\n        for ys, xs in zip(Y, inputs):\n            tags = []\n            for y, x in zip(ys, xs):\n                tags.append(self.tag_vocab.idx_to_token[int(y)])\n            yield tags\n\n    def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:\n        return isinstance(input[0], str)\n\n    def input_truth_output_to_str(self, input: List[str], truth: List[str], output: List[str]):\n        text = ''\n        for word, gold_tag, pred_tag in zip(input, truth, output):\n            text += ' '.join([word, gold_tag, pred_tag]) + '\\n'\n\n        text += '\\n'\n        return text\n", "hanlp/transform/glue_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 16:34\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.datasets.glu.glue import STANFORD_SENTIMENT_TREEBANK_2_TRAIN, MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV\nfrom hanlp.transform.table_tf import TableTransform\n\n\nclass StanfordSentimentTreebank2Transorm(TableTransform):\n    pass\n\n\nclass MicrosoftResearchParaphraseCorpus(TableTransform):\n\n    def __init__(self, config: SerializableDict = None, map_x=False, map_y=True, x_columns=(3, 4),\n                 y_column=0, skip_header=True, delimiter='auto', **kwargs) -> None:\n        super().__init__(config, map_x, map_y, x_columns, y_column, skip_header, delimiter, **kwargs)\n\n\ndef main():\n    # _test_sst2()\n    _test_mrpc()\n\n\ndef _test_sst2():\n    transform = StanfordSentimentTreebank2Transorm()\n    transform.fit(STANFORD_SENTIMENT_TREEBANK_2_TRAIN)\n    transform.lock_vocabs()\n    transform.label_vocab.summary()\n    transform.build_config()\n    dataset = transform.file_to_dataset(STANFORD_SENTIMENT_TREEBANK_2_TRAIN)\n    for batch in dataset.take(1):\n        print(batch)\n\n\ndef _test_mrpc():\n    transform = MicrosoftResearchParaphraseCorpus()\n    transform.fit(MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV)\n    transform.lock_vocabs()\n    transform.label_vocab.summary()\n    transform.build_config()\n    dataset = transform.file_to_dataset(MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV)\n    for batch in dataset.take(1):\n        print(batch)", "hanlp/common/transform_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-10-27 14:22\nimport inspect\nfrom abc import ABC, abstractmethod\nfrom typing import Generator, Tuple, Union, Iterable, Any\n\nimport tensorflow as tf\n\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp.utils.log_util import logger\n\n\nclass Transform(ABC):\n\n    def __init__(self, config: SerializableDict = None, map_x=True, map_y=True, **kwargs) -> None:\n        super().__init__()\n        self.map_y = map_y\n        self.map_x = map_x\n        if kwargs:\n            if not config:\n                config = SerializableDict()\n            for k, v in kwargs.items():\n                config[k] = v\n        self.config = config\n        self.output_types = None\n        self.output_shapes = None\n        self.padding_values = None\n        # Fix tf memory leak: https://github.com/tensorflow/tensorflow/issues/37653#issuecomment-1000517720\n        self.py_func_set_to_cleanup = set()\n\n    @abstractmethod\n    def fit(self, trn_path: str, **kwargs) -> int:\n        \"\"\"\n        Build the vocabulary from training file\n\n        Parameters\n        ----------\n        trn_path : path to training set\n        kwargs\n\n        Returns\n        -------\n        int\n            How many samples in the training set\n        \"\"\"\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    def build_config(self):\n        \"\"\"\n        By default, call build_types_shapes_values, usually called in component's build method.\n        You can perform other building task here. Remember to call super().build_config\n        \"\"\"\n        self.output_types, self.output_shapes, self.padding_values = self.create_types_shapes_values()\n        # We prefer list over shape here, as it's easier to type [] than ()\n        # if isinstance(self.output_shapes, tuple):\n        #     self.output_shapes = list(self.output_shapes)\n        # for i, shapes in enumerate(self.output_shapes):\n        #     if isinstance(shapes, tuple):\n        #         self.output_shapes[i] = list(shapes)\n        #     for j, shape in enumerate(shapes):\n        #         if isinstance(shape, tuple):\n        #             shapes[j] = list(shape)\n\n    @abstractmethod\n    def create_types_shapes_values(self) -> Tuple[Tuple, Tuple, Tuple]:\n        \"\"\"\n        Create dataset related values,\n        \"\"\"\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    @abstractmethod\n    def file_to_inputs(self, filepath: str, gold=True):\n        \"\"\"\n        Transform file to inputs. The inputs are defined as raw features (e.g. words) to be processed into more\n        features (e.g. forms and characters)\n\n        Parameters\n        ----------\n        filepath\n        gold\n        \"\"\"\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    def inputs_to_samples(self, inputs, gold=False):\n        if gold:\n            yield from inputs\n        else:\n            for x in inputs:\n                yield x, self.padding_values[-1]\n\n    def file_to_samples(self, filepath: str, gold=True):\n        \"\"\"\n        Transform file to samples\n        Parameters\n        ----------\n        filepath\n        gold\n        \"\"\"\n        filepath = get_resource(filepath)\n        inputs = self.file_to_inputs(filepath, gold)\n        yield from self.inputs_to_samples(inputs, gold)\n\n    def file_to_dataset(self, filepath: str, gold=True, map_x=None, map_y=None, batch_size=32, shuffle=None,\n                        repeat=None,\n                        drop_remainder=False,\n                        prefetch=1,\n                        cache=True,\n                        **kwargs) -> tf.data.Dataset:\n        \"\"\"\n        Transform file to dataset\n\n        Parameters\n        ----------\n        filepath\n        gold : bool\n            Whether it's processing gold data or not. Example: there is usually a column for gold answer\n            when gold = True.\n        map_x : bool\n            Whether call map_x or not. Default to self.map_x\n        map_y : bool\n            Whether call map_y or not. Default to self.map_y\n        batch_size\n        shuffle\n        repeat\n        prefetch\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n\n        # debug\n        # for sample in self.file_to_samples(filepath):\n        #     pass\n\n        def generator():\n            inputs = self.file_to_inputs(filepath, gold)\n            samples = self.inputs_to_samples(inputs, gold)\n            yield from samples\n\n        return self.samples_to_dataset(generator, map_x, map_y, batch_size, shuffle, repeat, drop_remainder, prefetch,\n                                       cache)\n\n    def inputs_to_dataset(self, inputs, gold=False, map_x=None, map_y=None, batch_size=32, shuffle=None, repeat=None,\n                          drop_remainder=False,\n                          prefetch=1, cache=False, **kwargs) -> tf.data.Dataset:\n        # debug\n        # for sample in self.inputs_to_samples(inputs):\n        #     pass\n\n        def generator():\n            samples = self.inputs_to_samples(inputs, gold)\n            yield from samples\n\n        return self.samples_to_dataset(generator, map_x, map_y, batch_size, shuffle, repeat, drop_remainder, prefetch,\n                                       cache)\n\n    def samples_to_dataset(self, samples: Generator, map_x=None, map_y=None, batch_size=32, shuffle=None, repeat=None,\n                           drop_remainder=False,\n                           prefetch=1, cache=True) -> tf.data.Dataset:\n        output_types, output_shapes, padding_values = self.output_types, self.output_shapes, self.padding_values\n        if not all(v for v in [output_shapes, output_shapes,\n                               padding_values]):\n            # print('Did you forget to call build_config() on your transform?')\n            self.build_config()\n            output_types, output_shapes, padding_values = self.output_types, self.output_shapes, self.padding_values\n        assert all(v for v in [output_shapes, output_shapes,\n                               padding_values]), 'Your create_types_shapes_values returns None, which is not allowed'\n        # if not callable(samples):\n        #     samples = Transform.generator_to_callable(samples)\n        if not hasattr(tf.compat.v1.get_default_graph(), '_py_funcs_used_in_graph'):\n            tf.compat.v1.get_default_graph()._py_funcs_used_in_graph = []\n        py_func_set_before = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph)\n        dataset = tf.data.Dataset.from_generator(samples, output_types=output_types, output_shapes=output_shapes)\n        if cache:\n            logger.debug('Dataset cache enabled')\n            dataset = dataset.cache(cache if isinstance(cache, str) else '')\n        if shuffle:\n            if isinstance(shuffle, bool):\n                shuffle = 1024\n            dataset = dataset.shuffle(shuffle)\n        if repeat:\n            dataset = dataset.repeat(repeat)\n        if batch_size:\n            dataset = dataset.padded_batch(batch_size, output_shapes, padding_values, drop_remainder)\n        if prefetch:\n            dataset = dataset.prefetch(prefetch)\n        if map_x is None:\n            map_x = self.map_x\n        if map_y is None:\n            map_y = self.map_y\n        if map_x or map_y:\n            def mapper(X, Y):\n                if map_x:\n                    X = self.x_to_idx(X)\n                if map_y:\n                    Y = self.y_to_idx(Y)\n                return X, Y\n\n            dataset = dataset.map(mapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        py_func_set_after = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph) - py_func_set_before\n        self.py_func_set_to_cleanup |= py_func_set_after\n        return dataset\n\n    @abstractmethod\n    def x_to_idx(self, x) -> Union[tf.Tensor, Tuple]:\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    @abstractmethod\n    def y_to_idx(self, y) -> tf.Tensor:\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    def lock_vocabs(self):\n        for key, value in vars(self).items():\n            if isinstance(value, VocabTF):\n                value.lock()\n\n    def summarize_vocabs(self, logger=None, header='Vocab summary:'):\n        output = header + '\\n'\n        vocabs = {}\n        for key, value in vars(self).items():\n            if isinstance(value, VocabTF):\n                vocabs[key] = value\n        # tag vocab comes last usually\n        for key, value in sorted(vocabs.items(), key=lambda kv: len(kv[1]), reverse=True):\n            output += f'{key}' + value.summary(verbose=False) + '\\n'\n        output = output.strip()\n        if logger:\n            logger.info(output)\n        else:\n            print(output)\n\n    @staticmethod\n    def generator_to_callable(generator: Generator):\n        return lambda: (x for x in generator)\n\n    def str_to_idx(self, X, Y) -> Tuple[Union[tf.Tensor, Tuple], tf.Tensor]:\n        return self.x_to_idx(X), self.y_to_idx(Y)\n\n    def X_to_inputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]]) -> Iterable:\n        return [repr(x) for x in X]\n\n    def Y_to_outputs(self, Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False, inputs=None, X=None,\n                     batch=None) -> Iterable:\n        return [repr(y) for y in Y]\n\n    def XY_to_inputs_outputs(self, X: Union[tf.Tensor, Tuple[tf.Tensor]],\n                             Y: Union[tf.Tensor, Tuple[tf.Tensor]], gold=False) -> Iterable:\n        \"\"\"\n        Convert predicted tensors to outputs\n\n        Parameters\n        ----------\n        X : Union[tf.Tensor, Tuple[tf.Tensor]]\n            The inputs of model\n        Y : Union[tf.Tensor, Tuple[tf.Tensor]]\n            The outputs of model\n\n        Returns\n        -------\n\n        \"\"\"\n        return [(x, y) for x, y in zip(self.X_to_inputs(X), self.Y_to_outputs(Y, gold))]\n\n    def input_is_single_sample(self, input: Any) -> bool:\n        return False\n\n    def input_to_inputs(self, input: Any) -> Tuple[Any, bool]:\n        \"\"\"\n        If input is one sample, convert it to a list which contains this unique sample\n\n        Parameters\n        ----------\n        input :\n            sample or samples\n\n        Returns\n        -------\n        (inputs, converted) : Tuple[Any, bool]\n\n        \"\"\"\n        flat = self.input_is_single_sample(input)\n        if flat:\n            input = [input]\n        return input, flat\n\n    def input_truth_output_to_str(self, input, truth, output):\n        \"\"\"\n        Convert input truth output to string representation, usually for writing to file during evaluation\n\n        Parameters\n        ----------\n        input\n        truth\n        output\n\n        Returns\n        -------\n\n        \"\"\"\n        return '\\t'.join([input, truth, output]) + '\\n'\n\n    def cleanup(self):\n        new_py_funcs = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph) - self.py_func_set_to_cleanup\n        tf.compat.v1.get_default_graph()._py_funcs_used_in_graph = list(new_py_funcs)\n        self.py_func_set_to_cleanup = set()\n", "hanlp/common/vocab.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-06-13 22:42\nfrom collections import Counter\nfrom typing import List, Dict, Union, Iterable\n\nfrom hanlp_common.constant import UNK, PAD\nfrom hanlp_common.structure import Serializable\nfrom hanlp_common.reflection import classpath_of\n\n\nclass Vocab(Serializable):\n    def __init__(self, idx_to_token: List[str] = None, token_to_idx: Dict = None, mutable=True, pad_token=PAD,\n                 unk_token=UNK) -> None:\n        \"\"\"Vocabulary base class which converts tokens to indices and vice versa.\n\n        Args:\n            idx_to_token: id to token mapping.\n            token_to_idx: token to id mapping.\n            mutable: ``True`` to allow adding new tokens, ``False`` to map OOV to ``unk``.\n            pad_token: The token representing padding.\n            unk_token: The token representing OOV.\n        \"\"\"\n        super().__init__()\n        if idx_to_token:\n            t2i = dict((token, idx) for idx, token in enumerate(idx_to_token))\n            if token_to_idx:\n                t2i.update(token_to_idx)\n            token_to_idx = t2i\n        if token_to_idx is None:\n            token_to_idx = {}\n            if pad_token is not None:\n                token_to_idx[pad_token] = len(token_to_idx)\n            if unk_token is not None:\n                token_to_idx[unk_token] = token_to_idx.get(unk_token, len(token_to_idx))\n        self.token_to_idx = token_to_idx\n        self.idx_to_token: List[str] = None\n        self.mutable = mutable\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n\n    def __setitem__(self, token: str, idx: int):\n        assert self.mutable, 'Update an immutable Vocab object is not allowed'\n        self.token_to_idx[token] = idx\n\n    def __getitem__(self, key: Union[str, int, List]) -> Union[int, str, List]:\n        \"\"\" Get the index/indices associated with a token or a list of tokens or vice versa.\n\n        Args:\n            key: ``str`` for token(s) and ``int`` for index/indices.\n\n        Returns: Associated indices or tokens.\n\n        \"\"\"\n        if isinstance(key, str):\n            return self.get_idx(key)\n        elif isinstance(key, int):\n            return self.get_token(key)\n        elif isinstance(key, list):\n            if len(key) == 0:\n                return []\n            elif isinstance(key[0], str):\n                return [self.get_idx(x) for x in key]\n            elif isinstance(key[0], int):\n                return [self.get_token(x) for x in key]\n\n    def __contains__(self, key: Union[str, int]):\n        if isinstance(key, str):\n            return key in self.token_to_idx\n        elif isinstance(key, int):\n            return 0 <= key < len(self.idx_to_token)\n        else:\n            return False\n\n    def add(self, token: str) -> int:\n        \"\"\" Tries to add a token into a vocab and returns its id. If it has already been there, its id will be returned\n        and the vocab won't be updated. If the vocab is locked, an assertion failure will occur.\n\n        Args:\n            token: A new or existing token.\n\n        Returns:\n            Its associated id.\n\n        \"\"\"\n        assert self.mutable, 'It is not allowed to call add on an immutable Vocab'\n        assert isinstance(token, str), f'Token type must be str but got {type(token)} from {token}'\n        assert token is not None, 'Token must not be None'\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            idx = len(self.token_to_idx)\n            self.token_to_idx[token] = idx\n        return idx\n\n    def update(self, tokens: Iterable[str]) -> None:\n        \"\"\"Update the vocab with these tokens by adding them to vocab one by one.\n\n        Args:\n          tokens (Iterable[str]): A list of tokens.\n        \"\"\"\n        assert self.mutable, 'It is not allowed to update an immutable Vocab'\n        for token in tokens:\n            self.add(token)\n\n    def get_idx(self, token: str) -> int:\n        \"\"\"Get the idx of a token. If it's not there, it will be added to the vocab when the vocab is locked otherwise\n        the id of UNK will be returned.\n\n        Args:\n            token: A token.\n\n        Returns:\n            The id of that token.\n\n        \"\"\"\n        assert isinstance(token, str), 'token has to be `str`'\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            if self.mutable:\n                idx = len(self.token_to_idx)\n                self.token_to_idx[token] = idx\n            else:\n                idx = self.token_to_idx.get(self.unk_token, None)\n        return idx\n\n    def get_idx_without_add(self, token: str) -> int:\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            idx = self.token_to_idx.get(self.safe_unk_token, None)\n        return idx\n\n    def get_token(self, idx: int) -> str:\n        \"\"\"Get the token using its index.\n\n        Args:\n            idx: The index to a token.\n\n        Returns:\n\n        \"\"\"\n        if self.idx_to_token:\n            return self.idx_to_token[idx]\n\n        if self.mutable:\n            for token in self.token_to_idx:\n                if self.token_to_idx[token] == idx:\n                    return token\n\n    def has_key(self, token):\n        return token in self.token_to_idx\n\n    def __len__(self):\n        return len(self.token_to_idx)\n\n    def lock(self):\n        \"\"\"Lock this vocab up so that it won't accept new tokens.\n\n        Returns:\n            Itself.\n\n        \"\"\"\n        if self.locked:\n            return self\n        self.mutable = False\n        self.build_idx_to_token()\n        return self\n\n    def build_idx_to_token(self):\n        max_idx = max(self.token_to_idx.values())\n        self.idx_to_token = [None] * (max_idx + 1)\n        for token, idx in self.token_to_idx.items():\n            self.idx_to_token[idx] = token\n\n    def unlock(self):\n        \"\"\"Unlock this vocab so that new tokens can be added in.\n\n        Returns:\n            Itself.\n\n        \"\"\"\n        if not self.locked:\n            return\n        self.mutable = True\n        self.idx_to_token = None\n        return self\n\n    @property\n    def locked(self):\n        \"\"\"\n        ``True`` indicates this vocab is locked.\n        \"\"\"\n        return not self.mutable\n\n    @property\n    def unk_idx(self):\n        \"\"\"\n        The index of ``UNK`` token.\n        \"\"\"\n        if self.unk_token is None:\n            return None\n        else:\n            return self.token_to_idx.get(self.unk_token, None)\n\n    @property\n    def pad_idx(self):\n        \"\"\"\n        The index of ``PAD`` token.\n        \"\"\"\n        if self.pad_token is None:\n            return None\n        else:\n            return self.token_to_idx.get(self.pad_token, None)\n\n    @property\n    def tokens(self):\n        \"\"\"\n        A set of all tokens in this vocab.\n        \"\"\"\n        return self.token_to_idx.keys()\n\n    def __str__(self) -> str:\n        return self.token_to_idx.__str__()\n\n    def summary(self, verbose=True) -> str:\n        \"\"\"Get or print a summary of this vocab.\n\n        Args:\n            verbose: ``True`` to print the summary to stdout.\n\n        Returns:\n            Summary in text form.\n\n        \"\"\"\n        # report = 'Length: {}\\n'.format(len(self))\n        # report += 'Samples: {}\\n'.format(str(list(self.token_to_idx.keys())[:min(50, len(self))]))\n        # report += 'Mutable: {}'.format(self.mutable)\n        # report = report.strip()\n        report = '[{}] = '.format(len(self))\n        report += str(list(self.token_to_idx.keys())[:min(50, len(self))])\n        if verbose:\n            print(report)\n        return report\n\n    def __call__(self, some_token: Union[str, Iterable[str]]) -> Union[int, List[int]]:\n        if isinstance(some_token, (list, tuple, set)):\n            indices = []\n            if len(some_token) and isinstance(some_token[0], (list, tuple, set)):\n                for sent in some_token:\n                    inside = []\n                    for token in sent:\n                        inside.append(self.get_idx(token))\n                    indices.append(inside)\n                return indices\n            for token in some_token:\n                indices.append(self.get_idx(token))\n            return indices\n        else:\n            return self.get_idx(some_token)\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert this vocab to a dict so that it can be json serialized.\n\n        Returns:\n            A dict.\n\n        \"\"\"\n        idx_to_token = self.idx_to_token\n        pad_token = self.pad_token\n        unk_token = self.unk_token\n        mutable = self.mutable\n        items = locals().copy()\n        items.pop('self')\n        return items\n\n    def copy_from(self, item: dict):\n        \"\"\"Copy properties from a dict so that it can json de-serialized.\n\n        Args:\n            item: A dict holding ``token_to_idx``\n\n        Returns:\n            Itself.\n\n        \"\"\"\n        for key, value in item.items():\n            setattr(self, key, value)\n        self.token_to_idx = {k: v for v, k in enumerate(self.idx_to_token)}\n        return self\n\n    def lower(self):\n        \"\"\"Convert all tokens to lower case.\n\n        Returns:\n            Itself.\n\n        \"\"\"\n        self.unlock()\n        token_to_idx = self.token_to_idx\n        self.token_to_idx = {}\n        for token in token_to_idx.keys():\n            self.add(token.lower())\n        return self\n\n    @property\n    def first_token(self):\n        \"\"\"The first token in this vocab.\n        \"\"\"\n        if self.idx_to_token:\n            return self.idx_to_token[0]\n        if self.token_to_idx:\n            return next(iter(self.token_to_idx))\n        return None\n\n    def merge(self, other):\n        \"\"\"Merge this with another vocab inplace.\n\n        Args:\n            other (Vocab): Another vocab.\n        \"\"\"\n        for word, idx in other.token_to_idx.items():\n            self.get_idx(word)\n\n    @property\n    def safe_pad_token(self) -> str:\n        \"\"\"Get the pad token safely. It always returns a pad token, which is the pad token or the first token\n        if pad does not present in the vocab.\n        \"\"\"\n        if self.pad_token:\n            return self.pad_token\n        if self.first_token:\n            return self.first_token\n        return PAD\n\n    @property\n    def safe_pad_token_idx(self) -> int:\n        \"\"\"Get the idx to the pad token safely. It always returns an index, which corresponds to the pad token or the\n        first token if pad does not present in the vocab.\n        \"\"\"\n        return self.token_to_idx.get(self.safe_pad_token, 0)\n\n    @property\n    def safe_unk_token(self) -> str:\n        \"\"\"Get the unk token safely. It always returns a unk token, which is the unk token or the first token if unk\n        does not presented in the vocab.\n        \"\"\"\n        if self.unk_token:\n            return self.unk_token\n        if self.first_token:\n            return self.first_token\n        return UNK\n\n    def __repr__(self) -> str:\n        if self.idx_to_token is not None:\n            return self.idx_to_token.__repr__()\n        return self.token_to_idx.__repr__()\n\n    def extend(self, tokens: Iterable[str]):\n        self.unlock()\n        self(tokens)\n\n    def reload_idx_to_token(self, idx_to_token: List[str], pad_idx=0, unk_idx=1):\n        self.idx_to_token = idx_to_token\n        self.token_to_idx = dict((s, i) for i, s in enumerate(idx_to_token))\n        if pad_idx is not None:\n            self.pad_token = idx_to_token[pad_idx]\n        if unk_idx is not None:\n            self.unk_token = idx_to_token[unk_idx]\n\n    def set_unk_as_safe_unk(self):\n        \"\"\"Set ``self.unk_token = self.safe_unk_token``. It's useful when the dev/test set contains OOV labels.\n        \"\"\"\n        self.unk_token = self.safe_unk_token\n\n    def clear(self):\n        self.unlock()\n        self.token_to_idx.clear()\n\n\nclass CustomVocab(Vocab):\n    def to_dict(self) -> dict:\n        d = super().to_dict()\n        d['type'] = classpath_of(self)\n        return d\n\n\nclass LowercaseVocab(CustomVocab):\n    def get_idx(self, token: str) -> int:\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            idx = self.token_to_idx.get(token.lower(), None)\n        if idx is None:\n            if self.mutable:\n                idx = len(self.token_to_idx)\n                self.token_to_idx[token] = idx\n            else:\n                idx = self.token_to_idx.get(self.unk_token, None)\n        return idx\n\n\nclass VocabWithNone(CustomVocab):\n    def get_idx(self, token: str) -> int:\n        if token is None:\n            return -1\n        return super().get_idx(token)\n\n\nclass VocabWithFrequency(CustomVocab):\n\n    def __init__(self, counter: Counter = None, min_occur_cnt=0, pad_token=PAD, unk_token=UNK, specials=None) -> None:\n        super().__init__(None, None, True, pad_token, unk_token)\n        if specials:\n            for each in specials:\n                counter.pop(each, None)\n                self.add(each)\n        self.frequencies = [1] * len(self)\n        if counter:\n            for token, freq in counter.most_common():\n                if freq >= min_occur_cnt:\n                    self.add(token)\n                    self.frequencies.append(freq)\n        self.lock()\n\n    def to_dict(self) -> dict:\n        d = super().to_dict()\n        d['frequencies'] = self.frequencies\n        return d\n\n    def copy_from(self, item: dict):\n        super().copy_from(item)\n        self.frequencies = item['frequencies']\n\n    def get_frequency(self, token):\n        idx = self.get_idx(token)\n        if idx is not None:\n            return self.frequencies[idx]\n        return 0\n\n\nclass VocabCounter(CustomVocab):\n\n    def __init__(self, idx_to_token: List[str] = None, token_to_idx: Dict = None, mutable=True, pad_token=PAD,\n                 unk_token=UNK) -> None:\n        super().__init__(idx_to_token, token_to_idx, mutable, pad_token, unk_token)\n        self.counter = Counter()\n\n    def get_idx(self, token: str) -> int:\n        if self.mutable:\n            self.counter[token] += 1\n        return super().get_idx(token)\n\n    def trim(self, min_frequency):\n        assert self.mutable\n        specials = {self.unk_token, self.pad_token}\n        survivors = list((token, freq) for token, freq in self.counter.most_common()\n                         if freq >= min_frequency and token not in specials)\n        survivors = [(x, -1) for x in specials if x] + survivors\n        self.counter = Counter(dict(survivors))\n        self.token_to_idx = dict()\n        self.idx_to_token = None\n        for token, freq in survivors:\n            idx = len(self.token_to_idx)\n            self.token_to_idx[token] = idx\n\n    def copy_from(self, item: dict):\n        super().copy_from(item)\n        self.counter = Counter(item['counter'].items()) if 'counter' in item else Counter()\n\n    def to_dict(self) -> dict:\n        d = super().to_dict()\n        d['counter'] = dict(self.counter.items())\n        return d\n\n\nclass Vocab3D(CustomVocab):\n    def __call__(self, some_token: Union[str, Iterable[str], Iterable[Iterable[str]]]) \\\n            -> Union[int, List[int], List[List[int]]]:\n        \"\"\"It supports 3D arrays of tokens.\n\n        Args:\n            some_token: Tokens of 1D to 3D\n\n        Returns:\n            A list of indices.\n\n        \"\"\"\n        if isinstance(some_token, (list, tuple, set)):\n            indices = []\n            if len(some_token) and isinstance(some_token[0], (list, tuple, set)):\n                for sent in some_token:\n                    inside = []\n                    for token in sent:\n                        inside.append(self.get_idx(token))\n                    indices.append(inside)\n                return indices\n            for token in some_token:\n                if isinstance(token, str):\n                    indices.append(self.get_idx(token))\n                else:\n                    indices.append([self.get_idx(x) for x in token])\n            return indices\n        else:\n            return self.get_idx(some_token)\n\n\ndef create_label_vocab() -> Vocab:\n    return Vocab(pad_token=None, unk_token=None)\n", "hanlp/common/torch_component.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-08 21:20\nimport logging\nimport os\nimport re\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, List, Union, Callable\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nimport hanlp\nfrom hanlp.common.component import Component\nfrom hanlp.common.dataset import TransformableDataset\nfrom hanlp.common.transform import VocabDict\nfrom hanlp.utils.io_util import get_resource, basename_no_ext\nfrom hanlp.utils.log_util import init_logger, flash\nfrom hanlp.utils.torch_util import cuda_devices, set_seed\nfrom hanlp_common.configurable import Configurable\nfrom hanlp_common.constant import IDX, HANLP_VERBOSE\nfrom hanlp_common.reflection import classpath_of\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp_common.util import merge_dict, isdebugging\n\n\nclass TorchComponent(Component, ABC):\n    def __init__(self, **kwargs) -> None:\n        \"\"\"The base class for all components using PyTorch as backend. It provides common workflows of building vocabs,\n        datasets, dataloaders and models. These workflows are more of a conventional guideline than en-forced\n        protocols, which means subclass has the freedom to override or completely skip some steps.\n\n        Args:\n            **kwargs: Addtional arguments to be stored in the ``config`` property.\n        \"\"\"\n        super().__init__()\n        self.model: Optional[torch.nn.Module] = None\n        self.config = SerializableDict(**kwargs)\n        self.vocabs = VocabDict()\n\n    def _capture_config(self, locals_: Dict,\n                        exclude=(\n                                'trn_data', 'dev_data', 'save_dir', 'kwargs', 'self', 'logger', 'verbose',\n                                'dev_batch_size', '__class__', 'devices', 'eval_trn')):\n        \"\"\"Save arguments to config\n\n        Args:\n          locals_: Dict: \n          exclude:  (Default value = ('trn_data')\n          'dev_data': \n          'save_dir': \n          'kwargs': \n          'self': \n          'logger': \n          'verbose': \n          'dev_batch_size': \n          '__class__': \n          'devices'): \n\n        Returns:\n\n        \n        \"\"\"\n        if 'kwargs' in locals_:\n            locals_.update(locals_['kwargs'])\n        locals_ = dict((k, v) for k, v in locals_.items() if k not in exclude and not k.startswith('_'))\n        self.config.update(locals_)\n        return self.config\n\n    def save_weights(self, save_dir, filename='model.pt', trainable_only=True, **kwargs):\n        \"\"\"Save model weights to a directory.\n\n        Args:\n            save_dir: The directory to save weights into.\n            filename: A file name for weights.\n            trainable_only: ``True`` to only save trainable weights. Useful when the model contains lots of static\n                embeddings.\n            **kwargs: Not used for now.\n        \"\"\"\n        model = self.model_\n        state_dict = model.state_dict()\n        if trainable_only:\n            trainable_names = set(n for n, p in model.named_parameters() if p.requires_grad)\n            state_dict = dict((n, p) for n, p in state_dict.items() if n in trainable_names)\n        torch.save(state_dict, os.path.join(save_dir, filename))\n\n    def load_weights(self, save_dir, filename='model.pt', **kwargs):\n        \"\"\"Load weights from a directory.\n\n        Args:\n            save_dir: The directory to load weights from.\n            filename: A file name for weights.\n            **kwargs: Not used.\n        \"\"\"\n        save_dir = get_resource(save_dir)\n        filename = os.path.join(save_dir, filename)\n        # flash(f'Loading model: {filename} [blink]...[/blink][/yellow]')\n        self.model_.load_state_dict(torch.load(filename, map_location='cpu'), strict=False)\n        # flash('')\n\n    def save_config(self, save_dir, filename='config.json'):\n        \"\"\"Save config into a directory.\n\n        Args:\n            save_dir: The directory to save config.\n            filename: A file name for config.\n        \"\"\"\n        self._savable_config.save_json(os.path.join(save_dir, filename))\n\n    def load_config(self, save_dir, filename='config.json', **kwargs):\n        \"\"\"Load config from a directory.\n\n        Args:\n            save_dir: The directory to load config.\n            filename: A file name for config.\n            **kwargs: K-V pairs to override config.\n        \"\"\"\n        save_dir = get_resource(save_dir)\n        self.config.load_json(os.path.join(save_dir, filename))\n        self.config.update(kwargs)  # overwrite config loaded from disk\n        for k, v in self.config.items():\n            if isinstance(v, dict) and 'classpath' in v:\n                self.config[k] = Configurable.from_config(v)\n        self.on_config_ready(**self.config, save_dir=save_dir)\n\n    def save_vocabs(self, save_dir, filename='vocabs.json'):\n        \"\"\"Save vocabularies to a directory.\n\n        Args:\n            save_dir: The directory to save vocabularies.\n            filename:  The name for vocabularies.\n        \"\"\"\n        if hasattr(self, 'vocabs'):\n            self.vocabs.save_vocabs(save_dir, filename)\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        \"\"\"Load vocabularies from a directory.\n\n        Args:\n            save_dir: The directory to load vocabularies.\n            filename:  The name for vocabularies.\n        \"\"\"\n        if hasattr(self, 'vocabs'):\n            self.vocabs = VocabDict()\n            self.vocabs.load_vocabs(save_dir, filename)\n\n    def save(self, save_dir: str, **kwargs):\n        \"\"\"Save this component to a directory.\n\n        Args:\n            save_dir: The directory to save this component.\n            **kwargs: Not used.\n        \"\"\"\n        self.save_config(save_dir)\n        self.save_vocabs(save_dir)\n        self.save_weights(save_dir)\n\n    def load(self, save_dir: str, devices=None, verbose=HANLP_VERBOSE, **kwargs):\n        \"\"\"Load from a local/remote component.\n\n        Args:\n            save_dir: An identifier which can be a local path or a remote URL or a pre-defined string.\n            devices: The devices this component will be moved onto.\n            verbose: ``True`` to log loading progress.\n            **kwargs: To override some configs.\n        \"\"\"\n        save_dir = get_resource(save_dir)\n        # flash('Loading config and vocabs [blink][yellow]...[/yellow][/blink]')\n        if devices is None and self.model:\n            devices = self.devices\n        self.load_config(save_dir, **kwargs)\n        self.load_vocabs(save_dir)\n        if verbose:\n            flash('Building model [blink][yellow]...[/yellow][/blink]')\n        self.config.pop('training', None)  # Some legacy versions accidentally put training into config file\n        self.model = self.build_model(\n            **merge_dict(self.config, **kwargs, overwrite=True, inplace=True), training=False, save_dir=save_dir)\n        if verbose:\n            flash('')\n        self.load_weights(save_dir, **kwargs)\n        self.to(devices, verbose=verbose)\n        self.model.eval()\n\n    def fit(self,\n            trn_data,\n            dev_data,\n            save_dir,\n            batch_size,\n            epochs,\n            devices=None,\n            logger=None,\n            seed=None,\n            finetune: Union[bool, str] = False,\n            eval_trn=True,\n            _device_placeholder=False,\n            **kwargs):\n        \"\"\"Fit to data, triggers the training procedure. For training set and dev set, they shall be local or remote\n        files.\n\n        Args:\n            trn_data: Training set.\n            dev_data: Development set.\n            save_dir: The directory to save trained component.\n            batch_size: The number of samples in a batch.\n            epochs: Number of epochs.\n            devices: Devices this component will live on.\n            logger: Any :class:`logging.Logger` instance.\n            seed: Random seed to reproduce this training.\n            finetune: ``True`` to load from ``save_dir`` instead of creating a randomly initialized component. ``str``\n                to specify a different ``save_dir`` to load from.\n            eval_trn: Evaluate training set after each update. This can slow down the training but provides a quick\n                diagnostic for debugging.\n            _device_placeholder: ``True`` to create a placeholder tensor which triggers PyTorch to occupy devices so\n                other components won't take these devices as first choices.\n            **kwargs: Hyperparameters used by sub-classes.\n\n        Returns:\n            Any results sub-classes would like to return. Usually the best metrics on training set.\n\n        \"\"\"\n        # Common initialization steps\n        config = self._capture_config(locals())\n        if not logger:\n            logger = self.build_logger('train', save_dir)\n        if seed is None:\n            self.config.seed = 233 if isdebugging() else int(time.time())\n        set_seed(self.config.seed)\n        logger.info(self._savable_config.to_json(sort=True))\n        if isinstance(devices, list) or devices is None or isinstance(devices, float):\n            flash('[yellow]Querying CUDA devices [blink]...[/blink][/yellow]')\n            devices = -1 if isdebugging() else cuda_devices(devices)\n            flash('')\n        # flash(f'Available GPUs: {devices}')\n        if isinstance(devices, list):\n            first_device = (devices[0] if devices else -1)\n        elif isinstance(devices, dict):\n            first_device = next(iter(devices.values()))\n        elif isinstance(devices, int):\n            first_device = devices\n        else:\n            first_device = -1\n        if _device_placeholder and first_device >= 0:\n            _dummy_placeholder = self._create_dummy_placeholder_on(first_device)\n        if finetune:\n            if isinstance(finetune, str):\n                self.load(finetune, devices=devices)\n            else:\n                self.load(save_dir, devices=devices)\n            self.config.finetune = finetune\n            self.vocabs.unlock()  # For extending vocabs\n            logger.info(\n                f'Finetune model loaded with {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}'\n                f'/{sum(p.numel() for p in self.model.parameters())} trainable/total parameters.')\n        self.on_config_ready(**self.config, save_dir=save_dir)\n        trn = self.build_dataloader(**merge_dict(config, data=trn_data, batch_size=batch_size, shuffle=True,\n                                                 training=True, device=first_device, logger=logger, vocabs=self.vocabs,\n                                                 overwrite=True))\n        dev = self.build_dataloader(**merge_dict(config, data=dev_data, batch_size=batch_size, shuffle=False,\n                                                 training=None, device=first_device, logger=logger, vocabs=self.vocabs,\n                                                 overwrite=True)) if dev_data else None\n        flash('[yellow]Building model [blink]...[/blink][/yellow]')\n        self.model = self.build_model(**merge_dict(config, training=True), logger=logger)\n        flash('')\n        logger.info(f'Model built with {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}'\n                    f'/{sum(p.numel() for p in self.model.parameters())} trainable/total parameters.')\n        assert self.model, 'build_model is not properly implemented.'\n        _description = repr(self.model)\n        if len(_description.split('\\n')) < 10:\n            logger.info(_description)\n        self.save_config(save_dir)\n        self.save_vocabs(save_dir)\n        self.to(devices, logger)\n        if _device_placeholder and first_device >= 0:\n            del _dummy_placeholder\n        criterion = self.build_criterion(**merge_dict(config, trn=trn))\n        optimizer = self.build_optimizer(**merge_dict(config, trn=trn, criterion=criterion))\n        metric = self.build_metric(**self.config)\n        if hasattr(trn, 'dataset') and dev and hasattr(dev, 'dataset'):\n            if trn.dataset and dev.dataset:\n                logger.info(f'{len(trn.dataset)}/{len(dev.dataset)} samples in trn/dev set.')\n        if hasattr(trn, '__len__') and dev and hasattr(dev, '__len__'):\n            trn_size = len(trn) // self.config.get('gradient_accumulation', 1)\n            ratio_width = len(f'{trn_size}/{trn_size}')\n        else:\n            ratio_width = None\n        return self.execute_training_loop(**merge_dict(config, trn=trn, dev=dev, epochs=epochs, criterion=criterion,\n                                                       optimizer=optimizer, metric=metric, logger=logger,\n                                                       save_dir=save_dir,\n                                                       devices=devices,\n                                                       ratio_width=ratio_width,\n                                                       trn_data=trn_data,\n                                                       dev_data=dev_data,\n                                                       eval_trn=eval_trn,\n                                                       overwrite=True))\n\n    def build_logger(self, name, save_dir):\n        \"\"\"Build a :class:`logging.Logger`.\n\n        Args:\n            name: The name of this logger.\n            save_dir: The directory this logger should save logs into.\n\n        Returns:\n            logging.Logger: A logger.\n        \"\"\"\n        logger = init_logger(name=name, root_dir=save_dir, level=logging.INFO, fmt=\"%(message)s\")\n        return logger\n\n    @abstractmethod\n    def build_dataloader(self, data, batch_size, shuffle=False, device=None, logger: logging.Logger = None,\n                         **kwargs) -> DataLoader:\n        \"\"\"Build dataloader for training, dev and test sets. It's suggested to build vocabs in this method if they are\n        not built yet.\n\n        Args:\n            data: Data representing samples, which can be a path or a list of samples.\n            batch_size: Number of samples per batch.\n            shuffle: Whether to shuffle this dataloader.\n            device: Device tensors should be loaded onto.\n            logger: Logger for reporting some message if dataloader takes a long time or if vocabs has to be built.\n            **kwargs: Arguments from ``**self.config``.\n        \"\"\"\n        pass\n\n    def build_vocabs(self, trn: torch.utils.data.Dataset, logger: logging.Logger):\n        \"\"\"Override this method to build vocabs.\n\n        Args:\n            trn: Training set.\n            logger: Logger for reporting progress.\n        \"\"\"\n        pass\n\n    @property\n    def _savable_config(self):\n        def convert(k, v):\n            if not isinstance(v, SerializableDict) and hasattr(v, 'config'):\n                v = v.config\n            elif isinstance(v, (set, tuple)):\n                v = list(v)\n            if isinstance(v, dict):\n                v = dict(convert(_k, _v) for _k, _v in v.items())\n            return k, v\n\n        config = SerializableDict(\n            convert(k, v) for k, v in sorted(self.config.items()))\n        config.update({\n            # 'create_time': now_datetime(),\n            'classpath': classpath_of(self),\n            'hanlp_version': hanlp.__version__,\n        })\n        return config\n\n    @abstractmethod\n    def build_optimizer(self, **kwargs):\n        \"\"\"Implement this method to build an optimizer.\n\n        Args:\n            **kwargs: The subclass decides the method signature.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def build_criterion(self, **kwargs):\n        \"\"\"Implement this method to build criterion (loss function).\n\n        Args:\n            **kwargs: The subclass decides the method signature.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def build_metric(self, **kwargs):\n        \"\"\"Implement this to build metric(s).\n\n        Args:\n            **kwargs: The subclass decides the method signature.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,\n                              logger: logging.Logger, devices, ratio_width=None,\n                              **kwargs):\n        \"\"\"Implement this to run training loop.\n\n        Args:\n            trn: Training set.\n            dev: Development set.\n            epochs: Number of epochs.\n            criterion: Loss function.\n            optimizer: Optimizer(s).\n            metric: Metric(s)\n            save_dir: The directory to save this component.\n            logger: Logger for reporting progress.\n            devices: Devices this component and dataloader will live on.\n            ratio_width: The width of dataset size measured in number of characters. Used for logger to align messages.\n            **kwargs: Other hyper-parameters passed from sub-class.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def fit_dataloader(self, trn: DataLoader, criterion, optimizer, metric, logger: logging.Logger, **kwargs):\n        \"\"\"Fit onto a dataloader.\n\n        Args:\n            trn: Training set.\n            criterion: Loss function.\n            optimizer: Optimizer.\n            metric: Metric(s).\n            logger: Logger for reporting progress.\n            **kwargs: Other hyper-parameters passed from sub-class.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def evaluate_dataloader(self, data: DataLoader, criterion: Callable, metric=None, output=False, **kwargs):\n        \"\"\"Evaluate on a dataloader.\n\n        Args:\n            data: Dataloader which can build from any data source.\n            criterion: Loss function.\n            metric: Metric(s).\n            output: Whether to save outputs into some file.\n            **kwargs: Not used.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def build_model(self, training=True, **kwargs) -> torch.nn.Module:\n        \"\"\"Build model.\n\n        Args:\n            training: ``True`` if called during training.\n            **kwargs: ``**self.config``.\n        \"\"\"\n        raise NotImplementedError\n\n    def evaluate(self, tst_data, save_dir=None, logger: logging.Logger = None, batch_size=None, output=False, **kwargs):\n        \"\"\"Evaluate test set.\n\n        Args:\n            tst_data: Test set, which is usually a file path.\n            save_dir: The directory to save evaluation scores or predictions.\n            logger: Logger for reporting progress.\n            batch_size: Batch size for test dataloader.\n            output: Whether to save outputs into some file.\n            **kwargs: Not used.\n\n        Returns:\n            (metric, outputs) where outputs are the return values of ``evaluate_dataloader``.\n        \"\"\"\n        if not self.model:\n            raise RuntimeError('Call fit or load before evaluate.')\n        if isinstance(tst_data, str):\n            tst_data = get_resource(tst_data)\n            filename = os.path.basename(tst_data)\n        else:\n            filename = None\n        if output is True:\n            output = self.generate_prediction_filename(tst_data if isinstance(tst_data, str) else 'test.txt', save_dir)\n        if logger is None:\n            _logger_name = basename_no_ext(filename) if filename else None\n            logger = self.build_logger(_logger_name, save_dir)\n        if not batch_size:\n            batch_size = self.config.get('batch_size', 32)\n        data = self.build_dataloader(**merge_dict(self.config, data=tst_data, batch_size=batch_size, shuffle=False,\n                                                  device=self.devices[0], logger=logger, overwrite=True))\n        dataset = data\n        while dataset and hasattr(dataset, 'dataset'):\n            dataset = dataset.dataset\n        num_samples = len(dataset) if dataset else None\n        if output and isinstance(dataset, TransformableDataset):\n            def add_idx(samples):\n                for idx, sample in enumerate(samples):\n                    if sample:\n                        sample[IDX] = idx\n\n            add_idx(dataset.data)\n            if dataset.cache:\n                add_idx(dataset.cache)\n\n        criterion = self.build_criterion(**self.config)\n        metric = self.build_metric(**self.config)\n        start = time.time()\n        outputs = self.evaluate_dataloader(data, criterion=criterion, filename=filename, output=output, input=tst_data,\n                                           save_dir=save_dir,\n                                           test=True,\n                                           num_samples=num_samples,\n                                           **merge_dict(self.config, batch_size=batch_size, metric=metric,\n                                                        logger=logger, **kwargs))\n        elapsed = time.time() - start\n        if logger:\n            if num_samples:\n                logger.info(f'speed: {num_samples / elapsed:.0f} samples/second')\n            else:\n                logger.info(f'speed: {len(data) / elapsed:.0f} batches/second')\n        return metric, outputs\n\n    def generate_prediction_filename(self, tst_data, save_dir):\n        assert isinstance(tst_data,\n                          str), 'tst_data has be a str in order to infer the output name'\n        output = os.path.splitext(os.path.basename(tst_data))\n        output = os.path.join(save_dir, output[0] + '.pred' + output[1])\n        return output\n\n    def to(self,\n           devices: Union[int, float, List[int], Dict[str, Union[int, torch.device]]] = None,\n           logger: logging.Logger = None, verbose=HANLP_VERBOSE):\n        \"\"\"Move this component to devices.\n\n        Args:\n            devices: Target devices.\n            logger: Logger for printing progress report, as copying a model from CPU to GPU can takes several seconds.\n            verbose: ``True`` to print progress when logger is None.\n        \"\"\"\n        if devices is None:\n            # if getattr(torch, 'has_mps', None):  # mac M1 chips\n            #     devices = torch.device('mps:0')\n            # else:\n            devices = cuda_devices(devices)\n        elif devices == -1 or devices == [-1]:\n            devices = []\n        elif isinstance(devices, (int, float)):\n            devices = cuda_devices(devices)\n        if devices:\n            if logger:\n                logger.info(f'Using GPUs: [on_blue][cyan][bold]{devices}[/bold][/cyan][/on_blue]')\n            if isinstance(devices, list):\n                if verbose:\n                    flash(f'Moving model to GPUs {devices} [blink][yellow]...[/yellow][/blink]')\n                self.model = self.model.to(devices[0])\n                if len(devices) > 1 and not isdebugging() and not isinstance(self.model, nn.DataParallel):\n                    self.model = self.parallelize(devices)\n            elif isinstance(devices, dict):\n                for name, module in self.model.named_modules():\n                    for regex, device in devices.items():\n                        try:\n                            on_device: torch.device = next(module.parameters()).device\n                        except StopIteration:\n                            continue\n                        if on_device == device:\n                            continue\n                        if isinstance(device, int):\n                            if on_device.index == device:\n                                continue\n                        if re.match(regex, name):\n                            if not name:\n                                name = '*'\n                            flash(f'Moving module [yellow]{name}[/yellow] to [on_yellow][magenta][bold]{device}'\n                                  f'[/bold][/magenta][/on_yellow]: [red]{regex}[/red]\\n')\n                            module.to(device)\n            elif isinstance(devices, torch.device):\n                if verbose:\n                    flash(f'Moving model to {devices} [blink][yellow]...[/yellow][/blink]')\n                self.model = self.model.to(devices)\n            else:\n                raise ValueError(f'Unrecognized devices {devices}')\n            if verbose:\n                flash('')\n        else:\n            if logger:\n                logger.info('Using [red]CPU[/red]')\n\n    def parallelize(self, devices: List[Union[int, torch.device]]):\n        return nn.DataParallel(self.model, device_ids=devices)\n\n    @property\n    def devices(self):\n        \"\"\"The devices this component lives on.\n        \"\"\"\n        if self.model is None:\n            return None\n        # next(parser.model.parameters()).device\n        if hasattr(self.model, 'device_ids'):\n            return self.model.device_ids\n        device: torch.device = next(self.model.parameters()).device\n        return [device]\n\n    @property\n    def device(self):\n        \"\"\"The first device this component lives on.\n        \"\"\"\n        devices = self.devices\n        if not devices:\n            return None\n        return devices[0]\n\n    def on_config_ready(self, **kwargs):\n        \"\"\"Called when config is ready, either during ``fit`` or ``load``. Subclass can perform extra initialization\n        tasks in this callback.\n\n        Args:\n            **kwargs: Not used.\n        \"\"\"\n        pass\n\n    @property\n    def model_(self) -> nn.Module:\n        \"\"\"\n        The actual model when it's wrapped by a `DataParallel`\n\n        Returns: The \"real\" model\n\n        \"\"\"\n        if isinstance(self.model, nn.DataParallel):\n            return self.model.module\n        return self.model\n\n    # noinspection PyMethodOverriding\n    @abstractmethod\n    def predict(self, *args, **kwargs):\n        \"\"\"Predict on data fed by user. Users shall avoid directly call this method since it is not guarded with\n        ``torch.no_grad`` and will introduces unnecessary gradient computation. Use ``__call__`` instead.\n\n        Args:\n            *args: Sentences or tokens.\n            **kwargs: Used in sub-classes.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def _create_dummy_placeholder_on(device):\n        if device < 0:\n            device = 'cpu:0'\n        return torch.zeros(16, 16, device=device)\n\n    @torch.no_grad()\n    def __call__(self, *args, **kwargs):\n        \"\"\"Predict on data fed by user. This method calls :meth:`~hanlp.common.torch_component.predict` but decorates\n        it with ``torch.no_grad``.\n\n        Args:\n            *args: Sentences or tokens.\n            **kwargs: Used in sub-classes.\n        \"\"\"\n        return super().__call__(*args, **merge_dict(self.config, overwrite=True, **kwargs))\n", "hanlp/common/keras_component.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 14:45\nimport logging\nimport math\nimport os\nimport sys\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, List, Any, Dict\n\nimport numpy as np\nimport tensorflow as tf\n\nimport hanlp.utils\nfrom hanlp_common.io import save_json, load_json\nfrom hanlp.callbacks.fine_csv_logger import FineCSVLogger\nfrom hanlp.common.component import Component\nfrom hanlp.common.transform_tf import Transform\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.metrics.chunking.iobes_tf import IOBES_F1_TF\nfrom hanlp.optimizers.adamw import AdamWeightDecay\nfrom hanlp.utils import io_util\nfrom hanlp.utils.io_util import get_resource, tempdir_human\nfrom hanlp.utils.log_util import init_logger, logger\nfrom hanlp.utils.string_util import format_scores\nfrom hanlp.utils.tf_util import format_metrics, size_of_dataset, summary_of_model, get_callback_by_class, NumpyEncoder\nfrom hanlp.utils.time_util import Timer, now_datetime\nfrom hanlp_common.reflection import str_to_type, classpath_of\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp_common.util import merge_dict\n\n\nclass KerasComponent(Component, ABC):\n    def __init__(self, transform: Transform) -> None:\n        super().__init__()\n        self.meta = {\n            'class_path': classpath_of(self),\n            'hanlp_version': hanlp.version.__version__,\n        }\n        self.model: Optional[tf.keras.Model] = None\n        self.config = SerializableDict()\n        self.transform = transform\n        # share config with transform for convenience, so we don't need to pass args around\n        if self.transform.config:\n            for k, v in self.transform.config.items():\n                self.config[k] = v\n        self.transform.config = self.config\n\n    def evaluate(self, input_path: str, save_dir=None, output=False, batch_size=128, logger: logging.Logger = None,\n                 callbacks: List[tf.keras.callbacks.Callback] = None, warm_up=True, verbose=True, **kwargs):\n        input_path = get_resource(input_path)\n        file_prefix, ext = os.path.splitext(input_path)\n        name = os.path.basename(file_prefix)\n        if not name:\n            name = 'evaluate'\n        if save_dir and not logger:\n            logger = init_logger(name=name, root_dir=save_dir, level=logging.INFO if verbose else logging.WARN,\n                                 mode='w')\n        tst_data = self.transform.file_to_dataset(input_path, batch_size=batch_size)\n        samples = self.num_samples_in(tst_data)\n        num_batches = math.ceil(samples / batch_size)\n        if warm_up:\n            for x, y in tst_data:\n                self.model.predict_on_batch(x)\n                break\n        if output:\n            assert save_dir, 'Must pass save_dir in order to output'\n            if isinstance(output, bool):\n                output = os.path.join(save_dir, name) + '.predict' + ext\n            elif isinstance(output, str):\n                output = output\n            else:\n                raise RuntimeError('output ({}) must be of type bool or str'.format(repr(output)))\n        timer = Timer()\n        eval_outputs = self.evaluate_dataset(tst_data, callbacks, output, num_batches, **kwargs)\n        loss, score, output = eval_outputs[0], eval_outputs[1], eval_outputs[2]\n        delta_time = timer.stop()\n        speed = samples / delta_time.delta_seconds\n\n        if logger:\n            f1: IOBES_F1_TF = None\n            for metric in self.model.metrics:\n                if isinstance(metric, IOBES_F1_TF):\n                    f1 = metric\n                    break\n            extra_report = ''\n            if f1:\n                overall, by_type, extra_report = f1.state.result(full=True, verbose=False)\n                extra_report = ' \\n' + extra_report\n            logger.info('Evaluation results for {} - '\n                        'loss: {:.4f} - {} - speed: {:.2f} sample/sec{}'\n                        .format(name + ext, loss,\n                                format_scores(score) if isinstance(score, dict) else format_metrics(self.model.metrics),\n                                speed, extra_report))\n        if output:\n            logger.info('Saving output to {}'.format(output))\n            with open(output, 'w', encoding='utf-8') as out:\n                self.evaluate_output(tst_data, out, num_batches, self.model.metrics)\n\n        return loss, score, speed\n\n    def num_samples_in(self, dataset):\n        return size_of_dataset(dataset)\n\n    def evaluate_dataset(self, tst_data, callbacks, output, num_batches, **kwargs):\n        loss, score = self.model.evaluate(tst_data, callbacks=callbacks, steps=num_batches)\n        return loss, score, output\n\n    def evaluate_output(self, tst_data, out, num_batches, metrics: List[tf.keras.metrics.Metric]):\n        # out.write('x\\ty_true\\ty_pred\\n')\n        for metric in metrics:\n            metric.reset_states()\n        for idx, batch in enumerate(tst_data):\n            outputs = self.model.predict_on_batch(batch[0])\n            for metric in metrics:\n                metric(batch[1], outputs, outputs._keras_mask if hasattr(outputs, '_keras_mask') else None)\n            self.evaluate_output_to_file(batch, outputs, out)\n            print('\\r{}/{} {}'.format(idx + 1, num_batches, format_metrics(metrics)), end='')\n        print()\n\n    def evaluate_output_to_file(self, batch, outputs, out):\n        for x, y_gold, y_pred in zip(self.transform.X_to_inputs(batch[0]),\n                                     self.transform.Y_to_outputs(batch[1], gold=True),\n                                     self.transform.Y_to_outputs(outputs, gold=False)):\n            out.write(self.transform.input_truth_output_to_str(x, y_gold, y_pred))\n\n    def _capture_config(self, config: Dict,\n                        exclude=(\n                                'trn_data', 'dev_data', 'save_dir', 'kwargs', 'self', 'logger', 'verbose',\n                                'dev_batch_size', '__class__')):\n        \"\"\"\n        Save arguments to config\n\n        Parameters\n        ----------\n        config\n            `locals()`\n        exclude\n        \"\"\"\n        if 'kwargs' in config:\n            config.update(config['kwargs'])\n        config = dict(\n            (key, tf.keras.utils.serialize_keras_object(value)) if hasattr(value, 'get_config') else (key, value) for\n            key, value in config.items())\n        for key in exclude:\n            config.pop(key, None)\n        self.config.update(config)\n\n    def save_meta(self, save_dir, filename='meta.json', **kwargs):\n        self.meta['create_time']: now_datetime()\n        self.meta.update(kwargs)\n        save_json(self.meta, os.path.join(save_dir, filename))\n\n    def load_meta(self, save_dir, filename='meta.json'):\n        save_dir = get_resource(save_dir)\n        metapath = os.path.join(save_dir, filename)\n        if os.path.isfile(metapath):\n            self.meta.update(load_json(metapath))\n\n    def save_config(self, save_dir, filename='config.json'):\n        self.config.save_json(os.path.join(save_dir, filename))\n\n    def load_config(self, save_dir, filename='config.json'):\n        save_dir = get_resource(save_dir)\n        self.config.load_json(os.path.join(save_dir, filename))\n\n    def save_weights(self, save_dir, filename='model.h5'):\n        self.model.save_weights(os.path.join(save_dir, filename))\n\n    def load_weights(self, save_dir, filename='model.h5', **kwargs):\n        assert self.model.built or self.model.weights, 'You must call self.model.built() in build_model() ' \\\n                                                       'in order to load it'\n        save_dir = get_resource(save_dir)\n        self.model.load_weights(os.path.join(save_dir, filename))\n\n    def save_vocabs(self, save_dir, filename='vocabs.json'):\n        vocabs = SerializableDict()\n        for key, value in vars(self.transform).items():\n            if isinstance(value, VocabTF):\n                vocabs[key] = value.to_dict()\n        vocabs.save_json(os.path.join(save_dir, filename))\n\n    def load_vocabs(self, save_dir, filename='vocabs.json'):\n        save_dir = get_resource(save_dir)\n        vocabs = SerializableDict()\n        vocabs.load_json(os.path.join(save_dir, filename))\n        for key, value in vocabs.items():\n            vocab = VocabTF()\n            vocab.copy_from(value)\n            setattr(self.transform, key, vocab)\n\n    def load_transform(self, save_dir) -> Transform:\n        \"\"\"\n        Try to load transform only. This method might fail due to the fact it avoids building the model.\n        If it do fail, then you have to use `load` which might be too heavy but that's the best we can do.\n        :param save_dir: The path to load.\n        \"\"\"\n        save_dir = get_resource(save_dir)\n        self.load_config(save_dir)\n        self.load_vocabs(save_dir)\n        self.transform.build_config()\n        self.transform.lock_vocabs()\n        return self.transform\n\n    def save(self, save_dir: str, **kwargs):\n        self.save_config(save_dir)\n        self.save_vocabs(save_dir)\n        self.save_weights(save_dir)\n\n    def load(self, save_dir: str, logger=hanlp.utils.log_util.logger, **kwargs):\n        self.meta['load_path'] = save_dir\n        save_dir = get_resource(save_dir)\n        self.load_config(save_dir)\n        self.load_vocabs(save_dir)\n        self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))\n        self.load_weights(save_dir, **kwargs)\n        self.load_meta(save_dir)\n\n    @property\n    def input_shape(self) -> List:\n        return self.transform.output_shapes[0]\n\n    def build(self, logger, **kwargs):\n        self.transform.build_config()\n        self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),\n                                                   loss=kwargs.get('loss', None)))\n        self.transform.lock_vocabs()\n        optimizer = self.build_optimizer(**self.config)\n        loss = self.build_loss(\n            **self.config if 'loss' in self.config else dict(list(self.config.items()) + [('loss', None)]))\n        # allow for different\n        metrics = self.build_metrics(**merge_dict(self.config, metrics=kwargs.get('metrics', 'accuracy'),\n                                                  logger=logger, overwrite=True))\n        if not isinstance(metrics, list):\n            if isinstance(metrics, tf.keras.metrics.Metric):\n                metrics = [metrics]\n        if not self.model.built:\n            sample_inputs = self.sample_data\n            if sample_inputs is not None:\n                self.model(sample_inputs)\n            else:\n                if len(self.transform.output_shapes[0]) == 1 and self.transform.output_shapes[0][0] is None:\n                    x_shape = self.transform.output_shapes[0]\n                else:\n                    x_shape = list(self.transform.output_shapes[0])\n                    for i, shape in enumerate(x_shape):\n                        x_shape[i] = [None] + shape  # batch + X.shape\n                self.model.build(input_shape=x_shape)\n        self.compile_model(optimizer, loss, metrics)\n        return self.model, optimizer, loss, metrics\n\n    def compile_model(self, optimizer, loss, metrics):\n        try:\n            self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics, run_eagerly=self.config.run_eagerly)\n        except ValueError:\n            from keras.saving.object_registration import CustomObjectScope\n            with CustomObjectScope({'adamweightdecay': AdamWeightDecay}):\n                self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics, run_eagerly=self.config.run_eagerly)\n\n    def build_optimizer(self, optimizer, **kwargs) -> tf.keras.optimizers.Optimizer:\n        if isinstance(optimizer, (str, dict)):\n            custom_objects = {'AdamWeightDecay': AdamWeightDecay}\n            try:\n                optimizer = tf.keras.utils.deserialize_keras_object(optimizer, module_objects=vars(tf.keras.optimizers),\n                                                                    custom_objects=custom_objects)\n            except ValueError:\n                optimizer['config'].pop('decay', None)\n                optimizer = tf.keras.utils.deserialize_keras_object(optimizer, module_objects=vars(tf.keras.optimizers),\n                                                                    custom_objects=custom_objects)\n        self.config.optimizer = tf.keras.utils.serialize_keras_object(optimizer)\n        return optimizer\n\n    def build_loss(self, loss, **kwargs):\n        if not loss:\n            loss = tf.keras.losses.SparseCategoricalCrossentropy(\n                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n                from_logits=True)\n        elif isinstance(loss, (str, dict)):\n            loss = tf.keras.utils.deserialize_keras_object(loss, module_objects=vars(tf.keras.losses))\n        if isinstance(loss, tf.keras.losses.Loss):\n            self.config.loss = tf.keras.utils.serialize_keras_object(loss)\n        return loss\n\n    def build_transform(self, **kwargs):\n        return self.transform\n\n    def build_vocab(self, trn_data, logger):\n        train_examples = self.transform.fit(trn_data, **self.config)\n        self.transform.summarize_vocabs(logger)\n        return train_examples\n\n    def build_metrics(self, metrics, logger: logging.Logger, **kwargs):\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n        return [metric]\n\n    @abstractmethod\n    def build_model(self, **kwargs) -> tf.keras.Model:\n        pass\n\n    def fit(self, trn_data, dev_data, save_dir, batch_size, epochs, run_eagerly=False, logger=None, verbose=True,\n            finetune: str = None, **kwargs):\n        self._capture_config(locals())\n        if sys.version_info >= (3, 10):\n            logger.warning(f'Training with TensorFlow {tf.__version__} has not been tested on Python '\n                           f'{sys.version_info.major}.{sys.version_info.minor}. Please downgrade to '\n                           f'Python<=3.9 in case any compatibility issues arise.')\n        self.transform = self.build_transform(**self.config)\n        if not save_dir:\n            save_dir = tempdir_human()\n        if not logger:\n            logger = init_logger(name='train', root_dir=save_dir, level=logging.INFO if verbose else logging.WARN)\n        logger.info('Hyperparameter:\\n' + self.config.to_json())\n        num_examples = self.build_vocab(trn_data, logger)\n        # assert num_examples, 'You forgot to return the number of training examples in your build_vocab'\n        logger.info('Building...')\n        train_steps_per_epoch = math.ceil(num_examples / batch_size) if num_examples else None\n        self.config.train_steps = train_steps_per_epoch * epochs if num_examples else None\n        model, optimizer, loss, metrics = self.build(**merge_dict(self.config, logger=logger, training=True))\n        logger.info('Model built:\\n' + summary_of_model(self.model))\n        if finetune:\n            finetune = get_resource(finetune)\n            if os.path.isdir(finetune):\n                finetune = os.path.join(finetune, 'model.h5')\n            model.load_weights(finetune, by_name=True, skip_mismatch=True)\n            logger.info(f'Loaded pretrained weights from {finetune} for finetuning')\n        self.save_config(save_dir)\n        self.save_vocabs(save_dir)\n        self.save_meta(save_dir)\n        trn_data = self.build_train_dataset(trn_data, batch_size, num_examples)\n        dev_data = self.build_valid_dataset(dev_data, batch_size)\n        callbacks = self.build_callbacks(save_dir, **merge_dict(self.config, overwrite=True, logger=logger))\n        # need to know #batches, otherwise progbar crashes\n        dev_steps = math.ceil(self.num_samples_in(dev_data) / batch_size)\n        checkpoint = get_callback_by_class(callbacks, tf.keras.callbacks.ModelCheckpoint)\n        timer = Timer()\n        try:\n            history = self.train_loop(**merge_dict(self.config, trn_data=trn_data, dev_data=dev_data, epochs=epochs,\n                                                   num_examples=num_examples,\n                                                   train_steps_per_epoch=train_steps_per_epoch, dev_steps=dev_steps,\n                                                   callbacks=callbacks, logger=logger, model=model, optimizer=optimizer,\n                                                   loss=loss,\n                                                   metrics=metrics, overwrite=True))\n        except KeyboardInterrupt:\n            print()\n            if not checkpoint or checkpoint.best in (np.Inf, -np.Inf):\n                self.save_weights(save_dir)\n                logger.info('Aborted with model saved')\n            else:\n                logger.info(f'Aborted with model saved with best {checkpoint.monitor} = {checkpoint.best:.4f}')\n            # noinspection PyTypeChecker\n            history: tf.keras.callbacks.History() = get_callback_by_class(callbacks, tf.keras.callbacks.History)\n        delta_time = timer.stop()\n        best_epoch_ago = 0\n        if history and hasattr(history, 'epoch'):\n            trained_epoch = len(history.epoch)\n            logger.info('Trained {} epochs in {}, each epoch takes {}'.\n                        format(trained_epoch, delta_time, delta_time / trained_epoch if trained_epoch else delta_time))\n            save_json(history.history, io_util.path_join(save_dir, 'history.json'), cls=NumpyEncoder)\n            monitor_history: List = history.history.get(checkpoint.monitor, None)\n            if monitor_history:\n                best_epoch_ago = len(monitor_history) - monitor_history.index(checkpoint.best)\n            if checkpoint and monitor_history and checkpoint.best != monitor_history[-1]:\n                logger.info(f'Restored the best model saved with best '\n                            f'{checkpoint.monitor} = {checkpoint.best:.4f} '\n                            f'saved {best_epoch_ago} epochs ago')\n                self.load_weights(save_dir)  # restore best model\n        return history\n\n    def train_loop(self, trn_data, dev_data, epochs, num_examples, train_steps_per_epoch, dev_steps, model, optimizer,\n                   loss, metrics, callbacks,\n                   logger, **kwargs):\n        history = self.model.fit(trn_data, epochs=epochs, steps_per_epoch=train_steps_per_epoch,\n                                 validation_data=dev_data,\n                                 callbacks=callbacks,\n                                 validation_steps=dev_steps,\n                                 )  # type:tf.keras.callbacks.History\n        return history\n\n    def build_valid_dataset(self, dev_data, batch_size):\n        dev_data = self.transform.file_to_dataset(dev_data, batch_size=batch_size, shuffle=False)\n        return dev_data\n\n    def build_train_dataset(self, trn_data, batch_size, num_examples):\n        trn_data = self.transform.file_to_dataset(trn_data, batch_size=batch_size,\n                                                  shuffle=True,\n                                                  repeat=-1 if self.config.train_steps else None)\n        return trn_data\n\n    def build_callbacks(self, save_dir, logger, **kwargs):\n        metrics = kwargs.get('metrics', 'accuracy')\n        if isinstance(metrics, (list, tuple)):\n            metrics = metrics[-1]\n        monitor = f'val_{metrics}'\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n            os.path.join(save_dir, 'model.h5'),\n            # verbose=1,\n            monitor=monitor, save_best_only=True,\n            mode='max',\n            save_weights_only=True)\n        logger.debug(f'Monitor {checkpoint.monitor} for checkpoint')\n        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n            log_dir=io_util.makedirs(io_util.path_join(save_dir, 'logs')))\n        csv_logger = FineCSVLogger(os.path.join(save_dir, 'train.log'), separator=' | ', append=True)\n        callbacks = [checkpoint, tensorboard_callback, csv_logger]\n        lr_decay_per_epoch = self.config.get('lr_decay_per_epoch', None)\n        if lr_decay_per_epoch:\n            learning_rate = self.model.optimizer.get_config().get('learning_rate', None)\n            if not learning_rate:\n                logger.warning('Learning rate decay not supported for optimizer={}'.format(repr(self.model.optimizer)))\n            else:\n                logger.debug(f'Created LearningRateScheduler with lr_decay_per_epoch={lr_decay_per_epoch}')\n                callbacks.append(tf.keras.callbacks.LearningRateScheduler(\n                    lambda epoch: learning_rate / (1 + lr_decay_per_epoch * epoch)))\n        anneal_factor = self.config.get('anneal_factor', None)\n        if anneal_factor:\n            callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(factor=anneal_factor,\n                                                                  patience=self.config.get('anneal_patience', 10)))\n        early_stopping_patience = self.config.get('early_stopping_patience', None)\n        if early_stopping_patience:\n            callbacks.append(tf.keras.callbacks.EarlyStopping(monitor=monitor, mode='max',\n                                                              verbose=1,\n                                                              patience=early_stopping_patience))\n        return callbacks\n\n    def on_train_begin(self):\n        \"\"\"\n        Callback before the training starts\n        \"\"\"\n        pass\n\n    def predict(self, data: Any, batch_size=None, **kwargs):\n        assert self.model, 'Please call fit or load before predict'\n        if not data:\n            return []\n        data, flat = self.transform.input_to_inputs(data)\n\n        if not batch_size:\n            batch_size = self.config.batch_size\n\n        dataset = self.transform.inputs_to_dataset(data, batch_size=batch_size, gold=kwargs.get('gold', False))\n\n        results = []\n        num_samples = 0\n        data_is_list = isinstance(data, list)\n        for idx, batch in enumerate(dataset):\n            samples_in_batch = tf.shape(batch[-1] if isinstance(batch[-1], tf.Tensor) else batch[-1][0])[0]\n            if data_is_list:\n                inputs = data[num_samples:num_samples + samples_in_batch]\n            else:\n                inputs = None  # if data is a generator, it's usually one-time, not able to transform into a list\n            for output in self.predict_batch(batch, inputs=inputs, **kwargs):\n                results.append(output)\n            num_samples += samples_in_batch\n        self.transform.cleanup()\n\n        if flat:\n            return results[0]\n        return results\n\n    def predict_batch(self, batch, inputs=None, **kwargs):\n        X = batch[0]\n        Y = self.model.predict_on_batch(X)\n        for output in self.transform.Y_to_outputs(Y, X=X, inputs=inputs, batch=batch, **kwargs):\n            yield output\n\n    @property\n    def sample_data(self):\n        return None\n\n    @staticmethod\n    def from_meta(meta: dict, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        meta\n        kwargs\n\n        Returns\n        -------\n        KerasComponent\n\n        \"\"\"\n        cls = str_to_type(meta['class_path'])\n        obj: KerasComponent = cls()\n        assert 'load_path' in meta, f'{meta} doesn\\'t contain load_path field'\n        obj.load(meta['load_path'])\n        return obj\n\n    def export_model_for_serving(self, export_dir=None, version=1, overwrite=False, show_hint=False):\n        assert self.model, 'You have to fit or load a model before exporting it'\n        if not export_dir:\n            assert 'load_path' in self.meta, 'When not specifying save_dir, load_path has to present'\n            export_dir = get_resource(self.meta['load_path'])\n        model_path = os.path.join(export_dir, str(version))\n        if os.path.isdir(model_path) and not overwrite:\n            logger.info(f'{model_path} exists, skip since overwrite = {overwrite}')\n            return export_dir\n        logger.info(f'Exporting to {export_dir} ...')\n        tf.saved_model.save(self.model, model_path)\n        logger.info(f'Successfully exported model to {export_dir}')\n        if show_hint:\n            logger.info(f'You can serve it through \\n'\n                        f'tensorflow_model_server --model_name={os.path.splitext(os.path.basename(self.meta[\"load_path\"]))[0]} '\n                        f'--model_base_path={export_dir} --rest_api_port=8888')\n        return export_dir\n\n    def serve(self, export_dir=None, grpc_port=8500, rest_api_port=0, overwrite=False, dry_run=False):\n        export_dir = self.export_model_for_serving(export_dir, show_hint=False, overwrite=overwrite)\n        if not dry_run:\n            del self.model  # free memory\n        logger.info('The inputs of exported model is shown below.')\n        os.system(f'saved_model_cli show --all --dir {export_dir}/1')\n        cmd = f'nohup tensorflow_model_server --model_name={os.path.splitext(os.path.basename(self.meta[\"load_path\"]))[0]} ' \\\n              f'--model_base_path={export_dir} --port={grpc_port} --rest_api_port={rest_api_port} ' \\\n              f'>serve.log 2>&1 &'\n        logger.info(f'Running ...\\n{cmd}')\n        if not dry_run:\n            os.system(cmd)\n", "hanlp/common/vocab_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-06-13 22:42\nfrom typing import List, Dict, Union, Iterable\n\nfrom hanlp_common.structure import Serializable\nfrom hanlp_common.constant import PAD, UNK\nimport tensorflow as tf\nfrom tensorflow.python.ops.lookup_ops import index_table_from_tensor\n\n\nclass VocabTF(Serializable):\n    def __init__(self, idx_to_token: List[str] = None, token_to_idx: Dict = None, mutable=True, pad_token=PAD,\n                 unk_token=UNK) -> None:\n        super().__init__()\n        if idx_to_token:\n            t2i = dict((token, idx) for idx, token in enumerate(idx_to_token))\n            if token_to_idx:\n                t2i.update(token_to_idx)\n            token_to_idx = t2i\n        if token_to_idx is None:\n            token_to_idx = {}\n            if pad_token:\n                token_to_idx[pad_token] = len(token_to_idx)\n            if unk_token:\n                token_to_idx[unk_token] = len(token_to_idx)\n        self.token_to_idx = token_to_idx\n        self.idx_to_token: list = None\n        self.mutable = mutable\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.token_to_idx_table: tf.lookup.StaticHashTable = None\n        self.idx_to_token_table = None\n\n    def __setitem__(self, token: str, idx: int):\n        assert self.mutable, 'Update an immutable Vocab object is not allowed'\n        self.token_to_idx[token] = idx\n\n    def __getitem__(self, key: Union[str, int, List]) -> Union[int, str, List]:\n        if isinstance(key, str):\n            return self.get_idx(key)\n        elif isinstance(key, int):\n            return self.get_token(key)\n        elif isinstance(key, list):\n            if len(key) == 0:\n                return []\n            elif isinstance(key[0], str):\n                return [self.get_idx(x) for x in key]\n            elif isinstance(key[0], int):\n                return [self.get_token(x) for x in key]\n\n    def __contains__(self, key: Union[str, int]):\n        if isinstance(key, str):\n            return key in self.token_to_idx\n        elif isinstance(key, int):\n            return 0 <= key < len(self.idx_to_token)\n        else:\n            return False\n\n    def add(self, token: str) -> int:\n        assert self.mutable, 'It is not allowed to call add on an immutable Vocab'\n        assert isinstance(token, str), f'Token type must be str but got {type(token)} from {token}'\n        assert token, 'Token must not be None or length 0'\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            idx = len(self.token_to_idx)\n            self.token_to_idx[token] = idx\n        return idx\n\n    def update(self, tokens: Iterable[str]) -> None:\n        \"\"\"Update the vocab with these tokens by adding them to vocab one by one.\n\n        Args:\n          tokens: Iterable[str]: \n\n        Returns:\n\n        \n        \"\"\"\n        assert self.mutable, 'It is not allowed to update an immutable Vocab'\n        for token in tokens:\n            self.add(token)\n\n    def get_idx(self, token: str) -> int:\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            if self.mutable:\n                idx = len(self.token_to_idx)\n                self.token_to_idx[token] = idx\n            else:\n                idx = self.token_to_idx.get(self.unk_token, None)\n        return idx\n\n    def get_idx_without_add(self, token: str) -> int:\n        idx = self.token_to_idx.get(token, None)\n        if idx is None:\n            idx = self.token_to_idx.get(self.safe_unk_token, None)\n        return idx\n\n    def get_token(self, idx: int) -> str:\n        if self.idx_to_token:\n            return self.idx_to_token[idx]\n\n        if self.mutable:\n            for token in self.token_to_idx:\n                if self.token_to_idx[token] == idx:\n                    return token\n\n    def has_key(self, token):\n        return token in self.token_to_idx\n\n    def __len__(self):\n        return len(self.token_to_idx)\n\n    def lock(self):\n        if self.locked:\n            return self\n        self.mutable = False\n        self.build_idx_to_token()\n        self.build_lookup_table()\n        return self\n\n    def build_idx_to_token(self):\n        max_idx = max(self.token_to_idx.values())\n        self.idx_to_token = [None] * (max_idx + 1)\n        for token, idx in self.token_to_idx.items():\n            self.idx_to_token[idx] = token\n\n    def build_lookup_table(self):\n        tensor = tf.constant(self.idx_to_token, dtype=tf.string)\n        self.token_to_idx_table = index_table_from_tensor(tensor, num_oov_buckets=1 if self.unk_idx is None else 0,\n                                                          default_value=-1 if self.unk_idx is None else self.unk_idx)\n        # self.idx_to_token_table = index_to_string_table_from_tensor(self.idx_to_token, self.safe_unk_token)\n\n    def unlock(self):\n        if not self.locked:\n            return\n        self.mutable = True\n        self.idx_to_token = None\n        self.idx_to_token_table = None\n        self.token_to_idx_table = None\n        return self\n\n    @property\n    def locked(self):\n        return not self.mutable\n\n    @property\n    def unk_idx(self):\n        if self.unk_token is None:\n            return None\n        else:\n            return self.token_to_idx.get(self.unk_token, None)\n\n    @property\n    def pad_idx(self):\n        if self.pad_token is None:\n            return None\n        else:\n            return self.token_to_idx.get(self.pad_token, None)\n\n    @property\n    def tokens(self):\n        return self.token_to_idx.keys()\n\n    def __str__(self) -> str:\n        return self.token_to_idx.__str__()\n\n    def summary(self, verbose=True) -> str:\n        # report = 'Length: {}\\n'.format(len(self))\n        # report += 'Samples: {}\\n'.format(str(list(self.token_to_idx.keys())[:min(50, len(self))]))\n        # report += 'Mutable: {}'.format(self.mutable)\n        # report = report.strip()\n        report = '[{}] = '.format(len(self))\n        report += str(list(self.token_to_idx.keys())[:min(50, len(self))])\n        if verbose:\n            print(report)\n        return report\n\n    def __call__(self, some_token: Union[str, List[str]]) -> Union[int, List[int]]:\n        if isinstance(some_token, list):\n            indices = []\n            for token in some_token:\n                indices.append(self.get_idx(token))\n            return indices\n        else:\n            return self.get_idx(some_token)\n\n    def lookup(self, token_tensor: tf.Tensor) -> tf.Tensor:\n        if self.mutable:\n            self.lock()\n        return self.token_to_idx_table.lookup(token_tensor)\n\n    def to_dict(self) -> dict:\n        idx_to_token = self.idx_to_token\n        pad_token = self.pad_token\n        unk_token = self.unk_token\n        mutable = self.mutable\n        items = locals().copy()\n        items.pop('self')\n        return items\n\n    def copy_from(self, item: dict):\n        for key, value in item.items():\n            setattr(self, key, value)\n        self.token_to_idx = {k: v for v, k in enumerate(self.idx_to_token)}\n        if not self.mutable:\n            self.build_lookup_table()\n\n    def lower(self):\n        self.unlock()\n        token_to_idx = self.token_to_idx\n        self.token_to_idx = {}\n        for token in token_to_idx.keys():\n            self.add(token.lower())\n        return self\n\n    @property\n    def first_token(self):\n        if self.idx_to_token:\n            return self.idx_to_token[0]\n        if self.token_to_idx:\n            return next(iter(self.token_to_idx))\n        return None\n\n    def merge(self, other):\n        for word, idx in other.token_to_idx.items():\n            self.get_idx(word)\n\n    @property\n    def safe_pad_token(self) -> str:\n        \"\"\"Get the pad token safely. It always returns a pad token, which is the token\n        closest to pad if not presented in the vocab.\n\n        Args:\n\n        Returns:\n\n        \n        \"\"\"\n        if self.pad_token:\n            return self.pad_token\n        if self.first_token:\n            return self.first_token\n        return PAD\n\n    @property\n    def safe_pad_token_idx(self) -> int:\n        return self.token_to_idx.get(self.safe_pad_token, 0)\n\n    @property\n    def safe_unk_token(self) -> str:\n        \"\"\"Get the unk token safely. It always returns a unk token, which is the token\n        closest to unk if not presented in the vocab.\n\n        Args:\n\n        Returns:\n\n        \n        \"\"\"\n        if self.unk_token:\n            return self.unk_token\n        if self.first_token:\n            return self.first_token\n        return UNK\n\n\ndef create_label_vocab() -> VocabTF:\n    return VocabTF(pad_token=None, unk_token=None)\n", "hanlp/common/component.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 14:45\nimport inspect\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nfrom hanlp_common.configurable import Configurable\n\n\nclass Component(Configurable, ABC):\n    @abstractmethod\n    def predict(self, *args, **kwargs):\n        \"\"\"Predict on data. This is the base class for all components, including rule based and statistical ones.\n\n        Args:\n          *args: Any type of data subject to sub-classes\n          **kwargs: Additional arguments\n\n        Returns: Any predicted annotations.\n\n        \"\"\"\n        raise NotImplementedError('%s.%s()' % (self.__class__.__name__, inspect.stack()[0][3]))\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        A shortcut for :func:`~hanlp.common.component.predict`.\n\n        Args:\n          *args: Any type of data subject to sub-classes\n          **kwargs: Additional arguments\n\n        Returns: Any predicted annotations.\n\n        \"\"\"\n        return self.predict(*args, **kwargs)\n", "hanlp/common/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 14:45\n", "hanlp/common/transform.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-05-03 14:44\nimport logging\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, Union, List\n\nfrom hanlp_common.constant import EOS, PAD\nfrom hanlp_common.structure import SerializableDict\nfrom hanlp_common.configurable import Configurable\nfrom hanlp.common.vocab import Vocab\nfrom hanlp.utils.io_util import get_resource\nfrom hanlp_common.io import load_json\nfrom hanlp_common.reflection import classpath_of, str_to_type\nfrom hanlp.utils.string_util import ispunct\n\n\nclass ToIndex(ABC):\n\n    def __init__(self, vocab: Vocab = None) -> None:\n        super().__init__()\n        if vocab is None:\n            vocab = Vocab()\n        self.vocab = vocab\n\n    @abstractmethod\n    def __call__(self, sample):\n        pass\n\n    def save_vocab(self, save_dir, filename='vocab.json'):\n        vocab = SerializableDict()\n        vocab.update(self.vocab.to_dict())\n        vocab.save_json(os.path.join(save_dir, filename))\n\n    def load_vocab(self, save_dir, filename='vocab.json'):\n        save_dir = get_resource(save_dir)\n        vocab = SerializableDict()\n        vocab.load_json(os.path.join(save_dir, filename))\n        self.vocab.copy_from(vocab)\n\n\nclass FieldToIndex(ToIndex):\n\n    def __init__(self, src, vocab: Vocab, dst=None) -> None:\n        super().__init__(vocab)\n        self.src = src\n        if not dst:\n            dst = f'{src}_id'\n        self.dst = dst\n\n    def __call__(self, sample: dict):\n        sample[self.dst] = self.vocab(sample[self.src])\n        return sample\n\n    def save_vocab(self, save_dir, filename=None):\n        if not filename:\n            filename = f'{self.dst}_vocab.json'\n        super().save_vocab(save_dir, filename)\n\n    def load_vocab(self, save_dir, filename=None):\n        if not filename:\n            filename = f'{self.dst}_vocab.json'\n        super().load_vocab(save_dir, filename)\n\n\nclass VocabList(list):\n\n    def __init__(self, *fields) -> None:\n        super().__init__()\n        for each in fields:\n            self.append(FieldToIndex(each))\n\n    def append(self, item: Union[str, Tuple[str, Vocab], Tuple[str, str, Vocab], FieldToIndex]) -> None:\n        if isinstance(item, str):\n            item = FieldToIndex(item)\n        elif isinstance(item, (list, tuple)):\n            if len(item) == 2:\n                item = FieldToIndex(src=item[0], vocab=item[1])\n            elif len(item) == 3:\n                item = FieldToIndex(src=item[0], dst=item[1], vocab=item[2])\n            else:\n                raise ValueError(f'Unsupported argument length: {item}')\n        elif isinstance(item, FieldToIndex):\n            pass\n        else:\n            raise ValueError(f'Unsupported argument type: {item}')\n        super(self).append(item)\n\n    def save_vocab(self, save_dir):\n        for each in self:\n            each.save_vocab(save_dir, None)\n\n    def load_vocab(self, save_dir):\n        for each in self:\n            each.load_vocab(save_dir, None)\n\n\nclass VocabDict(SerializableDict):\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"A dict holding :class:`hanlp.common.vocab.Vocab` instances. When used as a transform, it transforms the field\n        corresponding to each :class:`hanlp.common.vocab.Vocab` into indices.\n\n        Args:\n            *args: A list of vocab names.\n            **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.\n        \"\"\"\n        vocabs = dict(kwargs)\n        for each in args:\n            vocabs[each] = Vocab()\n        super().__init__(vocabs)\n\n    def save_vocabs(self, save_dir, filename='vocabs.json'):\n        \"\"\"Save vocabularies to a directory.\n\n        Args:\n            save_dir: The directory to save vocabularies.\n            filename:  The name for vocabularies.\n        \"\"\"\n        vocabs = SerializableDict()\n        for key, value in self.items():\n            if isinstance(value, Vocab):\n                vocabs[key] = value.to_dict()\n        vocabs.save_json(os.path.join(save_dir, filename))\n\n    def load_vocabs(self, save_dir, filename='vocabs.json', vocab_cls=Vocab):\n        \"\"\"Load vocabularies from a directory.\n\n        Args:\n            save_dir: The directory to load vocabularies.\n            filename:  The name for vocabularies.\n        \"\"\"\n        save_dir = get_resource(save_dir)\n        vocabs = SerializableDict()\n        vocabs.load_json(os.path.join(save_dir, filename))\n        self._load_vocabs(self, vocabs, vocab_cls)\n\n    @staticmethod\n    def _load_vocabs(vd, vocabs: dict, vocab_cls=Vocab):\n        \"\"\"\n\n        Args:\n            vd:\n            vocabs:\n            vocab_cls: Default class for the new vocab\n        \"\"\"\n        for key, value in vocabs.items():\n            if 'idx_to_token' in value:\n                cls = value.get('type', None)\n                if cls:\n                    cls = str_to_type(cls)\n                else:\n                    cls = vocab_cls\n                vocab = cls()\n                vocab.copy_from(value)\n                vd[key] = vocab\n            else:  # nested Vocab\n                # noinspection PyTypeChecker\n                vd[key] = nested = VocabDict()\n                VocabDict._load_vocabs(nested, value, vocab_cls)\n\n    def lock(self):\n        \"\"\"\n        Lock each vocab.\n        \"\"\"\n        for key, value in self.items():\n            if isinstance(value, Vocab):\n                value.lock()\n\n    def unlock(self):\n        \"\"\"\n        Unlock each vocab.\n        \"\"\"\n        for key, value in self.items():\n            if isinstance(value, Vocab):\n                value.unlock()\n\n    @property\n    def mutable(self):\n        status = [v.mutable for v in self.values() if isinstance(v, Vocab)]\n        return len(status) == 0 or any(status)\n\n    def __call__(self, sample: dict):\n        for key, value in self.items():\n            if isinstance(value, Vocab):\n                field = sample.get(key, None)\n                if field is not None:\n                    sample[f'{key}_id'] = value(field)\n        return sample\n\n    def __getattr__(self, key):\n        if key.startswith('__'):\n            return dict.__getattr__(key)\n        return self.__getitem__(key)\n\n    def __setattr__(self, key, value):\n        return self.__setitem__(key, value)\n\n    def __getitem__(self, k: str) -> Vocab:\n        return super().__getitem__(k)\n\n    def __setitem__(self, k: str, v: Vocab) -> None:\n        super().__setitem__(k, v)\n\n    def summary(self, logger: logging.Logger = None):\n        \"\"\"Log a summary of vocabs using a given logger.\n\n        Args:\n            logger: The logger to use.\n        \"\"\"\n        for key, value in self.items():\n            if isinstance(value, Vocab):\n                report = value.summary(verbose=False)\n                if logger:\n                    logger.info(f'{key}{report}')\n                else:\n                    print(f'{key}{report}')\n\n    def put(self, **kwargs):\n        \"\"\"Put names and corresponding :class:`hanlp.common.vocab.Vocab` instances into self.\n\n        Args:\n            **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.\n        \"\"\"\n        for k, v in kwargs.items():\n            self[k] = v\n\n\nclass NamedTransform(ABC):\n    def __init__(self, src: str, dst: str = None) -> None:\n        if dst is None:\n            dst = src\n        self.dst = dst\n        self.src = src\n\n    @abstractmethod\n    def __call__(self, sample: dict) -> dict:\n        return sample\n\n\nclass ConfigurableTransform(Configurable, ABC):\n    @property\n    def config(self):\n        return dict([('classpath', classpath_of(self))] +\n                    [(k, v) for k, v in self.__dict__.items() if not k.startswith('_')])\n\n    @classmethod\n    def from_config(cls, config: dict):\n        \"\"\"\n\n        Args:\n          config: \n          kwargs: \n          config: dict: \n\n        Returns:\n\n        \n        \"\"\"\n        cls = config.get('classpath', None)\n        assert cls, f'{config} doesn\\'t contain classpath field'\n        cls = str_to_type(cls)\n        config = dict(config)\n        config.pop('classpath')\n        return cls(**config)\n\n\nclass ConfigurableNamedTransform(NamedTransform, ConfigurableTransform, ABC):\n    pass\n\n\nclass EmbeddingNamedTransform(ConfigurableNamedTransform, ABC):\n\n    def __init__(self, output_dim: int, src: str, dst: str) -> None:\n        super().__init__(src, dst)\n        self.output_dim = output_dim\n\n\nclass RenameField(NamedTransform):\n\n    def __call__(self, sample: dict):\n        sample[self.dst] = sample.pop(self.src)\n        return sample\n\n\nclass CopyField(object):\n    def __init__(self, src, dst) -> None:\n        self.dst = dst\n        self.src = src\n\n    def __call__(self, sample: dict) -> dict:\n        sample[self.dst] = sample[self.src]\n        return sample\n\n\nclass FilterField(object):\n    def __init__(self, *keys) -> None:\n        self.keys = keys\n\n    def __call__(self, sample: dict):\n        sample = dict((k, sample[k]) for k in self.keys)\n        return sample\n\n\nclass TransformList(list):\n    \"\"\"Composes several transforms together.\n\n    Args:\n      transforms(list of ``Transform`` objects): list of transforms to compose.\n    Example:\n\n    Returns:\n\n    >>> transforms.TransformList(\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> )\n    \"\"\"\n\n    def __init__(self, *transforms) -> None:\n        super().__init__()\n        self.extend(transforms)\n\n    def __call__(self, sample):\n        for t in self:\n            sample = t(sample)\n        return sample\n\n    def index_by_type(self, t):\n        for i, trans in enumerate(self):\n            if isinstance(trans, t):\n                return i\n\n\nclass LowerCase(object):\n    def __init__(self, src, dst=None) -> None:\n        if dst is None:\n            dst = src\n        self.src = src\n        self.dst = dst\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if isinstance(src, str):\n            sample[self.dst] = src.lower()\n        elif isinstance(src, list):\n            sample[self.dst] = [x.lower() for x in src]\n        return sample\n\n\nclass LowerCase3D(LowerCase):\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        sample[self.dst] = [[y.lower() for y in x] for x in src]\n        return sample\n\n\nclass ToChar(object):\n    def __init__(self, src, dst='char', max_word_length=None, min_word_length=None, pad=PAD) -> None:\n        if dst is None:\n            dst = src\n        self.src = src\n        self.dst = dst\n        self.max_word_length = max_word_length\n        self.min_word_length = min_word_length\n        self.pad = pad\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if isinstance(src, str):\n            sample[self.dst] = self.to_chars(src)\n        elif isinstance(src, list):\n            sample[self.dst] = [self.to_chars(x) for x in src]\n        return sample\n\n    def to_chars(self, word: str):\n        chars = list(word)\n        if self.min_word_length and len(chars) < self.min_word_length:\n            chars = chars + [self.pad] * (self.min_word_length - len(chars))\n        if self.max_word_length:\n            chars = chars[:self.max_word_length]\n        return chars\n\n\nclass AppendEOS(NamedTransform):\n\n    def __init__(self, src: str, dst: str = None, eos=EOS) -> None:\n        super().__init__(src, dst)\n        self.eos = eos\n\n    def __call__(self, sample: dict) -> dict:\n        sample[self.dst] = sample[self.src] + [self.eos]\n        return sample\n\n\nclass WhitespaceTokenizer(NamedTransform):\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if isinstance(src, str):\n            sample[self.dst] = self.tokenize(src)\n        elif isinstance(src, list):\n            sample[self.dst] = [self.tokenize(x) for x in src]\n        return sample\n\n    @staticmethod\n    def tokenize(text: str):\n        return text.split()\n\n\nclass NormalizeDigit(object):\n    def __init__(self, src, dst=None) -> None:\n        if dst is None:\n            dst = src\n        self.src = src\n        self.dst = dst\n\n    @staticmethod\n    def transform(word: str):\n        new_word = \"\"\n        for char in word:\n            if char.isdigit():\n                new_word += '0'\n            else:\n                new_word += char\n        return new_word\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if isinstance(src, str):\n            sample[self.dst] = self.transform(src)\n        elif isinstance(src, list):\n            sample[self.dst] = [self.transform(x) for x in src]\n        return sample\n\n\nclass Bigram(NamedTransform):\n\n    def __init__(self, src: str, dst: str = None) -> None:\n        if not dst:\n            dst = f'{src}_bigram'\n        super().__init__(src, dst)\n\n    def __call__(self, sample: dict) -> dict:\n        src: List = sample[self.src]\n        dst = src + [EOS]\n        dst = [dst[i] + dst[i + 1] for i in range(len(src))]\n        sample[self.dst] = dst\n        return sample\n\n\nclass FieldLength(NamedTransform):\n\n    def __init__(self, src: str, dst: str = None, delta=0) -> None:\n        self.delta = delta\n        if not dst:\n            dst = f'{src}_length'\n        super().__init__(src, dst)\n\n    def __call__(self, sample: dict) -> dict:\n        sample[self.dst] = len(sample[self.src]) + self.delta\n        return sample\n\n\nclass BMESOtoIOBES(object):\n    def __init__(self, field='tag') -> None:\n        self.field = field\n\n    def __call__(self, sample: dict) -> dict:\n        sample[self.field] = [self.convert(y) for y in sample[self.field]]\n        return sample\n\n    @staticmethod\n    def convert(y: str):\n        if y.startswith('M-'):\n            return 'I-'\n        return y\n\n\nclass NormalizeToken(ConfigurableNamedTransform):\n\n    def __init__(self, mapper: Union[str, dict], src: str, dst: str = None) -> None:\n        super().__init__(src, dst)\n        self.mapper = mapper\n        if isinstance(mapper, str):\n            mapper = get_resource(mapper)\n        if isinstance(mapper, str):\n            self._table = load_json(mapper)\n        elif isinstance(mapper, dict):\n            self._table = mapper\n        else:\n            raise ValueError(f'Unrecognized mapper type {mapper}')\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if self.src == self.dst:\n            sample[f'{self.src}_'] = src\n        if isinstance(src, str):\n            src = self.convert(src)\n        else:\n            src = [self.convert(x) for x in src]\n        sample[self.dst] = src\n        return sample\n\n    def convert(self, token) -> str:\n        return self._table.get(token, token)\n\n\nclass PunctuationMask(ConfigurableNamedTransform):\n    def __init__(self, src: str, dst: str = None) -> None:\n        \"\"\"Mask out all punctuations (set mask of punctuations to False)\n\n        Args:\n          src:\n          dst:\n\n        Returns:\n\n        \"\"\"\n        if not dst:\n            dst = f'{src}_punct_mask'\n        super().__init__(src, dst)\n\n    def __call__(self, sample: dict) -> dict:\n        src = sample[self.src]\n        if isinstance(src, str):\n            dst = not ispunct(src)\n        else:\n            dst = [not ispunct(x) for x in src]\n        sample[self.dst] = dst\n        return sample\n\n\nclass NormalizeCharacter(NormalizeToken):\n    def convert(self, token) -> str:\n        return ''.join([NormalizeToken.convert(self, c) for c in token])\n", "hanlp/common/structure.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-08-26 14:58\nfrom typing import Dict\n\nfrom hanlp_common.configurable import Configurable\nfrom hanlp_common.reflection import classpath_of\nfrom hanlp_common.structure import SerializableDict\n\n\nclass ConfigTracker(Configurable):\n\n    def __init__(self, locals_: Dict, exclude=('kwargs', 'self', '__class__', 'locals_')) -> None:\n        \"\"\"This base class helps sub-classes to capture their arguments passed to ``__init__``, and also their types so\n        that they can be deserialized from a config in dict form.\n\n        Args:\n            locals_: Obtained by :meth:`locals`.\n            exclude: Arguments to be excluded.\n\n        Examples:\n            >>> class MyClass(ConfigTracker):\n            >>>     def __init__(self, i_need_this='yes') -> None:\n            >>>         super().__init__(locals())\n            >>> obj = MyClass()\n            >>> print(obj.config)\n            {'i_need_this': 'yes', 'classpath': 'test_config_tracker.MyClass'}\n\n        \"\"\"\n        if 'kwargs' in locals_:\n            locals_.update(locals_['kwargs'])\n        self.config = SerializableDict(\n            (k, v.config if hasattr(v, 'config') else v) for k, v in locals_.items() if k not in exclude)\n        self.config['classpath'] = classpath_of(self)\n\n\nclass History(object):\n    def __init__(self):\n        \"\"\" A history of training context. It records how many steps have passed and provides methods to decide whether\n        an update should be performed, and to caculate number of training steps given dataloader size and\n        ``gradient_accumulation``.\n        \"\"\"\n        self.num_mini_batches = 0\n\n    def step(self, gradient_accumulation):\n        \"\"\" Whether the training procedure should perform an update.\n\n        Args:\n            gradient_accumulation: Number of batches per update.\n\n        Returns:\n            bool: ``True`` to update.\n        \"\"\"\n        self.num_mini_batches += 1\n        return self.num_mini_batches % gradient_accumulation == 0\n\n    def num_training_steps(self, num_batches, gradient_accumulation):\n        \"\"\" Caculate number of training steps.\n\n        Args:\n            num_batches: Size of dataloader.\n            gradient_accumulation: Number of batches per update.\n\n        Returns:\n\n        \"\"\"\n        return len(\n            [i for i in range(self.num_mini_batches + 1, self.num_mini_batches + num_batches + 1) if\n             i % gradient_accumulation == 0])\n", "hanlp/metrics/spearman_correlation.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2021-05-23 16:12\nimport torch\n\nfrom hanlp.metrics.metric import Metric\n\n\ndef _get_ranks(x: torch.Tensor) -> torch.Tensor:\n    argsort = x.argsort()\n    ranks = torch.zeros_like(argsort, device=x.device)\n    ranks[argsort] = torch.arange(len(x), device=x.device)\n    return ranks\n\n\ndef spearman_correlation(x: torch.Tensor, y: torch.Tensor):\n    \"\"\"Compute correlation between 2 1-D vectors. Adopted from\n    https://discuss.pytorch.org/t/spearmans-correlation/91931/5\n\n    Args:\n        x: Shape (N, )\n        y: Shape (N, )\n\n    \"\"\"\n    x_rank = _get_ranks(x)\n    y_rank = _get_ranks(y)\n\n    n = x.size(0)\n    upper = 6 * torch.sum((x_rank - y_rank).pow(2))\n    down = n * (n ** 2 - 1.0)\n    return 1.0 - (upper / down)\n\n\nclass SpearmanCorrelation(Metric):\n    \"\"\"\n    This `Metric` calculates the sample Spearman correlation coefficient (r)\n    between two tensors. Each element in the two tensors is assumed to be\n    a different observation of the variable (i.e., the input tensors are\n    implicitly flattened into vectors and the correlation is calculated\n    between the vectors).\n\n    <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>\n    \"\"\"\n\n    @property\n    def score(self):\n        return spearman_correlation(self.total_predictions, self.total_gold_labels).item()\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.total_predictions = torch.zeros(0)\n        self.total_gold_labels = torch.zeros(0)\n\n    def __call__(\n            self,\n            predictions: torch.Tensor,\n            gold_labels: torch.Tensor,\n            mask=None\n    ):\n        \"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        \"\"\"\n        if mask is not None:\n            raise NotImplemented('mask not supported in SpearmanCorrelation for now.')\n        # Flatten predictions, gold_labels, and mask. We calculate the Spearman correlation between\n        # the vectors, since each element in the predictions and gold_labels tensor is assumed\n        # to be a separate observation.\n        predictions = predictions.reshape(-1)\n        gold_labels = gold_labels.reshape(-1)\n\n        self.total_predictions = self.total_predictions.to(predictions.device)\n        self.total_gold_labels = self.total_gold_labels.to(gold_labels.device)\n        self.total_predictions = torch.cat((self.total_predictions, predictions), 0)\n        self.total_gold_labels = torch.cat((self.total_gold_labels, gold_labels), 0)\n\n    def reset(self):\n        self.total_predictions = torch.zeros(0)\n        self.total_gold_labels = torch.zeros(0)\n\n    def __str__(self) -> str:\n        return f'spearman: {self.score * 100:.2f}'\n", "hanlp/metrics/mtl.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-03 00:16\nfrom hanlp.metrics.metric import Metric\n\n\nclass MetricDict(Metric, dict):\n    _COLORS = [\"magenta\", \"cyan\", \"green\", \"yellow\"]\n\n    @property\n    def score(self):\n        return sum(float(x) for x in self.values()) / len(self)\n\n    def __call__(self, pred, gold):\n        for metric in self.values():\n            metric(pred, gold)\n\n    def reset(self):\n        for metric in self.values():\n            metric.reset()\n\n    def __repr__(self) -> str:\n        return ' '.join(f'({k} {v})' for k, v in self.items())\n\n    def cstr(self, idx=None, level=0) -> str:\n        if idx is None:\n            idx = [0]\n        prefix = ''\n        for _, (k, v) in enumerate(self.items()):\n            color = self._COLORS[idx[0] % len(self._COLORS)]\n            idx[0] += 1\n            child_is_dict = isinstance(v, MetricDict)\n            _level = min(level, 2)\n            # if level != 0 and not child_is_dict:\n            #     _level = 2\n            lb = '{[('\n            rb = '}])'\n            k = f'[bold][underline]{k}[/underline][/bold]'\n            prefix += f'[{color}]{lb[_level]}{k} [/{color}]'\n            if child_is_dict:\n                prefix += v.cstr(idx, level + 1)\n            else:\n                prefix += f'[{color}]{v}[/{color}]'\n            prefix += f'[{color}]{rb[_level]}[/{color}]'\n        return prefix\n", "hanlp/metrics/metric.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-03 11:35\nfrom abc import ABC, abstractmethod\n\n\nclass Metric(ABC):\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __eq__(self, other):\n        return self.score == other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    def __ne__(self, other):\n        return self.score != other\n\n    @property\n    @abstractmethod\n    def score(self):\n        pass\n\n    @abstractmethod\n    def __call__(self, pred, gold, mask=None):\n        pass\n\n    def __repr__(self) -> str:\n        return f'{self.score}:.4f'\n\n    def __float__(self):\n        return self.score\n\n    @abstractmethod\n    def reset(self):\n        pass\n", "hanlp/metrics/f1.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-10 14:55\nfrom abc import ABC\n\nfrom hanlp.metrics.metric import Metric\n\n\nclass F1(Metric, ABC):\n    def __init__(self, nb_pred=0, nb_true=0, nb_correct=0) -> None:\n        super().__init__()\n        self.nb_correct = nb_correct\n        self.nb_pred = nb_pred\n        self.nb_true = nb_true\n\n    def __repr__(self) -> str:\n        p, r, f = self.prf\n        return f\"P: {p:.2%} R: {r:.2%} F1: {f:.2%}\"\n\n    @property\n    def prf(self):\n        nb_correct = self.nb_correct\n        nb_pred = self.nb_pred\n        nb_true = self.nb_true\n        p = nb_correct / nb_pred if nb_pred > 0 else .0\n        r = nb_correct / nb_true if nb_true > 0 else .0\n        f = 2 * p * r / (p + r) if p + r > 0 else .0\n        return p, r, f\n\n    @property\n    def score(self):\n        return self.prf[-1]\n\n    def reset(self):\n        self.nb_correct = 0\n        self.nb_pred = 0\n        self.nb_true = 0\n\n    def __call__(self, pred: set, gold: set):\n        self.nb_correct += len(pred & gold)\n        self.nb_pred += len(pred)\n        self.nb_true += len(gold)\n\n\nclass F1_(Metric):\n    def __init__(self, p, r, f) -> None:\n        super().__init__()\n        self.f = f\n        self.r = r\n        self.p = p\n\n    @property\n    def score(self):\n        return self.f\n\n    def __call__(self, pred, gold):\n        raise NotImplementedError()\n\n    def reset(self):\n        self.f = self.r = self.p = 0\n\n    def __repr__(self) -> str:\n        p, r, f = self.p, self.r, self.f\n        return f\"P: {p:.2%} R: {r:.2%} F1: {f:.2%}\"\n", "hanlp/metrics/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-09-14 21:55", "hanlp/metrics/accuracy.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-12 17:56\nfrom typing import Optional, Iterable\n\nimport torch\n\nfrom hanlp.metrics.metric import Metric\n\n\nclass CategoricalAccuracy(Metric):\n    \"\"\"\n    Categorical Top-K accuracy. Assumes integer labels, with\n    each item to be classified having a single correct class.\n    Tie break enables equal distribution of scores among the\n    classes with same maximum predicted scores.\n    Copied from AllenNLP and added several methods.\n    \"\"\"\n\n    def __init__(self, top_k: int = 1, tie_break: bool = False) -> None:\n        if top_k > 1 and tie_break:\n            raise ValueError(\n                \"Tie break in Categorical Accuracy can be done only for maximum (top_k = 1)\"\n            )\n        if top_k <= 0:\n            raise ValueError(\"top_k passed to Categorical Accuracy must be > 0\")\n        self._top_k = top_k\n        self._tie_break = tie_break\n        self.correct_count = 0.0\n        self.total_count = 0.0\n\n    def __call__(\n            self,\n            predictions: torch.Tensor,\n            gold_labels: torch.Tensor,\n            mask: Optional[torch.BoolTensor] = None,\n    ):\n        \"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ..., num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, ...). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A masking tensor the same size as `gold_labels`.\n        \"\"\"\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        num_classes = predictions.size(-1)\n        if gold_labels.dim() != predictions.dim() - 1:\n            raise ValueError(\n                \"gold_labels must have dimension == predictions.size() - 1 but \"\n                \"found tensor of shape: {}\".format(predictions.size())\n            )\n        if (gold_labels >= num_classes).any():\n            raise ValueError(\n                \"A gold label passed to Categorical Accuracy contains an id >= {}, \"\n                \"the number of classes.\".format(num_classes)\n            )\n\n        predictions = predictions.view((-1, num_classes))\n        gold_labels = gold_labels.view(-1).long()\n        if not self._tie_break:\n            # Top K indexes of the predictions (or fewer, if there aren't K of them).\n            # Special case topk == 1, because it's common and .max() is much faster than .topk().\n            if self._top_k == 1:\n                top_k = predictions.max(-1)[1].unsqueeze(-1)\n            else:\n                top_k = predictions.topk(min(self._top_k, predictions.shape[-1]), -1)[1]\n\n            # This is of shape (batch_size, ..., top_k).\n            correct = top_k.eq(gold_labels.unsqueeze(-1)).float()\n        else:\n            # prediction is correct if gold label falls on any of the max scores. distribute score by tie_counts\n            max_predictions = predictions.max(-1)[0]\n            max_predictions_mask = predictions.eq(max_predictions.unsqueeze(-1))\n            # max_predictions_mask is (rows X num_classes) and gold_labels is (batch_size)\n            # ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions\n            # For each row check if index pointed by gold_label is was 1 or not (among max scored classes)\n            correct = max_predictions_mask[\n                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\n            ].float()\n            tie_counts = max_predictions_mask.sum(-1)\n            correct /= tie_counts.float()\n            correct.unsqueeze_(-1)\n\n        if mask is not None:\n            correct *= mask.view(-1, 1)\n            self.total_count += mask.sum()\n        else:\n            self.total_count += gold_labels.numel()\n        self.correct_count += correct.sum()\n\n    @property\n    def score(self):\n        if self.total_count > 1e-12:\n            accuracy = float(self.correct_count) / float(self.total_count)\n        else:\n            accuracy = 0.0\n        return accuracy\n\n    def __repr__(self) -> str:\n        return f'Accuracy:{self.score:.2%}'\n\n    @staticmethod\n    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:\n        \"\"\"\n        If you actually passed gradient-tracking Tensors to a Metric, there will be\n        a huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures the tensors are detached.\n        \"\"\"\n        # Check if it's actually a tensor in case something else was passed.\n        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)\n\n    def reset(self):\n        self.correct_count = 0.0\n        self.total_count = 0.0\n\n\nclass BooleanAccuracy(Metric):\n    \"\"\"\n    Just checks batch-equality of two tensors and computes an accuracy metric based on that.\n    That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that\n    as a set of `batch_size` predictions and checks that each is *entirely* correct across the remaining dims.\n    This means the denominator in the accuracy computation is `batch_size`, with the caveat that predictions\n    that are totally masked are ignored (in which case the denominator is the number of predictions that have\n    at least one unmasked element).\n\n    This is similar to [`CategoricalAccuracy`](./categorical_accuracy.md), if you've already done a `.max()`\n    on your predictions.  If you have categorical output, though, you should typically just use\n    `CategoricalAccuracy`.  The reason you might want to use this instead is if you've done\n    some kind of constrained inference and don't have a prediction tensor that matches the API of\n    `CategoricalAccuracy`, which assumes a final dimension of size `num_classes`.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._correct_count = 0.0\n        self._total_count = 0.0\n\n    def __call__(\n            self,\n            predictions: torch.Tensor,\n            gold_labels: torch.Tensor,\n            mask: Optional[torch.BoolTensor] = None,\n    ):\n        \"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.BoolTensor`, optional (default = `None`).\n            A tensor of the same shape as `predictions`.\n        \"\"\"\n        predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n\n        # Some sanity checks.\n        if gold_labels.size() != predictions.size():\n            raise ValueError(\n                f\"gold_labels must have shape == predictions.size() but \"\n                f\"found tensor of shape: {gold_labels.size()}\"\n            )\n        if mask is not None and mask.size() != predictions.size():\n            raise ValueError(\n                f\"mask must have shape == predictions.size() but \"\n                f\"found tensor of shape: {mask.size()}\"\n            )\n\n        batch_size = predictions.size(0)\n\n        if mask is not None:\n            # We can multiply by the mask up front, because we're just checking equality below, and\n            # this way everything that's masked will be equal.\n            predictions = predictions * mask\n            gold_labels = gold_labels * mask\n\n            # We want to skip predictions that are completely masked;\n            # so we'll keep predictions that aren't.\n            keep = mask.view(batch_size, -1).max(dim=1)[0]\n        else:\n            keep = torch.ones(batch_size, device=predictions.device).bool()\n\n        predictions = predictions.view(batch_size, -1)\n        gold_labels = gold_labels.view(batch_size, -1)\n\n        # At this point, predictions is (batch_size, rest_of_dims_combined),\n        # so .eq -> .prod will be 1 if every element of the instance prediction is correct\n        # and 0 if at least one element of the instance prediction is wrong.\n        # Because of how we're handling masking, masked positions are automatically \"correct\".\n        correct = predictions.eq(gold_labels).prod(dim=1).float()\n\n        # Since masked positions are correct, we need to explicitly exclude instance predictions\n        # where the entire prediction is masked (because they look \"correct\").\n        self._correct_count += (correct * keep).sum()\n        self._total_count += keep.sum()\n\n    def get_metric(self, reset: bool = False):\n        \"\"\"\n        # Returns\n\n        The accumulated accuracy.\n        \"\"\"\n        if self._total_count > 0:\n            accuracy = float(self._correct_count) / float(self._total_count)\n        else:\n            accuracy = 0.0\n        if reset:\n            self.reset()\n        return accuracy\n\n    def reset(self):\n        self._correct_count = 0.0\n        self._total_count = 0.0\n\n    @staticmethod\n    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:\n        \"\"\"\n        If you actually passed gradient-tracking Tensors to a Metric, there will be\n        a huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures the tensors are detached.\n        \"\"\"\n        # Check if it's actually a tensor in case something else was passed.\n        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)\n", "hanlp/metrics/parsing/conllx_eval.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-03-08 22:35\nimport tempfile\n\nfrom hanlp.utils.io_util import get_resource, get_exitcode_stdout_stderr\n\nCONLLX_EVAL = get_resource(\n    'https://github.com/elikip/bist-parser/archive/master.zip' + '#bmstparser/src/utils/eval.pl')\n\n\ndef evaluate(gold_file, pred_file):\n    \"\"\"Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)\n\n    Args:\n      gold_file(str): The gold conllx file\n      pred_file(str): The pred conllx file\n\n    Returns:\n\n    \n    \"\"\"\n    gold_file = get_resource(gold_file)\n    fixed_pred_file = tempfile.NamedTemporaryFile().name\n    copy_cols(gold_file, pred_file, fixed_pred_file, keep_comments=False)\n    if gold_file.endswith('.conllu'):\n        fixed_gold_file = tempfile.NamedTemporaryFile().name\n        copy_cols(gold_file, gold_file, fixed_gold_file, keep_comments=False)\n        gold_file = fixed_gold_file\n\n    exitcode, out, err = get_exitcode_stdout_stderr(f'perl {CONLLX_EVAL} -q -b -g {gold_file} -s {fixed_pred_file}')\n    if exitcode:\n        raise RuntimeError(f'eval.pl exited with error code {exitcode} and error message {err} and output {out}.')\n    lines = out.split('\\n')[-4:]\n    las = int(lines[0].split()[3]) / int(lines[0].split()[5])\n    uas = int(lines[1].split()[3]) / int(lines[1].split()[5])\n    return uas, las\n\n\ndef copy_cols(gold_file, pred_file, copied_pred_file, keep_comments=True):\n    \"\"\"Copy the first 6 columns from gold file to pred file\n\n    Args:\n      gold_file: \n      pred_file: \n      copied_pred_file: \n      keep_comments:  (Default value = True)\n\n    Returns:\n\n    \n    \"\"\"\n    with open(copied_pred_file, 'w') as to_out, open(pred_file) as pred_file, open(gold_file) as gold_file:\n        for idx, (p, g) in enumerate(zip(pred_file, gold_file)):\n            while p.startswith('#'):\n                p = next(pred_file)\n            if not g.strip():\n                if p.strip():\n                    raise ValueError(\n                        f'Prediction file {pred_file.name} does not end a sentence at line {idx + 1}\\n{p.strip()}')\n                to_out.write('\\n')\n                continue\n            while g.startswith('#') or '-' in g.split('\\t')[0]:\n                if keep_comments or g.startswith('-'):\n                    to_out.write(g)\n                g = next(gold_file)\n            to_out.write('\\t'.join(str(x) for x in g.split('\\t')[:6] + p.split('\\t')[6:]))\n", "hanlp/metrics/parsing/span.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom collections import Counter\n\nfrom hanlp.metrics.metric import Metric\n\n\nclass SpanMetric(Metric):\n\n    def __init__(self, eps=1e-12):\n        super().__init__()\n        self.reset(eps)\n\n    # noinspection PyAttributeOutsideInit\n    def reset(self, eps=1e-12):\n        self.n = 0.0\n        self.n_ucm = 0.0\n        self.n_lcm = 0.0\n        self.utp = 0.0\n        self.ltp = 0.0\n        self.pred = 0.0\n        self.gold = 0.0\n        self.eps = eps\n\n    def __call__(self, preds, golds):\n        for pred, gold in zip(preds, golds):\n            upred = Counter([(i, j) for i, j, label in pred])\n            ugold = Counter([(i, j) for i, j, label in gold])\n            utp = list((upred & ugold).elements())\n            lpred = Counter(pred)\n            lgold = Counter(gold)\n            ltp = list((lpred & lgold).elements())\n            self.n += 1\n            self.n_ucm += len(utp) == len(pred) == len(gold)\n            self.n_lcm += len(ltp) == len(pred) == len(gold)\n            self.utp += len(utp)\n            self.ltp += len(ltp)\n            self.pred += len(pred)\n            self.gold += len(gold)\n        return self\n\n    def __repr__(self):\n        s = f\"UCM: {self.ucm:.2%} LCM: {self.lcm:.2%} \"\n        s += f\"UP: {self.up:.2%} UR: {self.ur:.2%} UF: {self.uf:.2%} \"\n        s += f\"LP: {self.lp:.2%} LR: {self.lr:.2%} LF: {self.lf:.2%}\"\n\n        return s\n\n    @property\n    def score(self):\n        return self.lf\n\n    @property\n    def ucm(self):\n        return self.n_ucm / (self.n + self.eps)\n\n    @property\n    def lcm(self):\n        return self.n_lcm / (self.n + self.eps)\n\n    @property\n    def up(self):\n        return self.utp / (self.pred + self.eps)\n\n    @property\n    def ur(self):\n        return self.utp / (self.gold + self.eps)\n\n    @property\n    def uf(self):\n        return 2 * self.utp / (self.pred + self.gold + self.eps)\n\n    @property\n    def lp(self):\n        return self.ltp / (self.pred + self.eps)\n\n    @property\n    def lr(self):\n        return self.ltp / (self.gold + self.eps)\n\n    @property\n    def lf(self):\n        return 2 * self.ltp / (self.pred + self.gold + self.eps)\n", "hanlp/metrics/parsing/labeled_f1_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-27 21:42\nimport tensorflow as tf\n\n\nclass LabeledF1TF(object):\n\n    def __init__(self):\n        super(LabeledF1TF, self).__init__()\n\n        self.sum_gold_arcs_wo_punc = 0.0\n        self.sum_pred_arcs_wo_punc = 0.0\n        self.correct_arcs_wo_punc = 0.0\n        self.correct_rels_wo_punc = 0.0\n\n    def __repr__(self):\n        return f\"UF: {self.uf:6.2%} LF: {self.lf:6.2%}\"\n\n    def __call__(self, arc_preds, rel_preds, arc_golds, rel_golds, mask):\n        mask = mask.unsqueeze(-1).expand_as(arc_preds)\n        mask = mask & mask.transpose(1, 2)\n\n        mask_gold = mask & arc_golds\n        mask_pred = mask & arc_preds\n        correct_arcs_wo_punc = (arc_preds == arc_golds)[mask_gold & mask_pred]\n        correct_rels_wo_punc = (rel_preds == rel_golds)[mask_gold & mask_pred] & correct_arcs_wo_punc\n\n        self.sum_gold_arcs_wo_punc += float(tf.math.count_nonzero(mask_gold))\n        self.sum_pred_arcs_wo_punc += float(tf.math.count_nonzero(mask_pred))\n        self.correct_arcs_wo_punc += float(tf.math.count_nonzero(correct_arcs_wo_punc))\n        self.correct_rels_wo_punc += float(tf.math.count_nonzero(correct_rels_wo_punc))\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    @property\n    def score(self):\n        return self.las\n\n    @property\n    def uas(self):\n        return self.uf\n\n    @property\n    def las(self):\n        return self.lf\n\n    @property\n    def ur(self):\n        if not self.sum_gold_arcs_wo_punc:\n            return 0\n        return self.correct_arcs_wo_punc / self.sum_gold_arcs_wo_punc\n\n    @property\n    def up(self):\n        if not self.sum_pred_arcs_wo_punc:\n            return 0\n        return self.correct_arcs_wo_punc / self.sum_pred_arcs_wo_punc\n\n    @property\n    def lr(self):\n        if not self.sum_gold_arcs_wo_punc:\n            return 0\n        return self.correct_rels_wo_punc / self.sum_gold_arcs_wo_punc\n\n    @property\n    def lp(self):\n        if not self.sum_pred_arcs_wo_punc:\n            return 0\n        return self.correct_rels_wo_punc / self.sum_pred_arcs_wo_punc\n\n    @property\n    def uf(self):\n        rp = self.ur + self.up\n        if not rp:\n            return 0\n        return 2 * self.ur * self.up / rp\n\n    @property\n    def lf(self):\n        rp = self.lr + self.lp\n        if not rp:\n            return 0\n        return 2 * self.lr * self.lp / rp\n\n    def reset_states(self):\n        self.sum_gold_arcs_wo_punc = 0.0\n        self.sum_pred_arcs_wo_punc = 0.0\n        self.correct_arcs_wo_punc = 0.0\n        self.correct_rels_wo_punc = 0.0\n\n    def to_dict(self) -> dict:\n        return {'UF': self.uf, 'LF': self.lf}\n", "hanlp/metrics/parsing/labeled_f1.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-27 21:42\n\nfrom hanlp.metrics.metric import Metric\n\n\nclass LabeledF1(Metric):\n\n    def __init__(self):\n        super(LabeledF1, self).__init__()\n\n        self.sum_gold_arcs_wo_punc = 0.0\n        self.sum_pred_arcs_wo_punc = 0.0\n        self.correct_arcs_wo_punc = 0.0\n        self.correct_rels_wo_punc = 0.0\n\n    def __repr__(self):\n        return f\"UF: {self.uf:4.2%} LF: {self.lf:4.2%}\"\n\n    def __call__(self, arc_preds, rel_preds, arc_golds, rel_golds, mask):\n        mask_gold = mask & arc_golds\n        mask_pred = mask & arc_preds\n\n        correct_mask = mask_gold & mask_pred\n        correct_arcs_wo_punc = (arc_preds == arc_golds)[correct_mask]\n        correct_rels_wo_punc = (rel_preds == rel_golds)[correct_mask] & correct_arcs_wo_punc\n\n        self.sum_gold_arcs_wo_punc += float(mask_gold.sum())\n        self.sum_pred_arcs_wo_punc += float(mask_pred.sum())\n        self.correct_arcs_wo_punc += float(correct_arcs_wo_punc.sum())\n        self.correct_rels_wo_punc += float(correct_rels_wo_punc.sum())\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    @property\n    def score(self):\n        return self.las\n\n    @property\n    def uas(self):\n        return self.uf\n\n    @property\n    def las(self):\n        return self.lf\n\n    @property\n    def ur(self):\n        if not self.sum_gold_arcs_wo_punc:\n            return .0\n        return self.correct_arcs_wo_punc / self.sum_gold_arcs_wo_punc\n\n    @property\n    def up(self):\n        if not self.sum_pred_arcs_wo_punc:\n            return .0\n        return self.correct_arcs_wo_punc / self.sum_pred_arcs_wo_punc\n\n    @property\n    def lr(self):\n        if not self.sum_gold_arcs_wo_punc:\n            return .0\n        return self.correct_rels_wo_punc / self.sum_gold_arcs_wo_punc\n\n    @property\n    def lp(self):\n        if not self.sum_pred_arcs_wo_punc:\n            return .0\n        return self.correct_rels_wo_punc / self.sum_pred_arcs_wo_punc\n\n    @property\n    def uf(self):\n        rp = self.ur + self.up\n        if not rp:\n            return .0\n        return 2 * self.ur * self.up / rp\n\n    @property\n    def lf(self):\n        rp = self.lr + self.lp\n        if not rp:\n            return .0\n        return 2 * self.lr * self.lp / rp\n\n    def reset(self):\n        self.sum_gold_arcs_wo_punc = 0.0\n        self.sum_pred_arcs_wo_punc = 0.0\n        self.correct_arcs_wo_punc = 0.0\n        self.correct_rels_wo_punc = 0.0\n\n    def to_dict(self) -> dict:\n        return {'UF': self.uf, 'LF': self.lf}\n", "hanlp/metrics/parsing/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-27 00:48", "hanlp/metrics/parsing/attachmentscore.py": "# MIT License\n#\n# Copyright (c) 2020 Yu Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom hanlp.metrics.metric import Metric\n\n\nclass AttachmentScore(Metric):\n\n    def __init__(self, eps=1e-12):\n        super(AttachmentScore, self).__init__()\n\n        self.eps = eps\n        self.total = 0.0\n        self.correct_arcs = 0.0\n        self.correct_rels = 0.0\n\n    def __repr__(self):\n        return f\"UAS: {self.uas:.2%} LAS: {self.las:.2%}\"\n\n    # noinspection PyMethodOverriding\n    def __call__(self, arc_preds, rel_preds, arc_golds, rel_golds, mask):\n        arc_mask = arc_preds.eq(arc_golds)[mask]\n        rel_mask = rel_preds.eq(rel_golds)[mask] & arc_mask\n\n        self.total += len(arc_mask)\n        self.correct_arcs += arc_mask.sum().item()\n        self.correct_rels += rel_mask.sum().item()\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    @property\n    def score(self):\n        return self.las\n\n    @property\n    def uas(self):\n        return self.correct_arcs / (self.total + self.eps)\n\n    @property\n    def las(self):\n        return self.correct_rels / (self.total + self.eps)\n\n    def reset(self):\n        self.total = 0.0\n        self.correct_arcs = 0.0\n        self.correct_rels = 0.0\n", "hanlp/metrics/parsing/labeled_score.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-27 00:49\n\nimport tensorflow as tf\n\n\nclass LabeledScore(object):\n\n    def __init__(self, eps=1e-5):\n        super(LabeledScore, self).__init__()\n\n        self.eps = eps\n        self.total = 0.0\n        self.correct_arcs = 0.0\n        self.correct_rels = 0.0\n\n    def __repr__(self):\n        return f\"UAS: {self.uas:6.2%} LAS: {self.las:6.2%}\"\n\n    def __call__(self, arc_preds, rel_preds, arc_golds, rel_golds, mask):\n        arc_mask = (arc_preds == arc_golds)[mask]\n        rel_mask = (rel_preds == rel_golds)[mask] & arc_mask\n\n        self.total += len(arc_mask)\n        self.correct_arcs += int(tf.math.count_nonzero(arc_mask))\n        self.correct_rels += int(tf.math.count_nonzero(rel_mask))\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    @property\n    def score(self):\n        return self.las\n\n    @property\n    def uas(self):\n        return self.correct_arcs / (self.total + self.eps)\n\n    @property\n    def las(self):\n        return self.correct_rels / (self.total + self.eps)\n\n    def reset_states(self):\n        self.total = 0.0\n        self.correct_arcs = 0.0\n        self.correct_rels = 0.0\n\n    def to_dict(self) -> dict:\n        return {'UAS': self.uas, 'LAS': self.las}\n", "hanlp/metrics/parsing/semdep_eval.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Timothy Dozat\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport sys\nfrom collections import namedtuple\n\n\n# ===============================================================\ndef sdp_eval(gold_files, sys_files, labeled=False):\n    \"\"\"Modified from https://github.com/tdozat/Parser-v3/blob/2ff4061373e8aac8c962537a6220e1d5b196abf6/scripts/semdep_eval.py\n    Dozat claimed \"I tested it against the official eval script and it reported identical LF1\".\n\n    Args:\n      gold_files: \n      sys_files: \n      labeled:  (Default value = False)\n\n    Returns:\n\n    \n    \"\"\"\n\n    correct = 0\n    predicted = 0\n    actual = 0\n    n_tokens = 0\n    n_sequences = 0\n    current_seq_correct = False\n    n_correct_sequences = 0\n    current_sent = 0\n    if isinstance(gold_files, str):\n        gold_files = [gold_files]\n    if isinstance(sys_files, str):\n        sys_files = [sys_files]\n\n    for gold_file, sys_file in zip(gold_files, sys_files):\n        with codecs.open(gold_file, encoding='utf-8') as gf, \\\n                codecs.open(sys_file, encoding='utf-8') as sf:\n            gold_line = gf.readline()\n            gold_i = 1\n            sys_i = 0\n            while gold_line:\n                while gold_line.startswith('#'):\n                    current_sent += 1\n                    gold_i += 1\n                    n_sequences += 1\n                    n_correct_sequences += current_seq_correct\n                    current_seq_correct = True\n                    gold_line = gf.readline()\n                if gold_line.rstrip() != '':\n                    sys_line = sf.readline()\n                    sys_i += 1\n                    while sys_line.startswith('#') or sys_line.rstrip() == '' or sys_line.split('\\t')[0] == '0':\n                        sys_line = sf.readline()\n                        sys_i += 1\n\n                    gold_line = gold_line.rstrip().split('\\t')\n                    sys_line = sys_line.rstrip().split('\\t')\n                    # assert sys_line[1] == gold_line[1], 'Files are misaligned at lines {}, {}'.format(gold_i, sys_i)\n\n                    # Compute the gold edges\n                    gold_node = gold_line[8]\n                    if gold_node != '_':\n                        gold_node = gold_node.split('|')\n                        if labeled:\n                            gold_edges = set(tuple(gold_edge.split(':', 1)) for gold_edge in gold_node)\n                        else:\n                            gold_edges = set(gold_edge.split(':', 1)[0] for gold_edge in gold_node)\n                    else:\n                        gold_edges = set()\n\n                    # Compute the sys edges\n                    sys_node = sys_line[8]\n                    if sys_node != '_':\n                        sys_node = sys_node.split('|')\n                        if labeled:\n                            sys_edges = set(tuple(sys_edge.split(':', 1)) for sys_edge in sys_node)\n                        else:\n                            sys_edges = set(sys_edge.split(':', 1)[0] for sys_edge in sys_node)\n                    else:\n                        sys_edges = set()\n\n                    correct_edges = gold_edges & sys_edges\n                    if len(correct_edges) != len(gold_edges):\n                        current_seq_correct = False\n                    correct += len(correct_edges)\n                    predicted += len(sys_edges)\n                    actual += len(gold_edges)\n                    n_tokens += 1\n                    # current_fp += len(sys_edges) - len(gold_edges & sys_edges)\n                gold_line = gf.readline()\n                gold_i += 1\n    # print(correct, predicted - correct, actual - correct)\n    Accuracy = namedtuple('Accuracy', ['precision', 'recall', 'F1', 'seq_acc'])\n    precision = correct / (predicted + 1e-12)\n    recall = correct / (actual + 1e-12)\n    F1 = 2 * precision * recall / (precision + recall + 1e-12)\n    seq_acc = n_correct_sequences / n_sequences\n    return Accuracy(precision, recall, F1, seq_acc)\n\n\n# ===============================================================\ndef main():\n    \"\"\" \"\"\"\n\n    files = sys.argv[1:]\n    n_files = len(files)\n    assert (n_files % 2) == 0\n    gold_files, sys_files = files[:n_files // 2], files[n_files // 2:]\n    UAS = sdp_eval(gold_files, sys_files, labeled=False)\n    LAS = sdp_eval(gold_files, sys_files, labeled=True)\n    # print(UAS.F1, UAS.seq_acc)\n    print('UAS={:0.1f}'.format(UAS.F1 * 100))\n    print('LAS={:0.1f}'.format(LAS.F1 * 100))\n\n\nif __name__ == '__main__':\n    main()\n", "hanlp/metrics/amr/smatch_eval.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-24 12:47\nimport os\nimport warnings\nfrom typing import Union\n\nfrom hanlp.metrics.f1 import F1_\nfrom hanlp.metrics.mtl import MetricDict\nfrom hanlp.utils.io_util import get_resource, run_cmd, pushd\nfrom hanlp.utils.log_util import flash\n\n_SMATCH_SCRIPT = 'https://github.com/ChunchuanLv/amr-evaluation-tool-enhanced/archive/master.zip#evaluation.sh'\n_FAST_SMATCH_SCRIPT = 'https://github.com/jcyk/AMR-gs/archive/master.zip#tools/fast_smatch/compute_smatch.sh'\n\n\nclass SmatchScores(MetricDict):\n    @property\n    def score(self):\n        return self['Smatch'].score\n\n\ndef smatch_eval(pred, gold, use_fast=False) -> Union[SmatchScores, F1_]:\n    script = get_resource(_FAST_SMATCH_SCRIPT if use_fast else _SMATCH_SCRIPT)\n    home = os.path.dirname(script)\n    pred = os.path.realpath(pred)\n    gold = os.path.realpath(gold)\n    with pushd(home):\n        flash('Running evaluation script [blink][yellow]...[/yellow][/blink]')\n        cmd = f'bash {script} {pred} {gold}'\n        text = run_cmd(cmd)\n        flash('')\n    return format_fast_scores(text) if use_fast else format_official_scores(text)\n\n\ndef post_process(pred, amr_version):\n    pred = os.path.realpath(pred)\n    utils_tar_gz = get_amr_utils(amr_version)\n    util_dir = get_resource(utils_tar_gz)\n    stog_home = get_resource('https://github.com/jcyk/AMR-gs/archive/master.zip')\n    with pushd(stog_home):\n        run_cmd(\n            f'python3 -u -m stog.data.dataset_readers.amr_parsing.postprocess.postprocess '\n            f'--amr_path {pred} --util_dir {util_dir} --v 2')\n    return pred + '.post'\n\n\ndef get_amr_utils(amr_version):\n    if amr_version == '1.0':\n        utils_tar_gz = 'https://www.cs.jhu.edu/~s.zhang/data/AMR/amr_1.0_utils.tar.gz'\n    elif amr_version == '2.0':\n        utils_tar_gz = 'https://www.cs.jhu.edu/~s.zhang/data/AMR/amr_2.0_utils.tar.gz'\n    elif amr_version == '3.0':\n        utils_tar_gz = 'https://file.hankcs.com/research/amr2020/amr_3.0_utils.tgz'\n    else:\n        raise ValueError(f'Unsupported AMR version {amr_version}')\n    return utils_tar_gz\n\n\ndef format_official_scores(text: str):\n    # Smatch -> P: 0.136, R: 0.107, F: 0.120\n    # Unlabeled -> P: 0.229, R: 0.180, F: 0.202\n    # No WSD -> P: 0.137, R: 0.108, F: 0.120\n    # Non_sense_frames -> P: 0.008, R: 0.008, F: 0.008\n    # Wikification -> P: 0.000, R: 0.000, F: 0.000\n    # Named Ent. -> P: 0.222, R: 0.092, F: 0.130\n    # Negations -> P: 0.000, R: 0.000, F: 0.000\n    # IgnoreVars -> P: 0.005, R: 0.003, F: 0.003\n    # Concepts -> P: 0.075, R: 0.036, F: 0.049\n    # Frames -> P: 0.007, R: 0.007, F: 0.007\n    # Reentrancies -> P: 0.113, R: 0.060, F: 0.079\n    # SRL -> P: 0.145, R: 0.104, F: 0.121\n    scores = SmatchScores()\n    for line in text.split('\\n'):\n        line = line.strip()\n        if not line:\n            continue\n        name, vs = line.split(' -> ')\n        try:\n            p, r, f = [float(x.split(': ')[-1]) for x in vs.split(', ')]\n        except ValueError:\n            warnings.warn(f'Failed to parse results from smatch: {line}')\n            p, r, f = float(\"nan\"), float(\"nan\"), float(\"nan\")\n        scores[name] = F1_(p, r, f)\n    return scores\n\n\ndef format_fast_scores(text: str):\n    # using fast smatch\n    # Precision: 0.137\n    # Recall: 0.108\n    # Document F-score: 0.121\n    scores = []\n    for line in text.split('\\n'):\n        line = line.strip()\n        if not line or ':' not in line:\n            continue\n        name, score = line.split(': ')\n        scores.append(float(score))\n    assert len(scores) == 3\n    return F1_(*scores)\n", "hanlp/metrics/amr/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-24 12:47", "hanlp/metrics/chunking/iobes_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-09-14 21:55\n\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.metrics.chunking.conlleval import SpanF1\nfrom hanlp.metrics.chunking.chunking_f1_tf import ChunkingF1_TF\n\n\nclass IOBES_F1_TF(ChunkingF1_TF):\n\n    def __init__(self, tag_vocab: VocabTF, from_logits=True, name='f1', dtype=None, **kwargs):\n        super().__init__(tag_vocab, from_logits, name, dtype, **kwargs)\n        self.state = SpanF1()\n\n    def update_tags(self, true_tags, pred_tags):\n        # true_tags = list(itertools.chain.from_iterable(true_tags))\n        # pred_tags = list(itertools.chain.from_iterable(pred_tags))\n        # self.state.update_state(true_tags, pred_tags)\n        for gold, pred in zip(true_tags, pred_tags):\n            self.state.update_state(gold, pred)\n        return self.result()\n\n    def result(self):\n        return self.state.result(full=False, verbose=False).fscore\n\n    def reset_states(self):\n        self.state.reset_state()\n", "hanlp/metrics/chunking/chunking_f1_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-29 23:09\nfrom abc import ABC, abstractmethod\n\nimport tensorflow as tf\n\nfrom hanlp.common.vocab_tf import VocabTF\n\n\nclass ChunkingF1_TF(tf.keras.metrics.Metric, ABC):\n\n    def __init__(self, tag_vocab: VocabTF, from_logits=True, name='f1', dtype=None, **kwargs):\n        super().__init__(name, dtype, dynamic=True, **kwargs)\n        self.tag_vocab = tag_vocab\n        self.from_logits = from_logits\n\n    def update_the_state(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight: tf.Tensor = None, **kwargs):\n        if sample_weight is None:\n            if hasattr(y_pred, '_keras_mask'):\n                mask = y_pred._keras_mask\n            else:\n                mask = None\n        else:\n            mask = sample_weight\n        if self.tag_vocab.pad_idx is not None and mask is None:\n            # in this case, the model doesn't compute mask but provide a masking index, it's ok to\n            mask = y_true != self.tag_vocab.pad_idx\n        assert mask is not None, 'ChunkingF1 requires masking, check your _keras_mask or compute_mask'\n        if self.from_logits:\n            y_pred = tf.argmax(y_pred, axis=-1)\n        y_true = self.to_tags(y_true, mask)\n        y_pred = self.to_tags(y_pred, mask)\n        return self.update_tags(y_true, y_pred)\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight: tf.Tensor = None, **kwargs):\n        return self.update_the_state(y_true, y_pred, sample_weight)\n\n    def update_state(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight: tf.Tensor = None, **kwargs):\n        return self.update_the_state(y_true, y_pred, sample_weight)\n\n    def to_tags(self, y: tf.Tensor, sample_weight: tf.Tensor):\n        batch = []\n        y = y.numpy()\n        sample_weight = sample_weight.numpy()\n        for sent, mask in zip(y, sample_weight):\n            tags = []\n            for tag, m in zip(sent, mask):\n                if not m:\n                    continue\n                tag = int(tag)\n                if self.tag_vocab.pad_idx is not None and tag == self.tag_vocab.pad_idx:\n                    # If model predicts <pad>, it will fail most metrics. So replace it with a valid one\n                    tag = 1\n                tags.append(self.tag_vocab.get_token(tag))\n            batch.append(tags)\n        return batch\n\n    @abstractmethod\n    def update_tags(self, true_tags, pred_tags):\n        pass\n\n    @abstractmethod\n    def result(self):\n        pass\n", "hanlp/metrics/chunking/binary_chunking_f1.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-08-02 14:27\nfrom collections import defaultdict\nfrom typing import List, Union\n\nimport torch\n\nfrom hanlp.metrics.f1 import F1\n\n\nclass BinaryChunkingF1(F1):\n    def __call__(self, pred_tags: torch.LongTensor, gold_tags: torch.LongTensor, lens: List[int] = None):\n        if lens is None:\n            lens = [gold_tags.size(1)] * gold_tags.size(0)\n        self.update(self.decode_spans(pred_tags, lens), self.decode_spans(gold_tags, lens))\n\n    def update(self, pred_tags, gold_tags):\n        for pred, gold in zip(pred_tags, gold_tags):\n            super().__call__(set(pred), set(gold))\n\n    @staticmethod\n    def decode_spans(pred_tags: torch.LongTensor, lens: Union[List[int], torch.LongTensor]):\n        if isinstance(lens, torch.Tensor):\n            lens = lens.tolist()\n        batch_pred = defaultdict(list)\n        for batch, offset in pred_tags.nonzero(as_tuple=False).tolist():\n            batch_pred[batch].append(offset)\n        batch_pred_spans = [[(0, l)] for l in lens]\n        for batch, offsets in batch_pred.items():\n            l = lens[batch]\n            batch_pred_spans[batch] = list(zip(offsets, offsets[1:] + [l]))\n        return batch_pred_spans\n", "hanlp/metrics/chunking/conlleval.py": "#!/usr/bin/env python\n\n# Python version of the evaluation script from CoNLL'00-\n\n# Intentional differences:\n# - accept any space as delimiter by default\n# - optional file argument (default STDIN)\n# - option to set boundary (-b argument)\n# - LaTeX output (-l argument) not supported\n# - raw tags (-r argument) not supported\nimport io\nimport sys\n\nfrom collections import defaultdict, namedtuple\nfrom typing import Tuple, Union, List\n\nfrom hanlp.utils.span_util import bio_tags_to_spans\n\nfrom hanlp.metrics.metric import Metric\n\nANY_SPACE = '<SPACE>'\n\n\nclass FormatError(Exception):\n    pass\n\n\nDetailedF1 = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n\n\nclass EvalCounts(object):\n    def __init__(self):\n        self.correct_chunk = 0  # number of correctly identified chunks\n        self.correct_tags = 0  # number of correct chunk tags\n        self.total_gold = 0  # number of chunks in corpus\n        self.total_pred = 0  # number of identified chunks\n        self.token_counter = 0  # token counter (ignores sentence breaks)\n\n        # counts by type\n        self.t_correct_chunk = defaultdict(int)\n        self.t_total_gold = defaultdict(int)\n        self.t_total_pred = defaultdict(int)\n\n    @property\n    def states(self):\n        return (self.t_correct_chunk, self.t_total_gold, self.t_total_pred)\n\n    def reset_state(self):\n        self.correct_chunk = 0  # number of correctly identified chunks\n        self.correct_tags = 0  # number of correct chunk tags\n        self.total_gold = 0  # number of chunks in corpus\n        self.total_pred = 0  # number of identified chunks\n        self.token_counter = 0  # token counter (ignores sentence breaks)\n        for state in self.states:\n            state.clear()\n\n\nclass SpanF1(Metric):\n\n    def __init__(self, label_encoding='IOBES') -> None:\n        super().__init__()\n        self.label_encoding = label_encoding\n        self.count = EvalCounts()\n\n    def reset(self):\n        self.count = EvalCounts()\n\n    @property\n    def score(self):\n        return self.result(False, False).fscore\n\n    def reset_state(self):\n        self.count.reset_state()\n\n    def update_state(self, true_seqs: List[str], pred_seqs: List[str]):\n        if self.label_encoding == 'IOBES':\n            count = evaluate_iobes(true_seqs, pred_seqs)\n        elif self.label_encoding in ['IOB2', 'BIO']:\n            count = evaluate_iob2(true_seqs, pred_seqs)\n        else:\n            raise ValueError(f'Unrecognized label encoding {self.label_encoding}')\n        self.count.correct_chunk += count.correct_chunk\n        self.count.correct_tags += count.correct_tags\n        self.count.total_gold += count.total_gold\n        self.count.total_pred += count.total_pred\n        self.count.token_counter += count.token_counter\n        for s, n in zip(self.count.states, count.states):\n            for k, v in n.items():\n                s[k] = s.get(k, 0) + v\n\n    def batch_update_state(self, true_seqs: List[List[str]], pred_seqs: List[List[str]]):\n        for t, p in zip(true_seqs, pred_seqs):\n            self.update_state(t, p)\n\n    def result(self, full=True, verbose=True) -> Union[Tuple[DetailedF1, dict, str], DetailedF1]:\n        if full:\n            out = io.StringIO()\n            overall, by_type = report(self.count, out)\n            text = out.getvalue()\n            if verbose:\n                print(text)\n            out.close()\n            return overall, by_type, text\n        else:\n            overall, _ = metrics(self.count)\n            return overall\n\n    # torch convention: put pred before gold\n    def __call__(self, pred_seqs: List[List[str]], true_seqs: List[List[str]]):\n        return self.batch_update_state(true_seqs, pred_seqs)\n\n    def __repr__(self) -> str:\n        result = self.result(False, False)\n        return f\"P: {result.prec:.2%} R: {result.rec:.2%} F: {result.fscore:.2%}\"\n\n\ndef parse_args(argv):\n    import argparse\n    parser = argparse.ArgumentParser(\n        description='evaluate tagging results using CoNLL criteria',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    arg = parser.add_argument\n    arg('-b', '--boundary', metavar='STR', default='-X-',\n        help='sentence boundary')\n    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n        help='character delimiting items in input')\n    arg('-o', '--otag', metavar='CHAR', default='O',\n        help='alternative outside tag')\n    arg('file', nargs='?', default=None)\n    return parser.parse_args(argv)\n\n\ndef split_tag(chunk_tag):\n    \"\"\"split chunk tag into IOBES prefix and chunk_type\n    e.g.\n    B-PER -> (B, PER)\n    O -> (O, None)\n\n    Args:\n      chunk_tag: \n\n    Returns:\n\n    \"\"\"\n    if chunk_tag == 'O':\n        return ('O', None)\n    return chunk_tag.split('-', maxsplit=1)\n\n\ndef evaluate_iobes(true_seqs, pred_seqs):\n    counts = EvalCounts()\n    in_correct = False  # currently processed chunks is correct until now\n    last_correct = 'O'  # previous chunk tag in corpus\n    last_correct_type = ''  # type of previously identified chunk tag\n    last_guessed = 'O'  # previously identified chunk tag\n    last_guessed_type = ''  # type of previous chunk tag in corpus\n\n    for true_tag, pred_tag in zip(true_seqs, pred_seqs):\n\n        guessed, guessed_type = split_tag(pred_tag)\n        correct, correct_type = split_tag(true_tag)\n\n        end_correct = end_of_chunk(last_correct, correct,\n                                   last_correct_type, correct_type)\n        end_guessed = end_of_chunk(last_guessed, guessed,\n                                   last_guessed_type, guessed_type)\n        start_correct = start_of_chunk(last_correct, correct,\n                                       last_correct_type, correct_type)\n        start_guessed = start_of_chunk(last_guessed, guessed,\n                                       last_guessed_type, guessed_type)\n\n        if in_correct:\n            if (end_correct and end_guessed and\n                    last_guessed_type == last_correct_type):\n                in_correct = False\n                counts.correct_chunk += 1\n                counts.t_correct_chunk[last_correct_type] += 1\n            elif (end_correct != end_guessed or guessed_type != correct_type):\n                in_correct = False\n\n        if start_correct and start_guessed and guessed_type == correct_type:\n            in_correct = True\n\n        if start_correct:\n            counts.total_gold += 1\n            counts.t_total_gold[correct_type] += 1\n        if start_guessed:\n            counts.total_pred += 1\n            counts.t_total_pred[guessed_type] += 1\n        if correct == guessed and guessed_type == correct_type:\n            counts.correct_tags += 1\n        counts.token_counter += 1\n\n        last_guessed = guessed\n        last_correct = correct\n        last_guessed_type = guessed_type\n        last_correct_type = correct_type\n\n    if in_correct:\n        counts.correct_chunk += 1\n        counts.t_correct_chunk[last_correct_type] += 1\n\n    return counts\n\n\ndef evaluate_iob2(true_seqs, pred_seqs):\n    counts = EvalCounts()\n    gold = set(bio_tags_to_spans(true_seqs))\n    pred = set(bio_tags_to_spans(pred_seqs))\n    counts.correct_chunk = len(gold & pred)\n    counts.total_pred = len(pred)\n    counts.total_gold = len(gold)\n    return counts\n\n\ndef uniq(iterable):\n    seen = set()\n    return [i for i in iterable if not (i in seen or seen.add(i))]\n\n\ndef calculate_metrics(correct, guessed, total):\n    tp, fp, fn = correct, guessed - correct, total - correct\n    p = 0. if tp + fp == 0 else 1. * tp / (tp + fp)\n    r = 0. if tp + fn == 0 else 1. * tp / (tp + fn)\n    f = 0. if p + r == 0 else 2 * p * r / (p + r)\n    return DetailedF1(tp, fp, fn, p, r, f)\n\n\ndef calc_metrics(tp, p, t, percent=True):\n    \"\"\"compute overall precision, recall and FB1 (default values are 0.0)\n    if percent is True, return 100 * original decimal value\n\n    Args:\n      tp: \n      p: \n      t: \n      percent:  (Default value = True)\n\n    Returns:\n\n    \"\"\"\n    precision = tp / p if p else 0\n    recall = tp / t if t else 0\n    fb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n    if percent:\n        return 100 * precision, 100 * recall, 100 * fb1\n    else:\n        return precision, recall, fb1\n\n\ndef metrics(counts):\n    c = counts\n    overall = calculate_metrics(\n        c.correct_chunk, c.total_pred, c.total_gold\n    )\n    by_type = {}\n    for t in uniq(list(c.t_total_gold.keys()) + list(c.t_total_pred.keys())):\n        by_type[t] = calculate_metrics(\n            c.t_correct_chunk[t], c.t_total_pred[t], c.t_total_gold[t]\n        )\n    return overall, by_type\n\n\ndef report(counts, out=None):\n    if out is None:\n        out = sys.stdout\n\n    overall, by_type = metrics(counts)\n\n    c = counts\n    out.write('processed %d tokens with %d phrases; ' %\n              (c.token_counter, c.total_gold))\n    out.write('found: %d phrases; correct: %d.\\n' %\n              (c.total_pred, c.correct_chunk))\n\n    if c.token_counter > 0:\n        out.write('accuracy: %6.2f%%; ' %\n                  (100. * c.correct_tags / c.token_counter))\n        out.write('precision: %6.2f%%; ' % (100. * overall.prec))\n        out.write('recall: %6.2f%%; ' % (100. * overall.rec))\n        out.write('FB1: %6.2f\\n' % (100. * overall.fscore))\n\n    for i, m in sorted(by_type.items()):\n        out.write('%17s: ' % i)\n        out.write('precision: %6.2f%%; ' % (100. * m.prec))\n        out.write('recall: %6.2f%%; ' % (100. * m.rec))\n        out.write('FB1: %6.2f  %d\\n' % (100. * m.fscore, c.t_total_pred[i]))\n    return overall, by_type\n\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk ended between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    return ((prev_tag == \"B\" and tag == \"B\") or\n            (prev_tag == \"B\" and tag == \"O\") or\n            (prev_tag == \"I\" and tag == \"B\") or\n            (prev_tag == \"I\" and tag == \"O\") or\n\n            (prev_tag == \"E\" and tag == \"E\") or\n            (prev_tag == \"E\" and tag == \"I\") or\n            (prev_tag == \"E\" and tag == \"O\") or\n            (prev_tag == \"I\" and tag == \"O\") or\n\n            (prev_tag != \"O\" and prev_tag != \".\" and prev_type != type_) or\n            (prev_tag == \"]\" or prev_tag == \"[\"))\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk started between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    chunkStart = ((prev_tag == \"B\" and tag == \"B\") or\n                  (prev_tag == \"B\" and tag == \"B\") or\n                  (prev_tag == \"I\" and tag == \"B\") or\n                  (prev_tag == \"O\" and tag == \"B\") or\n                  (prev_tag == \"O\" and tag == \"I\") or\n\n                  (prev_tag == \"E\" and tag == \"E\") or\n                  (prev_tag == \"E\" and tag == \"I\") or\n                  (prev_tag == \"O\" and tag == \"E\") or\n                  (prev_tag == \"O\" and tag == \"I\") or\n\n                  (tag != \"O\" and tag != \".\" and prev_type != type_) or\n                  (tag == \"]\" or tag == \"[\"))\n    # corrected 1998-12-22: these chunks are assumed to have length 1\n\n    # print(\"startOfChunk?\", prevTag, tag, prevType, type)\n    # print(chunkStart)\n    return chunkStart\n\n\ndef main(argv):\n    args = parse_args(argv[1:])\n\n    if args.file is None:\n        counts = evaluate_iobes(sys.stdin, args)\n    else:\n        with open(args.file, encoding='utf-8') as f:\n            counts = evaluate_iobes(f, args)\n    report(counts)\n\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n", "hanlp/metrics/chunking/chunking_f1.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-06-11 22:14\nimport io\nfrom collections import defaultdict\nfrom typing import List, Set, Tuple, Dict\n\nfrom hanlp.metrics.chunking.conlleval import calculate_metrics, DetailedF1, metrics\nfrom hanlp.metrics.chunking.sequence_labeling import get_entities\nfrom hanlp.metrics.f1 import F1\nfrom hanlp.metrics.metric import Metric\n\n\nclass ChunkingF1(F1):\n\n    def __call__(self, pred_tags: List[List[str]], gold_tags: List[List[str]]):\n        for p, g in zip(pred_tags, gold_tags):\n            pred = set(get_entities(p))\n            gold = set(get_entities(g))\n            self.nb_pred += len(pred)\n            self.nb_true += len(gold)\n            self.nb_correct += len(pred & gold)\n\n\nclass DetailedSpanF1(Metric):\n    def __init__(self, do_confusion_matrix=False):\n        self.correct_chunk = 0  # number of correctly identified chunks\n        self.correct_unlabeled = 0\n        self.total_gold = 0  # number of chunks in corpus\n        self.total_pred = 0  # number of identified chunks\n        self.token_counter = 0  # token counter (ignores sentence breaks)\n\n        # counts by type\n        self.t_correct_chunk = defaultdict(int)\n        self.t_total_gold = defaultdict(int)\n        self.t_total_pred = defaultdict(int)\n\n        self.do_confusion_matrix = do_confusion_matrix\n        if do_confusion_matrix:\n            self.pred_labels = []\n            self.gold_labels = []\n\n    @property\n    def states(self):\n        return (self.t_correct_chunk, self.t_total_gold, self.t_total_pred)\n\n    def reset_state(self):\n        self.correct_chunk = 0  # number of correctly identified chunks\n        self.total_gold = 0  # number of chunks in corpus\n        self.total_pred = 0  # number of identified chunks\n        self.token_counter = 0  # token counter (ignores sentence breaks)\n        for state in self.states:\n            state.clear()\n        if self.do_confusion_matrix:\n            self.pred_labels = []\n            self.gold_labels = []\n\n    @property\n    def score(self):\n        overall = calculate_metrics(\n            self.correct_chunk, self.total_pred, self.total_gold\n        )\n        return overall.fscore\n\n    def __call__(self, pred: Set[Tuple[int, int, str]], gold: Set[Tuple[int, int, str]], num_tokens=None):\n        pred_chunks_unlabeled = set((b, e) for b, e, l in pred)\n        gold_chunks_unlabeled = set((b, e) for b, e, l in gold)\n        self.correct_unlabeled += len(pred_chunks_unlabeled & gold_chunks_unlabeled)\n        self.correct_chunk += len(pred & gold)\n        self.total_gold += len(gold)\n        self.total_pred += len(pred)\n        if num_tokens:\n            self.token_counter += num_tokens\n\n        def group_by_tag(collection: Set[Tuple[int, int, str]]):\n            group = defaultdict(set)\n            for b, e, l in collection:\n                group[l].add((b, e))\n            return group\n\n        pred_tags = group_by_tag(pred)\n        gold_tags = group_by_tag(gold)\n        for l in pred_tags.keys() | gold_tags.keys():\n            self.t_correct_chunk[l] += len(pred_tags[l] & gold_tags[l])\n            self.t_total_gold[l] += len(gold_tags[l])\n            self.t_total_pred[l] += len(pred_tags[l])\n\n        if self.do_confusion_matrix:\n            def group_by_span(collection: Set[Tuple[int, int, str]]):\n                group = dict()\n                for b, e, l in collection:\n                    group[(b, e)] = l\n                return group\n\n            pred_spans = group_by_span(pred)\n            gold_spans = group_by_span(gold)\n            for span in pred_spans.keys() & gold_spans.keys():\n                self.pred_labels.append(pred_spans[span])\n                self.gold_labels.append(gold_spans[span])\n\n    def reset(self):\n        self.reset_state()\n\n    def report(self) -> Tuple[DetailedF1, Dict[str, DetailedF1], str]:\n        out = io.StringIO()\n\n        c = self\n        out.write('processed %d tokens with %d phrases; ' % (c.token_counter, c.total_gold))\n        out.write('found: %d phrases; correct: %d.\\n' % (c.total_pred, c.correct_chunk))\n\n        overall = calculate_metrics(c.correct_unlabeled, c.total_pred, c.total_gold)\n        out.write('%17s: ' % 'unlabeled overall')\n        out.write('precision: %6.2f%%; ' % (100. * overall.prec))\n        out.write('recall: %6.2f%%; ' % (100. * overall.rec))\n        out.write('FB1: %6.2f\\n' % (100. * overall.fscore))\n\n        overall, by_type = metrics(self)\n        out.write('%17s: ' % 'labeled overall')\n        out.write('precision: %6.2f%%; ' % (100. * overall.prec))\n        out.write('recall: %6.2f%%; ' % (100. * overall.rec))\n        out.write('FB1: %6.2f\\n' % (100. * overall.fscore))\n\n        for i, m in sorted(by_type.items()):\n            out.write('%17s: ' % i)\n            out.write('precision: %6.2f%%; ' % (100. * m.prec))\n            out.write('recall: %6.2f%%; ' % (100. * m.rec))\n            out.write('FB1: %6.2f  %d\\n' % (100. * m.fscore, c.t_total_pred[i]))\n        text = out.getvalue()\n        out.close()\n        return overall, by_type, text\n\n    def __str__(self) -> str:\n        return self.report()[-1]\n\n    def confusion_matrix(self):\n        from sklearn.metrics import confusion_matrix\n        labels = sorted(self.gold_labels + self.pred_labels)\n        return confusion_matrix(self.gold_labels, self.pred_labels, labels=labels), labels\n", "hanlp/metrics/chunking/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-12-21 03:49", "hanlp/metrics/chunking/bmes_tf.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2019-09-14 21:55\n\nfrom hanlp.common.vocab_tf import VocabTF\nfrom hanlp.metrics.chunking.chunking_f1_tf import ChunkingF1_TF\nfrom hanlp.metrics.chunking.sequence_labeling import get_entities\n\n\nclass BMES_F1_TF(ChunkingF1_TF):\n\n    def __init__(self, tag_vocab: VocabTF, from_logits=True, suffix=False, name='f1', dtype=None, **kwargs):\n        super().__init__(tag_vocab, from_logits, name, dtype, **kwargs)\n        self.nb_correct = 0\n        self.nb_pred = 0\n        self.nb_true = 0\n        self.suffix = suffix\n\n    def update_tags(self, true_tags, pred_tags):\n        for t, p in zip(true_tags, pred_tags):\n            self.update_entities(get_entities(t, self.suffix), get_entities(p, self.suffix))\n        return self.result()\n\n    def update_entities(self, true_entities, pred_entities):\n        true_entities = set(true_entities)\n        pred_entities = set(pred_entities)\n        nb_correct = len(true_entities & pred_entities)\n        nb_pred = len(pred_entities)\n        nb_true = len(true_entities)\n        self.nb_correct += nb_correct\n        self.nb_pred += nb_pred\n        self.nb_true += nb_true\n\n    def result(self):\n        nb_correct = self.nb_correct\n        nb_pred = self.nb_pred\n        nb_true = self.nb_true\n        p = nb_correct / nb_pred if nb_pred > 0 else 0\n        r = nb_correct / nb_true if nb_true > 0 else 0\n        score = 2 * p * r / (p + r) if p + r > 0 else 0\n\n        return score\n\n    def reset_states(self):\n        self.nb_correct = 0\n        self.nb_pred = 0\n        self.nb_true = 0\n", "hanlp/metrics/chunking/sequence_labeling.py": "# MIT License\n#\n# Copyright (c) 2018 chakki\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"Metrics to assess performance on sequence labeling task given prediction\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\"\"\"\n\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef iobes_to_span(words, tags):\n    delimiter = ' '\n    if all([len(w) == 1 for w in words]):\n        delimiter = ''  # might be Chinese\n    entities = []\n    for tag, start, end in get_entities(tags):\n        entities.append((delimiter.join(words[start:end]), tag, start, end))\n    yield entities\n\n\ndef get_entities(seq, suffix=False):\n    \"\"\"Gets entities from sequence.\n\n    Args:\n      seq(list): sequence of labels.\n      suffix:  (Default value = False)\n\n    Returns:\n      list: list of (chunk_type, chunk_start, chunk_end).\n      Example:\n\n    >>> from seqeval.metrics.sequence_labeling import get_entities\n        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n        >>> get_entities(seq)\n        [('PER', 0, 2), ('LOC', 3, 4)]\n    \"\"\"\n    # for nested list\n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + ['O']]\n\n    prev_tag = 'O'\n    prev_type = ''\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + ['O']):\n        if suffix:\n            tag = chunk[-1]\n            type_ = chunk[:-2]\n        else:\n            tag = chunk[0]\n            type_ = chunk[2:]\n\n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i))\n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n        prev_tag = tag\n        prev_type = type_\n\n    return chunks\n\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk ended between the previous and current word.\n\n    Args:\n      prev_tag: previous chunk tag.\n      tag: current chunk tag.\n      prev_type: previous type.\n      type_: current type.\n\n    Returns:\n      chunk_end: boolean.\n\n    \"\"\"\n    chunk_end = False\n\n    if prev_tag == 'E': chunk_end = True\n    if prev_tag == 'S': chunk_end = True\n\n    if prev_tag == 'B' and tag == 'B': chunk_end = True\n    if prev_tag == 'B' and tag == 'S': chunk_end = True\n    if prev_tag == 'B' and tag == 'O': chunk_end = True\n    if prev_tag == 'I' and tag == 'B': chunk_end = True\n    if prev_tag == 'I' and tag == 'S': chunk_end = True\n    if prev_tag == 'I' and tag == 'O': chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk started between the previous and current word.\n\n    Args:\n      prev_tag: previous chunk tag.\n      tag: current chunk tag.\n      prev_type: previous type.\n      type_: current type.\n\n    Returns:\n      chunk_start: boolean.\n\n    \"\"\"\n    chunk_start = False\n\n    if tag == 'B': chunk_start = True\n    if tag == 'S': chunk_start = True\n\n    if prev_tag == 'E' and tag == 'E': chunk_start = True\n    if prev_tag == 'E' and tag == 'I': chunk_start = True\n    if prev_tag == 'S' and tag == 'E': chunk_start = True\n    if prev_tag == 'S' and tag == 'I': chunk_start = True\n    if prev_tag == 'O' and tag == 'E': chunk_start = True\n    if prev_tag == 'O' and tag == 'I': chunk_start = True\n\n    if tag != 'O' and tag != '.' and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start\n\n\ndef f1_score(y_true, y_pred, average='micro', suffix=False):\n    \"\"\"Compute the F1 score.\n    \n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n    \n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a tagger.\n      average:  (Default value = 'micro')\n      suffix:  (Default value = False)\n\n    Returns:\n      score: float.\n      Example:\n\n    >>> from seqeval.metrics import f1_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> f1_score(y_true, y_pred)\n        0.50\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n    nb_true = len(true_entities)\n\n    p = nb_correct / nb_pred if nb_pred > 0 else 0\n    r = nb_correct / nb_true if nb_true > 0 else 0\n    score = 2 * p * r / (p + r) if p + r > 0 else 0\n\n    return score\n\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Accuracy classification score.\n    \n    In multilabel classification, this function computes subset accuracy:\n    the set of labels predicted for a sample must *exactly* match the\n    corresponding set of labels in y_true.\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n      score: float.\n      Example:\n\n    >>> from seqeval.metrics import accuracy_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> accuracy_score(y_true, y_pred)\n        0.80\n    \"\"\"\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n\n    nb_correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n    nb_true = len(y_true)\n\n    score = nb_correct / nb_true\n\n    return score\n\n\ndef precision_score(y_true, y_pred, average='micro', suffix=False):\n    \"\"\"Compute the precision.\n    \n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample.\n    \n    The best value is 1 and the worst value is 0.\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a tagger.\n      average:  (Default value = 'micro')\n      suffix:  (Default value = False)\n\n    Returns:\n      score: float.\n      Example:\n\n    >>> from seqeval.metrics import precision_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> precision_score(y_true, y_pred)\n        0.50\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n\n    score = nb_correct / nb_pred if nb_pred > 0 else 0\n\n    return score\n\n\ndef recall_score(y_true, y_pred, average='micro', suffix=False):\n    \"\"\"Compute the recall.\n    \n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n    \n    The best value is 1 and the worst value is 0.\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a tagger.\n      average:  (Default value = 'micro')\n      suffix:  (Default value = False)\n\n    Returns:\n      score: float.\n      Example:\n\n    >>> from seqeval.metrics import recall_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> recall_score(y_true, y_pred)\n        0.50\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_true = len(true_entities)\n\n    score = nb_correct / nb_true if nb_true > 0 else 0\n\n    return score\n\n\ndef performance_measure(y_true, y_pred):\n    \"\"\"Compute the performance metrics: TP, FP, FN, TN\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n      performance_dict: dict\n      Example:\n\n    >>> from seqeval.metrics import performance_measure\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> performance_measure(y_true, y_pred)\n        (3, 3, 1, 4)\n    \"\"\"\n    performace_dict = dict()\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n    performace_dict['TP'] = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred)\n                                if ((y_t != 'O') or (y_p != 'O')))\n    performace_dict['FP'] = sum(y_t != y_p for y_t, y_p in zip(y_true, y_pred))\n    performace_dict['FN'] = sum(((y_t != 'O') and (y_p == 'O'))\n                                for y_t, y_p in zip(y_true, y_pred))\n    performace_dict['TN'] = sum((y_t == y_p == 'O')\n                                for y_t, y_p in zip(y_true, y_pred))\n\n    return performace_dict\n\n\ndef classification_report(y_true, y_pred, digits=2, suffix=False):\n    \"\"\"Build a text report showing the main classification metrics.\n\n    Args:\n      y_true: 2d array. Ground truth (correct) target values.\n      y_pred: 2d array. Estimated targets as returned by a classifier.\n      digits: int. Number of digits for formatting output floating point values. (Default value = 2)\n      suffix:  (Default value = False)\n\n    Returns:\n      report: string. Text summary of the precision, recall, F1 score for each class.\n      Examples:\n\n    >>> from seqeval.metrics import classification_report\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> print(classification_report(y_true, y_pred))\n                     precision    recall  f1-score   support\n        <BLANKLINE>\n               MISC       0.00      0.00      0.00         1\n                PER       1.00      1.00      1.00         1\n        <BLANKLINE>\n          micro avg       0.50      0.50      0.50         2\n          macro avg       0.50      0.50      0.50         2\n        <BLANKLINE>\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    name_width = 0\n    d1 = defaultdict(set)\n    d2 = defaultdict(set)\n    for e in true_entities:\n        d1[e[0]].add((e[1], e[2]))\n        name_width = max(name_width, len(e[0]))\n    for e in pred_entities:\n        d2[e[0]].add((e[1], e[2]))\n\n    last_line_heading = 'macro avg'\n    width = max(name_width, len(last_line_heading), digits)\n\n    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n    report = head_fmt.format(u'', *headers, width=width)\n    report += u'\\n\\n'\n\n    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n\n    ps, rs, f1s, s = [], [], [], []\n    for type_name, true_entities in d1.items():\n        pred_entities = d2[type_name]\n        nb_correct = len(true_entities & pred_entities)\n        nb_pred = len(pred_entities)\n        nb_true = len(true_entities)\n\n        p = nb_correct / nb_pred if nb_pred > 0 else 0\n        r = nb_correct / nb_true if nb_true > 0 else 0\n        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n\n        ps.append(p)\n        rs.append(r)\n        f1s.append(f1)\n        s.append(nb_true)\n\n    report += u'\\n'\n\n    # compute averages\n    report += row_fmt.format('micro avg',\n                             precision_score(y_true, y_pred, suffix=suffix),\n                             recall_score(y_true, y_pred, suffix=suffix),\n                             f1_score(y_true, y_pred, suffix=suffix),\n                             np.sum(s),\n                             width=width, digits=digits)\n    report += row_fmt.format(last_line_heading,\n                             np.average(ps, weights=s),\n                             np.average(rs, weights=s),\n                             np.average(f1s, weights=s),\n                             np.sum(s),\n                             width=width, digits=digits)\n\n    return report\n", "hanlp/metrics/srl/srlconll.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-16 18:44\nimport os\n\nfrom hanlp.utils.io_util import get_resource, get_exitcode_stdout_stderr, run_cmd\n\n\ndef official_conll_05_evaluate(pred_path, gold_path):\n    script_root = get_resource('http://www.lsi.upc.edu/~srlconll/srlconll-1.1.tgz')\n    lib_path = f'{script_root}/lib'\n    if lib_path not in os.environ.get(\"PERL5LIB\", \"\"):\n        os.environ['PERL5LIB'] = f'{lib_path}:{os.environ.get(\"PERL5LIB\", \"\")}'\n    bin_path = f'{script_root}/bin'\n    if bin_path not in os.environ.get('PATH', ''):\n        os.environ['PATH'] = f'{bin_path}:{os.environ.get(\"PATH\", \"\")}'\n    eval_info_gold_pred = run_cmd(f'perl {script_root}/bin/srl-eval.pl {gold_path} {pred_path}')\n    eval_info_pred_gold = run_cmd(f'perl {script_root}/bin/srl-eval.pl {pred_path} {gold_path}')\n    conll_recall = float(eval_info_gold_pred.strip().split(\"\\n\")[6].strip().split()[5]) / 100\n    conll_precision = float(eval_info_pred_gold.strip().split(\"\\n\")[6].strip().split()[5]) / 100\n    if conll_recall + conll_precision > 0:\n        conll_f1 = 2 * conll_recall * conll_precision / (conll_recall + conll_precision)\n    else:\n        conll_f1 = 0\n    return conll_precision, conll_recall, conll_f1\n\n\ndef run_perl(script, src, dst=None):\n    os.environ['PERL5LIB'] = f''\n    exitcode, out, err = get_exitcode_stdout_stderr(\n        f'perl -I{os.path.expanduser(\"~/.local/lib/perl5\")} {script} {src}')\n    if exitcode:\n        # cpanm -l ~/.local namespace::autoclean\n        # cpanm -l ~/.local Moose\n        # cpanm -l ~/.local MooseX::SemiAffordanceAccessor module\n        raise RuntimeError(err)\n    with open(dst, 'w') as ofile:\n        ofile.write(out)\n    return dst\n", "hanlp/metrics/srl/__init__.py": "# -*- coding:utf-8 -*-\n# Author: hankcs\n# Date: 2020-07-16 18:44"}