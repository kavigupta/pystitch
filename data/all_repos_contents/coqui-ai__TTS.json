{"hubconf.py": "dependencies = [\n    'torch', 'gdown', 'pysbd', 'gruut', 'anyascii', 'pypinyin', 'coqpit', 'mecab-python3', 'unidic-lite'\n]\nimport torch\n\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\n\n\ndef tts(model_name='tts_models/en/ljspeech/tacotron2-DCA',\n        vocoder_name=None,\n        use_cuda=False):\n    \"\"\"TTS entry point for PyTorch Hub that provides a Synthesizer object to synthesize speech from a give text.\n\n    Example:\n        >>> synthesizer = torch.hub.load('coqui-ai/TTS', 'tts', source='github')\n        >>> wavs = synthesizer.tts(\"This is a test! This is also a test!!\")\n            wavs - is a list of values of the synthesized speech.\n\n    Args:\n        model_name (str, optional): One of the model names from .model.json. Defaults to 'tts_models/en/ljspeech/tacotron2-DCA'.\n        vocoder_name (str, optional): One of the model names from .model.json. Defaults to 'vocoder_models/en/ljspeech/multiband-melgan'.\n        pretrained (bool, optional): [description]. Defaults to True.\n\n    Returns:\n        TTS.utils.synthesizer.Synthesizer: Synthesizer object wrapping both vocoder and tts models.\n    \"\"\"\n    manager = ModelManager()\n\n    model_path, config_path, model_item = manager.download_model(model_name)\n    vocoder_name = model_item[\n        'default_vocoder'] if vocoder_name is None else vocoder_name\n    vocoder_path, vocoder_config_path, _ = manager.download_model(vocoder_name)\n\n    # create synthesizer\n    synt = Synthesizer(tts_checkpoint=model_path,\n                       tts_config_path=config_path,\n                       vocoder_checkpoint=vocoder_path,\n                       vocoder_config=vocoder_config_path,\n                       use_cuda=use_cuda)\n    return synt\n\n\nif __name__ == '__main__':\n    synthesizer = torch.hub.load('coqui-ai/TTS:dev', 'tts', source='github')\n    synthesizer.tts(\"This is a test!\")\n", "setup.py": "#!/usr/bin/env python\n#                   ,*++++++*,                ,*++++++*,\n#                *++.        .+++          *++.        .++*\n#              *+*     ,++++*   *+*      *+*   ,++++,     *+*\n#             ,+,   .++++++++++* ,++,,,,*+, ,++++++++++.   *+,\n#             *+.  .++++++++++++..++    *+.,++++++++++++.  .+*\n#             .+*   ++++++++++++.*+,    .+*.++++++++++++   *+,\n#              .++   *++++++++* ++,      .++.*++++++++*   ++,\n#               ,+++*.    . .*++,          ,++*.      .*+++*\n#              *+,   .,*++**.                  .**++**.   ,+*\n#             .+*                                          *+,\n#             *+.                   Coqui                  .+*\n#             *+*              +++   TTS  +++              *+*\n#             .+++*.            .          .             *+++.\n#              ,+* *+++*...                       ...*+++* *+,\n#               .++.    .\"\"\"\"+++++++****+++++++\"\"\"\".     ++.\n#                 ,++.                                .++,\n#                   .++*                            *++.\n#                       *+++,                  ,+++*\n#                           .,*++++::::::++++*,.\n#                                  ``````\n\nimport os\nimport subprocess\nimport sys\nfrom packaging.version import Version\n\nimport numpy\nimport setuptools.command.build_py\nimport setuptools.command.develop\nfrom Cython.Build import cythonize\nfrom setuptools import Extension, find_packages, setup\n\npython_version = sys.version.split()[0]\nif Version(python_version) < Version(\"3.9\") or Version(python_version) >= Version(\"3.12\"):\n    raise RuntimeError(\"TTS requires python >= 3.9 and < 3.12 \" \"but your Python version is {}\".format(sys.version))\n\n\ncwd = os.path.dirname(os.path.abspath(__file__))\nwith open(os.path.join(cwd, \"TTS\", \"VERSION\")) as fin:\n    version = fin.read().strip()\n\n\nclass build_py(setuptools.command.build_py.build_py):  # pylint: disable=too-many-ancestors\n    def run(self):\n        setuptools.command.build_py.build_py.run(self)\n\n\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        setuptools.command.develop.develop.run(self)\n\n\n# The documentation for this feature is in server/README.md\npackage_data = [\"TTS/server/templates/*\"]\n\n\ndef pip_install(package_name):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n\n\nrequirements = open(os.path.join(cwd, \"requirements.txt\"), \"r\").readlines()\nwith open(os.path.join(cwd, \"requirements.notebooks.txt\"), \"r\") as f:\n    requirements_notebooks = f.readlines()\nwith open(os.path.join(cwd, \"requirements.dev.txt\"), \"r\") as f:\n    requirements_dev = f.readlines()\nwith open(os.path.join(cwd, \"requirements.ja.txt\"), \"r\") as f:\n    requirements_ja = f.readlines()\nrequirements_all = requirements_dev + requirements_notebooks + requirements_ja\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()\n\nexts = [\n    Extension(\n        name=\"TTS.tts.utils.monotonic_align.core\",\n        sources=[\"TTS/tts/utils/monotonic_align/core.pyx\"],\n    )\n]\nsetup(\n    name=\"TTS\",\n    version=version,\n    url=\"https://github.com/coqui-ai/TTS\",\n    author=\"Eren G\u00f6lge\",\n    author_email=\"egolge@coqui.ai\",\n    description=\"Deep learning for Text to Speech by Coqui.\",\n    long_description=README,\n    long_description_content_type=\"text/markdown\",\n    license=\"MPL-2.0\",\n    # cython\n    include_dirs=numpy.get_include(),\n    ext_modules=cythonize(exts, language_level=3),\n    # ext_modules=find_cython_extensions(),\n    # package\n    include_package_data=True,\n    packages=find_packages(include=[\"TTS\"], exclude=[\"*.tests\", \"*tests.*\", \"tests.*\", \"*tests\", \"tests\"]),\n    package_data={\n        \"TTS\": [\n            \"VERSION\",\n        ]\n    },\n    project_urls={\n        \"Documentation\": \"https://github.com/coqui-ai/TTS/wiki\",\n        \"Tracker\": \"https://github.com/coqui-ai/TTS/issues\",\n        \"Repository\": \"https://github.com/coqui-ai/TTS\",\n        \"Discussions\": \"https://github.com/coqui-ai/TTS/discussions\",\n    },\n    cmdclass={\n        \"build_py\": build_py,\n        \"develop\": develop,\n        # 'build_ext': build_ext\n    },\n    install_requires=requirements,\n    extras_require={\n        \"all\": requirements_all,\n        \"dev\": requirements_dev,\n        \"notebooks\": requirements_notebooks,\n        \"ja\": requirements_ja,\n    },\n    python_requires=\">=3.9.0, <3.12\",\n    entry_points={\"console_scripts\": [\"tts=TTS.bin.synthesize:main\", \"tts-server = TTS.server.server:main\"]},\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Developers\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Topic :: Multimedia :: Sound/Audio :: Speech\",\n        \"Topic :: Multimedia :: Sound/Audio\",\n        \"Topic :: Multimedia\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    zip_safe=False,\n)\n", "scripts/sync_readme.py": "import argparse\nfrom pathlib import Path\n\n\ndef replace_between_markers(content, marker: str, replacement: str) -> str:\n    start_marker = f\"<!-- begin-{marker} -->\\n\\n\"\n    end_marker = f\"\\n\\n<!-- end-{marker} -->\\n\"\n    start_index = content.index(start_marker) + len(start_marker)\n    end_index = content.index(end_marker)\n    content = content[:start_index] + replacement + content[end_index:]\n    return content\n\n\ndef sync_readme():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--check\", action=\"store_true\", default=False)\n    args = ap.parse_args()\n    readme_path = Path(__file__).parent.parent / \"README.md\"\n    orig_content = readme_path.read_text()\n    from TTS.bin.synthesize import description\n\n    new_content = replace_between_markers(orig_content, \"tts-readme\", description.strip())\n    if args.check:\n        if orig_content != new_content:\n            print(\"README.md is out of sync; please edit TTS/bin/TTS_README.md and run scripts/sync_readme.py\")\n            exit(42)\n    readme_path.write_text(new_content)\n    print(\"Updated README.md\")\n\n\nif __name__ == \"__main__\":\n    sync_readme()\n", "recipes/ljspeech/speedy_speech/train_speedy_speech.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.speedy_speech_config import SpeedySpeechConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = SpeedySpeechConfig(\n    run_name=\"speedy_speech_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=4,\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    max_seq_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = ForwardTTS(config, ap, tokenizer)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/ljspeech/overflow/train_overflow.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.overflow_config import OverflowConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.overflow import Overflow\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = OverflowConfig(  # This is the config that is saved for the future use\n    run_name=\"overflow_ljspeech\",\n    audio=audio_config,\n    batch_size=30,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=8,\n    mel_statistics_parameter_path=os.path.join(output_path, \"lj_parameters.pt\"),\n    force_generate_statistics=False,\n    print_step=1,\n    print_eval=True,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = Overflow(config, ap, tokenizer)\n\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,\n)\ntrainer.fit()\n", "recipes/ljspeech/glow_tts/train_glowtts.py": "import os\n\n# Trainer: Where the \u2728\ufe0f happens.\n# TrainingArgs: Defines the set of arguments of the Trainer.\nfrom trainer import Trainer, TrainerArgs\n\n# GlowTTSConfig: all model related values for training, validating and testing.\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\n\n# BaseDatasetConfig: defines name, formatter and path of the dataset.\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.glow_tts import GlowTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\n# we use the same path as this script as our training folder.\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# DEFINE DATASET CONFIG\n# Set LJSpeech as our target dataset and define its path.\n# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/ljspeech/xtts_v1/train_gpt_xtts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\nfrom TTS.utils.manage import ModelManager\n\n# Logging parameters\nRUN_NAME = \"GPT_XTTS_LJSpeech_FT\"\nPROJECT_NAME = \"XTTS_trainer\"\nDASHBOARD_LOGGER = \"tensorboard\"\nLOGGER_URI = None\n\n# Set here the path that the checkpoints will be saved. Default: ./run/training/\nOUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run\", \"training\")\n\n# Training Parameters\nOPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\nSTART_WITH_EVAL = True  # if True it will star with evaluation\nBATCH_SIZE = 3  # set here the batch size\nGRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps\n# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n\n# Define here the dataset that you want to use for the fine-tuning on.\nconfig_dataset = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    dataset_name=\"ljspeech\",\n    path=\"/raid/datasets/LJSpeech-1.1_24khz/\",\n    meta_file_train=\"/raid/datasets/LJSpeech-1.1_24khz/metadata.csv\",\n    language=\"en\",\n)\n\n# Add here the configs of the datasets\nDATASETS_CONFIG_LIST = [config_dataset]\n\n# Define the path where XTTS v1.1.1 files will be downloaded\nCHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v1.1_original_model_files/\")\nos.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n\n\n# DVAE files\nDVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v1/v1.1.2/dvae.pth\"\nMEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v1/v1.1.2/mel_stats.pth\"\n\n# Set the path to the downloaded files\nDVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, DVAE_CHECKPOINT_LINK.split(\"/\")[-1])\nMEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, MEL_NORM_LINK.split(\"/\")[-1])\n\n# download DVAE files if needed\nif not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n    print(\" > Downloading DVAE files!\")\n    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n\n\n# Download XTTS v1.1 checkpoint if needed\nTOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v1/v1.1.2/vocab.json\"\nXTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v1/v1.1.2/model.pth\"\n\n# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\nTOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, TOKENIZER_FILE_LINK.split(\"/\")[-1])  # vocab.json file\nXTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, XTTS_CHECKPOINT_LINK.split(\"/\")[-1])  # model.pth file\n\n# download XTTS v1.1 files if needed\nif not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n    print(\" > Downloading XTTS v1.1 files!\")\n    ModelManager._download_model_files(\n        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n    )\n\n\n# Training sentences generations\nSPEAKER_REFERENCE = [\n    \"./tests/data/ljspeech/wavs/LJ001-0002.wav\"  # speaker reference to be used in training test sentences\n]\nLANGUAGE = config_dataset.language\n\n\ndef main():\n    # init args and config\n    model_args = GPTArgs(\n        max_conditioning_length=132300,  # 6 secs\n        min_conditioning_length=66150,  # 3 secs\n        debug_loading_failures=False,\n        max_wav_length=255995,  # ~11.6 seconds\n        max_text_length=200,\n        mel_norm_file=MEL_NORM_FILE,\n        dvae_checkpoint=DVAE_CHECKPOINT,\n        # tokenizer_file=\"/raid/datasets/xtts_models/vocab.json\", # vocab path of the model that you want to fine-tune\n        # xtts_checkpoint=\"https://huggingface.co/coqui/XTTS-v1/resolve/hifigan/model.pth\",\n        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n        tokenizer_file=TOKENIZER_FILE,\n        gpt_num_audio_tokens=8194,\n        gpt_start_audio_token=8192,\n        gpt_stop_audio_token=8193,\n    )\n    # define audio config\n    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n    # training parameters config\n    config = GPTTrainerConfig(\n        output_path=OUT_PATH,\n        model_args=model_args,\n        run_name=RUN_NAME,\n        project_name=PROJECT_NAME,\n        run_description=\"\"\"\n            GPT XTTS training\n            \"\"\",\n        dashboard_logger=DASHBOARD_LOGGER,\n        logger_uri=LOGGER_URI,\n        audio=audio_config,\n        batch_size=BATCH_SIZE,\n        batch_group_size=48,\n        eval_batch_size=BATCH_SIZE,\n        num_loader_workers=8,\n        eval_split_max_size=256,\n        print_step=50,\n        plot_step=100,\n        log_model_step=1000,\n        save_step=10000,\n        save_n_checkpoints=1,\n        save_checkpoints=True,\n        # target_loss=\"loss\",\n        print_eval=False,\n        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n        optimizer=\"AdamW\",\n        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n        lr=5e-06,  # learning rate\n        lr_scheduler=\"MultiStepLR\",\n        # it was adjusted accordly for the new step scheme\n        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n        test_sentences=[\n            {\n                \"text\": \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n                \"speaker_wav\": SPEAKER_REFERENCE,\n                \"language\": LANGUAGE,\n            },\n            {\n                \"text\": \"This cake is great. It's so delicious and moist.\",\n                \"speaker_wav\": SPEAKER_REFERENCE,\n                \"language\": LANGUAGE,\n            },\n        ],\n    )\n\n    # init the model from config\n    model = GPTTrainer.init_from_config(config)\n\n    # load training samples\n    train_samples, eval_samples = load_tts_samples(\n        DATASETS_CONFIG_LIST,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # init the trainer and \ud83d\ude80\n    trainer = Trainer(\n        TrainerArgs(\n            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n            skip_train_epoch=False,\n            start_with_eval=START_WITH_EVAL,\n            grad_accum_steps=GRAD_ACUMM_STEPS,\n        ),\n        config,\n        output_path=OUT_PATH,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n    )\n    trainer.fit()\n\n\nif __name__ == \"__main__\":\n    main()\n", "recipes/ljspeech/univnet/train.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs import UnivnetConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n    output_path=output_path,\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/fast_speech/train_fast_speech.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.fast_speech_config import FastSpeechConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.manage import ModelManager\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = FastSpeechConfig(\n    run_name=\"fast_speech_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=False,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=8,\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    max_seq_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# compute alignments\nif not config.model_args.use_aligner:\n    manager = ModelManager()\n    model_path, config_path, _ = manager.download_model(\"tts_models/en/ljspeech/tacotron2-DCA\")\n    # TODO: make compute_attention python callable\n    os.system(\n        f\"python TTS/bin/compute_attention_masks.py --model_path {model_path} --config_path {config_path} --dataset ljspeech --dataset_metafile metadata.csv --data_path ./recipes/ljspeech/LJSpeech-1.1/  --use_cuda true\"\n    )\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init the model\nmodel = ForwardTTS(config, ap, tokenizer)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/tacotron2-DCA/train_tacotron_dca.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\n# from TTS.tts.datasets.tokenizer import Tokenizer\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    ga_alpha=0.0,\n    decoder_loss_alpha=0.25,\n    postnet_loss_alpha=0.25,\n    postnet_diff_spec_alpha=0,\n    decoder_diff_spec_alpha=0,\n    decoder_ssim_alpha=0,\n    postnet_ssim_alpha=0,\n    r=2,\n    attention_type=\"dynamic_convolution\",\n    double_decoder_consistency=False,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = Tacotron2(config, ap, tokenizer)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/ljspeech/vits_tts/train_vits.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import Vits, VitsAudioConfig\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\n\nconfig = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_ljspeech\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    compute_input_seq_cache=True,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n    cudnn_benchmark=False,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()\n", "recipes/ljspeech/wavegrad/train_wavegrad.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs import WavegradConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.wavegrad import Wavegrad\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=50,\n    print_step=50,\n    print_eval=True,\n    mixed_precision=False,\n    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n    output_path=output_path,\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = Wavegrad(config)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()\n", "recipes/ljspeech/neuralhmm_tts/train_neuralhmmtts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.neuralhmm_tts_config import NeuralhmmTTSConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.neuralhmm_tts import NeuralhmmTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = NeuralhmmTTSConfig(  # This is the config that is saved for the future use\n    run_name=\"neuralhmmtts_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=8,\n    mel_statistics_parameter_path=os.path.join(output_path, \"lj_parameters.pt\"),\n    force_generate_statistics=False,\n    print_step=1,\n    print_eval=True,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = NeuralhmmTTS(config, ap, tokenizer)\n\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,\n)\ntrainer.fit()\n", "recipes/ljspeech/fastspeech2/train_fastspeech2.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.fastspeech2_config import Fastspeech2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.manage import ModelManager\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = Fastspeech2Config(\n    run_name=\"fastspeech2_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),\n    compute_energy=True,\n    energy_cache_path=os.path.join(output_path, \"energy_cache\"),\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=4,\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    max_seq_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# compute alignments\nif not config.model_args.use_aligner:\n    manager = ModelManager()\n    model_path, config_path, _ = manager.download_model(\"tts_models/en/ljspeech/tacotron2-DCA\")\n    # TODO: make compute_attention python callable\n    os.system(\n        f\"python TTS/bin/compute_attention_masks.py --model_path {model_path} --config_path {config_path} --dataset ljspeech --dataset_metafile metadata.csv --data_path ./recipes/ljspeech/LJSpeech-1.1/  --use_cuda true\"\n    )\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init the model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/xtts_v2/train_gpt_xtts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\nfrom TTS.utils.manage import ModelManager\n\n# Logging parameters\nRUN_NAME = \"GPT_XTTS_v2.0_LJSpeech_FT\"\nPROJECT_NAME = \"XTTS_trainer\"\nDASHBOARD_LOGGER = \"tensorboard\"\nLOGGER_URI = None\n\n# Set here the path that the checkpoints will be saved. Default: ./run/training/\nOUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run\", \"training\")\n\n# Training Parameters\nOPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\nSTART_WITH_EVAL = True  # if True it will star with evaluation\nBATCH_SIZE = 3  # set here the batch size\nGRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps\n# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n\n# Define here the dataset that you want to use for the fine-tuning on.\nconfig_dataset = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    dataset_name=\"ljspeech\",\n    path=\"/raid/datasets/LJSpeech-1.1_24khz/\",\n    meta_file_train=\"/raid/datasets/LJSpeech-1.1_24khz/metadata.csv\",\n    language=\"en\",\n)\n\n# Add here the configs of the datasets\nDATASETS_CONFIG_LIST = [config_dataset]\n\n# Define the path where XTTS v2.0.1 files will be downloaded\nCHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\nos.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n\n\n# DVAE files\nDVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\nMEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n\n# Set the path to the downloaded files\nDVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\nMEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n\n# download DVAE files if needed\nif not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n    print(\" > Downloading DVAE files!\")\n    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n\n\n# Download XTTS v2.0 checkpoint if needed\nTOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\nXTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n\n# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\nTOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\nXTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n\n# download XTTS v2.0 files if needed\nif not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n    print(\" > Downloading XTTS v2.0 files!\")\n    ModelManager._download_model_files(\n        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n    )\n\n\n# Training sentences generations\nSPEAKER_REFERENCE = [\n    \"./tests/data/ljspeech/wavs/LJ001-0002.wav\"  # speaker reference to be used in training test sentences\n]\nLANGUAGE = config_dataset.language\n\n\ndef main():\n    # init args and config\n    model_args = GPTArgs(\n        max_conditioning_length=132300,  # 6 secs\n        min_conditioning_length=66150,  # 3 secs\n        debug_loading_failures=False,\n        max_wav_length=255995,  # ~11.6 seconds\n        max_text_length=200,\n        mel_norm_file=MEL_NORM_FILE,\n        dvae_checkpoint=DVAE_CHECKPOINT,\n        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n        tokenizer_file=TOKENIZER_FILE,\n        gpt_num_audio_tokens=1026,\n        gpt_start_audio_token=1024,\n        gpt_stop_audio_token=1025,\n        gpt_use_masking_gt_prompt_approach=True,\n        gpt_use_perceiver_resampler=True,\n    )\n    # define audio config\n    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n    # training parameters config\n    config = GPTTrainerConfig(\n        output_path=OUT_PATH,\n        model_args=model_args,\n        run_name=RUN_NAME,\n        project_name=PROJECT_NAME,\n        run_description=\"\"\"\n            GPT XTTS training\n            \"\"\",\n        dashboard_logger=DASHBOARD_LOGGER,\n        logger_uri=LOGGER_URI,\n        audio=audio_config,\n        batch_size=BATCH_SIZE,\n        batch_group_size=48,\n        eval_batch_size=BATCH_SIZE,\n        num_loader_workers=8,\n        eval_split_max_size=256,\n        print_step=50,\n        plot_step=100,\n        log_model_step=1000,\n        save_step=10000,\n        save_n_checkpoints=1,\n        save_checkpoints=True,\n        # target_loss=\"loss\",\n        print_eval=False,\n        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n        optimizer=\"AdamW\",\n        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n        lr=5e-06,  # learning rate\n        lr_scheduler=\"MultiStepLR\",\n        # it was adjusted accordly for the new step scheme\n        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n        test_sentences=[\n            {\n                \"text\": \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n                \"speaker_wav\": SPEAKER_REFERENCE,\n                \"language\": LANGUAGE,\n            },\n            {\n                \"text\": \"This cake is great. It's so delicious and moist.\",\n                \"speaker_wav\": SPEAKER_REFERENCE,\n                \"language\": LANGUAGE,\n            },\n        ],\n    )\n\n    # init the model from config\n    model = GPTTrainer.init_from_config(config)\n\n    # load training samples\n    train_samples, eval_samples = load_tts_samples(\n        DATASETS_CONFIG_LIST,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # init the trainer and \ud83d\ude80\n    trainer = Trainer(\n        TrainerArgs(\n            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n            skip_train_epoch=False,\n            start_with_eval=START_WITH_EVAL,\n            grad_accum_steps=GRAD_ACUMM_STEPS,\n        ),\n        config,\n        output_path=OUT_PATH,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n    )\n    trainer.fit()\n\n\nif __name__ == \"__main__\":\n    main()\n", "recipes/ljspeech/fast_pitch/train_fast_pitch.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.fast_pitch_config import FastPitchConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.manage import ModelManager\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = FastPitchConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=4,\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    max_seq_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# compute alignments\nif not config.model_args.use_aligner:\n    manager = ModelManager()\n    model_path, config_path, _ = manager.download_model(\"tts_models/en/ljspeech/tacotron2-DCA\")\n    # TODO: make compute_attention python callable\n    os.system(\n        f\"python TTS/bin/compute_attention_masks.py --model_path {model_path} --config_path {config_path} --dataset ljspeech --dataset_metafile metadata.csv --data_path ./recipes/ljspeech/LJSpeech-1.1/  --use_cuda true\"\n    )\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init the model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/tacotron2-DDC/train_tacotron_ddc.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\n# from TTS.tts.datasets.tokenizer import Tokenizer\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,\n    gradual_training=[[0, 6, 64], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n    double_decoder_consistency=True,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=8,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/align_tts/train_aligntts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.align_tts_config import AlignTTSConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.align_tts import AlignTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\nconfig = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = AlignTTS(config, ap, tokenizer)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/ljspeech/hifigan/train_hifigan.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs import HifiganConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\nconfig = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n    output_path=output_path,\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/multiband_melgan/train_multiband_melgan.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs import MultibandMelganConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\nconfig = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n    output_path=output_path,\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/ljspeech/tacotron2-Capacitron/train_capacitron_t2.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig, CapacitronVAEConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\ndata_path = \"/srv/data/\"\n\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=11025,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\n# Using the standard Capacitron config\ncapacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0, capacitron_capacity=50)\n\nconfig = Tacotron2Config(\n    run_name=\"Capacitron-Tacotron2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=8 * 22050,  # Tune this to your gpu\n    min_audio_len=1 * 22050,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=8,\n    precompute_num_workers=24,\n    run_eval=True,\n    test_delay_epochs=25,\n    ga_alpha=0.0,\n    r=2,\n    optimizer=\"CapacitronOptimizer\",\n    optimizer_params={\"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}, \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9}},\n    attention_type=\"dynamic_convolution\",\n    grad_clip=0.0,  # Important! We overwrite the standard grad_clip with capacitron_grad_clip\n    double_decoder_consistency=False,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phonemizer=\"espeak\",\n    phoneme_cache_path=os.path.join(data_path, \"phoneme_cache\"),\n    stopnet_pos_weight=15,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    seq_len_norm=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n    lr=1e-3,\n    lr_scheduler=\"StepwiseGradualLR\",\n    lr_scheduler_params={\n        \"gradual_learning_rates\": [\n            [0, 1e-3],\n            [2e4, 5e-4],\n            [4e5, 3e-4],\n            [6e4, 1e-4],\n            [8e4, 5e-5],\n        ]\n    },\n    scheduler_after_epoch=False,  # scheduler doesn't work without this flag\n    # Need to experiment with these below for capacitron\n    loss_masking=False,\n    decoder_loss_alpha=1.0,\n    postnet_loss_alpha=1.0,\n    postnet_diff_spec_alpha=0.0,\n    decoder_diff_spec_alpha=0.0,\n    decoder_ssim_alpha=0.0,\n    postnet_ssim_alpha=0.0,\n)\n\nap = AudioProcessor(**config.audio.to_dict())\n\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\n\ntrainer.fit()\n", "recipes/ljspeech/wavernn/train_wavernn.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs import WavernnConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.wavernn import Wavernn\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,\n    pad_short=2000,\n    use_noise_augment=False,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    lr=1e-4,\n    grad_clip=4,\n    data_path=os.path.join(output_path, \"../LJSpeech-1.1/wavs/\"),\n    output_path=output_path,\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = Wavernn(config)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()\n", "recipes/ljspeech/delightful_tts/train_delightful_tts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.delightful_tts_config import DelightfulTtsAudioConfig, DelightfulTTSConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.delightful_tts import DelightfulTTS, DelightfulTtsArgs, VocoderConfig\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio.processor import AudioProcessor\n\ndata_path = \"\"\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\ndataset_config = BaseDatasetConfig(\n    dataset_name=\"ljspeech\", formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=data_path\n)\n\naudio_config = DelightfulTtsAudioConfig()\nmodel_args = DelightfulTtsArgs()\n\nvocoder_config = VocoderConfig()\n\ndelightful_tts_config = DelightfulTTSConfig(\n    run_name=\"delightful_tts_ljspeech\",\n    run_description=\"Train like in delightful tts paper.\",\n    model_args=model_args,\n    audio=audio_config,\n    vocoder=vocoder_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=10,\n    num_eval_loader_workers=10,\n    precompute_num_workers=10,\n    batch_group_size=2,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=50,\n    print_eval=False,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n    start_by_longest=False,\n    eval_split_size=0.1,\n    binary_align_loss_alpha=0.0,\n    use_attn_priors=False,\n    lr_gen=4e-1,\n    lr=4e-1,\n    lr_disc=4e-1,\n    max_text_len=130,\n)\n\ntokenizer, config = TTSTokenizer.init_from_config(delightful_tts_config)\n\nap = AudioProcessor.init_from_config(config)\n\n\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\nmodel = DelightfulTTS(ap=ap, config=config, tokenizer=tokenizer, speaker_manager=None)\n\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\n\ntrainer.fit()\n", "recipes/bel-alex73/train_hifigan.py": "import os\n\nfrom coqpit import Coqpit\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseAudioConfig\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.configs.hifigan_config import *\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = \"/storage/output-hifigan/\"\n\naudio_config = BaseAudioConfig(\n    mel_fmin=50,\n    mel_fmax=8000,\n    hop_length=256,\n    stats_path=\"/storage/TTS/scale_stats.npy\",\n)\n\nconfig = HifiganConfig(\n    batch_size=74,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=8,\n    lr_disc=0.0002,\n    lr_gen=0.0002,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    use_noise_augment=True,\n    seq_len=8192,\n    pad_short=2000,\n    save_step=5000,\n    print_step=50,\n    print_eval=True,\n    mixed_precision=False,\n    eval_split_size=30,\n    save_n_checkpoints=2,\n    save_best_after=5000,\n    data_path=\"/storage/filtered_dataset\",\n    output_path=output_path,\n    audio=audio_config,\n)\n\n# init audio processor\nap = AudioProcessor.init_from_config(config)\n\n# load training samples\nprint(\"config.eval_split_size = \", config.eval_split_size)\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/bel-alex73/train_glowtts.py": "import os\n\n# Trainer: Where the \u2728\ufe0f happens.\n# TrainingArgs: Defines the set of arguments of the Trainer.\nfrom trainer import Trainer, TrainerArgs\n\n# GlowTTSConfig: all model related values for training, validating and testing.\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\n\n# BaseDatasetConfig: defines name, formatter and path of the dataset.\nfrom TTS.tts.configs.shared_configs import BaseAudioConfig, BaseDatasetConfig, CharactersConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.glow_tts import GlowTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\n# we use the same path as this script as our training folder.\noutput_path = \"/storage/output-glowtts/\"\n\n\n# DEFINE DATASET CONFIG\n# Set LJSpeech as our target dataset and define its path.\n# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\ndataset_config = BaseDatasetConfig(\n    formatter=\"bel_tts_formatter\",\n    meta_file_train=\"ipa_final_dataset.csv\",\n    path=os.path.join(output_path, \"/storage/filtered_dataset/\"),\n)\n\ncharacters = CharactersConfig(\n    characters_class=\"TTS.tts.utils.text.characters.Graphemes\",\n    pad=\"_\",\n    eos=\"~\",\n    bos=\"^\",\n    blank=\"@\",\n    characters=\"Iabdfgijklmnprstuvxz\u0254\u025b\u0263\u0268\u026b\u0271\u0282\u0290\u02b2\u02c8\u02d0\u032f\u0361\u03b2\",\n    punctuations=\"!,.?: -\u2012\u2013\u2014\u2026\",\n)\n\naudio_config = BaseAudioConfig(\n    mel_fmin=50,\n    mel_fmax=8000,\n    hop_length=256,\n    stats_path=\"/storage/TTS/scale_stats.npy\",\n)\n\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(\n    batch_size=96,\n    eval_batch_size=32,\n    num_loader_workers=8,\n    num_eval_loader_workers=8,\n    use_noise_augment=True,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    print_step=50,\n    print_eval=True,\n    output_path=output_path,\n    add_blank=True,\n    datasets=[dataset_config],\n    #    characters=characters,\n    enable_eos_bos_chars=True,\n    mixed_precision=False,\n    save_step=10000,\n    save_n_checkpoints=2,\n    save_best_after=5000,\n    text_cleaner=\"no_cleaners\",\n    audio=audio_config,\n    test_sentences=[],\n    use_phonemes=True,\n    phoneme_language=\"be\",\n)\n\nif __name__ == \"__main__\":\n    # INITIALIZE THE AUDIO PROCESSOR\n    # Audio processor is used for feature extraction and audio I/O.\n    # It mainly serves to the dataloader and the training loggers.\n    ap = AudioProcessor.init_from_config(config)\n\n    # INITIALIZE THE TOKENIZER\n    # Tokenizer is used to convert text to sequences of token IDs.\n    # If characters are not defined in the config, default characters are passed to the config\n    tokenizer, config = TTSTokenizer.init_from_config(config)\n\n    # LOAD DATA SAMPLES\n    # Each sample is a list of ```[text, audio_file_path, speaker_name]```\n    # You can define your custom sample loader returning the list of samples.\n    # Or define your custom formatter and pass it to the `load_tts_samples`.\n    # Check `TTS.tts.datasets.load_tts_samples` for more details.\n    train_samples, eval_samples = load_tts_samples(\n        dataset_config,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # INITIALIZE THE MODEL\n    # Models take a config object and a speaker manager as input\n    # Config defines the details of the model like the number of layers, the size of the embedding, etc.\n    # Speaker manager is used by multi-speaker models.\n    model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n\n    # INITIALIZE THE TRAINER\n    # Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n    # distributed training, etc.\n    trainer = Trainer(\n        TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n    )\n\n    # AND... 3,2,1... \ud83d\ude80\n    trainer.fit()\n", "recipes/bel-alex73/dump_config.py": "import json\nimport re\n\nfrom train_glowtts import config\n\ns = json.dumps(config, default=vars, indent=2)\ns = re.sub(r'\"test_sentences\":\\s*\\[\\],', \"\", s)\nprint(s)\n", "recipes/blizzard2013/tacotron1-Capacitron/train_capacitron_t1.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig, CapacitronVAEConfig\nfrom TTS.tts.configs.tacotron_config import TacotronConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron import Tacotron\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\ndata_path = \"/srv/data/\"\n\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=data_path)\n\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,\n    spec_gain=20.0,\n    log_func=\"np.log10\",\n    ref_level_db=20,\n    preemphasis=0.0,\n    min_level_db=-100,\n)\n\n# Using the standard Capacitron config\ncapacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0)\n\nconfig = TacotronConfig(\n    run_name=\"Blizzard-Capacitron-T1\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=0.5 * 24000,\n    eval_batch_size=16,\n    num_loader_workers=12,\n    num_eval_loader_workers=8,\n    precompute_num_workers=24,\n    run_eval=True,\n    test_delay_epochs=5,\n    r=2,\n    optimizer=\"CapacitronOptimizer\",\n    optimizer_params={\"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}, \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9}},\n    attention_type=\"graves\",\n    attention_heads=5,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phonemizer=\"espeak\",\n    phoneme_cache_path=os.path.join(data_path, \"phoneme_cache\"),\n    stopnet_pos_weight=15,\n    print_step=50,\n    print_eval=True,\n    mixed_precision=False,\n    output_path=output_path,\n    datasets=[dataset_config],\n    lr=1e-3,\n    lr_scheduler=\"StepwiseGradualLR\",\n    lr_scheduler_params={\"gradual_learning_rates\": [[0, 1e-3], [2e4, 5e-4], [4e4, 3e-4], [6e4, 1e-4], [8e4, 5e-5]]},\n    scheduler_after_epoch=False,  # scheduler doesn't work without this flag\n    loss_masking=False,\n    decoder_loss_alpha=1.0,\n    postnet_loss_alpha=1.0,\n    postnet_diff_spec_alpha=1.0,\n    decoder_diff_spec_alpha=1.0,\n    decoder_ssim_alpha=1.0,\n    postnet_ssim_alpha=1.0,\n)\n\nap = AudioProcessor(**config.audio.to_dict())\n\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n\nmodel = Tacotron(config, ap, tokenizer, speaker_manager=None)\n\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\n\n# \ud83d\ude80\ntrainer.fit()\n", "recipes/blizzard2013/tacotron2-Capacitron/train_capacitron_t2.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig, CapacitronVAEConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\ndata_path = \"/srv/data/blizzard2013/segmented\"\n\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\n\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,\n    spec_gain=25.0,\n    log_func=\"np.log10\",\n    ref_level_db=20,\n    preemphasis=0.0,\n    min_level_db=-100,\n)\n\n# Using the standard Capacitron config\ncapacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0)\n\nconfig = Tacotron2Config(\n    run_name=\"Blizzard-Capacitron-T2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=246,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=1 * 24000,\n    eval_batch_size=16,\n    num_loader_workers=12,\n    num_eval_loader_workers=8,\n    precompute_num_workers=24,\n    run_eval=True,\n    test_delay_epochs=5,\n    r=2,\n    optimizer=\"CapacitronOptimizer\",\n    optimizer_params={\"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}, \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9}},\n    attention_type=\"dynamic_convolution\",\n    grad_clip=0.0,  # Important! We overwrite the standard grad_clip with capacitron_grad_clip\n    double_decoder_consistency=False,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phonemizer=\"espeak\",\n    phoneme_cache_path=os.path.join(data_path, \"phoneme_cache\"),\n    stopnet_pos_weight=15,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    output_path=output_path,\n    datasets=[dataset_config],\n    lr=1e-3,\n    lr_scheduler=\"StepwiseGradualLR\",\n    lr_scheduler_params={\n        \"gradual_learning_rates\": [\n            [0, 1e-3],\n            [2e4, 5e-4],\n            [4e4, 3e-4],\n            [6e4, 1e-4],\n            [8e4, 5e-5],\n        ]\n    },\n    scheduler_after_epoch=False,  # scheduler doesn't work without this flag\n    seq_len_norm=True,\n    loss_masking=False,\n    decoder_loss_alpha=1.0,\n    postnet_loss_alpha=1.0,\n    postnet_diff_spec_alpha=1.0,\n    decoder_diff_spec_alpha=1.0,\n    decoder_ssim_alpha=1.0,\n    postnet_ssim_alpha=1.0,\n)\n\nap = AudioProcessor(**config.audio.to_dict())\n\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\n\ntrainer.fit()\n", "recipes/vctk/speedy_speech/train_speedy_speech.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.speedy_speech_config import SpeedySpeechConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = SpeedySpeechConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/resnet_speaker_encoder/train_encoder.py": "import os\n\nfrom TTS.encoder.configs.speaker_encoder_config import SpeakerEncoderConfig\n\n# from TTS.encoder.configs.emotion_encoder_config import EmotionEncoderConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\n\nCURRENT_PATH = os.getcwd()\n# change the root path to the TTS root path\nos.chdir(\"../../../\")\n\n### Definitions ###\n# dataset\nVCTK_PATH = \"/raid/datasets/VCTK_NEW_16khz_removed_silence_silero_vad/\"  # download:  https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zipdddddddddd\nRIR_SIMULATED_PATH = \"/raid/datasets/DA/RIRS_NOISES/simulated_rirs/\"  # download: https://www.openslr.org/17/\nMUSAN_PATH = \"/raid/datasets/DA/musan/\"  # download: https://www.openslr.org/17/\n\n# training\nOUTPUT_PATH = os.path.join(\n    CURRENT_PATH, \"resnet_speaker_encoder_training_output/\"\n)  # path to save the train logs and checkpoint\nCONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n\n# instance the config\n# to speaker encoder\nconfig = SpeakerEncoderConfig()\n# to emotion encoder\n# config = EmotionEncoderConfig()\n\n\n#### DATASET CONFIG ####\n# The formatter need to return the key \"speaker_name\"  for the speaker encoder and the \"emotion_name\" for the emotion encoder\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=VCTK_PATH)\n\n# add the dataset to the config\nconfig.datasets = [dataset_config]\n\n\n#### TRAINING CONFIG ####\n# The encoder data loader balancer the dataset item equally to guarantee better training and to attend the losses requirements\n# It have two parameters to control the final batch size the number total of speaker used in each batch and the number of samples for each speaker\n\n# number total of speaker in batch in training\nconfig.num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in training\nconfig.num_utter_per_class = 4\n# final batch size = config.num_classes_in_batch * config.num_utter_per_class\n\n# number total of speaker in batch in evaluation\nconfig.eval_num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in evaluation\nconfig.eval_num_utter_per_class = 4\n\n# number of data loader workers\nconfig.num_loader_workers = 8\nconfig.num_val_loader_workers = 8\n\n# number of epochs\nconfig.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"\n\n# run eval\nconfig.run_eval = False\n\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH\n\n# Save local checkpoint every save_step steps\nconfig.save_step = 2000\n\n### Model Config ###\nconfig.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"\n    \"input_dim\": 64,\n    \"use_torch_spec\": True,\n    \"log_input\": True,\n    \"proj_dim\": 512,  # embedding dim\n}\n\n### Audio Config ###\n# To fast train the model divides the audio in small parts. it parameter defines the length in seconds of these \"parts\"\nconfig.voice_len = 2.0\n# all others configs\nconfig.audio = {\n    \"fft_size\": 512,\n    \"win_length\": 400,\n    \"hop_length\": 160,\n    \"frame_shift_ms\": None,\n    \"frame_length_ms\": None,\n    \"stft_pad_mode\": \"reflect\",\n    \"sample_rate\": 16000,\n    \"resample\": False,\n    \"preemphasis\": 0.97,\n    \"ref_level_db\": 20,\n    \"do_sound_norm\": False,\n    \"do_trim_silence\": False,\n    \"trim_db\": 60,\n    \"power\": 1.5,\n    \"griffin_lim_iters\": 60,\n    \"num_mels\": 64,\n    \"mel_fmin\": 0.0,\n    \"mel_fmax\": 8000.0,\n    \"spec_gain\": 20,\n    \"signal_norm\": False,\n    \"min_level_db\": -100,\n    \"symmetric_norm\": False,\n    \"max_norm\": 4.0,\n    \"clip_norm\": False,\n    \"stats_path\": None,\n    \"do_rms_norm\": True,\n    \"db_level\": -27.0,\n}\n\n\n### Augmentation Config ###\nconfig.audio_augmentation = {\n    # additive noise and room impulse response (RIR) simulation similar to: https://arxiv.org/pdf/2009.14153.pdf\n    \"p\": 0.5,  # probability to the use of one of the augmentation - 0 means disabled\n    \"rir\": {\"rir_path\": RIR_SIMULATED_PATH, \"conv_mode\": \"full\"},  # download: https://www.openslr.org/17/\n    \"additive\": {\n        \"sounds_path\": MUSAN_PATH,\n        \"speech\": {\"min_snr_in_db\": 13, \"max_snr_in_db\": 20, \"min_num_noises\": 1, \"max_num_noises\": 1},\n        \"noise\": {\"min_snr_in_db\": 0, \"max_snr_in_db\": 15, \"min_num_noises\": 1, \"max_num_noises\": 1},\n        \"music\": {\"min_snr_in_db\": 5, \"max_snr_in_db\": 15, \"min_num_noises\": 1, \"max_num_noises\": 1},\n    },\n    \"gaussian\": {\"p\": 0.7, \"min_amplitude\": 0.0, \"max_amplitude\": 1e-05},\n}\n\nconfig.save_json(CONFIG_OUT_PATH)\n\nprint(CONFIG_OUT_PATH)\nif RESTORE_PATH is not None:\n    command = f\"python TTS/bin/train_encoder.py --config_path {CONFIG_OUT_PATH} --restore_path {RESTORE_PATH}\"\nelse:\n    command = f\"python TTS/bin/train_encoder.py --config_path {CONFIG_OUT_PATH}\"\n\nos.system(command)\n", "recipes/vctk/yourtts/train_yourtts.py": "import os\n\nimport torch\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.bin.compute_embeddings import compute_embeddings\nfrom TTS.bin.resample import resample_files\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\nfrom TTS.utils.downloaders import download_vctk\n\ntorch.set_num_threads(24)\n\n# pylint: disable=W0105\n\"\"\"\n    This recipe replicates the first experiment proposed in the YourTTS paper (https://arxiv.org/abs/2112.02418).\n    YourTTS model is based on the VITS model however it uses external speaker embeddings extracted from a pre-trained speaker encoder and has small architecture changes.\n    In addition, YourTTS can be trained in multilingual data, however, this recipe replicates the single language training using the VCTK dataset.\n    If you are interested in multilingual training, we have commented on parameters on the VitsArgs class instance that should be enabled for multilingual training.\n    In addition, you will need to add the extra datasets following the VCTK as an example.\n\"\"\"\nCURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n\n# Name of the run for the Trainer\nRUN_NAME = \"YourTTS-EN-VCTK\"\n\n# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\nOUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n\n# If you want to do transfer learning and speedup your training you can set here the path to the original YourTTS model\nRESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 16000\n\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = 10\n\n### Download VCTK dataset\nVCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n# Define the number of threads used during the audio resampling\nNUM_RESAMPLE_THREADS = 10\n# Check if VCTK dataset is not already downloaded, if not download it\nif not os.path.exists(VCTK_DOWNLOAD_PATH):\n    print(\">>> Downloading VCTK dataset:\")\n    download_vctk(VCTK_DOWNLOAD_PATH)\n    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n\n# init configs\nvctk_config = BaseDatasetConfig(\n    formatter=\"vctk\",\n    dataset_name=\"vctk\",\n    meta_file_train=\"\",\n    meta_file_val=\"\",\n    path=VCTK_DOWNLOAD_PATH,\n    language=\"en\",\n    ignored_speakers=[\n        \"p261\",\n        \"p225\",\n        \"p294\",\n        \"p347\",\n        \"p238\",\n        \"p234\",\n        \"p248\",\n        \"p335\",\n        \"p245\",\n        \"p326\",\n        \"p302\",\n    ],  # Ignore the test speakers to full replicate the paper experiment\n)\n\n# Add here all datasets configs, in our case we just want to train with the VCTK dataset then we need to add just VCTK. Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\nDATASETS_CONFIG_LIST = [vctk_config]\n\n### Extract speaker embeddings\nSPEAKER_ENCODER_CHECKPOINT_PATH = (\n    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n)\nSPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n\nD_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it\n    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n    if not os.path.isfile(embeddings_file):\n        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n        compute_embeddings(\n            SPEAKER_ENCODER_CHECKPOINT_PATH,\n            SPEAKER_ENCODER_CONFIG_PATH,\n            embeddings_file,\n            old_speakers_file=None,\n            config_dataset_path=None,\n            formatter_name=dataset_conf.formatter,\n            dataset_name=dataset_conf.dataset_name,\n            dataset_path=dataset_conf.path,\n            meta_file_train=dataset_conf.meta_file_train,\n            meta_file_val=dataset_conf.meta_file_val,\n            disable_cuda=False,\n            no_eval=False,\n        )\n    D_VECTOR_FILES.append(embeddings_file)\n\n\n# Audio config used in training.\naudio_config = VitsAudioConfig(\n    sample_rate=SAMPLE_RATE,\n    hop_length=256,\n    win_length=1024,\n    fft_size=1024,\n    mel_fmin=0.0,\n    mel_fmax=None,\n    num_mels=80,\n)\n\n# Init VITSArgs setting the arguments that are needed for the YourTTS model\nmodel_args = VitsArgs(\n    d_vector_file=D_VECTOR_FILES,\n    use_d_vector_file=True,\n    d_vector_dim=512,\n    num_layers_text_encoder=10,\n    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n    resblock_type_decoder=\"2\",  # In the paper, we accidentally trained the YourTTS using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n    # use_speaker_encoder_as_loss=True,\n    # Useful parameters to enable multilingual training\n    # use_language_embedding=True,\n    # embedded_language_dim=4,\n)\n\n# General training config, here you can change the batch size and others useful parameters\nconfig = VitsConfig(\n    output_path=OUT_PATH,\n    model_args=model_args,\n    run_name=RUN_NAME,\n    project_name=\"YourTTS\",\n    run_description=\"\"\"\n            - Original YourTTS trained using VCTK dataset\n        \"\"\",\n    dashboard_logger=\"tensorboard\",\n    logger_uri=None,\n    audio=audio_config,\n    batch_size=BATCH_SIZE,\n    batch_group_size=48,\n    eval_batch_size=BATCH_SIZE,\n    num_loader_workers=8,\n    eval_split_max_size=256,\n    print_step=50,\n    plot_step=100,\n    log_model_step=1000,\n    save_step=5000,\n    save_n_checkpoints=2,\n    save_checkpoints=True,\n    target_loss=\"loss_1\",\n    print_eval=False,\n    use_phonemes=False,\n    phonemizer=\"espeak\",\n    phoneme_language=\"en\",\n    compute_input_seq_cache=True,\n    add_blank=True,\n    text_cleaner=\"multilingual_cleaners\",\n    characters=CharactersConfig(\n        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n        pad=\"_\",\n        eos=\"&\",\n        bos=\"*\",\n        blank=None,\n        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\\u2013!'(),-.:;? \",\n        punctuations=\"!'(),-.:;? \",\n        phonemes=\"\",\n        is_unique=True,\n        is_sorted=True,\n    ),\n    phoneme_cache_path=None,\n    precompute_num_workers=12,\n    start_by_longest=True,\n    datasets=DATASETS_CONFIG_LIST,\n    cudnn_benchmark=False,\n    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n    mixed_precision=False,\n    test_sentences=[\n        [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"VCTK_p277\",\n            None,\n            \"en\",\n        ],\n        [\n            \"Be a voice, not an echo.\",\n            \"VCTK_p239\",\n            None,\n            \"en\",\n        ],\n        [\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"VCTK_p258\",\n            None,\n            \"en\",\n        ],\n        [\n            \"This cake is great. It's so delicious and moist.\",\n            \"VCTK_p244\",\n            None,\n            \"en\",\n        ],\n        [\n            \"Prior to November 22, 1963.\",\n            \"VCTK_p305\",\n            None,\n            \"en\",\n        ],\n    ],\n    # Enable the weighted sampler\n    use_weighted_sampler=True,\n    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n    weighted_sampler_attrs={\"speaker_name\": 1.0},\n    weighted_sampler_multipliers={},\n    # It defines the Speaker Consistency Loss (SCL) \u03b1 to 9 like the paper\n    speaker_encoder_loss_alpha=9.0,\n)\n\n# Load all the datasets samples and split traning and evaluation sets\ntrain_samples, eval_samples = load_tts_samples(\n    config.datasets,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# Init the model\nmodel = Vits.init_from_config(config)\n\n# Init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n    config,\n    output_path=OUT_PATH,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()\n", "recipes/vctk/glow_tts/train_glow_tts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.glow_tts import GlowTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\n# set experiment paths\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_path = os.path.join(output_path, \"../VCTK/\")\n\n# download the dataset if not downloaded\nif not os.path.exists(dataset_path):\n    from TTS.utils.downloaders import download_vctk\n\n    download_vctk(dataset_path)\n\n# define dataset config\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=dataset_path)\n\n# define audio config\n# \u2757 resample the dataset externally using `TTS/bin/resample.py` and set `resample=False` for faster training\naudio_config = BaseAudioConfig(sample_rate=22050, resample=True, do_trim_silence=True, trim_db=23.0)\n\n# define model config\nconfig = GlowTTSConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=500000,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.num_speakers = speaker_manager.num_speakers\n\n# init model\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/fast_speech/train_fast_speech.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.fast_speech_config import FastSpeechConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = FastSpeechConfig(\n    run_name=\"fast_speech_vctk\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,\n)\n\n## INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/vits/train_vits.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import Vits, VitsArgs, VitsAudioConfig\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=os.path.join(output_path, \"../VCTK/\")\n)\n\n\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\n\nvitsArgs = VitsArgs(\n    use_speaker_embedding=True,\n)\n\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    compute_input_seq_cache=True,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    max_text_len=325,  # change this if you have a larger VRAM than 16GB\n    output_path=output_path,\n    datasets=[dataset_config],\n    cudnn_benchmark=False,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()\n", "recipes/vctk/fast_pitch/train_fast_pitch.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.fast_pitch_config import FastPitchConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = FastPitchConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/tacotron2-DDC/train_tacotron2-ddc.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    preemphasis=0.0,\n)\n\nconfig = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=2,\n    # gradual_training=[[0, 6, 48], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n    double_decoder_consistency=True,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=150,\n    print_eval=False,\n    mixed_precision=True,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=44000 * 10,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,  # set this to enable multi-sepeaker training\n    decoder_ssim_alpha=0.0,  # disable ssim losses that causes NaN for some runs.\n    postnet_ssim_alpha=0.0,\n    postnet_diff_spec_alpha=0.0,\n    decoder_diff_spec_alpha=0.0,\n    attention_norm=\"softmax\",\n    optimizer=\"Adam\",\n    lr_scheduler=None,\n    lr=3e-5,\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it mainly handles speaker-id to speaker-name for the model and the data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n\n# init model\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/tacotron2/train_tacotron2.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    preemphasis=0.0,\n)\n\nconfig = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=2,\n    # gradual_training=[[0, 6, 48], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n    double_decoder_consistency=False,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=150,\n    print_eval=False,\n    mixed_precision=True,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=44000 * 10,\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,  # set this to enable multi-sepeaker training\n    decoder_ssim_alpha=0.0,  # disable ssim losses that causes NaN for some runs.\n    postnet_ssim_alpha=0.0,\n    postnet_diff_spec_alpha=0.0,\n    decoder_diff_spec_alpha=0.0,\n    attention_norm=\"softmax\",\n    optimizer=\"Adam\",\n    lr_scheduler=None,\n    lr=3e-5,\n)\n\n## INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it mainly handles speaker-id to speaker-name for the model and the data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n\n# init model\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/tacotron-DDC/train_tacotron-DDC.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron_config import TacotronConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron import Tacotron\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=True,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = TacotronConfig(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=48,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,\n    gradual_training=[[0, 6, 48], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n    double_decoder_consistency=True,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    min_text_len=0,\n    max_text_len=500,\n    min_audio_len=0,\n    max_audio_len=44000 * 10,  # 44k is the original sampling rate before resampling, corresponds to 10 seconds of audio\n    output_path=output_path,\n    datasets=[dataset_config],\n    use_speaker_embedding=True,  # set this to enable multi-sepeaker training\n)\n\n## INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it mainly handles speaker-id to speaker-name for the model and the data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n\n# init model\nmodel = Tacotron(config, ap, tokenizer, speaker_manager)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/vctk/delightful_tts/train_delightful_tts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.delightful_tts_config import DelightfulTtsAudioConfig, DelightfulTTSConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.delightful_tts import DelightfulTTS, DelightfulTtsArgs, VocoderConfig\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio.processor import AudioProcessor\n\ndata_path = \"/raid/datasets/vctk_v092_48khz_removed_silence_silero_vad\"\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n\ndataset_config = BaseDatasetConfig(\n    dataset_name=\"vctk\", formatter=\"vctk\", meta_file_train=\"\", path=data_path, language=\"en-us\"\n)\n\naudio_config = DelightfulTtsAudioConfig()\n\nmodel_args = DelightfulTtsArgs()\n\nvocoder_config = VocoderConfig()\n\nsomething_tts_config = DelightfulTTSConfig(\n    run_name=\"delightful_tts_vctk\",\n    run_description=\"Train like in delightful tts paper.\",\n    model_args=model_args,\n    audio=audio_config,\n    vocoder=vocoder_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=10,\n    num_eval_loader_workers=10,\n    precompute_num_workers=40,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=50,\n    print_eval=False,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n    start_by_longest=True,\n    binary_align_loss_alpha=0.0,\n    use_attn_priors=False,\n    max_text_len=60,\n    steps_to_start_discriminator=10000,\n)\n\ntokenizer, config = TTSTokenizer.init_from_config(something_tts_config)\n\nap = AudioProcessor.init_from_config(config)\n\n\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\n\nmodel = DelightfulTTS(ap=ap, config=config, tokenizer=tokenizer, speaker_manager=speaker_manager, emotion_manager=None)\n\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\ntrainer.fit()\n", "recipes/thorsten_DE/speedy_speech/train_speedy_speech.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig\nfrom TTS.tts.configs.speedy_speech_config import SpeedySpeechConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.forward_tts import ForwardTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = SpeedySpeechConfig(\n    run_name=\"speedy_speech_thorsten-de\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    min_audio_len=11050,  # need to up min_audio_len to avois speedy speech error\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"de\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=4,\n    print_step=50,\n    print_eval=False,\n    mixed_precision=False,\n    test_sentences=[\n        \"Es hat mich viel Zeit gekostet ein Stimme zu entwickeln, jetzt wo ich sie habe werde ich nicht mehr schweigen.\",\n        \"Sei eine Stimme, kein Echo.\",\n        \"Es tut mir Leid David. Das kann ich leider nicht machen.\",\n        \"Dieser Kuchen ist gro\u00dfartig. Er ist so lecker und feucht.\",\n        \"Vor dem 22. November 1963.\",\n    ],\n    max_seq_len=500000,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = ForwardTTS(config, ap, tokenizer)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/thorsten_DE/glow_tts/train_glowtts.py": "import os\n\n# Trainer: Where the \u2728\ufe0f happens.\n# TrainingArgs: Defines the set of arguments of the Trainer.\nfrom trainer import Trainer, TrainerArgs\n\n# GlowTTSConfig: all model related values for training, validating and testing.\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\n\n# BaseDatasetConfig: defines name, formatter and path of the dataset.\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.glow_tts import GlowTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\n\n# we use the same path as this script as our training folder.\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# DEFINE DATASET CONFIG\n# Set LJSpeech as our target dataset and define its path.\n# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"de\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    test_sentences=[\n        \"Es hat mich viel Zeit gekostet ein Stimme zu entwickeln, jetzt wo ich sie habe werde ich nicht mehr schweigen.\",\n        \"Sei eine Stimme, kein Echo.\",\n        \"Es tut mir Leid David. Das kann ich leider nicht machen.\",\n        \"Dieser Kuchen ist gro\u00dfartig. Er ist so lecker und feucht.\",\n        \"Vor dem 22. November 1963.\",\n    ],\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/thorsten_DE/univnet/train_univnet.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\nfrom TTS.vocoder.configs import UnivnetConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../thorsten-de/wavs/\"),\n    output_path=output_path,\n)\n\n# download dataset if not already present\nif not os.path.exists(config.data_path):\n    print(\"Downloading dataset\")\n    download_path = os.path.abspath(os.path.join(os.path.abspath(config.data_path), \"../../\"))\n    download_thorsten_de(download_path)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/thorsten_DE/vits_tts/train_vits.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import Vits, VitsAudioConfig\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n\naudio_config = VitsAudioConfig(\n    sample_rate=22050,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\n\nconfig = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_thorsten-de\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=0,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"de\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    compute_input_seq_cache=True,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=True,\n    test_sentences=[\n        \"Es hat mich viel Zeit gekostet ein Stimme zu entwickeln, jetzt wo ich sie habe werde ich nicht mehr schweigen.\",\n        \"Sei eine Stimme, kein Echo.\",\n        \"Es tut mir Leid David. Das kann ich leider nicht machen.\",\n        \"Dieser Kuchen ist gro\u00dfartig. Er ist so lecker und feucht.\",\n        \"Vor dem 22. November 1963.\",\n    ],\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()\n", "recipes/thorsten_DE/wavegrad/train_wavegrad.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\nfrom TTS.vocoder.configs import WavegradConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.wavegrad import Wavegrad\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=50,\n    print_step=50,\n    print_eval=True,\n    mixed_precision=False,\n    data_path=os.path.join(output_path, \"../thorsten-de/wavs/\"),\n    output_path=output_path,\n)\n\n# download dataset if not already present\nif not os.path.exists(config.data_path):\n    print(\"Downloading dataset\")\n    download_path = os.path.abspath(os.path.join(os.path.abspath(config.data_path), \"../../\"))\n    download_thorsten_de(download_path)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = Wavegrad(config)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()\n", "recipes/thorsten_DE/tacotron2-DDC/train_tacotron_ddc.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseAudioConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.tacotron2_config import Tacotron2Config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.tacotron2 import Tacotron2\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\n\n# from TTS.tts.datasets.tokenizer import Tokenizer\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,\n    preemphasis=0.0,\n)\n\nconfig = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=40,  # BS of 40 and max length of 10s will use about 20GB of GPU memory\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,\n    gradual_training=[[0, 6, 64], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n    double_decoder_consistency=True,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"de\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    precompute_num_workers=8,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    test_sentences=[\n        \"Es hat mich viel Zeit gekostet ein Stimme zu entwickeln, jetzt wo ich sie habe werde ich nicht mehr schweigen.\",\n        \"Sei eine Stimme, kein Echo.\",\n        \"Es tut mir Leid David. Das kann ich leider nicht machen.\",\n        \"Dieser Kuchen ist gro\u00dfartig. Er ist so lecker und feucht.\",\n        \"Vor dem 22. November 1963.\",\n    ],\n    # max audio length of 10 seconds, feel free to increase if you got more than 20GB GPU memory\n    max_audio_len=22050 * 10,\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/thorsten_DE/align_tts/train_aligntts.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.align_tts_config import AlignTTSConfig\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.align_tts import AlignTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n\nconfig = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"de\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    test_sentences=[\n        \"Es hat mich viel Zeit gekostet ein Stimme zu entwickeln, jetzt wo ich sie habe werde ich nicht mehr schweigen.\",\n        \"Sei eine Stimme, kein Echo.\",\n        \"Es tut mir Leid David. Das kann ich leider nicht machen.\",\n        \"Dieser Kuchen ist gro\u00dfartig. Er ist so lecker und feucht.\",\n        \"Vor dem 22. November 1963.\",\n    ],\n    output_path=output_path,\n    datasets=[dataset_config],\n)\n\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init model\nmodel = AlignTTS(config, ap, tokenizer)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the \ud83d\udc38TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... \ud83d\ude80\ntrainer.fit()\n", "recipes/thorsten_DE/hifigan/train_hifigan.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\nfrom TTS.vocoder.configs import HifiganConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\nconfig = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../thorsten-de/wavs/\"),\n    output_path=output_path,\n)\n\n# download dataset if not already present\nif not os.path.exists(config.data_path):\n    print(\"Downloading dataset\")\n    download_path = os.path.abspath(os.path.join(os.path.abspath(config.data_path), \"../../\"))\n    download_thorsten_de(download_path)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/thorsten_DE/multiband_melgan/train_multiband_melgan.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\nfrom TTS.vocoder.configs import MultibandMelganConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.gan import GAN\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\nconfig = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,\n    use_noise_augment=True,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=False,\n    mixed_precision=False,\n    lr_gen=1e-4,\n    lr_disc=1e-4,\n    data_path=os.path.join(output_path, \"../thorsten-de/wavs/\"),\n    output_path=output_path,\n)\n\n# download dataset if not already present\nif not os.path.exists(config.data_path):\n    print(\"Downloading dataset\")\n    download_path = os.path.abspath(os.path.join(os.path.abspath(config.data_path), \"../../\"))\n    download_thorsten_de(download_path)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = GAN(config, ap)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/thorsten_DE/wavernn/train_wavernn.py": "import os\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.downloaders import download_thorsten_de\nfrom TTS.vocoder.configs import WavernnConfig\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.models.wavernn import Wavernn\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,\n    pad_short=2000,\n    use_noise_augment=False,\n    eval_split_size=10,\n    print_step=25,\n    print_eval=True,\n    mixed_precision=False,\n    lr=1e-4,\n    grad_clip=4,\n    data_path=os.path.join(output_path, \"../thorsten-de/wavs/\"),\n    output_path=output_path,\n)\n\n# download dataset if not already present\nif not os.path.exists(config.data_path):\n    print(\"Downloading dataset\")\n    download_path = os.path.abspath(os.path.join(os.path.abspath(config.data_path), \"../../\"))\n    download_thorsten_de(download_path)\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n# init model\nmodel = Wavernn(config)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()\n", "recipes/multilingual/cml_yourtts/train_yourtts.py": "import os\n\nimport torch\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.bin.compute_embeddings import compute_embeddings\nfrom TTS.bin.resample import resample_files\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\nfrom TTS.utils.downloaders import download_libri_tts\n\ntorch.set_num_threads(24)\n\n# pylint: disable=W0105\n\"\"\"\n    This recipe replicates the first experiment proposed in the CML-TTS paper (https://arxiv.org/abs/2306.10097). It uses the YourTTS model.\n    YourTTS model is based on the VITS model however it uses external speaker embeddings extracted from a pre-trained speaker encoder and has small architecture changes.\n\"\"\"\nCURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n\n# Name of the run for the Trainer\nRUN_NAME = \"YourTTS-CML-TTS\"\n\n# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\nOUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n\n# If you want to do transfer learning and speedup your training you can set here the path to the CML-TTS available checkpoint that cam be downloaded here:  https://drive.google.com/u/2/uc?id=1yDCSJ1pFZQTHhL09GMbOrdjcPULApa0p\nRESTORE_PATH = \"/raid/edresson/CML_YourTTS/checkpoints_yourtts_cml_tts_dataset/best_model.pth\"  # Download the checkpoint here:  https://drive.google.com/u/2/uc?id=1yDCSJ1pFZQTHhL09GMbOrdjcPULApa0p\n\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 24000\n\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = float(\"inf\")\n\n### Download CML-TTS dataset\n# You need to download the dataset for all languages manually and extract it to a path and then set the CML_DATASET_PATH to this path: https://github.com/freds0/CML-TTS-Dataset#download\nCML_DATASET_PATH = \"./datasets/CML-TTS-Dataset/\"\n\n\n### Download LibriTTS dataset\n# it will automatic download the dataset, if you have problems you can comment it and manually donwload and extract it ! Download link: https://www.openslr.org/resources/60/train-clean-360.tar.gz\nLIBRITTS_DOWNLOAD_PATH = \"./datasets/LibriTTS/\"\n# Check if LibriTTS dataset is not already downloaded, if not download it\nif not os.path.exists(LIBRITTS_DOWNLOAD_PATH):\n    print(\">>> Downloading LibriTTS dataset:\")\n    download_libri_tts(LIBRITTS_DOWNLOAD_PATH, subset=\"libri-tts-clean-360\")\n\n# init LibriTTS configs\nlibritts_config = BaseDatasetConfig(\n    formatter=\"libri_tts\",\n    dataset_name=\"libri_tts\",\n    meta_file_train=\"\",\n    meta_file_val=\"\",\n    path=os.path.join(LIBRITTS_DOWNLOAD_PATH, \"train-clean-360/\"),\n    language=\"en\",\n)\n\n# init CML-TTS configs\npt_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_portuguese_v0.1/\"),\n    language=\"pt-br\",\n)\n\npl_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_polish_v0.1/\"),\n    language=\"pl\",\n)\n\nit_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_italian_v0.1/\"),\n    language=\"it\",\n)\n\nfr_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_french_v0.1/\"),\n    language=\"fr\",\n)\n\ndu_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_dutch_v0.1/\"),\n    language=\"du\",\n)\n\nge_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_german_v0.1/\"),\n    language=\"ge\",\n)\n\nsp_config = BaseDatasetConfig(\n    formatter=\"cml_tts\",\n    dataset_name=\"cml_tts\",\n    meta_file_train=\"train.csv\",\n    meta_file_val=\"\",\n    path=os.path.join(CML_DATASET_PATH, \"cml_tts_dataset_spanish_v0.1/\"),\n    language=\"sp\",\n)\n\n# Add here all datasets configs Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\nDATASETS_CONFIG_LIST = [libritts_config, pt_config, pl_config, it_config, fr_config, du_config, ge_config, sp_config]\n\n### Extract speaker embeddings\nSPEAKER_ENCODER_CHECKPOINT_PATH = (\n    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n)\nSPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n\nD_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it\n    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n    if not os.path.isfile(embeddings_file):\n        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n        compute_embeddings(\n            SPEAKER_ENCODER_CHECKPOINT_PATH,\n            SPEAKER_ENCODER_CONFIG_PATH,\n            embeddings_file,\n            old_speakers_file=None,\n            config_dataset_path=None,\n            formatter_name=dataset_conf.formatter,\n            dataset_name=dataset_conf.dataset_name,\n            dataset_path=dataset_conf.path,\n            meta_file_train=dataset_conf.meta_file_train,\n            meta_file_val=dataset_conf.meta_file_val,\n            disable_cuda=False,\n            no_eval=False,\n        )\n    D_VECTOR_FILES.append(embeddings_file)\n\n\n# Audio config used in training.\naudio_config = VitsAudioConfig(\n    sample_rate=SAMPLE_RATE,\n    hop_length=256,\n    win_length=1024,\n    fft_size=1024,\n    mel_fmin=0.0,\n    mel_fmax=None,\n    num_mels=80,\n)\n\n# Init VITSArgs setting the arguments that are needed for the YourTTS model\nmodel_args = VitsArgs(\n    spec_segment_size=62,\n    hidden_channels=192,\n    hidden_channels_ffn_text_encoder=768,\n    num_heads_text_encoder=2,\n    num_layers_text_encoder=10,\n    kernel_size_text_encoder=3,\n    dropout_p_text_encoder=0.1,\n    d_vector_file=D_VECTOR_FILES,\n    use_d_vector_file=True,\n    d_vector_dim=512,\n    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n    resblock_type_decoder=\"2\",  # In the paper, we accidentally trained the YourTTS using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n    use_speaker_encoder_as_loss=False,\n    # Useful parameters to enable multilingual training\n    use_language_embedding=True,\n    embedded_language_dim=4,\n)\n\n# General training config, here you can change the batch size and others useful parameters\nconfig = VitsConfig(\n    output_path=OUT_PATH,\n    model_args=model_args,\n    run_name=RUN_NAME,\n    project_name=\"YourTTS\",\n    run_description=\"\"\"\n            - YourTTS trained using CML-TTS and LibriTTS datasets\n        \"\"\",\n    dashboard_logger=\"tensorboard\",\n    logger_uri=None,\n    audio=audio_config,\n    batch_size=BATCH_SIZE,\n    batch_group_size=48,\n    eval_batch_size=BATCH_SIZE,\n    num_loader_workers=8,\n    eval_split_max_size=256,\n    print_step=50,\n    plot_step=100,\n    log_model_step=1000,\n    save_step=5000,\n    save_n_checkpoints=2,\n    save_checkpoints=True,\n    target_loss=\"loss_1\",\n    print_eval=False,\n    use_phonemes=False,\n    phonemizer=\"espeak\",\n    phoneme_language=\"en\",\n    compute_input_seq_cache=True,\n    add_blank=True,\n    text_cleaner=\"multilingual_cleaners\",\n    characters=CharactersConfig(\n        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n        pad=\"_\",\n        eos=\"&\",\n        bos=\"*\",\n        blank=None,\n        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00a1\\u00a3\\u00b7\\u00b8\\u00c0\\u00c1\\u00c2\\u00c3\\u00c4\\u00c5\\u00c7\\u00c8\\u00c9\\u00ca\\u00cb\\u00cc\\u00cd\\u00ce\\u00cf\\u00d1\\u00d2\\u00d3\\u00d4\\u00d5\\u00d6\\u00d9\\u00da\\u00db\\u00dc\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e5\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u0101\\u0104\\u0105\\u0106\\u0107\\u010b\\u0119\\u0141\\u0142\\u0143\\u0144\\u0152\\u0153\\u015a\\u015b\\u0161\\u0178\\u0179\\u017a\\u017b\\u017c\\u020e\\u04e7\\u05c2\\u1b20\",\n        punctuations=\"\\u2014!'(),-.:;?\\u00bf \",\n        phonemes=\"iy\\u0268\\u0289\\u026fu\\u026a\\u028f\\u028ae\\u00f8\\u0258\\u0259\\u0275\\u0264o\\u025b\\u0153\\u025c\\u025e\\u028c\\u0254\\u00e6\\u0250a\\u0276\\u0251\\u0252\\u1d7b\\u0298\\u0253\\u01c0\\u0257\\u01c3\\u0284\\u01c2\\u0260\\u01c1\\u029bpbtd\\u0288\\u0256c\\u025fk\\u0261q\\u0262\\u0294\\u0274\\u014b\\u0272\\u0273n\\u0271m\\u0299r\\u0280\\u2c71\\u027e\\u027d\\u0278\\u03b2fv\\u03b8\\u00f0sz\\u0283\\u0292\\u0282\\u0290\\u00e7\\u029dx\\u0263\\u03c7\\u0281\\u0127\\u0295h\\u0266\\u026c\\u026e\\u028b\\u0279\\u027bj\\u0270l\\u026d\\u028e\\u029f\\u02c8\\u02cc\\u02d0\\u02d1\\u028dw\\u0265\\u029c\\u02a2\\u02a1\\u0255\\u0291\\u027a\\u0267\\u025a\\u02de\\u026b'\\u0303' \",\n        is_unique=True,\n        is_sorted=True,\n    ),\n    phoneme_cache_path=None,\n    precompute_num_workers=12,\n    start_by_longest=True,\n    datasets=DATASETS_CONFIG_LIST,\n    cudnn_benchmark=False,\n    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n    mixed_precision=False,\n    test_sentences=[\n        [\"Voc\\u00ea ter\\u00e1 a vista do topo da montanha que voc\\u00ea escalar.\", \"9351\", None, \"pt-br\"],\n        [\"Quando voc\\u00ea n\\u00e3o corre nenhum risco, voc\\u00ea arrisca tudo.\", \"12249\", None, \"pt-br\"],\n        [\n            \"S\\u00e3o necess\\u00e1rios muitos anos de trabalho para ter sucesso da noite para o dia.\",\n            \"2961\",\n            None,\n            \"pt-br\",\n        ],\n        [\"You'll have the view of the top of the mountain that you climb.\", \"LTTS_6574\", None, \"en\"],\n        [\"When you don\\u2019t take any risks, you risk everything.\", \"LTTS_6206\", None, \"en\"],\n        [\"Are necessary too many years of work to succeed overnight.\", \"LTTS_5717\", None, \"en\"],\n        [\"Je hebt uitzicht op de top van de berg die je beklimt.\", \"960\", None, \"du\"],\n        [\"Als je geen risico neemt, riskeer je alles.\", \"2450\", None, \"du\"],\n        [\"Zijn te veel jaren werk nodig om van de ene op de andere dag te slagen.\", \"10984\", None, \"du\"],\n        [\"Vous aurez la vue sur le sommet de la montagne que vous gravirez.\", \"6381\", None, \"fr\"],\n        [\"Quand tu ne prends aucun risque, tu risques tout.\", \"2825\", None, \"fr\"],\n        [\n            \"Sont n\\u00e9cessaires trop d'ann\\u00e9es de travail pour r\\u00e9ussir du jour au lendemain.\",\n            \"1844\",\n            None,\n            \"fr\",\n        ],\n        [\"Sie haben die Aussicht auf die Spitze des Berges, den Sie erklimmen.\", \"2314\", None, \"ge\"],\n        [\"Wer nichts riskiert, riskiert alles.\", \"7483\", None, \"ge\"],\n        [\"Es sind zu viele Jahre Arbeit notwendig, um \\u00fcber Nacht erfolgreich zu sein.\", \"12461\", None, \"ge\"],\n        [\"Avrai la vista della cima della montagna che sali.\", \"4998\", None, \"it\"],\n        [\"Quando non corri alcun rischio, rischi tutto.\", \"6744\", None, \"it\"],\n        [\"Are necessary too many years of work to succeed overnight.\", \"1157\", None, \"it\"],\n        [\n            \"B\\u0119dziesz mie\\u0107 widok na szczyt g\\u00f3ry, na kt\\u00f3r\\u0105 si\\u0119 wspinasz.\",\n            \"7014\",\n            None,\n            \"pl\",\n        ],\n        [\"Kiedy nie podejmujesz \\u017cadnego ryzyka, ryzykujesz wszystko.\", \"3492\", None, \"pl\"],\n        [\n            \"Potrzebne s\\u0105 zbyt wiele lat pracy, aby odnie\\u015b\\u0107 sukces z dnia na dzie\\u0144.\",\n            \"1890\",\n            None,\n            \"pl\",\n        ],\n        [\"Tendr\\u00e1s la vista de la cima de la monta\\u00f1a que subes\", \"101\", None, \"sp\"],\n        [\"Cuando no te arriesgas, lo arriesgas todo.\", \"5922\", None, \"sp\"],\n        [\n            \"Son necesarios demasiados a\\u00f1os de trabajo para triunfar de la noche a la ma\\u00f1ana.\",\n            \"10246\",\n            None,\n            \"sp\",\n        ],\n    ],\n    # Enable the weighted sampler\n    use_weighted_sampler=True,\n    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n    # weighted_sampler_attrs={\"language\": 1.0, \"speaker_name\": 1.0},\n    weighted_sampler_attrs={\"language\": 1.0},\n    weighted_sampler_multipliers={\n        # \"speaker_name\": {\n        # you can force the batching scheme to give a higher weight to a certain speaker and then this speaker will appears more frequently on the batch.\n        # It will speedup the speaker adaptation process. Considering the CML train dataset and \"new_speaker\" as the speaker name of the speaker that you want to adapt.\n        # The line above will make the balancer consider the \"new_speaker\" as 106 speakers so 1/4 of the number of speakers present on CML dataset.\n        # 'new_speaker': 106, # (CML tot. train speaker)/4 = (424/4) = 106\n        # }\n    },\n    # It defines the Speaker Consistency Loss (SCL) \u03b1 to 9 like the YourTTS paper\n    speaker_encoder_loss_alpha=9.0,\n)\n\n# Load all the datasets samples and split traning and evaluation sets\ntrain_samples, eval_samples = load_tts_samples(\n    config.datasets,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# Init the model\nmodel = Vits.init_from_config(config)\n\n# Init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n    config,\n    output_path=OUT_PATH,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()\n", "recipes/multilingual/vits_tts/train_vits_tts.py": "import os\nfrom glob import glob\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\nfrom TTS.tts.utils.languages import LanguageManager\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = os.path.dirname(os.path.abspath(__file__))\n\nmailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(formatter=\"mailabs\", meta_file_train=None, path=path, language=path.split(\"/\")[-1])\n    for path in dataset_paths\n]\n\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\n\nvitsArgs = VitsArgs(\n    use_language_embedding=True,\n    embedded_language_dim=4,\n    use_speaker_embedding=True,\n    use_sdp=False,\n)\n\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    use_speaker_embedding=True,\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=0,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"multilingual_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    compute_input_seq_cache=True,\n    print_step=25,\n    use_language_weighted_sampler=True,\n    print_eval=False,\n    mixed_precision=False,\n    min_audio_len=32 * 256 * 4,\n    max_audio_len=160000,\n    output_path=output_path,\n    datasets=dataset_config,\n    characters=CharactersConfig(\n        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n        pad=\"<PAD>\",\n        eos=\"<EOS>\",\n        bos=\"<BOS>\",\n        blank=\"<BLNK>\",\n        characters=\"!\u00a1'(),-.:;\u00bf?abcdefghijklmnopqrstuvwxyz\u00b5\u00df\u00e0\u00e1\u00e2\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f1\u00f2\u00f3\u00f4\u00f6\u00f9\u00fa\u00fb\u00fc\u0105\u0107\u0119\u0142\u0144\u0153\u015b\u015f\u017a\u017c\u0192\u0430\u0431\u0432\u0433\u0434\u0435\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f\u0451\u0454\u0456\u0457\u0491\u04e7 \u00ab\u00b0\u00b1\u00b5\u00bb$%&\u2018\u2019\u201a\u201c`\u201d\u201e\",\n        punctuations=\"!\u00a1'(),-.:;\u00bf? \",\n        phonemes=None,\n    ),\n    test_sentences=[\n        [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"mary_ann\",\n            None,\n            \"en_US\",\n        ],\n        [\n            \"Il m'a fallu beaucoup de temps pour d\\u00e9velopper une voix, et maintenant que je l'ai, je ne vais pas me taire.\",\n            \"ezwa\",\n            None,\n            \"fr_FR\",\n        ],\n        [\"Ich finde, dieses Startup ist wirklich unglaublich.\", \"eva_k\", None, \"de_DE\"],\n        [\"\u042f \u0434\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u044d\u0442\u043e\u0442 \u0441\u0442\u0430\u0440\u0442\u0430\u043f \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u0434\u0438\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439.\", \"oblomov\", None, \"ru_RU\"],\n    ],\n)\n\n# force the convertion of the custom characters to a config attribute\nconfig.from_dict(config.to_dict())\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "recipes/multilingual/vits_tts/train_vits_tts_phonemes.py": "import os\nfrom glob import glob\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.vits import Vits, VitsArgs, VitsAudioConfig\nfrom TTS.tts.utils.languages import LanguageManager\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\n\noutput_path = \"/media/julian/Workdisk/train\"\n\nmailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(\n        formatter=\"mailabs\",\n        meta_file_train=None,\n        path=path,\n        language=path.split(\"/\")[-1],  # language code is the folder name\n    )\n    for path in dataset_paths\n]\n\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\n\nvitsArgs = VitsArgs(\n    use_language_embedding=True,\n    embedded_language_dim=4,\n    use_speaker_embedding=True,\n    use_sdp=False,\n)\n\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    use_speaker_embedding=True,\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=0,\n    num_loader_workers=12,\n    num_eval_loader_workers=12,\n    precompute_num_workers=12,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"multilingual_cleaners\",\n    use_phonemes=True,\n    phoneme_language=None,\n    phonemizer=\"multi_phonemizer\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    compute_input_seq_cache=True,\n    print_step=25,\n    use_language_weighted_sampler=True,\n    print_eval=False,\n    mixed_precision=False,\n    min_audio_len=audio_config.sample_rate,\n    max_audio_len=audio_config.sample_rate * 10,\n    output_path=output_path,\n    datasets=dataset_config,\n    test_sentences=[\n        [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"mary_ann\",\n            None,\n            \"en-us\",\n        ],\n        [\n            \"Il m'a fallu beaucoup de temps pour d\\u00e9velopper une voix, et maintenant que je l'ai, je ne vais pas me taire.\",\n            \"ezwa\",\n            None,\n            \"fr-fr\",\n        ],\n        [\"Ich finde, dieses Startup ist wirklich unglaublich.\", \"eva_k\", None, \"de-de\"],\n        [\"\u042f \u0434\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u044d\u0442\u043e\u0442 \u0441\u0442\u0430\u0440\u0442\u0430\u043f \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u0434\u0438\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439.\", \"nikolaev\", None, \"ru\"],\n    ],\n)\n\n# force the convertion of the custom characters to a config attribute\nconfig.from_dict(config.to_dict())\n\n# init audio processor\nap = AudioProcessor(**config.audio.to_dict())\n\n# load training samples\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader\nspeaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n\n# init the trainer and \ud83d\ude80\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()\n", "TTS/model.py": "from abc import abstractmethod\nfrom typing import Dict\n\nimport torch\nfrom coqpit import Coqpit\nfrom trainer import TrainerModel\n\n# pylint: skip-file\n\n\nclass BaseTrainerModel(TrainerModel):\n    \"\"\"BaseTrainerModel model expanding TrainerModel with required functions by \ud83d\udc38TTS.\n\n    Every new \ud83d\udc38TTS model must inherit it.\n    \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def init_from_config(config: Coqpit):\n        \"\"\"Init the model and all its attributes from the given config.\n\n        Override this depending on your model.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def inference(self, input: torch.Tensor, aux_input={}) -> Dict:\n        \"\"\"Forward pass for inference.\n\n        It must return a dictionary with the main model output and all the auxiliary outputs. The key ```model_outputs```\n        is considered to be the main output and you can add any other auxiliary outputs as you want.\n\n        We don't use `*kwargs` since it is problematic with the TorchScript API.\n\n        Args:\n            input (torch.Tensor): [description]\n            aux_input (Dict): Auxiliary inputs like speaker embeddings, durations etc.\n\n        Returns:\n            Dict: [description]\n        \"\"\"\n        outputs_dict = {\"model_outputs\": None}\n        ...\n        return outputs_dict\n\n    @abstractmethod\n    def load_checkpoint(\n        self, config: Coqpit, checkpoint_path: str, eval: bool = False, strict: bool = True, cache=False\n    ) -> None:\n        \"\"\"Load a model checkpoint gile and get ready for training or inference.\n\n        Args:\n            config (Coqpit): Model configuration.\n            checkpoint_path (str): Path to the model checkpoint file.\n            eval (bool, optional): If true, init model for inference else for training. Defaults to False.\n            strict (bool, optional): Match all checkpoint keys to model's keys. Defaults to True.\n            cache (bool, optional): If True, cache the file locally for subsequent calls. It is cached under `get_user_data_dir()/tts_cache`. Defaults to False.\n        \"\"\"\n        ...\n", "TTS/api.py": "import tempfile\nimport warnings\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nfrom torch import nn\n\nfrom TTS.utils.audio.numpy_transforms import save_wav\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\nfrom TTS.config import load_config\n\n\nclass TTS(nn.Module):\n    \"\"\"TODO: Add voice conversion and Capacitron support.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"\",\n        model_path: str = None,\n        config_path: str = None,\n        vocoder_path: str = None,\n        vocoder_config_path: str = None,\n        progress_bar: bool = True,\n        gpu=False,\n    ):\n        \"\"\"\ud83d\udc38TTS python interface that allows to load and use the released models.\n\n        Example with a multi-speaker model:\n            >>> from TTS.api import TTS\n            >>> tts = TTS(TTS.list_models()[0])\n            >>> wav = tts.tts(\"This is a test! This is also a test!!\", speaker=tts.speakers[0], language=tts.languages[0])\n            >>> tts.tts_to_file(text=\"Hello world!\", speaker=tts.speakers[0], language=tts.languages[0], file_path=\"output.wav\")\n\n        Example with a single-speaker model:\n            >>> tts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False, gpu=False)\n            >>> tts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=\"output.wav\")\n\n        Example loading a model from a path:\n            >>> tts = TTS(model_path=\"/path/to/checkpoint_100000.pth\", config_path=\"/path/to/config.json\", progress_bar=False, gpu=False)\n            >>> tts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=\"output.wav\")\n\n        Example voice cloning with YourTTS in English, French and Portuguese:\n            >>> tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False, gpu=True)\n            >>> tts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"thisisit.wav\")\n            >>> tts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr\", file_path=\"thisisit.wav\")\n            >>> tts.tts_to_file(\"Isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt\", file_path=\"thisisit.wav\")\n\n        Example Fairseq TTS models (uses ISO language codes in https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html):\n            >>> tts = TTS(model_name=\"tts_models/eng/fairseq/vits\", progress_bar=False, gpu=True)\n            >>> tts.tts_to_file(\"This is a test.\", file_path=\"output.wav\")\n\n        Args:\n            model_name (str, optional): Model name to load. You can list models by ```tts.models```. Defaults to None.\n            model_path (str, optional): Path to the model checkpoint. Defaults to None.\n            config_path (str, optional): Path to the model config. Defaults to None.\n            vocoder_path (str, optional): Path to the vocoder checkpoint. Defaults to None.\n            vocoder_config_path (str, optional): Path to the vocoder config. Defaults to None.\n            progress_bar (bool, optional): Whether to pring a progress bar while downloading a model. Defaults to True.\n            gpu (bool, optional): Enable/disable GPU. Some models might be too slow on CPU. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.manager = ModelManager(models_file=self.get_models_file_path(), progress_bar=progress_bar, verbose=False)\n        self.config = load_config(config_path) if config_path else None\n        self.synthesizer = None\n        self.voice_converter = None\n        self.model_name = \"\"\n        if gpu:\n            warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n\n        if model_name is not None and len(model_name) > 0:\n            if \"tts_models\" in model_name:\n                self.load_tts_model_by_name(model_name, gpu)\n            elif \"voice_conversion_models\" in model_name:\n                self.load_vc_model_by_name(model_name, gpu)\n            else:\n                self.load_model_by_name(model_name, gpu)\n\n        if model_path:\n            self.load_tts_model_by_path(\n                model_path, config_path, vocoder_path=vocoder_path, vocoder_config=vocoder_config_path, gpu=gpu\n            )\n\n    @property\n    def models(self):\n        return self.manager.list_tts_models()\n\n    @property\n    def is_multi_speaker(self):\n        if hasattr(self.synthesizer.tts_model, \"speaker_manager\") and self.synthesizer.tts_model.speaker_manager:\n            return self.synthesizer.tts_model.speaker_manager.num_speakers > 1\n        return False\n\n    @property\n    def is_multi_lingual(self):\n        # Not sure what sets this to None, but applied a fix to prevent crashing.\n        if (\n            isinstance(self.model_name, str)\n            and \"xtts\" in self.model_name\n            or self.config\n            and (\"xtts\" in self.config.model or len(self.config.languages) > 1)\n        ):\n            return True\n        if hasattr(self.synthesizer.tts_model, \"language_manager\") and self.synthesizer.tts_model.language_manager:\n            return self.synthesizer.tts_model.language_manager.num_languages > 1\n        return False\n\n    @property\n    def speakers(self):\n        if not self.is_multi_speaker:\n            return None\n        return self.synthesizer.tts_model.speaker_manager.speaker_names\n\n    @property\n    def languages(self):\n        if not self.is_multi_lingual:\n            return None\n        return self.synthesizer.tts_model.language_manager.language_names\n\n    @staticmethod\n    def get_models_file_path():\n        return Path(__file__).parent / \".models.json\"\n\n    def list_models(self):\n        return ModelManager(models_file=TTS.get_models_file_path(), progress_bar=False, verbose=False)\n\n    def download_model_by_name(self, model_name: str):\n        model_path, config_path, model_item = self.manager.download_model(model_name)\n        if \"fairseq\" in model_name or (model_item is not None and isinstance(model_item[\"model_url\"], list)):\n            # return model directory if there are multiple files\n            # we assume that the model knows how to load itself\n            return None, None, None, None, model_path\n        if model_item.get(\"default_vocoder\") is None:\n            return model_path, config_path, None, None, None\n        vocoder_path, vocoder_config_path, _ = self.manager.download_model(model_item[\"default_vocoder\"])\n        return model_path, config_path, vocoder_path, vocoder_config_path, None\n\n    def load_model_by_name(self, model_name: str, gpu: bool = False):\n        \"\"\"Load one of the \ud83d\udc38TTS models by name.\n\n        Args:\n            model_name (str): Model name to load. You can list models by ```tts.models```.\n            gpu (bool, optional): Enable/disable GPU. Some models might be too slow on CPU. Defaults to False.\n        \"\"\"\n        self.load_tts_model_by_name(model_name, gpu)\n\n    def load_vc_model_by_name(self, model_name: str, gpu: bool = False):\n        \"\"\"Load one of the voice conversion models by name.\n\n        Args:\n            model_name (str): Model name to load. You can list models by ```tts.models```.\n            gpu (bool, optional): Enable/disable GPU. Some models might be too slow on CPU. Defaults to False.\n        \"\"\"\n        self.model_name = model_name\n        model_path, config_path, _, _, _ = self.download_model_by_name(model_name)\n        self.voice_converter = Synthesizer(vc_checkpoint=model_path, vc_config=config_path, use_cuda=gpu)\n\n    def load_tts_model_by_name(self, model_name: str, gpu: bool = False):\n        \"\"\"Load one of \ud83d\udc38TTS models by name.\n\n        Args:\n            model_name (str): Model name to load. You can list models by ```tts.models```.\n            gpu (bool, optional): Enable/disable GPU. Some models might be too slow on CPU. Defaults to False.\n\n        TODO: Add tests\n        \"\"\"\n        self.synthesizer = None\n        self.model_name = model_name\n\n        model_path, config_path, vocoder_path, vocoder_config_path, model_dir = self.download_model_by_name(\n            model_name\n        )\n\n        # init synthesizer\n        # None values are fetch from the model\n        self.synthesizer = Synthesizer(\n            tts_checkpoint=model_path,\n            tts_config_path=config_path,\n            tts_speakers_file=None,\n            tts_languages_file=None,\n            vocoder_checkpoint=vocoder_path,\n            vocoder_config=vocoder_config_path,\n            encoder_checkpoint=None,\n            encoder_config=None,\n            model_dir=model_dir,\n            use_cuda=gpu,\n        )\n\n    def load_tts_model_by_path(\n        self, model_path: str, config_path: str, vocoder_path: str = None, vocoder_config: str = None, gpu: bool = False\n    ):\n        \"\"\"Load a model from a path.\n\n        Args:\n            model_path (str): Path to the model checkpoint.\n            config_path (str): Path to the model config.\n            vocoder_path (str, optional): Path to the vocoder checkpoint. Defaults to None.\n            vocoder_config (str, optional): Path to the vocoder config. Defaults to None.\n            gpu (bool, optional): Enable/disable GPU. Some models might be too slow on CPU. Defaults to False.\n        \"\"\"\n\n        self.synthesizer = Synthesizer(\n            tts_checkpoint=model_path,\n            tts_config_path=config_path,\n            tts_speakers_file=None,\n            tts_languages_file=None,\n            vocoder_checkpoint=vocoder_path,\n            vocoder_config=vocoder_config,\n            encoder_checkpoint=None,\n            encoder_config=None,\n            use_cuda=gpu,\n        )\n\n    def _check_arguments(\n        self,\n        speaker: str = None,\n        language: str = None,\n        speaker_wav: str = None,\n        emotion: str = None,\n        speed: float = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"Check if the arguments are valid for the model.\"\"\"\n        # check for the coqui tts models\n        if self.is_multi_speaker and (speaker is None and speaker_wav is None):\n            raise ValueError(\"Model is multi-speaker but no `speaker` is provided.\")\n        if self.is_multi_lingual and language is None:\n            raise ValueError(\"Model is multi-lingual but no `language` is provided.\")\n        if not self.is_multi_speaker and speaker is not None and \"voice_dir\" not in kwargs:\n            raise ValueError(\"Model is not multi-speaker but `speaker` is provided.\")\n        if not self.is_multi_lingual and language is not None:\n            raise ValueError(\"Model is not multi-lingual but `language` is provided.\")\n        if not emotion is None and not speed is None:\n            raise ValueError(\"Emotion and speed can only be used with Coqui Studio models. Which is discontinued.\")\n\n    def tts(\n        self,\n        text: str,\n        speaker: str = None,\n        language: str = None,\n        speaker_wav: str = None,\n        emotion: str = None,\n        speed: float = None,\n        split_sentences: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Convert text to speech.\n\n        Args:\n            text (str):\n                Input text to synthesize.\n            speaker (str, optional):\n                Speaker name for multi-speaker. You can check whether loaded model is multi-speaker by\n                `tts.is_multi_speaker` and list speakers by `tts.speakers`. Defaults to None.\n            language (str): Language of the text. If None, the default language of the speaker is used. Language is only\n                supported by `XTTS` model.\n            speaker_wav (str, optional):\n                Path to a reference wav file to use for voice cloning with supporting models like YourTTS.\n                Defaults to None.\n            emotion (str, optional):\n                Emotion to use for \ud83d\udc38Coqui Studio models. If None, Studio models use \"Neutral\". Defaults to None.\n            speed (float, optional):\n                Speed factor to use for \ud83d\udc38Coqui Studio models, between 0 and 2.0. If None, Studio models use 1.0.\n                Defaults to None.\n            split_sentences (bool, optional):\n                Split text into sentences, synthesize them separately and concatenate the file audio.\n                Setting it False uses more VRAM and possibly hit model specific text length or VRAM limits. Only\n                applicable to the \ud83d\udc38TTS models. Defaults to True.\n            kwargs (dict, optional):\n                Additional arguments for the model.\n        \"\"\"\n        self._check_arguments(\n            speaker=speaker, language=language, speaker_wav=speaker_wav, emotion=emotion, speed=speed, **kwargs\n        )\n        wav = self.synthesizer.tts(\n            text=text,\n            speaker_name=speaker,\n            language_name=language,\n            speaker_wav=speaker_wav,\n            reference_wav=None,\n            style_wav=None,\n            style_text=None,\n            reference_speaker_name=None,\n            split_sentences=split_sentences,\n            **kwargs,\n        )\n        return wav\n\n    def tts_to_file(\n        self,\n        text: str,\n        speaker: str = None,\n        language: str = None,\n        speaker_wav: str = None,\n        emotion: str = None,\n        speed: float = 1.0,\n        pipe_out=None,\n        file_path: str = \"output.wav\",\n        split_sentences: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Convert text to speech.\n\n        Args:\n            text (str):\n                Input text to synthesize.\n            speaker (str, optional):\n                Speaker name for multi-speaker. You can check whether loaded model is multi-speaker by\n                `tts.is_multi_speaker` and list speakers by `tts.speakers`. Defaults to None.\n            language (str, optional):\n                Language code for multi-lingual models. You can check whether loaded model is multi-lingual\n                `tts.is_multi_lingual` and list available languages by `tts.languages`. Defaults to None.\n            speaker_wav (str, optional):\n                Path to a reference wav file to use for voice cloning with supporting models like YourTTS.\n                Defaults to None.\n            emotion (str, optional):\n                Emotion to use for \ud83d\udc38Coqui Studio models. Defaults to \"Neutral\".\n            speed (float, optional):\n                Speed factor to use for \ud83d\udc38Coqui Studio models, between 0.0 and 2.0. Defaults to None.\n            pipe_out (BytesIO, optional):\n                Flag to stdout the generated TTS wav file for shell pipe.\n            file_path (str, optional):\n                Output file path. Defaults to \"output.wav\".\n            split_sentences (bool, optional):\n                Split text into sentences, synthesize them separately and concatenate the file audio.\n                Setting it False uses more VRAM and possibly hit model specific text length or VRAM limits. Only\n                applicable to the \ud83d\udc38TTS models. Defaults to True.\n            kwargs (dict, optional):\n                Additional arguments for the model.\n        \"\"\"\n        self._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, **kwargs)\n\n        wav = self.tts(\n            text=text,\n            speaker=speaker,\n            language=language,\n            speaker_wav=speaker_wav,\n            split_sentences=split_sentences,\n            **kwargs,\n        )\n        self.synthesizer.save_wav(wav=wav, path=file_path, pipe_out=pipe_out)\n        return file_path\n\n    def voice_conversion(\n        self,\n        source_wav: str,\n        target_wav: str,\n    ):\n        \"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\n\n        Args:``\n            source_wav (str):\n                Path to the source wav file.\n            target_wav (str):`\n                Path to the target wav file.\n        \"\"\"\n        wav = self.voice_converter.voice_conversion(source_wav=source_wav, target_wav=target_wav)\n        return wav\n\n    def voice_conversion_to_file(\n        self,\n        source_wav: str,\n        target_wav: str,\n        file_path: str = \"output.wav\",\n    ):\n        \"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\n\n        Args:\n            source_wav (str):\n                Path to the source wav file.\n            target_wav (str):\n                Path to the target wav file.\n            file_path (str, optional):\n                Output file path. Defaults to \"output.wav\".\n        \"\"\"\n        wav = self.voice_conversion(source_wav=source_wav, target_wav=target_wav)\n        save_wav(wav=wav, path=file_path, sample_rate=self.voice_converter.vc_config.audio.output_sample_rate)\n        return file_path\n\n    def tts_with_vc(\n        self,\n        text: str,\n        language: str = None,\n        speaker_wav: str = None,\n        speaker: str = None,\n        split_sentences: bool = True,\n    ):\n        \"\"\"Convert text to speech with voice conversion.\n\n        It combines tts with voice conversion to fake voice cloning.\n\n        - Convert text to speech with tts.\n        - Convert the output wav to target speaker with voice conversion.\n\n        Args:\n            text (str):\n                Input text to synthesize.\n            language (str, optional):\n                Language code for multi-lingual models. You can check whether loaded model is multi-lingual\n                `tts.is_multi_lingual` and list available languages by `tts.languages`. Defaults to None.\n            speaker_wav (str, optional):\n                Path to a reference wav file to use for voice cloning with supporting models like YourTTS.\n                Defaults to None.\n            speaker (str, optional):\n                Speaker name for multi-speaker. You can check whether loaded model is multi-speaker by\n                `tts.is_multi_speaker` and list speakers by `tts.speakers`. Defaults to None.\n            split_sentences (bool, optional):\n                Split text into sentences, synthesize them separately and concatenate the file audio.\n                Setting it False uses more VRAM and possibly hit model specific text length or VRAM limits. Only\n                applicable to the \ud83d\udc38TTS models. Defaults to True.\n        \"\"\"\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n            # Lazy code... save it to a temp file to resample it while reading it for VC\n            self.tts_to_file(\n                text=text, speaker=speaker, language=language, file_path=fp.name, split_sentences=split_sentences\n            )\n        if self.voice_converter is None:\n            self.load_vc_model_by_name(\"voice_conversion_models/multilingual/vctk/freevc24\")\n        wav = self.voice_converter.voice_conversion(source_wav=fp.name, target_wav=speaker_wav)\n        return wav\n\n    def tts_with_vc_to_file(\n        self,\n        text: str,\n        language: str = None,\n        speaker_wav: str = None,\n        file_path: str = \"output.wav\",\n        speaker: str = None,\n        split_sentences: bool = True,\n    ):\n        \"\"\"Convert text to speech with voice conversion and save to file.\n\n        Check `tts_with_vc` for more details.\n\n        Args:\n            text (str):\n                Input text to synthesize.\n            language (str, optional):\n                Language code for multi-lingual models. You can check whether loaded model is multi-lingual\n                `tts.is_multi_lingual` and list available languages by `tts.languages`. Defaults to None.\n            speaker_wav (str, optional):\n                Path to a reference wav file to use for voice cloning with supporting models like YourTTS.\n                Defaults to None.\n            file_path (str, optional):\n                Output file path. Defaults to \"output.wav\".\n            speaker (str, optional):\n                Speaker name for multi-speaker. You can check whether loaded model is multi-speaker by\n                `tts.is_multi_speaker` and list speakers by `tts.speakers`. Defaults to None.\n            split_sentences (bool, optional):\n                Split text into sentences, synthesize them separately and concatenate the file audio.\n                Setting it False uses more VRAM and possibly hit model specific text length or VRAM limits. Only\n                applicable to the \ud83d\udc38TTS models. Defaults to True.\n        \"\"\"\n        wav = self.tts_with_vc(\n            text=text, language=language, speaker_wav=speaker_wav, speaker=speaker, split_sentences=split_sentences\n        )\n        save_wav(wav=wav, path=file_path, sample_rate=self.voice_converter.vc_config.audio.output_sample_rate)\n", "TTS/__init__.py": "import os\n\nwith open(os.path.join(os.path.dirname(__file__), \"VERSION\"), \"r\", encoding=\"utf-8\") as f:\n    version = f.read().strip()\n\n__version__ = version\n", "TTS/utils/training.py": "import numpy as np\nimport torch\n\n\ndef check_update(model, grad_clip, ignore_stopnet=False, amp_opt_params=None):\n    r\"\"\"Check model gradient against unexpected jumps and failures\"\"\"\n    skip_flag = False\n    if ignore_stopnet:\n        if not amp_opt_params:\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                [param for name, param in model.named_parameters() if \"stopnet\" not in name], grad_clip\n            )\n        else:\n            grad_norm = torch.nn.utils.clip_grad_norm_(amp_opt_params, grad_clip)\n    else:\n        if not amp_opt_params:\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        else:\n            grad_norm = torch.nn.utils.clip_grad_norm_(amp_opt_params, grad_clip)\n\n    # compatibility with different torch versions\n    if isinstance(grad_norm, float):\n        if np.isinf(grad_norm):\n            print(\" | > Gradient is INF !!\")\n            skip_flag = True\n    else:\n        if torch.isinf(grad_norm):\n            print(\" | > Gradient is INF !!\")\n            skip_flag = True\n    return grad_norm, skip_flag\n\n\ndef gradual_training_scheduler(global_step, config):\n    \"\"\"Setup the gradual training schedule wrt number\n    of active GPUs\"\"\"\n    num_gpus = torch.cuda.device_count()\n    if num_gpus == 0:\n        num_gpus = 1\n    new_values = None\n    # we set the scheduling wrt num_gpus\n    for values in config.gradual_training:\n        if global_step * num_gpus >= values[0]:\n            new_values = values\n    return new_values[1], new_values[2]\n", "TTS/utils/samplers.py": "import math\nimport random\nfrom typing import Callable, List, Union\n\nfrom torch.utils.data.sampler import BatchSampler, Sampler, SubsetRandomSampler\n\n\nclass SubsetSampler(Sampler):\n    \"\"\"\n    Samples elements sequentially from a given list of indices.\n\n    Args:\n        indices (list): a sequence of indices\n    \"\"\"\n\n    def __init__(self, indices):\n        super().__init__(indices)\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in range(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass PerfectBatchSampler(Sampler):\n    \"\"\"\n    Samples a mini-batch of indices for a balanced class batching\n\n    Args:\n        dataset_items(list): dataset items to sample from.\n        classes (list): list of classes of dataset_items to sample from.\n        batch_size (int): total number of samples to be sampled in a mini-batch.\n        num_gpus (int): number of GPU in the data parallel mode.\n        shuffle (bool): if True, samples randomly, otherwise samples sequentially.\n        drop_last (bool): if True, drops last incomplete batch.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_items,\n        classes,\n        batch_size,\n        num_classes_in_batch,\n        num_gpus=1,\n        shuffle=True,\n        drop_last=False,\n        label_key=\"class_name\",\n    ):\n        super().__init__(dataset_items)\n        assert (\n            batch_size % (num_classes_in_batch * num_gpus) == 0\n        ), \"Batch size must be divisible by number of classes times the number of data parallel devices (if enabled).\"\n\n        label_indices = {}\n        for idx, item in enumerate(dataset_items):\n            label = item[label_key]\n            if label not in label_indices.keys():\n                label_indices[label] = [idx]\n            else:\n                label_indices[label].append(idx)\n\n        if shuffle:\n            self._samplers = [SubsetRandomSampler(label_indices[key]) for key in classes]\n        else:\n            self._samplers = [SubsetSampler(label_indices[key]) for key in classes]\n\n        self._batch_size = batch_size\n        self._drop_last = drop_last\n        self._dp_devices = num_gpus\n        self._num_classes_in_batch = num_classes_in_batch\n\n    def __iter__(self):\n        batch = []\n        if self._num_classes_in_batch != len(self._samplers):\n            valid_samplers_idx = random.sample(range(len(self._samplers)), self._num_classes_in_batch)\n        else:\n            valid_samplers_idx = None\n\n        iters = [iter(s) for s in self._samplers]\n        done = False\n\n        while True:\n            b = []\n            for i, it in enumerate(iters):\n                if valid_samplers_idx is not None and i not in valid_samplers_idx:\n                    continue\n                idx = next(it, None)\n                if idx is None:\n                    done = True\n                    break\n                b.append(idx)\n            if done:\n                break\n            batch += b\n            if len(batch) == self._batch_size:\n                yield batch\n                batch = []\n                if valid_samplers_idx is not None:\n                    valid_samplers_idx = random.sample(range(len(self._samplers)), self._num_classes_in_batch)\n\n        if not self._drop_last:\n            if len(batch) > 0:\n                groups = len(batch) // self._num_classes_in_batch\n                if groups % self._dp_devices == 0:\n                    yield batch\n                else:\n                    batch = batch[: (groups // self._dp_devices) * self._dp_devices * self._num_classes_in_batch]\n                    if len(batch) > 0:\n                        yield batch\n\n    def __len__(self):\n        class_batch_size = self._batch_size // self._num_classes_in_batch\n        return min(((len(s) + class_batch_size - 1) // class_batch_size) for s in self._samplers)\n\n\ndef identity(x):\n    return x\n\n\nclass SortedSampler(Sampler):\n    \"\"\"Samples elements sequentially, always in the same order.\n\n    Taken from https://github.com/PetrochukM/PyTorch-NLP\n\n    Args:\n        data (iterable): Iterable data.\n        sort_key (callable): Specifies a function of one argument that is used to extract a\n            numerical comparison key from each list element.\n\n    Example:\n        >>> list(SortedSampler(range(10), sort_key=lambda i: -i))\n        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n    \"\"\"\n\n    def __init__(self, data, sort_key: Callable = identity):\n        super().__init__(data)\n        self.data = data\n        self.sort_key = sort_key\n        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n        zip_ = sorted(zip_, key=lambda r: r[1])\n        self.sorted_indexes = [item[0] for item in zip_]\n\n    def __iter__(self):\n        return iter(self.sorted_indexes)\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass BucketBatchSampler(BatchSampler):\n    \"\"\"Bucket batch sampler\n\n    Adapted from https://github.com/PetrochukM/PyTorch-NLP\n\n    Args:\n        sampler (torch.data.utils.sampler.Sampler):\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If `True` the sampler will drop the last batch if its size would be less\n            than `batch_size`.\n        data (list): List of data samples.\n        sort_key (callable, optional): Callable to specify a comparison key for sorting.\n        bucket_size_multiplier (int, optional): Buckets are of size\n            `batch_size * bucket_size_multiplier`.\n\n    Example:\n        >>> sampler = WeightedRandomSampler(weights, len(weights))\n        >>> sampler = BucketBatchSampler(sampler, data=data_items, batch_size=32, drop_last=True)\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler,\n        data,\n        batch_size,\n        drop_last,\n        sort_key: Union[Callable, List] = identity,\n        bucket_size_multiplier=100,\n    ):\n        super().__init__(sampler, batch_size, drop_last)\n        self.data = data\n        self.sort_key = sort_key\n        _bucket_size = batch_size * bucket_size_multiplier\n        if hasattr(sampler, \"__len__\"):\n            _bucket_size = min(_bucket_size, len(sampler))\n        self.bucket_sampler = BatchSampler(sampler, _bucket_size, False)\n\n    def __iter__(self):\n        for idxs in self.bucket_sampler:\n            bucket_data = [self.data[idx] for idx in idxs]\n            sorted_sampler = SortedSampler(bucket_data, self.sort_key)\n            for batch_idx in SubsetRandomSampler(list(BatchSampler(sorted_sampler, self.batch_size, self.drop_last))):\n                sorted_idxs = [idxs[i] for i in batch_idx]\n                yield sorted_idxs\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        return math.ceil(len(self.sampler) / self.batch_size)\n", "TTS/utils/radam.py": "# modified from https://github.com/LiyuanLucasLiu/RAdam\n\nimport math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if eps < 0.0:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if \"betas\" in param and (param[\"betas\"][0] != betas[0] or param[\"betas\"][1] != betas[1]):\n                    param[\"buffer\"] = [[None, None, None] for _ in range(10)]\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)]\n        )\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):  # pylint: disable=useless-super-delegation\n        super().__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                state[\"step\"] += 1\n                buffered = group[\"buffer\"][int(state[\"step\"] % 10)]\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group[\"lr\"])\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n                    p_data_fp32.add_(exp_avg, alpha=-step_size * group[\"lr\"])\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n", "TTS/utils/distribute.py": "# edited from https://github.com/fastai/imagenet-fast/blob/master/imagenet_nv/distributed.py\nimport torch\nimport torch.distributed as dist\n\n\ndef reduce_tensor(tensor, num_gpus):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= num_gpus\n    return rt\n\n\ndef init_distributed(rank, num_gpus, group_name, dist_backend, dist_url):\n    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n\n    # Initialize distributed communication\n    dist.init_process_group(dist_backend, init_method=dist_url, world_size=num_gpus, rank=rank, group_name=group_name)\n", "TTS/utils/manage.py": "import json\nimport os\nimport re\nimport tarfile\nimport zipfile\nfrom pathlib import Path\nfrom shutil import copyfile, rmtree\nfrom typing import Dict, List, Tuple\n\nimport fsspec\nimport requests\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config, read_json_with_comments\nfrom TTS.utils.generic_utils import get_user_data_dir\n\nLICENSE_URLS = {\n    \"cc by-nc-nd 4.0\": \"https://creativecommons.org/licenses/by-nc-nd/4.0/\",\n    \"mpl\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mpl2\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mpl 2.0\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mit\": \"https://choosealicense.com/licenses/mit/\",\n    \"apache 2.0\": \"https://choosealicense.com/licenses/apache-2.0/\",\n    \"apache2\": \"https://choosealicense.com/licenses/apache-2.0/\",\n    \"cc-by-sa 4.0\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n    \"cpml\": \"https://coqui.ai/cpml.txt\",\n}\n\n\nclass ModelManager(object):\n    tqdm_progress = None\n    \"\"\"Manage TTS models defined in .models.json.\n    It provides an interface to list and download\n    models defines in '.model.json'\n\n    Models are downloaded under '.TTS' folder in the user's\n    home path.\n\n    Args:\n        models_file (str): path to .model.json file. Defaults to None.\n        output_prefix (str): prefix to `tts` to download models. Defaults to None\n        progress_bar (bool): print a progress bar when donwloading a file. Defaults to False.\n        verbose (bool): print info. Defaults to True.\n    \"\"\"\n\n    def __init__(self, models_file=None, output_prefix=None, progress_bar=False, verbose=True):\n        super().__init__()\n        self.progress_bar = progress_bar\n        self.verbose = verbose\n        if output_prefix is None:\n            self.output_prefix = get_user_data_dir(\"tts\")\n        else:\n            self.output_prefix = os.path.join(output_prefix, \"tts\")\n        self.models_dict = None\n        if models_file is not None:\n            self.read_models_file(models_file)\n        else:\n            # try the default location\n            path = Path(__file__).parent / \"../.models.json\"\n            self.read_models_file(path)\n\n    def read_models_file(self, file_path):\n        \"\"\"Read .models.json as a dict\n\n        Args:\n            file_path (str): path to .models.json.\n        \"\"\"\n        self.models_dict = read_json_with_comments(file_path)\n\n    def _list_models(self, model_type, model_count=0):\n        if self.verbose:\n            print(\"\\n Name format: type/language/dataset/model\")\n        model_list = []\n        for lang in self.models_dict[model_type]:\n            for dataset in self.models_dict[model_type][lang]:\n                for model in self.models_dict[model_type][lang][dataset]:\n                    model_full_name = f\"{model_type}--{lang}--{dataset}--{model}\"\n                    output_path = os.path.join(self.output_prefix, model_full_name)\n                    if self.verbose:\n                        if os.path.exists(output_path):\n                            print(f\" {model_count}: {model_type}/{lang}/{dataset}/{model} [already downloaded]\")\n                        else:\n                            print(f\" {model_count}: {model_type}/{lang}/{dataset}/{model}\")\n                    model_list.append(f\"{model_type}/{lang}/{dataset}/{model}\")\n                    model_count += 1\n        return model_list\n\n    def _list_for_model_type(self, model_type):\n        models_name_list = []\n        model_count = 1\n        models_name_list.extend(self._list_models(model_type, model_count))\n        return models_name_list\n\n    def list_models(self):\n        models_name_list = []\n        model_count = 1\n        for model_type in self.models_dict:\n            model_list = self._list_models(model_type, model_count)\n            models_name_list.extend(model_list)\n        return models_name_list\n\n    def model_info_by_idx(self, model_query):\n        \"\"\"Print the description of the model from .models.json file using model_idx\n\n        Args:\n            model_query (str): <model_tye>/<model_idx>\n        \"\"\"\n        model_name_list = []\n        model_type, model_query_idx = model_query.split(\"/\")\n        try:\n            model_query_idx = int(model_query_idx)\n            if model_query_idx <= 0:\n                print(\"> model_query_idx should be a positive integer!\")\n                return\n        except:\n            print(\"> model_query_idx should be an integer!\")\n            return\n        model_count = 0\n        if model_type in self.models_dict:\n            for lang in self.models_dict[model_type]:\n                for dataset in self.models_dict[model_type][lang]:\n                    for model in self.models_dict[model_type][lang][dataset]:\n                        model_name_list.append(f\"{model_type}/{lang}/{dataset}/{model}\")\n                        model_count += 1\n        else:\n            print(f\"> model_type {model_type} does not exist in the list.\")\n            return\n        if model_query_idx > model_count:\n            print(f\"model query idx exceeds the number of available models [{model_count}] \")\n        else:\n            model_type, lang, dataset, model = model_name_list[model_query_idx - 1].split(\"/\")\n            print(f\"> model type : {model_type}\")\n            print(f\"> language supported : {lang}\")\n            print(f\"> dataset used : {dataset}\")\n            print(f\"> model name : {model}\")\n            if \"description\" in self.models_dict[model_type][lang][dataset][model]:\n                print(f\"> description : {self.models_dict[model_type][lang][dataset][model]['description']}\")\n            else:\n                print(\"> description : coming soon\")\n            if \"default_vocoder\" in self.models_dict[model_type][lang][dataset][model]:\n                print(f\"> default_vocoder : {self.models_dict[model_type][lang][dataset][model]['default_vocoder']}\")\n\n    def model_info_by_full_name(self, model_query_name):\n        \"\"\"Print the description of the model from .models.json file using model_full_name\n\n        Args:\n            model_query_name (str): Format is <model_type>/<language>/<dataset>/<model_name>\n        \"\"\"\n        model_type, lang, dataset, model = model_query_name.split(\"/\")\n        if model_type in self.models_dict:\n            if lang in self.models_dict[model_type]:\n                if dataset in self.models_dict[model_type][lang]:\n                    if model in self.models_dict[model_type][lang][dataset]:\n                        print(f\"> model type : {model_type}\")\n                        print(f\"> language supported : {lang}\")\n                        print(f\"> dataset used : {dataset}\")\n                        print(f\"> model name : {model}\")\n                        if \"description\" in self.models_dict[model_type][lang][dataset][model]:\n                            print(\n                                f\"> description : {self.models_dict[model_type][lang][dataset][model]['description']}\"\n                            )\n                        else:\n                            print(\"> description : coming soon\")\n                        if \"default_vocoder\" in self.models_dict[model_type][lang][dataset][model]:\n                            print(\n                                f\"> default_vocoder : {self.models_dict[model_type][lang][dataset][model]['default_vocoder']}\"\n                            )\n                    else:\n                        print(f\"> model {model} does not exist for {model_type}/{lang}/{dataset}.\")\n                else:\n                    print(f\"> dataset {dataset} does not exist for {model_type}/{lang}.\")\n            else:\n                print(f\"> lang {lang} does not exist for {model_type}.\")\n        else:\n            print(f\"> model_type {model_type} does not exist in the list.\")\n\n    def list_tts_models(self):\n        \"\"\"Print all `TTS` models and return a list of model names\n\n        Format is `language/dataset/model`\n        \"\"\"\n        return self._list_for_model_type(\"tts_models\")\n\n    def list_vocoder_models(self):\n        \"\"\"Print all the `vocoder` models and return a list of model names\n\n        Format is `language/dataset/model`\n        \"\"\"\n        return self._list_for_model_type(\"vocoder_models\")\n\n    def list_vc_models(self):\n        \"\"\"Print all the voice conversion models and return a list of model names\n\n        Format is `language/dataset/model`\n        \"\"\"\n        return self._list_for_model_type(\"voice_conversion_models\")\n\n    def list_langs(self):\n        \"\"\"Print all the available languages\"\"\"\n        print(\" Name format: type/language\")\n        for model_type in self.models_dict:\n            for lang in self.models_dict[model_type]:\n                print(f\" >: {model_type}/{lang} \")\n\n    def list_datasets(self):\n        \"\"\"Print all the datasets\"\"\"\n        print(\" Name format: type/language/dataset\")\n        for model_type in self.models_dict:\n            for lang in self.models_dict[model_type]:\n                for dataset in self.models_dict[model_type][lang]:\n                    print(f\" >: {model_type}/{lang}/{dataset}\")\n\n    @staticmethod\n    def print_model_license(model_item: Dict):\n        \"\"\"Print the license of a model\n\n        Args:\n            model_item (dict): model item in the models.json\n        \"\"\"\n        if \"license\" in model_item and model_item[\"license\"].strip() != \"\":\n            print(f\" > Model's license - {model_item['license']}\")\n            if model_item[\"license\"].lower() in LICENSE_URLS:\n                print(f\" > Check {LICENSE_URLS[model_item['license'].lower()]} for more info.\")\n            else:\n                print(\" > Check https://opensource.org/licenses for more info.\")\n        else:\n            print(\" > Model's license - No license information available\")\n\n    def _download_github_model(self, model_item: Dict, output_path: str):\n        if isinstance(model_item[\"github_rls_url\"], list):\n            self._download_model_files(model_item[\"github_rls_url\"], output_path, self.progress_bar)\n        else:\n            self._download_zip_file(model_item[\"github_rls_url\"], output_path, self.progress_bar)\n\n    def _download_hf_model(self, model_item: Dict, output_path: str):\n        if isinstance(model_item[\"hf_url\"], list):\n            self._download_model_files(model_item[\"hf_url\"], output_path, self.progress_bar)\n        else:\n            self._download_zip_file(model_item[\"hf_url\"], output_path, self.progress_bar)\n\n    def download_fairseq_model(self, model_name, output_path):\n        URI_PREFIX = \"https://coqui.gateway.scarf.sh/fairseq/\"\n        _, lang, _, _ = model_name.split(\"/\")\n        model_download_uri = os.path.join(URI_PREFIX, f\"{lang}.tar.gz\")\n        self._download_tar_file(model_download_uri, output_path, self.progress_bar)\n\n    @staticmethod\n    def set_model_url(model_item: Dict):\n        model_item[\"model_url\"] = None\n        if \"github_rls_url\" in model_item:\n            model_item[\"model_url\"] = model_item[\"github_rls_url\"]\n        elif \"hf_url\" in model_item:\n            model_item[\"model_url\"] = model_item[\"hf_url\"]\n        elif \"fairseq\" in model_item[\"model_name\"]:\n            model_item[\"model_url\"] = \"https://coqui.gateway.scarf.sh/fairseq/\"\n        elif \"xtts\" in model_item[\"model_name\"]:\n            model_item[\"model_url\"] = \"https://coqui.gateway.scarf.sh/xtts/\"\n        return model_item\n\n    def _set_model_item(self, model_name):\n        # fetch model info from the dict\n        if \"fairseq\" in model_name:\n            model_type = \"tts_models\"\n            lang = model_name.split(\"/\")[1]\n            model_item = {\n                \"model_type\": \"tts_models\",\n                \"license\": \"CC BY-NC 4.0\",\n                \"default_vocoder\": None,\n                \"author\": \"fairseq\",\n                \"description\": \"this model is released by Meta under Fairseq repo. Visit https://github.com/facebookresearch/fairseq/tree/main/examples/mms for more info.\",\n            }\n            model_item[\"model_name\"] = model_name\n        elif \"xtts\" in model_name and len(model_name.split(\"/\")) != 4:\n            # loading xtts models with only model name (e.g. xtts_v2.0.2)\n            # check model name has the version number with regex\n            version_regex = r\"v\\d+\\.\\d+\\.\\d+\"\n            if re.search(version_regex, model_name):\n                model_version = model_name.split(\"_\")[-1]\n            else:\n                model_version = \"main\"\n            model_type = \"tts_models\"\n            lang = \"multilingual\"\n            dataset = \"multi-dataset\"\n            model = model_name\n            model_item = {\n                \"default_vocoder\": None,\n                \"license\": \"CPML\",\n                \"contact\": \"info@coqui.ai\",\n                \"tos_required\": True,\n                \"hf_url\": [\n                    f\"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/{model_version}/model.pth\",\n                    f\"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/{model_version}/config.json\",\n                    f\"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/{model_version}/vocab.json\",\n                    f\"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/{model_version}/hash.md5\",\n                    f\"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/{model_version}/speakers_xtts.pth\",\n                ],\n            }\n        else:\n            # get model from models.json\n            model_type, lang, dataset, model = model_name.split(\"/\")\n            model_item = self.models_dict[model_type][lang][dataset][model]\n            model_item[\"model_type\"] = model_type\n\n        model_full_name = f\"{model_type}--{lang}--{dataset}--{model}\"\n        md5hash = model_item[\"model_hash\"] if \"model_hash\" in model_item else None\n        model_item = self.set_model_url(model_item)\n        return model_item, model_full_name, model, md5hash\n\n    @staticmethod\n    def ask_tos(model_full_path):\n        \"\"\"Ask the user to agree to the terms of service\"\"\"\n        tos_path = os.path.join(model_full_path, \"tos_agreed.txt\")\n        print(\" > You must confirm the following:\")\n        print(' | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"')\n        print(' | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]')\n        answer = input(\" | | > \")\n        if answer.lower() == \"y\":\n            with open(tos_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"I have read, understood and agreed to the Terms and Conditions.\")\n            return True\n        return False\n\n    @staticmethod\n    def tos_agreed(model_item, model_full_path):\n        \"\"\"Check if the user has agreed to the terms of service\"\"\"\n        if \"tos_required\" in model_item and model_item[\"tos_required\"]:\n            tos_path = os.path.join(model_full_path, \"tos_agreed.txt\")\n            if os.path.exists(tos_path) or os.environ.get(\"COQUI_TOS_AGREED\") == \"1\":\n                return True\n            return False\n        return True\n\n    def create_dir_and_download_model(self, model_name, model_item, output_path):\n        os.makedirs(output_path, exist_ok=True)\n        # handle TOS\n        if not self.tos_agreed(model_item, output_path):\n            if not self.ask_tos(output_path):\n                os.rmdir(output_path)\n                raise Exception(\" [!] You must agree to the terms of service to use this model.\")\n        print(f\" > Downloading model to {output_path}\")\n        try:\n            if \"fairseq\" in model_name:\n                self.download_fairseq_model(model_name, output_path)\n            elif \"github_rls_url\" in model_item:\n                self._download_github_model(model_item, output_path)\n            elif \"hf_url\" in model_item:\n                self._download_hf_model(model_item, output_path)\n\n        except requests.RequestException as e:\n            print(f\" > Failed to download the model file to {output_path}\")\n            rmtree(output_path)\n            raise e\n        self.print_model_license(model_item=model_item)\n\n    def check_if_configs_are_equal(self, model_name, model_item, output_path):\n        with fsspec.open(self._find_files(output_path)[1], \"r\", encoding=\"utf-8\") as f:\n            config_local = json.load(f)\n        remote_url = None\n        for url in model_item[\"hf_url\"]:\n            if \"config.json\" in url:\n                remote_url = url\n                break\n\n        with fsspec.open(remote_url, \"r\", encoding=\"utf-8\") as f:\n            config_remote = json.load(f)\n\n        if not config_local == config_remote:\n            print(f\" > {model_name} is already downloaded however it has been changed. Redownloading it...\")\n            self.create_dir_and_download_model(model_name, model_item, output_path)\n\n    def download_model(self, model_name):\n        \"\"\"Download model files given the full model name.\n        Model name is in the format\n            'type/language/dataset/model'\n            e.g. 'tts_model/en/ljspeech/tacotron'\n\n        Every model must have the following files:\n            - *.pth : pytorch model checkpoint file.\n            - config.json : model config file.\n            - scale_stats.npy (if exist): scale values for preprocessing.\n\n        Args:\n            model_name (str): model name as explained above.\n        \"\"\"\n        model_item, model_full_name, model, md5sum = self._set_model_item(model_name)\n        # set the model specific output path\n        output_path = os.path.join(self.output_prefix, model_full_name)\n        if os.path.exists(output_path):\n            if md5sum is not None:\n                md5sum_file = os.path.join(output_path, \"hash.md5\")\n                if os.path.isfile(md5sum_file):\n                    with open(md5sum_file, mode=\"r\") as f:\n                        if not f.read() == md5sum:\n                            print(f\" > {model_name} has been updated, clearing model cache...\")\n                            self.create_dir_and_download_model(model_name, model_item, output_path)\n                        else:\n                            print(f\" > {model_name} is already downloaded.\")\n                else:\n                    print(f\" > {model_name} has been updated, clearing model cache...\")\n                    self.create_dir_and_download_model(model_name, model_item, output_path)\n            # if the configs are different, redownload it\n            # ToDo: we need a better way to handle it\n            if \"xtts\" in model_name:\n                try:\n                    self.check_if_configs_are_equal(model_name, model_item, output_path)\n                except:\n                    pass\n            else:\n                print(f\" > {model_name} is already downloaded.\")\n        else:\n            self.create_dir_and_download_model(model_name, model_item, output_path)\n\n        # find downloaded files\n        output_model_path = output_path\n        output_config_path = None\n        if (\n            model not in [\"tortoise-v2\", \"bark\"] and \"fairseq\" not in model_name and \"xtts\" not in model_name\n        ):  # TODO:This is stupid but don't care for now.\n            output_model_path, output_config_path = self._find_files(output_path)\n        # update paths in the config.json\n        self._update_paths(output_path, output_config_path)\n        return output_model_path, output_config_path, model_item\n\n    @staticmethod\n    def _find_files(output_path: str) -> Tuple[str, str]:\n        \"\"\"Find the model and config files in the output path\n\n        Args:\n            output_path (str): path to the model files\n\n        Returns:\n            Tuple[str, str]: path to the model file and config file\n        \"\"\"\n        model_file = None\n        config_file = None\n        for file_name in os.listdir(output_path):\n            if file_name in [\"model_file.pth\", \"model_file.pth.tar\", \"model.pth\"]:\n                model_file = os.path.join(output_path, file_name)\n            elif file_name == \"config.json\":\n                config_file = os.path.join(output_path, file_name)\n        if model_file is None:\n            raise ValueError(\" [!] Model file not found in the output path\")\n        if config_file is None:\n            raise ValueError(\" [!] Config file not found in the output path\")\n        return model_file, config_file\n\n    @staticmethod\n    def _find_speaker_encoder(output_path: str) -> str:\n        \"\"\"Find the speaker encoder file in the output path\n\n        Args:\n            output_path (str): path to the model files\n\n        Returns:\n            str: path to the speaker encoder file\n        \"\"\"\n        speaker_encoder_file = None\n        for file_name in os.listdir(output_path):\n            if file_name in [\"model_se.pth\", \"model_se.pth.tar\"]:\n                speaker_encoder_file = os.path.join(output_path, file_name)\n        return speaker_encoder_file\n\n    def _update_paths(self, output_path: str, config_path: str) -> None:\n        \"\"\"Update paths for certain files in config.json after download.\n\n        Args:\n            output_path (str): local path the model is downloaded to.\n            config_path (str): local config.json path.\n        \"\"\"\n        output_stats_path = os.path.join(output_path, \"scale_stats.npy\")\n        output_d_vector_file_path = os.path.join(output_path, \"speakers.json\")\n        output_d_vector_file_pth_path = os.path.join(output_path, \"speakers.pth\")\n        output_speaker_ids_file_path = os.path.join(output_path, \"speaker_ids.json\")\n        output_speaker_ids_file_pth_path = os.path.join(output_path, \"speaker_ids.pth\")\n        speaker_encoder_config_path = os.path.join(output_path, \"config_se.json\")\n        speaker_encoder_model_path = self._find_speaker_encoder(output_path)\n\n        # update the scale_path.npy file path in the model config.json\n        self._update_path(\"audio.stats_path\", output_stats_path, config_path)\n\n        # update the speakers.json file path in the model config.json to the current path\n        self._update_path(\"d_vector_file\", output_d_vector_file_path, config_path)\n        self._update_path(\"d_vector_file\", output_d_vector_file_pth_path, config_path)\n        self._update_path(\"model_args.d_vector_file\", output_d_vector_file_path, config_path)\n        self._update_path(\"model_args.d_vector_file\", output_d_vector_file_pth_path, config_path)\n\n        # update the speaker_ids.json file path in the model config.json to the current path\n        self._update_path(\"speakers_file\", output_speaker_ids_file_path, config_path)\n        self._update_path(\"speakers_file\", output_speaker_ids_file_pth_path, config_path)\n        self._update_path(\"model_args.speakers_file\", output_speaker_ids_file_path, config_path)\n        self._update_path(\"model_args.speakers_file\", output_speaker_ids_file_pth_path, config_path)\n\n        # update the speaker_encoder file path in the model config.json to the current path\n        self._update_path(\"speaker_encoder_model_path\", speaker_encoder_model_path, config_path)\n        self._update_path(\"model_args.speaker_encoder_model_path\", speaker_encoder_model_path, config_path)\n        self._update_path(\"speaker_encoder_config_path\", speaker_encoder_config_path, config_path)\n        self._update_path(\"model_args.speaker_encoder_config_path\", speaker_encoder_config_path, config_path)\n\n    @staticmethod\n    def _update_path(field_name, new_path, config_path):\n        \"\"\"Update the path in the model config.json for the current environment after download\"\"\"\n        if new_path and os.path.exists(new_path):\n            config = load_config(config_path)\n            field_names = field_name.split(\".\")\n            if len(field_names) > 1:\n                # field name points to a sub-level field\n                sub_conf = config\n                for fd in field_names[:-1]:\n                    if fd in sub_conf:\n                        sub_conf = sub_conf[fd]\n                    else:\n                        return\n                if isinstance(sub_conf[field_names[-1]], list):\n                    sub_conf[field_names[-1]] = [new_path]\n                else:\n                    sub_conf[field_names[-1]] = new_path\n            else:\n                # field name points to a top-level field\n                if not field_name in config:\n                    return\n                if isinstance(config[field_name], list):\n                    config[field_name] = [new_path]\n                else:\n                    config[field_name] = new_path\n            config.save_json(config_path)\n\n    @staticmethod\n    def _download_zip_file(file_url, output_folder, progress_bar):\n        \"\"\"Download the github releases\"\"\"\n        # download the file\n        r = requests.get(file_url, stream=True)\n        # extract the file\n        try:\n            total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n            block_size = 1024  # 1 Kibibyte\n            if progress_bar:\n                ModelManager.tqdm_progress = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n            temp_zip_name = os.path.join(output_folder, file_url.split(\"/\")[-1])\n            with open(temp_zip_name, \"wb\") as file:\n                for data in r.iter_content(block_size):\n                    if progress_bar:\n                        ModelManager.tqdm_progress.update(len(data))\n                    file.write(data)\n            with zipfile.ZipFile(temp_zip_name) as z:\n                z.extractall(output_folder)\n            os.remove(temp_zip_name)  # delete zip after extract\n        except zipfile.BadZipFile:\n            print(f\" > Error: Bad zip file - {file_url}\")\n            raise zipfile.BadZipFile  # pylint: disable=raise-missing-from\n        # move the files to the outer path\n        for file_path in z.namelist():\n            src_path = os.path.join(output_folder, file_path)\n            if os.path.isfile(src_path):\n                dst_path = os.path.join(output_folder, os.path.basename(file_path))\n                if src_path != dst_path:\n                    copyfile(src_path, dst_path)\n        # remove redundant (hidden or not) folders\n        for file_path in z.namelist():\n            if os.path.isdir(os.path.join(output_folder, file_path)):\n                rmtree(os.path.join(output_folder, file_path))\n\n    @staticmethod\n    def _download_tar_file(file_url, output_folder, progress_bar):\n        \"\"\"Download the github releases\"\"\"\n        # download the file\n        r = requests.get(file_url, stream=True)\n        # extract the file\n        try:\n            total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n            block_size = 1024  # 1 Kibibyte\n            if progress_bar:\n                ModelManager.tqdm_progress = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n            temp_tar_name = os.path.join(output_folder, file_url.split(\"/\")[-1])\n            with open(temp_tar_name, \"wb\") as file:\n                for data in r.iter_content(block_size):\n                    if progress_bar:\n                        ModelManager.tqdm_progress.update(len(data))\n                    file.write(data)\n            with tarfile.open(temp_tar_name) as t:\n                t.extractall(output_folder)\n                tar_names = t.getnames()\n            os.remove(temp_tar_name)  # delete tar after extract\n        except tarfile.ReadError:\n            print(f\" > Error: Bad tar file - {file_url}\")\n            raise tarfile.ReadError  # pylint: disable=raise-missing-from\n        # move the files to the outer path\n        for file_path in os.listdir(os.path.join(output_folder, tar_names[0])):\n            src_path = os.path.join(output_folder, tar_names[0], file_path)\n            dst_path = os.path.join(output_folder, os.path.basename(file_path))\n            if src_path != dst_path:\n                copyfile(src_path, dst_path)\n        # remove the extracted folder\n        rmtree(os.path.join(output_folder, tar_names[0]))\n\n    @staticmethod\n    def _download_model_files(file_urls, output_folder, progress_bar):\n        \"\"\"Download the github releases\"\"\"\n        for file_url in file_urls:\n            # download the file\n            r = requests.get(file_url, stream=True)\n            # extract the file\n            bease_filename = file_url.split(\"/\")[-1]\n            temp_zip_name = os.path.join(output_folder, bease_filename)\n            total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n            block_size = 1024  # 1 Kibibyte\n            with open(temp_zip_name, \"wb\") as file:\n                if progress_bar:\n                    ModelManager.tqdm_progress = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n                for data in r.iter_content(block_size):\n                    if progress_bar:\n                        ModelManager.tqdm_progress.update(len(data))\n                    file.write(data)\n\n    @staticmethod\n    def _check_dict_key(my_dict, key):\n        if key in my_dict.keys() and my_dict[key] is not None:\n            if not isinstance(key, str):\n                return True\n            if isinstance(key, str) and len(my_dict[key]) > 0:\n                return True\n        return False\n", "TTS/utils/downloaders.py": "import os\nfrom typing import Optional\n\nfrom TTS.utils.download import download_kaggle_dataset, download_url, extract_archive\n\n\ndef download_ljspeech(path: str):\n    \"\"\"Download and extract LJSpeech dataset\n\n    Args:\n        path (str): path to the directory where the dataset will be stored.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n    download_url(url, path)\n    basename = os.path.basename(url)\n    archive = os.path.join(path, basename)\n    print(\" > Extracting archive file...\")\n    extract_archive(archive)\n\n\ndef download_vctk(path: str, use_kaggle: Optional[bool] = False):\n    \"\"\"Download and extract VCTK dataset.\n\n    Args:\n        path (str): path to the directory where the dataset will be stored.\n\n        use_kaggle (bool, optional): Downloads vctk dataset from kaggle. Is generally faster. Defaults to False.\n    \"\"\"\n    if use_kaggle:\n        download_kaggle_dataset(\"mfekadu/english-multispeaker-corpus-for-voice-cloning\", \"VCTK\", path)\n    else:\n        os.makedirs(path, exist_ok=True)\n        url = \"https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip\"\n        download_url(url, path)\n        basename = os.path.basename(url)\n        archive = os.path.join(path, basename)\n        print(\" > Extracting archive file...\")\n        extract_archive(archive)\n\n\ndef download_tweb(path: str):\n    \"\"\"Download and extract Tweb dataset\n\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n    \"\"\"\n    download_kaggle_dataset(\"bryanpark/the-world-english-bible-speech-dataset\", \"TWEB\", path)\n\n\ndef download_libri_tts(path: str, subset: Optional[str] = \"all\"):\n    \"\"\"Download and extract libri tts dataset.\n\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n\n        subset (str, optional): Name of the subset to download. If you only want to download a certain\n        portion specify it here. Defaults to 'all'.\n    \"\"\"\n\n    subset_dict = {\n        \"libri-tts-clean-100\": \"http://www.openslr.org/resources/60/train-clean-100.tar.gz\",\n        \"libri-tts-clean-360\": \"http://www.openslr.org/resources/60/train-clean-360.tar.gz\",\n        \"libri-tts-other-500\": \"http://www.openslr.org/resources/60/train-other-500.tar.gz\",\n        \"libri-tts-dev-clean\": \"http://www.openslr.org/resources/60/dev-clean.tar.gz\",\n        \"libri-tts-dev-other\": \"http://www.openslr.org/resources/60/dev-other.tar.gz\",\n        \"libri-tts-test-clean\": \"http://www.openslr.org/resources/60/test-clean.tar.gz\",\n        \"libri-tts-test-other\": \"http://www.openslr.org/resources/60/test-other.tar.gz\",\n    }\n\n    os.makedirs(path, exist_ok=True)\n    if subset == \"all\":\n        for sub, val in subset_dict.items():\n            print(f\" > Downloading {sub}...\")\n            download_url(val, path)\n            basename = os.path.basename(val)\n            archive = os.path.join(path, basename)\n            print(\" > Extracting archive file...\")\n            extract_archive(archive)\n        print(\" > All subsets downloaded\")\n    else:\n        url = subset_dict[subset]\n        download_url(url, path)\n        basename = os.path.basename(url)\n        archive = os.path.join(path, basename)\n        print(\" > Extracting archive file...\")\n        extract_archive(archive)\n\n\ndef download_thorsten_de(path: str):\n    \"\"\"Download and extract Thorsten german male voice dataset.\n\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    url = \"https://www.openslr.org/resources/95/thorsten-de_v02.tgz\"\n    download_url(url, path)\n    basename = os.path.basename(url)\n    archive = os.path.join(path, basename)\n    print(\" > Extracting archive file...\")\n    extract_archive(archive)\n\n\ndef download_mailabs(path: str, language: str = \"english\"):\n    \"\"\"Download and extract Mailabs dataset.\n\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n\n        language (str): Language subset to download. Defaults to english.\n    \"\"\"\n    language_dict = {\n        \"english\": \"https://data.solak.de/data/Training/stt_tts/en_US.tgz\",\n        \"german\": \"https://data.solak.de/data/Training/stt_tts/de_DE.tgz\",\n        \"french\": \"https://data.solak.de/data/Training/stt_tts/fr_FR.tgz\",\n        \"italian\": \"https://data.solak.de/data/Training/stt_tts/it_IT.tgz\",\n        \"spanish\": \"https://data.solak.de/data/Training/stt_tts/es_ES.tgz\",\n    }\n    os.makedirs(path, exist_ok=True)\n    url = language_dict[language]\n    download_url(url, path)\n    basename = os.path.basename(url)\n    archive = os.path.join(path, basename)\n    print(\" > Extracting archive file...\")\n    extract_archive(archive)\n", "TTS/utils/vad.py": "import torch\nimport torchaudio\n\n\ndef read_audio(path):\n    wav, sr = torchaudio.load(path)\n\n    if wav.size(0) > 1:\n        wav = wav.mean(dim=0, keepdim=True)\n\n    return wav.squeeze(0), sr\n\n\ndef resample_wav(wav, sr, new_sr):\n    wav = wav.unsqueeze(0)\n    transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=new_sr)\n    wav = transform(wav)\n    return wav.squeeze(0)\n\n\ndef map_timestamps_to_new_sr(vad_sr, new_sr, timestamps, just_begging_end=False):\n    factor = new_sr / vad_sr\n    new_timestamps = []\n    if just_begging_end and timestamps:\n        # get just the start and end timestamps\n        new_dict = {\"start\": int(timestamps[0][\"start\"] * factor), \"end\": int(timestamps[-1][\"end\"] * factor)}\n        new_timestamps.append(new_dict)\n    else:\n        for ts in timestamps:\n            # map to the new SR\n            new_dict = {\"start\": int(ts[\"start\"] * factor), \"end\": int(ts[\"end\"] * factor)}\n            new_timestamps.append(new_dict)\n\n    return new_timestamps\n\n\ndef get_vad_model_and_utils(use_cuda=False, use_onnx=False):\n    model, utils = torch.hub.load(\n        repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True, onnx=use_onnx, force_onnx_cpu=True\n    )\n    if use_cuda:\n        model = model.cuda()\n\n    get_speech_timestamps, save_audio, _, _, collect_chunks = utils\n    return model, get_speech_timestamps, save_audio, collect_chunks\n\n\ndef remove_silence(\n    model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n):\n    # get the VAD model and utils functions\n    model, get_speech_timestamps, _, collect_chunks = model_and_utils\n\n    # read ground truth wav and resample the audio for the VAD\n    try:\n        wav, gt_sample_rate = read_audio(audio_path)\n    except:\n        print(f\"> \u2757 Failed to read {audio_path}\")\n        return None, False\n\n    # if needed, resample the audio for the VAD model\n    if gt_sample_rate != vad_sample_rate:\n        wav_vad = resample_wav(wav, gt_sample_rate, vad_sample_rate)\n    else:\n        wav_vad = wav\n\n    if use_cuda:\n        wav_vad = wav_vad.cuda()\n\n    # get speech timestamps from full audio file\n    speech_timestamps = get_speech_timestamps(wav_vad, model, sampling_rate=vad_sample_rate, window_size_samples=768)\n\n    # map the current speech_timestamps to the sample rate of the ground truth audio\n    new_speech_timestamps = map_timestamps_to_new_sr(\n        vad_sample_rate, gt_sample_rate, speech_timestamps, trim_just_beginning_and_end\n    )\n\n    # if have speech timestamps else save the wav\n    if new_speech_timestamps:\n        wav = collect_chunks(new_speech_timestamps, wav)\n        is_speech = True\n    else:\n        print(f\"> The file {audio_path} probably does not have speech please check it !!\")\n        is_speech = False\n\n    # save\n    torchaudio.save(out_path, wav[None, :], gt_sample_rate)\n    return out_path, is_speech\n", "TTS/utils/callbacks.py": "class TrainerCallback:\n    @staticmethod\n    def on_init_start(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_init_start\"):\n                trainer.model.module.on_init_start(trainer)\n        else:\n            if hasattr(trainer.model, \"on_init_start\"):\n                trainer.model.on_init_start(trainer)\n\n        if hasattr(trainer.criterion, \"on_init_start\"):\n            trainer.criterion.on_init_start(trainer)\n\n        if hasattr(trainer.optimizer, \"on_init_start\"):\n            trainer.optimizer.on_init_start(trainer)\n\n    @staticmethod\n    def on_init_end(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_init_end\"):\n                trainer.model.module.on_init_end(trainer)\n        else:\n            if hasattr(trainer.model, \"on_init_end\"):\n                trainer.model.on_init_end(trainer)\n\n        if hasattr(trainer.criterion, \"on_init_end\"):\n            trainer.criterion.on_init_end(trainer)\n\n        if hasattr(trainer.optimizer, \"on_init_end\"):\n            trainer.optimizer.on_init_end(trainer)\n\n    @staticmethod\n    def on_epoch_start(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_epoch_start\"):\n                trainer.model.module.on_epoch_start(trainer)\n        else:\n            if hasattr(trainer.model, \"on_epoch_start\"):\n                trainer.model.on_epoch_start(trainer)\n\n        if hasattr(trainer.criterion, \"on_epoch_start\"):\n            trainer.criterion.on_epoch_start(trainer)\n\n        if hasattr(trainer.optimizer, \"on_epoch_start\"):\n            trainer.optimizer.on_epoch_start(trainer)\n\n    @staticmethod\n    def on_epoch_end(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_epoch_end\"):\n                trainer.model.module.on_epoch_end(trainer)\n        else:\n            if hasattr(trainer.model, \"on_epoch_end\"):\n                trainer.model.on_epoch_end(trainer)\n\n        if hasattr(trainer.criterion, \"on_epoch_end\"):\n            trainer.criterion.on_epoch_end(trainer)\n\n        if hasattr(trainer.optimizer, \"on_epoch_end\"):\n            trainer.optimizer.on_epoch_end(trainer)\n\n    @staticmethod\n    def on_train_step_start(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_train_step_start\"):\n                trainer.model.module.on_train_step_start(trainer)\n        else:\n            if hasattr(trainer.model, \"on_train_step_start\"):\n                trainer.model.on_train_step_start(trainer)\n\n        if hasattr(trainer.criterion, \"on_train_step_start\"):\n            trainer.criterion.on_train_step_start(trainer)\n\n        if hasattr(trainer.optimizer, \"on_train_step_start\"):\n            trainer.optimizer.on_train_step_start(trainer)\n\n    @staticmethod\n    def on_train_step_end(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_train_step_end\"):\n                trainer.model.module.on_train_step_end(trainer)\n        else:\n            if hasattr(trainer.model, \"on_train_step_end\"):\n                trainer.model.on_train_step_end(trainer)\n\n        if hasattr(trainer.criterion, \"on_train_step_end\"):\n            trainer.criterion.on_train_step_end(trainer)\n\n        if hasattr(trainer.optimizer, \"on_train_step_end\"):\n            trainer.optimizer.on_train_step_end(trainer)\n\n    @staticmethod\n    def on_keyboard_interrupt(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_keyboard_interrupt\"):\n                trainer.model.module.on_keyboard_interrupt(trainer)\n        else:\n            if hasattr(trainer.model, \"on_keyboard_interrupt\"):\n                trainer.model.on_keyboard_interrupt(trainer)\n\n        if hasattr(trainer.criterion, \"on_keyboard_interrupt\"):\n            trainer.criterion.on_keyboard_interrupt(trainer)\n\n        if hasattr(trainer.optimizer, \"on_keyboard_interrupt\"):\n            trainer.optimizer.on_keyboard_interrupt(trainer)\n", "TTS/utils/io.py": "import os\nimport pickle as pickle_tts\nfrom typing import Any, Callable, Dict, Union\n\nimport fsspec\nimport torch\n\nfrom TTS.utils.generic_utils import get_user_data_dir\n\n\nclass RenamingUnpickler(pickle_tts.Unpickler):\n    \"\"\"Overload default pickler to solve module renaming problem\"\"\"\n\n    def find_class(self, module, name):\n        return super().find_class(module.replace(\"mozilla_voice_tts\", \"TTS\"), name)\n\n\nclass AttrDict(dict):\n    \"\"\"A custom dict which converts dict keys\n    to class attributes\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef load_fsspec(\n    path: str,\n    map_location: Union[str, Callable, torch.device, Dict[Union[str, torch.device], Union[str, torch.device]]] = None,\n    cache: bool = True,\n    **kwargs,\n) -> Any:\n    \"\"\"Like torch.load but can load from other locations (e.g. s3:// , gs://).\n\n    Args:\n        path: Any path or url supported by fsspec.\n        map_location: torch.device or str.\n        cache: If True, cache a remote file locally for subsequent calls. It is cached under `get_user_data_dir()/tts_cache`. Defaults to True.\n        **kwargs: Keyword arguments forwarded to torch.load.\n\n    Returns:\n        Object stored in path.\n    \"\"\"\n    is_local = os.path.isdir(path) or os.path.isfile(path)\n    if cache and not is_local:\n        with fsspec.open(\n            f\"filecache::{path}\",\n            filecache={\"cache_storage\": str(get_user_data_dir(\"tts_cache\"))},\n            mode=\"rb\",\n        ) as f:\n            return torch.load(f, map_location=map_location, **kwargs)\n    else:\n        with fsspec.open(path, \"rb\") as f:\n            return torch.load(f, map_location=map_location, **kwargs)\n\n\ndef load_checkpoint(\n    model, checkpoint_path, use_cuda=False, eval=False, cache=False\n):  # pylint: disable=redefined-builtin\n    try:\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n    except ModuleNotFoundError:\n        pickle_tts.Unpickler = RenamingUnpickler\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), pickle_module=pickle_tts, cache=cache)\n    model.load_state_dict(state[\"model\"])\n    if use_cuda:\n        model.cuda()\n    if eval:\n        model.eval()\n    return model, state\n", "TTS/utils/__init__.py": "", "TTS/utils/capacitron_optimizer.py": "from typing import Generator\n\nfrom trainer.trainer_utils import get_optimizer\n\n\nclass CapacitronOptimizer:\n    \"\"\"Double optimizer class for the Capacitron model.\"\"\"\n\n    def __init__(self, config: dict, model_params: Generator) -> None:\n        self.primary_params, self.secondary_params = self.split_model_parameters(model_params)\n\n        optimizer_names = list(config.optimizer_params.keys())\n        optimizer_parameters = list(config.optimizer_params.values())\n\n        self.primary_optimizer = get_optimizer(\n            optimizer_names[0],\n            optimizer_parameters[0],\n            config.lr,\n            parameters=self.primary_params,\n        )\n\n        self.secondary_optimizer = get_optimizer(\n            optimizer_names[1],\n            self.extract_optimizer_parameters(optimizer_parameters[1]),\n            optimizer_parameters[1][\"lr\"],\n            parameters=self.secondary_params,\n        )\n\n        self.param_groups = self.primary_optimizer.param_groups\n\n    def first_step(self):\n        self.secondary_optimizer.step()\n        self.secondary_optimizer.zero_grad()\n        self.primary_optimizer.zero_grad()\n\n    def step(self):\n        # Update param groups to display the correct learning rate\n        self.param_groups = self.primary_optimizer.param_groups\n        self.primary_optimizer.step()\n\n    def zero_grad(self, set_to_none=False):\n        self.primary_optimizer.zero_grad(set_to_none)\n        self.secondary_optimizer.zero_grad(set_to_none)\n\n    def load_state_dict(self, state_dict):\n        self.primary_optimizer.load_state_dict(state_dict[0])\n        self.secondary_optimizer.load_state_dict(state_dict[1])\n\n    def state_dict(self):\n        return [self.primary_optimizer.state_dict(), self.secondary_optimizer.state_dict()]\n\n    @staticmethod\n    def split_model_parameters(model_params: Generator) -> list:\n        primary_params = []\n        secondary_params = []\n        for name, param in model_params:\n            if param.requires_grad:\n                if name == \"capacitron_vae_layer.beta\":\n                    secondary_params.append(param)\n                else:\n                    primary_params.append(param)\n        return [iter(primary_params), iter(secondary_params)]\n\n    @staticmethod\n    def extract_optimizer_parameters(params: dict) -> dict:\n        \"\"\"Extract parameters that are not the learning rate\"\"\"\n        return {k: v for k, v in params.items() if k != \"lr\"}\n", "TTS/utils/generic_utils.py": "# -*- coding: utf-8 -*-\nimport datetime\nimport importlib\nimport logging\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Dict\n\nimport fsspec\nimport torch\n\n\ndef to_cuda(x: torch.Tensor) -> torch.Tensor:\n    if x is None:\n        return None\n    if torch.is_tensor(x):\n        x = x.contiguous()\n        if torch.cuda.is_available():\n            x = x.cuda(non_blocking=True)\n    return x\n\n\ndef get_cuda():\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    return use_cuda, device\n\n\ndef get_git_branch():\n    try:\n        out = subprocess.check_output([\"git\", \"branch\"]).decode(\"utf8\")\n        current = next(line for line in out.split(\"\\n\") if line.startswith(\"*\"))\n        current.replace(\"* \", \"\")\n    except subprocess.CalledProcessError:\n        current = \"inside_docker\"\n    except (FileNotFoundError, StopIteration) as e:\n        current = \"unknown\"\n    return current\n\n\ndef get_commit_hash():\n    \"\"\"https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script\"\"\"\n    # try:\n    #     subprocess.check_output(['git', 'diff-index', '--quiet',\n    #                              'HEAD'])  # Verify client is clean\n    # except:\n    #     raise RuntimeError(\n    #         \" !! Commit before training to get the commit hash.\")\n    try:\n        commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n    # Not copying .git folder into docker container\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        commit = \"0000000\"\n    return commit\n\n\ndef get_experiment_folder_path(root_path, model_name):\n    \"\"\"Get an experiment folder path with the current date and time\"\"\"\n    date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")\n    commit_hash = get_commit_hash()\n    output_folder = os.path.join(root_path, model_name + \"-\" + date_str + \"-\" + commit_hash)\n    return output_folder\n\n\ndef remove_experiment_folder(experiment_path):\n    \"\"\"Check folder if there is a checkpoint, otherwise remove the folder\"\"\"\n    fs = fsspec.get_mapper(experiment_path).fs\n    checkpoint_files = fs.glob(experiment_path + \"/*.pth\")\n    if not checkpoint_files:\n        if fs.exists(experiment_path):\n            fs.rm(experiment_path, recursive=True)\n            print(\" ! Run is removed from {}\".format(experiment_path))\n    else:\n        print(\" ! Run is kept in {}\".format(experiment_path))\n\n\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef to_camel(text):\n    text = text.capitalize()\n    text = re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\n    text = text.replace(\"Tts\", \"TTS\")\n    text = text.replace(\"vc\", \"VC\")\n    return text\n\n\ndef find_module(module_path: str, module_name: str) -> object:\n    module_name = module_name.lower()\n    module = importlib.import_module(module_path + \".\" + module_name)\n    class_name = to_camel(module_name)\n    return getattr(module, class_name)\n\n\ndef import_class(module_path: str) -> object:\n    \"\"\"Import a class from a module path.\n\n    Args:\n        module_path (str): The module path of the class.\n\n    Returns:\n        object: The imported class.\n    \"\"\"\n    class_name = module_path.split(\".\")[-1]\n    module_path = \".\".join(module_path.split(\".\")[:-1])\n    module = importlib.import_module(module_path)\n    return getattr(module, class_name)\n\n\ndef get_import_path(obj: object) -> str:\n    \"\"\"Get the import path of a class.\n\n    Args:\n        obj (object): The class object.\n\n    Returns:\n        str: The import path of the class.\n    \"\"\"\n    return \".\".join([type(obj).__module__, type(obj).__name__])\n\n\ndef get_user_data_dir(appname):\n    TTS_HOME = os.environ.get(\"TTS_HOME\")\n    XDG_DATA_HOME = os.environ.get(\"XDG_DATA_HOME\")\n    if TTS_HOME is not None:\n        ans = Path(TTS_HOME).expanduser().resolve(strict=False)\n    elif XDG_DATA_HOME is not None:\n        ans = Path(XDG_DATA_HOME).expanduser().resolve(strict=False)\n    elif sys.platform == \"win32\":\n        import winreg  # pylint: disable=import-outside-toplevel\n\n        key = winreg.OpenKey(\n            winreg.HKEY_CURRENT_USER, r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n        )\n        dir_, _ = winreg.QueryValueEx(key, \"Local AppData\")\n        ans = Path(dir_).resolve(strict=False)\n    elif sys.platform == \"darwin\":\n        ans = Path(\"~/Library/Application Support/\").expanduser()\n    else:\n        ans = Path.home().joinpath(\".local/share\")\n    return ans.joinpath(appname)\n\n\ndef set_init_dict(model_dict, checkpoint_state, c):\n    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n    for k, v in checkpoint_state.items():\n        if k not in model_dict:\n            print(\" | > Layer missing in the model definition: {}\".format(k))\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in checkpoint_state.items() if k in model_dict}\n    # 2. filter out different size layers\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if v.numel() == model_dict[k].numel()}\n    # 3. skip reinit layers\n    if c.has(\"reinit_layers\") and c.reinit_layers is not None:\n        for reinit_layer_name in c.reinit_layers:\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if reinit_layer_name not in k}\n    # 4. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    print(\" | > {} / {} layers are restored.\".format(len(pretrained_dict), len(model_dict)))\n    return model_dict\n\n\ndef format_aux_input(def_args: Dict, kwargs: Dict) -> Dict:\n    \"\"\"Format kwargs to hande auxilary inputs to models.\n\n    Args:\n        def_args (Dict): A dictionary of argument names and their default values if not defined in `kwargs`.\n        kwargs (Dict): A `dict` or `kwargs` that includes auxilary inputs to the model.\n\n    Returns:\n        Dict: arguments with formatted auxilary inputs.\n    \"\"\"\n    kwargs = kwargs.copy()\n    for name in def_args:\n        if name not in kwargs or kwargs[name] is None:\n            kwargs[name] = def_args[name]\n    return kwargs\n\n\nclass KeepAverage:\n    def __init__(self):\n        self.avg_values = {}\n        self.iters = {}\n\n    def __getitem__(self, key):\n        return self.avg_values[key]\n\n    def items(self):\n        return self.avg_values.items()\n\n    def add_value(self, name, init_val=0, init_iter=0):\n        self.avg_values[name] = init_val\n        self.iters[name] = init_iter\n\n    def update_value(self, name, value, weighted_avg=False):\n        if name not in self.avg_values:\n            # add value if not exist before\n            self.add_value(name, init_val=value)\n        else:\n            # else update existing value\n            if weighted_avg:\n                self.avg_values[name] = 0.99 * self.avg_values[name] + 0.01 * value\n                self.iters[name] += 1\n            else:\n                self.avg_values[name] = self.avg_values[name] * self.iters[name] + value\n                self.iters[name] += 1\n                self.avg_values[name] /= self.iters[name]\n\n    def add_values(self, name_dict):\n        for key, value in name_dict.items():\n            self.add_value(key, init_val=value)\n\n    def update_values(self, value_dict):\n        for key, value in value_dict.items():\n            self.update_value(key, value)\n\n\ndef get_timestamp():\n    return datetime.now().strftime(\"%y%m%d-%H%M%S\")\n\n\ndef setup_logger(logger_name, root, phase, level=logging.INFO, screen=False, tofile=False):\n    lg = logging.getLogger(logger_name)\n    formatter = logging.Formatter(\"%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s\", datefmt=\"%y-%m-%d %H:%M:%S\")\n    lg.setLevel(level)\n    if tofile:\n        log_file = os.path.join(root, phase + \"_{}.log\".format(get_timestamp()))\n        fh = logging.FileHandler(log_file, mode=\"w\")\n        fh.setFormatter(formatter)\n        lg.addHandler(fh)\n    if screen:\n        sh = logging.StreamHandler()\n        sh.setFormatter(formatter)\n        lg.addHandler(sh)\n", "TTS/utils/synthesizer.py": "import os\nimport time\nfrom typing import List\n\nimport numpy as np\nimport pysbd\nimport torch\nfrom torch import nn\n\nfrom TTS.config import load_config\nfrom TTS.tts.configs.vits_config import VitsConfig\nfrom TTS.tts.models import setup_model as setup_tts_model\nfrom TTS.tts.models.vits import Vits\n\n# pylint: disable=unused-wildcard-import\n# pylint: disable=wildcard-import\nfrom TTS.tts.utils.synthesis import synthesis, transfer_voice, trim_silence\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.audio.numpy_transforms import save_wav\nfrom TTS.vc.models import setup_model as setup_vc_model\nfrom TTS.vocoder.models import setup_model as setup_vocoder_model\nfrom TTS.vocoder.utils.generic_utils import interpolate_vocoder_input\n\n\nclass Synthesizer(nn.Module):\n    def __init__(\n        self,\n        tts_checkpoint: str = \"\",\n        tts_config_path: str = \"\",\n        tts_speakers_file: str = \"\",\n        tts_languages_file: str = \"\",\n        vocoder_checkpoint: str = \"\",\n        vocoder_config: str = \"\",\n        encoder_checkpoint: str = \"\",\n        encoder_config: str = \"\",\n        vc_checkpoint: str = \"\",\n        vc_config: str = \"\",\n        model_dir: str = \"\",\n        voice_dir: str = None,\n        use_cuda: bool = False,\n    ) -> None:\n        \"\"\"General \ud83d\udc38 TTS interface for inference. It takes a tts and a vocoder\n        model and synthesize speech from the provided text.\n\n        The text is divided into a list of sentences using `pysbd` and synthesize\n        speech on each sentence separately.\n\n        If you have certain special characters in your text, you need to handle\n        them before providing the text to Synthesizer.\n\n        TODO: set the segmenter based on the source language\n\n        Args:\n            tts_checkpoint (str, optional): path to the tts model file.\n            tts_config_path (str, optional): path to the tts config file.\n            vocoder_checkpoint (str, optional): path to the vocoder model file. Defaults to None.\n            vocoder_config (str, optional): path to the vocoder config file. Defaults to None.\n            encoder_checkpoint (str, optional): path to the speaker encoder model file. Defaults to `\"\"`,\n            encoder_config (str, optional): path to the speaker encoder config file. Defaults to `\"\"`,\n            vc_checkpoint (str, optional): path to the voice conversion model file. Defaults to `\"\"`,\n            vc_config (str, optional): path to the voice conversion config file. Defaults to `\"\"`,\n            use_cuda (bool, optional): enable/disable cuda. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.tts_checkpoint = tts_checkpoint\n        self.tts_config_path = tts_config_path\n        self.tts_speakers_file = tts_speakers_file\n        self.tts_languages_file = tts_languages_file\n        self.vocoder_checkpoint = vocoder_checkpoint\n        self.vocoder_config = vocoder_config\n        self.encoder_checkpoint = encoder_checkpoint\n        self.encoder_config = encoder_config\n        self.vc_checkpoint = vc_checkpoint\n        self.vc_config = vc_config\n        self.use_cuda = use_cuda\n\n        self.tts_model = None\n        self.vocoder_model = None\n        self.vc_model = None\n        self.speaker_manager = None\n        self.tts_speakers = {}\n        self.language_manager = None\n        self.num_languages = 0\n        self.tts_languages = {}\n        self.d_vector_dim = 0\n        self.seg = self._get_segmenter(\"en\")\n        self.use_cuda = use_cuda\n        self.voice_dir = voice_dir\n        if self.use_cuda:\n            assert torch.cuda.is_available(), \"CUDA is not availabe on this machine.\"\n\n        if tts_checkpoint:\n            self._load_tts(tts_checkpoint, tts_config_path, use_cuda)\n            self.output_sample_rate = self.tts_config.audio[\"sample_rate\"]\n\n        if vocoder_checkpoint:\n            self._load_vocoder(vocoder_checkpoint, vocoder_config, use_cuda)\n            self.output_sample_rate = self.vocoder_config.audio[\"sample_rate\"]\n\n        if vc_checkpoint:\n            self._load_vc(vc_checkpoint, vc_config, use_cuda)\n            self.output_sample_rate = self.vc_config.audio[\"output_sample_rate\"]\n\n        if model_dir:\n            if \"fairseq\" in model_dir:\n                self._load_fairseq_from_dir(model_dir, use_cuda)\n                self.output_sample_rate = self.tts_config.audio[\"sample_rate\"]\n            else:\n                self._load_tts_from_dir(model_dir, use_cuda)\n                self.output_sample_rate = self.tts_config.audio[\"output_sample_rate\"]\n\n    @staticmethod\n    def _get_segmenter(lang: str):\n        \"\"\"get the sentence segmenter for the given language.\n\n        Args:\n            lang (str): target language code.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        return pysbd.Segmenter(language=lang, clean=True)\n\n    def _load_vc(self, vc_checkpoint: str, vc_config_path: str, use_cuda: bool) -> None:\n        \"\"\"Load the voice conversion model.\n\n        1. Load the model config.\n        2. Init the model from the config.\n        3. Load the model weights.\n        4. Move the model to the GPU if CUDA is enabled.\n\n        Args:\n            vc_checkpoint (str): path to the model checkpoint.\n            tts_config_path (str): path to the model config file.\n            use_cuda (bool): enable/disable CUDA use.\n        \"\"\"\n        # pylint: disable=global-statement\n        self.vc_config = load_config(vc_config_path)\n        self.vc_model = setup_vc_model(config=self.vc_config)\n        self.vc_model.load_checkpoint(self.vc_config, vc_checkpoint)\n        if use_cuda:\n            self.vc_model.cuda()\n\n    def _load_fairseq_from_dir(self, model_dir: str, use_cuda: bool) -> None:\n        \"\"\"Load the fairseq model from a directory.\n\n        We assume it is VITS and the model knows how to load itself from the directory and there is a config.json file in the directory.\n        \"\"\"\n        self.tts_config = VitsConfig()\n        self.tts_model = Vits.init_from_config(self.tts_config)\n        self.tts_model.load_fairseq_checkpoint(self.tts_config, checkpoint_dir=model_dir, eval=True)\n        self.tts_config = self.tts_model.config\n        if use_cuda:\n            self.tts_model.cuda()\n\n    def _load_tts_from_dir(self, model_dir: str, use_cuda: bool) -> None:\n        \"\"\"Load the TTS model from a directory.\n\n        We assume the model knows how to load itself from the directory and there is a config.json file in the directory.\n        \"\"\"\n        config = load_config(os.path.join(model_dir, \"config.json\"))\n        self.tts_config = config\n        self.tts_model = setup_tts_model(config)\n        self.tts_model.load_checkpoint(config, checkpoint_dir=model_dir, eval=True)\n        if use_cuda:\n            self.tts_model.cuda()\n\n    def _load_tts(self, tts_checkpoint: str, tts_config_path: str, use_cuda: bool) -> None:\n        \"\"\"Load the TTS model.\n\n        1. Load the model config.\n        2. Init the model from the config.\n        3. Load the model weights.\n        4. Move the model to the GPU if CUDA is enabled.\n        5. Init the speaker manager in the model.\n\n        Args:\n            tts_checkpoint (str): path to the model checkpoint.\n            tts_config_path (str): path to the model config file.\n            use_cuda (bool): enable/disable CUDA use.\n        \"\"\"\n        # pylint: disable=global-statement\n        self.tts_config = load_config(tts_config_path)\n        if self.tts_config[\"use_phonemes\"] and self.tts_config[\"phonemizer\"] is None:\n            raise ValueError(\"Phonemizer is not defined in the TTS config.\")\n\n        self.tts_model = setup_tts_model(config=self.tts_config)\n\n        if not self.encoder_checkpoint:\n            self._set_speaker_encoder_paths_from_tts_config()\n\n        self.tts_model.load_checkpoint(self.tts_config, tts_checkpoint, eval=True)\n        if use_cuda:\n            self.tts_model.cuda()\n\n        if self.encoder_checkpoint and hasattr(self.tts_model, \"speaker_manager\"):\n            self.tts_model.speaker_manager.init_encoder(self.encoder_checkpoint, self.encoder_config, use_cuda)\n\n    def _set_speaker_encoder_paths_from_tts_config(self):\n        \"\"\"Set the encoder paths from the tts model config for models with speaker encoders.\"\"\"\n        if hasattr(self.tts_config, \"model_args\") and hasattr(\n            self.tts_config.model_args, \"speaker_encoder_config_path\"\n        ):\n            self.encoder_checkpoint = self.tts_config.model_args.speaker_encoder_model_path\n            self.encoder_config = self.tts_config.model_args.speaker_encoder_config_path\n\n    def _load_vocoder(self, model_file: str, model_config: str, use_cuda: bool) -> None:\n        \"\"\"Load the vocoder model.\n\n        1. Load the vocoder config.\n        2. Init the AudioProcessor for the vocoder.\n        3. Init the vocoder model from the config.\n        4. Move the model to the GPU if CUDA is enabled.\n\n        Args:\n            model_file (str): path to the model checkpoint.\n            model_config (str): path to the model config file.\n            use_cuda (bool): enable/disable CUDA use.\n        \"\"\"\n        self.vocoder_config = load_config(model_config)\n        self.vocoder_ap = AudioProcessor(verbose=False, **self.vocoder_config.audio)\n        self.vocoder_model = setup_vocoder_model(self.vocoder_config)\n        self.vocoder_model.load_checkpoint(self.vocoder_config, model_file, eval=True)\n        if use_cuda:\n            self.vocoder_model.cuda()\n\n    def split_into_sentences(self, text) -> List[str]:\n        \"\"\"Split give text into sentences.\n\n        Args:\n            text (str): input text in string format.\n\n        Returns:\n            List[str]: list of sentences.\n        \"\"\"\n        return self.seg.segment(text)\n\n    def save_wav(self, wav: List[int], path: str, pipe_out=None) -> None:\n        \"\"\"Save the waveform as a file.\n\n        Args:\n            wav (List[int]): waveform as a list of values.\n            path (str): output path to save the waveform.\n            pipe_out (BytesIO, optional): Flag to stdout the generated TTS wav file for shell pipe.\n        \"\"\"\n        # if tensor convert to numpy\n        if torch.is_tensor(wav):\n            wav = wav.cpu().numpy()\n        if isinstance(wav, list):\n            wav = np.array(wav)\n        save_wav(wav=wav, path=path, sample_rate=self.output_sample_rate, pipe_out=pipe_out)\n\n    def voice_conversion(self, source_wav: str, target_wav: str) -> List[int]:\n        output_wav = self.vc_model.voice_conversion(source_wav, target_wav)\n        return output_wav\n\n    def tts(\n        self,\n        text: str = \"\",\n        speaker_name: str = \"\",\n        language_name: str = \"\",\n        speaker_wav=None,\n        style_wav=None,\n        style_text=None,\n        reference_wav=None,\n        reference_speaker_name=None,\n        split_sentences: bool = True,\n        **kwargs,\n    ) -> List[int]:\n        \"\"\"\ud83d\udc38 TTS magic. Run all the models and generate speech.\n\n        Args:\n            text (str): input text.\n            speaker_name (str, optional): speaker id for multi-speaker models. Defaults to \"\".\n            language_name (str, optional): language id for multi-language models. Defaults to \"\".\n            speaker_wav (Union[str, List[str]], optional): path to the speaker wav for voice cloning. Defaults to None.\n            style_wav ([type], optional): style waveform for GST. Defaults to None.\n            style_text ([type], optional): transcription of style_wav for Capacitron. Defaults to None.\n            reference_wav ([type], optional): reference waveform for voice conversion. Defaults to None.\n            reference_speaker_name ([type], optional): speaker id of reference waveform. Defaults to None.\n            split_sentences (bool, optional): split the input text into sentences. Defaults to True.\n            **kwargs: additional arguments to pass to the TTS model.\n        Returns:\n            List[int]: [description]\n        \"\"\"\n        start_time = time.time()\n        wavs = []\n\n        if not text and not reference_wav:\n            raise ValueError(\n                \"You need to define either `text` (for sythesis) or a `reference_wav` (for voice conversion) to use the Coqui TTS API.\"\n            )\n\n        if text:\n            sens = [text]\n            if split_sentences:\n                print(\" > Text splitted to sentences.\")\n                sens = self.split_into_sentences(text)\n            print(sens)\n\n        # handle multi-speaker\n        if \"voice_dir\" in kwargs:\n            self.voice_dir = kwargs[\"voice_dir\"]\n            kwargs.pop(\"voice_dir\")\n        speaker_embedding = None\n        speaker_id = None\n        if self.tts_speakers_file or hasattr(self.tts_model.speaker_manager, \"name_to_id\"):\n            if speaker_name and isinstance(speaker_name, str) and not self.tts_config.model == \"xtts\":\n                if self.tts_config.use_d_vector_file:\n                    # get the average speaker embedding from the saved d_vectors.\n                    speaker_embedding = self.tts_model.speaker_manager.get_mean_embedding(\n                        speaker_name, num_samples=None, randomize=False\n                    )\n                    speaker_embedding = np.array(speaker_embedding)[None, :]  # [1 x embedding_dim]\n                else:\n                    # get speaker idx from the speaker name\n                    speaker_id = self.tts_model.speaker_manager.name_to_id[speaker_name]\n            # handle Neon models with single speaker.\n            elif len(self.tts_model.speaker_manager.name_to_id) == 1:\n                speaker_id = list(self.tts_model.speaker_manager.name_to_id.values())[0]\n            elif not speaker_name and not speaker_wav:\n                raise ValueError(\n                    \" [!] Looks like you are using a multi-speaker model. \"\n                    \"You need to define either a `speaker_idx` or a `speaker_wav` to use a multi-speaker model.\"\n                )\n            else:\n                speaker_embedding = None\n        else:\n            if speaker_name and self.voice_dir is None:\n                raise ValueError(\n                    f\" [!] Missing speakers.json file path for selecting speaker {speaker_name}.\"\n                    \"Define path for speaker.json if it is a multi-speaker model or remove defined speaker idx. \"\n                )\n\n        # handle multi-lingual\n        language_id = None\n        if self.tts_languages_file or (\n            hasattr(self.tts_model, \"language_manager\") \n            and self.tts_model.language_manager is not None\n            and not self.tts_config.model == \"xtts\"\n        ):\n            if len(self.tts_model.language_manager.name_to_id) == 1:\n                language_id = list(self.tts_model.language_manager.name_to_id.values())[0]\n\n            elif language_name and isinstance(language_name, str):\n                try:\n                    language_id = self.tts_model.language_manager.name_to_id[language_name]\n                except KeyError as e:\n                    raise ValueError(\n                        f\" [!] Looks like you use a multi-lingual model. \"\n                        f\"Language {language_name} is not in the available languages: \"\n                        f\"{self.tts_model.language_manager.name_to_id.keys()}.\"\n                    ) from e\n\n            elif not language_name:\n                raise ValueError(\n                    \" [!] Look like you use a multi-lingual model. \"\n                    \"You need to define either a `language_name` or a `style_wav` to use a multi-lingual model.\"\n                )\n\n            else:\n                raise ValueError(\n                    f\" [!] Missing language_ids.json file path for selecting language {language_name}.\"\n                    \"Define path for language_ids.json if it is a multi-lingual model or remove defined language idx. \"\n                )\n\n        # compute a new d_vector from the given clip.\n        if (\n            speaker_wav is not None\n            and self.tts_model.speaker_manager is not None\n            and hasattr(self.tts_model.speaker_manager, \"encoder_ap\")\n            and self.tts_model.speaker_manager.encoder_ap is not None\n        ):\n            speaker_embedding = self.tts_model.speaker_manager.compute_embedding_from_clip(speaker_wav)\n\n        vocoder_device = \"cpu\"\n        use_gl = self.vocoder_model is None\n        if not use_gl:\n            vocoder_device = next(self.vocoder_model.parameters()).device\n        if self.use_cuda:\n            vocoder_device = \"cuda\"\n\n        if not reference_wav:  # not voice conversion\n            for sen in sens:\n                if hasattr(self.tts_model, \"synthesize\"):\n                    outputs = self.tts_model.synthesize(\n                        text=sen,\n                        config=self.tts_config,\n                        speaker_id=speaker_name,\n                        voice_dirs=self.voice_dir,\n                        d_vector=speaker_embedding,\n                        speaker_wav=speaker_wav,\n                        language=language_name,\n                        **kwargs,\n                    )\n                else:\n                    # synthesize voice\n                    outputs = synthesis(\n                        model=self.tts_model,\n                        text=sen,\n                        CONFIG=self.tts_config,\n                        use_cuda=self.use_cuda,\n                        speaker_id=speaker_id,\n                        style_wav=style_wav,\n                        style_text=style_text,\n                        use_griffin_lim=use_gl,\n                        d_vector=speaker_embedding,\n                        language_id=language_id,\n                    )\n                waveform = outputs[\"wav\"]\n                if not use_gl:\n                    mel_postnet_spec = outputs[\"outputs\"][\"model_outputs\"][0].detach().cpu().numpy()\n                    # denormalize tts output based on tts audio config\n                    mel_postnet_spec = self.tts_model.ap.denormalize(mel_postnet_spec.T).T\n                    # renormalize spectrogram based on vocoder config\n                    vocoder_input = self.vocoder_ap.normalize(mel_postnet_spec.T)\n                    # compute scale factor for possible sample rate mismatch\n                    scale_factor = [\n                        1,\n                        self.vocoder_config[\"audio\"][\"sample_rate\"] / self.tts_model.ap.sample_rate,\n                    ]\n                    if scale_factor[1] != 1:\n                        print(\" > interpolating tts model output.\")\n                        vocoder_input = interpolate_vocoder_input(scale_factor, vocoder_input)\n                    else:\n                        vocoder_input = torch.tensor(vocoder_input).unsqueeze(0)  # pylint: disable=not-callable\n                    # run vocoder model\n                    # [1, T, C]\n                    waveform = self.vocoder_model.inference(vocoder_input.to(vocoder_device))\n                if torch.is_tensor(waveform) and waveform.device != torch.device(\"cpu\") and not use_gl:\n                    waveform = waveform.cpu()\n                if not use_gl:\n                    waveform = waveform.numpy()\n                waveform = waveform.squeeze()\n\n                # trim silence\n                if \"do_trim_silence\" in self.tts_config.audio and self.tts_config.audio[\"do_trim_silence\"]:\n                    waveform = trim_silence(waveform, self.tts_model.ap)\n\n                wavs += list(waveform)\n                wavs += [0] * 10000\n        else:\n            # get the speaker embedding or speaker id for the reference wav file\n            reference_speaker_embedding = None\n            reference_speaker_id = None\n            if self.tts_speakers_file or hasattr(self.tts_model.speaker_manager, \"name_to_id\"):\n                if reference_speaker_name and isinstance(reference_speaker_name, str):\n                    if self.tts_config.use_d_vector_file:\n                        # get the speaker embedding from the saved d_vectors.\n                        reference_speaker_embedding = self.tts_model.speaker_manager.get_embeddings_by_name(\n                            reference_speaker_name\n                        )[0]\n                        reference_speaker_embedding = np.array(reference_speaker_embedding)[\n                            None, :\n                        ]  # [1 x embedding_dim]\n                    else:\n                        # get speaker idx from the speaker name\n                        reference_speaker_id = self.tts_model.speaker_manager.name_to_id[reference_speaker_name]\n                else:\n                    reference_speaker_embedding = self.tts_model.speaker_manager.compute_embedding_from_clip(\n                        reference_wav\n                    )\n            outputs = transfer_voice(\n                model=self.tts_model,\n                CONFIG=self.tts_config,\n                use_cuda=self.use_cuda,\n                reference_wav=reference_wav,\n                speaker_id=speaker_id,\n                d_vector=speaker_embedding,\n                use_griffin_lim=use_gl,\n                reference_speaker_id=reference_speaker_id,\n                reference_d_vector=reference_speaker_embedding,\n            )\n            waveform = outputs\n            if not use_gl:\n                mel_postnet_spec = outputs[0].detach().cpu().numpy()\n                # denormalize tts output based on tts audio config\n                mel_postnet_spec = self.tts_model.ap.denormalize(mel_postnet_spec.T).T\n                # renormalize spectrogram based on vocoder config\n                vocoder_input = self.vocoder_ap.normalize(mel_postnet_spec.T)\n                # compute scale factor for possible sample rate mismatch\n                scale_factor = [\n                    1,\n                    self.vocoder_config[\"audio\"][\"sample_rate\"] / self.tts_model.ap.sample_rate,\n                ]\n                if scale_factor[1] != 1:\n                    print(\" > interpolating tts model output.\")\n                    vocoder_input = interpolate_vocoder_input(scale_factor, vocoder_input)\n                else:\n                    vocoder_input = torch.tensor(vocoder_input).unsqueeze(0)  # pylint: disable=not-callable\n                # run vocoder model\n                # [1, T, C]\n                waveform = self.vocoder_model.inference(vocoder_input.to(vocoder_device))\n            if torch.is_tensor(waveform) and waveform.device != torch.device(\"cpu\"):\n                waveform = waveform.cpu()\n            if not use_gl:\n                waveform = waveform.numpy()\n            wavs = waveform.squeeze()\n\n        # compute stats\n        process_time = time.time() - start_time\n        audio_time = len(wavs) / self.tts_config.audio[\"sample_rate\"]\n        print(f\" > Processing time: {process_time}\")\n        print(f\" > Real-time factor: {process_time / audio_time}\")\n        return wavs\n", "TTS/utils/download.py": "# Adapted from https://github.com/pytorch/audio/\n\nimport hashlib\nimport logging\nimport os\nimport tarfile\nimport urllib\nimport urllib.request\nimport zipfile\nfrom os.path import expanduser\nfrom typing import Any, Iterable, List, Optional\n\nfrom torch.utils.model_zoo import tqdm\n\n\ndef stream_url(\n    url: str, start_byte: Optional[int] = None, block_size: int = 32 * 1024, progress_bar: bool = True\n) -> Iterable:\n    \"\"\"Stream url by chunk\n\n    Args:\n        url (str): Url.\n        start_byte (int or None, optional): Start streaming at that point (Default: ``None``).\n        block_size (int, optional): Size of chunks to stream (Default: ``32 * 1024``).\n        progress_bar (bool, optional): Display a progress bar (Default: ``True``).\n    \"\"\"\n\n    # If we already have the whole file, there is no need to download it again\n    req = urllib.request.Request(url, method=\"HEAD\")\n    with urllib.request.urlopen(req) as response:\n        url_size = int(response.info().get(\"Content-Length\", -1))\n    if url_size == start_byte:\n        return\n\n    req = urllib.request.Request(url)\n    if start_byte:\n        req.headers[\"Range\"] = \"bytes={}-\".format(start_byte)\n\n    with urllib.request.urlopen(req) as upointer, tqdm(\n        unit=\"B\",\n        unit_scale=True,\n        unit_divisor=1024,\n        total=url_size,\n        disable=not progress_bar,\n    ) as pbar:\n        num_bytes = 0\n        while True:\n            chunk = upointer.read(block_size)\n            if not chunk:\n                break\n            yield chunk\n            num_bytes += len(chunk)\n            pbar.update(len(chunk))\n\n\ndef download_url(\n    url: str,\n    download_folder: str,\n    filename: Optional[str] = None,\n    hash_value: Optional[str] = None,\n    hash_type: str = \"sha256\",\n    progress_bar: bool = True,\n    resume: bool = False,\n) -> None:\n    \"\"\"Download file to disk.\n\n    Args:\n        url (str): Url.\n        download_folder (str): Folder to download file.\n        filename (str or None, optional): Name of downloaded file. If None, it is inferred from the url\n            (Default: ``None``).\n        hash_value (str or None, optional): Hash for url (Default: ``None``).\n        hash_type (str, optional): Hash type, among \"sha256\" and \"md5\" (Default: ``\"sha256\"``).\n        progress_bar (bool, optional): Display a progress bar (Default: ``True``).\n        resume (bool, optional): Enable resuming download (Default: ``False``).\n    \"\"\"\n\n    req = urllib.request.Request(url, method=\"HEAD\")\n    req_info = urllib.request.urlopen(req).info()  # pylint: disable=consider-using-with\n\n    # Detect filename\n    filename = filename or req_info.get_filename() or os.path.basename(url)\n    filepath = os.path.join(download_folder, filename)\n    if resume and os.path.exists(filepath):\n        mode = \"ab\"\n        local_size: Optional[int] = os.path.getsize(filepath)\n\n    elif not resume and os.path.exists(filepath):\n        raise RuntimeError(\"{} already exists. Delete the file manually and retry.\".format(filepath))\n    else:\n        mode = \"wb\"\n        local_size = None\n\n    if hash_value and local_size == int(req_info.get(\"Content-Length\", -1)):\n        with open(filepath, \"rb\") as file_obj:\n            if validate_file(file_obj, hash_value, hash_type):\n                return\n        raise RuntimeError(\"The hash of {} does not match. Delete the file manually and retry.\".format(filepath))\n\n    with open(filepath, mode) as fpointer:\n        for chunk in stream_url(url, start_byte=local_size, progress_bar=progress_bar):\n            fpointer.write(chunk)\n\n    with open(filepath, \"rb\") as file_obj:\n        if hash_value and not validate_file(file_obj, hash_value, hash_type):\n            raise RuntimeError(\"The hash of {} does not match. Delete the file manually and retry.\".format(filepath))\n\n\ndef validate_file(file_obj: Any, hash_value: str, hash_type: str = \"sha256\") -> bool:\n    \"\"\"Validate a given file object with its hash.\n\n    Args:\n        file_obj: File object to read from.\n        hash_value (str): Hash for url.\n        hash_type (str, optional): Hash type, among \"sha256\" and \"md5\" (Default: ``\"sha256\"``).\n\n    Returns:\n        bool: return True if its a valid file, else False.\n    \"\"\"\n\n    if hash_type == \"sha256\":\n        hash_func = hashlib.sha256()\n    elif hash_type == \"md5\":\n        hash_func = hashlib.md5()\n    else:\n        raise ValueError\n\n    while True:\n        # Read by chunk to avoid filling memory\n        chunk = file_obj.read(1024**2)\n        if not chunk:\n            break\n        hash_func.update(chunk)\n\n    return hash_func.hexdigest() == hash_value\n\n\ndef extract_archive(from_path: str, to_path: Optional[str] = None, overwrite: bool = False) -> List[str]:\n    \"\"\"Extract archive.\n    Args:\n        from_path (str): the path of the archive.\n        to_path (str or None, optional): the root path of the extraced files (directory of from_path)\n            (Default: ``None``)\n        overwrite (bool, optional): overwrite existing files (Default: ``False``)\n\n    Returns:\n        list: List of paths to extracted files even if not overwritten.\n    \"\"\"\n\n    if to_path is None:\n        to_path = os.path.dirname(from_path)\n\n    try:\n        with tarfile.open(from_path, \"r\") as tar:\n            logging.info(\"Opened tar file %s.\", from_path)\n            files = []\n            for file_ in tar:  # type: Any\n                file_path = os.path.join(to_path, file_.name)\n                if file_.isfile():\n                    files.append(file_path)\n                    if os.path.exists(file_path):\n                        logging.info(\"%s already extracted.\", file_path)\n                        if not overwrite:\n                            continue\n                tar.extract(file_, to_path)\n            return files\n    except tarfile.ReadError:\n        pass\n\n    try:\n        with zipfile.ZipFile(from_path, \"r\") as zfile:\n            logging.info(\"Opened zip file %s.\", from_path)\n            files = zfile.namelist()\n            for file_ in files:\n                file_path = os.path.join(to_path, file_)\n                if os.path.exists(file_path):\n                    logging.info(\"%s already extracted.\", file_path)\n                    if not overwrite:\n                        continue\n                zfile.extract(file_, to_path)\n        return files\n    except zipfile.BadZipFile:\n        pass\n\n    raise NotImplementedError(\" > [!] only supports tar.gz, tgz, and zip achives.\")\n\n\ndef download_kaggle_dataset(dataset_path: str, dataset_name: str, output_path: str):\n    \"\"\"Download dataset from kaggle.\n    Args:\n        dataset_path (str):\n        This the kaggle link to the dataset. for example vctk is 'mfekadu/english-multispeaker-corpus-for-voice-cloning'\n        dataset_name (str): Name of the folder the dataset will be saved in.\n        output_path (str): Path of the location you want the dataset folder to be saved to.\n    \"\"\"\n    data_path = os.path.join(output_path, dataset_name)\n    try:\n        import kaggle  # pylint: disable=import-outside-toplevel\n\n        kaggle.api.authenticate()\n        print(f\"\"\"\\nDownloading {dataset_name}...\"\"\")\n        kaggle.api.dataset_download_files(dataset_path, path=data_path, unzip=True)\n    except OSError:\n        print(\n            f\"\"\"[!] in order to download kaggle datasets, you need to have a kaggle api token stored in your {os.path.join(expanduser('~'), '.kaggle/kaggle.json')}\"\"\"\n        )\n", "TTS/utils/audio/numpy_transforms.py": "from io import BytesIO\nfrom typing import Tuple\n\nimport librosa\nimport numpy as np\nimport scipy\nimport soundfile as sf\nfrom librosa import magphase, pyin\n\n# For using kwargs\n# pylint: disable=unused-argument\n\n\ndef build_mel_basis(\n    *,\n    sample_rate: int = None,\n    fft_size: int = None,\n    num_mels: int = None,\n    mel_fmax: int = None,\n    mel_fmin: int = None,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Build melspectrogram basis.\n\n    Returns:\n        np.ndarray: melspectrogram basis.\n    \"\"\"\n    if mel_fmax is not None:\n        assert mel_fmax <= sample_rate // 2\n        assert mel_fmax - mel_fmin > 0\n    return librosa.filters.mel(sr=sample_rate, n_fft=fft_size, n_mels=num_mels, fmin=mel_fmin, fmax=mel_fmax)\n\n\ndef millisec_to_length(\n    *, frame_length_ms: int = None, frame_shift_ms: int = None, sample_rate: int = None, **kwargs\n) -> Tuple[int, int]:\n    \"\"\"Compute hop and window length from milliseconds.\n\n    Returns:\n        Tuple[int, int]: hop length and window length for STFT.\n    \"\"\"\n    factor = frame_length_ms / frame_shift_ms\n    assert (factor).is_integer(), \" [!] frame_shift_ms should divide frame_length_ms\"\n    win_length = int(frame_length_ms / 1000.0 * sample_rate)\n    hop_length = int(win_length / float(factor))\n    return win_length, hop_length\n\n\ndef _log(x, base):\n    if base == 10:\n        return np.log10(x)\n    return np.log(x)\n\n\ndef _exp(x, base):\n    if base == 10:\n        return np.power(10, x)\n    return np.exp(x)\n\n\ndef amp_to_db(*, x: np.ndarray = None, gain: float = 1, base: int = 10, **kwargs) -> np.ndarray:\n    \"\"\"Convert amplitude values to decibels.\n\n    Args:\n        x (np.ndarray): Amplitude spectrogram.\n        gain (float): Gain factor. Defaults to 1.\n        base (int): Logarithm base. Defaults to 10.\n\n    Returns:\n        np.ndarray: Decibels spectrogram.\n    \"\"\"\n    assert (x < 0).sum() == 0, \" [!] Input values must be non-negative.\"\n    return gain * _log(np.maximum(1e-8, x), base)\n\n\n# pylint: disable=no-self-use\ndef db_to_amp(*, x: np.ndarray = None, gain: float = 1, base: int = 10, **kwargs) -> np.ndarray:\n    \"\"\"Convert decibels spectrogram to amplitude spectrogram.\n\n    Args:\n        x (np.ndarray): Decibels spectrogram.\n        gain (float): Gain factor. Defaults to 1.\n        base (int): Logarithm base. Defaults to 10.\n\n    Returns:\n        np.ndarray: Amplitude spectrogram.\n    \"\"\"\n    return _exp(x / gain, base)\n\n\ndef preemphasis(*, x: np.ndarray, coef: float = 0.97, **kwargs) -> np.ndarray:\n    \"\"\"Apply pre-emphasis to the audio signal. Useful to reduce the correlation between neighbouring signal values.\n\n    Args:\n        x (np.ndarray): Audio signal.\n\n    Raises:\n        RuntimeError: Preemphasis coeff is set to 0.\n\n    Returns:\n        np.ndarray: Decorrelated audio signal.\n    \"\"\"\n    if coef == 0:\n        raise RuntimeError(\" [!] Preemphasis is set 0.0.\")\n    return scipy.signal.lfilter([1, -coef], [1], x)\n\n\ndef deemphasis(*, x: np.ndarray = None, coef: float = 0.97, **kwargs) -> np.ndarray:\n    \"\"\"Reverse pre-emphasis.\"\"\"\n    if coef == 0:\n        raise RuntimeError(\" [!] Preemphasis is set 0.0.\")\n    return scipy.signal.lfilter([1], [1, -coef], x)\n\n\ndef spec_to_mel(*, spec: np.ndarray, mel_basis: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Convert a full scale linear spectrogram output of a network to a melspectrogram.\n\n    Args:\n        spec (np.ndarray): Normalized full scale linear spectrogram.\n\n    Shapes:\n        - spec: :math:`[C, T]`\n\n    Returns:\n        np.ndarray: Normalized melspectrogram.\n    \"\"\"\n    return np.dot(mel_basis, spec)\n\n\ndef mel_to_spec(*, mel: np.ndarray = None, mel_basis: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Convert a melspectrogram to full scale spectrogram.\"\"\"\n    assert (mel < 0).sum() == 0, \" [!] Input values must be non-negative.\"\n    inv_mel_basis = np.linalg.pinv(mel_basis)\n    return np.maximum(1e-10, np.dot(inv_mel_basis, mel))\n\n\ndef wav_to_spec(*, wav: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Compute a spectrogram from a waveform.\n\n    Args:\n        wav (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n\n    Returns:\n        np.ndarray: Spectrogram. Shape :math:`[C, T_spec]`. :math:`T_spec == T_wav / hop_length`\n    \"\"\"\n    D = stft(y=wav, **kwargs)\n    S = np.abs(D)\n    return S.astype(np.float32)\n\n\ndef wav_to_mel(*, wav: np.ndarray = None, mel_basis=None, **kwargs) -> np.ndarray:\n    \"\"\"Compute a melspectrogram from a waveform.\"\"\"\n    D = stft(y=wav, **kwargs)\n    S = spec_to_mel(spec=np.abs(D), mel_basis=mel_basis, **kwargs)\n    return S.astype(np.float32)\n\n\ndef spec_to_wav(*, spec: np.ndarray, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a spectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = spec.copy()\n    return griffin_lim(spec=S**power, **kwargs)\n\n\ndef mel_to_wav(*, mel: np.ndarray = None, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a melspectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = mel.copy()\n    S = mel_to_spec(mel=S, mel_basis=kwargs[\"mel_basis\"])  # Convert back to linear\n    return griffin_lim(spec=S**power, **kwargs)\n\n\n### STFT and ISTFT ###\ndef stft(\n    *,\n    y: np.ndarray = None,\n    fft_size: int = None,\n    hop_length: int = None,\n    win_length: int = None,\n    pad_mode: str = \"reflect\",\n    window: str = \"hann\",\n    center: bool = True,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Librosa STFT wrapper.\n\n    Check http://librosa.org/doc/main/generated/librosa.stft.html argument details.\n\n    Returns:\n        np.ndarray: Complex number array.\n    \"\"\"\n    return librosa.stft(\n        y=y,\n        n_fft=fft_size,\n        hop_length=hop_length,\n        win_length=win_length,\n        pad_mode=pad_mode,\n        window=window,\n        center=center,\n    )\n\n\ndef istft(\n    *,\n    y: np.ndarray = None,\n    hop_length: int = None,\n    win_length: int = None,\n    window: str = \"hann\",\n    center: bool = True,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Librosa iSTFT wrapper.\n\n    Check http://librosa.org/doc/main/generated/librosa.istft.html argument details.\n\n    Returns:\n        np.ndarray: Complex number array.\n    \"\"\"\n    return librosa.istft(y, hop_length=hop_length, win_length=win_length, center=center, window=window)\n\n\ndef griffin_lim(*, spec: np.ndarray = None, num_iter=60, **kwargs) -> np.ndarray:\n    angles = np.exp(2j * np.pi * np.random.rand(*spec.shape))\n    S_complex = np.abs(spec).astype(complex)\n    y = istft(y=S_complex * angles, **kwargs)\n    if not np.isfinite(y).all():\n        print(\" [!] Waveform is not finite everywhere. Skipping the GL.\")\n        return np.array([0.0])\n    for _ in range(num_iter):\n        angles = np.exp(1j * np.angle(stft(y=y, **kwargs)))\n        y = istft(y=S_complex * angles, **kwargs)\n    return y\n\n\ndef compute_stft_paddings(\n    *, x: np.ndarray = None, hop_length: int = None, pad_two_sides: bool = False, **kwargs\n) -> Tuple[int, int]:\n    \"\"\"Compute paddings used by Librosa's STFT. Compute right padding (final frame) or both sides padding\n    (first and final frames)\"\"\"\n    pad = (x.shape[0] // hop_length + 1) * hop_length - x.shape[0]\n    if not pad_two_sides:\n        return 0, pad\n    return pad // 2, pad // 2 + pad % 2\n\n\ndef compute_f0(\n    *,\n    x: np.ndarray = None,\n    pitch_fmax: float = None,\n    pitch_fmin: float = None,\n    hop_length: int = None,\n    win_length: int = None,\n    sample_rate: int = None,\n    stft_pad_mode: str = \"reflect\",\n    center: bool = True,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Compute pitch (f0) of a waveform using the same parameters used for computing melspectrogram.\n\n    Args:\n        x (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n        pitch_fmax (float): Pitch max value.\n        pitch_fmin (float): Pitch min value.\n        hop_length (int): Number of frames between STFT columns.\n        win_length (int): STFT window length.\n        sample_rate (int): Audio sampling rate.\n        stft_pad_mode (str): Padding mode for STFT.\n        center (bool): Centered padding.\n\n    Returns:\n        np.ndarray: Pitch. Shape :math:`[T_pitch,]`. :math:`T_pitch == T_wav / hop_length`\n\n    Examples:\n        >>> WAV_FILE = filename = librosa.example('vibeace')\n        >>> from TTS.config import BaseAudioConfig\n        >>> from TTS.utils.audio import AudioProcessor\n        >>> conf = BaseAudioConfig(pitch_fmax=640, pitch_fmin=1)\n        >>> ap = AudioProcessor(**conf)\n        >>> wav = ap.load_wav(WAV_FILE, sr=ap.sample_rate)[:5 * ap.sample_rate]\n        >>> pitch = ap.compute_f0(wav)\n    \"\"\"\n    assert pitch_fmax is not None, \" [!] Set `pitch_fmax` before caling `compute_f0`.\"\n    assert pitch_fmin is not None, \" [!] Set `pitch_fmin` before caling `compute_f0`.\"\n\n    f0, voiced_mask, _ = pyin(\n        y=x.astype(np.double),\n        fmin=pitch_fmin,\n        fmax=pitch_fmax,\n        sr=sample_rate,\n        frame_length=win_length,\n        win_length=win_length // 2,\n        hop_length=hop_length,\n        pad_mode=stft_pad_mode,\n        center=center,\n        n_thresholds=100,\n        beta_parameters=(2, 18),\n        boltzmann_parameter=2,\n        resolution=0.1,\n        max_transition_rate=35.92,\n        switch_prob=0.01,\n        no_trough_prob=0.01,\n    )\n    f0[~voiced_mask] = 0.0\n\n    return f0\n\n\ndef compute_energy(y: np.ndarray, **kwargs) -> np.ndarray:\n    \"\"\"Compute energy of a waveform using the same parameters used for computing melspectrogram.\n    Args:\n      x (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n    Returns:\n      np.ndarray: energy. Shape :math:`[T_energy,]`. :math:`T_energy == T_wav / hop_length`\n    Examples:\n      >>> WAV_FILE = filename = librosa.example('vibeace')\n      >>> from TTS.config import BaseAudioConfig\n      >>> from TTS.utils.audio import AudioProcessor\n      >>> conf = BaseAudioConfig()\n      >>> ap = AudioProcessor(**conf)\n      >>> wav = ap.load_wav(WAV_FILE, sr=ap.sample_rate)[:5 * ap.sample_rate]\n      >>> energy = ap.compute_energy(wav)\n    \"\"\"\n    x = stft(y=y, **kwargs)\n    mag, _ = magphase(x)\n    energy = np.sqrt(np.sum(mag**2, axis=0))\n    return energy\n\n\n### Audio Processing ###\ndef find_endpoint(\n    *,\n    wav: np.ndarray = None,\n    trim_db: float = -40,\n    sample_rate: int = None,\n    min_silence_sec=0.8,\n    gain: float = None,\n    base: int = None,\n    **kwargs,\n) -> int:\n    \"\"\"Find the last point without silence at the end of a audio signal.\n\n    Args:\n        wav (np.ndarray): Audio signal.\n        threshold_db (int, optional): Silence threshold in decibels. Defaults to -40.\n        min_silence_sec (float, optional): Ignore silences that are shorter then this in secs. Defaults to 0.8.\n        gian (float, optional): Gain to be used to convert trim_db to trim_amp. Defaults to None.\n        base (int, optional): Base of the logarithm used to convert trim_db to trim_amp. Defaults to 10.\n\n    Returns:\n        int: Last point without silence.\n    \"\"\"\n    window_length = int(sample_rate * min_silence_sec)\n    hop_length = int(window_length / 4)\n    threshold = db_to_amp(x=-trim_db, gain=gain, base=base)\n    for x in range(hop_length, len(wav) - window_length, hop_length):\n        if np.max(wav[x : x + window_length]) < threshold:\n            return x + hop_length\n    return len(wav)\n\n\ndef trim_silence(\n    *,\n    wav: np.ndarray = None,\n    sample_rate: int = None,\n    trim_db: float = None,\n    win_length: int = None,\n    hop_length: int = None,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Trim silent parts with a threshold and 0.01 sec margin\"\"\"\n    margin = int(sample_rate * 0.01)\n    wav = wav[margin:-margin]\n    return librosa.effects.trim(wav, top_db=trim_db, frame_length=win_length, hop_length=hop_length)[0]\n\n\ndef volume_norm(*, x: np.ndarray = None, coef: float = 0.95, **kwargs) -> np.ndarray:\n    \"\"\"Normalize the volume of an audio signal.\n\n    Args:\n        x (np.ndarray): Raw waveform.\n        coef (float): Coefficient to rescale the maximum value. Defaults to 0.95.\n\n    Returns:\n        np.ndarray: Volume normalized waveform.\n    \"\"\"\n    return x / abs(x).max() * coef\n\n\ndef rms_norm(*, wav: np.ndarray = None, db_level: float = -27.0, **kwargs) -> np.ndarray:\n    r = 10 ** (db_level / 20)\n    a = np.sqrt((len(wav) * (r**2)) / np.sum(wav**2))\n    return wav * a\n\n\ndef rms_volume_norm(*, x: np.ndarray, db_level: float = -27.0, **kwargs) -> np.ndarray:\n    \"\"\"Normalize the volume based on RMS of the signal.\n\n    Args:\n        x (np.ndarray): Raw waveform.\n        db_level (float): Target dB level in RMS. Defaults to -27.0.\n\n    Returns:\n        np.ndarray: RMS normalized waveform.\n    \"\"\"\n    assert -99 <= db_level <= 0, \" [!] db_level should be between -99 and 0\"\n    wav = rms_norm(wav=x, db_level=db_level)\n    return wav\n\n\ndef load_wav(*, filename: str, sample_rate: int = None, resample: bool = False, **kwargs) -> np.ndarray:\n    \"\"\"Read a wav file using Librosa and optionally resample, silence trim, volume normalize.\n\n    Resampling slows down loading the file significantly. Therefore it is recommended to resample the file before.\n\n    Args:\n        filename (str): Path to the wav file.\n        sr (int, optional): Sampling rate for resampling. Defaults to None.\n        resample (bool, optional): Resample the audio file when loading. Slows down the I/O time. Defaults to False.\n\n    Returns:\n        np.ndarray: Loaded waveform.\n    \"\"\"\n    if resample:\n        # loading with resampling. It is significantly slower.\n        x, _ = librosa.load(filename, sr=sample_rate)\n    else:\n        # SF is faster than librosa for loading files\n        x, _ = sf.read(filename)\n    return x\n\n\ndef save_wav(*, wav: np.ndarray, path: str, sample_rate: int = None, pipe_out=None, **kwargs) -> None:\n    \"\"\"Save float waveform to a file using Scipy.\n\n    Args:\n        wav (np.ndarray): Waveform with float values in range [-1, 1] to save.\n        path (str): Path to a output file.\n        sr (int, optional): Sampling rate used for saving to the file. Defaults to None.\n        pipe_out (BytesIO, optional): Flag to stdout the generated TTS wav file for shell pipe.\n    \"\"\"\n    wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n\n    wav_norm = wav_norm.astype(np.int16)\n    if pipe_out:\n        wav_buffer = BytesIO()\n        scipy.io.wavfile.write(wav_buffer, sample_rate, wav_norm)\n        wav_buffer.seek(0)\n        pipe_out.buffer.write(wav_buffer.read())\n    scipy.io.wavfile.write(path, sample_rate, wav_norm)\n\n\ndef mulaw_encode(*, wav: np.ndarray, mulaw_qc: int, **kwargs) -> np.ndarray:\n    mu = 2**mulaw_qc - 1\n    signal = np.sign(wav) * np.log(1 + mu * np.abs(wav)) / np.log(1.0 + mu)\n    signal = (signal + 1) / 2 * mu + 0.5\n    return np.floor(\n        signal,\n    )\n\n\ndef mulaw_decode(*, wav, mulaw_qc: int, **kwargs) -> np.ndarray:\n    \"\"\"Recovers waveform from quantized values.\"\"\"\n    mu = 2**mulaw_qc - 1\n    x = np.sign(wav) / mu * ((1 + mu) ** np.abs(wav) - 1)\n    return x\n\n\ndef encode_16bits(*, x: np.ndarray, **kwargs) -> np.ndarray:\n    return np.clip(x * 2**15, -(2**15), 2**15 - 1).astype(np.int16)\n\n\ndef quantize(*, x: np.ndarray, quantize_bits: int, **kwargs) -> np.ndarray:\n    \"\"\"Quantize a waveform to a given number of bits.\n\n    Args:\n        x (np.ndarray): Waveform to quantize. Must be normalized into the range `[-1, 1]`.\n        quantize_bits (int): Number of quantization bits.\n\n    Returns:\n        np.ndarray: Quantized waveform.\n    \"\"\"\n    return (x + 1.0) * (2**quantize_bits - 1) / 2\n\n\ndef dequantize(*, x, quantize_bits, **kwargs) -> np.ndarray:\n    \"\"\"Dequantize a waveform from the given number of bits.\"\"\"\n    return 2 * x / (2**quantize_bits - 1) - 1\n", "TTS/utils/audio/processor.py": "from io import BytesIO\nfrom typing import Dict, Tuple\n\nimport librosa\nimport numpy as np\nimport scipy.io.wavfile\nimport scipy.signal\n\nfrom TTS.tts.utils.helpers import StandardScaler\nfrom TTS.utils.audio.numpy_transforms import (\n    amp_to_db,\n    build_mel_basis,\n    compute_f0,\n    db_to_amp,\n    deemphasis,\n    find_endpoint,\n    griffin_lim,\n    load_wav,\n    mel_to_spec,\n    millisec_to_length,\n    preemphasis,\n    rms_volume_norm,\n    spec_to_mel,\n    stft,\n    trim_silence,\n    volume_norm,\n)\n\n# pylint: disable=too-many-public-methods\n\n\nclass AudioProcessor(object):\n    \"\"\"Audio Processor for TTS.\n\n    Note:\n        All the class arguments are set to default values to enable a flexible initialization\n        of the class with the model config. They are not meaningful for all the arguments.\n\n    Args:\n        sample_rate (int, optional):\n            target audio sampling rate. Defaults to None.\n\n        resample (bool, optional):\n            enable/disable resampling of the audio clips when the target sampling rate does not match the original sampling rate. Defaults to False.\n\n        num_mels (int, optional):\n            number of melspectrogram dimensions. Defaults to None.\n\n        log_func (int, optional):\n            log exponent used for converting spectrogram aplitude to DB.\n\n        min_level_db (int, optional):\n            minimum db threshold for the computed melspectrograms. Defaults to None.\n\n        frame_shift_ms (int, optional):\n            milliseconds of frames between STFT columns. Defaults to None.\n\n        frame_length_ms (int, optional):\n            milliseconds of STFT window length. Defaults to None.\n\n        hop_length (int, optional):\n            number of frames between STFT columns. Used if ```frame_shift_ms``` is None. Defaults to None.\n\n        win_length (int, optional):\n            STFT window length. Used if ```frame_length_ms``` is None. Defaults to None.\n\n        ref_level_db (int, optional):\n            reference DB level to avoid background noise. In general <20DB corresponds to the air noise. Defaults to None.\n\n        fft_size (int, optional):\n            FFT window size for STFT. Defaults to 1024.\n\n        power (int, optional):\n            Exponent value applied to the spectrogram before GriffinLim. Defaults to None.\n\n        preemphasis (float, optional):\n            Preemphasis coefficient. Preemphasis is disabled if == 0.0. Defaults to 0.0.\n\n        signal_norm (bool, optional):\n            enable/disable signal normalization. Defaults to None.\n\n        symmetric_norm (bool, optional):\n            enable/disable symmetric normalization. If set True normalization is performed in the range [-k, k] else [0, k], Defaults to None.\n\n        max_norm (float, optional):\n            ```k``` defining the normalization range. Defaults to None.\n\n        mel_fmin (int, optional):\n            minimum filter frequency for computing melspectrograms. Defaults to None.\n\n        mel_fmax (int, optional):\n            maximum filter frequency for computing melspectrograms. Defaults to None.\n\n        pitch_fmin (int, optional):\n            minimum filter frequency for computing pitch. Defaults to None.\n\n        pitch_fmax (int, optional):\n            maximum filter frequency for computing pitch. Defaults to None.\n\n        spec_gain (int, optional):\n            gain applied when converting amplitude to DB. Defaults to 20.\n\n        stft_pad_mode (str, optional):\n            Padding mode for STFT. Defaults to 'reflect'.\n\n        clip_norm (bool, optional):\n            enable/disable clipping the our of range values in the normalized audio signal. Defaults to True.\n\n        griffin_lim_iters (int, optional):\n            Number of GriffinLim iterations. Defaults to None.\n\n        do_trim_silence (bool, optional):\n            enable/disable silence trimming when loading the audio signal. Defaults to False.\n\n        trim_db (int, optional):\n            DB threshold used for silence trimming. Defaults to 60.\n\n        do_sound_norm (bool, optional):\n            enable/disable signal normalization. Defaults to False.\n\n        do_amp_to_db_linear (bool, optional):\n            enable/disable amplitude to dB conversion of linear spectrograms. Defaults to True.\n\n        do_amp_to_db_mel (bool, optional):\n            enable/disable amplitude to dB conversion of mel spectrograms. Defaults to True.\n\n        do_rms_norm (bool, optional):\n            enable/disable RMS volume normalization when loading an audio file. Defaults to False.\n\n        db_level (int, optional):\n            dB level used for rms normalization. The range is -99 to 0. Defaults to None.\n\n        stats_path (str, optional):\n            Path to the computed stats file. Defaults to None.\n\n        verbose (bool, optional):\n            enable/disable logging. Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sample_rate=None,\n        resample=False,\n        num_mels=None,\n        log_func=\"np.log10\",\n        min_level_db=None,\n        frame_shift_ms=None,\n        frame_length_ms=None,\n        hop_length=None,\n        win_length=None,\n        ref_level_db=None,\n        fft_size=1024,\n        power=None,\n        preemphasis=0.0,\n        signal_norm=None,\n        symmetric_norm=None,\n        max_norm=None,\n        mel_fmin=None,\n        mel_fmax=None,\n        pitch_fmax=None,\n        pitch_fmin=None,\n        spec_gain=20,\n        stft_pad_mode=\"reflect\",\n        clip_norm=True,\n        griffin_lim_iters=None,\n        do_trim_silence=False,\n        trim_db=60,\n        do_sound_norm=False,\n        do_amp_to_db_linear=True,\n        do_amp_to_db_mel=True,\n        do_rms_norm=False,\n        db_level=None,\n        stats_path=None,\n        verbose=True,\n        **_,\n    ):\n        # setup class attributed\n        self.sample_rate = sample_rate\n        self.resample = resample\n        self.num_mels = num_mels\n        self.log_func = log_func\n        self.min_level_db = min_level_db or 0\n        self.frame_shift_ms = frame_shift_ms\n        self.frame_length_ms = frame_length_ms\n        self.ref_level_db = ref_level_db\n        self.fft_size = fft_size\n        self.power = power\n        self.preemphasis = preemphasis\n        self.griffin_lim_iters = griffin_lim_iters\n        self.signal_norm = signal_norm\n        self.symmetric_norm = symmetric_norm\n        self.mel_fmin = mel_fmin or 0\n        self.mel_fmax = mel_fmax\n        self.pitch_fmin = pitch_fmin\n        self.pitch_fmax = pitch_fmax\n        self.spec_gain = float(spec_gain)\n        self.stft_pad_mode = stft_pad_mode\n        self.max_norm = 1.0 if max_norm is None else float(max_norm)\n        self.clip_norm = clip_norm\n        self.do_trim_silence = do_trim_silence\n        self.trim_db = trim_db\n        self.do_sound_norm = do_sound_norm\n        self.do_amp_to_db_linear = do_amp_to_db_linear\n        self.do_amp_to_db_mel = do_amp_to_db_mel\n        self.do_rms_norm = do_rms_norm\n        self.db_level = db_level\n        self.stats_path = stats_path\n        # setup exp_func for db to amp conversion\n        if log_func == \"np.log\":\n            self.base = np.e\n        elif log_func == \"np.log10\":\n            self.base = 10\n        else:\n            raise ValueError(\" [!] unknown `log_func` value.\")\n        # setup stft parameters\n        if hop_length is None:\n            # compute stft parameters from given time values\n            self.win_length, self.hop_length = millisec_to_length(\n                frame_length_ms=self.frame_length_ms, frame_shift_ms=self.frame_shift_ms, sample_rate=self.sample_rate\n            )\n        else:\n            # use stft parameters from config file\n            self.hop_length = hop_length\n            self.win_length = win_length\n        assert min_level_db != 0.0, \" [!] min_level_db is 0\"\n        assert (\n            self.win_length <= self.fft_size\n        ), f\" [!] win_length cannot be larger than fft_size - {self.win_length} vs {self.fft_size}\"\n        members = vars(self)\n        if verbose:\n            print(\" > Setting up Audio Processor...\")\n            for key, value in members.items():\n                print(\" | > {}:{}\".format(key, value))\n        # create spectrogram utils\n        self.mel_basis = build_mel_basis(\n            sample_rate=self.sample_rate,\n            fft_size=self.fft_size,\n            num_mels=self.num_mels,\n            mel_fmax=self.mel_fmax,\n            mel_fmin=self.mel_fmin,\n        )\n        # setup scaler\n        if stats_path and signal_norm:\n            mel_mean, mel_std, linear_mean, linear_std, _ = self.load_stats(stats_path)\n            self.setup_scaler(mel_mean, mel_std, linear_mean, linear_std)\n            self.signal_norm = True\n            self.max_norm = None\n            self.clip_norm = None\n            self.symmetric_norm = None\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\", verbose=True):\n        if \"audio\" in config:\n            return AudioProcessor(verbose=verbose, **config.audio)\n        return AudioProcessor(verbose=verbose, **config)\n\n    ### normalization ###\n    def normalize(self, S: np.ndarray) -> np.ndarray:\n        \"\"\"Normalize values into `[0, self.max_norm]` or `[-self.max_norm, self.max_norm]`\n\n        Args:\n            S (np.ndarray): Spectrogram to normalize.\n\n        Raises:\n            RuntimeError: Mean and variance is computed from incompatible parameters.\n\n        Returns:\n            np.ndarray: Normalized spectrogram.\n        \"\"\"\n        # pylint: disable=no-else-return\n        S = S.copy()\n        if self.signal_norm:\n            # mean-var scaling\n            if hasattr(self, \"mel_scaler\"):\n                if S.shape[0] == self.num_mels:\n                    return self.mel_scaler.transform(S.T).T\n                elif S.shape[0] == self.fft_size / 2:\n                    return self.linear_scaler.transform(S.T).T\n                else:\n                    raise RuntimeError(\" [!] Mean-Var stats does not match the given feature dimensions.\")\n            # range normalization\n            S -= self.ref_level_db  # discard certain range of DB assuming it is air noise\n            S_norm = (S - self.min_level_db) / (-self.min_level_db)\n            if self.symmetric_norm:\n                S_norm = ((2 * self.max_norm) * S_norm) - self.max_norm\n                if self.clip_norm:\n                    S_norm = np.clip(\n                        S_norm, -self.max_norm, self.max_norm  # pylint: disable=invalid-unary-operand-type\n                    )\n                return S_norm\n            else:\n                S_norm = self.max_norm * S_norm\n                if self.clip_norm:\n                    S_norm = np.clip(S_norm, 0, self.max_norm)\n                return S_norm\n        else:\n            return S\n\n    def denormalize(self, S: np.ndarray) -> np.ndarray:\n        \"\"\"Denormalize spectrogram values.\n\n        Args:\n            S (np.ndarray): Spectrogram to denormalize.\n\n        Raises:\n            RuntimeError: Mean and variance are incompatible.\n\n        Returns:\n            np.ndarray: Denormalized spectrogram.\n        \"\"\"\n        # pylint: disable=no-else-return\n        S_denorm = S.copy()\n        if self.signal_norm:\n            # mean-var scaling\n            if hasattr(self, \"mel_scaler\"):\n                if S_denorm.shape[0] == self.num_mels:\n                    return self.mel_scaler.inverse_transform(S_denorm.T).T\n                elif S_denorm.shape[0] == self.fft_size / 2:\n                    return self.linear_scaler.inverse_transform(S_denorm.T).T\n                else:\n                    raise RuntimeError(\" [!] Mean-Var stats does not match the given feature dimensions.\")\n            if self.symmetric_norm:\n                if self.clip_norm:\n                    S_denorm = np.clip(\n                        S_denorm, -self.max_norm, self.max_norm  # pylint: disable=invalid-unary-operand-type\n                    )\n                S_denorm = ((S_denorm + self.max_norm) * -self.min_level_db / (2 * self.max_norm)) + self.min_level_db\n                return S_denorm + self.ref_level_db\n            else:\n                if self.clip_norm:\n                    S_denorm = np.clip(S_denorm, 0, self.max_norm)\n                S_denorm = (S_denorm * -self.min_level_db / self.max_norm) + self.min_level_db\n                return S_denorm + self.ref_level_db\n        else:\n            return S_denorm\n\n    ### Mean-STD scaling ###\n    def load_stats(self, stats_path: str) -> Tuple[np.array, np.array, np.array, np.array, Dict]:\n        \"\"\"Loading mean and variance statistics from a `npy` file.\n\n        Args:\n            stats_path (str): Path to the `npy` file containing\n\n        Returns:\n            Tuple[np.array, np.array, np.array, np.array, Dict]: loaded statistics and the config used to\n                compute them.\n        \"\"\"\n        stats = np.load(stats_path, allow_pickle=True).item()  # pylint: disable=unexpected-keyword-arg\n        mel_mean = stats[\"mel_mean\"]\n        mel_std = stats[\"mel_std\"]\n        linear_mean = stats[\"linear_mean\"]\n        linear_std = stats[\"linear_std\"]\n        stats_config = stats[\"audio_config\"]\n        # check all audio parameters used for computing stats\n        skip_parameters = [\"griffin_lim_iters\", \"stats_path\", \"do_trim_silence\", \"ref_level_db\", \"power\"]\n        for key in stats_config.keys():\n            if key in skip_parameters:\n                continue\n            if key not in [\"sample_rate\", \"trim_db\"]:\n                assert (\n                    stats_config[key] == self.__dict__[key]\n                ), f\" [!] Audio param {key} does not match the value used for computing mean-var stats. {stats_config[key]} vs {self.__dict__[key]}\"\n        return mel_mean, mel_std, linear_mean, linear_std, stats_config\n\n    # pylint: disable=attribute-defined-outside-init\n    def setup_scaler(\n        self, mel_mean: np.ndarray, mel_std: np.ndarray, linear_mean: np.ndarray, linear_std: np.ndarray\n    ) -> None:\n        \"\"\"Initialize scaler objects used in mean-std normalization.\n\n        Args:\n            mel_mean (np.ndarray): Mean for melspectrograms.\n            mel_std (np.ndarray): STD for melspectrograms.\n            linear_mean (np.ndarray): Mean for full scale spectrograms.\n            linear_std (np.ndarray): STD for full scale spectrograms.\n        \"\"\"\n        self.mel_scaler = StandardScaler()\n        self.mel_scaler.set_stats(mel_mean, mel_std)\n        self.linear_scaler = StandardScaler()\n        self.linear_scaler.set_stats(linear_mean, linear_std)\n\n    ### Preemphasis ###\n    def apply_preemphasis(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Apply pre-emphasis to the audio signal. Useful to reduce the correlation between neighbouring signal values.\n\n        Args:\n            x (np.ndarray): Audio signal.\n\n        Raises:\n            RuntimeError: Preemphasis coeff is set to 0.\n\n        Returns:\n            np.ndarray: Decorrelated audio signal.\n        \"\"\"\n        return preemphasis(x=x, coef=self.preemphasis)\n\n    def apply_inv_preemphasis(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Reverse pre-emphasis.\"\"\"\n        return deemphasis(x=x, coef=self.preemphasis)\n\n    ### SPECTROGRAMs ###\n    def spectrogram(self, y: np.ndarray) -> np.ndarray:\n        \"\"\"Compute a spectrogram from a waveform.\n\n        Args:\n            y (np.ndarray): Waveform.\n\n        Returns:\n            np.ndarray: Spectrogram.\n        \"\"\"\n        if self.preemphasis != 0:\n            y = self.apply_preemphasis(y)\n        D = stft(\n            y=y,\n            fft_size=self.fft_size,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            pad_mode=self.stft_pad_mode,\n        )\n        if self.do_amp_to_db_linear:\n            S = amp_to_db(x=np.abs(D), gain=self.spec_gain, base=self.base)\n        else:\n            S = np.abs(D)\n        return self.normalize(S).astype(np.float32)\n\n    def melspectrogram(self, y: np.ndarray) -> np.ndarray:\n        \"\"\"Compute a melspectrogram from a waveform.\"\"\"\n        if self.preemphasis != 0:\n            y = self.apply_preemphasis(y)\n        D = stft(\n            y=y,\n            fft_size=self.fft_size,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            pad_mode=self.stft_pad_mode,\n        )\n        S = spec_to_mel(spec=np.abs(D), mel_basis=self.mel_basis)\n        if self.do_amp_to_db_mel:\n            S = amp_to_db(x=S, gain=self.spec_gain, base=self.base)\n\n        return self.normalize(S).astype(np.float32)\n\n    def inv_spectrogram(self, spectrogram: np.ndarray) -> np.ndarray:\n        \"\"\"Convert a spectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n        S = self.denormalize(spectrogram)\n        S = db_to_amp(x=S, gain=self.spec_gain, base=self.base)\n        # Reconstruct phase\n        W = self._griffin_lim(S**self.power)\n        return self.apply_inv_preemphasis(W) if self.preemphasis != 0 else W\n\n    def inv_melspectrogram(self, mel_spectrogram: np.ndarray) -> np.ndarray:\n        \"\"\"Convert a melspectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n        D = self.denormalize(mel_spectrogram)\n        S = db_to_amp(x=D, gain=self.spec_gain, base=self.base)\n        S = mel_to_spec(mel=S, mel_basis=self.mel_basis)  # Convert back to linear\n        W = self._griffin_lim(S**self.power)\n        return self.apply_inv_preemphasis(W) if self.preemphasis != 0 else W\n\n    def out_linear_to_mel(self, linear_spec: np.ndarray) -> np.ndarray:\n        \"\"\"Convert a full scale linear spectrogram output of a network to a melspectrogram.\n\n        Args:\n            linear_spec (np.ndarray): Normalized full scale linear spectrogram.\n\n        Returns:\n            np.ndarray: Normalized melspectrogram.\n        \"\"\"\n        S = self.denormalize(linear_spec)\n        S = db_to_amp(x=S, gain=self.spec_gain, base=self.base)\n        S = spec_to_mel(spec=np.abs(S), mel_basis=self.mel_basis)\n        S = amp_to_db(x=S, gain=self.spec_gain, base=self.base)\n        mel = self.normalize(S)\n        return mel\n\n    def _griffin_lim(self, S):\n        return griffin_lim(\n            spec=S,\n            num_iter=self.griffin_lim_iters,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            fft_size=self.fft_size,\n            pad_mode=self.stft_pad_mode,\n        )\n\n    def compute_f0(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Compute pitch (f0) of a waveform using the same parameters used for computing melspectrogram.\n\n        Args:\n            x (np.ndarray): Waveform.\n\n        Returns:\n            np.ndarray: Pitch.\n\n        Examples:\n            >>> WAV_FILE = filename = librosa.example('vibeace')\n            >>> from TTS.config import BaseAudioConfig\n            >>> from TTS.utils.audio import AudioProcessor\n            >>> conf = BaseAudioConfig(pitch_fmax=640, pitch_fmin=1)\n            >>> ap = AudioProcessor(**conf)\n            >>> wav = ap.load_wav(WAV_FILE, sr=ap.sample_rate)[:5 * ap.sample_rate]\n            >>> pitch = ap.compute_f0(wav)\n        \"\"\"\n        # align F0 length to the spectrogram length\n        if len(x) % self.hop_length == 0:\n            x = np.pad(x, (0, self.hop_length // 2), mode=self.stft_pad_mode)\n\n        f0 = compute_f0(\n            x=x,\n            pitch_fmax=self.pitch_fmax,\n            pitch_fmin=self.pitch_fmin,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            sample_rate=self.sample_rate,\n            stft_pad_mode=self.stft_pad_mode,\n            center=True,\n        )\n\n        return f0\n\n    ### Audio Processing ###\n    def find_endpoint(self, wav: np.ndarray, min_silence_sec=0.8) -> int:\n        \"\"\"Find the last point without silence at the end of a audio signal.\n\n        Args:\n            wav (np.ndarray): Audio signal.\n            threshold_db (int, optional): Silence threshold in decibels. Defaults to -40.\n            min_silence_sec (float, optional): Ignore silences that are shorter then this in secs. Defaults to 0.8.\n\n        Returns:\n            int: Last point without silence.\n        \"\"\"\n        return find_endpoint(\n            wav=wav,\n            trim_db=self.trim_db,\n            sample_rate=self.sample_rate,\n            min_silence_sec=min_silence_sec,\n            gain=self.spec_gain,\n            base=self.base,\n        )\n\n    def trim_silence(self, wav):\n        \"\"\"Trim silent parts with a threshold and 0.01 sec margin\"\"\"\n        return trim_silence(\n            wav=wav,\n            sample_rate=self.sample_rate,\n            trim_db=self.trim_db,\n            win_length=self.win_length,\n            hop_length=self.hop_length,\n        )\n\n    @staticmethod\n    def sound_norm(x: np.ndarray) -> np.ndarray:\n        \"\"\"Normalize the volume of an audio signal.\n\n        Args:\n            x (np.ndarray): Raw waveform.\n\n        Returns:\n            np.ndarray: Volume normalized waveform.\n        \"\"\"\n        return volume_norm(x=x)\n\n    def rms_volume_norm(self, x: np.ndarray, db_level: float = None) -> np.ndarray:\n        \"\"\"Normalize the volume based on RMS of the signal.\n\n        Args:\n            x (np.ndarray): Raw waveform.\n\n        Returns:\n            np.ndarray: RMS normalized waveform.\n        \"\"\"\n        if db_level is None:\n            db_level = self.db_level\n        return rms_volume_norm(x=x, db_level=db_level)\n\n    ### save and load ###\n    def load_wav(self, filename: str, sr: int = None) -> np.ndarray:\n        \"\"\"Read a wav file using Librosa and optionally resample, silence trim, volume normalize.\n\n        Resampling slows down loading the file significantly. Therefore it is recommended to resample the file before.\n\n        Args:\n            filename (str): Path to the wav file.\n            sr (int, optional): Sampling rate for resampling. Defaults to None.\n\n        Returns:\n            np.ndarray: Loaded waveform.\n        \"\"\"\n        if sr is not None:\n            x = load_wav(filename=filename, sample_rate=sr, resample=True)\n        else:\n            x = load_wav(filename=filename, sample_rate=self.sample_rate, resample=self.resample)\n        if self.do_trim_silence:\n            try:\n                x = self.trim_silence(x)\n            except ValueError:\n                print(f\" [!] File cannot be trimmed for silence - {filename}\")\n        if self.do_sound_norm:\n            x = self.sound_norm(x)\n        if self.do_rms_norm:\n            x = self.rms_volume_norm(x, self.db_level)\n        return x\n\n    def save_wav(self, wav: np.ndarray, path: str, sr: int = None, pipe_out=None) -> None:\n        \"\"\"Save a waveform to a file using Scipy.\n\n        Args:\n            wav (np.ndarray): Waveform to save.\n            path (str): Path to a output file.\n            sr (int, optional): Sampling rate used for saving to the file. Defaults to None.\n            pipe_out (BytesIO, optional): Flag to stdout the generated TTS wav file for shell pipe.\n        \"\"\"\n        if self.do_rms_norm:\n            wav_norm = self.rms_volume_norm(wav, self.db_level) * 32767\n        else:\n            wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n\n        wav_norm = wav_norm.astype(np.int16)\n        if pipe_out:\n            wav_buffer = BytesIO()\n            scipy.io.wavfile.write(wav_buffer, sr if sr else self.sample_rate, wav_norm)\n            wav_buffer.seek(0)\n            pipe_out.buffer.write(wav_buffer.read())\n        scipy.io.wavfile.write(path, sr if sr else self.sample_rate, wav_norm)\n\n    def get_duration(self, filename: str) -> float:\n        \"\"\"Get the duration of a wav file using Librosa.\n\n        Args:\n            filename (str): Path to the wav file.\n        \"\"\"\n        return librosa.get_duration(filename=filename)\n", "TTS/utils/audio/torch_transforms.py": "import librosa\nimport torch\nfrom torch import nn\n\n\nclass TorchSTFT(nn.Module):  # pylint: disable=abstract-method\n    \"\"\"Some of the audio processing funtions using Torch for faster batch processing.\n\n    Args:\n\n        n_fft (int):\n            FFT window size for STFT.\n\n        hop_length (int):\n            number of frames between STFT columns.\n\n        win_length (int, optional):\n            STFT window length.\n\n        pad_wav (bool, optional):\n            If True pad the audio with (n_fft - hop_length) / 2). Defaults to False.\n\n        window (str, optional):\n            The name of a function to create a window tensor that is applied/multiplied to each frame/window. Defaults to \"hann_window\"\n\n        sample_rate (int, optional):\n            target audio sampling rate. Defaults to None.\n\n        mel_fmin (int, optional):\n            minimum filter frequency for computing melspectrograms. Defaults to None.\n\n        mel_fmax (int, optional):\n            maximum filter frequency for computing melspectrograms. Defaults to None.\n\n        n_mels (int, optional):\n            number of melspectrogram dimensions. Defaults to None.\n\n        use_mel (bool, optional):\n            If True compute the melspectrograms otherwise. Defaults to False.\n\n        do_amp_to_db_linear (bool, optional):\n            enable/disable amplitude to dB conversion of linear spectrograms. Defaults to False.\n\n        spec_gain (float, optional):\n            gain applied when converting amplitude to DB. Defaults to 1.0.\n\n        power (float, optional):\n            Exponent for the magnitude spectrogram, e.g., 1 for energy, 2 for power, etc.  Defaults to None.\n\n        use_htk (bool, optional):\n            Use HTK formula in mel filter instead of Slaney.\n\n        mel_norm (None, 'slaney', or number, optional):\n            If 'slaney', divide the triangular mel weights by the width of the mel band\n            (area normalization).\n\n            If numeric, use `librosa.util.normalize` to normalize each filter by to unit l_p norm.\n            See `librosa.util.normalize` for a full description of supported norm values\n            (including `+-np.inf`).\n\n            Otherwise, leave all the triangles aiming for a peak value of 1.0. Defaults to \"slaney\".\n    \"\"\"\n\n    def __init__(\n        self,\n        n_fft,\n        hop_length,\n        win_length,\n        pad_wav=False,\n        window=\"hann_window\",\n        sample_rate=None,\n        mel_fmin=0,\n        mel_fmax=None,\n        n_mels=80,\n        use_mel=False,\n        do_amp_to_db=False,\n        spec_gain=1.0,\n        power=None,\n        use_htk=False,\n        mel_norm=\"slaney\",\n        normalized=False,\n    ):\n        super().__init__()\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.pad_wav = pad_wav\n        self.sample_rate = sample_rate\n        self.mel_fmin = mel_fmin\n        self.mel_fmax = mel_fmax\n        self.n_mels = n_mels\n        self.use_mel = use_mel\n        self.do_amp_to_db = do_amp_to_db\n        self.spec_gain = spec_gain\n        self.power = power\n        self.use_htk = use_htk\n        self.mel_norm = mel_norm\n        self.window = nn.Parameter(getattr(torch, window)(win_length), requires_grad=False)\n        self.mel_basis = None\n        self.normalized = normalized\n        if use_mel:\n            self._build_mel_basis()\n\n    def __call__(self, x):\n        \"\"\"Compute spectrogram frames by torch based stft.\n\n        Args:\n            x (Tensor): input waveform\n\n        Returns:\n            Tensor: spectrogram frames.\n\n        Shapes:\n            x: [B x T] or [:math:`[B, 1, T]`]\n        \"\"\"\n        if x.ndim == 2:\n            x = x.unsqueeze(1)\n        if self.pad_wav:\n            padding = int((self.n_fft - self.hop_length) / 2)\n            x = torch.nn.functional.pad(x, (padding, padding), mode=\"reflect\")\n        # B x D x T x 2\n        o = torch.stft(\n            x.squeeze(1),\n            self.n_fft,\n            self.hop_length,\n            self.win_length,\n            self.window,\n            center=True,\n            pad_mode=\"reflect\",  # compatible with audio.py\n            normalized=self.normalized,\n            onesided=True,\n            return_complex=False,\n        )\n        M = o[:, :, :, 0]\n        P = o[:, :, :, 1]\n        S = torch.sqrt(torch.clamp(M**2 + P**2, min=1e-8))\n\n        if self.power is not None:\n            S = S**self.power\n\n        if self.use_mel:\n            S = torch.matmul(self.mel_basis.to(x), S)\n        if self.do_amp_to_db:\n            S = self._amp_to_db(S, spec_gain=self.spec_gain)\n        return S\n\n    def _build_mel_basis(self):\n        mel_basis = librosa.filters.mel(\n            sr=self.sample_rate,\n            n_fft=self.n_fft,\n            n_mels=self.n_mels,\n            fmin=self.mel_fmin,\n            fmax=self.mel_fmax,\n            htk=self.use_htk,\n            norm=self.mel_norm,\n        )\n        self.mel_basis = torch.from_numpy(mel_basis).float()\n\n    @staticmethod\n    def _amp_to_db(x, spec_gain=1.0):\n        return torch.log(torch.clamp(x, min=1e-5) * spec_gain)\n\n    @staticmethod\n    def _db_to_amp(x, spec_gain=1.0):\n        return torch.exp(x) / spec_gain\n", "TTS/utils/audio/__init__.py": "from TTS.utils.audio.processor import AudioProcessor\n", "TTS/vocoder/__init__.py": "", "TTS/vocoder/models/wavernn.py": "import sys\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom TTS.tts.utils.visual import plot_spectrogram\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.audio.numpy_transforms import mulaw_decode\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.datasets.wavernn_dataset import WaveRNNDataset\nfrom TTS.vocoder.layers.losses import WaveRNNLoss\nfrom TTS.vocoder.models.base_vocoder import BaseVocoder\nfrom TTS.vocoder.utils.distribution import sample_from_discretized_mix_logistic, sample_from_gaussian\n\n\ndef stream(string, variables):\n    sys.stdout.write(f\"\\r{string}\" % variables)\n\n\n# pylint: disable=abstract-method\n# relates https://github.com/pytorch/pytorch/issues/42305\nclass ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        return x + residual\n\n\nclass MelResNet(nn.Module):\n    def __init__(self, num_res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for _ in range(num_res_blocks):\n            self.layers.append(ResBlock(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.batch_norm(x)\n        x = F.relu(x)\n        for f in self.layers:\n            x = f(x)\n        x = self.conv_out(x)\n        return x\n\n\nclass Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n\n\nclass UpsampleNetwork(nn.Module):\n    def __init__(\n        self,\n        feat_dims,\n        upsample_scales,\n        compute_dims,\n        num_res_blocks,\n        res_out_dims,\n        pad,\n        use_aux_net,\n    ):\n        super().__init__()\n        self.total_scale = np.cumproduct(upsample_scales)[-1]\n        self.indent = pad * self.total_scale\n        self.use_aux_net = use_aux_net\n        if use_aux_net:\n            self.resnet = MelResNet(num_res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n            self.resnet_stretch = Stretch2d(self.total_scale, 1)\n        self.up_layers = nn.ModuleList()\n        for scale in upsample_scales:\n            k_size = (1, scale * 2 + 1)\n            padding = (0, scale)\n            stretch = Stretch2d(scale, 1)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1.0 / k_size[1])\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n\n    def forward(self, m):\n        if self.use_aux_net:\n            aux = self.resnet(m).unsqueeze(1)\n            aux = self.resnet_stretch(aux)\n            aux = aux.squeeze(1)\n            aux = aux.transpose(1, 2)\n        else:\n            aux = None\n        m = m.unsqueeze(1)\n        for f in self.up_layers:\n            m = f(m)\n        m = m.squeeze(1)[:, :, self.indent : -self.indent]\n        return m.transpose(1, 2), aux\n\n\nclass Upsample(nn.Module):\n    def __init__(self, scale, pad, num_res_blocks, feat_dims, compute_dims, res_out_dims, use_aux_net):\n        super().__init__()\n        self.scale = scale\n        self.pad = pad\n        self.indent = pad * scale\n        self.use_aux_net = use_aux_net\n        self.resnet = MelResNet(num_res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n\n    def forward(self, m):\n        if self.use_aux_net:\n            aux = self.resnet(m)\n            aux = torch.nn.functional.interpolate(aux, scale_factor=self.scale, mode=\"linear\", align_corners=True)\n            aux = aux.transpose(1, 2)\n        else:\n            aux = None\n        m = torch.nn.functional.interpolate(m, scale_factor=self.scale, mode=\"linear\", align_corners=True)\n        m = m[:, :, self.indent : -self.indent]\n        m = m * 0.045  # empirically found\n\n        return m.transpose(1, 2), aux\n\n\n@dataclass\nclass WavernnArgs(Coqpit):\n    \"\"\"\ud83d\udc38 WaveRNN model arguments.\n\n    rnn_dims (int):\n        Number of hidden channels in RNN layers. Defaults to 512.\n    fc_dims (int):\n        Number of hidden channels in fully-conntected layers. Defaults to 512.\n    compute_dims (int):\n        Number of hidden channels in the feature ResNet. Defaults to 128.\n    res_out_dim (int):\n        Number of hidden channels in the feature ResNet output. Defaults to 128.\n    num_res_blocks (int):\n        Number of residual blocks in the ResNet. Defaults to 10.\n    use_aux_net (bool):\n        enable/disable the feature ResNet. Defaults to True.\n    use_upsample_net (bool):\n        enable/ disable the upsampling networl. If False, basic upsampling is used. Defaults to True.\n    upsample_factors (list):\n        Upsampling factors. The multiply of the values must match the `hop_length`. Defaults to ```[4, 8, 8]```.\n    mode (str):\n        Output mode of the WaveRNN vocoder. `mold` for Mixture of Logistic Distribution, `gauss` for a single\n        Gaussian Distribution and `bits` for quantized bits as the model's output.\n    mulaw (bool):\n        enable / disable the use of Mulaw quantization for training. Only applicable if `mode == 'bits'`. Defaults\n        to `True`.\n    pad (int):\n            Padding applied to the input feature frames against the convolution layers of the feature network.\n            Defaults to 2.\n    \"\"\"\n\n    rnn_dims: int = 512\n    fc_dims: int = 512\n    compute_dims: int = 128\n    res_out_dims: int = 128\n    num_res_blocks: int = 10\n    use_aux_net: bool = True\n    use_upsample_net: bool = True\n    upsample_factors: List[int] = field(default_factory=lambda: [4, 8, 8])\n    mode: str = \"mold\"  # mold [string], gauss [string], bits [int]\n    mulaw: bool = True  # apply mulaw if mode is bits\n    pad: int = 2\n    feat_dims: int = 80\n\n\nclass Wavernn(BaseVocoder):\n    def __init__(self, config: Coqpit):\n        \"\"\"\ud83d\udc38 WaveRNN model.\n        Original paper - https://arxiv.org/abs/1802.08435\n        Official implementation - https://github.com/fatchord/WaveRNN\n\n        Args:\n            config (Coqpit): [description]\n\n        Raises:\n            RuntimeError: [description]\n\n        Examples:\n            >>> from TTS.vocoder.configs import WavernnConfig\n            >>> config = WavernnConfig()\n            >>> model = Wavernn(config)\n\n        Paper Abstract:\n            Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to\n            both estimating the data distribution and generating high-quality samples. Efficient sampling for this\n            class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we\n            describe a set of general techniques for reducing sampling time while maintaining high output quality.\n            We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that\n            matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it\n            possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight\n            pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of\n            parameters, large sparse networks perform better than small dense networks and this relationship holds for\n            sparsity levels beyond 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample\n            high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on\n            subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple\n            samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an\n            orthogonal method for increasing sampling efficiency.\n        \"\"\"\n        super().__init__(config)\n\n        if isinstance(self.args.mode, int):\n            self.n_classes = 2**self.args.mode\n        elif self.args.mode == \"mold\":\n            self.n_classes = 3 * 10\n        elif self.args.mode == \"gauss\":\n            self.n_classes = 2\n        else:\n            raise RuntimeError(\"Unknown model mode value - \", self.args.mode)\n\n        self.ap = AudioProcessor(**config.audio.to_dict())\n        self.aux_dims = self.args.res_out_dims // 4\n\n        if self.args.use_upsample_net:\n            assert (\n                np.cumproduct(self.args.upsample_factors)[-1] == config.audio.hop_length\n            ), \" [!] upsample scales needs to be equal to hop_length\"\n            self.upsample = UpsampleNetwork(\n                self.args.feat_dims,\n                self.args.upsample_factors,\n                self.args.compute_dims,\n                self.args.num_res_blocks,\n                self.args.res_out_dims,\n                self.args.pad,\n                self.args.use_aux_net,\n            )\n        else:\n            self.upsample = Upsample(\n                config.audio.hop_length,\n                self.args.pad,\n                self.args.num_res_blocks,\n                self.args.feat_dims,\n                self.args.compute_dims,\n                self.args.res_out_dims,\n                self.args.use_aux_net,\n            )\n        if self.args.use_aux_net:\n            self.I = nn.Linear(self.args.feat_dims + self.aux_dims + 1, self.args.rnn_dims)\n            self.rnn1 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n            self.rnn2 = nn.GRU(self.args.rnn_dims + self.aux_dims, self.args.rnn_dims, batch_first=True)\n            self.fc1 = nn.Linear(self.args.rnn_dims + self.aux_dims, self.args.fc_dims)\n            self.fc2 = nn.Linear(self.args.fc_dims + self.aux_dims, self.args.fc_dims)\n            self.fc3 = nn.Linear(self.args.fc_dims, self.n_classes)\n        else:\n            self.I = nn.Linear(self.args.feat_dims + 1, self.args.rnn_dims)\n            self.rnn1 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n            self.rnn2 = nn.GRU(self.args.rnn_dims, self.args.rnn_dims, batch_first=True)\n            self.fc1 = nn.Linear(self.args.rnn_dims, self.args.fc_dims)\n            self.fc2 = nn.Linear(self.args.fc_dims, self.args.fc_dims)\n            self.fc3 = nn.Linear(self.args.fc_dims, self.n_classes)\n\n    def forward(self, x, mels):\n        bsize = x.size(0)\n        h1 = torch.zeros(1, bsize, self.args.rnn_dims).to(x.device)\n        h2 = torch.zeros(1, bsize, self.args.rnn_dims).to(x.device)\n        mels, aux = self.upsample(mels)\n\n        if self.args.use_aux_net:\n            aux_idx = [self.aux_dims * i for i in range(5)]\n            a1 = aux[:, :, aux_idx[0] : aux_idx[1]]\n            a2 = aux[:, :, aux_idx[1] : aux_idx[2]]\n            a3 = aux[:, :, aux_idx[2] : aux_idx[3]]\n            a4 = aux[:, :, aux_idx[3] : aux_idx[4]]\n\n        x = (\n            torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n            if self.args.use_aux_net\n            else torch.cat([x.unsqueeze(-1), mels], dim=2)\n        )\n        x = self.I(x)\n        res = x\n        self.rnn1.flatten_parameters()\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2) if self.args.use_aux_net else x\n        self.rnn2.flatten_parameters()\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2) if self.args.use_aux_net else x\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2) if self.args.use_aux_net else x\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def inference(self, mels, batched=None, target=None, overlap=None):\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            if isinstance(mels, np.ndarray):\n                mels = torch.FloatTensor(mels).to(str(next(self.parameters()).device))\n\n            if mels.ndim == 2:\n                mels = mels.unsqueeze(0)\n            wave_len = (mels.size(-1) - 1) * self.config.audio.hop_length\n\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.args.pad, side=\"both\")\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                if aux is not None:\n                    aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            h1 = torch.zeros(b_size, self.args.rnn_dims).type_as(mels)\n            h2 = torch.zeros(b_size, self.args.rnn_dims).type_as(mels)\n            x = torch.zeros(b_size, 1).type_as(mels)\n\n            if self.args.use_aux_net:\n                d = self.aux_dims\n                aux_split = [aux[:, :, d * i : d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n                m_t = mels[:, i, :]\n\n                if self.args.use_aux_net:\n                    a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1) if self.args.use_aux_net else torch.cat([x, m_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1) if self.args.use_aux_net else x\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1) if self.args.use_aux_net else x\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1) if self.args.use_aux_net else x\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.args.mode == \"mold\":\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    x = sample.transpose(0, 1).type_as(mels)\n                elif self.args.mode == \"gauss\":\n                    sample = sample_from_gaussian(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    x = sample.transpose(0, 1).type_as(mels)\n                elif isinstance(self.args.mode, int):\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.0) - 1.0\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(\"Unknown model mode value - \", self.args.mode)\n\n                if i % 100 == 0:\n                    self.gen_display(i, seq_len, b_size, start)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu()\n        if batched:\n            output = output.numpy()\n            output = output.astype(np.float64)\n\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if self.args.mulaw and isinstance(self.args.mode, int):\n            output = mulaw_decode(wav=output, mulaw_qc=self.args.mode)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.config.audio.hop_length)\n        output = output[:wave_len]\n\n        if wave_len > len(fade_out):\n            output[-20 * self.config.audio.hop_length :] *= fade_out\n\n        self.train()\n        return output\n\n    def gen_display(self, i, seq_len, b_size, start):\n        gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n        realtime_ratio = gen_rate * 1000 / self.config.audio.sample_rate\n        stream(\n            \"%i/%i -- batch_size: %i -- gen_rate: %.1f kHz -- x_realtime: %.1f  \",\n            (i * b_size, seq_len * b_size, b_size, gen_rate, realtime_ratio),\n        )\n\n    def fold_with_overlap(self, x, target, overlap):\n        \"\"\"Fold the tensor with overlap for quick batched inference.\n            Overlap will be used for crossfading in xfade_and_unfold()\n        Args:\n            x (tensor)    : Upsampled conditioning features.\n                            shape=(1, timesteps, features)\n            target (int)  : Target timesteps for each index of batch\n            overlap (int) : Timesteps for both xfade and rnn warmup\n        Return:\n            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n        Details:\n            x = [[h1, h2, ... hn]]\n            Where each h is a vector of conditioning features\n            Eg: target=2, overlap=1 with x.size(1)=10\n            folded = [[h1, h2, h3, h4],\n                      [h4, h5, h6, h7],\n                      [h7, h8, h9, h10]]\n        \"\"\"\n\n        _, total_len, features = x.size()\n\n        # Calculate variables needed\n        num_folds = (total_len - overlap) // (target + overlap)\n        extended_len = num_folds * (overlap + target) + overlap\n        remaining = total_len - extended_len\n\n        # Pad if some time steps poking out\n        if remaining != 0:\n            num_folds += 1\n            padding = target + 2 * overlap - remaining\n            x = self.pad_tensor(x, padding, side=\"after\")\n\n        folded = torch.zeros(num_folds, target + 2 * overlap, features).to(x.device)\n\n        # Get the values for the folded tensor\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            folded[i] = x[:, start:end, :]\n\n        return folded\n\n    @staticmethod\n    def get_gru_cell(gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n    @staticmethod\n    def pad_tensor(x, pad, side=\"both\"):\n        # NB - this is just a quick method i need right now\n        # i.e., it won't generalise to other shapes/dims\n        b, t, c = x.size()\n        total = t + 2 * pad if side == \"both\" else t + pad\n        padded = torch.zeros(b, total, c).to(x.device)\n        if side in (\"before\", \"both\"):\n            padded[:, pad : pad + t, :] = x\n        elif side == \"after\":\n            padded[:, :t, :] = x\n        return padded\n\n    @staticmethod\n    def xfade_and_unfold(y, target, overlap):\n        \"\"\"Applies a crossfade and unfolds into a 1d array.\n        Args:\n            y (ndarry)    : Batched sequences of audio samples\n                            shape=(num_folds, target + 2 * overlap)\n                            dtype=np.float64\n            overlap (int) : Timesteps for both xfade and rnn warmup\n        Return:\n            (ndarry) : audio samples in a 1d array\n                       shape=(total_len)\n                       dtype=np.float64\n        Details:\n            y = [[seq1],\n                 [seq2],\n                 [seq3]]\n            Apply a gain envelope at both ends of the sequences\n            y = [[seq1_in, seq1_target, seq1_out],\n                 [seq2_in, seq2_target, seq2_out],\n                 [seq3_in, seq3_target, seq3_out]]\n            Stagger and add up the groups of samples:\n            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n        \"\"\"\n\n        num_folds, length = y.shape\n        target = length - 2 * overlap\n        total_len = num_folds * (target + overlap) + overlap\n\n        # Need some silence for the rnn warmup\n        silence_len = overlap // 2\n        fade_len = overlap - silence_len\n        silence = np.zeros((silence_len), dtype=np.float64)\n\n        # Equal power crossfade\n        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n        fade_in = np.sqrt(0.5 * (1 + t))\n        fade_out = np.sqrt(0.5 * (1 - t))\n\n        # Concat the silence to the fades\n        fade_in = np.concatenate([silence, fade_in])\n        fade_out = np.concatenate([fade_out, silence])\n\n        # Apply the gain to the overlap samples\n        y[:, :overlap] *= fade_in\n        y[:, -overlap:] *= fade_out\n\n        unfolded = np.zeros((total_len), dtype=np.float64)\n\n        # Loop to add up all the samples\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            unfolded[start:end] += y[i]\n\n        return unfolded\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def train_step(self, batch: Dict, criterion: Dict) -> Tuple[Dict, Dict]:\n        mels = batch[\"input\"]\n        waveform = batch[\"waveform\"]\n        waveform_coarse = batch[\"waveform_coarse\"]\n\n        y_hat = self.forward(waveform, mels)\n        if isinstance(self.args.mode, int):\n            y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n        else:\n            waveform_coarse = waveform_coarse.float()\n        waveform_coarse = waveform_coarse.unsqueeze(-1)\n        # compute losses\n        loss_dict = criterion(y_hat, waveform_coarse)\n        return {\"model_output\": y_hat}, loss_dict\n\n    def eval_step(self, batch: Dict, criterion: Dict) -> Tuple[Dict, Dict]:\n        return self.train_step(batch, criterion)\n\n    @torch.no_grad()\n    def test(\n        self, assets: Dict, test_loader: \"DataLoader\", output: Dict  # pylint: disable=unused-argument\n    ) -> Tuple[Dict, Dict]:\n        ap = self.ap\n        figures = {}\n        audios = {}\n        samples = test_loader.dataset.load_test_samples(1)\n        for idx, sample in enumerate(samples):\n            x = torch.FloatTensor(sample[0])\n            x = x.to(next(self.parameters()).device)\n            y_hat = self.inference(x, self.config.batched, self.config.target_samples, self.config.overlap_samples)\n            x_hat = ap.melspectrogram(y_hat)\n            figures.update(\n                {\n                    f\"test_{idx}/ground_truth\": plot_spectrogram(x.T),\n                    f\"test_{idx}/prediction\": plot_spectrogram(x_hat.T),\n                }\n            )\n            audios.update({f\"test_{idx}/audio\": y_hat})\n            # audios.update({f\"real_{idx}/audio\": y_hat})\n        return figures, audios\n\n    def test_log(\n        self, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int  # pylint: disable=unused-argument\n    ) -> Tuple[Dict, np.ndarray]:\n        figures, audios = outputs\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    @staticmethod\n    def format_batch(batch: Dict) -> Dict:\n        waveform = batch[0]\n        mels = batch[1]\n        waveform_coarse = batch[2]\n        return {\"input\": mels, \"waveform\": waveform, \"waveform_coarse\": waveform_coarse}\n\n    def get_data_loader(  # pylint: disable=no-self-use\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: True,\n        samples: List,\n        verbose: bool,\n        num_gpus: int,\n    ):\n        ap = self.ap\n        dataset = WaveRNNDataset(\n            ap=ap,\n            items=samples,\n            seq_len=config.seq_len,\n            hop_len=ap.hop_length,\n            pad=config.model_args.pad,\n            mode=config.model_args.mode,\n            mulaw=config.model_args.mulaw,\n            is_training=not is_eval,\n            verbose=verbose,\n        )\n        sampler = DistributedSampler(dataset, shuffle=True) if num_gpus > 1 else None\n        loader = DataLoader(\n            dataset,\n            batch_size=1 if is_eval else config.batch_size,\n            shuffle=num_gpus == 0,\n            collate_fn=dataset.collate,\n            sampler=sampler,\n            num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n            pin_memory=True,\n        )\n        return loader\n\n    def get_criterion(self):\n        # define train functions\n        return WaveRNNLoss(self.args.mode)\n\n    @staticmethod\n    def init_from_config(config: \"WavernnConfig\"):\n        return Wavernn(config)\n", "TTS/vocoder/models/parallel_wavegan_generator.py": "import math\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.layers.parallel_wavegan import ResidualBlock\nfrom TTS.vocoder.layers.upsample import ConvUpsample\n\n\nclass ParallelWaveganGenerator(torch.nn.Module):\n    \"\"\"PWGAN generator as in https://arxiv.org/pdf/1910.11480.pdf.\n    It is similar to WaveNet with no causal convolution.\n        It is conditioned on an aux feature (spectrogram) to generate\n    an output waveform from an input noise.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_res_blocks=30,\n        stacks=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,\n        aux_channels=80,\n        dropout=0.0,\n        bias=True,\n        use_weight_norm=True,\n        upsample_factors=[4, 4, 4, 4],\n        inference_padding=2,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.aux_channels = aux_channels\n        self.num_res_blocks = num_res_blocks\n        self.stacks = stacks\n        self.kernel_size = kernel_size\n        self.upsample_factors = upsample_factors\n        self.upsample_scale = np.prod(upsample_factors)\n        self.inference_padding = inference_padding\n        self.use_weight_norm = use_weight_norm\n\n        # check the number of layers and stacks\n        assert num_res_blocks % stacks == 0\n        layers_per_stack = num_res_blocks // stacks\n\n        # define first convolution\n        self.first_conv = torch.nn.Conv1d(in_channels, res_channels, kernel_size=1, bias=True)\n\n        # define conv + upsampling network\n        self.upsample_net = ConvUpsample(upsample_factors=upsample_factors)\n\n        # define residual blocks\n        self.conv_layers = torch.nn.ModuleList()\n        for layer in range(num_res_blocks):\n            dilation = 2 ** (layer % layers_per_stack)\n            conv = ResidualBlock(\n                kernel_size=kernel_size,\n                res_channels=res_channels,\n                gate_channels=gate_channels,\n                skip_channels=skip_channels,\n                aux_channels=aux_channels,\n                dilation=dilation,\n                dropout=dropout,\n                bias=bias,\n            )\n            self.conv_layers += [conv]\n\n        # define output layers\n        self.last_conv_layers = torch.nn.ModuleList(\n            [\n                torch.nn.ReLU(inplace=True),\n                torch.nn.Conv1d(skip_channels, skip_channels, kernel_size=1, bias=True),\n                torch.nn.ReLU(inplace=True),\n                torch.nn.Conv1d(skip_channels, out_channels, kernel_size=1, bias=True),\n            ]\n        )\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, c):\n        \"\"\"\n        c: (B, C ,T').\n        o: Output tensor (B, out_channels, T)\n        \"\"\"\n        # random noise\n        x = torch.randn([c.shape[0], 1, c.shape[2] * self.upsample_scale])\n        x = x.to(self.first_conv.bias.device)\n\n        # perform upsampling\n        if c is not None and self.upsample_net is not None:\n            c = self.upsample_net(c)\n            assert (\n                c.shape[-1] == x.shape[-1]\n            ), f\" [!] Upsampling scale does not match the expected output. {c.shape} vs {x.shape}\"\n\n        # encode to hidden representation\n        x = self.first_conv(x)\n        skips = 0\n        for f in self.conv_layers:\n            x, h = f(x, c)\n            skips += h\n        skips *= math.sqrt(1.0 / len(self.conv_layers))\n\n        # apply final layers\n        x = skips\n        for f in self.last_conv_layers:\n            x = f(x)\n\n        return x\n\n    @torch.no_grad()\n    def inference(self, c):\n        c = c.to(self.first_conv.weight.device)\n        c = torch.nn.functional.pad(c, (self.inference_padding, self.inference_padding), \"replicate\")\n        return self.forward(c)\n\n    def remove_weight_norm(self):\n        def _remove_weight_norm(m):\n            try:\n                # print(f\"Weight norm is removed from {m}.\")\n                remove_parametrizations(m, \"weight\")\n            except ValueError:  # this module didn't have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n    def apply_weight_norm(self):\n        def _apply_weight_norm(m):\n            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):\n                torch.nn.utils.parametrizations.weight_norm(m)\n                # print(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n    @staticmethod\n    def _get_receptive_field_size(layers, stacks, kernel_size, dilation=lambda x: 2**x):\n        assert layers % stacks == 0\n        layers_per_cycle = layers // stacks\n        dilations = [dilation(i % layers_per_cycle) for i in range(layers)]\n        return (kernel_size - 1) * sum(dilations) + 1\n\n    @property\n    def receptive_field_size(self):\n        return self._get_receptive_field_size(self.layers, self.stacks, self.kernel_size)\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n            if self.use_weight_norm:\n                self.remove_weight_norm()\n", "TTS/vocoder/models/random_window_discriminator.py": "import numpy as np\nfrom torch import nn\n\n\nclass GBlock(nn.Module):\n    def __init__(self, in_channels, cond_channels, downsample_factor):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.cond_channels = cond_channels\n        self.downsample_factor = downsample_factor\n\n        self.start = nn.Sequential(\n            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n            nn.ReLU(),\n            nn.Conv1d(in_channels, in_channels * 2, kernel_size=3, padding=1),\n        )\n        self.lc_conv1d = nn.Conv1d(cond_channels, in_channels * 2, kernel_size=1)\n        self.end = nn.Sequential(\n            nn.ReLU(), nn.Conv1d(in_channels * 2, in_channels * 2, kernel_size=3, dilation=2, padding=2)\n        )\n        self.residual = nn.Sequential(\n            nn.Conv1d(in_channels, in_channels * 2, kernel_size=1),\n            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n        )\n\n    def forward(self, inputs, conditions):\n        outputs = self.start(inputs) + self.lc_conv1d(conditions)\n        outputs = self.end(outputs)\n        residual_outputs = self.residual(inputs)\n        outputs = outputs + residual_outputs\n\n        return outputs\n\n\nclass DBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample_factor):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.downsample_factor = downsample_factor\n        self.out_channels = out_channels\n\n        self.donwsample_layer = nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n        self.layers = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2),\n        )\n        self.residual = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n        )\n\n    def forward(self, inputs):\n        if self.downsample_factor > 1:\n            outputs = self.layers(self.donwsample_layer(inputs)) + self.donwsample_layer(self.residual(inputs))\n        else:\n            outputs = self.layers(inputs) + self.residual(inputs)\n        return outputs\n\n\nclass ConditionalDiscriminator(nn.Module):\n    def __init__(self, in_channels, cond_channels, downsample_factors=(2, 2, 2), out_channels=(128, 256)):\n        super().__init__()\n\n        assert len(downsample_factors) == len(out_channels) + 1\n\n        self.in_channels = in_channels\n        self.cond_channels = cond_channels\n        self.downsample_factors = downsample_factors\n        self.out_channels = out_channels\n\n        self.pre_cond_layers = nn.ModuleList()\n        self.post_cond_layers = nn.ModuleList()\n\n        # layers before condition features\n        self.pre_cond_layers += [DBlock(in_channels, 64, 1)]\n        in_channels = 64\n        for i, channel in enumerate(out_channels):\n            self.pre_cond_layers.append(DBlock(in_channels, channel, downsample_factors[i]))\n            in_channels = channel\n\n        # condition block\n        self.cond_block = GBlock(in_channels, cond_channels, downsample_factors[-1])\n\n        # layers after condition block\n        self.post_cond_layers += [\n            DBlock(in_channels * 2, in_channels * 2, 1),\n            DBlock(in_channels * 2, in_channels * 2, 1),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(in_channels * 2, 1, kernel_size=1),\n        ]\n\n    def forward(self, inputs, conditions):\n        batch_size = inputs.size()[0]\n        outputs = inputs.view(batch_size, self.in_channels, -1)\n        for layer in self.pre_cond_layers:\n            outputs = layer(outputs)\n        outputs = self.cond_block(outputs, conditions)\n        for layer in self.post_cond_layers:\n            outputs = layer(outputs)\n\n        return outputs\n\n\nclass UnconditionalDiscriminator(nn.Module):\n    def __init__(self, in_channels, base_channels=64, downsample_factors=(8, 4), out_channels=(128, 256)):\n        super().__init__()\n\n        self.downsample_factors = downsample_factors\n        self.in_channels = in_channels\n        self.downsample_factors = downsample_factors\n        self.out_channels = out_channels\n\n        self.layers = nn.ModuleList()\n        self.layers += [DBlock(self.in_channels, base_channels, 1)]\n        in_channels = base_channels\n        for i, factor in enumerate(downsample_factors):\n            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n            in_channels *= 2\n        self.layers += [\n            DBlock(in_channels, in_channels, 1),\n            DBlock(in_channels, in_channels, 1),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(in_channels, 1, kernel_size=1),\n        ]\n\n    def forward(self, inputs):\n        batch_size = inputs.size()[0]\n        outputs = inputs.view(batch_size, self.in_channels, -1)\n        for layer in self.layers:\n            outputs = layer(outputs)\n        return outputs\n\n\nclass RandomWindowDiscriminator(nn.Module):\n    \"\"\"Random Window Discriminator as described in\n    http://arxiv.org/abs/1909.11646\"\"\"\n\n    def __init__(\n        self,\n        cond_channels,\n        hop_length,\n        uncond_disc_donwsample_factors=(8, 4),\n        cond_disc_downsample_factors=((8, 4, 2, 2, 2), (8, 4, 2, 2), (8, 4, 2), (8, 4), (4, 2, 2)),\n        cond_disc_out_channels=((128, 128, 256, 256), (128, 256, 256), (128, 256), (256,), (128, 256)),\n        window_sizes=(512, 1024, 2048, 4096, 8192),\n    ):\n        super().__init__()\n        self.cond_channels = cond_channels\n        self.window_sizes = window_sizes\n        self.hop_length = hop_length\n        self.base_window_size = self.hop_length * 2\n        self.ks = [ws // self.base_window_size for ws in window_sizes]\n\n        # check arguments\n        assert len(cond_disc_downsample_factors) == len(cond_disc_out_channels) == len(window_sizes)\n        for ws in window_sizes:\n            assert ws % hop_length == 0\n\n        for idx, cf in enumerate(cond_disc_downsample_factors):\n            assert np.prod(cf) == hop_length // self.ks[idx]\n\n        # define layers\n        self.unconditional_discriminators = nn.ModuleList([])\n        for k in self.ks:\n            layer = UnconditionalDiscriminator(\n                in_channels=k, base_channels=64, downsample_factors=uncond_disc_donwsample_factors\n            )\n            self.unconditional_discriminators.append(layer)\n\n        self.conditional_discriminators = nn.ModuleList([])\n        for idx, k in enumerate(self.ks):\n            layer = ConditionalDiscriminator(\n                in_channels=k,\n                cond_channels=cond_channels,\n                downsample_factors=cond_disc_downsample_factors[idx],\n                out_channels=cond_disc_out_channels[idx],\n            )\n            self.conditional_discriminators.append(layer)\n\n    def forward(self, x, c):\n        scores = []\n        feats = []\n        # unconditional pass\n        for window_size, layer in zip(self.window_sizes, self.unconditional_discriminators):\n            index = np.random.randint(x.shape[-1] - window_size)\n\n            score = layer(x[:, :, index : index + window_size])\n            scores.append(score)\n\n        # conditional pass\n        for window_size, layer in zip(self.window_sizes, self.conditional_discriminators):\n            frame_size = window_size // self.hop_length\n            lc_index = np.random.randint(c.shape[-1] - frame_size)\n            sample_index = lc_index * self.hop_length\n            x_sub = x[:, :, sample_index : (lc_index + frame_size) * self.hop_length]\n            c_sub = c[:, :, lc_index : lc_index + frame_size]\n\n            score = layer(x_sub, c_sub)\n            scores.append(score)\n        return scores, feats\n", "TTS/vocoder/models/melgan_generator.py": "import torch\nfrom torch import nn\nfrom torch.nn.utils.parametrizations import weight_norm\n\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.layers.melgan import ResidualStack\n\n\nclass MelganGenerator(nn.Module):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        proj_kernel=7,\n        base_channels=512,\n        upsample_factors=(8, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=3,\n    ):\n        super().__init__()\n\n        # assert model parameters\n        assert (proj_kernel - 1) % 2 == 0, \" [!] proj_kernel should be an odd number.\"\n\n        # setup additional model parameters\n        base_padding = (proj_kernel - 1) // 2\n        act_slope = 0.2\n        self.inference_padding = 2\n\n        # initial layer\n        layers = []\n        layers += [\n            nn.ReflectionPad1d(base_padding),\n            weight_norm(nn.Conv1d(in_channels, base_channels, kernel_size=proj_kernel, stride=1, bias=True)),\n        ]\n\n        # upsampling layers and residual stacks\n        for idx, upsample_factor in enumerate(upsample_factors):\n            layer_in_channels = base_channels // (2**idx)\n            layer_out_channels = base_channels // (2 ** (idx + 1))\n            layer_filter_size = upsample_factor * 2\n            layer_stride = upsample_factor\n            layer_output_padding = upsample_factor % 2\n            layer_padding = upsample_factor // 2 + layer_output_padding\n            layers += [\n                nn.LeakyReLU(act_slope),\n                weight_norm(\n                    nn.ConvTranspose1d(\n                        layer_in_channels,\n                        layer_out_channels,\n                        layer_filter_size,\n                        stride=layer_stride,\n                        padding=layer_padding,\n                        output_padding=layer_output_padding,\n                        bias=True,\n                    )\n                ),\n                ResidualStack(channels=layer_out_channels, num_res_blocks=num_res_blocks, kernel_size=res_kernel),\n            ]\n\n        layers += [nn.LeakyReLU(act_slope)]\n\n        # final layer\n        layers += [\n            nn.ReflectionPad1d(base_padding),\n            weight_norm(nn.Conv1d(layer_out_channels, out_channels, proj_kernel, stride=1, bias=True)),\n            nn.Tanh(),\n        ]\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, c):\n        return self.layers(c)\n\n    def inference(self, c):\n        c = c.to(self.layers[1].weight.device)\n        c = torch.nn.functional.pad(c, (self.inference_padding, self.inference_padding), \"replicate\")\n        return self.layers(c)\n\n    def remove_weight_norm(self):\n        for _, layer in enumerate(self.layers):\n            if len(layer.state_dict()) != 0:\n                try:\n                    nn.utils.parametrize.remove_parametrizations(layer, \"weight\")\n                except ValueError:\n                    layer.remove_weight_norm()\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n            self.remove_weight_norm()\n", "TTS/vocoder/models/wavegrad.py": "from dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.datasets import WaveGradDataset\nfrom TTS.vocoder.layers.wavegrad import Conv1d, DBlock, FiLM, UBlock\nfrom TTS.vocoder.models.base_vocoder import BaseVocoder\nfrom TTS.vocoder.utils.generic_utils import plot_results\n\n\n@dataclass\nclass WavegradArgs(Coqpit):\n    in_channels: int = 80\n    out_channels: int = 1\n    use_weight_norm: bool = False\n    y_conv_channels: int = 32\n    x_conv_channels: int = 768\n    dblock_out_channels: List[int] = field(default_factory=lambda: [128, 128, 256, 512])\n    ublock_out_channels: List[int] = field(default_factory=lambda: [512, 512, 256, 128, 128])\n    upsample_factors: List[int] = field(default_factory=lambda: [4, 4, 4, 2, 2])\n    upsample_dilations: List[List[int]] = field(\n        default_factory=lambda: [[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 4, 8], [1, 2, 4, 8], [1, 2, 4, 8]]\n    )\n\n\nclass Wavegrad(BaseVocoder):\n    \"\"\"\ud83d\udc38 \ud83c\udf0a WaveGrad \ud83c\udf0a model.\n    Paper - https://arxiv.org/abs/2009.00713\n\n    Examples:\n        Initializing the model.\n\n        >>> from TTS.vocoder.configs import WavegradConfig\n        >>> config = WavegradConfig()\n        >>> model = Wavegrad(config)\n\n    Paper Abstract:\n        This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the\n        data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts\n        from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned\n        on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting\n        the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in\n        terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations.\n        Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive\n        baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations.\n        Audio samples are available at this https URL.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(self, config: Coqpit):\n        super().__init__(config)\n        self.config = config\n        self.use_weight_norm = config.model_params.use_weight_norm\n        self.hop_len = np.prod(config.model_params.upsample_factors)\n        self.noise_level = None\n        self.num_steps = None\n        self.beta = None\n        self.alpha = None\n        self.alpha_hat = None\n        self.c1 = None\n        self.c2 = None\n        self.sigma = None\n\n        # dblocks\n        self.y_conv = Conv1d(1, config.model_params.y_conv_channels, 5, padding=2)\n        self.dblocks = nn.ModuleList([])\n        ic = config.model_params.y_conv_channels\n        for oc, df in zip(config.model_params.dblock_out_channels, reversed(config.model_params.upsample_factors)):\n            self.dblocks.append(DBlock(ic, oc, df))\n            ic = oc\n\n        # film\n        self.film = nn.ModuleList([])\n        ic = config.model_params.y_conv_channels\n        for oc in reversed(config.model_params.ublock_out_channels):\n            self.film.append(FiLM(ic, oc))\n            ic = oc\n\n        # ublocksn\n        self.ublocks = nn.ModuleList([])\n        ic = config.model_params.x_conv_channels\n        for oc, uf, ud in zip(\n            config.model_params.ublock_out_channels,\n            config.model_params.upsample_factors,\n            config.model_params.upsample_dilations,\n        ):\n            self.ublocks.append(UBlock(ic, oc, uf, ud))\n            ic = oc\n\n        self.x_conv = Conv1d(config.model_params.in_channels, config.model_params.x_conv_channels, 3, padding=1)\n        self.out_conv = Conv1d(oc, config.model_params.out_channels, 3, padding=1)\n\n        if config.model_params.use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, x, spectrogram, noise_scale):\n        shift_and_scale = []\n\n        x = self.y_conv(x)\n        shift_and_scale.append(self.film[0](x, noise_scale))\n\n        for film, layer in zip(self.film[1:], self.dblocks):\n            x = layer(x)\n            shift_and_scale.append(film(x, noise_scale))\n\n        x = self.x_conv(spectrogram)\n        for layer, (film_shift, film_scale) in zip(self.ublocks, reversed(shift_and_scale)):\n            x = layer(x, film_shift, film_scale)\n        x = self.out_conv(x)\n        return x\n\n    def load_noise_schedule(self, path):\n        beta = np.load(path, allow_pickle=True).item()[\"beta\"]  # pylint: disable=unexpected-keyword-arg\n        self.compute_noise_level(beta)\n\n    @torch.no_grad()\n    def inference(self, x, y_n=None):\n        \"\"\"\n        Shapes:\n            x: :math:`[B, C , T]`\n            y_n: :math:`[B, 1, T]`\n        \"\"\"\n        if y_n is None:\n            y_n = torch.randn(x.shape[0], 1, self.hop_len * x.shape[-1])\n        else:\n            y_n = torch.FloatTensor(y_n).unsqueeze(0).unsqueeze(0)\n        y_n = y_n.type_as(x)\n        sqrt_alpha_hat = self.noise_level.to(x)\n        for n in range(len(self.alpha) - 1, -1, -1):\n            y_n = self.c1[n] * (y_n - self.c2[n] * self.forward(y_n, x, sqrt_alpha_hat[n].repeat(x.shape[0])))\n            if n > 0:\n                z = torch.randn_like(y_n)\n                y_n += self.sigma[n - 1] * z\n            y_n.clamp_(-1.0, 1.0)\n        return y_n\n\n    def compute_y_n(self, y_0):\n        \"\"\"Compute noisy audio based on noise schedule\"\"\"\n        self.noise_level = self.noise_level.to(y_0)\n        if len(y_0.shape) == 3:\n            y_0 = y_0.squeeze(1)\n        s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])\n        l_a, l_b = self.noise_level[s], self.noise_level[s + 1]\n        noise_scale = l_a + torch.rand(y_0.shape[0]).to(y_0) * (l_b - l_a)\n        noise_scale = noise_scale.unsqueeze(1)\n        noise = torch.randn_like(y_0)\n        noisy_audio = noise_scale * y_0 + (1.0 - noise_scale**2) ** 0.5 * noise\n        return noise.unsqueeze(1), noisy_audio.unsqueeze(1), noise_scale[:, 0]\n\n    def compute_noise_level(self, beta):\n        \"\"\"Compute noise schedule parameters\"\"\"\n        self.num_steps = len(beta)\n        alpha = 1 - beta\n        alpha_hat = np.cumprod(alpha)\n        noise_level = np.concatenate([[1.0], alpha_hat**0.5], axis=0)\n        noise_level = alpha_hat**0.5\n\n        # pylint: disable=not-callable\n        self.beta = torch.tensor(beta.astype(np.float32))\n        self.alpha = torch.tensor(alpha.astype(np.float32))\n        self.alpha_hat = torch.tensor(alpha_hat.astype(np.float32))\n        self.noise_level = torch.tensor(noise_level.astype(np.float32))\n\n        self.c1 = 1 / self.alpha**0.5\n        self.c2 = (1 - self.alpha) / (1 - self.alpha_hat) ** 0.5\n        self.sigma = ((1.0 - self.alpha_hat[:-1]) / (1.0 - self.alpha_hat[1:]) * self.beta[1:]) ** 0.5\n\n    def remove_weight_norm(self):\n        for _, layer in enumerate(self.dblocks):\n            if len(layer.state_dict()) != 0:\n                try:\n                    remove_parametrizations(layer, \"weight\")\n                except ValueError:\n                    layer.remove_weight_norm()\n\n        for _, layer in enumerate(self.film):\n            if len(layer.state_dict()) != 0:\n                try:\n                    remove_parametrizations(layer, \"weight\")\n                except ValueError:\n                    layer.remove_weight_norm()\n\n        for _, layer in enumerate(self.ublocks):\n            if len(layer.state_dict()) != 0:\n                try:\n                    remove_parametrizations(layer, \"weight\")\n                except ValueError:\n                    layer.remove_weight_norm()\n\n        remove_parametrizations(self.x_conv, \"weight\")\n        remove_parametrizations(self.out_conv, \"weight\")\n        remove_parametrizations(self.y_conv, \"weight\")\n\n    def apply_weight_norm(self):\n        for _, layer in enumerate(self.dblocks):\n            if len(layer.state_dict()) != 0:\n                layer.apply_weight_norm()\n\n        for _, layer in enumerate(self.film):\n            if len(layer.state_dict()) != 0:\n                layer.apply_weight_norm()\n\n        for _, layer in enumerate(self.ublocks):\n            if len(layer.state_dict()) != 0:\n                layer.apply_weight_norm()\n\n        self.x_conv = weight_norm(self.x_conv)\n        self.out_conv = weight_norm(self.out_conv)\n        self.y_conv = weight_norm(self.y_conv)\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n            if self.config.model_params.use_weight_norm:\n                self.remove_weight_norm()\n            betas = np.linspace(\n                config[\"test_noise_schedule\"][\"min_val\"],\n                config[\"test_noise_schedule\"][\"max_val\"],\n                config[\"test_noise_schedule\"][\"num_steps\"],\n            )\n            self.compute_noise_level(betas)\n        else:\n            betas = np.linspace(\n                config[\"train_noise_schedule\"][\"min_val\"],\n                config[\"train_noise_schedule\"][\"max_val\"],\n                config[\"train_noise_schedule\"][\"num_steps\"],\n            )\n            self.compute_noise_level(betas)\n\n    def train_step(self, batch: Dict, criterion: Dict) -> Tuple[Dict, Dict]:\n        # format data\n        x = batch[\"input\"]\n        y = batch[\"waveform\"]\n\n        # set noise scale\n        noise, x_noisy, noise_scale = self.compute_y_n(y)\n\n        # forward pass\n        noise_hat = self.forward(x_noisy, x, noise_scale)\n\n        # compute losses\n        loss = criterion(noise, noise_hat)\n        return {\"model_output\": noise_hat}, {\"loss\": loss}\n\n    def train_log(  # pylint: disable=no-self-use\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int  # pylint: disable=unused-argument\n    ) -> Tuple[Dict, np.ndarray]:\n        pass\n\n    @torch.no_grad()\n    def eval_step(self, batch: Dict, criterion: nn.Module) -> Tuple[Dict, Dict]:\n        return self.train_step(batch, criterion)\n\n    def eval_log(  # pylint: disable=no-self-use\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        pass\n\n    def test(self, assets: Dict, test_loader: \"DataLoader\", outputs=None):  # pylint: disable=unused-argument\n        # setup noise schedule and inference\n        ap = assets[\"audio_processor\"]\n        noise_schedule = self.config[\"test_noise_schedule\"]\n        betas = np.linspace(noise_schedule[\"min_val\"], noise_schedule[\"max_val\"], noise_schedule[\"num_steps\"])\n        self.compute_noise_level(betas)\n        samples = test_loader.dataset.load_test_samples(1)\n        for sample in samples:\n            x = sample[0]\n            x = x[None, :, :].to(next(self.parameters()).device)\n            y = sample[1]\n            y = y[None, :]\n            # compute voice\n            y_pred = self.inference(x)\n            # compute spectrograms\n            figures = plot_results(y_pred, y, ap, \"test\")\n            # Sample audio\n            sample_voice = y_pred[0].squeeze(0).detach().cpu().numpy()\n        return figures, {\"test/audio\": sample_voice}\n\n    def get_optimizer(self):\n        return get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr, self)\n\n    def get_scheduler(self, optimizer):\n        return get_scheduler(self.config.lr_scheduler, self.config.lr_scheduler_params, optimizer)\n\n    @staticmethod\n    def get_criterion():\n        return torch.nn.L1Loss()\n\n    @staticmethod\n    def format_batch(batch: Dict) -> Dict:\n        # return a whole audio segment\n        m, y = batch[0], batch[1]\n        y = y.unsqueeze(1)\n        return {\"input\": m, \"waveform\": y}\n\n    def get_data_loader(self, config: Coqpit, assets: Dict, is_eval: True, samples: List, verbose: bool, num_gpus: int):\n        ap = assets[\"audio_processor\"]\n        dataset = WaveGradDataset(\n            ap=ap,\n            items=samples,\n            seq_len=self.config.seq_len,\n            hop_len=ap.hop_length,\n            pad_short=self.config.pad_short,\n            conv_pad=self.config.conv_pad,\n            is_training=not is_eval,\n            return_segments=True,\n            use_noise_augment=False,\n            use_cache=config.use_cache,\n            verbose=verbose,\n        )\n        sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        loader = DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            shuffle=num_gpus <= 1,\n            drop_last=False,\n            sampler=sampler,\n            num_workers=self.config.num_eval_loader_workers if is_eval else self.config.num_loader_workers,\n            pin_memory=False,\n        )\n        return loader\n\n    def on_epoch_start(self, trainer):  # pylint: disable=unused-argument\n        noise_schedule = self.config[\"train_noise_schedule\"]\n        betas = np.linspace(noise_schedule[\"min_val\"], noise_schedule[\"max_val\"], noise_schedule[\"num_steps\"])\n        self.compute_noise_level(betas)\n\n    @staticmethod\n    def init_from_config(config: \"WavegradConfig\"):\n        return Wavegrad(config)\n", "TTS/vocoder/models/univnet_generator.py": "from typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils import parametrize\n\nfrom TTS.vocoder.layers.lvc_block import LVCBlock\n\nLRELU_SLOPE = 0.1\n\n\nclass UnivnetGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        cond_channels: int,\n        upsample_factors: List[int],\n        lvc_layers_each_block: int,\n        lvc_kernel_size: int,\n        kpnet_hidden_channels: int,\n        kpnet_conv_size: int,\n        dropout: float,\n        use_weight_norm=True,\n    ):\n        \"\"\"Univnet Generator network.\n\n        Paper: https://arxiv.org/pdf/2106.07889.pdf\n\n        Args:\n            in_channels (int): Number of input tensor channels.\n            out_channels (int): Number of channels of the output tensor.\n            hidden_channels (int): Number of hidden network channels.\n            cond_channels (int): Number of channels of the conditioning tensors.\n            upsample_factors (List[int]): List of uplsample factors for the upsampling layers.\n            lvc_layers_each_block (int): Number of LVC layers in each block.\n            lvc_kernel_size (int): Kernel size of the LVC layers.\n            kpnet_hidden_channels (int): Number of hidden channels in the key-point network.\n            kpnet_conv_size (int): Number of convolution channels in the key-point network.\n            dropout (float): Dropout rate.\n            use_weight_norm (bool, optional): Enable/disable weight norm. Defaults to True.\n        \"\"\"\n\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cond_channels = cond_channels\n        self.upsample_scale = np.prod(upsample_factors)\n        self.lvc_block_nums = len(upsample_factors)\n\n        # define first convolution\n        self.first_conv = torch.nn.Conv1d(\n            in_channels, hidden_channels, kernel_size=7, padding=(7 - 1) // 2, dilation=1, bias=True\n        )\n\n        # define residual blocks\n        self.lvc_blocks = torch.nn.ModuleList()\n        cond_hop_length = 1\n        for n in range(self.lvc_block_nums):\n            cond_hop_length = cond_hop_length * upsample_factors[n]\n            lvcb = LVCBlock(\n                in_channels=hidden_channels,\n                cond_channels=cond_channels,\n                upsample_ratio=upsample_factors[n],\n                conv_layers=lvc_layers_each_block,\n                conv_kernel_size=lvc_kernel_size,\n                cond_hop_length=cond_hop_length,\n                kpnet_hidden_channels=kpnet_hidden_channels,\n                kpnet_conv_size=kpnet_conv_size,\n                kpnet_dropout=dropout,\n            )\n            self.lvc_blocks += [lvcb]\n\n        # define output layers\n        self.last_conv_layers = torch.nn.ModuleList(\n            [\n                torch.nn.Conv1d(\n                    hidden_channels, out_channels, kernel_size=7, padding=(7 - 1) // 2, dilation=1, bias=True\n                ),\n            ]\n        )\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, c):\n        \"\"\"Calculate forward propagation.\n        Args:\n            c (Tensor): Local conditioning auxiliary features (B, C ,T').\n        Returns:\n            Tensor: Output tensor (B, out_channels, T)\n        \"\"\"\n        # random noise\n        x = torch.randn([c.shape[0], self.in_channels, c.shape[2]])\n        x = x.to(self.first_conv.bias.device)\n        x = self.first_conv(x)\n\n        for n in range(self.lvc_block_nums):\n            x = self.lvc_blocks[n](x, c)\n\n        # apply final layers\n        for f in self.last_conv_layers:\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            x = f(x)\n        x = torch.tanh(x)\n        return x\n\n    def remove_weight_norm(self):\n        \"\"\"Remove weight normalization module from all of the layers.\"\"\"\n\n        def _remove_weight_norm(m):\n            try:\n                # print(f\"Weight norm is removed from {m}.\")\n                parametrize.remove_parametrizations(m, \"weight\")\n            except ValueError:  # this module didn't have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):\n                torch.nn.utils.parametrizations.weight_norm(m)\n                # print(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n    @staticmethod\n    def _get_receptive_field_size(layers, stacks, kernel_size, dilation=lambda x: 2**x):\n        assert layers % stacks == 0\n        layers_per_cycle = layers // stacks\n        dilations = [dilation(i % layers_per_cycle) for i in range(layers)]\n        return (kernel_size - 1) * sum(dilations) + 1\n\n    @property\n    def receptive_field_size(self):\n        \"\"\"Return receptive field size.\"\"\"\n        return self._get_receptive_field_size(self.layers, self.stacks, self.kernel_size)\n\n    @torch.no_grad()\n    def inference(self, c):\n        \"\"\"Perform inference.\n        Args:\n            c (Tensor): Local conditioning auxiliary features :math:`(B, C, T)`.\n        Returns:\n            Tensor: Output tensor (T, out_channels)\n        \"\"\"\n        x = torch.randn([c.shape[0], self.in_channels, c.shape[2]])\n        x = x.to(self.first_conv.bias.device)\n\n        c = c.to(next(self.parameters()))\n        return self.forward(c)\n", "TTS/vocoder/models/base_vocoder.py": "from coqpit import Coqpit\n\nfrom TTS.model import BaseTrainerModel\n\n# pylint: skip-file\n\n\nclass BaseVocoder(BaseTrainerModel):\n    \"\"\"Base `vocoder` class. Every new `vocoder` model must inherit this.\n\n    It defines `vocoder` specific functions on top of `Model`.\n\n    Notes on input/output tensor shapes:\n        Any input or output tensor of the model must be shaped as\n\n        - 3D tensors `batch x time x channels`\n        - 2D tensors `batch x channels`\n        - 1D tensors `batch x 1`\n    \"\"\"\n\n    MODEL_TYPE = \"vocoder\"\n\n    def __init__(self, config):\n        super().__init__()\n        self._set_model_args(config)\n\n    def _set_model_args(self, config: Coqpit):\n        \"\"\"Setup model args based on the config type.\n\n        If the config is for training with a name like \"*Config\", then the model args are embeded in the\n        config.model_args\n\n        If the config is for the model with a name like \"*Args\", then we assign the directly.\n        \"\"\"\n        # don't use isintance not to import recursively\n        if \"Config\" in config.__class__.__name__:\n            if \"characters\" in config:\n                _, self.config, num_chars = self.get_characters(config)\n                self.config.num_chars = num_chars\n                if hasattr(self.config, \"model_args\"):\n                    config.model_args.num_chars = num_chars\n                    if \"model_args\" in config:\n                        self.args = self.config.model_args\n                    # This is for backward compatibility\n                    if \"model_params\" in config:\n                        self.args = self.config.model_params\n            else:\n                self.config = config\n                if \"model_args\" in config:\n                    self.args = self.config.model_args\n                # This is for backward compatibility\n                if \"model_params\" in config:\n                    self.args = self.config.model_params\n        else:\n            raise ValueError(\"config must be either a *Config or *Args\")\n", "TTS/vocoder/models/melgan_multiscale_discriminator.py": "from torch import nn\n\nfrom TTS.vocoder.models.melgan_discriminator import MelganDiscriminator\n\n\nclass MelganMultiscaleDiscriminator(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        num_scales=3,\n        kernel_sizes=(5, 3),\n        base_channels=16,\n        max_channels=1024,\n        downsample_factors=(4, 4, 4),\n        pooling_kernel_size=4,\n        pooling_stride=2,\n        pooling_padding=2,\n        groups_denominator=4,\n    ):\n        super().__init__()\n\n        self.discriminators = nn.ModuleList(\n            [\n                MelganDiscriminator(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    kernel_sizes=kernel_sizes,\n                    base_channels=base_channels,\n                    max_channels=max_channels,\n                    downsample_factors=downsample_factors,\n                    groups_denominator=groups_denominator,\n                )\n                for _ in range(num_scales)\n            ]\n        )\n\n        self.pooling = nn.AvgPool1d(\n            kernel_size=pooling_kernel_size, stride=pooling_stride, padding=pooling_padding, count_include_pad=False\n        )\n\n    def forward(self, x):\n        scores = []\n        feats = []\n        for disc in self.discriminators:\n            score, feat = disc(x)\n            scores.append(score)\n            feats.append(feat)\n            x = self.pooling(x)\n        return scores, feats\n", "TTS/vocoder/models/parallel_wavegan_discriminator.py": "import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nfrom TTS.vocoder.layers.parallel_wavegan import ResidualBlock\n\n\nclass ParallelWaveganDiscriminator(nn.Module):\n    \"\"\"PWGAN discriminator as in https://arxiv.org/abs/1910.11480.\n    It classifies each audio window real/fake and returns a sequence\n    of predictions.\n        It is a stack of convolutional blocks with dilation.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_layers=10,\n        conv_channels=64,\n        dilation_factor=1,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.2},\n        bias=True,\n    ):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0, \" [!] does not support even number kernel size.\"\n        assert dilation_factor > 0, \" [!] dilation factor must be > 0.\"\n        self.conv_layers = nn.ModuleList()\n        conv_in_channels = in_channels\n        for i in range(num_layers - 1):\n            if i == 0:\n                dilation = 1\n            else:\n                dilation = i if dilation_factor == 1 else dilation_factor**i\n                conv_in_channels = conv_channels\n            padding = (kernel_size - 1) // 2 * dilation\n            conv_layer = [\n                nn.Conv1d(\n                    conv_in_channels,\n                    conv_channels,\n                    kernel_size=kernel_size,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias,\n                ),\n                getattr(nn, nonlinear_activation)(inplace=True, **nonlinear_activation_params),\n            ]\n            self.conv_layers += conv_layer\n        padding = (kernel_size - 1) // 2\n        last_conv_layer = nn.Conv1d(conv_in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n        self.conv_layers += [last_conv_layer]\n        self.apply_weight_norm()\n\n    def forward(self, x):\n        \"\"\"\n            x : (B, 1, T).\n        Returns:\n            Tensor: (B, 1, T)\n        \"\"\"\n        for f in self.conv_layers:\n            x = f(x)\n        return x\n\n    def apply_weight_norm(self):\n        def _apply_weight_norm(m):\n            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):\n                torch.nn.utils.parametrizations.weight_norm(m)\n\n        self.apply(_apply_weight_norm)\n\n    def remove_weight_norm(self):\n        def _remove_weight_norm(m):\n            try:\n                # print(f\"Weight norm is removed from {m}.\")\n                remove_parametrizations(m, \"weight\")\n            except ValueError:  # this module didn't have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n\nclass ResidualParallelWaveganDiscriminator(nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_layers=30,\n        stacks=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,\n        dropout=0.0,\n        bias=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.2},\n    ):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0, \"Not support even number kernel size.\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_layers = num_layers\n        self.stacks = stacks\n        self.kernel_size = kernel_size\n        self.res_factor = math.sqrt(1.0 / num_layers)\n\n        # check the number of num_layers and stacks\n        assert num_layers % stacks == 0\n        layers_per_stack = num_layers // stacks\n\n        # define first convolution\n        self.first_conv = nn.Sequential(\n            nn.Conv1d(in_channels, res_channels, kernel_size=1, padding=0, dilation=1, bias=True),\n            getattr(nn, nonlinear_activation)(inplace=True, **nonlinear_activation_params),\n        )\n\n        # define residual blocks\n        self.conv_layers = nn.ModuleList()\n        for layer in range(num_layers):\n            dilation = 2 ** (layer % layers_per_stack)\n            conv = ResidualBlock(\n                kernel_size=kernel_size,\n                res_channels=res_channels,\n                gate_channels=gate_channels,\n                skip_channels=skip_channels,\n                aux_channels=-1,\n                dilation=dilation,\n                dropout=dropout,\n                bias=bias,\n                use_causal_conv=False,\n            )\n            self.conv_layers += [conv]\n\n        # define output layers\n        self.last_conv_layers = nn.ModuleList(\n            [\n                getattr(nn, nonlinear_activation)(inplace=True, **nonlinear_activation_params),\n                nn.Conv1d(skip_channels, skip_channels, kernel_size=1, padding=0, dilation=1, bias=True),\n                getattr(nn, nonlinear_activation)(inplace=True, **nonlinear_activation_params),\n                nn.Conv1d(skip_channels, out_channels, kernel_size=1, padding=0, dilation=1, bias=True),\n            ]\n        )\n\n        # apply weight norm\n        self.apply_weight_norm()\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, 1, T).\n        \"\"\"\n        x = self.first_conv(x)\n\n        skips = 0\n        for f in self.conv_layers:\n            x, h = f(x, None)\n            skips += h\n        skips *= self.res_factor\n\n        # apply final layers\n        x = skips\n        for f in self.last_conv_layers:\n            x = f(x)\n        return x\n\n    def apply_weight_norm(self):\n        def _apply_weight_norm(m):\n            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):\n                torch.nn.utils.parametrizations.weight_norm(m)\n\n        self.apply(_apply_weight_norm)\n\n    def remove_weight_norm(self):\n        def _remove_weight_norm(m):\n            try:\n                print(f\"Weight norm is removed from {m}.\")\n                remove_parametrizations(m, \"weight\")\n            except ValueError:  # this module didn't have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n", "TTS/vocoder/models/hifigan_generator.py": "# adopted from https://github.com/jik876/hifi-gan/blob/master/models.py\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nfrom TTS.utils.io import load_fsspec\n\nLRELU_SLOPE = 0.1\n\n\ndef get_padding(k, d):\n    return int((k * d - d) / 2)\n\n\nclass ResBlock1(torch.nn.Module):\n    \"\"\"Residual Block Type 1. It has 3 convolutional layers in each convolutional block.\n\n    Network::\n\n        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o\n        |--------------------------------------------------------------------------------------------------|\n\n\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super().__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n            ]\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input tensor.\n        Returns:\n            Tensor: output tensor.\n        Shapes:\n            x: [B, C, T]\n        \"\"\"\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_parametrizations(l, \"weight\")\n        for l in self.convs2:\n            remove_parametrizations(l, \"weight\")\n\n\nclass ResBlock2(torch.nn.Module):\n    \"\"\"Residual Block Type 2. It has 1 convolutional layers in each convolutional block.\n\n    Network::\n\n        x -> lrelu -> conv1-> -> z -> lrelu -> conv2-> o -> + -> o\n        |---------------------------------------------------|\n\n\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super().__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_parametrizations(l, \"weight\")\n\n\nclass HifiganGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        resblock_type,\n        resblock_dilation_sizes,\n        resblock_kernel_sizes,\n        upsample_kernel_sizes,\n        upsample_initial_channel,\n        upsample_factors,\n        inference_padding=5,\n        cond_channels=0,\n        conv_pre_weight_norm=True,\n        conv_post_weight_norm=True,\n        conv_post_bias=True,\n    ):\n        r\"\"\"HiFiGAN Generator with Multi-Receptive Field Fusion (MRF)\n\n        Network:\n            x -> lrelu -> upsampling_layer -> resblock1_k1x1 -> z1 -> + -> z_sum / #resblocks -> lrelu -> conv_post_7x1 -> tanh -> o\n                                                 ..          -> zI ---|\n                                              resblockN_kNx1 -> zN ---'\n\n        Args:\n            in_channels (int): number of input tensor channels.\n            out_channels (int): number of output tensor channels.\n            resblock_type (str): type of the `ResBlock`. '1' or '2'.\n            resblock_dilation_sizes (List[List[int]]): list of dilation values in each layer of a `ResBlock`.\n            resblock_kernel_sizes (List[int]): list of kernel sizes for each `ResBlock`.\n            upsample_kernel_sizes (List[int]): list of kernel sizes for each transposed convolution.\n            upsample_initial_channel (int): number of channels for the first upsampling layer. This is divided by 2\n                for each consecutive upsampling layer.\n            upsample_factors (List[int]): upsampling factors (stride) for each upsampling layer.\n            inference_padding (int): constant padding applied to the input at inference time. Defaults to 5.\n        \"\"\"\n        super().__init__()\n        self.inference_padding = inference_padding\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_factors)\n        # initial upsampling layers\n        self.conv_pre = weight_norm(Conv1d(in_channels, upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if resblock_type == \"1\" else ResBlock2\n        # upsampling layers\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_factors, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n        # MRF blocks\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for _, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n        # post convolution layer\n        self.conv_post = weight_norm(Conv1d(ch, out_channels, 7, 1, padding=3, bias=conv_post_bias))\n        if cond_channels > 0:\n            self.cond_layer = nn.Conv1d(cond_channels, upsample_initial_channel, 1)\n\n        if not conv_pre_weight_norm:\n            remove_parametrizations(self.conv_pre, \"weight\")\n\n        if not conv_post_weight_norm:\n            remove_parametrizations(self.conv_post, \"weight\")\n\n    def forward(self, x, g=None):\n        \"\"\"\n        Args:\n            x (Tensor): feature input tensor.\n            g (Tensor): global conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n        o = self.conv_pre(x)\n        if hasattr(self, \"cond_layer\"):\n            o = o + self.cond_layer(g)\n        for i in range(self.num_upsamples):\n            o = F.leaky_relu(o, LRELU_SLOPE)\n            o = self.ups[i](o)\n            z_sum = None\n            for j in range(self.num_kernels):\n                if z_sum is None:\n                    z_sum = self.resblocks[i * self.num_kernels + j](o)\n                else:\n                    z_sum += self.resblocks[i * self.num_kernels + j](o)\n            o = z_sum / self.num_kernels\n        o = F.leaky_relu(o)\n        o = self.conv_post(o)\n        o = torch.tanh(o)\n        return o\n\n    @torch.no_grad()\n    def inference(self, c):\n        \"\"\"\n        Args:\n            x (Tensor): conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n        c = c.to(self.conv_pre.weight.device)\n        c = torch.nn.functional.pad(c, (self.inference_padding, self.inference_padding), \"replicate\")\n        return self.forward(c)\n\n    def remove_weight_norm(self):\n        print(\"Removing weight norm...\")\n        for l in self.ups:\n            remove_parametrizations(l, \"weight\")\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        remove_parametrizations(self.conv_pre, \"weight\")\n        remove_parametrizations(self.conv_post, \"weight\")\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n            self.remove_weight_norm()\n", "TTS/vocoder/models/hifigan_discriminator.py": "# adopted from https://github.com/jik876/hifi-gan/blob/master/models.py\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nLRELU_SLOPE = 0.1\n\n\nclass DiscriminatorP(torch.nn.Module):\n    \"\"\"HiFiGAN Periodic Discriminator\n\n    Takes every Pth value from the input waveform and applied a stack of convoluations.\n\n    Note:\n        if `period` is 2\n        `waveform = [1, 2, 3, 4, 5, 6 ...] --> [1, 3, 5 ... ] --> convs -> score, feat`\n\n    Args:\n        x (Tensor): input waveform.\n\n    Returns:\n        [Tensor]: discriminator scores per sample in the batch.\n        [List[Tensor]]: list of features from each convolutional layer.\n\n    Shapes:\n        x: [B, 1, T]\n    \"\"\"\n\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super().__init__()\n        self.period = period\n        get_padding = lambda k, d: int((k * d - d) / 2)\n        norm_f = nn.utils.spectral_norm if use_spectral_norm else nn.utils.parametrizations.weight_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(nn.Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(nn.Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(nn.Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(nn.Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(nn.Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n            ]\n        )\n        self.conv_post = norm_f(nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            [Tensor]: discriminator scores per sample in the batch.\n            [List[Tensor]]: list of features from each convolutional layer.\n\n        Shapes:\n            x: [B, 1, T]\n        \"\"\"\n        feat = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            feat.append(x)\n        x = self.conv_post(x)\n        feat.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, feat\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    \"\"\"HiFiGAN Multi-Period Discriminator (MPD)\n    Wrapper for the `PeriodDiscriminator` to apply it in different periods.\n    Periods are suggested to be prime numbers to reduce the overlap between each discriminator.\n    \"\"\"\n\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorP(2, use_spectral_norm=use_spectral_norm),\n                DiscriminatorP(3, use_spectral_norm=use_spectral_norm),\n                DiscriminatorP(5, use_spectral_norm=use_spectral_norm),\n                DiscriminatorP(7, use_spectral_norm=use_spectral_norm),\n                DiscriminatorP(11, use_spectral_norm=use_spectral_norm),\n            ]\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n        [List[Tensor]]: list of scores from each discriminator.\n            [List[List[Tensor]]]: list of list of features from each discriminator's each convolutional layer.\n\n        Shapes:\n            x: [B, 1, T]\n        \"\"\"\n        scores = []\n        feats = []\n        for _, d in enumerate(self.discriminators):\n            score, feat = d(x)\n            scores.append(score)\n            feats.append(feat)\n        return scores, feats\n\n\nclass DiscriminatorS(torch.nn.Module):\n    \"\"\"HiFiGAN Scale Discriminator.\n    It is similar to `MelganDiscriminator` but with a specific architecture explained in the paper.\n\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n\n    \"\"\"\n\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        norm_f = nn.utils.spectral_norm if use_spectral_norm else nn.utils.parametrizations.weight_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(nn.Conv1d(1, 128, 15, 1, padding=7)),\n                norm_f(nn.Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n                norm_f(nn.Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n                norm_f(nn.Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n                norm_f(nn.Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n                norm_f(nn.Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n                norm_f(nn.Conv1d(1024, 1024, 5, 1, padding=2)),\n            ]\n        )\n        self.conv_post = norm_f(nn.Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            Tensor: discriminator scores.\n            List[Tensor]: list of features from the convolutiona layers.\n        \"\"\"\n        feat = []\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            feat.append(x)\n        x = self.conv_post(x)\n        feat.append(x)\n        x = torch.flatten(x, 1, -1)\n        return x, feat\n\n\nclass MultiScaleDiscriminator(torch.nn.Module):\n    \"\"\"HiFiGAN Multi-Scale Discriminator.\n    It is similar to `MultiScaleMelganDiscriminator` but specially tailored for HiFiGAN as in the paper.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorS(use_spectral_norm=True),\n                DiscriminatorS(),\n                DiscriminatorS(),\n            ]\n        )\n        self.meanpools = nn.ModuleList([nn.AvgPool1d(4, 2, padding=2), nn.AvgPool1d(4, 2, padding=2)])\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            List[Tensor]: discriminator scores.\n            List[List[Tensor]]: list of list of features from each layers of each discriminator.\n        \"\"\"\n        scores = []\n        feats = []\n        for i, d in enumerate(self.discriminators):\n            if i != 0:\n                x = self.meanpools[i - 1](x)\n            score, feat = d(x)\n            scores.append(score)\n            feats.append(feat)\n        return scores, feats\n\n\nclass HifiganDiscriminator(nn.Module):\n    \"\"\"HiFiGAN discriminator wrapping MPD and MSD.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.mpd = MultiPeriodDiscriminator()\n        self.msd = MultiScaleDiscriminator()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            List[Tensor]: discriminator scores.\n            List[List[Tensor]]: list of list of features from each layers of each discriminator.\n        \"\"\"\n        scores, feats = self.mpd(x)\n        scores_, feats_ = self.msd(x)\n        return scores + scores_, feats + feats_\n", "TTS/vocoder/models/gan.py": "from inspect import signature\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.datasets.gan_dataset import GANDataset\nfrom TTS.vocoder.layers.losses import DiscriminatorLoss, GeneratorLoss\nfrom TTS.vocoder.models import setup_discriminator, setup_generator\nfrom TTS.vocoder.models.base_vocoder import BaseVocoder\nfrom TTS.vocoder.utils.generic_utils import plot_results\n\n\nclass GAN(BaseVocoder):\n    def __init__(self, config: Coqpit, ap: AudioProcessor = None):\n        \"\"\"Wrap a generator and a discriminator network. It provides a compatible interface for the trainer.\n        It also helps mixing and matching different generator and disciminator networks easily.\n\n        To implement a new GAN models, you just need to define the generator and the discriminator networks, the rest\n        is handled by the `GAN` class.\n\n        Args:\n            config (Coqpit): Model configuration.\n            ap (AudioProcessor): \ud83d\udc38TTS AudioProcessor instance. Defaults to None.\n\n        Examples:\n            Initializing the GAN model with HifiGAN generator and discriminator.\n            >>> from TTS.vocoder.configs import HifiganConfig\n            >>> config = HifiganConfig()\n            >>> model = GAN(config)\n        \"\"\"\n        super().__init__(config)\n        self.config = config\n        self.model_g = setup_generator(config)\n        self.model_d = setup_discriminator(config)\n        self.train_disc = False  # if False, train only the generator.\n        self.y_hat_g = None  # the last generator prediction to be passed onto the discriminator\n        self.ap = ap\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Run the generator's forward pass.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: output of the GAN generator network.\n        \"\"\"\n        return self.model_g.forward(x)\n\n    def inference(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Run the generator's inference pass.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n        Returns:\n            torch.Tensor: output of the GAN generator network.\n        \"\"\"\n        return self.model_g.inference(x)\n\n    def train_step(self, batch: Dict, criterion: Dict, optimizer_idx: int) -> Tuple[Dict, Dict]:\n        \"\"\"Compute model outputs and the loss values. `optimizer_idx` selects the generator or the discriminator for\n        network on the current pass.\n\n        Args:\n            batch (Dict): Batch of samples returned by the dataloader.\n            criterion (Dict): Criterion used to compute the losses.\n            optimizer_idx (int): ID of the optimizer in use on the current pass.\n\n        Raises:\n            ValueError: `optimizer_idx` is an unexpected value.\n\n        Returns:\n            Tuple[Dict, Dict]: model outputs and the computed loss values.\n        \"\"\"\n        outputs = {}\n        loss_dict = {}\n\n        x = batch[\"input\"]\n        y = batch[\"waveform\"]\n\n        if optimizer_idx not in [0, 1]:\n            raise ValueError(\" [!] Unexpected `optimizer_idx`.\")\n\n        if optimizer_idx == 0:\n            # DISCRIMINATOR optimization\n\n            # generator pass\n            y_hat = self.model_g(x)[:, :, : y.size(2)]\n\n            # cache for generator loss\n            # pylint: disable=W0201\n            self.y_hat_g = y_hat\n            self.y_hat_sub = None\n            self.y_sub_g = None\n\n            # PQMF formatting\n            if y_hat.shape[1] > 1:\n                self.y_hat_sub = y_hat\n                y_hat = self.model_g.pqmf_synthesis(y_hat)\n                self.y_hat_g = y_hat  # save for generator loss\n                self.y_sub_g = self.model_g.pqmf_analysis(y)\n\n            scores_fake, feats_fake, feats_real = None, None, None\n\n            if self.train_disc:\n                # use different samples for G and D trainings\n                if self.config.diff_samples_for_G_and_D:\n                    x_d = batch[\"input_disc\"]\n                    y_d = batch[\"waveform_disc\"]\n                    # use a different sample than generator\n                    with torch.no_grad():\n                        y_hat = self.model_g(x_d)\n\n                    # PQMF formatting\n                    if y_hat.shape[1] > 1:\n                        y_hat = self.model_g.pqmf_synthesis(y_hat)\n                else:\n                    # use the same samples as generator\n                    x_d = x.clone()\n                    y_d = y.clone()\n                    y_hat = self.y_hat_g\n\n                # run D with or without cond. features\n                if len(signature(self.model_d.forward).parameters) == 2:\n                    D_out_fake = self.model_d(y_hat.detach().clone(), x_d)\n                    D_out_real = self.model_d(y_d, x_d)\n                else:\n                    D_out_fake = self.model_d(y_hat.detach())\n                    D_out_real = self.model_d(y_d)\n\n                # format D outputs\n                if isinstance(D_out_fake, tuple):\n                    # self.model_d returns scores and features\n                    scores_fake, feats_fake = D_out_fake\n                    if D_out_real is None:\n                        scores_real, feats_real = None, None\n                    else:\n                        scores_real, feats_real = D_out_real\n                else:\n                    # model D returns only scores\n                    scores_fake = D_out_fake\n                    scores_real = D_out_real\n\n                # compute losses\n                loss_dict = criterion[optimizer_idx](scores_fake, scores_real)\n                outputs = {\"model_outputs\": y_hat}\n\n        if optimizer_idx == 1:\n            # GENERATOR loss\n            scores_fake, feats_fake, feats_real = None, None, None\n            if self.train_disc:\n                if len(signature(self.model_d.forward).parameters) == 2:\n                    D_out_fake = self.model_d(self.y_hat_g, x)\n                else:\n                    D_out_fake = self.model_d(self.y_hat_g)\n                D_out_real = None\n\n                if self.config.use_feat_match_loss:\n                    with torch.no_grad():\n                        D_out_real = self.model_d(y)\n\n                # format D outputs\n                if isinstance(D_out_fake, tuple):\n                    scores_fake, feats_fake = D_out_fake\n                    if D_out_real is None:\n                        feats_real = None\n                    else:\n                        _, feats_real = D_out_real\n                else:\n                    scores_fake = D_out_fake\n                    feats_fake, feats_real = None, None\n\n            # compute losses\n            loss_dict = criterion[optimizer_idx](\n                self.y_hat_g, y, scores_fake, feats_fake, feats_real, self.y_hat_sub, self.y_sub_g\n            )\n            outputs = {\"model_outputs\": self.y_hat_g}\n        return outputs, loss_dict\n\n    def _log(self, name: str, ap: AudioProcessor, batch: Dict, outputs: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Logging shared by the training and evaluation.\n\n        Args:\n            name (str): Name of the run. `train` or `eval`,\n            ap (AudioProcessor): Audio processor used in training.\n            batch (Dict): Batch used in the last train/eval step.\n            outputs (Dict): Model outputs from the last train/eval step.\n\n        Returns:\n            Tuple[Dict, Dict]: log figures and audio samples.\n        \"\"\"\n        y_hat = outputs[0][\"model_outputs\"] if self.train_disc else outputs[1][\"model_outputs\"]\n        y = batch[\"waveform\"]\n        figures = plot_results(y_hat, y, ap, name)\n        sample_voice = y_hat[0].squeeze(0).detach().cpu().numpy()\n        audios = {f\"{name}/audio\": sample_voice}\n        return figures, audios\n\n    def train_log(\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int  # pylint: disable=unused-argument\n    ) -> Tuple[Dict, np.ndarray]:\n        \"\"\"Call `_log()` for training.\"\"\"\n        figures, audios = self._log(\"eval\", self.ap, batch, outputs)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    @torch.no_grad()\n    def eval_step(self, batch: Dict, criterion: nn.Module, optimizer_idx: int) -> Tuple[Dict, Dict]:\n        \"\"\"Call `train_step()` with `no_grad()`\"\"\"\n        self.train_disc = True  # Avoid a bug in the Training with the missing discriminator loss\n        return self.train_step(batch, criterion, optimizer_idx)\n\n    def eval_log(\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int  # pylint: disable=unused-argument\n    ) -> Tuple[Dict, np.ndarray]:\n        \"\"\"Call `_log()` for evaluation.\"\"\"\n        figures, audios = self._log(\"eval\", self.ap, batch, outputs)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def load_checkpoint(\n        self,\n        config: Coqpit,\n        checkpoint_path: str,\n        eval: bool = False,  # pylint: disable=unused-argument, redefined-builtin\n        cache: bool = False,\n    ) -> None:\n        \"\"\"Load a GAN checkpoint and initialize model parameters.\n\n        Args:\n            config (Coqpit): Model config.\n            checkpoint_path (str): Checkpoint file path.\n            eval (bool, optional): If true, load the model for inference. If falseDefaults to False.\n        \"\"\"\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        # band-aid for older than v0.0.15 GAN models\n        if \"model_disc\" in state:\n            self.model_g.load_checkpoint(config, checkpoint_path, eval)\n        else:\n            self.load_state_dict(state[\"model\"])\n            if eval:\n                self.model_d = None\n                if hasattr(self.model_g, \"remove_weight_norm\"):\n                    self.model_g.remove_weight_norm()\n\n    def on_train_step_start(self, trainer) -> None:\n        \"\"\"Enable the discriminator training based on `steps_to_start_discriminator`\n\n        Args:\n            trainer (Trainer): Trainer object.\n        \"\"\"\n        self.train_disc = trainer.total_steps_done >= self.config.steps_to_start_discriminator\n\n    def get_optimizer(self) -> List:\n        \"\"\"Initiate and return the GAN optimizers based on the config parameters.\n\n        It returnes 2 optimizers in a list. First one is for the generator and the second one is for the discriminator.\n\n        Returns:\n            List: optimizers.\n        \"\"\"\n        optimizer1 = get_optimizer(\n            self.config.optimizer, self.config.optimizer_params, self.config.lr_gen, self.model_g\n        )\n        optimizer2 = get_optimizer(\n            self.config.optimizer, self.config.optimizer_params, self.config.lr_disc, self.model_d\n        )\n        return [optimizer2, optimizer1]\n\n    def get_lr(self) -> List:\n        \"\"\"Set the initial learning rates for each optimizer.\n\n        Returns:\n            List: learning rates for each optimizer.\n        \"\"\"\n        return [self.config.lr_disc, self.config.lr_gen]\n\n    def get_scheduler(self, optimizer) -> List:\n        \"\"\"Set the schedulers for each optimizer.\n\n        Args:\n            optimizer (List[`torch.optim.Optimizer`]): List of optimizers.\n\n        Returns:\n            List: Schedulers, one for each optimizer.\n        \"\"\"\n        scheduler1 = get_scheduler(self.config.lr_scheduler_gen, self.config.lr_scheduler_gen_params, optimizer[0])\n        scheduler2 = get_scheduler(self.config.lr_scheduler_disc, self.config.lr_scheduler_disc_params, optimizer[1])\n        return [scheduler2, scheduler1]\n\n    @staticmethod\n    def format_batch(batch: List) -> Dict:\n        \"\"\"Format the batch for training.\n\n        Args:\n            batch (List): Batch out of the dataloader.\n\n        Returns:\n            Dict: formatted model inputs.\n        \"\"\"\n        if isinstance(batch[0], list):\n            x_G, y_G = batch[0]\n            x_D, y_D = batch[1]\n            return {\"input\": x_G, \"waveform\": y_G, \"input_disc\": x_D, \"waveform_disc\": y_D}\n        x, y = batch\n        return {\"input\": x, \"waveform\": y}\n\n    def get_data_loader(  # pylint: disable=no-self-use, unused-argument\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: True,\n        samples: List,\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,  # pylint: disable=unused-argument\n    ):\n        \"\"\"Initiate and return the GAN dataloader.\n\n        Args:\n            config (Coqpit): Model config.\n            ap (AudioProcessor): Audio processor.\n            is_eval (True): Set the dataloader for evaluation if true.\n            samples (List): Data samples.\n            verbose (bool): Log information if true.\n            num_gpus (int): Number of GPUs in use.\n            rank (int): Rank of the current GPU. Defaults to None.\n\n        Returns:\n            DataLoader: Torch dataloader.\n        \"\"\"\n        dataset = GANDataset(\n            ap=self.ap,\n            items=samples,\n            seq_len=config.seq_len,\n            hop_len=self.ap.hop_length,\n            pad_short=config.pad_short,\n            conv_pad=config.conv_pad,\n            return_pairs=config.diff_samples_for_G_and_D if \"diff_samples_for_G_and_D\" in config else False,\n            is_training=not is_eval,\n            return_segments=not is_eval,\n            use_noise_augment=config.use_noise_augment,\n            use_cache=config.use_cache,\n            verbose=verbose,\n        )\n        dataset.shuffle_mapping()\n        sampler = DistributedSampler(dataset, shuffle=True) if num_gpus > 1 else None\n        loader = DataLoader(\n            dataset,\n            batch_size=1 if is_eval else config.batch_size,\n            shuffle=num_gpus == 0,\n            drop_last=False,\n            sampler=sampler,\n            num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n            pin_memory=False,\n        )\n        return loader\n\n    def get_criterion(self):\n        \"\"\"Return criterions for the optimizers\"\"\"\n        return [DiscriminatorLoss(self.config), GeneratorLoss(self.config)]\n\n    @staticmethod\n    def init_from_config(config: Coqpit, verbose=True) -> \"GAN\":\n        ap = AudioProcessor.init_from_config(config, verbose=verbose)\n        return GAN(config, ap=ap)\n", "TTS/vocoder/models/multiband_melgan_generator.py": "import torch\n\nfrom TTS.vocoder.layers.pqmf import PQMF\nfrom TTS.vocoder.models.melgan_generator import MelganGenerator\n\n\nclass MultibandMelganGenerator(MelganGenerator):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=4,\n        proj_kernel=7,\n        base_channels=384,\n        upsample_factors=(2, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=3,\n    ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            proj_kernel=proj_kernel,\n            base_channels=base_channels,\n            upsample_factors=upsample_factors,\n            res_kernel=res_kernel,\n            num_res_blocks=num_res_blocks,\n        )\n        self.pqmf_layer = PQMF(N=4, taps=62, cutoff=0.15, beta=9.0)\n\n    def pqmf_analysis(self, x):\n        return self.pqmf_layer.analysis(x)\n\n    def pqmf_synthesis(self, x):\n        return self.pqmf_layer.synthesis(x)\n\n    @torch.no_grad()\n    def inference(self, cond_features):\n        cond_features = cond_features.to(self.layers[1].weight.device)\n        cond_features = torch.nn.functional.pad(\n            cond_features, (self.inference_padding, self.inference_padding), \"replicate\"\n        )\n        return self.pqmf_synthesis(self.layers(cond_features))\n", "TTS/vocoder/models/__init__.py": "import importlib\nimport re\n\nfrom coqpit import Coqpit\n\n\ndef to_camel(text):\n    text = text.capitalize()\n    return re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\n\n\ndef setup_model(config: Coqpit):\n    \"\"\"Load models directly from configuration.\"\"\"\n    if \"discriminator_model\" in config and \"generator_model\" in config:\n        MyModel = importlib.import_module(\"TTS.vocoder.models.gan\")\n        MyModel = getattr(MyModel, \"GAN\")\n    else:\n        MyModel = importlib.import_module(\"TTS.vocoder.models.\" + config.model.lower())\n        if config.model.lower() == \"wavernn\":\n            MyModel = getattr(MyModel, \"Wavernn\")\n        elif config.model.lower() == \"gan\":\n            MyModel = getattr(MyModel, \"GAN\")\n        elif config.model.lower() == \"wavegrad\":\n            MyModel = getattr(MyModel, \"Wavegrad\")\n        else:\n            try:\n                MyModel = getattr(MyModel, to_camel(config.model))\n            except ModuleNotFoundError as e:\n                raise ValueError(f\"Model {config.model} not exist!\") from e\n    print(\" > Vocoder Model: {}\".format(config.model))\n    return MyModel.init_from_config(config)\n\n\ndef setup_generator(c):\n    \"\"\"TODO: use config object as arguments\"\"\"\n    print(\" > Generator Model: {}\".format(c.generator_model))\n    MyModel = importlib.import_module(\"TTS.vocoder.models.\" + c.generator_model.lower())\n    MyModel = getattr(MyModel, to_camel(c.generator_model))\n    # this is to preserve the Wavernn class name (instead of Wavernn)\n    if c.generator_model.lower() in \"hifigan_generator\":\n        model = MyModel(in_channels=c.audio[\"num_mels\"], out_channels=1, **c.generator_model_params)\n    elif c.generator_model.lower() in \"melgan_generator\":\n        model = MyModel(\n            in_channels=c.audio[\"num_mels\"],\n            out_channels=1,\n            proj_kernel=7,\n            base_channels=512,\n            upsample_factors=c.generator_model_params[\"upsample_factors\"],\n            res_kernel=3,\n            num_res_blocks=c.generator_model_params[\"num_res_blocks\"],\n        )\n    elif c.generator_model in \"melgan_fb_generator\":\n        raise ValueError(\"melgan_fb_generator is now fullband_melgan_generator\")\n    elif c.generator_model.lower() in \"multiband_melgan_generator\":\n        model = MyModel(\n            in_channels=c.audio[\"num_mels\"],\n            out_channels=4,\n            proj_kernel=7,\n            base_channels=384,\n            upsample_factors=c.generator_model_params[\"upsample_factors\"],\n            res_kernel=3,\n            num_res_blocks=c.generator_model_params[\"num_res_blocks\"],\n        )\n    elif c.generator_model.lower() in \"fullband_melgan_generator\":\n        model = MyModel(\n            in_channels=c.audio[\"num_mels\"],\n            out_channels=1,\n            proj_kernel=7,\n            base_channels=512,\n            upsample_factors=c.generator_model_params[\"upsample_factors\"],\n            res_kernel=3,\n            num_res_blocks=c.generator_model_params[\"num_res_blocks\"],\n        )\n    elif c.generator_model.lower() in \"parallel_wavegan_generator\":\n        model = MyModel(\n            in_channels=1,\n            out_channels=1,\n            kernel_size=3,\n            num_res_blocks=c.generator_model_params[\"num_res_blocks\"],\n            stacks=c.generator_model_params[\"stacks\"],\n            res_channels=64,\n            gate_channels=128,\n            skip_channels=64,\n            aux_channels=c.audio[\"num_mels\"],\n            dropout=0.0,\n            bias=True,\n            use_weight_norm=True,\n            upsample_factors=c.generator_model_params[\"upsample_factors\"],\n        )\n    elif c.generator_model.lower() in \"univnet_generator\":\n        model = MyModel(**c.generator_model_params)\n    else:\n        raise NotImplementedError(f\"Model {c.generator_model} not implemented!\")\n    return model\n\n\ndef setup_discriminator(c):\n    \"\"\"TODO: use config objekt as arguments\"\"\"\n    print(\" > Discriminator Model: {}\".format(c.discriminator_model))\n    if \"parallel_wavegan\" in c.discriminator_model:\n        MyModel = importlib.import_module(\"TTS.vocoder.models.parallel_wavegan_discriminator\")\n    else:\n        MyModel = importlib.import_module(\"TTS.vocoder.models.\" + c.discriminator_model.lower())\n    MyModel = getattr(MyModel, to_camel(c.discriminator_model.lower()))\n    if c.discriminator_model in \"hifigan_discriminator\":\n        model = MyModel()\n    if c.discriminator_model in \"random_window_discriminator\":\n        model = MyModel(\n            cond_channels=c.audio[\"num_mels\"],\n            hop_length=c.audio[\"hop_length\"],\n            uncond_disc_donwsample_factors=c.discriminator_model_params[\"uncond_disc_donwsample_factors\"],\n            cond_disc_downsample_factors=c.discriminator_model_params[\"cond_disc_downsample_factors\"],\n            cond_disc_out_channels=c.discriminator_model_params[\"cond_disc_out_channels\"],\n            window_sizes=c.discriminator_model_params[\"window_sizes\"],\n        )\n    if c.discriminator_model in \"melgan_multiscale_discriminator\":\n        model = MyModel(\n            in_channels=1,\n            out_channels=1,\n            kernel_sizes=(5, 3),\n            base_channels=c.discriminator_model_params[\"base_channels\"],\n            max_channels=c.discriminator_model_params[\"max_channels\"],\n            downsample_factors=c.discriminator_model_params[\"downsample_factors\"],\n        )\n    if c.discriminator_model == \"residual_parallel_wavegan_discriminator\":\n        model = MyModel(\n            in_channels=1,\n            out_channels=1,\n            kernel_size=3,\n            num_layers=c.discriminator_model_params[\"num_layers\"],\n            stacks=c.discriminator_model_params[\"stacks\"],\n            res_channels=64,\n            gate_channels=128,\n            skip_channels=64,\n            dropout=0.0,\n            bias=True,\n            nonlinear_activation=\"LeakyReLU\",\n            nonlinear_activation_params={\"negative_slope\": 0.2},\n        )\n    if c.discriminator_model == \"parallel_wavegan_discriminator\":\n        model = MyModel(\n            in_channels=1,\n            out_channels=1,\n            kernel_size=3,\n            num_layers=c.discriminator_model_params[\"num_layers\"],\n            conv_channels=64,\n            dilation_factor=1,\n            nonlinear_activation=\"LeakyReLU\",\n            nonlinear_activation_params={\"negative_slope\": 0.2},\n            bias=True,\n        )\n    if c.discriminator_model == \"univnet_discriminator\":\n        model = MyModel()\n    return model\n", "TTS/vocoder/models/fullband_melgan_generator.py": "import torch\n\nfrom TTS.vocoder.models.melgan_generator import MelganGenerator\n\n\nclass FullbandMelganGenerator(MelganGenerator):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        proj_kernel=7,\n        base_channels=512,\n        upsample_factors=(2, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=4,\n    ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            proj_kernel=proj_kernel,\n            base_channels=base_channels,\n            upsample_factors=upsample_factors,\n            res_kernel=res_kernel,\n            num_res_blocks=num_res_blocks,\n        )\n\n    @torch.no_grad()\n    def inference(self, cond_features):\n        cond_features = cond_features.to(self.layers[1].weight.device)\n        cond_features = torch.nn.functional.pad(\n            cond_features, (self.inference_padding, self.inference_padding), \"replicate\"\n        )\n        return self.layers(cond_features)\n", "TTS/vocoder/models/melgan_discriminator.py": "import numpy as np\nfrom torch import nn\nfrom torch.nn.utils.parametrizations import weight_norm\n\n\nclass MelganDiscriminator(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_sizes=(5, 3),\n        base_channels=16,\n        max_channels=1024,\n        downsample_factors=(4, 4, 4, 4),\n        groups_denominator=4,\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList()\n\n        layer_kernel_size = np.prod(kernel_sizes)\n        layer_padding = (layer_kernel_size - 1) // 2\n\n        # initial layer\n        self.layers += [\n            nn.Sequential(\n                nn.ReflectionPad1d(layer_padding),\n                weight_norm(nn.Conv1d(in_channels, base_channels, layer_kernel_size, stride=1)),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        ]\n\n        # downsampling layers\n        layer_in_channels = base_channels\n        for downsample_factor in downsample_factors:\n            layer_out_channels = min(layer_in_channels * downsample_factor, max_channels)\n            layer_kernel_size = downsample_factor * 10 + 1\n            layer_padding = (layer_kernel_size - 1) // 2\n            layer_groups = layer_in_channels // groups_denominator\n            self.layers += [\n                nn.Sequential(\n                    weight_norm(\n                        nn.Conv1d(\n                            layer_in_channels,\n                            layer_out_channels,\n                            kernel_size=layer_kernel_size,\n                            stride=downsample_factor,\n                            padding=layer_padding,\n                            groups=layer_groups,\n                        )\n                    ),\n                    nn.LeakyReLU(0.2, inplace=True),\n                )\n            ]\n            layer_in_channels = layer_out_channels\n\n        # last 2 layers\n        layer_padding1 = (kernel_sizes[0] - 1) // 2\n        layer_padding2 = (kernel_sizes[1] - 1) // 2\n        self.layers += [\n            nn.Sequential(\n                weight_norm(\n                    nn.Conv1d(\n                        layer_out_channels,\n                        layer_out_channels,\n                        kernel_size=kernel_sizes[0],\n                        stride=1,\n                        padding=layer_padding1,\n                    )\n                ),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            weight_norm(\n                nn.Conv1d(\n                    layer_out_channels, out_channels, kernel_size=kernel_sizes[1], stride=1, padding=layer_padding2\n                )\n            ),\n        ]\n\n    def forward(self, x):\n        feats = []\n        for layer in self.layers:\n            x = layer(x)\n            feats.append(x)\n        return x, feats\n", "TTS/vocoder/models/univnet_discriminator.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.utils import spectral_norm\nfrom torch.nn.utils.parametrizations import weight_norm\n\nfrom TTS.utils.audio.torch_transforms import TorchSTFT\nfrom TTS.vocoder.models.hifigan_discriminator import MultiPeriodDiscriminator\n\nLRELU_SLOPE = 0.1\n\n\nclass SpecDiscriminator(nn.Module):\n    \"\"\"docstring for Discriminator.\"\"\"\n\n    def __init__(self, fft_size=1024, hop_length=120, win_length=600, use_spectral_norm=False):\n        super().__init__()\n        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n        self.fft_size = fft_size\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = TorchSTFT(fft_size, hop_length, win_length)\n        self.discriminators = nn.ModuleList(\n            [\n                norm_f(nn.Conv2d(1, 32, kernel_size=(3, 9), padding=(1, 4))),\n                norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 2), padding=(1, 4))),\n                norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 2), padding=(1, 4))),\n                norm_f(nn.Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 2), padding=(1, 4))),\n                norm_f(nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n            ]\n        )\n\n        self.out = norm_f(nn.Conv2d(32, 1, 3, 1, 1))\n\n    def forward(self, y):\n        fmap = []\n        with torch.no_grad():\n            y = y.squeeze(1)\n            y = self.stft(y)\n        y = y.unsqueeze(1)\n        for _, d in enumerate(self.discriminators):\n            y = d(y)\n            y = F.leaky_relu(y, LRELU_SLOPE)\n            fmap.append(y)\n\n        y = self.out(y)\n        fmap.append(y)\n\n        return torch.flatten(y, 1, -1), fmap\n\n\nclass MultiResSpecDiscriminator(torch.nn.Module):\n    def __init__(  # pylint: disable=dangerous-default-value\n        self, fft_sizes=[1024, 2048, 512], hop_sizes=[120, 240, 50], win_lengths=[600, 1200, 240], window=\"hann_window\"\n    ):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                SpecDiscriminator(fft_sizes[0], hop_sizes[0], win_lengths[0], window),\n                SpecDiscriminator(fft_sizes[1], hop_sizes[1], win_lengths[1], window),\n                SpecDiscriminator(fft_sizes[2], hop_sizes[2], win_lengths[2], window),\n            ]\n        )\n\n    def forward(self, x):\n        scores = []\n        feats = []\n        for d in self.discriminators:\n            score, feat = d(x)\n            scores.append(score)\n            feats.append(feat)\n\n        return scores, feats\n\n\nclass UnivnetDiscriminator(nn.Module):\n    \"\"\"Univnet discriminator wrapping MPD and MSD.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.mpd = MultiPeriodDiscriminator()\n        self.msd = MultiResSpecDiscriminator()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            List[Tensor]: discriminator scores.\n            List[List[Tensor]]: list of list of features from each layers of each discriminator.\n        \"\"\"\n        scores, feats = self.mpd(x)\n        scores_, feats_ = self.msd(x)\n        return scores + scores_, feats + feats_\n", "TTS/vocoder/utils/distribution.py": "import math\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions.normal import Normal\n\n\ndef gaussian_loss(y_hat, y, log_std_min=-7.0):\n    assert y_hat.dim() == 3\n    assert y_hat.size(2) == 2\n    mean = y_hat[:, :, :1]\n    log_std = torch.clamp(y_hat[:, :, 1:], min=log_std_min)\n    # TODO: replace with pytorch dist\n    log_probs = -0.5 * (-math.log(2.0 * math.pi) - 2.0 * log_std - torch.pow(y - mean, 2) * torch.exp((-2.0 * log_std)))\n    return log_probs.squeeze().mean()\n\n\ndef sample_from_gaussian(y_hat, log_std_min=-7.0, scale_factor=1.0):\n    assert y_hat.size(2) == 2\n    mean = y_hat[:, :, :1]\n    log_std = torch.clamp(y_hat[:, :, 1:], min=log_std_min)\n    dist = Normal(\n        mean,\n        torch.exp(log_std),\n    )\n    sample = dist.sample()\n    sample = torch.clamp(torch.clamp(sample, min=-scale_factor), max=scale_factor)\n    del dist\n    return sample\n\n\ndef log_sum_exp(x):\n    \"\"\"numerically stable log_sum_exp implementation that prevents overflow\"\"\"\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\n# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536, log_scale_min=None, reduce=True):\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0, 2, 1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix : 2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix : 3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1.0 / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1.0 / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(F.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - F.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2.0 * F.softplus(mid_in)\n\n    # tf equivalent\n\n    # log_probs = tf.where(x < -0.999, log_cdf_plus,\n    #                      tf.where(x > 0.999, log_one_minus_cdf_min,\n    #                               tf.where(cdf_delta > 1e-5,\n    #                                        tf.log(tf.maximum(cdf_delta, 1e-12)),\n    #                                        log_pdf_mid - np.log(127.5))))\n\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1.0 - inner_inner_cond) * (\n        log_pdf_mid - np.log((num_classes - 1) / 2)\n    )\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1.0 - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1.0 - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.mean(log_sum_exp(log_probs))\n    return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    \"\"\"\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): :math:`[B, C, T]`\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    \"\"\"\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(-torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = to_one_hot(argmax, nr_mix)\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix : 2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.clamp(torch.sum(y[:, :, 2 * nr_mix : 3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don't actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1.0 - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.0), max=1.0)\n\n    return x\n\n\ndef to_one_hot(tensor, n, fill_with=1.0):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_().type_as(tensor)\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot\n", "TTS/vocoder/utils/__init__.py": "", "TTS/vocoder/utils/generic_utils.py": "from typing import Dict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom TTS.tts.utils.visual import plot_spectrogram\nfrom TTS.utils.audio import AudioProcessor\n\n\ndef interpolate_vocoder_input(scale_factor, spec):\n    \"\"\"Interpolate spectrogram by the scale factor.\n    It is mainly used to match the sampling rates of\n    the tts and vocoder models.\n\n    Args:\n        scale_factor (float): scale factor to interpolate the spectrogram\n        spec (np.array): spectrogram to be interpolated\n\n    Returns:\n        torch.tensor: interpolated spectrogram.\n    \"\"\"\n    print(\" > before interpolation :\", spec.shape)\n    spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)  # pylint: disable=not-callable\n    spec = torch.nn.functional.interpolate(\n        spec, scale_factor=scale_factor, recompute_scale_factor=True, mode=\"bilinear\", align_corners=False\n    ).squeeze(0)\n    print(\" > after interpolation :\", spec.shape)\n    return spec\n\n\ndef plot_results(y_hat: torch.tensor, y: torch.tensor, ap: AudioProcessor, name_prefix: str = None) -> Dict:\n    \"\"\"Plot the predicted and the real waveform and their spectrograms.\n\n    Args:\n        y_hat (torch.tensor): Predicted waveform.\n        y (torch.tensor): Real waveform.\n        ap (AudioProcessor): Audio processor used to process the waveform.\n        name_prefix (str, optional): Name prefix used to name the figures. Defaults to None.\n\n    Returns:\n        Dict: output figures keyed by the name of the figures.\n    \"\"\" \"\"\"Plot vocoder model results\"\"\"\n    if name_prefix is None:\n        name_prefix = \"\"\n\n    # select an instance from batch\n    y_hat = y_hat[0].squeeze().detach().cpu().numpy()\n    y = y[0].squeeze().detach().cpu().numpy()\n\n    spec_fake = ap.melspectrogram(y_hat).T\n    spec_real = ap.melspectrogram(y).T\n    spec_diff = np.abs(spec_fake - spec_real)\n\n    # plot figure and save it\n    fig_wave = plt.figure()\n    plt.subplot(2, 1, 1)\n    plt.plot(y)\n    plt.title(\"groundtruth speech\")\n    plt.subplot(2, 1, 2)\n    plt.plot(y_hat)\n    plt.title(\"generated speech\")\n    plt.tight_layout()\n    plt.close()\n\n    figures = {\n        name_prefix + \"spectrogram/fake\": plot_spectrogram(spec_fake),\n        name_prefix + \"spectrogram/real\": plot_spectrogram(spec_real),\n        name_prefix + \"spectrogram/diff\": plot_spectrogram(spec_diff),\n        name_prefix + \"speech_comparison\": fig_wave,\n    }\n    return figures\n", "TTS/vocoder/layers/losses.py": "from typing import Dict, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom TTS.utils.audio.torch_transforms import TorchSTFT\nfrom TTS.vocoder.utils.distribution import discretized_mix_logistic_loss, gaussian_loss\n\n#################################\n# GENERATOR LOSSES\n#################################\n\n\nclass STFTLoss(nn.Module):\n    \"\"\"STFT loss. Input generate and real waveforms are converted\n    to spectrograms compared with L1 and Spectral convergence losses.\n    It is from ParallelWaveGAN paper https://arxiv.org/pdf/1910.11480.pdf\"\"\"\n\n    def __init__(self, n_fft, hop_length, win_length):\n        super().__init__()\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = TorchSTFT(n_fft, hop_length, win_length)\n\n    def forward(self, y_hat, y):\n        y_hat_M = self.stft(y_hat)\n        y_M = self.stft(y)\n        # magnitude loss\n        loss_mag = F.l1_loss(torch.log(y_M), torch.log(y_hat_M))\n        # spectral convergence loss\n        loss_sc = torch.norm(y_M - y_hat_M, p=\"fro\") / torch.norm(y_M, p=\"fro\")\n        return loss_mag, loss_sc\n\n\nclass MultiScaleSTFTLoss(torch.nn.Module):\n    \"\"\"Multi-scale STFT loss. Input generate and real waveforms are converted\n    to spectrograms compared with L1 and Spectral convergence losses.\n    It is from ParallelWaveGAN paper https://arxiv.org/pdf/1910.11480.pdf\"\"\"\n\n    def __init__(self, n_ffts=(1024, 2048, 512), hop_lengths=(120, 240, 50), win_lengths=(600, 1200, 240)):\n        super().__init__()\n        self.loss_funcs = torch.nn.ModuleList()\n        for n_fft, hop_length, win_length in zip(n_ffts, hop_lengths, win_lengths):\n            self.loss_funcs.append(STFTLoss(n_fft, hop_length, win_length))\n\n    def forward(self, y_hat, y):\n        N = len(self.loss_funcs)\n        loss_sc = 0\n        loss_mag = 0\n        for f in self.loss_funcs:\n            lm, lsc = f(y_hat, y)\n            loss_mag += lm\n            loss_sc += lsc\n        loss_sc /= N\n        loss_mag /= N\n        return loss_mag, loss_sc\n\n\nclass L1SpecLoss(nn.Module):\n    \"\"\"L1 Loss over Spectrograms as described in HiFiGAN paper https://arxiv.org/pdf/2010.05646.pdf\"\"\"\n\n    def __init__(\n        self, sample_rate, n_fft, hop_length, win_length, mel_fmin=None, mel_fmax=None, n_mels=None, use_mel=True\n    ):\n        super().__init__()\n        self.use_mel = use_mel\n        self.stft = TorchSTFT(\n            n_fft,\n            hop_length,\n            win_length,\n            sample_rate=sample_rate,\n            mel_fmin=mel_fmin,\n            mel_fmax=mel_fmax,\n            n_mels=n_mels,\n            use_mel=use_mel,\n        )\n\n    def forward(self, y_hat, y):\n        y_hat_M = self.stft(y_hat)\n        y_M = self.stft(y)\n        # magnitude loss\n        loss_mag = F.l1_loss(torch.log(y_M), torch.log(y_hat_M))\n        return loss_mag\n\n\nclass MultiScaleSubbandSTFTLoss(MultiScaleSTFTLoss):\n    \"\"\"Multiscale STFT loss for multi band model outputs.\n    From MultiBand-MelGAN paper https://arxiv.org/abs/2005.05106\"\"\"\n\n    # pylint: disable=no-self-use\n    def forward(self, y_hat, y):\n        y_hat = y_hat.view(-1, 1, y_hat.shape[2])\n        y = y.view(-1, 1, y.shape[2])\n        return super().forward(y_hat.squeeze(1), y.squeeze(1))\n\n\nclass MSEGLoss(nn.Module):\n    \"\"\"Mean Squared Generator Loss\"\"\"\n\n    # pylint: disable=no-self-use\n    def forward(self, score_real):\n        loss_fake = F.mse_loss(score_real, score_real.new_ones(score_real.shape))\n        return loss_fake\n\n\nclass HingeGLoss(nn.Module):\n    \"\"\"Hinge Discriminator Loss\"\"\"\n\n    # pylint: disable=no-self-use\n    def forward(self, score_real):\n        # TODO: this might be wrong\n        loss_fake = torch.mean(F.relu(1.0 - score_real))\n        return loss_fake\n\n\n##################################\n# DISCRIMINATOR LOSSES\n##################################\n\n\nclass MSEDLoss(nn.Module):\n    \"\"\"Mean Squared Discriminator Loss\"\"\"\n\n    def __init__(\n        self,\n    ):\n        super().__init__()\n        self.loss_func = nn.MSELoss()\n\n    # pylint: disable=no-self-use\n    def forward(self, score_fake, score_real):\n        loss_real = self.loss_func(score_real, score_real.new_ones(score_real.shape))\n        loss_fake = self.loss_func(score_fake, score_fake.new_zeros(score_fake.shape))\n        loss_d = loss_real + loss_fake\n        return loss_d, loss_real, loss_fake\n\n\nclass HingeDLoss(nn.Module):\n    \"\"\"Hinge Discriminator Loss\"\"\"\n\n    # pylint: disable=no-self-use\n    def forward(self, score_fake, score_real):\n        loss_real = torch.mean(F.relu(1.0 - score_real))\n        loss_fake = torch.mean(F.relu(1.0 + score_fake))\n        loss_d = loss_real + loss_fake\n        return loss_d, loss_real, loss_fake\n\n\nclass MelganFeatureLoss(nn.Module):\n    def __init__(\n        self,\n    ):\n        super().__init__()\n        self.loss_func = nn.L1Loss()\n\n    # pylint: disable=no-self-use\n    def forward(self, fake_feats, real_feats):\n        loss_feats = 0\n        num_feats = 0\n        for idx, _ in enumerate(fake_feats):\n            for fake_feat, real_feat in zip(fake_feats[idx], real_feats[idx]):\n                loss_feats += self.loss_func(fake_feat, real_feat)\n                num_feats += 1\n        loss_feats = loss_feats / num_feats\n        return loss_feats\n\n\n#####################################\n# LOSS WRAPPERS\n#####################################\n\n\ndef _apply_G_adv_loss(scores_fake, loss_func):\n    \"\"\"Compute G adversarial loss function\n    and normalize values\"\"\"\n    adv_loss = 0\n    if isinstance(scores_fake, list):\n        for score_fake in scores_fake:\n            fake_loss = loss_func(score_fake)\n            adv_loss += fake_loss\n        adv_loss /= len(scores_fake)\n    else:\n        fake_loss = loss_func(scores_fake)\n        adv_loss = fake_loss\n    return adv_loss\n\n\ndef _apply_D_loss(scores_fake, scores_real, loss_func):\n    \"\"\"Compute D loss func and normalize loss values\"\"\"\n    loss = 0\n    real_loss = 0\n    fake_loss = 0\n    if isinstance(scores_fake, list):\n        # multi-scale loss\n        for score_fake, score_real in zip(scores_fake, scores_real):\n            total_loss, real_loss_, fake_loss_ = loss_func(score_fake=score_fake, score_real=score_real)\n            loss += total_loss\n            real_loss += real_loss_\n            fake_loss += fake_loss_\n        # normalize loss values with number of scales (discriminators)\n        loss /= len(scores_fake)\n        real_loss /= len(scores_real)\n        fake_loss /= len(scores_fake)\n    else:\n        # single scale loss\n        total_loss, real_loss, fake_loss = loss_func(scores_fake, scores_real)\n        loss = total_loss\n    return loss, real_loss, fake_loss\n\n\n##################################\n# MODEL LOSSES\n##################################\n\n\nclass GeneratorLoss(nn.Module):\n    \"\"\"Generator Loss Wrapper. Based on model configuration it sets a right set of loss functions and computes\n    losses. It allows to experiment with different combinations of loss functions with different models by just\n    changing configurations.\n\n    Args:\n        C (AttrDict): model configuration.\n    \"\"\"\n\n    def __init__(self, C):\n        super().__init__()\n        assert not (\n            C.use_mse_gan_loss and C.use_hinge_gan_loss\n        ), \" [!] Cannot use HingeGANLoss and MSEGANLoss together.\"\n\n        self.use_stft_loss = C.use_stft_loss if \"use_stft_loss\" in C else False\n        self.use_subband_stft_loss = C.use_subband_stft_loss if \"use_subband_stft_loss\" in C else False\n        self.use_mse_gan_loss = C.use_mse_gan_loss if \"use_mse_gan_loss\" in C else False\n        self.use_hinge_gan_loss = C.use_hinge_gan_loss if \"use_hinge_gan_loss\" in C else False\n        self.use_feat_match_loss = C.use_feat_match_loss if \"use_feat_match_loss\" in C else False\n        self.use_l1_spec_loss = C.use_l1_spec_loss if \"use_l1_spec_loss\" in C else False\n\n        self.stft_loss_weight = C.stft_loss_weight if \"stft_loss_weight\" in C else 0.0\n        self.subband_stft_loss_weight = C.subband_stft_loss_weight if \"subband_stft_loss_weight\" in C else 0.0\n        self.mse_gan_loss_weight = C.mse_G_loss_weight if \"mse_G_loss_weight\" in C else 0.0\n        self.hinge_gan_loss_weight = C.hinge_G_loss_weight if \"hinde_G_loss_weight\" in C else 0.0\n        self.feat_match_loss_weight = C.feat_match_loss_weight if \"feat_match_loss_weight\" in C else 0.0\n        self.l1_spec_loss_weight = C.l1_spec_loss_weight if \"l1_spec_loss_weight\" in C else 0.0\n\n        if C.use_stft_loss:\n            self.stft_loss = MultiScaleSTFTLoss(**C.stft_loss_params)\n        if C.use_subband_stft_loss:\n            self.subband_stft_loss = MultiScaleSubbandSTFTLoss(**C.subband_stft_loss_params)\n        if C.use_mse_gan_loss:\n            self.mse_loss = MSEGLoss()\n        if C.use_hinge_gan_loss:\n            self.hinge_loss = HingeGLoss()\n        if C.use_feat_match_loss:\n            self.feat_match_loss = MelganFeatureLoss()\n        if C.use_l1_spec_loss:\n            assert C.audio[\"sample_rate\"] == C.l1_spec_loss_params[\"sample_rate\"]\n            self.l1_spec_loss = L1SpecLoss(**C.l1_spec_loss_params)\n\n    def forward(\n        self, y_hat=None, y=None, scores_fake=None, feats_fake=None, feats_real=None, y_hat_sub=None, y_sub=None\n    ):\n        gen_loss = 0\n        adv_loss = 0\n        return_dict = {}\n\n        # STFT Loss\n        if self.use_stft_loss:\n            stft_loss_mg, stft_loss_sc = self.stft_loss(y_hat[:, :, : y.size(2)].squeeze(1), y.squeeze(1))\n            return_dict[\"G_stft_loss_mg\"] = stft_loss_mg\n            return_dict[\"G_stft_loss_sc\"] = stft_loss_sc\n            gen_loss = gen_loss + self.stft_loss_weight * (stft_loss_mg + stft_loss_sc)\n\n        # L1 Spec loss\n        if self.use_l1_spec_loss:\n            l1_spec_loss = self.l1_spec_loss(y_hat, y)\n            return_dict[\"G_l1_spec_loss\"] = l1_spec_loss\n            gen_loss = gen_loss + self.l1_spec_loss_weight * l1_spec_loss\n\n        # subband STFT Loss\n        if self.use_subband_stft_loss:\n            subband_stft_loss_mg, subband_stft_loss_sc = self.subband_stft_loss(y_hat_sub, y_sub)\n            return_dict[\"G_subband_stft_loss_mg\"] = subband_stft_loss_mg\n            return_dict[\"G_subband_stft_loss_sc\"] = subband_stft_loss_sc\n            gen_loss = gen_loss + self.subband_stft_loss_weight * (subband_stft_loss_mg + subband_stft_loss_sc)\n\n        # multiscale MSE adversarial loss\n        if self.use_mse_gan_loss and scores_fake is not None:\n            mse_fake_loss = _apply_G_adv_loss(scores_fake, self.mse_loss)\n            return_dict[\"G_mse_fake_loss\"] = mse_fake_loss\n            adv_loss = adv_loss + self.mse_gan_loss_weight * mse_fake_loss\n\n        # multiscale Hinge adversarial loss\n        if self.use_hinge_gan_loss and not scores_fake is not None:\n            hinge_fake_loss = _apply_G_adv_loss(scores_fake, self.hinge_loss)\n            return_dict[\"G_hinge_fake_loss\"] = hinge_fake_loss\n            adv_loss = adv_loss + self.hinge_gan_loss_weight * hinge_fake_loss\n\n        # Feature Matching Loss\n        if self.use_feat_match_loss and not feats_fake is None:\n            feat_match_loss = self.feat_match_loss(feats_fake, feats_real)\n            return_dict[\"G_feat_match_loss\"] = feat_match_loss\n            adv_loss = adv_loss + self.feat_match_loss_weight * feat_match_loss\n        return_dict[\"loss\"] = gen_loss + adv_loss\n        return_dict[\"G_gen_loss\"] = gen_loss\n        return_dict[\"G_adv_loss\"] = adv_loss\n        return return_dict\n\n\nclass DiscriminatorLoss(nn.Module):\n    \"\"\"Like ```GeneratorLoss```\"\"\"\n\n    def __init__(self, C):\n        super().__init__()\n        assert not (\n            C.use_mse_gan_loss and C.use_hinge_gan_loss\n        ), \" [!] Cannot use HingeGANLoss and MSEGANLoss together.\"\n\n        self.use_mse_gan_loss = C.use_mse_gan_loss\n        self.use_hinge_gan_loss = C.use_hinge_gan_loss\n\n        if C.use_mse_gan_loss:\n            self.mse_loss = MSEDLoss()\n        if C.use_hinge_gan_loss:\n            self.hinge_loss = HingeDLoss()\n\n    def forward(self, scores_fake, scores_real):\n        loss = 0\n        return_dict = {}\n\n        if self.use_mse_gan_loss:\n            mse_D_loss, mse_D_real_loss, mse_D_fake_loss = _apply_D_loss(\n                scores_fake=scores_fake, scores_real=scores_real, loss_func=self.mse_loss\n            )\n            return_dict[\"D_mse_gan_loss\"] = mse_D_loss\n            return_dict[\"D_mse_gan_real_loss\"] = mse_D_real_loss\n            return_dict[\"D_mse_gan_fake_loss\"] = mse_D_fake_loss\n            loss += mse_D_loss\n\n        if self.use_hinge_gan_loss:\n            hinge_D_loss, hinge_D_real_loss, hinge_D_fake_loss = _apply_D_loss(\n                scores_fake=scores_fake, scores_real=scores_real, loss_func=self.hinge_loss\n            )\n            return_dict[\"D_hinge_gan_loss\"] = hinge_D_loss\n            return_dict[\"D_hinge_gan_real_loss\"] = hinge_D_real_loss\n            return_dict[\"D_hinge_gan_fake_loss\"] = hinge_D_fake_loss\n            loss += hinge_D_loss\n\n        return_dict[\"loss\"] = loss\n        return return_dict\n\n\nclass WaveRNNLoss(nn.Module):\n    def __init__(self, wave_rnn_mode: Union[str, int]):\n        super().__init__()\n        if wave_rnn_mode == \"mold\":\n            self.loss_func = discretized_mix_logistic_loss\n        elif wave_rnn_mode == \"gauss\":\n            self.loss_func = gaussian_loss\n        elif isinstance(wave_rnn_mode, int):\n            self.loss_func = torch.nn.CrossEntropyLoss()\n        else:\n            raise ValueError(\" [!] Unknown mode for Wavernn.\")\n\n    def forward(self, y_hat, y) -> Dict:\n        loss = self.loss_func(y_hat, y)\n        return {\"loss\": loss}\n", "TTS/vocoder/layers/hifigan.py": "from torch import nn\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\n\n# pylint: disable=dangerous-default-value\nclass ResStack(nn.Module):\n    def __init__(self, kernel, channel, padding, dilations=[1, 3, 5]):\n        super().__init__()\n        resstack = []\n        for dilation in dilations:\n            resstack += [\n                nn.LeakyReLU(0.2),\n                nn.ReflectionPad1d(dilation),\n                nn.utils.parametrizations.weight_norm(\n                    nn.Conv1d(channel, channel, kernel_size=kernel, dilation=dilation)\n                ),\n                nn.LeakyReLU(0.2),\n                nn.ReflectionPad1d(padding),\n                nn.utils.parametrizations.weight_norm(nn.Conv1d(channel, channel, kernel_size=1)),\n            ]\n        self.resstack = nn.Sequential(*resstack)\n\n        self.shortcut = nn.utils.parametrizations.weight_norm(nn.Conv1d(channel, channel, kernel_size=1))\n\n    def forward(self, x):\n        x1 = self.shortcut(x)\n        x2 = self.resstack(x)\n        return x1 + x2\n\n    def remove_weight_norm(self):\n        remove_parametrizations(self.shortcut, \"weight\")\n        remove_parametrizations(self.resstack[2], \"weight\")\n        remove_parametrizations(self.resstack[5], \"weight\")\n        remove_parametrizations(self.resstack[8], \"weight\")\n        remove_parametrizations(self.resstack[11], \"weight\")\n        remove_parametrizations(self.resstack[14], \"weight\")\n        remove_parametrizations(self.resstack[17], \"weight\")\n\n\nclass MRF(nn.Module):\n    def __init__(self, kernels, channel, dilations=[1, 3, 5]):  # # pylint: disable=dangerous-default-value\n        super().__init__()\n        self.resblock1 = ResStack(kernels[0], channel, 0, dilations)\n        self.resblock2 = ResStack(kernels[1], channel, 6, dilations)\n        self.resblock3 = ResStack(kernels[2], channel, 12, dilations)\n\n    def forward(self, x):\n        x1 = self.resblock1(x)\n        x2 = self.resblock2(x)\n        x3 = self.resblock3(x)\n        return x1 + x2 + x3\n\n    def remove_weight_norm(self):\n        self.resblock1.remove_weight_norm()\n        self.resblock2.remove_weight_norm()\n        self.resblock3.remove_weight_norm()\n", "TTS/vocoder/layers/pqmf.py": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom scipy import signal as sig\n\n\n# adapted from\n# https://github.com/kan-bayashi/ParallelWaveGAN/tree/master/parallel_wavegan\nclass PQMF(torch.nn.Module):\n    def __init__(self, N=4, taps=62, cutoff=0.15, beta=9.0):\n        super().__init__()\n\n        self.N = N\n        self.taps = taps\n        self.cutoff = cutoff\n        self.beta = beta\n\n        QMF = sig.firwin(taps + 1, cutoff, window=(\"kaiser\", beta))\n        H = np.zeros((N, len(QMF)))\n        G = np.zeros((N, len(QMF)))\n        for k in range(N):\n            constant_factor = (\n                (2 * k + 1) * (np.pi / (2 * N)) * (np.arange(taps + 1) - ((taps - 1) / 2))\n            )  # TODO: (taps - 1) -> taps\n            phase = (-1) ** k * np.pi / 4\n            H[k] = 2 * QMF * np.cos(constant_factor + phase)\n\n            G[k] = 2 * QMF * np.cos(constant_factor - phase)\n\n        H = torch.from_numpy(H[:, None, :]).float()\n        G = torch.from_numpy(G[None, :, :]).float()\n\n        self.register_buffer(\"H\", H)\n        self.register_buffer(\"G\", G)\n\n        updown_filter = torch.zeros((N, N, N)).float()\n        for k in range(N):\n            updown_filter[k, k, 0] = 1.0\n        self.register_buffer(\"updown_filter\", updown_filter)\n        self.N = N\n\n        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n\n    def forward(self, x):\n        return self.analysis(x)\n\n    def analysis(self, x):\n        return F.conv1d(x, self.H, padding=self.taps // 2, stride=self.N)\n\n    def synthesis(self, x):\n        x = F.conv_transpose1d(x, self.updown_filter * self.N, stride=self.N)\n        x = F.conv1d(x, self.G, padding=self.taps // 2)\n        return x\n", "TTS/vocoder/layers/wavegrad.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\n\nclass Conv1d(nn.Conv1d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        nn.init.orthogonal_(self.weight)\n        nn.init.zeros_(self.bias)\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding with noise level conditioning\"\"\"\n\n    def __init__(self, n_channels, max_len=10000):\n        super().__init__()\n        self.n_channels = n_channels\n        self.max_len = max_len\n        self.C = 5000\n        self.pe = torch.zeros(0, 0)\n\n    def forward(self, x, noise_level):\n        if x.shape[2] > self.pe.shape[1]:\n            self.init_pe_matrix(x.shape[1], x.shape[2], x)\n        return x + noise_level[..., None, None] + self.pe[:, : x.size(2)].repeat(x.shape[0], 1, 1) / self.C\n\n    def init_pe_matrix(self, n_channels, max_len, x):\n        pe = torch.zeros(max_len, n_channels)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(10000, torch.arange(0, n_channels, 2).float() / n_channels)\n\n        pe[:, 0::2] = torch.sin(position / div_term)\n        pe[:, 1::2] = torch.cos(position / div_term)\n        self.pe = pe.transpose(0, 1).to(x)\n\n\nclass FiLM(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.encoding = PositionalEncoding(input_size)\n        self.input_conv = nn.Conv1d(input_size, input_size, 3, padding=1)\n        self.output_conv = nn.Conv1d(input_size, output_size * 2, 3, padding=1)\n\n        nn.init.xavier_uniform_(self.input_conv.weight)\n        nn.init.xavier_uniform_(self.output_conv.weight)\n        nn.init.zeros_(self.input_conv.bias)\n        nn.init.zeros_(self.output_conv.bias)\n\n    def forward(self, x, noise_scale):\n        o = self.input_conv(x)\n        o = F.leaky_relu(o, 0.2)\n        o = self.encoding(o, noise_scale)\n        shift, scale = torch.chunk(self.output_conv(o), 2, dim=1)\n        return shift, scale\n\n    def remove_weight_norm(self):\n        remove_parametrizations(self.input_conv, \"weight\")\n        remove_parametrizations(self.output_conv, \"weight\")\n\n    def apply_weight_norm(self):\n        self.input_conv = weight_norm(self.input_conv)\n        self.output_conv = weight_norm(self.output_conv)\n\n\n@torch.jit.script\ndef shif_and_scale(x, scale, shift):\n    o = shift + scale * x\n    return o\n\n\nclass UBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, factor, dilation):\n        super().__init__()\n        assert isinstance(dilation, (list, tuple))\n        assert len(dilation) == 4\n\n        self.factor = factor\n        self.res_block = Conv1d(input_size, hidden_size, 1)\n        self.main_block = nn.ModuleList(\n            [\n                Conv1d(input_size, hidden_size, 3, dilation=dilation[0], padding=dilation[0]),\n                Conv1d(hidden_size, hidden_size, 3, dilation=dilation[1], padding=dilation[1]),\n            ]\n        )\n        self.out_block = nn.ModuleList(\n            [\n                Conv1d(hidden_size, hidden_size, 3, dilation=dilation[2], padding=dilation[2]),\n                Conv1d(hidden_size, hidden_size, 3, dilation=dilation[3], padding=dilation[3]),\n            ]\n        )\n\n    def forward(self, x, shift, scale):\n        x_inter = F.interpolate(x, size=x.shape[-1] * self.factor)\n        res = self.res_block(x_inter)\n        o = F.leaky_relu(x_inter, 0.2)\n        o = F.interpolate(o, size=x.shape[-1] * self.factor)\n        o = self.main_block[0](o)\n        o = shif_and_scale(o, scale, shift)\n        o = F.leaky_relu(o, 0.2)\n        o = self.main_block[1](o)\n        res2 = res + o\n        o = shif_and_scale(res2, scale, shift)\n        o = F.leaky_relu(o, 0.2)\n        o = self.out_block[0](o)\n        o = shif_and_scale(o, scale, shift)\n        o = F.leaky_relu(o, 0.2)\n        o = self.out_block[1](o)\n        o = o + res2\n        return o\n\n    def remove_weight_norm(self):\n        remove_parametrizations(self.res_block, \"weight\")\n        for _, layer in enumerate(self.main_block):\n            if len(layer.state_dict()) != 0:\n                remove_parametrizations(layer, \"weight\")\n        for _, layer in enumerate(self.out_block):\n            if len(layer.state_dict()) != 0:\n                remove_parametrizations(layer, \"weight\")\n\n    def apply_weight_norm(self):\n        self.res_block = weight_norm(self.res_block)\n        for idx, layer in enumerate(self.main_block):\n            if len(layer.state_dict()) != 0:\n                self.main_block[idx] = weight_norm(layer)\n        for idx, layer in enumerate(self.out_block):\n            if len(layer.state_dict()) != 0:\n                self.out_block[idx] = weight_norm(layer)\n\n\nclass DBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, factor):\n        super().__init__()\n        self.factor = factor\n        self.res_block = Conv1d(input_size, hidden_size, 1)\n        self.main_block = nn.ModuleList(\n            [\n                Conv1d(input_size, hidden_size, 3, dilation=1, padding=1),\n                Conv1d(hidden_size, hidden_size, 3, dilation=2, padding=2),\n                Conv1d(hidden_size, hidden_size, 3, dilation=4, padding=4),\n            ]\n        )\n\n    def forward(self, x):\n        size = x.shape[-1] // self.factor\n        res = self.res_block(x)\n        res = F.interpolate(res, size=size)\n        o = F.interpolate(x, size=size)\n        for layer in self.main_block:\n            o = F.leaky_relu(o, 0.2)\n            o = layer(o)\n        return o + res\n\n    def remove_weight_norm(self):\n        remove_parametrizations(self.res_block, \"weight\")\n        for _, layer in enumerate(self.main_block):\n            if len(layer.state_dict()) != 0:\n                remove_parametrizations(layer, \"weight\")\n\n    def apply_weight_norm(self):\n        self.res_block = weight_norm(self.res_block)\n        for idx, layer in enumerate(self.main_block):\n            if len(layer.state_dict()) != 0:\n                self.main_block[idx] = weight_norm(layer)\n", "TTS/vocoder/layers/parallel_wavegan.py": "import torch\nfrom torch.nn import functional as F\n\n\nclass ResidualBlock(torch.nn.Module):\n    \"\"\"Residual block module in WaveNet.\"\"\"\n\n    def __init__(\n        self,\n        kernel_size=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,\n        aux_channels=80,\n        dropout=0.0,\n        dilation=1,\n        bias=True,\n        use_causal_conv=False,\n    ):\n        super().__init__()\n        self.dropout = dropout\n        # no future time stamps available\n        if use_causal_conv:\n            padding = (kernel_size - 1) * dilation\n        else:\n            assert (kernel_size - 1) % 2 == 0, \"Not support even number kernel size.\"\n            padding = (kernel_size - 1) // 2 * dilation\n        self.use_causal_conv = use_causal_conv\n\n        # dilation conv\n        self.conv = torch.nn.Conv1d(\n            res_channels, gate_channels, kernel_size, padding=padding, dilation=dilation, bias=bias\n        )\n\n        # local conditioning\n        if aux_channels > 0:\n            self.conv1x1_aux = torch.nn.Conv1d(aux_channels, gate_channels, 1, bias=False)\n        else:\n            self.conv1x1_aux = None\n\n        # conv output is split into two groups\n        gate_out_channels = gate_channels // 2\n        self.conv1x1_out = torch.nn.Conv1d(gate_out_channels, res_channels, 1, bias=bias)\n        self.conv1x1_skip = torch.nn.Conv1d(gate_out_channels, skip_channels, 1, bias=bias)\n\n    def forward(self, x, c):\n        \"\"\"\n        x: B x D_res x T\n        c: B x D_aux x T\n        \"\"\"\n        residual = x\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.conv(x)\n\n        # remove future time steps if use_causal_conv conv\n        x = x[:, :, : residual.size(-1)] if self.use_causal_conv else x\n\n        # split into two part for gated activation\n        splitdim = 1\n        xa, xb = x.split(x.size(splitdim) // 2, dim=splitdim)\n\n        # local conditioning\n        if c is not None:\n            assert self.conv1x1_aux is not None\n            c = self.conv1x1_aux(c)\n            ca, cb = c.split(c.size(splitdim) // 2, dim=splitdim)\n            xa, xb = xa + ca, xb + cb\n\n        x = torch.tanh(xa) * torch.sigmoid(xb)\n\n        # for skip connection\n        s = self.conv1x1_skip(x)\n\n        # for residual connection\n        x = (self.conv1x1_out(x) + residual) * (0.5**2)\n\n        return x, s\n", "TTS/vocoder/layers/melgan.py": "from torch import nn\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\n\nclass ResidualStack(nn.Module):\n    def __init__(self, channels, num_res_blocks, kernel_size):\n        super().__init__()\n\n        assert (kernel_size - 1) % 2 == 0, \" [!] kernel_size has to be odd.\"\n        base_padding = (kernel_size - 1) // 2\n\n        self.blocks = nn.ModuleList()\n        for idx in range(num_res_blocks):\n            layer_kernel_size = kernel_size\n            layer_dilation = layer_kernel_size**idx\n            layer_padding = base_padding * layer_dilation\n            self.blocks += [\n                nn.Sequential(\n                    nn.LeakyReLU(0.2),\n                    nn.ReflectionPad1d(layer_padding),\n                    weight_norm(\n                        nn.Conv1d(channels, channels, kernel_size=kernel_size, dilation=layer_dilation, bias=True)\n                    ),\n                    nn.LeakyReLU(0.2),\n                    weight_norm(nn.Conv1d(channels, channels, kernel_size=1, bias=True)),\n                )\n            ]\n\n        self.shortcuts = nn.ModuleList(\n            [weight_norm(nn.Conv1d(channels, channels, kernel_size=1, bias=True)) for _ in range(num_res_blocks)]\n        )\n\n    def forward(self, x):\n        for block, shortcut in zip(self.blocks, self.shortcuts):\n            x = shortcut(x) + block(x)\n        return x\n\n    def remove_weight_norm(self):\n        for block, shortcut in zip(self.blocks, self.shortcuts):\n            remove_parametrizations(block[2], \"weight\")\n            remove_parametrizations(block[4], \"weight\")\n            remove_parametrizations(shortcut, \"weight\")\n", "TTS/vocoder/layers/upsample.py": "import torch\nfrom torch.nn import functional as F\n\n\nclass Stretch2d(torch.nn.Module):\n    def __init__(self, x_scale, y_scale, mode=\"nearest\"):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n        self.mode = mode\n\n    def forward(self, x):\n        \"\"\"\n        x (Tensor): Input tensor (B, C, F, T).\n        Tensor: Interpolated tensor (B, C, F * y_scale, T * x_scale),\n        \"\"\"\n        return F.interpolate(x, scale_factor=(self.y_scale, self.x_scale), mode=self.mode)\n\n\nclass UpsampleNetwork(torch.nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        upsample_factors,\n        nonlinear_activation=None,\n        nonlinear_activation_params={},\n        interpolate_mode=\"nearest\",\n        freq_axis_kernel_size=1,\n        use_causal_conv=False,\n    ):\n        super().__init__()\n        self.use_causal_conv = use_causal_conv\n        self.up_layers = torch.nn.ModuleList()\n        for scale in upsample_factors:\n            # interpolation layer\n            stretch = Stretch2d(scale, 1, interpolate_mode)\n            self.up_layers += [stretch]\n\n            # conv layer\n            assert (freq_axis_kernel_size - 1) % 2 == 0, \"Not support even number freq axis kernel size.\"\n            freq_axis_padding = (freq_axis_kernel_size - 1) // 2\n            kernel_size = (freq_axis_kernel_size, scale * 2 + 1)\n            if use_causal_conv:\n                padding = (freq_axis_padding, scale * 2)\n            else:\n                padding = (freq_axis_padding, scale)\n            conv = torch.nn.Conv2d(1, 1, kernel_size=kernel_size, padding=padding, bias=False)\n            self.up_layers += [conv]\n\n            # nonlinear\n            if nonlinear_activation is not None:\n                nonlinear = getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)\n                self.up_layers += [nonlinear]\n\n    def forward(self, c):\n        \"\"\"\n        c :  (B, C, T_in).\n        Tensor: (B, C, T_upsample)\n        \"\"\"\n        c = c.unsqueeze(1)  # (B, 1, C, T)\n        for f in self.up_layers:\n            c = f(c)\n        return c.squeeze(1)  # (B, C, T')\n\n\nclass ConvUpsample(torch.nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        upsample_factors,\n        nonlinear_activation=None,\n        nonlinear_activation_params={},\n        interpolate_mode=\"nearest\",\n        freq_axis_kernel_size=1,\n        aux_channels=80,\n        aux_context_window=0,\n        use_causal_conv=False,\n    ):\n        super().__init__()\n        self.aux_context_window = aux_context_window\n        self.use_causal_conv = use_causal_conv and aux_context_window > 0\n        # To capture wide-context information in conditional features\n        kernel_size = aux_context_window + 1 if use_causal_conv else 2 * aux_context_window + 1\n        # NOTE(kan-bayashi): Here do not use padding because the input is already padded\n        self.conv_in = torch.nn.Conv1d(aux_channels, aux_channels, kernel_size=kernel_size, bias=False)\n        self.upsample = UpsampleNetwork(\n            upsample_factors=upsample_factors,\n            nonlinear_activation=nonlinear_activation,\n            nonlinear_activation_params=nonlinear_activation_params,\n            interpolate_mode=interpolate_mode,\n            freq_axis_kernel_size=freq_axis_kernel_size,\n            use_causal_conv=use_causal_conv,\n        )\n\n    def forward(self, c):\n        \"\"\"\n        c : (B, C, T_in).\n        Tensor: (B, C, T_upsampled),\n        \"\"\"\n        c_ = self.conv_in(c)\n        c = c_[:, :, : -self.aux_context_window] if self.use_causal_conv else c_\n        return self.upsample(c)\n", "TTS/vocoder/layers/__init__.py": "", "TTS/vocoder/layers/lvc_block.py": "import torch\nimport torch.nn.functional as F\n\n\nclass KernelPredictor(torch.nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\"\"\"\n\n    def __init__(  # pylint: disable=dangerous-default-value\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n        kpnet_nonlinear_activation=\"LeakyReLU\",\n        kpnet_nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        \"\"\"\n        Args:\n            cond_channels (int): number of channel for the conditioning sequence,\n            conv_in_channels (int): number of channel for the input sequence,\n            conv_out_channels (int): number of channel for the output sequence,\n            conv_layers (int):\n            kpnet_\n        \"\"\"\n        super().__init__()\n\n        self.conv_in_channels = conv_in_channels\n        self.conv_out_channels = conv_out_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_layers = conv_layers\n\n        l_w = conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers\n        l_b = conv_out_channels * conv_layers\n\n        padding = (kpnet_conv_size - 1) // 2\n        self.input_conv = torch.nn.Sequential(\n            torch.nn.Conv1d(cond_channels, kpnet_hidden_channels, 5, padding=(5 - 1) // 2, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.residual_conv = torch.nn.Sequential(\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Dropout(kpnet_dropout),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n            torch.nn.Conv1d(kpnet_hidden_channels, kpnet_hidden_channels, kpnet_conv_size, padding=padding, bias=True),\n            getattr(torch.nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.kernel_conv = torch.nn.Conv1d(kpnet_hidden_channels, l_w, kpnet_conv_size, padding=padding, bias=True)\n        self.bias_conv = torch.nn.Conv1d(kpnet_hidden_channels, l_b, kpnet_conv_size, padding=padding, bias=True)\n\n    def forward(self, c):\n        \"\"\"\n        Args:\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n        Returns:\n        \"\"\"\n        batch, _, cond_length = c.shape\n\n        c = self.input_conv(c)\n        c = c + self.residual_conv(c)\n        k = self.kernel_conv(c)\n        b = self.bias_conv(c)\n\n        kernels = k.contiguous().view(\n            batch, self.conv_layers, self.conv_in_channels, self.conv_out_channels, self.conv_kernel_size, cond_length\n        )\n        bias = b.contiguous().view(batch, self.conv_layers, self.conv_out_channels, cond_length)\n        return kernels, bias\n\n\nclass LVCBlock(torch.nn.Module):\n    \"\"\"the location-variable convolutions\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        cond_channels,\n        upsample_ratio,\n        conv_layers=4,\n        conv_kernel_size=3,\n        cond_hop_length=256,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n    ):\n        super().__init__()\n\n        self.cond_hop_length = cond_hop_length\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.convs = torch.nn.ModuleList()\n\n        self.upsample = torch.nn.ConvTranspose1d(\n            in_channels,\n            in_channels,\n            kernel_size=upsample_ratio * 2,\n            stride=upsample_ratio,\n            padding=upsample_ratio // 2 + upsample_ratio % 2,\n            output_padding=upsample_ratio % 2,\n        )\n\n        self.kernel_predictor = KernelPredictor(\n            cond_channels=cond_channels,\n            conv_in_channels=in_channels,\n            conv_out_channels=2 * in_channels,\n            conv_layers=conv_layers,\n            conv_kernel_size=conv_kernel_size,\n            kpnet_hidden_channels=kpnet_hidden_channels,\n            kpnet_conv_size=kpnet_conv_size,\n            kpnet_dropout=kpnet_dropout,\n        )\n\n        for i in range(conv_layers):\n            padding = (3**i) * int((conv_kernel_size - 1) / 2)\n            conv = torch.nn.Conv1d(\n                in_channels, in_channels, kernel_size=conv_kernel_size, padding=padding, dilation=3**i\n            )\n\n            self.convs.append(conv)\n\n    def forward(self, x, c):\n        \"\"\"forward propagation of the location-variable convolutions.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length)\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n\n        Returns:\n            Tensor: the output sequence (batch, in_channels, in_length)\n        \"\"\"\n        in_channels = x.shape[1]\n        kernels, bias = self.kernel_predictor(c)\n\n        x = F.leaky_relu(x, 0.2)\n        x = self.upsample(x)\n\n        for i in range(self.conv_layers):\n            y = F.leaky_relu(x, 0.2)\n            y = self.convs[i](y)\n            y = F.leaky_relu(y, 0.2)\n\n            k = kernels[:, i, :, :, :, :]\n            b = bias[:, i, :, :]\n            y = self.location_variable_convolution(y, k, b, 1, self.cond_hop_length)\n            x = x + torch.sigmoid(y[:, :in_channels, :]) * torch.tanh(y[:, in_channels:, :])\n        return x\n\n    @staticmethod\n    def location_variable_convolution(x, kernel, bias, dilation, hop_size):\n        \"\"\"perform location-variable convolution operation on the input sequence (x) using the local convolution kernl.\n        Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length).\n            kernel (Tensor): the local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length)\n            bias (Tensor): the bias for the local convolution (batch, out_channels, kernel_length)\n            dilation (int): the dilation of convolution.\n            hop_size (int): the hop_size of the conditioning sequence.\n        Returns:\n            (Tensor): the output sequence after performing local convolution. (batch, out_channels, in_length).\n        \"\"\"\n        batch, _, in_length = x.shape\n        batch, _, out_channels, kernel_size, kernel_length = kernel.shape\n\n        assert in_length == (\n            kernel_length * hop_size\n        ), f\"length of (x, kernel) is not matched, {in_length} vs {kernel_length * hop_size}\"\n\n        padding = dilation * int((kernel_size - 1) / 2)\n        x = F.pad(x, (padding, padding), \"constant\", 0)  # (batch, in_channels, in_length + 2*padding)\n        x = x.unfold(2, hop_size + 2 * padding, hop_size)  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n        if hop_size < dilation:\n            x = F.pad(x, (0, dilation), \"constant\", 0)\n        x = x.unfold(\n            3, dilation, dilation\n        )  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n        x = x[:, :, :, :, :hop_size]\n        x = x.transpose(3, 4)  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n        x = x.unfold(4, kernel_size, 1)  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n        o = torch.einsum(\"bildsk,biokl->bolsd\", x, kernel)\n        o = o + bias.unsqueeze(-1).unsqueeze(-1)\n        o = o.contiguous().view(batch, out_channels, -1)\n        return o\n", "TTS/vocoder/configs/wavegrad_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.vocoder.configs.shared_configs import BaseVocoderConfig\nfrom TTS.vocoder.models.wavegrad import WavegradArgs\n\n\n@dataclass\nclass WavegradConfig(BaseVocoderConfig):\n    \"\"\"Defines parameters for WaveGrad vocoder.\n    Example:\n\n        >>> from TTS.vocoder.configs import WavegradConfig\n        >>> config = WavegradConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `wavegrad`.\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `wavegrad`.\n        model_params (WavegradArgs): Model parameters. Check `WavegradArgs` for default values.\n        target_loss (str):\n            Target loss name that defines the quality of the model. Defaults to `avg_wavegrad_loss`.\n        epochs (int):\n            Number of epochs to traing the model. Defaults to 10000.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 96.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 6144.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        mixed_precision (bool):\n            enable / disable mixed precision training. Default is True.\n        eval_split_size (int):\n            Number of samples used for evalutaion. Defaults to 50.\n        train_noise_schedule (dict):\n            Training noise schedule. Defaults to\n            `{\"min_val\": 1e-6, \"max_val\": 1e-2, \"num_steps\": 1000}`\n        test_noise_schedule (dict):\n            Inference noise schedule. For a better performance, you may need to use `bin/tune_wavegrad.py` to find a\n            better schedule. Defaults to\n            `\n            {\n                \"min_val\": 1e-6,\n                \"max_val\": 1e-2,\n                \"num_steps\": 50,\n            }\n            `\n        grad_clip (float):\n            Gradient clipping threshold. If <= 0.0, no clipping is applied. Defaults to 1.0\n        lr (float):\n            Initila leraning rate. Defaults to 1e-4.\n        lr_scheduler (str):\n            One of the learning rate schedulers from `torch.optim.scheduler.*`. Defaults to `MultiStepLR`.\n        lr_scheduler_params (dict):\n            kwargs for the scheduler. Defaults to `{\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}`\n    \"\"\"\n\n    model: str = \"wavegrad\"\n    # Model specific params\n    generator_model: str = \"wavegrad\"\n    model_params: WavegradArgs = field(default_factory=WavegradArgs)\n    target_loss: str = \"loss\"  # loss value to pick the best model to save after each epoch\n\n    # Training - overrides\n    epochs: int = 10000\n    batch_size: int = 96\n    seq_len: int = 6144\n    use_cache: bool = True\n    mixed_precision: bool = True\n    eval_split_size: int = 50\n\n    # NOISE SCHEDULE PARAMS\n    train_noise_schedule: dict = field(default_factory=lambda: {\"min_val\": 1e-6, \"max_val\": 1e-2, \"num_steps\": 1000})\n\n    test_noise_schedule: dict = field(\n        default_factory=lambda: {  # inference noise schedule. Try TTS/bin/tune_wavegrad.py to find the optimal values.\n            \"min_val\": 1e-6,\n            \"max_val\": 1e-2,\n            \"num_steps\": 50,\n        }\n    )\n\n    # optimizer overrides\n    grad_clip: float = 1.0\n    lr: float = 1e-4  # Initial learning rate.\n    lr_scheduler: str = \"MultiStepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_params: dict = field(\n        default_factory=lambda: {\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}\n    )\n", "TTS/vocoder/configs/hifigan_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.vocoder.configs.shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass HifiganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for FullBand MelGAN vocoder.\n\n    Example:\n\n        >>> from TTS.vocoder.configs import HifiganConfig\n        >>> config = HifiganConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `hifigan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'hifigan_discriminator`.\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `hifigan_generator`.\n        generator_model_params (dict): Parameters of the generator model. Defaults to\n            `\n            {\n                \"upsample_factors\": [8, 8, 2, 2],\n                \"upsample_kernel_sizes\": [16, 16, 4, 4],\n                \"upsample_initial_channel\": 512,\n                \"resblock_kernel_sizes\": [3, 7, 11],\n                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n                \"resblock_type\": \"1\",\n            }\n            `\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 16.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        use_stft_loss (bool):\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by HifiGAN model. Defaults to False.\n        stft_loss_params (dict):\n            STFT loss parameters. Default to\n            `{\n                \"n_ffts\": [1024, 2048, 512],\n                \"hop_lengths\": [120, 240, 50],\n                \"win_lengths\": [600, 1200, 240]\n            }`\n        l1_spec_loss_params (dict):\n            L1 spectrogram loss parameters. Default to\n            `{\n                \"use_mel\": True,\n                \"sample_rate\": 22050,\n                \"n_fft\": 1024,\n                \"hop_length\": 256,\n                \"win_length\": 1024,\n                \"n_mels\": 80,\n                \"mel_fmin\": 0.0,\n                \"mel_fmax\": None,\n            }`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 108.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n    \"\"\"\n\n    model: str = \"hifigan\"\n    # model specific params\n    discriminator_model: str = \"hifigan_discriminator\"\n    generator_model: str = \"hifigan_generator\"\n    generator_model_params: dict = field(\n        default_factory=lambda: {\n            \"upsample_factors\": [8, 8, 2, 2],\n            \"upsample_kernel_sizes\": [16, 16, 4, 4],\n            \"upsample_initial_channel\": 512,\n            \"resblock_kernel_sizes\": [3, 7, 11],\n            \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n            \"resblock_type\": \"1\",\n        }\n    )\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = False\n    use_subband_stft_loss: bool = False\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = True  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = True\n\n    # loss weights - overrides\n    stft_loss_weight: float = 0\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 1\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 108\n    l1_spec_loss_weight: float = 45\n    l1_spec_loss_params: dict = field(\n        default_factory=lambda: {\n            \"use_mel\": True,\n            \"sample_rate\": 22050,\n            \"n_fft\": 1024,\n            \"hop_length\": 256,\n            \"win_length\": 1024,\n            \"n_mels\": 80,\n            \"mel_fmin\": 0.0,\n            \"mel_fmax\": None,\n        }\n    )\n\n    # optimizer parameters\n    lr: float = 1e-4\n    wd: float = 1e-6\n", "TTS/vocoder/configs/multiband_melgan_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.vocoder.configs.shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass MultibandMelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for MultiBandMelGAN vocoder.\n\n    Example:\n\n        >>> from TTS.vocoder.configs import MultibandMelganConfig\n        >>> config = MultibandMelganConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `multiband_melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.\n        discriminator_model_params (dict): The discriminator model parameters. Defaults to\n            '{\n                \"base_channels\": 16,\n                \"max_channels\": 512,\n                \"downsample_factors\": [4, 4, 4]\n            }`\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `melgan_generator`.\n        generator_model_param (dict):\n            The generator model parameters. Defaults to `{\"upsample_factors\": [8, 4, 2], \"num_res_blocks\": 4}`.\n        use_pqmf (bool):\n            enable / disable PQMF modulation for multi-band training. Defaults to True.\n        lr_gen (float):\n            Initial learning rate for the generator model. Defaults to 0.0001.\n        lr_disc (float):\n            Initial learning rate for the discriminator model. Defaults to 0.0001.\n        optimizer (torch.optim.Optimizer):\n            Optimizer used for the training. Defaults to `AdamW`.\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n        lr_scheduler_gen (torch.optim.Scheduler):\n            Learning rate scheduler for the generator. Defaults to `MultiStepLR`.\n        lr_scheduler_gen_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to\n            `{\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}`.\n        lr_scheduler_disc (torch.optim.Scheduler):\n            Learning rate scheduler for the discriminator. Defaults to `MultiStepLR`.\n        lr_scheduler_dict_params (dict):\n            Parameters for the discriminator learning rate scheduler. Defaults to\n            `{\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}`.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 16.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        steps_to_start_discriminator (int):\n            Number of steps required to start training the discriminator. Defaults to 0.\n        use_stft_loss (bool):`\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by HifiGAN model. Defaults to False.\n        stft_loss_params (dict): STFT loss parameters. Default to\n            `{\"n_ffts\": [1024, 2048, 512], \"hop_lengths\": [120, 240, 50], \"win_lengths\": [600, 1200, 240]}`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 108.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n    \"\"\"\n\n    model: str = \"multiband_melgan\"\n\n    # Model specific params\n    discriminator_model: str = \"melgan_multiscale_discriminator\"\n    discriminator_model_params: dict = field(\n        default_factory=lambda: {\"base_channels\": 16, \"max_channels\": 512, \"downsample_factors\": [4, 4, 4]}\n    )\n    generator_model: str = \"multiband_melgan_generator\"\n    generator_model_params: dict = field(default_factory=lambda: {\"upsample_factors\": [8, 4, 2], \"num_res_blocks\": 4})\n    use_pqmf: bool = True\n\n    # optimizer - overrides\n    lr_gen: float = 0.0001  # Initial learning rate.\n    lr_disc: float = 0.0001  # Initial learning rate.\n    optimizer: str = \"AdamW\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.8, 0.99], \"weight_decay\": 0.0})\n    lr_scheduler_gen: str = \"MultiStepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_gen_params: dict = field(\n        default_factory=lambda: {\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}\n    )\n    lr_scheduler_disc: str = \"MultiStepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_disc_params: dict = field(\n        default_factory=lambda: {\"gamma\": 0.5, \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]}\n    )\n\n    # Training - overrides\n    batch_size: int = 64\n    seq_len: int = 16384\n    pad_short: int = 2000\n    use_noise_augment: bool = False\n    use_cache: bool = True\n    steps_to_start_discriminator: bool = 200000\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = True\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = False  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = False\n\n    subband_stft_loss_params: dict = field(\n        default_factory=lambda: {\"n_ffts\": [384, 683, 171], \"hop_lengths\": [30, 60, 10], \"win_lengths\": [150, 300, 60]}\n    )\n\n    # loss weights - overrides\n    stft_loss_weight: float = 0.5\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 2.5\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 108\n    l1_spec_loss_weight: float = 0\n", "TTS/vocoder/configs/univnet_config.py": "from dataclasses import dataclass, field\nfrom typing import Dict\n\nfrom TTS.vocoder.configs.shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass UnivnetConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for UnivNet vocoder.\n\n    Example:\n\n        >>> from TTS.vocoder.configs import UnivNetConfig\n        >>> config = UnivNetConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `UnivNet`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'UnivNet_discriminator`.\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `UnivNet_generator`.\n        generator_model_params (dict): Parameters of the generator model. Defaults to\n            `\n            {\n                \"use_mel\": True,\n                \"sample_rate\": 22050,\n                \"n_fft\": 1024,\n                \"hop_length\": 256,\n                \"win_length\": 1024,\n                \"n_mels\": 80,\n                \"mel_fmin\": 0.0,\n                \"mel_fmax\": None,\n            }\n            `\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 32.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        use_stft_loss (bool):\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by univnet model. Defaults to False.\n        stft_loss_params (dict):\n            STFT loss parameters. Default to\n            `{\n                \"n_ffts\": [1024, 2048, 512],\n                \"hop_lengths\": [120, 240, 50],\n                \"win_lengths\": [600, 1200, 240]\n            }`\n        l1_spec_loss_params (dict):\n            L1 spectrogram loss parameters. Default to\n            `{\n                \"use_mel\": True,\n                \"sample_rate\": 22050,\n                \"n_fft\": 1024,\n                \"hop_length\": 256,\n                \"win_length\": 1024,\n                \"n_mels\": 80,\n                \"mel_fmin\": 0.0,\n                \"mel_fmax\": None,\n            }`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 108.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n    \"\"\"\n\n    model: str = \"univnet\"\n    batch_size: int = 32\n    # model specific params\n    discriminator_model: str = \"univnet_discriminator\"\n    generator_model: str = \"univnet_generator\"\n    generator_model_params: Dict = field(\n        default_factory=lambda: {\n            \"in_channels\": 64,\n            \"out_channels\": 1,\n            \"hidden_channels\": 32,\n            \"cond_channels\": 80,\n            \"upsample_factors\": [8, 8, 4],\n            \"lvc_layers_each_block\": 4,\n            \"lvc_kernel_size\": 3,\n            \"kpnet_hidden_channels\": 64,\n            \"kpnet_conv_size\": 3,\n            \"dropout\": 0.0,\n        }\n    )\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = False\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = False  # requires MelGAN Discriminators (MelGAN and univnet)\n    use_l1_spec_loss: bool = False\n\n    # loss weights - overrides\n    stft_loss_weight: float = 2.5\n    stft_loss_params: Dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 1\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 0\n    l1_spec_loss_weight: float = 0\n    l1_spec_loss_params: Dict = field(\n        default_factory=lambda: {\n            \"use_mel\": True,\n            \"sample_rate\": 22050,\n            \"n_fft\": 1024,\n            \"hop_length\": 256,\n            \"win_length\": 1024,\n            \"n_mels\": 80,\n            \"mel_fmin\": 0.0,\n            \"mel_fmax\": None,\n        }\n    )\n\n    # optimizer parameters\n    lr_gen: float = 1e-4  # Initial learning rate.\n    lr_disc: float = 1e-4  # Initial learning rate.\n    lr_scheduler_gen: str = None  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    # lr_scheduler_gen_params: dict = field(default_factory=lambda: {\"gamma\": 0.999, \"last_epoch\": -1})\n    lr_scheduler_disc: str = None  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    # lr_scheduler_disc_params: dict = field(default_factory=lambda: {\"gamma\": 0.999, \"last_epoch\": -1})\n    optimizer_params: Dict = field(default_factory=lambda: {\"betas\": [0.5, 0.9], \"weight_decay\": 0.0})\n    steps_to_start_discriminator: int = 200000\n\n    def __post_init__(self):\n        super().__post_init__()\n        self.generator_model_params[\"cond_channels\"] = self.audio.num_mels\n", "TTS/vocoder/configs/parallel_wavegan_config.py": "from dataclasses import dataclass, field\n\nfrom .shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass ParallelWaveganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for ParallelWavegan vocoder.\n\n    Args:\n        model (str):\n            Model name used for selecting the right configuration at initialization. Defaults to `gan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'parallel_wavegan_discriminator`.\n        discriminator_model_params (dict): The discriminator model kwargs. Defaults to\n            '{\"num_layers\": 10}`\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `parallel_wavegan_generator`.\n        generator_model_param (dict):\n            The generator model kwargs. Defaults to `{\"upsample_factors\": [4, 4, 4, 4], \"stacks\": 3, \"num_res_blocks\": 30}`.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 16.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        steps_to_start_discriminator (int):\n            Number of steps required to start training the discriminator. Defaults to 0.\n        use_stft_loss (bool):`\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by HifiGAN model. Defaults to False.\n        stft_loss_params (dict): STFT loss parameters. Default to\n            `{\"n_ffts\": [1024, 2048, 512], \"hop_lengths\": [120, 240, 50], \"win_lengths\": [600, 1200, 240]}`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 0.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        lr_gen (float):\n            Generator model initial learning rate. Defaults to 0.0002.\n        lr_disc (float):\n            Discriminator model initial learning rate. Defaults to 0.0002.\n        optimizer (torch.optim.Optimizer):\n            Optimizer used for the training. Defaults to `AdamW`.\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n        lr_scheduler_gen (torch.optim.Scheduler):\n            Learning rate scheduler for the generator. Defaults to `ExponentialLR`.\n        lr_scheduler_gen_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to `{\"gamma\": 0.5, \"step_size\": 200000, \"last_epoch\": -1}`.\n        lr_scheduler_disc (torch.optim.Scheduler):\n            Learning rate scheduler for the discriminator. Defaults to `ExponentialLR`.\n        lr_scheduler_dict_params (dict):\n            Parameters for the discriminator learning rate scheduler. Defaults to `{\"gamma\": 0.5, \"step_size\": 200000, \"last_epoch\": -1}`.\n    \"\"\"\n\n    model: str = \"parallel_wavegan\"\n\n    # Model specific params\n    discriminator_model: str = \"parallel_wavegan_discriminator\"\n    discriminator_model_params: dict = field(default_factory=lambda: {\"num_layers\": 10})\n    generator_model: str = \"parallel_wavegan_generator\"\n    generator_model_params: dict = field(\n        default_factory=lambda: {\"upsample_factors\": [4, 4, 4, 4], \"stacks\": 3, \"num_res_blocks\": 30}\n    )\n\n    # Training - overrides\n    batch_size: int = 6\n    seq_len: int = 25600\n    pad_short: int = 2000\n    use_noise_augment: bool = False\n    use_cache: bool = True\n    steps_to_start_discriminator: int = 200000\n    target_loss: str = \"loss_1\"\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = False\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = False  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = False\n\n    stft_loss_params: dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n\n    # loss weights - overrides\n    stft_loss_weight: float = 0.5\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 2.5\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 0\n    l1_spec_loss_weight: float = 0\n\n    # optimizer overrides\n    lr_gen: float = 0.0002  # Initial learning rate.\n    lr_disc: float = 0.0002  # Initial learning rate.\n    optimizer: str = \"AdamW\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.8, 0.99], \"weight_decay\": 0.0})\n    lr_scheduler_gen: str = \"StepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_gen_params: dict = field(default_factory=lambda: {\"gamma\": 0.5, \"step_size\": 200000, \"last_epoch\": -1})\n    lr_scheduler_disc: str = \"StepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_disc_params: dict = field(\n        default_factory=lambda: {\"gamma\": 0.5, \"step_size\": 200000, \"last_epoch\": -1}\n    )\n    scheduler_after_epoch: bool = False\n", "TTS/vocoder/configs/shared_configs.py": "from dataclasses import dataclass, field\n\nfrom TTS.config import BaseAudioConfig, BaseTrainingConfig\n\n\n@dataclass\nclass BaseVocoderConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the vocoder models.\n    Args:\n        audio (BaseAudioConfig):\n            Audio processor config instance. Defaultsto `BaseAudioConfig()`.\n        use_noise_augment (bool):\n            Augment the input audio with random noise. Defaults to False/\n        eval_split_size (int):\n            Number of instances used for evaluation. Defaults to 10.\n        data_path (str):\n            Root path of the training data. All the audio files found recursively from this root path are used for\n            training. Defaults to `\"\"`.\n        feature_path (str):\n            Root path to the precomputed feature files. Defaults to None.\n        seq_len (int):\n            Length of the waveform segments used for training. Defaults to 1000.\n        pad_short (int):\n            Extra padding for the waveforms shorter than `seq_len`. Defaults to 0.\n        conv_path (int):\n            Extra padding for the feature frames against convolution of the edge frames. Defaults to MISSING.\n            Defaults to 0.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. If the RAM is not enough, if may cause OOM.\n            Defaults to False.\n        epochs (int):\n            Number of training epochs to. Defaults to 10000.\n        wd (float):\n            Weight decay.\n         optimizer (torch.optim.Optimizer):\n            Optimizer used for the training. Defaults to `AdamW`.\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n    \"\"\"\n\n    audio: BaseAudioConfig = field(default_factory=BaseAudioConfig)\n    # dataloading\n    use_noise_augment: bool = False  # enable/disable random noise augmentation in spectrograms.\n    eval_split_size: int = 10  # number of samples used for evaluation.\n    # dataset\n    data_path: str = \"\"  # root data path. It finds all wav files recursively from there.\n    feature_path: str = None  # if you use precomputed features\n    seq_len: int = 1000  # signal length used in training.\n    pad_short: int = 0  # additional padding for short wavs\n    conv_pad: int = 0  # additional padding against convolutions applied to spectrograms\n    use_cache: bool = False  # use in memory cache to keep the computed features. This might cause OOM.\n    # OPTIMIZER\n    epochs: int = 10000  # total number of epochs to train.\n    wd: float = 0.0  # Weight decay weight.\n    optimizer: str = \"AdamW\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.8, 0.99], \"weight_decay\": 0.0})\n\n\n@dataclass\nclass BaseGANVocoderConfig(BaseVocoderConfig):\n    \"\"\"Base config class used among all the GAN based vocoders.\n    Args:\n        use_stft_loss (bool):\n            enable / disable the use of STFT loss. Defaults to True.\n        use_subband_stft_loss (bool):\n            enable / disable the use of Subband STFT loss. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable the use of Mean Squared Error based GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable the use of Hinge GAN loss. Defaults to True.\n        use_feat_match_loss (bool):\n            enable / disable feature matching loss. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable L1 spectrogram loss. Defaults to True.\n        stft_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 0.\n        subband_stft_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 0.\n        mse_G_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 1.\n        hinge_G_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 0.\n        feat_match_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 100.\n        l1_spec_loss_weight (float):\n            Loss weight that multiplies the computed loss value. Defaults to 45.\n        stft_loss_params (dict):\n            Parameters for the STFT loss. Defaults to `{\"n_ffts\": [1024, 2048, 512], \"hop_lengths\": [120, 240, 50], \"win_lengths\": [600, 1200, 240]}`.\n        l1_spec_loss_params (dict):\n            Parameters for the L1 spectrogram loss. Defaults to\n            `{\n                \"use_mel\": True,\n                \"sample_rate\": 22050,\n                \"n_fft\": 1024,\n                \"hop_length\": 256,\n                \"win_length\": 1024,\n                \"n_mels\": 80,\n                \"mel_fmin\": 0.0,\n                \"mel_fmax\": None,\n            }`\n        target_loss (str):\n            Target loss name that defines the quality of the model. Defaults to `G_avg_loss`.\n        grad_clip (list):\n            A list of gradient clipping theresholds for each optimizer. Any value less than 0 disables clipping.\n            Defaults to [5, 5].\n        lr_gen (float):\n            Generator model initial learning rate. Defaults to 0.0002.\n        lr_disc (float):\n            Discriminator model initial learning rate. Defaults to 0.0002.\n        lr_scheduler_gen (torch.optim.Scheduler):\n            Learning rate scheduler for the generator. Defaults to `ExponentialLR`.\n        lr_scheduler_gen_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to `{\"gamma\": 0.999, \"last_epoch\": -1}`.\n        lr_scheduler_disc (torch.optim.Scheduler):\n            Learning rate scheduler for the discriminator. Defaults to `ExponentialLR`.\n        lr_scheduler_disc_params (dict):\n            Parameters for the discriminator learning rate scheduler. Defaults to `{\"gamma\": 0.999, \"last_epoch\": -1}`.\n        scheduler_after_epoch (bool):\n            Whether to update the learning rate schedulers after each epoch. Defaults to True.\n        use_pqmf (bool):\n            enable / disable PQMF for subband approximation at training. Defaults to False.\n        steps_to_start_discriminator (int):\n            Number of steps required to start training the discriminator. Defaults to 0.\n        diff_samples_for_G_and_D (bool):\n            enable / disable use of different training samples for the generator and the discriminator iterations.\n            Enabling it results in slower iterations but faster convergance in some cases. Defaults to False.\n    \"\"\"\n\n    model: str = \"gan\"\n\n    # LOSS PARAMETERS\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = True\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = True\n    use_feat_match_loss: bool = True  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = True\n\n    # loss weights\n    stft_loss_weight: float = 0\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 1\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 100\n    l1_spec_loss_weight: float = 45\n\n    stft_loss_params: dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n\n    l1_spec_loss_params: dict = field(\n        default_factory=lambda: {\n            \"use_mel\": True,\n            \"sample_rate\": 22050,\n            \"n_fft\": 1024,\n            \"hop_length\": 256,\n            \"win_length\": 1024,\n            \"n_mels\": 80,\n            \"mel_fmin\": 0.0,\n            \"mel_fmax\": None,\n        }\n    )\n\n    target_loss: str = \"loss_0\"  # loss value to pick the best model to save after each epoch\n\n    # optimizer\n    grad_clip: float = field(default_factory=lambda: [5, 5])\n    lr_gen: float = 0.0002  # Initial learning rate.\n    lr_disc: float = 0.0002  # Initial learning rate.\n    lr_scheduler_gen: str = \"ExponentialLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_gen_params: dict = field(default_factory=lambda: {\"gamma\": 0.999, \"last_epoch\": -1})\n    lr_scheduler_disc: str = \"ExponentialLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_disc_params: dict = field(default_factory=lambda: {\"gamma\": 0.999, \"last_epoch\": -1})\n    scheduler_after_epoch: bool = True\n\n    use_pqmf: bool = False  # enable/disable using pqmf for multi-band training. (Multi-band MelGAN)\n    steps_to_start_discriminator = 0  # start training the discriminator after this number of steps.\n    diff_samples_for_G_and_D: bool = False  # use different samples for G and D training steps.\n", "TTS/vocoder/configs/__init__.py": "import importlib\nimport os\nfrom inspect import isclass\n\n# import all files under configs/\nconfigs_dir = os.path.dirname(__file__)\nfor file in os.listdir(configs_dir):\n    path = os.path.join(configs_dir, file)\n    if not file.startswith(\"_\") and not file.startswith(\".\") and (file.endswith(\".py\") or os.path.isdir(path)):\n        config_name = file[: file.find(\".py\")] if file.endswith(\".py\") else file\n        module = importlib.import_module(\"TTS.vocoder.configs.\" + config_name)\n        for attribute_name in dir(module):\n            attribute = getattr(module, attribute_name)\n\n            if isclass(attribute):\n                # Add the class to this package's variables\n                globals()[attribute_name] = attribute\n", "TTS/vocoder/configs/fullband_melgan_config.py": "from dataclasses import dataclass, field\n\nfrom .shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass FullbandMelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for FullBand MelGAN vocoder.\n\n    Example:\n\n        >>> from TTS.vocoder.configs import FullbandMelganConfig\n        >>> config = FullbandMelganConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fullband_melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.\n        discriminator_model_params (dict): The discriminator model parameters. Defaults to\n            '{\"base_channels\": 16, \"max_channels\": 1024, \"downsample_factors\": [4, 4, 4, 4]}`\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `melgan_generator`.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 16.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        use_stft_loss (bool):\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by HifiGAN model. Defaults to False.\n        stft_loss_params (dict): STFT loss parameters. Default to\n        `{\"n_ffts\": [1024, 2048, 512], \"hop_lengths\": [120, 240, 50], \"win_lengths\": [600, 1200, 240]}`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 108.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n    \"\"\"\n\n    model: str = \"fullband_melgan\"\n\n    # Model specific params\n    discriminator_model: str = \"melgan_multiscale_discriminator\"\n    discriminator_model_params: dict = field(\n        default_factory=lambda: {\"base_channels\": 16, \"max_channels\": 512, \"downsample_factors\": [4, 4, 4]}\n    )\n    generator_model: str = \"melgan_generator\"\n    generator_model_params: dict = field(\n        default_factory=lambda: {\"upsample_factors\": [8, 8, 2, 2], \"num_res_blocks\": 4}\n    )\n\n    # Training - overrides\n    batch_size: int = 16\n    seq_len: int = 8192\n    pad_short: int = 2000\n    use_noise_augment: bool = True\n    use_cache: bool = True\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = False\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = True  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = False\n\n    stft_loss_params: dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n\n    # loss weights - overrides\n    stft_loss_weight: float = 0.5\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 2.5\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 108\n    l1_spec_loss_weight: float = 0.0\n", "TTS/vocoder/configs/wavernn_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.vocoder.configs.shared_configs import BaseVocoderConfig\nfrom TTS.vocoder.models.wavernn import WavernnArgs\n\n\n@dataclass\nclass WavernnConfig(BaseVocoderConfig):\n    \"\"\"Defines parameters for Wavernn vocoder.\n    Example:\n\n        >>> from TTS.vocoder.configs import WavernnConfig\n        >>> config = WavernnConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `wavernn`.\n        mode (str):\n            Output mode of the WaveRNN vocoder. `mold` for Mixture of Logistic Distribution, `gauss` for a single\n            Gaussian Distribution and `bits` for quantized bits as the model's output.\n        mulaw (bool):\n            enable / disable the use of Mulaw quantization for training. Only applicable if `mode == 'bits'`. Defaults\n            to `True`.\n        generator_model (str):\n            One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `WaveRNN`.\n        wavernn_model_params (dict):\n            kwargs for the WaveRNN model. Defaults to\n            `{\n                \"rnn_dims\": 512,\n                \"fc_dims\": 512,\n                \"compute_dims\": 128,\n                \"res_out_dims\": 128,\n                \"num_res_blocks\": 10,\n                \"use_aux_net\": True,\n                \"use_upsample_net\": True,\n                \"upsample_factors\": [4, 8, 8]\n            }`\n        batched (bool):\n            enable / disable the batched inference. It speeds up the inference by splitting the input into segments and\n            processing the segments in a batch. Then it merges the outputs with a certain overlap and smoothing. If\n            you set it False, without CUDA, it is too slow to be practical. Defaults to True.\n        target_samples (int):\n            Size of the segments in batched mode. Defaults to 11000.\n        overlap_sampels (int):\n            Size of the overlap between consecutive segments. Defaults to 550.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 256.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 1280.\n\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        mixed_precision (bool):\n            enable / disable mixed precision training. Default is True.\n        eval_split_size (int):\n            Number of samples used for evalutaion. Defaults to 50.\n        num_epochs_before_test (int):\n            Number of epochs waited to run the next evalution. Since inference takes some time, it is better to\n            wait some number of epochs not ot waste training time. Defaults to 10.\n        grad_clip (float):\n            Gradient clipping threshold. If <= 0.0, no clipping is applied. Defaults to 4.0\n        lr (float):\n            Initila leraning rate. Defaults to 1e-4.\n        lr_scheduler (str):\n            One of the learning rate schedulers from `torch.optim.scheduler.*`. Defaults to `MultiStepLR`.\n        lr_scheduler_params (dict):\n            kwargs for the scheduler. Defaults to `{\"gamma\": 0.5, \"milestones\": [200000, 400000, 600000]}`\n    \"\"\"\n\n    model: str = \"wavernn\"\n\n    # Model specific params\n    model_args: WavernnArgs = field(default_factory=WavernnArgs)\n    target_loss: str = \"loss\"\n\n    # Inference\n    batched: bool = True\n    target_samples: int = 11000\n    overlap_samples: int = 550\n\n    # Training - overrides\n    epochs: int = 10000\n    batch_size: int = 256\n    seq_len: int = 1280\n    use_noise_augment: bool = False\n    use_cache: bool = True\n    mixed_precision: bool = True\n    eval_split_size: int = 50\n    num_epochs_before_test: int = (\n        10  # number of epochs to wait until the next test run (synthesizing a full audio clip).\n    )\n\n    # optimizer overrides\n    grad_clip: float = 4.0\n    lr: float = 1e-4  # Initial learning rate.\n    lr_scheduler: str = \"MultiStepLR\"  # one of the schedulers from https:#pytorch.org/docs/stable/optim.html\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"gamma\": 0.5, \"milestones\": [200000, 400000, 600000]})\n", "TTS/vocoder/configs/melgan_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.vocoder.configs.shared_configs import BaseGANVocoderConfig\n\n\n@dataclass\nclass MelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for MelGAN vocoder.\n\n    Example:\n\n        >>> from TTS.vocoder.configs import MelganConfig\n        >>> config = MelganConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.\n        discriminator_model_params (dict): The discriminator model parameters. Defaults to\n            '{\"base_channels\": 16, \"max_channels\": 1024, \"downsample_factors\": [4, 4, 4, 4]}`\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `melgan_generator`.\n        batch_size (int):\n            Batch size used at training. Larger values use more memory. Defaults to 16.\n        seq_len (int):\n            Audio segment length used at training. Larger values use more memory. Defaults to 8192.\n        pad_short (int):\n            Additional padding applied to the audio samples shorter than `seq_len`. Defaults to 0.\n        use_noise_augment (bool):\n            enable / disable random noise added to the input waveform. The noise is added after computing the\n            features. Defaults to True.\n        use_cache (bool):\n            enable / disable in memory caching of the computed features. It can cause OOM error if the system RAM is\n            not large enough. Defaults to True.\n        use_stft_loss (bool):\n            enable / disable use of STFT loss originally used by ParallelWaveGAN model. Defaults to True.\n        use_subband_stft (bool):\n            enable / disable use of subband loss computation originally used by MultiBandMelgan model. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable using Mean Squeare Error GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):\n            enable / disable using Hinge GAN loss. You should choose either Hinge or MSE loss for training GAN models.\n            Defaults to False.\n        use_feat_match_loss (bool):\n            enable / disable using Feature Matching loss originally used by MelGAN model. Defaults to True.\n        use_l1_spec_loss (bool):\n            enable / disable using L1 spectrogram loss originally used by HifiGAN model. Defaults to False.\n        stft_loss_params (dict): STFT loss parameters. Default to\n        `{\"n_ffts\": [1024, 2048, 512], \"hop_lengths\": [120, 240, 50], \"win_lengths\": [600, 1200, 240]}`\n        stft_loss_weight (float): STFT loss weight that multiplies the computed loss before summing up the total\n            model loss. Defaults to 0.5.\n        subband_stft_loss_weight (float):\n            Subband STFT loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        mse_G_loss_weight (float):\n            MSE generator loss weight that multiplies the computed loss before summing up the total loss. faults to 2.5.\n        hinge_G_loss_weight (float):\n            Hinge generator loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n        feat_match_loss_weight (float):\n            Feature matching loss weight that multiplies the computed loss before summing up the total loss. faults to 108.\n        l1_spec_loss_weight (float):\n            L1 spectrogram loss weight that multiplies the computed loss before summing up the total loss. Defaults to 0.\n    \"\"\"\n\n    model: str = \"melgan\"\n\n    # Model specific params\n    discriminator_model: str = \"melgan_multiscale_discriminator\"\n    discriminator_model_params: dict = field(\n        default_factory=lambda: {\"base_channels\": 16, \"max_channels\": 1024, \"downsample_factors\": [4, 4, 4, 4]}\n    )\n    generator_model: str = \"melgan_generator\"\n    generator_model_params: dict = field(\n        default_factory=lambda: {\"upsample_factors\": [8, 8, 2, 2], \"num_res_blocks\": 3}\n    )\n\n    # Training - overrides\n    batch_size: int = 16\n    seq_len: int = 8192\n    pad_short: int = 2000\n    use_noise_augment: bool = True\n    use_cache: bool = True\n\n    # LOSS PARAMETERS - overrides\n    use_stft_loss: bool = True\n    use_subband_stft_loss: bool = False\n    use_mse_gan_loss: bool = True\n    use_hinge_gan_loss: bool = False\n    use_feat_match_loss: bool = True  # requires MelGAN Discriminators (MelGAN and HifiGAN)\n    use_l1_spec_loss: bool = False\n\n    stft_loss_params: dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n\n    # loss weights - overrides\n    stft_loss_weight: float = 0.5\n    subband_stft_loss_weight: float = 0\n    mse_G_loss_weight: float = 2.5\n    hinge_G_loss_weight: float = 0\n    feat_match_loss_weight: float = 108\n    l1_spec_loss_weight: float = 0\n", "TTS/server/__init__.py": "", "TTS/server/server.py": "#!flask/bin/python\nimport argparse\nimport io\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Union\nfrom urllib.parse import parse_qs\n\nfrom flask import Flask, render_template, render_template_string, request, send_file\n\nfrom TTS.config import load_config\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\n\n\ndef create_argparser():\n    def convert_boolean(x):\n        return x.lower() in [\"true\", \"1\", \"yes\"]\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--list_models\",\n        type=convert_boolean,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"list available pre-trained tts and vocoder models.\",\n    )\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=\"tts_models/en/ljspeech/tacotron2-DDC\",\n        help=\"Name of one of the pre-trained tts models in format <language>/<dataset>/<model_name>\",\n    )\n    parser.add_argument(\"--vocoder_name\", type=str, default=None, help=\"name of one of the released vocoder models.\")\n\n    # Args for running custom models\n    parser.add_argument(\"--config_path\", default=None, type=str, help=\"Path to model config file.\")\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        default=None,\n        help=\"Path to model file.\",\n    )\n    parser.add_argument(\n        \"--vocoder_path\",\n        type=str,\n        help=\"Path to vocoder model file. If it is not defined, model uses GL as vocoder. Please make sure that you installed vocoder library before (WaveRNN).\",\n        default=None,\n    )\n    parser.add_argument(\"--vocoder_config_path\", type=str, help=\"Path to vocoder model config file.\", default=None)\n    parser.add_argument(\"--speakers_file_path\", type=str, help=\"JSON file for multi-speaker model.\", default=None)\n    parser.add_argument(\"--port\", type=int, default=5002, help=\"port to listen on.\")\n    parser.add_argument(\"--use_cuda\", type=convert_boolean, default=False, help=\"true to use CUDA.\")\n    parser.add_argument(\"--debug\", type=convert_boolean, default=False, help=\"true to enable Flask debug mode.\")\n    parser.add_argument(\"--show_details\", type=convert_boolean, default=False, help=\"Generate model detail page.\")\n    return parser\n\n\n# parse the args\nargs = create_argparser().parse_args()\n\npath = Path(__file__).parent / \"../.models.json\"\nmanager = ModelManager(path)\n\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n\n# update in-use models to the specified released models.\nmodel_path = None\nconfig_path = None\nspeakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None\n\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:\n    model_path, config_path, model_item = manager.download_model(args.model_name)\n    args.vocoder_name = model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name\n\nif args.vocoder_name is not None and not args.vocoder_path:\n    vocoder_path, vocoder_config_path, _ = manager.download_model(args.vocoder_name)\n\n# CASE3: set custom model paths\nif args.model_path is not None:\n    model_path = args.model_path\n    config_path = args.config_path\n    speakers_file_path = args.speakers_file_path\n\nif args.vocoder_path is not None:\n    vocoder_path = args.vocoder_path\n    vocoder_config_path = args.vocoder_config_path\n\n# load models\nsynthesizer = Synthesizer(\n    tts_checkpoint=model_path,\n    tts_config_path=config_path,\n    tts_speakers_file=speakers_file_path,\n    tts_languages_file=None,\n    vocoder_checkpoint=vocoder_path,\n    vocoder_config=vocoder_config_path,\n    encoder_checkpoint=\"\",\n    encoder_config=\"\",\n    use_cuda=args.use_cuda,\n)\n\nuse_multi_speaker = hasattr(synthesizer.tts_model, \"num_speakers\") and (\n    synthesizer.tts_model.num_speakers > 1 or synthesizer.tts_speakers_file is not None\n)\nspeaker_manager = getattr(synthesizer.tts_model, \"speaker_manager\", None)\n\nuse_multi_language = hasattr(synthesizer.tts_model, \"num_languages\") and (\n    synthesizer.tts_model.num_languages > 1 or synthesizer.tts_languages_file is not None\n)\nlanguage_manager = getattr(synthesizer.tts_model, \"language_manager\", None)\n\n# TODO: set this from SpeakerManager\nuse_gst = synthesizer.tts_config.get(\"use_gst\", False)\napp = Flask(__name__)\n\n\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)\n\n    Args:\n        style_wav (str): uri\n\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"\n    if style_wav:\n        if os.path.isfile(style_wav) and style_wav.endswith(\".wav\"):\n            return style_wav  # style_wav is a .wav file located on the server\n\n        style_wav = json.loads(style_wav)\n        return style_wav  # style_wav is a gst dictionary with {token1_id : token1_weigth, ...}\n    return None\n\n\n@app.route(\"/\")\ndef index():\n    return render_template(\n        \"index.html\",\n        show_details=args.show_details,\n        use_multi_speaker=use_multi_speaker,\n        use_multi_language=use_multi_language,\n        speaker_ids=speaker_manager.name_to_id if speaker_manager is not None else None,\n        language_ids=language_manager.name_to_id if language_manager is not None else None,\n        use_gst=use_gst,\n    )\n\n\n@app.route(\"/details\")\ndef details():\n    if args.config_path is not None and os.path.isfile(args.config_path):\n        model_config = load_config(args.config_path)\n    else:\n        if args.model_name is not None:\n            model_config = load_config(config_path)\n\n    if args.vocoder_config_path is not None and os.path.isfile(args.vocoder_config_path):\n        vocoder_config = load_config(args.vocoder_config_path)\n    else:\n        if args.vocoder_name is not None:\n            vocoder_config = load_config(vocoder_config_path)\n        else:\n            vocoder_config = None\n\n    return render_template(\n        \"details.html\",\n        show_details=args.show_details,\n        model_config=model_config,\n        vocoder_config=vocoder_config,\n        args=args.__dict__,\n    )\n\n\nlock = Lock()\n\n\n@app.route(\"/api/tts\", methods=[\"GET\", \"POST\"])\ndef tts():\n    with lock:\n        text = request.headers.get(\"text\") or request.values.get(\"text\", \"\")\n        speaker_idx = request.headers.get(\"speaker-id\") or request.values.get(\"speaker_id\", \"\")\n        language_idx = request.headers.get(\"language-id\") or request.values.get(\"language_id\", \"\")\n        style_wav = request.headers.get(\"style-wav\") or request.values.get(\"style_wav\", \"\")\n        style_wav = style_wav_uri_to_dict(style_wav)\n\n        print(f\" > Model input: {text}\")\n        print(f\" > Speaker Idx: {speaker_idx}\")\n        print(f\" > Language Idx: {language_idx}\")\n        wavs = synthesizer.tts(text, speaker_name=speaker_idx, language_name=language_idx, style_wav=style_wav)\n        out = io.BytesIO()\n        synthesizer.save_wav(wavs, out)\n    return send_file(out, mimetype=\"audio/wav\")\n\n\n# Basic MaryTTS compatibility layer\n\n\n@app.route(\"/locales\", methods=[\"GET\"])\ndef mary_tts_api_locales():\n    \"\"\"MaryTTS-compatible /locales endpoint\"\"\"\n    # NOTE: We currently assume there is only one model active at the same time\n    if args.model_name is not None:\n        model_details = args.model_name.split(\"/\")\n    else:\n        model_details = [\"\", \"en\", \"\", \"default\"]\n    return render_template_string(\"{{ locale }}\\n\", locale=model_details[1])\n\n\n@app.route(\"/voices\", methods=[\"GET\"])\ndef mary_tts_api_voices():\n    \"\"\"MaryTTS-compatible /voices endpoint\"\"\"\n    # NOTE: We currently assume there is only one model active at the same time\n    if args.model_name is not None:\n        model_details = args.model_name.split(\"/\")\n    else:\n        model_details = [\"\", \"en\", \"\", \"default\"]\n    return render_template_string(\n        \"{{ name }} {{ locale }} {{ gender }}\\n\", name=model_details[3], locale=model_details[1], gender=\"u\"\n    )\n\n\n@app.route(\"/process\", methods=[\"GET\", \"POST\"])\ndef mary_tts_api_process():\n    \"\"\"MaryTTS-compatible /process endpoint\"\"\"\n    with lock:\n        if request.method == \"POST\":\n            data = parse_qs(request.get_data(as_text=True))\n            # NOTE: we ignore param. LOCALE and VOICE for now since we have only one active model\n            text = data.get(\"INPUT_TEXT\", [\"\"])[0]\n        else:\n            text = request.args.get(\"INPUT_TEXT\", \"\")\n        print(f\" > Model input: {text}\")\n        wavs = synthesizer.tts(text)\n        out = io.BytesIO()\n        synthesizer.save_wav(wavs, out)\n    return send_file(out, mimetype=\"audio/wav\")\n\n\ndef main():\n    app.run(debug=args.debug, host=\"::\", port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/encoder/losses.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\n# adapted from https://github.com/cvqluu/GE2E-Loss\nclass GE2ELoss(nn.Module):\n    def __init__(self, init_w=10.0, init_b=-5.0, loss_method=\"softmax\"):\n        \"\"\"\n        Implementation of the Generalized End-to-End loss defined in https://arxiv.org/abs/1710.10467 [1]\n        Accepts an input of size (N, M, D)\n            where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector (e.g. d-vector)\n        Args:\n            - init_w (float): defines the initial value of w in Equation (5) of [1]\n            - init_b (float): definies the initial value of b in Equation (5) of [1]\n        \"\"\"\n        super().__init__()\n        # pylint: disable=E1102\n        self.w = nn.Parameter(torch.tensor(init_w))\n        # pylint: disable=E1102\n        self.b = nn.Parameter(torch.tensor(init_b))\n        self.loss_method = loss_method\n\n        print(\" > Initialized Generalized End-to-End loss\")\n\n        assert self.loss_method in [\"softmax\", \"contrast\"]\n\n        if self.loss_method == \"softmax\":\n            self.embed_loss = self.embed_loss_softmax\n        if self.loss_method == \"contrast\":\n            self.embed_loss = self.embed_loss_contrast\n\n    # pylint: disable=R0201\n    def calc_new_centroids(self, dvecs, centroids, spkr, utt):\n        \"\"\"\n        Calculates the new centroids excluding the reference utterance\n        \"\"\"\n        excl = torch.cat((dvecs[spkr, :utt], dvecs[spkr, utt + 1 :]))\n        excl = torch.mean(excl, 0)\n        new_centroids = []\n        for i, centroid in enumerate(centroids):\n            if i == spkr:\n                new_centroids.append(excl)\n            else:\n                new_centroids.append(centroid)\n        return torch.stack(new_centroids)\n\n    def calc_cosine_sim(self, dvecs, centroids):\n        \"\"\"\n        Make the cosine similarity matrix with dims (N,M,N)\n        \"\"\"\n        cos_sim_matrix = []\n        for spkr_idx, speaker in enumerate(dvecs):\n            cs_row = []\n            for utt_idx, utterance in enumerate(speaker):\n                new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n                # vector based cosine similarity for speed\n                cs_row.append(\n                    torch.clamp(\n                        torch.mm(\n                            utterance.unsqueeze(1).transpose(0, 1),\n                            new_centroids.transpose(0, 1),\n                        )\n                        / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)),\n                        1e-6,\n                    )\n                )\n            cs_row = torch.cat(cs_row, dim=0)\n            cos_sim_matrix.append(cs_row)\n        return torch.stack(cos_sim_matrix)\n\n    # pylint: disable=R0201\n    def embed_loss_softmax(self, dvecs, cos_sim_matrix):\n        \"\"\"\n        Calculates the loss on each embedding $L(e_{ji})$ by taking softmax\n        \"\"\"\n        N, M, _ = dvecs.shape\n        L = []\n        for j in range(N):\n            L_row = []\n            for i in range(M):\n                L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0)[j])\n            L_row = torch.stack(L_row)\n            L.append(L_row)\n        return torch.stack(L)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n        \"\"\"\n        Calculates the loss on each embedding $L(e_{ji})$ by contrast loss with closest centroid\n        \"\"\"\n        N, M, _ = dvecs.shape\n        L = []\n        for j in range(N):\n            L_row = []\n            for i in range(M):\n                centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n                excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:j], centroids_sigmoids[j + 1 :]))\n                L_row.append(1.0 - torch.sigmoid(cos_sim_matrix[j, i, j]) + torch.max(excl_centroids_sigmoids))\n            L_row = torch.stack(L_row)\n            L.append(L_row)\n        return torch.stack(L)\n\n    def forward(self, x, _label=None):\n        \"\"\"\n        Calculates the GE2E loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n        \"\"\"\n\n        assert x.size()[1] >= 2\n\n        centroids = torch.mean(x, 1)\n        cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n        torch.clamp(self.w, 1e-6)\n        cos_sim_matrix = self.w * cos_sim_matrix + self.b\n        L = self.embed_loss(x, cos_sim_matrix)\n        return L.mean()\n\n\n# adapted from https://github.com/clovaai/voxceleb_trainer/blob/master/loss/angleproto.py\nclass AngleProtoLoss(nn.Module):\n    \"\"\"\n    Implementation of the Angular Prototypical loss defined in https://arxiv.org/abs/2003.11982\n        Accepts an input of size (N, M, D)\n            where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector\n        Args:\n            - init_w (float): defines the initial value of w\n            - init_b (float): definies the initial value of b\n    \"\"\"\n\n    def __init__(self, init_w=10.0, init_b=-5.0):\n        super().__init__()\n        # pylint: disable=E1102\n        self.w = nn.Parameter(torch.tensor(init_w))\n        # pylint: disable=E1102\n        self.b = nn.Parameter(torch.tensor(init_b))\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        print(\" > Initialized Angular Prototypical loss\")\n\n    def forward(self, x, _label=None):\n        \"\"\"\n        Calculates the AngleProto loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n        \"\"\"\n\n        assert x.size()[1] >= 2\n\n        out_anchor = torch.mean(x[:, 1:, :], 1)\n        out_positive = x[:, 0, :]\n        num_speakers = out_anchor.size()[0]\n\n        cos_sim_matrix = F.cosine_similarity(\n            out_positive.unsqueeze(-1).expand(-1, -1, num_speakers),\n            out_anchor.unsqueeze(-1).expand(-1, -1, num_speakers).transpose(0, 2),\n        )\n        torch.clamp(self.w, 1e-6)\n        cos_sim_matrix = cos_sim_matrix * self.w + self.b\n        label = torch.arange(num_speakers).to(cos_sim_matrix.device)\n        L = self.criterion(cos_sim_matrix, label)\n        return L\n\n\nclass SoftmaxLoss(nn.Module):\n    \"\"\"\n    Implementation of the Softmax loss as defined in https://arxiv.org/abs/2003.11982\n        Args:\n            - embedding_dim (float): speaker embedding dim\n            - n_speakers (float): number of speakers\n    \"\"\"\n\n    def __init__(self, embedding_dim, n_speakers):\n        super().__init__()\n\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.fc = nn.Linear(embedding_dim, n_speakers)\n\n        print(\"Initialised Softmax Loss\")\n\n    def forward(self, x, label=None):\n        # reshape for compatibility\n        x = x.reshape(-1, x.size()[-1])\n        label = label.reshape(-1)\n\n        x = self.fc(x)\n        L = self.criterion(x, label)\n\n        return L\n\n    def inference(self, embedding):\n        x = self.fc(embedding)\n        activations = torch.nn.functional.softmax(x, dim=1).squeeze(0)\n        class_id = torch.argmax(activations)\n        return class_id\n\n\nclass SoftmaxAngleProtoLoss(nn.Module):\n    \"\"\"\n    Implementation of the Softmax AnglePrototypical loss as defined in https://arxiv.org/abs/2009.14153\n        Args:\n            - embedding_dim (float): speaker embedding dim\n            - n_speakers (float): number of speakers\n            - init_w (float): defines the initial value of w\n            - init_b (float): definies the initial value of b\n    \"\"\"\n\n    def __init__(self, embedding_dim, n_speakers, init_w=10.0, init_b=-5.0):\n        super().__init__()\n\n        self.softmax = SoftmaxLoss(embedding_dim, n_speakers)\n        self.angleproto = AngleProtoLoss(init_w, init_b)\n\n        print(\"Initialised SoftmaxAnglePrototypical Loss\")\n\n    def forward(self, x, label=None):\n        \"\"\"\n        Calculates the SoftmaxAnglePrototypical loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n        \"\"\"\n\n        Lp = self.angleproto(x)\n\n        Ls = self.softmax(x, label)\n\n        return Ls + Lp\n", "TTS/encoder/__init__.py": "", "TTS/encoder/models/base_encoder.py": "import numpy as np\nimport torch\nimport torchaudio\nfrom coqpit import Coqpit\nfrom torch import nn\n\nfrom TTS.encoder.losses import AngleProtoLoss, GE2ELoss, SoftmaxAngleProtoLoss\nfrom TTS.utils.generic_utils import set_init_dict\nfrom TTS.utils.io import load_fsspec\n\n\nclass PreEmphasis(nn.Module):\n    def __init__(self, coefficient=0.97):\n        super().__init__()\n        self.coefficient = coefficient\n        self.register_buffer(\"filter\", torch.FloatTensor([-self.coefficient, 1.0]).unsqueeze(0).unsqueeze(0))\n\n    def forward(self, x):\n        assert len(x.size()) == 2\n\n        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), \"reflect\")\n        return torch.nn.functional.conv1d(x, self.filter).squeeze(1)\n\n\nclass BaseEncoder(nn.Module):\n    \"\"\"Base `encoder` class. Every new `encoder` model must inherit this.\n\n    It defines common `encoder` specific functions.\n    \"\"\"\n\n    # pylint: disable=W0102\n    def __init__(self):\n        super(BaseEncoder, self).__init__()\n\n    def get_torch_mel_spectrogram_class(self, audio_config):\n        return torch.nn.Sequential(\n            PreEmphasis(audio_config[\"preemphasis\"]),\n            # TorchSTFT(\n            #     n_fft=audio_config[\"fft_size\"],\n            #     hop_length=audio_config[\"hop_length\"],\n            #     win_length=audio_config[\"win_length\"],\n            #     sample_rate=audio_config[\"sample_rate\"],\n            #     window=\"hamming_window\",\n            #     mel_fmin=0.0,\n            #     mel_fmax=None,\n            #     use_htk=True,\n            #     do_amp_to_db=False,\n            #     n_mels=audio_config[\"num_mels\"],\n            #     power=2.0,\n            #     use_mel=True,\n            #     mel_norm=None,\n            # )\n            torchaudio.transforms.MelSpectrogram(\n                sample_rate=audio_config[\"sample_rate\"],\n                n_fft=audio_config[\"fft_size\"],\n                win_length=audio_config[\"win_length\"],\n                hop_length=audio_config[\"hop_length\"],\n                window_fn=torch.hamming_window,\n                n_mels=audio_config[\"num_mels\"],\n            ),\n        )\n\n    @torch.no_grad()\n    def inference(self, x, l2_norm=True):\n        return self.forward(x, l2_norm)\n\n    @torch.no_grad()\n    def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True):\n        \"\"\"\n        Generate embeddings for a batch of utterances\n        x: 1xTxD\n        \"\"\"\n        # map to the waveform size\n        if self.use_torch_spec:\n            num_frames = num_frames * self.audio_config[\"hop_length\"]\n\n        max_len = x.shape[1]\n\n        if max_len < num_frames:\n            num_frames = max_len\n\n        offsets = np.linspace(0, max_len - num_frames, num=num_eval)\n\n        frames_batch = []\n        for offset in offsets:\n            offset = int(offset)\n            end_offset = int(offset + num_frames)\n            frames = x[:, offset:end_offset]\n            frames_batch.append(frames)\n\n        frames_batch = torch.cat(frames_batch, dim=0)\n        embeddings = self.inference(frames_batch, l2_norm=l2_norm)\n\n        if return_mean:\n            embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n        return embeddings\n\n    def get_criterion(self, c: Coqpit, num_classes=None):\n        if c.loss == \"ge2e\":\n            criterion = GE2ELoss(loss_method=\"softmax\")\n        elif c.loss == \"angleproto\":\n            criterion = AngleProtoLoss()\n        elif c.loss == \"softmaxproto\":\n            criterion = SoftmaxAngleProtoLoss(c.model_params[\"proj_dim\"], num_classes)\n        else:\n            raise Exception(\"The %s  not is a loss supported\" % c.loss)\n        return criterion\n\n    def load_checkpoint(\n        self,\n        config: Coqpit,\n        checkpoint_path: str,\n        eval: bool = False,\n        use_cuda: bool = False,\n        criterion=None,\n        cache=False,\n    ):\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        try:\n            self.load_state_dict(state[\"model\"])\n            print(\" > Model fully restored. \")\n        except (KeyError, RuntimeError) as error:\n            # If eval raise the error\n            if eval:\n                raise error\n\n            print(\" > Partial model initialization.\")\n            model_dict = self.state_dict()\n            model_dict = set_init_dict(model_dict, state[\"model\"], c)\n            self.load_state_dict(model_dict)\n            del model_dict\n\n        # load the criterion for restore_path\n        if criterion is not None and \"criterion\" in state:\n            try:\n                criterion.load_state_dict(state[\"criterion\"])\n            except (KeyError, RuntimeError) as error:\n                print(\" > Criterion load ignored because of:\", error)\n\n        # instance and load the criterion for the encoder classifier in inference time\n        if (\n            eval\n            and criterion is None\n            and \"criterion\" in state\n            and getattr(config, \"map_classid_to_classname\", None) is not None\n        ):\n            criterion = self.get_criterion(config, len(config.map_classid_to_classname))\n            criterion.load_state_dict(state[\"criterion\"])\n\n        if use_cuda:\n            self.cuda()\n            if criterion is not None:\n                criterion = criterion.cuda()\n\n        if eval:\n            self.eval()\n            assert not self.training\n\n        if not eval:\n            return criterion, state[\"step\"]\n        return criterion\n", "TTS/encoder/models/lstm.py": "import torch\nfrom torch import nn\n\nfrom TTS.encoder.models.base_encoder import BaseEncoder\n\n\nclass LSTMWithProjection(nn.Module):\n    def __init__(self, input_size, hidden_size, proj_size):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.proj_size = proj_size\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.linear = nn.Linear(hidden_size, proj_size, bias=False)\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        o, (_, _) = self.lstm(x)\n        return self.linear(o)\n\n\nclass LSTMWithoutProjection(nn.Module):\n    def __init__(self, input_dim, lstm_dim, proj_dim, num_lstm_layers):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_dim, num_layers=num_lstm_layers, batch_first=True)\n        self.linear = nn.Linear(lstm_dim, proj_dim, bias=True)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        _, (hidden, _) = self.lstm(x)\n        return self.relu(self.linear(hidden[-1]))\n\n\nclass LSTMSpeakerEncoder(BaseEncoder):\n    def __init__(\n        self,\n        input_dim,\n        proj_dim=256,\n        lstm_dim=768,\n        num_lstm_layers=3,\n        use_lstm_with_projection=True,\n        use_torch_spec=False,\n        audio_config=None,\n    ):\n        super().__init__()\n        self.use_lstm_with_projection = use_lstm_with_projection\n        self.use_torch_spec = use_torch_spec\n        self.audio_config = audio_config\n        self.proj_dim = proj_dim\n\n        layers = []\n        # choise LSTM layer\n        if use_lstm_with_projection:\n            layers.append(LSTMWithProjection(input_dim, lstm_dim, proj_dim))\n            for _ in range(num_lstm_layers - 1):\n                layers.append(LSTMWithProjection(proj_dim, lstm_dim, proj_dim))\n            self.layers = nn.Sequential(*layers)\n        else:\n            self.layers = LSTMWithoutProjection(input_dim, lstm_dim, proj_dim, num_lstm_layers)\n\n        self.instancenorm = nn.InstanceNorm1d(input_dim)\n\n        if self.use_torch_spec:\n            self.torch_spec = self.get_torch_mel_spectrogram_class(audio_config)\n        else:\n            self.torch_spec = None\n\n        self._init_layers()\n\n    def _init_layers(self):\n        for name, param in self.layers.named_parameters():\n            if \"bias\" in name:\n                nn.init.constant_(param, 0.0)\n            elif \"weight\" in name:\n                nn.init.xavier_normal_(param)\n\n    def forward(self, x, l2_norm=True):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x (Tensor): Raw waveform signal or spectrogram frames. If input is a waveform, `torch_spec` must be `True`\n                to compute the spectrogram on-the-fly.\n            l2_norm (bool): Whether to L2-normalize the outputs.\n\n        Shapes:\n            - x: :math:`(N, 1, T_{in})` or :math:`(N, D_{spec}, T_{in})`\n        \"\"\"\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=False):\n                if self.use_torch_spec:\n                    x.squeeze_(1)\n                    x = self.torch_spec(x)\n                x = self.instancenorm(x).transpose(1, 2)\n        d = self.layers(x)\n        if self.use_lstm_with_projection:\n            d = d[:, -1]\n        if l2_norm:\n            d = torch.nn.functional.normalize(d, p=2, dim=1)\n        return d\n", "TTS/encoder/models/resnet.py": "import torch\nfrom torch import nn\n\n# from TTS.utils.audio.torch_transforms import TorchSTFT\nfrom TTS.encoder.models.base_encoder import BaseEncoder\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=8):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.bn1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNetSpeakerEncoder(BaseEncoder):\n    \"\"\"Implementation of the model H/ASP without batch normalization in speaker embedding. This model was proposed in: https://arxiv.org/abs/2009.14153\n    Adapted from: https://github.com/clovaai/voxceleb_trainer\n    \"\"\"\n\n    # pylint: disable=W0102\n    def __init__(\n        self,\n        input_dim=64,\n        proj_dim=512,\n        layers=[3, 4, 6, 3],\n        num_filters=[32, 64, 128, 256],\n        encoder_type=\"ASP\",\n        log_input=False,\n        use_torch_spec=False,\n        audio_config=None,\n    ):\n        super(ResNetSpeakerEncoder, self).__init__()\n\n        self.encoder_type = encoder_type\n        self.input_dim = input_dim\n        self.log_input = log_input\n        self.use_torch_spec = use_torch_spec\n        self.audio_config = audio_config\n        self.proj_dim = proj_dim\n\n        self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn1 = nn.BatchNorm2d(num_filters[0])\n\n        self.inplanes = num_filters[0]\n        self.layer1 = self.create_layer(SEBasicBlock, num_filters[0], layers[0])\n        self.layer2 = self.create_layer(SEBasicBlock, num_filters[1], layers[1], stride=(2, 2))\n        self.layer3 = self.create_layer(SEBasicBlock, num_filters[2], layers[2], stride=(2, 2))\n        self.layer4 = self.create_layer(SEBasicBlock, num_filters[3], layers[3], stride=(2, 2))\n\n        self.instancenorm = nn.InstanceNorm1d(input_dim)\n\n        if self.use_torch_spec:\n            self.torch_spec = self.get_torch_mel_spectrogram_class(audio_config)\n        else:\n            self.torch_spec = None\n\n        outmap_size = int(self.input_dim / 8)\n\n        self.attention = nn.Sequential(\n            nn.Conv1d(num_filters[3] * outmap_size, 128, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Conv1d(128, num_filters[3] * outmap_size, kernel_size=1),\n            nn.Softmax(dim=2),\n        )\n\n        if self.encoder_type == \"SAP\":\n            out_dim = num_filters[3] * outmap_size\n        elif self.encoder_type == \"ASP\":\n            out_dim = num_filters[3] * outmap_size * 2\n        else:\n            raise ValueError(\"Undefined encoder\")\n\n        self.fc = nn.Linear(out_dim, proj_dim)\n\n        self._init_layers()\n\n    def _init_layers(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def create_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    # pylint: disable=R0201\n    def new_parameter(self, *size):\n        out = nn.Parameter(torch.FloatTensor(*size))\n        nn.init.xavier_normal_(out)\n        return out\n\n    def forward(self, x, l2_norm=False):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x (Tensor): Raw waveform signal or spectrogram frames. If input is a waveform, `torch_spec` must be `True`\n                to compute the spectrogram on-the-fly.\n            l2_norm (bool): Whether to L2-normalize the outputs.\n\n        Shapes:\n            - x: :math:`(N, 1, T_{in})` or :math:`(N, D_{spec}, T_{in})`\n        \"\"\"\n        x.squeeze_(1)\n        # if you torch spec compute it otherwise use the mel spec computed by the AP\n        if self.use_torch_spec:\n            x = self.torch_spec(x)\n\n        if self.log_input:\n            x = (x + 1e-6).log()\n        x = self.instancenorm(x).unsqueeze(1)\n\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.bn1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = x.reshape(x.size()[0], -1, x.size()[-1])\n\n        w = self.attention(x)\n\n        if self.encoder_type == \"SAP\":\n            x = torch.sum(x * w, dim=2)\n        elif self.encoder_type == \"ASP\":\n            mu = torch.sum(x * w, dim=2)\n            sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-5))\n            x = torch.cat((mu, sg), 1)\n\n        x = x.view(x.size()[0], -1)\n        x = self.fc(x)\n\n        if l2_norm:\n            x = torch.nn.functional.normalize(x, p=2, dim=1)\n        return x\n", "TTS/encoder/utils/training.py": "import os\nfrom dataclasses import dataclass, field\n\nfrom coqpit import Coqpit\nfrom trainer import TrainerArgs, get_last_checkpoint\nfrom trainer.io import copy_model_files\nfrom trainer.logging import logger_factory\nfrom trainer.logging.console_logger import ConsoleLogger\n\nfrom TTS.config import load_config, register_config\nfrom TTS.tts.utils.text.characters import parse_symbols\nfrom TTS.utils.generic_utils import get_experiment_folder_path, get_git_branch\n\n\n@dataclass\nclass TrainArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\n\n\ndef getarguments():\n    train_config = TrainArgs()\n    parser = train_config.init_argparse(arg_prefix=\"\")\n    return parser\n\n\ndef process_args(args, config=None):\n    \"\"\"Process parsed comand line arguments and initialize the config if not provided.\n    Args:\n        args (argparse.Namespace or dict like): Parsed input arguments.\n        config (Coqpit): Model config. If none, it is generated from `args`. Defaults to None.\n    Returns:\n        c (TTS.utils.io.AttrDict): Config paramaters.\n        out_path (str): Path to save models and logging.\n        audio_path (str): Path to save generated test audios.\n        c_logger (TTS.utils.console_logger.ConsoleLogger): Class that does\n            logging to the console.\n        dashboard_logger (WandbLogger or TensorboardLogger): Class that does the dashboard Logging\n    TODO:\n        - Interactive config definition.\n    \"\"\"\n    if isinstance(args, tuple):\n        args, coqpit_overrides = args\n    if args.continue_path:\n        # continue a previous training from its output folder\n        experiment_path = args.continue_path\n        args.config_path = os.path.join(args.continue_path, \"config.json\")\n        args.restore_path, best_model = get_last_checkpoint(args.continue_path)\n        if not args.best_path:\n            args.best_path = best_model\n    # init config if not already defined\n    if config is None:\n        if args.config_path:\n            # init from a file\n            config = load_config(args.config_path)\n        else:\n            # init from console args\n            from TTS.config.shared_configs import BaseTrainingConfig  # pylint: disable=import-outside-toplevel\n\n            config_base = BaseTrainingConfig()\n            config_base.parse_known_args(coqpit_overrides)\n            config = register_config(config_base.model)()\n    # override values from command-line args\n    config.parse_known_args(coqpit_overrides, relaxed_parser=True)\n    experiment_path = args.continue_path\n    if not experiment_path:\n        experiment_path = get_experiment_folder_path(config.output_path, config.run_name)\n    audio_path = os.path.join(experiment_path, \"test_audios\")\n    config.output_log_path = experiment_path\n    # setup rank 0 process in distributed training\n    dashboard_logger = None\n    if args.rank == 0:\n        new_fields = {}\n        if args.restore_path:\n            new_fields[\"restore_path\"] = args.restore_path\n        new_fields[\"github_branch\"] = get_git_branch()\n        # if model characters are not set in the config file\n        # save the default set to the config file for future\n        # compatibility.\n        if config.has(\"characters\") and config.characters is None:\n            used_characters = parse_symbols()\n            new_fields[\"characters\"] = used_characters\n        copy_model_files(config, experiment_path, new_fields)\n        dashboard_logger = logger_factory(config, experiment_path)\n    c_logger = ConsoleLogger()\n    return config, experiment_path, audio_path, c_logger, dashboard_logger\n\n\ndef init_arguments():\n    train_config = TrainArgs()\n    parser = train_config.init_argparse(arg_prefix=\"\")\n    return parser\n\n\ndef init_training(config: Coqpit = None):\n    \"\"\"Initialization of a training run.\"\"\"\n    parser = init_arguments()\n    args = parser.parse_known_args()\n    config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger = process_args(args, config)\n    return args[0], config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger\n", "TTS/encoder/utils/prepare_voxceleb.py": "# coding=utf-8\n# Copyright (C) 2020 ATHENA AUTHORS; Yiping Peng; Ne Luo\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Only support eager mode and TF>=2.0.0\n# pylint: disable=no-member, invalid-name, relative-beyond-top-level\n# pylint: disable=too-many-locals, too-many-statements, too-many-arguments, too-many-instance-attributes\n\"\"\" voxceleb 1 & 2 \"\"\"\n\nimport hashlib\nimport os\nimport subprocess\nimport sys\nimport zipfile\n\nimport pandas\nimport soundfile as sf\nfrom absl import logging\n\nSUBSETS = {\n    \"vox1_dev_wav\": [\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\",\n    ],\n    \"vox1_test_wav\": [\"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\"],\n    \"vox2_dev_aac\": [\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partaa\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partab\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partac\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partad\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partae\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partaf\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partag\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partah\",\n    ],\n    \"vox2_test_aac\": [\"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_test_aac.zip\"],\n}\n\nMD5SUM = {\n    \"vox1_dev_wav\": \"ae63e55b951748cc486645f532ba230b\",\n    \"vox2_dev_aac\": \"bbc063c46078a602ca71605645c2a402\",\n    \"vox1_test_wav\": \"185fdc63c3c739954633d50379a3d102\",\n    \"vox2_test_aac\": \"0d2b3ea430a821c33263b5ea37ede312\",\n}\n\nUSER = {\"user\": \"\", \"password\": \"\"}\n\nspeaker_id_dict = {}\n\n\ndef download_and_extract(directory, subset, urls):\n    \"\"\"Download and extract the given split of dataset.\n\n    Args:\n        directory: the directory where to put the downloaded data.\n        subset: subset name of the corpus.\n        urls: the list of urls to download the data file.\n    \"\"\"\n    os.makedirs(directory, exist_ok=True)\n\n    try:\n        for url in urls:\n            zip_filepath = os.path.join(directory, url.split(\"/\")[-1])\n            if os.path.exists(zip_filepath):\n                continue\n            logging.info(\"Downloading %s to %s\" % (url, zip_filepath))\n            subprocess.call(\n                \"wget %s --user %s --password %s -O %s\" % (url, USER[\"user\"], USER[\"password\"], zip_filepath),\n                shell=True,\n            )\n\n            statinfo = os.stat(zip_filepath)\n            logging.info(\"Successfully downloaded %s, size(bytes): %d\" % (url, statinfo.st_size))\n\n        # concatenate all parts into zip files\n        if \".zip\" not in zip_filepath:\n            zip_filepath = \"_\".join(zip_filepath.split(\"_\")[:-1])\n            subprocess.call(\"cat %s* > %s.zip\" % (zip_filepath, zip_filepath), shell=True)\n            zip_filepath += \".zip\"\n        extract_path = zip_filepath.strip(\".zip\")\n\n        # check zip file md5sum\n        with open(zip_filepath, \"rb\") as f_zip:\n            md5 = hashlib.md5(f_zip.read()).hexdigest()\n        if md5 != MD5SUM[subset]:\n            raise ValueError(\"md5sum of %s mismatch\" % zip_filepath)\n\n        with zipfile.ZipFile(zip_filepath, \"r\") as zfile:\n            zfile.extractall(directory)\n            extract_path_ori = os.path.join(directory, zfile.infolist()[0].filename)\n            subprocess.call(\"mv %s %s\" % (extract_path_ori, extract_path), shell=True)\n    finally:\n        # os.remove(zip_filepath)\n        pass\n\n\ndef exec_cmd(cmd):\n    \"\"\"Run a command in a subprocess.\n    Args:\n        cmd: command line to be executed.\n    Return:\n        int, the return code.\n    \"\"\"\n    try:\n        retcode = subprocess.call(cmd, shell=True)\n        if retcode < 0:\n            logging.info(f\"Child was terminated by signal {retcode}\")\n    except OSError as e:\n        logging.info(f\"Execution failed: {e}\")\n        retcode = -999\n    return retcode\n\n\ndef decode_aac_with_ffmpeg(aac_file, wav_file):\n    \"\"\"Decode a given AAC file into WAV using ffmpeg.\n    Args:\n        aac_file: file path to input AAC file.\n        wav_file: file path to output WAV file.\n    Return:\n        bool, True if success.\n    \"\"\"\n    cmd = f\"ffmpeg -i {aac_file} {wav_file}\"\n    logging.info(f\"Decoding aac file using command line: {cmd}\")\n    ret = exec_cmd(cmd)\n    if ret != 0:\n        logging.error(f\"Failed to decode aac file with retcode {ret}\")\n        logging.error(\"Please check your ffmpeg installation.\")\n        return False\n    return True\n\n\ndef convert_audio_and_make_label(input_dir, subset, output_dir, output_file):\n    \"\"\"Optionally convert AAC to WAV and make speaker labels.\n    Args:\n        input_dir: the directory which holds the input dataset.\n        subset: the name of the specified subset. e.g. vox1_dev_wav\n        output_dir: the directory to place the newly generated csv files.\n        output_file: the name of the newly generated csv file. e.g. vox1_dev_wav.csv\n    \"\"\"\n\n    logging.info(\"Preprocessing audio and label for subset %s\" % subset)\n    source_dir = os.path.join(input_dir, subset)\n\n    files = []\n    # Convert all AAC file into WAV format. At the same time, generate the csv\n    for root, _, filenames in os.walk(source_dir):\n        for filename in filenames:\n            name, ext = os.path.splitext(filename)\n            if ext.lower() == \".wav\":\n                _, ext2 = os.path.splitext(name)\n                if ext2:\n                    continue\n                wav_file = os.path.join(root, filename)\n            elif ext.lower() == \".m4a\":\n                # Convert AAC to WAV.\n                aac_file = os.path.join(root, filename)\n                wav_file = aac_file + \".wav\"\n                if not os.path.exists(wav_file):\n                    if not decode_aac_with_ffmpeg(aac_file, wav_file):\n                        raise RuntimeError(\"Audio decoding failed.\")\n            else:\n                continue\n            speaker_name = root.split(os.path.sep)[-2]\n            if speaker_name not in speaker_id_dict:\n                num = len(speaker_id_dict)\n                speaker_id_dict[speaker_name] = num\n            # wav_filesize = os.path.getsize(wav_file)\n            wav_length = len(sf.read(wav_file)[0])\n            files.append((os.path.abspath(wav_file), wav_length, speaker_id_dict[speaker_name], speaker_name))\n\n    # Write to CSV file which contains four columns:\n    # \"wav_filename\", \"wav_length_ms\", \"speaker_id\", \"speaker_name\".\n    csv_file_path = os.path.join(output_dir, output_file)\n    df = pandas.DataFrame(data=files, columns=[\"wav_filename\", \"wav_length_ms\", \"speaker_id\", \"speaker_name\"])\n    df.to_csv(csv_file_path, index=False, sep=\"\\t\")\n    logging.info(\"Successfully generated csv file {}\".format(csv_file_path))\n\n\ndef processor(directory, subset, force_process):\n    \"\"\"download and process\"\"\"\n    urls = SUBSETS\n    if subset not in urls:\n        raise ValueError(subset, \"is not in voxceleb\")\n\n    subset_csv = os.path.join(directory, subset + \".csv\")\n    if not force_process and os.path.exists(subset_csv):\n        return subset_csv\n\n    logging.info(\"Downloading and process the voxceleb in %s\", directory)\n    logging.info(\"Preparing subset %s\", subset)\n    download_and_extract(directory, subset, urls[subset])\n    convert_audio_and_make_label(directory, subset, directory, subset + \".csv\")\n    logging.info(\"Finished downloading and processing\")\n    return subset_csv\n\n\nif __name__ == \"__main__\":\n    logging.set_verbosity(logging.INFO)\n    if len(sys.argv) != 4:\n        print(\"Usage: python prepare_data.py save_directory user password\")\n        sys.exit()\n\n    DIR, USER[\"user\"], USER[\"password\"] = sys.argv[1], sys.argv[2], sys.argv[3]\n    for SUBSET in SUBSETS:\n        processor(DIR, SUBSET, False)\n", "TTS/encoder/utils/__init__.py": "", "TTS/encoder/utils/generic_utils.py": "import glob\nimport os\nimport random\n\nimport numpy as np\nfrom scipy import signal\n\nfrom TTS.encoder.models.lstm import LSTMSpeakerEncoder\nfrom TTS.encoder.models.resnet import ResNetSpeakerEncoder\n\n\nclass AugmentWAV(object):\n    def __init__(self, ap, augmentation_config):\n        self.ap = ap\n        self.use_additive_noise = False\n\n        if \"additive\" in augmentation_config.keys():\n            self.additive_noise_config = augmentation_config[\"additive\"]\n            additive_path = self.additive_noise_config[\"sounds_path\"]\n            if additive_path:\n                self.use_additive_noise = True\n                # get noise types\n                self.additive_noise_types = []\n                for key in self.additive_noise_config.keys():\n                    if isinstance(self.additive_noise_config[key], dict):\n                        self.additive_noise_types.append(key)\n\n                additive_files = glob.glob(os.path.join(additive_path, \"**/*.wav\"), recursive=True)\n\n                self.noise_list = {}\n\n                for wav_file in additive_files:\n                    noise_dir = wav_file.replace(additive_path, \"\").split(os.sep)[0]\n                    # ignore not listed directories\n                    if noise_dir not in self.additive_noise_types:\n                        continue\n                    if not noise_dir in self.noise_list:\n                        self.noise_list[noise_dir] = []\n                    self.noise_list[noise_dir].append(wav_file)\n\n                print(\n                    f\" | > Using Additive Noise Augmentation: with {len(additive_files)} audios instances from {self.additive_noise_types}\"\n                )\n\n        self.use_rir = False\n\n        if \"rir\" in augmentation_config.keys():\n            self.rir_config = augmentation_config[\"rir\"]\n            if self.rir_config[\"rir_path\"]:\n                self.rir_files = glob.glob(os.path.join(self.rir_config[\"rir_path\"], \"**/*.wav\"), recursive=True)\n                self.use_rir = True\n\n            print(f\" | > Using RIR Noise Augmentation: with {len(self.rir_files)} audios instances\")\n\n        self.create_augmentation_global_list()\n\n    def create_augmentation_global_list(self):\n        if self.use_additive_noise:\n            self.global_noise_list = self.additive_noise_types\n        else:\n            self.global_noise_list = []\n        if self.use_rir:\n            self.global_noise_list.append(\"RIR_AUG\")\n\n    def additive_noise(self, noise_type, audio):\n        clean_db = 10 * np.log10(np.mean(audio**2) + 1e-4)\n\n        noise_list = random.sample(\n            self.noise_list[noise_type],\n            random.randint(\n                self.additive_noise_config[noise_type][\"min_num_noises\"],\n                self.additive_noise_config[noise_type][\"max_num_noises\"],\n            ),\n        )\n\n        audio_len = audio.shape[0]\n        noises_wav = None\n        for noise in noise_list:\n            noiseaudio = self.ap.load_wav(noise, sr=self.ap.sample_rate)[:audio_len]\n\n            if noiseaudio.shape[0] < audio_len:\n                continue\n\n            noise_snr = random.uniform(\n                self.additive_noise_config[noise_type][\"min_snr_in_db\"],\n                self.additive_noise_config[noise_type][\"max_num_noises\"],\n            )\n            noise_db = 10 * np.log10(np.mean(noiseaudio**2) + 1e-4)\n            noise_wav = np.sqrt(10 ** ((clean_db - noise_db - noise_snr) / 10)) * noiseaudio\n\n            if noises_wav is None:\n                noises_wav = noise_wav\n            else:\n                noises_wav += noise_wav\n\n        # if all possible files is less than audio, choose other files\n        if noises_wav is None:\n            return self.additive_noise(noise_type, audio)\n\n        return audio + noises_wav\n\n    def reverberate(self, audio):\n        audio_len = audio.shape[0]\n\n        rir_file = random.choice(self.rir_files)\n        rir = self.ap.load_wav(rir_file, sr=self.ap.sample_rate)\n        rir = rir / np.sqrt(np.sum(rir**2))\n        return signal.convolve(audio, rir, mode=self.rir_config[\"conv_mode\"])[:audio_len]\n\n    def apply_one(self, audio):\n        noise_type = random.choice(self.global_noise_list)\n        if noise_type == \"RIR_AUG\":\n            return self.reverberate(audio)\n\n        return self.additive_noise(noise_type, audio)\n\n\ndef setup_encoder_model(config: \"Coqpit\"):\n    if config.model_params[\"model_name\"].lower() == \"lstm\":\n        model = LSTMSpeakerEncoder(\n            config.model_params[\"input_dim\"],\n            config.model_params[\"proj_dim\"],\n            config.model_params[\"lstm_dim\"],\n            config.model_params[\"num_lstm_layers\"],\n            use_torch_spec=config.model_params.get(\"use_torch_spec\", False),\n            audio_config=config.audio,\n        )\n    elif config.model_params[\"model_name\"].lower() == \"resnet\":\n        model = ResNetSpeakerEncoder(\n            input_dim=config.model_params[\"input_dim\"],\n            proj_dim=config.model_params[\"proj_dim\"],\n            log_input=config.model_params.get(\"log_input\", False),\n            use_torch_spec=config.model_params.get(\"use_torch_spec\", False),\n            audio_config=config.audio,\n        )\n    return model\n", "TTS/encoder/utils/visual.py": "import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport umap\n\nmatplotlib.use(\"Agg\")\n\n\ncolormap = (\n    np.array(\n        [\n            [76, 255, 0],\n            [0, 127, 70],\n            [255, 0, 0],\n            [255, 217, 38],\n            [0, 135, 255],\n            [165, 0, 165],\n            [255, 167, 255],\n            [0, 255, 255],\n            [255, 96, 38],\n            [142, 76, 0],\n            [33, 0, 127],\n            [0, 0, 0],\n            [183, 183, 183],\n        ],\n        dtype=float,\n    )\n    / 255\n)\n\n\ndef plot_embeddings(embeddings, num_classes_in_batch):\n    num_utter_per_class = embeddings.shape[0] // num_classes_in_batch\n\n    # if necessary get just the first 10 classes\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n        embeddings = embeddings[: num_classes_in_batch * num_utter_per_class]\n\n    model = umap.UMAP()\n    projection = model.fit_transform(embeddings)\n    ground_truth = np.repeat(np.arange(num_classes_in_batch), num_utter_per_class)\n    colors = [colormap[i] for i in ground_truth]\n    fig, ax = plt.subplots(figsize=(16, 10))\n    _ = ax.scatter(projection[:, 0], projection[:, 1], c=colors)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection\")\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig\n", "TTS/encoder/configs/base_encoder_config.py": "from dataclasses import asdict, dataclass, field\nfrom typing import Dict, List\n\nfrom coqpit import MISSING\n\nfrom TTS.config.shared_configs import BaseAudioConfig, BaseDatasetConfig, BaseTrainingConfig\n\n\n@dataclass\nclass BaseEncoderConfig(BaseTrainingConfig):\n    \"\"\"Defines parameters for a Generic Encoder model.\"\"\"\n\n    model: str = None\n    audio: BaseAudioConfig = field(default_factory=BaseAudioConfig)\n    datasets: List[BaseDatasetConfig] = field(default_factory=lambda: [BaseDatasetConfig()])\n    # model params\n    model_params: Dict = field(\n        default_factory=lambda: {\n            \"model_name\": \"lstm\",\n            \"input_dim\": 80,\n            \"proj_dim\": 256,\n            \"lstm_dim\": 768,\n            \"num_lstm_layers\": 3,\n            \"use_lstm_with_projection\": True,\n        }\n    )\n\n    audio_augmentation: Dict = field(default_factory=lambda: {})\n\n    # training params\n    epochs: int = 10000\n    loss: str = \"angleproto\"\n    grad_clip: float = 3.0\n    lr: float = 0.0001\n    optimizer: str = \"radam\"\n    optimizer_params: Dict = field(default_factory=lambda: {\"betas\": [0.9, 0.999], \"weight_decay\": 0})\n    lr_decay: bool = False\n    warmup_steps: int = 4000\n\n    # logging params\n    tb_model_param_stats: bool = False\n    steps_plot_stats: int = 10\n    save_step: int = 1000\n    print_step: int = 20\n    run_eval: bool = False\n\n    # data loader\n    num_classes_in_batch: int = MISSING\n    num_utter_per_class: int = MISSING\n    eval_num_classes_in_batch: int = None\n    eval_num_utter_per_class: int = None\n\n    num_loader_workers: int = MISSING\n    voice_len: float = 1.6\n\n    def check_values(self):\n        super().check_values()\n        c = asdict(self)\n        assert (\n            c[\"model_params\"][\"input_dim\"] == self.audio.num_mels\n        ), \" [!] model input dimendion must be equal to melspectrogram dimension.\"\n", "TTS/encoder/configs/speaker_encoder_config.py": "from dataclasses import asdict, dataclass\n\nfrom TTS.encoder.configs.base_encoder_config import BaseEncoderConfig\n\n\n@dataclass\nclass SpeakerEncoderConfig(BaseEncoderConfig):\n    \"\"\"Defines parameters for Speaker Encoder model.\"\"\"\n\n    model: str = \"speaker_encoder\"\n    class_name_key: str = \"speaker_name\"\n", "TTS/encoder/configs/emotion_encoder_config.py": "from dataclasses import asdict, dataclass\n\nfrom TTS.encoder.configs.base_encoder_config import BaseEncoderConfig\n\n\n@dataclass\nclass EmotionEncoderConfig(BaseEncoderConfig):\n    \"\"\"Defines parameters for Emotion Encoder model.\"\"\"\n\n    model: str = \"emotion_encoder\"\n    map_classid_to_classname: dict = None\n    class_name_key: str = \"emotion_name\"\n", "TTS/bin/compute_embeddings.py": "import argparse\nimport os\nfrom argparse import RawTextHelpFormatter\n\nimport torch\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.utils.managers import save_file\nfrom TTS.tts.utils.speakers import SpeakerManager\n\n\ndef compute_embeddings(\n    model_path,\n    config_path,\n    output_path,\n    old_speakers_file=None,\n    old_append=False,\n    config_dataset_path=None,\n    formatter_name=None,\n    dataset_name=None,\n    dataset_path=None,\n    meta_file_train=None,\n    meta_file_val=None,\n    disable_cuda=False,\n    no_eval=False,\n):\n    use_cuda = torch.cuda.is_available() and not disable_cuda\n\n    if config_dataset_path is not None:\n        c_dataset = load_config(config_dataset_path)\n        meta_data_train, meta_data_eval = load_tts_samples(c_dataset.datasets, eval_split=not no_eval)\n    else:\n        c_dataset = BaseDatasetConfig()\n        c_dataset.formatter = formatter_name\n        c_dataset.dataset_name = dataset_name\n        c_dataset.path = dataset_path\n        if meta_file_train is not None:\n            c_dataset.meta_file_train = meta_file_train\n        if meta_file_val is not None:\n            c_dataset.meta_file_val = meta_file_val\n        meta_data_train, meta_data_eval = load_tts_samples(c_dataset, eval_split=not no_eval)\n\n    if meta_data_eval is None:\n        samples = meta_data_train\n    else:\n        samples = meta_data_train + meta_data_eval\n\n    encoder_manager = SpeakerManager(\n        encoder_model_path=model_path,\n        encoder_config_path=config_path,\n        d_vectors_file_path=old_speakers_file,\n        use_cuda=use_cuda,\n    )\n\n    class_name_key = encoder_manager.encoder_config.class_name_key\n\n    # compute speaker embeddings\n    if old_speakers_file is not None and old_append:\n        speaker_mapping = encoder_manager.embeddings\n    else:\n        speaker_mapping = {}\n\n    for fields in tqdm(samples):\n        class_name = fields[class_name_key]\n        audio_file = fields[\"audio_file\"]\n        embedding_key = fields[\"audio_unique_name\"]\n\n        # Only update the speaker name when the embedding is already in the old file.\n        if embedding_key in speaker_mapping:\n            speaker_mapping[embedding_key][\"name\"] = class_name\n            continue\n\n        if old_speakers_file is not None and embedding_key in encoder_manager.clip_ids:\n            # get the embedding from the old file\n            embedd = encoder_manager.get_embedding_by_clip(embedding_key)\n        else:\n            # extract the embedding\n            embedd = encoder_manager.compute_embedding_from_clip(audio_file)\n\n        # create speaker_mapping if target dataset is defined\n        speaker_mapping[embedding_key] = {}\n        speaker_mapping[embedding_key][\"name\"] = class_name\n        speaker_mapping[embedding_key][\"embedding\"] = embedd\n\n    if speaker_mapping:\n        # save speaker_mapping if target dataset is defined\n        if os.path.isdir(output_path):\n            mapping_file_path = os.path.join(output_path, \"speakers.pth\")\n        else:\n            mapping_file_path = output_path\n\n        if os.path.dirname(mapping_file_path) != \"\":\n            os.makedirs(os.path.dirname(mapping_file_path), exist_ok=True)\n\n        save_file(speaker_mapping, mapping_file_path)\n        print(\"Speaker embeddings saved at:\", mapping_file_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Compute embedding vectors for each audio file in a dataset and store them keyed by `{dataset_name}#{file_path}` in a .pth file\\n\\n\"\"\"\n        \"\"\"\n        Example runs:\n        python TTS/bin/compute_embeddings.py --model_path speaker_encoder_model.pth --config_path speaker_encoder_config.json  --config_dataset_path dataset_config.json\n\n        python TTS/bin/compute_embeddings.py --model_path speaker_encoder_model.pth --config_path speaker_encoder_config.json  --formatter_name coqui --dataset_path /path/to/vctk/dataset --dataset_name my_vctk --meta_file_train /path/to/vctk/metafile_train.csv --meta_file_val /path/to/vctk/metafile_eval.csv\n        \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        help=\"Path to model checkpoint file. It defaults to the released speaker encoder.\",\n        default=\"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\",\n    )\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        help=\"Path to model config file. It defaults to the released speaker encoder config.\",\n        default=\"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\",\n    )\n    parser.add_argument(\n        \"--config_dataset_path\",\n        type=str,\n        help=\"Path to dataset config file. You either need to provide this or `formatter_name`, `dataset_name` and `dataset_path` arguments.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        help=\"Path for output `pth` or `json` file.\",\n        default=\"speakers.pth\",\n    )\n    parser.add_argument(\n        \"--old_file\",\n        type=str,\n        help=\"The old existing embedding file, from which the embeddings will be directly loaded for already computed audio clips.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--old_append\",\n        help=\"Append new audio clip embeddings to the old embedding file, generate a new non-duplicated merged embedding file. Default False\",\n        default=False,\n        action=\"store_true\",\n    )\n    parser.add_argument(\"--disable_cuda\", type=bool, help=\"Flag to disable cuda.\", default=False)\n    parser.add_argument(\"--no_eval\", help=\"Do not compute eval?. Default False\", default=False, action=\"store_true\")\n    parser.add_argument(\n        \"--formatter_name\",\n        type=str,\n        help=\"Name of the formatter to use. You either need to provide this or `config_dataset_path`\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        help=\"Name of the dataset to use. You either need to provide this or `config_dataset_path`\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--dataset_path\",\n        type=str,\n        help=\"Path to the dataset. You either need to provide this or `config_dataset_path`\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--meta_file_train\",\n        type=str,\n        help=\"Path to the train meta file. If not set, dataset formatter uses the default metafile if it is defined in the formatter. You either need to provide this or `config_dataset_path`\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--meta_file_val\",\n        type=str,\n        help=\"Path to the evaluation meta file. If not set, dataset formatter uses the default metafile if it is defined in the formatter. You either need to provide this or `config_dataset_path`\",\n        default=None,\n    )\n    args = parser.parse_args()\n\n    compute_embeddings(\n        args.model_path,\n        args.config_path,\n        args.output_path,\n        old_speakers_file=args.old_file,\n        old_append=args.old_append,\n        config_dataset_path=args.config_dataset_path,\n        formatter_name=args.formatter_name,\n        dataset_name=args.dataset_name,\n        dataset_path=args.dataset_path,\n        meta_file_train=args.meta_file_train,\n        meta_file_val=args.meta_file_val,\n        disable_cuda=args.disable_cuda,\n        no_eval=args.no_eval,\n    )\n", "TTS/bin/compute_statistics.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport glob\nimport os\n\nimport numpy as np\nfrom tqdm import tqdm\n\n# from TTS.utils.io import load_config\nfrom TTS.config import load_config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.utils.audio import AudioProcessor\n\n\ndef main():\n    \"\"\"Run preprocessing process.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Compute mean and variance of spectrogtram features.\")\n    parser.add_argument(\"config_path\", type=str, help=\"TTS config file path to define audio processin parameters.\")\n    parser.add_argument(\"out_path\", type=str, help=\"save path (directory and filename).\")\n    parser.add_argument(\n        \"--data_path\",\n        type=str,\n        required=False,\n        help=\"folder including the target set of wavs overriding dataset config.\",\n    )\n    args, overrides = parser.parse_known_args()\n\n    CONFIG = load_config(args.config_path)\n    CONFIG.parse_known_args(overrides, relaxed_parser=True)\n\n    # load config\n    CONFIG.audio.signal_norm = False  # do not apply earlier normalization\n    CONFIG.audio.stats_path = None  # discard pre-defined stats\n\n    # load audio processor\n    ap = AudioProcessor(**CONFIG.audio.to_dict())\n\n    # load the meta data of target dataset\n    if args.data_path:\n        dataset_items = glob.glob(os.path.join(args.data_path, \"**\", \"*.wav\"), recursive=True)\n    else:\n        dataset_items = load_tts_samples(CONFIG.datasets)[0]  # take only train data\n    print(f\" > There are {len(dataset_items)} files.\")\n\n    mel_sum = 0\n    mel_square_sum = 0\n    linear_sum = 0\n    linear_square_sum = 0\n    N = 0\n    for item in tqdm(dataset_items):\n        # compute features\n        wav = ap.load_wav(item if isinstance(item, str) else item[\"audio_file\"])\n        linear = ap.spectrogram(wav)\n        mel = ap.melspectrogram(wav)\n\n        # compute stats\n        N += mel.shape[1]\n        mel_sum += mel.sum(1)\n        linear_sum += linear.sum(1)\n        mel_square_sum += (mel**2).sum(axis=1)\n        linear_square_sum += (linear**2).sum(axis=1)\n\n    mel_mean = mel_sum / N\n    mel_scale = np.sqrt(mel_square_sum / N - mel_mean**2)\n    linear_mean = linear_sum / N\n    linear_scale = np.sqrt(linear_square_sum / N - linear_mean**2)\n\n    output_file_path = args.out_path\n    stats = {}\n    stats[\"mel_mean\"] = mel_mean\n    stats[\"mel_std\"] = mel_scale\n    stats[\"linear_mean\"] = linear_mean\n    stats[\"linear_std\"] = linear_scale\n\n    print(f\" > Avg mel spec mean: {mel_mean.mean()}\")\n    print(f\" > Avg mel spec scale: {mel_scale.mean()}\")\n    print(f\" > Avg linear spec mean: {linear_mean.mean()}\")\n    print(f\" > Avg linear spec scale: {linear_scale.mean()}\")\n\n    # set default config values for mean-var scaling\n    CONFIG.audio.stats_path = output_file_path\n    CONFIG.audio.signal_norm = True\n    # remove redundant values\n    del CONFIG.audio.max_norm\n    del CONFIG.audio.min_level_db\n    del CONFIG.audio.symmetric_norm\n    del CONFIG.audio.clip_norm\n    stats[\"audio_config\"] = CONFIG.audio.to_dict()\n    np.save(output_file_path, stats, allow_pickle=True)\n    print(f\" > stats saved to {output_file_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/train_vocoder.py": "import os\nfrom dataclasses import dataclass, field\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import load_config, register_config\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.datasets.preprocess import load_wav_data, load_wav_feat_data\nfrom TTS.vocoder.models import setup_model\n\n\n@dataclass\nclass TrainVocoderArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\n\n\ndef main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainVocoderArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)\n\n    # load config.json and register\n    if args.config_path or args.continue_path:\n        if args.config_path:\n            # init from a file\n            config = load_config(args.config_path)\n            if len(config_overrides) > 0:\n                config.parse_known_args(config_overrides, relaxed_parser=True)\n        elif args.continue_path:\n            # continue from a prev experiment\n            config = load_config(os.path.join(args.continue_path, \"config.json\"))\n            if len(config_overrides) > 0:\n                config.parse_known_args(config_overrides, relaxed_parser=True)\n        else:\n            # init from console args\n            from TTS.config.shared_configs import BaseTrainingConfig  # pylint: disable=import-outside-toplevel\n\n            config_base = BaseTrainingConfig()\n            config_base.parse_known_args(config_overrides)\n            config = register_config(config_base.model)()\n\n    # load training samples\n    if \"feature_path\" in config and config.feature_path:\n        # load pre-computed features\n        print(f\" > Loading features from: {config.feature_path}\")\n        eval_samples, train_samples = load_wav_feat_data(config.data_path, config.feature_path, config.eval_split_size)\n    else:\n        # load data raw wav files\n        eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n\n    # setup audio processor\n    ap = AudioProcessor(**config.audio)\n\n    # init the model from config\n    model = setup_model(config)\n\n    # init the trainer and \ud83d\ude80\n    trainer = Trainer(\n        train_args,\n        config,\n        config.output_path,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n        training_assets={\"audio_processor\": ap},\n        parse_command_line_args=False,\n    )\n    trainer.fit()\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/resample.py": "import argparse\nimport glob\nimport os\nfrom argparse import RawTextHelpFormatter\nfrom multiprocessing import Pool\nfrom shutil import copytree\n\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\n\n\ndef resample_file(func_args):\n    filename, output_sr = func_args\n    y, sr = librosa.load(filename, sr=output_sr)\n    sf.write(filename, y, sr)\n\n\ndef resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    if output_dir:\n        print(\"Recursively copying the input folder...\")\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n\n    print(\"Resampling the audio files...\")\n    audio_files = glob.glob(os.path.join(input_dir, f\"**/*.{file_ext}\"), recursive=True)\n    print(f\"Found {len(audio_files)} files...\")\n    audio_files = list(zip(audio_files, len(audio_files) * [output_sr]))\n    with Pool(processes=n_jobs) as p:\n        with tqdm(total=len(audio_files)) as pbar:\n            for _, _ in enumerate(p.imap_unordered(resample_file, audio_files)):\n                pbar.update()\n\n    print(\"Done !\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Resample a folder recusively with librosa\n                       Can be used in place or create a copy of the folder as an output.\\n\\n\n                       Example run:\n                            python TTS/bin/resample.py\n                                --input_dir /root/LJSpeech-1.1/\n                                --output_sr 22050\n                                --output_dir /root/resampled_LJSpeech-1.1/\n                                --file_ext wav\n                                --n_jobs 24\n                    \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path of the folder containing the audio files to resample\",\n    )\n\n    parser.add_argument(\n        \"--output_sr\",\n        type=int,\n        default=22050,\n        required=False,\n        help=\"Samlple rate to which the audio files should be resampled\",\n    )\n\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Path of the destination folder. If not defined, the operation is done in place\",\n    )\n\n    parser.add_argument(\n        \"--file_ext\",\n        type=str,\n        default=\"wav\",\n        required=False,\n        help=\"Extension of the audio files to resample\",\n    )\n\n    parser.add_argument(\n        \"--n_jobs\", type=int, default=None, help=\"Number of threads to use, by default it uses all cores\"\n    )\n\n    args = parser.parse_args()\n\n    resample_files(args.input_dir, args.output_sr, args.output_dir, args.file_ext, args.n_jobs)\n", "TTS/bin/find_unique_phonemes.py": "\"\"\"Find all the unique characters in a dataset\"\"\"\nimport argparse\nimport multiprocessing\nfrom argparse import RawTextHelpFormatter\n\nfrom tqdm.contrib.concurrent import process_map\n\nfrom TTS.config import load_config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.utils.text.phonemizers import Gruut\n\n\ndef compute_phonemes(item):\n    text = item[\"text\"]\n    ph = phonemizer.phonemize(text).replace(\"|\", \"\")\n    return set(list(ph))\n\n\ndef main():\n    # pylint: disable=W0601\n    global c, phonemizer\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Find all the unique characters or phonemes in a dataset.\\n\\n\"\"\"\n        \"\"\"\n    Example runs:\n\n    python TTS/bin/find_unique_phonemes.py --config_path config.json\n    \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\"--config_path\", type=str, help=\"Path to dataset config file.\", required=True)\n    args = parser.parse_args()\n\n    c = load_config(args.config_path)\n\n    # load all datasets\n    train_items, eval_items = load_tts_samples(\n        c.datasets, eval_split=True, eval_split_max_size=c.eval_split_max_size, eval_split_size=c.eval_split_size\n    )\n    items = train_items + eval_items\n    print(\"Num items:\", len(items))\n\n    language_list = [item[\"language\"] for item in items]\n    is_lang_def = all(language_list)\n\n    if not c.phoneme_language or not is_lang_def:\n        raise ValueError(\"Phoneme language must be defined in config.\")\n\n    if not language_list.count(language_list[0]) == len(language_list):\n        raise ValueError(\n            \"Currently, just one phoneme language per config file is supported !! Please split the dataset config into different configs and run it individually for each language !!\"\n        )\n\n    phonemizer = Gruut(language=language_list[0], keep_puncs=True)\n\n    phonemes = process_map(compute_phonemes, items, max_workers=multiprocessing.cpu_count(), chunksize=15)\n    phones = []\n    for ph in phonemes:\n        phones.extend(ph)\n\n    phones = set(phones)\n    lower_phones = filter(lambda c: c.islower(), phones)\n    phones_force_lower = [c.lower() for c in phones]\n    phones_force_lower = set(phones_force_lower)\n\n    print(f\" > Number of unique phonemes: {len(phones)}\")\n    print(f\" > Unique phonemes: {''.join(sorted(phones))}\")\n    print(f\" > Unique lower phonemes: {''.join(sorted(lower_phones))}\")\n    print(f\" > Unique all forced to lower phonemes: {''.join(sorted(phones_force_lower))}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/train_encoder.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport time\nimport traceback\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom trainer.io import copy_model_files, save_best_model, save_checkpoint\nfrom trainer.torch import NoamLR\nfrom trainer.trainer_utils import get_optimizer\n\nfrom TTS.encoder.dataset import EncoderDataset\nfrom TTS.encoder.utils.generic_utils import setup_encoder_model\nfrom TTS.encoder.utils.training import init_training\nfrom TTS.encoder.utils.visual import plot_embeddings\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.generic_utils import count_parameters, remove_experiment_folder\nfrom TTS.utils.samplers import PerfectBatchSampler\nfrom TTS.utils.training import check_update\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\ntorch.manual_seed(54321)\nuse_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint(\" > Using CUDA: \", use_cuda)\nprint(\" > Number of GPUs: \", num_gpus)\n\n\ndef setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch\n\n    dataset = EncoderDataset(\n        c,\n        ap,\n        meta_data_eval if is_val else meta_data_train,\n        voice_len=c.voice_len,\n        num_utter_per_class=num_utter_per_class,\n        num_classes_in_batch=num_classes_in_batch,\n        verbose=verbose,\n        augmentation_config=c.audio_augmentation if not is_val else None,\n        use_torch_spec=c.model_params.get(\"use_torch_spec\", False),\n    )\n    # get classes list\n    classes = dataset.get_class_list()\n\n    sampler = PerfectBatchSampler(\n        dataset.items,\n        classes,\n        batch_size=num_classes_in_batch * num_utter_per_class,  # total batch size\n        num_classes_in_batch=num_classes_in_batch,\n        num_gpus=1,\n        shuffle=not is_val,\n        drop_last=True,\n    )\n\n    if len(classes) < num_classes_in_batch:\n        if is_val:\n            raise RuntimeError(\n                f\"config.eval_num_classes_in_batch ({num_classes_in_batch}) need to be <= {len(classes)} (Number total of Classes in the Eval dataset) !\"\n            )\n        raise RuntimeError(\n            f\"config.num_classes_in_batch ({num_classes_in_batch}) need to be <= {len(classes)} (Number total of Classes in the Train dataset) !\"\n        )\n\n    # set the classes to avoid get wrong class_id when the number of training and eval classes are not equal\n    if is_val:\n        dataset.set_classes(train_classes)\n\n    loader = DataLoader(\n        dataset,\n        num_workers=c.num_loader_workers,\n        batch_sampler=sampler,\n        collate_fn=dataset.collate_fn,\n    )\n\n    return loader, classes, dataset.get_map_classid_to_classname()\n\n\ndef evaluation(model, criterion, data_loader, global_step):\n    eval_loss = 0\n    for _, data in enumerate(data_loader):\n        with torch.no_grad():\n            # setup input data\n            inputs, labels = data\n\n            # agroup samples of each class in the batch. perfect sampler produces [3,2,1,3,2,1] we need [3,3,2,2,1,1]\n            labels = torch.transpose(\n                labels.view(c.eval_num_utter_per_class, c.eval_num_classes_in_batch), 0, 1\n            ).reshape(labels.shape)\n            inputs = torch.transpose(\n                inputs.view(c.eval_num_utter_per_class, c.eval_num_classes_in_batch, -1), 0, 1\n            ).reshape(inputs.shape)\n\n            # dispatch data to GPU\n            if use_cuda:\n                inputs = inputs.cuda(non_blocking=True)\n                labels = labels.cuda(non_blocking=True)\n\n            # forward pass model\n            outputs = model(inputs)\n\n            # loss computation\n            loss = criterion(\n                outputs.view(c.eval_num_classes_in_batch, outputs.shape[0] // c.eval_num_classes_in_batch, -1), labels\n            )\n\n            eval_loss += loss.item()\n\n    eval_avg_loss = eval_loss / len(data_loader)\n    # save stats\n    dashboard_logger.eval_stats(global_step, {\"loss\": eval_avg_loss})\n    # plot the last batch in the evaluation\n    figures = {\n        \"UMAP Plot\": plot_embeddings(outputs.detach().cpu().numpy(), c.num_classes_in_batch),\n    }\n    dashboard_logger.eval_figures(global_step, figures)\n    return eval_avg_loss\n\n\ndef train(model, optimizer, scheduler, criterion, data_loader, eval_data_loader, global_step):\n    model.train()\n    best_loss = {\"train_loss\": None, \"eval_loss\": float(\"inf\")}\n    avg_loader_time = 0\n    end_time = time.time()\n    for epoch in range(c.epochs):\n        tot_loss = 0\n        epoch_time = 0\n        for _, data in enumerate(data_loader):\n            start_time = time.time()\n\n            # setup input data\n            inputs, labels = data\n            # agroup samples of each class in the batch. perfect sampler produces [3,2,1,3,2,1] we need [3,3,2,2,1,1]\n            labels = torch.transpose(labels.view(c.num_utter_per_class, c.num_classes_in_batch), 0, 1).reshape(\n                labels.shape\n            )\n            inputs = torch.transpose(inputs.view(c.num_utter_per_class, c.num_classes_in_batch, -1), 0, 1).reshape(\n                inputs.shape\n            )\n            # ToDo: move it to a unit test\n            # labels_converted = torch.transpose(labels.view(c.num_utter_per_class, c.num_classes_in_batch), 0, 1).reshape(labels.shape)\n            # inputs_converted = torch.transpose(inputs.view(c.num_utter_per_class, c.num_classes_in_batch, -1), 0, 1).reshape(inputs.shape)\n            # idx = 0\n            # for j in range(0, c.num_classes_in_batch, 1):\n            #     for i in range(j, len(labels), c.num_classes_in_batch):\n            #         if not torch.all(labels[i].eq(labels_converted[idx])) or not torch.all(inputs[i].eq(inputs_converted[idx])):\n            #             print(\"Invalid\")\n            #             print(labels)\n            #             exit()\n            #         idx += 1\n            # labels = labels_converted\n            # inputs = inputs_converted\n\n            loader_time = time.time() - end_time\n            global_step += 1\n\n            # setup lr\n            if c.lr_decay:\n                scheduler.step()\n            optimizer.zero_grad()\n\n            # dispatch data to GPU\n            if use_cuda:\n                inputs = inputs.cuda(non_blocking=True)\n                labels = labels.cuda(non_blocking=True)\n\n            # forward pass model\n            outputs = model(inputs)\n\n            # loss computation\n            loss = criterion(\n                outputs.view(c.num_classes_in_batch, outputs.shape[0] // c.num_classes_in_batch, -1), labels\n            )\n            loss.backward()\n            grad_norm, _ = check_update(model, c.grad_clip)\n            optimizer.step()\n\n            step_time = time.time() - start_time\n            epoch_time += step_time\n\n            # acumulate the total epoch loss\n            tot_loss += loss.item()\n\n            # Averaged Loader Time\n            num_loader_workers = c.num_loader_workers if c.num_loader_workers > 0 else 1\n            avg_loader_time = (\n                1 / num_loader_workers * loader_time + (num_loader_workers - 1) / num_loader_workers * avg_loader_time\n                if avg_loader_time != 0\n                else loader_time\n            )\n            current_lr = optimizer.param_groups[0][\"lr\"]\n\n            if global_step % c.steps_plot_stats == 0:\n                # Plot Training Epoch Stats\n                train_stats = {\n                    \"loss\": loss.item(),\n                    \"lr\": current_lr,\n                    \"grad_norm\": grad_norm,\n                    \"step_time\": step_time,\n                    \"avg_loader_time\": avg_loader_time,\n                }\n                dashboard_logger.train_epoch_stats(global_step, train_stats)\n                figures = {\n                    \"UMAP Plot\": plot_embeddings(outputs.detach().cpu().numpy(), c.num_classes_in_batch),\n                }\n                dashboard_logger.train_figures(global_step, figures)\n\n            if global_step % c.print_step == 0:\n                print(\n                    \"   | > Step:{}  Loss:{:.5f}  GradNorm:{:.5f}  \"\n                    \"StepTime:{:.2f}  LoaderTime:{:.2f}  AvGLoaderTime:{:.2f}  LR:{:.6f}\".format(\n                        global_step, loss.item(), grad_norm, step_time, loader_time, avg_loader_time, current_lr\n                    ),\n                    flush=True,\n                )\n\n            if global_step % c.save_step == 0:\n                # save model\n                save_checkpoint(\n                    c, model, optimizer, None, global_step, epoch, OUT_PATH, criterion=criterion.state_dict()\n                )\n\n            end_time = time.time()\n\n        print(\"\")\n        print(\n            \">>> Epoch:{}  AvgLoss: {:.5f} GradNorm:{:.5f}  \"\n            \"EpochTime:{:.2f} AvGLoaderTime:{:.2f} \".format(\n                epoch, tot_loss / len(data_loader), grad_norm, epoch_time, avg_loader_time\n            ),\n            flush=True,\n        )\n        # evaluation\n        if c.run_eval:\n            model.eval()\n            eval_loss = evaluation(model, criterion, eval_data_loader, global_step)\n            print(\"\\n\\n\")\n            print(\"--> EVAL PERFORMANCE\")\n            print(\n                \"   | > Epoch:{}  AvgLoss: {:.5f} \".format(epoch, eval_loss),\n                flush=True,\n            )\n            # save the best checkpoint\n            best_loss = save_best_model(\n                {\"train_loss\": None, \"eval_loss\": eval_loss},\n                best_loss,\n                c,\n                model,\n                optimizer,\n                None,\n                global_step,\n                epoch,\n                OUT_PATH,\n                criterion=criterion.state_dict(),\n            )\n            model.train()\n\n    return best_loss, global_step\n\n\ndef main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data_train\n    global meta_data_eval\n    global train_classes\n\n    ap = AudioProcessor(**c.audio)\n    model = setup_encoder_model(c)\n\n    optimizer = get_optimizer(c.optimizer, c.optimizer_params, c.lr, model)\n\n    # pylint: disable=redefined-outer-name\n    meta_data_train, meta_data_eval = load_tts_samples(c.datasets, eval_split=True)\n\n    train_data_loader, train_classes, map_classid_to_classname = setup_loader(ap, is_val=False, verbose=True)\n    if c.run_eval:\n        eval_data_loader, _, _ = setup_loader(ap, is_val=True, verbose=True)\n    else:\n        eval_data_loader = None\n\n    num_classes = len(train_classes)\n    criterion = model.get_criterion(c, num_classes)\n\n    if c.loss == \"softmaxproto\" and c.model != \"speaker_encoder\":\n        c.map_classid_to_classname = map_classid_to_classname\n        copy_model_files(c, OUT_PATH, new_fields={})\n\n    if args.restore_path:\n        criterion, args.restore_step = model.load_checkpoint(\n            c, args.restore_path, eval=False, use_cuda=use_cuda, criterion=criterion\n        )\n        print(\" > Model restored from step %d\" % args.restore_step, flush=True)\n    else:\n        args.restore_step = 0\n\n    if c.lr_decay:\n        scheduler = NoamLR(optimizer, warmup_steps=c.warmup_steps, last_epoch=args.restore_step - 1)\n    else:\n        scheduler = None\n\n    num_params = count_parameters(model)\n    print(\"\\n > Model has {} parameters\".format(num_params), flush=True)\n\n    if use_cuda:\n        model = model.cuda()\n        criterion.cuda()\n\n    global_step = args.restore_step\n    _, global_step = train(model, optimizer, scheduler, criterion, train_data_loader, eval_data_loader, global_step)\n\n\nif __name__ == \"__main__\":\n    args, c, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger = init_training()\n\n    try:\n        main(args)\n    except KeyboardInterrupt:\n        remove_experiment_folder(OUT_PATH)\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)  # pylint: disable=protected-access\n    except Exception:  # pylint: disable=broad-except\n        remove_experiment_folder(OUT_PATH)\n        traceback.print_exc()\n        sys.exit(1)\n", "TTS/bin/extract_tts_spectrograms.py": "#!/usr/bin/env python3\n\"\"\"Extract Mel spectrograms with teacher forcing.\"\"\"\n\nimport argparse\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config\nfrom TTS.tts.datasets import TTSDataset, load_tts_samples\nfrom TTS.tts.models import setup_model\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.audio.numpy_transforms import quantize\nfrom TTS.utils.generic_utils import count_parameters\n\nuse_cuda = torch.cuda.is_available()\n\n\ndef setup_loader(ap, r, verbose=False):\n    tokenizer, _ = TTSTokenizer.init_from_config(c)\n    dataset = TTSDataset(\n        outputs_per_step=r,\n        compute_linear_spec=False,\n        samples=meta_data,\n        tokenizer=tokenizer,\n        ap=ap,\n        batch_group_size=0,\n        min_text_len=c.min_text_len,\n        max_text_len=c.max_text_len,\n        min_audio_len=c.min_audio_len,\n        max_audio_len=c.max_audio_len,\n        phoneme_cache_path=c.phoneme_cache_path,\n        precompute_num_workers=0,\n        use_noise_augment=False,\n        verbose=verbose,\n        speaker_id_mapping=speaker_manager.name_to_id if c.use_speaker_embedding else None,\n        d_vector_mapping=speaker_manager.embeddings if c.use_d_vector_file else None,\n    )\n\n    if c.use_phonemes and c.compute_input_seq_cache:\n        # precompute phonemes to have a better estimate of sequence lengths.\n        dataset.compute_input_seq(c.num_loader_workers)\n    dataset.preprocess_samples()\n\n    loader = DataLoader(\n        dataset,\n        batch_size=c.batch_size,\n        shuffle=False,\n        collate_fn=dataset.collate_fn,\n        drop_last=False,\n        sampler=None,\n        num_workers=c.num_loader_workers,\n        pin_memory=False,\n    )\n    return loader\n\n\ndef set_filename(wav_path, out_path):\n    wav_file = os.path.basename(wav_path)\n    file_name = wav_file.split(\".\")[0]\n    os.makedirs(os.path.join(out_path, \"quant\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"mel\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"wav_gl\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"wav\"), exist_ok=True)\n    wavq_path = os.path.join(out_path, \"quant\", file_name)\n    mel_path = os.path.join(out_path, \"mel\", file_name)\n    wav_gl_path = os.path.join(out_path, \"wav_gl\", file_name + \".wav\")\n    wav_path = os.path.join(out_path, \"wav\", file_name + \".wav\")\n    return file_name, wavq_path, mel_path, wav_gl_path, wav_path\n\n\ndef format_data(data):\n    # setup input data\n    text_input = data[\"token_id\"]\n    text_lengths = data[\"token_id_lengths\"]\n    mel_input = data[\"mel\"]\n    mel_lengths = data[\"mel_lengths\"]\n    item_idx = data[\"item_idxs\"]\n    d_vectors = data[\"d_vectors\"]\n    speaker_ids = data[\"speaker_ids\"]\n    attn_mask = data[\"attns\"]\n    avg_text_length = torch.mean(text_lengths.float())\n    avg_spec_length = torch.mean(mel_lengths.float())\n\n    # dispatch data to GPU\n    if use_cuda:\n        text_input = text_input.cuda(non_blocking=True)\n        text_lengths = text_lengths.cuda(non_blocking=True)\n        mel_input = mel_input.cuda(non_blocking=True)\n        mel_lengths = mel_lengths.cuda(non_blocking=True)\n        if speaker_ids is not None:\n            speaker_ids = speaker_ids.cuda(non_blocking=True)\n        if d_vectors is not None:\n            d_vectors = d_vectors.cuda(non_blocking=True)\n        if attn_mask is not None:\n            attn_mask = attn_mask.cuda(non_blocking=True)\n    return (\n        text_input,\n        text_lengths,\n        mel_input,\n        mel_lengths,\n        speaker_ids,\n        d_vectors,\n        avg_text_length,\n        avg_spec_length,\n        attn_mask,\n        item_idx,\n    )\n\n\n@torch.no_grad()\ndef inference(\n    model_name,\n    model,\n    ap,\n    text_input,\n    text_lengths,\n    mel_input,\n    mel_lengths,\n    speaker_ids=None,\n    d_vectors=None,\n):\n    if model_name == \"glow_tts\":\n        speaker_c = None\n        if speaker_ids is not None:\n            speaker_c = speaker_ids\n        elif d_vectors is not None:\n            speaker_c = d_vectors\n        outputs = model.inference_with_MAS(\n            text_input,\n            text_lengths,\n            mel_input,\n            mel_lengths,\n            aux_input={\"d_vectors\": speaker_c, \"speaker_ids\": speaker_ids},\n        )\n        model_output = outputs[\"model_outputs\"]\n        model_output = model_output.detach().cpu().numpy()\n\n    elif \"tacotron\" in model_name:\n        aux_input = {\"speaker_ids\": speaker_ids, \"d_vectors\": d_vectors}\n        outputs = model(text_input, text_lengths, mel_input, mel_lengths, aux_input)\n        postnet_outputs = outputs[\"model_outputs\"]\n        # normalize tacotron output\n        if model_name == \"tacotron\":\n            mel_specs = []\n            postnet_outputs = postnet_outputs.data.cpu().numpy()\n            for b in range(postnet_outputs.shape[0]):\n                postnet_output = postnet_outputs[b]\n                mel_specs.append(torch.FloatTensor(ap.out_linear_to_mel(postnet_output.T).T))\n            model_output = torch.stack(mel_specs).cpu().numpy()\n\n        elif model_name == \"tacotron2\":\n            model_output = postnet_outputs.detach().cpu().numpy()\n    return model_output\n\n\ndef extract_spectrograms(\n    data_loader, model, ap, output_path, quantize_bits=0, save_audio=False, debug=False, metada_name=\"metada.txt\"\n):\n    model.eval()\n    export_metadata = []\n    for _, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n        # format data\n        (\n            text_input,\n            text_lengths,\n            mel_input,\n            mel_lengths,\n            speaker_ids,\n            d_vectors,\n            _,\n            _,\n            _,\n            item_idx,\n        ) = format_data(data)\n\n        model_output = inference(\n            c.model.lower(),\n            model,\n            ap,\n            text_input,\n            text_lengths,\n            mel_input,\n            mel_lengths,\n            speaker_ids,\n            d_vectors,\n        )\n\n        for idx in range(text_input.shape[0]):\n            wav_file_path = item_idx[idx]\n            wav = ap.load_wav(wav_file_path)\n            _, wavq_path, mel_path, wav_gl_path, wav_path = set_filename(wav_file_path, output_path)\n\n            # quantize and save wav\n            if quantize_bits > 0:\n                wavq = quantize(wav, quantize_bits)\n                np.save(wavq_path, wavq)\n\n            # save TTS mel\n            mel = model_output[idx]\n            mel_length = mel_lengths[idx]\n            mel = mel[:mel_length, :].T\n            np.save(mel_path, mel)\n\n            export_metadata.append([wav_file_path, mel_path])\n            if save_audio:\n                ap.save_wav(wav, wav_path)\n\n            if debug:\n                print(\"Audio for debug saved at:\", wav_gl_path)\n                wav = ap.inv_melspectrogram(mel)\n                ap.save_wav(wav, wav_gl_path)\n\n    with open(os.path.join(output_path, metada_name), \"w\", encoding=\"utf-8\") as f:\n        for data in export_metadata:\n            f.write(f\"{data[0]}|{data[1]+'.npy'}\\n\")\n\n\ndef main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data, speaker_manager\n\n    # Audio processor\n    ap = AudioProcessor(**c.audio)\n\n    # load data instances\n    meta_data_train, meta_data_eval = load_tts_samples(\n        c.datasets, eval_split=args.eval, eval_split_max_size=c.eval_split_max_size, eval_split_size=c.eval_split_size\n    )\n\n    # use eval and training partitions\n    meta_data = meta_data_train + meta_data_eval\n\n    # init speaker manager\n    if c.use_speaker_embedding:\n        speaker_manager = SpeakerManager(data_items=meta_data)\n    elif c.use_d_vector_file:\n        speaker_manager = SpeakerManager(d_vectors_file_path=c.d_vector_file)\n    else:\n        speaker_manager = None\n\n    # setup model\n    model = setup_model(c)\n\n    # restore model\n    model.load_checkpoint(c, args.checkpoint_path, eval=True)\n\n    if use_cuda:\n        model.cuda()\n\n    num_params = count_parameters(model)\n    print(\"\\n > Model has {} parameters\".format(num_params), flush=True)\n    # set r\n    r = 1 if c.model.lower() == \"glow_tts\" else model.decoder.r\n    own_loader = setup_loader(ap, r, verbose=True)\n\n    extract_spectrograms(\n        own_loader,\n        model,\n        ap,\n        args.output_path,\n        quantize_bits=args.quantize_bits,\n        save_audio=args.save_audio,\n        debug=args.debug,\n        metada_name=\"metada.txt\",\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config_path\", type=str, help=\"Path to config file for training.\", required=True)\n    parser.add_argument(\"--checkpoint_path\", type=str, help=\"Model file to be restored.\", required=True)\n    parser.add_argument(\"--output_path\", type=str, help=\"Path to save mel specs\", required=True)\n    parser.add_argument(\"--debug\", default=False, action=\"store_true\", help=\"Save audio files for debug\")\n    parser.add_argument(\"--save_audio\", default=False, action=\"store_true\", help=\"Save audio files\")\n    parser.add_argument(\"--quantize_bits\", type=int, default=0, help=\"Save quantized audio files if non-zero\")\n    parser.add_argument(\"--eval\", type=bool, help=\"compute eval.\", default=True)\n    args = parser.parse_args()\n\n    c = load_config(args.config_path)\n    c.audio.trim_silence = False\n    main(args)\n", "TTS/bin/remove_silence_using_vad.py": "import argparse\nimport glob\nimport multiprocessing\nimport os\nimport pathlib\n\nimport torch\nfrom tqdm import tqdm\n\nfrom TTS.utils.vad import get_vad_model_and_utils, remove_silence\n\ntorch.set_num_threads(1)\n\n\ndef adjust_path_and_remove_silence(audio_path):\n    output_path = audio_path.replace(os.path.join(args.input_dir, \"\"), os.path.join(args.output_dir, \"\"))\n    # ignore if the file exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n\n    # create all directory structure\n    pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    # remove the silence and save the audio\n    output_path, is_speech = remove_silence(\n        model_and_utils,\n        audio_path,\n        output_path,\n        trim_just_beginning_and_end=args.trim_just_beginning_and_end,\n        use_cuda=args.use_cuda,\n    )\n    return output_path, is_speech\n\n\ndef preprocess_audios():\n    files = sorted(glob.glob(os.path.join(args.input_dir, args.glob), recursive=True))\n    print(\"> Number of files: \", len(files))\n    if not args.force:\n        print(\"> Ignoring files that already exist in the output idrectory.\")\n\n    if args.trim_just_beginning_and_end:\n        print(\"> Trimming just the beginning and the end with nonspeech parts.\")\n    else:\n        print(\"> Trimming all nonspeech parts.\")\n\n    filtered_files = []\n    if files:\n        # create threads\n        # num_threads = multiprocessing.cpu_count()\n        # process_map(adjust_path_and_remove_silence, files, max_workers=num_threads, chunksize=15)\n\n        if args.num_processes > 1:\n            with multiprocessing.Pool(processes=args.num_processes) as pool:\n                results = list(\n                    tqdm(\n                        pool.imap_unordered(adjust_path_and_remove_silence, files),\n                        total=len(files),\n                        desc=\"Processing audio files\",\n                    )\n                )\n            for output_path, is_speech in results:\n                if not is_speech:\n                    filtered_files.append(output_path)\n        else:\n            for f in tqdm(files):\n                output_path, is_speech = adjust_path_and_remove_silence(f)\n                if not is_speech:\n                    filtered_files.append(output_path)\n\n        # write files that do not have speech\n        with open(os.path.join(args.output_dir, \"filtered_files.txt\"), \"w\", encoding=\"utf-8\") as f:\n            for file in filtered_files:\n                f.write(str(file) + \"\\n\")\n    else:\n        print(\"> No files Found !\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"python TTS/bin/remove_silence_using_vad.py -i=VCTK-Corpus/ -o=VCTK-Corpus-removed-silence/ -g=wav48_silence_trimmed/*/*_mic1.flac --trim_just_beginning_and_end True\"\n    )\n    parser.add_argument(\"-i\", \"--input_dir\", type=str, help=\"Dataset root dir\", required=True)\n    parser.add_argument(\"-o\", \"--output_dir\", type=str, help=\"Output Dataset dir\", default=\"\")\n    parser.add_argument(\"-f\", \"--force\", default=False, action=\"store_true\", help=\"Force the replace of exists files\")\n    parser.add_argument(\n        \"-g\",\n        \"--glob\",\n        type=str,\n        default=\"**/*.wav\",\n        help=\"path in glob format for acess wavs from input_dir. ex: wav48/*/*.wav\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--trim_just_beginning_and_end\",\n        type=bool,\n        default=True,\n        help=\"If True this script will trim just the beginning and end nonspeech parts. If False all nonspeech parts will be trim. Default True\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--use_cuda\",\n        type=bool,\n        default=False,\n        help=\"If True use cuda\",\n    )\n    parser.add_argument(\n        \"--use_onnx\",\n        type=bool,\n        default=False,\n        help=\"If True use onnx\",\n    )\n    parser.add_argument(\n        \"--num_processes\",\n        type=int,\n        default=1,\n        help=\"Number of processes to use\",\n    )\n    args = parser.parse_args()\n\n    if args.output_dir == \"\":\n        args.output_dir = args.input_dir\n\n    # load the model and utils\n    model_and_utils = get_vad_model_and_utils(use_cuda=args.use_cuda, use_onnx=args.use_onnx)\n    preprocess_audios()\n", "TTS/bin/find_unique_chars.py": "\"\"\"Find all the unique characters in a dataset\"\"\"\nimport argparse\nfrom argparse import RawTextHelpFormatter\n\nfrom TTS.config import load_config\nfrom TTS.tts.datasets import load_tts_samples\n\n\ndef main():\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Find all the unique characters or phonemes in a dataset.\\n\\n\"\"\"\n        \"\"\"\n    Example runs:\n\n    python TTS/bin/find_unique_chars.py --config_path config.json\n    \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\"--config_path\", type=str, help=\"Path to dataset config file.\", required=True)\n    args = parser.parse_args()\n\n    c = load_config(args.config_path)\n\n    # load all datasets\n    train_items, eval_items = load_tts_samples(\n        c.datasets, eval_split=True, eval_split_max_size=c.eval_split_max_size, eval_split_size=c.eval_split_size\n    )\n\n    items = train_items + eval_items\n\n    texts = \"\".join(item[\"text\"] for item in items)\n    chars = set(texts)\n    lower_chars = filter(lambda c: c.islower(), chars)\n    chars_force_lower = [c.lower() for c in chars]\n    chars_force_lower = set(chars_force_lower)\n\n    print(f\" > Number of unique characters: {len(chars)}\")\n    print(f\" > Unique characters: {''.join(sorted(chars))}\")\n    print(f\" > Unique lower characters: {''.join(sorted(lower_chars))}\")\n    print(f\" > Unique all forced to lower characters: {''.join(sorted(chars_force_lower))}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/collect_env_info.py": "\"\"\"Get detailed info about the working environment.\"\"\"\nimport os\nimport platform\nimport sys\n\nimport numpy\nimport torch\n\nsys.path += [os.path.abspath(\"..\"), os.path.abspath(\".\")]\nimport json\n\nimport TTS\n\n\ndef system_info():\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python\": platform.python_version(),\n    }\n\n\ndef cuda_info():\n    return {\n        \"GPU\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n        \"available\": torch.cuda.is_available(),\n        \"version\": torch.version.cuda,\n    }\n\n\ndef package_info():\n    return {\n        \"numpy\": numpy.__version__,\n        \"PyTorch_version\": torch.__version__,\n        \"PyTorch_debug\": torch.version.debug,\n        \"TTS\": TTS.__version__,\n    }\n\n\ndef main():\n    details = {\"System\": system_info(), \"CUDA\": cuda_info(), \"Packages\": package_info()}\n    print(json.dumps(details, indent=4, sort_keys=True))\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/train_tts.py": "import os\nfrom dataclasses import dataclass, field\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config import load_config, register_config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models import setup_model\n\n\n@dataclass\nclass TrainTTSArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\n\n\ndef main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainTTSArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)\n\n    # load config.json and register\n    if args.config_path or args.continue_path:\n        if args.config_path:\n            # init from a file\n            config = load_config(args.config_path)\n            if len(config_overrides) > 0:\n                config.parse_known_args(config_overrides, relaxed_parser=True)\n        elif args.continue_path:\n            # continue from a prev experiment\n            config = load_config(os.path.join(args.continue_path, \"config.json\"))\n            if len(config_overrides) > 0:\n                config.parse_known_args(config_overrides, relaxed_parser=True)\n        else:\n            # init from console args\n            from TTS.config.shared_configs import BaseTrainingConfig  # pylint: disable=import-outside-toplevel\n\n            config_base = BaseTrainingConfig()\n            config_base.parse_known_args(config_overrides)\n            config = register_config(config_base.model)()\n\n    # load training samples\n    train_samples, eval_samples = load_tts_samples(\n        config.datasets,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # init the model from config\n    model = setup_model(config, train_samples + eval_samples)\n\n    # init the trainer and \ud83d\ude80\n    trainer = Trainer(\n        train_args,\n        model.config,\n        config.output_path,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n        parse_command_line_args=False,\n    )\n    trainer.fit()\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/bin/__init__.py": "", "TTS/bin/eval_encoder.py": "import argparse\nfrom argparse import RawTextHelpFormatter\n\nimport torch\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.utils.speakers import SpeakerManager\n\n\ndef compute_encoder_accuracy(dataset_items, encoder_manager):\n    class_name_key = encoder_manager.encoder_config.class_name_key\n    map_classid_to_classname = getattr(encoder_manager.encoder_config, \"map_classid_to_classname\", None)\n\n    class_acc_dict = {}\n\n    # compute embeddings for all wav_files\n    for item in tqdm(dataset_items):\n        class_name = item[class_name_key]\n        wav_file = item[\"audio_file\"]\n\n        # extract the embedding\n        embedd = encoder_manager.compute_embedding_from_clip(wav_file)\n        if encoder_manager.encoder_criterion is not None and map_classid_to_classname is not None:\n            embedding = torch.FloatTensor(embedd).unsqueeze(0)\n            if encoder_manager.use_cuda:\n                embedding = embedding.cuda()\n\n            class_id = encoder_manager.encoder_criterion.softmax.inference(embedding).item()\n            predicted_label = map_classid_to_classname[str(class_id)]\n        else:\n            predicted_label = None\n\n        if class_name is not None and predicted_label is not None:\n            is_equal = int(class_name == predicted_label)\n            if class_name not in class_acc_dict:\n                class_acc_dict[class_name] = [is_equal]\n            else:\n                class_acc_dict[class_name].append(is_equal)\n        else:\n            raise RuntimeError(\"Error: class_name or/and predicted_label are None\")\n\n    acc_avg = 0\n    for key, values in class_acc_dict.items():\n        acc = sum(values) / len(values)\n        print(\"Class\", key, \"Accuracy:\", acc)\n        acc_avg += acc\n\n    print(\"Average Accuracy:\", acc_avg / len(class_acc_dict))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Compute the accuracy of the encoder.\\n\\n\"\"\"\n        \"\"\"\n        Example runs:\n        python TTS/bin/eval_encoder.py emotion_encoder_model.pth emotion_encoder_config.json  dataset_config.json\n        \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\"model_path\", type=str, help=\"Path to model checkpoint file.\")\n    parser.add_argument(\n        \"config_path\",\n        type=str,\n        help=\"Path to model config file.\",\n    )\n\n    parser.add_argument(\n        \"config_dataset_path\",\n        type=str,\n        help=\"Path to dataset config file.\",\n    )\n    parser.add_argument(\"--use_cuda\", type=bool, help=\"flag to set cuda.\", default=True)\n    parser.add_argument(\"--eval\", type=bool, help=\"compute eval.\", default=True)\n\n    args = parser.parse_args()\n\n    c_dataset = load_config(args.config_dataset_path)\n\n    meta_data_train, meta_data_eval = load_tts_samples(c_dataset.datasets, eval_split=args.eval)\n    items = meta_data_train + meta_data_eval\n\n    enc_manager = SpeakerManager(\n        encoder_model_path=args.model_path, encoder_config_path=args.config_path, use_cuda=args.use_cuda\n    )\n\n    compute_encoder_accuracy(items, enc_manager)\n", "TTS/bin/tune_wavegrad.py": "\"\"\"Search a good noise schedule for WaveGrad for a given number of inference iterations\"\"\"\nimport argparse\nfrom itertools import product as cartesian_product\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.vocoder.datasets.preprocess import load_wav_data\nfrom TTS.vocoder.datasets.wavegrad_dataset import WaveGradDataset\nfrom TTS.vocoder.models import setup_model\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, help=\"Path to model checkpoint.\")\n    parser.add_argument(\"--config_path\", type=str, help=\"Path to model config file.\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path to data directory.\")\n    parser.add_argument(\"--output_path\", type=str, help=\"path for output file including file name and extension.\")\n    parser.add_argument(\n        \"--num_iter\",\n        type=int,\n        help=\"Number of model inference iterations that you like to optimize noise schedule for.\",\n    )\n    parser.add_argument(\"--use_cuda\", action=\"store_true\", help=\"enable CUDA.\")\n    parser.add_argument(\"--num_samples\", type=int, default=1, help=\"Number of datasamples used for inference.\")\n    parser.add_argument(\n        \"--search_depth\",\n        type=int,\n        default=3,\n        help=\"Search granularity. Increasing this increases the run-time exponentially.\",\n    )\n\n    # load config\n    args = parser.parse_args()\n    config = load_config(args.config_path)\n\n    # setup audio processor\n    ap = AudioProcessor(**config.audio)\n\n    # load dataset\n    _, train_data = load_wav_data(args.data_path, 0)\n    train_data = train_data[: args.num_samples]\n    dataset = WaveGradDataset(\n        ap=ap,\n        items=train_data,\n        seq_len=-1,\n        hop_len=ap.hop_length,\n        pad_short=config.pad_short,\n        conv_pad=config.conv_pad,\n        is_training=True,\n        return_segments=False,\n        use_noise_augment=False,\n        use_cache=False,\n        verbose=True,\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=1,\n        shuffle=False,\n        collate_fn=dataset.collate_full_clips,\n        drop_last=False,\n        num_workers=config.num_loader_workers,\n        pin_memory=False,\n    )\n\n    # setup the model\n    model = setup_model(config)\n    if args.use_cuda:\n        model.cuda()\n\n    # setup optimization parameters\n    base_values = sorted(10 * np.random.uniform(size=args.search_depth))\n    print(f\" > base values: {base_values}\")\n    exponents = 10 ** np.linspace(-6, -1, num=args.num_iter)\n    best_error = float(\"inf\")\n    best_schedule = None  # pylint: disable=C0103\n    total_search_iter = len(base_values) ** args.num_iter\n    for base in tqdm(cartesian_product(base_values, repeat=args.num_iter), total=total_search_iter):\n        beta = exponents * base\n        model.compute_noise_level(beta)\n        for data in loader:\n            mel, audio = data\n            y_hat = model.inference(mel.cuda() if args.use_cuda else mel)\n\n            if args.use_cuda:\n                y_hat = y_hat.cpu()\n            y_hat = y_hat.numpy()\n\n            mel_hat = []\n            for i in range(y_hat.shape[0]):\n                m = ap.melspectrogram(y_hat[i, 0])[:, :-1]\n                mel_hat.append(torch.from_numpy(m))\n\n            mel_hat = torch.stack(mel_hat)\n            mse = torch.sum((mel - mel_hat) ** 2).mean()\n            if mse.item() < best_error:\n                best_error = mse.item()\n                best_schedule = {\"beta\": beta}\n                print(f\" > Found a better schedule. - MSE: {mse.item()}\")\n                np.save(args.output_path, best_schedule)\n", "TTS/bin/compute_attention_masks.py": "import argparse\nimport importlib\nimport os\nfrom argparse import RawTextHelpFormatter\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom TTS.config import load_config\nfrom TTS.tts.datasets.TTSDataset import TTSDataset\nfrom TTS.tts.models import setup_model\nfrom TTS.tts.utils.text.characters import make_symbols, phonemes, symbols\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.io import load_checkpoint\n\nif __name__ == \"__main__\":\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Extract attention masks from trained Tacotron/Tacotron2 models.\nThese masks can be used for different purposes including training a TTS model with a Duration Predictor.\\n\\n\"\"\"\n        \"\"\"Each attention mask is written to the same path as the input wav file with \".npy\" file extension.\n(e.g. path/bla.wav (wav file) --> path/bla.npy (attention mask))\\n\"\"\"\n        \"\"\"\nExample run:\n    CUDA_VISIBLE_DEVICE=\"0\" python TTS/bin/compute_attention_masks.py\n        --model_path /data/rw/home/Models/ljspeech-dcattn-December-14-2020_11+10AM-9d0e8c7/checkpoint_200000.pth\n        --config_path /data/rw/home/Models/ljspeech-dcattn-December-14-2020_11+10AM-9d0e8c7/config.json\n        --dataset_metafile metadata.csv\n        --data_path /root/LJSpeech-1.1/\n        --batch_size 32\n        --dataset ljspeech\n        --use_cuda True\n\"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )\n    parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to Tacotron/Tacotron2 model file \")\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        required=True,\n        help=\"Path to Tacotron/Tacotron2 config file.\",\n    )\n    parser.add_argument(\n        \"--dataset\",\n        type=str,\n        default=\"\",\n        required=True,\n        help=\"Target dataset processor name from TTS.tts.dataset.preprocess.\",\n    )\n\n    parser.add_argument(\n        \"--dataset_metafile\",\n        type=str,\n        default=\"\",\n        required=True,\n        help=\"Dataset metafile inclusing file paths with transcripts.\",\n    )\n    parser.add_argument(\"--data_path\", type=str, default=\"\", help=\"Defines the data path. It overwrites config.json.\")\n    parser.add_argument(\"--use_cuda\", type=bool, default=False, help=\"enable/disable cuda.\")\n\n    parser.add_argument(\n        \"--batch_size\", default=16, type=int, help=\"Batch size for the model. Use batch_size=1 if you have no CUDA.\"\n    )\n    args = parser.parse_args()\n\n    C = load_config(args.config_path)\n    ap = AudioProcessor(**C.audio)\n\n    # if the vocabulary was passed, replace the default\n    if \"characters\" in C.keys():\n        symbols, phonemes = make_symbols(**C.characters)\n\n    # load the model\n    num_chars = len(phonemes) if C.use_phonemes else len(symbols)\n    # TODO: handle multi-speaker\n    model = setup_model(C)\n    model, _ = load_checkpoint(model, args.model_path, args.use_cuda, True)\n\n    # data loader\n    preprocessor = importlib.import_module(\"TTS.tts.datasets.formatters\")\n    preprocessor = getattr(preprocessor, args.dataset)\n    meta_data = preprocessor(args.data_path, args.dataset_metafile)\n    dataset = TTSDataset(\n        model.decoder.r,\n        C.text_cleaner,\n        compute_linear_spec=False,\n        ap=ap,\n        meta_data=meta_data,\n        characters=C.characters if \"characters\" in C.keys() else None,\n        add_blank=C[\"add_blank\"] if \"add_blank\" in C.keys() else False,\n        use_phonemes=C.use_phonemes,\n        phoneme_cache_path=C.phoneme_cache_path,\n        phoneme_language=C.phoneme_language,\n        enable_eos_bos=C.enable_eos_bos_chars,\n    )\n\n    dataset.sort_and_filter_items(C.get(\"sort_by_audio_len\", default=False))\n    loader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        num_workers=4,\n        collate_fn=dataset.collate_fn,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    # compute attentions\n    file_paths = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            # setup input data\n            text_input = data[0]\n            text_lengths = data[1]\n            linear_input = data[3]\n            mel_input = data[4]\n            mel_lengths = data[5]\n            stop_targets = data[6]\n            item_idxs = data[7]\n\n            # dispatch data to GPU\n            if args.use_cuda:\n                text_input = text_input.cuda()\n                text_lengths = text_lengths.cuda()\n                mel_input = mel_input.cuda()\n                mel_lengths = mel_lengths.cuda()\n\n            model_outputs = model.forward(text_input, text_lengths, mel_input)\n\n            alignments = model_outputs[\"alignments\"].detach()\n            for idx, alignment in enumerate(alignments):\n                item_idx = item_idxs[idx]\n                # interpolate if r > 1\n                alignment = (\n                    torch.nn.functional.interpolate(\n                        alignment.transpose(0, 1).unsqueeze(0),\n                        size=None,\n                        scale_factor=model.decoder.r,\n                        mode=\"nearest\",\n                        align_corners=None,\n                        recompute_scale_factor=None,\n                    )\n                    .squeeze(0)\n                    .transpose(0, 1)\n                )\n                # remove paddings\n                alignment = alignment[: mel_lengths[idx], : text_lengths[idx]].cpu().numpy()\n                # set file paths\n                wav_file_name = os.path.basename(item_idx)\n                align_file_name = os.path.splitext(wav_file_name)[0] + \"_attn.npy\"\n                file_path = item_idx.replace(wav_file_name, align_file_name)\n                # save output\n                wav_file_abs_path = os.path.abspath(item_idx)\n                file_abs_path = os.path.abspath(file_path)\n                file_paths.append([wav_file_abs_path, file_abs_path])\n                np.save(file_path, alignment)\n\n        # ourput metafile\n        metafile = os.path.join(args.data_path, \"metadata_attn_mask.txt\")\n\n        with open(metafile, \"w\", encoding=\"utf-8\") as f:\n            for p in file_paths:\n                f.write(f\"{p[0]}|{p[1]}\\n\")\n        print(f\" >> Metafile created: {metafile}\")\n", "TTS/bin/synthesize.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport contextlib\nimport sys\nfrom argparse import RawTextHelpFormatter\n\n# pylint: disable=redefined-outer-name, unused-argument\nfrom pathlib import Path\n\ndescription = \"\"\"\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don't specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\"\"\"\n\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    if v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=description.replace(\"    ```\\n\", \"\"),\n        formatter_class=RawTextHelpFormatter,\n    )\n\n    parser.add_argument(\n        \"--list_models\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"list available pre-trained TTS and vocoder models.\",\n    )\n\n    parser.add_argument(\n        \"--model_info_by_idx\",\n        type=str,\n        default=None,\n        help=\"model info using query format: <model_type>/<model_query_idx>\",\n    )\n\n    parser.add_argument(\n        \"--model_info_by_name\",\n        type=str,\n        default=None,\n        help=\"model info using query format: <model_type>/<language>/<dataset>/<model_name>\",\n    )\n\n    parser.add_argument(\"--text\", type=str, default=None, help=\"Text to generate speech.\")\n\n    # Args for running pre-trained TTS models.\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=\"tts_models/en/ljspeech/tacotron2-DDC\",\n        help=\"Name of one of the pre-trained TTS models in format <language>/<dataset>/<model_name>\",\n    )\n    parser.add_argument(\n        \"--vocoder_name\",\n        type=str,\n        default=None,\n        help=\"Name of one of the pre-trained  vocoder models in format <language>/<dataset>/<model_name>\",\n    )\n\n    # Args for running custom models\n    parser.add_argument(\"--config_path\", default=None, type=str, help=\"Path to model config file.\")\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        default=None,\n        help=\"Path to model file.\",\n    )\n    parser.add_argument(\n        \"--out_path\",\n        type=str,\n        default=\"tts_output.wav\",\n        help=\"Output wav file path.\",\n    )\n    parser.add_argument(\"--use_cuda\", type=bool, help=\"Run model on CUDA.\", default=False)\n    parser.add_argument(\"--device\", type=str, help=\"Device to run model on.\", default=\"cpu\")\n    parser.add_argument(\n        \"--vocoder_path\",\n        type=str,\n        help=\"Path to vocoder model file. If it is not defined, model uses GL as vocoder. Please make sure that you installed vocoder library before (WaveRNN).\",\n        default=None,\n    )\n    parser.add_argument(\"--vocoder_config_path\", type=str, help=\"Path to vocoder model config file.\", default=None)\n    parser.add_argument(\n        \"--encoder_path\",\n        type=str,\n        help=\"Path to speaker encoder model file.\",\n        default=None,\n    )\n    parser.add_argument(\"--encoder_config_path\", type=str, help=\"Path to speaker encoder config file.\", default=None)\n    parser.add_argument(\n        \"--pipe_out\",\n        help=\"stdout the generated TTS wav file for shell pipe.\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n    )\n    \n    # args for multi-speaker synthesis\n    parser.add_argument(\"--speakers_file_path\", type=str, help=\"JSON file for multi-speaker model.\", default=None)\n    parser.add_argument(\"--language_ids_file_path\", type=str, help=\"JSON file for multi-lingual model.\", default=None)\n    parser.add_argument(\n        \"--speaker_idx\",\n        type=str,\n        help=\"Target speaker ID for a multi-speaker TTS model.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--language_idx\",\n        type=str,\n        help=\"Target language ID for a multi-lingual TTS model.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--speaker_wav\",\n        nargs=\"+\",\n        help=\"wav file(s) to condition a multi-speaker TTS model with a Speaker Encoder. You can give multiple file paths. The d_vectors is computed as their average.\",\n        default=None,\n    )\n    parser.add_argument(\"--gst_style\", help=\"Wav path file for GST style reference.\", default=None)\n    parser.add_argument(\n        \"--capacitron_style_wav\", type=str, help=\"Wav path file for Capacitron prosody reference.\", default=None\n    )\n    parser.add_argument(\"--capacitron_style_text\", type=str, help=\"Transcription of the reference.\", default=None)\n    parser.add_argument(\n        \"--list_speaker_idxs\",\n        help=\"List available speaker ids for the defined multi-speaker model.\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n    )\n    parser.add_argument(\n        \"--list_language_idxs\",\n        help=\"List available language ids for the defined multi-lingual model.\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n    )\n    # aux args\n    parser.add_argument(\n        \"--save_spectogram\",\n        type=bool,\n        help=\"If true save raw spectogram for further (vocoder) processing in out_path.\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--reference_wav\",\n        type=str,\n        help=\"Reference wav file to convert in the voice of the speaker_idx or speaker_wav\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--reference_speaker_idx\",\n        type=str,\n        help=\"speaker ID of the reference_wav speaker (If not provided the embedding will be computed using the Speaker Encoder).\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--progress_bar\",\n        type=str2bool,\n        help=\"If true shows a progress bar for the model download. Defaults to True\",\n        default=True,\n    )\n\n    # voice conversion args\n    parser.add_argument(\n        \"--source_wav\",\n        type=str,\n        default=None,\n        help=\"Original audio file to convert in the voice of the target_wav\",\n    )\n    parser.add_argument(\n        \"--target_wav\",\n        type=str,\n        default=None,\n        help=\"Target audio file to convert in the voice of the source_wav\",\n    )\n\n    parser.add_argument(\n        \"--voice_dir\",\n        type=str,\n        default=None,\n        help=\"Voice dir for tortoise model\",\n    )\n\n    args = parser.parse_args()\n\n    # print the description if either text or list_models is not set\n    check_args = [\n        args.text,\n        args.list_models,\n        args.list_speaker_idxs,\n        args.list_language_idxs,\n        args.reference_wav,\n        args.model_info_by_idx,\n        args.model_info_by_name,\n        args.source_wav,\n        args.target_wav,\n    ]\n    if not any(check_args):\n        parser.parse_args([\"-h\"])\n\n    pipe_out = sys.stdout if args.pipe_out else None\n\n    with contextlib.redirect_stdout(None if args.pipe_out else sys.stdout):\n        # Late-import to make things load faster\n        from TTS.api import TTS\n        from TTS.utils.manage import ModelManager\n        from TTS.utils.synthesizer import Synthesizer\n\n        # load model manager\n        path = Path(__file__).parent / \"../.models.json\"\n        manager = ModelManager(path, progress_bar=args.progress_bar)\n        api = TTS()\n\n        tts_path = None\n        tts_config_path = None\n        speakers_file_path = None\n        language_ids_file_path = None\n        vocoder_path = None\n        vocoder_config_path = None\n        encoder_path = None\n        encoder_config_path = None\n        vc_path = None\n        vc_config_path = None\n        model_dir = None\n\n        # CASE1 #list : list pre-trained TTS models\n        if args.list_models:\n            manager.list_models()\n            sys.exit()\n\n        # CASE2 #info : model info for pre-trained TTS models\n        if args.model_info_by_idx:\n            model_query = args.model_info_by_idx\n            manager.model_info_by_idx(model_query)\n            sys.exit()\n\n        if args.model_info_by_name:\n            model_query_full_name = args.model_info_by_name\n            manager.model_info_by_full_name(model_query_full_name)\n            sys.exit()\n\n        # CASE3: load pre-trained model paths\n        if args.model_name is not None and not args.model_path:\n            model_path, config_path, model_item = manager.download_model(args.model_name)\n            # tts model\n            if model_item[\"model_type\"] == \"tts_models\":\n                tts_path = model_path\n                tts_config_path = config_path\n                if \"default_vocoder\" in model_item:\n                    args.vocoder_name = (\n                        model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name\n                    )\n\n            # voice conversion model\n            if model_item[\"model_type\"] == \"voice_conversion_models\":\n                vc_path = model_path\n                vc_config_path = config_path\n\n            # tts model with multiple files to be loaded from the directory path\n            if model_item.get(\"author\", None) == \"fairseq\" or isinstance(model_item[\"model_url\"], list):\n                model_dir = model_path\n                tts_path = None\n                tts_config_path = None\n                args.vocoder_name = None\n\n        # load vocoder\n        if args.vocoder_name is not None and not args.vocoder_path:\n            vocoder_path, vocoder_config_path, _ = manager.download_model(args.vocoder_name)\n\n        # CASE4: set custom model paths\n        if args.model_path is not None:\n            tts_path = args.model_path\n            tts_config_path = args.config_path\n            speakers_file_path = args.speakers_file_path\n            language_ids_file_path = args.language_ids_file_path\n\n        if args.vocoder_path is not None:\n            vocoder_path = args.vocoder_path\n            vocoder_config_path = args.vocoder_config_path\n\n        if args.encoder_path is not None:\n            encoder_path = args.encoder_path\n            encoder_config_path = args.encoder_config_path\n\n        device = args.device\n        if args.use_cuda:\n            device = \"cuda\"\n\n        # load models\n        synthesizer = Synthesizer(\n            tts_path,\n            tts_config_path,\n            speakers_file_path,\n            language_ids_file_path,\n            vocoder_path,\n            vocoder_config_path,\n            encoder_path,\n            encoder_config_path,\n            vc_path,\n            vc_config_path,\n            model_dir,\n            args.voice_dir,\n        ).to(device)\n\n        # query speaker ids of a multi-speaker model.\n        if args.list_speaker_idxs:\n            print(\n                \" > Available speaker ids: (Set --speaker_idx flag to one of these values to use the multi-speaker model.\"\n            )\n            print(synthesizer.tts_model.speaker_manager.name_to_id)\n            return\n\n        # query langauge ids of a multi-lingual model.\n        if args.list_language_idxs:\n            print(\n                \" > Available language ids: (Set --language_idx flag to one of these values to use the multi-lingual model.\"\n            )\n            print(synthesizer.tts_model.language_manager.name_to_id)\n            return\n\n        # check the arguments against a multi-speaker model.\n        if synthesizer.tts_speakers_file and (not args.speaker_idx and not args.speaker_wav):\n            print(\n                \" [!] Looks like you use a multi-speaker model. Define `--speaker_idx` to \"\n                \"select the target speaker. You can list the available speakers for this model by `--list_speaker_idxs`.\"\n            )\n            return\n\n        # RUN THE SYNTHESIS\n        if args.text:\n            print(\" > Text: {}\".format(args.text))\n\n        # kick it\n        if tts_path is not None:\n            wav = synthesizer.tts(\n                args.text,\n                speaker_name=args.speaker_idx,\n                language_name=args.language_idx,\n                speaker_wav=args.speaker_wav,\n                reference_wav=args.reference_wav,\n                style_wav=args.capacitron_style_wav,\n                style_text=args.capacitron_style_text,\n                reference_speaker_name=args.reference_speaker_idx,\n            )\n        elif vc_path is not None:\n            wav = synthesizer.voice_conversion(\n                source_wav=args.source_wav,\n                target_wav=args.target_wav,\n            )\n        elif model_dir is not None:\n            wav = synthesizer.tts(\n                args.text, speaker_name=args.speaker_idx, language_name=args.language_idx, speaker_wav=args.speaker_wav\n            )\n\n        # save the results\n        print(\" > Saving output to {}\".format(args.out_path))\n        synthesizer.save_wav(wav, args.out_path, pipe_out=pipe_out)\n\n\nif __name__ == \"__main__\":\n    main()\n", "TTS/vc/models/freevc.py": "from typing import Dict, List, Optional, Tuple, Union\n\nimport librosa\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.nn import Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import spectral_norm\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nimport TTS.vc.modules.freevc.commons as commons\nimport TTS.vc.modules.freevc.modules as modules\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vc.configs.freevc_config import FreeVCConfig\nfrom TTS.vc.models.base_vc import BaseVC\nfrom TTS.vc.modules.freevc.commons import get_padding, init_weights\nfrom TTS.vc.modules.freevc.mel_processing import mel_spectrogram_torch\nfrom TTS.vc.modules.freevc.speaker_encoder.speaker_encoder import SpeakerEncoder as SpeakerEncoderEx\nfrom TTS.vc.modules.freevc.wavlm import get_wavlm\n\n\nclass ResidualCouplingBlock(nn.Module):\n    def __init__(self, channels, hidden_channels, kernel_size, dilation_rate, n_layers, n_flows=4, gin_channels=0):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(\n                modules.ResidualCouplingLayer(\n                    channels,\n                    hidden_channels,\n                    kernel_size,\n                    dilation_rate,\n                    n_layers,\n                    gin_channels=gin_channels,\n                    mean_only=True,\n                )\n            )\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.gin_channels = gin_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n        return z, m, logs, x_mask\n\n\nclass Generator(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        gin_channels=0,\n    ):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == \"1\" else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n            x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print(\"Removing weight norm...\")\n        for l in self.ups:\n            remove_parametrizations(l, \"weight\")\n        for l in self.resblocks:\n            remove_parametrizations(l, \"weight\")\n\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n            ]\n        )\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n            ]\n        )\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2, 3, 5, 7, 11]\n\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n        self.discriminators = nn.ModuleList(discs)\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            y_d_gs.append(y_d_g)\n            fmap_rs.append(fmap_r)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass SpeakerEncoder(torch.nn.Module):\n    def __init__(self, mel_n_channels=80, model_num_layers=3, model_hidden_size=256, model_embedding_size=256):\n        super(SpeakerEncoder, self).__init__()\n        self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n        self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, mels):\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(mels)\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n\n    def compute_partial_slices(self, total_frames, partial_frames, partial_hop):\n        mel_slices = []\n        for i in range(0, total_frames - partial_frames, partial_hop):\n            mel_range = torch.arange(i, i + partial_frames)\n            mel_slices.append(mel_range)\n\n        return mel_slices\n\n    def embed_utterance(self, mel, partial_frames=128, partial_hop=64):\n        mel_len = mel.size(1)\n        last_mel = mel[:, -partial_frames:]\n\n        if mel_len > partial_frames:\n            mel_slices = self.compute_partial_slices(mel_len, partial_frames, partial_hop)\n            mels = list(mel[:, s] for s in mel_slices)\n            mels.append(last_mel)\n            mels = torch.stack(tuple(mels), 0).squeeze(1)\n\n            with torch.no_grad():\n                partial_embeds = self(mels)\n            embed = torch.mean(partial_embeds, axis=0).unsqueeze(0)\n            # embed = embed / torch.linalg.norm(embed, 2)\n        else:\n            with torch.no_grad():\n                embed = self(last_mel)\n\n        return embed\n\n\nclass FreeVC(BaseVC):\n    \"\"\"\n\n    Papaer::\n        https://arxiv.org/abs/2210.15418#\n\n    Paper Abstract::\n        Voice conversion (VC) can be achieved by first extracting source content information and target speaker\n        information, and then reconstructing waveform with these information. However, current approaches normally\n        either extract dirty content information with speaker information leaked in, or demand a large amount of\n        annotated data for training. Besides, the quality of reconstructed waveform can be degraded by the\n        mismatch between conversion model and vocoder. In this paper, we adopt the end-to-end framework of VITS for\n        high-quality waveform reconstruction, and propose strategies for clean content information extraction without\n        text annotation. We disentangle content information by imposing an information bottleneck to WavLM features,\n        and propose the spectrogram-resize based data augmentation to improve the purity of extracted content\n        information. Experimental results show that the proposed method outperforms the latest VC models trained with\n        annotated data and has greater robustness.\n\n    Original Code::\n        https://github.com/OlaWod/FreeVC\n\n    Examples:\n        >>> from TTS.vc.configs.freevc_config import FreeVCConfig\n        >>> from TTS.vc.models.freevc import FreeVC\n        >>> config = FreeVCConfig()\n        >>> model = FreeVC(config)\n    \"\"\"\n\n    def __init__(self, config: Coqpit, speaker_manager: SpeakerManager = None):\n        super().__init__(config, None, speaker_manager, None)\n\n        self.init_multispeaker(config)\n\n        self.spec_channels = self.args.spec_channels\n        self.inter_channels = self.args.inter_channels\n        self.hidden_channels = self.args.hidden_channels\n        self.filter_channels = self.args.filter_channels\n        self.n_heads = self.args.n_heads\n        self.n_layers = self.args.n_layers\n        self.kernel_size = self.args.kernel_size\n        self.p_dropout = self.args.p_dropout\n        self.resblock = self.args.resblock\n        self.resblock_kernel_sizes = self.args.resblock_kernel_sizes\n        self.resblock_dilation_sizes = self.args.resblock_dilation_sizes\n        self.upsample_rates = self.args.upsample_rates\n        self.upsample_initial_channel = self.args.upsample_initial_channel\n        self.upsample_kernel_sizes = self.args.upsample_kernel_sizes\n        self.segment_size = self.args.segment_size\n        self.gin_channels = self.args.gin_channels\n        self.ssl_dim = self.args.ssl_dim\n        self.use_spk = self.args.use_spk\n\n        self.enc_p = Encoder(self.args.ssl_dim, self.inter_channels, self.hidden_channels, 5, 1, 16)\n        self.dec = Generator(\n            self.inter_channels,\n            self.resblock,\n            self.resblock_kernel_sizes,\n            self.resblock_dilation_sizes,\n            self.upsample_rates,\n            self.upsample_initial_channel,\n            self.upsample_kernel_sizes,\n            gin_channels=self.gin_channels,\n        )\n        self.enc_q = Encoder(\n            self.spec_channels, self.inter_channels, self.hidden_channels, 5, 1, 16, gin_channels=self.gin_channels\n        )\n        self.flow = ResidualCouplingBlock(\n            self.inter_channels, self.hidden_channels, 5, 1, 4, gin_channels=self.gin_channels\n        )\n        if not self.use_spk:\n            self.enc_spk = SpeakerEncoder(model_hidden_size=self.gin_channels, model_embedding_size=self.gin_channels)\n        else:\n            self.load_pretrained_speaker_encoder()\n\n        self.wavlm = get_wavlm()\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def load_pretrained_speaker_encoder(self):\n        \"\"\"Load pretrained speaker encoder model as mentioned in the paper.\"\"\"\n        print(\" > Loading pretrained speaker encoder model ...\")\n        self.enc_spk_ex = SpeakerEncoderEx(\n            \"https://github.com/coqui-ai/TTS/releases/download/v0.13.0_models/speaker_encoder.pt\"\n        )\n\n    def init_multispeaker(self, config: Coqpit):\n        \"\"\"Initialize multi-speaker modules of a model. A model can be trained either with a speaker embedding layer\n        or with external `d_vectors` computed from a speaker encoder model.\n\n        You must provide a `speaker_manager` at initialization to set up the multi-speaker modules.\n\n        Args:\n            config (Coqpit): Model configuration.\n            data (List, optional): Dataset items to infer number of speakers. Defaults to None.\n        \"\"\"\n        self.num_spks = self.args.num_spks\n        if self.speaker_manager:\n            self.num_spks = self.speaker_manager.num_spks\n\n    def forward(\n        self,\n        c: torch.Tensor,\n        spec: torch.Tensor,\n        g: Optional[torch.Tensor] = None,\n        mel: Optional[torch.Tensor] = None,\n        c_lengths: Optional[torch.Tensor] = None,\n        spec_lengths: Optional[torch.Tensor] = None,\n    ) -> Tuple[\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n    ]:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            c: WavLM features. Shape: (batch_size, c_seq_len).\n            spec: The input spectrogram. Shape: (batch_size, spec_seq_len, spec_dim).\n            g: The speaker embedding. Shape: (batch_size, spk_emb_dim).\n            mel: The input mel-spectrogram for the speaker encoder. Shape: (batch_size, mel_seq_len, mel_dim).\n            c_lengths: The lengths of the WavLM features. Shape: (batch_size,).\n            spec_lengths: The lengths of the spectrogram. Shape: (batch_size,).\n\n        Returns:\n            o: The output spectrogram. Shape: (batch_size, spec_seq_len, spec_dim).\n            ids_slice: The slice indices. Shape: (batch_size, num_slices).\n            spec_mask: The spectrogram mask. Shape: (batch_size, spec_seq_len).\n            (z, z_p, m_p, logs_p, m_q, logs_q): A tuple of latent variables.\n        \"\"\"\n\n        # If c_lengths is None, set it to the length of the last dimension of c\n        if c_lengths is None:\n            c_lengths = (torch.ones(c.size(0)) * c.size(-1)).to(c.device)\n\n        # If spec_lengths is None, set it to the length of the last dimension of spec\n        if spec_lengths is None:\n            spec_lengths = (torch.ones(spec.size(0)) * spec.size(-1)).to(spec.device)\n\n        # If use_spk is False, compute g from mel using enc_spk\n        g = None\n        if not self.use_spk:\n            g = self.enc_spk(mel).unsqueeze(-1)\n\n        # Compute m_p, logs_p, z, m_q, logs_q, and spec_mask using enc_p and enc_q\n        _, m_p, logs_p, _ = self.enc_p(c, c_lengths)\n        z, m_q, logs_q, spec_mask = self.enc_q(spec.transpose(1, 2), spec_lengths, g=g)\n\n        # Compute z_p using flow\n        z_p = self.flow(z, spec_mask, g=g)\n\n        # Randomly slice z and compute o using dec\n        z_slice, ids_slice = commons.rand_slice_segments(z, spec_lengths, self.segment_size)\n        o = self.dec(z_slice, g=g)\n\n        return o, ids_slice, spec_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n    @torch.no_grad()\n    def inference(self, c, g=None, mel=None, c_lengths=None):\n        \"\"\"\n        Inference pass of the model\n\n        Args:\n            c (torch.Tensor): Input tensor. Shape: (batch_size, c_seq_len).\n            g (torch.Tensor): Speaker embedding tensor. Shape: (batch_size, spk_emb_dim).\n            mel (torch.Tensor): Mel-spectrogram tensor. Shape: (batch_size, mel_seq_len, mel_dim).\n            c_lengths (torch.Tensor): Lengths of the input tensor. Shape: (batch_size,).\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        if c_lengths == None:\n            c_lengths = (torch.ones(c.size(0)) * c.size(-1)).to(c.device)\n        if not self.use_spk:\n            g = self.enc_spk.embed_utterance(mel)\n            g = g.unsqueeze(-1)\n        z_p, m_p, logs_p, c_mask = self.enc_p(c, c_lengths)\n        z = self.flow(z_p, c_mask, g=g, reverse=True)\n        o = self.dec(z * c_mask, g=g)\n        return o\n\n    def extract_wavlm_features(self, y):\n        \"\"\"Extract WavLM features from an audio tensor.\n\n        Args:\n            y (torch.Tensor): Audio tensor. Shape: (batch_size, audio_seq_len).\n        \"\"\"\n\n        with torch.no_grad():\n            c = self.wavlm.extract_features(y)[0]\n        c = c.transpose(1, 2)\n        return c\n\n    def load_audio(self, wav):\n        \"\"\"Read and format the input audio.\"\"\"\n        if isinstance(wav, str):\n            wav, _ = librosa.load(wav, sr=self.config.audio.input_sample_rate)\n        if isinstance(wav, np.ndarray):\n            wav = torch.from_numpy(wav).to(self.device)\n        if isinstance(wav, torch.Tensor):\n            wav = wav.to(self.device)\n        if isinstance(wav, list):\n            wav = torch.from_numpy(np.array(wav)).to(self.device)\n        return wav.float()\n\n    @torch.inference_mode()\n    def voice_conversion(self, src, tgt):\n        \"\"\"\n        Voice conversion pass of the model.\n\n        Args:\n            src (str or torch.Tensor): Source utterance.\n            tgt (str or torch.Tensor): Target utterance.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n\n        wav_tgt = self.load_audio(tgt).cpu().numpy()\n        wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n\n        if self.config.model_args.use_spk:\n            g_tgt = self.enc_spk_ex.embed_utterance(wav_tgt)\n            g_tgt = torch.from_numpy(g_tgt)[None, :, None].to(self.device)\n        else:\n            wav_tgt = torch.from_numpy(wav_tgt).unsqueeze(0).to(self.device)\n            mel_tgt = mel_spectrogram_torch(\n                wav_tgt,\n                self.config.audio.filter_length,\n                self.config.audio.n_mel_channels,\n                self.config.audio.input_sample_rate,\n                self.config.audio.hop_length,\n                self.config.audio.win_length,\n                self.config.audio.mel_fmin,\n                self.config.audio.mel_fmax,\n            )\n        # src\n        wav_src = self.load_audio(src)\n        c = self.extract_wavlm_features(wav_src[None, :])\n\n        if self.config.model_args.use_spk:\n            audio = self.inference(c, g=g_tgt)\n        else:\n            audio = self.inference(c, mel=mel_tgt.transpose(1, 2))\n        audio = audio[0][0].data.cpu().float().numpy()\n        return audio\n\n    def eval_step():\n        ...\n\n    @staticmethod\n    def init_from_config(config: FreeVCConfig, samples: Union[List[List], List[Dict]] = None, verbose=True):\n        model = FreeVC(config)\n        return model\n\n    def load_checkpoint(self, config, checkpoint_path, eval=False, strict=True, cache=False):\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"], strict=strict)\n        if eval:\n            self.eval()\n\n    def train_step():\n        ...\n", "TTS/vc/models/base_vc.py": "import os\nimport random\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nimport torch.distributed as dist\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom trainer.torch import DistributedSampler, DistributedSamplerWrapper\n\nfrom TTS.model import BaseTrainerModel\nfrom TTS.tts.datasets.dataset import TTSDataset\nfrom TTS.tts.utils.data import get_length_balancer_weights\nfrom TTS.tts.utils.languages import LanguageManager, get_language_balancer_weights\nfrom TTS.tts.utils.speakers import SpeakerManager, get_speaker_balancer_weights\nfrom TTS.tts.utils.synthesis import synthesis\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n\n# pylint: skip-file\n\n\nclass BaseVC(BaseTrainerModel):\n    \"\"\"Base `vc` class. Every new `vc` model must inherit this.\n\n    It defines common `vc` specific functions on top of `Model` implementation.\n    \"\"\"\n\n    MODEL_TYPE = \"vc\"\n\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\",\n        speaker_manager: SpeakerManager = None,\n        language_manager: LanguageManager = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.ap = ap\n        self.speaker_manager = speaker_manager\n        self.language_manager = language_manager\n        self._set_model_args(config)\n\n    def _set_model_args(self, config: Coqpit):\n        \"\"\"Setup model args based on the config type (`ModelConfig` or `ModelArgs`).\n\n        `ModelArgs` has all the fields reuqired to initialize the model architecture.\n\n        `ModelConfig` has all the fields required for training, inference and containes `ModelArgs`.\n\n        If the config is for training with a name like \"*Config\", then the model args are embeded in the\n        config.model_args\n\n        If the config is for the model with a name like \"*Args\", then we assign the directly.\n        \"\"\"\n        # don't use isintance not to import recursively\n        if \"Config\" in config.__class__.__name__:\n            self.config = config\n            self.args = config.model_args\n        elif \"Args\" in config.__class__.__name__:\n            self.args = config\n        else:\n            raise ValueError(\"config must be either a *Config or *Args\")\n\n    def init_multispeaker(self, config: Coqpit, data: List = None):\n        \"\"\"Initialize a speaker embedding layer if needen and define expected embedding channel size for defining\n        `in_channels` size of the connected layers.\n\n        This implementation yields 3 possible outcomes:\n\n        1. If `config.use_speaker_embedding` and `config.use_d_vector_file are False, do nothing.\n        2. If `config.use_d_vector_file` is True, set expected embedding channel size to `config.d_vector_dim` or 512.\n        3. If `config.use_speaker_embedding`, initialize a speaker embedding layer with channel size of\n        `config.d_vector_dim` or 512.\n\n        You can override this function for new models.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        # set number of speakers\n        if self.speaker_manager is not None:\n            self.num_speakers = self.speaker_manager.num_speakers\n        elif hasattr(config, \"num_speakers\"):\n            self.num_speakers = config.num_speakers\n\n        # set ultimate speaker embedding size\n        if config.use_speaker_embedding or config.use_d_vector_file:\n            self.embedded_speaker_dim = (\n                config.d_vector_dim if \"d_vector_dim\" in config and config.d_vector_dim is not None else 512\n            )\n        # init speaker embedding layer\n        if config.use_speaker_embedding and not config.use_d_vector_file:\n            print(\" > Init speaker_embedding layer.\")\n            self.speaker_embedding = nn.Embedding(self.num_speakers, self.embedded_speaker_dim)\n            self.speaker_embedding.weight.data.normal_(0, 0.3)\n\n    def get_aux_input(self, **kwargs) -> Dict:\n        \"\"\"Prepare and return `aux_input` used by `forward()`\"\"\"\n        return {\"speaker_id\": None, \"style_wav\": None, \"d_vector\": None, \"language_id\": None}\n\n    def get_aux_input_from_test_sentences(self, sentence_info):\n        if hasattr(self.config, \"model_args\"):\n            config = self.config.model_args\n        else:\n            config = self.config\n\n        # extract speaker and language info\n        text, speaker_name, style_wav, language_name = None, None, None, None\n\n        if isinstance(sentence_info, list):\n            if len(sentence_info) == 1:\n                text = sentence_info[0]\n            elif len(sentence_info) == 2:\n                text, speaker_name = sentence_info\n            elif len(sentence_info) == 3:\n                text, speaker_name, style_wav = sentence_info\n            elif len(sentence_info) == 4:\n                text, speaker_name, style_wav, language_name = sentence_info\n        else:\n            text = sentence_info\n\n        # get speaker  id/d_vector\n        speaker_id, d_vector, language_id = None, None, None\n        if self.speaker_manager is not None:\n            if config.use_d_vector_file:\n                if speaker_name is None:\n                    d_vector = self.speaker_manager.get_random_embedding()\n                else:\n                    d_vector = self.speaker_manager.get_d_vector_by_name(speaker_name)\n            elif config.use_speaker_embedding:\n                if speaker_name is None:\n                    speaker_id = self.speaker_manager.get_random_id()\n                else:\n                    speaker_id = self.speaker_manager.name_to_id[speaker_name]\n\n        # get language id\n        if self.language_manager is not None and config.use_language_embedding and language_name is not None:\n            language_id = self.language_manager.name_to_id[language_name]\n\n        return {\n            \"text\": text,\n            \"speaker_id\": speaker_id,\n            \"style_wav\": style_wav,\n            \"d_vector\": d_vector,\n            \"language_id\": language_id,\n        }\n\n    def format_batch(self, batch: Dict) -> Dict:\n        \"\"\"Generic batch formatting for `VCDataset`.\n\n        You must override this if you use a custom dataset.\n\n        Args:\n            batch (Dict): [description]\n\n        Returns:\n            Dict: [description]\n        \"\"\"\n        # setup input batch\n        text_input = batch[\"token_id\"]\n        text_lengths = batch[\"token_id_lengths\"]\n        speaker_names = batch[\"speaker_names\"]\n        linear_input = batch[\"linear\"]\n        mel_input = batch[\"mel\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        stop_targets = batch[\"stop_targets\"]\n        item_idx = batch[\"item_idxs\"]\n        d_vectors = batch[\"d_vectors\"]\n        speaker_ids = batch[\"speaker_ids\"]\n        attn_mask = batch[\"attns\"]\n        waveform = batch[\"waveform\"]\n        pitch = batch[\"pitch\"]\n        energy = batch[\"energy\"]\n        language_ids = batch[\"language_ids\"]\n        max_text_length = torch.max(text_lengths.float())\n        max_spec_length = torch.max(mel_lengths.float())\n\n        # compute durations from attention masks\n        durations = None\n        if attn_mask is not None:\n            durations = torch.zeros(attn_mask.shape[0], attn_mask.shape[2])\n            for idx, am in enumerate(attn_mask):\n                # compute raw durations\n                c_idxs = am[:, : text_lengths[idx], : mel_lengths[idx]].max(1)[1]\n                # c_idxs, counts = torch.unique_consecutive(c_idxs, return_counts=True)\n                c_idxs, counts = torch.unique(c_idxs, return_counts=True)\n                dur = torch.ones([text_lengths[idx]]).to(counts.dtype)\n                dur[c_idxs] = counts\n                # smooth the durations and set any 0 duration to 1\n                # by cutting off from the largest duration indeces.\n                extra_frames = dur.sum() - mel_lengths[idx]\n                largest_idxs = torch.argsort(-dur)[:extra_frames]\n                dur[largest_idxs] -= 1\n                assert (\n                    dur.sum() == mel_lengths[idx]\n                ), f\" [!] total duration {dur.sum()} vs spectrogram length {mel_lengths[idx]}\"\n                durations[idx, : text_lengths[idx]] = dur\n\n        # set stop targets wrt reduction factor\n        stop_targets = stop_targets.view(text_input.shape[0], stop_targets.size(1) // self.config.r, -1)\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze(2)\n        stop_target_lengths = torch.divide(mel_lengths, self.config.r).ceil_()\n\n        return {\n            \"text_input\": text_input,\n            \"text_lengths\": text_lengths,\n            \"speaker_names\": speaker_names,\n            \"mel_input\": mel_input,\n            \"mel_lengths\": mel_lengths,\n            \"linear_input\": linear_input,\n            \"stop_targets\": stop_targets,\n            \"stop_target_lengths\": stop_target_lengths,\n            \"attn_mask\": attn_mask,\n            \"durations\": durations,\n            \"speaker_ids\": speaker_ids,\n            \"d_vectors\": d_vectors,\n            \"max_text_length\": float(max_text_length),\n            \"max_spec_length\": float(max_spec_length),\n            \"item_idx\": item_idx,\n            \"waveform\": waveform,\n            \"pitch\": pitch,\n            \"energy\": energy,\n            \"language_ids\": language_ids,\n            \"audio_unique_names\": batch[\"audio_unique_names\"],\n        }\n\n    def get_sampler(self, config: Coqpit, dataset: TTSDataset, num_gpus=1):\n        weights = None\n        data_items = dataset.samples\n\n        if getattr(config, \"use_language_weighted_sampler\", False):\n            alpha = getattr(config, \"language_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Language weighted sampler with alpha:\", alpha)\n            weights = get_language_balancer_weights(data_items) * alpha\n\n        if getattr(config, \"use_speaker_weighted_sampler\", False):\n            alpha = getattr(config, \"speaker_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Speaker weighted sampler with alpha:\", alpha)\n            if weights is not None:\n                weights += get_speaker_balancer_weights(data_items) * alpha\n            else:\n                weights = get_speaker_balancer_weights(data_items) * alpha\n\n        if getattr(config, \"use_length_weighted_sampler\", False):\n            alpha = getattr(config, \"length_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Length weighted sampler with alpha:\", alpha)\n            if weights is not None:\n                weights += get_length_balancer_weights(data_items) * alpha\n            else:\n                weights = get_length_balancer_weights(data_items) * alpha\n\n        if weights is not None:\n            sampler = WeightedRandomSampler(weights, len(weights))\n        else:\n            sampler = None\n\n        # sampler for DDP\n        if sampler is None:\n            sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        else:  # If a sampler is already defined use this sampler and DDP sampler together\n            sampler = DistributedSamplerWrapper(sampler) if num_gpus > 1 else sampler\n\n        return sampler\n\n    def get_data_loader(\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: bool,\n        samples: Union[List[Dict], List[List]],\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,\n    ) -> \"DataLoader\":\n        if is_eval and not config.run_eval:\n            loader = None\n        else:\n            # setup multi-speaker attributes\n            if self.speaker_manager is not None:\n                if hasattr(config, \"model_args\"):\n                    speaker_id_mapping = (\n                        self.speaker_manager.name_to_id if config.model_args.use_speaker_embedding else None\n                    )\n                    d_vector_mapping = self.speaker_manager.embeddings if config.model_args.use_d_vector_file else None\n                    config.use_d_vector_file = config.model_args.use_d_vector_file\n                else:\n                    speaker_id_mapping = self.speaker_manager.name_to_id if config.use_speaker_embedding else None\n                    d_vector_mapping = self.speaker_manager.embeddings if config.use_d_vector_file else None\n            else:\n                speaker_id_mapping = None\n                d_vector_mapping = None\n\n            # setup multi-lingual attributes\n            if self.language_manager is not None:\n                language_id_mapping = self.language_manager.name_to_id if self.args.use_language_embedding else None\n            else:\n                language_id_mapping = None\n\n            # init dataloader\n            dataset = TTSDataset(\n                outputs_per_step=config.r if \"r\" in config else 1,\n                compute_linear_spec=config.model.lower() == \"tacotron\" or config.compute_linear_spec,\n                compute_f0=config.get(\"compute_f0\", False),\n                f0_cache_path=config.get(\"f0_cache_path\", None),\n                compute_energy=config.get(\"compute_energy\", False),\n                energy_cache_path=config.get(\"energy_cache_path\", None),\n                samples=samples,\n                ap=self.ap,\n                return_wav=config.return_wav if \"return_wav\" in config else False,\n                batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n                min_text_len=config.min_text_len,\n                max_text_len=config.max_text_len,\n                min_audio_len=config.min_audio_len,\n                max_audio_len=config.max_audio_len,\n                phoneme_cache_path=config.phoneme_cache_path,\n                precompute_num_workers=config.precompute_num_workers,\n                use_noise_augment=False if is_eval else config.use_noise_augment,\n                verbose=verbose,\n                speaker_id_mapping=speaker_id_mapping,\n                d_vector_mapping=d_vector_mapping if config.use_d_vector_file else None,\n                tokenizer=None,\n                start_by_longest=config.start_by_longest,\n                language_id_mapping=language_id_mapping,\n            )\n\n            # wait all the DDP process to be ready\n            if num_gpus > 1:\n                dist.barrier()\n\n            # sort input sequences from short to long\n            dataset.preprocess_samples()\n\n            # get samplers\n            sampler = self.get_sampler(config, dataset, num_gpus)\n\n            loader = DataLoader(\n                dataset,\n                batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                shuffle=config.shuffle if sampler is None else False,  # if there is no other sampler\n                collate_fn=dataset.collate_fn,\n                drop_last=config.drop_last,  # setting this False might cause issues in AMP training.\n                sampler=sampler,\n                num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                pin_memory=False,\n            )\n        return loader\n\n    def _get_test_aux_input(\n        self,\n    ) -> Dict:\n        d_vector = None\n        if self.config.use_d_vector_file:\n            d_vector = [self.speaker_manager.embeddings[name][\"embedding\"] for name in self.speaker_manager.embeddings]\n            d_vector = (random.sample(sorted(d_vector), 1),)\n\n        aux_inputs = {\n            \"speaker_id\": None\n            if not self.config.use_speaker_embedding\n            else random.sample(sorted(self.speaker_manager.name_to_id.values()), 1),\n            \"d_vector\": d_vector,\n            \"style_wav\": None,  # TODO: handle GST style input\n        }\n        return aux_inputs\n\n    def test_run(self, assets: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `vc` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Args:\n            assets (dict): A dict of training assets. For `vc` models, it must include `{'audio_processor': ap}`.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        aux_inputs = self._get_test_aux_input()\n        for idx, sen in enumerate(test_sentences):\n            if isinstance(sen, list):\n                aux_inputs = self.get_aux_input_from_test_sentences(sen)\n                sen = aux_inputs[\"text\"]\n            outputs_dict = synthesis(\n                self,\n                sen,\n                self.config,\n                \"cuda\" in str(next(self.parameters()).device),\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n                style_wav=aux_inputs[\"style_wav\"],\n                use_griffin_lim=True,\n                do_trim_silence=False,\n            )\n            test_audios[\"{}-audio\".format(idx)] = outputs_dict[\"wav\"]\n            test_figures[\"{}-prediction\".format(idx)] = plot_spectrogram(\n                outputs_dict[\"outputs\"][\"model_outputs\"], self.ap, output_fig=False\n            )\n            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(\n                outputs_dict[\"outputs\"][\"alignments\"], output_fig=False\n            )\n        return test_figures, test_audios\n\n    def on_init_start(self, trainer):\n        \"\"\"Save the speaker.pth and language_ids.json at the beginning of the training. Also update both paths.\"\"\"\n        if self.speaker_manager is not None:\n            output_path = os.path.join(trainer.output_path, \"speakers.pth\")\n            self.speaker_manager.save_ids_to_file(output_path)\n            trainer.config.speakers_file = output_path\n            # some models don't have `model_args` set\n            if hasattr(trainer.config, \"model_args\"):\n                trainer.config.model_args.speakers_file = output_path\n            trainer.config.save_json(os.path.join(trainer.output_path, \"config.json\"))\n            print(f\" > `speakers.pth` is saved to {output_path}.\")\n            print(\" > `speakers_file` is updated in the config.json.\")\n\n        if self.language_manager is not None:\n            output_path = os.path.join(trainer.output_path, \"language_ids.json\")\n            self.language_manager.save_ids_to_file(output_path)\n            trainer.config.language_ids_file = output_path\n            if hasattr(trainer.config, \"model_args\"):\n                trainer.config.model_args.language_ids_file = output_path\n            trainer.config.save_json(os.path.join(trainer.output_path, \"config.json\"))\n            print(f\" > `language_ids.json` is saved to {output_path}.\")\n            print(\" > `language_ids_file` is updated in the config.json.\")\n", "TTS/vc/models/__init__.py": "import importlib\nimport re\nfrom typing import Dict, List, Union\n\n\ndef to_camel(text):\n    text = text.capitalize()\n    return re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\n\n\ndef setup_model(config: \"Coqpit\", samples: Union[List[List], List[Dict]] = None) -> \"BaseVC\":\n    print(\" > Using model: {}\".format(config.model))\n    # fetch the right model implementation.\n    if \"model\" in config and config[\"model\"].lower() == \"freevc\":\n        MyModel = importlib.import_module(\"TTS.vc.models.freevc\").FreeVC\n        model = MyModel.init_from_config(config, samples)\n    return model\n", "TTS/vc/configs/shared_configs.py": "from dataclasses import asdict, dataclass, field\nfrom typing import Dict, List\n\nfrom coqpit import Coqpit, check_argument\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig, BaseTrainingConfig\n\n\n@dataclass\nclass BaseVCConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the tts models.\n\n    Args:\n\n        audio (BaseAudioConfig):\n            Audio processor config object instance.\n\n        batch_group_size (int):\n            Size of the batch groups used for bucketing. By default, the dataloader orders samples by the sequence\n            length for a more efficient and stable training. If `batch_group_size > 1` then it performs bucketing to\n            prevent using the same batches for each epoch.\n\n        loss_masking (bool):\n            enable / disable masking loss values against padded segments of samples in a batch.\n\n        min_text_len (int):\n            Minimum length of input text to be used. All shorter samples will be ignored. Defaults to 0.\n\n        max_text_len (int):\n            Maximum length of input text to be used. All longer samples will be ignored. Defaults to float(\"inf\").\n\n        min_audio_len (int):\n            Minimum length of input audio to be used. All shorter samples will be ignored. Defaults to 0.\n\n        max_audio_len (int):\n            Maximum length of input audio to be used. All longer samples will be ignored. The maximum length in the\n            dataset defines the VRAM used in the training. Hence, pay attention to this value if you encounter an\n            OOM error in training. Defaults to float(\"inf\").\n\n        compute_f0 (int):\n            (Not in use yet).\n\n        compute_energy (int):\n            (Not in use yet).\n\n        compute_linear_spec (bool):\n            If True data loader computes and returns linear spectrograms alongside the other data.\n\n        precompute_num_workers (int):\n            Number of workers to precompute features. Defaults to 0.\n\n        use_noise_augment (bool):\n            Augment the input audio with random noise.\n\n        start_by_longest (bool):\n            If True, the data loader will start loading the longest batch first. It is useful for checking OOM issues.\n            Defaults to False.\n\n        shuffle (bool):\n            If True, the data loader will shuffle the dataset when there is not sampler defined. Defaults to True.\n\n        drop_last (bool):\n            If True, the data loader will drop the last batch if it is not complete. It helps to prevent\n            issues that emerge from the partial batch statistics. Defaults to True.\n\n        add_blank (bool):\n            Add blank characters between each other two characters. It improves performance for some models at expense\n            of slower run-time due to the longer input sequence.\n\n        datasets (List[BaseDatasetConfig]):\n            List of datasets used for training. If multiple datasets are provided, they are merged and used together\n            for training.\n\n        optimizer (str):\n            Optimizer used for the training. Set one from `torch.optim.Optimizer` or `TTS.utils.training`.\n            Defaults to ``.\n\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n\n        lr_scheduler (str):\n            Learning rate scheduler for the training. Use one from `torch.optim.Scheduler` schedulers or\n            `TTS.utils.training`. Defaults to ``.\n\n        lr_scheduler_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to `{\"warmup\": 4000}`.\n\n        test_sentences (List[str]):\n            List of sentences to be used at testing. Defaults to '[]'\n\n        eval_split_max_size (int):\n            Number maximum of samples to be used for evaluation in proportion split. Defaults to None (Disabled).\n\n        eval_split_size (float):\n            If between 0.0 and 1.0 represents the proportion of the dataset to include in the evaluation set.\n            If > 1, represents the absolute number of evaluation samples. Defaults to 0.01 (1%).\n\n        use_speaker_weighted_sampler (bool):\n            Enable / Disable the batch balancer by speaker. Defaults to ```False```.\n\n        speaker_weighted_sampler_alpha (float):\n            Number that control the influence of the speaker sampler weights. Defaults to ```1.0```.\n\n        use_language_weighted_sampler (bool):\n            Enable / Disable the batch balancer by language. Defaults to ```False```.\n\n        language_weighted_sampler_alpha (float):\n            Number that control the influence of the language sampler weights. Defaults to ```1.0```.\n\n        use_length_weighted_sampler (bool):\n            Enable / Disable the batch balancer by audio length. If enabled the dataset will be divided\n            into 10 buckets considering the min and max audio of the dataset. The sampler weights will be\n            computed forcing to have the same quantity of data for each bucket in each training batch. Defaults to ```False```.\n\n        length_weighted_sampler_alpha (float):\n            Number that control the influence of the length sampler weights. Defaults to ```1.0```.\n    \"\"\"\n\n    audio: BaseAudioConfig = field(default_factory=BaseAudioConfig)\n    # training params\n    batch_group_size: int = 0\n    loss_masking: bool = None\n    # dataloading\n    min_audio_len: int = 1\n    max_audio_len: int = float(\"inf\")\n    min_text_len: int = 1\n    max_text_len: int = float(\"inf\")\n    compute_f0: bool = False\n    compute_energy: bool = False\n    compute_linear_spec: bool = False\n    precompute_num_workers: int = 0\n    use_noise_augment: bool = False\n    start_by_longest: bool = False\n    shuffle: bool = False\n    drop_last: bool = False\n    # dataset\n    datasets: List[BaseDatasetConfig] = field(default_factory=lambda: [BaseDatasetConfig()])\n    # optimizer\n    optimizer: str = \"radam\"\n    optimizer_params: dict = None\n    # scheduler\n    lr_scheduler: str = None\n    lr_scheduler_params: dict = field(default_factory=lambda: {})\n    # testing\n    test_sentences: List[str] = field(default_factory=lambda: [])\n    # evaluation\n    eval_split_max_size: int = None\n    eval_split_size: float = 0.01\n    # weighted samplers\n    use_speaker_weighted_sampler: bool = False\n    speaker_weighted_sampler_alpha: float = 1.0\n    use_language_weighted_sampler: bool = False\n    language_weighted_sampler_alpha: float = 1.0\n    use_length_weighted_sampler: bool = False\n    length_weighted_sampler_alpha: float = 1.0\n", "TTS/vc/configs/__init__.py": "", "TTS/vc/configs/freevc_config.py": "from dataclasses import dataclass, field\nfrom typing import List, Optional\n\nfrom coqpit import Coqpit\n\nfrom TTS.vc.configs.shared_configs import BaseVCConfig\n\n\n@dataclass\nclass FreeVCAudioConfig(Coqpit):\n    \"\"\"Audio configuration\n\n    Args:\n        max_wav_value (float):\n            The maximum value of the waveform.\n\n        input_sample_rate (int):\n            The sampling rate of the input waveform.\n\n        output_sample_rate (int):\n            The sampling rate of the output waveform.\n\n        filter_length (int):\n            The length of the filter.\n\n        hop_length (int):\n            The hop length.\n\n        win_length (int):\n            The window length.\n\n        n_mel_channels (int):\n            The number of mel channels.\n\n        mel_fmin (float):\n            The minimum frequency of the mel filterbank.\n\n        mel_fmax (Optional[float]):\n            The maximum frequency of the mel filterbank.\n    \"\"\"\n\n    max_wav_value: float = field(default=32768.0)\n    input_sample_rate: int = field(default=16000)\n    output_sample_rate: int = field(default=24000)\n    filter_length: int = field(default=1280)\n    hop_length: int = field(default=320)\n    win_length: int = field(default=1280)\n    n_mel_channels: int = field(default=80)\n    mel_fmin: float = field(default=0.0)\n    mel_fmax: Optional[float] = field(default=None)\n\n\n@dataclass\nclass FreeVCArgs(Coqpit):\n    \"\"\"FreeVC model arguments\n\n    Args:\n        spec_channels (int):\n            The number of channels in the spectrogram.\n\n        inter_channels (int):\n            The number of channels in the intermediate layers.\n\n        hidden_channels (int):\n            The number of channels in the hidden layers.\n\n        filter_channels (int):\n            The number of channels in the filter layers.\n\n        n_heads (int):\n            The number of attention heads.\n\n        n_layers (int):\n            The number of layers.\n\n        kernel_size (int):\n            The size of the kernel.\n\n        p_dropout (float):\n            The dropout probability.\n\n        resblock (str):\n            The type of residual block.\n\n        resblock_kernel_sizes (List[int]):\n            The kernel sizes for the residual blocks.\n\n        resblock_dilation_sizes (List[List[int]]):\n            The dilation sizes for the residual blocks.\n\n        upsample_rates (List[int]):\n            The upsample rates.\n\n        upsample_initial_channel (int):\n            The number of channels in the initial upsample layer.\n\n        upsample_kernel_sizes (List[int]):\n            The kernel sizes for the upsample layers.\n\n        n_layers_q (int):\n            The number of layers in the quantization network.\n\n        use_spectral_norm (bool):\n            Whether to use spectral normalization.\n\n        gin_channels (int):\n            The number of channels in the global conditioning vector.\n\n        ssl_dim (int):\n            The dimension of the self-supervised learning embedding.\n\n        use_spk (bool):\n            Whether to use external speaker encoder.\n    \"\"\"\n\n    spec_channels: int = field(default=641)\n    inter_channels: int = field(default=192)\n    hidden_channels: int = field(default=192)\n    filter_channels: int = field(default=768)\n    n_heads: int = field(default=2)\n    n_layers: int = field(default=6)\n    kernel_size: int = field(default=3)\n    p_dropout: float = field(default=0.1)\n    resblock: str = field(default=\"1\")\n    resblock_kernel_sizes: List[int] = field(default_factory=lambda: [3, 7, 11])\n    resblock_dilation_sizes: List[List[int]] = field(default_factory=lambda: [[1, 3, 5], [1, 3, 5], [1, 3, 5]])\n    upsample_rates: List[int] = field(default_factory=lambda: [10, 8, 2, 2])\n    upsample_initial_channel: int = field(default=512)\n    upsample_kernel_sizes: List[int] = field(default_factory=lambda: [16, 16, 4, 4])\n    n_layers_q: int = field(default=3)\n    use_spectral_norm: bool = field(default=False)\n    gin_channels: int = field(default=256)\n    ssl_dim: int = field(default=1024)\n    use_spk: bool = field(default=False)\n    num_spks: int = field(default=0)\n    segment_size: int = field(default=8960)\n\n\n@dataclass\nclass FreeVCConfig(BaseVCConfig):\n    \"\"\"Defines parameters for FreeVC End2End TTS model.\n\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n\n        model_args (FreeVCArgs):\n            Model architecture arguments. Defaults to `FreeVCArgs()`.\n\n        audio (FreeVCAudioConfig):\n            Audio processing configuration. Defaults to `FreeVCAudioConfig()`.\n\n        grad_clip (List):\n            Gradient clipping thresholds for each optimizer. Defaults to `[1000.0, 1000.0]`.\n\n        lr_gen (float):\n            Initial learning rate for the generator. Defaults to 0.0002.\n\n        lr_disc (float):\n            Initial learning rate for the discriminator. Defaults to 0.0002.\n\n        lr_scheduler_gen (str):\n            Name of the learning rate scheduler for the generator. One of the `torch.optim.lr_scheduler.*`. Defaults to\n            `ExponentialLR`.\n\n        lr_scheduler_gen_params (dict):\n            Parameters for the learning rate scheduler of the generator. Defaults to `{'gamma': 0.999875, \"last_epoch\":-1}`.\n\n        lr_scheduler_disc (str):\n            Name of the learning rate scheduler for the discriminator. One of the `torch.optim.lr_scheduler.*`. Defaults to\n            `ExponentialLR`.\n\n        lr_scheduler_disc_params (dict):\n            Parameters for the learning rate scheduler of the discriminator. Defaults to `{'gamma': 0.999875, \"last_epoch\":-1}`.\n\n        scheduler_after_epoch (bool):\n            If true, step the schedulers after each epoch else after each step. Defaults to `False`.\n\n        optimizer (str):\n            Name of the optimizer to use with both the generator and the discriminator networks. One of the\n            `torch.optim.*`. Defaults to `AdamW`.\n\n        kl_loss_alpha (float):\n            Loss weight for KL loss. Defaults to 1.0.\n\n        disc_loss_alpha (float):\n            Loss weight for the discriminator loss. Defaults to 1.0.\n\n        gen_loss_alpha (float):\n            Loss weight for the generator loss. Defaults to 1.0.\n\n        feat_loss_alpha (float):\n            Loss weight for the feature matching loss. Defaults to 1.0.\n\n        mel_loss_alpha (float):\n            Loss weight for the mel loss. Defaults to 45.0.\n\n        return_wav (bool):\n            If true, data loader returns the waveform as well as the other outputs. Do not change. Defaults to `True`.\n\n        compute_linear_spec (bool):\n            If true, the linear spectrogram is computed and returned alongside the mel output. Do not change. Defaults to `True`.\n\n        use_weighted_sampler (bool):\n            If true, use weighted sampler with bucketing for balancing samples between datasets used in training. Defaults to `False`.\n\n        weighted_sampler_attrs (dict):\n            Key retuned by the formatter to be used for weighted sampler. For example `{\"root_path\": 2.0, \"speaker_name\": 1.0}` sets sample probabilities\n            by overweighting `root_path` by 2.0. Defaults to `{}`.\n\n        weighted_sampler_multipliers (dict):\n            Weight each unique value of a key returned by the formatter for weighted sampling.\n            For example `{\"root_path\":{\"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-100/\":1.0, \"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-360/\": 0.5}`.\n            It will sample instances from `train-clean-100` 2 times more than `train-clean-360`. Defaults to `{}`.\n\n        r (int):\n            Number of spectrogram frames to be generated at a time. Do not change. Defaults to `1`.\n\n        add_blank (bool):\n            If true, a blank token is added in between every character. Defaults to `True`.\n\n        test_sentences (List[List]):\n            List of sentences with speaker and language information to be used for testing.\n\n        language_ids_file (str):\n            Path to the language ids file.\n\n        use_language_embedding (bool):\n            If true, language embedding is used. Defaults to `False`.\n\n    Note:\n        Check :class:`TTS.tts.configs.shared_configs.BaseTTSConfig` for the inherited parameters.\n\n    Example:\n\n        >>> from TTS.vc.configs.freevc_config import FreeVCConfig\n        >>> config = FreeVCConfig()\n    \"\"\"\n\n    model: str = \"freevc\"\n    # model specific params\n    model_args: FreeVCArgs = field(default_factory=FreeVCArgs)\n    audio: FreeVCAudioConfig = field(default_factory=FreeVCAudioConfig)\n\n    # optimizer\n    # TODO with training support\n\n    # loss params\n    # TODO with training support\n\n    # data loader params\n    return_wav: bool = True\n    compute_linear_spec: bool = True\n\n    # sampler params\n    use_weighted_sampler: bool = False  # TODO: move it to the base config\n    weighted_sampler_attrs: dict = field(default_factory=lambda: {})\n    weighted_sampler_multipliers: dict = field(default_factory=lambda: {})\n\n    # overrides\n    r: int = 1  # DO NOT CHANGE\n    add_blank: bool = True\n\n    # multi-speaker settings\n    # use speaker embedding layer\n    num_speakers: int = 0\n    speakers_file: str = None\n    speaker_embedding_channels: int = 256\n\n    # use d-vectors\n    use_d_vector_file: bool = False\n    d_vector_file: List[str] = None\n    d_vector_dim: int = None\n\n    def __post_init__(self):\n        for key, val in self.model_args.items():\n            if hasattr(self, key):\n                self[key] = val\n", "TTS/vc/modules/__init__.py": "", "TTS/vc/modules/freevc/commons.py": "import math\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\n\n\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\n\n\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\n\n\ndef intersperse(lst, item):\n    result = [item] * (len(lst) * 2 + 1)\n    result[1::2] = lst\n    return result\n\n\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += 0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n    return kl\n\n\ndef rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))\n\n\ndef rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\n\n\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret\n\n\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size + 1\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str\n\n\ndef rand_spec_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str\n\n\ndef get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)\n    num_timescales = channels // 2\n    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (num_timescales - 1)\n    inv_timescales = min_timescale * torch.exp(\n        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n    )\n    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n    signal = F.pad(signal, [0, 0, 0, channels % 2])\n    signal = signal.view(1, channels, length)\n    return signal\n\n\ndef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return x + signal.to(dtype=x.dtype, device=x.device)\n\n\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n\n\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\n\n\ndef shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\n\n\ndef sequence_mask(length, max_length=None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)\n\n\ndef generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"\n    device = duration.device\n\n    b, _, t_y, t_x = mask.shape\n    cum_duration = torch.cumsum(duration, -1)\n\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n    path = path.view(b, t_x, t_y)\n    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n    path = path.unsqueeze(1).transpose(2, 3) * mask\n    return path\n\n\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    if clip_value is not None:\n        clip_value = float(clip_value)\n\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item() ** norm_type\n        if clip_value is not None:\n            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n    total_norm = total_norm ** (1.0 / norm_type)\n    return total_norm\n", "TTS/vc/modules/freevc/modules.py": "import torch\nfrom torch import nn\nfrom torch.nn import Conv1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nimport TTS.vc.modules.freevc.commons as commons\nfrom TTS.vc.modules.freevc.commons import get_padding, init_weights\n\nLRELU_SLOPE = 0.1\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n        return x.transpose(1, -1)\n\n\nclass ConvReluNorm(nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n        assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n        self.conv_layers = nn.ModuleList()\n        self.norm_layers = nn.ModuleList()\n        self.conv_layers.append(nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2))\n        self.norm_layers.append(LayerNorm(hidden_channels))\n        self.relu_drop = nn.Sequential(nn.ReLU(), nn.Dropout(p_dropout))\n        for _ in range(n_layers - 1):\n            self.conv_layers.append(nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size // 2))\n            self.norm_layers.append(LayerNorm(hidden_channels))\n        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask):\n        x_org = x\n        for i in range(self.n_layers):\n            x = self.conv_layers[i](x * x_mask)\n            x = self.norm_layers[i](x)\n            x = self.relu_drop(x)\n        x = x_org + self.proj(x)\n        return x * x_mask\n\n\nclass DDSConv(nn.Module):\n    \"\"\"\n    Dialted and Depth-Separable Convolution\n    \"\"\"\n\n    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n        super().__init__()\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n\n        self.drop = nn.Dropout(p_dropout)\n        self.convs_sep = nn.ModuleList()\n        self.convs_1x1 = nn.ModuleList()\n        self.norms_1 = nn.ModuleList()\n        self.norms_2 = nn.ModuleList()\n        for i in range(n_layers):\n            dilation = kernel_size**i\n            padding = (kernel_size * dilation - dilation) // 2\n            self.convs_sep.append(\n                nn.Conv1d(channels, channels, kernel_size, groups=channels, dilation=dilation, padding=padding)\n            )\n            self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n            self.norms_1.append(LayerNorm(channels))\n            self.norms_2.append(LayerNorm(channels))\n\n    def forward(self, x, x_mask, g=None):\n        if g is not None:\n            x = x + g\n        for i in range(self.n_layers):\n            y = self.convs_sep[i](x * x_mask)\n            y = self.norms_1[i](y)\n            y = F.gelu(y)\n            y = self.convs_1x1[i](y)\n            y = self.norms_2[i](y)\n            y = F.gelu(y)\n            y = self.drop(y)\n            x = x + y\n        return x * x_mask\n\n\nclass WN(torch.nn.Module):\n    def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n        super(WN, self).__init__()\n        assert kernel_size % 2 == 1\n        self.hidden_channels = hidden_channels\n        self.kernel_size = (kernel_size,)\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.gin_channels = gin_channels\n        self.p_dropout = p_dropout\n\n        self.in_layers = torch.nn.ModuleList()\n        self.res_skip_layers = torch.nn.ModuleList()\n        self.drop = nn.Dropout(p_dropout)\n\n        if gin_channels != 0:\n            cond_layer = torch.nn.Conv1d(gin_channels, 2 * hidden_channels * n_layers, 1)\n            self.cond_layer = torch.nn.utils.parametrizations.weight_norm(cond_layer, name=\"weight\")\n\n        for i in range(n_layers):\n            dilation = dilation_rate**i\n            padding = int((kernel_size * dilation - dilation) / 2)\n            in_layer = torch.nn.Conv1d(\n                hidden_channels, 2 * hidden_channels, kernel_size, dilation=dilation, padding=padding\n            )\n            in_layer = torch.nn.utils.parametrizations.weight_norm(in_layer, name=\"weight\")\n            self.in_layers.append(in_layer)\n\n            # last one is not necessary\n            if i < n_layers - 1:\n                res_skip_channels = 2 * hidden_channels\n            else:\n                res_skip_channels = hidden_channels\n\n            res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n            res_skip_layer = torch.nn.utils.parametrizations.weight_norm(res_skip_layer, name=\"weight\")\n            self.res_skip_layers.append(res_skip_layer)\n\n    def forward(self, x, x_mask, g=None, **kwargs):\n        output = torch.zeros_like(x)\n        n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n        if g is not None:\n            g = self.cond_layer(g)\n\n        for i in range(self.n_layers):\n            x_in = self.in_layers[i](x)\n            if g is not None:\n                cond_offset = i * 2 * self.hidden_channels\n                g_l = g[:, cond_offset : cond_offset + 2 * self.hidden_channels, :]\n            else:\n                g_l = torch.zeros_like(x_in)\n\n            acts = commons.fused_add_tanh_sigmoid_multiply(x_in, g_l, n_channels_tensor)\n            acts = self.drop(acts)\n\n            res_skip_acts = self.res_skip_layers[i](acts)\n            if i < self.n_layers - 1:\n                res_acts = res_skip_acts[:, : self.hidden_channels, :]\n                x = (x + res_acts) * x_mask\n                output = output + res_skip_acts[:, self.hidden_channels :, :]\n            else:\n                output = output + res_skip_acts\n        return output * x_mask\n\n    def remove_weight_norm(self):\n        if self.gin_channels != 0:\n            remove_parametrizations(self.cond_layer, \"weight\")\n        for l in self.in_layers:\n            remove_parametrizations(l, \"weight\")\n        for l in self.res_skip_layers:\n            remove_parametrizations(l, \"weight\")\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n                weight_norm(\n                    Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))\n                ),\n            ]\n        )\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_parametrizations(l, \"weight\")\n        for l in self.convs2:\n            remove_parametrizations(l, \"weight\")\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_parametrizations(l, \"weight\")\n\n\nclass Log(nn.Module):\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n            logdet = torch.sum(-y, [1, 2])\n            return y, logdet\n        else:\n            x = torch.exp(x) * x_mask\n            return x\n\n\nclass Flip(nn.Module):\n    def forward(self, x, *args, reverse=False, **kwargs):\n        x = torch.flip(x, [1])\n        if not reverse:\n            logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n            return x, logdet\n        else:\n            return x\n\n\nclass ElementwiseAffine(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.m = nn.Parameter(torch.zeros(channels, 1))\n        self.logs = nn.Parameter(torch.zeros(channels, 1))\n\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = self.m + torch.exp(self.logs) * x\n            y = y * x_mask\n            logdet = torch.sum(self.logs * x_mask, [1, 2])\n            return y, logdet\n        else:\n            x = (x - self.m) * torch.exp(-self.logs) * x_mask\n            return x\n\n\nclass ResidualCouplingLayer(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        p_dropout=0,\n        gin_channels=0,\n        mean_only=False,\n    ):\n        assert channels % 2 == 0, \"channels should be divisible by 2\"\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.half_channels = channels // 2\n        self.mean_only = mean_only\n\n        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n        self.enc = WN(\n            hidden_channels, kernel_size, dilation_rate, n_layers, p_dropout=p_dropout, gin_channels=gin_channels\n        )\n        self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n        self.post.weight.data.zero_()\n        self.post.bias.data.zero_()\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n        h = self.pre(x0) * x_mask\n        h = self.enc(h, x_mask, g=g)\n        stats = self.post(h) * x_mask\n        if not self.mean_only:\n            m, logs = torch.split(stats, [self.half_channels] * 2, 1)\n        else:\n            m = stats\n            logs = torch.zeros_like(m)\n\n        if not reverse:\n            x1 = m + x1 * torch.exp(logs) * x_mask\n            x = torch.cat([x0, x1], 1)\n            logdet = torch.sum(logs, [1, 2])\n            return x, logdet\n        else:\n            x1 = (x1 - m) * torch.exp(-logs) * x_mask\n            x = torch.cat([x0, x1], 1)\n            return x\n", "TTS/vc/modules/freevc/__init__.py": "", "TTS/vc/modules/freevc/mel_processing.py": "import torch\nimport torch.utils.data\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\n\n\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\n\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1), (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)), mode=\"reflect\"\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_size,\n        win_length=win_size,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec\n\n\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1), (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)), mode=\"reflect\"\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_size,\n        win_length=win_size,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec\n", "TTS/vc/modules/freevc/wavlm/modules.py": "# --------------------------------------------------------\n# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\nimport math\nimport warnings\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\n\n\nclass TransposeLast(nn.Module):\n    def __init__(self, deconstruct_idx=None):\n        super().__init__()\n        self.deconstruct_idx = deconstruct_idx\n\n    def forward(self, x):\n        if self.deconstruct_idx is not None:\n            x = x[self.deconstruct_idx]\n        return x.transpose(-2, -1)\n\n\nclass Fp32LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n\n\nclass Fp32GroupNorm(nn.GroupNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.group_norm(\n            input.float(),\n            self.num_groups,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n\n\nclass GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None\n\n\nclass SamePad(nn.Module):\n    def __init__(self, kernel_size, causal=False):\n        super().__init__()\n        if causal:\n            self.remove = kernel_size - 1\n        else:\n            self.remove = 1 if kernel_size % 2 == 0 else 0\n\n    def forward(self, x):\n        if self.remove > 0:\n            x = x[:, :, : -self.remove]\n        return x\n\n\nclass Swish(nn.Module):\n    \"\"\"Swish function\"\"\"\n\n    def __init__(self):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(Swish, self).__init__()\n        self.act = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        return x * self.act(x)\n\n\nclass GLU_Linear(nn.Module):\n    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n        super(GLU_Linear, self).__init__()\n\n        self.glu_type = glu_type\n        self.output_dim = output_dim\n\n        if glu_type == \"sigmoid\":\n            self.glu_act = torch.nn.Sigmoid()\n        elif glu_type == \"swish\":\n            self.glu_act = Swish()\n        elif glu_type == \"relu\":\n            self.glu_act = torch.nn.ReLU()\n        elif glu_type == \"gelu\":\n            self.glu_act = torch.nn.GELU()\n\n        if bias_in_glu:\n            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n        else:\n            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n\n    def forward(self, x):\n        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n        x = self.linear(x)\n\n        if self.glu_type == \"bilinear\":\n            x = x[:, :, 0 : self.output_dim] * x[:, :, self.output_dim : self.output_dim * 2]\n        else:\n            x = x[:, :, 0 : self.output_dim] * self.glu_act(x[:, :, self.output_dim : self.output_dim * 2])\n\n        return x\n\n\ndef gelu_accurate(x):\n    if not hasattr(gelu_accurate, \"_a\"):\n        gelu_accurate._a = math.sqrt(2 / math.pi)\n    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)\n\n\ndef get_activation_fn(activation: str):\n    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return gelu\n    elif activation == \"gelu_fast\":\n        warnings.warn(\"--activation-fn=gelu_fast has been renamed to gelu_accurate\")\n        return gelu_accurate\n    elif activation == \"gelu_accurate\":\n        return gelu_accurate\n    elif activation == \"tanh\":\n        return torch.tanh\n    elif activation == \"linear\":\n        return lambda x: x\n    elif activation == \"glu\":\n        return lambda x: x\n    else:\n        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n\n\ndef init_bert_params(module):\n    \"\"\"\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of\n           in_project_weight for MultiHeadAttention initialized using\n           the normal distribution (to be validated).\n    \"\"\"\n\n    def normal_(data):\n        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n        # so that the RNG is consistent with and without FSDP\n        data.copy_(data.cpu().normal_(mean=0.0, std=0.02).to(data.device))\n\n    if isinstance(module, nn.Linear):\n        normal_(module.weight.data)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        normal_(module.weight.data)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    if isinstance(module, MultiheadAttention):\n        normal_(module.q_proj.weight.data)\n        normal_(module.k_proj.weight.data)\n        normal_(module.v_proj.weight.data)\n\n\ndef quant_noise(module, p, block_size):\n    \"\"\"\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in \"Training with Quantization Noise for Extreme Model Compression\"\n\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n\n    Remarks:\n        - Module weights must have the right sizes wrt the block size\n        - Only Linear, Embedding and Conv2d modules are supported for the moment\n        - For more detail on how to quantize by blocks with convolutional weights,\n          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n        - We implement the simplest form of noise here as stated in the paper\n          which consists in randomly dropping blocks\n    \"\"\"\n\n    # if no quantization noise, don't register hook\n    if p <= 0:\n        return module\n\n    # supported modules\n    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n\n    # test whether module.weight has the right sizes wrt block_size\n    is_conv = module.weight.ndim == 4\n\n    # 2D matrix\n    if not is_conv:\n        assert module.weight.size(1) % block_size == 0, \"Input features must be a multiple of block sizes\"\n\n    # 4D matrix\n    else:\n        # 1x1 convolutions\n        if module.kernel_size == (1, 1):\n            assert module.in_channels % block_size == 0, \"Input channels must be a multiple of block sizes\"\n        # regular convolutions\n        else:\n            k = module.kernel_size[0] * module.kernel_size[1]\n            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n\n    def _forward_pre_hook(mod, input):\n        # no noise for evaluation\n        if mod.training:\n            if not is_conv:\n                # gather weight and sizes\n                weight = mod.weight\n                in_features = weight.size(1)\n                out_features = weight.size(0)\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                mask = torch.zeros(in_features // block_size * out_features, device=weight.device)\n                mask.bernoulli_(p)\n                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n\n            else:\n                # gather weight and sizes\n                weight = mod.weight\n                in_channels = mod.in_channels\n                out_channels = mod.out_channels\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                if mod.kernel_size == (1, 1):\n                    mask = torch.zeros(\n                        int(in_channels // block_size * out_channels),\n                        device=weight.device,\n                    )\n                    mask.bernoulli_(p)\n                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n                else:\n                    mask = torch.zeros(weight.size(0), weight.size(1), device=weight.device)\n                    mask.bernoulli_(p)\n                    mask = mask.unsqueeze(2).unsqueeze(3).repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n\n            # scale weights and apply mask\n            mask = mask.to(torch.bool)  # x.bool() is not currently supported in TorchScript\n            s = 1 / (1 - p)\n            mod.weight.data = s * weight.masked_fill(mask, 0)\n\n    module.register_forward_pre_hook(_forward_pre_hook)\n    return module\n\n\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n        has_relative_attention_bias=False,\n        num_buckets=32,\n        max_distance=128,\n        gru_rel_pos=False,\n        rescale_init=False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout_module = nn.Dropout(dropout)\n\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n\n        self.head_dim = embed_dim // num_heads\n        self.q_head_dim = self.head_dim\n        self.k_head_dim = self.head_dim\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim**-0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        k_bias = True\n        if rescale_init:\n            k_bias = False\n\n        k_embed_dim = embed_dim\n        q_embed_dim = embed_dim\n\n        self.k_proj = quant_noise(nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size)\n        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)\n        self.q_proj = quant_noise(nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size)\n\n        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.gru_rel_pos = gru_rel_pos\n        if self.gru_rel_pos:\n            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n        if self.has_relative_attention_bias:\n            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n\n    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n        num_buckets = self.num_buckets\n        max_distance = self.max_distance\n        relative_buckets = 0\n\n        if bidirectional:\n            num_buckets = num_buckets // 2\n            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n            relative_positions = torch.abs(relative_positions)\n        else:\n            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n\n        max_exact = num_buckets // 2\n        is_small = relative_positions < max_exact\n\n        relative_postion_if_large = max_exact + (\n            torch.log(relative_positions.float() / max_exact)\n            / math.log(max_distance / max_exact)\n            * (num_buckets - max_exact)\n        ).to(torch.long)\n        relative_postion_if_large = torch.min(\n            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n        )\n\n        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length):\n        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_positions_bucket(relative_position, bidirectional=True)\n        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.permute([2, 0, 1])\n        return values\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        before_softmax: bool = False,\n        need_head_weights: bool = False,\n        position_bias: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        is_tpu = query.device.type == \"xla\"\n\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        if key is not None:\n            src_len, key_bsz, _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert src_len, bsz == value.shape[:2]\n\n        if self.has_relative_attention_bias and position_bias is None:\n            position_bias = self.compute_bias(tgt_len, src_len)\n            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n\n        if (\n            not is_tpu  # don't use PyTorch version on TPUs\n            and incremental_state is None\n            and not static_kv\n            # A workaround for quantization to work. Otherwise JIT compilation\n            # treats bias in linear module as method.\n            and not torch.jit.is_scripting()\n            and self.q_head_dim == self.head_dim\n        ):\n            assert key is not None and value is not None\n            assert attn_mask is None\n\n            attn_mask_rel_pos = None\n            if position_bias is not None:\n                attn_mask_rel_pos = position_bias\n                if self.gru_rel_pos:\n                    query_layer = query.transpose(0, 1)\n                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n                    query_layer = query_layer.view(*new_x_shape)\n                    query_layer = query_layer.permute(0, 2, 1, 3)\n                    _B, _H, _L, __ = query_layer.size()\n\n                    gate_a, gate_b = torch.sigmoid(\n                        self.grep_linear(query_layer).view(_B, _H, _L, 2, 4).sum(-1, keepdim=False)\n                    ).chunk(2, dim=-1)\n                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n\n                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n            k_proj_bias = self.k_proj.bias\n            if k_proj_bias is None:\n                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n\n            x, attn = F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                torch.empty([0]),\n                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout_module.p,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                self.training,\n                # self.training or self.dropout_module.apply_during_inference,\n                key_padding_mask,\n                need_weights,\n                attn_mask_rel_pos,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj.weight,\n                k_proj_weight=self.k_proj.weight,\n                v_proj_weight=self.v_proj.weight,\n            )\n            return x, attn, position_bias\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.q_head_dim).transpose(0, 1)\n        if k is not None:\n            k = k.contiguous().view(-1, bsz * self.num_heads, self.k_head_dim).transpose(0, 1)\n        if v is not None:\n            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n                src_len = k.size(1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        assert k.size(1) == src_len\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask),\n                    ],\n                    dim=1,\n                )\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            if not is_tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if before_softmax:\n            return attn_weights, v, position_bias\n\n        if position_bias is not None:\n            if self.gru_rel_pos == 1:\n                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n                _B, _H, _L, __ = query_layer.size()\n                gate_a, gate_b = torch.sigmoid(\n                    self.grep_linear(query_layer).view(_B, _H, _L, 2, 4).sum(-1, keepdim=False)\n                ).chunk(2, dim=-1)\n                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n\n            position_bias = position_bias.view(attn_weights.size())\n\n            attn_weights = attn_weights + position_bias\n\n        attn_weights_float = F.softmax(attn_weights, dim=-1)\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights, position_bias\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            if src_len > prev_key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n                    device=prev_key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)\n            else:\n                new_key_padding_mask = prev_key_padding_mask.float()\n        elif key_padding_mask is not None:\n            if src_len > key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - key_padding_mask.size(1)),\n                    device=key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)\n            else:\n                new_key_padding_mask = key_padding_mask.float()\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n", "TTS/vc/modules/freevc/wavlm/__init__.py": "import os\nimport urllib.request\n\nimport torch\n\nfrom TTS.utils.generic_utils import get_user_data_dir\nfrom TTS.vc.modules.freevc.wavlm.wavlm import WavLM, WavLMConfig\n\nmodel_uri = \"https://github.com/coqui-ai/TTS/releases/download/v0.13.0_models/WavLM-Large.pt\"\n\n\ndef get_wavlm(device=\"cpu\"):\n    \"\"\"Download the model and return the model object.\"\"\"\n\n    output_path = get_user_data_dir(\"tts\")\n\n    output_path = os.path.join(output_path, \"wavlm\")\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    output_path = os.path.join(output_path, \"WavLM-Large.pt\")\n    if not os.path.exists(output_path):\n        print(f\" > Downloading WavLM model to {output_path} ...\")\n        urllib.request.urlretrieve(model_uri, output_path)\n\n    checkpoint = torch.load(output_path, map_location=torch.device(device))\n    cfg = WavLMConfig(checkpoint[\"cfg\"])\n    wavlm = WavLM(cfg).to(device)\n    wavlm.load_state_dict(checkpoint[\"model\"])\n    wavlm.eval()\n    return wavlm\n\n\nif __name__ == \"__main__\":\n    wavlm = get_wavlm()\n", "TTS/vc/modules/freevc/wavlm/wavlm.py": "# --------------------------------------------------------\n# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\nimport logging\nimport math\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nfrom TTS.vc.modules.freevc.wavlm.modules import (\n    Fp32GroupNorm,\n    Fp32LayerNorm,\n    GLU_Linear,\n    GradMultiply,\n    MultiheadAttention,\n    SamePad,\n    TransposeLast,\n    get_activation_fn,\n    init_bert_params,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef compute_mask_indices(\n    shape: Tuple[int, int],\n    padding_mask: Optional[torch.Tensor],\n    mask_prob: float,\n    mask_length: int,\n    mask_type: str = \"static\",\n    mask_other: float = 0.0,\n    min_masks: int = 0,\n    no_overlap: bool = False,\n    min_space: int = 0,\n) -> np.ndarray:\n    \"\"\"\n    Computes random mask spans for a given shape\n\n    Args:\n        shape: the the shape for which to compute masks.\n            should be of size 2 where first element is batch size and 2nd is timesteps\n        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements\n        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n            however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n        mask_type: how to compute mask lengths\n            static = fixed size\n            uniform = sample from uniform distribution [mask_other, mask_length*2]\n            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element\n            poisson = sample from possion distribution with lambda = mask length\n        min_masks: minimum number of masked spans\n        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping\n        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans\n    \"\"\"\n\n    bsz, all_sz = shape\n    mask = np.full((bsz, all_sz), False)\n\n    all_num_mask = int(\n        # add a random number for probabilistic rounding\n        mask_prob * all_sz / float(mask_length)\n        + np.random.rand()\n    )\n\n    all_num_mask = max(min_masks, all_num_mask)\n\n    mask_idcs = []\n    for i in range(bsz):\n        if padding_mask is not None:\n            sz = all_sz - padding_mask[i].long().sum().item()\n            num_mask = int(\n                # add a random number for probabilistic rounding\n                mask_prob * sz / float(mask_length)\n                + np.random.rand()\n            )\n            num_mask = max(min_masks, num_mask)\n        else:\n            sz = all_sz\n            num_mask = all_num_mask\n\n        if mask_type == \"static\":\n            lengths = np.full(num_mask, mask_length)\n        elif mask_type == \"uniform\":\n            lengths = np.random.randint(mask_other, mask_length * 2 + 1, size=num_mask)\n        elif mask_type == \"normal\":\n            lengths = np.random.normal(mask_length, mask_other, size=num_mask)\n            lengths = [max(1, int(round(x))) for x in lengths]\n        elif mask_type == \"poisson\":\n            lengths = np.random.poisson(mask_length, size=num_mask)\n            lengths = [int(round(x)) for x in lengths]\n        else:\n            raise Exception(\"unknown mask selection \" + mask_type)\n\n        if sum(lengths) == 0:\n            lengths[0] = min(mask_length, sz - 1)\n\n        if no_overlap:\n            mask_idc = []\n\n            def arrange(s, e, length, keep_length):\n                span_start = np.random.randint(s, e - length)\n                mask_idc.extend(span_start + i for i in range(length))\n\n                new_parts = []\n                if span_start - s - min_space >= keep_length:\n                    new_parts.append((s, span_start - min_space + 1))\n                if e - span_start - keep_length - min_space > keep_length:\n                    new_parts.append((span_start + length + min_space, e))\n                return new_parts\n\n            parts = [(0, sz)]\n            min_length = min(lengths)\n            for length in sorted(lengths, reverse=True):\n                lens = np.fromiter(\n                    (e - s if e - s >= length + min_space else 0 for s, e in parts),\n                    np.int,\n                )\n                l_sum = np.sum(lens)\n                if l_sum == 0:\n                    break\n                probs = lens / np.sum(lens)\n                c = np.random.choice(len(parts), p=probs)\n                s, e = parts.pop(c)\n                parts.extend(arrange(s, e, length, min_length))\n            mask_idc = np.asarray(mask_idc)\n        else:\n            min_len = min(lengths)\n            if sz - min_len <= num_mask:\n                min_len = sz - num_mask - 1\n\n            mask_idc = np.random.choice(sz - min_len, num_mask, replace=False)\n\n            mask_idc = np.asarray([mask_idc[j] + offset for j in range(len(mask_idc)) for offset in range(lengths[j])])\n\n        mask_idcs.append(np.unique(mask_idc[mask_idc < sz]))\n\n    min_len = min([len(m) for m in mask_idcs])\n    for i, mask_idc in enumerate(mask_idcs):\n        if len(mask_idc) > min_len:\n            mask_idc = np.random.choice(mask_idc, min_len, replace=False)\n        mask[i, mask_idc] = True\n\n    return mask\n\n\nclass WavLMConfig:\n    def __init__(self, cfg=None):\n        self.extractor_mode: str = \"default\"  # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n        self.encoder_layers: int = 12  # num encoder layers in the transformer\n\n        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n        self.encoder_attention_heads: int = 12  # num encoder attention heads\n        self.activation_fn: str = \"gelu\"  # activation function to use\n\n        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n        self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"  # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]\n        self.conv_bias: bool = False  # include bias in conv encoder\n        self.feature_grad_mult: float = 1.0  # multiply feature extractor var grads by this\n\n        self.normalize: bool = False  # normalize input to have 0 mean and unit variance during training\n\n        # dropouts\n        self.dropout: float = 0.1  # dropout probability for the transformer\n        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n        self.dropout_features: float = 0.0  # dropout to apply to the features (after feat extr)\n\n        # masking\n        self.mask_length: int = 10  # mask length\n        self.mask_prob: float = 0.65  # probability of replacing a token with mask\n        self.mask_selection: str = \"static\"  # how to choose mask length\n        self.mask_other: float = (\n            0  # secondary mask argument (used for more complex distributions), see help in compute_mask_indicesh\n        )\n        self.no_mask_overlap: bool = False  # whether to allow masks to overlap\n        self.mask_min_space: int = 1  # min space between spans (if no overlap is enabled)\n\n        # channel masking\n        self.mask_channel_length: int = 10  # length of the mask for features (channels)\n        self.mask_channel_prob: float = 0.0  # probability of replacing a feature with 0\n        self.mask_channel_selection: str = \"static\"  # how to choose mask length for channel masking\n        self.mask_channel_other: float = (\n            0  # secondary mask argument (used for more complex distributions), see help in compute_mask_indices\n        )\n        self.no_mask_channel_overlap: bool = False  # whether to allow channel masks to overlap\n        self.mask_channel_min_space: int = 1  # min space between spans (if no overlap is enabled)\n\n        # positional embeddings\n        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n\n        # relative position embedding\n        self.relative_position_embedding: bool = False  # apply relative position embedding\n        self.num_buckets: int = 320  # number of buckets for relative position embedding\n        self.max_distance: int = 1280  # maximum distance for relative position embedding\n        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n\n        if cfg is not None:\n            self.update(cfg)\n\n    def update(self, cfg: dict):\n        self.__dict__.update(cfg)\n\n\nclass WavLM(nn.Module):\n    def __init__(\n        self,\n        cfg: WavLMConfig,\n    ) -> None:\n        super().__init__()\n        logger.info(f\"WavLM Config: {cfg.__dict__}\")\n\n        self.cfg = cfg\n        feature_enc_layers = eval(cfg.conv_feature_layers)\n        self.embed = feature_enc_layers[-1][0]\n\n        self.feature_extractor = ConvFeatureExtractionModel(\n            conv_layers=feature_enc_layers,\n            dropout=0.0,\n            mode=cfg.extractor_mode,\n            conv_bias=cfg.conv_bias,\n        )\n\n        self.post_extract_proj = (\n            nn.Linear(self.embed, cfg.encoder_embed_dim) if self.embed != cfg.encoder_embed_dim else None\n        )\n\n        self.mask_prob = cfg.mask_prob\n        self.mask_selection = cfg.mask_selection\n        self.mask_other = cfg.mask_other\n        self.mask_length = cfg.mask_length\n        self.no_mask_overlap = cfg.no_mask_overlap\n        self.mask_min_space = cfg.mask_min_space\n\n        self.mask_channel_prob = cfg.mask_channel_prob\n        self.mask_channel_selection = cfg.mask_channel_selection\n        self.mask_channel_other = cfg.mask_channel_other\n        self.mask_channel_length = cfg.mask_channel_length\n        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n        self.mask_channel_min_space = cfg.mask_channel_min_space\n\n        self.dropout_input = nn.Dropout(cfg.dropout_input)\n        self.dropout_features = nn.Dropout(cfg.dropout_features)\n\n        self.feature_grad_mult = cfg.feature_grad_mult\n\n        self.mask_emb = nn.Parameter(torch.FloatTensor(cfg.encoder_embed_dim).uniform_())\n\n        self.encoder = TransformerEncoder(cfg)\n        self.layer_norm = LayerNorm(self.embed)\n\n    def apply_mask(self, x, padding_mask):\n        B, T, C = x.shape\n        if self.mask_prob > 0:\n            mask_indices = compute_mask_indices(\n                (B, T),\n                padding_mask,\n                self.mask_prob,\n                self.mask_length,\n                self.mask_selection,\n                self.mask_other,\n                min_masks=2,\n                no_overlap=self.no_mask_overlap,\n                min_space=self.mask_min_space,\n            )\n            mask_indices = torch.from_numpy(mask_indices).to(x.device)\n            x[mask_indices] = self.mask_emb\n        else:\n            mask_indices = None\n\n        if self.mask_channel_prob > 0:\n            mask_channel_indices = compute_mask_indices(\n                (B, C),\n                None,\n                self.mask_channel_prob,\n                self.mask_channel_length,\n                self.mask_channel_selection,\n                self.mask_channel_other,\n                no_overlap=self.no_mask_channel_overlap,\n                min_space=self.mask_channel_min_space,\n            )\n            mask_channel_indices = torch.from_numpy(mask_channel_indices).to(x.device).unsqueeze(1).expand(-1, T, -1)\n            x[mask_channel_indices] = 0\n\n        return x, mask_indices\n\n    def forward_padding_mask(\n        self,\n        features: torch.Tensor,\n        padding_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        extra = padding_mask.size(1) % features.size(1)\n        if extra > 0:\n            padding_mask = padding_mask[:, :-extra]\n        padding_mask = padding_mask.view(padding_mask.size(0), features.size(1), -1)\n        # padding_mask = padding_mask.all(-1)\n        padding_mask = padding_mask.any(-1)\n        return padding_mask\n\n    def extract_features(\n        self,\n        source: torch.Tensor,\n        padding_mask: Optional[torch.Tensor] = None,\n        mask: bool = False,\n        ret_conv: bool = False,\n        output_layer: Optional[int] = None,\n        ret_layer_results: bool = False,\n    ):\n        if self.feature_grad_mult > 0:\n            features = self.feature_extractor(source)\n            if self.feature_grad_mult != 1.0:\n                features = GradMultiply.apply(features, self.feature_grad_mult)\n        else:\n            with torch.no_grad():\n                features = self.feature_extractor(source)\n\n        features = features.transpose(1, 2)\n        features = self.layer_norm(features)\n\n        if padding_mask is not None:\n            padding_mask = self.forward_padding_mask(features, padding_mask)\n\n        if self.post_extract_proj is not None:\n            features = self.post_extract_proj(features)\n\n        features = self.dropout_input(features)\n\n        if mask:\n            x, mask_indices = self.apply_mask(features, padding_mask)\n        else:\n            x = features\n\n        # feature: (B, T, D), float\n        # target: (B, T), long\n        # x: (B, T, D), float\n        # padding_mask: (B, T), bool\n        # mask_indices: (B, T), bool\n        x, layer_results = self.encoder(\n            x, padding_mask=padding_mask, layer=None if output_layer is None else output_layer - 1\n        )\n\n        res = {\"x\": x, \"padding_mask\": padding_mask, \"features\": features, \"layer_results\": layer_results}\n\n        feature = res[\"features\"] if ret_conv else res[\"x\"]\n        if ret_layer_results:\n            feature = (feature, res[\"layer_results\"])\n        return feature, res[\"padding_mask\"]\n\n\nclass ConvFeatureExtractionModel(nn.Module):\n    def __init__(\n        self,\n        conv_layers: List[Tuple[int, int, int]],\n        dropout: float = 0.0,\n        mode: str = \"default\",\n        conv_bias: bool = False,\n        conv_type: str = \"default\",\n    ):\n        super().__init__()\n\n        assert mode in {\"default\", \"layer_norm\"}\n\n        def block(\n            n_in,\n            n_out,\n            k,\n            stride,\n            is_layer_norm=False,\n            is_group_norm=False,\n            conv_bias=False,\n        ):\n            def make_conv():\n                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)\n                nn.init.kaiming_normal_(conv.weight)\n                return conv\n\n            assert (is_layer_norm and is_group_norm) == False, \"layer norm and group norm are exclusive\"\n\n            if is_layer_norm:\n                return nn.Sequential(\n                    make_conv(),\n                    nn.Dropout(p=dropout),\n                    nn.Sequential(\n                        TransposeLast(),\n                        Fp32LayerNorm(dim, elementwise_affine=True),\n                        TransposeLast(),\n                    ),\n                    nn.GELU(),\n                )\n            elif is_group_norm:\n                return nn.Sequential(\n                    make_conv(),\n                    nn.Dropout(p=dropout),\n                    Fp32GroupNorm(dim, dim, affine=True),\n                    nn.GELU(),\n                )\n            else:\n                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n\n        self.conv_type = conv_type\n        if self.conv_type == \"default\":\n            in_d = 1\n            self.conv_layers = nn.ModuleList()\n            for i, cl in enumerate(conv_layers):\n                assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n                (dim, k, stride) = cl\n\n                self.conv_layers.append(\n                    block(\n                        in_d,\n                        dim,\n                        k,\n                        stride,\n                        is_layer_norm=mode == \"layer_norm\",\n                        is_group_norm=mode == \"default\" and i == 0,\n                        conv_bias=conv_bias,\n                    )\n                )\n                in_d = dim\n        elif self.conv_type == \"conv2d\":\n            in_d = 1\n            self.conv_layers = nn.ModuleList()\n            for i, cl in enumerate(conv_layers):\n                assert len(cl) == 3\n                (dim, k, stride) = cl\n\n                self.conv_layers.append(torch.nn.Conv2d(in_d, dim, k, stride))\n                self.conv_layers.append(torch.nn.ReLU())\n                in_d = dim\n        elif self.conv_type == \"custom\":\n            in_d = 1\n            idim = 80\n            self.conv_layers = nn.ModuleList()\n            for i, cl in enumerate(conv_layers):\n                assert len(cl) == 3\n                (dim, k, stride) = cl\n                self.conv_layers.append(torch.nn.Conv2d(in_d, dim, k, stride, padding=1))\n                self.conv_layers.append(torch.nn.LayerNorm([dim, idim]))\n                self.conv_layers.append(torch.nn.ReLU())\n                in_d = dim\n                if (i + 1) % 2 == 0:\n                    self.conv_layers.append(torch.nn.MaxPool2d(2, stride=2, ceil_mode=True))\n                    idim = int(math.ceil(idim / 2))\n        else:\n            pass\n\n    def forward(self, x, mask=None):\n        # BxT -> BxCxT\n        x = x.unsqueeze(1)\n        if self.conv_type == \"custom\":\n            for conv in self.conv_layers:\n                if isinstance(conv, nn.LayerNorm):\n                    x = x.transpose(1, 2)\n                    x = conv(x).transpose(1, 2)\n                else:\n                    x = conv(x)\n            x = x.transpose(2, 3).contiguous()\n            x = x.view(x.size(0), -1, x.size(-1))\n        else:\n            for conv in self.conv_layers:\n                x = conv(x)\n            if self.conv_type == \"conv2d\":\n                b, c, t, f = x.size()\n                x = x.transpose(2, 3).contiguous().view(b, c * f, t)\n        return x\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        self.dropout = args.dropout\n        self.embedding_dim = args.encoder_embed_dim\n\n        self.pos_conv = nn.Conv1d(\n            self.embedding_dim,\n            self.embedding_dim,\n            kernel_size=args.conv_pos,\n            padding=args.conv_pos // 2,\n            groups=args.conv_pos_groups,\n        )\n        dropout = 0\n        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n        nn.init.constant_(self.pos_conv.bias, 0)\n\n        self.pos_conv = nn.utils.parametrizations.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n\n        if hasattr(args, \"relative_position_embedding\"):\n            self.relative_position_embedding = args.relative_position_embedding\n            self.num_buckets = args.num_buckets\n            self.max_distance = args.max_distance\n        else:\n            self.relative_position_embedding = False\n            self.num_buckets = 0\n            self.max_distance = 0\n\n        self.layers = nn.ModuleList(\n            [\n                TransformerSentenceEncoderLayer(\n                    embedding_dim=self.embedding_dim,\n                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n                    num_attention_heads=args.encoder_attention_heads,\n                    dropout=self.dropout,\n                    attention_dropout=args.attention_dropout,\n                    activation_dropout=args.activation_dropout,\n                    activation_fn=args.activation_fn,\n                    layer_norm_first=args.layer_norm_first,\n                    has_relative_attention_bias=(self.relative_position_embedding and i == 0),\n                    num_buckets=self.num_buckets,\n                    max_distance=self.max_distance,\n                    gru_rel_pos=args.gru_rel_pos,\n                )\n                for i in range(args.encoder_layers)\n            ]\n        )\n\n        self.layer_norm_first = args.layer_norm_first\n        self.layer_norm = LayerNorm(self.embedding_dim)\n        self.layerdrop = args.encoder_layerdrop\n\n        self.apply(init_bert_params)\n\n    def forward(self, x, padding_mask=None, streaming_mask=None, layer=None):\n        x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)\n\n        if self.layer_norm_first and layer is None:\n            x = self.layer_norm(x)\n\n        return x, layer_results\n\n    def extract_features(self, x, padding_mask=None, streaming_mask=None, tgt_layer=None):\n        if padding_mask is not None:\n            x[padding_mask] = 0\n\n        x_conv = self.pos_conv(x.transpose(1, 2))\n        x_conv = x_conv.transpose(1, 2)\n        x += x_conv\n\n        if not self.layer_norm_first:\n            x = self.layer_norm(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        layer_results = []\n        z = None\n        if tgt_layer is not None:\n            layer_results.append((x, z))\n        r = None\n        pos_bias = None\n        for i, layer in enumerate(self.layers):\n            dropout_probability = np.random.random()\n            if not self.training or (dropout_probability > self.layerdrop):\n                x, z, pos_bias = layer(\n                    x,\n                    self_attn_padding_mask=padding_mask,\n                    need_weights=False,\n                    self_attn_mask=streaming_mask,\n                    pos_bias=pos_bias,\n                )\n            if tgt_layer is not None:\n                layer_results.append((x, z))\n            if i == tgt_layer:\n                r = x\n                break\n\n        if r is not None:\n            x = r\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        return x, layer_results\n\n\nclass TransformerSentenceEncoderLayer(nn.Module):\n    \"\"\"\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: float = 768,\n        ffn_embedding_dim: float = 3072,\n        num_attention_heads: float = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        activation_fn: str = \"relu\",\n        layer_norm_first: bool = False,\n        has_relative_attention_bias: bool = False,\n        num_buckets: int = 0,\n        max_distance: int = 0,\n        rescale_init: bool = False,\n        gru_rel_pos: bool = False,\n    ) -> None:\n        super().__init__()\n        # Initialize parameters\n        self.embedding_dim = embedding_dim\n        self.dropout = dropout\n        self.activation_dropout = activation_dropout\n\n        # Initialize blocks\n        self.activation_name = activation_fn\n        self.activation_fn = get_activation_fn(activation_fn)\n        self.self_attn = MultiheadAttention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            self_attention=True,\n            has_relative_attention_bias=has_relative_attention_bias,\n            num_buckets=num_buckets,\n            max_distance=max_distance,\n            rescale_init=rescale_init,\n            gru_rel_pos=gru_rel_pos,\n        )\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(self.activation_dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.layer_norm_first = layer_norm_first\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n\n        if self.activation_name == \"glu\":\n            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n        else:\n            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = LayerNorm(self.embedding_dim)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        self_attn_mask: torch.Tensor = None,\n        self_attn_padding_mask: torch.Tensor = None,\n        need_weights: bool = False,\n        pos_bias=None,\n    ):\n        \"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer imlementation.\n        \"\"\"\n        residual = x\n\n        if self.layer_norm_first:\n            x = self.self_attn_layer_norm(x)\n            x, attn, pos_bias = self.self_attn(\n                query=x,\n                key=x,\n                value=x,\n                key_padding_mask=self_attn_padding_mask,\n                need_weights=False,\n                attn_mask=self_attn_mask,\n                position_bias=pos_bias,\n            )\n            x = self.dropout1(x)\n            x = residual + x\n\n            residual = x\n            x = self.final_layer_norm(x)\n            if self.activation_name == \"glu\":\n                x = self.fc1(x)\n            else:\n                x = self.activation_fn(self.fc1(x))\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            x = self.dropout3(x)\n            x = residual + x\n        else:\n            x, attn, pos_bias = self.self_attn(\n                query=x,\n                key=x,\n                value=x,\n                key_padding_mask=self_attn_padding_mask,\n                need_weights=need_weights,\n                attn_mask=self_attn_mask,\n                position_bias=pos_bias,\n            )\n\n            x = self.dropout1(x)\n            x = residual + x\n\n            x = self.self_attn_layer_norm(x)\n\n            residual = x\n            if self.activation_name == \"glu\":\n                x = self.fc1(x)\n            else:\n                x = self.activation_fn(self.fc1(x))\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            x = self.dropout3(x)\n            x = residual + x\n            x = self.final_layer_norm(x)\n\n        return x, attn, pos_bias\n", "TTS/vc/modules/freevc/speaker_encoder/speaker_encoder.py": "from pathlib import Path\nfrom time import perf_counter as timer\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vc.modules.freevc.speaker_encoder import audio\nfrom TTS.vc.modules.freevc.speaker_encoder.hparams import *\n\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(self, weights_fpath, device: Union[str, torch.device] = None, verbose=True):\n        \"\"\"\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\n        \"\"\"\n        super().__init__()\n\n        # Define the network\n        self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n        self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n        self.relu = nn.ReLU()\n\n        # Get the target device\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        elif isinstance(device, str):\n            device = torch.device(device)\n        self.device = device\n\n        # Load the pretrained model'speaker weights\n        # weights_fpath = Path(__file__).resolve().parent.joinpath(\"pretrained.pt\")\n        # if not weights_fpath.exists():\n        #     raise Exception(\"Couldn't find the voice encoder pretrained model at %s.\" %\n        #                     weights_fpath)\n\n        start = timer()\n        checkpoint = load_fsspec(weights_fpath, map_location=\"cpu\")\n\n        self.load_state_dict(checkpoint[\"model_state\"], strict=False)\n        self.to(device)\n\n        if verbose:\n            print(\"Loaded the voice encoder model on %s in %.2f seconds.\" % (device.type, timer() - start))\n\n    def forward(self, mels: torch.FloatTensor):\n        \"\"\"\n        Computes the embeddings of a batch of utterance spectrograms.\n        :param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\n        (batch_size, n_frames, n_channels)\n        :return: the embeddings as a float 32 tensor of shape (batch_size, embedding_size).\n        Embeddings are positive and L2-normed, thus they lay in the range [0, 1].\n        \"\"\"\n        # Pass the input through the LSTM layers and retrieve the final hidden state of the last\n        # layer. Apply a cutoff to 0 for negative values and L2 normalize the embeddings.\n        _, (hidden, _) = self.lstm(mels)\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n\n    @staticmethod\n    def compute_partial_slices(n_samples: int, rate, min_coverage):\n        \"\"\"\n        Computes where to split an utterance waveform and its corresponding mel spectrogram to\n        obtain partial utterances of <partials_n_frames> each. Both the waveform and the\n        mel spectrogram slices are returned, so as to make each partial utterance waveform\n        correspond to its spectrogram.\n\n        The returned ranges may be indexing further than the length of the waveform. It is\n        recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\n\n        :param n_samples: the number of samples in the waveform\n        :param rate: how many partial utterances should occur per second. Partial utterances must\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n        the minimum rate is thus 0.625.\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n        it will be discarded. If there aren't enough frames for one partial utterance,\n        this parameter is ignored so that the function always returns at least one slice.\n        :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n        respectively the waveform and the mel spectrogram with these slices to obtain the partial\n        utterances.\n        \"\"\"\n        assert 0 < min_coverage <= 1\n\n        # Compute how many frames separate two partial utterances\n        samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n        n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n        frame_step = int(np.round((sampling_rate / rate) / samples_per_frame))\n        assert 0 < frame_step, \"The rate is too high\"\n        assert frame_step <= partials_n_frames, \"The rate is too low, it should be %f at least\" % (\n            sampling_rate / (samples_per_frame * partials_n_frames)\n        )\n\n        # Compute the slices\n        wav_slices, mel_slices = [], []\n        steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n        for i in range(0, steps, frame_step):\n            mel_range = np.array([i, i + partials_n_frames])\n            wav_range = mel_range * samples_per_frame\n            mel_slices.append(slice(*mel_range))\n            wav_slices.append(slice(*wav_range))\n\n        # Evaluate whether extra padding is warranted or not\n        last_wav_range = wav_slices[-1]\n        coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n        if coverage < min_coverage and len(mel_slices) > 1:\n            mel_slices = mel_slices[:-1]\n            wav_slices = wav_slices[:-1]\n\n        return wav_slices, mel_slices\n\n    def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75):\n        \"\"\"\n        Computes an embedding for a single utterance. The utterance is divided in partial\n        utterances and an embedding is computed for each. The complete utterance embedding is the\n        L2-normed average embedding of the partial utterances.\n\n        TODO: independent batched version of this function\n\n        :param wav: a preprocessed utterance waveform as a numpy array of float32\n        :param return_partials: if True, the partial embeddings will also be returned along with\n        the wav slices corresponding to each partial utterance.\n        :param rate: how many partial utterances should occur per second. Partial utterances must\n        cover the span of the entire utterance, thus the rate should not be lower than the inverse\n        of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n        the minimum rate is thus 0.625.\n        :param min_coverage: when reaching the last partial utterance, it may or may not have\n        enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n        then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n        it will be discarded. If there aren't enough frames for one partial utterance,\n        this parameter is ignored so that the function always returns at least one slice.\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\n        <return_partials> is True, the partial utterances as a numpy array of float32 of shape\n        (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\n        returned.\n        \"\"\"\n        # Compute where to split the utterance into partials and pad the waveform with zeros if\n        # the partial utterances cover a larger range.\n        wav_slices, mel_slices = self.compute_partial_slices(len(wav), rate, min_coverage)\n        max_wave_length = wav_slices[-1].stop\n        if max_wave_length >= len(wav):\n            wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n\n        # Split the utterance into partials and forward them through the model\n        mel = audio.wav_to_mel_spectrogram(wav)\n        mels = np.array([mel[s] for s in mel_slices])\n        with torch.no_grad():\n            mels = torch.from_numpy(mels).to(self.device)\n            partial_embeds = self(mels).cpu().numpy()\n\n        # Compute the utterance embedding from the partial embeddings\n        raw_embed = np.mean(partial_embeds, axis=0)\n        embed = raw_embed / np.linalg.norm(raw_embed, 2)\n\n        if return_partials:\n            return embed, partial_embeds, wav_slices\n        return embed\n\n    def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n        \"\"\"\n        Compute the embedding of a collection of wavs (presumably from the same speaker) by\n        averaging their embedding and L2-normalizing it.\n\n        :param wavs: list of wavs a numpy arrays of float32.\n        :param kwargs: extra arguments to embed_utterance()\n        :return: the embedding as a numpy array of float32 of shape (model_embedding_size,).\n        \"\"\"\n        raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n        return raw_embed / np.linalg.norm(raw_embed, 2)\n", "TTS/vc/modules/freevc/speaker_encoder/audio.py": "import struct\nfrom pathlib import Path\nfrom typing import Optional, Union\n\n# import webrtcvad\nimport librosa\nimport numpy as np\nfrom scipy.ndimage.morphology import binary_dilation\n\nfrom TTS.vc.modules.freevc.speaker_encoder.hparams import *\n\nint16_max = (2**15) - 1\n\n\ndef preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray], source_sr: Optional[int] = None):\n    \"\"\"\n    Applies the preprocessing operations used in training the Speaker Encoder to a waveform\n    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not\n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before\n    preprocessing. After preprocessing, the waveform's sampling rate will match the data\n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and\n    this argument will be ignored.\n    \"\"\"\n    # Load the wav from disk if needed\n    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n        wav, source_sr = librosa.load(fpath_or_wav, sr=None)\n    else:\n        wav = fpath_or_wav\n\n    # Resample the wav if needed\n    if source_sr is not None and source_sr != sampling_rate:\n        wav = librosa.resample(wav, source_sr, sampling_rate)\n\n    # Apply the preprocessing: normalize volume and shorten long silences\n    wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)\n    wav = trim_long_silences(wav)\n\n    return wav\n\n\ndef wav_to_mel_spectrogram(wav):\n    \"\"\"\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    \"\"\"\n    frames = librosa.feature.melspectrogram(\n        y=wav,\n        sr=sampling_rate,\n        n_fft=int(sampling_rate * mel_window_length / 1000),\n        hop_length=int(sampling_rate * mel_window_step / 1000),\n        n_mels=mel_n_channels,\n    )\n    return frames.astype(np.float32).T\n\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav**2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n", "TTS/vc/modules/freevc/speaker_encoder/hparams.py": "## Mel-filterbank\nmel_window_length = 25  # In milliseconds\nmel_window_step = 10  # In milliseconds\nmel_n_channels = 40\n\n\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160  # 1600 ms\n\n\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out.\nvad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6\n\n\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n\n\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3\n", "TTS/vc/modules/freevc/speaker_encoder/__init__.py": "", "TTS/demos/xtts_ft_demo/xtts_demo.py": "import argparse\nimport os\nimport sys\nimport tempfile\n\nimport gradio as gr\nimport librosa.display\nimport numpy as np\n\nimport os\nimport torch\nimport torchaudio\nimport traceback\nfrom TTS.demos.xtts_ft_demo.utils.formatter import format_audio_list\nfrom TTS.demos.xtts_ft_demo.utils.gpt_train import train_gpt\n\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\n\ndef clear_gpu_cache():\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nXTTS_MODEL = None\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    global XTTS_MODEL\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!\"\n    config = XttsConfig()\n    config.load_json(xtts_config)\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS model! \")\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab, use_deepspeed=False)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"Model Loaded!\")\n    return \"Model Loaded!\"\n\ndef run_tts(lang, tts_text, speaker_audio_file):\n    if XTTS_MODEL is None or not speaker_audio_file:\n        return \"You need to run the previous step to load the model !!\", None, None\n\n    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(audio_path=speaker_audio_file, gpt_cond_len=XTTS_MODEL.config.gpt_cond_len, max_ref_length=XTTS_MODEL.config.max_ref_len, sound_norm_refs=XTTS_MODEL.config.sound_norm_refs)\n    out = XTTS_MODEL.inference(\n        text=tts_text,\n        language=lang,\n        gpt_cond_latent=gpt_cond_latent,\n        speaker_embedding=speaker_embedding,\n        temperature=XTTS_MODEL.config.temperature, # Add custom parameters here\n        length_penalty=XTTS_MODEL.config.length_penalty,\n        repetition_penalty=XTTS_MODEL.config.repetition_penalty,\n        top_k=XTTS_MODEL.config.top_k,\n        top_p=XTTS_MODEL.config.top_p,\n    )\n\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        out[\"wav\"] = torch.tensor(out[\"wav\"]).unsqueeze(0)\n        out_path = fp.name\n        torchaudio.save(out_path, out[\"wav\"], 24000)\n\n    return \"Speech generated !\", out_path, speaker_audio_file\n\n\n\n\n# define a logger to redirect \nclass Logger:\n    def __init__(self, filename=\"log.out\"):\n        self.log_file = filename\n        self.terminal = sys.stdout\n        self.log = open(self.log_file, \"w\")\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        self.terminal.flush()\n        self.log.flush()\n\n    def isatty(self):\n        return False\n\n# redirect stdout and stderr to a file\nsys.stdout = Logger()\nsys.stderr = sys.stdout\n\n\n# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\ndef read_logs():\n    sys.stdout.flush()\n    with open(sys.stdout.log_file, \"r\") as f:\n        return f.read()\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(\n        description=\"\"\"XTTS fine-tuning demo\\n\\n\"\"\"\n        \"\"\"\n        Example runs:\n        python3 TTS/demos/xtts_ft_demo/xtts_demo.py --port \n        \"\"\",\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n    parser.add_argument(\n        \"--port\",\n        type=int,\n        help=\"Port to run the gradio demo. Default: 5003\",\n        default=5003,\n    )\n    parser.add_argument(\n        \"--out_path\",\n        type=str,\n        help=\"Output path (where data and checkpoints will be saved) Default: /tmp/xtts_ft/\",\n        default=\"/tmp/xtts_ft/\",\n    )\n\n    parser.add_argument(\n        \"--num_epochs\",\n        type=int,\n        help=\"Number of epochs to train. Default: 10\",\n        default=10,\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        help=\"Batch size. Default: 4\",\n        default=4,\n    )\n    parser.add_argument(\n        \"--grad_acumm\",\n        type=int,\n        help=\"Grad accumulation steps. Default: 1\",\n        default=1,\n    )\n    parser.add_argument(\n        \"--max_audio_length\",\n        type=int,\n        help=\"Max permitted audio size in seconds. Default: 11\",\n        default=11,\n    )\n\n    args = parser.parse_args()\n\n    with gr.Blocks() as demo:\n        with gr.Tab(\"1 - Data processing\"):\n            out_path = gr.Textbox(\n                label=\"Output path (where data and checkpoints will be saved):\",\n                value=args.out_path,\n            )\n            # upload_file = gr.Audio(\n            #     sources=\"upload\",\n            #     label=\"Select here the audio files that you want to use for XTTS trainining !\",\n            #     type=\"filepath\",\n            # )\n            upload_file = gr.File(\n                file_count=\"multiple\",\n                label=\"Select here the audio files that you want to use for XTTS trainining (Supported formats: wav, mp3, and flac)\",\n            )\n            lang = gr.Dropdown(\n                label=\"Dataset Language\",\n                value=\"en\",\n                choices=[\n                    \"en\",\n                    \"es\",\n                    \"fr\",\n                    \"de\",\n                    \"it\",\n                    \"pt\",\n                    \"pl\",\n                    \"tr\",\n                    \"ru\",\n                    \"nl\",\n                    \"cs\",\n                    \"ar\",\n                    \"zh\",\n                    \"hu\",\n                    \"ko\",\n                    \"ja\"\n                ],\n            )\n            progress_data = gr.Label(\n                label=\"Progress:\"\n            )\n            logs = gr.Textbox(\n                label=\"Logs:\",\n                interactive=False,\n            )\n            demo.load(read_logs, None, logs, every=1)\n\n            prompt_compute_btn = gr.Button(value=\"Step 1 - Create dataset\")\n        \n            def preprocess_dataset(audio_path, language, out_path, progress=gr.Progress(track_tqdm=True)):\n                clear_gpu_cache()\n                out_path = os.path.join(out_path, \"dataset\")\n                os.makedirs(out_path, exist_ok=True)\n                if audio_path is None:\n                    return \"You should provide one or multiple audio files! If you provided it, probably the upload of the files is not finished yet!\", \"\", \"\"\n                else:\n                    try:\n                        train_meta, eval_meta, audio_total_size = format_audio_list(audio_path, target_language=language, out_path=out_path, gradio_progress=progress)\n                    except:\n                        traceback.print_exc()\n                        error = traceback.format_exc()\n                        return f\"The data processing was interrupted due an error !! Please check the console to verify the full error message! \\n Error summary: {error}\", \"\", \"\"\n\n                clear_gpu_cache()\n\n                # if audio total len is less than 2 minutes raise an error\n                if audio_total_size < 120:\n                    message = \"The sum of the duration of the audios that you provided should be at least 2 minutes!\"\n                    print(message)\n                    return message, \"\", \"\"\n\n                print(\"Dataset Processed!\")\n                return \"Dataset Processed!\", train_meta, eval_meta\n\n        with gr.Tab(\"2 - Fine-tuning XTTS Encoder\"):\n            train_csv = gr.Textbox(\n                label=\"Train CSV:\",\n            )\n            eval_csv = gr.Textbox(\n                label=\"Eval CSV:\",\n            )\n            num_epochs =  gr.Slider(\n                label=\"Number of epochs:\",\n                minimum=1,\n                maximum=100,\n                step=1,\n                value=args.num_epochs,\n            )\n            batch_size = gr.Slider(\n                label=\"Batch size:\",\n                minimum=2,\n                maximum=512,\n                step=1,\n                value=args.batch_size,\n            )\n            grad_acumm = gr.Slider(\n                label=\"Grad accumulation steps:\",\n                minimum=2,\n                maximum=128,\n                step=1,\n                value=args.grad_acumm,\n            )\n            max_audio_length = gr.Slider(\n                label=\"Max permitted audio size in seconds:\",\n                minimum=2,\n                maximum=20,\n                step=1,\n                value=args.max_audio_length,\n            )\n            progress_train = gr.Label(\n                label=\"Progress:\"\n            )\n            logs_tts_train = gr.Textbox(\n                label=\"Logs:\",\n                interactive=False,\n            )\n            demo.load(read_logs, None, logs_tts_train, every=1)\n            train_btn = gr.Button(value=\"Step 2 - Run the training\")\n\n            def train_model(language, train_csv, eval_csv, num_epochs, batch_size, grad_acumm, output_path, max_audio_length):\n                clear_gpu_cache()\n                if not train_csv or not eval_csv:\n                    return \"You need to run the data processing step or manually set `Train CSV` and `Eval CSV` fields !\", \"\", \"\", \"\", \"\"\n                try:\n                    # convert seconds to waveform frames\n                    max_audio_length = int(max_audio_length * 22050)\n                    config_path, original_xtts_checkpoint, vocab_file, exp_path, speaker_wav = train_gpt(language, num_epochs, batch_size, grad_acumm, train_csv, eval_csv, output_path=output_path, max_audio_length=max_audio_length)\n                except:\n                    traceback.print_exc()\n                    error = traceback.format_exc()\n                    return f\"The training was interrupted due an error !! Please check the console to check the full error message! \\n Error summary: {error}\", \"\", \"\", \"\", \"\"\n\n                # copy original files to avoid parameters changes issues\n                os.system(f\"cp {config_path} {exp_path}\")\n                os.system(f\"cp {vocab_file} {exp_path}\")\n\n                ft_xtts_checkpoint = os.path.join(exp_path, \"best_model.pth\")\n                print(\"Model training done!\")\n                clear_gpu_cache()\n                return \"Model training done!\", config_path, vocab_file, ft_xtts_checkpoint, speaker_wav\n\n        with gr.Tab(\"3 - Inference\"):\n            with gr.Row():\n                with gr.Column() as col1:\n                    xtts_checkpoint = gr.Textbox(\n                        label=\"XTTS checkpoint path:\",\n                        value=\"\",\n                    )\n                    xtts_config = gr.Textbox(\n                        label=\"XTTS config path:\",\n                        value=\"\",\n                    )\n\n                    xtts_vocab = gr.Textbox(\n                        label=\"XTTS vocab path:\",\n                        value=\"\",\n                    )\n                    progress_load = gr.Label(\n                        label=\"Progress:\"\n                    )\n                    load_btn = gr.Button(value=\"Step 3 - Load Fine-tuned XTTS model\")\n\n                with gr.Column() as col2:\n                    speaker_reference_audio = gr.Textbox(\n                        label=\"Speaker reference audio:\",\n                        value=\"\",\n                    )\n                    tts_language = gr.Dropdown(\n                        label=\"Language\",\n                        value=\"en\",\n                        choices=[\n                            \"en\",\n                            \"es\",\n                            \"fr\",\n                            \"de\",\n                            \"it\",\n                            \"pt\",\n                            \"pl\",\n                            \"tr\",\n                            \"ru\",\n                            \"nl\",\n                            \"cs\",\n                            \"ar\",\n                            \"zh\",\n                            \"hu\",\n                            \"ko\",\n                            \"ja\",\n                        ]\n                    )\n                    tts_text = gr.Textbox(\n                        label=\"Input Text.\",\n                        value=\"This model sounds really good and above all, it's reasonably fast.\",\n                    )\n                    tts_btn = gr.Button(value=\"Step 4 - Inference\")\n\n                with gr.Column() as col3:\n                    progress_gen = gr.Label(\n                        label=\"Progress:\"\n                    )\n                    tts_output_audio = gr.Audio(label=\"Generated Audio.\")\n                    reference_audio = gr.Audio(label=\"Reference audio used.\")\n\n            prompt_compute_btn.click(\n                fn=preprocess_dataset,\n                inputs=[\n                    upload_file,\n                    lang,\n                    out_path,\n                ],\n                outputs=[\n                    progress_data,\n                    train_csv,\n                    eval_csv,\n                ],\n            )\n\n\n            train_btn.click(\n                fn=train_model,\n                inputs=[\n                    lang,\n                    train_csv,\n                    eval_csv,\n                    num_epochs,\n                    batch_size,\n                    grad_acumm,\n                    out_path,\n                    max_audio_length,\n                ],\n                outputs=[progress_train, xtts_config, xtts_vocab, xtts_checkpoint, speaker_reference_audio],\n            )\n            \n            load_btn.click(\n                fn=load_model,\n                inputs=[\n                    xtts_checkpoint,\n                    xtts_config,\n                    xtts_vocab\n                ],\n                outputs=[progress_load],\n            )\n\n            tts_btn.click(\n                fn=run_tts,\n                inputs=[\n                    tts_language,\n                    tts_text,\n                    speaker_reference_audio,\n                ],\n                outputs=[progress_gen, tts_output_audio, reference_audio],\n            )\n\n    demo.launch(\n        share=True,\n        debug=False,\n        server_port=args.port,\n        server_name=\"0.0.0.0\"\n    )\n", "TTS/demos/xtts_ft_demo/utils/formatter.py": "import os\nimport gc\nimport torchaudio\nimport pandas\nfrom faster_whisper import WhisperModel\nfrom glob import glob\n\nfrom tqdm import tqdm\n\nimport torch\nimport torchaudio\n# torch.set_num_threads(1)\n\nfrom TTS.tts.layers.xtts.tokenizer import multilingual_cleaners\n\ntorch.set_num_threads(16)\n\n\nimport os\n\naudio_types = (\".wav\", \".mp3\", \".flac\")\n\n\ndef list_audios(basePath, contains=None):\n    # return the set of files that are valid\n    return list_files(basePath, validExts=audio_types, contains=contains)\n\ndef list_files(basePath, validExts=None, contains=None):\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an audio and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the audio and yield it\n                audioPath = os.path.join(rootDir, filename)\n                yield audioPath\n\ndef format_audio_list(audio_files, target_language=\"en\", out_path=None, buffer=0.2, eval_percentage=0.15, speaker_name=\"coqui\", gradio_progress=None):\n    audio_total_size = 0\n    # make sure that ooutput file exists\n    os.makedirs(out_path, exist_ok=True)\n\n    # Loading Whisper\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n\n    print(\"Loading Whisper Model!\")\n    asr_model = WhisperModel(\"large-v2\", device=device, compute_type=\"float16\")\n\n    metadata = {\"audio_file\": [], \"text\": [], \"speaker_name\": []}\n\n    if gradio_progress is not None:\n        tqdm_object = gradio_progress.tqdm(audio_files, desc=\"Formatting...\")\n    else:\n        tqdm_object = tqdm(audio_files)\n\n    for audio_path in tqdm_object:\n        wav, sr = torchaudio.load(audio_path)\n        # stereo to mono if needed\n        if wav.size(0) != 1:\n            wav = torch.mean(wav, dim=0, keepdim=True)\n\n        wav = wav.squeeze()\n        audio_total_size += (wav.size(-1) / sr)\n\n        segments, _ = asr_model.transcribe(audio_path, word_timestamps=True, language=target_language)\n        segments = list(segments)\n        i = 0\n        sentence = \"\"\n        sentence_start = None\n        first_word = True\n        # added all segments words in a unique list\n        words_list = []\n        for _, segment in enumerate(segments):\n            words = list(segment.words)\n            words_list.extend(words)\n\n        # process each word\n        for word_idx, word in enumerate(words_list):\n            if first_word:\n                sentence_start = word.start\n                # If it is the first sentence, add buffer or get the begining of the file\n                if word_idx == 0:\n                    sentence_start = max(sentence_start - buffer, 0)  # Add buffer to the sentence start\n                else:\n                    # get previous sentence end\n                    previous_word_end = words_list[word_idx - 1].end\n                    # add buffer or get the silence midle between the previous sentence and the current one\n                    sentence_start = max(sentence_start - buffer, (previous_word_end + sentence_start)/2)\n\n                sentence = word.word\n                first_word = False\n            else:\n                sentence += word.word\n\n            if word.word[-1] in [\"!\", \".\", \"?\"]:\n                sentence = sentence[1:]\n                # Expand number and abbreviations plus normalization\n                sentence = multilingual_cleaners(sentence, target_language)\n                audio_file_name, _ = os.path.splitext(os.path.basename(audio_path))\n\n                audio_file = f\"wavs/{audio_file_name}_{str(i).zfill(8)}.wav\"\n\n                # Check for the next word's existence\n                if word_idx + 1 < len(words_list):\n                    next_word_start = words_list[word_idx + 1].start\n                else:\n                    # If don't have more words it means that it is the last sentence then use the audio len as next word start\n                    next_word_start = (wav.shape[0] - 1) / sr\n\n                # Average the current word end and next word start\n                word_end = min((word.end + next_word_start) / 2, word.end + buffer)\n                \n                absoulte_path = os.path.join(out_path, audio_file)\n                os.makedirs(os.path.dirname(absoulte_path), exist_ok=True)\n                i += 1\n                first_word = True\n\n                audio = wav[int(sr*sentence_start):int(sr*word_end)].unsqueeze(0)\n                # if the audio is too short ignore it (i.e < 0.33 seconds)\n                if audio.size(-1) >= sr/3:\n                    torchaudio.save(absoulte_path,\n                        audio,\n                        sr\n                    )\n                else:\n                    continue\n\n                metadata[\"audio_file\"].append(audio_file)\n                metadata[\"text\"].append(sentence)\n                metadata[\"speaker_name\"].append(speaker_name)\n\n    df = pandas.DataFrame(metadata)\n    df = df.sample(frac=1)\n    num_val_samples = int(len(df)*eval_percentage)\n\n    df_eval = df[:num_val_samples]\n    df_train = df[num_val_samples:]\n\n    df_train = df_train.sort_values('audio_file')\n    train_metadata_path = os.path.join(out_path, \"metadata_train.csv\")\n    df_train.to_csv(train_metadata_path, sep=\"|\", index=False)\n\n    eval_metadata_path = os.path.join(out_path, \"metadata_eval.csv\")\n    df_eval = df_eval.sort_values('audio_file')\n    df_eval.to_csv(eval_metadata_path, sep=\"|\", index=False)\n\n    # deallocate VRAM and RAM\n    del asr_model, df_train, df_eval, df, metadata\n    gc.collect()\n\n    return train_metadata_path, eval_metadata_path, audio_total_size", "TTS/demos/xtts_ft_demo/utils/gpt_train.py": "import os\nimport gc\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\nfrom TTS.utils.manage import ModelManager\n\n\ndef train_gpt(language, num_epochs, batch_size, grad_acumm, train_csv, eval_csv, output_path, max_audio_length=255995):\n    #  Logging parameters\n    RUN_NAME = \"GPT_XTTS_FT\"\n    PROJECT_NAME = \"XTTS_trainer\"\n    DASHBOARD_LOGGER = \"tensorboard\"\n    LOGGER_URI = None\n\n    # Set here the path that the checkpoints will be saved. Default: ./run/training/\n    OUT_PATH = os.path.join(output_path, \"run\", \"training\")\n\n    # Training Parameters\n    OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n    START_WITH_EVAL = False  # if True it will star with evaluation\n    BATCH_SIZE = batch_size  # set here the batch size\n    GRAD_ACUMM_STEPS = grad_acumm  # set here the grad accumulation steps\n\n\n    # Define here the dataset that you want to use for the fine-tuning on.\n    config_dataset = BaseDatasetConfig(\n        formatter=\"coqui\",\n        dataset_name=\"ft_dataset\",\n        path=os.path.dirname(train_csv),\n        meta_file_train=train_csv,\n        meta_file_val=eval_csv,\n        language=language,\n    )\n\n    # Add here the configs of the datasets\n    DATASETS_CONFIG_LIST = [config_dataset]\n\n    # Define the path where XTTS v2.0.1 files will be downloaded\n    CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n    os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n\n\n    # DVAE files\n    DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n    MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n\n    # Set the path to the downloaded files\n    DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n    MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n\n    # download DVAE files if needed\n    if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n        print(\" > Downloading DVAE files!\")\n        ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n\n\n    # Download XTTS v2.0 checkpoint if needed\n    TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n    XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n    XTTS_CONFIG_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/config.json\"\n\n    # XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n    TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n    XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n    XTTS_CONFIG_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CONFIG_LINK))  # config.json file\n\n    # download XTTS v2.0 files if needed\n    if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n        print(\" > Downloading XTTS v2.0 files!\")\n        ModelManager._download_model_files(\n            [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK, XTTS_CONFIG_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n        )\n\n    # init args and config\n    model_args = GPTArgs(\n        max_conditioning_length=132300,  # 6 secs\n        min_conditioning_length=66150,  # 3 secs\n        debug_loading_failures=False,\n        max_wav_length=max_audio_length,  # ~11.6 seconds\n        max_text_length=200,\n        mel_norm_file=MEL_NORM_FILE,\n        dvae_checkpoint=DVAE_CHECKPOINT,\n        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n        tokenizer_file=TOKENIZER_FILE,\n        gpt_num_audio_tokens=1026,\n        gpt_start_audio_token=1024,\n        gpt_stop_audio_token=1025,\n        gpt_use_masking_gt_prompt_approach=True,\n        gpt_use_perceiver_resampler=True,\n    )\n    # define audio config\n    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n    # training parameters config\n    config = GPTTrainerConfig(\n        epochs=num_epochs,\n        output_path=OUT_PATH,\n        model_args=model_args,\n        run_name=RUN_NAME,\n        project_name=PROJECT_NAME,\n        run_description=\"\"\"\n            GPT XTTS training\n            \"\"\",\n        dashboard_logger=DASHBOARD_LOGGER,\n        logger_uri=LOGGER_URI,\n        audio=audio_config,\n        batch_size=BATCH_SIZE,\n        batch_group_size=48,\n        eval_batch_size=BATCH_SIZE,\n        num_loader_workers=8,\n        eval_split_max_size=256,\n        print_step=50,\n        plot_step=100,\n        log_model_step=100,\n        save_step=1000,\n        save_n_checkpoints=1,\n        save_checkpoints=True,\n        # target_loss=\"loss\",\n        print_eval=False,\n        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n        optimizer=\"AdamW\",\n        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n        lr=5e-06,  # learning rate\n        lr_scheduler=\"MultiStepLR\",\n        # it was adjusted accordly for the new step scheme\n        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n        test_sentences=[],\n    )\n\n    # init the model from config\n    model = GPTTrainer.init_from_config(config)\n\n    # load training samples\n    train_samples, eval_samples = load_tts_samples(\n        DATASETS_CONFIG_LIST,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # init the trainer and \ud83d\ude80\n    trainer = Trainer(\n        TrainerArgs(\n            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n            skip_train_epoch=False,\n            start_with_eval=START_WITH_EVAL,\n            grad_accum_steps=GRAD_ACUMM_STEPS,\n        ),\n        config,\n        output_path=OUT_PATH,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n    )\n    trainer.fit()\n\n    # get the longest text audio file to use as speaker reference\n    samples_len = [len(item[\"text\"].split(\" \")) for item in train_samples]\n    longest_text_idx =  samples_len.index(max(samples_len))\n    speaker_ref = train_samples[longest_text_idx][\"audio_file\"]\n\n    trainer_out_path = trainer.output_path\n\n    # deallocate VRAM and RAM\n    del model, trainer, train_samples, eval_samples\n    gc.collect()\n\n    return XTTS_CONFIG_FILE, XTTS_CHECKPOINT, TOKENIZER_FILE, trainer_out_path, speaker_ref\n", "TTS/config/shared_configs.py": "from dataclasses import asdict, dataclass\nfrom typing import List\n\nfrom coqpit import Coqpit, check_argument\nfrom trainer import TrainerConfig\n\n\n@dataclass\nclass BaseAudioConfig(Coqpit):\n    \"\"\"Base config to definge audio processing parameters. It is used to initialize\n    ```TTS.utils.audio.AudioProcessor.```\n\n    Args:\n        fft_size (int):\n            Number of STFT frequency levels aka.size of the linear spectogram frame. Defaults to 1024.\n\n        win_length (int):\n            Each frame of audio is windowed by window of length ```win_length``` and then padded with zeros to match\n            ```fft_size```. Defaults to 1024.\n\n        hop_length (int):\n            Number of audio samples between adjacent STFT columns. Defaults to 1024.\n\n        frame_shift_ms (int):\n            Set ```hop_length``` based on milliseconds and sampling rate.\n\n        frame_length_ms (int):\n            Set ```win_length``` based on milliseconds and sampling rate.\n\n        stft_pad_mode (str):\n            Padding method used in STFT. 'reflect' or 'center'. Defaults to 'reflect'.\n\n        sample_rate (int):\n            Audio sampling rate. Defaults to 22050.\n\n        resample (bool):\n            Enable / Disable resampling audio to ```sample_rate```. Defaults to ```False```.\n\n        preemphasis (float):\n            Preemphasis coefficient. Defaults to 0.0.\n\n        ref_level_db (int): 20\n            Reference Db level to rebase the audio signal and ignore the level below. 20Db is assumed the sound of air.\n            Defaults to 20.\n\n        do_sound_norm (bool):\n            Enable / Disable sound normalization to reconcile the volume differences among samples. Defaults to False.\n\n        log_func (str):\n            Numpy log function used for amplitude to DB conversion. Defaults to 'np.log10'.\n\n        do_trim_silence (bool):\n            Enable / Disable trimming silences at the beginning and the end of the audio clip. Defaults to ```True```.\n\n        do_amp_to_db_linear (bool, optional):\n            enable/disable amplitude to dB conversion of linear spectrograms. Defaults to True.\n\n        do_amp_to_db_mel (bool, optional):\n            enable/disable amplitude to dB conversion of mel spectrograms. Defaults to True.\n\n        pitch_fmax (float, optional):\n            Maximum frequency of the F0 frames. Defaults to ```640```.\n\n        pitch_fmin (float, optional):\n            Minimum frequency of the F0 frames. Defaults to ```1```.\n\n        trim_db (int):\n            Silence threshold used for silence trimming. Defaults to 45.\n\n        do_rms_norm (bool, optional):\n            enable/disable RMS volume normalization when loading an audio file. Defaults to False.\n\n        db_level (int, optional):\n            dB level used for rms normalization. The range is -99 to 0. Defaults to None.\n\n        power (float):\n            Exponent used for expanding spectrogra levels before running Griffin Lim. It helps to reduce the\n            artifacts in the synthesized voice. Defaults to 1.5.\n\n        griffin_lim_iters (int):\n            Number of Griffing Lim iterations. Defaults to 60.\n\n        num_mels (int):\n            Number of mel-basis frames that defines the frame lengths of each mel-spectrogram frame. Defaults to 80.\n\n        mel_fmin (float): Min frequency level used for the mel-basis filters. ~50 for male and ~95 for female voices.\n            It needs to be adjusted for a dataset. Defaults to 0.\n\n        mel_fmax (float):\n            Max frequency level used for the mel-basis filters. It needs to be adjusted for a dataset.\n\n        spec_gain (int):\n            Gain applied when converting amplitude to DB. Defaults to 20.\n\n        signal_norm (bool):\n            enable/disable signal normalization. Defaults to True.\n\n        min_level_db (int):\n            minimum db threshold for the computed melspectrograms. Defaults to -100.\n\n        symmetric_norm (bool):\n            enable/disable symmetric normalization. If set True normalization is performed in the range [-k, k] else\n            [0, k], Defaults to True.\n\n        max_norm (float):\n            ```k``` defining the normalization range. Defaults to 4.0.\n\n        clip_norm (bool):\n            enable/disable clipping the our of range values in the normalized audio signal. Defaults to True.\n\n        stats_path (str):\n            Path to the computed stats file. Defaults to None.\n    \"\"\"\n\n    # stft parameters\n    fft_size: int = 1024\n    win_length: int = 1024\n    hop_length: int = 256\n    frame_shift_ms: int = None\n    frame_length_ms: int = None\n    stft_pad_mode: str = \"reflect\"\n    # audio processing parameters\n    sample_rate: int = 22050\n    resample: bool = False\n    preemphasis: float = 0.0\n    ref_level_db: int = 20\n    do_sound_norm: bool = False\n    log_func: str = \"np.log10\"\n    # silence trimming\n    do_trim_silence: bool = True\n    trim_db: int = 45\n    # rms volume normalization\n    do_rms_norm: bool = False\n    db_level: float = None\n    # griffin-lim params\n    power: float = 1.5\n    griffin_lim_iters: int = 60\n    # mel-spec params\n    num_mels: int = 80\n    mel_fmin: float = 0.0\n    mel_fmax: float = None\n    spec_gain: int = 20\n    do_amp_to_db_linear: bool = True\n    do_amp_to_db_mel: bool = True\n    # f0 params\n    pitch_fmax: float = 640.0\n    pitch_fmin: float = 1.0\n    # normalization params\n    signal_norm: bool = True\n    min_level_db: int = -100\n    symmetric_norm: bool = True\n    max_norm: float = 4.0\n    clip_norm: bool = True\n    stats_path: str = None\n\n    def check_values(\n        self,\n    ):\n        \"\"\"Check config fields\"\"\"\n        c = asdict(self)\n        check_argument(\"num_mels\", c, restricted=True, min_val=10, max_val=2056)\n        check_argument(\"fft_size\", c, restricted=True, min_val=128, max_val=4058)\n        check_argument(\"sample_rate\", c, restricted=True, min_val=512, max_val=100000)\n        check_argument(\n            \"frame_length_ms\",\n            c,\n            restricted=True,\n            min_val=10,\n            max_val=1000,\n            alternative=\"win_length\",\n        )\n        check_argument(\"frame_shift_ms\", c, restricted=True, min_val=1, max_val=1000, alternative=\"hop_length\")\n        check_argument(\"preemphasis\", c, restricted=True, min_val=0, max_val=1)\n        check_argument(\"min_level_db\", c, restricted=True, min_val=-1000, max_val=10)\n        check_argument(\"ref_level_db\", c, restricted=True, min_val=0, max_val=1000)\n        check_argument(\"power\", c, restricted=True, min_val=1, max_val=5)\n        check_argument(\"griffin_lim_iters\", c, restricted=True, min_val=10, max_val=1000)\n\n        # normalization parameters\n        check_argument(\"signal_norm\", c, restricted=True)\n        check_argument(\"symmetric_norm\", c, restricted=True)\n        check_argument(\"max_norm\", c, restricted=True, min_val=0.1, max_val=1000)\n        check_argument(\"clip_norm\", c, restricted=True)\n        check_argument(\"mel_fmin\", c, restricted=True, min_val=0.0, max_val=1000)\n        check_argument(\"mel_fmax\", c, restricted=True, min_val=500.0, allow_none=True)\n        check_argument(\"spec_gain\", c, restricted=True, min_val=1, max_val=100)\n        check_argument(\"do_trim_silence\", c, restricted=True)\n        check_argument(\"trim_db\", c, restricted=True)\n\n\n@dataclass\nclass BaseDatasetConfig(Coqpit):\n    \"\"\"Base config for TTS datasets.\n\n    Args:\n        formatter (str):\n            Formatter name that defines used formatter in ```TTS.tts.datasets.formatter```. Defaults to `\"\"`.\n\n        dataset_name (str):\n            Unique name for the dataset. Defaults to `\"\"`.\n\n        path (str):\n            Root path to the dataset files. Defaults to `\"\"`.\n\n        meta_file_train (str):\n            Name of the dataset meta file. Or a list of speakers to be ignored at training for multi-speaker datasets.\n            Defaults to `\"\"`.\n\n        ignored_speakers (List):\n            List of speakers IDs that are not used at the training. Default None.\n\n        language (str):\n            Language code of the dataset. If defined, it overrides `phoneme_language`. Defaults to `\"\"`.\n\n        phonemizer (str):\n            Phonemizer used for that dataset's language. By default it uses `DEF_LANG_TO_PHONEMIZER`. Defaults to `\"\"`.\n\n        meta_file_val (str):\n            Name of the dataset meta file that defines the instances used at validation.\n\n        meta_file_attn_mask (str):\n            Path to the file that lists the attention mask files used with models that require attention masks to\n            train the duration predictor.\n    \"\"\"\n\n    formatter: str = \"\"\n    dataset_name: str = \"\"\n    path: str = \"\"\n    meta_file_train: str = \"\"\n    ignored_speakers: List[str] = None\n    language: str = \"\"\n    phonemizer: str = \"\"\n    meta_file_val: str = \"\"\n    meta_file_attn_mask: str = \"\"\n\n    def check_values(\n        self,\n    ):\n        \"\"\"Check config fields\"\"\"\n        c = asdict(self)\n        check_argument(\"formatter\", c, restricted=True)\n        check_argument(\"path\", c, restricted=True)\n        check_argument(\"meta_file_train\", c, restricted=True)\n        check_argument(\"meta_file_val\", c, restricted=False)\n        check_argument(\"meta_file_attn_mask\", c, restricted=False)\n\n\n@dataclass\nclass BaseTrainingConfig(TrainerConfig):\n    \"\"\"Base config to define the basic \ud83d\udc38TTS training parameters that are shared\n    among all the models. It is based on ```Trainer.TrainingConfig```.\n\n    Args:\n        model (str):\n            Name of the model that is used in the training.\n\n        num_loader_workers (int):\n            Number of workers for training time dataloader.\n\n        num_eval_loader_workers (int):\n            Number of workers for evaluation time dataloader.\n    \"\"\"\n\n    model: str = None\n    # dataloading\n    num_loader_workers: int = 0\n    num_eval_loader_workers: int = 0\n    use_noise_augment: bool = False\n", "TTS/config/__init__.py": "import json\nimport os\nimport re\nfrom typing import Dict\n\nimport fsspec\nimport yaml\nfrom coqpit import Coqpit\n\nfrom TTS.config.shared_configs import *\nfrom TTS.utils.generic_utils import find_module\n\n\ndef read_json_with_comments(json_path):\n    \"\"\"for backward compat.\"\"\"\n    # fallback to json\n    with fsspec.open(json_path, \"r\", encoding=\"utf-8\") as f:\n        input_str = f.read()\n    # handle comments but not urls with //\n    input_str = re.sub(r\"(\\\"(?:[^\\\"\\\\]|\\\\.)*\\\")|(/\\*(?:.|[\\\\n\\\\r])*?\\*/)|(//.*)\", lambda m: m.group(1) or m.group(2) or \"\", input_str)\n    return json.loads(input_str)\n\ndef register_config(model_name: str) -> Coqpit:\n    \"\"\"Find the right config for the given model name.\n\n    Args:\n        model_name (str): Model name.\n\n    Raises:\n        ModuleNotFoundError: No matching config for the model name.\n\n    Returns:\n        Coqpit: config class.\n    \"\"\"\n    config_class = None\n    config_name = model_name + \"_config\"\n\n    # TODO: fix this\n    if model_name == \"xtts\":\n        from TTS.tts.configs.xtts_config import XttsConfig\n\n        config_class = XttsConfig\n    paths = [\"TTS.tts.configs\", \"TTS.vocoder.configs\", \"TTS.encoder.configs\", \"TTS.vc.configs\"]\n    for path in paths:\n        try:\n            config_class = find_module(path, config_name)\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class\n\n\ndef _process_model_name(config_dict: Dict) -> str:\n    \"\"\"Format the model name as expected. It is a band-aid for the old `vocoder` model names.\n\n    Args:\n        config_dict (Dict): A dictionary including the config fields.\n\n    Returns:\n        str: Formatted modelname.\n    \"\"\"\n    model_name = config_dict[\"model\"] if \"model\" in config_dict else config_dict[\"generator_model\"]\n    model_name = model_name.replace(\"_generator\", \"\").replace(\"_discriminator\", \"\")\n    return model_name\n\n\ndef load_config(config_path: str) -> Coqpit:\n    \"\"\"Import `json` or `yaml` files as TTS configs. First, load the input file as a `dict` and check the model name\n    to find the corresponding Config class. Then initialize the Config.\n\n    Args:\n        config_path (str): path to the config file.\n\n    Raises:\n        TypeError: given config file has an unknown type.\n\n    Returns:\n        Coqpit: TTS config object.\n    \"\"\"\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = yaml.safe_load(f)\n    elif ext == \".json\":\n        try:\n            with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n        except json.decoder.JSONDecodeError:\n            # backwards compat.\n            data = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type {ext}\")\n    config_dict.update(data)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name.lower())\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n\ndef check_config_and_model_args(config, arg_name, value):\n    \"\"\"Check the give argument in `config.model_args` if exist or in `config` for\n    the given value.\n\n    Return False if the argument does not exist in `config.model_args` or `config`.\n    This is to patch up the compatibility between models with and without `model_args`.\n\n    TODO: Remove this in the future with a unified approach.\n    \"\"\"\n    if hasattr(config, \"model_args\"):\n        if arg_name in config.model_args:\n            return config.model_args[arg_name] == value\n    if hasattr(config, arg_name):\n        return config[arg_name] == value\n    return False\n\n\ndef get_from_config_or_model_args(config, arg_name):\n    \"\"\"Get the given argument from `config.model_args` if exist or in `config`.\"\"\"\n    if hasattr(config, \"model_args\"):\n        if arg_name in config.model_args:\n            return config.model_args[arg_name]\n    return config[arg_name]\n\n\ndef get_from_config_or_model_args_with_default(config, arg_name, def_val):\n    \"\"\"Get the given argument from `config.model_args` if exist or in `config`.\"\"\"\n    if hasattr(config, \"model_args\"):\n        if arg_name in config.model_args:\n            return config.model_args[arg_name]\n    if hasattr(config, arg_name):\n        return config[arg_name]\n    return def_val\n", "TTS/tts/__init__.py": "", "TTS/tts/models/bark.py": "import os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport numpy as np\nfrom coqpit import Coqpit\nfrom encodec import EncodecModel\nfrom transformers import BertTokenizer\n\nfrom TTS.tts.layers.bark.inference_funcs import (\n    codec_decode,\n    generate_coarse,\n    generate_fine,\n    generate_text_semantic,\n    generate_voice,\n    load_voice,\n)\nfrom TTS.tts.layers.bark.load_model import load_model\nfrom TTS.tts.layers.bark.model import GPT\nfrom TTS.tts.layers.bark.model_fine import FineGPT\nfrom TTS.tts.models.base_tts import BaseTTS\n\n\n@dataclass\nclass BarkAudioConfig(Coqpit):\n    sample_rate: int = 24000\n    output_sample_rate: int = 24000\n\n\nclass Bark(BaseTTS):\n    def __init__(\n        self,\n        config: Coqpit,\n        tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\"),\n    ) -> None:\n        super().__init__(config=config, ap=None, tokenizer=None, speaker_manager=None, language_manager=None)\n        self.config.num_chars = len(tokenizer)\n        self.tokenizer = tokenizer\n        self.semantic_model = GPT(config.semantic_config)\n        self.coarse_model = GPT(config.coarse_config)\n        self.fine_model = FineGPT(config.fine_config)\n        self.encodec = EncodecModel.encodec_model_24khz()\n        self.encodec.set_target_bandwidth(6.0)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def load_bark_models(self):\n        self.semantic_model, self.config = load_model(\n            ckpt_path=self.config.LOCAL_MODEL_PATHS[\"text\"], device=self.device, config=self.config, model_type=\"text\"\n        )\n        self.coarse_model, self.config = load_model(\n            ckpt_path=self.config.LOCAL_MODEL_PATHS[\"coarse\"],\n            device=self.device,\n            config=self.config,\n            model_type=\"coarse\",\n        )\n        self.fine_model, self.config = load_model(\n            ckpt_path=self.config.LOCAL_MODEL_PATHS[\"fine\"], device=self.device, config=self.config, model_type=\"fine\"\n        )\n\n    def train_step(\n        self,\n    ):\n        pass\n\n    def text_to_semantic(\n        self,\n        text: str,\n        history_prompt: Optional[str] = None,\n        temp: float = 0.7,\n        base=None,\n        allow_early_stop=True,\n        **kwargs,\n    ):\n        \"\"\"Generate semantic array from text.\n\n        Args:\n            text: text to be turned into audio\n            history_prompt: history choice for audio cloning\n            temp: generation temperature (1.0 more diverse, 0.0 more conservative)\n\n        Returns:\n            numpy semantic array to be fed into `semantic_to_waveform`\n        \"\"\"\n        x_semantic = generate_text_semantic(\n            text,\n            self,\n            history_prompt=history_prompt,\n            temp=temp,\n            base=base,\n            allow_early_stop=allow_early_stop,\n            **kwargs,\n        )\n        return x_semantic\n\n    def semantic_to_waveform(\n        self,\n        semantic_tokens: np.ndarray,\n        history_prompt: Optional[str] = None,\n        temp: float = 0.7,\n        base=None,\n    ):\n        \"\"\"Generate audio array from semantic input.\n\n        Args:\n            semantic_tokens: semantic token output from `text_to_semantic`\n            history_prompt: history choice for audio cloning\n            temp: generation temperature (1.0 more diverse, 0.0 more conservative)\n\n        Returns:\n            numpy audio array at sample frequency 24khz\n        \"\"\"\n        x_coarse_gen = generate_coarse(\n            semantic_tokens,\n            self,\n            history_prompt=history_prompt,\n            temp=temp,\n            base=base,\n        )\n        x_fine_gen = generate_fine(\n            x_coarse_gen,\n            self,\n            history_prompt=history_prompt,\n            temp=0.5,\n            base=base,\n        )\n        audio_arr = codec_decode(x_fine_gen, self)\n        return audio_arr, x_coarse_gen, x_fine_gen\n\n    def generate_audio(\n        self,\n        text: str,\n        history_prompt: Optional[str] = None,\n        text_temp: float = 0.7,\n        waveform_temp: float = 0.7,\n        base=None,\n        allow_early_stop=True,\n        **kwargs,\n    ):\n        \"\"\"Generate audio array from input text.\n\n        Args:\n            text: text to be turned into audio\n            history_prompt: history choice for audio cloning\n            text_temp: generation temperature (1.0 more diverse, 0.0 more conservative)\n            waveform_temp: generation temperature (1.0 more diverse, 0.0 more conservative)\n\n        Returns:\n            numpy audio array at sample frequency 24khz\n        \"\"\"\n        x_semantic = self.text_to_semantic(\n            text,\n            history_prompt=history_prompt,\n            temp=text_temp,\n            base=base,\n            allow_early_stop=allow_early_stop,\n            **kwargs,\n        )\n        audio_arr, c, f = self.semantic_to_waveform(\n            x_semantic, history_prompt=history_prompt, temp=waveform_temp, base=base\n        )\n        return audio_arr, [x_semantic, c, f]\n\n    def generate_voice(self, audio, speaker_id, voice_dir):\n        \"\"\"Generate a voice from the given audio and text.\n\n        Args:\n            audio (str): Path to the audio file.\n            speaker_id (str): Speaker name.\n            voice_dir (str): Path to the directory to save the generate voice.\n        \"\"\"\n        if voice_dir is not None:\n            voice_dirs = [voice_dir]\n            try:\n                _ = load_voice(speaker_id, voice_dirs)\n            except (KeyError, FileNotFoundError):\n                output_path = os.path.join(voice_dir, speaker_id + \".npz\")\n                os.makedirs(voice_dir, exist_ok=True)\n                generate_voice(audio, self, output_path)\n\n    def _set_voice_dirs(self, voice_dirs):\n        def_voice_dir = None\n        if isinstance(self.config.DEF_SPEAKER_DIR, str):\n            os.makedirs(self.config.DEF_SPEAKER_DIR, exist_ok=True)\n            if os.path.isdir(self.config.DEF_SPEAKER_DIR):\n                def_voice_dir = self.config.DEF_SPEAKER_DIR\n        _voice_dirs = [def_voice_dir] if def_voice_dir is not None else []\n        if voice_dirs is not None:\n            if isinstance(voice_dirs, str):\n                voice_dirs = [voice_dirs]\n            _voice_dirs = voice_dirs + _voice_dirs\n        return _voice_dirs\n\n    # TODO: remove config from synthesize\n    def synthesize(\n        self, text, config, speaker_id=\"random\", voice_dirs=None, **kwargs\n    ):  # pylint: disable=unused-argument\n        \"\"\"Synthesize speech with the given input text.\n\n        Args:\n            text (str): Input text.\n            config (BarkConfig): Config with inference parameters.\n            speaker_id (str): One of the available speaker names. If `random`, it generates a random speaker.\n            speaker_wav (str): Path to the speaker audio file for cloning a new voice. It is cloned and saved in\n                `voice_dirs` with the name `speaker_id`. Defaults to None.\n            voice_dirs (List[str]): List of paths that host reference audio files for speakers. Defaults to None.\n            **kwargs: Model specific inference settings used by `generate_audio()` and `TTS.tts.layers.bark.inference_funcs.generate_text_semantic().\n\n        Returns:\n            A dictionary of the output values with `wav` as output waveform, `deterministic_seed` as seed used at inference,\n            `text_input` as text token IDs after tokenizer, `voice_samples` as samples used for cloning, `conditioning_latents`\n            as latents used at inference.\n\n        \"\"\"\n        speaker_id = \"random\" if speaker_id is None else speaker_id\n        voice_dirs = self._set_voice_dirs(voice_dirs)\n        history_prompt = load_voice(self, speaker_id, voice_dirs)\n        outputs = self.generate_audio(text, history_prompt=history_prompt, **kwargs)\n        return_dict = {\n            \"wav\": outputs[0],\n            \"text_inputs\": text,\n        }\n\n        return return_dict\n\n    def eval_step(self):\n        ...\n\n    def forward(self):\n        ...\n\n    def inference(self):\n        ...\n\n    @staticmethod\n    def init_from_config(config: \"BarkConfig\", **kwargs):  # pylint: disable=unused-argument\n        return Bark(config)\n\n    # pylint: disable=unused-argument, redefined-builtin\n    def load_checkpoint(\n        self,\n        config,\n        checkpoint_dir,\n        text_model_path=None,\n        coarse_model_path=None,\n        fine_model_path=None,\n        hubert_model_path=None,\n        hubert_tokenizer_path=None,\n        eval=False,\n        strict=True,\n        **kwargs,\n    ):\n        \"\"\"Load a model checkpoints from a directory. This model is with multiple checkpoint files and it\n        expects to have all the files to be under the given `checkpoint_dir` with the rigth names.\n        If eval is True, set the model to eval mode.\n\n        Args:\n            config (TortoiseConfig): The model config.\n            checkpoint_dir (str): The directory where the checkpoints are stored.\n            ar_checkpoint_path (str, optional): The path to the autoregressive checkpoint. Defaults to None.\n            diff_checkpoint_path (str, optional): The path to the diffusion checkpoint. Defaults to None.\n            clvp_checkpoint_path (str, optional): The path to the CLVP checkpoint. Defaults to None.\n            vocoder_checkpoint_path (str, optional): The path to the vocoder checkpoint. Defaults to None.\n            eval (bool, optional): Whether to set the model to eval mode. Defaults to False.\n            strict (bool, optional): Whether to load the model strictly. Defaults to True.\n        \"\"\"\n        text_model_path = text_model_path or os.path.join(checkpoint_dir, \"text_2.pt\")\n        coarse_model_path = coarse_model_path or os.path.join(checkpoint_dir, \"coarse_2.pt\")\n        fine_model_path = fine_model_path or os.path.join(checkpoint_dir, \"fine_2.pt\")\n        hubert_model_path = hubert_model_path or os.path.join(checkpoint_dir, \"hubert.pt\")\n        hubert_tokenizer_path = hubert_tokenizer_path or os.path.join(checkpoint_dir, \"tokenizer.pth\")\n\n        self.config.LOCAL_MODEL_PATHS[\"text\"] = text_model_path\n        self.config.LOCAL_MODEL_PATHS[\"coarse\"] = coarse_model_path\n        self.config.LOCAL_MODEL_PATHS[\"fine\"] = fine_model_path\n        self.config.LOCAL_MODEL_PATHS[\"hubert\"] = hubert_model_path\n        self.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"] = hubert_tokenizer_path\n\n        self.load_bark_models()\n\n        if eval:\n            self.eval()\n", "TTS/tts/models/overflow.py": "import os\nfrom typing import Dict, List, Union\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom trainer.logging.tensorboard_logger import TensorboardLogger\n\nfrom TTS.tts.layers.overflow.common_layers import Encoder, OverflowUtils\nfrom TTS.tts.layers.overflow.decoder import Decoder\nfrom TTS.tts.layers.overflow.neural_hmm import NeuralHMM\nfrom TTS.tts.layers.overflow.plotting_utils import (\n    get_spec_from_most_probable_state,\n    plot_transition_probabilities_to_numpy,\n)\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.generic_utils import format_aux_input\nfrom TTS.utils.io import load_fsspec\n\n\nclass Overflow(BaseTTS):\n    \"\"\"OverFlow TTS model.\n\n    Paper::\n        https://arxiv.org/abs/2211.06892\n\n    Paper abstract::\n        Neural HMMs are a type of neural transducer recently proposed for\n    sequence-to-sequence modelling in text-to-speech. They combine the best features\n    of classic statistical speech synthesis and modern neural TTS, requiring less\n    data and fewer training updates, and are less prone to gibberish output caused\n    by neural attention failures. In this paper, we combine neural HMM TTS with\n    normalising flows for describing the highly non-Gaussian distribution of speech\n    acoustics. The result is a powerful, fully probabilistic model of durations and\n    acoustics that can be trained using exact maximum likelihood. Compared to\n    dominant flow-based acoustic models, our approach integrates autoregression for\n    improved modelling of long-range dependences such as utterance-level prosody.\n    Experiments show that a system based on our proposal gives more accurate\n    pronunciations and better subjective speech quality than comparable methods,\n    whilst retaining the original advantages of neural HMMs. Audio examples and code\n    are available at https://shivammehta25.github.io/OverFlow/.\n\n    Note:\n        - Neural HMMs uses flat start initialization i.e it computes the means and std and transition probabilities\n        of the dataset and uses them to initialize the model. This benefits the model and helps with faster learning\n        If you change the dataset or want to regenerate the parameters change the `force_generate_statistics` and\n        `mel_statistics_parameter_path` accordingly.\n\n        - To enable multi-GPU training, set the `use_grad_checkpointing=False` in config.\n        This will significantly increase the memory usage.  This is because to compute\n        the actual data likelihood (not an approximation using MAS/Viterbi) we must use\n        all the states at the previous time step during the forward pass to decide the\n        probability distribution at the current step i.e the difference between the forward\n        algorithm and viterbi approximation.\n\n    Check :class:`TTS.tts.configs.overflow.OverFlowConfig` for class arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: \"OverFlowConfig\",\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        # pass all config fields to `self`\n        # for fewer code change\n        self.config = config\n        for key in config:\n            setattr(self, key, config[key])\n\n        self.decoder_output_dim = config.out_channels\n\n        self.encoder = Encoder(config.num_chars, config.state_per_phone, config.encoder_in_out_features)\n        self.neural_hmm = NeuralHMM(\n            frame_channels=self.out_channels,\n            ar_order=self.ar_order,\n            deterministic_transition=self.deterministic_transition,\n            encoder_dim=self.encoder_in_out_features,\n            prenet_type=self.prenet_type,\n            prenet_dim=self.prenet_dim,\n            prenet_n_layers=self.prenet_n_layers,\n            prenet_dropout=self.prenet_dropout,\n            prenet_dropout_at_inference=self.prenet_dropout_at_inference,\n            memory_rnn_dim=self.memory_rnn_dim,\n            outputnet_size=self.outputnet_size,\n            flat_start_params=self.flat_start_params,\n            std_floor=self.std_floor,\n            use_grad_checkpointing=self.use_grad_checkpointing,\n        )\n\n        self.decoder = Decoder(\n            self.out_channels,\n            self.hidden_channels_dec,\n            self.kernel_size_dec,\n            self.dilation_rate,\n            self.num_flow_blocks_dec,\n            self.num_block_layers,\n            dropout_p=self.dropout_p_dec,\n            num_splits=self.num_splits,\n            num_squeeze=self.num_squeeze,\n            sigmoid_scale=self.sigmoid_scale,\n            c_in_channels=self.c_in_channels,\n        )\n\n        self.register_buffer(\"mean\", torch.tensor(0))\n        self.register_buffer(\"std\", torch.tensor(1))\n\n    def update_mean_std(self, statistics_dict: Dict):\n        self.mean.data = torch.tensor(statistics_dict[\"mean\"])\n        self.std.data = torch.tensor(statistics_dict[\"std\"])\n\n    def preprocess_batch(self, text, text_len, mels, mel_len):\n        if self.mean.item() == 0 or self.std.item() == 1:\n            statistics_dict = torch.load(self.mel_statistics_parameter_path)\n            self.update_mean_std(statistics_dict)\n\n        mels = self.normalize(mels)\n        return text, text_len, mels, mel_len\n\n    def normalize(self, x):\n        return x.sub(self.mean).div(self.std)\n\n    def inverse_normalize(self, x):\n        return x.mul(self.std).add(self.mean)\n\n    def forward(self, text, text_len, mels, mel_len):\n        \"\"\"\n        Forward pass for training and computing the log likelihood of a given batch.\n\n        Shapes:\n            Shapes:\n            text: :math:`[B, T_in]`\n            text_len: :math:`[B]`\n            mels: :math:`[B, T_out, C]`\n            mel_len: :math:`[B]`\n        \"\"\"\n        text, text_len, mels, mel_len = self.preprocess_batch(text, text_len, mels, mel_len)\n        encoder_outputs, encoder_output_len = self.encoder(text, text_len)\n        z, z_lengths, logdet = self.decoder(mels.transpose(1, 2), mel_len)\n        log_probs, fwd_alignments, transition_vectors, means = self.neural_hmm(\n            encoder_outputs, encoder_output_len, z, z_lengths\n        )\n\n        outputs = {\n            \"log_probs\": log_probs + logdet,\n            \"alignments\": fwd_alignments,\n            \"transition_vectors\": transition_vectors,\n            \"means\": means,\n        }\n\n        return outputs\n\n    @staticmethod\n    def _training_stats(batch):\n        stats = {}\n        stats[\"avg_text_length\"] = batch[\"text_lengths\"].float().mean()\n        stats[\"avg_spec_length\"] = batch[\"mel_lengths\"].float().mean()\n        stats[\"avg_text_batch_occupancy\"] = (batch[\"text_lengths\"].float() / batch[\"text_lengths\"].float().max()).mean()\n        stats[\"avg_spec_batch_occupancy\"] = (batch[\"mel_lengths\"].float() / batch[\"mel_lengths\"].float().max()).mean()\n        return stats\n\n    def train_step(self, batch: dict, criterion: nn.Module):\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n\n        outputs = self.forward(\n            text=text_input,\n            text_len=text_lengths,\n            mels=mel_input,\n            mel_len=mel_lengths,\n        )\n        loss_dict = criterion(outputs[\"log_probs\"] / (mel_lengths.sum() + text_lengths.sum()))\n\n        # for printing useful statistics on terminal\n        loss_dict.update(self._training_stats(batch))\n        return outputs, loss_dict\n\n    def eval_step(self, batch: Dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def _format_aux_input(self, aux_input: Dict, default_input_dict):\n        \"\"\"Set missing fields to their default value.\n\n        Args:\n            aux_inputs (Dict): Dictionary containing the auxiliary inputs.\n        \"\"\"\n        default_input_dict = default_input_dict.copy()\n        default_input_dict.update(\n            {\n                \"sampling_temp\": self.sampling_temp,\n                \"max_sampling_time\": self.max_sampling_time,\n                \"duration_threshold\": self.duration_threshold,\n            }\n        )\n        if aux_input:\n            return format_aux_input(default_input_dict, aux_input)\n        return default_input_dict\n\n    @torch.no_grad()\n    def inference(\n        self,\n        text: torch.Tensor,\n        aux_input={\"x_lengths\": None, \"sampling_temp\": None, \"max_sampling_time\": None, \"duration_threshold\": None},\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"Sampling from the model\n\n        Args:\n            text (torch.Tensor): :math:`[B, T_in]`\n            aux_inputs (_type_, optional): _description_. Defaults to None.\n\n        Returns:\n            outputs: Dictionary containing the following\n                - mel (torch.Tensor): :math:`[B, T_out, C]`\n                - hmm_outputs_len (torch.Tensor): :math:`[B]`\n                - state_travelled (List[List[int]]): List of lists containing the state travelled for each sample in the batch.\n                - input_parameters (list[torch.FloatTensor]): Input parameters to the neural HMM.\n                - output_parameters (list[torch.FloatTensor]): Output parameters to the neural HMM.\n        \"\"\"\n        default_input_dict = {\n            \"x_lengths\": torch.sum(text != 0, dim=1),\n        }\n        aux_input = self._format_aux_input(aux_input, default_input_dict)\n        encoder_outputs, encoder_output_len = self.encoder.inference(text, aux_input[\"x_lengths\"])\n        outputs = self.neural_hmm.inference(\n            encoder_outputs,\n            encoder_output_len,\n            sampling_temp=aux_input[\"sampling_temp\"],\n            max_sampling_time=aux_input[\"max_sampling_time\"],\n            duration_threshold=aux_input[\"duration_threshold\"],\n        )\n\n        mels, mel_outputs_len, _ = self.decoder(\n            outputs[\"hmm_outputs\"].transpose(1, 2), outputs[\"hmm_outputs_len\"], reverse=True\n        )\n        mels = self.inverse_normalize(mels.transpose(1, 2))\n        outputs.update({\"model_outputs\": mels, \"model_outputs_len\": mel_outputs_len})\n        outputs[\"alignments\"] = OverflowUtils.double_pad(outputs[\"alignments\"])\n        return outputs\n\n    @staticmethod\n    def get_criterion():\n        return NLLLoss()\n\n    @staticmethod\n    def init_from_config(config: \"OverFlowConfig\", samples: Union[List[List], List[Dict]] = None, verbose=True):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (VitsConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n            verbose (bool): If True, print init messages. Defaults to True.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config, verbose)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return Overflow(new_config, ap, tokenizer, speaker_manager)\n\n    def load_checkpoint(\n        self, config: Coqpit, checkpoint_path: str, eval: bool = False, strict: bool = True, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            self.decoder.store_inverse()\n            assert not self.training\n\n    def on_init_start(self, trainer):\n        \"\"\"If the current dataset does not have normalisation statistics and initialisation transition_probability it computes them otherwise loads.\"\"\"\n        if not os.path.isfile(trainer.config.mel_statistics_parameter_path) or trainer.config.force_generate_statistics:\n            dataloader = trainer.get_train_dataloader(\n                training_assets=None, samples=trainer.train_samples, verbose=False\n            )\n            print(\n                f\" | > Data parameters not found for: {trainer.config.mel_statistics_parameter_path}. Computing mel normalization parameters...\"\n            )\n            data_mean, data_std, init_transition_prob = OverflowUtils.get_data_parameters_for_flat_start(\n                dataloader, trainer.config.out_channels, trainer.config.state_per_phone\n            )\n            print(\n                f\" | > Saving data parameters to: {trainer.config.mel_statistics_parameter_path}: value: {data_mean, data_std, init_transition_prob}\"\n            )\n            statistics = {\n                \"mean\": data_mean.item(),\n                \"std\": data_std.item(),\n                \"init_transition_prob\": init_transition_prob.item(),\n            }\n            torch.save(statistics, trainer.config.mel_statistics_parameter_path)\n\n        else:\n            print(\n                f\" | > Data parameters found for: {trainer.config.mel_statistics_parameter_path}. Loading mel normalization parameters...\"\n            )\n            statistics = torch.load(trainer.config.mel_statistics_parameter_path)\n            data_mean, data_std, init_transition_prob = (\n                statistics[\"mean\"],\n                statistics[\"std\"],\n                statistics[\"init_transition_prob\"],\n            )\n            print(f\" | > Data parameters loaded with value: {data_mean, data_std, init_transition_prob}\")\n\n        trainer.config.flat_start_params[\"transition_p\"] = (\n            init_transition_prob.item() if torch.is_tensor(init_transition_prob) else init_transition_prob\n        )\n        OverflowUtils.update_flat_start_transition(trainer.model, init_transition_prob)\n        trainer.model.update_mean_std(statistics)\n\n    @torch.inference_mode()\n    def _create_logs(self, batch, outputs, ap):  # pylint: disable=no-self-use, unused-argument\n        alignments, transition_vectors = outputs[\"alignments\"], outputs[\"transition_vectors\"]\n        means = torch.stack(outputs[\"means\"], dim=1)\n\n        figures = {\n            \"alignment\": plot_alignment(alignments[0].exp(), title=\"Forward alignment\", fig_size=(20, 20)),\n            \"log_alignment\": plot_alignment(\n                alignments[0].exp(), title=\"Forward log alignment\", plot_log=True, fig_size=(20, 20)\n            ),\n            \"transition_vectors\": plot_alignment(transition_vectors[0], title=\"Transition vectors\", fig_size=(20, 20)),\n            \"mel_from_most_probable_state\": plot_spectrogram(\n                get_spec_from_most_probable_state(alignments[0], means[0], self.decoder), fig_size=(12, 3)\n            ),\n            \"mel_target\": plot_spectrogram(batch[\"mel_input\"][0], fig_size=(12, 3)),\n        }\n\n        # sample one item from the batch -1 will give the smalles item\n        print(\" | > Synthesising audio from the model...\")\n        inference_output = self.inference(\n            batch[\"text_input\"][-1].unsqueeze(0), aux_input={\"x_lengths\": batch[\"text_lengths\"][-1].unsqueeze(0)}\n        )\n        figures[\"synthesised\"] = plot_spectrogram(inference_output[\"model_outputs\"][0], fig_size=(12, 3))\n\n        states = [p[1] for p in inference_output[\"input_parameters\"][0]]\n        transition_probability_synthesising = [p[2].cpu().numpy() for p in inference_output[\"output_parameters\"][0]]\n\n        for i in range((len(transition_probability_synthesising) // 200) + 1):\n            start = i * 200\n            end = (i + 1) * 200\n            figures[f\"synthesised_transition_probabilities/{i}\"] = plot_transition_probabilities_to_numpy(\n                states[start:end], transition_probability_synthesising[start:end]\n            )\n\n        audio = ap.inv_melspectrogram(inference_output[\"model_outputs\"][0].T.cpu().numpy())\n        return figures, {\"audios\": audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ):  # pylint: disable=unused-argument\n        \"\"\"Log training progress.\"\"\"\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_log(\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int\n    ):  # pylint: disable=unused-argument\n        \"\"\"Compute and log evaluation metrics.\"\"\"\n        # Plot model parameters histograms\n        if isinstance(logger, TensorboardLogger):\n            # I don't know if any other loggers supports this\n            for tag, value in self.named_parameters():\n                tag = tag.replace(\".\", \"/\")\n                logger.writer.add_histogram(tag, value.data.cpu().numpy(), steps)\n\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[1], self.ap.sample_rate)\n        logger.test_figures(steps, outputs[0])\n\n\nclass NLLLoss(nn.Module):\n    \"\"\"Negative log likelihood loss.\"\"\"\n\n    def forward(self, log_prob: torch.Tensor) -> dict:  # pylint: disable=no-self-use\n        \"\"\"Compute the loss.\n\n        Args:\n            logits (Tensor): [B, T, D]\n\n        Returns:\n            Tensor: [1]\n\n        \"\"\"\n        return_dict = {}\n        return_dict[\"loss\"] = -log_prob.mean()\n        return return_dict\n", "TTS/tts/models/forward_tts.py": "from dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\n\nfrom TTS.tts.layers.feed_forward.decoder import Decoder\nfrom TTS.tts.layers.feed_forward.encoder import Encoder\nfrom TTS.tts.layers.generic.aligner import AlignmentNetwork\nfrom TTS.tts.layers.generic.pos_encoding import PositionalEncoding\nfrom TTS.tts.layers.glow_tts.duration_predictor import DurationPredictor\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.helpers import average_over_durations, generate_path, maximum_path, sequence_mask\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_avg_energy, plot_avg_pitch, plot_spectrogram\nfrom TTS.utils.io import load_fsspec\n\n\n@dataclass\nclass ForwardTTSArgs(Coqpit):\n    \"\"\"ForwardTTS Model arguments.\n\n    Args:\n\n        num_chars (int):\n            Number of characters in the vocabulary. Defaults to 100.\n\n        out_channels (int):\n            Number of output channels. Defaults to 80.\n\n        hidden_channels (int):\n            Number of base hidden channels of the model. Defaults to 512.\n\n        use_aligner (bool):\n            Whether to use aligner network to learn the text to speech alignment or use pre-computed durations.\n            If set False, durations should be computed by `TTS/bin/compute_attention_masks.py` and path to the\n            pre-computed durations must be provided to `config.datasets[0].meta_file_attn_mask`. Defaults to True.\n\n        use_pitch (bool):\n            Use pitch predictor to learn the pitch. Defaults to True.\n\n        use_energy (bool):\n            Use energy predictor to learn the energy. Defaults to True.\n\n        duration_predictor_hidden_channels (int):\n            Number of hidden channels in the duration predictor. Defaults to 256.\n\n        duration_predictor_dropout_p (float):\n            Dropout rate for the duration predictor. Defaults to 0.1.\n\n        duration_predictor_kernel_size (int):\n            Kernel size of conv layers in the duration predictor. Defaults to 3.\n\n        pitch_predictor_hidden_channels (int):\n            Number of hidden channels in the pitch predictor. Defaults to 256.\n\n        pitch_predictor_dropout_p (float):\n            Dropout rate for the pitch predictor. Defaults to 0.1.\n\n        pitch_predictor_kernel_size (int):\n            Kernel size of conv layers in the pitch predictor. Defaults to 3.\n\n        pitch_embedding_kernel_size (int):\n            Kernel size of the projection layer in the pitch predictor. Defaults to 3.\n\n        energy_predictor_hidden_channels (int):\n            Number of hidden channels in the energy predictor. Defaults to 256.\n\n        energy_predictor_dropout_p (float):\n            Dropout rate for the energy predictor. Defaults to 0.1.\n\n        energy_predictor_kernel_size (int):\n            Kernel size of conv layers in the energy predictor. Defaults to 3.\n\n        energy_embedding_kernel_size (int):\n            Kernel size of the projection layer in the energy predictor. Defaults to 3.\n\n        positional_encoding (bool):\n            Whether to use positional encoding. Defaults to True.\n\n        positional_encoding_use_scale (bool):\n            Whether to use a learnable scale coeff in the positional encoding. Defaults to True.\n\n        length_scale (int):\n            Length scale that multiplies the predicted durations. Larger values result slower speech. Defaults to 1.0.\n\n        encoder_type (str):\n            Type of the encoder module. One of the encoders available in :class:`TTS.tts.layers.feed_forward.encoder`.\n            Defaults to `fftransformer` as in the paper.\n\n        encoder_params (dict):\n            Parameters of the encoder module. Defaults to ```{\"hidden_channels_ffn\": 1024, \"num_heads\": 1, \"num_layers\": 6, \"dropout_p\": 0.1}```\n\n        decoder_type (str):\n            Type of the decoder module. One of the decoders available in :class:`TTS.tts.layers.feed_forward.decoder`.\n            Defaults to `fftransformer` as in the paper.\n\n        decoder_params (str):\n            Parameters of the decoder module. Defaults to ```{\"hidden_channels_ffn\": 1024, \"num_heads\": 1, \"num_layers\": 6, \"dropout_p\": 0.1}```\n\n        detach_duration_predictor (bool):\n            Detach the input to the duration predictor from the earlier computation graph so that the duraiton loss\n            does not pass to the earlier layers. Defaults to True.\n\n        max_duration (int):\n            Maximum duration accepted by the model. Defaults to 75.\n\n        num_speakers (int):\n            Number of speakers for the speaker embedding layer. Defaults to 0.\n\n        speakers_file (str):\n            Path to the speaker mapping file for the Speaker Manager. Defaults to None.\n\n        speaker_embedding_channels (int):\n            Number of speaker embedding channels. Defaults to 256.\n\n        use_d_vector_file (bool):\n            Enable/Disable the use of d-vectors for multi-speaker training. Defaults to False.\n\n        d_vector_dim (int):\n            Number of d-vector channels. Defaults to 0.\n\n    \"\"\"\n\n    num_chars: int = None\n    out_channels: int = 80\n    hidden_channels: int = 384\n    use_aligner: bool = True\n    # pitch params\n    use_pitch: bool = True\n    pitch_predictor_hidden_channels: int = 256\n    pitch_predictor_kernel_size: int = 3\n    pitch_predictor_dropout_p: float = 0.1\n    pitch_embedding_kernel_size: int = 3\n\n    # energy params\n    use_energy: bool = False\n    energy_predictor_hidden_channels: int = 256\n    energy_predictor_kernel_size: int = 3\n    energy_predictor_dropout_p: float = 0.1\n    energy_embedding_kernel_size: int = 3\n\n    # duration params\n    duration_predictor_hidden_channels: int = 256\n    duration_predictor_kernel_size: int = 3\n    duration_predictor_dropout_p: float = 0.1\n\n    positional_encoding: bool = True\n    poisitonal_encoding_use_scale: bool = True\n    length_scale: int = 1\n    encoder_type: str = \"fftransformer\"\n    encoder_params: dict = field(\n        default_factory=lambda: {\"hidden_channels_ffn\": 1024, \"num_heads\": 1, \"num_layers\": 6, \"dropout_p\": 0.1}\n    )\n    decoder_type: str = \"fftransformer\"\n    decoder_params: dict = field(\n        default_factory=lambda: {\"hidden_channels_ffn\": 1024, \"num_heads\": 1, \"num_layers\": 6, \"dropout_p\": 0.1}\n    )\n    detach_duration_predictor: bool = False\n    max_duration: int = 75\n    num_speakers: int = 1\n    use_speaker_embedding: bool = False\n    speakers_file: str = None\n    use_d_vector_file: bool = False\n    d_vector_dim: int = None\n    d_vector_file: str = None\n\n\nclass ForwardTTS(BaseTTS):\n    \"\"\"General forward TTS model implementation that uses an encoder-decoder architecture with an optional alignment\n    network and a pitch predictor.\n\n    If the alignment network is used, the model learns the text-to-speech alignment\n    from the data instead of using pre-computed durations.\n\n    If the pitch predictor is used, the model trains a pitch predictor that predicts average pitch value for each\n    input character as in the FastPitch model.\n\n    `ForwardTTS` can be configured to one of these architectures,\n\n        - FastPitch\n        - SpeedySpeech\n        - FastSpeech\n        - FastSpeech2 (requires average speech energy predictor)\n\n    Args:\n        config (Coqpit): Model coqpit class.\n        speaker_manager (SpeakerManager): Speaker manager for multi-speaker training. Only used for multi-speaker models.\n            Defaults to None.\n\n    Examples:\n        >>> from TTS.tts.models.fast_pitch import ForwardTTS, ForwardTTSArgs\n        >>> config = ForwardTTSArgs()\n        >>> model = ForwardTTS(config)\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n        self._set_model_args(config)\n\n        self.init_multispeaker(config)\n\n        self.max_duration = self.args.max_duration\n        self.use_aligner = self.args.use_aligner\n        self.use_pitch = self.args.use_pitch\n        self.use_energy = self.args.use_energy\n        self.binary_loss_weight = 0.0\n\n        self.length_scale = (\n            float(self.args.length_scale) if isinstance(self.args.length_scale, int) else self.args.length_scale\n        )\n\n        self.emb = nn.Embedding(self.args.num_chars, self.args.hidden_channels)\n\n        self.encoder = Encoder(\n            self.args.hidden_channels,\n            self.args.hidden_channels,\n            self.args.encoder_type,\n            self.args.encoder_params,\n            self.embedded_speaker_dim,\n        )\n\n        if self.args.positional_encoding:\n            self.pos_encoder = PositionalEncoding(self.args.hidden_channels)\n\n        self.decoder = Decoder(\n            self.args.out_channels,\n            self.args.hidden_channels,\n            self.args.decoder_type,\n            self.args.decoder_params,\n        )\n\n        self.duration_predictor = DurationPredictor(\n            self.args.hidden_channels,\n            self.args.duration_predictor_hidden_channels,\n            self.args.duration_predictor_kernel_size,\n            self.args.duration_predictor_dropout_p,\n        )\n\n        if self.args.use_pitch:\n            self.pitch_predictor = DurationPredictor(\n                self.args.hidden_channels,\n                self.args.pitch_predictor_hidden_channels,\n                self.args.pitch_predictor_kernel_size,\n                self.args.pitch_predictor_dropout_p,\n            )\n            self.pitch_emb = nn.Conv1d(\n                1,\n                self.args.hidden_channels,\n                kernel_size=self.args.pitch_embedding_kernel_size,\n                padding=int((self.args.pitch_embedding_kernel_size - 1) / 2),\n            )\n\n        if self.args.use_energy:\n            self.energy_predictor = DurationPredictor(\n                self.args.hidden_channels,\n                self.args.energy_predictor_hidden_channels,\n                self.args.energy_predictor_kernel_size,\n                self.args.energy_predictor_dropout_p,\n            )\n            self.energy_emb = nn.Conv1d(\n                1,\n                self.args.hidden_channels,\n                kernel_size=self.args.energy_embedding_kernel_size,\n                padding=int((self.args.energy_embedding_kernel_size - 1) / 2),\n            )\n\n        if self.args.use_aligner:\n            self.aligner = AlignmentNetwork(\n                in_query_channels=self.args.out_channels, in_key_channels=self.args.hidden_channels\n            )\n\n    def init_multispeaker(self, config: Coqpit):\n        \"\"\"Init for multi-speaker training.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        self.embedded_speaker_dim = 0\n        # init speaker manager\n        if self.speaker_manager is None and (config.use_d_vector_file or config.use_speaker_embedding):\n            raise ValueError(\n                \" > SpeakerManager is not provided. You must provide the SpeakerManager before initializing a multi-speaker model.\"\n            )\n        # set number of speakers\n        if self.speaker_manager is not None:\n            self.num_speakers = self.speaker_manager.num_speakers\n        # init d-vector embedding\n        if config.use_d_vector_file:\n            self.embedded_speaker_dim = config.d_vector_dim\n            if self.args.d_vector_dim != self.args.hidden_channels:\n                #self.proj_g = nn.Conv1d(self.args.d_vector_dim, self.args.hidden_channels, 1)\n                self.proj_g = nn.Linear(in_features=self.args.d_vector_dim, out_features=self.args.hidden_channels)\n        # init speaker embedding layer\n        if config.use_speaker_embedding and not config.use_d_vector_file:\n            print(\" > Init speaker_embedding layer.\")\n            self.emb_g = nn.Embedding(self.num_speakers, self.args.hidden_channels)\n            nn.init.uniform_(self.emb_g.weight, -0.1, 0.1)\n\n    @staticmethod\n    def generate_attn(dr, x_mask, y_mask=None):\n        \"\"\"Generate an attention mask from the durations.\n\n        Shapes\n           - dr: :math:`(B, T_{en})`\n           - x_mask: :math:`(B, T_{en})`\n           - y_mask: :math:`(B, T_{de})`\n        \"\"\"\n        # compute decode mask from the durations\n        if y_mask is None:\n            y_lengths = dr.sum(1).long()\n            y_lengths[y_lengths < 1] = 1\n            y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(dr.dtype)\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        attn = generate_path(dr, attn_mask.squeeze(1)).to(dr.dtype)\n        return attn\n\n    def expand_encoder_outputs(self, en, dr, x_mask, y_mask):\n        \"\"\"Generate attention alignment map from durations and\n        expand encoder outputs\n\n        Shapes:\n            - en: :math:`(B, D_{en}, T_{en})`\n            - dr: :math:`(B, T_{en})`\n            - x_mask: :math:`(B, T_{en})`\n            - y_mask: :math:`(B, T_{de})`\n\n        Examples::\n\n            encoder output: [a,b,c,d]\n            durations: [1, 3, 2, 1]\n\n            expanded: [a, b, b, b, c, c, d]\n            attention map: [[0, 0, 0, 0, 0, 0, 1],\n                            [0, 0, 0, 0, 1, 1, 0],\n                            [0, 1, 1, 1, 0, 0, 0],\n                            [1, 0, 0, 0, 0, 0, 0]]\n        \"\"\"\n        attn = self.generate_attn(dr, x_mask, y_mask)\n        o_en_ex = torch.matmul(attn.squeeze(1).transpose(1, 2).to(en.dtype), en.transpose(1, 2)).transpose(1, 2)\n        return o_en_ex, attn\n\n    def format_durations(self, o_dr_log, x_mask):\n        \"\"\"Format predicted durations.\n        1. Convert to linear scale from log scale\n        2. Apply the length scale for speed adjustment\n        3. Apply masking.\n        4. Cast 0 durations to 1.\n        5. Round the duration values.\n\n        Args:\n            o_dr_log: Log scale durations.\n            x_mask: Input text mask.\n\n        Shapes:\n            - o_dr_log: :math:`(B, T_{de})`\n            - x_mask: :math:`(B, T_{en})`\n        \"\"\"\n        o_dr = (torch.exp(o_dr_log) - 1) * x_mask * self.length_scale\n        o_dr[o_dr < 1] = 1.0\n        o_dr = torch.round(o_dr)\n        return o_dr\n\n    def _forward_encoder(\n        self, x: torch.LongTensor, x_mask: torch.FloatTensor, g: torch.FloatTensor = None\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Encoding forward pass.\n\n        1. Embed speaker IDs if multi-speaker mode.\n        2. Embed character sequences.\n        3. Run the encoder network.\n        4. Sum encoder outputs and speaker embeddings\n\n        Args:\n            x (torch.LongTensor): Input sequence IDs.\n            x_mask (torch.FloatTensor): Input squence mask.\n            g (torch.FloatTensor, optional): Conditioning vectors. In general speaker embeddings. Defaults to None.\n\n        Returns:\n            Tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n                encoder output, encoder output for the duration predictor, input sequence mask, speaker embeddings,\n                character embeddings\n\n        Shapes:\n            - x: :math:`(B, T_{en})`\n            - x_mask: :math:`(B, 1, T_{en})`\n            - g: :math:`(B, C)`\n        \"\"\"\n        if hasattr(self, \"emb_g\"):\n            g = g.type(torch.LongTensor)\n            g = self.emb_g(g)  # [B, C, 1]\n        if g is not None:\n            g = g.unsqueeze(-1)\n        # [B, T, C]\n        x_emb = self.emb(x)\n        # encoder pass\n\t#o_en = self.encoder(torch.transpose(x_emb, 1, -1), x_mask)\n        o_en = self.encoder(torch.transpose(x_emb, 1, -1), x_mask, g)\n        # speaker conditioning\n        # TODO: try different ways of conditioning\n        if g is not None: \n            if hasattr(self, \"proj_g\"):\n                g = self.proj_g(g.view(g.shape[0], -1)).unsqueeze(-1)            \n            o_en = o_en + g\n        return o_en, x_mask, g, x_emb\n\n    def _forward_decoder(\n        self,\n        o_en: torch.FloatTensor,\n        dr: torch.IntTensor,\n        x_mask: torch.FloatTensor,\n        y_lengths: torch.IntTensor,\n        g: torch.FloatTensor,\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Decoding forward pass.\n\n        1. Compute the decoder output mask\n        2. Expand encoder output with the durations.\n        3. Apply position encoding.\n        4. Add speaker embeddings if multi-speaker mode.\n        5. Run the decoder.\n\n        Args:\n            o_en (torch.FloatTensor): Encoder output.\n            dr (torch.IntTensor): Ground truth durations or alignment network durations.\n            x_mask (torch.IntTensor): Input sequence mask.\n            y_lengths (torch.IntTensor): Output sequence lengths.\n            g (torch.FloatTensor): Conditioning vectors. In general speaker embeddings.\n\n        Returns:\n            Tuple[torch.FloatTensor, torch.FloatTensor]: Decoder output, attention map from durations.\n        \"\"\"\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(o_en.dtype)\n        # expand o_en with durations\n        o_en_ex, attn = self.expand_encoder_outputs(o_en, dr, x_mask, y_mask)\n        # positional encoding\n        if hasattr(self, \"pos_encoder\"):\n            o_en_ex = self.pos_encoder(o_en_ex, y_mask)\n        # decoder pass\n        o_de = self.decoder(o_en_ex, y_mask, g=g)\n        return o_de.transpose(1, 2), attn.transpose(1, 2)\n\n    def _forward_pitch_predictor(\n        self,\n        o_en: torch.FloatTensor,\n        x_mask: torch.IntTensor,\n        pitch: torch.FloatTensor = None,\n        dr: torch.IntTensor = None,\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Pitch predictor forward pass.\n\n        1. Predict pitch from encoder outputs.\n        2. In training - Compute average pitch values for each input character from the ground truth pitch values.\n        3. Embed average pitch values.\n\n        Args:\n            o_en (torch.FloatTensor): Encoder output.\n            x_mask (torch.IntTensor): Input sequence mask.\n            pitch (torch.FloatTensor, optional): Ground truth pitch values. Defaults to None.\n            dr (torch.IntTensor, optional): Ground truth durations. Defaults to None.\n\n        Returns:\n            Tuple[torch.FloatTensor, torch.FloatTensor]: Pitch embedding, pitch prediction.\n\n        Shapes:\n            - o_en: :math:`(B, C, T_{en})`\n            - x_mask: :math:`(B, 1, T_{en})`\n            - pitch: :math:`(B, 1, T_{de})`\n            - dr: :math:`(B, T_{en})`\n        \"\"\"\n        o_pitch = self.pitch_predictor(o_en, x_mask)\n        if pitch is not None:\n            avg_pitch = average_over_durations(pitch, dr)\n            o_pitch_emb = self.pitch_emb(avg_pitch)\n            return o_pitch_emb, o_pitch, avg_pitch\n        o_pitch_emb = self.pitch_emb(o_pitch)\n        return o_pitch_emb, o_pitch\n\n    def _forward_energy_predictor(\n        self,\n        o_en: torch.FloatTensor,\n        x_mask: torch.IntTensor,\n        energy: torch.FloatTensor = None,\n        dr: torch.IntTensor = None,\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Energy predictor forward pass.\n\n        1. Predict energy from encoder outputs.\n        2. In training - Compute average pitch values for each input character from the ground truth pitch values.\n        3. Embed average energy values.\n\n        Args:\n            o_en (torch.FloatTensor): Encoder output.\n            x_mask (torch.IntTensor): Input sequence mask.\n            energy (torch.FloatTensor, optional): Ground truth energy values. Defaults to None.\n            dr (torch.IntTensor, optional): Ground truth durations. Defaults to None.\n\n        Returns:\n            Tuple[torch.FloatTensor, torch.FloatTensor]: Energy embedding, energy prediction.\n\n        Shapes:\n            - o_en: :math:`(B, C, T_{en})`\n            - x_mask: :math:`(B, 1, T_{en})`\n            - pitch: :math:`(B, 1, T_{de})`\n            - dr: :math:`(B, T_{en})`\n        \"\"\"\n        o_energy = self.energy_predictor(o_en, x_mask)\n        if energy is not None:\n            avg_energy = average_over_durations(energy, dr)\n            o_energy_emb = self.energy_emb(avg_energy)\n            return o_energy_emb, o_energy, avg_energy\n        o_energy_emb = self.energy_emb(o_energy)\n        return o_energy_emb, o_energy\n\n    def _forward_aligner(\n        self, x: torch.FloatTensor, y: torch.FloatTensor, x_mask: torch.IntTensor, y_mask: torch.IntTensor\n    ) -> Tuple[torch.IntTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Aligner forward pass.\n\n        1. Compute a mask to apply to the attention map.\n        2. Run the alignment network.\n        3. Apply MAS to compute the hard alignment map.\n        4. Compute the durations from the hard alignment map.\n\n        Args:\n            x (torch.FloatTensor): Input sequence.\n            y (torch.FloatTensor): Output sequence.\n            x_mask (torch.IntTensor): Input sequence mask.\n            y_mask (torch.IntTensor): Output sequence mask.\n\n        Returns:\n            Tuple[torch.IntTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n                Durations from the hard alignment map, soft alignment potentials, log scale alignment potentials,\n                hard alignment map.\n\n        Shapes:\n            - x: :math:`[B, T_en, C_en]`\n            - y: :math:`[B, T_de, C_de]`\n            - x_mask: :math:`[B, 1, T_en]`\n            - y_mask: :math:`[B, 1, T_de]`\n\n            - o_alignment_dur: :math:`[B, T_en]`\n            - alignment_soft: :math:`[B, T_en, T_de]`\n            - alignment_logprob: :math:`[B, 1, T_de, T_en]`\n            - alignment_mas: :math:`[B, T_en, T_de]`\n        \"\"\"\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        alignment_soft, alignment_logprob = self.aligner(y.transpose(1, 2), x.transpose(1, 2), x_mask, None)\n        alignment_mas = maximum_path(\n            alignment_soft.squeeze(1).transpose(1, 2).contiguous(), attn_mask.squeeze(1).contiguous()\n        )\n        o_alignment_dur = torch.sum(alignment_mas, -1).int()\n        alignment_soft = alignment_soft.squeeze(1).transpose(1, 2)\n        return o_alignment_dur, alignment_soft, alignment_logprob, alignment_mas\n\n    def _set_speaker_input(self, aux_input: Dict):\n        d_vectors = aux_input.get(\"d_vectors\", None)\n        speaker_ids = aux_input.get(\"speaker_ids\", None)\n\n        if d_vectors is not None and speaker_ids is not None:\n            raise ValueError(\"[!] Cannot use d-vectors and speaker-ids together.\")\n\n        if speaker_ids is not None and not hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Cannot use speaker-ids without enabling speaker embedding.\")\n\n        g = speaker_ids if speaker_ids is not None else d_vectors\n        return g\n\n    def forward(\n        self,\n        x: torch.LongTensor,\n        x_lengths: torch.LongTensor,\n        y_lengths: torch.LongTensor,\n        y: torch.FloatTensor = None,\n        dr: torch.IntTensor = None,\n        pitch: torch.FloatTensor = None,\n        energy: torch.FloatTensor = None,\n        aux_input: Dict = {\"d_vectors\": None, \"speaker_ids\": None},  # pylint: disable=unused-argument\n    ) -> Dict:\n        \"\"\"Model's forward pass.\n\n        Args:\n            x (torch.LongTensor): Input character sequences.\n            x_lengths (torch.LongTensor): Input sequence lengths.\n            y_lengths (torch.LongTensor): Output sequnce lengths. Defaults to None.\n            y (torch.FloatTensor): Spectrogram frames. Only used when the alignment network is on. Defaults to None.\n            dr (torch.IntTensor): Character durations over the spectrogram frames. Only used when the alignment network is off. Defaults to None.\n            pitch (torch.FloatTensor): Pitch values for each spectrogram frame. Only used when the pitch predictor is on. Defaults to None.\n            energy (torch.FloatTensor): energy values for each spectrogram frame. Only used when the energy predictor is on. Defaults to None.\n            aux_input (Dict): Auxiliary model inputs for multi-speaker training. Defaults to `{\"d_vectors\": 0, \"speaker_ids\": None}`.\n\n        Shapes:\n            - x: :math:`[B, T_max]`\n            - x_lengths: :math:`[B]`\n            - y_lengths: :math:`[B]`\n            - y: :math:`[B, T_max2]`\n            - dr: :math:`[B, T_max]`\n            - g: :math:`[B, C]`\n            - pitch: :math:`[B, 1, T]`\n        \"\"\"\n        g = self._set_speaker_input(aux_input)\n        # compute sequence masks\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).float()\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.shape[1]), 1).float()\n        # encoder pass\n        o_en, x_mask, g, x_emb = self._forward_encoder(x, x_mask, g)\n        # duration predictor pass\n        if self.args.detach_duration_predictor:\n            o_dr_log = self.duration_predictor(o_en.detach(), x_mask)\n        else:\n            o_dr_log = self.duration_predictor(o_en, x_mask)\n        o_dr = torch.clamp(torch.exp(o_dr_log) - 1, 0, self.max_duration)\n        # generate attn mask from predicted durations\n        o_attn = self.generate_attn(o_dr.squeeze(1), x_mask)\n        # aligner\n        o_alignment_dur = None\n        alignment_soft = None\n        alignment_logprob = None\n        alignment_mas = None\n        if self.use_aligner:\n            o_alignment_dur, alignment_soft, alignment_logprob, alignment_mas = self._forward_aligner(\n                x_emb, y, x_mask, y_mask\n            )\n            alignment_soft = alignment_soft.transpose(1, 2)\n            alignment_mas = alignment_mas.transpose(1, 2)\n            dr = o_alignment_dur\n        # pitch predictor pass\n        o_pitch = None\n        avg_pitch = None\n        if self.args.use_pitch:\n            o_pitch_emb, o_pitch, avg_pitch = self._forward_pitch_predictor(o_en, x_mask, pitch, dr)\n            o_en = o_en + o_pitch_emb\n        # energy predictor pass\n        o_energy = None\n        avg_energy = None\n        if self.args.use_energy:\n            o_energy_emb, o_energy, avg_energy = self._forward_energy_predictor(o_en, x_mask, energy, dr)\n            o_en = o_en + o_energy_emb\n        # decoder pass\n        o_de, attn = self._forward_decoder(\n            o_en, dr, x_mask, y_lengths, g=None\n        )  # TODO: maybe pass speaker embedding (g) too\n        outputs = {\n            \"model_outputs\": o_de,  # [B, T, C]\n            \"durations_log\": o_dr_log.squeeze(1),  # [B, T]\n            \"durations\": o_dr.squeeze(1),  # [B, T]\n            \"attn_durations\": o_attn,  # for visualization [B, T_en, T_de']\n            \"pitch_avg\": o_pitch,\n            \"pitch_avg_gt\": avg_pitch,\n            \"energy_avg\": o_energy,\n            \"energy_avg_gt\": avg_energy,\n            \"alignments\": attn,  # [B, T_de, T_en]\n            \"alignment_soft\": alignment_soft,\n            \"alignment_mas\": alignment_mas,\n            \"o_alignment_dur\": o_alignment_dur,\n            \"alignment_logprob\": alignment_logprob,\n            \"x_mask\": x_mask,\n            \"y_mask\": y_mask,\n        }\n        return outputs\n\n    @torch.no_grad()\n    def inference(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):  # pylint: disable=unused-argument\n        \"\"\"Model's inference pass.\n\n        Args:\n            x (torch.LongTensor): Input character sequence.\n            aux_input (Dict): Auxiliary model inputs. Defaults to `{\"d_vectors\": None, \"speaker_ids\": None}`.\n\n        Shapes:\n            - x: [B, T_max]\n            - x_lengths: [B]\n            - g: [B, C]\n        \"\"\"\n        g = self._set_speaker_input(aux_input)\n        x_lengths = torch.tensor(x.shape[1:2]).to(x.device)\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.shape[1]), 1).to(x.dtype).float()\n        # encoder pass\n        o_en, x_mask, g, _ = self._forward_encoder(x, x_mask, g)\n        # duration predictor pass\n        o_dr_log = self.duration_predictor(o_en.squeeze(), x_mask)\n        o_dr = self.format_durations(o_dr_log, x_mask).squeeze(1)\n        y_lengths = o_dr.sum(1)\n\n        # pitch predictor pass\n        o_pitch = None\n        if self.args.use_pitch:\n            o_pitch_emb, o_pitch = self._forward_pitch_predictor(o_en, x_mask)\n            o_en = o_en + o_pitch_emb\n        # energy predictor pass\n        o_energy = None\n        if self.args.use_energy:\n            o_energy_emb, o_energy = self._forward_energy_predictor(o_en, x_mask)\n            o_en = o_en + o_energy_emb\n        # decoder pass\n        o_de, attn = self._forward_decoder(o_en, o_dr, x_mask, y_lengths, g=None)\n        outputs = {\n            \"model_outputs\": o_de,\n            \"alignments\": attn,\n            \"pitch\": o_pitch,\n            \"energy\": o_energy,\n            \"durations_log\": o_dr_log,\n        }\n        return outputs\n\n    def train_step(self, batch: dict, criterion: nn.Module):\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        pitch = batch[\"pitch\"] if self.args.use_pitch else None\n        energy = batch[\"energy\"] if self.args.use_energy else None\n        d_vectors = batch[\"d_vectors\"]\n        speaker_ids = batch[\"speaker_ids\"]\n        durations = batch[\"durations\"]\n        aux_input = {\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids}\n\n        # forward pass\n        outputs = self.forward(\n            text_input,\n            text_lengths,\n            mel_lengths,\n            y=mel_input,\n            dr=durations,\n            pitch=pitch,\n            energy=energy,\n            aux_input=aux_input,\n        )\n        # use aligner's output as the duration target\n        if self.use_aligner:\n            durations = outputs[\"o_alignment_dur\"]\n        # use float32 in AMP\n        with autocast(enabled=False):\n            # compute loss\n            loss_dict = criterion(\n                decoder_output=outputs[\"model_outputs\"],\n                decoder_target=mel_input,\n                decoder_output_lens=mel_lengths,\n                dur_output=outputs[\"durations_log\"],\n                dur_target=durations,\n                pitch_output=outputs[\"pitch_avg\"] if self.use_pitch else None,\n                pitch_target=outputs[\"pitch_avg_gt\"] if self.use_pitch else None,\n                energy_output=outputs[\"energy_avg\"] if self.use_energy else None,\n                energy_target=outputs[\"energy_avg_gt\"] if self.use_energy else None,\n                input_lens=text_lengths,\n                alignment_logprob=outputs[\"alignment_logprob\"] if self.use_aligner else None,\n                alignment_soft=outputs[\"alignment_soft\"],\n                alignment_hard=outputs[\"alignment_mas\"],\n                binary_loss_weight=self.binary_loss_weight,\n            )\n            # compute duration error\n            durations_pred = outputs[\"durations\"]\n            duration_error = torch.abs(durations - durations_pred).sum() / text_lengths.sum()\n            loss_dict[\"duration_error\"] = duration_error\n\n        return outputs, loss_dict\n\n    def _create_logs(self, batch, outputs, ap):\n        \"\"\"Create common logger outputs.\"\"\"\n        model_outputs = outputs[\"model_outputs\"]\n        alignments = outputs[\"alignments\"]\n        mel_input = batch[\"mel_input\"]\n\n        pred_spec = model_outputs[0].data.cpu().numpy()\n        gt_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"prediction\": plot_spectrogram(pred_spec, ap, output_fig=False),\n            \"ground_truth\": plot_spectrogram(gt_spec, ap, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        # plot pitch figures\n        if self.args.use_pitch:\n            pitch_avg = abs(outputs[\"pitch_avg_gt\"][0, 0].data.cpu().numpy())\n            pitch_avg_hat = abs(outputs[\"pitch_avg\"][0, 0].data.cpu().numpy())\n            chars = self.tokenizer.decode(batch[\"text_input\"][0].data.cpu().numpy())\n            pitch_figures = {\n                \"pitch_ground_truth\": plot_avg_pitch(pitch_avg, chars, output_fig=False),\n                \"pitch_avg_predicted\": plot_avg_pitch(pitch_avg_hat, chars, output_fig=False),\n            }\n            figures.update(pitch_figures)\n\n        # plot energy figures\n        if self.args.use_energy:\n            energy_avg = abs(outputs[\"energy_avg_gt\"][0, 0].data.cpu().numpy())\n            energy_avg_hat = abs(outputs[\"energy_avg\"][0, 0].data.cpu().numpy())\n            chars = self.tokenizer.decode(batch[\"text_input\"][0].data.cpu().numpy())\n            energy_figures = {\n                \"energy_ground_truth\": plot_avg_energy(energy_avg, chars, output_fig=False),\n                \"energy_avg_predicted\": plot_avg_energy(energy_avg_hat, chars, output_fig=False),\n            }\n            figures.update(energy_figures)\n\n        # plot the attention mask computed from the predicted durations\n        if \"attn_durations\" in outputs:\n            alignments_hat = outputs[\"attn_durations\"][0].data.cpu().numpy()\n            figures[\"alignment_hat\"] = plot_alignment(alignments_hat.T, output_fig=False)\n\n        # Sample audio\n        train_audio = ap.inv_melspectrogram(pred_spec.T)\n        return figures, {\"audio\": train_audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ) -> None:  # pylint: disable=no-self-use\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_step(self, batch: dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def get_criterion(self):\n        from TTS.tts.layers.losses import ForwardTTSLoss  # pylint: disable=import-outside-toplevel\n\n        return ForwardTTSLoss(self.config)\n\n    def on_train_step_start(self, trainer):\n        \"\"\"Schedule binary loss weight.\"\"\"\n        self.binary_loss_weight = min(trainer.epochs_done / self.config.binary_loss_warmup_epochs, 1.0) * 1.0\n\n    @staticmethod\n    def init_from_config(config: \"ForwardTTSConfig\", samples: Union[List[List], List[Dict]] = None):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (ForwardTTSConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return ForwardTTS(new_config, ap, tokenizer, speaker_manager)\n", "TTS/tts/models/xtts.py": "import os\nfrom dataclasses import dataclass\n\nimport librosa\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom coqpit import Coqpit\n\nfrom TTS.tts.layers.xtts.gpt import GPT\nfrom TTS.tts.layers.xtts.hifigan_decoder import HifiDecoder\nfrom TTS.tts.layers.xtts.stream_generator import init_stream_support\nfrom TTS.tts.layers.xtts.tokenizer import VoiceBpeTokenizer, split_sentence\nfrom TTS.tts.layers.xtts.xtts_manager import SpeakerManager, LanguageManager\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.utils.io import load_fsspec\n\ninit_stream_support()\n\n\ndef wav_to_mel_cloning(\n    wav,\n    mel_norms_file=\"../experiments/clips_mel_norms.pth\",\n    mel_norms=None,\n    device=torch.device(\"cpu\"),\n    n_fft=4096,\n    hop_length=1024,\n    win_length=4096,\n    power=2,\n    normalized=False,\n    sample_rate=22050,\n    f_min=0,\n    f_max=8000,\n    n_mels=80,\n):\n    \"\"\"\n    Convert waveform to mel-spectrogram with hard-coded parameters for cloning.\n\n    Args:\n        wav (torch.Tensor): Input waveform tensor.\n        mel_norms_file (str): Path to mel-spectrogram normalization file.\n        mel_norms (torch.Tensor): Mel-spectrogram normalization tensor.\n        device (torch.device): Device to use for computation.\n\n    Returns:\n        torch.Tensor: Mel-spectrogram tensor.\n    \"\"\"\n    mel_stft = torchaudio.transforms.MelSpectrogram(\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        power=power,\n        normalized=normalized,\n        sample_rate=sample_rate,\n        f_min=f_min,\n        f_max=f_max,\n        n_mels=n_mels,\n        norm=\"slaney\",\n    ).to(device)\n    wav = wav.to(device)\n    mel = mel_stft(wav)\n    mel = torch.log(torch.clamp(mel, min=1e-5))\n    if mel_norms is None:\n        mel_norms = torch.load(mel_norms_file, map_location=device)\n    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)\n    return mel\n\n\ndef load_audio(audiopath, sampling_rate):\n    # better load setting following: https://github.com/faroit/python_audio_loading_benchmark\n\n    # torchaudio should chose proper backend to load audio depending on platform\n    audio, lsr = torchaudio.load(audiopath)\n\n    # stereo to mono if needed\n    if audio.size(0) != 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n\n    if lsr != sampling_rate:\n        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)\n\n    # Check some assumptions about audio range. This should be automatically fixed in load_wav_to_torch, but might not be in some edge cases, where we should squawk.\n    # '10' is arbitrarily chosen since it seems like audio will often \"overdrive\" the [-1,1] bounds.\n    if torch.any(audio > 10) or not torch.any(audio < 0):\n        print(f\"Error with {audiopath}. Max={audio.max()} min={audio.min()}\")\n    # clip audio invalid values\n    audio.clip_(-1, 1)\n    return audio\n\n\ndef pad_or_truncate(t, length):\n    \"\"\"\n    Ensure a given tensor t has a specified sequence length by either padding it with zeros or clipping it.\n\n    Args:\n        t (torch.Tensor): The input tensor to be padded or truncated.\n        length (int): The desired length of the tensor.\n\n    Returns:\n        torch.Tensor: The padded or truncated tensor.\n    \"\"\"\n    tp = t[..., :length]\n    if t.shape[-1] == length:\n        tp = t\n    elif t.shape[-1] < length:\n        tp = F.pad(t, (0, length - t.shape[-1]))\n    return tp\n\n\n@dataclass\nclass XttsAudioConfig(Coqpit):\n    \"\"\"\n    Configuration class for audio-related parameters in the XTTS model.\n\n    Args:\n        sample_rate (int): The sample rate in which the GPT operates.\n        output_sample_rate (int): The sample rate of the output audio waveform.\n    \"\"\"\n\n    sample_rate: int = 22050\n    output_sample_rate: int = 24000\n\n\n@dataclass\nclass XttsArgs(Coqpit):\n    \"\"\"A dataclass to represent XTTS model arguments that define the model structure.\n\n    Args:\n        gpt_batch_size (int): The size of the auto-regressive batch.\n        enable_redaction (bool, optional): Whether to enable redaction. Defaults to True.\n        kv_cache (bool, optional): Whether to use the kv_cache. Defaults to True.\n        gpt_checkpoint (str, optional): The checkpoint for the autoregressive model. Defaults to None.\n        clvp_checkpoint (str, optional): The checkpoint for the ConditionalLatentVariablePerseq model. Defaults to None.\n        decoder_checkpoint (str, optional): The checkpoint for the DiffTTS model. Defaults to None.\n        num_chars (int, optional): The maximum number of characters to generate. Defaults to 255.\n\n        For GPT model:\n        gpt_max_audio_tokens (int, optional): The maximum mel tokens for the autoregressive model. Defaults to 604.\n        gpt_max_text_tokens (int, optional): The maximum text tokens for the autoregressive model. Defaults to 402.\n        gpt_max_prompt_tokens (int, optional): The maximum prompt tokens or the autoregressive model. Defaults to 70.\n        gpt_layers (int, optional): The number of layers for the autoregressive model. Defaults to 30.\n        gpt_n_model_channels (int, optional): The model dimension for the autoregressive model. Defaults to 1024.\n        gpt_n_heads (int, optional): The number of heads for the autoregressive model. Defaults to 16.\n        gpt_number_text_tokens (int, optional): The number of text tokens for the autoregressive model. Defaults to 255.\n        gpt_start_text_token (int, optional): The start text token for the autoregressive model. Defaults to 255.\n        gpt_checkpointing (bool, optional): Whether to use checkpointing for the autoregressive model. Defaults to False.\n        gpt_train_solo_embeddings (bool, optional): Whether to train embeddings for the autoregressive model. Defaults to False.\n        gpt_code_stride_len (int, optional): The hop_size of dvae and consequently of the gpt output. Defaults to 1024.\n        gpt_use_masking_gt_prompt_approach (bool, optional):  If True, it will use ground truth as prompt and it will mask the loss to avoid repetition. Defaults to True.\n        gpt_use_perceiver_resampler (bool, optional):  If True, it will use perceiver resampler from flamingo paper - https://arxiv.org/abs/2204.14198. Defaults to False.\n    \"\"\"\n\n    gpt_batch_size: int = 1\n    enable_redaction: bool = False\n    kv_cache: bool = True\n    gpt_checkpoint: str = None\n    clvp_checkpoint: str = None\n    decoder_checkpoint: str = None\n    num_chars: int = 255\n\n    # XTTS GPT Encoder params\n    tokenizer_file: str = \"\"\n    gpt_max_audio_tokens: int = 605\n    gpt_max_text_tokens: int = 402\n    gpt_max_prompt_tokens: int = 70\n    gpt_layers: int = 30\n    gpt_n_model_channels: int = 1024\n    gpt_n_heads: int = 16\n    gpt_number_text_tokens: int = None\n    gpt_start_text_token: int = None\n    gpt_stop_text_token: int = None\n    gpt_num_audio_tokens: int = 8194\n    gpt_start_audio_token: int = 8192\n    gpt_stop_audio_token: int = 8193\n    gpt_code_stride_len: int = 1024\n    gpt_use_masking_gt_prompt_approach: bool = True\n    gpt_use_perceiver_resampler: bool = False\n\n    # HifiGAN Decoder params\n    input_sample_rate: int = 22050\n    output_sample_rate: int = 24000\n    output_hop_length: int = 256\n    decoder_input_dim: int = 1024\n    d_vector_dim: int = 512\n    cond_d_vector_in_each_upsampling_layer: bool = True\n\n    # constants\n    duration_const: int = 102400\n\n\nclass Xtts(BaseTTS):\n    \"\"\"\u24cdTTS model implementation.\n\n    \u2757 Currently it only supports inference.\n\n    Examples:\n        >>> from TTS.tts.configs.xtts_config import XttsConfig\n        >>> from TTS.tts.models.xtts import Xtts\n        >>> config = XttsConfig()\n        >>> model = Xtts.inif_from_config(config)\n        >>> model.load_checkpoint(config, checkpoint_dir=\"paths/to/models_dir/\", eval=True)\n    \"\"\"\n\n    def __init__(self, config: Coqpit):\n        super().__init__(config, ap=None, tokenizer=None)\n        self.mel_stats_path = None\n        self.config = config\n        self.gpt_checkpoint = self.args.gpt_checkpoint\n        self.decoder_checkpoint = self.args.decoder_checkpoint  # TODO: check if this is even needed\n        self.models_dir = config.model_dir\n        self.gpt_batch_size = self.args.gpt_batch_size\n\n        self.tokenizer = VoiceBpeTokenizer()\n        self.gpt = None\n        self.init_models()\n        self.register_buffer(\"mel_stats\", torch.ones(80))\n\n    def init_models(self):\n        \"\"\"Initialize the models. We do it here since we need to load the tokenizer first.\"\"\"\n        if self.tokenizer.tokenizer is not None:\n            self.args.gpt_number_text_tokens = self.tokenizer.get_number_tokens()\n            self.args.gpt_start_text_token = self.tokenizer.tokenizer.token_to_id(\"[START]\")\n            self.args.gpt_stop_text_token = self.tokenizer.tokenizer.token_to_id(\"[STOP]\")\n\n        if self.args.gpt_number_text_tokens:\n            self.gpt = GPT(\n                layers=self.args.gpt_layers,\n                model_dim=self.args.gpt_n_model_channels,\n                start_text_token=self.args.gpt_start_text_token,\n                stop_text_token=self.args.gpt_stop_text_token,\n                heads=self.args.gpt_n_heads,\n                max_text_tokens=self.args.gpt_max_text_tokens,\n                max_mel_tokens=self.args.gpt_max_audio_tokens,\n                max_prompt_tokens=self.args.gpt_max_prompt_tokens,\n                number_text_tokens=self.args.gpt_number_text_tokens,\n                num_audio_tokens=self.args.gpt_num_audio_tokens,\n                start_audio_token=self.args.gpt_start_audio_token,\n                stop_audio_token=self.args.gpt_stop_audio_token,\n                use_perceiver_resampler=self.args.gpt_use_perceiver_resampler,\n                code_stride_len=self.args.gpt_code_stride_len,\n            )\n\n        self.hifigan_decoder = HifiDecoder(\n            input_sample_rate=self.args.input_sample_rate,\n            output_sample_rate=self.args.output_sample_rate,\n            output_hop_length=self.args.output_hop_length,\n            ar_mel_length_compression=self.args.gpt_code_stride_len,\n            decoder_input_dim=self.args.decoder_input_dim,\n            d_vector_dim=self.args.d_vector_dim,\n            cond_d_vector_in_each_upsampling_layer=self.args.cond_d_vector_in_each_upsampling_layer,\n        )\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @torch.inference_mode()\n    def get_gpt_cond_latents(self, audio, sr, length: int = 30, chunk_length: int = 6):\n        \"\"\"Compute the conditioning latents for the GPT model from the given audio.\n\n        Args:\n            audio (tensor): audio tensor.\n            sr (int): Sample rate of the audio.\n            length (int): Length of the audio in seconds. If < 0, use the whole audio. Defaults to 30.\n            chunk_length (int): Length of the audio chunks in seconds. When `length == chunk_length`, the whole audio\n                is being used without chunking. It must be < `length`. Defaults to 6.\n        \"\"\"\n        if sr != 22050:\n            audio = torchaudio.functional.resample(audio, sr, 22050)\n        if length > 0:\n            audio = audio[:, : 22050 * length]\n        if self.args.gpt_use_perceiver_resampler:\n            style_embs = []\n            for i in range(0, audio.shape[1], 22050 * chunk_length):\n                audio_chunk = audio[:, i : i + 22050 * chunk_length]\n\n                # if the chunk is too short ignore it \n                if audio_chunk.size(-1) < 22050 * 0.33:\n                    continue\n\n                mel_chunk = wav_to_mel_cloning(\n                    audio_chunk,\n                    mel_norms=self.mel_stats.cpu(),\n                    n_fft=2048,\n                    hop_length=256,\n                    win_length=1024,\n                    power=2,\n                    normalized=False,\n                    sample_rate=22050,\n                    f_min=0,\n                    f_max=8000,\n                    n_mels=80,\n                )\n                style_emb = self.gpt.get_style_emb(mel_chunk.to(self.device), None)\n                style_embs.append(style_emb)\n\n            # mean style embedding\n            cond_latent = torch.stack(style_embs).mean(dim=0)\n        else:\n            mel = wav_to_mel_cloning(\n                audio,\n                mel_norms=self.mel_stats.cpu(),\n                n_fft=4096,\n                hop_length=1024,\n                win_length=4096,\n                power=2,\n                normalized=False,\n                sample_rate=22050,\n                f_min=0,\n                f_max=8000,\n                n_mels=80,\n            )\n            cond_latent = self.gpt.get_style_emb(mel.to(self.device))\n        return cond_latent.transpose(1, 2)\n\n    @torch.inference_mode()\n    def get_speaker_embedding(self, audio, sr):\n        audio_16k = torchaudio.functional.resample(audio, sr, 16000)\n        return (\n            self.hifigan_decoder.speaker_encoder.forward(audio_16k.to(self.device), l2_norm=True)\n            .unsqueeze(-1)\n            .to(self.device)\n        )\n\n    @torch.inference_mode()\n    def get_conditioning_latents(\n        self,\n        audio_path,\n        max_ref_length=30,\n        gpt_cond_len=6,\n        gpt_cond_chunk_len=6,\n        librosa_trim_db=None,\n        sound_norm_refs=False,\n        load_sr=22050,\n    ):\n        \"\"\"Get the conditioning latents for the GPT model from the given audio.\n\n        Args:\n            audio_path (str or List[str]): Path to reference audio file(s).\n            max_ref_length (int): Maximum length of each reference audio in seconds. Defaults to 30.\n            gpt_cond_len (int): Length of the audio used for gpt latents. Defaults to 6.\n            gpt_cond_chunk_len (int): Chunk length used for gpt latents. It must be <= gpt_conf_len. Defaults to 6.\n            librosa_trim_db (int, optional): Trim the audio using this value. If None, not trimming. Defaults to None.\n            sound_norm_refs (bool, optional): Whether to normalize the audio. Defaults to False.\n            load_sr (int, optional): Sample rate to load the audio. Defaults to 24000.\n        \"\"\"\n        # deal with multiples references\n        if not isinstance(audio_path, list):\n            audio_paths = [audio_path]\n        else:\n            audio_paths = audio_path\n\n        speaker_embeddings = []\n        audios = []\n        speaker_embedding = None\n        for file_path in audio_paths:\n            audio = load_audio(file_path, load_sr)\n            audio = audio[:, : load_sr * max_ref_length].to(self.device)\n            if sound_norm_refs:\n                audio = (audio / torch.abs(audio).max()) * 0.75\n            if librosa_trim_db is not None:\n                audio = librosa.effects.trim(audio, top_db=librosa_trim_db)[0]\n\n            # compute latents for the decoder\n            speaker_embedding = self.get_speaker_embedding(audio, load_sr)\n            speaker_embeddings.append(speaker_embedding)\n\n            audios.append(audio)\n\n        # merge all the audios and compute the latents for the gpt\n        full_audio = torch.cat(audios, dim=-1)\n        gpt_cond_latents = self.get_gpt_cond_latents(\n            full_audio, load_sr, length=gpt_cond_len, chunk_length=gpt_cond_chunk_len\n        )  # [1, 1024, T]\n\n        if speaker_embeddings:\n            speaker_embedding = torch.stack(speaker_embeddings)\n            speaker_embedding = speaker_embedding.mean(dim=0)\n\n        return gpt_cond_latents, speaker_embedding\n\n    def synthesize(self, text, config, speaker_wav, language, speaker_id=None, **kwargs):\n        \"\"\"Synthesize speech with the given input text.\n\n        Args:\n            text (str): Input text.\n            config (XttsConfig): Config with inference parameters.\n            speaker_wav (list): List of paths to the speaker audio files to be used for cloning.\n            language (str): Language ID of the speaker.\n            **kwargs: Inference settings. See `inference()`.\n\n        Returns:\n            A dictionary of the output values with `wav` as output waveform, `deterministic_seed` as seed used at inference,\n            `text_input` as text token IDs after tokenizer, `voice_samples` as samples used for cloning, `conditioning_latents`\n            as latents used at inference.\n\n        \"\"\"\n        assert (\n            \"zh-cn\" if language == \"zh\" else language in self.config.languages\n        ), f\" \u2757 Language {language} is not supported. Supported languages are {self.config.languages}\"\n        # Use generally found best tuning knobs for generation.\n        settings = {\n            \"temperature\": config.temperature,\n            \"length_penalty\": config.length_penalty,\n            \"repetition_penalty\": config.repetition_penalty,\n            \"top_k\": config.top_k,\n            \"top_p\": config.top_p,\n        }\n        settings.update(kwargs)  # allow overriding of preset settings with kwargs\n        if speaker_id is not None:\n            gpt_cond_latent, speaker_embedding = self.speaker_manager.speakers[speaker_id].values()\n            return self.inference(text, language, gpt_cond_latent, speaker_embedding, **settings)\n        settings.update({\n            \"gpt_cond_len\": config.gpt_cond_len,\n            \"gpt_cond_chunk_len\": config.gpt_cond_chunk_len,\n            \"max_ref_len\": config.max_ref_len,\n            \"sound_norm_refs\": config.sound_norm_refs,\n        })\n        return self.full_inference(text, speaker_wav, language, **settings)\n\n    @torch.inference_mode()\n    def full_inference(\n        self,\n        text,\n        ref_audio_path,\n        language,\n        # GPT inference\n        temperature=0.75,\n        length_penalty=1.0,\n        repetition_penalty=10.0,\n        top_k=50,\n        top_p=0.85,\n        do_sample=True,\n        # Cloning\n        gpt_cond_len=30,\n        gpt_cond_chunk_len=6,\n        max_ref_len=10,\n        sound_norm_refs=False,\n        **hf_generate_kwargs,\n    ):\n        \"\"\"\n        This function produces an audio clip of the given text being spoken with the given reference voice.\n\n        Args:\n            text: (str) Text to be spoken.\n\n            ref_audio_path: (str) Path to a reference audio file to be used for cloning. This audio file should be >3\n                seconds long.\n\n            language: (str) Language of the voice to be generated.\n\n            temperature: (float) The softmax temperature of the autoregressive model. Defaults to 0.65.\n\n            length_penalty: (float) A length penalty applied to the autoregressive decoder. Higher settings causes the\n                model to produce more terse outputs. Defaults to 1.0.\n\n            repetition_penalty: (float) A penalty that prevents the autoregressive decoder from repeating itself during\n                decoding. Can be used to reduce the incidence of long silences or \"uhhhhhhs\", etc. Defaults to 2.0.\n\n            top_k: (int) K value used in top-k sampling. [0,inf]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 50.\n\n            top_p: (float) P value used in nucleus sampling. (0,1]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 0.8.\n\n            gpt_cond_len: (int) Length of the audio used for cloning. If audio is shorter, then audio length is used\n                else the first `gpt_cond_len` secs is used. Defaults to 30 seconds.\n\n            gpt_cond_chunk_len: (int) Chunk length used for cloning. It must be <= `gpt_cond_len`.\n                If gpt_cond_len == gpt_cond_chunk_len, no chunking. Defaults to 6 seconds.\n\n            hf_generate_kwargs: (**kwargs) The huggingface Transformers generate API is used for the autoregressive\n                transformer. Extra keyword args fed to this function get forwarded directly to that API. Documentation\n                here: https://huggingface.co/docs/transformers/internal/generation_utils\n\n        Returns:\n            Generated audio clip(s) as a torch tensor. Shape 1,S if k=1 else, (k,1,S) where S is the sample length.\n            Sample rate is 24kHz.\n        \"\"\"\n        (gpt_cond_latent, speaker_embedding) = self.get_conditioning_latents(\n            audio_path=ref_audio_path,\n            gpt_cond_len=gpt_cond_len,\n            gpt_cond_chunk_len=gpt_cond_chunk_len,\n            max_ref_length=max_ref_len,\n            sound_norm_refs=sound_norm_refs,\n        )\n\n        return self.inference(\n            text,\n            language,\n            gpt_cond_latent,\n            speaker_embedding,\n            temperature=temperature,\n            length_penalty=length_penalty,\n            repetition_penalty=repetition_penalty,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=do_sample,\n            **hf_generate_kwargs,\n        )\n\n    @torch.inference_mode()\n    def inference(\n        self,\n        text,\n        language,\n        gpt_cond_latent,\n        speaker_embedding,\n        # GPT inference\n        temperature=0.75,\n        length_penalty=1.0,\n        repetition_penalty=10.0,\n        top_k=50,\n        top_p=0.85,\n        do_sample=True,\n        num_beams=1,\n        speed=1.0,\n        enable_text_splitting=False,\n        **hf_generate_kwargs,\n    ):\n        language = language.split(\"-\")[0]  # remove the country code\n        length_scale = 1.0 / max(speed, 0.05)\n        gpt_cond_latent = gpt_cond_latent.to(self.device)\n        speaker_embedding = speaker_embedding.to(self.device)\n        if enable_text_splitting:\n            text = split_sentence(text, language, self.tokenizer.char_limits[language])\n        else:\n            text = [text]\n\n        wavs = []\n        gpt_latents_list = []\n        for sent in text:\n            sent = sent.strip().lower()\n            text_tokens = torch.IntTensor(self.tokenizer.encode(sent, lang=language)).unsqueeze(0).to(self.device)\n\n            assert (\n                text_tokens.shape[-1] < self.args.gpt_max_text_tokens\n            ), \" \u2757 XTTS can only generate text with a maximum of 400 tokens.\"\n\n            with torch.no_grad():\n                gpt_codes = self.gpt.generate(\n                    cond_latents=gpt_cond_latent,\n                    text_inputs=text_tokens,\n                    input_tokens=None,\n                    do_sample=do_sample,\n                    top_p=top_p,\n                    top_k=top_k,\n                    temperature=temperature,\n                    num_return_sequences=self.gpt_batch_size,\n                    num_beams=num_beams,\n                    length_penalty=length_penalty,\n                    repetition_penalty=repetition_penalty,\n                    output_attentions=False,\n                    **hf_generate_kwargs,\n                )\n                expected_output_len = torch.tensor(\n                    [gpt_codes.shape[-1] * self.gpt.code_stride_len], device=text_tokens.device\n                )\n\n                text_len = torch.tensor([text_tokens.shape[-1]], device=self.device)\n                gpt_latents = self.gpt(\n                    text_tokens,\n                    text_len,\n                    gpt_codes,\n                    expected_output_len,\n                    cond_latents=gpt_cond_latent,\n                    return_attentions=False,\n                    return_latent=True,\n                )\n\n                if length_scale != 1.0:\n                    gpt_latents = F.interpolate(\n                        gpt_latents.transpose(1, 2), scale_factor=length_scale, mode=\"linear\"\n                    ).transpose(1, 2)\n\n                gpt_latents_list.append(gpt_latents.cpu())\n                wavs.append(self.hifigan_decoder(gpt_latents, g=speaker_embedding).cpu().squeeze())\n\n        return {\n            \"wav\": torch.cat(wavs, dim=0).numpy(),\n            \"gpt_latents\": torch.cat(gpt_latents_list, dim=1).numpy(),\n            \"speaker_embedding\": speaker_embedding,\n        }\n\n    def handle_chunks(self, wav_gen, wav_gen_prev, wav_overlap, overlap_len):\n        \"\"\"Handle chunk formatting in streaming mode\"\"\"\n        wav_chunk = wav_gen[:-overlap_len]\n        if wav_gen_prev is not None:\n            wav_chunk = wav_gen[(wav_gen_prev.shape[0] - overlap_len) : -overlap_len]\n        if wav_overlap is not None:\n            # cross fade the overlap section\n            if overlap_len > len(wav_chunk):\n                # wav_chunk is smaller than overlap_len, pass on last wav_gen\n                if wav_gen_prev is not None:\n                    wav_chunk = wav_gen[(wav_gen_prev.shape[0] - overlap_len) :]\n                else:\n                    # not expecting will hit here as problem happens on last chunk\n                    wav_chunk = wav_gen[-overlap_len:]\n                return wav_chunk, wav_gen, None\n            else:\n                crossfade_wav = wav_chunk[:overlap_len]\n                crossfade_wav = crossfade_wav * torch.linspace(0.0, 1.0, overlap_len).to(crossfade_wav.device)\n                wav_chunk[:overlap_len] = wav_overlap * torch.linspace(1.0, 0.0, overlap_len).to(wav_overlap.device)\n                wav_chunk[:overlap_len] += crossfade_wav\n\n        wav_overlap = wav_gen[-overlap_len:]\n        wav_gen_prev = wav_gen\n        return wav_chunk, wav_gen_prev, wav_overlap\n\n    @torch.inference_mode()\n    def inference_stream(\n        self,\n        text,\n        language,\n        gpt_cond_latent,\n        speaker_embedding,\n        # Streaming\n        stream_chunk_size=20,\n        overlap_wav_len=1024,\n        # GPT inference\n        temperature=0.75,\n        length_penalty=1.0,\n        repetition_penalty=10.0,\n        top_k=50,\n        top_p=0.85,\n        do_sample=True,\n        speed=1.0,\n        enable_text_splitting=False,\n        **hf_generate_kwargs,\n    ):\n        language = language.split(\"-\")[0]  # remove the country code\n        length_scale = 1.0 / max(speed, 0.05)\n        gpt_cond_latent = gpt_cond_latent.to(self.device)\n        speaker_embedding = speaker_embedding.to(self.device)\n        if enable_text_splitting:\n            text = split_sentence(text, language, self.tokenizer.char_limits[language])\n        else:\n            text = [text]\n\n        for sent in text:\n            sent = sent.strip().lower()\n            text_tokens = torch.IntTensor(self.tokenizer.encode(sent, lang=language)).unsqueeze(0).to(self.device)\n\n            assert (\n                text_tokens.shape[-1] < self.args.gpt_max_text_tokens\n            ), \" \u2757 XTTS can only generate text with a maximum of 400 tokens.\"\n\n            fake_inputs = self.gpt.compute_embeddings(\n                gpt_cond_latent.to(self.device),\n                text_tokens,\n            )\n            gpt_generator = self.gpt.get_generator(\n                fake_inputs=fake_inputs,\n                top_k=top_k,\n                top_p=top_p,\n                temperature=temperature,\n                do_sample=do_sample,\n                num_beams=1,\n                num_return_sequences=1,\n                length_penalty=float(length_penalty),\n                repetition_penalty=float(repetition_penalty),\n                output_attentions=False,\n                output_hidden_states=True,\n                **hf_generate_kwargs,\n            )\n\n            last_tokens = []\n            all_latents = []\n            wav_gen_prev = None\n            wav_overlap = None\n            is_end = False\n\n            while not is_end:\n                try:\n                    x, latent = next(gpt_generator)\n                    last_tokens += [x]\n                    all_latents += [latent]\n                except StopIteration:\n                    is_end = True\n\n                if is_end or (stream_chunk_size > 0 and len(last_tokens) >= stream_chunk_size):\n                    gpt_latents = torch.cat(all_latents, dim=0)[None, :]\n                    if length_scale != 1.0:\n                        gpt_latents = F.interpolate(\n                            gpt_latents.transpose(1, 2), scale_factor=length_scale, mode=\"linear\"\n                        ).transpose(1, 2)\n                    wav_gen = self.hifigan_decoder(gpt_latents, g=speaker_embedding.to(self.device))\n                    wav_chunk, wav_gen_prev, wav_overlap = self.handle_chunks(\n                        wav_gen.squeeze(), wav_gen_prev, wav_overlap, overlap_wav_len\n                    )\n                    last_tokens = []\n                    yield wav_chunk\n\n    def forward(self):\n        raise NotImplementedError(\n            \"XTTS has a dedicated trainer, please check the XTTS docs: https://tts.readthedocs.io/en/dev/models/xtts.html#training\"\n        )\n\n    def eval_step(self):\n        raise NotImplementedError(\n            \"XTTS has a dedicated trainer, please check the XTTS docs: https://tts.readthedocs.io/en/dev/models/xtts.html#training\"\n        )\n\n    @staticmethod\n    def init_from_config(config: \"XttsConfig\", **kwargs):  # pylint: disable=unused-argument\n        return Xtts(config)\n\n    def eval(self):  # pylint: disable=redefined-builtin\n        \"\"\"Sets the model to evaluation mode. Overrides the default eval() method to also set the GPT model to eval mode.\"\"\"\n        self.gpt.init_gpt_for_inference()\n        super().eval()\n\n    def get_compatible_checkpoint_state_dict(self, model_path):\n        checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]\n        # remove xtts gpt trainer extra keys\n        ignore_keys = [\"torch_mel_spectrogram_style_encoder\", \"torch_mel_spectrogram_dvae\", \"dvae\"]\n        for key in list(checkpoint.keys()):\n            # check if it is from the coqui Trainer if so convert it\n            if key.startswith(\"xtts.\"):\n                new_key = key.replace(\"xtts.\", \"\")\n                checkpoint[new_key] = checkpoint[key]\n                del checkpoint[key]\n                key = new_key\n\n            # remove unused keys\n            if key.split(\".\")[0] in ignore_keys:\n                del checkpoint[key]\n\n        return checkpoint\n\n    def load_checkpoint(\n        self,\n        config,\n        checkpoint_dir=None,\n        checkpoint_path=None,\n        vocab_path=None,\n        eval=True,\n        strict=True,\n        use_deepspeed=False,\n        speaker_file_path=None,\n    ):\n        \"\"\"\n        Loads a checkpoint from disk and initializes the model's state and tokenizer.\n\n        Args:\n            config (dict): The configuration dictionary for the model.\n            checkpoint_dir (str, optional): The directory where the checkpoint is stored. Defaults to None.\n            checkpoint_path (str, optional): The path to the checkpoint file. Defaults to None.\n            vocab_path (str, optional): The path to the vocabulary file. Defaults to None.\n            eval (bool, optional): Whether to set the model to evaluation mode. Defaults to True.\n            strict (bool, optional): Whether to strictly enforce that the keys in the checkpoint match the keys in the model. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n\n        model_path = checkpoint_path or os.path.join(checkpoint_dir, \"model.pth\")\n        vocab_path = vocab_path or os.path.join(checkpoint_dir, \"vocab.json\")\n\n        if speaker_file_path is None and checkpoint_dir is not None:\n            speaker_file_path = os.path.join(checkpoint_dir, \"speakers_xtts.pth\")\n\n        self.language_manager = LanguageManager(config)\n        self.speaker_manager = None\n        if speaker_file_path is not None and os.path.exists(speaker_file_path):\n            self.speaker_manager = SpeakerManager(speaker_file_path)\n\n        if os.path.exists(vocab_path):\n            self.tokenizer = VoiceBpeTokenizer(vocab_file=vocab_path)\n\n        self.init_models()\n\n        checkpoint = self.get_compatible_checkpoint_state_dict(model_path)\n\n        # deal with v1 and v1.1. V1 has the init_gpt_for_inference keys, v1.1 do not\n        try:\n            self.load_state_dict(checkpoint, strict=strict)\n        except:\n            if eval:\n                self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache)\n            self.load_state_dict(checkpoint, strict=strict)\n\n        if eval:\n            self.hifigan_decoder.eval()\n            self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache, use_deepspeed=use_deepspeed)\n            self.gpt.eval()\n\n    def train_step(self):\n        raise NotImplementedError(\n            \"XTTS has a dedicated trainer, please check the XTTS docs: https://tts.readthedocs.io/en/dev/models/xtts.html#training\"\n        )\n", "TTS/tts/models/tacotron.py": "# coding: utf-8\n\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.tts.layers.tacotron.capacitron_layers import CapacitronVAE\nfrom TTS.tts.layers.tacotron.gst_layers import GST\nfrom TTS.tts.layers.tacotron.tacotron import Decoder, Encoder, PostCBHG\nfrom TTS.tts.models.base_tacotron import BaseTacotron\nfrom TTS.tts.utils.measures import alignment_diagonal_score\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.capacitron_optimizer import CapacitronOptimizer\n\n\nclass Tacotron(BaseTacotron):\n    \"\"\"Tacotron as in https://arxiv.org/abs/1703.10135\n    It's an autoregressive encoder-attention-decoder-postnet architecture.\n    Check `TacotronConfig` for the arguments.\n\n    Args:\n        config (TacotronConfig): Configuration for the Tacotron model.\n        speaker_manager (SpeakerManager): Speaker manager to handle multi-speaker settings. Only use if the model is\n            a multi-speaker model. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: \"TacotronConfig\",\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        # pass all config fields to `self`\n        # for fewer code change\n        for key in config:\n            setattr(self, key, config[key])\n\n        # set speaker embedding channel size for determining `in_channels` for the connected layers.\n        # `init_multispeaker` needs to be called once more in training to initialize the speaker embedding layer based\n        # on the number of speakers infered from the dataset.\n        if self.use_speaker_embedding or self.use_d_vector_file:\n            self.init_multispeaker(config)\n            self.decoder_in_features += self.embedded_speaker_dim  # add speaker embedding dim\n\n        if self.use_gst:\n            self.decoder_in_features += self.gst.gst_embedding_dim\n\n        if self.use_capacitron_vae:\n            self.decoder_in_features += self.capacitron_vae.capacitron_VAE_embedding_dim\n\n        # embedding layer\n        self.embedding = nn.Embedding(self.num_chars, 256, padding_idx=0)\n        self.embedding.weight.data.normal_(0, 0.3)\n\n        # base model layers\n        self.encoder = Encoder(self.encoder_in_features)\n        self.decoder = Decoder(\n            self.decoder_in_features,\n            self.decoder_output_dim,\n            self.r,\n            self.memory_size,\n            self.attention_type,\n            self.windowing,\n            self.attention_norm,\n            self.prenet_type,\n            self.prenet_dropout,\n            self.use_forward_attn,\n            self.transition_agent,\n            self.forward_attn_mask,\n            self.location_attn,\n            self.attention_heads,\n            self.separate_stopnet,\n            self.max_decoder_steps,\n        )\n        self.postnet = PostCBHG(self.decoder_output_dim)\n        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, self.out_channels)\n\n        # setup prenet dropout\n        self.decoder.prenet.dropout_at_inference = self.prenet_dropout_at_inference\n\n        # global style token layers\n        if self.gst and self.use_gst:\n            self.gst_layer = GST(\n                num_mel=self.decoder_output_dim,\n                num_heads=self.gst.gst_num_heads,\n                num_style_tokens=self.gst.gst_num_style_tokens,\n                gst_embedding_dim=self.gst.gst_embedding_dim,\n            )\n\n        # Capacitron layers\n        if self.capacitron_vae and self.use_capacitron_vae:\n            self.capacitron_vae_layer = CapacitronVAE(\n                num_mel=self.decoder_output_dim,\n                encoder_output_dim=self.encoder_in_features,\n                capacitron_VAE_embedding_dim=self.capacitron_vae.capacitron_VAE_embedding_dim,\n                speaker_embedding_dim=self.embedded_speaker_dim\n                if self.use_speaker_embedding and self.capacitron_vae.capacitron_use_speaker_embedding\n                else None,\n                text_summary_embedding_dim=self.capacitron_vae.capacitron_text_summary_embedding_dim\n                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n                else None,\n            )\n\n        # backward pass decoder\n        if self.bidirectional_decoder:\n            self._init_backward_decoder()\n        # setup DDC\n        if self.double_decoder_consistency:\n            self.coarse_decoder = Decoder(\n                self.decoder_in_features,\n                self.decoder_output_dim,\n                self.ddc_r,\n                self.memory_size,\n                self.attention_type,\n                self.windowing,\n                self.attention_norm,\n                self.prenet_type,\n                self.prenet_dropout,\n                self.use_forward_attn,\n                self.transition_agent,\n                self.forward_attn_mask,\n                self.location_attn,\n                self.attention_heads,\n                self.separate_stopnet,\n                self.max_decoder_steps,\n            )\n\n    def forward(  # pylint: disable=dangerous-default-value\n        self, text, text_lengths, mel_specs=None, mel_lengths=None, aux_input={\"speaker_ids\": None, \"d_vectors\": None}\n    ):\n        \"\"\"\n        Shapes:\n            text: [B, T_in]\n            text_lengths: [B]\n            mel_specs: [B, T_out, C]\n            mel_lengths: [B]\n            aux_input: 'speaker_ids': [B, 1] and  'd_vectors':[B, C]\n        \"\"\"\n        aux_input = self._format_aux_input(aux_input)\n        outputs = {\"alignments_backward\": None, \"decoder_outputs_backward\": None}\n        inputs = self.embedding(text)\n        input_mask, output_mask = self.compute_masks(text_lengths, mel_lengths)\n        # B x T_in x encoder_in_features\n        encoder_outputs = self.encoder(inputs)\n        # sequence masking\n        encoder_outputs = encoder_outputs * input_mask.unsqueeze(2).expand_as(encoder_outputs)\n        # global style token\n        if self.gst and self.use_gst:\n            # B x gst_dim\n            encoder_outputs = self.compute_gst(encoder_outputs, mel_specs)\n        # speaker embedding\n        if self.use_speaker_embedding or self.use_d_vector_file:\n            if not self.use_d_vector_file:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])[:, None]\n            else:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = torch.unsqueeze(aux_input[\"d_vectors\"], 1)\n            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n        # Capacitron\n        if self.capacitron_vae and self.use_capacitron_vae:\n            # B x capacitron_VAE_embedding_dim\n            encoder_outputs, *capacitron_vae_outputs = self.compute_capacitron_VAE_embedding(\n                encoder_outputs,\n                reference_mel_info=[mel_specs, mel_lengths],\n                text_info=[inputs, text_lengths]\n                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n                else None,\n                speaker_embedding=embedded_speakers if self.capacitron_vae.capacitron_use_speaker_embedding else None,\n            )\n        else:\n            capacitron_vae_outputs = None\n        # decoder_outputs: B x decoder_in_features x T_out\n        # alignments: B x T_in x encoder_in_features\n        # stop_tokens: B x T_in\n        decoder_outputs, alignments, stop_tokens = self.decoder(encoder_outputs, mel_specs, input_mask)\n        # sequence masking\n        if output_mask is not None:\n            decoder_outputs = decoder_outputs * output_mask.unsqueeze(1).expand_as(decoder_outputs)\n        # B x T_out x decoder_in_features\n        postnet_outputs = self.postnet(decoder_outputs)\n        # sequence masking\n        if output_mask is not None:\n            postnet_outputs = postnet_outputs * output_mask.unsqueeze(2).expand_as(postnet_outputs)\n        # B x T_out x posnet_dim\n        postnet_outputs = self.last_linear(postnet_outputs)\n        # B x T_out x decoder_in_features\n        decoder_outputs = decoder_outputs.transpose(1, 2).contiguous()\n        if self.bidirectional_decoder:\n            decoder_outputs_backward, alignments_backward = self._backward_pass(mel_specs, encoder_outputs, input_mask)\n            outputs[\"alignments_backward\"] = alignments_backward\n            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n        if self.double_decoder_consistency:\n            decoder_outputs_backward, alignments_backward = self._coarse_decoder_pass(\n                mel_specs, encoder_outputs, alignments, input_mask\n            )\n            outputs[\"alignments_backward\"] = alignments_backward\n            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n        outputs.update(\n            {\n                \"model_outputs\": postnet_outputs,\n                \"decoder_outputs\": decoder_outputs,\n                \"alignments\": alignments,\n                \"stop_tokens\": stop_tokens,\n                \"capacitron_vae_outputs\": capacitron_vae_outputs,\n            }\n        )\n        return outputs\n\n    @torch.no_grad()\n    def inference(self, text_input, aux_input=None):\n        aux_input = self._format_aux_input(aux_input)\n        inputs = self.embedding(text_input)\n        encoder_outputs = self.encoder(inputs)\n        if self.gst and self.use_gst:\n            # B x gst_dim\n            encoder_outputs = self.compute_gst(encoder_outputs, aux_input[\"style_mel\"], aux_input[\"d_vectors\"])\n        if self.capacitron_vae and self.use_capacitron_vae:\n            if aux_input[\"style_text\"] is not None:\n                style_text_embedding = self.embedding(aux_input[\"style_text\"])\n                style_text_length = torch.tensor([style_text_embedding.size(1)], dtype=torch.int64).to(\n                    encoder_outputs.device\n                )  # pylint: disable=not-callable\n            reference_mel_length = (\n                torch.tensor([aux_input[\"style_mel\"].size(1)], dtype=torch.int64).to(encoder_outputs.device)\n                if aux_input[\"style_mel\"] is not None\n                else None\n            )  # pylint: disable=not-callable\n            # B x capacitron_VAE_embedding_dim\n            encoder_outputs, *_ = self.compute_capacitron_VAE_embedding(\n                encoder_outputs,\n                reference_mel_info=[aux_input[\"style_mel\"], reference_mel_length]\n                if aux_input[\"style_mel\"] is not None\n                else None,\n                text_info=[style_text_embedding, style_text_length] if aux_input[\"style_text\"] is not None else None,\n                speaker_embedding=aux_input[\"d_vectors\"]\n                if self.capacitron_vae.capacitron_use_speaker_embedding\n                else None,\n            )\n        if self.num_speakers > 1:\n            if not self.use_d_vector_file:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])\n                # reshape embedded_speakers\n                if embedded_speakers.ndim == 1:\n                    embedded_speakers = embedded_speakers[None, None, :]\n                elif embedded_speakers.ndim == 2:\n                    embedded_speakers = embedded_speakers[None, :]\n            else:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = torch.unsqueeze(aux_input[\"d_vectors\"], 1)\n            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n        decoder_outputs, alignments, stop_tokens = self.decoder.inference(encoder_outputs)\n        postnet_outputs = self.postnet(decoder_outputs)\n        postnet_outputs = self.last_linear(postnet_outputs)\n        decoder_outputs = decoder_outputs.transpose(1, 2)\n        outputs = {\n            \"model_outputs\": postnet_outputs,\n            \"decoder_outputs\": decoder_outputs,\n            \"alignments\": alignments,\n            \"stop_tokens\": stop_tokens,\n        }\n        return outputs\n\n    def before_backward_pass(self, loss_dict, optimizer) -> None:\n        # Extracting custom training specific operations for capacitron\n        # from the trainer\n        if self.use_capacitron_vae:\n            loss_dict[\"capacitron_vae_beta_loss\"].backward()\n            optimizer.first_step()\n\n    def train_step(self, batch: Dict, criterion: torch.nn.Module) -> Tuple[Dict, Dict]:\n        \"\"\"Perform a single training step by fetching the right set of samples from the batch.\n\n        Args:\n            batch ([Dict]): A dictionary of input tensors.\n            criterion ([torch.nn.Module]): Callable criterion to compute model loss.\n        \"\"\"\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        linear_input = batch[\"linear_input\"]\n        stop_targets = batch[\"stop_targets\"]\n        stop_target_lengths = batch[\"stop_target_lengths\"]\n        speaker_ids = batch[\"speaker_ids\"]\n        d_vectors = batch[\"d_vectors\"]\n\n        aux_input = {\"speaker_ids\": speaker_ids, \"d_vectors\": d_vectors}\n        outputs = self.forward(text_input, text_lengths, mel_input, mel_lengths, aux_input)\n\n        # set the [alignment] lengths wrt reduction factor for guided attention\n        if mel_lengths.max() % self.decoder.r != 0:\n            alignment_lengths = (\n                mel_lengths + (self.decoder.r - (mel_lengths.max() % self.decoder.r))\n            ) // self.decoder.r\n        else:\n            alignment_lengths = mel_lengths // self.decoder.r\n\n        # compute loss\n        with autocast(enabled=False):  # use float32 for the criterion\n            loss_dict = criterion(\n                outputs[\"model_outputs\"].float(),\n                outputs[\"decoder_outputs\"].float(),\n                mel_input.float(),\n                linear_input.float(),\n                outputs[\"stop_tokens\"].float(),\n                stop_targets.float(),\n                stop_target_lengths,\n                outputs[\"capacitron_vae_outputs\"] if self.capacitron_vae else None,\n                mel_lengths,\n                None if outputs[\"decoder_outputs_backward\"] is None else outputs[\"decoder_outputs_backward\"].float(),\n                outputs[\"alignments\"].float(),\n                alignment_lengths,\n                None if outputs[\"alignments_backward\"] is None else outputs[\"alignments_backward\"].float(),\n                text_lengths,\n            )\n\n        # compute alignment error (the lower the better )\n        align_error = 1 - alignment_diagonal_score(outputs[\"alignments\"])\n        loss_dict[\"align_error\"] = align_error\n        return outputs, loss_dict\n\n    def get_optimizer(self) -> List:\n        if self.use_capacitron_vae:\n            return CapacitronOptimizer(self.config, self.named_parameters())\n        return get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr, self)\n\n    def get_scheduler(self, optimizer: object):\n        opt = optimizer.primary_optimizer if self.use_capacitron_vae else optimizer\n        return get_scheduler(self.config.lr_scheduler, self.config.lr_scheduler_params, opt)\n\n    def before_gradient_clipping(self):\n        if self.use_capacitron_vae:\n            # Capacitron model specific gradient clipping\n            model_params_to_clip = []\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    if name != \"capacitron_vae_layer.beta\":\n                        model_params_to_clip.append(param)\n            torch.nn.utils.clip_grad_norm_(model_params_to_clip, self.capacitron_vae.capacitron_grad_clip)\n\n    def _create_logs(self, batch, outputs, ap):\n        postnet_outputs = outputs[\"model_outputs\"]\n        decoder_outputs = outputs[\"decoder_outputs\"]\n        alignments = outputs[\"alignments\"]\n        alignments_backward = outputs[\"alignments_backward\"]\n        mel_input = batch[\"mel_input\"]\n        linear_input = batch[\"linear_input\"]\n\n        pred_linear_spec = postnet_outputs[0].data.cpu().numpy()\n        pred_mel_spec = decoder_outputs[0].data.cpu().numpy()\n        gt_linear_spec = linear_input[0].data.cpu().numpy()\n        gt_mel_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"pred_linear_spec\": plot_spectrogram(pred_linear_spec, ap, output_fig=False),\n            \"real_linear_spec\": plot_spectrogram(gt_linear_spec, ap, output_fig=False),\n            \"pred_mel_spec\": plot_spectrogram(pred_mel_spec, ap, output_fig=False),\n            \"real_mel_spec\": plot_spectrogram(gt_mel_spec, ap, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        if self.bidirectional_decoder or self.double_decoder_consistency:\n            figures[\"alignment_backward\"] = plot_alignment(alignments_backward[0].data.cpu().numpy(), output_fig=False)\n\n        # Sample audio\n        audio = ap.inv_spectrogram(pred_linear_spec.T)\n        return figures, {\"audio\": audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ) -> None:  # pylint: disable=no-self-use\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_step(self, batch: dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    @staticmethod\n    def init_from_config(config: \"TacotronConfig\", samples: Union[List[List], List[Dict]] = None):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (TacotronConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return Tacotron(new_config, ap, tokenizer, speaker_manager)\n", "TTS/tts/models/tortoise.py": "import os\nimport random\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom time import time\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom coqpit import Coqpit\nfrom tqdm import tqdm\n\nfrom TTS.tts.layers.tortoise.arch_utils import TorchMelSpectrogram\nfrom TTS.tts.layers.tortoise.audio_utils import denormalize_tacotron_mel, load_voice, wav_to_univnet_mel\nfrom TTS.tts.layers.tortoise.autoregressive import UnifiedVoice\nfrom TTS.tts.layers.tortoise.classifier import AudioMiniEncoderWithClassifierHead\nfrom TTS.tts.layers.tortoise.clvp import CLVP\nfrom TTS.tts.layers.tortoise.diffusion import SpacedDiffusion, get_named_beta_schedule, space_timesteps\nfrom TTS.tts.layers.tortoise.diffusion_decoder import DiffusionTts\nfrom TTS.tts.layers.tortoise.random_latent_generator import RandomLatentConverter\nfrom TTS.tts.layers.tortoise.tokenizer import VoiceBpeTokenizer\nfrom TTS.tts.layers.tortoise.vocoder import VocConf, VocType\nfrom TTS.tts.layers.tortoise.wav2vec_alignment import Wav2VecAlignment\nfrom TTS.tts.models.base_tts import BaseTTS\n\n\ndef pad_or_truncate(t, length):\n    \"\"\"\n    Utility function for forcing <t> to have the specified sequence length, whether by clipping it or padding it with 0s.\n    \"\"\"\n    tp = t[..., :length]\n    if t.shape[-1] == length:\n        tp = t\n    elif t.shape[-1] < length:\n        tp = F.pad(t, (0, length - t.shape[-1]))\n    return tp\n\n\ndef deterministic_state(seed=None):\n    \"\"\"\n    Sets the random seeds that tortoise uses to the current time() and returns that seed so results can be\n    reproduced.\n    \"\"\"\n    seed = int(time()) if seed is None else seed\n    torch.manual_seed(seed)\n    random.seed(seed)\n    # Can't currently set this because of CUBLAS. TODO: potentially enable it if necessary.\n    # torch.use_deterministic_algorithms(True)\n\n    return seed\n\n\ndef load_discrete_vocoder_diffuser(\n    trained_diffusion_steps=4000,\n    desired_diffusion_steps=200,\n    cond_free=True,\n    cond_free_k=1,\n    sampler=\"ddim\",\n):\n    \"\"\"\n    Helper function to load a GaussianDiffusion instance configured for use as a vocoder.\n    \"\"\"\n    return SpacedDiffusion(\n        use_timesteps=space_timesteps(trained_diffusion_steps, [desired_diffusion_steps]),\n        model_mean_type=\"epsilon\",\n        model_var_type=\"learned_range\",\n        loss_type=\"mse\",\n        betas=get_named_beta_schedule(\"linear\", trained_diffusion_steps),\n        conditioning_free=cond_free,\n        conditioning_free_k=cond_free_k,\n        sampler=sampler,\n    )\n\n\ndef format_conditioning(clip, cond_length=132300, device=\"cuda\", **kwargs):\n    \"\"\"\n    Converts the given conditioning signal to a MEL spectrogram and clips it as expected by the models.\n    \"\"\"\n    gap = clip.shape[-1] - cond_length\n    if gap < 0:\n        clip = F.pad(clip, pad=(0, abs(gap)))\n    elif gap > 0:\n        rand_start = random.randint(0, gap)\n        clip = clip[:, rand_start : rand_start + cond_length]\n    mel_clip = TorchMelSpectrogram(**kwargs)(clip.unsqueeze(0)).squeeze(0)\n    return mel_clip.unsqueeze(0).to(device)\n\n\ndef fix_autoregressive_output(codes, stop_token, complain=True):\n    \"\"\"\n    This function performs some padding on coded audio that fixes a mismatch issue between what the diffusion model was\n    trained on and what the autoregressive code generator creates (which has no padding or end).\n    This is highly specific to the DVAE being used, so this particular coding will not necessarily work if used with\n    a different DVAE. This can be inferred by feeding a audio clip padded with lots of zeros on the end through the DVAE\n    and copying out the last few codes.\n\n    Failing to do this padding will produce speech with a harsh end that sounds like \"BLAH\" or similar.\n    \"\"\"\n    # Strip off the autoregressive stop token and add padding.\n    stop_token_indices = (codes == stop_token).nonzero()\n    if len(stop_token_indices) == 0:\n        if complain:\n            print(\n                \"No stop tokens found in one of the generated voice clips. This typically means the spoken audio is \"\n                \"too long. In some cases, the output will still be good, though. Listen to it and if it is missing words, \"\n                \"try breaking up your input text.\"\n            )\n        return codes\n    codes[stop_token_indices] = 83\n    stm = stop_token_indices.min().item()\n    codes[stm:] = 83\n    if stm - 3 < codes.shape[0]:\n        codes[-3] = 45\n        codes[-2] = 45\n        codes[-1] = 248\n    return codes\n\n\ndef do_spectrogram_diffusion(\n    diffusion_model,\n    diffuser,\n    latents,\n    conditioning_latents,\n    temperature=1,\n    verbose=True,\n):\n    \"\"\"\n    Uses the specified diffusion model to convert discrete codes into a spectrogram.\n    \"\"\"\n    with torch.no_grad():\n        output_seq_len = (\n            latents.shape[1] * 4 * 24000 // 22050\n        )  # This diffusion model converts from 22kHz spectrogram codes to a 24kHz spectrogram signal.\n        output_shape = (latents.shape[0], 100, output_seq_len)\n        precomputed_embeddings = diffusion_model.timestep_independent(\n            latents, conditioning_latents, output_seq_len, False\n        )\n\n        noise = torch.randn(output_shape, device=latents.device) * temperature\n        mel = diffuser.sample_loop(\n            diffusion_model,\n            output_shape,\n            noise=noise,\n            model_kwargs={\"precomputed_aligned_embeddings\": precomputed_embeddings},\n            progress=verbose,\n        )\n        return denormalize_tacotron_mel(mel)[:, :, :output_seq_len]\n\n\ndef classify_audio_clip(clip, model_dir):\n    \"\"\"\n    Returns whether or not Tortoises' classifier thinks the given clip came from Tortoise.\n    :param clip: torch tensor containing audio waveform data (get it from load_audio)\n    :return: True if the clip was classified as coming from Tortoise and false if it was classified as real.\n    \"\"\"\n    classifier = AudioMiniEncoderWithClassifierHead(\n        2,\n        spec_dim=1,\n        embedding_dim=512,\n        depth=5,\n        downsample_factor=4,\n        resnet_blocks=2,\n        attn_blocks=4,\n        num_attn_heads=4,\n        base_channels=32,\n        dropout=0,\n        kernel_size=5,\n        distribute_zero_label=False,\n    )\n    classifier.load_state_dict(torch.load(os.path.join(model_dir, \"classifier.pth\"), map_location=torch.device(\"cpu\")))\n    clip = clip.cpu().unsqueeze(0)\n    results = F.softmax(classifier(clip), dim=-1)\n    return results[0][0]\n\n\ndef pick_best_batch_size_for_gpu():\n    \"\"\"\n    Tries to pick a batch size that will fit in your GPU. These sizes aren't guaranteed to work, but they should give\n    you a good shot.\n    \"\"\"\n    if torch.cuda.is_available():\n        _, available = torch.cuda.mem_get_info()\n        availableGb = available / (1024**3)\n        batch_size = 1\n        if availableGb > 14:\n            batch_size = 16\n        elif availableGb > 10:\n            batch_size = 8\n        elif availableGb > 7:\n            batch_size = 4\n    return batch_size\n\n\n@dataclass\nclass TortoiseAudioConfig(Coqpit):\n    sample_rate: int = 22050\n    diffusion_sample_rate: int = 24000\n    output_sample_rate: int = 24000\n\n\n@dataclass\nclass TortoiseArgs(Coqpit):\n    \"\"\"A dataclass to represent Tortoise model arguments that define the model structure.\n\n    Args:\n        autoregressive_batch_size (int): The size of the auto-regressive batch.\n        enable_redaction (bool, optional): Whether to enable redaction. Defaults to True.\n        high_vram (bool, optional): Whether to use high VRAM. Defaults to False.\n        kv_cache (bool, optional): Whether to use the kv_cache. Defaults to True.\n        ar_checkpoint (str, optional): The checkpoint for the autoregressive model. Defaults to None.\n        clvp_checkpoint (str, optional): The checkpoint for the ConditionalLatentVariablePerseq model. Defaults to None.\n        diff_checkpoint (str, optional): The checkpoint for the DiffTTS model. Defaults to None.\n        num_chars (int, optional): The maximum number of characters to generate. Defaults to 255.\n        vocoder (VocType, optional): The vocoder to use for synthesis. Defaults to VocConf.Univnet.\n\n        For UnifiedVoice model:\n        ar_max_mel_tokens (int, optional): The maximum mel tokens for the autoregressive model. Defaults to 604.\n        ar_max_text_tokens (int, optional): The maximum text tokens for the autoregressive model. Defaults to 402.\n        ar_max_conditioning_inputs (int, optional): The maximum conditioning inputs for the autoregressive model. Defaults to 2.\n        ar_layers (int, optional): The number of layers for the autoregressive model. Defaults to 30.\n        ar_model_dim (int, optional): The model dimension for the autoregressive model. Defaults to 1024.\n        ar_heads (int, optional): The number of heads for the autoregressive model. Defaults to 16.\n        ar_number_text_tokens (int, optional): The number of text tokens for the autoregressive model. Defaults to 255.\n        ar_start_text_token (int, optional): The start text token for the autoregressive model. Defaults to 255.\n        ar_checkpointing (bool, optional): Whether to use checkpointing for the autoregressive model. Defaults to False.\n        ar_train_solo_embeddings (bool, optional): Whether to train embeddings for the autoregressive model. Defaults to False.\n\n        For DiffTTS model:\n        diff_model_channels (int, optional): The number of channels for the DiffTTS model. Defaults to 1024.\n        diff_num_layers (int, optional): The number of layers for the DiffTTS model. Defaults to 10.\n        diff_in_channels (int, optional): The input channels for the DiffTTS model. Defaults to 100.\n        diff_out_channels (int, optional): The output channels for the DiffTTS model. Defaults to 200.\n        diff_in_latent_channels (int, optional): The input latent channels for the DiffTTS model. Defaults to 1024.\n        diff_in_tokens (int, optional): The input tokens for the DiffTTS model. Defaults to 8193.\n        diff_dropout (int, optional): The dropout percentage for the DiffTTS model. Defaults to 0.\n        diff_use_fp16 (bool, optional): Whether to use fp16 for the DiffTTS model. Defaults to False.\n        diff_num_heads (int, optional): The number of heads for the DiffTTS model. Defaults to 16.\n        diff_layer_drop (int, optional): The layer dropout percentage for the DiffTTS model. Defaults to 0.\n        diff_unconditioned_percentage (int, optional): The percentage of unconditioned inputs for the DiffTTS model. Defaults to 0.\n\n        For ConditionalLatentVariablePerseq model:\n        clvp_dim_text (int): The dimension of the text input for the CLVP module. Defaults to 768.\n        clvp_dim_speech (int): The dimension of the speech input for the CLVP module. Defaults to 768.\n        clvp_dim_latent (int): The dimension of the latent representation for the CLVP module. Defaults to 768.\n        clvp_num_text_tokens (int): The number of text tokens used by the CLVP module. Defaults to 256.\n        clvp_text_enc_depth (int): The depth of the text encoder in the CLVP module. Defaults to 20.\n        clvp_text_seq_len (int): The maximum sequence length of the text input for the CLVP module. Defaults to 350.\n        clvp_text_heads (int): The number of attention heads used by the text encoder in the CLVP module. Defaults to 12.\n        clvp_num_speech_tokens (int): The number of speech tokens used by the CLVP module. Defaults to 8192.\n        clvp_speech_enc_depth (int): The depth of the speech encoder in the CLVP module. Defaults to 20.\n        clvp_speech_heads (int): The number of attention heads used by the speech encoder in the CLVP module. Defaults to 12.\n        clvp_speech_seq_len (int): The maximum sequence length of the speech input for the CLVP module. Defaults to 430.\n        clvp_use_xformers (bool): A flag indicating whether the model uses transformers in the CLVP module. Defaults to True.\n        duration_const (int): A constant value used in the model. Defaults to 102400.\n    \"\"\"\n\n    autoregressive_batch_size: int = 1\n    enable_redaction: bool = False\n    high_vram: bool = False\n    kv_cache: bool = True\n    ar_checkpoint: str = None\n    clvp_checkpoint: str = None\n    diff_checkpoint: str = None\n    num_chars: int = 255\n    vocoder: VocType = VocConf.Univnet\n\n    # UnifiedVoice params\n    ar_max_mel_tokens: int = 604\n    ar_max_text_tokens: int = 402\n    ar_max_conditioning_inputs: int = 2\n    ar_layers: int = 30\n    ar_model_dim: int = 1024\n    ar_heads: int = 16\n    ar_number_text_tokens: int = 255\n    ar_start_text_token: int = 255\n    ar_checkpointing: bool = False\n    ar_train_solo_embeddings: bool = False\n\n    # DiffTTS params\n    diff_model_channels: int = 1024\n    diff_num_layers: int = 10\n    diff_in_channels: int = 100\n    diff_out_channels: int = 200\n    diff_in_latent_channels: int = 1024\n    diff_in_tokens: int = 8193\n    diff_dropout: int = 0\n    diff_use_fp16: bool = False\n    diff_num_heads: int = 16\n    diff_layer_drop: int = 0\n    diff_unconditioned_percentage: int = 0\n\n    # clvp params\n    clvp_dim_text: int = 768\n    clvp_dim_speech: int = 768\n    clvp_dim_latent: int = 768\n    clvp_num_text_tokens: int = 256\n    clvp_text_enc_depth: int = 20\n    clvp_text_seq_len: int = 350\n    clvp_text_heads: int = 12\n    clvp_num_speech_tokens: int = 8192\n    clvp_speech_enc_depth: int = 20\n    clvp_speech_heads: int = 12\n    clvp_speech_seq_len: int = 430\n    clvp_use_xformers: bool = True\n    # constants\n    duration_const: int = 102400\n\n\nclass Tortoise(BaseTTS):\n    \"\"\"Tortoise model class.\n\n    Currently only supports inference.\n\n    Examples:\n        >>> from TTS.tts.configs.tortoise_config import TortoiseConfig\n        >>> from TTS.tts.models.tortoise import Tortoise\n        >>> config = TortoiseConfig()\n        >>> model = Tortoise.inif_from_config(config)\n        >>> model.load_checkpoint(config, checkpoint_dir=\"paths/to/models_dir/\", eval=True)\n    \"\"\"\n\n    def __init__(self, config: Coqpit):\n        super().__init__(config, ap=None, tokenizer=None)\n        self.mel_norm_path = None\n        self.config = config\n        self.ar_checkpoint = self.args.ar_checkpoint\n        self.diff_checkpoint = self.args.diff_checkpoint  # TODO: check if this is even needed\n        self.models_dir = config.model_dir\n        self.autoregressive_batch_size = (\n            pick_best_batch_size_for_gpu()\n            if self.args.autoregressive_batch_size is None\n            else self.args.autoregressive_batch_size\n        )\n        self.enable_redaction = self.args.enable_redaction\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.enable_redaction:\n            self.aligner = Wav2VecAlignment()\n\n        self.tokenizer = VoiceBpeTokenizer()\n\n        self.autoregressive = UnifiedVoice(\n            max_mel_tokens=self.args.ar_max_mel_tokens,\n            max_text_tokens=self.args.ar_max_text_tokens,\n            max_conditioning_inputs=self.args.ar_max_conditioning_inputs,\n            layers=self.args.ar_layers,\n            model_dim=self.args.ar_model_dim,\n            heads=self.args.ar_heads,\n            number_text_tokens=self.args.ar_number_text_tokens,\n            start_text_token=self.args.ar_start_text_token,\n            checkpointing=self.args.ar_checkpointing,\n            train_solo_embeddings=self.args.ar_train_solo_embeddings,\n        ).cpu()\n\n        self.diffusion = DiffusionTts(\n            model_channels=self.args.diff_model_channels,\n            num_layers=self.args.diff_num_layers,\n            in_channels=self.args.diff_in_channels,\n            out_channels=self.args.diff_out_channels,\n            in_latent_channels=self.args.diff_in_latent_channels,\n            in_tokens=self.args.diff_in_tokens,\n            dropout=self.args.diff_dropout,\n            use_fp16=self.args.diff_use_fp16,\n            num_heads=self.args.diff_num_heads,\n            layer_drop=self.args.diff_layer_drop,\n            unconditioned_percentage=self.args.diff_unconditioned_percentage,\n        ).cpu()\n\n        self.clvp = CLVP(\n            dim_text=self.args.clvp_dim_text,\n            dim_speech=self.args.clvp_dim_speech,\n            dim_latent=self.args.clvp_dim_latent,\n            num_text_tokens=self.args.clvp_num_text_tokens,\n            text_enc_depth=self.args.clvp_text_enc_depth,\n            text_seq_len=self.args.clvp_text_seq_len,\n            text_heads=self.args.clvp_text_heads,\n            num_speech_tokens=self.args.clvp_num_speech_tokens,\n            speech_enc_depth=self.args.clvp_speech_enc_depth,\n            speech_heads=self.args.clvp_speech_heads,\n            speech_seq_len=self.args.clvp_speech_seq_len,\n            use_xformers=self.args.clvp_use_xformers,\n        ).cpu()\n\n        self.vocoder = self.args.vocoder.value.constructor().cpu()\n\n        # Random latent generators (RLGs) are loaded lazily.\n        self.rlg_auto = None\n        self.rlg_diffusion = None\n\n        if self.args.high_vram:\n            self.autoregressive = self.autoregressive.to(self.device)\n            self.diffusion = self.diffusion.to(self.device)\n            self.clvp = self.clvp.to(self.device)\n            self.vocoder = self.vocoder.to(self.device)\n        self.high_vram = self.args.high_vram\n\n    @contextmanager\n    def temporary_cuda(self, model):\n        if self.high_vram:\n            yield model\n        else:\n            m = model.to(self.device)\n            yield m\n            m = model.cpu()\n\n    def get_conditioning_latents(\n        self,\n        voice_samples,\n        return_mels=False,\n        latent_averaging_mode=0,\n        original_tortoise=False,\n    ):\n        \"\"\"\n        Transforms one or more voice_samples into a tuple (autoregressive_conditioning_latent, diffusion_conditioning_latent).\n        These are expressive learned latents that encode aspects of the provided clips like voice, intonation, and acoustic\n        properties.\n        :param voice_samples: List of arbitrary reference clips, which should be *pairs* of torch tensors containing arbitrary kHz waveform data.\n        :param latent_averaging_mode: 0/1/2 for following modes:\n            0 - latents will be generated as in original tortoise, using ~4.27s from each voice sample, averaging latent across all samples\n            1 - latents will be generated using (almost) entire voice samples, averaged across all the ~4.27s chunks\n            2 - latents will be generated using (almost) entire voice samples, averaged per voice sample\n        \"\"\"\n        assert latent_averaging_mode in [\n            0,\n            1,\n            2,\n        ], \"latent_averaging mode has to be one of (0, 1, 2)\"\n\n        with torch.no_grad():\n            voice_samples = [[v.to(self.device) for v in ls] for ls in voice_samples]\n\n            auto_conds = []\n            for ls in voice_samples:\n                auto_conds.append(format_conditioning(ls[0], device=self.device, mel_norm_file=self.mel_norm_path))\n            auto_conds = torch.stack(auto_conds, dim=1)\n            with self.temporary_cuda(self.autoregressive) as ar:\n                auto_latent = ar.get_conditioning(auto_conds)\n\n            diffusion_conds = []\n\n            DURS_CONST = self.args.duration_const\n            for ls in voice_samples:\n                # The diffuser operates at a sample rate of 24000 (except for the latent inputs)\n                sample = torchaudio.functional.resample(ls[0], 22050, 24000) if original_tortoise else ls[1]\n                if latent_averaging_mode == 0:\n                    sample = pad_or_truncate(sample, DURS_CONST)\n                    cond_mel = wav_to_univnet_mel(\n                        sample.to(self.device),\n                        do_normalization=False,\n                        device=self.device,\n                    )\n                    diffusion_conds.append(cond_mel)\n                else:\n                    from math import ceil\n\n                    if latent_averaging_mode == 2:\n                        temp_diffusion_conds = []\n                    for chunk in range(ceil(sample.shape[1] / DURS_CONST)):\n                        current_sample = sample[:, chunk * DURS_CONST : (chunk + 1) * DURS_CONST]\n                        current_sample = pad_or_truncate(current_sample, DURS_CONST)\n                        cond_mel = wav_to_univnet_mel(\n                            current_sample.to(self.device),\n                            do_normalization=False,\n                            device=self.device,\n                        )\n                        if latent_averaging_mode == 1:\n                            diffusion_conds.append(cond_mel)\n                        elif latent_averaging_mode == 2:\n                            temp_diffusion_conds.append(cond_mel)\n                    if latent_averaging_mode == 2:\n                        diffusion_conds.append(torch.stack(temp_diffusion_conds).mean(0))\n            diffusion_conds = torch.stack(diffusion_conds, dim=1)\n\n            with self.temporary_cuda(self.diffusion) as diffusion:\n                diffusion_latent = diffusion.get_conditioning(diffusion_conds)\n\n        if return_mels:\n            return auto_latent, diffusion_latent, auto_conds, diffusion_conds\n        return auto_latent, diffusion_latent\n\n    def get_random_conditioning_latents(self):\n        # Lazy-load the RLG models.\n        if self.rlg_auto is None:\n            self.rlg_auto = RandomLatentConverter(1024).eval()\n            self.rlg_auto.load_state_dict(\n                torch.load(\n                    os.path.join(self.models_dir, \"rlg_auto.pth\"),\n                    map_location=torch.device(\"cpu\"),\n                )\n            )\n            self.rlg_diffusion = RandomLatentConverter(2048).eval()\n            self.rlg_diffusion.load_state_dict(\n                torch.load(\n                    os.path.join(self.models_dir, \"rlg_diffuser.pth\"),\n                    map_location=torch.device(\"cpu\"),\n                )\n            )\n        with torch.no_grad():\n            return self.rlg_auto(torch.tensor([0.0])), self.rlg_diffusion(torch.tensor([0.0]))\n\n    def synthesize(self, text, config, speaker_id=\"random\", voice_dirs=None, **kwargs):\n        \"\"\"Synthesize speech with the given input text.\n\n        Args:\n            text (str): Input text.\n            config (TortoiseConfig): Config with inference parameters.\n            speaker_id (str): One of the available speaker names. If `random`, it generates a random speaker.\n            voice_dirs (List[str]): List of paths that host reference audio files for speakers. Defaults to None.\n            **kwargs: Inference settings. See `inference()`.\n\n        Returns:\n            A dictionary of the output values with `wav` as output waveform, `deterministic_seed` as seed used at inference,\n            `text_input` as text token IDs after tokenizer, `voice_samples` as samples used for cloning, `conditioning_latents`\n            as latents used at inference.\n\n        \"\"\"\n\n        speaker_id = \"random\" if speaker_id is None else speaker_id\n\n        if voice_dirs is not None:\n            voice_dirs = [voice_dirs]\n            voice_samples, conditioning_latents = load_voice(speaker_id, voice_dirs)\n\n        else:\n            voice_samples, conditioning_latents = load_voice(speaker_id)\n\n        outputs = self.inference_with_config(\n            text, config, voice_samples=voice_samples, conditioning_latents=conditioning_latents, **kwargs\n        )\n\n        return_dict = {\n            \"wav\": outputs[\"wav\"],\n            \"deterministic_seed\": outputs[\"deterministic_seed\"],\n            \"text_inputs\": outputs[\"text\"],\n            \"voice_samples\": outputs[\"voice_samples\"],\n            \"conditioning_latents\": outputs[\"conditioning_latents\"],\n        }\n\n        return return_dict\n\n    def inference_with_config(self, text, config, **kwargs):\n        \"\"\"\n        inference with config\n        #TODO describe in detail\n        \"\"\"\n        # Use generally found best tuning knobs for generation.\n        settings = {\n            \"temperature\": config.temperature,\n            \"length_penalty\": config.length_penalty,\n            \"repetition_penalty\": config.repetition_penalty,\n            \"top_p\": config.top_p,\n            \"cond_free_k\": config.cond_free_k,\n            \"diffusion_temperature\": config.diffusion_temperature,\n            \"sampler\": config.sampler,\n        }\n        # Presets are defined here.\n        presets = {\n            \"single_sample\": {\n                \"num_autoregressive_samples\": 8,\n                \"diffusion_iterations\": 10,\n                \"sampler\": \"ddim\",\n            },\n            \"ultra_fast\": {\n                \"num_autoregressive_samples\": 16,\n                \"diffusion_iterations\": 10,\n                \"sampler\": \"ddim\",\n            },\n            \"ultra_fast_old\": {\n                \"num_autoregressive_samples\": 16,\n                \"diffusion_iterations\": 30,\n                \"cond_free\": False,\n            },\n            \"very_fast\": {\n                \"num_autoregressive_samples\": 32,\n                \"diffusion_iterations\": 30,\n                \"sampler\": \"dpm++2m\",\n            },\n            \"fast\": {\n                \"num_autoregressive_samples\": 5,\n                \"diffusion_iterations\": 50,\n                \"sampler\": \"ddim\",\n            },\n            \"fast_old\": {\"num_autoregressive_samples\": 96, \"diffusion_iterations\": 80},\n            \"standard\": {\n                \"num_autoregressive_samples\": 5,\n                \"diffusion_iterations\": 200,\n            },\n            \"high_quality\": {\n                \"num_autoregressive_samples\": 256,\n                \"diffusion_iterations\": 400,\n            },\n        }\n        if \"preset\" in kwargs:\n            settings.update(presets[kwargs[\"preset\"]])\n            kwargs.pop(\"preset\")\n        settings.update(kwargs)  # allow overriding of preset settings with kwargs\n        return self.inference(text, **settings)\n\n    def inference(\n        self,\n        text,\n        voice_samples=None,\n        conditioning_latents=None,\n        k=1,\n        verbose=True,\n        use_deterministic_seed=None,\n        return_deterministic_state=False,\n        latent_averaging_mode=0,\n        # autoregressive generation parameters follow\n        num_autoregressive_samples=16,\n        temperature=0.8,\n        length_penalty=1,\n        repetition_penalty=2.0,\n        top_p=0.8,\n        max_mel_tokens=500,\n        # diffusion generation parameters follow\n        diffusion_iterations=100,\n        cond_free=True,\n        cond_free_k=2,\n        diffusion_temperature=1.0,\n        sampler=\"ddim\",\n        half=True,\n        original_tortoise=False,\n        **hf_generate_kwargs,\n    ):\n        \"\"\"\n        This function produces an audio clip of the given text being spoken with the given reference voice.\n\n        Args:\n            text: (str) Text to be spoken.\n            voice_samples: (List[Tuple[torch.Tensor]]) List of an arbitrary number of reference clips, which should be tuple-pairs\n                of torch tensors containing arbitrary kHz waveform data.\n            conditioning_latents: (Tuple[autoregressive_conditioning_latent, diffusion_conditioning_latent]) A tuple of\n                (autoregressive_conditioning_latent, diffusion_conditioning_latent), which can be provided in lieu\n                of voice_samples. This is ignored unless `voice_samples=None`. Conditioning latents can be retrieved\n                via `get_conditioning_latents()`.\n            k: (int) The number of returned clips. The most likely (as determined by Tortoises' CLVP model) clips are returned.\n                latent_averaging_mode: (int) 0/1/2 for following modes:\n                0 - latents will be generated as in original tortoise, using ~4.27s from each voice sample, averaging latent across all samples\n                1 - latents will be generated using (almost) entire voice samples, averaged across all the ~4.27s chunks\n                2 - latents will be generated using (almost) entire voice samples, averaged per voice sample\n            verbose: (bool) Whether or not to print log messages indicating the progress of creating a clip. Default=true.\n            num_autoregressive_samples: (int) Number of samples taken from the autoregressive model, all of which are filtered using CLVP.\n                As Tortoise is a probabilistic model, more samples means a higher probability of creating something \"great\".\n            temperature: (float) The softmax temperature of the autoregressive model.\n            length_penalty: (float) A length penalty applied to the autoregressive decoder. Higher settings causes the model to produce more terse outputs.\n            repetition_penalty: (float) A penalty that prevents the autoregressive decoder from repeating itself during decoding. Can be used to reduce\n                the incidence of long silences or \"uhhhhhhs\", etc.\n            top_p: (float) P value used in nucleus sampling. (0,1]. Lower values mean the decoder produces more \"likely\" (aka boring) outputs.\n            max_mel_tokens: (int) Restricts the output length. (0,600] integer. Each unit is 1/20 of a second.\n            typical_sampling: (bool) Turns typical sampling on or off. This sampling mode is discussed in this paper: https://arxiv.org/abs/2202.00666\n                I was interested in the premise, but the results were not as good as I was hoping. This is off by default, but could use some tuning.\n            typical_mass: (float) The typical_mass parameter from the typical_sampling algorithm.\n            diffusion_iterations: (int) Number of diffusion steps to perform. [0,4000]. More steps means the network has more chances to iteratively\n                refine the output, which should theoretically mean a higher quality output. Generally a value above 250 is not noticeably better, however.\n            cond_free: (bool) Whether or not to perform conditioning-free diffusion. Conditioning-free diffusion performs two forward passes for\n                each diffusion step: one with the outputs of the autoregressive model and one with no conditioning priors. The output of the two\n                is blended according to the cond_free_k value below. Conditioning-free diffusion is the real deal, and dramatically improves realism.\n            cond_free_k: (float) Knob that determines how to balance the conditioning free signal with the conditioning-present signal. [0,inf].\n                As cond_free_k increases, the output becomes dominated by the conditioning-free signal.\n            diffusion_temperature: (float) Controls the variance of the noise fed into the diffusion model. [0,1]. Values at 0\n                                      are the \"mean\" prediction of the diffusion network and will sound bland and smeared.\n            hf_generate_kwargs: (**kwargs) The huggingface Transformers generate API is used for the autoregressive transformer.\n                                    Extra keyword args fed to this function get forwarded directly to that API. Documentation\n                                    here: https://huggingface.co/docs/transformers/internal/generation_utils\n\n        Returns:\n            Generated audio clip(s) as a torch tensor. Shape 1,S if k=1 else, (k,1,S) where S is the sample length.\n            Sample rate is 24kHz.\n        \"\"\"\n        deterministic_seed = deterministic_state(seed=use_deterministic_seed)\n\n        text_tokens = torch.IntTensor(self.tokenizer.encode(text)).unsqueeze(0).to(self.device)\n        text_tokens = F.pad(text_tokens, (0, 1))  # This may not be necessary.\n        assert (\n            text_tokens.shape[-1] < 400\n        ), \"Too much text provided. Break the text up into separate segments and re-try inference.\"\n\n        if voice_samples is not None:\n            (\n                auto_conditioning,\n                diffusion_conditioning,\n                _,\n                _,\n            ) = self.get_conditioning_latents(\n                voice_samples,\n                return_mels=True,\n                latent_averaging_mode=latent_averaging_mode,\n                original_tortoise=original_tortoise,\n            )\n        elif conditioning_latents is not None:\n            auto_conditioning, diffusion_conditioning = conditioning_latents\n        else:\n            (\n                auto_conditioning,\n                diffusion_conditioning,\n            ) = self.get_random_conditioning_latents()\n        auto_conditioning = auto_conditioning.to(self.device)\n        diffusion_conditioning = diffusion_conditioning.to(self.device)\n\n        diffuser = load_discrete_vocoder_diffuser(\n            desired_diffusion_steps=diffusion_iterations, cond_free=cond_free, cond_free_k=cond_free_k, sampler=sampler\n        )\n\n        # in the case of single_sample,\n        orig_batch_size = self.autoregressive_batch_size\n        while num_autoregressive_samples % self.autoregressive_batch_size:\n            self.autoregressive_batch_size //= 2\n        with torch.no_grad():\n            samples = []\n            num_batches = num_autoregressive_samples // self.autoregressive_batch_size\n            stop_mel_token = self.autoregressive.stop_mel_token\n            calm_token = (\n                83  # This is the token for coding silence, which is fixed in place with \"fix_autoregressive_output\"\n            )\n            self.autoregressive = self.autoregressive.to(self.device)\n            if verbose:\n                print(\"Generating autoregressive samples..\")\n            with self.temporary_cuda(self.autoregressive) as autoregressive, torch.autocast(\n                device_type=\"cuda\", dtype=torch.float16, enabled=half\n            ):\n                for b in tqdm(range(num_batches), disable=not verbose):\n                    codes = autoregressive.inference_speech(\n                        auto_conditioning,\n                        text_tokens,\n                        do_sample=True,\n                        top_p=top_p,\n                        temperature=temperature,\n                        num_return_sequences=self.autoregressive_batch_size,\n                        length_penalty=length_penalty,\n                        repetition_penalty=repetition_penalty,\n                        max_generate_length=max_mel_tokens,\n                        **hf_generate_kwargs,\n                    )\n                    padding_needed = max_mel_tokens - codes.shape[1]\n                    codes = F.pad(codes, (0, padding_needed), value=stop_mel_token)\n                    samples.append(codes)\n            self.autoregressive_batch_size = orig_batch_size  # in the case of single_sample\n\n            clip_results = []\n            with self.temporary_cuda(self.clvp) as clvp, torch.autocast(\n                device_type=\"cuda\", dtype=torch.float16, enabled=half\n            ):\n                for batch in tqdm(samples, disable=not verbose):\n                    for i in range(batch.shape[0]):\n                        batch[i] = fix_autoregressive_output(batch[i], stop_mel_token)\n                    clvp_res = clvp(\n                        text_tokens.repeat(batch.shape[0], 1),\n                        batch,\n                        return_loss=False,\n                    )\n                    clip_results.append(clvp_res)\n\n                clip_results = torch.cat(clip_results, dim=0)\n                samples = torch.cat(samples, dim=0)\n                best_results = samples[torch.topk(clip_results, k=k).indices]\n            del samples\n\n            # The diffusion model actually wants the last hidden layer from the autoregressive model as conditioning\n            # inputs. Re-produce those for the top results. This could be made more efficient by storing all of these\n            # results, but will increase memory usage.\n            with self.temporary_cuda(self.autoregressive) as autoregressive:\n                best_latents = autoregressive(\n                    auto_conditioning.repeat(k, 1),\n                    text_tokens.repeat(k, 1),\n                    torch.tensor([text_tokens.shape[-1]], device=text_tokens.device),\n                    best_results,\n                    torch.tensor(\n                        [best_results.shape[-1] * self.autoregressive.mel_length_compression],\n                        device=text_tokens.device,\n                    ),\n                    return_latent=True,\n                    clip_inputs=False,\n                )\n            del auto_conditioning\n\n            if verbose:\n                print(\"Transforming autoregressive outputs into audio..\")\n            wav_candidates = []\n            for b in range(best_results.shape[0]):\n                codes = best_results[b].unsqueeze(0)\n                latents = best_latents[b].unsqueeze(0)\n\n                # Find the first occurrence of the \"calm\" token and trim the codes to that.\n                ctokens = 0\n                for code in range(codes.shape[-1]):\n                    if codes[0, code] == calm_token:\n                        ctokens += 1\n                    else:\n                        ctokens = 0\n                    if ctokens > 8:  # 8 tokens gives the diffusion model some \"breathing room\" to terminate speech.\n                        latents = latents[:, :code]\n                        break\n                with self.temporary_cuda(self.diffusion) as diffusion:\n                    mel = do_spectrogram_diffusion(\n                        diffusion,\n                        diffuser,\n                        latents,\n                        diffusion_conditioning,\n                        temperature=diffusion_temperature,\n                        verbose=verbose,\n                    )\n                with self.temporary_cuda(self.vocoder) as vocoder:\n                    wav = vocoder.inference(mel)\n                    wav_candidates.append(wav.cpu())\n\n            def potentially_redact(clip, text):\n                if self.enable_redaction:\n                    return self.aligner.redact(clip.squeeze(1), text).unsqueeze(1)\n                return clip\n\n            wav_candidates = [potentially_redact(wav_candidate, text) for wav_candidate in wav_candidates]\n\n            if len(wav_candidates) > 1:\n                res = wav_candidates\n            else:\n                res = wav_candidates[0]\n\n        return_dict = {\n            \"wav\": res,\n            \"deterministic_seed\": None,\n            \"text\": None,\n            \"voice_samples\": None,\n            \"conditioning_latents\": None,\n        }\n        if return_deterministic_state:\n            return_dict = {\n                \"wav\": res,\n                \"deterministic_seed\": deterministic_seed,\n                \"text\": text,\n                \"voice_samples\": voice_samples,\n                \"conditioning_latents\": conditioning_latents,\n            }\n        return return_dict\n\n    def forward(self):\n        raise NotImplementedError(\"Tortoise Training is not implemented\")\n\n    def eval_step(self):\n        raise NotImplementedError(\"Tortoise Training is not implemented\")\n\n    @staticmethod\n    def init_from_config(config: \"TortoiseConfig\", **kwargs):  # pylint: disable=unused-argument\n        return Tortoise(config)\n\n    def load_checkpoint(\n        self,\n        config,\n        checkpoint_dir,\n        ar_checkpoint_path=None,\n        diff_checkpoint_path=None,\n        clvp_checkpoint_path=None,\n        vocoder_checkpoint_path=None,\n        eval=False,\n        strict=True,\n        **kwargs,\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        \"\"\"Load a model checkpoints from a directory. This model is with multiple checkpoint files and it\n        expects to have all the files to be under the given `checkpoint_dir` with the rigth names.\n        If eval is True, set the model to eval mode.\n\n        Args:\n            config (TortoiseConfig): The model config.\n            checkpoint_dir (str): The directory where the checkpoints are stored.\n            ar_checkpoint_path (str, optional): The path to the autoregressive checkpoint. Defaults to None.\n            diff_checkpoint_path (str, optional): The path to the diffusion checkpoint. Defaults to None.\n            clvp_checkpoint_path (str, optional): The path to the CLVP checkpoint. Defaults to None.\n            vocoder_checkpoint_path (str, optional): The path to the vocoder checkpoint. Defaults to None.\n            eval (bool, optional): Whether to set the model to eval mode. Defaults to False.\n            strict (bool, optional): Whether to load the model strictly. Defaults to True.\n        \"\"\"\n        if self.models_dir is None:\n            self.models_dir = checkpoint_dir\n        ar_path = ar_checkpoint_path or os.path.join(checkpoint_dir, \"autoregressive.pth\")\n        diff_path = diff_checkpoint_path or os.path.join(checkpoint_dir, \"diffusion_decoder.pth\")\n        clvp_path = clvp_checkpoint_path or os.path.join(checkpoint_dir, \"clvp2.pth\")\n        vocoder_checkpoint_path = vocoder_checkpoint_path or os.path.join(checkpoint_dir, \"vocoder.pth\")\n        self.mel_norm_path = os.path.join(checkpoint_dir, \"mel_norms.pth\")\n\n        if os.path.exists(ar_path):\n            # remove keys from the checkpoint that are not in the model\n            checkpoint = torch.load(ar_path, map_location=torch.device(\"cpu\"))\n\n            # strict set False\n            # due to removed `bias` and `masked_bias` changes in Transformers\n            self.autoregressive.load_state_dict(checkpoint, strict=False)\n\n        if os.path.exists(diff_path):\n            self.diffusion.load_state_dict(torch.load(diff_path), strict=strict)\n\n        if os.path.exists(clvp_path):\n            self.clvp.load_state_dict(torch.load(clvp_path), strict=strict)\n\n        if os.path.exists(vocoder_checkpoint_path):\n            self.vocoder.load_state_dict(\n                config.model_args.vocoder.value.optionally_index(\n                    torch.load(\n                        vocoder_checkpoint_path,\n                        map_location=torch.device(\"cpu\"),\n                    )\n                )\n            )\n\n        if eval:\n            self.autoregressive.post_init_gpt2_config(self.args.kv_cache)\n            self.autoregressive.eval()\n            self.diffusion.eval()\n            self.clvp.eval()\n            self.vocoder.eval()\n\n    def train_step(self):\n        raise NotImplementedError(\"Tortoise Training is not implemented\")\n", "TTS/tts/models/base_tts.py": "import os\nimport random\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nimport torch.distributed as dist\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom trainer.torch import DistributedSampler, DistributedSamplerWrapper\n\nfrom TTS.model import BaseTrainerModel\nfrom TTS.tts.datasets.dataset import TTSDataset\nfrom TTS.tts.utils.data import get_length_balancer_weights\nfrom TTS.tts.utils.languages import LanguageManager, get_language_balancer_weights\nfrom TTS.tts.utils.speakers import SpeakerManager, get_speaker_balancer_weights, get_speaker_manager\nfrom TTS.tts.utils.synthesis import synthesis\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n\n# pylint: skip-file\n\n\nclass BaseTTS(BaseTrainerModel):\n    \"\"\"Base `tts` class. Every new `tts` model must inherit this.\n\n    It defines common `tts` specific functions on top of `Model` implementation.\n    \"\"\"\n\n    MODEL_TYPE = \"tts\"\n\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\",\n        tokenizer: \"TTSTokenizer\",\n        speaker_manager: SpeakerManager = None,\n        language_manager: LanguageManager = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.ap = ap\n        self.tokenizer = tokenizer\n        self.speaker_manager = speaker_manager\n        self.language_manager = language_manager\n        self._set_model_args(config)\n\n    def _set_model_args(self, config: Coqpit):\n        \"\"\"Setup model args based on the config type (`ModelConfig` or `ModelArgs`).\n\n        `ModelArgs` has all the fields reuqired to initialize the model architecture.\n\n        `ModelConfig` has all the fields required for training, inference and containes `ModelArgs`.\n\n        If the config is for training with a name like \"*Config\", then the model args are embeded in the\n        config.model_args\n\n        If the config is for the model with a name like \"*Args\", then we assign the directly.\n        \"\"\"\n        # don't use isintance not to import recursively\n        if \"Config\" in config.__class__.__name__:\n            config_num_chars = (\n                self.config.model_args.num_chars if hasattr(self.config, \"model_args\") else self.config.num_chars\n            )\n            num_chars = config_num_chars if self.tokenizer is None else self.tokenizer.characters.num_chars\n            if \"characters\" in config:\n                self.config.num_chars = num_chars\n                if hasattr(self.config, \"model_args\"):\n                    config.model_args.num_chars = num_chars\n                    self.args = self.config.model_args\n            else:\n                self.config = config\n                self.args = config.model_args\n        elif \"Args\" in config.__class__.__name__:\n            self.args = config\n        else:\n            raise ValueError(\"config must be either a *Config or *Args\")\n\n    def init_multispeaker(self, config: Coqpit, data: List = None):\n        \"\"\"Initialize a speaker embedding layer if needen and define expected embedding channel size for defining\n        `in_channels` size of the connected layers.\n\n        This implementation yields 3 possible outcomes:\n\n        1. If `config.use_speaker_embedding` and `config.use_d_vector_file are False, do nothing.\n        2. If `config.use_d_vector_file` is True, set expected embedding channel size to `config.d_vector_dim` or 512.\n        3. If `config.use_speaker_embedding`, initialize a speaker embedding layer with channel size of\n        `config.d_vector_dim` or 512.\n\n        You can override this function for new models.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        # set number of speakers\n        if self.speaker_manager is not None:\n            self.num_speakers = self.speaker_manager.num_speakers\n        elif hasattr(config, \"num_speakers\"):\n            self.num_speakers = config.num_speakers\n\n        # set ultimate speaker embedding size\n        if config.use_speaker_embedding or config.use_d_vector_file:\n            self.embedded_speaker_dim = (\n                config.d_vector_dim if \"d_vector_dim\" in config and config.d_vector_dim is not None else 512\n            )\n        # init speaker embedding layer\n        if config.use_speaker_embedding and not config.use_d_vector_file:\n            print(\" > Init speaker_embedding layer.\")\n            self.speaker_embedding = nn.Embedding(self.num_speakers, self.embedded_speaker_dim)\n            self.speaker_embedding.weight.data.normal_(0, 0.3)\n\n    def get_aux_input(self, **kwargs) -> Dict:\n        \"\"\"Prepare and return `aux_input` used by `forward()`\"\"\"\n        return {\"speaker_id\": None, \"style_wav\": None, \"d_vector\": None, \"language_id\": None}\n\n    def get_aux_input_from_test_sentences(self, sentence_info):\n        if hasattr(self.config, \"model_args\"):\n            config = self.config.model_args\n        else:\n            config = self.config\n\n        # extract speaker and language info\n        text, speaker_name, style_wav, language_name = None, None, None, None\n\n        if isinstance(sentence_info, list):\n            if len(sentence_info) == 1:\n                text = sentence_info[0]\n            elif len(sentence_info) == 2:\n                text, speaker_name = sentence_info\n            elif len(sentence_info) == 3:\n                text, speaker_name, style_wav = sentence_info\n            elif len(sentence_info) == 4:\n                text, speaker_name, style_wav, language_name = sentence_info\n        else:\n            text = sentence_info\n\n        # get speaker  id/d_vector\n        speaker_id, d_vector, language_id = None, None, None\n        if self.speaker_manager is not None:\n            if config.use_d_vector_file:\n                if speaker_name is None:\n                    d_vector = self.speaker_manager.get_random_embedding()\n                else:\n                    d_vector = self.speaker_manager.get_d_vector_by_name(speaker_name)\n            elif config.use_speaker_embedding:\n                if speaker_name is None:\n                    speaker_id = self.speaker_manager.get_random_id()\n                else:\n                    speaker_id = self.speaker_manager.name_to_id[speaker_name]\n\n        # get language id\n        if self.language_manager is not None and config.use_language_embedding and language_name is not None:\n            language_id = self.language_manager.name_to_id[language_name]\n\n        return {\n            \"text\": text,\n            \"speaker_id\": speaker_id,\n            \"style_wav\": style_wav,\n            \"d_vector\": d_vector,\n            \"language_id\": language_id,\n        }\n\n    def format_batch(self, batch: Dict) -> Dict:\n        \"\"\"Generic batch formatting for `TTSDataset`.\n\n        You must override this if you use a custom dataset.\n\n        Args:\n            batch (Dict): [description]\n\n        Returns:\n            Dict: [description]\n        \"\"\"\n        # setup input batch\n        text_input = batch[\"token_id\"]\n        text_lengths = batch[\"token_id_lengths\"]\n        speaker_names = batch[\"speaker_names\"]\n        linear_input = batch[\"linear\"]\n        mel_input = batch[\"mel\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        stop_targets = batch[\"stop_targets\"]\n        item_idx = batch[\"item_idxs\"]\n        d_vectors = batch[\"d_vectors\"]\n        speaker_ids = batch[\"speaker_ids\"]\n        attn_mask = batch[\"attns\"]\n        waveform = batch[\"waveform\"]\n        pitch = batch[\"pitch\"]\n        energy = batch[\"energy\"]\n        language_ids = batch[\"language_ids\"]\n        max_text_length = torch.max(text_lengths.float())\n        max_spec_length = torch.max(mel_lengths.float())\n\n        # compute durations from attention masks\n        durations = None\n        if attn_mask is not None:\n            durations = torch.zeros(attn_mask.shape[0], attn_mask.shape[2])\n            for idx, am in enumerate(attn_mask):\n                # compute raw durations\n                c_idxs = am[:, : text_lengths[idx], : mel_lengths[idx]].max(1)[1]\n                # c_idxs, counts = torch.unique_consecutive(c_idxs, return_counts=True)\n                c_idxs, counts = torch.unique(c_idxs, return_counts=True)\n                dur = torch.ones([text_lengths[idx]]).to(counts.dtype)\n                dur[c_idxs] = counts\n                # smooth the durations and set any 0 duration to 1\n                # by cutting off from the largest duration indeces.\n                extra_frames = dur.sum() - mel_lengths[idx]\n                largest_idxs = torch.argsort(-dur)[:extra_frames]\n                dur[largest_idxs] -= 1\n                assert (\n                    dur.sum() == mel_lengths[idx]\n                ), f\" [!] total duration {dur.sum()} vs spectrogram length {mel_lengths[idx]}\"\n                durations[idx, : text_lengths[idx]] = dur\n\n        # set stop targets wrt reduction factor\n        stop_targets = stop_targets.view(text_input.shape[0], stop_targets.size(1) // self.config.r, -1)\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze(2)\n        stop_target_lengths = torch.divide(mel_lengths, self.config.r).ceil_()\n\n        return {\n            \"text_input\": text_input,\n            \"text_lengths\": text_lengths,\n            \"speaker_names\": speaker_names,\n            \"mel_input\": mel_input,\n            \"mel_lengths\": mel_lengths,\n            \"linear_input\": linear_input,\n            \"stop_targets\": stop_targets,\n            \"stop_target_lengths\": stop_target_lengths,\n            \"attn_mask\": attn_mask,\n            \"durations\": durations,\n            \"speaker_ids\": speaker_ids,\n            \"d_vectors\": d_vectors,\n            \"max_text_length\": float(max_text_length),\n            \"max_spec_length\": float(max_spec_length),\n            \"item_idx\": item_idx,\n            \"waveform\": waveform,\n            \"pitch\": pitch,\n            \"energy\": energy,\n            \"language_ids\": language_ids,\n            \"audio_unique_names\": batch[\"audio_unique_names\"],\n        }\n\n    def get_sampler(self, config: Coqpit, dataset: TTSDataset, num_gpus=1):\n        weights = None\n        data_items = dataset.samples\n\n        if getattr(config, \"use_language_weighted_sampler\", False):\n            alpha = getattr(config, \"language_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Language weighted sampler with alpha:\", alpha)\n            weights = get_language_balancer_weights(data_items) * alpha\n\n        if getattr(config, \"use_speaker_weighted_sampler\", False):\n            alpha = getattr(config, \"speaker_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Speaker weighted sampler with alpha:\", alpha)\n            if weights is not None:\n                weights += get_speaker_balancer_weights(data_items) * alpha\n            else:\n                weights = get_speaker_balancer_weights(data_items) * alpha\n\n        if getattr(config, \"use_length_weighted_sampler\", False):\n            alpha = getattr(config, \"length_weighted_sampler_alpha\", 1.0)\n            print(\" > Using Length weighted sampler with alpha:\", alpha)\n            if weights is not None:\n                weights += get_length_balancer_weights(data_items) * alpha\n            else:\n                weights = get_length_balancer_weights(data_items) * alpha\n\n        if weights is not None:\n            sampler = WeightedRandomSampler(weights, len(weights))\n        else:\n            sampler = None\n\n        # sampler for DDP\n        if sampler is None:\n            sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        else:  # If a sampler is already defined use this sampler and DDP sampler together\n            sampler = DistributedSamplerWrapper(sampler) if num_gpus > 1 else sampler\n\n        return sampler\n\n    def get_data_loader(\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: bool,\n        samples: Union[List[Dict], List[List]],\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,\n    ) -> \"DataLoader\":\n        if is_eval and not config.run_eval:\n            loader = None\n        else:\n            # setup multi-speaker attributes\n            if self.speaker_manager is not None:\n                if hasattr(config, \"model_args\"):\n                    speaker_id_mapping = (\n                        self.speaker_manager.name_to_id if config.model_args.use_speaker_embedding else None\n                    )\n                    d_vector_mapping = self.speaker_manager.embeddings if config.model_args.use_d_vector_file else None\n                    config.use_d_vector_file = config.model_args.use_d_vector_file\n                else:\n                    speaker_id_mapping = self.speaker_manager.name_to_id if config.use_speaker_embedding else None\n                    d_vector_mapping = self.speaker_manager.embeddings if config.use_d_vector_file else None\n            else:\n                speaker_id_mapping = None\n                d_vector_mapping = None\n\n            # setup multi-lingual attributes\n            if self.language_manager is not None:\n                language_id_mapping = self.language_manager.name_to_id if self.args.use_language_embedding else None\n            else:\n                language_id_mapping = None\n\n            # init dataloader\n            dataset = TTSDataset(\n                outputs_per_step=config.r if \"r\" in config else 1,\n                compute_linear_spec=config.model.lower() == \"tacotron\" or config.compute_linear_spec,\n                compute_f0=config.get(\"compute_f0\", False),\n                f0_cache_path=config.get(\"f0_cache_path\", None),\n                compute_energy=config.get(\"compute_energy\", False),\n                energy_cache_path=config.get(\"energy_cache_path\", None),\n                samples=samples,\n                ap=self.ap,\n                return_wav=config.return_wav if \"return_wav\" in config else False,\n                batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n                min_text_len=config.min_text_len,\n                max_text_len=config.max_text_len,\n                min_audio_len=config.min_audio_len,\n                max_audio_len=config.max_audio_len,\n                phoneme_cache_path=config.phoneme_cache_path,\n                precompute_num_workers=config.precompute_num_workers,\n                use_noise_augment=False if is_eval else config.use_noise_augment,\n                verbose=verbose,\n                speaker_id_mapping=speaker_id_mapping,\n                d_vector_mapping=d_vector_mapping if config.use_d_vector_file else None,\n                tokenizer=self.tokenizer,\n                start_by_longest=config.start_by_longest,\n                language_id_mapping=language_id_mapping,\n            )\n\n            # wait all the DDP process to be ready\n            if num_gpus > 1:\n                dist.barrier()\n\n            # sort input sequences from short to long\n            dataset.preprocess_samples()\n\n            # get samplers\n            sampler = self.get_sampler(config, dataset, num_gpus)\n\n            loader = DataLoader(\n                dataset,\n                batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                shuffle=config.shuffle if sampler is None else False,  # if there is no other sampler\n                collate_fn=dataset.collate_fn,\n                drop_last=config.drop_last,  # setting this False might cause issues in AMP training.\n                sampler=sampler,\n                num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                pin_memory=False,\n            )\n        return loader\n\n    def _get_test_aux_input(\n        self,\n    ) -> Dict:\n        d_vector = None\n        if self.config.use_d_vector_file:\n            d_vector = [self.speaker_manager.embeddings[name][\"embedding\"] for name in self.speaker_manager.embeddings]\n            d_vector = (random.sample(sorted(d_vector), 1),)\n\n        aux_inputs = {\n            \"speaker_id\": None\n            if not self.config.use_speaker_embedding\n            else random.sample(sorted(self.speaker_manager.name_to_id.values()), 1),\n            \"d_vector\": d_vector,\n            \"style_wav\": None,  # TODO: handle GST style input\n        }\n        return aux_inputs\n\n    def test_run(self, assets: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `tts` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Args:\n            assets (dict): A dict of training assets. For `tts` models, it must include `{'audio_processor': ap}`.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        aux_inputs = self._get_test_aux_input()\n        for idx, sen in enumerate(test_sentences):\n            if isinstance(sen, list):\n                aux_inputs = self.get_aux_input_from_test_sentences(sen)\n                sen = aux_inputs[\"text\"]\n            outputs_dict = synthesis(\n                self,\n                sen,\n                self.config,\n                \"cuda\" in str(next(self.parameters()).device),\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n                style_wav=aux_inputs[\"style_wav\"],\n                use_griffin_lim=True,\n                do_trim_silence=False,\n            )\n            test_audios[\"{}-audio\".format(idx)] = outputs_dict[\"wav\"]\n            test_figures[\"{}-prediction\".format(idx)] = plot_spectrogram(\n                outputs_dict[\"outputs\"][\"model_outputs\"], self.ap, output_fig=False\n            )\n            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(\n                outputs_dict[\"outputs\"][\"alignments\"], output_fig=False\n            )\n        return test_figures, test_audios\n\n    def on_init_start(self, trainer):\n        \"\"\"Save the speaker.pth and language_ids.json at the beginning of the training. Also update both paths.\"\"\"\n        if self.speaker_manager is not None:\n            output_path = os.path.join(trainer.output_path, \"speakers.pth\")\n            self.speaker_manager.save_ids_to_file(output_path)\n            trainer.config.speakers_file = output_path\n            # some models don't have `model_args` set\n            if hasattr(trainer.config, \"model_args\"):\n                trainer.config.model_args.speakers_file = output_path\n            trainer.config.save_json(os.path.join(trainer.output_path, \"config.json\"))\n            print(f\" > `speakers.pth` is saved to {output_path}.\")\n            print(\" > `speakers_file` is updated in the config.json.\")\n\n        if self.language_manager is not None:\n            output_path = os.path.join(trainer.output_path, \"language_ids.json\")\n            self.language_manager.save_ids_to_file(output_path)\n            trainer.config.language_ids_file = output_path\n            if hasattr(trainer.config, \"model_args\"):\n                trainer.config.model_args.language_ids_file = output_path\n            trainer.config.save_json(os.path.join(trainer.output_path, \"config.json\"))\n            print(f\" > `language_ids.json` is saved to {output_path}.\")\n            print(\" > `language_ids_file` is updated in the config.json.\")\n\n\nclass BaseTTSE2E(BaseTTS):\n    def _set_model_args(self, config: Coqpit):\n        self.config = config\n        if \"Config\" in config.__class__.__name__:\n            num_chars = (\n                self.config.model_args.num_chars if self.tokenizer is None else self.tokenizer.characters.num_chars\n            )\n            self.config.model_args.num_chars = num_chars\n            self.config.num_chars = num_chars\n            self.args = config.model_args\n            self.args.num_chars = num_chars\n        elif \"Args\" in config.__class__.__name__:\n            self.args = config\n            self.args.num_chars = self.args.num_chars\n        else:\n            raise ValueError(\"config must be either a *Config or *Args\")\n", "TTS/tts/models/vits.py": "import math\nimport os\nfrom dataclasses import dataclass, field, replace\nfrom itertools import chain\nfrom typing import Dict, List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torchaudio\nfrom coqpit import Coqpit\nfrom librosa.filters import mel as librosa_mel_fn\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom trainer.torch import DistributedSampler, DistributedSamplerWrapper\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.tts.configs.shared_configs import CharactersConfig\nfrom TTS.tts.datasets.dataset import TTSDataset, _parse_sample\nfrom TTS.tts.layers.glow_tts.duration_predictor import DurationPredictor\nfrom TTS.tts.layers.vits.discriminator import VitsDiscriminator\nfrom TTS.tts.layers.vits.networks import PosteriorEncoder, ResidualCouplingBlocks, TextEncoder\nfrom TTS.tts.layers.vits.stochastic_duration_predictor import StochasticDurationPredictor\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.fairseq import rehash_fairseq_vits_checkpoint\nfrom TTS.tts.utils.helpers import generate_path, maximum_path, rand_segments, segment, sequence_mask\nfrom TTS.tts.utils.languages import LanguageManager\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.synthesis import synthesis\nfrom TTS.tts.utils.text.characters import BaseCharacters, BaseVocabulary, _characters, _pad, _phonemes, _punctuations\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment\nfrom TTS.utils.io import load_fsspec\nfrom TTS.utils.samplers import BucketBatchSampler\nfrom TTS.vocoder.models.hifigan_generator import HifiganGenerator\nfrom TTS.vocoder.utils.generic_utils import plot_results\n\n##############################\n# IO / Feature extraction\n##############################\n\n# pylint: disable=global-statement\nhann_window = {}\nmel_basis = {}\n\n\n@torch.no_grad()\ndef weights_reset(m: nn.Module):\n    # check if the current module has reset_parameters and if it is reset the weight\n    reset_parameters = getattr(m, \"reset_parameters\", None)\n    if callable(reset_parameters):\n        m.reset_parameters()\n\n\ndef get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}\n    for name, w in mdl.named_parameters():\n        if \"weight\" in name:\n            value = w.data.sum().item()\n            dict_sums[name] = value\n    return dict_sums\n\n\ndef load_audio(file_path):\n    \"\"\"Load the audio file normalized in [-1, 1]\n\n    Return Shapes:\n        - x: :math:`[1, T]`\n    \"\"\"\n    x, sr = torchaudio.load(file_path)\n    assert (x > 1).sum() + (x < -1).sum() == 0\n    return x, sr\n\n\ndef _amp_to_db(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef _db_to_amp(x, C=1):\n    return torch.exp(x) / C\n\n\ndef amp_to_db(magnitudes):\n    output = _amp_to_db(magnitudes)\n    return output\n\n\ndef db_to_amp(magnitudes):\n    output = _db_to_amp(magnitudes)\n    return output\n\n\ndef wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"\n    y = y.squeeze(1)\n\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax):\n    \"\"\"\n    Args Shapes:\n        - spec : :math:`[B,C,T]`\n\n    Return Shapes:\n        - mel : :math:`[B,C,T]`\n    \"\"\"\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    mel = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    mel = amp_to_db(mel)\n    return mel\n\n\ndef wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"\n    y = y.squeeze(1)\n\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = amp_to_db(spec)\n    return spec\n\n\n#############################\n# CONFIGS\n#############################\n\n\n@dataclass\nclass VitsAudioConfig(Coqpit):\n    fft_size: int = 1024\n    sample_rate: int = 22050\n    win_length: int = 1024\n    hop_length: int = 256\n    num_mels: int = 80\n    mel_fmin: int = 0\n    mel_fmax: int = None\n\n\n##############################\n# DATASET\n##############################\n\n\ndef get_attribute_balancer_weights(items: list, attr_name: str, multi_dict: dict = None):\n    \"\"\"Create inverse frequency weights for balancing the dataset.\n    Use `multi_dict` to scale relative weights.\"\"\"\n    attr_names_samples = np.array([item[attr_name] for item in items])\n    unique_attr_names = np.unique(attr_names_samples).tolist()\n    attr_idx = [unique_attr_names.index(l) for l in attr_names_samples]\n    attr_count = np.array([len(np.where(attr_names_samples == l)[0]) for l in unique_attr_names])\n    weight_attr = 1.0 / attr_count\n    dataset_samples_weight = np.array([weight_attr[l] for l in attr_idx])\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n    if multi_dict is not None:\n        # check if all keys are in the multi_dict\n        for k in multi_dict:\n            assert k in unique_attr_names, f\"{k} not in {unique_attr_names}\"\n        # scale weights\n        multiplier_samples = np.array([multi_dict.get(item[attr_name], 1.0) for item in items])\n        dataset_samples_weight *= multiplier_samples\n    return (\n        torch.from_numpy(dataset_samples_weight).float(),\n        unique_attr_names,\n        np.unique(dataset_samples_weight).tolist(),\n    )\n\n\nclass VitsDataset(TTSDataset):\n    def __init__(self, model_args, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pad_id = self.tokenizer.characters.pad_id\n        self.model_args = model_args\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n        raw_text = item[\"text\"]\n\n        wav, _ = load_audio(item[\"audio_file\"])\n        if self.model_args.encoder_sample_rate is not None:\n            if wav.size(1) % self.model_args.encoder_sample_rate != 0:\n                wav = wav[:, : -int(wav.size(1) % self.model_args.encoder_sample_rate)]\n\n        wav_filename = os.path.basename(item[\"audio_file\"])\n\n        token_ids = self.get_token_ids(idx, item[\"text\"])\n\n        # after phonemization the text length may change\n        # this is a shameful \ud83e\udd2d hack to prevent longer phonemes\n        # TODO: find a better fix\n        if len(token_ids) > self.max_text_len or wav.shape[1] < self.min_audio_len:\n            self.rescue_item_idx += 1\n            return self.__getitem__(self.rescue_item_idx)\n\n        return {\n            \"raw_text\": raw_text,\n            \"token_ids\": token_ids,\n            \"token_len\": len(token_ids),\n            \"wav\": wav,\n            \"wav_file\": wav_filename,\n            \"speaker_name\": item[\"speaker_name\"],\n            \"language_name\": item[\"language\"],\n            \"audio_unique_name\": item[\"audio_unique_name\"],\n        }\n\n    @property\n    def lengths(self):\n        lens = []\n        for item in self.samples:\n            _, wav_file, *_ = _parse_sample(item)\n            audio_len = os.path.getsize(wav_file) / 16 * 8  # assuming 16bit audio\n            lens.append(audio_len)\n        return lens\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Return Shapes:\n            - tokens: :math:`[B, T]`\n            - token_lens :math:`[B]`\n            - token_rel_lens :math:`[B]`\n            - waveform: :math:`[B, 1, T]`\n            - waveform_lens: :math:`[B]`\n            - waveform_rel_lens: :math:`[B]`\n            - speaker_names: :math:`[B]`\n            - language_names: :math:`[B]`\n            - audiofile_paths: :math:`[B]`\n            - raw_texts: :math:`[B]`\n            - audio_unique_names: :math:`[B]`\n        \"\"\"\n        # convert list of dicts to dict of lists\n        B = len(batch)\n        batch = {k: [dic[k] for dic in batch] for k in batch[0]}\n\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x.size(1) for x in batch[\"wav\"]]), dim=0, descending=True\n        )\n\n        max_text_len = max([len(x) for x in batch[\"token_ids\"]])\n        token_lens = torch.LongTensor(batch[\"token_len\"])\n        token_rel_lens = token_lens / token_lens.max()\n\n        wav_lens = [w.shape[1] for w in batch[\"wav\"]]\n        wav_lens = torch.LongTensor(wav_lens)\n        wav_lens_max = torch.max(wav_lens)\n        wav_rel_lens = wav_lens / wav_lens_max\n\n        token_padded = torch.LongTensor(B, max_text_len)\n        wav_padded = torch.FloatTensor(B, 1, wav_lens_max)\n        token_padded = token_padded.zero_() + self.pad_id\n        wav_padded = wav_padded.zero_() + self.pad_id\n        for i in range(len(ids_sorted_decreasing)):\n            token_ids = batch[\"token_ids\"][i]\n            token_padded[i, : batch[\"token_len\"][i]] = torch.LongTensor(token_ids)\n\n            wav = batch[\"wav\"][i]\n            wav_padded[i, :, : wav.size(1)] = torch.FloatTensor(wav)\n\n        return {\n            \"tokens\": token_padded,\n            \"token_lens\": token_lens,\n            \"token_rel_lens\": token_rel_lens,\n            \"waveform\": wav_padded,  # (B x T)\n            \"waveform_lens\": wav_lens,  # (B)\n            \"waveform_rel_lens\": wav_rel_lens,\n            \"speaker_names\": batch[\"speaker_name\"],\n            \"language_names\": batch[\"language_name\"],\n            \"audio_files\": batch[\"wav_file\"],\n            \"raw_text\": batch[\"raw_text\"],\n            \"audio_unique_names\": batch[\"audio_unique_name\"],\n        }\n\n\n##############################\n# MODEL DEFINITION\n##############################\n\n\n@dataclass\nclass VitsArgs(Coqpit):\n    \"\"\"VITS model arguments.\n\n    Args:\n\n        num_chars (int):\n            Number of characters in the vocabulary. Defaults to 100.\n\n        out_channels (int):\n            Number of output channels of the decoder. Defaults to 513.\n\n        spec_segment_size (int):\n            Decoder input segment size. Defaults to 32 `(32 * hoplength = waveform length)`.\n\n        hidden_channels (int):\n            Number of hidden channels of the model. Defaults to 192.\n\n        hidden_channels_ffn_text_encoder (int):\n            Number of hidden channels of the feed-forward layers of the text encoder transformer. Defaults to 256.\n\n        num_heads_text_encoder (int):\n            Number of attention heads of the text encoder transformer. Defaults to 2.\n\n        num_layers_text_encoder (int):\n            Number of transformer layers in the text encoder. Defaults to 6.\n\n        kernel_size_text_encoder (int):\n            Kernel size of the text encoder transformer FFN layers. Defaults to 3.\n\n        dropout_p_text_encoder (float):\n            Dropout rate of the text encoder. Defaults to 0.1.\n\n        dropout_p_duration_predictor (float):\n            Dropout rate of the duration predictor. Defaults to 0.1.\n\n        kernel_size_posterior_encoder (int):\n            Kernel size of the posterior encoder's WaveNet layers. Defaults to 5.\n\n        dilatation_posterior_encoder (int):\n            Dilation rate of the posterior encoder's WaveNet layers. Defaults to 1.\n\n        num_layers_posterior_encoder (int):\n            Number of posterior encoder's WaveNet layers. Defaults to 16.\n\n        kernel_size_flow (int):\n            Kernel size of the Residual Coupling layers of the flow network. Defaults to 5.\n\n        dilatation_flow (int):\n            Dilation rate of the Residual Coupling WaveNet layers of the flow network. Defaults to 1.\n\n        num_layers_flow (int):\n            Number of Residual Coupling WaveNet layers of the flow network. Defaults to 6.\n\n        resblock_type_decoder (str):\n            Type of the residual block in the decoder network. Defaults to \"1\".\n\n        resblock_kernel_sizes_decoder (List[int]):\n            Kernel sizes of the residual blocks in the decoder network. Defaults to `[3, 7, 11]`.\n\n        resblock_dilation_sizes_decoder (List[List[int]]):\n            Dilation sizes of the residual blocks in the decoder network. Defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`.\n\n        upsample_rates_decoder (List[int]):\n            Upsampling rates for each concecutive upsampling layer in the decoder network. The multiply of these\n            values must be equal to the kop length used for computing spectrograms. Defaults to `[8, 8, 2, 2]`.\n\n        upsample_initial_channel_decoder (int):\n            Number of hidden channels of the first upsampling convolution layer of the decoder network. Defaults to 512.\n\n        upsample_kernel_sizes_decoder (List[int]):\n            Kernel sizes for each upsampling layer of the decoder network. Defaults to `[16, 16, 4, 4]`.\n\n        periods_multi_period_discriminator (List[int]):\n            Periods values for Vits Multi-Period Discriminator. Defaults to `[2, 3, 5, 7, 11]`.\n\n        use_sdp (bool):\n            Use Stochastic Duration Predictor. Defaults to True.\n\n        noise_scale (float):\n            Noise scale used for the sample noise tensor in training. Defaults to 1.0.\n\n        inference_noise_scale (float):\n            Noise scale used for the sample noise tensor in inference. Defaults to 0.667.\n\n        length_scale (float):\n            Scale factor for the predicted duration values. Smaller values result faster speech. Defaults to 1.\n\n        noise_scale_dp (float):\n            Noise scale used by the Stochastic Duration Predictor sample noise in training. Defaults to 1.0.\n\n        inference_noise_scale_dp (float):\n            Noise scale for the Stochastic Duration Predictor in inference. Defaults to 0.8.\n\n        max_inference_len (int):\n            Maximum inference length to limit the memory use. Defaults to None.\n\n        init_discriminator (bool):\n            Initialize the disciminator network if set True. Set False for inference. Defaults to True.\n\n        use_spectral_norm_disriminator (bool):\n            Use spectral normalization over weight norm in the discriminator. Defaults to False.\n\n        use_speaker_embedding (bool):\n            Enable/Disable speaker embedding for multi-speaker models. Defaults to False.\n\n        num_speakers (int):\n            Number of speakers for the speaker embedding layer. Defaults to 0.\n\n        speakers_file (str):\n            Path to the speaker mapping file for the Speaker Manager. Defaults to None.\n\n        speaker_embedding_channels (int):\n            Number of speaker embedding channels. Defaults to 256.\n\n        use_d_vector_file (bool):\n            Enable/Disable the use of d-vectors for multi-speaker training. Defaults to False.\n\n        d_vector_file (List[str]):\n            List of paths to the files including pre-computed speaker embeddings. Defaults to None.\n\n        d_vector_dim (int):\n            Number of d-vector channels. Defaults to 0.\n\n        detach_dp_input (bool):\n            Detach duration predictor's input from the network for stopping the gradients. Defaults to True.\n\n        use_language_embedding (bool):\n            Enable/Disable language embedding for multilingual models. Defaults to False.\n\n        embedded_language_dim (int):\n            Number of language embedding channels. Defaults to 4.\n\n        num_languages (int):\n            Number of languages for the language embedding layer. Defaults to 0.\n\n        language_ids_file (str):\n            Path to the language mapping file for the Language Manager. Defaults to None.\n\n        use_speaker_encoder_as_loss (bool):\n            Enable/Disable Speaker Consistency Loss (SCL). Defaults to False.\n\n        speaker_encoder_config_path (str):\n            Path to the file speaker encoder config file, to use for SCL. Defaults to \"\".\n\n        speaker_encoder_model_path (str):\n            Path to the file speaker encoder checkpoint file, to use for SCL. Defaults to \"\".\n\n        condition_dp_on_speaker (bool):\n            Condition the duration predictor on the speaker embedding. Defaults to True.\n\n        freeze_encoder (bool):\n            Freeze the encoder weigths during training. Defaults to False.\n\n        freeze_DP (bool):\n            Freeze the duration predictor weigths during training. Defaults to False.\n\n        freeze_PE (bool):\n            Freeze the posterior encoder weigths during training. Defaults to False.\n\n        freeze_flow_encoder (bool):\n            Freeze the flow encoder weigths during training. Defaults to False.\n\n        freeze_waveform_decoder (bool):\n            Freeze the waveform decoder weigths during training. Defaults to False.\n\n        encoder_sample_rate (int):\n            If not None this sample rate will be used for training the Posterior Encoder,\n            flow, text_encoder and duration predictor. The decoder part (vocoder) will be\n            trained with the `config.audio.sample_rate`. Defaults to None.\n\n        interpolate_z (bool):\n            If `encoder_sample_rate` not None and  this parameter True the nearest interpolation\n            will be used to upsampling the latent variable z with the sampling rate `encoder_sample_rate`\n            to the `config.audio.sample_rate`. If it is False you will need to add extra\n            `upsample_rates_decoder` to match the shape. Defaults to True.\n\n    \"\"\"\n\n    num_chars: int = 100\n    out_channels: int = 513\n    spec_segment_size: int = 32\n    hidden_channels: int = 192\n    hidden_channels_ffn_text_encoder: int = 768\n    num_heads_text_encoder: int = 2\n    num_layers_text_encoder: int = 6\n    kernel_size_text_encoder: int = 3\n    dropout_p_text_encoder: float = 0.1\n    dropout_p_duration_predictor: float = 0.5\n    kernel_size_posterior_encoder: int = 5\n    dilation_rate_posterior_encoder: int = 1\n    num_layers_posterior_encoder: int = 16\n    kernel_size_flow: int = 5\n    dilation_rate_flow: int = 1\n    num_layers_flow: int = 4\n    resblock_type_decoder: str = \"1\"\n    resblock_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [3, 7, 11])\n    resblock_dilation_sizes_decoder: List[List[int]] = field(default_factory=lambda: [[1, 3, 5], [1, 3, 5], [1, 3, 5]])\n    upsample_rates_decoder: List[int] = field(default_factory=lambda: [8, 8, 2, 2])\n    upsample_initial_channel_decoder: int = 512\n    upsample_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [16, 16, 4, 4])\n    periods_multi_period_discriminator: List[int] = field(default_factory=lambda: [2, 3, 5, 7, 11])\n    use_sdp: bool = True\n    noise_scale: float = 1.0\n    inference_noise_scale: float = 0.667\n    length_scale: float = 1\n    noise_scale_dp: float = 1.0\n    inference_noise_scale_dp: float = 1.0\n    max_inference_len: int = None\n    init_discriminator: bool = True\n    use_spectral_norm_disriminator: bool = False\n    use_speaker_embedding: bool = False\n    num_speakers: int = 0\n    speakers_file: str = None\n    d_vector_file: List[str] = None\n    speaker_embedding_channels: int = 256\n    use_d_vector_file: bool = False\n    d_vector_dim: int = 0\n    detach_dp_input: bool = True\n    use_language_embedding: bool = False\n    embedded_language_dim: int = 4\n    num_languages: int = 0\n    language_ids_file: str = None\n    use_speaker_encoder_as_loss: bool = False\n    speaker_encoder_config_path: str = \"\"\n    speaker_encoder_model_path: str = \"\"\n    condition_dp_on_speaker: bool = True\n    freeze_encoder: bool = False\n    freeze_DP: bool = False\n    freeze_PE: bool = False\n    freeze_flow_decoder: bool = False\n    freeze_waveform_decoder: bool = False\n    encoder_sample_rate: int = None\n    interpolate_z: bool = True\n    reinit_DP: bool = False\n    reinit_text_encoder: bool = False\n\n\nclass Vits(BaseTTS):\n    \"\"\"VITS TTS model\n\n    Paper::\n        https://arxiv.org/pdf/2106.06103.pdf\n\n    Paper Abstract::\n        Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel\n        sampling have been proposed, but their sample quality does not match that of two-stage TTS systems.\n        In this work, we present a parallel endto-end TTS method that generates more natural sounding audio than\n        current two-stage models. Our method adopts variational inference augmented with normalizing flows and\n        an adversarial training process, which improves the expressive power of generative modeling. We also propose a\n        stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the\n        uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the\n        natural one-to-many relationship in which a text input can be spoken in multiple ways\n        with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS)\n        on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly\n        available TTS systems and achieves a MOS comparable to ground truth.\n\n    Check :class:`TTS.tts.configs.vits_config.VitsConfig` for class arguments.\n\n    Examples:\n        >>> from TTS.tts.configs.vits_config import VitsConfig\n        >>> from TTS.tts.models.vits import Vits\n        >>> config = VitsConfig()\n        >>> model = Vits(config)\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n        language_manager: LanguageManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager, language_manager)\n\n        self.init_multispeaker(config)\n        self.init_multilingual(config)\n        self.init_upsampling()\n\n        self.length_scale = self.args.length_scale\n        self.noise_scale = self.args.noise_scale\n        self.inference_noise_scale = self.args.inference_noise_scale\n        self.inference_noise_scale_dp = self.args.inference_noise_scale_dp\n        self.noise_scale_dp = self.args.noise_scale_dp\n        self.max_inference_len = self.args.max_inference_len\n        self.spec_segment_size = self.args.spec_segment_size\n\n        self.text_encoder = TextEncoder(\n            self.args.num_chars,\n            self.args.hidden_channels,\n            self.args.hidden_channels,\n            self.args.hidden_channels_ffn_text_encoder,\n            self.args.num_heads_text_encoder,\n            self.args.num_layers_text_encoder,\n            self.args.kernel_size_text_encoder,\n            self.args.dropout_p_text_encoder,\n            language_emb_dim=self.embedded_language_dim,\n        )\n\n        self.posterior_encoder = PosteriorEncoder(\n            self.args.out_channels,\n            self.args.hidden_channels,\n            self.args.hidden_channels,\n            kernel_size=self.args.kernel_size_posterior_encoder,\n            dilation_rate=self.args.dilation_rate_posterior_encoder,\n            num_layers=self.args.num_layers_posterior_encoder,\n            cond_channels=self.embedded_speaker_dim,\n        )\n\n        self.flow = ResidualCouplingBlocks(\n            self.args.hidden_channels,\n            self.args.hidden_channels,\n            kernel_size=self.args.kernel_size_flow,\n            dilation_rate=self.args.dilation_rate_flow,\n            num_layers=self.args.num_layers_flow,\n            cond_channels=self.embedded_speaker_dim,\n        )\n\n        if self.args.use_sdp:\n            self.duration_predictor = StochasticDurationPredictor(\n                self.args.hidden_channels,\n                192,\n                3,\n                self.args.dropout_p_duration_predictor,\n                4,\n                cond_channels=self.embedded_speaker_dim if self.args.condition_dp_on_speaker else 0,\n                language_emb_dim=self.embedded_language_dim,\n            )\n        else:\n            self.duration_predictor = DurationPredictor(\n                self.args.hidden_channels,\n                256,\n                3,\n                self.args.dropout_p_duration_predictor,\n                cond_channels=self.embedded_speaker_dim,\n                language_emb_dim=self.embedded_language_dim,\n            )\n\n        self.waveform_decoder = HifiganGenerator(\n            self.args.hidden_channels,\n            1,\n            self.args.resblock_type_decoder,\n            self.args.resblock_dilation_sizes_decoder,\n            self.args.resblock_kernel_sizes_decoder,\n            self.args.upsample_kernel_sizes_decoder,\n            self.args.upsample_initial_channel_decoder,\n            self.args.upsample_rates_decoder,\n            inference_padding=0,\n            cond_channels=self.embedded_speaker_dim,\n            conv_pre_weight_norm=False,\n            conv_post_weight_norm=False,\n            conv_post_bias=False,\n        )\n\n        if self.args.init_discriminator:\n            self.disc = VitsDiscriminator(\n                periods=self.args.periods_multi_period_discriminator,\n                use_spectral_norm=self.args.use_spectral_norm_disriminator,\n            )\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def init_multispeaker(self, config: Coqpit):\n        \"\"\"Initialize multi-speaker modules of a model. A model can be trained either with a speaker embedding layer\n        or with external `d_vectors` computed from a speaker encoder model.\n\n        You must provide a `speaker_manager` at initialization to set up the multi-speaker modules.\n\n        Args:\n            config (Coqpit): Model configuration.\n            data (List, optional): Dataset items to infer number of speakers. Defaults to None.\n        \"\"\"\n        self.embedded_speaker_dim = 0\n        self.num_speakers = self.args.num_speakers\n        self.audio_transform = None\n\n        if self.speaker_manager:\n            self.num_speakers = self.speaker_manager.num_speakers\n\n        if self.args.use_speaker_embedding:\n            self._init_speaker_embedding()\n\n        if self.args.use_d_vector_file:\n            self._init_d_vector()\n\n        # TODO: make this a function\n        if self.args.use_speaker_encoder_as_loss:\n            if self.speaker_manager.encoder is None and (\n                not self.args.speaker_encoder_model_path or not self.args.speaker_encoder_config_path\n            ):\n                raise RuntimeError(\n                    \" [!] To use the speaker consistency loss (SCL) you need to specify speaker_encoder_model_path and speaker_encoder_config_path !!\"\n                )\n\n            self.speaker_manager.encoder.eval()\n            print(\" > External Speaker Encoder Loaded !!\")\n\n            if (\n                hasattr(self.speaker_manager.encoder, \"audio_config\")\n                and self.config.audio.sample_rate != self.speaker_manager.encoder.audio_config[\"sample_rate\"]\n            ):\n                self.audio_transform = torchaudio.transforms.Resample(\n                    orig_freq=self.config.audio.sample_rate,\n                    new_freq=self.speaker_manager.encoder.audio_config[\"sample_rate\"],\n                )\n\n    def _init_speaker_embedding(self):\n        # pylint: disable=attribute-defined-outside-init\n        if self.num_speakers > 0:\n            print(\" > initialization of speaker-embedding layers.\")\n            self.embedded_speaker_dim = self.args.speaker_embedding_channels\n            self.emb_g = nn.Embedding(self.num_speakers, self.embedded_speaker_dim)\n\n    def _init_d_vector(self):\n        # pylint: disable=attribute-defined-outside-init\n        if hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Speaker embedding layer already initialized before d_vector settings.\")\n        self.embedded_speaker_dim = self.args.d_vector_dim\n\n    def init_multilingual(self, config: Coqpit):\n        \"\"\"Initialize multilingual modules of a model.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        if self.args.language_ids_file is not None:\n            self.language_manager = LanguageManager(language_ids_file_path=config.language_ids_file)\n\n        if self.args.use_language_embedding and self.language_manager:\n            print(\" > initialization of language-embedding layers.\")\n            self.num_languages = self.language_manager.num_languages\n            self.embedded_language_dim = self.args.embedded_language_dim\n            self.emb_l = nn.Embedding(self.num_languages, self.embedded_language_dim)\n            torch.nn.init.xavier_uniform_(self.emb_l.weight)\n        else:\n            self.embedded_language_dim = 0\n\n    def init_upsampling(self):\n        \"\"\"\n        Initialize upsampling modules of a model.\n        \"\"\"\n        if self.args.encoder_sample_rate:\n            self.interpolate_factor = self.config.audio[\"sample_rate\"] / self.args.encoder_sample_rate\n            self.audio_resampler = torchaudio.transforms.Resample(\n                orig_freq=self.config.audio[\"sample_rate\"], new_freq=self.args.encoder_sample_rate\n            )  # pylint: disable=W0201\n\n    def on_epoch_start(self, trainer):  # pylint: disable=W0613\n        \"\"\"Freeze layers at the beginning of an epoch\"\"\"\n        self._freeze_layers()\n        # set the device of speaker encoder\n        if self.args.use_speaker_encoder_as_loss:\n            self.speaker_manager.encoder = self.speaker_manager.encoder.to(self.device)\n\n    def on_init_end(self, trainer):  # pylint: disable=W0613\n        \"\"\"Reinit layes if needed\"\"\"\n        if self.args.reinit_DP:\n            before_dict = get_module_weights_sum(self.duration_predictor)\n            # Applies weights_reset recursively to every submodule of the duration predictor\n            self.duration_predictor.apply(fn=weights_reset)\n            after_dict = get_module_weights_sum(self.duration_predictor)\n            for key, value in after_dict.items():\n                if value == before_dict[key]:\n                    raise RuntimeError(\" [!] The weights of Duration Predictor was not reinit check it !\")\n            print(\" > Duration Predictor was reinit.\")\n\n        if self.args.reinit_text_encoder:\n            before_dict = get_module_weights_sum(self.text_encoder)\n            # Applies weights_reset recursively to every submodule of the duration predictor\n            self.text_encoder.apply(fn=weights_reset)\n            after_dict = get_module_weights_sum(self.text_encoder)\n            for key, value in after_dict.items():\n                if value == before_dict[key]:\n                    raise RuntimeError(\" [!] The weights of Text Encoder was not reinit check it !\")\n            print(\" > Text Encoder was reinit.\")\n\n    def get_aux_input(self, aux_input: Dict):\n        sid, g, lid, _ = self._set_cond_input(aux_input)\n        return {\"speaker_ids\": sid, \"style_wav\": None, \"d_vectors\": g, \"language_ids\": lid}\n\n    def _freeze_layers(self):\n        if self.args.freeze_encoder:\n            for param in self.text_encoder.parameters():\n                param.requires_grad = False\n\n            if hasattr(self, \"emb_l\"):\n                for param in self.emb_l.parameters():\n                    param.requires_grad = False\n\n        if self.args.freeze_PE:\n            for param in self.posterior_encoder.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_DP:\n            for param in self.duration_predictor.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_flow_decoder:\n            for param in self.flow.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_waveform_decoder:\n            for param in self.waveform_decoder.parameters():\n                param.requires_grad = False\n\n    @staticmethod\n    def _set_cond_input(aux_input: Dict):\n        \"\"\"Set the speaker conditioning input based on the multi-speaker mode.\"\"\"\n        sid, g, lid, durations = None, None, None, None\n        if \"speaker_ids\" in aux_input and aux_input[\"speaker_ids\"] is not None:\n            sid = aux_input[\"speaker_ids\"]\n            if sid.ndim == 0:\n                sid = sid.unsqueeze_(0)\n        if \"d_vectors\" in aux_input and aux_input[\"d_vectors\"] is not None:\n            g = F.normalize(aux_input[\"d_vectors\"]).unsqueeze(-1)\n            if g.ndim == 2:\n                g = g.unsqueeze_(0)\n\n        if \"language_ids\" in aux_input and aux_input[\"language_ids\"] is not None:\n            lid = aux_input[\"language_ids\"]\n            if lid.ndim == 0:\n                lid = lid.unsqueeze_(0)\n\n        if \"durations\" in aux_input and aux_input[\"durations\"] is not None:\n            durations = aux_input[\"durations\"]\n\n        return sid, g, lid, durations\n\n    def _set_speaker_input(self, aux_input: Dict):\n        d_vectors = aux_input.get(\"d_vectors\", None)\n        speaker_ids = aux_input.get(\"speaker_ids\", None)\n\n        if d_vectors is not None and speaker_ids is not None:\n            raise ValueError(\"[!] Cannot use d-vectors and speaker-ids together.\")\n\n        if speaker_ids is not None and not hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Cannot use speaker-ids without enabling speaker embedding.\")\n\n        g = speaker_ids if speaker_ids is not None else d_vectors\n        return g\n\n    def forward_mas(self, outputs, z_p, m_p, logs_p, x, x_mask, y_mask, g, lang_emb):\n        # find the alignment path\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        with torch.no_grad():\n            o_scale = torch.exp(-2 * logs_p)\n            logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1]).unsqueeze(-1)  # [b, t, 1]\n            logp2 = torch.einsum(\"klm, kln -> kmn\", [o_scale, -0.5 * (z_p**2)])\n            logp3 = torch.einsum(\"klm, kln -> kmn\", [m_p * o_scale, z_p])\n            logp4 = torch.sum(-0.5 * (m_p**2) * o_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n            logp = logp2 + logp3 + logp1 + logp4\n            attn = maximum_path(logp, attn_mask.squeeze(1)).unsqueeze(1).detach()  # [b, 1, t, t']\n\n        # duration predictor\n        attn_durations = attn.sum(3)\n        if self.args.use_sdp:\n            loss_duration = self.duration_predictor(\n                x.detach() if self.args.detach_dp_input else x,\n                x_mask,\n                attn_durations,\n                g=g.detach() if self.args.detach_dp_input and g is not None else g,\n                lang_emb=lang_emb.detach() if self.args.detach_dp_input and lang_emb is not None else lang_emb,\n            )\n            loss_duration = loss_duration / torch.sum(x_mask)\n        else:\n            attn_log_durations = torch.log(attn_durations + 1e-6) * x_mask\n            log_durations = self.duration_predictor(\n                x.detach() if self.args.detach_dp_input else x,\n                x_mask,\n                g=g.detach() if self.args.detach_dp_input and g is not None else g,\n                lang_emb=lang_emb.detach() if self.args.detach_dp_input and lang_emb is not None else lang_emb,\n            )\n            loss_duration = torch.sum((log_durations - attn_log_durations) ** 2, [1, 2]) / torch.sum(x_mask)\n        outputs[\"loss_duration\"] = loss_duration\n        return outputs, attn\n\n    def upsampling_z(self, z, slice_ids=None, y_lengths=None, y_mask=None):\n        spec_segment_size = self.spec_segment_size\n        if self.args.encoder_sample_rate:\n            # recompute the slices and spec_segment_size if needed\n            slice_ids = slice_ids * int(self.interpolate_factor) if slice_ids is not None else slice_ids\n            spec_segment_size = spec_segment_size * int(self.interpolate_factor)\n            # interpolate z if needed\n            if self.args.interpolate_z:\n                z = torch.nn.functional.interpolate(z, scale_factor=[self.interpolate_factor], mode=\"linear\").squeeze(0)\n                # recompute the mask if needed\n                if y_lengths is not None and y_mask is not None:\n                    y_mask = (\n                        sequence_mask(y_lengths * self.interpolate_factor, None).to(y_mask.dtype).unsqueeze(1)\n                    )  # [B, 1, T_dec_resampled]\n\n        return z, spec_segment_size, slice_ids, y_mask\n\n    def forward(  # pylint: disable=dangerous-default-value\n        self,\n        x: torch.tensor,\n        x_lengths: torch.tensor,\n        y: torch.tensor,\n        y_lengths: torch.tensor,\n        waveform: torch.tensor,\n        aux_input={\"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None},\n    ) -> Dict:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x (torch.tensor): Batch of input character sequence IDs.\n            x_lengths (torch.tensor): Batch of input character sequence lengths.\n            y (torch.tensor): Batch of input spectrograms.\n            y_lengths (torch.tensor): Batch of input spectrogram lengths.\n            waveform (torch.tensor): Batch of ground truth waveforms per sample.\n            aux_input (dict, optional): Auxiliary inputs for multi-speaker and multi-lingual training.\n                Defaults to {\"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None}.\n\n        Returns:\n            Dict: model outputs keyed by the output name.\n\n        Shapes:\n            - x: :math:`[B, T_seq]`\n            - x_lengths: :math:`[B]`\n            - y: :math:`[B, C, T_spec]`\n            - y_lengths: :math:`[B]`\n            - waveform: :math:`[B, 1, T_wav]`\n            - d_vectors: :math:`[B, C, 1]`\n            - speaker_ids: :math:`[B]`\n            - language_ids: :math:`[B]`\n\n        Return Shapes:\n            - model_outputs: :math:`[B, 1, T_wav]`\n            - alignments: :math:`[B, T_seq, T_dec]`\n            - z: :math:`[B, C, T_dec]`\n            - z_p: :math:`[B, C, T_dec]`\n            - m_p: :math:`[B, C, T_dec]`\n            - logs_p: :math:`[B, C, T_dec]`\n            - m_q: :math:`[B, C, T_dec]`\n            - logs_q: :math:`[B, C, T_dec]`\n            - waveform_seg: :math:`[B, 1, spec_seg_size * hop_length]`\n            - gt_spk_emb: :math:`[B, 1, speaker_encoder.proj_dim]`\n            - syn_spk_emb: :math:`[B, 1, speaker_encoder.proj_dim]`\n        \"\"\"\n        outputs = {}\n        sid, g, lid, _ = self._set_cond_input(aux_input)\n        # speaker embedding\n        if self.args.use_speaker_embedding and sid is not None:\n            g = self.emb_g(sid).unsqueeze(-1)  # [b, h, 1]\n\n        # language embedding\n        lang_emb = None\n        if self.args.use_language_embedding and lid is not None:\n            lang_emb = self.emb_l(lid).unsqueeze(-1)\n\n        x, m_p, logs_p, x_mask = self.text_encoder(x, x_lengths, lang_emb=lang_emb)\n\n        # posterior encoder\n        z, m_q, logs_q, y_mask = self.posterior_encoder(y, y_lengths, g=g)\n\n        # flow layers\n        z_p = self.flow(z, y_mask, g=g)\n\n        # duration predictor\n        outputs, attn = self.forward_mas(outputs, z_p, m_p, logs_p, x, x_mask, y_mask, g=g, lang_emb=lang_emb)\n\n        # expand prior\n        m_p = torch.einsum(\"klmn, kjm -> kjn\", [attn, m_p])\n        logs_p = torch.einsum(\"klmn, kjm -> kjn\", [attn, logs_p])\n\n        # select a random feature segment for the waveform decoder\n        z_slice, slice_ids = rand_segments(z, y_lengths, self.spec_segment_size, let_short_samples=True, pad_short=True)\n\n        # interpolate z if needed\n        z_slice, spec_segment_size, slice_ids, _ = self.upsampling_z(z_slice, slice_ids=slice_ids)\n\n        o = self.waveform_decoder(z_slice, g=g)\n\n        wav_seg = segment(\n            waveform,\n            slice_ids * self.config.audio.hop_length,\n            spec_segment_size * self.config.audio.hop_length,\n            pad_short=True,\n        )\n\n        if self.args.use_speaker_encoder_as_loss and self.speaker_manager.encoder is not None:\n            # concate generated and GT waveforms\n            wavs_batch = torch.cat((wav_seg, o), dim=0)\n\n            # resample audio to speaker encoder sample_rate\n            # pylint: disable=W0105\n            if self.audio_transform is not None:\n                wavs_batch = self.audio_transform(wavs_batch)\n\n            pred_embs = self.speaker_manager.encoder.forward(wavs_batch, l2_norm=True)\n\n            # split generated and GT speaker embeddings\n            gt_spk_emb, syn_spk_emb = torch.chunk(pred_embs, 2, dim=0)\n        else:\n            gt_spk_emb, syn_spk_emb = None, None\n\n        outputs.update(\n            {\n                \"model_outputs\": o,\n                \"alignments\": attn.squeeze(1),\n                \"m_p\": m_p,\n                \"logs_p\": logs_p,\n                \"z\": z,\n                \"z_p\": z_p,\n                \"m_q\": m_q,\n                \"logs_q\": logs_q,\n                \"waveform_seg\": wav_seg,\n                \"gt_spk_emb\": gt_spk_emb,\n                \"syn_spk_emb\": syn_spk_emb,\n                \"slice_ids\": slice_ids,\n            }\n        )\n        return outputs\n\n    @staticmethod\n    def _set_x_lengths(x, aux_input):\n        if \"x_lengths\" in aux_input and aux_input[\"x_lengths\"] is not None:\n            return aux_input[\"x_lengths\"]\n        return torch.tensor(x.shape[1:2]).to(x.device)\n\n    @torch.no_grad()\n    def inference(\n        self,\n        x,\n        aux_input={\"x_lengths\": None, \"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None, \"durations\": None},\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"\n        Note:\n            To run in batch mode, provide `x_lengths` else model assumes that the batch size is 1.\n\n        Shapes:\n            - x: :math:`[B, T_seq]`\n            - x_lengths: :math:`[B]`\n            - d_vectors: :math:`[B, C]`\n            - speaker_ids: :math:`[B]`\n\n        Return Shapes:\n            - model_outputs: :math:`[B, 1, T_wav]`\n            - alignments: :math:`[B, T_seq, T_dec]`\n            - z: :math:`[B, C, T_dec]`\n            - z_p: :math:`[B, C, T_dec]`\n            - m_p: :math:`[B, C, T_dec]`\n            - logs_p: :math:`[B, C, T_dec]`\n        \"\"\"\n        sid, g, lid, durations = self._set_cond_input(aux_input)\n        x_lengths = self._set_x_lengths(x, aux_input)\n\n        # speaker embedding\n        if self.args.use_speaker_embedding and sid is not None:\n            g = self.emb_g(sid).unsqueeze(-1)\n\n        # language embedding\n        lang_emb = None\n        if self.args.use_language_embedding and lid is not None:\n            lang_emb = self.emb_l(lid).unsqueeze(-1)\n\n        x, m_p, logs_p, x_mask = self.text_encoder(x, x_lengths, lang_emb=lang_emb)\n\n        if durations is None:\n            if self.args.use_sdp:\n                logw = self.duration_predictor(\n                    x,\n                    x_mask,\n                    g=g if self.args.condition_dp_on_speaker else None,\n                    reverse=True,\n                    noise_scale=self.inference_noise_scale_dp,\n                    lang_emb=lang_emb,\n                )\n            else:\n                logw = self.duration_predictor(\n                    x, x_mask, g=g if self.args.condition_dp_on_speaker else None, lang_emb=lang_emb\n                )\n            w = torch.exp(logw) * x_mask * self.length_scale\n        else:\n            assert durations.shape[-1] == x.shape[-1]\n            w = durations.unsqueeze(0)\n\n        w_ceil = torch.ceil(w)\n        y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n        y_mask = sequence_mask(y_lengths, None).to(x_mask.dtype).unsqueeze(1)  # [B, 1, T_dec]\n\n        attn_mask = x_mask * y_mask.transpose(1, 2)  # [B, 1, T_enc] * [B, T_dec, 1]\n        attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1).transpose(1, 2))\n\n        m_p = torch.matmul(attn.transpose(1, 2), m_p.transpose(1, 2)).transpose(1, 2)\n        logs_p = torch.matmul(attn.transpose(1, 2), logs_p.transpose(1, 2)).transpose(1, 2)\n\n        z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * self.inference_noise_scale\n        z = self.flow(z_p, y_mask, g=g, reverse=True)\n\n        # upsampling if needed\n        z, _, _, y_mask = self.upsampling_z(z, y_lengths=y_lengths, y_mask=y_mask)\n\n        o = self.waveform_decoder((z * y_mask)[:, :, : self.max_inference_len], g=g)\n\n        outputs = {\n            \"model_outputs\": o,\n            \"alignments\": attn.squeeze(1),\n            \"durations\": w_ceil,\n            \"z\": z,\n            \"z_p\": z_p,\n            \"m_p\": m_p,\n            \"logs_p\": logs_p,\n            \"y_mask\": y_mask,\n        }\n        return outputs\n\n    @torch.no_grad()\n    def inference_voice_conversion(\n        self, reference_wav, speaker_id=None, d_vector=None, reference_speaker_id=None, reference_d_vector=None\n    ):\n        \"\"\"Inference for voice conversion\n\n        Args:\n            reference_wav (Tensor): Reference wavform. Tensor of shape [B, T]\n            speaker_id (Tensor): speaker_id of the target speaker. Tensor of shape [B]\n            d_vector (Tensor): d_vector embedding of target speaker. Tensor of shape `[B, C]`\n            reference_speaker_id (Tensor): speaker_id of the reference_wav speaker. Tensor of shape [B]\n            reference_d_vector (Tensor): d_vector embedding of the reference_wav speaker. Tensor of shape `[B, C]`\n        \"\"\"\n        # compute spectrograms\n        y = wav_to_spec(\n            reference_wav,\n            self.config.audio.fft_size,\n            self.config.audio.hop_length,\n            self.config.audio.win_length,\n            center=False,\n        )\n        y_lengths = torch.tensor([y.size(-1)]).to(y.device)\n        speaker_cond_src = reference_speaker_id if reference_speaker_id is not None else reference_d_vector\n        speaker_cond_tgt = speaker_id if speaker_id is not None else d_vector\n        wav, _, _ = self.voice_conversion(y, y_lengths, speaker_cond_src, speaker_cond_tgt)\n        return wav\n\n    def voice_conversion(self, y, y_lengths, speaker_cond_src, speaker_cond_tgt):\n        \"\"\"Forward pass for voice conversion\n\n        TODO: create an end-point for voice conversion\n\n        Args:\n            y (Tensor): Reference spectrograms. Tensor of shape [B, T, C]\n            y_lengths (Tensor): Length of each reference spectrogram. Tensor of shape [B]\n            speaker_cond_src (Tensor): Reference speaker ID. Tensor of shape [B,]\n            speaker_cond_tgt (Tensor): Target speaker ID. Tensor of shape [B,]\n        \"\"\"\n        assert self.num_speakers > 0, \"num_speakers have to be larger than 0.\"\n        # speaker embedding\n        if self.args.use_speaker_embedding and not self.args.use_d_vector_file:\n            g_src = self.emb_g(torch.from_numpy((np.array(speaker_cond_src))).unsqueeze(0)).unsqueeze(-1)\n            g_tgt = self.emb_g(torch.from_numpy((np.array(speaker_cond_tgt))).unsqueeze(0)).unsqueeze(-1)\n        elif not self.args.use_speaker_embedding and self.args.use_d_vector_file:\n            g_src = F.normalize(speaker_cond_src).unsqueeze(-1)\n            g_tgt = F.normalize(speaker_cond_tgt).unsqueeze(-1)\n        else:\n            raise RuntimeError(\" [!] Voice conversion is only supported on multi-speaker models.\")\n\n        z, _, _, y_mask = self.posterior_encoder(y, y_lengths, g=g_src)\n        z_p = self.flow(z, y_mask, g=g_src)\n        z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n        o_hat = self.waveform_decoder(z_hat * y_mask, g=g_tgt)\n        return o_hat, y_mask, (z, z_p, z_hat)\n\n    def train_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int) -> Tuple[Dict, Dict]:\n        \"\"\"Perform a single training step. Run the model forward pass and compute losses.\n\n        Args:\n            batch (Dict): Input tensors.\n            criterion (nn.Module): Loss layer designed for the model.\n            optimizer_idx (int): Index of optimizer to use. 0 for the generator and 1 for the discriminator networks.\n\n        Returns:\n            Tuple[Dict, Dict]: Model ouputs and computed losses.\n        \"\"\"\n\n        spec_lens = batch[\"spec_lens\"]\n\n        if optimizer_idx == 0:\n            tokens = batch[\"tokens\"]\n            token_lenghts = batch[\"token_lens\"]\n            spec = batch[\"spec\"]\n\n            d_vectors = batch[\"d_vectors\"]\n            speaker_ids = batch[\"speaker_ids\"]\n            language_ids = batch[\"language_ids\"]\n            waveform = batch[\"waveform\"]\n\n            # generator pass\n            outputs = self.forward(\n                tokens,\n                token_lenghts,\n                spec,\n                spec_lens,\n                waveform,\n                aux_input={\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids, \"language_ids\": language_ids},\n            )\n\n            # cache tensors for the generator pass\n            self.model_outputs_cache = outputs  # pylint: disable=attribute-defined-outside-init\n\n            # compute scores and features\n            scores_disc_fake, _, scores_disc_real, _ = self.disc(\n                outputs[\"model_outputs\"].detach(), outputs[\"waveform_seg\"]\n            )\n\n            # compute loss\n            with autocast(enabled=False):  # use float32 for the criterion\n                loss_dict = criterion[optimizer_idx](\n                    scores_disc_real,\n                    scores_disc_fake,\n                )\n            return outputs, loss_dict\n\n        if optimizer_idx == 1:\n            mel = batch[\"mel\"]\n\n            # compute melspec segment\n            with autocast(enabled=False):\n                if self.args.encoder_sample_rate:\n                    spec_segment_size = self.spec_segment_size * int(self.interpolate_factor)\n                else:\n                    spec_segment_size = self.spec_segment_size\n\n                mel_slice = segment(\n                    mel.float(), self.model_outputs_cache[\"slice_ids\"], spec_segment_size, pad_short=True\n                )\n                mel_slice_hat = wav_to_mel(\n                    y=self.model_outputs_cache[\"model_outputs\"].float(),\n                    n_fft=self.config.audio.fft_size,\n                    sample_rate=self.config.audio.sample_rate,\n                    num_mels=self.config.audio.num_mels,\n                    hop_length=self.config.audio.hop_length,\n                    win_length=self.config.audio.win_length,\n                    fmin=self.config.audio.mel_fmin,\n                    fmax=self.config.audio.mel_fmax,\n                    center=False,\n                )\n\n            # compute discriminator scores and features\n            scores_disc_fake, feats_disc_fake, _, feats_disc_real = self.disc(\n                self.model_outputs_cache[\"model_outputs\"], self.model_outputs_cache[\"waveform_seg\"]\n            )\n\n            # compute losses\n            with autocast(enabled=False):  # use float32 for the criterion\n                loss_dict = criterion[optimizer_idx](\n                    mel_slice_hat=mel_slice.float(),\n                    mel_slice=mel_slice_hat.float(),\n                    z_p=self.model_outputs_cache[\"z_p\"].float(),\n                    logs_q=self.model_outputs_cache[\"logs_q\"].float(),\n                    m_p=self.model_outputs_cache[\"m_p\"].float(),\n                    logs_p=self.model_outputs_cache[\"logs_p\"].float(),\n                    z_len=spec_lens,\n                    scores_disc_fake=scores_disc_fake,\n                    feats_disc_fake=feats_disc_fake,\n                    feats_disc_real=feats_disc_real,\n                    loss_duration=self.model_outputs_cache[\"loss_duration\"],\n                    use_speaker_encoder_as_loss=self.args.use_speaker_encoder_as_loss,\n                    gt_spk_emb=self.model_outputs_cache[\"gt_spk_emb\"],\n                    syn_spk_emb=self.model_outputs_cache[\"syn_spk_emb\"],\n                )\n\n            return self.model_outputs_cache, loss_dict\n\n        raise ValueError(\" [!] Unexpected `optimizer_idx`.\")\n\n    def _log(self, ap, batch, outputs, name_prefix=\"train\"):  # pylint: disable=unused-argument,no-self-use\n        y_hat = outputs[1][\"model_outputs\"]\n        y = outputs[1][\"waveform_seg\"]\n        figures = plot_results(y_hat, y, ap, name_prefix)\n        sample_voice = y_hat[0].squeeze(0).detach().cpu().numpy()\n        audios = {f\"{name_prefix}/audio\": sample_voice}\n\n        alignments = outputs[1][\"alignments\"]\n        align_img = alignments[0].data.cpu().numpy().T\n\n        figures.update(\n            {\n                \"alignment\": plot_alignment(align_img, output_fig=False),\n            }\n        )\n        return figures, audios\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ):  # pylint: disable=no-self-use\n        \"\"\"Create visualizations and waveform examples.\n\n        For example, here you can plot spectrograms and generate sample sample waveforms from these spectrograms to\n        be projected onto Tensorboard.\n\n        Args:\n            ap (AudioProcessor): audio processor used at training.\n            batch (Dict): Model inputs used at the previous training step.\n            outputs (Dict): Model outputs generated at the previoud training step.\n\n        Returns:\n            Tuple[Dict, np.ndarray]: training plots and output waveform.\n        \"\"\"\n        figures, audios = self._log(self.ap, batch, outputs, \"train\")\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    @torch.no_grad()\n    def eval_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int):\n        return self.train_step(batch, criterion, optimizer_idx)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._log(self.ap, batch, outputs, \"eval\")\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def get_aux_input_from_test_sentences(self, sentence_info):\n        if hasattr(self.config, \"model_args\"):\n            config = self.config.model_args\n        else:\n            config = self.config\n\n        # extract speaker and language info\n        text, speaker_name, style_wav, language_name = None, None, None, None\n\n        if isinstance(sentence_info, list):\n            if len(sentence_info) == 1:\n                text = sentence_info[0]\n            elif len(sentence_info) == 2:\n                text, speaker_name = sentence_info\n            elif len(sentence_info) == 3:\n                text, speaker_name, style_wav = sentence_info\n            elif len(sentence_info) == 4:\n                text, speaker_name, style_wav, language_name = sentence_info\n        else:\n            text = sentence_info\n\n        # get speaker  id/d_vector\n        speaker_id, d_vector, language_id = None, None, None\n        if hasattr(self, \"speaker_manager\"):\n            if config.use_d_vector_file:\n                if speaker_name is None:\n                    d_vector = self.speaker_manager.get_random_embedding()\n                else:\n                    d_vector = self.speaker_manager.get_mean_embedding(speaker_name, num_samples=None, randomize=False)\n            elif config.use_speaker_embedding:\n                if speaker_name is None:\n                    speaker_id = self.speaker_manager.get_random_id()\n                else:\n                    speaker_id = self.speaker_manager.name_to_id[speaker_name]\n\n        # get language id\n        if hasattr(self, \"language_manager\") and config.use_language_embedding and language_name is not None:\n            language_id = self.language_manager.name_to_id[language_name]\n\n        return {\n            \"text\": text,\n            \"speaker_id\": speaker_id,\n            \"style_wav\": style_wav,\n            \"d_vector\": d_vector,\n            \"language_id\": language_id,\n            \"language_name\": language_name,\n        }\n\n    @torch.no_grad()\n    def test_run(self, assets) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `tts` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        for idx, s_info in enumerate(test_sentences):\n            aux_inputs = self.get_aux_input_from_test_sentences(s_info)\n            wav, alignment, _, _ = synthesis(\n                self,\n                aux_inputs[\"text\"],\n                self.config,\n                \"cuda\" in str(next(self.parameters()).device),\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n                style_wav=aux_inputs[\"style_wav\"],\n                language_id=aux_inputs[\"language_id\"],\n                use_griffin_lim=True,\n                do_trim_silence=False,\n            ).values()\n            test_audios[\"{}-audio\".format(idx)] = wav\n            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(alignment.T, output_fig=False)\n        return {\"figures\": test_figures, \"audios\": test_audios}\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[\"audios\"], self.ap.sample_rate)\n        logger.test_figures(steps, outputs[\"figures\"])\n\n    def format_batch(self, batch: Dict) -> Dict:\n        \"\"\"Compute speaker, langugage IDs and d_vector for the batch if necessary.\"\"\"\n        speaker_ids = None\n        language_ids = None\n        d_vectors = None\n\n        # get numerical speaker ids from speaker names\n        if self.speaker_manager is not None and self.speaker_manager.name_to_id and self.args.use_speaker_embedding:\n            speaker_ids = [self.speaker_manager.name_to_id[sn] for sn in batch[\"speaker_names\"]]\n\n        if speaker_ids is not None:\n            speaker_ids = torch.LongTensor(speaker_ids)\n\n        # get d_vectors from audio file names\n        if self.speaker_manager is not None and self.speaker_manager.embeddings and self.args.use_d_vector_file:\n            d_vector_mapping = self.speaker_manager.embeddings\n            d_vectors = [d_vector_mapping[w][\"embedding\"] for w in batch[\"audio_unique_names\"]]\n            d_vectors = torch.FloatTensor(d_vectors)\n\n        # get language ids from language names\n        if self.language_manager is not None and self.language_manager.name_to_id and self.args.use_language_embedding:\n            language_ids = [self.language_manager.name_to_id[ln] for ln in batch[\"language_names\"]]\n\n        if language_ids is not None:\n            language_ids = torch.LongTensor(language_ids)\n\n        batch[\"language_ids\"] = language_ids\n        batch[\"d_vectors\"] = d_vectors\n        batch[\"speaker_ids\"] = speaker_ids\n        return batch\n\n    def format_batch_on_device(self, batch):\n        \"\"\"Compute spectrograms on the device.\"\"\"\n        ac = self.config.audio\n\n        if self.args.encoder_sample_rate:\n            wav = self.audio_resampler(batch[\"waveform\"])\n        else:\n            wav = batch[\"waveform\"]\n\n        # compute spectrograms\n        batch[\"spec\"] = wav_to_spec(wav, ac.fft_size, ac.hop_length, ac.win_length, center=False)\n\n        if self.args.encoder_sample_rate:\n            # recompute spec with high sampling rate to the loss\n            spec_mel = wav_to_spec(batch[\"waveform\"], ac.fft_size, ac.hop_length, ac.win_length, center=False)\n            # remove extra stft frames if needed\n            if spec_mel.size(2) > int(batch[\"spec\"].size(2) * self.interpolate_factor):\n                spec_mel = spec_mel[:, :, : int(batch[\"spec\"].size(2) * self.interpolate_factor)]\n            else:\n                batch[\"spec\"] = batch[\"spec\"][:, :, : int(spec_mel.size(2) / self.interpolate_factor)]\n        else:\n            spec_mel = batch[\"spec\"]\n\n        batch[\"mel\"] = spec_to_mel(\n            spec=spec_mel,\n            n_fft=ac.fft_size,\n            num_mels=ac.num_mels,\n            sample_rate=ac.sample_rate,\n            fmin=ac.mel_fmin,\n            fmax=ac.mel_fmax,\n        )\n\n        if self.args.encoder_sample_rate:\n            assert batch[\"spec\"].shape[2] == int(\n                batch[\"mel\"].shape[2] / self.interpolate_factor\n            ), f\"{batch['spec'].shape[2]}, {batch['mel'].shape[2]}\"\n        else:\n            assert batch[\"spec\"].shape[2] == batch[\"mel\"].shape[2], f\"{batch['spec'].shape[2]}, {batch['mel'].shape[2]}\"\n\n        # compute spectrogram frame lengths\n        batch[\"spec_lens\"] = (batch[\"spec\"].shape[2] * batch[\"waveform_rel_lens\"]).int()\n        batch[\"mel_lens\"] = (batch[\"mel\"].shape[2] * batch[\"waveform_rel_lens\"]).int()\n\n        if self.args.encoder_sample_rate:\n            assert (batch[\"spec_lens\"] - (batch[\"mel_lens\"] / self.interpolate_factor).int()).sum() == 0\n        else:\n            assert (batch[\"spec_lens\"] - batch[\"mel_lens\"]).sum() == 0\n\n        # zero the padding frames\n        batch[\"spec\"] = batch[\"spec\"] * sequence_mask(batch[\"spec_lens\"]).unsqueeze(1)\n        batch[\"mel\"] = batch[\"mel\"] * sequence_mask(batch[\"mel_lens\"]).unsqueeze(1)\n        return batch\n\n    def get_sampler(self, config: Coqpit, dataset: TTSDataset, num_gpus=1, is_eval=False):\n        weights = None\n        data_items = dataset.samples\n        if getattr(config, \"use_weighted_sampler\", False):\n            for attr_name, alpha in config.weighted_sampler_attrs.items():\n                print(f\" > Using weighted sampler for attribute '{attr_name}' with alpha '{alpha}'\")\n                multi_dict = config.weighted_sampler_multipliers.get(attr_name, None)\n                print(multi_dict)\n                weights, attr_names, attr_weights = get_attribute_balancer_weights(\n                    attr_name=attr_name, items=data_items, multi_dict=multi_dict\n                )\n                weights = weights * alpha\n                print(f\" > Attribute weights for '{attr_names}' \\n | > {attr_weights}\")\n\n        # input_audio_lenghts = [os.path.getsize(x[\"audio_file\"]) for x in data_items]\n\n        if weights is not None:\n            w_sampler = WeightedRandomSampler(weights, len(weights))\n            batch_sampler = BucketBatchSampler(\n                w_sampler,\n                data=data_items,\n                batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                sort_key=lambda x: os.path.getsize(x[\"audio_file\"]),\n                drop_last=True,\n            )\n        else:\n            batch_sampler = None\n        # sampler for DDP\n        if batch_sampler is None:\n            batch_sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        else:  # If a sampler is already defined use this sampler and DDP sampler together\n            batch_sampler = (\n                DistributedSamplerWrapper(batch_sampler) if num_gpus > 1 else batch_sampler\n            )  # TODO: check batch_sampler with multi-gpu\n        return batch_sampler\n\n    def get_data_loader(\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: bool,\n        samples: Union[List[Dict], List[List]],\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,\n    ) -> \"DataLoader\":\n        if is_eval and not config.run_eval:\n            loader = None\n        else:\n            # init dataloader\n            dataset = VitsDataset(\n                model_args=self.args,\n                samples=samples,\n                batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n                min_text_len=config.min_text_len,\n                max_text_len=config.max_text_len,\n                min_audio_len=config.min_audio_len,\n                max_audio_len=config.max_audio_len,\n                phoneme_cache_path=config.phoneme_cache_path,\n                precompute_num_workers=config.precompute_num_workers,\n                verbose=verbose,\n                tokenizer=self.tokenizer,\n                start_by_longest=config.start_by_longest,\n            )\n\n            # wait all the DDP process to be ready\n            if num_gpus > 1:\n                dist.barrier()\n\n            # sort input sequences from short to long\n            dataset.preprocess_samples()\n\n            # get samplers\n            sampler = self.get_sampler(config, dataset, num_gpus)\n            if sampler is None:\n                loader = DataLoader(\n                    dataset,\n                    batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                    shuffle=False,  # shuffle is done in the dataset.\n                    collate_fn=dataset.collate_fn,\n                    drop_last=False,  # setting this False might cause issues in AMP training.\n                    num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                    pin_memory=False,\n                )\n            else:\n                if num_gpus > 1:\n                    loader = DataLoader(\n                        dataset,\n                        sampler=sampler,\n                        batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                        collate_fn=dataset.collate_fn,\n                        num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                        pin_memory=False,\n                    )\n                else:\n                    loader = DataLoader(\n                        dataset,\n                        batch_sampler=sampler,\n                        collate_fn=dataset.collate_fn,\n                        num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                        pin_memory=False,\n                    )\n        return loader\n\n    def get_optimizer(self) -> List:\n        \"\"\"Initiate and return the GAN optimizers based on the config parameters.\n        It returnes 2 optimizers in a list. First one is for the generator and the second one is for the discriminator.\n        Returns:\n            List: optimizers.\n        \"\"\"\n        # select generator parameters\n        optimizer0 = get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr_disc, self.disc)\n\n        gen_parameters = chain(params for k, params in self.named_parameters() if not k.startswith(\"disc.\"))\n        optimizer1 = get_optimizer(\n            self.config.optimizer, self.config.optimizer_params, self.config.lr_gen, parameters=gen_parameters\n        )\n        return [optimizer0, optimizer1]\n\n    def get_lr(self) -> List:\n        \"\"\"Set the initial learning rates for each optimizer.\n\n        Returns:\n            List: learning rates for each optimizer.\n        \"\"\"\n        return [self.config.lr_disc, self.config.lr_gen]\n\n    def get_scheduler(self, optimizer) -> List:\n        \"\"\"Set the schedulers for each optimizer.\n\n        Args:\n            optimizer (List[`torch.optim.Optimizer`]): List of optimizers.\n\n        Returns:\n            List: Schedulers, one for each optimizer.\n        \"\"\"\n        scheduler_D = get_scheduler(self.config.lr_scheduler_disc, self.config.lr_scheduler_disc_params, optimizer[0])\n        scheduler_G = get_scheduler(self.config.lr_scheduler_gen, self.config.lr_scheduler_gen_params, optimizer[1])\n        return [scheduler_D, scheduler_G]\n\n    def get_criterion(self):\n        \"\"\"Get criterions for each optimizer. The index in the output list matches the optimizer idx used in\n        `train_step()`\"\"\"\n        from TTS.tts.layers.losses import (  # pylint: disable=import-outside-toplevel\n            VitsDiscriminatorLoss,\n            VitsGeneratorLoss,\n        )\n\n        return [VitsDiscriminatorLoss(self.config), VitsGeneratorLoss(self.config)]\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, strict=True, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        \"\"\"Load the model checkpoint and setup for training or inference\"\"\"\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        # compat band-aid for the pre-trained models to not use the encoder baked into the model\n        # TODO: consider baking the speaker encoder into the model and call it from there.\n        # as it is probably easier for model distribution.\n        state[\"model\"] = {k: v for k, v in state[\"model\"].items() if \"speaker_encoder\" not in k}\n\n        if self.args.encoder_sample_rate is not None and eval:\n            # audio resampler is not used in inference time\n            self.audio_resampler = None\n\n        # handle fine-tuning from a checkpoint with additional speakers\n        if hasattr(self, \"emb_g\") and state[\"model\"][\"emb_g.weight\"].shape != self.emb_g.weight.shape:\n            num_new_speakers = self.emb_g.weight.shape[0] - state[\"model\"][\"emb_g.weight\"].shape[0]\n            print(f\" > Loading checkpoint with {num_new_speakers} additional speakers.\")\n            emb_g = state[\"model\"][\"emb_g.weight\"]\n            new_row = torch.randn(num_new_speakers, emb_g.shape[1])\n            emb_g = torch.cat([emb_g, new_row], axis=0)\n            state[\"model\"][\"emb_g.weight\"] = emb_g\n        # load the model weights\n        self.load_state_dict(state[\"model\"], strict=strict)\n\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def load_fairseq_checkpoint(\n        self, config, checkpoint_dir, eval=False, strict=True\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        \"\"\"Load VITS checkpoints released by fairseq here: https://github.com/facebookresearch/fairseq/tree/main/examples/mms\n        Performs some changes for compatibility.\n\n        Args:\n            config (Coqpit): \ud83d\udc38TTS model config.\n            checkpoint_dir (str): Path to the checkpoint directory.\n            eval (bool, optional): Set to True for evaluation. Defaults to False.\n        \"\"\"\n        import json\n\n        from TTS.tts.utils.text.cleaners import basic_cleaners\n\n        self.disc = None\n        # set paths\n        config_file = os.path.join(checkpoint_dir, \"config.json\")\n        checkpoint_file = os.path.join(checkpoint_dir, \"G_100000.pth\")\n        vocab_file = os.path.join(checkpoint_dir, \"vocab.txt\")\n        # set config params\n        with open(config_file, \"r\", encoding=\"utf-8\") as file:\n            # Load the JSON data as a dictionary\n            config_org = json.load(file)\n        self.config.audio.sample_rate = config_org[\"data\"][\"sampling_rate\"]\n        # self.config.add_blank = config['add_blank']\n        # set tokenizer\n        vocab = FairseqVocab(vocab_file)\n        self.text_encoder.emb = nn.Embedding(vocab.num_chars, config.model_args.hidden_channels)\n        self.tokenizer = TTSTokenizer(\n            use_phonemes=False,\n            text_cleaner=basic_cleaners,\n            characters=vocab,\n            phonemizer=None,\n            add_blank=config_org[\"data\"][\"add_blank\"],\n            use_eos_bos=False,\n        )\n        # load fairseq checkpoint\n        new_chk = rehash_fairseq_vits_checkpoint(checkpoint_file)\n        self.load_state_dict(new_chk, strict=strict)\n        if eval:\n            self.eval()\n            assert not self.training\n\n    @staticmethod\n    def init_from_config(config: \"VitsConfig\", samples: Union[List[List], List[Dict]] = None, verbose=True):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (VitsConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        upsample_rate = torch.prod(torch.as_tensor(config.model_args.upsample_rates_decoder)).item()\n\n        if not config.model_args.encoder_sample_rate:\n            assert (\n                upsample_rate == config.audio.hop_length\n            ), f\" [!] Product of upsample rates must be equal to the hop length - {upsample_rate} vs {config.audio.hop_length}\"\n        else:\n            encoder_to_vocoder_upsampling_factor = config.audio.sample_rate / config.model_args.encoder_sample_rate\n            effective_hop_length = config.audio.hop_length * encoder_to_vocoder_upsampling_factor\n            assert (\n                upsample_rate == effective_hop_length\n            ), f\" [!] Product of upsample rates must be equal to the hop length - {upsample_rate} vs {effective_hop_length}\"\n\n        ap = AudioProcessor.init_from_config(config, verbose=verbose)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        language_manager = LanguageManager.init_from_config(config)\n\n        if config.model_args.speaker_encoder_model_path:\n            speaker_manager.init_encoder(\n                config.model_args.speaker_encoder_model_path, config.model_args.speaker_encoder_config_path\n            )\n        return Vits(new_config, ap, tokenizer, speaker_manager, language_manager)\n\n    def export_onnx(self, output_path: str = \"coqui_vits.onnx\", verbose: bool = True):\n        \"\"\"Export model to ONNX format for inference\n\n        Args:\n            output_path (str): Path to save the exported model.\n            verbose (bool): Print verbose information. Defaults to True.\n        \"\"\"\n\n        # rollback values\n        _forward = self.forward\n        disc = None\n        if hasattr(self, \"disc\"):\n            disc = self.disc\n        training = self.training\n\n        # set export mode\n        self.disc = None\n        self.eval()\n\n        def onnx_inference(text, text_lengths, scales, sid=None, langid=None):\n            noise_scale = scales[0]\n            length_scale = scales[1]\n            noise_scale_dp = scales[2]\n            self.noise_scale = noise_scale\n            self.length_scale = length_scale\n            self.noise_scale_dp = noise_scale_dp\n            return self.inference(\n                text,\n                aux_input={\n                    \"x_lengths\": text_lengths,\n                    \"d_vectors\": None,\n                    \"speaker_ids\": sid,\n                    \"language_ids\": langid,\n                    \"durations\": None,\n                },\n            )[\"model_outputs\"]\n\n        self.forward = onnx_inference\n\n        # set dummy inputs\n        dummy_input_length = 100\n        sequences = torch.randint(low=0, high=2, size=(1, dummy_input_length), dtype=torch.long)\n        sequence_lengths = torch.LongTensor([sequences.size(1)])\n        scales = torch.FloatTensor([self.inference_noise_scale, self.length_scale, self.inference_noise_scale_dp])\n        dummy_input = (sequences, sequence_lengths, scales)\n        input_names = [\"input\", \"input_lengths\", \"scales\"]\n\n        if self.num_speakers > 0:\n            speaker_id = torch.LongTensor([0])\n            dummy_input += (speaker_id,)\n            input_names.append(\"sid\")\n\n        if hasattr(self, \"num_languages\") and self.num_languages > 0 and self.embedded_language_dim > 0:\n            language_id = torch.LongTensor([0])\n            dummy_input += (language_id,)\n            input_names.append(\"langid\")\n\n        # export to ONNX\n        torch.onnx.export(\n            model=self,\n            args=dummy_input,\n            opset_version=15,\n            f=output_path,\n            verbose=verbose,\n            input_names=input_names,\n            output_names=[\"output\"],\n            dynamic_axes={\n                \"input\": {0: \"batch_size\", 1: \"phonemes\"},\n                \"input_lengths\": {0: \"batch_size\"},\n                \"output\": {0: \"batch_size\", 1: \"time1\", 2: \"time2\"},\n            },\n        )\n\n        # rollback\n        self.forward = _forward\n        if training:\n            self.train()\n        if not disc is None:\n            self.disc = disc\n\n    def load_onnx(self, model_path: str, cuda=False):\n        import onnxruntime as ort\n\n        providers = [\n            \"CPUExecutionProvider\"\n            if cuda is False\n            else (\"CUDAExecutionProvider\", {\"cudnn_conv_algo_search\": \"DEFAULT\"})\n        ]\n        sess_options = ort.SessionOptions()\n        self.onnx_sess = ort.InferenceSession(\n            model_path,\n            sess_options=sess_options,\n            providers=providers,\n        )\n\n    def inference_onnx(self, x, x_lengths=None, speaker_id=None, language_id=None):\n        \"\"\"ONNX inference\"\"\"\n\n        if isinstance(x, torch.Tensor):\n            x = x.cpu().numpy()\n\n        if x_lengths is None:\n            x_lengths = np.array([x.shape[1]], dtype=np.int64)\n\n        if isinstance(x_lengths, torch.Tensor):\n            x_lengths = x_lengths.cpu().numpy()\n        scales = np.array(\n            [self.inference_noise_scale, self.length_scale, self.inference_noise_scale_dp],\n            dtype=np.float32,\n        )\n        input_params = {\"input\": x, \"input_lengths\": x_lengths, \"scales\": scales}\n        if not speaker_id is None:\n            input_params[\"sid\"] = torch.tensor([speaker_id]).cpu().numpy()\n        if not language_id is None:\n            input_params[\"langid\"] = torch.tensor([language_id]).cpu().numpy()\n\n        audio = self.onnx_sess.run(\n            [\"output\"],\n            input_params,\n        )\n        return audio[0][0]\n\n\n##################################\n# VITS CHARACTERS\n##################################\n\n\nclass VitsCharacters(BaseCharacters):\n    \"\"\"Characters class for VITs model for compatibility with pre-trained models\"\"\"\n\n    def __init__(\n        self,\n        graphemes: str = _characters,\n        punctuations: str = _punctuations,\n        pad: str = _pad,\n        ipa_characters: str = _phonemes,\n    ) -> None:\n        if ipa_characters is not None:\n            graphemes += ipa_characters\n        super().__init__(graphemes, punctuations, pad, None, None, \"<BLNK>\", is_unique=False, is_sorted=True)\n\n    def _create_vocab(self):\n        self._vocab = [self._pad] + list(self._punctuations) + list(self._characters) + [self._blank]\n        self._char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        # pylint: disable=unnecessary-comprehension\n        self._id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n\n    @staticmethod\n    def init_from_config(config: Coqpit):\n        if config.characters is not None:\n            _pad = config.characters[\"pad\"]\n            _punctuations = config.characters[\"punctuations\"]\n            _letters = config.characters[\"characters\"]\n            _letters_ipa = config.characters[\"phonemes\"]\n            return (\n                VitsCharacters(graphemes=_letters, ipa_characters=_letters_ipa, punctuations=_punctuations, pad=_pad),\n                config,\n            )\n        characters = VitsCharacters()\n        new_config = replace(config, characters=characters.to_config())\n        return characters, new_config\n\n    def to_config(self) -> \"CharactersConfig\":\n        return CharactersConfig(\n            characters=self._characters,\n            punctuations=self._punctuations,\n            pad=self._pad,\n            eos=None,\n            bos=None,\n            blank=self._blank,\n            is_unique=False,\n            is_sorted=True,\n        )\n\n\nclass FairseqVocab(BaseVocabulary):\n    def __init__(self, vocab: str):\n        super(FairseqVocab).__init__()\n        self.vocab = vocab\n\n    @property\n    def vocab(self):\n        \"\"\"Return the vocabulary dictionary.\"\"\"\n        return self._vocab\n\n    @vocab.setter\n    def vocab(self, vocab_file):\n        with open(vocab_file, encoding=\"utf-8\") as f:\n            self._vocab = [x.replace(\"\\n\", \"\") for x in f.readlines()]\n        self.blank = self._vocab[0]\n        self.pad = \" \"\n        self._char_to_id = {s: i for i, s in enumerate(self._vocab)}  # pylint: disable=unnecessary-comprehension\n        self._id_to_char = {i: s for i, s in enumerate(self._vocab)}  # pylint: disable=unnecessary-comprehension\n", "TTS/tts/models/base_tacotron.py": "import copy\nfrom abc import abstractmethod\nfrom typing import Dict, Tuple\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\n\nfrom TTS.tts.layers.losses import TacotronLoss\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.helpers import sequence_mask\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.synthesis import synthesis\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.generic_utils import format_aux_input\nfrom TTS.utils.io import load_fsspec\nfrom TTS.utils.training import gradual_training_scheduler\n\n\nclass BaseTacotron(BaseTTS):\n    \"\"\"Base class shared by Tacotron and Tacotron2\"\"\"\n\n    def __init__(\n        self,\n        config: \"TacotronConfig\",\n        ap: \"AudioProcessor\",\n        tokenizer: \"TTSTokenizer\",\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        # pass all config fields as class attributes\n        for key in config:\n            setattr(self, key, config[key])\n\n        # layers\n        self.embedding = None\n        self.encoder = None\n        self.decoder = None\n        self.postnet = None\n\n        # init tensors\n        self.embedded_speakers = None\n        self.embedded_speakers_projected = None\n\n        # global style token\n        if self.gst and self.use_gst:\n            self.decoder_in_features += self.gst.gst_embedding_dim  # add gst embedding dim\n            self.gst_layer = None\n\n        # Capacitron\n        if self.capacitron_vae and self.use_capacitron_vae:\n            self.decoder_in_features += self.capacitron_vae.capacitron_VAE_embedding_dim  # add capacitron embedding dim\n            self.capacitron_vae_layer = None\n\n        # additional layers\n        self.decoder_backward = None\n        self.coarse_decoder = None\n\n    @staticmethod\n    def _format_aux_input(aux_input: Dict) -> Dict:\n        \"\"\"Set missing fields to their default values\"\"\"\n        if aux_input:\n            return format_aux_input({\"d_vectors\": None, \"speaker_ids\": None}, aux_input)\n        return None\n\n    #############################\n    # INIT FUNCTIONS\n    #############################\n\n    def _init_backward_decoder(self):\n        \"\"\"Init the backward decoder for Forward-Backward decoding.\"\"\"\n        self.decoder_backward = copy.deepcopy(self.decoder)\n\n    def _init_coarse_decoder(self):\n        \"\"\"Init the coarse decoder for Double-Decoder Consistency.\"\"\"\n        self.coarse_decoder = copy.deepcopy(self.decoder)\n        self.coarse_decoder.r_init = self.ddc_r\n        self.coarse_decoder.set_r(self.ddc_r)\n\n    #############################\n    # CORE FUNCTIONS\n    #############################\n\n    @abstractmethod\n    def forward(self):\n        pass\n\n    @abstractmethod\n    def inference(self):\n        pass\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        \"\"\"Load model checkpoint and set up internals.\n\n        Args:\n            config (Coqpi): model configuration.\n            checkpoint_path (str): path to checkpoint file.\n            eval (bool, optional): whether to load model for evaluation.\n            cache (bool, optional): If True, cache the file locally for subsequent calls. It is cached under `get_user_data_dir()/tts_cache`. Defaults to False.\n        \"\"\"\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        # TODO: set r in run-time by taking it from the new config\n        if \"r\" in state:\n            # set r from the state (for compatibility with older checkpoints)\n            self.decoder.set_r(state[\"r\"])\n        elif \"config\" in state:\n            # set r from config used at training time (for inference)\n            self.decoder.set_r(state[\"config\"][\"r\"])\n        else:\n            # set r from the new config (for new-models)\n            self.decoder.set_r(config.r)\n        if eval:\n            self.eval()\n            print(f\" > Model's reduction rate `r` is set to: {self.decoder.r}\")\n            assert not self.training\n\n    def get_criterion(self) -> nn.Module:\n        \"\"\"Get the model criterion used in training.\"\"\"\n        return TacotronLoss(self.config)\n\n    @staticmethod\n    def init_from_config(config: Coqpit):\n        \"\"\"Initialize model from config.\"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config)\n        tokenizer = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config)\n        return BaseTacotron(config, ap, tokenizer, speaker_manager)\n\n    ##########################\n    # TEST AND LOG FUNCTIONS #\n    ##########################\n\n    def test_run(self, assets: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `tts` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Args:\n            assets (dict): A dict of training assets. For `tts` models, it must include `{'audio_processor': ap}`.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        aux_inputs = self._get_test_aux_input()\n        for idx, sen in enumerate(test_sentences):\n            outputs_dict = synthesis(\n                self,\n                sen,\n                self.config,\n                \"cuda\" in str(next(self.parameters()).device),\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n                style_wav=aux_inputs[\"style_wav\"],\n                use_griffin_lim=True,\n                do_trim_silence=False,\n            )\n            test_audios[\"{}-audio\".format(idx)] = outputs_dict[\"wav\"]\n            test_figures[\"{}-prediction\".format(idx)] = plot_spectrogram(\n                outputs_dict[\"outputs\"][\"model_outputs\"], self.ap, output_fig=False\n            )\n            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(\n                outputs_dict[\"outputs\"][\"alignments\"], output_fig=False\n            )\n        return {\"figures\": test_figures, \"audios\": test_audios}\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[\"audios\"], self.ap.sample_rate)\n        logger.test_figures(steps, outputs[\"figures\"])\n\n    #############################\n    # COMMON COMPUTE FUNCTIONS\n    #############################\n\n    def compute_masks(self, text_lengths, mel_lengths):\n        \"\"\"Compute masks  against sequence paddings.\"\"\"\n        # B x T_in_max (boolean)\n        input_mask = sequence_mask(text_lengths)\n        output_mask = None\n        if mel_lengths is not None:\n            max_len = mel_lengths.max()\n            r = self.decoder.r\n            max_len = max_len + (r - (max_len % r)) if max_len % r > 0 else max_len\n            output_mask = sequence_mask(mel_lengths, max_len=max_len)\n        return input_mask, output_mask\n\n    def _backward_pass(self, mel_specs, encoder_outputs, mask):\n        \"\"\"Run backwards decoder\"\"\"\n        decoder_outputs_b, alignments_b, _ = self.decoder_backward(\n            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask\n        )\n        decoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()\n        return decoder_outputs_b, alignments_b\n\n    def _coarse_decoder_pass(self, mel_specs, encoder_outputs, alignments, input_mask):\n        \"\"\"Double Decoder Consistency\"\"\"\n        T = mel_specs.shape[1]\n        if T % self.coarse_decoder.r > 0:\n            padding_size = self.coarse_decoder.r - (T % self.coarse_decoder.r)\n            mel_specs = torch.nn.functional.pad(mel_specs, (0, 0, 0, padding_size, 0, 0))\n        decoder_outputs_backward, alignments_backward, _ = self.coarse_decoder(\n            encoder_outputs.detach(), mel_specs, input_mask\n        )\n        # scale_factor = self.decoder.r_init / self.decoder.r\n        alignments_backward = torch.nn.functional.interpolate(\n            alignments_backward.transpose(1, 2),\n            size=alignments.shape[1],\n            mode=\"nearest\",\n        ).transpose(1, 2)\n        decoder_outputs_backward = decoder_outputs_backward.transpose(1, 2)\n        decoder_outputs_backward = decoder_outputs_backward[:, :T, :]\n        return decoder_outputs_backward, alignments_backward\n\n    #############################\n    # EMBEDDING FUNCTIONS\n    #############################\n\n    def compute_gst(self, inputs, style_input, speaker_embedding=None):\n        \"\"\"Compute global style token\"\"\"\n        if isinstance(style_input, dict):\n            # multiply each style token with a weight\n            query = torch.zeros(1, 1, self.gst.gst_embedding_dim // 2).type_as(inputs)\n            if speaker_embedding is not None:\n                query = torch.cat([query, speaker_embedding.reshape(1, 1, -1)], dim=-1)\n\n            _GST = torch.tanh(self.gst_layer.style_token_layer.style_tokens)\n            gst_outputs = torch.zeros(1, 1, self.gst.gst_embedding_dim).type_as(inputs)\n            for k_token, v_amplifier in style_input.items():\n                key = _GST[int(k_token)].unsqueeze(0).expand(1, -1, -1)\n                gst_outputs_att = self.gst_layer.style_token_layer.attention(query, key)\n                gst_outputs = gst_outputs + gst_outputs_att * v_amplifier\n        elif style_input is None:\n            # ignore style token and return zero tensor\n            gst_outputs = torch.zeros(1, 1, self.gst.gst_embedding_dim).type_as(inputs)\n        else:\n            # compute style tokens\n            gst_outputs = self.gst_layer(style_input, speaker_embedding)  # pylint: disable=not-callable\n        inputs = self._concat_speaker_embedding(inputs, gst_outputs)\n        return inputs\n\n    def compute_capacitron_VAE_embedding(self, inputs, reference_mel_info, text_info=None, speaker_embedding=None):\n        \"\"\"Capacitron Variational Autoencoder\"\"\"\n        (\n            VAE_outputs,\n            posterior_distribution,\n            prior_distribution,\n            capacitron_beta,\n        ) = self.capacitron_vae_layer(\n            reference_mel_info,\n            text_info,\n            speaker_embedding,  # pylint: disable=not-callable\n        )\n\n        VAE_outputs = VAE_outputs.to(inputs.device)\n        encoder_output = self._concat_speaker_embedding(\n            inputs, VAE_outputs\n        )  # concatenate to the output of the basic tacotron encoder\n        return (\n            encoder_output,\n            posterior_distribution,\n            prior_distribution,\n            capacitron_beta,\n        )\n\n    @staticmethod\n    def _add_speaker_embedding(outputs, embedded_speakers):\n        embedded_speakers_ = embedded_speakers.expand(outputs.size(0), outputs.size(1), -1)\n        outputs = outputs + embedded_speakers_\n        return outputs\n\n    @staticmethod\n    def _concat_speaker_embedding(outputs, embedded_speakers):\n        embedded_speakers_ = embedded_speakers.expand(outputs.size(0), outputs.size(1), -1)\n        outputs = torch.cat([outputs, embedded_speakers_], dim=-1)\n        return outputs\n\n    #############################\n    # CALLBACKS\n    #############################\n\n    def on_epoch_start(self, trainer):\n        \"\"\"Callback for setting values wrt gradual training schedule.\n\n        Args:\n            trainer (TrainerTTS): TTS trainer object that is used to train this model.\n        \"\"\"\n        if self.gradual_training:\n            r, trainer.config.batch_size = gradual_training_scheduler(trainer.total_steps_done, trainer.config)\n            trainer.config.r = r\n            self.decoder.set_r(r)\n            if trainer.config.bidirectional_decoder:\n                trainer.model.decoder_backward.set_r(r)\n            print(f\"\\n > Number of output frames: {self.decoder.r}\")\n", "TTS/tts/models/delightful_tts.py": "import os\nfrom dataclasses import dataclass, field\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torchaudio\nfrom coqpit import Coqpit\nfrom librosa.filters import mel as librosa_mel_fn\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom trainer.torch import DistributedSampler, DistributedSamplerWrapper\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.tts.datasets.dataset import F0Dataset, TTSDataset, _parse_sample\nfrom TTS.tts.layers.delightful_tts.acoustic_model import AcousticModel\nfrom TTS.tts.layers.losses import ForwardSumLoss, VitsDiscriminatorLoss\nfrom TTS.tts.layers.vits.discriminator import VitsDiscriminator\nfrom TTS.tts.models.base_tts import BaseTTSE2E\nfrom TTS.tts.utils.helpers import average_over_durations, compute_attn_prior, rand_segments, segment, sequence_mask\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_avg_pitch, plot_pitch, plot_spectrogram\nfrom TTS.utils.audio.numpy_transforms import build_mel_basis, compute_f0\nfrom TTS.utils.audio.numpy_transforms import db_to_amp as db_to_amp_numpy\nfrom TTS.utils.audio.numpy_transforms import mel_to_wav as mel_to_wav_numpy\nfrom TTS.utils.audio.processor import AudioProcessor\nfrom TTS.utils.io import load_fsspec\nfrom TTS.vocoder.layers.losses import MultiScaleSTFTLoss\nfrom TTS.vocoder.models.hifigan_generator import HifiganGenerator\nfrom TTS.vocoder.utils.generic_utils import plot_results\n\n\ndef id_to_torch(aux_id, cuda=False):\n    if aux_id is not None:\n        aux_id = np.asarray(aux_id)\n        aux_id = torch.from_numpy(aux_id)\n    if cuda:\n        return aux_id.cuda()\n    return aux_id\n\n\ndef embedding_to_torch(d_vector, cuda=False):\n    if d_vector is not None:\n        d_vector = np.asarray(d_vector)\n        d_vector = torch.from_numpy(d_vector).float()\n        d_vector = d_vector.squeeze().unsqueeze(0)\n    if cuda:\n        return d_vector.cuda()\n    return d_vector\n\n\ndef numpy_to_torch(np_array, dtype, cuda=False):\n    if np_array is None:\n        return None\n    tensor = torch.as_tensor(np_array, dtype=dtype)\n    if cuda:\n        return tensor.cuda()\n    return tensor\n\n\ndef get_mask_from_lengths(lengths: torch.Tensor) -> torch.Tensor:\n    batch_size = lengths.shape[0]\n    max_len = torch.max(lengths).item()\n    ids = torch.arange(0, max_len, device=lengths.device).unsqueeze(0).expand(batch_size, -1)\n    mask = ids >= lengths.unsqueeze(1).expand(-1, max_len)\n    return mask\n\n\ndef pad(input_ele: List[torch.Tensor], max_len: int) -> torch.Tensor:\n    out_list = torch.jit.annotate(List[torch.Tensor], [])\n    for batch in input_ele:\n        if len(batch.shape) == 1:\n            one_batch_padded = F.pad(batch, (0, max_len - batch.size(0)), \"constant\", 0.0)\n        else:\n            one_batch_padded = F.pad(batch, (0, 0, 0, max_len - batch.size(0)), \"constant\", 0.0)\n        out_list.append(one_batch_padded)\n    out_padded = torch.stack(out_list)\n    return out_padded\n\n\ndef init_weights(m: nn.Module, mean: float = 0.0, std: float = 0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\n\n\ndef stride_lens(lens: torch.Tensor, stride: int = 2) -> torch.Tensor:\n    return torch.ceil(lens / stride).int()\n\n\ndef initialize_embeddings(shape: Tuple[int]) -> torch.Tensor:\n    assert len(shape) == 2, \"Can only initialize 2-D embedding matrices ...\"\n    return torch.randn(shape) * np.sqrt(2 / shape[1])\n\n\n# pylint: disable=redefined-outer-name\ndef calc_same_padding(kernel_size: int) -> Tuple[int, int]:\n    pad = kernel_size // 2\n    return (pad, pad - (kernel_size + 1) % 2)\n\n\nhann_window = {}\nmel_basis = {}\n\n\n@torch.no_grad()\ndef weights_reset(m: nn.Module):\n    # check if the current module has reset_parameters and if it is reset the weight\n    reset_parameters = getattr(m, \"reset_parameters\", None)\n    if callable(reset_parameters):\n        m.reset_parameters()\n\n\ndef get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}\n    for name, w in mdl.named_parameters():\n        if \"weight\" in name:\n            value = w.data.sum().item()\n            dict_sums[name] = value\n    return dict_sums\n\n\ndef load_audio(file_path: str):\n    \"\"\"Load the audio file normalized in [-1, 1]\n\n    Return Shapes:\n        - x: :math:`[1, T]`\n    \"\"\"\n    x, sr = torchaudio.load(\n        file_path,\n    )\n    assert (x > 1).sum() + (x < -1).sum() == 0\n    return x, sr\n\n\ndef _amp_to_db(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef _db_to_amp(x, C=1):\n    return torch.exp(x) / C\n\n\ndef amp_to_db(magnitudes):\n    output = _amp_to_db(magnitudes)\n    return output\n\n\ndef db_to_amp(magnitudes):\n    output = _db_to_amp(magnitudes)\n    return output\n\n\ndef _wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    y = y.squeeze(1)\n\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global hann_window  # pylint: disable=global-statement\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    return spec\n\n\ndef wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"\n    spec = _wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef wav_to_energy(y, n_fft, hop_length, win_length, center=False):\n    spec = _wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return torch.norm(spec, dim=1, keepdim=True)\n\n\ndef name_mel_basis(spec, n_fft, fmax):\n    n_fft_len = f\"{n_fft}_{fmax}_{spec.dtype}_{spec.device}\"\n    return n_fft_len\n\n\ndef spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax):\n    \"\"\"\n    Args Shapes:\n        - spec : :math:`[B,C,T]`\n\n    Return Shapes:\n        - mel : :math:`[B,C,T]`\n    \"\"\"\n    global mel_basis  # pylint: disable=global-statement\n    mel_basis_key = name_mel_basis(spec, n_fft, fmax)\n    # pylint: disable=too-many-function-args\n    if mel_basis_key not in mel_basis:\n        # pylint: disable=missing-kwoa\n        mel = librosa_mel_fn(sample_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[mel_basis_key] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    mel = torch.matmul(mel_basis[mel_basis_key], spec)\n    mel = amp_to_db(mel)\n    return mel\n\n\ndef wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T_y]`\n\n    Return Shapes:\n        - spec : :math:`[B,C,T_spec]`\n    \"\"\"\n    y = y.squeeze(1)\n\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n\n    global mel_basis, hann_window  # pylint: disable=global-statement\n    mel_basis_key = name_mel_basis(y, n_fft, fmax)\n    wnsize_dtype_device = str(win_length) + \"_\" + str(y.dtype) + \"_\" + str(y.device)\n    if mel_basis_key not in mel_basis:\n        # pylint: disable=missing-kwoa\n        mel = librosa_mel_fn(\n            sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax\n        )  # pylint: disable=too-many-function-args\n        mel_basis[mel_basis_key] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    )\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    spec = torch.matmul(mel_basis[mel_basis_key], spec)\n    spec = amp_to_db(spec)\n    return spec\n\n\n##############################\n# DATASET\n##############################\n\n\ndef get_attribute_balancer_weights(items: list, attr_name: str, multi_dict: dict = None):\n    \"\"\"Create balancer weight for torch WeightedSampler\"\"\"\n    attr_names_samples = np.array([item[attr_name] for item in items])\n    unique_attr_names = np.unique(attr_names_samples).tolist()\n    attr_idx = [unique_attr_names.index(l) for l in attr_names_samples]\n    attr_count = np.array([len(np.where(attr_names_samples == l)[0]) for l in unique_attr_names])\n    weight_attr = 1.0 / attr_count\n    dataset_samples_weight = np.array([weight_attr[l] for l in attr_idx])\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n    if multi_dict is not None:\n        multiplier_samples = np.array([multi_dict.get(item[attr_name], 1.0) for item in items])\n        dataset_samples_weight *= multiplier_samples\n    return (\n        torch.from_numpy(dataset_samples_weight).float(),\n        unique_attr_names,\n        np.unique(dataset_samples_weight).tolist(),\n    )\n\n\nclass ForwardTTSE2eF0Dataset(F0Dataset):\n    \"\"\"Override F0Dataset to avoid slow computing of pitches\"\"\"\n\n    def __init__(\n        self,\n        ap,\n        samples: Union[List[List], List[Dict]],\n        verbose=False,\n        cache_path: str = None,\n        precompute_num_workers=0,\n        normalize_f0=True,\n    ):\n        super().__init__(\n            samples=samples,\n            ap=ap,\n            verbose=verbose,\n            cache_path=cache_path,\n            precompute_num_workers=precompute_num_workers,\n            normalize_f0=normalize_f0,\n        )\n\n    def _compute_and_save_pitch(self, wav_file, pitch_file=None):\n        wav, _ = load_audio(wav_file)\n        f0 = compute_f0(\n            x=wav.numpy()[0],\n            sample_rate=self.ap.sample_rate,\n            hop_length=self.ap.hop_length,\n            pitch_fmax=self.ap.pitch_fmax,\n            pitch_fmin=self.ap.pitch_fmin,\n            win_length=self.ap.win_length,\n        )\n        # skip the last F0 value to align with the spectrogram\n        if wav.shape[1] % self.ap.hop_length != 0:\n            f0 = f0[:-1]\n        if pitch_file:\n            np.save(pitch_file, f0)\n        return f0\n\n    def compute_or_load(self, wav_file, audio_name):\n        \"\"\"\n        compute pitch and return a numpy array of pitch values\n        \"\"\"\n        pitch_file = self.create_pitch_file_path(audio_name, self.cache_path)\n        if not os.path.exists(pitch_file):\n            pitch = self._compute_and_save_pitch(wav_file=wav_file, pitch_file=pitch_file)\n        else:\n            pitch = np.load(pitch_file)\n        return pitch.astype(np.float32)\n\n\nclass ForwardTTSE2eDataset(TTSDataset):\n    def __init__(self, *args, **kwargs):\n        # don't init the default F0Dataset in TTSDataset\n        compute_f0 = kwargs.pop(\"compute_f0\", False)\n        kwargs[\"compute_f0\"] = False\n        self.attn_prior_cache_path = kwargs.pop(\"attn_prior_cache_path\")\n\n        super().__init__(*args, **kwargs)\n\n        self.compute_f0 = compute_f0\n        self.pad_id = self.tokenizer.characters.pad_id\n        self.ap = kwargs[\"ap\"]\n\n        if self.compute_f0:\n            self.f0_dataset = ForwardTTSE2eF0Dataset(\n                ap=self.ap,\n                samples=self.samples,\n                cache_path=kwargs[\"f0_cache_path\"],\n                precompute_num_workers=kwargs[\"precompute_num_workers\"],\n            )\n\n        if self.attn_prior_cache_path is not None:\n            os.makedirs(self.attn_prior_cache_path, exist_ok=True)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n\n        rel_wav_path = Path(item[\"audio_file\"]).relative_to(item[\"root_path\"]).with_suffix(\"\")\n        rel_wav_path = str(rel_wav_path).replace(\"/\", \"_\")\n\n        raw_text = item[\"text\"]\n        wav, _ = load_audio(item[\"audio_file\"])\n        wav_filename = os.path.basename(item[\"audio_file\"])\n\n        try:\n            token_ids = self.get_token_ids(idx, item[\"text\"])\n        except:\n            print(idx, item)\n            # pylint: disable=raise-missing-from\n            raise OSError\n        f0 = None\n        if self.compute_f0:\n            f0 = self.get_f0(idx)[\"f0\"]\n\n        # after phonemization the text length may change\n        # this is a shameful \ud83e\udd2d hack to prevent longer phonemes\n        # TODO: find a better fix\n        if len(token_ids) > self.max_text_len or wav.shape[1] < self.min_audio_len:\n            self.rescue_item_idx += 1\n            return self.__getitem__(self.rescue_item_idx)\n\n        attn_prior = None\n        if self.attn_prior_cache_path is not None:\n            attn_prior = self.load_or_compute_attn_prior(token_ids, wav, rel_wav_path)\n\n        return {\n            \"raw_text\": raw_text,\n            \"token_ids\": token_ids,\n            \"token_len\": len(token_ids),\n            \"wav\": wav,\n            \"pitch\": f0,\n            \"wav_file\": wav_filename,\n            \"speaker_name\": item[\"speaker_name\"],\n            \"language_name\": item[\"language\"],\n            \"attn_prior\": attn_prior,\n            \"audio_unique_name\": item[\"audio_unique_name\"],\n        }\n\n    def load_or_compute_attn_prior(self, token_ids, wav, rel_wav_path):\n        \"\"\"Load or compute and save the attention prior.\"\"\"\n        attn_prior_file = os.path.join(self.attn_prior_cache_path, f\"{rel_wav_path}.npy\")\n        # pylint: disable=no-else-return\n        if os.path.exists(attn_prior_file):\n            return np.load(attn_prior_file)\n        else:\n            token_len = len(token_ids)\n            mel_len = wav.shape[1] // self.ap.hop_length\n            attn_prior = compute_attn_prior(token_len, mel_len)\n            np.save(attn_prior_file, attn_prior)\n            return attn_prior\n\n    @property\n    def lengths(self):\n        lens = []\n        for item in self.samples:\n            _, wav_file, *_ = _parse_sample(item)\n            audio_len = os.path.getsize(wav_file) / 16 * 8  # assuming 16bit audio\n            lens.append(audio_len)\n        return lens\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Return Shapes:\n            - tokens: :math:`[B, T]`\n            - token_lens :math:`[B]`\n            - token_rel_lens :math:`[B]`\n            - pitch :math:`[B, T]`\n            - waveform: :math:`[B, 1, T]`\n            - waveform_lens: :math:`[B]`\n            - waveform_rel_lens: :math:`[B]`\n            - speaker_names: :math:`[B]`\n            - language_names: :math:`[B]`\n            - audiofile_paths: :math:`[B]`\n            - raw_texts: :math:`[B]`\n            - attn_prior: :math:`[[T_token, T_mel]]`\n        \"\"\"\n        B = len(batch)\n        batch = {k: [dic[k] for dic in batch] for k in batch[0]}\n\n        max_text_len = max([len(x) for x in batch[\"token_ids\"]])\n        token_lens = torch.LongTensor(batch[\"token_len\"])\n        token_rel_lens = token_lens / token_lens.max()\n\n        wav_lens = [w.shape[1] for w in batch[\"wav\"]]\n        wav_lens = torch.LongTensor(wav_lens)\n        wav_lens_max = torch.max(wav_lens)\n        wav_rel_lens = wav_lens / wav_lens_max\n\n        pitch_padded = None\n        if self.compute_f0:\n            pitch_lens = [p.shape[0] for p in batch[\"pitch\"]]\n            pitch_lens = torch.LongTensor(pitch_lens)\n            pitch_lens_max = torch.max(pitch_lens)\n            pitch_padded = torch.FloatTensor(B, 1, pitch_lens_max)\n            pitch_padded = pitch_padded.zero_() + self.pad_id\n\n        token_padded = torch.LongTensor(B, max_text_len)\n        wav_padded = torch.FloatTensor(B, 1, wav_lens_max)\n\n        token_padded = token_padded.zero_() + self.pad_id\n        wav_padded = wav_padded.zero_() + self.pad_id\n\n        for i in range(B):\n            token_ids = batch[\"token_ids\"][i]\n            token_padded[i, : batch[\"token_len\"][i]] = torch.LongTensor(token_ids)\n\n            wav = batch[\"wav\"][i]\n            wav_padded[i, :, : wav.size(1)] = torch.FloatTensor(wav)\n\n            if self.compute_f0:\n                pitch = batch[\"pitch\"][i]\n                pitch_padded[i, 0, : len(pitch)] = torch.FloatTensor(pitch)\n\n        return {\n            \"text_input\": token_padded,\n            \"text_lengths\": token_lens,\n            \"text_rel_lens\": token_rel_lens,\n            \"pitch\": pitch_padded,\n            \"waveform\": wav_padded,  # (B x T)\n            \"waveform_lens\": wav_lens,  # (B)\n            \"waveform_rel_lens\": wav_rel_lens,\n            \"speaker_names\": batch[\"speaker_name\"],\n            \"language_names\": batch[\"language_name\"],\n            \"audio_unique_names\": batch[\"audio_unique_name\"],\n            \"audio_files\": batch[\"wav_file\"],\n            \"raw_text\": batch[\"raw_text\"],\n            \"attn_priors\": batch[\"attn_prior\"] if batch[\"attn_prior\"][0] is not None else None,\n        }\n\n\n##############################\n# CONFIG DEFINITIONS\n##############################\n\n\n@dataclass\nclass VocoderConfig(Coqpit):\n    resblock_type_decoder: str = \"1\"\n    resblock_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [3, 7, 11])\n    resblock_dilation_sizes_decoder: List[List[int]] = field(default_factory=lambda: [[1, 3, 5], [1, 3, 5], [1, 3, 5]])\n    upsample_rates_decoder: List[int] = field(default_factory=lambda: [8, 8, 2, 2])\n    upsample_initial_channel_decoder: int = 512\n    upsample_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [16, 16, 4, 4])\n    use_spectral_norm_discriminator: bool = False\n    upsampling_rates_discriminator: List[int] = field(default_factory=lambda: [4, 4, 4, 4])\n    periods_discriminator: List[int] = field(default_factory=lambda: [2, 3, 5, 7, 11])\n    pretrained_model_path: Optional[str] = None\n\n\n@dataclass\nclass DelightfulTtsAudioConfig(Coqpit):\n    sample_rate: int = 22050\n    hop_length: int = 256\n    win_length: int = 1024\n    fft_size: int = 1024\n    mel_fmin: float = 0.0\n    mel_fmax: float = 8000\n    num_mels: int = 100\n    pitch_fmax: float = 640.0\n    pitch_fmin: float = 1.0\n    resample: bool = False\n    preemphasis: float = 0.0\n    ref_level_db: int = 20\n    do_sound_norm: bool = False\n    log_func: str = \"np.log10\"\n    do_trim_silence: bool = True\n    trim_db: int = 45\n    do_rms_norm: bool = False\n    db_level: float = None\n    power: float = 1.5\n    griffin_lim_iters: int = 60\n    spec_gain: int = 20\n    do_amp_to_db_linear: bool = True\n    do_amp_to_db_mel: bool = True\n    min_level_db: int = -100\n    max_norm: float = 4.0\n\n\n@dataclass\nclass DelightfulTtsArgs(Coqpit):\n    num_chars: int = 100\n    spec_segment_size: int = 32\n    n_hidden_conformer_encoder: int = 512\n    n_layers_conformer_encoder: int = 6\n    n_heads_conformer_encoder: int = 8\n    dropout_conformer_encoder: float = 0.1\n    kernel_size_conv_mod_conformer_encoder: int = 7\n    kernel_size_depthwise_conformer_encoder: int = 7\n    lrelu_slope: float = 0.3\n    n_hidden_conformer_decoder: int = 512\n    n_layers_conformer_decoder: int = 6\n    n_heads_conformer_decoder: int = 8\n    dropout_conformer_decoder: float = 0.1\n    kernel_size_conv_mod_conformer_decoder: int = 11\n    kernel_size_depthwise_conformer_decoder: int = 11\n    bottleneck_size_p_reference_encoder: int = 4\n    bottleneck_size_u_reference_encoder: int = 512\n    ref_enc_filters_reference_encoder = [32, 32, 64, 64, 128, 128]\n    ref_enc_size_reference_encoder: int = 3\n    ref_enc_strides_reference_encoder = [1, 2, 1, 2, 1]\n    ref_enc_pad_reference_encoder = [1, 1]\n    ref_enc_gru_size_reference_encoder: int = 32\n    ref_attention_dropout_reference_encoder: float = 0.2\n    token_num_reference_encoder: int = 32\n    predictor_kernel_size_reference_encoder: int = 5\n    n_hidden_variance_adaptor: int = 512\n    kernel_size_variance_adaptor: int = 5\n    dropout_variance_adaptor: float = 0.5\n    n_bins_variance_adaptor: int = 256\n    emb_kernel_size_variance_adaptor: int = 3\n    use_speaker_embedding: bool = False\n    num_speakers: int = 0\n    speakers_file: str = None\n    d_vector_file: str = None\n    speaker_embedding_channels: int = 384\n    use_d_vector_file: bool = False\n    d_vector_dim: int = 0\n    freeze_vocoder: bool = False\n    freeze_text_encoder: bool = False\n    freeze_duration_predictor: bool = False\n    freeze_pitch_predictor: bool = False\n    freeze_energy_predictor: bool = False\n    freeze_basis_vectors_predictor: bool = False\n    freeze_decoder: bool = False\n    length_scale: float = 1.0\n\n\n##############################\n# MODEL DEFINITION\n##############################\nclass DelightfulTTS(BaseTTSE2E):\n    \"\"\"\n    Paper::\n        https://arxiv.org/pdf/2110.12612.pdf\n\n    Paper Abstract::\n        This paper describes the Microsoft end-to-end neural text to speech (TTS) system: DelightfulTTS for Blizzard Challenge 2021.\n        The goal of this challenge is to synthesize natural and high-quality speech from text, and we approach this goal in two perspectives:\n        The first is to directly model and generate waveform in 48 kHz sampling rate, which brings higher perception quality than previous systems\n        with 16 kHz or 24 kHz sampling rate; The second is to model the variation information in speech through a systematic design, which improves\n        the prosody and naturalness. Specifically, for 48 kHz modeling, we predict 16 kHz mel-spectrogram in acoustic model, and\n        propose a vocoder called HiFiNet to directly generate 48 kHz waveform from predicted 16 kHz mel-spectrogram, which can better trade off training\n        efficiency, modelling stability and voice quality. We model variation information systematically from both explicit (speaker ID, language ID, pitch and duration) and\n        implicit (utterance-level and phoneme-level prosody) perspectives: 1) For speaker and language ID, we use lookup embedding in training and\n        inference; 2) For pitch and duration, we extract the values from paired text-speech data in training and use two predictors to predict the values in inference; 3)\n        For utterance-level and phoneme-level prosody, we use two reference encoders to extract the values in training, and use two separate predictors to predict the values in inference.\n        Additionally, we introduce an improved Conformer block to better model the local and global dependency in acoustic model. For task SH1, DelightfulTTS achieves 4.17 mean score in MOS test\n        and 4.35 in SMOS test, which indicates the effectiveness of our proposed system\n\n\n    Model training::\n        text --> ForwardTTS() --> spec_hat --> rand_seg_select()--> GANVocoder() --> waveform_seg\n        spec --------^\n\n    Examples:\n        >>> from TTS.tts.models.forward_tts_e2e import ForwardTTSE2e, ForwardTTSE2eConfig\n        >>> config = ForwardTTSE2eConfig()\n        >>> model = ForwardTTSE2e(config)\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        config: Coqpit,\n        ap,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config=config, ap=ap, tokenizer=tokenizer, speaker_manager=speaker_manager)\n        self.ap = ap\n\n        self._set_model_args(config)\n        self.init_multispeaker(config)\n        self.binary_loss_weight = None\n\n        self.args.out_channels = self.config.audio.num_mels\n        self.args.num_mels = self.config.audio.num_mels\n        self.acoustic_model = AcousticModel(args=self.args, tokenizer=tokenizer, speaker_manager=speaker_manager)\n\n        self.waveform_decoder = HifiganGenerator(\n            self.config.audio.num_mels,\n            1,\n            self.config.vocoder.resblock_type_decoder,\n            self.config.vocoder.resblock_dilation_sizes_decoder,\n            self.config.vocoder.resblock_kernel_sizes_decoder,\n            self.config.vocoder.upsample_kernel_sizes_decoder,\n            self.config.vocoder.upsample_initial_channel_decoder,\n            self.config.vocoder.upsample_rates_decoder,\n            inference_padding=0,\n            # cond_channels=self.embedded_speaker_dim,\n            conv_pre_weight_norm=False,\n            conv_post_weight_norm=False,\n            conv_post_bias=False,\n        )\n\n        if self.config.init_discriminator:\n            self.disc = VitsDiscriminator(\n                use_spectral_norm=self.config.vocoder.use_spectral_norm_discriminator,\n                periods=self.config.vocoder.periods_discriminator,\n            )\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def energy_scaler(self):\n        return self.acoustic_model.energy_scaler\n\n    @property\n    def length_scale(self):\n        return self.acoustic_model.length_scale\n\n    @length_scale.setter\n    def length_scale(self, value):\n        self.acoustic_model.length_scale = value\n\n    @property\n    def pitch_mean(self):\n        return self.acoustic_model.pitch_mean\n\n    @pitch_mean.setter\n    def pitch_mean(self, value):\n        self.acoustic_model.pitch_mean = value\n\n    @property\n    def pitch_std(self):\n        return self.acoustic_model.pitch_std\n\n    @pitch_std.setter\n    def pitch_std(self, value):\n        self.acoustic_model.pitch_std = value\n\n    @property\n    def mel_basis(self):\n        return build_mel_basis(\n            sample_rate=self.ap.sample_rate,\n            fft_size=self.ap.fft_size,\n            num_mels=self.ap.num_mels,\n            mel_fmax=self.ap.mel_fmax,\n            mel_fmin=self.ap.mel_fmin,\n        )  # pylint: disable=function-redefined\n\n    def init_for_training(self) -> None:\n        self.train_disc = (  # pylint: disable=attribute-defined-outside-init\n            self.config.steps_to_start_discriminator <= 0\n        )  # pylint: disable=attribute-defined-outside-init\n        self.update_energy_scaler = True  # pylint: disable=attribute-defined-outside-init\n\n    def init_multispeaker(self, config: Coqpit):\n        \"\"\"Init for multi-speaker training.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        self.embedded_speaker_dim = 0\n        self.num_speakers = self.args.num_speakers\n        self.audio_transform = None\n\n        if self.speaker_manager:\n            self.num_speakers = self.speaker_manager.num_speakers\n            self.args.num_speakers = self.speaker_manager.num_speakers\n\n        if self.args.use_speaker_embedding:\n            self._init_speaker_embedding()\n\n        if self.args.use_d_vector_file:\n            self._init_d_vector()\n\n    def _init_speaker_embedding(self):\n        # pylint: disable=attribute-defined-outside-init\n        if self.num_speakers > 0:\n            print(\" > initialization of speaker-embedding layers.\")\n            self.embedded_speaker_dim = self.args.speaker_embedding_channels\n            self.args.embedded_speaker_dim = self.args.speaker_embedding_channels\n\n    def _init_d_vector(self):\n        # pylint: disable=attribute-defined-outside-init\n        if hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Speaker embedding layer already initialized before d_vector settings.\")\n        self.embedded_speaker_dim = self.args.d_vector_dim\n        self.args.embedded_speaker_dim = self.args.d_vector_dim\n\n    def _freeze_layers(self):\n        if self.args.freeze_vocoder:\n            for param in self.vocoder.paramseters():\n                param.requires_grad = False\n\n        if self.args.freeze_text_encoder:\n            for param in self.text_encoder.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_duration_predictor:\n            for param in self.durarion_predictor.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_pitch_predictor:\n            for param in self.pitch_predictor.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_energy_predictor:\n            for param in self.energy_predictor.parameters():\n                param.requires_grad = False\n\n        if self.args.freeze_decoder:\n            for param in self.decoder.parameters():\n                param.requires_grad = False\n\n    def forward(\n        self,\n        x: torch.LongTensor,\n        x_lengths: torch.LongTensor,\n        spec_lengths: torch.LongTensor,\n        spec: torch.FloatTensor,\n        waveform: torch.FloatTensor,\n        pitch: torch.FloatTensor = None,\n        energy: torch.FloatTensor = None,\n        attn_priors: torch.FloatTensor = None,\n        d_vectors: torch.FloatTensor = None,\n        speaker_idx: torch.LongTensor = None,\n    ) -> Dict:\n        \"\"\"Model's forward pass.\n\n        Args:\n            x (torch.LongTensor): Input character sequences.\n            x_lengths (torch.LongTensor): Input sequence lengths.\n            spec_lengths (torch.LongTensor): Spectrogram sequnce lengths. Defaults to None.\n            spec (torch.FloatTensor): Spectrogram frames. Only used when the alignment network is on. Defaults to None.\n            waveform (torch.FloatTensor): Waveform. Defaults to None.\n            pitch (torch.FloatTensor): Pitch values for each spectrogram frame. Only used when the pitch predictor is on. Defaults to None.\n            energy (torch.FloatTensor): Spectral energy values for each spectrogram frame. Only used when the energy predictor is on. Defaults to None.\n            attn_priors (torch.FloatTentrasor): Attention priors for the aligner network. Defaults to None.\n            aux_input (Dict): Auxiliary model inputs for multi-speaker training. Defaults to `{\"d_vectors\": 0, \"speaker_ids\": None}`.\n\n        Shapes:\n            - x: :math:`[B, T_max]`\n            - x_lengths: :math:`[B]`\n            - spec_lengths: :math:`[B]`\n            - spec: :math:`[B, T_max2, C_spec]`\n            - waveform: :math:`[B, 1, T_max2 * hop_length]`\n            - g: :math:`[B, C]`\n            - pitch: :math:`[B, 1, T_max2]`\n            - energy: :math:`[B, 1, T_max2]`\n        \"\"\"\n        encoder_outputs = self.acoustic_model(\n            tokens=x,\n            src_lens=x_lengths,\n            mel_lens=spec_lengths,\n            mels=spec,\n            pitches=pitch,\n            energies=energy,\n            attn_priors=attn_priors,\n            d_vectors=d_vectors,\n            speaker_idx=speaker_idx,\n        )\n\n        # use mel-spec from the decoder\n        vocoder_input = encoder_outputs[\"model_outputs\"]  # [B, T_max2, C_mel]\n\n        vocoder_input_slices, slice_ids = rand_segments(\n            x=vocoder_input.transpose(1, 2),\n            x_lengths=spec_lengths,\n            segment_size=self.args.spec_segment_size,\n            let_short_samples=True,\n            pad_short=True,\n        )\n        if encoder_outputs[\"spk_emb\"] is not None:\n            g = encoder_outputs[\"spk_emb\"].unsqueeze(-1)\n        else:\n            g = None\n\n        vocoder_output = self.waveform_decoder(x=vocoder_input_slices.detach(), g=g)\n        wav_seg = segment(\n            waveform,\n            slice_ids * self.ap.hop_length,\n            self.args.spec_segment_size * self.ap.hop_length,\n            pad_short=True,\n        )\n        model_outputs = {**encoder_outputs}\n        model_outputs[\"acoustic_model_outputs\"] = encoder_outputs[\"model_outputs\"]\n        model_outputs[\"model_outputs\"] = vocoder_output\n        model_outputs[\"waveform_seg\"] = wav_seg\n        model_outputs[\"slice_ids\"] = slice_ids\n        return model_outputs\n\n    @torch.no_grad()\n    def inference(\n        self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}, pitch_transform=None, energy_transform=None\n    ):\n        encoder_outputs = self.acoustic_model.inference(\n            tokens=x,\n            d_vectors=aux_input[\"d_vectors\"],\n            speaker_idx=aux_input[\"speaker_ids\"],\n            pitch_transform=pitch_transform,\n            energy_transform=energy_transform,\n            p_control=None,\n            d_control=None,\n        )\n        vocoder_input = encoder_outputs[\"model_outputs\"].transpose(1, 2)  # [B, T_max2, C_mel] -> [B, C_mel, T_max2]\n        if encoder_outputs[\"spk_emb\"] is not None:\n            g = encoder_outputs[\"spk_emb\"].unsqueeze(-1)\n        else:\n            g = None\n\n        vocoder_output = self.waveform_decoder(x=vocoder_input, g=g)\n        model_outputs = {**encoder_outputs}\n        model_outputs[\"model_outputs\"] = vocoder_output\n        return model_outputs\n\n    @torch.no_grad()\n    def inference_spec_decoder(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):\n        encoder_outputs = self.acoustic_model.inference(\n            tokens=x,\n            d_vectors=aux_input[\"d_vectors\"],\n            speaker_idx=aux_input[\"speaker_ids\"],\n        )\n        model_outputs = {**encoder_outputs}\n        return model_outputs\n\n    def train_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int):\n        if optimizer_idx == 0:\n            tokens = batch[\"text_input\"]\n            token_lenghts = batch[\"text_lengths\"]\n            mel = batch[\"mel_input\"]\n            mel_lens = batch[\"mel_lengths\"]\n            waveform = batch[\"waveform\"]  # [B, T, C] -> [B, C, T]\n            pitch = batch[\"pitch\"]\n            d_vectors = batch[\"d_vectors\"]\n            speaker_ids = batch[\"speaker_ids\"]\n            attn_priors = batch[\"attn_priors\"]\n            energy = batch[\"energy\"]\n\n            # generator pass\n            outputs = self.forward(\n                x=tokens,\n                x_lengths=token_lenghts,\n                spec_lengths=mel_lens,\n                spec=mel,\n                waveform=waveform,\n                pitch=pitch,\n                energy=energy,\n                attn_priors=attn_priors,\n                d_vectors=d_vectors,\n                speaker_idx=speaker_ids,\n            )\n\n            # cache tensors for the generator pass\n            self.model_outputs_cache = outputs  # pylint: disable=attribute-defined-outside-init\n\n            if self.train_disc:\n                # compute scores and features\n                scores_d_fake, _, scores_d_real, _ = self.disc(\n                    outputs[\"model_outputs\"].detach(), outputs[\"waveform_seg\"]\n                )\n\n                # compute loss\n                with autocast(enabled=False):  # use float32 for the criterion\n                    loss_dict = criterion[optimizer_idx](\n                        scores_disc_fake=scores_d_fake,\n                        scores_disc_real=scores_d_real,\n                    )\n                return outputs, loss_dict\n            return None, None\n\n        if optimizer_idx == 1:\n            mel = batch[\"mel_input\"]\n            # compute melspec segment\n            with autocast(enabled=False):\n                mel_slice = segment(\n                    mel.float(), self.model_outputs_cache[\"slice_ids\"], self.args.spec_segment_size, pad_short=True\n                )\n\n                mel_slice_hat = wav_to_mel(\n                    y=self.model_outputs_cache[\"model_outputs\"].float(),\n                    n_fft=self.ap.fft_size,\n                    sample_rate=self.ap.sample_rate,\n                    num_mels=self.ap.num_mels,\n                    hop_length=self.ap.hop_length,\n                    win_length=self.ap.win_length,\n                    fmin=self.ap.mel_fmin,\n                    fmax=self.ap.mel_fmax,\n                    center=False,\n                )\n\n                scores_d_fake = None\n                feats_d_fake = None\n                feats_d_real = None\n\n            if self.train_disc:\n                # compute discriminator scores and features\n                scores_d_fake, feats_d_fake, _, feats_d_real = self.disc(\n                    self.model_outputs_cache[\"model_outputs\"], self.model_outputs_cache[\"waveform_seg\"]\n                )\n\n            # compute losses\n            with autocast(enabled=True):  # use float32 for the criterion\n                loss_dict = criterion[optimizer_idx](\n                    mel_output=self.model_outputs_cache[\"acoustic_model_outputs\"].transpose(1, 2),\n                    mel_target=batch[\"mel_input\"],\n                    mel_lens=batch[\"mel_lengths\"],\n                    dur_output=self.model_outputs_cache[\"dr_log_pred\"],\n                    dur_target=self.model_outputs_cache[\"dr_log_target\"].detach(),\n                    pitch_output=self.model_outputs_cache[\"pitch_pred\"],\n                    pitch_target=self.model_outputs_cache[\"pitch_target\"],\n                    energy_output=self.model_outputs_cache[\"energy_pred\"],\n                    energy_target=self.model_outputs_cache[\"energy_target\"],\n                    src_lens=batch[\"text_lengths\"],\n                    waveform=self.model_outputs_cache[\"waveform_seg\"],\n                    waveform_hat=self.model_outputs_cache[\"model_outputs\"],\n                    p_prosody_ref=self.model_outputs_cache[\"p_prosody_ref\"],\n                    p_prosody_pred=self.model_outputs_cache[\"p_prosody_pred\"],\n                    u_prosody_ref=self.model_outputs_cache[\"u_prosody_ref\"],\n                    u_prosody_pred=self.model_outputs_cache[\"u_prosody_pred\"],\n                    aligner_logprob=self.model_outputs_cache[\"aligner_logprob\"],\n                    aligner_hard=self.model_outputs_cache[\"aligner_mas\"],\n                    aligner_soft=self.model_outputs_cache[\"aligner_soft\"],\n                    binary_loss_weight=self.binary_loss_weight,\n                    feats_fake=feats_d_fake,\n                    feats_real=feats_d_real,\n                    scores_fake=scores_d_fake,\n                    spec_slice=mel_slice,\n                    spec_slice_hat=mel_slice_hat,\n                    skip_disc=not self.train_disc,\n                )\n\n                loss_dict[\"avg_text_length\"] = batch[\"text_lengths\"].float().mean()\n                loss_dict[\"avg_mel_length\"] = batch[\"mel_lengths\"].float().mean()\n                loss_dict[\"avg_text_batch_occupancy\"] = (\n                    batch[\"text_lengths\"].float() / batch[\"text_lengths\"].float().max()\n                ).mean()\n                loss_dict[\"avg_mel_batch_occupancy\"] = (\n                    batch[\"mel_lengths\"].float() / batch[\"mel_lengths\"].float().max()\n                ).mean()\n\n            return self.model_outputs_cache, loss_dict\n        raise ValueError(\" [!] Unexpected `optimizer_idx`.\")\n\n    def eval_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int):\n        return self.train_step(batch, criterion, optimizer_idx)\n\n    def _log(self, batch, outputs, name_prefix=\"train\"):\n        figures, audios = {}, {}\n\n        # encoder outputs\n        model_outputs = outputs[1][\"acoustic_model_outputs\"]\n        alignments = outputs[1][\"alignments\"]\n        mel_input = batch[\"mel_input\"]\n\n        pred_spec = model_outputs[0].data.cpu().numpy()\n        gt_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"prediction\": plot_spectrogram(pred_spec, None, output_fig=False),\n            \"ground_truth\": plot_spectrogram(gt_spec.T, None, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        # plot pitch figures\n        pitch_avg = abs(outputs[1][\"pitch_target\"][0, 0].data.cpu().numpy())\n        pitch_avg_hat = abs(outputs[1][\"pitch_pred\"][0, 0].data.cpu().numpy())\n        chars = self.tokenizer.decode(batch[\"text_input\"][0].data.cpu().numpy())\n        pitch_figures = {\n            \"pitch_ground_truth\": plot_avg_pitch(pitch_avg, chars, output_fig=False),\n            \"pitch_avg_predicted\": plot_avg_pitch(pitch_avg_hat, chars, output_fig=False),\n        }\n        figures.update(pitch_figures)\n\n        # plot energy figures\n        energy_avg = abs(outputs[1][\"energy_target\"][0, 0].data.cpu().numpy())\n        energy_avg_hat = abs(outputs[1][\"energy_pred\"][0, 0].data.cpu().numpy())\n        chars = self.tokenizer.decode(batch[\"text_input\"][0].data.cpu().numpy())\n        energy_figures = {\n            \"energy_ground_truth\": plot_avg_pitch(energy_avg, chars, output_fig=False),\n            \"energy_avg_predicted\": plot_avg_pitch(energy_avg_hat, chars, output_fig=False),\n        }\n        figures.update(energy_figures)\n\n        # plot the attention mask computed from the predicted durations\n        alignments_hat = outputs[1][\"alignments_dp\"][0].data.cpu().numpy()\n        figures[\"alignment_hat\"] = plot_alignment(alignments_hat.T, output_fig=False)\n\n        # Sample audio\n        encoder_audio = mel_to_wav_numpy(\n            mel=db_to_amp_numpy(x=pred_spec.T, gain=1, base=None), mel_basis=self.mel_basis, **self.config.audio\n        )\n        audios[f\"{name_prefix}/encoder_audio\"] = encoder_audio\n\n        # vocoder outputs\n        y_hat = outputs[1][\"model_outputs\"]\n        y = outputs[1][\"waveform_seg\"]\n\n        vocoder_figures = plot_results(y_hat=y_hat, y=y, ap=self.ap, name_prefix=name_prefix)\n        figures.update(vocoder_figures)\n\n        sample_voice = y_hat[0].squeeze(0).detach().cpu().numpy()\n        audios[f\"{name_prefix}/vocoder_audio\"] = sample_voice\n        return figures, audios\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ):  # pylint: disable=no-self-use, unused-argument\n        \"\"\"Create visualizations and waveform examples.\n\n        For example, here you can plot spectrograms and generate sample sample waveforms from these spectrograms to\n        be projected onto Tensorboard.\n\n        Args:\n            batch (Dict): Model inputs used at the previous training step.\n            outputs (Dict): Model outputs generated at the previous training step.\n\n        Returns:\n            Tuple[Dict, np.ndarray]: training plots and output waveform.\n        \"\"\"\n        figures, audios = self._log(batch=batch, outputs=outputs, name_prefix=\"vocoder/\")\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._log(batch=batch, outputs=outputs, name_prefix=\"vocoder/\")\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def get_aux_input_from_test_sentences(self, sentence_info):\n        if hasattr(self.config, \"model_args\"):\n            config = self.config.model_args\n        else:\n            config = self.config\n\n        # extract speaker and language info\n        text, speaker_name, style_wav = None, None, None\n\n        if isinstance(sentence_info, list):\n            if len(sentence_info) == 1:\n                text = sentence_info[0]\n            elif len(sentence_info) == 2:\n                text, speaker_name = sentence_info\n            elif len(sentence_info) == 3:\n                text, speaker_name, style_wav = sentence_info\n        else:\n            text = sentence_info\n\n        # get speaker  id/d_vector\n        speaker_id, d_vector = None, None\n        if hasattr(self, \"speaker_manager\"):\n            if config.use_d_vector_file:\n                if speaker_name is None:\n                    d_vector = self.speaker_manager.get_random_embedding()\n                else:\n                    d_vector = self.speaker_manager.get_mean_embedding(speaker_name, num_samples=None, randomize=False)\n            elif config.use_speaker_embedding:\n                if speaker_name is None:\n                    speaker_id = self.speaker_manager.get_random_id()\n                else:\n                    speaker_id = self.speaker_manager.name_to_id[speaker_name]\n\n        return {\"text\": text, \"speaker_id\": speaker_id, \"style_wav\": style_wav, \"d_vector\": d_vector}\n\n    def plot_outputs(self, text, wav, alignment, outputs):\n        figures = {}\n        pitch_avg_pred = outputs[\"pitch\"].cpu()\n        energy_avg_pred = outputs[\"energy\"].cpu()\n        spec = wav_to_mel(\n            y=torch.from_numpy(wav[None, :]),\n            n_fft=self.ap.fft_size,\n            sample_rate=self.ap.sample_rate,\n            num_mels=self.ap.num_mels,\n            hop_length=self.ap.hop_length,\n            win_length=self.ap.win_length,\n            fmin=self.ap.mel_fmin,\n            fmax=self.ap.mel_fmax,\n            center=False,\n        )[0].transpose(0, 1)\n        pitch = compute_f0(\n            x=wav[0],\n            sample_rate=self.ap.sample_rate,\n            hop_length=self.ap.hop_length,\n            pitch_fmax=self.ap.pitch_fmax,\n        )\n        input_text = self.tokenizer.ids_to_text(self.tokenizer.text_to_ids(text, language=\"en\"))\n        input_text = input_text.replace(\"<BLNK>\", \"_\")\n        durations = outputs[\"durations\"]\n        pitch_avg = average_over_durations(torch.from_numpy(pitch)[None, None, :], durations.cpu())  # [1, 1, n_frames]\n        pitch_avg_pred_denorm = (pitch_avg_pred * self.pitch_std) + self.pitch_mean\n        figures[\"alignment\"] = plot_alignment(alignment.transpose(1, 2), output_fig=False)\n        figures[\"spectrogram\"] = plot_spectrogram(spec)\n        figures[\"pitch_from_wav\"] = plot_pitch(pitch, spec)\n        figures[\"pitch_avg_from_wav\"] = plot_avg_pitch(pitch_avg.squeeze(), input_text)\n        figures[\"pitch_avg_pred\"] = plot_avg_pitch(pitch_avg_pred_denorm.squeeze(), input_text)\n        figures[\"energy_avg_pred\"] = plot_avg_pitch(energy_avg_pred.squeeze(), input_text)\n        return figures\n\n    def synthesize(\n        self,\n        text: str,\n        speaker_id: str = None,\n        d_vector: torch.tensor = None,\n        pitch_transform=None,\n        **kwargs,\n    ):  # pylint: disable=unused-argument\n        # TODO: add cloning support with ref_waveform\n        is_cuda = next(self.parameters()).is_cuda\n\n        # convert text to sequence of token IDs\n        text_inputs = np.asarray(\n            self.tokenizer.text_to_ids(text, language=None),\n            dtype=np.int32,\n        )\n\n        # set speaker inputs\n        _speaker_id = None\n        if speaker_id is not None and self.args.use_speaker_embedding:\n            if isinstance(speaker_id, str) and self.args.use_speaker_embedding:\n                # get the speaker id for the speaker embedding layer\n                _speaker_id = self.speaker_manager.name_to_id[speaker_id]\n                _speaker_id = id_to_torch(_speaker_id, cuda=is_cuda)\n\n        if speaker_id is not None and self.args.use_d_vector_file:\n            # get the average d_vector for the speaker\n            d_vector = self.speaker_manager.get_mean_embedding(speaker_id, num_samples=None, randomize=False)\n        d_vector = embedding_to_torch(d_vector, cuda=is_cuda)\n\n        text_inputs = numpy_to_torch(text_inputs, torch.long, cuda=is_cuda)\n        text_inputs = text_inputs.unsqueeze(0)\n\n        # synthesize voice\n        outputs = self.inference(\n            text_inputs,\n            aux_input={\"d_vectors\": d_vector, \"speaker_ids\": _speaker_id},\n            pitch_transform=pitch_transform,\n            # energy_transform=energy_transform\n        )\n\n        # collect outputs\n        wav = outputs[\"model_outputs\"][0].data.cpu().numpy()\n        alignments = outputs[\"alignments\"]\n        return_dict = {\n            \"wav\": wav,\n            \"alignments\": alignments,\n            \"text_inputs\": text_inputs,\n            \"outputs\": outputs,\n        }\n        return return_dict\n\n    def synthesize_with_gl(self, text: str, speaker_id, d_vector):\n        is_cuda = next(self.parameters()).is_cuda\n\n        # convert text to sequence of token IDs\n        text_inputs = np.asarray(\n            self.tokenizer.text_to_ids(text, language=None),\n            dtype=np.int32,\n        )\n        # pass tensors to backend\n        if speaker_id is not None:\n            speaker_id = id_to_torch(speaker_id, cuda=is_cuda)\n\n        if d_vector is not None:\n            d_vector = embedding_to_torch(d_vector, cuda=is_cuda)\n\n        text_inputs = numpy_to_torch(text_inputs, torch.long, cuda=is_cuda)\n        text_inputs = text_inputs.unsqueeze(0)\n\n        # synthesize voice\n        outputs = self.inference_spec_decoder(\n            x=text_inputs,\n            aux_input={\"d_vectors\": d_vector, \"speaker_ids\": speaker_id},\n        )\n\n        # collect outputs\n        S = outputs[\"model_outputs\"].cpu().numpy()[0].T\n        S = db_to_amp_numpy(x=S, gain=1, base=None)\n        wav = mel_to_wav_numpy(mel=S, mel_basis=self.mel_basis, **self.config.audio)\n        alignments = outputs[\"alignments\"]\n        return_dict = {\n            \"wav\": wav[None, :],\n            \"alignments\": alignments,\n            \"text_inputs\": text_inputs,\n            \"outputs\": outputs,\n        }\n        return return_dict\n\n    @torch.no_grad()\n    def test_run(self, assets) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `tts` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        for idx, s_info in enumerate(test_sentences):\n            aux_inputs = self.get_aux_input_from_test_sentences(s_info)\n            outputs = self.synthesize(\n                aux_inputs[\"text\"],\n                config=self.config,\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n            )\n            outputs_gl = self.synthesize_with_gl(\n                aux_inputs[\"text\"],\n                speaker_id=aux_inputs[\"speaker_id\"],\n                d_vector=aux_inputs[\"d_vector\"],\n            )\n            # speaker_name = self.speaker_manager.speaker_names[aux_inputs[\"speaker_id\"]]\n            test_audios[\"{}-audio\".format(idx)] = outputs[\"wav\"].T\n            test_audios[\"{}-audio_encoder\".format(idx)] = outputs_gl[\"wav\"].T\n            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(outputs[\"alignments\"], output_fig=False)\n        return {\"figures\": test_figures, \"audios\": test_audios}\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[\"audios\"], self.config.audio.sample_rate)\n        logger.test_figures(steps, outputs[\"figures\"])\n\n    def format_batch(self, batch: Dict) -> Dict:\n        \"\"\"Compute speaker, langugage IDs and d_vector for the batch if necessary.\"\"\"\n        speaker_ids = None\n        d_vectors = None\n\n        # get numerical speaker ids from speaker names\n        if self.speaker_manager is not None and self.speaker_manager.speaker_names and self.args.use_speaker_embedding:\n            speaker_ids = [self.speaker_manager.name_to_id[sn] for sn in batch[\"speaker_names\"]]\n\n        if speaker_ids is not None:\n            speaker_ids = torch.LongTensor(speaker_ids)\n            batch[\"speaker_ids\"] = speaker_ids\n\n        # get d_vectors from audio file names\n        if self.speaker_manager is not None and self.speaker_manager.embeddings and self.args.use_d_vector_file:\n            d_vector_mapping = self.speaker_manager.embeddings\n            d_vectors = [d_vector_mapping[w][\"embedding\"] for w in batch[\"audio_unique_names\"]]\n            d_vectors = torch.FloatTensor(d_vectors)\n\n        batch[\"d_vectors\"] = d_vectors\n        batch[\"speaker_ids\"] = speaker_ids\n        return batch\n\n    def format_batch_on_device(self, batch):\n        \"\"\"Compute spectrograms on the device.\"\"\"\n\n        ac = self.ap\n\n        # compute spectrograms\n        batch[\"mel_input\"] = wav_to_mel(\n            batch[\"waveform\"],\n            hop_length=ac.hop_length,\n            win_length=ac.win_length,\n            n_fft=ac.fft_size,\n            num_mels=ac.num_mels,\n            sample_rate=ac.sample_rate,\n            fmin=ac.mel_fmin,\n            fmax=ac.mel_fmax,\n            center=False,\n        )\n\n        # TODO: Align pitch properly\n        # assert (\n        #     batch[\"pitch\"].shape[2] == batch[\"mel_input\"].shape[2]\n        # ), f\"{batch['pitch'].shape[2]}, {batch['mel_input'].shape[2]}\"\n        batch[\"pitch\"] = batch[\"pitch\"][:, :, : batch[\"mel_input\"].shape[2]] if batch[\"pitch\"] is not None else None\n        batch[\"mel_lengths\"] = (batch[\"mel_input\"].shape[2] * batch[\"waveform_rel_lens\"]).int()\n\n        # zero the padding frames\n        batch[\"mel_input\"] = batch[\"mel_input\"] * sequence_mask(batch[\"mel_lengths\"]).unsqueeze(1)\n\n        # format attn priors as we now the max mel length\n        # TODO: fix 1 diff b/w mel_lengths and attn_priors\n\n        if self.config.use_attn_priors:\n            attn_priors_np = batch[\"attn_priors\"]\n\n            batch[\"attn_priors\"] = torch.zeros(\n                batch[\"mel_input\"].shape[0],\n                batch[\"mel_lengths\"].max(),\n                batch[\"text_lengths\"].max(),\n                device=batch[\"mel_input\"].device,\n            )\n\n            for i in range(batch[\"mel_input\"].shape[0]):\n                batch[\"attn_priors\"][i, : attn_priors_np[i].shape[0], : attn_priors_np[i].shape[1]] = torch.from_numpy(\n                    attn_priors_np[i]\n                )\n\n        batch[\"energy\"] = None\n        batch[\"energy\"] = wav_to_energy(  # [B, 1, T_max2]\n            batch[\"waveform\"],\n            hop_length=ac.hop_length,\n            win_length=ac.win_length,\n            n_fft=ac.fft_size,\n            center=False,\n        )\n        batch[\"energy\"] = self.energy_scaler(batch[\"energy\"])\n        return batch\n\n    def get_sampler(self, config: Coqpit, dataset: TTSDataset, num_gpus=1):\n        weights = None\n        data_items = dataset.samples\n        if getattr(config, \"use_weighted_sampler\", False):\n            for attr_name, alpha in config.weighted_sampler_attrs.items():\n                print(f\" > Using weighted sampler for attribute '{attr_name}' with alpha '{alpha}'\")\n                multi_dict = config.weighted_sampler_multipliers.get(attr_name, None)\n                print(multi_dict)\n                weights, attr_names, attr_weights = get_attribute_balancer_weights(\n                    attr_name=attr_name, items=data_items, multi_dict=multi_dict\n                )\n                weights = weights * alpha\n                print(f\" > Attribute weights for '{attr_names}' \\n | > {attr_weights}\")\n\n        if weights is not None:\n            sampler = WeightedRandomSampler(weights, len(weights))\n        else:\n            sampler = None\n        # sampler for DDP\n        if sampler is None:\n            sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        else:  # If a sampler is already defined use this sampler and DDP sampler together\n            sampler = DistributedSamplerWrapper(sampler) if num_gpus > 1 else sampler\n        return sampler\n\n    def get_data_loader(\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: bool,\n        samples: Union[List[Dict], List[List]],\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,\n    ) -> \"DataLoader\":\n        if is_eval and not config.run_eval:\n            loader = None\n        else:\n            # init dataloader\n            dataset = ForwardTTSE2eDataset(\n                samples=samples,\n                ap=self.ap,\n                batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n                min_text_len=config.min_text_len,\n                max_text_len=config.max_text_len,\n                min_audio_len=config.min_audio_len,\n                max_audio_len=config.max_audio_len,\n                phoneme_cache_path=config.phoneme_cache_path,\n                precompute_num_workers=config.precompute_num_workers,\n                compute_f0=config.compute_f0,\n                f0_cache_path=config.f0_cache_path,\n                attn_prior_cache_path=config.attn_prior_cache_path if config.use_attn_priors else None,\n                verbose=verbose,\n                tokenizer=self.tokenizer,\n                start_by_longest=config.start_by_longest,\n            )\n\n            # wait all the DDP process to be ready\n            if num_gpus > 1:\n                dist.barrier()\n\n            # sort input sequences ascendingly by length\n            dataset.preprocess_samples()\n\n            # get samplers\n            sampler = self.get_sampler(config, dataset, num_gpus)\n\n            loader = DataLoader(\n                dataset,\n                batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                shuffle=False,  # shuffle is done in the dataset.\n                drop_last=False,  # setting this False might cause issues in AMP training.\n                sampler=sampler,\n                collate_fn=dataset.collate_fn,\n                num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                pin_memory=True,\n            )\n\n            # get pitch mean and std\n            self.pitch_mean = dataset.f0_dataset.mean\n            self.pitch_std = dataset.f0_dataset.std\n        return loader\n\n    def get_criterion(self):\n        return [VitsDiscriminatorLoss(self.config), DelightfulTTSLoss(self.config)]\n\n    def get_optimizer(self) -> List:\n        \"\"\"Initiate and return the GAN optimizers based on the config parameters.\n        It returnes 2 optimizers in a list. First one is for the generator and the second one is for the discriminator.\n        Returns:\n            List: optimizers.\n        \"\"\"\n        optimizer_disc = get_optimizer(\n            self.config.optimizer, self.config.optimizer_params, self.config.lr_disc, self.disc\n        )\n        gen_parameters = chain(params for k, params in self.named_parameters() if not k.startswith(\"disc.\"))\n        optimizer_gen = get_optimizer(\n            self.config.optimizer, self.config.optimizer_params, self.config.lr_gen, parameters=gen_parameters\n        )\n        return [optimizer_disc, optimizer_gen]\n\n    def get_lr(self) -> List:\n        \"\"\"Set the initial learning rates for each optimizer.\n\n        Returns:\n            List: learning rates for each optimizer.\n        \"\"\"\n        return [self.config.lr_disc, self.config.lr_gen]\n\n    def get_scheduler(self, optimizer) -> List:\n        \"\"\"Set the schedulers for each optimizer.\n\n        Args:\n            optimizer (List[`torch.optim.Optimizer`]): List of optimizers.\n\n        Returns:\n            List: Schedulers, one for each optimizer.\n        \"\"\"\n        scheduler_D = get_scheduler(self.config.lr_scheduler_gen, self.config.lr_scheduler_gen_params, optimizer[0])\n        scheduler_G = get_scheduler(self.config.lr_scheduler_disc, self.config.lr_scheduler_disc_params, optimizer[1])\n        return [scheduler_D, scheduler_G]\n\n    def on_epoch_end(self, trainer):  # pylint: disable=unused-argument\n        # stop updating mean and var\n        # TODO: do the same for F0\n        self.energy_scaler.eval()\n\n    @staticmethod\n    def init_from_config(\n        config: \"DelightfulTTSConfig\", samples: Union[List[List], List[Dict]] = None, verbose=False\n    ):  # pylint: disable=unused-argument\n        \"\"\"Initiate model from config\n\n        Args:\n            config (ForwardTTSE2eConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config.model_args, samples)\n        ap = AudioProcessor.init_from_config(config=config)\n        return DelightfulTTS(config=new_config, tokenizer=tokenizer, speaker_manager=speaker_manager, ap=ap)\n\n    def load_checkpoint(self, config, checkpoint_path, eval=False):\n        \"\"\"Load model from a checkpoint created by the \ud83d\udc5f\"\"\"\n        # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def get_state_dict(self):\n        \"\"\"Custom state dict of the model with all the necessary components for inference.\"\"\"\n        save_state = {\"config\": self.config.to_dict(), \"args\": self.args.to_dict(), \"model\": self.state_dict}\n\n        if hasattr(self, \"emb_g\"):\n            save_state[\"speaker_ids\"] = self.speaker_manager.speaker_names\n\n        if self.args.use_d_vector_file:\n            # TODO: implement saving of d_vectors\n            ...\n        return save_state\n\n    def save(self, config, checkpoint_path):\n        \"\"\"Save model to a file.\"\"\"\n        save_state = self.get_state_dict(config, checkpoint_path)  # pylint: disable=too-many-function-args\n        save_state[\"pitch_mean\"] = self.pitch_mean\n        save_state[\"pitch_std\"] = self.pitch_std\n        torch.save(save_state, checkpoint_path)\n\n    def on_train_step_start(self, trainer) -> None:\n        \"\"\"Enable the discriminator training based on `steps_to_start_discriminator`\n\n        Args:\n            trainer (Trainer): Trainer object.\n        \"\"\"\n        self.binary_loss_weight = min(trainer.epochs_done / self.config.binary_loss_warmup_epochs, 1.0) * 1.0\n        self.train_disc = (  # pylint: disable=attribute-defined-outside-init\n            trainer.total_steps_done >= self.config.steps_to_start_discriminator\n        )\n\n\nclass DelightfulTTSLoss(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.mse_loss = nn.MSELoss()\n        self.mae_loss = nn.L1Loss()\n        self.forward_sum_loss = ForwardSumLoss()\n        self.multi_scale_stft_loss = MultiScaleSTFTLoss(**config.multi_scale_stft_loss_params)\n\n        self.mel_loss_alpha = config.mel_loss_alpha\n        self.aligner_loss_alpha = config.aligner_loss_alpha\n        self.pitch_loss_alpha = config.pitch_loss_alpha\n        self.energy_loss_alpha = config.energy_loss_alpha\n        self.u_prosody_loss_alpha = config.u_prosody_loss_alpha\n        self.p_prosody_loss_alpha = config.p_prosody_loss_alpha\n        self.dur_loss_alpha = config.dur_loss_alpha\n        self.char_dur_loss_alpha = config.char_dur_loss_alpha\n        self.binary_alignment_loss_alpha = config.binary_align_loss_alpha\n\n        self.vocoder_mel_loss_alpha = config.vocoder_mel_loss_alpha\n        self.feat_loss_alpha = config.feat_loss_alpha\n        self.gen_loss_alpha = config.gen_loss_alpha\n        self.multi_scale_stft_loss_alpha = config.multi_scale_stft_loss_alpha\n\n    @staticmethod\n    def _binary_alignment_loss(alignment_hard, alignment_soft):\n        \"\"\"Binary loss that forces soft alignments to match the hard alignments as\n        explained in `https://arxiv.org/pdf/2108.10447.pdf`.\n        \"\"\"\n        log_sum = torch.log(torch.clamp(alignment_soft[alignment_hard == 1], min=1e-12)).sum()\n        return -log_sum / alignment_hard.sum()\n\n    @staticmethod\n    def feature_loss(feats_real, feats_generated):\n        loss = 0\n        for dr, dg in zip(feats_real, feats_generated):\n            for rl, gl in zip(dr, dg):\n                rl = rl.float().detach()\n                gl = gl.float()\n                loss += torch.mean(torch.abs(rl - gl))\n        return loss * 2\n\n    @staticmethod\n    def generator_loss(scores_fake):\n        loss = 0\n        gen_losses = []\n        for dg in scores_fake:\n            dg = dg.float()\n            l = torch.mean((1 - dg) ** 2)\n            gen_losses.append(l)\n            loss += l\n\n        return loss, gen_losses\n\n    def forward(\n        self,\n        mel_output,\n        mel_target,\n        mel_lens,\n        dur_output,\n        dur_target,\n        pitch_output,\n        pitch_target,\n        energy_output,\n        energy_target,\n        src_lens,\n        waveform,\n        waveform_hat,\n        p_prosody_ref,\n        p_prosody_pred,\n        u_prosody_ref,\n        u_prosody_pred,\n        aligner_logprob,\n        aligner_hard,\n        aligner_soft,\n        binary_loss_weight=None,\n        feats_fake=None,\n        feats_real=None,\n        scores_fake=None,\n        spec_slice=None,\n        spec_slice_hat=None,\n        skip_disc=False,\n    ):\n        \"\"\"\n        Shapes:\n            - mel_output: :math:`(B, C_mel, T_mel)`\n            - mel_target: :math:`(B, C_mel, T_mel)`\n            - mel_lens: :math:`(B)`\n            - dur_output: :math:`(B, T_src)`\n            - dur_target: :math:`(B, T_src)`\n            - pitch_output: :math:`(B, 1, T_src)`\n            - pitch_target: :math:`(B, 1, T_src)`\n            - energy_output: :math:`(B, 1, T_src)`\n            - energy_target: :math:`(B, 1, T_src)`\n            - src_lens: :math:`(B)`\n            - waveform: :math:`(B, 1, T_wav)`\n            - waveform_hat: :math:`(B, 1, T_wav)`\n            - p_prosody_ref: :math:`(B, T_src, 4)`\n            - p_prosody_pred: :math:`(B, T_src, 4)`\n            - u_prosody_ref: :math:`(B, 1, 256)\n            - u_prosody_pred: :math:`(B, 1, 256)\n            - aligner_logprob: :math:`(B, 1, T_mel, T_src)`\n            - aligner_hard: :math:`(B, T_mel, T_src)`\n            - aligner_soft: :math:`(B, T_mel, T_src)`\n            - spec_slice: :math:`(B, C_mel, T_mel)`\n            - spec_slice_hat: :math:`(B, C_mel, T_mel)`\n        \"\"\"\n        loss_dict = {}\n        src_mask = sequence_mask(src_lens).to(mel_output.device)  # (B, T_src)\n        mel_mask = sequence_mask(mel_lens).to(mel_output.device)  # (B, T_mel)\n\n        dur_target.requires_grad = False\n        mel_target.requires_grad = False\n        pitch_target.requires_grad = False\n\n        masked_mel_predictions = mel_output.masked_select(mel_mask[:, None])\n        mel_targets = mel_target.masked_select(mel_mask[:, None])\n        mel_loss = self.mae_loss(masked_mel_predictions, mel_targets)\n\n        p_prosody_ref = p_prosody_ref.detach()\n        p_prosody_loss = 0.5 * self.mae_loss(\n            p_prosody_ref.masked_select(src_mask.unsqueeze(-1)),\n            p_prosody_pred.masked_select(src_mask.unsqueeze(-1)),\n        )\n\n        u_prosody_ref = u_prosody_ref.detach()\n        u_prosody_loss = 0.5 * self.mae_loss(u_prosody_ref, u_prosody_pred)\n\n        duration_loss = self.mse_loss(dur_output, dur_target)\n\n        pitch_output = pitch_output.masked_select(src_mask[:, None])\n        pitch_target = pitch_target.masked_select(src_mask[:, None])\n        pitch_loss = self.mse_loss(pitch_output, pitch_target)\n\n        energy_output = energy_output.masked_select(src_mask[:, None])\n        energy_target = energy_target.masked_select(src_mask[:, None])\n        energy_loss = self.mse_loss(energy_output, energy_target)\n\n        forward_sum_loss = self.forward_sum_loss(aligner_logprob, src_lens, mel_lens)\n\n        total_loss = (\n            (mel_loss * self.mel_loss_alpha)\n            + (duration_loss * self.dur_loss_alpha)\n            + (u_prosody_loss * self.u_prosody_loss_alpha)\n            + (p_prosody_loss * self.p_prosody_loss_alpha)\n            + (pitch_loss * self.pitch_loss_alpha)\n            + (energy_loss * self.energy_loss_alpha)\n            + (forward_sum_loss * self.aligner_loss_alpha)\n        )\n\n        if self.binary_alignment_loss_alpha > 0 and aligner_hard is not None:\n            binary_alignment_loss = self._binary_alignment_loss(aligner_hard, aligner_soft)\n            total_loss = total_loss + self.binary_alignment_loss_alpha * binary_alignment_loss * binary_loss_weight\n            if binary_loss_weight:\n                loss_dict[\"loss_binary_alignment\"] = (\n                    self.binary_alignment_loss_alpha * binary_alignment_loss * binary_loss_weight\n                )\n            else:\n                loss_dict[\"loss_binary_alignment\"] = self.binary_alignment_loss_alpha * binary_alignment_loss\n\n        loss_dict[\"loss_aligner\"] = self.aligner_loss_alpha * forward_sum_loss\n        loss_dict[\"loss_mel\"] = self.mel_loss_alpha * mel_loss\n        loss_dict[\"loss_duration\"] = self.dur_loss_alpha * duration_loss\n        loss_dict[\"loss_u_prosody\"] = self.u_prosody_loss_alpha * u_prosody_loss\n        loss_dict[\"loss_p_prosody\"] = self.p_prosody_loss_alpha * p_prosody_loss\n        loss_dict[\"loss_pitch\"] = self.pitch_loss_alpha * pitch_loss\n        loss_dict[\"loss_energy\"] = self.energy_loss_alpha * energy_loss\n        loss_dict[\"loss\"] = total_loss\n\n        # vocoder losses\n        if not skip_disc:\n            loss_feat = self.feature_loss(feats_real=feats_real, feats_generated=feats_fake) * self.feat_loss_alpha\n            loss_gen = self.generator_loss(scores_fake=scores_fake)[0] * self.gen_loss_alpha\n            loss_dict[\"vocoder_loss_feat\"] = loss_feat\n            loss_dict[\"vocoder_loss_gen\"] = loss_gen\n            loss_dict[\"loss\"] = loss_dict[\"loss\"] + loss_feat + loss_gen\n\n        loss_mel = torch.nn.functional.l1_loss(spec_slice, spec_slice_hat) * self.vocoder_mel_loss_alpha\n        loss_stft_mg, loss_stft_sc = self.multi_scale_stft_loss(y_hat=waveform_hat, y=waveform)\n        loss_stft_mg = loss_stft_mg * self.multi_scale_stft_loss_alpha\n        loss_stft_sc = loss_stft_sc * self.multi_scale_stft_loss_alpha\n\n        loss_dict[\"vocoder_loss_mel\"] = loss_mel\n        loss_dict[\"vocoder_loss_stft_mg\"] = loss_stft_mg\n        loss_dict[\"vocoder_loss_stft_sc\"] = loss_stft_sc\n\n        loss_dict[\"loss\"] = loss_dict[\"loss\"] + loss_mel + loss_stft_sc + loss_stft_mg\n        return loss_dict\n", "TTS/tts/models/neuralhmm_tts.py": "import os\nfrom typing import Dict, List, Union\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom trainer.logging.tensorboard_logger import TensorboardLogger\n\nfrom TTS.tts.layers.overflow.common_layers import Encoder, OverflowUtils\nfrom TTS.tts.layers.overflow.neural_hmm import NeuralHMM\nfrom TTS.tts.layers.overflow.plotting_utils import (\n    get_spec_from_most_probable_state,\n    plot_transition_probabilities_to_numpy,\n)\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.generic_utils import format_aux_input\nfrom TTS.utils.io import load_fsspec\n\n\nclass NeuralhmmTTS(BaseTTS):\n    \"\"\"Neural HMM TTS model.\n\n    Paper::\n        https://arxiv.org/abs/2108.13320\n\n    Paper abstract::\n        Neural sequence-to-sequence TTS has achieved significantly better output quality\n    than statistical speech synthesis using HMMs.However, neural TTS is generally not probabilistic\n    and uses non-monotonic attention. Attention failures increase training time and can make\n    synthesis babble incoherently. This paper describes how the old and new paradigms can be\n    combined to obtain the advantages of both worlds, by replacing attention in neural TTS with\n    an autoregressive left-right no-skip hidden Markov model defined by a neural network.\n    Based on this proposal, we modify Tacotron 2 to obtain an HMM-based neural TTS model with\n    monotonic alignment, trained to maximise the full sequence likelihood without approximation.\n    We also describe how to combine ideas from classical and contemporary TTS for best results.\n    The resulting example system is smaller and simpler than Tacotron 2, and learns to speak with\n    fewer iterations and less data, whilst achieving comparable naturalness prior to the post-net.\n    Our approach also allows easy control over speaking rate. Audio examples and code\n    are available at https://shivammehta25.github.io/Neural-HMM/ .\n\n    Note:\n        - This is a parameter efficient version of OverFlow (15.3M vs 28.6M). Since it has half the\n        number of parameters as OverFlow the synthesis output quality is suboptimal (but comparable to Tacotron2\n        without Postnet), but it learns to speak with even lesser amount of data and is still significantly faster\n        than other attention-based methods.\n\n        - Neural HMMs uses flat start initialization i.e it computes the means and std and transition probabilities\n        of the dataset and uses them to initialize the model. This benefits the model and helps with faster learning\n        If you change the dataset or want to regenerate the parameters change the `force_generate_statistics` and\n        `mel_statistics_parameter_path` accordingly.\n\n        - To enable multi-GPU training, set the `use_grad_checkpointing=False` in config.\n        This will significantly increase the memory usage.  This is because to compute\n        the actual data likelihood (not an approximation using MAS/Viterbi) we must use\n        all the states at the previous time step during the forward pass to decide the\n        probability distribution at the current step i.e the difference between the forward\n        algorithm and viterbi approximation.\n\n    Check :class:`TTS.tts.configs.neuralhmm_tts_config.NeuralhmmTTSConfig` for class arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: \"NeuralhmmTTSConfig\",\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        # pass all config fields to `self`\n        # for fewer code change\n        self.config = config\n        for key in config:\n            setattr(self, key, config[key])\n\n        self.encoder = Encoder(config.num_chars, config.state_per_phone, config.encoder_in_out_features)\n        self.neural_hmm = NeuralHMM(\n            frame_channels=self.out_channels,\n            ar_order=self.ar_order,\n            deterministic_transition=self.deterministic_transition,\n            encoder_dim=self.encoder_in_out_features,\n            prenet_type=self.prenet_type,\n            prenet_dim=self.prenet_dim,\n            prenet_n_layers=self.prenet_n_layers,\n            prenet_dropout=self.prenet_dropout,\n            prenet_dropout_at_inference=self.prenet_dropout_at_inference,\n            memory_rnn_dim=self.memory_rnn_dim,\n            outputnet_size=self.outputnet_size,\n            flat_start_params=self.flat_start_params,\n            std_floor=self.std_floor,\n            use_grad_checkpointing=self.use_grad_checkpointing,\n        )\n\n        self.register_buffer(\"mean\", torch.tensor(0))\n        self.register_buffer(\"std\", torch.tensor(1))\n\n    def update_mean_std(self, statistics_dict: Dict):\n        self.mean.data = torch.tensor(statistics_dict[\"mean\"])\n        self.std.data = torch.tensor(statistics_dict[\"std\"])\n\n    def preprocess_batch(self, text, text_len, mels, mel_len):\n        if self.mean.item() == 0 or self.std.item() == 1:\n            statistics_dict = torch.load(self.mel_statistics_parameter_path)\n            self.update_mean_std(statistics_dict)\n\n        mels = self.normalize(mels)\n        return text, text_len, mels, mel_len\n\n    def normalize(self, x):\n        return x.sub(self.mean).div(self.std)\n\n    def inverse_normalize(self, x):\n        return x.mul(self.std).add(self.mean)\n\n    def forward(self, text, text_len, mels, mel_len):\n        \"\"\"\n        Forward pass for training and computing the log likelihood of a given batch.\n\n        Shapes:\n            Shapes:\n            text: :math:`[B, T_in]`\n            text_len: :math:`[B]`\n            mels: :math:`[B, T_out, C]`\n            mel_len: :math:`[B]`\n        \"\"\"\n        text, text_len, mels, mel_len = self.preprocess_batch(text, text_len, mels, mel_len)\n        encoder_outputs, encoder_output_len = self.encoder(text, text_len)\n\n        log_probs, fwd_alignments, transition_vectors, means = self.neural_hmm(\n            encoder_outputs, encoder_output_len, mels.transpose(1, 2), mel_len\n        )\n\n        outputs = {\n            \"log_probs\": log_probs,\n            \"alignments\": fwd_alignments,\n            \"transition_vectors\": transition_vectors,\n            \"means\": means,\n        }\n\n        return outputs\n\n    @staticmethod\n    def _training_stats(batch):\n        stats = {}\n        stats[\"avg_text_length\"] = batch[\"text_lengths\"].float().mean()\n        stats[\"avg_spec_length\"] = batch[\"mel_lengths\"].float().mean()\n        stats[\"avg_text_batch_occupancy\"] = (batch[\"text_lengths\"].float() / batch[\"text_lengths\"].float().max()).mean()\n        stats[\"avg_spec_batch_occupancy\"] = (batch[\"mel_lengths\"].float() / batch[\"mel_lengths\"].float().max()).mean()\n        return stats\n\n    def train_step(self, batch: dict, criterion: nn.Module):\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n\n        outputs = self.forward(\n            text=text_input,\n            text_len=text_lengths,\n            mels=mel_input,\n            mel_len=mel_lengths,\n        )\n        loss_dict = criterion(outputs[\"log_probs\"] / (mel_lengths.sum() + text_lengths.sum()))\n\n        # for printing useful statistics on terminal\n        loss_dict.update(self._training_stats(batch))\n        return outputs, loss_dict\n\n    def eval_step(self, batch: Dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def _format_aux_input(self, aux_input: Dict, default_input_dict):\n        \"\"\"Set missing fields to their default value.\n\n        Args:\n            aux_inputs (Dict): Dictionary containing the auxiliary inputs.\n        \"\"\"\n        default_input_dict = default_input_dict.copy()\n        default_input_dict.update(\n            {\n                \"sampling_temp\": self.sampling_temp,\n                \"max_sampling_time\": self.max_sampling_time,\n                \"duration_threshold\": self.duration_threshold,\n            }\n        )\n        if aux_input:\n            return format_aux_input(default_input_dict, aux_input)\n        return default_input_dict\n\n    @torch.no_grad()\n    def inference(\n        self,\n        text: torch.Tensor,\n        aux_input={\"x_lengths\": None, \"sampling_temp\": None, \"max_sampling_time\": None, \"duration_threshold\": None},\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"Sampling from the model\n\n        Args:\n            text (torch.Tensor): :math:`[B, T_in]`\n            aux_inputs (_type_, optional): _description_. Defaults to None.\n\n        Returns:\n            outputs: Dictionary containing the following\n                - mel (torch.Tensor): :math:`[B, T_out, C]`\n                - hmm_outputs_len (torch.Tensor): :math:`[B]`\n                - state_travelled (List[List[int]]): List of lists containing the state travelled for each sample in the batch.\n                - input_parameters (list[torch.FloatTensor]): Input parameters to the neural HMM.\n                - output_parameters (list[torch.FloatTensor]): Output parameters to the neural HMM.\n        \"\"\"\n        default_input_dict = {\n            \"x_lengths\": torch.sum(text != 0, dim=1),\n        }\n        aux_input = self._format_aux_input(aux_input, default_input_dict)\n        encoder_outputs, encoder_output_len = self.encoder.inference(text, aux_input[\"x_lengths\"])\n        outputs = self.neural_hmm.inference(\n            encoder_outputs,\n            encoder_output_len,\n            sampling_temp=aux_input[\"sampling_temp\"],\n            max_sampling_time=aux_input[\"max_sampling_time\"],\n            duration_threshold=aux_input[\"duration_threshold\"],\n        )\n        mels, mel_outputs_len = outputs[\"hmm_outputs\"], outputs[\"hmm_outputs_len\"]\n\n        mels = self.inverse_normalize(mels)\n        outputs.update({\"model_outputs\": mels, \"model_outputs_len\": mel_outputs_len})\n        outputs[\"alignments\"] = OverflowUtils.double_pad(outputs[\"alignments\"])\n        return outputs\n\n    @staticmethod\n    def get_criterion():\n        return NLLLoss()\n\n    @staticmethod\n    def init_from_config(config: \"NeuralhmmTTSConfig\", samples: Union[List[List], List[Dict]] = None, verbose=True):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (VitsConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n            verbose (bool): If True, print init messages. Defaults to True.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config, verbose)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return NeuralhmmTTS(new_config, ap, tokenizer, speaker_manager)\n\n    def load_checkpoint(\n        self, config: Coqpit, checkpoint_path: str, eval: bool = False, strict: bool = True, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def on_init_start(self, trainer):\n        \"\"\"If the current dataset does not have normalisation statistics and initialisation transition_probability it computes them otherwise loads.\"\"\"\n        if not os.path.isfile(trainer.config.mel_statistics_parameter_path) or trainer.config.force_generate_statistics:\n            dataloader = trainer.get_train_dataloader(\n                training_assets=None, samples=trainer.train_samples, verbose=False\n            )\n            print(\n                f\" | > Data parameters not found for: {trainer.config.mel_statistics_parameter_path}. Computing mel normalization parameters...\"\n            )\n            data_mean, data_std, init_transition_prob = OverflowUtils.get_data_parameters_for_flat_start(\n                dataloader, trainer.config.out_channels, trainer.config.state_per_phone\n            )\n            print(\n                f\" | > Saving data parameters to: {trainer.config.mel_statistics_parameter_path}: value: {data_mean, data_std, init_transition_prob}\"\n            )\n            statistics = {\n                \"mean\": data_mean.item(),\n                \"std\": data_std.item(),\n                \"init_transition_prob\": init_transition_prob.item(),\n            }\n            torch.save(statistics, trainer.config.mel_statistics_parameter_path)\n\n        else:\n            print(\n                f\" | > Data parameters found for: {trainer.config.mel_statistics_parameter_path}. Loading mel normalization parameters...\"\n            )\n            statistics = torch.load(trainer.config.mel_statistics_parameter_path)\n            data_mean, data_std, init_transition_prob = (\n                statistics[\"mean\"],\n                statistics[\"std\"],\n                statistics[\"init_transition_prob\"],\n            )\n            print(f\" | > Data parameters loaded with value: {data_mean, data_std, init_transition_prob}\")\n\n        trainer.config.flat_start_params[\"transition_p\"] = (\n            init_transition_prob.item() if torch.is_tensor(init_transition_prob) else init_transition_prob\n        )\n        OverflowUtils.update_flat_start_transition(trainer.model, init_transition_prob)\n        trainer.model.update_mean_std(statistics)\n\n    @torch.inference_mode()\n    def _create_logs(self, batch, outputs, ap):  # pylint: disable=no-self-use, unused-argument\n        alignments, transition_vectors = outputs[\"alignments\"], outputs[\"transition_vectors\"]\n        means = torch.stack(outputs[\"means\"], dim=1)\n\n        figures = {\n            \"alignment\": plot_alignment(alignments[0].exp(), title=\"Forward alignment\", fig_size=(20, 20)),\n            \"log_alignment\": plot_alignment(\n                alignments[0].exp(), title=\"Forward log alignment\", plot_log=True, fig_size=(20, 20)\n            ),\n            \"transition_vectors\": plot_alignment(transition_vectors[0], title=\"Transition vectors\", fig_size=(20, 20)),\n            \"mel_from_most_probable_state\": plot_spectrogram(\n                get_spec_from_most_probable_state(alignments[0], means[0]), fig_size=(12, 3)\n            ),\n            \"mel_target\": plot_spectrogram(batch[\"mel_input\"][0], fig_size=(12, 3)),\n        }\n\n        # sample one item from the batch -1 will give the smalles item\n        print(\" | > Synthesising audio from the model...\")\n        inference_output = self.inference(\n            batch[\"text_input\"][-1].unsqueeze(0), aux_input={\"x_lengths\": batch[\"text_lengths\"][-1].unsqueeze(0)}\n        )\n        figures[\"synthesised\"] = plot_spectrogram(inference_output[\"model_outputs\"][0], fig_size=(12, 3))\n\n        states = [p[1] for p in inference_output[\"input_parameters\"][0]]\n        transition_probability_synthesising = [p[2].cpu().numpy() for p in inference_output[\"output_parameters\"][0]]\n\n        for i in range((len(transition_probability_synthesising) // 200) + 1):\n            start = i * 200\n            end = (i + 1) * 200\n            figures[f\"synthesised_transition_probabilities/{i}\"] = plot_transition_probabilities_to_numpy(\n                states[start:end], transition_probability_synthesising[start:end]\n            )\n\n        audio = ap.inv_melspectrogram(inference_output[\"model_outputs\"][0].T.cpu().numpy())\n        return figures, {\"audios\": audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ):  # pylint: disable=unused-argument\n        \"\"\"Log training progress.\"\"\"\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_log(\n        self, batch: Dict, outputs: Dict, logger: \"Logger\", assets: Dict, steps: int\n    ):  # pylint: disable=unused-argument\n        \"\"\"Compute and log evaluation metrics.\"\"\"\n        # Plot model parameters histograms\n        if isinstance(logger, TensorboardLogger):\n            # I don't know if any other loggers supports this\n            for tag, value in self.named_parameters():\n                tag = tag.replace(\".\", \"/\")\n                logger.writer.add_histogram(tag, value.data.cpu().numpy(), steps)\n\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[1], self.ap.sample_rate)\n        logger.test_figures(steps, outputs[0])\n\n\nclass NLLLoss(nn.Module):\n    \"\"\"Negative log likelihood loss.\"\"\"\n\n    def forward(self, log_prob: torch.Tensor) -> dict:  # pylint: disable=no-self-use\n        \"\"\"Compute the loss.\n\n        Args:\n            logits (Tensor): [B, T, D]\n\n        Returns:\n            Tensor: [1]\n\n        \"\"\"\n        return_dict = {}\n        return_dict[\"loss\"] = -log_prob.mean()\n        return return_dict\n", "TTS/tts/models/__init__.py": "from typing import Dict, List, Union\n\nfrom TTS.utils.generic_utils import find_module\n\n\ndef setup_model(config: \"Coqpit\", samples: Union[List[List], List[Dict]] = None) -> \"BaseTTS\":\n    print(\" > Using model: {}\".format(config.model))\n    # fetch the right model implementation.\n    if \"base_model\" in config and config[\"base_model\"] is not None:\n        MyModel = find_module(\"TTS.tts.models\", config.base_model.lower())\n    else:\n        MyModel = find_module(\"TTS.tts.models\", config.model.lower())\n    model = MyModel.init_from_config(config=config, samples=samples)\n    return model\n", "TTS/tts/models/glow_tts.py": "import math\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom torch.nn import functional as F\n\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\nfrom TTS.tts.layers.glow_tts.decoder import Decoder\nfrom TTS.tts.layers.glow_tts.encoder import Encoder\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.helpers import generate_path, maximum_path, sequence_mask\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.synthesis import synthesis\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.io import load_fsspec\n\n\nclass GlowTTS(BaseTTS):\n    \"\"\"GlowTTS model.\n\n    Paper::\n        https://arxiv.org/abs/2005.11129\n\n    Paper abstract::\n        Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate\n        mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained\n        without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS,\n        a flow-based generative model for parallel TTS that does not require any external aligner. By combining the\n        properties of flows and dynamic programming, the proposed model searches for the most probable monotonic\n        alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard\n        monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows\n        enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over\n        the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our\n        model can be easily extended to a multi-speaker setting.\n\n    Check :class:`TTS.tts.configs.glow_tts_config.GlowTTSConfig` for class arguments.\n\n    Examples:\n        Init only model layers.\n\n        >>> from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n        >>> from TTS.tts.models.glow_tts import GlowTTS\n        >>> config = GlowTTSConfig(num_chars=2)\n        >>> model = GlowTTS(config)\n\n        Fully init a model ready for action. All the class attributes and class members\n        (e.g Tokenizer, AudioProcessor, etc.). are initialized internally based on config values.\n\n        >>> from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n        >>> from TTS.tts.models.glow_tts import GlowTTS\n        >>> config = GlowTTSConfig()\n        >>> model = GlowTTS.init_from_config(config, verbose=False)\n    \"\"\"\n\n    def __init__(\n        self,\n        config: GlowTTSConfig,\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        # pass all config fields to `self`\n        # for fewer code change\n        self.config = config\n        for key in config:\n            setattr(self, key, config[key])\n\n        self.decoder_output_dim = config.out_channels\n\n        # init multi-speaker layers if necessary\n        self.init_multispeaker(config)\n\n        self.run_data_dep_init = config.data_dep_init_steps > 0\n        self.encoder = Encoder(\n            self.num_chars,\n            out_channels=self.out_channels,\n            hidden_channels=self.hidden_channels_enc,\n            hidden_channels_dp=self.hidden_channels_dp,\n            encoder_type=self.encoder_type,\n            encoder_params=self.encoder_params,\n            mean_only=self.mean_only,\n            use_prenet=self.use_encoder_prenet,\n            dropout_p_dp=self.dropout_p_dp,\n            c_in_channels=self.c_in_channels,\n        )\n\n        self.decoder = Decoder(\n            self.out_channels,\n            self.hidden_channels_dec,\n            self.kernel_size_dec,\n            self.dilation_rate,\n            self.num_flow_blocks_dec,\n            self.num_block_layers,\n            dropout_p=self.dropout_p_dec,\n            num_splits=self.num_splits,\n            num_squeeze=self.num_squeeze,\n            sigmoid_scale=self.sigmoid_scale,\n            c_in_channels=self.c_in_channels,\n        )\n\n    def init_multispeaker(self, config: Coqpit):\n        \"\"\"Init speaker embedding layer if `use_speaker_embedding` is True and set the expected speaker embedding\n        vector dimension to the encoder layer channel size. If model uses d-vectors, then it only sets\n        speaker embedding vector dimension to the d-vector dimension from the config.\n\n        Args:\n            config (Coqpit): Model configuration.\n        \"\"\"\n        self.embedded_speaker_dim = 0\n        # set number of speakers - if num_speakers is set in config, use it, otherwise use speaker_manager\n        if self.speaker_manager is not None:\n            self.num_speakers = self.speaker_manager.num_speakers\n        # set ultimate speaker embedding size\n        if config.use_d_vector_file:\n            self.embedded_speaker_dim = (\n                config.d_vector_dim if \"d_vector_dim\" in config and config.d_vector_dim is not None else 512\n            )\n            if self.speaker_manager is not None:\n                assert (\n                    config.d_vector_dim == self.speaker_manager.embedding_dim\n                ), \" [!] d-vector dimension mismatch b/w config and speaker manager.\"\n        # init speaker embedding layer\n        if config.use_speaker_embedding and not config.use_d_vector_file:\n            print(\" > Init speaker_embedding layer.\")\n            self.embedded_speaker_dim = self.hidden_channels_enc\n            self.emb_g = nn.Embedding(self.num_speakers, self.hidden_channels_enc)\n            nn.init.uniform_(self.emb_g.weight, -0.1, 0.1)\n        # set conditioning dimensions\n        self.c_in_channels = self.embedded_speaker_dim\n\n    @staticmethod\n    def compute_outputs(attn, o_mean, o_log_scale, x_mask):\n        \"\"\"Compute and format the mode outputs with the given alignment map\"\"\"\n        y_mean = torch.matmul(attn.squeeze(1).transpose(1, 2), o_mean.transpose(1, 2)).transpose(\n            1, 2\n        )  # [b, t', t], [b, t, d] -> [b, d, t']\n        y_log_scale = torch.matmul(attn.squeeze(1).transpose(1, 2), o_log_scale.transpose(1, 2)).transpose(\n            1, 2\n        )  # [b, t', t], [b, t, d] -> [b, d, t']\n        # compute total duration with adjustment\n        o_attn_dur = torch.log(1 + torch.sum(attn, -1)) * x_mask\n        return y_mean, y_log_scale, o_attn_dur\n\n    def unlock_act_norm_layers(self):\n        \"\"\"Unlock activation normalization layers for data depended initalization.\"\"\"\n        for f in self.decoder.flows:\n            if getattr(f, \"set_ddi\", False):\n                f.set_ddi(True)\n\n    def lock_act_norm_layers(self):\n        \"\"\"Lock activation normalization layers.\"\"\"\n        for f in self.decoder.flows:\n            if getattr(f, \"set_ddi\", False):\n                f.set_ddi(False)\n\n    def _set_speaker_input(self, aux_input: Dict):\n        if aux_input is None:\n            d_vectors = None\n            speaker_ids = None\n        else:\n            d_vectors = aux_input.get(\"d_vectors\", None)\n            speaker_ids = aux_input.get(\"speaker_ids\", None)\n\n        if d_vectors is not None and speaker_ids is not None:\n            raise ValueError(\"[!] Cannot use d-vectors and speaker-ids together.\")\n\n        if speaker_ids is not None and not hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Cannot use speaker-ids without enabling speaker embedding.\")\n\n        g = speaker_ids if speaker_ids is not None else d_vectors\n        return g\n\n    def _speaker_embedding(self, aux_input: Dict) -> Union[torch.tensor, None]:\n        g = self._set_speaker_input(aux_input)\n        # speaker embedding\n        if g is not None:\n            if hasattr(self, \"emb_g\"):\n                # use speaker embedding layer\n                if not g.size():  # if is a scalar\n                    g = g.unsqueeze(0)  # unsqueeze\n                g = F.normalize(self.emb_g(g)).unsqueeze(-1)  # [b, h, 1]\n            else:\n                # use d-vector\n                g = F.normalize(g).unsqueeze(-1)  # [b, h, 1]\n        return g\n\n    def forward(\n        self, x, x_lengths, y, y_lengths=None, aux_input={\"d_vectors\": None, \"speaker_ids\": None}\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"\n        Args:\n            x (torch.Tensor):\n                Input text sequence ids. :math:`[B, T_en]`\n\n            x_lengths (torch.Tensor):\n                Lengths of input text sequences. :math:`[B]`\n\n            y (torch.Tensor):\n                Target mel-spectrogram frames. :math:`[B, T_de, C_mel]`\n\n            y_lengths (torch.Tensor):\n                Lengths of target mel-spectrogram frames. :math:`[B]`\n\n            aux_input (Dict):\n                Auxiliary inputs. `d_vectors` is speaker embedding vectors for a multi-speaker model.\n                :math:`[B, D_vec]`. `speaker_ids` is speaker ids for a multi-speaker model usind speaker-embedding\n                layer. :math:`B`\n\n        Returns:\n            Dict:\n                - z: :math: `[B, T_de, C]`\n                - logdet: :math:`B`\n                - y_mean: :math:`[B, T_de, C]`\n                - y_log_scale: :math:`[B, T_de, C]`\n                - alignments: :math:`[B, T_en, T_de]`\n                - durations_log: :math:`[B, T_en, 1]`\n                - total_durations_log: :math:`[B, T_en, 1]`\n        \"\"\"\n        # [B, T, C] -> [B, C, T]\n        y = y.transpose(1, 2)\n        y_max_length = y.size(2)\n        # norm speaker embeddings\n        g = self._speaker_embedding(aux_input)\n        # embedding pass\n        o_mean, o_log_scale, o_dur_log, x_mask = self.encoder(x, x_lengths, g=g)\n        # drop redisual frames wrt num_squeeze and set y_lengths.\n        y, y_lengths, y_max_length, attn = self.preprocess(y, y_lengths, y_max_length, None)\n        # create masks\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, y_max_length), 1).to(x_mask.dtype)\n        # [B, 1, T_en, T_de]\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        # decoder pass\n        z, logdet = self.decoder(y, y_mask, g=g, reverse=False)\n        # find the alignment path\n        with torch.no_grad():\n            o_scale = torch.exp(-2 * o_log_scale)\n            logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - o_log_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n            logp2 = torch.matmul(o_scale.transpose(1, 2), -0.5 * (z**2))  # [b, t, d] x [b, d, t'] = [b, t, t']\n            logp3 = torch.matmul((o_mean * o_scale).transpose(1, 2), z)  # [b, t, d] x [b, d, t'] = [b, t, t']\n            logp4 = torch.sum(-0.5 * (o_mean**2) * o_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n            logp = logp1 + logp2 + logp3 + logp4  # [b, t, t']\n            attn = maximum_path(logp, attn_mask.squeeze(1)).unsqueeze(1).detach()\n        y_mean, y_log_scale, o_attn_dur = self.compute_outputs(attn, o_mean, o_log_scale, x_mask)\n        attn = attn.squeeze(1).permute(0, 2, 1)\n        outputs = {\n            \"z\": z.transpose(1, 2),\n            \"logdet\": logdet,\n            \"y_mean\": y_mean.transpose(1, 2),\n            \"y_log_scale\": y_log_scale.transpose(1, 2),\n            \"alignments\": attn,\n            \"durations_log\": o_dur_log.transpose(1, 2),\n            \"total_durations_log\": o_attn_dur.transpose(1, 2),\n        }\n        return outputs\n\n    @torch.no_grad()\n    def inference_with_MAS(\n        self, x, x_lengths, y=None, y_lengths=None, aux_input={\"d_vectors\": None, \"speaker_ids\": None}\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"\n        It's similar to the teacher forcing in Tacotron.\n        It was proposed in: https://arxiv.org/abs/2104.05557\n\n        Shapes:\n            - x: :math:`[B, T]`\n            - x_lenghts: :math:`B`\n            - y: :math:`[B, T, C]`\n            - y_lengths: :math:`B`\n            - g: :math:`[B, C] or B`\n        \"\"\"\n        y = y.transpose(1, 2)\n        y_max_length = y.size(2)\n        # norm speaker embeddings\n        g = self._speaker_embedding(aux_input)\n        # embedding pass\n        o_mean, o_log_scale, o_dur_log, x_mask = self.encoder(x, x_lengths, g=g)\n        # drop redisual frames wrt num_squeeze and set y_lengths.\n        y, y_lengths, y_max_length, attn = self.preprocess(y, y_lengths, y_max_length, None)\n        # create masks\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, y_max_length), 1).to(x_mask.dtype)\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        # decoder pass\n        z, logdet = self.decoder(y, y_mask, g=g, reverse=False)\n        # find the alignment path between z and encoder output\n        o_scale = torch.exp(-2 * o_log_scale)\n        logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - o_log_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n        logp2 = torch.matmul(o_scale.transpose(1, 2), -0.5 * (z**2))  # [b, t, d] x [b, d, t'] = [b, t, t']\n        logp3 = torch.matmul((o_mean * o_scale).transpose(1, 2), z)  # [b, t, d] x [b, d, t'] = [b, t, t']\n        logp4 = torch.sum(-0.5 * (o_mean**2) * o_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n        logp = logp1 + logp2 + logp3 + logp4  # [b, t, t']\n        attn = maximum_path(logp, attn_mask.squeeze(1)).unsqueeze(1).detach()\n\n        y_mean, y_log_scale, o_attn_dur = self.compute_outputs(attn, o_mean, o_log_scale, x_mask)\n        attn = attn.squeeze(1).permute(0, 2, 1)\n\n        # get predited aligned distribution\n        z = y_mean * y_mask\n\n        # reverse the decoder and predict using the aligned distribution\n        y, logdet = self.decoder(z, y_mask, g=g, reverse=True)\n        outputs = {\n            \"model_outputs\": z.transpose(1, 2),\n            \"logdet\": logdet,\n            \"y_mean\": y_mean.transpose(1, 2),\n            \"y_log_scale\": y_log_scale.transpose(1, 2),\n            \"alignments\": attn,\n            \"durations_log\": o_dur_log.transpose(1, 2),\n            \"total_durations_log\": o_attn_dur.transpose(1, 2),\n        }\n        return outputs\n\n    @torch.no_grad()\n    def decoder_inference(\n        self, y, y_lengths=None, aux_input={\"d_vectors\": None, \"speaker_ids\": None}\n    ):  # pylint: disable=dangerous-default-value\n        \"\"\"\n        Shapes:\n            - y: :math:`[B, T, C]`\n            - y_lengths: :math:`B`\n            - g: :math:`[B, C] or B`\n        \"\"\"\n        y = y.transpose(1, 2)\n        y_max_length = y.size(2)\n        g = self._speaker_embedding(aux_input)\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, y_max_length), 1).to(y.dtype)\n        # decoder pass\n        z, logdet = self.decoder(y, y_mask, g=g, reverse=False)\n        # reverse decoder and predict\n        y, logdet = self.decoder(z, y_mask, g=g, reverse=True)\n        outputs = {}\n        outputs[\"model_outputs\"] = y.transpose(1, 2)\n        outputs[\"logdet\"] = logdet\n        return outputs\n\n    @torch.no_grad()\n    def inference(\n        self, x, aux_input={\"x_lengths\": None, \"d_vectors\": None, \"speaker_ids\": None}\n    ):  # pylint: disable=dangerous-default-value\n        x_lengths = aux_input[\"x_lengths\"]\n        g = self._speaker_embedding(aux_input)\n        # embedding pass\n        o_mean, o_log_scale, o_dur_log, x_mask = self.encoder(x, x_lengths, g=g)\n        # compute output durations\n        w = (torch.exp(o_dur_log) - 1) * x_mask * self.length_scale\n        w_ceil = torch.clamp_min(torch.ceil(w), 1)\n        y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n        y_max_length = None\n        # compute masks\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, y_max_length), 1).to(x_mask.dtype)\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        # compute attention mask\n        attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)\n        y_mean, y_log_scale, o_attn_dur = self.compute_outputs(attn, o_mean, o_log_scale, x_mask)\n\n        z = (y_mean + torch.exp(y_log_scale) * torch.randn_like(y_mean) * self.inference_noise_scale) * y_mask\n        # decoder pass\n        y, logdet = self.decoder(z, y_mask, g=g, reverse=True)\n        attn = attn.squeeze(1).permute(0, 2, 1)\n        outputs = {\n            \"model_outputs\": y.transpose(1, 2),\n            \"logdet\": logdet,\n            \"y_mean\": y_mean.transpose(1, 2),\n            \"y_log_scale\": y_log_scale.transpose(1, 2),\n            \"alignments\": attn,\n            \"durations_log\": o_dur_log.transpose(1, 2),\n            \"total_durations_log\": o_attn_dur.transpose(1, 2),\n        }\n        return outputs\n\n    def train_step(self, batch: dict, criterion: nn.Module):\n        \"\"\"A single training step. Forward pass and loss computation. Run data depended initialization for the\n        first `config.data_dep_init_steps` steps.\n\n        Args:\n            batch (dict): [description]\n            criterion (nn.Module): [description]\n        \"\"\"\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        d_vectors = batch[\"d_vectors\"]\n        speaker_ids = batch[\"speaker_ids\"]\n\n        if self.run_data_dep_init and self.training:\n            # compute data-dependent initialization of activation norm layers\n            self.unlock_act_norm_layers()\n            with torch.no_grad():\n                _ = self.forward(\n                    text_input,\n                    text_lengths,\n                    mel_input,\n                    mel_lengths,\n                    aux_input={\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids},\n                )\n            outputs = None\n            loss_dict = None\n            self.lock_act_norm_layers()\n        else:\n            # normal training step\n            outputs = self.forward(\n                text_input,\n                text_lengths,\n                mel_input,\n                mel_lengths,\n                aux_input={\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids},\n            )\n\n            with autocast(enabled=False):  # avoid mixed_precision in criterion\n                loss_dict = criterion(\n                    outputs[\"z\"].float(),\n                    outputs[\"y_mean\"].float(),\n                    outputs[\"y_log_scale\"].float(),\n                    outputs[\"logdet\"].float(),\n                    mel_lengths,\n                    outputs[\"durations_log\"].float(),\n                    outputs[\"total_durations_log\"].float(),\n                    text_lengths,\n                )\n        return outputs, loss_dict\n\n    def _create_logs(self, batch, outputs, ap):\n        alignments = outputs[\"alignments\"]\n        text_input = batch[\"text_input\"][:1] if batch[\"text_input\"] is not None else None\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        d_vectors = batch[\"d_vectors\"][:1] if batch[\"d_vectors\"] is not None else None\n        speaker_ids = batch[\"speaker_ids\"][:1] if batch[\"speaker_ids\"] is not None else None\n\n        # model runs reverse flow to predict spectrograms\n        pred_outputs = self.inference(\n            text_input,\n            aux_input={\"x_lengths\": text_lengths[:1], \"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids},\n        )\n        model_outputs = pred_outputs[\"model_outputs\"]\n\n        pred_spec = model_outputs[0].data.cpu().numpy()\n        gt_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"prediction\": plot_spectrogram(pred_spec, ap, output_fig=False),\n            \"ground_truth\": plot_spectrogram(gt_spec, ap, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        # Sample audio\n        train_audio = ap.inv_melspectrogram(pred_spec.T)\n        return figures, {\"audio\": train_audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ) -> None:  # pylint: disable=no-self-use\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    @torch.no_grad()\n    def eval_step(self, batch: dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    @torch.no_grad()\n    def test_run(self, assets: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Generic test run for `tts` models used by `Trainer`.\n\n        You can override this for a different behaviour.\n\n        Returns:\n            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n        \"\"\"\n        print(\" | > Synthesizing test sentences.\")\n        test_audios = {}\n        test_figures = {}\n        test_sentences = self.config.test_sentences\n        aux_inputs = self._get_test_aux_input()\n        if len(test_sentences) == 0:\n            print(\" | [!] No test sentences provided.\")\n        else:\n            for idx, sen in enumerate(test_sentences):\n                outputs = synthesis(\n                    self,\n                    sen,\n                    self.config,\n                    \"cuda\" in str(next(self.parameters()).device),\n                    speaker_id=aux_inputs[\"speaker_id\"],\n                    d_vector=aux_inputs[\"d_vector\"],\n                    style_wav=aux_inputs[\"style_wav\"],\n                    use_griffin_lim=True,\n                    do_trim_silence=False,\n                )\n\n                test_audios[\"{}-audio\".format(idx)] = outputs[\"wav\"]\n                test_figures[\"{}-prediction\".format(idx)] = plot_spectrogram(\n                    outputs[\"outputs\"][\"model_outputs\"], self.ap, output_fig=False\n                )\n                test_figures[\"{}-alignment\".format(idx)] = plot_alignment(outputs[\"alignments\"], output_fig=False)\n        return test_figures, test_audios\n\n    def preprocess(self, y, y_lengths, y_max_length, attn=None):\n        if y_max_length is not None:\n            y_max_length = (y_max_length // self.num_squeeze) * self.num_squeeze\n            y = y[:, :, :y_max_length]\n            if attn is not None:\n                attn = attn[:, :, :, :y_max_length]\n        y_lengths = torch.div(y_lengths, self.num_squeeze, rounding_mode=\"floor\") * self.num_squeeze\n        return y, y_lengths, y_max_length, attn\n\n    def store_inverse(self):\n        self.decoder.store_inverse()\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            self.store_inverse()\n            assert not self.training\n\n    @staticmethod\n    def get_criterion():\n        from TTS.tts.layers.losses import GlowTTSLoss  # pylint: disable=import-outside-toplevel\n\n        return GlowTTSLoss()\n\n    def on_train_step_start(self, trainer):\n        \"\"\"Decide on every training step wheter enable/disable data depended initialization.\"\"\"\n        self.run_data_dep_init = trainer.total_steps_done < self.data_dep_init_steps\n\n    @staticmethod\n    def init_from_config(config: \"GlowTTSConfig\", samples: Union[List[List], List[Dict]] = None, verbose=True):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (VitsConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n            verbose (bool): If True, print init messages. Defaults to True.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config, verbose)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return GlowTTS(new_config, ap, tokenizer, speaker_manager)\n", "TTS/tts/models/align_tts.py": "from dataclasses import dataclass, field\nfrom typing import Dict, List, Union\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\n\nfrom TTS.tts.layers.align_tts.mdn import MDNBlock\nfrom TTS.tts.layers.feed_forward.decoder import Decoder\nfrom TTS.tts.layers.feed_forward.duration_predictor import DurationPredictor\nfrom TTS.tts.layers.feed_forward.encoder import Encoder\nfrom TTS.tts.layers.generic.pos_encoding import PositionalEncoding\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.utils.helpers import generate_path, maximum_path, sequence_mask\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.io import load_fsspec\n\n\n@dataclass\nclass AlignTTSArgs(Coqpit):\n    \"\"\"\n    Args:\n        num_chars (int):\n            number of unique input to characters\n        out_channels (int):\n            number of output tensor channels. It is equal to the expected spectrogram size.\n        hidden_channels (int):\n            number of channels in all the model layers.\n        hidden_channels_ffn (int):\n            number of channels in transformer's conv layers.\n        hidden_channels_dp (int):\n            number of channels in duration predictor network.\n        num_heads (int):\n            number of attention heads in transformer networks.\n        num_transformer_layers (int):\n            number of layers in encoder and decoder transformer blocks.\n        dropout_p (int):\n            dropout rate in transformer layers.\n        length_scale (int, optional):\n            coefficient to set the speech speed. <1 slower, >1 faster. Defaults to 1.\n        num_speakers (int, optional):\n            number of speakers for multi-speaker training. Defaults to 0.\n        external_c (bool, optional):\n            enable external speaker embeddings. Defaults to False.\n        c_in_channels (int, optional):\n            number of channels in speaker embedding vectors. Defaults to 0.\n    \"\"\"\n\n    num_chars: int = None\n    out_channels: int = 80\n    hidden_channels: int = 256\n    hidden_channels_dp: int = 256\n    encoder_type: str = \"fftransformer\"\n    encoder_params: dict = field(\n        default_factory=lambda: {\"hidden_channels_ffn\": 1024, \"num_heads\": 2, \"num_layers\": 6, \"dropout_p\": 0.1}\n    )\n    decoder_type: str = \"fftransformer\"\n    decoder_params: dict = field(\n        default_factory=lambda: {\"hidden_channels_ffn\": 1024, \"num_heads\": 2, \"num_layers\": 6, \"dropout_p\": 0.1}\n    )\n    length_scale: float = 1.0\n    num_speakers: int = 0\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_dim: int = 0\n\n\nclass AlignTTS(BaseTTS):\n    \"\"\"AlignTTS with modified duration predictor.\n    https://arxiv.org/pdf/2003.01950.pdf\n\n    Encoder -> DurationPredictor -> Decoder\n\n    Check :class:`AlignTTSArgs` for the class arguments.\n\n    Paper Abstract:\n        Targeting at both high efficiency and performance, we propose AlignTTS to predict the\n        mel-spectrum in parallel. AlignTTS is based on a Feed-Forward Transformer which generates mel-spectrum from a\n        sequence of characters, and the duration of each character is determined by a duration predictor.Instead of\n        adopting the attention mechanism in Transformer TTS to align text to mel-spectrum, the alignment loss is presented\n        to consider all possible alignments in training by use of dynamic programming. Experiments on the LJSpeech dataset s\n        how that our model achieves not only state-of-the-art performance which outperforms Transformer TTS by 0.03 in mean\n        option score (MOS), but also a high efficiency which is more than 50 times faster than real-time.\n\n    Note:\n        Original model uses a separate character embedding layer for duration predictor. However, it causes the\n        duration predictor to overfit and prevents learning higher level interactions among characters. Therefore,\n        we predict durations based on encoder outputs which has higher level information about input characters. This\n        enables training without phases as in the original paper.\n\n        Original model uses Transormers in encoder and decoder layers. However, here you can set the architecture\n        differently based on your requirements using ```encoder_type``` and ```decoder_type``` parameters.\n\n    Examples:\n        >>> from TTS.tts.configs.align_tts_config import AlignTTSConfig\n        >>> config = AlignTTSConfig()\n        >>> model = AlignTTS(config)\n\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n\n    def __init__(\n        self,\n        config: \"AlignTTSConfig\",\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n        self.speaker_manager = speaker_manager\n        self.phase = -1\n        self.length_scale = (\n            float(config.model_args.length_scale)\n            if isinstance(config.model_args.length_scale, int)\n            else config.model_args.length_scale\n        )\n\n        self.emb = nn.Embedding(self.config.model_args.num_chars, self.config.model_args.hidden_channels)\n\n        self.embedded_speaker_dim = 0\n        self.init_multispeaker(config)\n\n        self.pos_encoder = PositionalEncoding(config.model_args.hidden_channels)\n        self.encoder = Encoder(\n            config.model_args.hidden_channels,\n            config.model_args.hidden_channels,\n            config.model_args.encoder_type,\n            config.model_args.encoder_params,\n            self.embedded_speaker_dim,\n        )\n        self.decoder = Decoder(\n            config.model_args.out_channels,\n            config.model_args.hidden_channels,\n            config.model_args.decoder_type,\n            config.model_args.decoder_params,\n        )\n        self.duration_predictor = DurationPredictor(config.model_args.hidden_channels_dp)\n\n        self.mod_layer = nn.Conv1d(config.model_args.hidden_channels, config.model_args.hidden_channels, 1)\n\n        self.mdn_block = MDNBlock(config.model_args.hidden_channels, 2 * config.model_args.out_channels)\n\n        if self.embedded_speaker_dim > 0 and self.embedded_speaker_dim != config.model_args.hidden_channels:\n            self.proj_g = nn.Conv1d(self.embedded_speaker_dim, config.model_args.hidden_channels, 1)\n\n    @staticmethod\n    def compute_log_probs(mu, log_sigma, y):\n        # pylint: disable=protected-access, c-extension-no-member\n        y = y.transpose(1, 2).unsqueeze(1)  # [B, 1, T1, D]\n        mu = mu.transpose(1, 2).unsqueeze(2)  # [B, T2, 1, D]\n        log_sigma = log_sigma.transpose(1, 2).unsqueeze(2)  # [B, T2, 1, D]\n        expanded_y, expanded_mu = torch.broadcast_tensors(y, mu)\n        exponential = -0.5 * torch.mean(\n            torch._C._nn.mse_loss(expanded_y, expanded_mu, 0) / torch.pow(log_sigma.exp(), 2), dim=-1\n        )  # B, L, T\n        logp = exponential - 0.5 * log_sigma.mean(dim=-1)\n        return logp\n\n    def compute_align_path(self, mu, log_sigma, y, x_mask, y_mask):\n        # find the max alignment path\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        log_p = self.compute_log_probs(mu, log_sigma, y)\n        # [B, T_en, T_dec]\n        attn = maximum_path(log_p, attn_mask.squeeze(1)).unsqueeze(1)\n        dr_mas = torch.sum(attn, -1)\n        return dr_mas.squeeze(1), log_p\n\n    @staticmethod\n    def generate_attn(dr, x_mask, y_mask=None):\n        # compute decode mask from the durations\n        if y_mask is None:\n            y_lengths = dr.sum(1).long()\n            y_lengths[y_lengths < 1] = 1\n            y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(dr.dtype)\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        attn = generate_path(dr, attn_mask.squeeze(1)).to(dr.dtype)\n        return attn\n\n    def expand_encoder_outputs(self, en, dr, x_mask, y_mask):\n        \"\"\"Generate attention alignment map from durations and\n        expand encoder outputs\n\n        Examples::\n            - encoder output: [a,b,c,d]\n            - durations: [1, 3, 2, 1]\n\n            - expanded: [a, b, b, b, c, c, d]\n            - attention map: [[0, 0, 0, 0, 0, 0, 1],\n                             [0, 0, 0, 0, 1, 1, 0],\n                             [0, 1, 1, 1, 0, 0, 0],\n                             [1, 0, 0, 0, 0, 0, 0]]\n        \"\"\"\n        attn = self.generate_attn(dr, x_mask, y_mask)\n        o_en_ex = torch.matmul(attn.squeeze(1).transpose(1, 2), en.transpose(1, 2)).transpose(1, 2)\n        return o_en_ex, attn\n\n    def format_durations(self, o_dr_log, x_mask):\n        o_dr = (torch.exp(o_dr_log) - 1) * x_mask * self.length_scale\n        o_dr[o_dr < 1] = 1.0\n        o_dr = torch.round(o_dr)\n        return o_dr\n\n    @staticmethod\n    def _concat_speaker_embedding(o_en, g):\n        g_exp = g.expand(-1, -1, o_en.size(-1))  # [B, C, T_en]\n        o_en = torch.cat([o_en, g_exp], 1)\n        return o_en\n\n    def _sum_speaker_embedding(self, x, g):\n        # project g to decoder dim.\n        if hasattr(self, \"proj_g\"):\n            g = self.proj_g(g)\n\n        return x + g\n\n    def _forward_encoder(self, x, x_lengths, g=None):\n        if hasattr(self, \"emb_g\"):\n            g = nn.functional.normalize(self.speaker_embedding(g))  # [B, C, 1]\n\n        if g is not None:\n            g = g.unsqueeze(-1)\n\n        # [B, T, C]\n        x_emb = self.emb(x)\n        # [B, C, T]\n        x_emb = torch.transpose(x_emb, 1, -1)\n\n        # compute sequence masks\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.shape[1]), 1).to(x.dtype)\n\n        # encoder pass\n        o_en = self.encoder(x_emb, x_mask)\n\n        # speaker conditioning for duration predictor\n        if g is not None:\n            o_en_dp = self._concat_speaker_embedding(o_en, g)\n        else:\n            o_en_dp = o_en\n        return o_en, o_en_dp, x_mask, g\n\n    def _forward_decoder(self, o_en, o_en_dp, dr, x_mask, y_lengths, g):\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(o_en_dp.dtype)\n        # expand o_en with durations\n        o_en_ex, attn = self.expand_encoder_outputs(o_en, dr, x_mask, y_mask)\n        # positional encoding\n        if hasattr(self, \"pos_encoder\"):\n            o_en_ex = self.pos_encoder(o_en_ex, y_mask)\n        # speaker embedding\n        if g is not None:\n            o_en_ex = self._sum_speaker_embedding(o_en_ex, g)\n        # decoder pass\n        o_de = self.decoder(o_en_ex, y_mask, g=g)\n        return o_de, attn.transpose(1, 2)\n\n    def _forward_mdn(self, o_en, y, y_lengths, x_mask):\n        # MAS potentials and alignment\n        mu, log_sigma = self.mdn_block(o_en)\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(o_en.dtype)\n        dr_mas, logp = self.compute_align_path(mu, log_sigma, y, x_mask, y_mask)\n        return dr_mas, mu, log_sigma, logp\n\n    def forward(\n        self, x, x_lengths, y, y_lengths, aux_input={\"d_vectors\": None}, phase=None\n    ):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, T_max]`\n            - x_lengths: :math:`[B]`\n            - y_lengths: :math:`[B]`\n            - dr: :math:`[B, T_max]`\n            - g: :math:`[B, C]`\n        \"\"\"\n        y = y.transpose(1, 2)\n        g = aux_input[\"d_vectors\"] if \"d_vectors\" in aux_input else None\n        o_de, o_dr_log, dr_mas_log, attn, mu, log_sigma, logp = None, None, None, None, None, None, None\n        if phase == 0:\n            # train encoder and MDN\n            o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n            dr_mas, mu, log_sigma, logp = self._forward_mdn(o_en, y, y_lengths, x_mask)\n            y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(o_en_dp.dtype)\n            attn = self.generate_attn(dr_mas, x_mask, y_mask)\n        elif phase == 1:\n            # train decoder\n            o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n            dr_mas, _, _, _ = self._forward_mdn(o_en, y, y_lengths, x_mask)\n            o_de, attn = self._forward_decoder(o_en.detach(), o_en_dp.detach(), dr_mas.detach(), x_mask, y_lengths, g=g)\n        elif phase == 2:\n            # train the whole except duration predictor\n            o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n            dr_mas, mu, log_sigma, logp = self._forward_mdn(o_en, y, y_lengths, x_mask)\n            o_de, attn = self._forward_decoder(o_en, o_en_dp, dr_mas, x_mask, y_lengths, g=g)\n        elif phase == 3:\n            # train duration predictor\n            o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n            o_dr_log = self.duration_predictor(x, x_mask)\n            dr_mas, mu, log_sigma, logp = self._forward_mdn(o_en, y, y_lengths, x_mask)\n            o_de, attn = self._forward_decoder(o_en, o_en_dp, dr_mas, x_mask, y_lengths, g=g)\n            o_dr_log = o_dr_log.squeeze(1)\n        else:\n            o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n            o_dr_log = self.duration_predictor(o_en_dp.detach(), x_mask)\n            dr_mas, mu, log_sigma, logp = self._forward_mdn(o_en, y, y_lengths, x_mask)\n            o_de, attn = self._forward_decoder(o_en, o_en_dp, dr_mas, x_mask, y_lengths, g=g)\n            o_dr_log = o_dr_log.squeeze(1)\n        dr_mas_log = torch.log(dr_mas + 1).squeeze(1)\n        outputs = {\n            \"model_outputs\": o_de.transpose(1, 2),\n            \"alignments\": attn,\n            \"durations_log\": o_dr_log,\n            \"durations_mas_log\": dr_mas_log,\n            \"mu\": mu,\n            \"log_sigma\": log_sigma,\n            \"logp\": logp,\n        }\n        return outputs\n\n    @torch.no_grad()\n    def inference(self, x, aux_input={\"d_vectors\": None}):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, T_max]`\n            - x_lengths: :math:`[B]`\n            - g: :math:`[B, C]`\n        \"\"\"\n        g = aux_input[\"d_vectors\"] if \"d_vectors\" in aux_input else None\n        x_lengths = torch.tensor(x.shape[1:2]).to(x.device)\n        # pad input to prevent dropping the last word\n        # x = torch.nn.functional.pad(x, pad=(0, 5), mode='constant', value=0)\n        o_en, o_en_dp, x_mask, g = self._forward_encoder(x, x_lengths, g)\n        # o_dr_log = self.duration_predictor(x, x_mask)\n        o_dr_log = self.duration_predictor(o_en_dp, x_mask)\n        # duration predictor pass\n        o_dr = self.format_durations(o_dr_log, x_mask).squeeze(1)\n        y_lengths = o_dr.sum(1)\n        o_de, attn = self._forward_decoder(o_en, o_en_dp, o_dr, x_mask, y_lengths, g=g)\n        outputs = {\"model_outputs\": o_de.transpose(1, 2), \"alignments\": attn}\n        return outputs\n\n    def train_step(self, batch: dict, criterion: nn.Module):\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        d_vectors = batch[\"d_vectors\"]\n        speaker_ids = batch[\"speaker_ids\"]\n\n        aux_input = {\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids}\n        outputs = self.forward(text_input, text_lengths, mel_input, mel_lengths, aux_input, self.phase)\n        loss_dict = criterion(\n            outputs[\"logp\"],\n            outputs[\"model_outputs\"],\n            mel_input,\n            mel_lengths,\n            outputs[\"durations_log\"],\n            outputs[\"durations_mas_log\"],\n            text_lengths,\n            phase=self.phase,\n        )\n\n        return outputs, loss_dict\n\n    def _create_logs(self, batch, outputs, ap):  # pylint: disable=no-self-use\n        model_outputs = outputs[\"model_outputs\"]\n        alignments = outputs[\"alignments\"]\n        mel_input = batch[\"mel_input\"]\n\n        pred_spec = model_outputs[0].data.cpu().numpy()\n        gt_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"prediction\": plot_spectrogram(pred_spec, ap, output_fig=False),\n            \"ground_truth\": plot_spectrogram(gt_spec, ap, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        # Sample audio\n        train_audio = ap.inv_melspectrogram(pred_spec.T)\n        return figures, {\"audio\": train_audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ) -> None:  # pylint: disable=no-self-use\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_step(self, batch: dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n\n    def get_criterion(self):\n        from TTS.tts.layers.losses import AlignTTSLoss  # pylint: disable=import-outside-toplevel\n\n        return AlignTTSLoss(self.config)\n\n    @staticmethod\n    def _set_phase(config, global_step):\n        \"\"\"Decide AlignTTS training phase\"\"\"\n        if isinstance(config.phase_start_steps, list):\n            vals = [i < global_step for i in config.phase_start_steps]\n            if not True in vals:\n                phase = 0\n            else:\n                phase = (\n                    len(config.phase_start_steps)\n                    - [i < global_step for i in config.phase_start_steps][::-1].index(True)\n                    - 1\n                )\n        else:\n            phase = None\n        return phase\n\n    def on_epoch_start(self, trainer):\n        \"\"\"Set AlignTTS training phase on epoch start.\"\"\"\n        self.phase = self._set_phase(trainer.config, trainer.total_steps_done)\n\n    @staticmethod\n    def init_from_config(config: \"AlignTTSConfig\", samples: Union[List[List], List[Dict]] = None):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (AlignTTSConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(config, samples)\n        return AlignTTS(new_config, ap, tokenizer, speaker_manager)\n", "TTS/tts/models/tacotron2.py": "# coding: utf-8\n\nfrom typing import Dict, List, Union\n\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.tts.layers.tacotron.capacitron_layers import CapacitronVAE\nfrom TTS.tts.layers.tacotron.gst_layers import GST\nfrom TTS.tts.layers.tacotron.tacotron2 import Decoder, Encoder, Postnet\nfrom TTS.tts.models.base_tacotron import BaseTacotron\nfrom TTS.tts.utils.measures import alignment_diagonal_score\nfrom TTS.tts.utils.speakers import SpeakerManager\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.tts.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.utils.capacitron_optimizer import CapacitronOptimizer\n\n\nclass Tacotron2(BaseTacotron):\n    \"\"\"Tacotron2 model implementation inherited from :class:`TTS.tts.models.base_tacotron.BaseTacotron`.\n\n    Paper::\n        https://arxiv.org/abs/1712.05884\n\n    Paper abstract::\n        This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.\n        The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character\n        embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize\n        timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable\n        to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation\n        studies of key components of our system and evaluate the impact of using mel spectrograms as the input to\n        WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic\n        intermediate representation enables significant simplification of the WaveNet architecture.\n\n    Check :class:`TTS.tts.configs.tacotron2_config.Tacotron2Config` for model arguments.\n\n    Args:\n        config (TacotronConfig):\n            Configuration for the Tacotron2 model.\n        speaker_manager (SpeakerManager):\n            Speaker manager for multi-speaker training. Uuse only for multi-speaker training. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: \"Tacotron2Config\",\n        ap: \"AudioProcessor\" = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)\n\n        self.decoder_output_dim = config.out_channels\n\n        # pass all config fields to `self`\n        # for fewer code change\n        for key in config:\n            setattr(self, key, config[key])\n\n        # init multi-speaker layers\n        if self.use_speaker_embedding or self.use_d_vector_file:\n            self.init_multispeaker(config)\n            self.decoder_in_features += self.embedded_speaker_dim  # add speaker embedding dim\n\n        if self.use_gst:\n            self.decoder_in_features += self.gst.gst_embedding_dim\n\n        if self.use_capacitron_vae:\n            self.decoder_in_features += self.capacitron_vae.capacitron_VAE_embedding_dim\n\n        # embedding layer\n        self.embedding = nn.Embedding(self.num_chars, 512, padding_idx=0)\n\n        # base model layers\n        self.encoder = Encoder(self.encoder_in_features)\n\n        self.decoder = Decoder(\n            self.decoder_in_features,\n            self.decoder_output_dim,\n            self.r,\n            self.attention_type,\n            self.attention_win,\n            self.attention_norm,\n            self.prenet_type,\n            self.prenet_dropout,\n            self.use_forward_attn,\n            self.transition_agent,\n            self.forward_attn_mask,\n            self.location_attn,\n            self.attention_heads,\n            self.separate_stopnet,\n            self.max_decoder_steps,\n        )\n        self.postnet = Postnet(self.out_channels)\n\n        # setup prenet dropout\n        self.decoder.prenet.dropout_at_inference = self.prenet_dropout_at_inference\n\n        # global style token layers\n        if self.gst and self.use_gst:\n            self.gst_layer = GST(\n                num_mel=self.decoder_output_dim,\n                num_heads=self.gst.gst_num_heads,\n                num_style_tokens=self.gst.gst_num_style_tokens,\n                gst_embedding_dim=self.gst.gst_embedding_dim,\n            )\n\n        # Capacitron VAE Layers\n        if self.capacitron_vae and self.use_capacitron_vae:\n            self.capacitron_vae_layer = CapacitronVAE(\n                num_mel=self.decoder_output_dim,\n                encoder_output_dim=self.encoder_in_features,\n                capacitron_VAE_embedding_dim=self.capacitron_vae.capacitron_VAE_embedding_dim,\n                speaker_embedding_dim=self.embedded_speaker_dim\n                if self.capacitron_vae.capacitron_use_speaker_embedding\n                else None,\n                text_summary_embedding_dim=self.capacitron_vae.capacitron_text_summary_embedding_dim\n                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n                else None,\n            )\n\n        # backward pass decoder\n        if self.bidirectional_decoder:\n            self._init_backward_decoder()\n        # setup DDC\n        if self.double_decoder_consistency:\n            self.coarse_decoder = Decoder(\n                self.decoder_in_features,\n                self.decoder_output_dim,\n                self.ddc_r,\n                self.attention_type,\n                self.attention_win,\n                self.attention_norm,\n                self.prenet_type,\n                self.prenet_dropout,\n                self.use_forward_attn,\n                self.transition_agent,\n                self.forward_attn_mask,\n                self.location_attn,\n                self.attention_heads,\n                self.separate_stopnet,\n                self.max_decoder_steps,\n            )\n\n    @staticmethod\n    def shape_outputs(mel_outputs, mel_outputs_postnet, alignments):\n        \"\"\"Final reshape of the model output tensors.\"\"\"\n        mel_outputs = mel_outputs.transpose(1, 2)\n        mel_outputs_postnet = mel_outputs_postnet.transpose(1, 2)\n        return mel_outputs, mel_outputs_postnet, alignments\n\n    def forward(  # pylint: disable=dangerous-default-value\n        self, text, text_lengths, mel_specs=None, mel_lengths=None, aux_input={\"speaker_ids\": None, \"d_vectors\": None}\n    ):\n        \"\"\"Forward pass for training with Teacher Forcing.\n\n        Shapes:\n            text: :math:`[B, T_in]`\n            text_lengths: :math:`[B]`\n            mel_specs: :math:`[B, T_out, C]`\n            mel_lengths: :math:`[B]`\n            aux_input: 'speaker_ids': :math:`[B, 1]` and  'd_vectors': :math:`[B, C]`\n        \"\"\"\n        aux_input = self._format_aux_input(aux_input)\n        outputs = {\"alignments_backward\": None, \"decoder_outputs_backward\": None}\n        # compute mask for padding\n        # B x T_in_max (boolean)\n        input_mask, output_mask = self.compute_masks(text_lengths, mel_lengths)\n        # B x D_embed x T_in_max\n        embedded_inputs = self.embedding(text).transpose(1, 2)\n        # B x T_in_max x D_en\n        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n        if self.gst and self.use_gst:\n            # B x gst_dim\n            encoder_outputs = self.compute_gst(encoder_outputs, mel_specs)\n\n        if self.use_speaker_embedding or self.use_d_vector_file:\n            if not self.use_d_vector_file:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])[:, None]\n            else:\n                # B x 1 x speaker_embed_dim\n                embedded_speakers = torch.unsqueeze(aux_input[\"d_vectors\"], 1)\n            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n\n        # capacitron\n        if self.capacitron_vae and self.use_capacitron_vae:\n            # B x capacitron_VAE_embedding_dim\n            encoder_outputs, *capacitron_vae_outputs = self.compute_capacitron_VAE_embedding(\n                encoder_outputs,\n                reference_mel_info=[mel_specs, mel_lengths],\n                text_info=[embedded_inputs.transpose(1, 2), text_lengths]\n                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n                else None,\n                speaker_embedding=embedded_speakers if self.capacitron_vae.capacitron_use_speaker_embedding else None,\n            )\n        else:\n            capacitron_vae_outputs = None\n\n        encoder_outputs = encoder_outputs * input_mask.unsqueeze(2).expand_as(encoder_outputs)\n\n        # B x mel_dim x T_out -- B x T_out//r x T_in -- B x T_out//r\n        decoder_outputs, alignments, stop_tokens = self.decoder(encoder_outputs, mel_specs, input_mask)\n        # sequence masking\n        if mel_lengths is not None:\n            decoder_outputs = decoder_outputs * output_mask.unsqueeze(1).expand_as(decoder_outputs)\n        # B x mel_dim x T_out\n        postnet_outputs = self.postnet(decoder_outputs)\n        postnet_outputs = decoder_outputs + postnet_outputs\n        # sequence masking\n        if output_mask is not None:\n            postnet_outputs = postnet_outputs * output_mask.unsqueeze(1).expand_as(postnet_outputs)\n        # B x T_out x mel_dim -- B x T_out x mel_dim -- B x T_out//r x T_in\n        decoder_outputs, postnet_outputs, alignments = self.shape_outputs(decoder_outputs, postnet_outputs, alignments)\n        if self.bidirectional_decoder:\n            decoder_outputs_backward, alignments_backward = self._backward_pass(mel_specs, encoder_outputs, input_mask)\n            outputs[\"alignments_backward\"] = alignments_backward\n            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n        if self.double_decoder_consistency:\n            decoder_outputs_backward, alignments_backward = self._coarse_decoder_pass(\n                mel_specs, encoder_outputs, alignments, input_mask\n            )\n            outputs[\"alignments_backward\"] = alignments_backward\n            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n        outputs.update(\n            {\n                \"model_outputs\": postnet_outputs,\n                \"decoder_outputs\": decoder_outputs,\n                \"alignments\": alignments,\n                \"stop_tokens\": stop_tokens,\n                \"capacitron_vae_outputs\": capacitron_vae_outputs,\n            }\n        )\n        return outputs\n\n    @torch.no_grad()\n    def inference(self, text, aux_input=None):\n        \"\"\"Forward pass for inference with no Teacher-Forcing.\n\n        Shapes:\n           text: :math:`[B, T_in]`\n           text_lengths: :math:`[B]`\n        \"\"\"\n        aux_input = self._format_aux_input(aux_input)\n        embedded_inputs = self.embedding(text).transpose(1, 2)\n        encoder_outputs = self.encoder.inference(embedded_inputs)\n\n        if self.gst and self.use_gst:\n            # B x gst_dim\n            encoder_outputs = self.compute_gst(encoder_outputs, aux_input[\"style_mel\"], aux_input[\"d_vectors\"])\n\n        if self.capacitron_vae and self.use_capacitron_vae:\n            if aux_input[\"style_text\"] is not None:\n                style_text_embedding = self.embedding(aux_input[\"style_text\"])\n                style_text_length = torch.tensor([style_text_embedding.size(1)], dtype=torch.int64).to(\n                    encoder_outputs.device\n                )  # pylint: disable=not-callable\n            reference_mel_length = (\n                torch.tensor([aux_input[\"style_mel\"].size(1)], dtype=torch.int64).to(encoder_outputs.device)\n                if aux_input[\"style_mel\"] is not None\n                else None\n            )  # pylint: disable=not-callable\n            # B x capacitron_VAE_embedding_dim\n            encoder_outputs, *_ = self.compute_capacitron_VAE_embedding(\n                encoder_outputs,\n                reference_mel_info=[aux_input[\"style_mel\"], reference_mel_length]\n                if aux_input[\"style_mel\"] is not None\n                else None,\n                text_info=[style_text_embedding, style_text_length] if aux_input[\"style_text\"] is not None else None,\n                speaker_embedding=aux_input[\"d_vectors\"]\n                if self.capacitron_vae.capacitron_use_speaker_embedding\n                else None,\n            )\n\n        if self.num_speakers > 1:\n            if not self.use_d_vector_file:\n                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])[None]\n                # reshape embedded_speakers\n                if embedded_speakers.ndim == 1:\n                    embedded_speakers = embedded_speakers[None, None, :]\n                elif embedded_speakers.ndim == 2:\n                    embedded_speakers = embedded_speakers[None, :]\n            else:\n                embedded_speakers = aux_input[\"d_vectors\"]\n\n            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n\n        decoder_outputs, alignments, stop_tokens = self.decoder.inference(encoder_outputs)\n        postnet_outputs = self.postnet(decoder_outputs)\n        postnet_outputs = decoder_outputs + postnet_outputs\n        decoder_outputs, postnet_outputs, alignments = self.shape_outputs(decoder_outputs, postnet_outputs, alignments)\n        outputs = {\n            \"model_outputs\": postnet_outputs,\n            \"decoder_outputs\": decoder_outputs,\n            \"alignments\": alignments,\n            \"stop_tokens\": stop_tokens,\n        }\n        return outputs\n\n    def before_backward_pass(self, loss_dict, optimizer) -> None:\n        # Extracting custom training specific operations for capacitron\n        # from the trainer\n        if self.use_capacitron_vae:\n            loss_dict[\"capacitron_vae_beta_loss\"].backward()\n            optimizer.first_step()\n\n    def train_step(self, batch: Dict, criterion: torch.nn.Module):\n        \"\"\"A single training step. Forward pass and loss computation.\n\n        Args:\n            batch ([Dict]): A dictionary of input tensors.\n            criterion ([type]): Callable criterion to compute model loss.\n        \"\"\"\n        text_input = batch[\"text_input\"]\n        text_lengths = batch[\"text_lengths\"]\n        mel_input = batch[\"mel_input\"]\n        mel_lengths = batch[\"mel_lengths\"]\n        stop_targets = batch[\"stop_targets\"]\n        stop_target_lengths = batch[\"stop_target_lengths\"]\n        speaker_ids = batch[\"speaker_ids\"]\n        d_vectors = batch[\"d_vectors\"]\n\n        aux_input = {\"speaker_ids\": speaker_ids, \"d_vectors\": d_vectors}\n        outputs = self.forward(text_input, text_lengths, mel_input, mel_lengths, aux_input)\n\n        # set the [alignment] lengths wrt reduction factor for guided attention\n        if mel_lengths.max() % self.decoder.r != 0:\n            alignment_lengths = (\n                mel_lengths + (self.decoder.r - (mel_lengths.max() % self.decoder.r))\n            ) // self.decoder.r\n        else:\n            alignment_lengths = mel_lengths // self.decoder.r\n\n        # compute loss\n        with autocast(enabled=False):  # use float32 for the criterion\n            loss_dict = criterion(\n                outputs[\"model_outputs\"].float(),\n                outputs[\"decoder_outputs\"].float(),\n                mel_input.float(),\n                None,\n                outputs[\"stop_tokens\"].float(),\n                stop_targets.float(),\n                stop_target_lengths,\n                outputs[\"capacitron_vae_outputs\"] if self.capacitron_vae else None,\n                mel_lengths,\n                None if outputs[\"decoder_outputs_backward\"] is None else outputs[\"decoder_outputs_backward\"].float(),\n                outputs[\"alignments\"].float(),\n                alignment_lengths,\n                None if outputs[\"alignments_backward\"] is None else outputs[\"alignments_backward\"].float(),\n                text_lengths,\n            )\n\n        # compute alignment error (the lower the better )\n        align_error = 1 - alignment_diagonal_score(outputs[\"alignments\"])\n        loss_dict[\"align_error\"] = align_error\n        return outputs, loss_dict\n\n    def get_optimizer(self) -> List:\n        if self.use_capacitron_vae:\n            return CapacitronOptimizer(self.config, self.named_parameters())\n        return get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr, self)\n\n    def get_scheduler(self, optimizer: object):\n        opt = optimizer.primary_optimizer if self.use_capacitron_vae else optimizer\n        return get_scheduler(self.config.lr_scheduler, self.config.lr_scheduler_params, opt)\n\n    def before_gradient_clipping(self):\n        if self.use_capacitron_vae:\n            # Capacitron model specific gradient clipping\n            model_params_to_clip = []\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    if name != \"capacitron_vae_layer.beta\":\n                        model_params_to_clip.append(param)\n            torch.nn.utils.clip_grad_norm_(model_params_to_clip, self.capacitron_vae.capacitron_grad_clip)\n\n    def _create_logs(self, batch, outputs, ap):\n        \"\"\"Create dashboard log information.\"\"\"\n        postnet_outputs = outputs[\"model_outputs\"]\n        alignments = outputs[\"alignments\"]\n        alignments_backward = outputs[\"alignments_backward\"]\n        mel_input = batch[\"mel_input\"]\n\n        pred_spec = postnet_outputs[0].data.cpu().numpy()\n        gt_spec = mel_input[0].data.cpu().numpy()\n        align_img = alignments[0].data.cpu().numpy()\n\n        figures = {\n            \"prediction\": plot_spectrogram(pred_spec, ap, output_fig=False),\n            \"ground_truth\": plot_spectrogram(gt_spec, ap, output_fig=False),\n            \"alignment\": plot_alignment(align_img, output_fig=False),\n        }\n\n        if self.bidirectional_decoder or self.double_decoder_consistency:\n            figures[\"alignment_backward\"] = plot_alignment(alignments_backward[0].data.cpu().numpy(), output_fig=False)\n\n        # Sample audio\n        audio = ap.inv_melspectrogram(pred_spec.T)\n        return figures, {\"audio\": audio}\n\n    def train_log(\n        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n    ) -> None:  # pylint: disable=no-self-use\n        \"\"\"Log training progress.\"\"\"\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.train_figures(steps, figures)\n        logger.train_audios(steps, audios, self.ap.sample_rate)\n\n    def eval_step(self, batch: dict, criterion: nn.Module):\n        return self.train_step(batch, criterion)\n\n    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n        figures, audios = self._create_logs(batch, outputs, self.ap)\n        logger.eval_figures(steps, figures)\n        logger.eval_audios(steps, audios, self.ap.sample_rate)\n\n    @staticmethod\n    def init_from_config(config: \"Tacotron2Config\", samples: Union[List[List], List[Dict]] = None):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (Tacotron2Config): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        from TTS.utils.audio import AudioProcessor\n\n        ap = AudioProcessor.init_from_config(config)\n        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n        speaker_manager = SpeakerManager.init_from_config(new_config, samples)\n        return Tacotron2(new_config, ap, tokenizer, speaker_manager)\n", "TTS/tts/utils/managers.py": "import json\nimport random\nfrom typing import Any, Dict, List, Tuple, Union\n\nimport fsspec\nimport numpy as np\nimport torch\n\nfrom TTS.config import load_config\nfrom TTS.encoder.utils.generic_utils import setup_encoder_model\nfrom TTS.utils.audio import AudioProcessor\n\n\ndef load_file(path: str):\n    if path.endswith(\".json\"):\n        with fsspec.open(path, \"r\") as f:\n            return json.load(f)\n    elif path.endswith(\".pth\"):\n        with fsspec.open(path, \"rb\") as f:\n            return torch.load(f, map_location=\"cpu\")\n    else:\n        raise ValueError(\"Unsupported file type\")\n\n\ndef save_file(obj: Any, path: str):\n    if path.endswith(\".json\"):\n        with fsspec.open(path, \"w\") as f:\n            json.dump(obj, f, indent=4)\n    elif path.endswith(\".pth\"):\n        with fsspec.open(path, \"wb\") as f:\n            torch.save(obj, f)\n    else:\n        raise ValueError(\"Unsupported file type\")\n\n\nclass BaseIDManager:\n    \"\"\"Base `ID` Manager class. Every new `ID` manager must inherit this.\n    It defines common `ID` manager specific functions.\n    \"\"\"\n\n    def __init__(self, id_file_path: str = \"\"):\n        self.name_to_id = {}\n\n        if id_file_path:\n            self.load_ids_from_file(id_file_path)\n\n    @staticmethod\n    def _load_json(json_file_path: str) -> Dict:\n        with fsspec.open(json_file_path, \"r\") as f:\n            return json.load(f)\n\n    @staticmethod\n    def _save_json(json_file_path: str, data: dict) -> None:\n        with fsspec.open(json_file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n    def set_ids_from_data(self, items: List, parse_key: str) -> None:\n        \"\"\"Set IDs from data samples.\n\n        Args:\n            items (List): Data sampled returned by `load_tts_samples()`.\n        \"\"\"\n        self.name_to_id = self.parse_ids_from_data(items, parse_key=parse_key)\n\n    def load_ids_from_file(self, file_path: str) -> None:\n        \"\"\"Set IDs from a file.\n\n        Args:\n            file_path (str): Path to the file.\n        \"\"\"\n        self.name_to_id = load_file(file_path)\n\n    def save_ids_to_file(self, file_path: str) -> None:\n        \"\"\"Save IDs to a json file.\n\n        Args:\n            file_path (str): Path to the output file.\n        \"\"\"\n        save_file(self.name_to_id, file_path)\n\n    def get_random_id(self) -> Any:\n        \"\"\"Get a random embedding.\n\n        Args:\n\n        Returns:\n            np.ndarray: embedding.\n        \"\"\"\n        if self.name_to_id:\n            return self.name_to_id[random.choices(list(self.name_to_id.keys()))[0]]\n\n        return None\n\n    @staticmethod\n    def parse_ids_from_data(items: List, parse_key: str) -> Tuple[Dict]:\n        \"\"\"Parse IDs from data samples retured by `load_tts_samples()`.\n\n        Args:\n            items (list): Data sampled returned by `load_tts_samples()`.\n            parse_key (str): The key to being used to parse the data.\n        Returns:\n            Tuple[Dict]: speaker IDs.\n        \"\"\"\n        classes = sorted({item[parse_key] for item in items})\n        ids = {name: i for i, name in enumerate(classes)}\n        return ids\n\n\nclass EmbeddingManager(BaseIDManager):\n    \"\"\"Base `Embedding` Manager class. Every new `Embedding` manager must inherit this.\n    It defines common `Embedding` manager specific functions.\n\n    It expects embeddings files in the following format:\n\n    ::\n\n        {\n            'audio_file_key':{\n                'name': 'category_name',\n                'embedding'[<embedding_values>]\n            },\n            ...\n        }\n\n    `audio_file_key` is a unique key to the audio file in the dataset. It can be the path to the file or any other unique key.\n    `embedding` is the embedding vector of the audio file.\n    `name` can be name of the speaker of the audio file.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_file_path: Union[str, List[str]] = \"\",\n        id_file_path: str = \"\",\n        encoder_model_path: str = \"\",\n        encoder_config_path: str = \"\",\n        use_cuda: bool = False,\n    ):\n        super().__init__(id_file_path=id_file_path)\n\n        self.embeddings = {}\n        self.embeddings_by_names = {}\n        self.clip_ids = []\n        self.encoder = None\n        self.encoder_ap = None\n        self.use_cuda = use_cuda\n\n        if embedding_file_path:\n            if isinstance(embedding_file_path, list):\n                self.load_embeddings_from_list_of_files(embedding_file_path)\n            else:\n                self.load_embeddings_from_file(embedding_file_path)\n\n        if encoder_model_path and encoder_config_path:\n            self.init_encoder(encoder_model_path, encoder_config_path, use_cuda)\n\n    @property\n    def num_embeddings(self):\n        \"\"\"Get number of embeddings.\"\"\"\n        return len(self.embeddings)\n\n    @property\n    def num_names(self):\n        \"\"\"Get number of embeddings.\"\"\"\n        return len(self.embeddings_by_names)\n\n    @property\n    def embedding_dim(self):\n        \"\"\"Dimensionality of embeddings. If embeddings are not loaded, returns zero.\"\"\"\n        if self.embeddings:\n            return len(self.embeddings[list(self.embeddings.keys())[0]][\"embedding\"])\n        return 0\n\n    @property\n    def embedding_names(self):\n        \"\"\"Get embedding names.\"\"\"\n        return list(self.embeddings_by_names.keys())\n\n    def save_embeddings_to_file(self, file_path: str) -> None:\n        \"\"\"Save embeddings to a json file.\n\n        Args:\n            file_path (str): Path to the output file.\n        \"\"\"\n        save_file(self.embeddings, file_path)\n\n    @staticmethod\n    def read_embeddings_from_file(file_path: str):\n        \"\"\"Load embeddings from a json file.\n\n        Args:\n            file_path (str): Path to the file.\n        \"\"\"\n        embeddings = load_file(file_path)\n        speakers = sorted({x[\"name\"] for x in embeddings.values()})\n        name_to_id = {name: i for i, name in enumerate(speakers)}\n        clip_ids = list(set(sorted(clip_name for clip_name in embeddings.keys())))\n        # cache embeddings_by_names for fast inference using a bigger speakers.json\n        embeddings_by_names = {}\n        for x in embeddings.values():\n            if x[\"name\"] not in embeddings_by_names.keys():\n                embeddings_by_names[x[\"name\"]] = [x[\"embedding\"]]\n            else:\n                embeddings_by_names[x[\"name\"]].append(x[\"embedding\"])\n        return name_to_id, clip_ids, embeddings, embeddings_by_names\n\n    def load_embeddings_from_file(self, file_path: str) -> None:\n        \"\"\"Load embeddings from a json file.\n\n        Args:\n            file_path (str): Path to the target json file.\n        \"\"\"\n        self.name_to_id, self.clip_ids, self.embeddings, self.embeddings_by_names = self.read_embeddings_from_file(\n            file_path\n        )\n\n    def load_embeddings_from_list_of_files(self, file_paths: List[str]) -> None:\n        \"\"\"Load embeddings from a list of json files and don't allow duplicate keys.\n\n        Args:\n            file_paths (List[str]): List of paths to the target json files.\n        \"\"\"\n        self.name_to_id = {}\n        self.clip_ids = []\n        self.embeddings_by_names = {}\n        self.embeddings = {}\n        for file_path in file_paths:\n            ids, clip_ids, embeddings, embeddings_by_names = self.read_embeddings_from_file(file_path)\n            # check colliding keys\n            duplicates = set(self.embeddings.keys()) & set(embeddings.keys())\n            if duplicates:\n                raise ValueError(f\" [!] Duplicate embedding names <{duplicates}> in {file_path}\")\n            # store values\n            self.name_to_id.update(ids)\n            self.clip_ids.extend(clip_ids)\n            self.embeddings_by_names.update(embeddings_by_names)\n            self.embeddings.update(embeddings)\n\n        # reset name_to_id to get the right speaker ids\n        self.name_to_id = {name: i for i, name in enumerate(self.name_to_id)}\n\n    def get_embedding_by_clip(self, clip_idx: str) -> List:\n        \"\"\"Get embedding by clip ID.\n\n        Args:\n            clip_idx (str): Target clip ID.\n\n        Returns:\n            List: embedding as a list.\n        \"\"\"\n        return self.embeddings[clip_idx][\"embedding\"]\n\n    def get_embeddings_by_name(self, idx: str) -> List[List]:\n        \"\"\"Get all embeddings of a speaker.\n\n        Args:\n            idx (str): Target name.\n\n        Returns:\n            List[List]: all the embeddings of the given speaker.\n        \"\"\"\n        return self.embeddings_by_names[idx]\n\n    def get_embeddings_by_names(self) -> Dict:\n        \"\"\"Get all embeddings by names.\n\n        Returns:\n            Dict: all the embeddings of each speaker.\n        \"\"\"\n        embeddings_by_names = {}\n        for x in self.embeddings.values():\n            if x[\"name\"] not in embeddings_by_names.keys():\n                embeddings_by_names[x[\"name\"]] = [x[\"embedding\"]]\n            else:\n                embeddings_by_names[x[\"name\"]].append(x[\"embedding\"])\n        return embeddings_by_names\n\n    def get_mean_embedding(self, idx: str, num_samples: int = None, randomize: bool = False) -> np.ndarray:\n        \"\"\"Get mean embedding of a idx.\n\n        Args:\n            idx (str): Target name.\n            num_samples (int, optional): Number of samples to be averaged. Defaults to None.\n            randomize (bool, optional): Pick random `num_samples` of embeddings. Defaults to False.\n\n        Returns:\n            np.ndarray: Mean embedding.\n        \"\"\"\n        embeddings = self.get_embeddings_by_name(idx)\n        if num_samples is None:\n            embeddings = np.stack(embeddings).mean(0)\n        else:\n            assert len(embeddings) >= num_samples, f\" [!] {idx} has number of samples < {num_samples}\"\n            if randomize:\n                embeddings = np.stack(random.choices(embeddings, k=num_samples)).mean(0)\n            else:\n                embeddings = np.stack(embeddings[:num_samples]).mean(0)\n        return embeddings\n\n    def get_random_embedding(self) -> Any:\n        \"\"\"Get a random embedding.\n\n        Args:\n\n        Returns:\n            np.ndarray: embedding.\n        \"\"\"\n        if self.embeddings:\n            return self.embeddings[random.choices(list(self.embeddings.keys()))[0]][\"embedding\"]\n\n        return None\n\n    def get_clips(self) -> List:\n        return sorted(self.embeddings.keys())\n\n    def init_encoder(self, model_path: str, config_path: str, use_cuda=False) -> None:\n        \"\"\"Initialize a speaker encoder model.\n\n        Args:\n            model_path (str): Model file path.\n            config_path (str): Model config file path.\n            use_cuda (bool, optional): Use CUDA. Defaults to False.\n        \"\"\"\n        self.use_cuda = use_cuda\n        self.encoder_config = load_config(config_path)\n        self.encoder = setup_encoder_model(self.encoder_config)\n        self.encoder_criterion = self.encoder.load_checkpoint(\n            self.encoder_config, model_path, eval=True, use_cuda=use_cuda, cache=True\n        )\n        self.encoder_ap = AudioProcessor(**self.encoder_config.audio)\n\n    def compute_embedding_from_clip(self, wav_file: Union[str, List[str]]) -> list:\n        \"\"\"Compute a embedding from a given audio file.\n\n        Args:\n            wav_file (Union[str, List[str]]): Target file path.\n\n        Returns:\n            list: Computed embedding.\n        \"\"\"\n\n        def _compute(wav_file: str):\n            waveform = self.encoder_ap.load_wav(wav_file, sr=self.encoder_ap.sample_rate)\n            if not self.encoder_config.model_params.get(\"use_torch_spec\", False):\n                m_input = self.encoder_ap.melspectrogram(waveform)\n                m_input = torch.from_numpy(m_input)\n            else:\n                m_input = torch.from_numpy(waveform)\n\n            if self.use_cuda:\n                m_input = m_input.cuda()\n            m_input = m_input.unsqueeze(0)\n            embedding = self.encoder.compute_embedding(m_input)\n            return embedding\n\n        if isinstance(wav_file, list):\n            # compute the mean embedding\n            embeddings = None\n            for wf in wav_file:\n                embedding = _compute(wf)\n                if embeddings is None:\n                    embeddings = embedding\n                else:\n                    embeddings += embedding\n            return (embeddings / len(wav_file))[0].tolist()\n        embedding = _compute(wav_file)\n        return embedding[0].tolist()\n\n    def compute_embeddings(self, feats: Union[torch.Tensor, np.ndarray]) -> List:\n        \"\"\"Compute embedding from features.\n\n        Args:\n            feats (Union[torch.Tensor, np.ndarray]): Input features.\n\n        Returns:\n            List: computed embedding.\n        \"\"\"\n        if isinstance(feats, np.ndarray):\n            feats = torch.from_numpy(feats)\n        if feats.ndim == 2:\n            feats = feats.unsqueeze(0)\n        if self.use_cuda:\n            feats = feats.cuda()\n        return self.encoder.compute_embedding(feats)\n", "TTS/tts/utils/ssim.py": "# Adopted from https://github.com/photosynthesis-team/piq\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _Loss\n\n\ndef _reduce(x: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n    r\"\"\"Reduce input in batch dimension if needed.\n    Args:\n        x: Tensor with shape (N, *).\n        reduction: Specifies the reduction type:\n            ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``\n    \"\"\"\n    if reduction == \"none\":\n        return x\n    if reduction == \"mean\":\n        return x.mean(dim=0)\n    if reduction == \"sum\":\n        return x.sum(dim=0)\n    raise ValueError(\"Unknown reduction. Expected one of {'none', 'mean', 'sum'}\")\n\n\ndef _validate_input(\n    tensors: List[torch.Tensor],\n    dim_range: Tuple[int, int] = (0, -1),\n    data_range: Tuple[float, float] = (0.0, -1.0),\n    # size_dim_range: Tuple[float, float] = (0., -1.),\n    size_range: Optional[Tuple[int, int]] = None,\n) -> None:\n    r\"\"\"Check that input(-s)  satisfies the requirements\n    Args:\n        tensors: Tensors to check\n        dim_range: Allowed number of dimensions. (min, max)\n        data_range: Allowed range of values in tensors. (min, max)\n        size_range: Dimensions to include in size comparison. (start_dim, end_dim + 1)\n    \"\"\"\n\n    if not __debug__:\n        return\n\n    x = tensors[0]\n\n    for t in tensors:\n        assert torch.is_tensor(t), f\"Expected torch.Tensor, got {type(t)}\"\n        assert t.device == x.device, f\"Expected tensors to be on {x.device}, got {t.device}\"\n\n        if size_range is None:\n            assert t.size() == x.size(), f\"Expected tensors with same size, got {t.size()} and {x.size()}\"\n        else:\n            assert (\n                t.size()[size_range[0] : size_range[1]] == x.size()[size_range[0] : size_range[1]]\n            ), f\"Expected tensors with same size at given dimensions, got {t.size()} and {x.size()}\"\n\n        if dim_range[0] == dim_range[1]:\n            assert t.dim() == dim_range[0], f\"Expected number of dimensions to be {dim_range[0]}, got {t.dim()}\"\n        elif dim_range[0] < dim_range[1]:\n            assert (\n                dim_range[0] <= t.dim() <= dim_range[1]\n            ), f\"Expected number of dimensions to be between {dim_range[0]} and {dim_range[1]}, got {t.dim()}\"\n\n        if data_range[0] < data_range[1]:\n            assert data_range[0] <= t.min(), f\"Expected values to be greater or equal to {data_range[0]}, got {t.min()}\"\n            assert t.max() <= data_range[1], f\"Expected values to be lower or equal to {data_range[1]}, got {t.max()}\"\n\n\ndef gaussian_filter(kernel_size: int, sigma: float) -> torch.Tensor:\n    r\"\"\"Returns 2D Gaussian kernel N(0,`sigma`^2)\n    Args:\n        size: Size of the kernel\n        sigma: Std of the distribution\n    Returns:\n        gaussian_kernel: Tensor with shape (1, kernel_size, kernel_size)\n    \"\"\"\n    coords = torch.arange(kernel_size, dtype=torch.float32)\n    coords -= (kernel_size - 1) / 2.0\n\n    g = coords**2\n    g = (-(g.unsqueeze(0) + g.unsqueeze(1)) / (2 * sigma**2)).exp()\n\n    g /= g.sum()\n    return g.unsqueeze(0)\n\n\ndef ssim(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    kernel_size: int = 11,\n    kernel_sigma: float = 1.5,\n    data_range: Union[int, float] = 1.0,\n    reduction: str = \"mean\",\n    full: bool = False,\n    downsample: bool = True,\n    k1: float = 0.01,\n    k2: float = 0.03,\n) -> List[torch.Tensor]:\n    r\"\"\"Interface of Structural Similarity (SSIM) index.\n    Inputs supposed to be in range ``[0, data_range]``.\n    To match performance with skimage and tensorflow set ``'downsample' = True``.\n\n    Args:\n        x: An input tensor. Shape :math:`(N, C, H, W)` or :math:`(N, C, H, W, 2)`.\n        y: A target tensor. Shape :math:`(N, C, H, W)` or :math:`(N, C, H, W, 2)`.\n        kernel_size: The side-length of the sliding window used in comparison. Must be an odd value.\n        kernel_sigma: Sigma of normal distribution.\n        data_range: Maximum value range of images (usually 1.0 or 255).\n        reduction: Specifies the reduction type:\n            ``'none'`` | ``'mean'`` | ``'sum'``. Default:``'mean'``\n        full: Return cs map or not.\n        downsample: Perform average pool before SSIM computation. Default: True\n        k1: Algorithm parameter, K1 (small constant).\n        k2: Algorithm parameter, K2 (small constant).\n            Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\n    Returns:\n        Value of Structural Similarity (SSIM) index. In case of 5D input tensors, complex value is returned\n        as a tensor of size 2.\n\n    References:\n        Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004).\n        Image quality assessment: From error visibility to structural similarity.\n        IEEE Transactions on Image Processing, 13, 600-612.\n        https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf,\n        DOI: `10.1109/TIP.2003.819861`\n    \"\"\"\n    assert kernel_size % 2 == 1, f\"Kernel size must be odd, got [{kernel_size}]\"\n    _validate_input([x, y], dim_range=(4, 5), data_range=(0, data_range))\n\n    x = x / float(data_range)\n    y = y / float(data_range)\n\n    # Averagepool image if the size is large enough\n    f = max(1, round(min(x.size()[-2:]) / 256))\n    if (f > 1) and downsample:\n        x = F.avg_pool2d(x, kernel_size=f)\n        y = F.avg_pool2d(y, kernel_size=f)\n\n    kernel = gaussian_filter(kernel_size, kernel_sigma).repeat(x.size(1), 1, 1, 1).to(y)\n    _compute_ssim_per_channel = _ssim_per_channel_complex if x.dim() == 5 else _ssim_per_channel\n    ssim_map, cs_map = _compute_ssim_per_channel(x=x, y=y, kernel=kernel, k1=k1, k2=k2)\n    ssim_val = ssim_map.mean(1)\n    cs = cs_map.mean(1)\n\n    ssim_val = _reduce(ssim_val, reduction)\n    cs = _reduce(cs, reduction)\n\n    if full:\n        return [ssim_val, cs]\n\n    return ssim_val\n\n\nclass SSIMLoss(_Loss):\n    r\"\"\"Creates a criterion that measures the structural similarity index error between\n    each element in the input :math:`x` and target :math:`y`.\n\n    To match performance with skimage and tensorflow set ``'downsample' = True``.\n\n    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n    .. math::\n        SSIM = \\{ssim_1,\\dots,ssim_{N \\times C}\\}\\\\\n        ssim_{l}(x, y) = \\frac{(2 \\mu_x \\mu_y + c_1) (2 \\sigma_{xy} + c_2)}\n        {(\\mu_x^2 +\\mu_y^2 + c_1)(\\sigma_x^2 +\\sigma_y^2 + c_2)},\n\n    where :math:`N` is the batch size, `C` is the channel size. If :attr:`reduction` is not ``'none'``\n    (default ``'mean'``), then:\n\n    .. math::\n        SSIMLoss(x, y) =\n        \\begin{cases}\n            \\operatorname{mean}(1 - SSIM), &  \\text{if reduction} = \\text{'mean';}\\\\\n            \\operatorname{sum}(1 - SSIM),  &  \\text{if reduction} = \\text{'sum'.}\n        \\end{cases}\n\n    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n    of :math:`n` elements each.\n\n    The sum operation still operates over all the elements, and divides by :math:`n`.\n    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n    In case of 5D input tensors, complex value is returned as a tensor of size 2.\n\n    Args:\n        kernel_size: By default, the mean and covariance of a pixel is obtained\n            by convolution with given filter_size.\n        kernel_sigma: Standard deviation for Gaussian kernel.\n        k1: Coefficient related to c1 in the above equation.\n        k2: Coefficient related to c2 in the above equation.\n        downsample: Perform average pool before SSIM computation. Default: True\n        reduction: Specifies the reduction type:\n            ``'none'`` | ``'mean'`` | ``'sum'``. Default:``'mean'``\n        data_range: Maximum value range of images (usually 1.0 or 255).\n\n    Examples:\n        >>> loss = SSIMLoss()\n        >>> x = torch.rand(3, 3, 256, 256, requires_grad=True)\n        >>> y = torch.rand(3, 3, 256, 256)\n        >>> output = loss(x, y)\n        >>> output.backward()\n\n    References:\n        Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004).\n        Image quality assessment: From error visibility to structural similarity.\n        IEEE Transactions on Image Processing, 13, 600-612.\n        https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf,\n        DOI:`10.1109/TIP.2003.819861`\n    \"\"\"\n    __constants__ = [\"kernel_size\", \"k1\", \"k2\", \"sigma\", \"kernel\", \"reduction\"]\n\n    def __init__(\n        self,\n        kernel_size: int = 11,\n        kernel_sigma: float = 1.5,\n        k1: float = 0.01,\n        k2: float = 0.03,\n        downsample: bool = True,\n        reduction: str = \"mean\",\n        data_range: Union[int, float] = 1.0,\n    ) -> None:\n        super().__init__()\n\n        # Generic loss parameters.\n        self.reduction = reduction\n\n        # Loss-specific parameters.\n        self.kernel_size = kernel_size\n\n        # This check might look redundant because kernel size is checked within the ssim function anyway.\n        # However, this check allows to fail fast when the loss is being initialised and training has not been started.\n        assert kernel_size % 2 == 1, f\"Kernel size must be odd, got [{kernel_size}]\"\n        self.kernel_sigma = kernel_sigma\n        self.k1 = k1\n        self.k2 = k2\n        self.downsample = downsample\n        self.data_range = data_range\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Computation of Structural Similarity (SSIM) index as a loss function.\n\n        Args:\n            x: An input tensor. Shape :math:`(N, C, H, W)` or :math:`(N, C, H, W, 2)`.\n            y: A target tensor. Shape :math:`(N, C, H, W)` or :math:`(N, C, H, W, 2)`.\n\n        Returns:\n            Value of SSIM loss to be minimized, i.e ``1 - ssim`` in [0, 1] range. In case of 5D input tensors,\n            complex value is returned as a tensor of size 2.\n        \"\"\"\n\n        score = ssim(\n            x=x,\n            y=y,\n            kernel_size=self.kernel_size,\n            kernel_sigma=self.kernel_sigma,\n            downsample=self.downsample,\n            data_range=self.data_range,\n            reduction=self.reduction,\n            full=False,\n            k1=self.k1,\n            k2=self.k2,\n        )\n        return torch.ones_like(score) - score\n\n\ndef _ssim_per_channel(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    kernel: torch.Tensor,\n    k1: float = 0.01,\n    k2: float = 0.03,\n) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    r\"\"\"Calculate Structural Similarity (SSIM) index for X and Y per channel.\n\n    Args:\n        x: An input tensor. Shape :math:`(N, C, H, W)`.\n        y: A target tensor. Shape :math:`(N, C, H, W)`.\n        kernel: 2D Gaussian kernel.\n        k1: Algorithm parameter, K1 (small constant, see [1]).\n        k2: Algorithm parameter, K2 (small constant, see [1]).\n            Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\n    Returns:\n        Full Value of Structural Similarity (SSIM) index.\n    \"\"\"\n    if x.size(-1) < kernel.size(-1) or x.size(-2) < kernel.size(-2):\n        raise ValueError(\n            f\"Kernel size can't be greater than actual input size. Input size: {x.size()}. \"\n            f\"Kernel size: {kernel.size()}\"\n        )\n\n    c1 = k1**2\n    c2 = k2**2\n    n_channels = x.size(1)\n    mu_x = F.conv2d(x, weight=kernel, stride=1, padding=0, groups=n_channels)\n    mu_y = F.conv2d(y, weight=kernel, stride=1, padding=0, groups=n_channels)\n\n    mu_xx = mu_x**2\n    mu_yy = mu_y**2\n    mu_xy = mu_x * mu_y\n\n    sigma_xx = F.conv2d(x**2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xx\n    sigma_yy = F.conv2d(y**2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_yy\n    sigma_xy = F.conv2d(x * y, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xy\n\n    # Contrast sensitivity (CS) with alpha = beta = gamma = 1.\n    cs = (2.0 * sigma_xy + c2) / (sigma_xx + sigma_yy + c2)\n\n    # Structural similarity (SSIM)\n    ss = (2.0 * mu_xy + c1) / (mu_xx + mu_yy + c1) * cs\n\n    ssim_val = ss.mean(dim=(-1, -2))\n    cs = cs.mean(dim=(-1, -2))\n    return ssim_val, cs\n\n\ndef _ssim_per_channel_complex(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    kernel: torch.Tensor,\n    k1: float = 0.01,\n    k2: float = 0.03,\n) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    r\"\"\"Calculate Structural Similarity (SSIM) index for Complex X and Y per channel.\n\n    Args:\n        x: An input tensor. Shape :math:`(N, C, H, W, 2)`.\n        y: A target tensor. Shape :math:`(N, C, H, W, 2)`.\n        kernel: 2-D gauss kernel.\n        k1: Algorithm parameter, K1 (small constant, see [1]).\n        k2: Algorithm parameter, K2 (small constant, see [1]).\n            Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\n    Returns:\n        Full Value of Complex Structural Similarity (SSIM) index.\n    \"\"\"\n    n_channels = x.size(1)\n    if x.size(-2) < kernel.size(-1) or x.size(-3) < kernel.size(-2):\n        raise ValueError(\n            f\"Kernel size can't be greater than actual input size. Input size: {x.size()}. \"\n            f\"Kernel size: {kernel.size()}\"\n        )\n\n    c1 = k1**2\n    c2 = k2**2\n\n    x_real = x[..., 0]\n    x_imag = x[..., 1]\n    y_real = y[..., 0]\n    y_imag = y[..., 1]\n\n    mu1_real = F.conv2d(x_real, weight=kernel, stride=1, padding=0, groups=n_channels)\n    mu1_imag = F.conv2d(x_imag, weight=kernel, stride=1, padding=0, groups=n_channels)\n    mu2_real = F.conv2d(y_real, weight=kernel, stride=1, padding=0, groups=n_channels)\n    mu2_imag = F.conv2d(y_imag, weight=kernel, stride=1, padding=0, groups=n_channels)\n\n    mu1_sq = mu1_real.pow(2) + mu1_imag.pow(2)\n    mu2_sq = mu2_real.pow(2) + mu2_imag.pow(2)\n    mu1_mu2_real = mu1_real * mu2_real - mu1_imag * mu2_imag\n    mu1_mu2_imag = mu1_real * mu2_imag + mu1_imag * mu2_real\n\n    compensation = 1.0\n\n    x_sq = x_real.pow(2) + x_imag.pow(2)\n    y_sq = y_real.pow(2) + y_imag.pow(2)\n    x_y_real = x_real * y_real - x_imag * y_imag\n    x_y_imag = x_real * y_imag + x_imag * y_real\n\n    sigma1_sq = F.conv2d(x_sq, weight=kernel, stride=1, padding=0, groups=n_channels) - mu1_sq\n    sigma2_sq = F.conv2d(y_sq, weight=kernel, stride=1, padding=0, groups=n_channels) - mu2_sq\n    sigma12_real = F.conv2d(x_y_real, weight=kernel, stride=1, padding=0, groups=n_channels) - mu1_mu2_real\n    sigma12_imag = F.conv2d(x_y_imag, weight=kernel, stride=1, padding=0, groups=n_channels) - mu1_mu2_imag\n    sigma12 = torch.stack((sigma12_imag, sigma12_real), dim=-1)\n    mu1_mu2 = torch.stack((mu1_mu2_real, mu1_mu2_imag), dim=-1)\n    # Set alpha = beta = gamma = 1.\n    cs_map = (sigma12 * 2 + c2 * compensation) / (sigma1_sq.unsqueeze(-1) + sigma2_sq.unsqueeze(-1) + c2 * compensation)\n    ssim_map = (mu1_mu2 * 2 + c1 * compensation) / (mu1_sq.unsqueeze(-1) + mu2_sq.unsqueeze(-1) + c1 * compensation)\n    ssim_map = ssim_map * cs_map\n\n    ssim_val = ssim_map.mean(dim=(-2, -3))\n    cs = cs_map.mean(dim=(-2, -3))\n\n    return ssim_val, cs\n", "TTS/tts/utils/fairseq.py": "import torch\n\n\ndef rehash_fairseq_vits_checkpoint(checkpoint_file):\n    chk = torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))[\"model\"]\n    new_chk = {}\n    for k, v in chk.items():\n        if \"enc_p.\" in k:\n            new_chk[k.replace(\"enc_p.\", \"text_encoder.\")] = v\n        elif \"dec.\" in k:\n            new_chk[k.replace(\"dec.\", \"waveform_decoder.\")] = v\n        elif \"enc_q.\" in k:\n            new_chk[k.replace(\"enc_q.\", \"posterior_encoder.\")] = v\n        elif \"flow.flows.2.\" in k:\n            new_chk[k.replace(\"flow.flows.2.\", \"flow.flows.1.\")] = v\n        elif \"flow.flows.4.\" in k:\n            new_chk[k.replace(\"flow.flows.4.\", \"flow.flows.2.\")] = v\n        elif \"flow.flows.6.\" in k:\n            new_chk[k.replace(\"flow.flows.6.\", \"flow.flows.3.\")] = v\n        elif \"dp.flows.0.m\" in k:\n            new_chk[k.replace(\"dp.flows.0.m\", \"duration_predictor.flows.0.translation\")] = v\n        elif \"dp.flows.0.logs\" in k:\n            new_chk[k.replace(\"dp.flows.0.logs\", \"duration_predictor.flows.0.log_scale\")] = v\n        elif \"dp.flows.1\" in k:\n            new_chk[k.replace(\"dp.flows.1\", \"duration_predictor.flows.1\")] = v\n        elif \"dp.flows.3\" in k:\n            new_chk[k.replace(\"dp.flows.3\", \"duration_predictor.flows.2\")] = v\n        elif \"dp.flows.5\" in k:\n            new_chk[k.replace(\"dp.flows.5\", \"duration_predictor.flows.3\")] = v\n        elif \"dp.flows.7\" in k:\n            new_chk[k.replace(\"dp.flows.7\", \"duration_predictor.flows.4\")] = v\n        elif \"dp.post_flows.0.m\" in k:\n            new_chk[k.replace(\"dp.post_flows.0.m\", \"duration_predictor.post_flows.0.translation\")] = v\n        elif \"dp.post_flows.0.logs\" in k:\n            new_chk[k.replace(\"dp.post_flows.0.logs\", \"duration_predictor.post_flows.0.log_scale\")] = v\n        elif \"dp.post_flows.1\" in k:\n            new_chk[k.replace(\"dp.post_flows.1\", \"duration_predictor.post_flows.1\")] = v\n        elif \"dp.post_flows.3\" in k:\n            new_chk[k.replace(\"dp.post_flows.3\", \"duration_predictor.post_flows.2\")] = v\n        elif \"dp.post_flows.5\" in k:\n            new_chk[k.replace(\"dp.post_flows.5\", \"duration_predictor.post_flows.3\")] = v\n        elif \"dp.post_flows.7\" in k:\n            new_chk[k.replace(\"dp.post_flows.7\", \"duration_predictor.post_flows.4\")] = v\n        elif \"dp.\" in k:\n            new_chk[k.replace(\"dp.\", \"duration_predictor.\")] = v\n        else:\n            new_chk[k] = v\n    return new_chk\n", "TTS/tts/utils/helpers.py": "import numpy as np\nimport torch\nfrom scipy.stats import betabinom\nfrom torch.nn import functional as F\n\ntry:\n    from TTS.tts.utils.monotonic_align.core import maximum_path_c\n\n    CYTHON = True\nexcept ModuleNotFoundError:\n    CYTHON = False\n\n\nclass StandardScaler:\n    \"\"\"StandardScaler for mean-scale normalization with the given mean and scale values.\"\"\"\n\n    def __init__(self, mean: np.ndarray = None, scale: np.ndarray = None) -> None:\n        self.mean_ = mean\n        self.scale_ = scale\n\n    def set_stats(self, mean, scale):\n        self.mean_ = mean\n        self.scale_ = scale\n\n    def reset_stats(self):\n        delattr(self, \"mean_\")\n        delattr(self, \"scale_\")\n\n    def transform(self, X):\n        X = np.asarray(X)\n        X -= self.mean_\n        X /= self.scale_\n        return X\n\n    def inverse_transform(self, X):\n        X = np.asarray(X)\n        X *= self.scale_\n        X += self.mean_\n        return X\n\n\n# from https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1\ndef sequence_mask(sequence_length, max_len=None):\n    \"\"\"Create a sequence mask for filtering padding in a sequence tensor.\n\n    Args:\n        sequence_length (torch.tensor): Sequence lengths.\n        max_len (int, Optional): Maximum sequence length. Defaults to None.\n\n    Shapes:\n        - mask: :math:`[B, T_max]`\n    \"\"\"\n    if max_len is None:\n        max_len = sequence_length.max()\n    seq_range = torch.arange(max_len, dtype=sequence_length.dtype, device=sequence_length.device)\n    # B x T_max\n    return seq_range.unsqueeze(0) < sequence_length.unsqueeze(1)\n\n\ndef segment(x: torch.tensor, segment_indices: torch.tensor, segment_size=4, pad_short=False):\n    \"\"\"Segment each sample in a batch based on the provided segment indices\n\n    Args:\n        x (torch.tensor): Input tensor.\n        segment_indices (torch.tensor): Segment indices.\n        segment_size (int): Expected output segment size.\n        pad_short (bool): Pad the end of input tensor with zeros if shorter than the segment size.\n    \"\"\"\n    # pad the input tensor if it is shorter than the segment size\n    if pad_short and x.shape[-1] < segment_size:\n        x = torch.nn.functional.pad(x, (0, segment_size - x.size(2)))\n\n    segments = torch.zeros_like(x[:, :, :segment_size])\n\n    for i in range(x.size(0)):\n        index_start = segment_indices[i]\n        index_end = index_start + segment_size\n        x_i = x[i]\n        if pad_short and index_end >= x.size(2):\n            # pad the sample if it is shorter than the segment size\n            x_i = torch.nn.functional.pad(x_i, (0, (index_end + 1) - x.size(2)))\n        segments[i] = x_i[:, index_start:index_end]\n    return segments\n\n\ndef rand_segments(\n    x: torch.tensor, x_lengths: torch.tensor = None, segment_size=4, let_short_samples=False, pad_short=False\n):\n    \"\"\"Create random segments based on the input lengths.\n\n    Args:\n        x (torch.tensor): Input tensor.\n        x_lengths (torch.tensor): Input lengths.\n        segment_size (int): Expected output segment size.\n        let_short_samples (bool): Allow shorter samples than the segment size.\n        pad_short (bool): Pad the end of input tensor with zeros if shorter than the segment size.\n\n    Shapes:\n        - x: :math:`[B, C, T]`\n        - x_lengths: :math:`[B]`\n    \"\"\"\n    _x_lenghts = x_lengths.clone()\n    B, _, T = x.size()\n    if pad_short:\n        if T < segment_size:\n            x = torch.nn.functional.pad(x, (0, segment_size - T))\n            T = segment_size\n    if _x_lenghts is None:\n        _x_lenghts = T\n    len_diff = _x_lenghts - segment_size\n    if let_short_samples:\n        _x_lenghts[len_diff < 0] = segment_size\n        len_diff = _x_lenghts - segment_size\n    else:\n        assert all(\n            len_diff > 0\n        ), f\" [!] At least one sample is shorter than the segment size ({segment_size}). \\n {_x_lenghts}\"\n    segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long()\n    ret = segment(x, segment_indices, segment_size, pad_short=pad_short)\n    return ret, segment_indices\n\n\ndef average_over_durations(values, durs):\n    \"\"\"Average values over durations.\n\n    Shapes:\n        - values: :math:`[B, 1, T_de]`\n        - durs: :math:`[B, T_en]`\n        - avg: :math:`[B, 1, T_en]`\n    \"\"\"\n    durs_cums_ends = torch.cumsum(durs, dim=1).long()\n    durs_cums_starts = torch.nn.functional.pad(durs_cums_ends[:, :-1], (1, 0))\n    values_nonzero_cums = torch.nn.functional.pad(torch.cumsum(values != 0.0, dim=2), (1, 0))\n    values_cums = torch.nn.functional.pad(torch.cumsum(values, dim=2), (1, 0))\n\n    bs, l = durs_cums_ends.size()\n    n_formants = values.size(1)\n    dcs = durs_cums_starts[:, None, :].expand(bs, n_formants, l)\n    dce = durs_cums_ends[:, None, :].expand(bs, n_formants, l)\n\n    values_sums = (torch.gather(values_cums, 2, dce) - torch.gather(values_cums, 2, dcs)).float()\n    values_nelems = (torch.gather(values_nonzero_cums, 2, dce) - torch.gather(values_nonzero_cums, 2, dcs)).float()\n\n    avg = torch.where(values_nelems == 0.0, values_nelems, values_sums / values_nelems)\n    return avg\n\n\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\n\n\ndef generate_path(duration, mask):\n    \"\"\"\n    Shapes:\n        - duration: :math:`[B, T_en]`\n        - mask: :math:'[B, T_en, T_de]`\n        - path: :math:`[B, T_en, T_de]`\n    \"\"\"\n    b, t_x, t_y = mask.shape\n    cum_duration = torch.cumsum(duration, 1)\n\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n    path = path.view(b, t_x, t_y)\n    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n    path = path * mask\n    return path\n\n\ndef maximum_path(value, mask):\n    if CYTHON:\n        return maximum_path_cython(value, mask)\n    return maximum_path_numpy(value, mask)\n\n\ndef maximum_path_cython(value, mask):\n    \"\"\"Cython optimised version.\n    Shapes:\n        - value: :math:`[B, T_en, T_de]`\n        - mask: :math:`[B, T_en, T_de]`\n    \"\"\"\n    value = value * mask\n    device = value.device\n    dtype = value.dtype\n    value = value.data.cpu().numpy().astype(np.float32)\n    path = np.zeros_like(value).astype(np.int32)\n    mask = mask.data.cpu().numpy()\n\n    t_x_max = mask.sum(1)[:, 0].astype(np.int32)\n    t_y_max = mask.sum(2)[:, 0].astype(np.int32)\n    maximum_path_c(path, value, t_x_max, t_y_max)\n    return torch.from_numpy(path).to(device=device, dtype=dtype)\n\n\ndef maximum_path_numpy(value, mask, max_neg_val=None):\n    \"\"\"\n    Monotonic alignment search algorithm\n    Numpy-friendly version. It's about 4 times faster than torch version.\n    value: [b, t_x, t_y]\n    mask: [b, t_x, t_y]\n    \"\"\"\n    if max_neg_val is None:\n        max_neg_val = -np.inf  # Patch for Sphinx complaint\n    value = value * mask\n\n    device = value.device\n    dtype = value.dtype\n    value = value.cpu().detach().numpy()\n    mask = mask.cpu().detach().numpy().astype(bool)\n\n    b, t_x, t_y = value.shape\n    direction = np.zeros(value.shape, dtype=np.int64)\n    v = np.zeros((b, t_x), dtype=np.float32)\n    x_range = np.arange(t_x, dtype=np.float32).reshape(1, -1)\n    for j in range(t_y):\n        v0 = np.pad(v, [[0, 0], [1, 0]], mode=\"constant\", constant_values=max_neg_val)[:, :-1]\n        v1 = v\n        max_mask = v1 >= v0\n        v_max = np.where(max_mask, v1, v0)\n        direction[:, :, j] = max_mask\n\n        index_mask = x_range <= j\n        v = np.where(index_mask, v_max + value[:, :, j], max_neg_val)\n    direction = np.where(mask, direction, 1)\n\n    path = np.zeros(value.shape, dtype=np.float32)\n    index = mask[:, :, 0].sum(1).astype(np.int64) - 1\n    index_range = np.arange(b)\n    for j in reversed(range(t_y)):\n        path[index_range, index, j] = 1\n        index = index + direction[index_range, index, j] - 1\n    path = path * mask.astype(np.float32)\n    path = torch.from_numpy(path).to(device=device, dtype=dtype)\n    return path\n\n\ndef beta_binomial_prior_distribution(phoneme_count, mel_count, scaling_factor=1.0):\n    P, M = phoneme_count, mel_count\n    x = np.arange(0, P)\n    mel_text_probs = []\n    for i in range(1, M + 1):\n        a, b = scaling_factor * i, scaling_factor * (M + 1 - i)\n        rv = betabinom(P, a, b)\n        mel_i_prob = rv.pmf(x)\n        mel_text_probs.append(mel_i_prob)\n    return np.array(mel_text_probs)\n\n\ndef compute_attn_prior(x_len, y_len, scaling_factor=1.0):\n    \"\"\"Compute attention priors for the alignment network.\"\"\"\n    attn_prior = beta_binomial_prior_distribution(\n        x_len,\n        y_len,\n        scaling_factor,\n    )\n    return attn_prior  # [y_len, x_len]\n", "TTS/tts/utils/synthesis.py": "from typing import Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\n\ndef numpy_to_torch(np_array, dtype, cuda=False, device=\"cpu\"):\n    if cuda:\n        device = \"cuda\"\n    if np_array is None:\n        return None\n    tensor = torch.as_tensor(np_array, dtype=dtype, device=device)\n    return tensor\n\n\ndef compute_style_mel(style_wav, ap, cuda=False, device=\"cpu\"):\n    if cuda:\n        device = \"cuda\"\n    style_mel = torch.FloatTensor(\n        ap.melspectrogram(ap.load_wav(style_wav, sr=ap.sample_rate)),\n        device=device,\n    ).unsqueeze(0)\n    return style_mel\n\n\ndef run_model_torch(\n    model: nn.Module,\n    inputs: torch.Tensor,\n    speaker_id: int = None,\n    style_mel: torch.Tensor = None,\n    style_text: str = None,\n    d_vector: torch.Tensor = None,\n    language_id: torch.Tensor = None,\n) -> Dict:\n    \"\"\"Run a torch model for inference. It does not support batch inference.\n\n    Args:\n        model (nn.Module): The model to run inference.\n        inputs (torch.Tensor): Input tensor with character ids.\n        speaker_id (int, optional): Input speaker ids for multi-speaker models. Defaults to None.\n        style_mel (torch.Tensor, optional): Spectrograms used for voice styling . Defaults to None.\n        d_vector (torch.Tensor, optional): d-vector for multi-speaker models    . Defaults to None.\n\n    Returns:\n        Dict: model outputs.\n    \"\"\"\n    input_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)\n    if hasattr(model, \"module\"):\n        _func = model.module.inference\n    else:\n        _func = model.inference\n    outputs = _func(\n        inputs,\n        aux_input={\n            \"x_lengths\": input_lengths,\n            \"speaker_ids\": speaker_id,\n            \"d_vectors\": d_vector,\n            \"style_mel\": style_mel,\n            \"style_text\": style_text,\n            \"language_ids\": language_id,\n        },\n    )\n    return outputs\n\n\ndef trim_silence(wav, ap):\n    return wav[: ap.find_endpoint(wav)]\n\n\ndef inv_spectrogram(postnet_output, ap, CONFIG):\n    if CONFIG.model.lower() in [\"tacotron\"]:\n        wav = ap.inv_spectrogram(postnet_output.T)\n    else:\n        wav = ap.inv_melspectrogram(postnet_output.T)\n    return wav\n\n\ndef id_to_torch(aux_id, cuda=False, device=\"cpu\"):\n    if cuda:\n        device = \"cuda\"\n    if aux_id is not None:\n        aux_id = np.asarray(aux_id)\n        aux_id = torch.from_numpy(aux_id).to(device)\n    return aux_id\n\n\ndef embedding_to_torch(d_vector, cuda=False, device=\"cpu\"):\n    if cuda:\n        device = \"cuda\"\n    if d_vector is not None:\n        d_vector = np.asarray(d_vector)\n        d_vector = torch.from_numpy(d_vector).type(torch.FloatTensor)\n        d_vector = d_vector.squeeze().unsqueeze(0).to(device)\n    return d_vector\n\n\n# TODO: perform GL with pytorch for batching\ndef apply_griffin_lim(inputs, input_lens, CONFIG, ap):\n    \"\"\"Apply griffin-lim to each sample iterating throught the first dimension.\n    Args:\n        inputs (Tensor or np.Array): Features to be converted by GL. First dimension is the batch size.\n        input_lens (Tensor or np.Array): 1D array of sample lengths.\n        CONFIG (Dict): TTS config.\n        ap (AudioProcessor): TTS audio processor.\n    \"\"\"\n    wavs = []\n    for idx, spec in enumerate(inputs):\n        wav_len = (input_lens[idx] * ap.hop_length) - ap.hop_length  # inverse librosa padding\n        wav = inv_spectrogram(spec, ap, CONFIG)\n        # assert len(wav) == wav_len, f\" [!] wav lenght: {len(wav)} vs expected: {wav_len}\"\n        wavs.append(wav[:wav_len])\n    return wavs\n\n\ndef synthesis(\n    model,\n    text,\n    CONFIG,\n    use_cuda,\n    speaker_id=None,\n    style_wav=None,\n    style_text=None,\n    use_griffin_lim=False,\n    do_trim_silence=False,\n    d_vector=None,\n    language_id=None,\n):\n    \"\"\"Synthesize voice for the given text using Griffin-Lim vocoder or just compute output features to be passed to\n    the vocoder model.\n\n    Args:\n        model (TTS.tts.models):\n            The TTS model to synthesize audio with.\n\n        text (str):\n            The input text to convert to speech.\n\n        CONFIG (Coqpit):\n            Model configuration.\n\n        use_cuda (bool):\n            Enable/disable CUDA.\n\n        speaker_id (int):\n            Speaker ID passed to the speaker embedding layer in multi-speaker model. Defaults to None.\n\n        style_wav (str | Dict[str, float]):\n            Path or tensor to/of a waveform used for computing the style embedding based on GST or Capacitron.\n            Defaults to None, meaning that Capacitron models will sample from the prior distribution to\n            generate random but realistic prosody.\n\n        style_text (str):\n            Transcription of style_wav for Capacitron models. Defaults to None.\n\n        enable_eos_bos_chars (bool):\n            enable special chars for end of sentence and start of sentence. Defaults to False.\n\n        do_trim_silence (bool):\n            trim silence after synthesis. Defaults to False.\n\n        d_vector (torch.Tensor):\n            d-vector for multi-speaker models in share :math:`[1, D]`. Defaults to None.\n\n        language_id (int):\n            Language ID passed to the language embedding layer in multi-langual model. Defaults to None.\n    \"\"\"\n    # device\n    device = next(model.parameters()).device\n    if use_cuda:\n        device = \"cuda\"\n\n    # GST or Capacitron processing\n    # TODO: need to handle the case of setting both gst and capacitron to true somewhere\n    style_mel = None\n    if CONFIG.has(\"gst\") and CONFIG.gst and style_wav is not None:\n        if isinstance(style_wav, dict):\n            style_mel = style_wav\n        else:\n            style_mel = compute_style_mel(style_wav, model.ap, device=device)\n\n    if CONFIG.has(\"capacitron_vae\") and CONFIG.use_capacitron_vae and style_wav is not None:\n        style_mel = compute_style_mel(style_wav, model.ap, device=device)\n        style_mel = style_mel.transpose(1, 2)  # [1, time, depth]\n\n    language_name = None\n    if language_id is not None:\n        language = [k for k, v in model.language_manager.name_to_id.items() if v == language_id]\n        assert len(language) == 1, \"language_id must be a valid language\"\n        language_name = language[0]\n\n    # convert text to sequence of token IDs\n    text_inputs = np.asarray(\n        model.tokenizer.text_to_ids(text, language=language_name),\n        dtype=np.int32,\n    )\n    # pass tensors to backend\n    if speaker_id is not None:\n        speaker_id = id_to_torch(speaker_id, device=device)\n\n    if d_vector is not None:\n        d_vector = embedding_to_torch(d_vector, device=device)\n\n    if language_id is not None:\n        language_id = id_to_torch(language_id, device=device)\n\n    if not isinstance(style_mel, dict):\n        # GST or Capacitron style mel\n        style_mel = numpy_to_torch(style_mel, torch.float, device=device)\n        if style_text is not None:\n            style_text = np.asarray(\n                model.tokenizer.text_to_ids(style_text, language=language_id),\n                dtype=np.int32,\n            )\n            style_text = numpy_to_torch(style_text, torch.long, device=device)\n            style_text = style_text.unsqueeze(0)\n\n    text_inputs = numpy_to_torch(text_inputs, torch.long, device=device)\n    text_inputs = text_inputs.unsqueeze(0)\n    # synthesize voice\n    outputs = run_model_torch(\n        model,\n        text_inputs,\n        speaker_id,\n        style_mel,\n        style_text,\n        d_vector=d_vector,\n        language_id=language_id,\n    )\n    model_outputs = outputs[\"model_outputs\"]\n    model_outputs = model_outputs[0].data.cpu().numpy()\n    alignments = outputs[\"alignments\"]\n\n    # convert outputs to numpy\n    # plot results\n    wav = None\n    model_outputs = model_outputs.squeeze()\n    if model_outputs.ndim == 2:  # [T, C_spec]\n        if use_griffin_lim:\n            wav = inv_spectrogram(model_outputs, model.ap, CONFIG)\n            # trim silence\n            if do_trim_silence:\n                wav = trim_silence(wav, model.ap)\n    else:  # [T,]\n        wav = model_outputs\n    return_dict = {\n        \"wav\": wav,\n        \"alignments\": alignments,\n        \"text_inputs\": text_inputs,\n        \"outputs\": outputs,\n    }\n    return return_dict\n\n\ndef transfer_voice(\n    model,\n    CONFIG,\n    use_cuda,\n    reference_wav,\n    speaker_id=None,\n    d_vector=None,\n    reference_speaker_id=None,\n    reference_d_vector=None,\n    do_trim_silence=False,\n    use_griffin_lim=False,\n):\n    \"\"\"Synthesize voice for the given text using Griffin-Lim vocoder or just compute output features to be passed to\n    the vocoder model.\n\n    Args:\n        model (TTS.tts.models):\n            The TTS model to synthesize audio with.\n\n        CONFIG (Coqpit):\n            Model configuration.\n\n        use_cuda (bool):\n            Enable/disable CUDA.\n\n        reference_wav (str):\n            Path of reference_wav to be used to voice conversion.\n\n        speaker_id (int):\n            Speaker ID passed to the speaker embedding layer in multi-speaker model. Defaults to None.\n\n        d_vector (torch.Tensor):\n            d-vector for multi-speaker models in share :math:`[1, D]`. Defaults to None.\n\n        reference_speaker_id (int):\n            Reference Speaker ID passed to the speaker embedding layer in multi-speaker model. Defaults to None.\n\n        reference_d_vector (torch.Tensor):\n            Reference d-vector for multi-speaker models in share :math:`[1, D]`. Defaults to None.\n\n        enable_eos_bos_chars (bool):\n            enable special chars for end of sentence and start of sentence. Defaults to False.\n\n        do_trim_silence (bool):\n            trim silence after synthesis. Defaults to False.\n    \"\"\"\n    # device\n    device = next(model.parameters()).device\n    if use_cuda:\n        device = \"cuda\"\n\n    # pass tensors to backend\n    if speaker_id is not None:\n        speaker_id = id_to_torch(speaker_id, device=device)\n\n    if d_vector is not None:\n        d_vector = embedding_to_torch(d_vector, device=device)\n\n    if reference_d_vector is not None:\n        reference_d_vector = embedding_to_torch(reference_d_vector, device=device)\n\n    # load reference_wav audio\n    reference_wav = embedding_to_torch(\n        model.ap.load_wav(\n            reference_wav, sr=model.args.encoder_sample_rate if model.args.encoder_sample_rate else model.ap.sample_rate\n        ),\n        device=device,\n    )\n\n    if hasattr(model, \"module\"):\n        _func = model.module.inference_voice_conversion\n    else:\n        _func = model.inference_voice_conversion\n    model_outputs = _func(reference_wav, speaker_id, d_vector, reference_speaker_id, reference_d_vector)\n\n    # convert outputs to numpy\n    # plot results\n    wav = None\n    model_outputs = model_outputs.squeeze()\n    if model_outputs.ndim == 2:  # [T, C_spec]\n        if use_griffin_lim:\n            wav = inv_spectrogram(model_outputs, model.ap, CONFIG)\n            # trim silence\n            if do_trim_silence:\n                wav = trim_silence(wav, model.ap)\n    else:  # [T,]\n        wav = model_outputs\n\n    return wav\n", "TTS/tts/utils/measures.py": "def alignment_diagonal_score(alignments, binary=False):\n    \"\"\"\n    Compute how diagonal alignment predictions are. It is useful\n    to measure the alignment consistency of a model\n    Args:\n        alignments (torch.Tensor): batch of alignments.\n        binary (bool): if True, ignore scores and consider attention\n        as a binary mask.\n    Shape:\n        - alignments : :math:`[B, T_de, T_en]`\n    \"\"\"\n    maxs = alignments.max(dim=1)[0]\n    if binary:\n        maxs[maxs > 0] = 1\n    return maxs.mean(dim=1).mean(dim=0).item()\n", "TTS/tts/utils/speakers.py": "import json\nimport os\nfrom typing import Any, Dict, List, Union\n\nimport fsspec\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\n\nfrom TTS.config import get_from_config_or_model_args_with_default\nfrom TTS.tts.utils.managers import EmbeddingManager\n\n\nclass SpeakerManager(EmbeddingManager):\n    \"\"\"Manage the speakers for multi-speaker \ud83d\udc38TTS models. Load a datafile and parse the information\n    in a way that can be queried by speaker or clip.\n\n    There are 3 different scenarios considered:\n\n    1. Models using speaker embedding layers. The datafile only maps speaker names to ids used by the embedding layer.\n    2. Models using d-vectors. The datafile includes a dictionary in the following format.\n\n    ::\n\n        {\n            'clip_name.wav':{\n                'name': 'speakerA',\n                'embedding'[<d_vector_values>]\n            },\n            ...\n        }\n\n\n    3. Computing the d-vectors by the speaker encoder. It loads the speaker encoder model and\n    computes the d-vectors for a given clip or speaker.\n\n    Args:\n        d_vectors_file_path (str, optional): Path to the metafile including x vectors. Defaults to \"\".\n        speaker_id_file_path (str, optional): Path to the metafile that maps speaker names to ids used by\n        TTS models. Defaults to \"\".\n        encoder_model_path (str, optional): Path to the speaker encoder model file. Defaults to \"\".\n        encoder_config_path (str, optional): Path to the spealer encoder config file. Defaults to \"\".\n\n    Examples:\n        >>> # load audio processor and speaker encoder\n        >>> ap = AudioProcessor(**config.audio)\n        >>> manager = SpeakerManager(encoder_model_path=encoder_model_path, encoder_config_path=encoder_config_path)\n        >>> # load a sample audio and compute embedding\n        >>> waveform = ap.load_wav(sample_wav_path)\n        >>> mel = ap.melspectrogram(waveform)\n        >>> d_vector = manager.compute_embeddings(mel.T)\n    \"\"\"\n\n    def __init__(\n        self,\n        data_items: List[List[Any]] = None,\n        d_vectors_file_path: str = \"\",\n        speaker_id_file_path: str = \"\",\n        encoder_model_path: str = \"\",\n        encoder_config_path: str = \"\",\n        use_cuda: bool = False,\n    ):\n        super().__init__(\n            embedding_file_path=d_vectors_file_path,\n            id_file_path=speaker_id_file_path,\n            encoder_model_path=encoder_model_path,\n            encoder_config_path=encoder_config_path,\n            use_cuda=use_cuda,\n        )\n\n        if data_items:\n            self.set_ids_from_data(data_items, parse_key=\"speaker_name\")\n\n    @property\n    def num_speakers(self):\n        return len(self.name_to_id)\n\n    @property\n    def speaker_names(self):\n        return list(self.name_to_id.keys())\n\n    def get_speakers(self) -> List:\n        return self.name_to_id\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\", samples: Union[List[List], List[Dict]] = None) -> \"SpeakerManager\":\n        \"\"\"Initialize a speaker manager from config\n\n        Args:\n            config (Coqpit): Config object.\n            samples (Union[List[List], List[Dict]], optional): List of data samples to parse out the speaker names.\n                Defaults to None.\n\n        Returns:\n            SpeakerEncoder: Speaker encoder object.\n        \"\"\"\n        speaker_manager = None\n        if get_from_config_or_model_args_with_default(config, \"use_speaker_embedding\", False):\n            if samples:\n                speaker_manager = SpeakerManager(data_items=samples)\n            if get_from_config_or_model_args_with_default(config, \"speaker_file\", None):\n                speaker_manager = SpeakerManager(\n                    speaker_id_file_path=get_from_config_or_model_args_with_default(config, \"speaker_file\", None)\n                )\n            if get_from_config_or_model_args_with_default(config, \"speakers_file\", None):\n                speaker_manager = SpeakerManager(\n                    speaker_id_file_path=get_from_config_or_model_args_with_default(config, \"speakers_file\", None)\n                )\n\n        if get_from_config_or_model_args_with_default(config, \"use_d_vector_file\", False):\n            speaker_manager = SpeakerManager()\n            if get_from_config_or_model_args_with_default(config, \"d_vector_file\", None):\n                speaker_manager = SpeakerManager(\n                    d_vectors_file_path=get_from_config_or_model_args_with_default(config, \"d_vector_file\", None)\n                )\n        return speaker_manager\n\n\ndef _set_file_path(path):\n    \"\"\"Find the speakers.json under the given path or the above it.\n    Intended to band aid the different paths returned in restored and continued training.\"\"\"\n    path_restore = os.path.join(os.path.dirname(path), \"speakers.json\")\n    path_continue = os.path.join(path, \"speakers.json\")\n    fs = fsspec.get_mapper(path).fs\n    if fs.exists(path_restore):\n        return path_restore\n    if fs.exists(path_continue):\n        return path_continue\n    raise FileNotFoundError(f\" [!] `speakers.json` not found in {path}\")\n\n\ndef load_speaker_mapping(out_path):\n    \"\"\"Loads speaker mapping if already present.\"\"\"\n    if os.path.splitext(out_path)[1] == \".json\":\n        json_file = out_path\n    else:\n        json_file = _set_file_path(out_path)\n    with fsspec.open(json_file, \"r\") as f:\n        return json.load(f)\n\n\ndef save_speaker_mapping(out_path, speaker_mapping):\n    \"\"\"Saves speaker mapping if not yet present.\"\"\"\n    if out_path is not None:\n        speakers_json_path = _set_file_path(out_path)\n        with fsspec.open(speakers_json_path, \"w\") as f:\n            json.dump(speaker_mapping, f, indent=4)\n\n\ndef get_speaker_manager(c: Coqpit, data: List = None, restore_path: str = None, out_path: str = None) -> SpeakerManager:\n    \"\"\"Initiate a `SpeakerManager` instance by the provided config.\n\n    Args:\n        c (Coqpit): Model configuration.\n        restore_path (str): Path to a previous training folder.\n        data (List): Data samples used in training to infer speakers from. It must be provided if speaker embedding\n            layers is used. Defaults to None.\n        out_path (str, optional): Save the generated speaker IDs to a output path. Defaults to None.\n\n    Returns:\n        SpeakerManager: initialized and ready to use instance.\n    \"\"\"\n    speaker_manager = SpeakerManager()\n    if c.use_speaker_embedding:\n        if data is not None:\n            speaker_manager.set_ids_from_data(data, parse_key=\"speaker_name\")\n        if restore_path:\n            speakers_file = _set_file_path(restore_path)\n            # restoring speaker manager from a previous run.\n            if c.use_d_vector_file:\n                # restore speaker manager with the embedding file\n                if not os.path.exists(speakers_file):\n                    print(\"WARNING: speakers.json was not found in restore_path, trying to use CONFIG.d_vector_file\")\n                    if not os.path.exists(c.d_vector_file):\n                        raise RuntimeError(\n                            \"You must copy the file speakers.json to restore_path, or set a valid file in CONFIG.d_vector_file\"\n                        )\n                    speaker_manager.load_embeddings_from_file(c.d_vector_file)\n                speaker_manager.load_embeddings_from_file(speakers_file)\n            elif not c.use_d_vector_file:  # restor speaker manager with speaker ID file.\n                speaker_ids_from_data = speaker_manager.name_to_id\n                speaker_manager.load_ids_from_file(speakers_file)\n                assert all(\n                    speaker in speaker_manager.name_to_id for speaker in speaker_ids_from_data\n                ), \" [!] You cannot introduce new speakers to a pre-trained model.\"\n        elif c.use_d_vector_file and c.d_vector_file:\n            # new speaker manager with external speaker embeddings.\n            speaker_manager.load_embeddings_from_file(c.d_vector_file)\n        elif c.use_d_vector_file and not c.d_vector_file:\n            raise \"use_d_vector_file is True, so you need pass a external speaker embedding file.\"\n        elif c.use_speaker_embedding and \"speakers_file\" in c and c.speakers_file:\n            # new speaker manager with speaker IDs file.\n            speaker_manager.load_ids_from_file(c.speakers_file)\n\n        if speaker_manager.num_speakers > 0:\n            print(\n                \" > Speaker manager is loaded with {} speakers: {}\".format(\n                    speaker_manager.num_speakers, \", \".join(speaker_manager.name_to_id)\n                )\n            )\n\n        # save file if path is defined\n        if out_path:\n            out_file_path = os.path.join(out_path, \"speakers.json\")\n            print(f\" > Saving `speakers.json` to {out_file_path}.\")\n            if c.use_d_vector_file and c.d_vector_file:\n                speaker_manager.save_embeddings_to_file(out_file_path)\n            else:\n                speaker_manager.save_ids_to_file(out_file_path)\n    return speaker_manager\n\n\ndef get_speaker_balancer_weights(items: list):\n    speaker_names = np.array([item[\"speaker_name\"] for item in items])\n    unique_speaker_names = np.unique(speaker_names).tolist()\n    speaker_ids = [unique_speaker_names.index(l) for l in speaker_names]\n    speaker_count = np.array([len(np.where(speaker_names == l)[0]) for l in unique_speaker_names])\n    weight_speaker = 1.0 / speaker_count\n    dataset_samples_weight = np.array([weight_speaker[l] for l in speaker_ids])\n    # normalize\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n    return torch.from_numpy(dataset_samples_weight).float()\n", "TTS/tts/utils/__init__.py": "", "TTS/tts/utils/languages.py": "import os\nfrom typing import Any, Dict, List\n\nimport fsspec\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\n\nfrom TTS.config import check_config_and_model_args\nfrom TTS.tts.utils.managers import BaseIDManager\n\n\nclass LanguageManager(BaseIDManager):\n    \"\"\"Manage the languages for multi-lingual \ud83d\udc38TTS models. Load a datafile and parse the information\n    in a way that can be queried by language.\n\n    Args:\n        language_ids_file_path (str, optional): Path to the metafile that maps language names to ids used by\n        TTS models. Defaults to \"\".\n        config (Coqpit, optional): Coqpit config that contains the language information in the datasets filed.\n        Defaults to None.\n\n    Examples:\n        >>> manager = LanguageManager(language_ids_file_path=language_ids_file_path)\n        >>> language_id_mapper = manager.language_ids\n    \"\"\"\n\n    def __init__(\n        self,\n        language_ids_file_path: str = \"\",\n        config: Coqpit = None,\n    ):\n        super().__init__(id_file_path=language_ids_file_path)\n\n        if config:\n            self.set_language_ids_from_config(config)\n\n    @property\n    def num_languages(self) -> int:\n        return len(list(self.name_to_id.keys()))\n\n    @property\n    def language_names(self) -> List:\n        return list(self.name_to_id.keys())\n\n    @staticmethod\n    def parse_language_ids_from_config(c: Coqpit) -> Dict:\n        \"\"\"Set language id from config.\n\n        Args:\n            c (Coqpit): Config\n\n        Returns:\n            Tuple[Dict, int]: Language ID mapping and the number of languages.\n        \"\"\"\n        languages = set({})\n        for dataset in c.datasets:\n            if \"language\" in dataset:\n                languages.add(dataset[\"language\"])\n            else:\n                raise ValueError(f\"Dataset {dataset['name']} has no language specified.\")\n        return {name: i for i, name in enumerate(sorted(list(languages)))}\n\n    def set_language_ids_from_config(self, c: Coqpit) -> None:\n        \"\"\"Set language IDs from config samples.\n\n        Args:\n            c (Coqpit): Config.\n        \"\"\"\n        self.name_to_id = self.parse_language_ids_from_config(c)\n\n    @staticmethod\n    def parse_ids_from_data(items: List, parse_key: str) -> Any:\n        raise NotImplementedError\n\n    def set_ids_from_data(self, items: List, parse_key: str) -> Any:\n        raise NotImplementedError\n\n    def save_ids_to_file(self, file_path: str) -> None:\n        \"\"\"Save language IDs to a json file.\n\n        Args:\n            file_path (str): Path to the output file.\n        \"\"\"\n        self._save_json(file_path, self.name_to_id)\n\n    @staticmethod\n    def init_from_config(config: Coqpit) -> \"LanguageManager\":\n        \"\"\"Initialize the language manager from a Coqpit config.\n\n        Args:\n            config (Coqpit): Coqpit config.\n        \"\"\"\n        language_manager = None\n        if check_config_and_model_args(config, \"use_language_embedding\", True):\n            if config.get(\"language_ids_file\", None):\n                language_manager = LanguageManager(language_ids_file_path=config.language_ids_file)\n            language_manager = LanguageManager(config=config)\n        return language_manager\n\n\ndef _set_file_path(path):\n    \"\"\"Find the language_ids.json under the given path or the above it.\n    Intended to band aid the different paths returned in restored and continued training.\"\"\"\n    path_restore = os.path.join(os.path.dirname(path), \"language_ids.json\")\n    path_continue = os.path.join(path, \"language_ids.json\")\n    fs = fsspec.get_mapper(path).fs\n    if fs.exists(path_restore):\n        return path_restore\n    if fs.exists(path_continue):\n        return path_continue\n    return None\n\n\ndef get_language_balancer_weights(items: list):\n    language_names = np.array([item[\"language\"] for item in items])\n    unique_language_names = np.unique(language_names).tolist()\n    language_ids = [unique_language_names.index(l) for l in language_names]\n    language_count = np.array([len(np.where(language_names == l)[0]) for l in unique_language_names])\n    weight_language = 1.0 / language_count\n    # get weight for each sample\n    dataset_samples_weight = np.array([weight_language[l] for l in language_ids])\n    # normalize\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n    return torch.from_numpy(dataset_samples_weight).float()\n", "TTS/tts/utils/visual.py": "import librosa\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom matplotlib.colors import LogNorm\n\nmatplotlib.use(\"Agg\")\n\n\ndef plot_alignment(alignment, info=None, fig_size=(16, 10), title=None, output_fig=False, plot_log=False):\n    if isinstance(alignment, torch.Tensor):\n        alignment_ = alignment.detach().cpu().numpy().squeeze()\n    else:\n        alignment_ = alignment\n    alignment_ = alignment_.astype(np.float32) if alignment_.dtype == np.float16 else alignment_\n    fig, ax = plt.subplots(figsize=fig_size)\n    im = ax.imshow(\n        alignment_.T, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", norm=LogNorm() if plot_log else None\n    )\n    fig.colorbar(im, ax=ax)\n    xlabel = \"Decoder timestep\"\n    if info is not None:\n        xlabel += \"\\n\\n\" + info\n    plt.xlabel(xlabel)\n    plt.ylabel(\"Encoder timestep\")\n    # plt.yticks(range(len(text)), list(text))\n    plt.tight_layout()\n    if title is not None:\n        plt.title(title)\n    if not output_fig:\n        plt.close()\n    return fig\n\n\ndef plot_spectrogram(spectrogram, ap=None, fig_size=(16, 10), output_fig=False):\n    if isinstance(spectrogram, torch.Tensor):\n        spectrogram_ = spectrogram.detach().cpu().numpy().squeeze().T\n    else:\n        spectrogram_ = spectrogram.T\n    spectrogram_ = spectrogram_.astype(np.float32) if spectrogram_.dtype == np.float16 else spectrogram_\n    if ap is not None:\n        spectrogram_ = ap.denormalize(spectrogram_)  # pylint: disable=protected-access\n    fig = plt.figure(figsize=fig_size)\n    plt.imshow(spectrogram_, aspect=\"auto\", origin=\"lower\")\n    plt.colorbar()\n    plt.tight_layout()\n    if not output_fig:\n        plt.close()\n    return fig\n\n\ndef plot_pitch(pitch, spectrogram, ap=None, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot pitch curves on top of the spectrogram.\n\n    Args:\n        pitch (np.array): Pitch values.\n        spectrogram (np.array): Spectrogram values.\n\n    Shapes:\n        pitch: :math:`(T,)`\n        spec: :math:`(C, T)`\n    \"\"\"\n\n    if isinstance(spectrogram, torch.Tensor):\n        spectrogram_ = spectrogram.detach().cpu().numpy().squeeze().T\n    else:\n        spectrogram_ = spectrogram.T\n    spectrogram_ = spectrogram_.astype(np.float32) if spectrogram_.dtype == np.float16 else spectrogram_\n    if ap is not None:\n        spectrogram_ = ap.denormalize(spectrogram_)  # pylint: disable=protected-access\n\n    old_fig_size = plt.rcParams[\"figure.figsize\"]\n    if fig_size is not None:\n        plt.rcParams[\"figure.figsize\"] = fig_size\n\n    fig, ax = plt.subplots()\n\n    ax.imshow(spectrogram_, aspect=\"auto\", origin=\"lower\")\n    ax.set_xlabel(\"time\")\n    ax.set_ylabel(\"spec_freq\")\n\n    ax2 = ax.twinx()\n    ax2.plot(pitch, linewidth=5.0, color=\"red\")\n    ax2.set_ylabel(\"F0\")\n\n    plt.rcParams[\"figure.figsize\"] = old_fig_size\n    if not output_fig:\n        plt.close()\n    return fig\n\n\ndef plot_avg_pitch(pitch, chars, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot pitch curves on top of the input characters.\n\n    Args:\n        pitch (np.array): Pitch values.\n        chars (str): Characters to place to the x-axis.\n\n    Shapes:\n        pitch: :math:`(T,)`\n    \"\"\"\n    old_fig_size = plt.rcParams[\"figure.figsize\"]\n    if fig_size is not None:\n        plt.rcParams[\"figure.figsize\"] = fig_size\n\n    fig, ax = plt.subplots()\n\n    x = np.array(range(len(chars)))\n    my_xticks = chars\n    plt.xticks(x, my_xticks)\n\n    ax.set_xlabel(\"characters\")\n    ax.set_ylabel(\"freq\")\n\n    ax2 = ax.twinx()\n    ax2.plot(pitch, linewidth=5.0, color=\"red\")\n    ax2.set_ylabel(\"F0\")\n\n    plt.rcParams[\"figure.figsize\"] = old_fig_size\n    if not output_fig:\n        plt.close()\n    return fig\n\n\ndef plot_avg_energy(energy, chars, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot energy curves on top of the input characters.\n\n    Args:\n        energy (np.array): energy values.\n        chars (str): Characters to place to the x-axis.\n\n    Shapes:\n        energy: :math:`(T,)`\n    \"\"\"\n    old_fig_size = plt.rcParams[\"figure.figsize\"]\n    if fig_size is not None:\n        plt.rcParams[\"figure.figsize\"] = fig_size\n\n    fig, ax = plt.subplots()\n\n    x = np.array(range(len(chars)))\n    my_xticks = chars\n    plt.xticks(x, my_xticks)\n\n    ax.set_xlabel(\"characters\")\n    ax.set_ylabel(\"freq\")\n\n    ax2 = ax.twinx()\n    ax2.plot(energy, linewidth=5.0, color=\"red\")\n    ax2.set_ylabel(\"energy\")\n\n    plt.rcParams[\"figure.figsize\"] = old_fig_size\n    if not output_fig:\n        plt.close()\n    return fig\n\n\ndef visualize(\n    alignment,\n    postnet_output,\n    text,\n    hop_length,\n    CONFIG,\n    tokenizer,\n    stop_tokens=None,\n    decoder_output=None,\n    output_path=None,\n    figsize=(8, 24),\n    output_fig=False,\n):\n    \"\"\"Intended to be used in Notebooks.\"\"\"\n\n    if decoder_output is not None:\n        num_plot = 4\n    else:\n        num_plot = 3\n\n    label_fontsize = 16\n    fig = plt.figure(figsize=figsize)\n\n    plt.subplot(num_plot, 1, 1)\n    plt.imshow(alignment.T, aspect=\"auto\", origin=\"lower\", interpolation=None)\n    plt.xlabel(\"Decoder timestamp\", fontsize=label_fontsize)\n    plt.ylabel(\"Encoder timestamp\", fontsize=label_fontsize)\n    # compute phoneme representation and back\n    if CONFIG.use_phonemes:\n        seq = tokenizer.text_to_ids(text)\n        text = tokenizer.ids_to_text(seq)\n        print(text)\n    plt.yticks(range(len(text)), list(text))\n    plt.colorbar()\n\n    if stop_tokens is not None:\n        # plot stopnet predictions\n        plt.subplot(num_plot, 1, 2)\n        plt.plot(range(len(stop_tokens)), list(stop_tokens))\n\n    # plot postnet spectrogram\n    plt.subplot(num_plot, 1, 3)\n    librosa.display.specshow(\n        postnet_output.T,\n        sr=CONFIG.audio[\"sample_rate\"],\n        hop_length=hop_length,\n        x_axis=\"time\",\n        y_axis=\"linear\",\n        fmin=CONFIG.audio[\"mel_fmin\"],\n        fmax=CONFIG.audio[\"mel_fmax\"],\n    )\n\n    plt.xlabel(\"Time\", fontsize=label_fontsize)\n    plt.ylabel(\"Hz\", fontsize=label_fontsize)\n    plt.tight_layout()\n    plt.colorbar()\n\n    if decoder_output is not None:\n        plt.subplot(num_plot, 1, 4)\n        librosa.display.specshow(\n            decoder_output.T,\n            sr=CONFIG.audio[\"sample_rate\"],\n            hop_length=hop_length,\n            x_axis=\"time\",\n            y_axis=\"linear\",\n            fmin=CONFIG.audio[\"mel_fmin\"],\n            fmax=CONFIG.audio[\"mel_fmax\"],\n        )\n        plt.xlabel(\"Time\", fontsize=label_fontsize)\n        plt.ylabel(\"Hz\", fontsize=label_fontsize)\n        plt.tight_layout()\n        plt.colorbar()\n\n    if output_path:\n        print(output_path)\n        fig.savefig(output_path)\n        plt.close()\n\n    if not output_fig:\n        plt.close()\n", "TTS/tts/utils/text/punctuation.py": "import collections\nimport re\nfrom enum import Enum\n\nimport six\n\n_DEF_PUNCS = ';:,.!?\u00a1\u00bf\u2014\u2026\"\u00ab\u00bb\u201c\u201d'\n\n_PUNC_IDX = collections.namedtuple(\"_punc_index\", [\"punc\", \"position\"])\n\n\nclass PuncPosition(Enum):\n    \"\"\"Enum for the punctuations positions\"\"\"\n\n    BEGIN = 0\n    END = 1\n    MIDDLE = 2\n\n\nclass Punctuation:\n    \"\"\"Handle punctuations in text.\n\n    Just strip punctuations from text or strip and restore them later.\n\n    Args:\n        puncs (str): The punctuations to be processed. Defaults to `_DEF_PUNCS`.\n\n    Example:\n        >>> punc = Punctuation()\n        >>> punc.strip(\"This is. example !\")\n        'This is example'\n\n        >>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n        >>> ' '.join(text_striped)\n        'This is example'\n\n        >>> text_restored = punc.restore(text_striped, punc_map)\n        >>> text_restored[0]\n        'This is. example !'\n    \"\"\"\n\n    def __init__(self, puncs: str = _DEF_PUNCS):\n        self.puncs = puncs\n\n    @staticmethod\n    def default_puncs():\n        \"\"\"Return default set of punctuations.\"\"\"\n        return _DEF_PUNCS\n\n    @property\n    def puncs(self):\n        return self._puncs\n\n    @puncs.setter\n    def puncs(self, value):\n        if not isinstance(value, six.string_types):\n            raise ValueError(\"[!] Punctuations must be of type str.\")\n        self._puncs = \"\".join(list(dict.fromkeys(list(value))))  # remove duplicates without changing the oreder\n        self.puncs_regular_exp = re.compile(rf\"(\\s*[{re.escape(self._puncs)}]+\\s*)+\")\n\n    def strip(self, text):\n        \"\"\"Remove all the punctuations by replacing with `space`.\n\n        Args:\n            text (str): The text to be processed.\n\n        Example::\n\n            \"This is. example !\" -> \"This is example \"\n        \"\"\"\n        return re.sub(self.puncs_regular_exp, \" \", text).rstrip().lstrip()\n\n    def strip_to_restore(self, text):\n        \"\"\"Remove punctuations from text to restore them later.\n\n        Args:\n            text (str): The text to be processed.\n\n        Examples ::\n\n            \"This is. example !\" -> [[\"This is\", \"example\"], [\".\", \"!\"]]\n\n        \"\"\"\n        text, puncs = self._strip_to_restore(text)\n        return text, puncs\n\n    def _strip_to_restore(self, text):\n        \"\"\"Auxiliary method for Punctuation.preserve()\"\"\"\n        matches = list(re.finditer(self.puncs_regular_exp, text))\n        if not matches:\n            return [text], []\n        # the text is only punctuations\n        if len(matches) == 1 and matches[0].group() == text:\n            return [], [_PUNC_IDX(text, PuncPosition.BEGIN)]\n        # build a punctuation map to be used later to restore punctuations\n        puncs = []\n        for match in matches:\n            position = PuncPosition.MIDDLE\n            if match == matches[0] and text.startswith(match.group()):\n                position = PuncPosition.BEGIN\n            elif match == matches[-1] and text.endswith(match.group()):\n                position = PuncPosition.END\n            puncs.append(_PUNC_IDX(match.group(), position))\n        # convert str text to a List[str], each item is separated by a punctuation\n        splitted_text = []\n        for idx, punc in enumerate(puncs):\n            split = text.split(punc.punc)\n            prefix, suffix = split[0], punc.punc.join(split[1:])\n            text = suffix\n            if prefix == \"\":\n                # We don't want to insert an empty string in case of initial punctuation\n                continue\n            splitted_text.append(prefix)\n            # if the text does not end with a punctuation, add it to the last item\n            if idx == len(puncs) - 1 and len(suffix) > 0:\n                splitted_text.append(suffix)\n        return splitted_text, puncs\n\n    @classmethod\n    def restore(cls, text, puncs):\n        \"\"\"Restore punctuation in a text.\n\n        Args:\n            text (str): The text to be processed.\n            puncs (List[str]): The list of punctuations map to be used for restoring.\n\n        Examples ::\n\n            ['This is', 'example'], ['.', '!'] -> \"This is. example!\"\n\n        \"\"\"\n        return cls._restore(text, puncs)\n\n    @classmethod\n    def _restore(cls, text, puncs):  # pylint: disable=too-many-return-statements\n        \"\"\"Auxiliary method for Punctuation.restore()\"\"\"\n        if not puncs:\n            return text\n\n        # nothing have been phonemized, returns the puncs alone\n        if not text:\n            return [\"\".join(m.punc for m in puncs)]\n\n        current = puncs[0]\n\n        if current.position == PuncPosition.BEGIN:\n            return cls._restore([current.punc + text[0]] + text[1:], puncs[1:])\n\n        if current.position == PuncPosition.END:\n            return [text[0] + current.punc] + cls._restore(text[1:], puncs[1:])\n\n        # POSITION == MIDDLE\n        if len(text) == 1:  # pragma: nocover\n            # a corner case where the final part of an intermediate\n            # mark (I) has not been phonemized\n            return cls._restore([text[0] + current.punc], puncs[1:])\n\n        return cls._restore([text[0] + current.punc + text[1]] + text[2:], puncs[1:])\n\n\n# if __name__ == \"__main__\":\n#     punc = Punctuation()\n#     text = \"This is. This is, example!\"\n\n#     print(punc.strip(text))\n\n#     split_text, puncs = punc.strip_to_restore(text)\n#     print(split_text, \" ---- \", puncs)\n\n#     restored_text = punc.restore(split_text, puncs)\n#     print(restored_text)\n", "TTS/tts/utils/text/cmudict.py": "# -*- coding: utf-8 -*-\n\nimport re\n\nVALID_SYMBOLS = [\n    \"AA\",\n    \"AA0\",\n    \"AA1\",\n    \"AA2\",\n    \"AE\",\n    \"AE0\",\n    \"AE1\",\n    \"AE2\",\n    \"AH\",\n    \"AH0\",\n    \"AH1\",\n    \"AH2\",\n    \"AO\",\n    \"AO0\",\n    \"AO1\",\n    \"AO2\",\n    \"AW\",\n    \"AW0\",\n    \"AW1\",\n    \"AW2\",\n    \"AY\",\n    \"AY0\",\n    \"AY1\",\n    \"AY2\",\n    \"B\",\n    \"CH\",\n    \"D\",\n    \"DH\",\n    \"EH\",\n    \"EH0\",\n    \"EH1\",\n    \"EH2\",\n    \"ER\",\n    \"ER0\",\n    \"ER1\",\n    \"ER2\",\n    \"EY\",\n    \"EY0\",\n    \"EY1\",\n    \"EY2\",\n    \"F\",\n    \"G\",\n    \"HH\",\n    \"IH\",\n    \"IH0\",\n    \"IH1\",\n    \"IH2\",\n    \"IY\",\n    \"IY0\",\n    \"IY1\",\n    \"IY2\",\n    \"JH\",\n    \"K\",\n    \"L\",\n    \"M\",\n    \"N\",\n    \"NG\",\n    \"OW\",\n    \"OW0\",\n    \"OW1\",\n    \"OW2\",\n    \"OY\",\n    \"OY0\",\n    \"OY1\",\n    \"OY2\",\n    \"P\",\n    \"R\",\n    \"S\",\n    \"SH\",\n    \"T\",\n    \"TH\",\n    \"UH\",\n    \"UH0\",\n    \"UH1\",\n    \"UH2\",\n    \"UW\",\n    \"UW0\",\n    \"UW1\",\n    \"UW2\",\n    \"V\",\n    \"W\",\n    \"Y\",\n    \"Z\",\n    \"ZH\",\n]\n\n\nclass CMUDict:\n    \"\"\"Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\"\"\"\n\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\"latin-1\") as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n        self._entries = entries\n\n    def __len__(self):\n        return len(self._entries)\n\n    def lookup(self, word):\n        \"\"\"Returns list of ARPAbet pronunciations of the given word.\"\"\"\n        return self._entries.get(word.upper())\n\n    @staticmethod\n    def get_arpabet(word, cmudict, punctuation_symbols):\n        first_symbol, last_symbol = \"\", \"\"\n        if word and word[0] in punctuation_symbols:\n            first_symbol = word[0]\n            word = word[1:]\n        if word and word[-1] in punctuation_symbols:\n            last_symbol = word[-1]\n            word = word[:-1]\n        arpabet = cmudict.lookup(word)\n        if arpabet is not None:\n            return first_symbol + \"{%s}\" % arpabet[0] + last_symbol\n        return first_symbol + word + last_symbol\n\n\n_alt_re = re.compile(r\"\\([0-9]+\\)\")\n\n\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if line and (line[0] >= \"A\" and line[0] <= \"Z\" or line[0] == \"'\"):\n            parts = line.split(\"  \")\n            word = re.sub(_alt_re, \"\", parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:\n                    cmudict[word].append(pronunciation)\n                else:\n                    cmudict[word] = [pronunciation]\n    return cmudict\n\n\ndef _get_pronunciation(s):\n    parts = s.strip().split(\" \")\n    for part in parts:\n        if part not in VALID_SYMBOLS:\n            return None\n    return \" \".join(parts)\n", "TTS/tts/utils/text/characters.py": "from dataclasses import replace\nfrom typing import Dict\n\nfrom TTS.tts.configs.shared_configs import CharactersConfig\n\n\ndef parse_symbols():\n    return {\n        \"pad\": _pad,\n        \"eos\": _eos,\n        \"bos\": _bos,\n        \"characters\": _characters,\n        \"punctuations\": _punctuations,\n        \"phonemes\": _phonemes,\n    }\n\n\n# DEFAULT SET OF GRAPHEMES\n_pad = \"<PAD>\"\n_eos = \"<EOS>\"\n_bos = \"<BOS>\"\n_blank = \"<BLNK>\"  # TODO: check if we need this alongside with PAD\n_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n\n\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iy\u0268\u0289\u026fu\u026a\u028f\u028ae\u00f8\u0258\u0259\u0275\u0264o\u025b\u0153\u025c\u025e\u028c\u0254\u00e6\u0250a\u0276\u0251\u0252\u1d7b\"\n_non_pulmonic_consonants = \"\u0298\u0253\u01c0\u0257\u01c3\u0284\u01c2\u0260\u01c1\u029b\"\n_pulmonic_consonants = \"pbtd\u0288\u0256c\u025fk\u0261q\u0262\u0294\u0274\u014b\u0272\u0273n\u0271m\u0299r\u0280\u2c71\u027e\u027d\u0278\u03b2fv\u03b8\u00f0sz\u0283\u0292\u0282\u0290\u00e7\u029dx\u0263\u03c7\u0281\u0127\u0295h\u0266\u026c\u026e\u028b\u0279\u027bj\u0270l\u026d\u028e\u029f\"\n_suprasegmentals = \"\u02c8\u02cc\u02d0\u02d1\"\n_other_symbols = \"\u028dw\u0265\u029c\u02a2\u02a1\u0255\u0291\u027a\u0267\u02b2\"\n_diacrilics = \"\u025a\u02de\u026b\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\n\n\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n\n    This class only needs a vocabulary dictionary without specifying the characters.\n\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"\n\n    def __init__(self, vocab: Dict, pad: str = None, blank: str = None, bos: str = None, eos: str = None):\n        self.vocab = vocab\n        self.pad = pad\n        self.blank = blank\n        self.bos = bos\n        self.eos = eos\n\n    @property\n    def pad_id(self) -> int:\n        \"\"\"Return the index of the padding character. If the padding character is not specified, return the length\n        of the vocabulary.\"\"\"\n        return self.char_to_id(self.pad) if self.pad else len(self.vocab)\n\n    @property\n    def blank_id(self) -> int:\n        \"\"\"Return the index of the blank character. If the blank character is not specified, return the length of\n        the vocabulary.\"\"\"\n        return self.char_to_id(self.blank) if self.blank else len(self.vocab)\n\n    @property\n    def bos_id(self) -> int:\n        \"\"\"Return the index of the bos character. If the bos character is not specified, return the length of the\n        vocabulary.\"\"\"\n        return self.char_to_id(self.bos) if self.bos else len(self.vocab)\n\n    @property\n    def eos_id(self) -> int:\n        \"\"\"Return the index of the eos character. If the eos character is not specified, return the length of the\n        vocabulary.\"\"\"\n        return self.char_to_id(self.eos) if self.eos else len(self.vocab)\n\n    @property\n    def vocab(self):\n        \"\"\"Return the vocabulary dictionary.\"\"\"\n        return self._vocab\n\n    @vocab.setter\n    def vocab(self, vocab):\n        \"\"\"Set the vocabulary dictionary and character mapping dictionaries.\"\"\"\n        self._vocab, self._char_to_id, self._id_to_char = None, None, None\n        if vocab is not None:\n            self._vocab = vocab\n            self._char_to_id = {char: idx for idx, char in enumerate(self._vocab)}\n            self._id_to_char = {\n                idx: char for idx, char in enumerate(self._vocab)  # pylint: disable=unnecessary-comprehension\n            }\n\n    @staticmethod\n    def init_from_config(config, **kwargs):\n        \"\"\"Initialize from the given config.\"\"\"\n        if config.characters is not None and \"vocab_dict\" in config.characters and config.characters.vocab_dict:\n            return (\n                BaseVocabulary(\n                    config.characters.vocab_dict,\n                    config.characters.pad,\n                    config.characters.blank,\n                    config.characters.bos,\n                    config.characters.eos,\n                ),\n                config,\n            )\n        return BaseVocabulary(**kwargs), config\n\n    def to_config(self) -> \"CharactersConfig\":\n        return CharactersConfig(\n            vocab_dict=self._vocab,\n            pad=self.pad,\n            eos=self.eos,\n            bos=self.bos,\n            blank=self.blank,\n            is_unique=False,\n            is_sorted=False,\n        )\n\n    @property\n    def num_chars(self):\n        \"\"\"Return number of tokens in the vocabulary.\"\"\"\n        return len(self._vocab)\n\n    def char_to_id(self, char: str) -> int:\n        \"\"\"Map a character to an token ID.\"\"\"\n        try:\n            return self._char_to_id[char]\n        except KeyError as e:\n            raise KeyError(f\" [!] {repr(char)} is not in the vocabulary.\") from e\n\n    def id_to_char(self, idx: int) -> str:\n        \"\"\"Map an token ID to a character.\"\"\"\n        return self._id_to_char[idx]\n\n\nclass BaseCharacters:\n    \"\"\"\ud83d\udc38BaseCharacters class\n\n        Every new character class should inherit from this.\n\n        Characters are oredered as follows ```[PAD, EOS, BOS, BLANK, CHARACTERS, PUNCTUATIONS]```.\n\n        If you need a custom order, you need to define inherit from this class and override the ```_create_vocab``` method.\n\n        Args:\n            characters (str):\n                Main set of characters to be used in the vocabulary.\n\n            punctuations (str):\n                Characters to be treated as punctuation.\n\n            pad (str):\n                Special padding character that would be ignored by the model.\n\n            eos (str):\n                End of the sentence character.\n\n            bos (str):\n                Beginning of the sentence character.\n\n            blank (str):\n                Optional character used between characters by some models for better prosody.\n\n            is_unique (bool):\n                Remove duplicates from the provided characters. Defaults to True.\n    el\n            is_sorted (bool):\n                Sort the characters in alphabetical order. Only applies to `self.characters`. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        characters: str = None,\n        punctuations: str = None,\n        pad: str = None,\n        eos: str = None,\n        bos: str = None,\n        blank: str = None,\n        is_unique: bool = False,\n        is_sorted: bool = True,\n    ) -> None:\n        self._characters = characters\n        self._punctuations = punctuations\n        self._pad = pad\n        self._eos = eos\n        self._bos = bos\n        self._blank = blank\n        self.is_unique = is_unique\n        self.is_sorted = is_sorted\n        self._create_vocab()\n\n    @property\n    def pad_id(self) -> int:\n        return self.char_to_id(self.pad) if self.pad else len(self.vocab)\n\n    @property\n    def blank_id(self) -> int:\n        return self.char_to_id(self.blank) if self.blank else len(self.vocab)\n\n    @property\n    def eos_id(self) -> int:\n        return self.char_to_id(self.eos) if self.eos else len(self.vocab)\n\n    @property\n    def bos_id(self) -> int:\n        return self.char_to_id(self.bos) if self.bos else len(self.vocab)\n\n    @property\n    def characters(self):\n        return self._characters\n\n    @characters.setter\n    def characters(self, characters):\n        self._characters = characters\n        self._create_vocab()\n\n    @property\n    def punctuations(self):\n        return self._punctuations\n\n    @punctuations.setter\n    def punctuations(self, punctuations):\n        self._punctuations = punctuations\n        self._create_vocab()\n\n    @property\n    def pad(self):\n        return self._pad\n\n    @pad.setter\n    def pad(self, pad):\n        self._pad = pad\n        self._create_vocab()\n\n    @property\n    def eos(self):\n        return self._eos\n\n    @eos.setter\n    def eos(self, eos):\n        self._eos = eos\n        self._create_vocab()\n\n    @property\n    def bos(self):\n        return self._bos\n\n    @bos.setter\n    def bos(self, bos):\n        self._bos = bos\n        self._create_vocab()\n\n    @property\n    def blank(self):\n        return self._blank\n\n    @blank.setter\n    def blank(self, blank):\n        self._blank = blank\n        self._create_vocab()\n\n    @property\n    def vocab(self):\n        return self._vocab\n\n    @vocab.setter\n    def vocab(self, vocab):\n        self._vocab = vocab\n        self._char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        self._id_to_char = {\n            idx: char for idx, char in enumerate(self.vocab)  # pylint: disable=unnecessary-comprehension\n        }\n\n    @property\n    def num_chars(self):\n        return len(self._vocab)\n\n    def _create_vocab(self):\n        _vocab = self._characters\n        if self.is_unique:\n            _vocab = list(set(_vocab))\n        if self.is_sorted:\n            _vocab = sorted(_vocab)\n        _vocab = list(_vocab)\n        _vocab = [self._blank] + _vocab if self._blank is not None and len(self._blank) > 0 else _vocab\n        _vocab = [self._bos] + _vocab if self._bos is not None and len(self._bos) > 0 else _vocab\n        _vocab = [self._eos] + _vocab if self._eos is not None and len(self._eos) > 0 else _vocab\n        _vocab = [self._pad] + _vocab if self._pad is not None and len(self._pad) > 0 else _vocab\n        self.vocab = _vocab + list(self._punctuations)\n        if self.is_unique:\n            duplicates = {x for x in self.vocab if self.vocab.count(x) > 1}\n            assert (\n                len(self.vocab) == len(self._char_to_id) == len(self._id_to_char)\n            ), f\" [!] There are duplicate characters in the character set. {duplicates}\"\n\n    def char_to_id(self, char: str) -> int:\n        try:\n            return self._char_to_id[char]\n        except KeyError as e:\n            raise KeyError(f\" [!] {repr(char)} is not in the vocabulary.\") from e\n\n    def id_to_char(self, idx: int) -> str:\n        return self._id_to_char[idx]\n\n    def print_log(self, level: int = 0):\n        \"\"\"\n        Prints the vocabulary in a nice format.\n        \"\"\"\n        indent = \"\\t\" * level\n        print(f\"{indent}| > Characters: {self._characters}\")\n        print(f\"{indent}| > Punctuations: {self._punctuations}\")\n        print(f\"{indent}| > Pad: {self._pad}\")\n        print(f\"{indent}| > EOS: {self._eos}\")\n        print(f\"{indent}| > BOS: {self._bos}\")\n        print(f\"{indent}| > Blank: {self._blank}\")\n        print(f\"{indent}| > Vocab: {self.vocab}\")\n        print(f\"{indent}| > Num chars: {self.num_chars}\")\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\"):  # pylint: disable=unused-argument\n        \"\"\"Init your character class from a config.\n\n        Implement this method for your subclass.\n        \"\"\"\n        # use character set from config\n        if config.characters is not None:\n            return BaseCharacters(**config.characters), config\n        # return default character set\n        characters = BaseCharacters()\n        new_config = replace(config, characters=characters.to_config())\n        return characters, new_config\n\n    def to_config(self) -> \"CharactersConfig\":\n        return CharactersConfig(\n            characters=self._characters,\n            punctuations=self._punctuations,\n            pad=self._pad,\n            eos=self._eos,\n            bos=self._bos,\n            blank=self._blank,\n            is_unique=self.is_unique,\n            is_sorted=self.is_sorted,\n        )\n\n\nclass IPAPhonemes(BaseCharacters):\n    \"\"\"\ud83d\udc38IPAPhonemes class to manage `TTS.tts` model vocabulary\n\n    Intended to be used with models using IPAPhonemes as input.\n    It uses system defaults for the undefined class arguments.\n\n    Args:\n        characters (str):\n            Main set of case-sensitive characters to be used in the vocabulary. Defaults to `_phonemes`.\n\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `_punctuations`.\n\n        pad (str):\n            Special padding character that would be ignored by the model. Defaults to `_pad`.\n\n        eos (str):\n            End of the sentence character. Defaults to `_eos`.\n\n        bos (str):\n            Beginning of the sentence character. Defaults to `_bos`.\n\n        blank (str):\n            Optional character used between characters by some models for better prosody. Defaults to `_blank`.\n\n        is_unique (bool):\n            Remove duplicates from the provided characters. Defaults to True.\n\n        is_sorted (bool):\n            Sort the characters in alphabetical order. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        characters: str = _phonemes,\n        punctuations: str = _punctuations,\n        pad: str = _pad,\n        eos: str = _eos,\n        bos: str = _bos,\n        blank: str = _blank,\n        is_unique: bool = False,\n        is_sorted: bool = True,\n    ) -> None:\n        super().__init__(characters, punctuations, pad, eos, bos, blank, is_unique, is_sorted)\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\"):\n        \"\"\"Init a IPAPhonemes object from a model config\n\n        If characters are not defined in the config, it will be set to the default characters and the config\n        will be updated.\n        \"\"\"\n        # band-aid for compatibility with old models\n        if \"characters\" in config and config.characters is not None:\n            if \"phonemes\" in config.characters and config.characters.phonemes is not None:\n                config.characters[\"characters\"] = config.characters[\"phonemes\"]\n            return (\n                IPAPhonemes(\n                    characters=config.characters[\"characters\"],\n                    punctuations=config.characters[\"punctuations\"],\n                    pad=config.characters[\"pad\"],\n                    eos=config.characters[\"eos\"],\n                    bos=config.characters[\"bos\"],\n                    blank=config.characters[\"blank\"],\n                    is_unique=config.characters[\"is_unique\"],\n                    is_sorted=config.characters[\"is_sorted\"],\n                ),\n                config,\n            )\n        # use character set from config\n        if config.characters is not None:\n            return IPAPhonemes(**config.characters), config\n        # return default character set\n        characters = IPAPhonemes()\n        new_config = replace(config, characters=characters.to_config())\n        return characters, new_config\n\n\nclass Graphemes(BaseCharacters):\n    \"\"\"\ud83d\udc38Graphemes class to manage `TTS.tts` model vocabulary\n\n    Intended to be used with models using graphemes as input.\n    It uses system defaults for the undefined class arguments.\n\n    Args:\n        characters (str):\n            Main set of case-sensitive characters to be used in the vocabulary. Defaults to `_characters`.\n\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `_punctuations`.\n\n        pad (str):\n            Special padding character that would be ignored by the model. Defaults to `_pad`.\n\n        eos (str):\n            End of the sentence character. Defaults to `_eos`.\n\n        bos (str):\n            Beginning of the sentence character. Defaults to `_bos`.\n\n        is_unique (bool):\n            Remove duplicates from the provided characters. Defaults to True.\n\n        is_sorted (bool):\n            Sort the characters in alphabetical order. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        characters: str = _characters,\n        punctuations: str = _punctuations,\n        pad: str = _pad,\n        eos: str = _eos,\n        bos: str = _bos,\n        blank: str = _blank,\n        is_unique: bool = False,\n        is_sorted: bool = True,\n    ) -> None:\n        super().__init__(characters, punctuations, pad, eos, bos, blank, is_unique, is_sorted)\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\"):\n        \"\"\"Init a Graphemes object from a model config\n\n        If characters are not defined in the config, it will be set to the default characters and the config\n        will be updated.\n        \"\"\"\n        if config.characters is not None:\n            # band-aid for compatibility with old models\n            if \"phonemes\" in config.characters:\n                return (\n                    Graphemes(\n                        characters=config.characters[\"characters\"],\n                        punctuations=config.characters[\"punctuations\"],\n                        pad=config.characters[\"pad\"],\n                        eos=config.characters[\"eos\"],\n                        bos=config.characters[\"bos\"],\n                        blank=config.characters[\"blank\"],\n                        is_unique=config.characters[\"is_unique\"],\n                        is_sorted=config.characters[\"is_sorted\"],\n                    ),\n                    config,\n                )\n            return Graphemes(**config.characters), config\n        characters = Graphemes()\n        new_config = replace(config, characters=characters.to_config())\n        return characters, new_config\n\n\nif __name__ == \"__main__\":\n    gr = Graphemes()\n    ph = IPAPhonemes()\n    gr.print_log()\n    ph.print_log()\n", "TTS/tts/utils/text/cleaners.py": "\"\"\"Set of default text cleaners\"\"\"\n# TODO: pick the cleaner for languages dynamically\n\nimport re\n\nfrom anyascii import anyascii\n\nfrom TTS.tts.utils.text.chinese_mandarin.numbers import replace_numbers_to_characters_in_text\n\nfrom .english.abbreviations import abbreviations_en\nfrom .english.number_norm import normalize_numbers as en_normalize_numbers\nfrom .english.time_norm import expand_time_english\nfrom .french.abbreviations import abbreviations_fr\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\"\\s+\")\n\n\ndef expand_abbreviations(text, lang=\"en\"):\n    if lang == \"en\":\n        _abbreviations = abbreviations_en\n    elif lang == \"fr\":\n        _abbreviations = abbreviations_fr\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef lowercase(text):\n    return text.lower()\n\n\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text).strip()\n\n\ndef convert_to_ascii(text):\n    return anyascii(text)\n\n\ndef remove_aux_symbols(text):\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n    return text\n\n\ndef replace_symbols(text, lang=\"en\"):\n    \"\"\"Replace symbols based on the lenguage tag.\n\n    Args:\n      text:\n       Input text.\n      lang:\n        Lenguage identifier. ex: \"en\", \"fr\", \"pt\", \"ca\".\n\n    Returns:\n      The modified text\n      example:\n        input args:\n            text: \"si l'avi cau, diguem-ho\"\n            lang: \"ca\"\n        Output:\n            text: \"si lavi cau, diguemho\"\n    \"\"\"\n    text = text.replace(\";\", \",\")\n    text = text.replace(\"-\", \" \") if lang != \"ca\" else text.replace(\"-\", \"\")\n    text = text.replace(\":\", \",\")\n    if lang == \"en\":\n        text = text.replace(\"&\", \" and \")\n    elif lang == \"fr\":\n        text = text.replace(\"&\", \" et \")\n    elif lang == \"pt\":\n        text = text.replace(\"&\", \" e \")\n    elif lang == \"ca\":\n        text = text.replace(\"&\", \" i \")\n        text = text.replace(\"'\", \"\")\n    return text\n\n\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef basic_german_cleaners(text):\n    \"\"\"Pipeline for German text\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\n# TODO: elaborate it\ndef basic_turkish_cleaners(text):\n    \"\"\"Pipeline for Turkish text\"\"\"\n    text = text.replace(\"I\", \"\u0131\")\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_time_english(text)\n    text = en_normalize_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef phoneme_cleaners(text):\n    \"\"\"Pipeline for phonemes mode, including number and abbreviation expansion.\"\"\"\n    text = en_normalize_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef french_cleaners(text):\n    \"\"\"Pipeline for French text. There is no need to expand numbers, phonemizer already does that\"\"\"\n    text = expand_abbreviations(text, lang=\"fr\")\n    text = lowercase(text)\n    text = replace_symbols(text, lang=\"fr\")\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef portuguese_cleaners(text):\n    \"\"\"Basic pipeline for Portuguese text. There is no need to expand abbreviation and\n    numbers, phonemizer already does that\"\"\"\n    text = lowercase(text)\n    text = replace_symbols(text, lang=\"pt\")\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef chinese_mandarin_cleaners(text: str) -> str:\n    \"\"\"Basic pipeline for chinese\"\"\"\n    text = replace_numbers_to_characters_in_text(text)\n    return text\n\n\ndef multilingual_cleaners(text):\n    \"\"\"Pipeline for multilingual text\"\"\"\n    text = lowercase(text)\n    text = replace_symbols(text, lang=None)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef no_cleaners(text):\n    # remove newline characters\n    text = text.replace(\"\\n\", \"\")\n    return text\n", "TTS/tts/utils/text/tokenizer.py": "from typing import Callable, Dict, List, Union\n\nfrom TTS.tts.utils.text import cleaners\nfrom TTS.tts.utils.text.characters import Graphemes, IPAPhonemes\nfrom TTS.tts.utils.text.phonemizers import DEF_LANG_TO_PHONEMIZER, get_phonemizer_by_name\nfrom TTS.tts.utils.text.phonemizers.multi_phonemizer import MultiPhonemizer\nfrom TTS.utils.generic_utils import get_import_path, import_class\n\n\nclass TTSTokenizer:\n    \"\"\"\ud83d\udc38TTS tokenizer to convert input characters to token IDs and back.\n\n    Token IDs for OOV chars are discarded but those are stored in `self.not_found_characters` for later.\n\n    Args:\n        use_phonemes (bool):\n            Whether to use phonemes instead of characters. Defaults to False.\n\n        characters (Characters):\n            A Characters object to use for character-to-ID and ID-to-character mappings.\n\n        text_cleaner (callable):\n            A function to pre-process the text before tokenization and phonemization. Defaults to None.\n\n        phonemizer (Phonemizer):\n            A phonemizer object or a dict that maps language codes to phonemizer objects. Defaults to None.\n\n    Example:\n\n        >>> from TTS.tts.utils.text.tokenizer import TTSTokenizer\n        >>> tokenizer = TTSTokenizer(use_phonemes=False, characters=Graphemes())\n        >>> text = \"Hello world!\"\n        >>> ids = tokenizer.text_to_ids(text)\n        >>> text_hat = tokenizer.ids_to_text(ids)\n        >>> assert text == text_hat\n    \"\"\"\n\n    def __init__(\n        self,\n        use_phonemes=False,\n        text_cleaner: Callable = None,\n        characters: \"BaseCharacters\" = None,\n        phonemizer: Union[\"Phonemizer\", Dict] = None,\n        add_blank: bool = False,\n        use_eos_bos=False,\n    ):\n        self.text_cleaner = text_cleaner\n        self.use_phonemes = use_phonemes\n        self.add_blank = add_blank\n        self.use_eos_bos = use_eos_bos\n        self.characters = characters\n        self.not_found_characters = []\n        self.phonemizer = phonemizer\n\n    @property\n    def characters(self):\n        return self._characters\n\n    @characters.setter\n    def characters(self, new_characters):\n        self._characters = new_characters\n        self.pad_id = self.characters.char_to_id(self.characters.pad) if self.characters.pad else None\n        self.blank_id = self.characters.char_to_id(self.characters.blank) if self.characters.blank else None\n\n    def encode(self, text: str) -> List[int]:\n        \"\"\"Encodes a string of text as a sequence of IDs.\"\"\"\n        token_ids = []\n        for char in text:\n            try:\n                idx = self.characters.char_to_id(char)\n                token_ids.append(idx)\n            except KeyError:\n                # discard but store not found characters\n                if char not in self.not_found_characters:\n                    self.not_found_characters.append(char)\n                    print(text)\n                    print(f\" [!] Character {repr(char)} not found in the vocabulary. Discarding it.\")\n        return token_ids\n\n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"Decodes a sequence of IDs to a string of text.\"\"\"\n        text = \"\"\n        for token_id in token_ids:\n            text += self.characters.id_to_char(token_id)\n        return text\n\n    def text_to_ids(self, text: str, language: str = None) -> List[int]:  # pylint: disable=unused-argument\n        \"\"\"Converts a string of text to a sequence of token IDs.\n\n        Args:\n            text(str):\n                The text to convert to token IDs.\n\n            language(str):\n                The language code of the text. Defaults to None.\n\n        TODO:\n            - Add support for language-specific processing.\n\n        1. Text normalizatin\n        2. Phonemization (if use_phonemes is True)\n        3. Add blank char between characters\n        4. Add BOS and EOS characters\n        5. Text to token IDs\n        \"\"\"\n        # TODO: text cleaner should pick the right routine based on the language\n        if self.text_cleaner is not None:\n            text = self.text_cleaner(text)\n        if self.use_phonemes:\n            text = self.phonemizer.phonemize(text, separator=\"\", language=language)\n        text = self.encode(text)\n        if self.add_blank:\n            text = self.intersperse_blank_char(text, True)\n        if self.use_eos_bos:\n            text = self.pad_with_bos_eos(text)\n        return text\n\n    def ids_to_text(self, id_sequence: List[int]) -> str:\n        \"\"\"Converts a sequence of token IDs to a string of text.\"\"\"\n        return self.decode(id_sequence)\n\n    def pad_with_bos_eos(self, char_sequence: List[str]):\n        \"\"\"Pads a sequence with the special BOS and EOS characters.\"\"\"\n        return [self.characters.bos_id] + list(char_sequence) + [self.characters.eos_id]\n\n    def intersperse_blank_char(self, char_sequence: List[str], use_blank_char: bool = False):\n        \"\"\"Intersperses the blank character between characters in a sequence.\n\n        Use the ```blank``` character if defined else use the ```pad``` character.\n        \"\"\"\n        char_to_use = self.characters.blank_id if use_blank_char else self.characters.pad\n        result = [char_to_use] * (len(char_sequence) * 2 + 1)\n        result[1::2] = char_sequence\n        return result\n\n    def print_logs(self, level: int = 0):\n        indent = \"\\t\" * level\n        print(f\"{indent}| > add_blank: {self.add_blank}\")\n        print(f\"{indent}| > use_eos_bos: {self.use_eos_bos}\")\n        print(f\"{indent}| > use_phonemes: {self.use_phonemes}\")\n        if self.use_phonemes:\n            print(f\"{indent}| > phonemizer:\")\n            self.phonemizer.print_logs(level + 1)\n        if len(self.not_found_characters) > 0:\n            print(f\"{indent}| > {len(self.not_found_characters)} not found characters:\")\n            for char in self.not_found_characters:\n                print(f\"{indent}| > {char}\")\n\n    @staticmethod\n    def init_from_config(config: \"Coqpit\", characters: \"BaseCharacters\" = None):\n        \"\"\"Init Tokenizer object from config\n\n        Args:\n            config (Coqpit): Coqpit model config.\n            characters (BaseCharacters): Defines the model character set. If not set, use the default options based on\n                the config values. Defaults to None.\n        \"\"\"\n        # init cleaners\n        text_cleaner = None\n        if isinstance(config.text_cleaner, (str, list)):\n            text_cleaner = getattr(cleaners, config.text_cleaner)\n\n        # init characters\n        if characters is None:\n            # set characters based on defined characters class\n            if config.characters and config.characters.characters_class:\n                CharactersClass = import_class(config.characters.characters_class)\n                characters, new_config = CharactersClass.init_from_config(config)\n            # set characters based on config\n            else:\n                if config.use_phonemes:\n                    # init phoneme set\n                    characters, new_config = IPAPhonemes().init_from_config(config)\n                else:\n                    # init character set\n                    characters, new_config = Graphemes().init_from_config(config)\n\n        else:\n            characters, new_config = characters.init_from_config(config)\n\n        # set characters class\n        new_config.characters.characters_class = get_import_path(characters)\n\n        # init phonemizer\n        phonemizer = None\n        if config.use_phonemes:\n            if \"phonemizer\" in config and config.phonemizer == \"multi_phonemizer\":\n                lang_to_phonemizer_name = {}\n                for dataset in config.datasets:\n                    if dataset.language != \"\":\n                        lang_to_phonemizer_name[dataset.language] = dataset.phonemizer\n                    else:\n                        raise ValueError(\"Multi phonemizer requires language to be set for each dataset.\")\n                phonemizer = MultiPhonemizer(lang_to_phonemizer_name)\n            else:\n                phonemizer_kwargs = {\"language\": config.phoneme_language}\n                if \"phonemizer\" in config and config.phonemizer:\n                    phonemizer = get_phonemizer_by_name(config.phonemizer, **phonemizer_kwargs)\n                else:\n                    try:\n                        phonemizer = get_phonemizer_by_name(\n                            DEF_LANG_TO_PHONEMIZER[config.phoneme_language], **phonemizer_kwargs\n                        )\n                        new_config.phonemizer = phonemizer.name()\n                    except KeyError as e:\n                        raise ValueError(\n                            f\"\"\"No phonemizer found for language {config.phoneme_language}.\n                            You may need to install a third party library for this language.\"\"\"\n                        ) from e\n\n        return (\n            TTSTokenizer(\n                config.use_phonemes, text_cleaner, characters, phonemizer, config.add_blank, config.enable_eos_bos_chars\n            ),\n            new_config,\n        )\n", "TTS/tts/utils/text/__init__.py": "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n", "TTS/tts/utils/text/phonemizers/zh_cn_phonemizer.py": "from typing import Dict\n\nfrom TTS.tts.utils.text.chinese_mandarin.phonemizer import chinese_text_to_phonemes\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\n\n_DEF_ZH_PUNCS = \"\u3001.,[]()?!\u303d~\u300e\u300f\u300c\u300d\u3010\u3011\"\n\n\nclass ZH_CN_Phonemizer(BasePhonemizer):\n    \"\"\"\ud83d\udc38TTS Zh-Cn phonemizer using functions in `TTS.tts.utils.text.chinese_mandarin.phonemizer`\n\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n\n    Example ::\n\n        \"\u8fd9\u662f\uff0c\u6837\u672c\u4e2d\u6587\u3002\" -> `d|\u0292|\u00f8|4| |\u0282|\u028f|4| |\uff0c| |i|\u0251|\u014b|4|b|\u0153|n|3| |d|\u0292|o|\u014b|1|w|\u0153|n|2| |\u3002`\n\n    TODO: someone with Mandarin knowledge should check this implementation\n    \"\"\"\n\n    language = \"zh-cn\"\n\n    def __init__(self, punctuations=_DEF_ZH_PUNCS, keep_puncs=False, **kwargs):  # pylint: disable=unused-argument\n        super().__init__(self.language, punctuations=punctuations, keep_puncs=keep_puncs)\n\n    @staticmethod\n    def name():\n        return \"zh_cn_phonemizer\"\n\n    @staticmethod\n    def phonemize_zh_cn(text: str, separator: str = \"|\") -> str:\n        ph = chinese_text_to_phonemes(text, separator)\n        return ph\n\n    def _phonemize(self, text, separator):\n        return self.phonemize_zh_cn(text, separator)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        return {\"zh-cn\": \"Chinese (China)\"}\n\n    def version(self) -> str:\n        return \"0.0.1\"\n\n    def is_available(self) -> bool:\n        return True\n\n\n# if __name__ == \"__main__\":\n#     text = \"\u8fd9\u662f\uff0c\u6837\u672c\u4e2d\u6587\u3002\"\n#     e = ZH_CN_Phonemizer()\n#     print(e.supported_languages())\n#     print(e.version())\n#     print(e.language)\n#     print(e.name())\n#     print(e.is_available())\n#     print(\"`\" + e.phonemize(text) + \"`\")\n", "TTS/tts/utils/text/phonemizers/bangla_phonemizer.py": "from typing import Dict\n\nfrom TTS.tts.utils.text.bangla.phonemizer import bangla_text_to_phonemes\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\n\n_DEF_ZH_PUNCS = \"\u3001.,[]()?!\u303d~\u300e\u300f\u300c\u300d\u3010\u3011\"\n\n\nclass BN_Phonemizer(BasePhonemizer):\n    \"\"\"\ud83d\udc38TTS bn phonemizer using functions in `TTS.tts.utils.text.bangla.phonemizer`\n\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n\n    Example ::\n\n        \"\u8fd9\u662f\uff0c\u6837\u672c\u4e2d\u6587\u3002\" -> `d|\u0292|\u00f8|4| |\u0282|\u028f|4| |\uff0c| |i|\u0251|\u014b|4|b|\u0153|n|3| |d|\u0292|o|\u014b|1|w|\u0153|n|2| |\u3002`\n\n    TODO: someone with Bangla knowledge should check this implementation\n    \"\"\"\n\n    language = \"bn\"\n\n    def __init__(self, punctuations=_DEF_ZH_PUNCS, keep_puncs=False, **kwargs):  # pylint: disable=unused-argument\n        super().__init__(self.language, punctuations=punctuations, keep_puncs=keep_puncs)\n\n    @staticmethod\n    def name():\n        return \"bn_phonemizer\"\n\n    @staticmethod\n    def phonemize_bn(text: str, separator: str = \"|\") -> str:  # pylint: disable=unused-argument\n        ph = bangla_text_to_phonemes(text)\n        return ph\n\n    def _phonemize(self, text, separator):\n        return self.phonemize_bn(text, separator)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        return {\"bn\": \"Bangla\"}\n\n    def version(self) -> str:\n        return \"0.0.1\"\n\n    def is_available(self) -> bool:\n        return True\n\n\nif __name__ == \"__main__\":\n    txt = \"\u09b0\u09be\u09b8\u09c2\u09b2\u09c1\u09b2\u09cd\u09b2\u09be\u09b9 \u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09b2\u09cd\u09b2\u09be\u09b9\u09c1 \u0986\u09b2\u09be\u0987\u09b9\u09bf \u0993\u09df\u09be \u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09ae \u09b6\u09bf\u0995\u09cd\u09b7\u09be \u09a6\u09bf\u09df\u09c7\u099b\u09c7\u09a8 \u09af\u09c7, \u0995\u09c7\u0989 \u09af\u09a6\u09bf \u0995\u09cb\u09a8 \u0996\u09be\u09b0\u09be\u09aa \u0995\u09bf\u099b\u09c1\u09b0 \u09b8\u09ae\u09cd\u09ae\u09c1\u0996\u09c0\u09a8 \u09b9\u09df, \u09a4\u0996\u09a8\u0993 \u09af\u09c7\u09a8 \u09ac\u09b2\u09c7.\"\n    e = BN_Phonemizer()\n    print(e.supported_languages())\n    print(e.version())\n    print(e.language)\n    print(e.name())\n    print(e.is_available())\n    print(\"`\" + e.phonemize(txt) + \"`\")\n", "TTS/tts/utils/text/phonemizers/belarusian_phonemizer.py": "from typing import Dict\n\nfrom TTS.tts.utils.text.belarusian.phonemizer import belarusian_text_to_phonemes\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\n\n_DEF_BE_PUNCS = \",!.\"  # TODO\n\n\nclass BEL_Phonemizer(BasePhonemizer):\n    \"\"\"\ud83d\udc38TTS be phonemizer using functions in `TTS.tts.utils.text.belarusian.phonemizer`\n\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_BE_PUNCS`.\n\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n    \"\"\"\n\n    language = \"be\"\n\n    def __init__(self, punctuations=_DEF_BE_PUNCS, keep_puncs=True, **kwargs):  # pylint: disable=unused-argument\n        super().__init__(self.language, punctuations=punctuations, keep_puncs=keep_puncs)\n\n    @staticmethod\n    def name():\n        return \"be_phonemizer\"\n\n    @staticmethod\n    def phonemize_be(text: str, separator: str = \"|\") -> str:  # pylint: disable=unused-argument\n        return belarusian_text_to_phonemes(text)\n\n    def _phonemize(self, text, separator):\n        return self.phonemize_be(text, separator)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        return {\"be\": \"Belarusian\"}\n\n    def version(self) -> str:\n        return \"0.0.1\"\n\n    def is_available(self) -> bool:\n        return True\n\n\nif __name__ == \"__main__\":\n    txt = \"\u0442\u044d\u0441\u0442\"\n    e = BEL_Phonemizer()\n    print(e.supported_languages())\n    print(e.version())\n    print(e.language)\n    print(e.name())\n    print(e.is_available())\n    print(\"`\" + e.phonemize(txt) + \"`\")\n", "TTS/tts/utils/text/phonemizers/ja_jp_phonemizer.py": "from typing import Dict\n\nfrom TTS.tts.utils.text.japanese.phonemizer import japanese_text_to_phonemes\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\n\n_DEF_JA_PUNCS = \"\u3001.,[]()?!\u303d~\u300e\u300f\u300c\u300d\u3010\u3011\"\n\n_TRANS_TABLE = {\"\u3001\": \",\"}\n\n\ndef trans(text):\n    for i, j in _TRANS_TABLE.items():\n        text = text.replace(i, j)\n    return text\n\n\nclass JA_JP_Phonemizer(BasePhonemizer):\n    \"\"\"\ud83d\udc38TTS Ja-Jp phonemizer using functions in `TTS.tts.utils.text.japanese.phonemizer`\n\n    TODO: someone with JA knowledge should check this implementation\n\n    Example:\n\n        >>> from TTS.tts.utils.text.phonemizers import JA_JP_Phonemizer\n        >>> phonemizer = JA_JP_Phonemizer()\n        >>> phonemizer.phonemize(\"\u3069\u3061\u3089\u306b\u884c\u304d\u307e\u3059\u304b\uff1f\", separator=\"|\")\n        'd|o|c|h|i|r|a|n|i|i|k|i|m|a|s|u|k|a|?'\n\n    \"\"\"\n\n    language = \"ja-jp\"\n\n    def __init__(self, punctuations=_DEF_JA_PUNCS, keep_puncs=True, **kwargs):  # pylint: disable=unused-argument\n        super().__init__(self.language, punctuations=punctuations, keep_puncs=keep_puncs)\n\n    @staticmethod\n    def name():\n        return \"ja_jp_phonemizer\"\n\n    def _phonemize(self, text: str, separator: str = \"|\") -> str:\n        ph = japanese_text_to_phonemes(text)\n        if separator is not None or separator != \"\":\n            return separator.join(ph)\n        return ph\n\n    def phonemize(self, text: str, separator=\"|\", language=None) -> str:\n        \"\"\"Custom phonemize for JP_JA\n\n        Skip pre-post processing steps used by the other phonemizers.\n        \"\"\"\n        return self._phonemize(text, separator)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        return {\"ja-jp\": \"Japanese (Japan)\"}\n\n    def version(self) -> str:\n        return \"0.0.1\"\n\n    def is_available(self) -> bool:\n        return True\n\n\n# if __name__ == \"__main__\":\n#     text = \"\u3053\u308c\u306f\u3001\u96fb\u8a71\u3092\u304b\u3051\u308b\u305f\u3081\u306e\u79c1\u306e\u65e5\u672c\u8a9e\u306e\u4f8b\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u3059\u3002\"\n#     e = JA_JP_Phonemizer()\n#     print(e.supported_languages())\n#     print(e.version())\n#     print(e.language)\n#     print(e.name())\n#     print(e.is_available())\n#     print(\"`\" + e.phonemize(text) + \"`\")\n", "TTS/tts/utils/text/phonemizers/multi_phonemizer.py": "from typing import Dict, List\n\nfrom TTS.tts.utils.text.phonemizers import DEF_LANG_TO_PHONEMIZER, get_phonemizer_by_name\n\n\nclass MultiPhonemizer:\n    \"\"\"\ud83d\udc38TTS multi-phonemizer that operates phonemizers for multiple langugages\n\n    Args:\n        custom_lang_to_phonemizer (Dict):\n            Custom phonemizer mapping if you want to change the defaults. In the format of\n            `{\"lang_code\", \"phonemizer_name\"}`. When it is None, `DEF_LANG_TO_PHONEMIZER` is used. Defaults to `{}`.\n\n    TODO: find a way to pass custom kwargs to the phonemizers\n    \"\"\"\n\n    lang_to_phonemizer = {}\n\n    def __init__(self, lang_to_phonemizer_name: Dict = {}) -> None:  # pylint: disable=dangerous-default-value\n        for k, v in lang_to_phonemizer_name.items():\n            if v == \"\" and k in DEF_LANG_TO_PHONEMIZER.keys():\n                lang_to_phonemizer_name[k] = DEF_LANG_TO_PHONEMIZER[k]\n            elif v == \"\":\n                raise ValueError(f\"Phonemizer wasn't set for language {k} and doesn't have a default.\")\n        self.lang_to_phonemizer_name = lang_to_phonemizer_name\n        self.lang_to_phonemizer = self.init_phonemizers(self.lang_to_phonemizer_name)\n\n    @staticmethod\n    def init_phonemizers(lang_to_phonemizer_name: Dict) -> Dict:\n        lang_to_phonemizer = {}\n        for k, v in lang_to_phonemizer_name.items():\n            lang_to_phonemizer[k] = get_phonemizer_by_name(v, language=k)\n        return lang_to_phonemizer\n\n    @staticmethod\n    def name():\n        return \"multi-phonemizer\"\n\n    def phonemize(self, text, separator=\"|\", language=\"\"):\n        if language == \"\":\n            raise ValueError(\"Language must be set for multi-phonemizer to phonemize.\")\n        return self.lang_to_phonemizer[language].phonemize(text, separator)\n\n    def supported_languages(self) -> List:\n        return list(self.lang_to_phonemizer.keys())\n\n    def print_logs(self, level: int = 0):\n        indent = \"\\t\" * level\n        print(f\"{indent}| > phoneme language: {self.supported_languages()}\")\n        print(f\"{indent}| > phoneme backend: {self.name()}\")\n\n\n# if __name__ == \"__main__\":\n#     texts = {\n#         \"tr\": \"Merhaba, bu T\u00fcrk\u00e7e bit \u00f6rnek!\",\n#         \"en-us\": \"Hello, this is English example!\",\n#         \"de\": \"Hallo, das ist ein Deutches Beipiel!\",\n#         \"zh-cn\": \"\u8fd9\u662f\u4e2d\u56fd\u7684\u4f8b\u5b50\",\n#     }\n#     phonemes = {}\n#     ph = MultiPhonemizer({\"tr\": \"espeak\", \"en-us\": \"\", \"de\": \"gruut\", \"zh-cn\": \"\"})\n#     for lang, text in texts.items():\n#         phoneme = ph.phonemize(text, lang)\n#         phonemes[lang] = phoneme\n#     print(phonemes)\n", "TTS/tts/utils/text/phonemizers/gruut_wrapper.py": "import importlib\nfrom typing import List\n\nimport gruut\nfrom gruut_ipa import IPA\n\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\nfrom TTS.tts.utils.text.punctuation import Punctuation\n\n# Table for str.translate to fix gruut/TTS phoneme mismatch\nGRUUT_TRANS_TABLE = str.maketrans(\"g\", \"\u0261\")\n\n\nclass Gruut(BasePhonemizer):\n    \"\"\"Gruut wrapper for G2P\n\n    Args:\n        language (str):\n            Valid language code for the used backend.\n\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `Punctuation.default_puncs()`.\n\n        keep_puncs (bool):\n            If true, keep the punctuations after phonemization. Defaults to True.\n\n        use_espeak_phonemes (bool):\n            If true, use espeak lexicons instead of default Gruut lexicons. Defaults to False.\n\n        keep_stress (bool):\n            If true, keep the stress characters after phonemization. Defaults to False.\n\n    Example:\n\n        >>> from TTS.tts.utils.text.phonemizers.gruut_wrapper import Gruut\n        >>> phonemizer = Gruut('en-us')\n        >>> phonemizer.phonemize(\"Be a voice, not an! echo?\", separator=\"|\")\n        'b|i| \u0259| v|\u0254|\u026a|s, n|\u0251|t| \u0259|n! \u025b|k|o|\u028a?'\n    \"\"\"\n\n    def __init__(\n        self,\n        language: str,\n        punctuations=Punctuation.default_puncs(),\n        keep_puncs=True,\n        use_espeak_phonemes=False,\n        keep_stress=False,\n    ):\n        super().__init__(language, punctuations=punctuations, keep_puncs=keep_puncs)\n        self.use_espeak_phonemes = use_espeak_phonemes\n        self.keep_stress = keep_stress\n\n    @staticmethod\n    def name():\n        return \"gruut\"\n\n    def phonemize_gruut(self, text: str, separator: str = \"|\", tie=False) -> str:  # pylint: disable=unused-argument\n        \"\"\"Convert input text to phonemes.\n\n        Gruut phonemizes the given `str` by seperating each phoneme character with `separator`, even for characters\n        that constitude a single sound.\n\n        It doesn't affect \ud83d\udc38TTS since it individually converts each character to token IDs.\n\n        Examples::\n            \"hello how are you today?\" -> `h|\u025b|l|o|\u028a| h|a|\u028a| \u0251|\u0279| j|u| t|\u0259|d|e|\u026a`\n\n        Args:\n            text (str):\n                Text to be converted to phonemes.\n\n            tie (bool, optional) : When True use a '\u0361' character between\n                consecutive characters of a single phoneme. Else separate phoneme\n                with '_'. This option requires espeak>=1.49. Default to False.\n        \"\"\"\n        ph_list = []\n        for sentence in gruut.sentences(text, lang=self.language, espeak=self.use_espeak_phonemes):\n            for word in sentence:\n                if word.is_break:\n                    # Use actual character for break phoneme (e.g., comma)\n                    if ph_list:\n                        # Join with previous word\n                        ph_list[-1].append(word.text)\n                    else:\n                        # First word is punctuation\n                        ph_list.append([word.text])\n                elif word.phonemes:\n                    # Add phonemes for word\n                    word_phonemes = []\n\n                    for word_phoneme in word.phonemes:\n                        if not self.keep_stress:\n                            # Remove primary/secondary stress\n                            word_phoneme = IPA.without_stress(word_phoneme)\n\n                        word_phoneme = word_phoneme.translate(GRUUT_TRANS_TABLE)\n\n                        if word_phoneme:\n                            # Flatten phonemes\n                            word_phonemes.extend(word_phoneme)\n\n                    if word_phonemes:\n                        ph_list.append(word_phonemes)\n\n        ph_words = [separator.join(word_phonemes) for word_phonemes in ph_list]\n        ph = f\"{separator} \".join(ph_words)\n        return ph\n\n    def _phonemize(self, text, separator):\n        return self.phonemize_gruut(text, separator, tie=False)\n\n    def is_supported_language(self, language):\n        \"\"\"Returns True if `language` is supported by the backend\"\"\"\n        return gruut.is_language_supported(language)\n\n    @staticmethod\n    def supported_languages() -> List:\n        \"\"\"Get a dictionary of supported languages.\n\n        Returns:\n            List: List of language codes.\n        \"\"\"\n        return list(gruut.get_supported_languages())\n\n    def version(self):\n        \"\"\"Get the version of the used backend.\n\n        Returns:\n            str: Version of the used backend.\n        \"\"\"\n        return gruut.__version__\n\n    @classmethod\n    def is_available(cls):\n        \"\"\"Return true if ESpeak is available else false\"\"\"\n        return importlib.util.find_spec(\"gruut\") is not None\n\n\nif __name__ == \"__main__\":\n    e = Gruut(language=\"en-us\")\n    print(e.supported_languages())\n    print(e.version())\n    print(e.language)\n    print(e.name())\n    print(e.is_available())\n\n    e = Gruut(language=\"en-us\", keep_puncs=False)\n    print(\"`\" + e.phonemize(\"hello how are you today?\") + \"`\")\n\n    e = Gruut(language=\"en-us\", keep_puncs=True)\n    print(\"`\" + e.phonemize(\"hello how, are you today?\") + \"`\")\n", "TTS/tts/utils/text/phonemizers/base.py": "import abc\nfrom typing import List, Tuple\n\nfrom TTS.tts.utils.text.punctuation import Punctuation\n\n\nclass BasePhonemizer(abc.ABC):\n    \"\"\"Base phonemizer class\n\n    Phonemization follows the following steps:\n        1. Preprocessing:\n            - remove empty lines\n            - remove punctuation\n            - keep track of punctuation marks\n\n        2. Phonemization:\n            - convert text to phonemes\n\n        3. Postprocessing:\n            - join phonemes\n            - restore punctuation marks\n\n    Args:\n        language (str):\n            Language used by the phonemizer.\n\n        punctuations (List[str]):\n            List of punctuation marks to be preserved.\n\n        keep_puncs (bool):\n            Whether to preserve punctuation marks or not.\n    \"\"\"\n\n    def __init__(self, language, punctuations=Punctuation.default_puncs(), keep_puncs=False):\n        # ensure the backend is installed on the system\n        if not self.is_available():\n            raise RuntimeError(\"{} not installed on your system\".format(self.name()))  # pragma: nocover\n\n        # ensure the backend support the requested language\n        self._language = self._init_language(language)\n\n        # setup punctuation processing\n        self._keep_puncs = keep_puncs\n        self._punctuator = Punctuation(punctuations)\n\n    def _init_language(self, language):\n        \"\"\"Language initialization\n\n        This method may be overloaded in child classes (see Segments backend)\n\n        \"\"\"\n        if not self.is_supported_language(language):\n            raise RuntimeError(f'language \"{language}\" is not supported by the ' f\"{self.name()} backend\")\n        return language\n\n    @property\n    def language(self):\n        \"\"\"The language code configured to be used for phonemization\"\"\"\n        return self._language\n\n    @staticmethod\n    @abc.abstractmethod\n    def name():\n        \"\"\"The name of the backend\"\"\"\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def is_available(cls):\n        \"\"\"Returns True if the backend is installed, False otherwise\"\"\"\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def version(cls):\n        \"\"\"Return the backend version as a tuple (major, minor, patch)\"\"\"\n        ...\n\n    @staticmethod\n    @abc.abstractmethod\n    def supported_languages():\n        \"\"\"Return a dict of language codes -> name supported by the backend\"\"\"\n        ...\n\n    def is_supported_language(self, language):\n        \"\"\"Returns True if `language` is supported by the backend\"\"\"\n        return language in self.supported_languages()\n\n    @abc.abstractmethod\n    def _phonemize(self, text, separator):\n        \"\"\"The main phonemization method\"\"\"\n\n    def _phonemize_preprocess(self, text) -> Tuple[List[str], List]:\n        \"\"\"Preprocess the text before phonemization\n\n        1. remove spaces\n        2. remove punctuation\n\n        Override this if you need a different behaviour\n        \"\"\"\n        text = text.strip()\n        if self._keep_puncs:\n            # a tuple (text, punctuation marks)\n            return self._punctuator.strip_to_restore(text)\n        return [self._punctuator.strip(text)], []\n\n    def _phonemize_postprocess(self, phonemized, punctuations) -> str:\n        \"\"\"Postprocess the raw phonemized output\n\n        Override this if you need a different behaviour\n        \"\"\"\n        if self._keep_puncs:\n            return self._punctuator.restore(phonemized, punctuations)[0]\n        return phonemized[0]\n\n    def phonemize(self, text: str, separator=\"|\", language: str = None) -> str:  # pylint: disable=unused-argument\n        \"\"\"Returns the `text` phonemized for the given language\n\n        Args:\n            text (str):\n                Text to be phonemized.\n\n            separator (str):\n                string separator used between phonemes. Default to '_'.\n\n        Returns:\n            (str): Phonemized text\n        \"\"\"\n        text, punctuations = self._phonemize_preprocess(text)\n        phonemized = []\n        for t in text:\n            p = self._phonemize(t, separator)\n            phonemized.append(p)\n        phonemized = self._phonemize_postprocess(phonemized, punctuations)\n        return phonemized\n\n    def print_logs(self, level: int = 0):\n        indent = \"\\t\" * level\n        print(f\"{indent}| > phoneme language: {self.language}\")\n        print(f\"{indent}| > phoneme backend: {self.name()}\")\n", "TTS/tts/utils/text/phonemizers/espeak_wrapper.py": "import logging\nimport re\nimport subprocess\nfrom typing import Dict, List\n\nfrom packaging.version import Version\n\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\nfrom TTS.tts.utils.text.punctuation import Punctuation\n\n\ndef is_tool(name):\n    from shutil import which\n\n    return which(name) is not None\n\n\n# Use a regex pattern to match the espeak version, because it may be\n# symlinked to espeak-ng, which moves the version bits to another spot.\nespeak_version_pattern = re.compile(r\"text-to-speech:\\s(?P<version>\\d+\\.\\d+(\\.\\d+)?)\")\n\n\ndef get_espeak_version():\n    output = subprocess.getoutput(\"espeak --version\")\n    match = espeak_version_pattern.search(output)\n\n    return match.group(\"version\")\n\n\ndef get_espeakng_version():\n    output = subprocess.getoutput(\"espeak-ng --version\")\n    return output.split()[3]\n\n\n# priority: espeakng > espeak\nif is_tool(\"espeak-ng\"):\n    _DEF_ESPEAK_LIB = \"espeak-ng\"\n    _DEF_ESPEAK_VER = get_espeakng_version()\nelif is_tool(\"espeak\"):\n    _DEF_ESPEAK_LIB = \"espeak\"\n    _DEF_ESPEAK_VER = get_espeak_version()\nelse:\n    _DEF_ESPEAK_LIB = None\n    _DEF_ESPEAK_VER = None\n\n\ndef _espeak_exe(espeak_lib: str, args: List, sync=False) -> List[str]:\n    \"\"\"Run espeak with the given arguments.\"\"\"\n    cmd = [\n        espeak_lib,\n        \"-q\",\n        \"-b\",\n        \"1\",  # UTF8 text encoding\n    ]\n    cmd.extend(args)\n    logging.debug(\"espeakng: executing %s\", repr(cmd))\n\n    with subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    ) as p:\n        res = iter(p.stdout.readline, b\"\")\n        if not sync:\n            p.stdout.close()\n            if p.stderr:\n                p.stderr.close()\n            if p.stdin:\n                p.stdin.close()\n            return res\n        res2 = []\n        for line in res:\n            res2.append(line)\n        p.stdout.close()\n        if p.stderr:\n            p.stderr.close()\n        if p.stdin:\n            p.stdin.close()\n        p.wait()\n    return res2\n\n\nclass ESpeak(BasePhonemizer):\n    \"\"\"ESpeak wrapper calling `espeak` or `espeak-ng` from the command-line the perform G2P\n\n    Args:\n        language (str):\n            Valid language code for the used backend.\n\n        backend (str):\n            Name of the backend library to use. `espeak` or `espeak-ng`. If None, set automatically\n            prefering `espeak-ng` over `espeak`. Defaults to None.\n\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to Punctuation.default_puncs().\n\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to True.\n\n    Example:\n\n        >>> from TTS.tts.utils.text.phonemizers import ESpeak\n        >>> phonemizer = ESpeak(\"tr\")\n        >>> phonemizer.phonemize(\"Bu T\u00fcrk\u00e7e, bir \u00f6rnektir.\", separator=\"|\")\n        'b|\u028a t|\u02c8\u00f8|r|k|t\u0283|\u025b, b|\u026a|r \u0153|r|n|\u02c8\u025b|c|t|\u026a|r.'\n\n    \"\"\"\n\n    _ESPEAK_LIB = _DEF_ESPEAK_LIB\n    _ESPEAK_VER = _DEF_ESPEAK_VER\n\n    def __init__(self, language: str, backend=None, punctuations=Punctuation.default_puncs(), keep_puncs=True):\n        if self._ESPEAK_LIB is None:\n            raise Exception(\" [!] No espeak backend found. Install espeak-ng or espeak to your system.\")\n        self.backend = self._ESPEAK_LIB\n\n        # band-aid for backwards compatibility\n        if language == \"en\":\n            language = \"en-us\"\n        if language == \"zh-cn\":\n            language = \"cmn\"\n\n        super().__init__(language, punctuations=punctuations, keep_puncs=keep_puncs)\n        if backend is not None:\n            self.backend = backend\n\n    @property\n    def backend(self):\n        return self._ESPEAK_LIB\n\n    @property\n    def backend_version(self):\n        return self._ESPEAK_VER\n\n    @backend.setter\n    def backend(self, backend):\n        if backend not in [\"espeak\", \"espeak-ng\"]:\n            raise Exception(\"Unknown backend: %s\" % backend)\n        self._ESPEAK_LIB = backend\n        self._ESPEAK_VER = get_espeakng_version() if backend == \"espeak-ng\" else get_espeak_version()\n\n    def auto_set_espeak_lib(self) -> None:\n        if is_tool(\"espeak-ng\"):\n            self._ESPEAK_LIB = \"espeak-ng\"\n            self._ESPEAK_VER = get_espeakng_version()\n        elif is_tool(\"espeak\"):\n            self._ESPEAK_LIB = \"espeak\"\n            self._ESPEAK_VER = get_espeak_version()\n        else:\n            raise Exception(\"Cannot set backend automatically. espeak-ng or espeak not found\")\n\n    @staticmethod\n    def name():\n        return \"espeak\"\n\n    def phonemize_espeak(self, text: str, separator: str = \"|\", tie=False) -> str:\n        \"\"\"Convert input text to phonemes.\n\n        Args:\n            text (str):\n                Text to be converted to phonemes.\n\n            tie (bool, optional) : When True use a '\u0361' character between\n                consecutive characters of a single phoneme. Else separate phoneme\n                with '_'. This option requires espeak>=1.49. Default to False.\n        \"\"\"\n        # set arguments\n        args = [\"-v\", f\"{self._language}\"]\n        # espeak and espeak-ng parses `ipa` differently\n        if tie:\n            # use '\u0361' between phonemes\n            if self.backend == \"espeak\":\n                args.append(\"--ipa=1\")\n            else:\n                args.append(\"--ipa=3\")\n        else:\n            # split with '_'\n            if self.backend == \"espeak\":\n                if Version(self.backend_version) >= Version(\"1.48.15\"):\n                    args.append(\"--ipa=1\")\n                else:\n                    args.append(\"--ipa=3\")\n            else:\n                args.append(\"--ipa=1\")\n        if tie:\n            args.append(\"--tie=%s\" % tie)\n\n        args.append(text)\n        # compute phonemes\n        phonemes = \"\"\n        for line in _espeak_exe(self._ESPEAK_LIB, args, sync=True):\n            logging.debug(\"line: %s\", repr(line))\n            ph_decoded = line.decode(\"utf8\").strip()\n            # espeak:\n            #   version 1.48.15: \" p_\u0279_\u02c8a\u026a_\u025a t_\u0259 n_o\u028a_v_\u02c8\u025b_m_b_\u025a t_w_\u02c8\u025b_n_t_i t_\u02c8u\u02d0\\n\"\n            # espeak-ng:\n            #   \"p_\u0279_\u02c8a\u026a_\u025a t_\u0259 n_o\u028a_v_\u02c8\u025b_m_b_\u025a t_w_\u02c8\u025b_n_t_i t_\u02c8u\u02d0\\n\"\n\n            # espeak-ng backend can add language flags that need to be removed:\n            #   \"s\u025b\u0281t\u02c8\u025b\u0303 m\u02c8o k\u0254m (en)f\u02c8\u028atb\u0254\u02d0l(fr) \u0292en\u02c8\u025b\u0281 de- fl\u02c8a\u0261 d\u0259- l\u02c8\u0251\u0303\u0261.\"\n            # phonemize needs to remove the language flags of the returned text:\n            #   \"s\u025b\u0281t\u02c8\u025b\u0303 m\u02c8o k\u0254m f\u02c8\u028atb\u0254\u02d0l \u0292en\u02c8\u025b\u0281 de- fl\u02c8a\u0261 d\u0259- l\u02c8\u0251\u0303\u0261.\"\n            ph_decoded = re.sub(r\"\\(.+?\\)\", \"\", ph_decoded)\n\n            phonemes += ph_decoded.strip()\n        return phonemes.replace(\"_\", separator)\n\n    def _phonemize(self, text, separator=None):\n        return self.phonemize_espeak(text, separator, tie=False)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        \"\"\"Get a dictionary of supported languages.\n\n        Returns:\n            Dict: Dictionary of language codes.\n        \"\"\"\n        if _DEF_ESPEAK_LIB is None:\n            return {}\n        args = [\"--voices\"]\n        langs = {}\n        count = 0\n        for line in _espeak_exe(_DEF_ESPEAK_LIB, args, sync=True):\n            line = line.decode(\"utf8\").strip()\n            if count > 0:\n                cols = line.split()\n                lang_code = cols[1]\n                lang_name = cols[3]\n                langs[lang_code] = lang_name\n            logging.debug(\"line: %s\", repr(line))\n            count += 1\n        return langs\n\n    def version(self) -> str:\n        \"\"\"Get the version of the used backend.\n\n        Returns:\n            str: Version of the used backend.\n        \"\"\"\n        args = [\"--version\"]\n        for line in _espeak_exe(self.backend, args, sync=True):\n            version = line.decode(\"utf8\").strip().split()[2]\n            logging.debug(\"line: %s\", repr(line))\n            return version\n\n    @classmethod\n    def is_available(cls):\n        \"\"\"Return true if ESpeak is available else false\"\"\"\n        return is_tool(\"espeak\") or is_tool(\"espeak-ng\")\n\n\nif __name__ == \"__main__\":\n    e = ESpeak(language=\"en-us\")\n    print(e.supported_languages())\n    print(e.version())\n    print(e.language)\n    print(e.name())\n    print(e.is_available())\n\n    e = ESpeak(language=\"en-us\", keep_puncs=False)\n    print(\"`\" + e.phonemize(\"hello how are you today?\") + \"`\")\n\n    e = ESpeak(language=\"en-us\", keep_puncs=True)\n    print(\"`\" + e.phonemize(\"hello how are you today?\") + \"`\")\n", "TTS/tts/utils/text/phonemizers/ko_kr_phonemizer.py": "from typing import Dict\n\nfrom TTS.tts.utils.text.korean.phonemizer import korean_text_to_phonemes\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\n\n_DEF_KO_PUNCS = \"\u3001.,[]()?!\u303d~\u300e\u300f\u300c\u300d\u3010\u3011\"\n\n\nclass KO_KR_Phonemizer(BasePhonemizer):\n    \"\"\"\ud83d\udc38TTS ko_kr_phonemizer using functions in `TTS.tts.utils.text.korean.phonemizer`\n\n    TODO: Add Korean to character (\u1100\u1101\u1102\u1103\u1104\u1105\u1106\u1107\u1108\u1109\u110a\u110b\u110c\u110d\u110e\u110f\u1110\u1111\u1112\u1161\u1162\u1163\u1164\u1165\u1166\u1167\u1168\u1169\u116a\u116b\u116c\u116d\u116e\u116f\u1170\u1171\u1172\u1173\u1174\u1175\u11a8\u11a9\u11aa\u11ab\u11ac\u11ad\u11ae\u11af\u11b0\u11b1\u11b2\u11b3\u11b4\u11b5\u11b6\u11b7\u11b8\u11b9\u11ba\u11bb\u11bc\u11bd\u11be\u11bf\u11c0\u11c1\u11c2)\n\n    Example:\n\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer\n        >>> phonemizer = KO_KR_Phonemizer()\n        >>> phonemizer.phonemize(\"\uc774 \ubb38\uc7a5\uc740 \uc74c\uc131\ud569\uc131 \ud14c\uc2a4\ud2b8\ub97c \uc704\ud55c \ubb38\uc7a5\uc785\ub2c8\ub2e4.\", separator=\"|\")\n        '\u110b|\u1175| |\u1106|\u116e|\u11ab|\u110c|\u1161|\u11bc|\u110b|\u1173| |\u1102|\u1173|\u11b7|\u1109|\u1165|\u11bc|\u1112|\u1161|\u11b8|\u110a|\u1165|\u11bc| |\u1110|\u1166|\u1109|\u1173|\u1110|\u1173|\u1105|\u1173| |\u1105|\u1171|\u1112|\u1161|\u11ab| |\u1106|\u116e|\u11ab|\u110c|\u1161|\u11bc|\u110b|\u1175|\u11b7|\u1102|\u1175|\u1103|\u1161|.'\n\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer\n        >>> phonemizer = KO_KR_Phonemizer()\n        >>> phonemizer.phonemize(\"\uc774 \ubb38\uc7a5\uc740 \uc74c\uc131\ud569\uc131 \ud14c\uc2a4\ud2b8\ub97c \uc704\ud55c \ubb38\uc7a5\uc785\ub2c8\ub2e4.\", separator=\"|\", character='english')\n        'I| |M|u|n|J|a|n|g|E|u| |N|e|u|m|S|e|o|n|g|H|a|b|S|s|e|o|n|g| |T|e|S|e|u|T|e|u|L|e|u| |L|w|i|H|a|n| |M|u|n|J|a|n|g|I|m|N|i|D|a|.'\n\n    \"\"\"\n\n    language = \"ko-kr\"\n\n    def __init__(self, punctuations=_DEF_KO_PUNCS, keep_puncs=True, **kwargs):  # pylint: disable=unused-argument\n        super().__init__(self.language, punctuations=punctuations, keep_puncs=keep_puncs)\n\n    @staticmethod\n    def name():\n        return \"ko_kr_phonemizer\"\n\n    def _phonemize(self, text: str, separator: str = \"\", character: str = \"hangeul\") -> str:\n        ph = korean_text_to_phonemes(text, character=character)\n        if separator is not None or separator != \"\":\n            return separator.join(ph)\n        return ph\n\n    def phonemize(self, text: str, separator: str = \"\", character: str = \"hangeul\", language=None) -> str:\n        return self._phonemize(text, separator, character)\n\n    @staticmethod\n    def supported_languages() -> Dict:\n        return {\"ko-kr\": \"hangeul(korean)\"}\n\n    def version(self) -> str:\n        return \"0.0.2\"\n\n    def is_available(self) -> bool:\n        return True\n\n\nif __name__ == \"__main__\":\n    texts = \"\uc774 \ubb38\uc7a5\uc740 \uc74c\uc131\ud569\uc131 \ud14c\uc2a4\ud2b8\ub97c \uc704\ud55c \ubb38\uc7a5\uc785\ub2c8\ub2e4.\"\n    e = KO_KR_Phonemizer()\n    print(e.supported_languages())\n    print(e.version())\n    print(e.language)\n    print(e.name())\n    print(e.is_available())\n    print(e.phonemize(texts))\n", "TTS/tts/utils/text/phonemizers/__init__.py": "from TTS.tts.utils.text.phonemizers.bangla_phonemizer import BN_Phonemizer\nfrom TTS.tts.utils.text.phonemizers.base import BasePhonemizer\nfrom TTS.tts.utils.text.phonemizers.belarusian_phonemizer import BEL_Phonemizer\nfrom TTS.tts.utils.text.phonemizers.espeak_wrapper import ESpeak\nfrom TTS.tts.utils.text.phonemizers.gruut_wrapper import Gruut\nfrom TTS.tts.utils.text.phonemizers.ko_kr_phonemizer import KO_KR_Phonemizer\nfrom TTS.tts.utils.text.phonemizers.zh_cn_phonemizer import ZH_CN_Phonemizer\n\ntry:\n    from TTS.tts.utils.text.phonemizers.ja_jp_phonemizer import JA_JP_Phonemizer\nexcept ImportError:\n    JA_JP_Phonemizer = None\n    pass\n\nPHONEMIZERS = {b.name(): b for b in (ESpeak, Gruut, KO_KR_Phonemizer, BN_Phonemizer)}\n\n\nESPEAK_LANGS = list(ESpeak.supported_languages().keys())\nGRUUT_LANGS = list(Gruut.supported_languages())\n\n\n# Dict setting default phonemizers for each language\n# Add Gruut languages\n_ = [Gruut.name()] * len(GRUUT_LANGS)\nDEF_LANG_TO_PHONEMIZER = dict(list(zip(GRUUT_LANGS, _)))\n\n\n# Add ESpeak languages and override any existing ones\n_ = [ESpeak.name()] * len(ESPEAK_LANGS)\n_new_dict = dict(list(zip(list(ESPEAK_LANGS), _)))\nDEF_LANG_TO_PHONEMIZER.update(_new_dict)\n\n\n# Force default for some languages\nDEF_LANG_TO_PHONEMIZER[\"en\"] = DEF_LANG_TO_PHONEMIZER[\"en-us\"]\nDEF_LANG_TO_PHONEMIZER[\"zh-cn\"] = ZH_CN_Phonemizer.name()\nDEF_LANG_TO_PHONEMIZER[\"ko-kr\"] = KO_KR_Phonemizer.name()\nDEF_LANG_TO_PHONEMIZER[\"bn\"] = BN_Phonemizer.name()\nDEF_LANG_TO_PHONEMIZER[\"be\"] = BEL_Phonemizer.name()\n\n\n# JA phonemizer has deal breaking dependencies like MeCab for some systems.\n# So we only have it when we have it.\nif JA_JP_Phonemizer is not None:\n    PHONEMIZERS[JA_JP_Phonemizer.name()] = JA_JP_Phonemizer\n    DEF_LANG_TO_PHONEMIZER[\"ja-jp\"] = JA_JP_Phonemizer.name()\n\n\ndef get_phonemizer_by_name(name: str, **kwargs) -> BasePhonemizer:\n    \"\"\"Initiate a phonemizer by name\n\n    Args:\n        name (str):\n            Name of the phonemizer that should match `phonemizer.name()`.\n\n        kwargs (dict):\n            Extra keyword arguments that should be passed to the phonemizer.\n    \"\"\"\n    if name == \"espeak\":\n        return ESpeak(**kwargs)\n    if name == \"gruut\":\n        return Gruut(**kwargs)\n    if name == \"zh_cn_phonemizer\":\n        return ZH_CN_Phonemizer(**kwargs)\n    if name == \"ja_jp_phonemizer\":\n        if JA_JP_Phonemizer is None:\n            raise ValueError(\" \u2757 You need to install JA phonemizer dependencies. Try `pip install TTS[ja]`.\")\n        return JA_JP_Phonemizer(**kwargs)\n    if name == \"ko_kr_phonemizer\":\n        return KO_KR_Phonemizer(**kwargs)\n    if name == \"bn_phonemizer\":\n        return BN_Phonemizer(**kwargs)\n    if name == \"be_phonemizer\":\n        return BEL_Phonemizer(**kwargs)\n    raise ValueError(f\"Phonemizer {name} not found\")\n\n\nif __name__ == \"__main__\":\n    print(DEF_LANG_TO_PHONEMIZER)\n", "TTS/tts/utils/text/english/abbreviations.py": "import re\n\n# List of (regular expression, replacement) pairs for abbreviations in english:\nabbreviations_en = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"mrs\", \"misess\"),\n        (\"mr\", \"mister\"),\n        (\"dr\", \"doctor\"),\n        (\"st\", \"saint\"),\n        (\"co\", \"company\"),\n        (\"jr\", \"junior\"),\n        (\"maj\", \"major\"),\n        (\"gen\", \"general\"),\n        (\"drs\", \"doctors\"),\n        (\"rev\", \"reverend\"),\n        (\"lt\", \"lieutenant\"),\n        (\"hon\", \"honorable\"),\n        (\"sgt\", \"sergeant\"),\n        (\"capt\", \"captain\"),\n        (\"esq\", \"esquire\"),\n        (\"ltd\", \"limited\"),\n        (\"col\", \"colonel\"),\n        (\"ft\", \"fort\"),\n    ]\n]\n", "TTS/tts/utils/text/english/number_norm.py": "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n\nimport re\nfrom typing import Dict\n\nimport inflect\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_currency_re = re.compile(r\"(\u00a3|\\$|\u00a5)([0-9\\,\\.]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\n\n\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\n\n\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\n\n\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:\n    parts = value.replace(\",\", \"\").split(\".\")\n    if len(parts) > 2:\n        return f\"{value} {inflection[2]}\"  # Unexpected format\n    text = []\n    integer = int(parts[0]) if parts[0] else 0\n    if integer > 0:\n        integer_unit = inflection.get(integer, inflection[2])\n        text.append(f\"{integer} {integer_unit}\")\n    fraction = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if fraction > 0:\n        fraction_unit = inflection.get(fraction / 100, inflection[0.02])\n        text.append(f\"{fraction} {fraction_unit}\")\n    if len(text) == 0:\n        return f\"zero {inflection[2]}\"\n    return \" \".join(text)\n\n\ndef _expand_currency(m: \"re.Match\") -> str:\n    currencies = {\n        \"$\": {\n            0.01: \"cent\",\n            0.02: \"cents\",\n            1: \"dollar\",\n            2: \"dollars\",\n        },\n        \"\u20ac\": {\n            0.01: \"cent\",\n            0.02: \"cents\",\n            1: \"euro\",\n            2: \"euros\",\n        },\n        \"\u00a3\": {\n            0.01: \"penny\",\n            0.02: \"pence\",\n            1: \"pound sterling\",\n            2: \"pounds sterling\",\n        },\n        \"\u00a5\": {\n            # TODO rin\n            0.02: \"sen\",\n            2: \"yen\",\n        },\n    }\n    unit = m.group(1)\n    currency = currencies[unit]\n    value = m.group(2)\n    return __expand_currency(value, currency)\n\n\ndef _expand_ordinal(m):\n    return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n    num = int(m.group(0))\n    if 1000 < num < 3000:\n        if num == 2000:\n            return \"two thousand\"\n        if 2000 < num < 2010:\n            return \"two thousand \" + _inflect.number_to_words(num % 100)\n        if num % 100 == 0:\n            return _inflect.number_to_words(num // 100) + \" hundred\"\n        return _inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n    return _inflect.number_to_words(num, andword=\"\")\n\n\ndef normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_currency_re, _expand_currency, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text\n", "TTS/tts/utils/text/english/time_norm.py": "import re\n\nimport inflect\n\n_inflect = inflect.engine()\n\n_time_re = re.compile(\n    r\"\"\"\\b\n                          ((0?[0-9])|(1[0-1])|(1[2-9])|(2[0-3]))  # hours\n                          :\n                          ([0-5][0-9])                            # minutes\n                          \\s*(a\\\\.m\\\\.|am|pm|p\\\\.m\\\\.|a\\\\.m|p\\\\.m)? # am/pm\n                          \\b\"\"\",\n    re.IGNORECASE | re.X,\n)\n\n\ndef _expand_num(n: int) -> str:\n    return _inflect.number_to_words(n)\n\n\ndef _expand_time_english(match: \"re.Match\") -> str:\n    hour = int(match.group(1))\n    past_noon = hour >= 12\n    time = []\n    if hour > 12:\n        hour -= 12\n    elif hour == 0:\n        hour = 12\n        past_noon = True\n    time.append(_expand_num(hour))\n\n    minute = int(match.group(6))\n    if minute > 0:\n        if minute < 10:\n            time.append(\"oh\")\n        time.append(_expand_num(minute))\n    am_pm = match.group(7)\n    if am_pm is None:\n        time.append(\"p m\" if past_noon else \"a m\")\n    else:\n        time.extend(list(am_pm.replace(\".\", \"\")))\n    return \" \".join(time)\n\n\ndef expand_time_english(text: str) -> str:\n    return re.sub(_time_re, _expand_time_english, text)\n", "TTS/tts/utils/text/english/__init__.py": "", "TTS/tts/utils/text/french/abbreviations.py": "import re\n\n# List of (regular expression, replacement) pairs for abbreviations in french:\nabbreviations_fr = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"M\", \"monsieur\"),\n        (\"Mlle\", \"mademoiselle\"),\n        (\"Mlles\", \"mesdemoiselles\"),\n        (\"Mme\", \"Madame\"),\n        (\"Mmes\", \"Mesdames\"),\n        (\"N.B\", \"nota bene\"),\n        (\"M\", \"monsieur\"),\n        (\"p.c.q\", \"parce que\"),\n        (\"Pr\", \"professeur\"),\n        (\"qqch\", \"quelque chose\"),\n        (\"rdv\", \"rendez-vous\"),\n        (\"max\", \"maximum\"),\n        (\"min\", \"minimum\"),\n        (\"no\", \"num\u00e9ro\"),\n        (\"adr\", \"adresse\"),\n        (\"dr\", \"docteur\"),\n        (\"st\", \"saint\"),\n        (\"co\", \"companie\"),\n        (\"jr\", \"junior\"),\n        (\"sgt\", \"sergent\"),\n        (\"capt\", \"capitain\"),\n        (\"col\", \"colonel\"),\n        (\"av\", \"avenue\"),\n        (\"av. J.-C\", \"avant J\u00e9sus-Christ\"),\n        (\"apr. J.-C\", \"apr\u00e8s J\u00e9sus-Christ\"),\n        (\"art\", \"article\"),\n        (\"boul\", \"boulevard\"),\n        (\"c.-\u00e0-d\", \"c\u2019est-\u00e0-dire\"),\n        (\"etc\", \"et cetera\"),\n        (\"ex\", \"exemple\"),\n        (\"excl\", \"exclusivement\"),\n        (\"boul\", \"boulevard\"),\n    ]\n] + [\n    (re.compile(\"\\\\b%s\" % x[0]), x[1])\n    for x in [\n        (\"Mlle\", \"mademoiselle\"),\n        (\"Mlles\", \"mesdemoiselles\"),\n        (\"Mme\", \"Madame\"),\n        (\"Mmes\", \"Mesdames\"),\n    ]\n]\n", "TTS/tts/utils/text/french/__init__.py": "", "TTS/tts/utils/text/korean/phonemizer.py": "from jamo import hangul_to_jamo\n\nfrom TTS.tts.utils.text.korean.korean import normalize\n\ng2p = None\n\n\ndef korean_text_to_phonemes(text, character: str = \"hangeul\") -> str:\n    \"\"\"\n\n    The input and output values look the same, but they are different in Unicode.\n\n    example :\n\n        input = '\ud558\ub298' (Unicode : \\ud558\\ub298), (\ud558 + \ub298)\n        output = '\u1112\u1161\u1102\u1173\u11af' (Unicode :\\u1112\\u1161\\u1102\\u1173\\u11af), (\u1112 + \u1161 + \u1102 + \u1173 + \u11af)\n\n    \"\"\"\n    global g2p  # pylint: disable=global-statement\n    if g2p is None:\n        from g2pkk import G2p\n\n        g2p = G2p()\n\n    if character == \"english\":\n        from anyascii import anyascii\n\n        text = normalize(text)\n        text = g2p(text)\n        text = anyascii(text)\n        return text\n\n    text = normalize(text)\n    text = g2p(text)\n    text = list(hangul_to_jamo(text))  # '\ud558\ub298' --> ['\u1112', '\u1161', '\u1102', '\u1173', '\u11af']\n    return \"\".join(text)\n", "TTS/tts/utils/text/korean/ko_dictionary.py": "# coding: utf-8\n# Add the word you want to the dictionary.\netc_dictionary = {\"1+1\": \"\uc6d0\ud50c\ub7ec\uc2a4\uc6d0\", \"2+1\": \"\ud22c\ud50c\ub7ec\uc2a4\uc6d0\"}\n\n\nenglish_dictionary = {\n    \"KOREA\": \"\ucf54\ub9ac\uc544\",\n    \"IDOL\": \"\uc544\uc774\ub3cc\",\n    \"IT\": \"\uc544\uc774\ud2f0\",\n    \"IQ\": \"\uc544\uc774\ud050\",\n    \"UP\": \"\uc5c5\",\n    \"DOWN\": \"\ub2e4\uc6b4\",\n    \"PC\": \"\ud53c\uc528\",\n    \"CCTV\": \"\uc528\uc528\ud2f0\ube44\",\n    \"SNS\": \"\uc5d0\uc2a4\uc5d4\uc5d0\uc2a4\",\n    \"AI\": \"\uc5d0\uc774\uc544\uc774\",\n    \"CEO\": \"\uc528\uc774\uc624\",\n    \"A\": \"\uc5d0\uc774\",\n    \"B\": \"\ube44\",\n    \"C\": \"\uc528\",\n    \"D\": \"\ub514\",\n    \"E\": \"\uc774\",\n    \"F\": \"\uc5d0\ud504\",\n    \"G\": \"\uc9c0\",\n    \"H\": \"\uc5d0\uc774\uce58\",\n    \"I\": \"\uc544\uc774\",\n    \"J\": \"\uc81c\uc774\",\n    \"K\": \"\ucf00\uc774\",\n    \"L\": \"\uc5d8\",\n    \"M\": \"\uc5e0\",\n    \"N\": \"\uc5d4\",\n    \"O\": \"\uc624\",\n    \"P\": \"\ud53c\",\n    \"Q\": \"\ud050\",\n    \"R\": \"\uc54c\",\n    \"S\": \"\uc5d0\uc2a4\",\n    \"T\": \"\ud2f0\",\n    \"U\": \"\uc720\",\n    \"V\": \"\ube0c\uc774\",\n    \"W\": \"\ub354\ube14\uc720\",\n    \"X\": \"\uc5d1\uc2a4\",\n    \"Y\": \"\uc640\uc774\",\n    \"Z\": \"\uc81c\ud2b8\",\n}\n", "TTS/tts/utils/text/korean/__init__.py": "", "TTS/tts/utils/text/bangla/phonemizer.py": "import re\n\nimport bangla\nfrom bnnumerizer import numerize\nfrom bnunicodenormalizer import Normalizer\n\n# initialize\nbnorm = Normalizer()\n\n\nattribution_dict = {\n    \"\u09b8\u09be\u0983\": \"\u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09b2\u09cd\u09b2\u09be\u09b9\u09c1 \u0986\u09b2\u09be\u0987\u09b9\u09bf \u0993\u09af\u09bc\u09be \u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09ae\",\n    \"\u0986\u0983\": \"\u0986\u09b2\u09be\u0987\u09b9\u09bf\u09b8 \u09b8\u09be\u09b2\u09be\u09ae\",\n    \"\u09b0\u09be\u0983\": \"\u09b0\u09be\u09a6\u09bf\u0986\u09b2\u09cd\u09b2\u09be\u09b9\u09c1 \u0986\u09a8\u09b9\u09c1\",\n    \"\u09b0\u09b9\u0983\": \"\u09b0\u09b9\u09ae\u09be\u09a4\u09c1\u09b2\u09cd\u09b2\u09be\u09b9\u09bf \u0986\u09b2\u09be\u0987\u09b9\u09bf\",\n    \"\u09b0\u09b9\u09bf\u0983\": \"\u09b0\u09b9\u09bf\u09ae\u09be\u09b9\u09c1\u09b2\u09cd\u09b2\u09be\u09b9\",\n    \"\u09b9\u09be\u09ab\u09bf\u0983\": \"\u09b9\u09be\u09ab\u09bf\u09af\u09be\u09b9\u09c1\u09b2\u09cd\u09b2\u09be\u09b9\",\n    \"\u09ac\u09be\u09af\u09bc\u09be\u09a8\": \"\u09ac\u09be\u0987\u0986\u09a8\",\n    \"\u09a6\u09be\u0983\u09ac\u09be\u0983\": \"\u09a6\u09be\u09ae\u09be\u09a4 \u09ac\u09be\u09b0\u09be\u0995\u09be\u09a4\u09c1\u09b9\u09c1\u09ae,\u09a6\u09be\u09ae\u09be\u09a4 \u09ac\u09be\u09b0\u09be\u0995\u09be\u09a4\u09c1\u09b2\u09cd\u09b2\u09be\u09b9\",\n    # \"\u0986\u09df\u09be\u09a4\" : \"\u0986\u0987\u0986\u09a4\",#\u0986\u0987\u0986\u09a4\n    # \"\u0993\u09df\u09be\" : \"\u0993\u0986\",\n    # \"\u0993\u09df\u09be\u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09ae\"  : \"\u0993\u0986\u09b8\u09be\u09b2\u09cd\u09b2\u09be\u09ae\",\n    # \"\u0995\u09c7\u09a8\"  : \"\u0995\u09c7\u09a8\u09cb\",\n    # \"\u0995\u09cb\u09a8\" : \"\u0995\u09cb\u09a8\u09cb\",\n    # \"\u09ac\u09b2\"   : \"\u09ac\u09b2\u09cb\",\n    # \"\u099a\u09b2\"   : \"\u099a\u09b2\u09cb\",\n    # \"\u0995\u09b0\"   : \"\u0995\u09b0\u09cb\",\n    # \"\u09b0\u09be\u0996\"   : \"\u09b0\u09be\u0996\u09cb\",\n    \"\u2019\": \"\",\n    \"\u2018\": \"\",\n    # \"\u09df\"     : \"\u0985\",\n    # \"\u09b8\u09ae\u09cd\u09aa\u09cd\u09b0\u09a6\u09be\u09df\" : \"\u09b8\u09ae\u09cd\u09aa\u09cd\u09b0\u09a6\u09be\u0987\",\n    # \"\u09b0\u09df\u09c7\u099b\u09c7\"   : \"\u09b0\u0987\u099b\u09c7\",\n    # \"\u09b0\u09df\u09c7\u099b\"    : \"\u09b0\u0987\u099b\",\n    \"/\": \" \u09ac\u09be\u0987 \",\n}\n\n\ndef tag_text(text: str):\n    # remove multiple spaces\n    text = re.sub(\" +\", \" \", text)\n    # create start and end\n    text = \"start\" + text + \"end\"\n    # tag text\n    parts = re.split(\"[\\u0600-\\u06FF]+\", text)\n    # remove non chars\n    parts = [p for p in parts if p.strip()]\n    # unique parts\n    parts = set(parts)\n    # tag the text\n    for m in parts:\n        if len(m.strip()) > 1:\n            text = text.replace(m, f\"{m}\")\n    # clean-tags\n    text = text.replace(\"start\", \"\")\n    text = text.replace(\"end\", \"\")\n    return text\n\n\ndef normalize(sen):\n    global bnorm  # pylint: disable=global-statement\n    _words = [bnorm(word)[\"normalized\"] for word in sen.split()]\n    return \" \".join([word for word in _words if word is not None])\n\n\ndef expand_full_attribution(text):\n    for word, attr in attribution_dict.items():\n        if word in text:\n            text = text.replace(word, normalize(attr))\n    return text\n\n\ndef collapse_whitespace(text):\n    # Regular expression matching whitespace:\n    _whitespace_re = re.compile(r\"\\s+\")\n    return re.sub(_whitespace_re, \" \", text)\n\n\ndef bangla_text_to_phonemes(text: str) -> str:\n    # english numbers to bangla conversion\n    res = re.search(\"[0-9]\", text)\n    if res is not None:\n        text = bangla.convert_english_digit_to_bangla_digit(text)\n\n    # replace ':' in between two bangla numbers with ' \u098f\u09b0 '\n    pattern = r\"[\u09e6, \u09e7, \u09e8, \u09e9, \u09ea, \u09eb, \u09ec, \u09ed, \u09ee, \u09ef]:[\u09e6, \u09e7, \u09e8, \u09e9, \u09ea, \u09eb, \u09ec, \u09ed, \u09ee, \u09ef]\"\n    matches = re.findall(pattern, text)\n    for m in matches:\n        r = m.replace(\":\", \" \u098f\u09b0 \")\n        text = text.replace(m, r)\n\n    # numerize text\n    text = numerize(text)\n\n    # tag sections\n    text = tag_text(text)\n\n    # text blocks\n    # blocks = text.split(\"\")\n    # blocks = [b for b in blocks if b.strip()]\n\n    # create tuple of (lang,text)\n    if \"\" in text:\n        text = text.replace(\"\", \"\").replace(\"\", \"\")\n    # Split based on sentence ending Characters\n    bn_text = text.strip()\n\n    sentenceEnders = re.compile(\"[\u0964!?]\")\n    sentences = sentenceEnders.split(str(bn_text))\n\n    data = \"\"\n    for sent in sentences:\n        res = re.sub(\"\\n\", \"\", sent)\n        res = normalize(res)\n        # expand attributes\n        res = expand_full_attribution(res)\n\n        res = collapse_whitespace(res)\n        res += \"\u0964\"\n        data += res\n    return data\n", "TTS/tts/utils/text/bangla/__init__.py": "", "TTS/tts/utils/text/belarusian/phonemizer.py": "import os\n\nfinder = None\n\n\ndef init():\n    try:\n        import jpype\n        import jpype.imports\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Belarusian phonemizer requires to install module 'jpype1' manually. Try `pip install jpype1`.\"\n        )\n\n    try:\n        jar_path = os.environ[\"BEL_FANETYKA_JAR\"]\n    except KeyError:\n        raise KeyError(\"You need to define 'BEL_FANETYKA_JAR' environment variable as path to the fanetyka.jar file\")\n\n    jpype.startJVM(classpath=[jar_path])\n\n    # import the Java modules\n    from org.alex73.korpus.base import GrammarDB2, GrammarFinder\n\n    grammar_db = GrammarDB2.initializeFromJar()\n    global finder\n    finder = GrammarFinder(grammar_db)\n\n\ndef belarusian_text_to_phonemes(text: str) -> str:\n    # Initialize only on first run\n    if finder is None:\n        init()\n\n    from org.alex73.fanetyka.impl import FanetykaText\n\n    return str(FanetykaText(finder, text).ipa)\n", "TTS/tts/utils/text/belarusian/__init__.py": "", "TTS/tts/utils/text/chinese_mandarin/phonemizer.py": "from typing import List\n\nimport jieba\nimport pypinyin\n\nfrom .pinyinToPhonemes import PINYIN_DICT\n\n\ndef _chinese_character_to_pinyin(text: str) -> List[str]:\n    pinyins = pypinyin.pinyin(text, style=pypinyin.Style.TONE3, heteronym=False, neutral_tone_with_five=True)\n    pinyins_flat_list = [item for sublist in pinyins for item in sublist]\n    return pinyins_flat_list\n\n\ndef _chinese_pinyin_to_phoneme(pinyin: str) -> str:\n    segment = pinyin[:-1]\n    tone = pinyin[-1]\n    phoneme = PINYIN_DICT.get(segment, [\"\"])[0]\n    return phoneme + tone\n\n\ndef chinese_text_to_phonemes(text: str, seperator: str = \"|\") -> str:\n    tokenized_text = jieba.cut(text, HMM=False)\n    tokenized_text = \" \".join(tokenized_text)\n    pinyined_text: List[str] = _chinese_character_to_pinyin(tokenized_text)\n\n    results: List[str] = []\n\n    for token in pinyined_text:\n        if token[-1] in \"12345\":  # TODO transform to is_pinyin()\n            pinyin_phonemes = _chinese_pinyin_to_phoneme(token)\n\n            results += list(pinyin_phonemes)\n        else:  # is ponctuation or other\n            results += list(token)\n\n    return seperator.join(results)\n", "TTS/tts/utils/text/chinese_mandarin/__init__.py": "", "TTS/tts/utils/text/chinese_mandarin/numbers.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Licensed under WTFPL or the Unlicense or CC0.\n# This uses Python 3, but it's easy to port to Python 2 by changing\n# strings to u'xx'.\n\nimport itertools\nimport re\n\n\ndef _num2chinese(num: str, big=False, simp=True, o=False, twoalt=False) -> str:\n    \"\"\"Convert numerical arabic numbers (0->9) to chinese hanzi numbers (\u3007 -> \u4e5d)\n\n    Args:\n        num (str): arabic number to convert\n        big (bool, optional): use financial characters. Defaults to False.\n        simp (bool, optional): use simplified characters instead of tradictional characters. Defaults to True.\n        o (bool, optional): use \u3007 for 'zero'. Defaults to False.\n        twoalt (bool, optional): use \u4e24/\u5169 for 'two' when appropriate. Defaults to False.\n\n    Raises:\n        ValueError: if number is more than 1e48\n        ValueError: if 'e' exposent in number\n\n    Returns:\n        str: converted number as hanzi characters\n    \"\"\"\n\n    # check num first\n    nd = str(num)\n    if abs(float(nd)) >= 1e48:\n        raise ValueError(\"number out of range\")\n    if \"e\" in nd:\n        raise ValueError(\"scientific notation is not supported\")\n    c_symbol = \"\u6b63\u8d1f\u70b9\" if simp else \"\u6b63\u8ca0\u9ede\"\n    if o:  # formal\n        twoalt = False\n    if big:\n        c_basic = \"\u96f6\u58f9\u8d30\u53c1\u8086\u4f0d\u9646\u67d2\u634c\u7396\" if simp else \"\u96f6\u58f9\u8cb3\u53c3\u8086\u4f0d\u9678\u67d2\u634c\u7396\"\n        c_unit1 = \"\u62fe\u4f70\u4edf\"\n        c_twoalt = \"\u8d30\" if simp else \"\u8cb3\"\n    else:\n        c_basic = \"\u3007\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\" if o else \"\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\"\n        c_unit1 = \"\u5341\u767e\u5343\"\n        if twoalt:\n            c_twoalt = \"\u4e24\" if simp else \"\u5169\"\n        else:\n            c_twoalt = \"\u4e8c\"\n    c_unit2 = \"\u4e07\u4ebf\u5146\u4eac\u5793\u79ed\u7a70\u6c9f\u6da7\u6b63\u8f7d\" if simp else \"\u842c\u5104\u5146\u4eac\u5793\u79ed\u7a70\u6e9d\u6f97\u6b63\u8f09\"\n    revuniq = lambda l: \"\".join(k for k, g in itertools.groupby(reversed(l)))\n    nd = str(num)\n    result = []\n    if nd[0] == \"+\":\n        result.append(c_symbol[0])\n    elif nd[0] == \"-\":\n        result.append(c_symbol[1])\n    if \".\" in nd:\n        integer, remainder = nd.lstrip(\"+-\").split(\".\")\n    else:\n        integer, remainder = nd.lstrip(\"+-\"), None\n    if int(integer):\n        splitted = [integer[max(i - 4, 0) : i] for i in range(len(integer), 0, -4)]\n        intresult = []\n        for nu, unit in enumerate(splitted):\n            # special cases\n            if int(unit) == 0:  # 0000\n                intresult.append(c_basic[0])\n                continue\n            if nu > 0 and int(unit) == 2:  # 0002\n                intresult.append(c_twoalt + c_unit2[nu - 1])\n                continue\n            ulist = []\n            unit = unit.zfill(4)\n            for nc, ch in enumerate(reversed(unit)):\n                if ch == \"0\":\n                    if ulist:  # ???0\n                        ulist.append(c_basic[0])\n                elif nc == 0:\n                    ulist.append(c_basic[int(ch)])\n                elif nc == 1 and ch == \"1\" and unit[1] == \"0\":\n                    # special case for tens\n                    # edit the 'elif' if you don't like\n                    # \u5341\u56db, \u4e09\u5343\u96f6\u5341\u56db, \u4e09\u5343\u4e09\u767e\u4e00\u5341\u56db\n                    ulist.append(c_unit1[0])\n                elif nc > 1 and ch == \"2\":\n                    ulist.append(c_twoalt + c_unit1[nc - 1])\n                else:\n                    ulist.append(c_basic[int(ch)] + c_unit1[nc - 1])\n            ustr = revuniq(ulist)\n            if nu == 0:\n                intresult.append(ustr)\n            else:\n                intresult.append(ustr + c_unit2[nu - 1])\n        result.append(revuniq(intresult).strip(c_basic[0]))\n    else:\n        result.append(c_basic[0])\n    if remainder:\n        result.append(c_symbol[2])\n        result.append(\"\".join(c_basic[int(ch)] for ch in remainder))\n    return \"\".join(result)\n\n\ndef _number_replace(match) -> str:\n    \"\"\"function to apply in a match, transform all numbers in a match by chinese characters\n\n    Args:\n        match (re.Match): numbers regex matches\n\n    Returns:\n        str: replaced characters for the numbers\n    \"\"\"\n    match_str: str = match.group()\n    return _num2chinese(match_str)\n\n\ndef replace_numbers_to_characters_in_text(text: str) -> str:\n    \"\"\"Replace all arabic numbers in a text by their equivalent in chinese characters (simplified)\n\n    Args:\n        text (str): input text to transform\n\n    Returns:\n        str: output text\n    \"\"\"\n    text = re.sub(r\"[0-9]+\", _number_replace, text)\n    return text\n", "TTS/tts/utils/text/chinese_mandarin/pinyinToPhonemes.py": "PINYIN_DICT = {\n    \"a\": [\"a\"],\n    \"ai\": [\"ai\"],\n    \"an\": [\"an\"],\n    \"ang\": [\"\u0251\u014b\"],\n    \"ao\": [\"a\u028c\"],\n    \"ba\": [\"ba\"],\n    \"bai\": [\"bai\"],\n    \"ban\": [\"ban\"],\n    \"bang\": [\"b\u0251\u014b\"],\n    \"bao\": [\"ba\u028c\"],\n    # \"be\": [\"be\"], doesnt exist\n    \"bei\": [\"b\u025bi\"],\n    \"ben\": [\"b\u0153n\"],\n    \"beng\": [\"b\u0275\u014b\"],\n    \"bi\": [\"bi\"],\n    \"bian\": [\"bi\u025bn\"],\n    \"biao\": [\"bia\u028c\"],\n    \"bie\": [\"bie\"],\n    \"bin\": [\"bin\"],\n    \"bing\": [\"b\u0268\u014b\"],\n    \"bo\": [\"bo\"],\n    \"bu\": [\"bu\"],\n    \"ca\": [\"tsa\"],\n    \"cai\": [\"tsai\"],\n    \"can\": [\"tsan\"],\n    \"cang\": [\"ts\u0251\u014b\"],\n    \"cao\": [\"tsa\u028c\"],\n    \"ce\": [\"ts\u00f8\"],\n    \"cen\": [\"ts\u0153n\"],\n    \"ceng\": [\"ts\u0275\u014b\"],\n    \"cha\": [\"\u0288\u0282a\"],\n    \"chai\": [\"\u0288\u0282ai\"],\n    \"chan\": [\"\u0288\u0282an\"],\n    \"chang\": [\"\u0288\u0282\u0251\u014b\"],\n    \"chao\": [\"\u0288\u0282a\u028c\"],\n    \"che\": [\"\u0288\u0282\u00f8\"],\n    \"chen\": [\"\u0288\u0282\u0153n\"],\n    \"cheng\": [\"\u0288\u0282\u0275\u014b\"],\n    \"chi\": [\"\u0288\u0282\u028f\"],\n    \"chong\": [\"\u0288\u0282o\u014b\"],\n    \"chou\": [\"\u0288\u0282ou\"],\n    \"chu\": [\"\u0288\u0282u\"],\n    \"chua\": [\"\u0288\u0282ua\"],\n    \"chuai\": [\"\u0288\u0282uai\"],\n    \"chuan\": [\"\u0288\u0282uan\"],\n    \"chuang\": [\"\u0288\u0282u\u0251\u014b\"],\n    \"chui\": [\"\u0288\u0282uei\"],\n    \"chun\": [\"\u0288\u0282un\"],\n    \"chuo\": [\"\u0288\u0282uo\"],\n    \"ci\": [\"ts\u026a\"],\n    \"cong\": [\"tso\u014b\"],\n    \"cou\": [\"tsou\"],\n    \"cu\": [\"tsu\"],\n    \"cuan\": [\"tsuan\"],\n    \"cui\": [\"tsuei\"],\n    \"cun\": [\"tsun\"],\n    \"cuo\": [\"tsuo\"],\n    \"da\": [\"da\"],\n    \"dai\": [\"dai\"],\n    \"dan\": [\"dan\"],\n    \"dang\": [\"d\u0251\u014b\"],\n    \"dao\": [\"da\u028c\"],\n    \"de\": [\"d\u00f8\"],\n    \"dei\": [\"dei\"],\n    # \"den\": [\"d\u0153n\"],\n    \"deng\": [\"d\u0275\u014b\"],\n    \"di\": [\"di\"],\n    \"dia\": [\"dia\"],\n    \"dian\": [\"di\u025bn\"],\n    \"diao\": [\"dia\u028c\"],\n    \"die\": [\"die\"],\n    \"ding\": [\"d\u0268\u014b\"],\n    \"diu\": [\"dio\"],\n    \"dong\": [\"do\u014b\"],\n    \"dou\": [\"dou\"],\n    \"du\": [\"du\"],\n    \"duan\": [\"duan\"],\n    \"dui\": [\"duei\"],\n    \"dun\": [\"dun\"],\n    \"duo\": [\"duo\"],\n    \"e\": [\"\u00f8\"],\n    \"ei\": [\"ei\"],\n    \"en\": [\"\u0153n\"],\n    # \"ng\": [\"\u0153n\"],\n    # \"eng\": [\"\u0275\u014b\"],\n    \"er\": [\"er\"],\n    \"fa\": [\"fa\"],\n    \"fan\": [\"fan\"],\n    \"fang\": [\"f\u0251\u014b\"],\n    \"fei\": [\"fei\"],\n    \"fen\": [\"f\u0153n\"],\n    \"feng\": [\"f\u0275\u014b\"],\n    \"fo\": [\"fo\"],\n    \"fou\": [\"fou\"],\n    \"fu\": [\"fu\"],\n    \"ga\": [\"ga\"],\n    \"gai\": [\"gai\"],\n    \"gan\": [\"gan\"],\n    \"gang\": [\"g\u0251\u014b\"],\n    \"gao\": [\"ga\u028c\"],\n    \"ge\": [\"g\u00f8\"],\n    \"gei\": [\"gei\"],\n    \"gen\": [\"g\u0153n\"],\n    \"geng\": [\"g\u0275\u014b\"],\n    \"gong\": [\"go\u014b\"],\n    \"gou\": [\"gou\"],\n    \"gu\": [\"gu\"],\n    \"gua\": [\"gua\"],\n    \"guai\": [\"guai\"],\n    \"guan\": [\"guan\"],\n    \"guang\": [\"gu\u0251\u014b\"],\n    \"gui\": [\"guei\"],\n    \"gun\": [\"gun\"],\n    \"guo\": [\"guo\"],\n    \"ha\": [\"xa\"],\n    \"hai\": [\"xai\"],\n    \"han\": [\"xan\"],\n    \"hang\": [\"x\u0251\u014b\"],\n    \"hao\": [\"xa\u028c\"],\n    \"he\": [\"x\u00f8\"],\n    \"hei\": [\"xei\"],\n    \"hen\": [\"x\u0153n\"],\n    \"heng\": [\"x\u0275\u014b\"],\n    \"hong\": [\"xo\u014b\"],\n    \"hou\": [\"xou\"],\n    \"hu\": [\"xu\"],\n    \"hua\": [\"xua\"],\n    \"huai\": [\"xuai\"],\n    \"huan\": [\"xuan\"],\n    \"huang\": [\"xu\u0251\u014b\"],\n    \"hui\": [\"xuei\"],\n    \"hun\": [\"xun\"],\n    \"huo\": [\"xuo\"],\n    \"ji\": [\"d\u0291i\"],\n    \"jia\": [\"d\u0291ia\"],\n    \"jian\": [\"d\u0291i\u025bn\"],\n    \"jiang\": [\"d\u0291i\u0251\u014b\"],\n    \"jiao\": [\"d\u0291ia\u028c\"],\n    \"jie\": [\"d\u0291ie\"],\n    \"jin\": [\"d\u0291in\"],\n    \"jing\": [\"d\u0291\u0268\u014b\"],\n    \"jiong\": [\"d\u0291io\u014b\"],\n    \"jiu\": [\"d\u0291io\"],\n    \"ju\": [\"d\u0291y\"],\n    \"juan\": [\"d\u0291y\u025bn\"],\n    \"jue\": [\"d\u0291ye\"],\n    \"jun\": [\"d\u0291yn\"],\n    \"ka\": [\"ka\"],\n    \"kai\": [\"kai\"],\n    \"kan\": [\"kan\"],\n    \"kang\": [\"k\u0251\u014b\"],\n    \"kao\": [\"ka\u028c\"],\n    \"ke\": [\"k\u00f8\"],\n    \"kei\": [\"kei\"],\n    \"ken\": [\"k\u0153n\"],\n    \"keng\": [\"k\u0275\u014b\"],\n    \"kong\": [\"ko\u014b\"],\n    \"kou\": [\"kou\"],\n    \"ku\": [\"ku\"],\n    \"kua\": [\"kua\"],\n    \"kuai\": [\"kuai\"],\n    \"kuan\": [\"kuan\"],\n    \"kuang\": [\"ku\u0251\u014b\"],\n    \"kui\": [\"kuei\"],\n    \"kun\": [\"kun\"],\n    \"kuo\": [\"kuo\"],\n    \"la\": [\"la\"],\n    \"lai\": [\"lai\"],\n    \"lan\": [\"lan\"],\n    \"lang\": [\"l\u0251\u014b\"],\n    \"lao\": [\"la\u028c\"],\n    \"le\": [\"l\u00f8\"],\n    \"lei\": [\"lei\"],\n    \"leng\": [\"l\u0275\u014b\"],\n    \"li\": [\"li\"],\n    \"lia\": [\"lia\"],\n    \"lian\": [\"li\u025bn\"],\n    \"liang\": [\"li\u0251\u014b\"],\n    \"liao\": [\"lia\u028c\"],\n    \"lie\": [\"lie\"],\n    \"lin\": [\"lin\"],\n    \"ling\": [\"l\u0268\u014b\"],\n    \"liu\": [\"lio\"],\n    \"lo\": [\"lo\"],\n    \"long\": [\"lo\u014b\"],\n    \"lou\": [\"lou\"],\n    \"lu\": [\"lu\"],\n    \"lv\": [\"ly\"],\n    \"luan\": [\"luan\"],\n    \"lve\": [\"lye\"],\n    \"lue\": [\"lue\"],\n    \"lun\": [\"lun\"],\n    \"luo\": [\"luo\"],\n    \"ma\": [\"ma\"],\n    \"mai\": [\"mai\"],\n    \"man\": [\"man\"],\n    \"mang\": [\"m\u0251\u014b\"],\n    \"mao\": [\"ma\u028c\"],\n    \"me\": [\"m\u00f8\"],\n    \"mei\": [\"mei\"],\n    \"men\": [\"m\u0153n\"],\n    \"meng\": [\"m\u0275\u014b\"],\n    \"mi\": [\"mi\"],\n    \"mian\": [\"mi\u025bn\"],\n    \"miao\": [\"mia\u028c\"],\n    \"mie\": [\"mie\"],\n    \"min\": [\"min\"],\n    \"ming\": [\"m\u0268\u014b\"],\n    \"miu\": [\"mio\"],\n    \"mo\": [\"mo\"],\n    \"mou\": [\"mou\"],\n    \"mu\": [\"mu\"],\n    \"na\": [\"na\"],\n    \"nai\": [\"nai\"],\n    \"nan\": [\"nan\"],\n    \"nang\": [\"n\u0251\u014b\"],\n    \"nao\": [\"na\u028c\"],\n    \"ne\": [\"n\u00f8\"],\n    \"nei\": [\"nei\"],\n    \"nen\": [\"n\u0153n\"],\n    \"neng\": [\"n\u0275\u014b\"],\n    \"ni\": [\"ni\"],\n    \"nia\": [\"nia\"],\n    \"nian\": [\"ni\u025bn\"],\n    \"niang\": [\"ni\u0251\u014b\"],\n    \"niao\": [\"nia\u028c\"],\n    \"nie\": [\"nie\"],\n    \"nin\": [\"nin\"],\n    \"ning\": [\"n\u0268\u014b\"],\n    \"niu\": [\"nio\"],\n    \"nong\": [\"no\u014b\"],\n    \"nou\": [\"nou\"],\n    \"nu\": [\"nu\"],\n    \"nv\": [\"ny\"],\n    \"nuan\": [\"nuan\"],\n    \"nve\": [\"nye\"],\n    \"nue\": [\"nye\"],\n    \"nuo\": [\"nuo\"],\n    \"o\": [\"o\"],\n    \"ou\": [\"ou\"],\n    \"pa\": [\"pa\"],\n    \"pai\": [\"pai\"],\n    \"pan\": [\"pan\"],\n    \"pang\": [\"p\u0251\u014b\"],\n    \"pao\": [\"pa\u028c\"],\n    \"pe\": [\"p\u00f8\"],\n    \"pei\": [\"pei\"],\n    \"pen\": [\"p\u0153n\"],\n    \"peng\": [\"p\u0275\u014b\"],\n    \"pi\": [\"pi\"],\n    \"pian\": [\"pi\u025bn\"],\n    \"piao\": [\"pia\u028c\"],\n    \"pie\": [\"pie\"],\n    \"pin\": [\"pin\"],\n    \"ping\": [\"p\u0268\u014b\"],\n    \"po\": [\"po\"],\n    \"pou\": [\"pou\"],\n    \"pu\": [\"pu\"],\n    \"qi\": [\"t\u0255i\"],\n    \"qia\": [\"t\u0255ia\"],\n    \"qian\": [\"t\u0255i\u025bn\"],\n    \"qiang\": [\"t\u0255i\u0251\u014b\"],\n    \"qiao\": [\"t\u0255ia\u028c\"],\n    \"qie\": [\"t\u0255ie\"],\n    \"qin\": [\"t\u0255in\"],\n    \"qing\": [\"t\u0255\u0268\u014b\"],\n    \"qiong\": [\"t\u0255io\u014b\"],\n    \"qiu\": [\"t\u0255io\"],\n    \"qu\": [\"t\u0255y\"],\n    \"quan\": [\"t\u0255y\u025bn\"],\n    \"que\": [\"t\u0255ye\"],\n    \"qun\": [\"t\u0255yn\"],\n    \"ran\": [\"\u0290an\"],\n    \"rang\": [\"\u0290\u0251\u014b\"],\n    \"rao\": [\"\u0290a\u028c\"],\n    \"re\": [\"\u0290\u00f8\"],\n    \"ren\": [\"\u0290\u0153n\"],\n    \"reng\": [\"\u0290\u0275\u014b\"],\n    \"ri\": [\"\u0290\u028f\"],\n    \"rong\": [\"\u0290o\u014b\"],\n    \"rou\": [\"\u0290ou\"],\n    \"ru\": [\"\u0290u\"],\n    \"rua\": [\"\u0290ua\"],\n    \"ruan\": [\"\u0290uan\"],\n    \"rui\": [\"\u0290uei\"],\n    \"run\": [\"\u0290un\"],\n    \"ruo\": [\"\u0290uo\"],\n    \"sa\": [\"sa\"],\n    \"sai\": [\"sai\"],\n    \"san\": [\"san\"],\n    \"sang\": [\"s\u0251\u014b\"],\n    \"sao\": [\"sa\u028c\"],\n    \"se\": [\"s\u00f8\"],\n    \"sen\": [\"s\u0153n\"],\n    \"seng\": [\"s\u0275\u014b\"],\n    \"sha\": [\"\u0282a\"],\n    \"shai\": [\"\u0282ai\"],\n    \"shan\": [\"\u0282an\"],\n    \"shang\": [\"\u0282\u0251\u014b\"],\n    \"shao\": [\"\u0282a\u028c\"],\n    \"she\": [\"\u0282\u00f8\"],\n    \"shei\": [\"\u0282ei\"],\n    \"shen\": [\"\u0282\u0153n\"],\n    \"sheng\": [\"\u0282\u0275\u014b\"],\n    \"shi\": [\"\u0282\u028f\"],\n    \"shou\": [\"\u0282ou\"],\n    \"shu\": [\"\u0282u\"],\n    \"shua\": [\"\u0282ua\"],\n    \"shuai\": [\"\u0282uai\"],\n    \"shuan\": [\"\u0282uan\"],\n    \"shuang\": [\"\u0282u\u0251\u014b\"],\n    \"shui\": [\"\u0282uei\"],\n    \"shun\": [\"\u0282un\"],\n    \"shuo\": [\"\u0282uo\"],\n    \"si\": [\"s\u026a\"],\n    \"song\": [\"so\u014b\"],\n    \"sou\": [\"sou\"],\n    \"su\": [\"su\"],\n    \"suan\": [\"suan\"],\n    \"sui\": [\"suei\"],\n    \"sun\": [\"sun\"],\n    \"suo\": [\"suo\"],\n    \"ta\": [\"ta\"],\n    \"tai\": [\"tai\"],\n    \"tan\": [\"tan\"],\n    \"tang\": [\"t\u0251\u014b\"],\n    \"tao\": [\"ta\u028c\"],\n    \"te\": [\"t\u00f8\"],\n    \"tei\": [\"tei\"],\n    \"teng\": [\"t\u0275\u014b\"],\n    \"ti\": [\"ti\"],\n    \"tian\": [\"ti\u025bn\"],\n    \"tiao\": [\"tia\u028c\"],\n    \"tie\": [\"tie\"],\n    \"ting\": [\"t\u0268\u014b\"],\n    \"tong\": [\"to\u014b\"],\n    \"tou\": [\"tou\"],\n    \"tu\": [\"tu\"],\n    \"tuan\": [\"tuan\"],\n    \"tui\": [\"tuei\"],\n    \"tun\": [\"tun\"],\n    \"tuo\": [\"tuo\"],\n    \"wa\": [\"wa\"],\n    \"wai\": [\"wai\"],\n    \"wan\": [\"wan\"],\n    \"wang\": [\"w\u0251\u014b\"],\n    \"wei\": [\"wei\"],\n    \"wen\": [\"w\u0153n\"],\n    \"weng\": [\"w\u0275\u014b\"],\n    \"wo\": [\"wo\"],\n    \"wu\": [\"wu\"],\n    \"xi\": [\"\u0255i\"],\n    \"xia\": [\"\u0255ia\"],\n    \"xian\": [\"\u0255i\u025bn\"],\n    \"xiang\": [\"\u0255i\u0251\u014b\"],\n    \"xiao\": [\"\u0255ia\u028c\"],\n    \"xie\": [\"\u0255ie\"],\n    \"xin\": [\"\u0255in\"],\n    \"xing\": [\"\u0255\u0268\u014b\"],\n    \"xiong\": [\"\u0255io\u014b\"],\n    \"xiu\": [\"\u0255io\"],\n    \"xu\": [\"\u0255y\"],\n    \"xuan\": [\"\u0255y\u025bn\"],\n    \"xue\": [\"\u0255ye\"],\n    \"xun\": [\"\u0255yn\"],\n    \"ya\": [\"ia\"],\n    \"yan\": [\"i\u025bn\"],\n    \"yang\": [\"i\u0251\u014b\"],\n    \"yao\": [\"ia\u028c\"],\n    \"ye\": [\"ie\"],\n    \"yi\": [\"i\"],\n    \"yin\": [\"in\"],\n    \"ying\": [\"\u0268\u014b\"],\n    \"yo\": [\"io\"],\n    \"yong\": [\"io\u014b\"],\n    \"you\": [\"io\"],\n    \"yu\": [\"y\"],\n    \"yuan\": [\"y\u025bn\"],\n    \"yue\": [\"ye\"],\n    \"yun\": [\"yn\"],\n    \"za\": [\"dza\"],\n    \"zai\": [\"dzai\"],\n    \"zan\": [\"dzan\"],\n    \"zang\": [\"dz\u0251\u014b\"],\n    \"zao\": [\"dza\u028c\"],\n    \"ze\": [\"dz\u00f8\"],\n    \"zei\": [\"dzei\"],\n    \"zen\": [\"dz\u0153n\"],\n    \"zeng\": [\"dz\u0275\u014b\"],\n    \"zha\": [\"d\u0292a\"],\n    \"zhai\": [\"d\u0292ai\"],\n    \"zhan\": [\"d\u0292an\"],\n    \"zhang\": [\"d\u0292\u0251\u014b\"],\n    \"zhao\": [\"d\u0292a\u028c\"],\n    \"zhe\": [\"d\u0292\u00f8\"],\n    # \"zhei\": [\"d\u0292ei\"], it doesn't exist\n    \"zhen\": [\"d\u0292\u0153n\"],\n    \"zheng\": [\"d\u0292\u0275\u014b\"],\n    \"zhi\": [\"d\u0292\u028f\"],\n    \"zhong\": [\"d\u0292o\u014b\"],\n    \"zhou\": [\"d\u0292ou\"],\n    \"zhu\": [\"d\u0292u\"],\n    \"zhua\": [\"d\u0292ua\"],\n    \"zhuai\": [\"d\u0292uai\"],\n    \"zhuan\": [\"d\u0292uan\"],\n    \"zhuang\": [\"d\u0292u\u0251\u014b\"],\n    \"zhui\": [\"d\u0292uei\"],\n    \"zhun\": [\"d\u0292un\"],\n    \"zhuo\": [\"d\u0292uo\"],\n    \"zi\": [\"dz\u026a\"],\n    \"zong\": [\"dzo\u014b\"],\n    \"zou\": [\"dzou\"],\n    \"zu\": [\"dzu\"],\n    \"zuan\": [\"dzuan\"],\n    \"zui\": [\"dzuei\"],\n    \"zun\": [\"dzun\"],\n    \"zuo\": [\"dzuo\"],\n}\n", "TTS/tts/utils/text/japanese/phonemizer.py": "# Convert Japanese text to phonemes which is\n# compatible with Julius https://github.com/julius-speech/segmentation-kit\n\nimport re\nimport unicodedata\n\ntry:\n    import MeCab\nexcept ImportError as e:\n    raise ImportError(\"Japanese requires mecab-python3 and unidic-lite.\") from e\nfrom num2words import num2words\n\n_CONVRULES = [\n    # Conversion of 2 letters\n    \"\u30a2\u30a1/ a a\",\n    \"\u30a4\u30a3/ i i\",\n    \"\u30a4\u30a7/ i e\",\n    \"\u30a4\u30e3/ y a\",\n    \"\u30a6\u30a5/ u:\",\n    \"\u30a8\u30a7/ e e\",\n    \"\u30aa\u30a9/ o:\",\n    \"\u30ab\u30a1/ k a:\",\n    \"\u30ad\u30a3/ k i:\",\n    \"\u30af\u30a5/ k u:\",\n    \"\u30af\u30e3/ ky a\",\n    \"\u30af\u30e5/ ky u\",\n    \"\u30af\u30e7/ ky o\",\n    \"\u30b1\u30a7/ k e:\",\n    \"\u30b3\u30a9/ k o:\",\n    \"\u30ac\u30a1/ g a:\",\n    \"\u30ae\u30a3/ g i:\",\n    \"\u30b0\u30a5/ g u:\",\n    \"\u30b0\u30e3/ gy a\",\n    \"\u30b0\u30e5/ gy u\",\n    \"\u30b0\u30e7/ gy o\",\n    \"\u30b2\u30a7/ g e:\",\n    \"\u30b4\u30a9/ g o:\",\n    \"\u30b5\u30a1/ s a:\",\n    \"\u30b7\u30a3/ sh i:\",\n    \"\u30b9\u30a5/ s u:\",\n    \"\u30b9\u30e3/ sh a\",\n    \"\u30b9\u30e5/ sh u\",\n    \"\u30b9\u30e7/ sh o\",\n    \"\u30bb\u30a7/ s e:\",\n    \"\u30bd\u30a9/ s o:\",\n    \"\u30b6\u30a1/ z a:\",\n    \"\u30b8\u30a3/ j i:\",\n    \"\u30ba\u30a5/ z u:\",\n    \"\u30ba\u30e3/ zy a\",\n    \"\u30ba\u30e5/ zy u\",\n    \"\u30ba\u30e7/ zy o\",\n    \"\u30bc\u30a7/ z e:\",\n    \"\u30be\u30a9/ z o:\",\n    \"\u30bf\u30a1/ t a:\",\n    \"\u30c1\u30a3/ ch i:\",\n    \"\u30c4\u30a1/ ts a\",\n    \"\u30c4\u30a3/ ts i\",\n    \"\u30c4\u30a5/ ts u:\",\n    \"\u30c4\u30e3/ ch a\",\n    \"\u30c4\u30e5/ ch u\",\n    \"\u30c4\u30e7/ ch o\",\n    \"\u30c4\u30a7/ ts e\",\n    \"\u30c4\u30a9/ ts o\",\n    \"\u30c6\u30a7/ t e:\",\n    \"\u30c8\u30a9/ t o:\",\n    \"\u30c0\u30a1/ d a:\",\n    \"\u30c2\u30a3/ j i:\",\n    \"\u30c5\u30a5/ d u:\",\n    \"\u30c5\u30e3/ zy a\",\n    \"\u30c5\u30e5/ zy u\",\n    \"\u30c5\u30e7/ zy o\",\n    \"\u30c7\u30a7/ d e:\",\n    \"\u30c9\u30a9/ d o:\",\n    \"\u30ca\u30a1/ n a:\",\n    \"\u30cb\u30a3/ n i:\",\n    \"\u30cc\u30a5/ n u:\",\n    \"\u30cc\u30e3/ ny a\",\n    \"\u30cc\u30e5/ ny u\",\n    \"\u30cc\u30e7/ ny o\",\n    \"\u30cd\u30a7/ n e:\",\n    \"\u30ce\u30a9/ n o:\",\n    \"\u30cf\u30a1/ h a:\",\n    \"\u30d2\u30a3/ h i:\",\n    \"\u30d5\u30a5/ f u:\",\n    \"\u30d5\u30e3/ hy a\",\n    \"\u30d5\u30e5/ hy u\",\n    \"\u30d5\u30e7/ hy o\",\n    \"\u30d8\u30a7/ h e:\",\n    \"\u30db\u30a9/ h o:\",\n    \"\u30d0\u30a1/ b a:\",\n    \"\u30d3\u30a3/ b i:\",\n    \"\u30d6\u30a5/ b u:\",\n    \"\u30d5\u30e3/ hy a\",\n    \"\u30d6\u30e5/ by u\",\n    \"\u30d5\u30e7/ hy o\",\n    \"\u30d9\u30a7/ b e:\",\n    \"\u30dc\u30a9/ b o:\",\n    \"\u30d1\u30a1/ p a:\",\n    \"\u30d4\u30a3/ p i:\",\n    \"\u30d7\u30a5/ p u:\",\n    \"\u30d7\u30e3/ py a\",\n    \"\u30d7\u30e5/ py u\",\n    \"\u30d7\u30e7/ py o\",\n    \"\u30da\u30a7/ p e:\",\n    \"\u30dd\u30a9/ p o:\",\n    \"\u30de\u30a1/ m a:\",\n    \"\u30df\u30a3/ m i:\",\n    \"\u30e0\u30a5/ m u:\",\n    \"\u30e0\u30e3/ my a\",\n    \"\u30e0\u30e5/ my u\",\n    \"\u30e0\u30e7/ my o\",\n    \"\u30e1\u30a7/ m e:\",\n    \"\u30e2\u30a9/ m o:\",\n    \"\u30e4\u30a1/ y a:\",\n    \"\u30e6\u30a5/ y u:\",\n    \"\u30e6\u30e3/ y a:\",\n    \"\u30e6\u30e5/ y u:\",\n    \"\u30e6\u30e7/ y o:\",\n    \"\u30e8\u30a9/ y o:\",\n    \"\u30e9\u30a1/ r a:\",\n    \"\u30ea\u30a3/ r i:\",\n    \"\u30eb\u30a5/ r u:\",\n    \"\u30eb\u30e3/ ry a\",\n    \"\u30eb\u30e5/ ry u\",\n    \"\u30eb\u30e7/ ry o\",\n    \"\u30ec\u30a7/ r e:\",\n    \"\u30ed\u30a9/ r o:\",\n    \"\u30ef\u30a1/ w a:\",\n    \"\u30f2\u30a9/ o:\",\n    \"\u30c7\u30a3/ d i\",\n    \"\u30c7\u30a7/ d e:\",\n    \"\u30c7\u30e3/ dy a\",\n    \"\u30c7\u30e5/ dy u\",\n    \"\u30c7\u30e7/ dy o\",\n    \"\u30c6\u30a3/ t i\",\n    \"\u30c6\u30a7/ t e:\",\n    \"\u30c6\u30e3/ ty a\",\n    \"\u30c6\u30e5/ ty u\",\n    \"\u30c6\u30e7/ ty o\",\n    \"\u30b9\u30a3/ s i\",\n    \"\u30ba\u30a1/ z u a\",\n    \"\u30ba\u30a3/ z i\",\n    \"\u30ba\u30a5/ z u\",\n    \"\u30ba\u30e3/ zy a\",\n    \"\u30ba\u30e5/ zy u\",\n    \"\u30ba\u30e7/ zy o\",\n    \"\u30ba\u30a7/ z e\",\n    \"\u30ba\u30a9/ z o\",\n    \"\u30ad\u30e3/ ky a\",\n    \"\u30ad\u30e5/ ky u\",\n    \"\u30ad\u30e7/ ky o\",\n    \"\u30b7\u30e3/ sh a\",\n    \"\u30b7\u30e5/ sh u\",\n    \"\u30b7\u30a7/ sh e\",\n    \"\u30b7\u30e7/ sh o\",\n    \"\u30c1\u30e3/ ch a\",\n    \"\u30c1\u30e5/ ch u\",\n    \"\u30c1\u30a7/ ch e\",\n    \"\u30c1\u30e7/ ch o\",\n    \"\u30c8\u30a5/ t u\",\n    \"\u30c8\u30e3/ ty a\",\n    \"\u30c8\u30e5/ ty u\",\n    \"\u30c8\u30e7/ ty o\",\n    \"\u30c9\u30a1/ d o a\",\n    \"\u30c9\u30a5/ d u\",\n    \"\u30c9\u30e3/ dy a\",\n    \"\u30c9\u30e5/ dy u\",\n    \"\u30c9\u30e7/ dy o\",\n    \"\u30c9\u30a9/ d o:\",\n    \"\u30cb\u30e3/ ny a\",\n    \"\u30cb\u30e5/ ny u\",\n    \"\u30cb\u30e7/ ny o\",\n    \"\u30d2\u30e3/ hy a\",\n    \"\u30d2\u30e5/ hy u\",\n    \"\u30d2\u30e7/ hy o\",\n    \"\u30df\u30e3/ my a\",\n    \"\u30df\u30e5/ my u\",\n    \"\u30df\u30e7/ my o\",\n    \"\u30ea\u30e3/ ry a\",\n    \"\u30ea\u30e5/ ry u\",\n    \"\u30ea\u30e7/ ry o\",\n    \"\u30ae\u30e3/ gy a\",\n    \"\u30ae\u30e5/ gy u\",\n    \"\u30ae\u30e7/ gy o\",\n    \"\u30c2\u30a7/ j e\",\n    \"\u30c2\u30e3/ j a\",\n    \"\u30c2\u30e5/ j u\",\n    \"\u30c2\u30e7/ j o\",\n    \"\u30b8\u30a7/ j e\",\n    \"\u30b8\u30e3/ j a\",\n    \"\u30b8\u30e5/ j u\",\n    \"\u30b8\u30e7/ j o\",\n    \"\u30d3\u30e3/ by a\",\n    \"\u30d3\u30e5/ by u\",\n    \"\u30d3\u30e7/ by o\",\n    \"\u30d4\u30e3/ py a\",\n    \"\u30d4\u30e5/ py u\",\n    \"\u30d4\u30e7/ py o\",\n    \"\u30a6\u30a1/ u a\",\n    \"\u30a6\u30a3/ w i\",\n    \"\u30a6\u30a7/ w e\",\n    \"\u30a6\u30a9/ w o\",\n    \"\u30d5\u30a1/ f a\",\n    \"\u30d5\u30a3/ f i\",\n    \"\u30d5\u30a5/ f u\",\n    \"\u30d5\u30e3/ hy a\",\n    \"\u30d5\u30e5/ hy u\",\n    \"\u30d5\u30e7/ hy o\",\n    \"\u30d5\u30a7/ f e\",\n    \"\u30d5\u30a9/ f o\",\n    \"\u30f4\u30a1/ b a\",\n    \"\u30f4\u30a3/ b i\",\n    \"\u30f4\u30a7/ b e\",\n    \"\u30f4\u30a9/ b o\",\n    \"\u30f4\u30e5/ by u\",\n    # Conversion of 1 letter\n    \"\u30a2/ a\",\n    \"\u30a4/ i\",\n    \"\u30a6/ u\",\n    \"\u30a8/ e\",\n    \"\u30aa/ o\",\n    \"\u30ab/ k a\",\n    \"\u30ad/ k i\",\n    \"\u30af/ k u\",\n    \"\u30b1/ k e\",\n    \"\u30b3/ k o\",\n    \"\u30b5/ s a\",\n    \"\u30b7/ sh i\",\n    \"\u30b9/ s u\",\n    \"\u30bb/ s e\",\n    \"\u30bd/ s o\",\n    \"\u30bf/ t a\",\n    \"\u30c1/ ch i\",\n    \"\u30c4/ ts u\",\n    \"\u30c6/ t e\",\n    \"\u30c8/ t o\",\n    \"\u30ca/ n a\",\n    \"\u30cb/ n i\",\n    \"\u30cc/ n u\",\n    \"\u30cd/ n e\",\n    \"\u30ce/ n o\",\n    \"\u30cf/ h a\",\n    \"\u30d2/ h i\",\n    \"\u30d5/ f u\",\n    \"\u30d8/ h e\",\n    \"\u30db/ h o\",\n    \"\u30de/ m a\",\n    \"\u30df/ m i\",\n    \"\u30e0/ m u\",\n    \"\u30e1/ m e\",\n    \"\u30e2/ m o\",\n    \"\u30e9/ r a\",\n    \"\u30ea/ r i\",\n    \"\u30eb/ r u\",\n    \"\u30ec/ r e\",\n    \"\u30ed/ r o\",\n    \"\u30ac/ g a\",\n    \"\u30ae/ g i\",\n    \"\u30b0/ g u\",\n    \"\u30b2/ g e\",\n    \"\u30b4/ g o\",\n    \"\u30b6/ z a\",\n    \"\u30b8/ j i\",\n    \"\u30ba/ z u\",\n    \"\u30bc/ z e\",\n    \"\u30be/ z o\",\n    \"\u30c0/ d a\",\n    \"\u30c2/ j i\",\n    \"\u30c5/ z u\",\n    \"\u30c7/ d e\",\n    \"\u30c9/ d o\",\n    \"\u30d0/ b a\",\n    \"\u30d3/ b i\",\n    \"\u30d6/ b u\",\n    \"\u30d9/ b e\",\n    \"\u30dc/ b o\",\n    \"\u30d1/ p a\",\n    \"\u30d4/ p i\",\n    \"\u30d7/ p u\",\n    \"\u30da/ p e\",\n    \"\u30dd/ p o\",\n    \"\u30e4/ y a\",\n    \"\u30e6/ y u\",\n    \"\u30e8/ y o\",\n    \"\u30ef/ w a\",\n    \"\u30f0/ i\",\n    \"\u30f1/ e\",\n    \"\u30f2/ o\",\n    \"\u30f3/ N\",\n    \"\u30c3/ q\",\n    \"\u30f4/ b u\",\n    \"\u30fc/:\",\n    # Try converting broken text\n    \"\u30a1/ a\",\n    \"\u30a3/ i\",\n    \"\u30a5/ u\",\n    \"\u30a7/ e\",\n    \"\u30a9/ o\",\n    \"\u30ee/ w a\",\n    \"\u30a9/ o\",\n    # Symbols\n    \"\u3001/ ,\",\n    \"\u3002/ .\",\n    \"\uff01/ !\",\n    \"\uff1f/ ?\",\n    \"\u30fb/ ,\",\n]\n\n_COLON_RX = re.compile(\":+\")\n_REJECT_RX = re.compile(\"[^ a-zA-Z:,.?]\")\n\n\ndef _makerulemap():\n    l = [tuple(x.split(\"/\")) for x in _CONVRULES]\n    return tuple({k: v for k, v in l if len(k) == i} for i in (1, 2))\n\n\n_RULEMAP1, _RULEMAP2 = _makerulemap()\n\n\ndef kata2phoneme(text: str) -> str:\n    \"\"\"Convert katakana text to phonemes.\"\"\"\n    text = text.strip()\n    res = \"\"\n    while text:\n        if len(text) >= 2:\n            x = _RULEMAP2.get(text[:2])\n            if x is not None:\n                text = text[2:]\n                res += x\n                continue\n        x = _RULEMAP1.get(text[0])\n        if x is not None:\n            text = text[1:]\n            res += x\n            continue\n        res += \" \" + text[0]\n        text = text[1:]\n    res = _COLON_RX.sub(\":\", res)\n    return res[1:]\n\n\n_KATAKANA = \"\".join(chr(ch) for ch in range(ord(\"\u30a1\"), ord(\"\u30f3\") + 1))\n_HIRAGANA = \"\".join(chr(ch) for ch in range(ord(\"\u3041\"), ord(\"\u3093\") + 1))\n_HIRA2KATATRANS = str.maketrans(_HIRAGANA, _KATAKANA)\n\n\ndef hira2kata(text: str) -> str:\n    text = text.translate(_HIRA2KATATRANS)\n    return text.replace(\"\u3046\u309b\", \"\u30f4\")\n\n\n_SYMBOL_TOKENS = set(list(\"\u30fb\u3001\u3002\uff1f\uff01\"))\n_NO_YOMI_TOKENS = set(list(\"\u300c\u300d\u300e\u300f\u2015\uff08\uff09\uff3b\uff3d[]\u3000\u2026\"))\n_TAGGER = MeCab.Tagger()\n\n\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):\n        if line == \"EOS\":\n            break\n        parts = line.split(\"\\t\")\n\n        word, yomi = parts[0], parts[1]\n        if yomi:\n            res.append(yomi)\n        else:\n            if word in _SYMBOL_TOKENS:\n                res.append(word)\n            elif word in (\"\u3063\", \"\u30c3\"):\n                res.append(\"\u30c3\")\n            elif word in _NO_YOMI_TOKENS:\n                pass\n            else:\n                res.append(word)\n    return hira2kata(\"\".join(res))\n\n\n_ALPHASYMBOL_YOMI = {\n    \"#\": \"\u30b7\u30e3\u30fc\u30d7\",\n    \"%\": \"\u30d1\u30fc\u30bb\u30f3\u30c8\",\n    \"&\": \"\u30a2\u30f3\u30c9\",\n    \"+\": \"\u30d7\u30e9\u30b9\",\n    \"-\": \"\u30de\u30a4\u30ca\u30b9\",\n    \":\": \"\u30b3\u30ed\u30f3\",\n    \";\": \"\u30bb\u30df\u30b3\u30ed\u30f3\",\n    \"<\": \"\u5c0f\u306a\u308a\",\n    \"=\": \"\u30a4\u30b3\u30fc\u30eb\",\n    \">\": \"\u5927\u306a\u308a\",\n    \"@\": \"\u30a2\u30c3\u30c8\",\n    \"a\": \"\u30a8\u30fc\",\n    \"b\": \"\u30d3\u30fc\",\n    \"c\": \"\u30b7\u30fc\",\n    \"d\": \"\u30c7\u30a3\u30fc\",\n    \"e\": \"\u30a4\u30fc\",\n    \"f\": \"\u30a8\u30d5\",\n    \"g\": \"\u30b8\u30fc\",\n    \"h\": \"\u30a8\u30a4\u30c1\",\n    \"i\": \"\u30a2\u30a4\",\n    \"j\": \"\u30b8\u30a7\u30fc\",\n    \"k\": \"\u30b1\u30fc\",\n    \"l\": \"\u30a8\u30eb\",\n    \"m\": \"\u30a8\u30e0\",\n    \"n\": \"\u30a8\u30cc\",\n    \"o\": \"\u30aa\u30fc\",\n    \"p\": \"\u30d4\u30fc\",\n    \"q\": \"\u30ad\u30e5\u30fc\",\n    \"r\": \"\u30a2\u30fc\u30eb\",\n    \"s\": \"\u30a8\u30b9\",\n    \"t\": \"\u30c6\u30a3\u30fc\",\n    \"u\": \"\u30e6\u30fc\",\n    \"v\": \"\u30d6\u30a4\",\n    \"w\": \"\u30c0\u30d6\u30ea\u30e5\u30fc\",\n    \"x\": \"\u30a8\u30c3\u30af\u30b9\",\n    \"y\": \"\u30ef\u30a4\",\n    \"z\": \"\u30bc\u30c3\u30c8\",\n    \"\u03b1\": \"\u30a2\u30eb\u30d5\u30a1\",\n    \"\u03b2\": \"\u30d9\u30fc\u30bf\",\n    \"\u03b3\": \"\u30ac\u30f3\u30de\",\n    \"\u03b4\": \"\u30c7\u30eb\u30bf\",\n    \"\u03b5\": \"\u30a4\u30d7\u30b7\u30ed\u30f3\",\n    \"\u03b6\": \"\u30bc\u30fc\u30bf\",\n    \"\u03b7\": \"\u30a4\u30fc\u30bf\",\n    \"\u03b8\": \"\u30b7\u30fc\u30bf\",\n    \"\u03b9\": \"\u30a4\u30aa\u30bf\",\n    \"\u03ba\": \"\u30ab\u30c3\u30d1\",\n    \"\u03bb\": \"\u30e9\u30e0\u30c0\",\n    \"\u03bc\": \"\u30df\u30e5\u30fc\",\n    \"\u03bd\": \"\u30cb\u30e5\u30fc\",\n    \"\u03be\": \"\u30af\u30b5\u30a4\",\n    \"\u03bf\": \"\u30aa\u30df\u30af\u30ed\u30f3\",\n    \"\u03c0\": \"\u30d1\u30a4\",\n    \"\u03c1\": \"\u30ed\u30fc\",\n    \"\u03c3\": \"\u30b7\u30b0\u30de\",\n    \"\u03c4\": \"\u30bf\u30a6\",\n    \"\u03c5\": \"\u30a6\u30d7\u30b7\u30ed\u30f3\",\n    \"\u03c6\": \"\u30d5\u30a1\u30a4\",\n    \"\u03c7\": \"\u30ab\u30a4\",\n    \"\u03c8\": \"\u30d7\u30b5\u30a4\",\n    \"\u03c9\": \"\u30aa\u30e1\u30ac\",\n}\n\n\n_NUMBER_WITH_SEPARATOR_RX = re.compile(\"[0-9]{1,3}(,[0-9]{3})+\")\n_CURRENCY_MAP = {\"$\": \"\u30c9\u30eb\", \"\u00a5\": \"\u5186\", \"\u00a3\": \"\u30dd\u30f3\u30c9\", \"\u20ac\": \"\u30e6\u30fc\u30ed\"}\n_CURRENCY_RX = re.compile(r\"([$\u00a5\u00a3\u20ac])([0-9.]*[0-9])\")\n_NUMBER_RX = re.compile(r\"[0-9]+(\\.[0-9]+)?\")\n\n\ndef japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\n\n\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])\n\n\ndef japanese_text_to_phonemes(text: str) -> str:\n    \"\"\"Convert Japanese text to phonemes.\"\"\"\n    res = unicodedata.normalize(\"NFKC\", text)\n    res = japanese_convert_numbers_to_words(res)\n    res = japanese_convert_alpha_symbols_to_words(res)\n    res = text2kata(res)\n    res = kata2phoneme(res)\n    return res.replace(\" \", \"\")\n", "TTS/tts/utils/text/japanese/__init__.py": "", "TTS/tts/utils/monotonic_align/setup.py": "# from distutils.core import setup\n# from Cython.Build import cythonize\n# import numpy\n\n# setup(name='monotonic_align',\n#       ext_modules=cythonize(\"core.pyx\"),\n#       include_dirs=[numpy.get_include()])\n", "TTS/tts/utils/monotonic_align/__init__.py": "", "TTS/tts/layers/losses.py": "import math\n\nimport numpy as np\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.nn import functional\n\nfrom TTS.tts.utils.helpers import sequence_mask\nfrom TTS.tts.utils.ssim import SSIMLoss as _SSIMLoss\nfrom TTS.utils.audio.torch_transforms import TorchSTFT\n\n\n# pylint: disable=abstract-method\n# relates https://github.com/pytorch/pytorch/issues/42305\nclass L1LossMasked(nn.Module):\n    def __init__(self, seq_len_norm):\n        super().__init__()\n        self.seq_len_norm = seq_len_norm\n\n    def forward(self, x, target, length):\n        \"\"\"\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len, dim) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Shapes:\n            x: B x T X D\n            target: B x T x D\n            length: B\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        \"\"\"\n        # mask: (batch, max_len, 1)\n        target.requires_grad = False\n        mask = sequence_mask(sequence_length=length, max_len=target.size(1)).unsqueeze(2).float()\n        if self.seq_len_norm:\n            norm_w = mask / mask.sum(dim=1, keepdim=True)\n            out_weights = norm_w.div(target.shape[0] * target.shape[2])\n            mask = mask.expand_as(x)\n            loss = functional.l1_loss(x * mask, target * mask, reduction=\"none\")\n            loss = loss.mul(out_weights.to(loss.device)).sum()\n        else:\n            mask = mask.expand_as(x)\n            loss = functional.l1_loss(x * mask, target * mask, reduction=\"sum\")\n            loss = loss / mask.sum()\n        return loss\n\n\nclass MSELossMasked(nn.Module):\n    def __init__(self, seq_len_norm):\n        super().__init__()\n        self.seq_len_norm = seq_len_norm\n\n    def forward(self, x, target, length):\n        \"\"\"\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len, dim) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Shapes:\n            - x: :math:`[B, T, D]`\n            - target: :math:`[B, T, D]`\n            - length: :math:`B`\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        \"\"\"\n        # mask: (batch, max_len, 1)\n        target.requires_grad = False\n        mask = sequence_mask(sequence_length=length, max_len=target.size(1)).unsqueeze(2).float()\n        if self.seq_len_norm:\n            norm_w = mask / mask.sum(dim=1, keepdim=True)\n            out_weights = norm_w.div(target.shape[0] * target.shape[2])\n            mask = mask.expand_as(x)\n            loss = functional.mse_loss(x * mask, target * mask, reduction=\"none\")\n            loss = loss.mul(out_weights.to(loss.device)).sum()\n        else:\n            mask = mask.expand_as(x)\n            loss = functional.mse_loss(x * mask, target * mask, reduction=\"sum\")\n            loss = loss / mask.sum()\n        return loss\n\n\ndef sample_wise_min_max(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Min-Max normalize tensor through first dimension\n    Shapes:\n        - x: :math:`[B, D1, D2]`\n        - m: :math:`[B, D1, 1]`\n    \"\"\"\n    maximum = torch.amax(x.masked_fill(~mask, 0), dim=(1, 2), keepdim=True)\n    minimum = torch.amin(x.masked_fill(~mask, np.inf), dim=(1, 2), keepdim=True)\n    return (x - minimum) / (maximum - minimum + 1e-8)\n\n\nclass SSIMLoss(torch.nn.Module):\n    \"\"\"SSIM loss as (1 - SSIM)\n    SSIM is explained here https://en.wikipedia.org/wiki/Structural_similarity\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.loss_func = _SSIMLoss()\n\n    def forward(self, y_hat, y, length):\n        \"\"\"\n        Args:\n            y_hat (tensor): model prediction values.\n            y (tensor): target values.\n            length (tensor): length of each sample in a batch for masking.\n\n        Shapes:\n            y_hat: B x T X D\n            y: B x T x D\n            length: B\n\n         Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        \"\"\"\n        mask = sequence_mask(sequence_length=length, max_len=y.size(1)).unsqueeze(2)\n        y_norm = sample_wise_min_max(y, mask)\n        y_hat_norm = sample_wise_min_max(y_hat, mask)\n        ssim_loss = self.loss_func((y_norm * mask).unsqueeze(1), (y_hat_norm * mask).unsqueeze(1))\n\n        if ssim_loss.item() > 1.0:\n            print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 1.0\")\n            ssim_loss = torch.tensor(1.0, device=ssim_loss.device)\n\n        if ssim_loss.item() < 0.0:\n            print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")\n            ssim_loss = torch.tensor(0.0, device=ssim_loss.device)\n\n        return ssim_loss\n\n\nclass AttentionEntropyLoss(nn.Module):\n    # pylint: disable=R0201\n    def forward(self, align):\n        \"\"\"\n        Forces attention to be more decisive by penalizing\n        soft attention weights\n        \"\"\"\n        entropy = torch.distributions.Categorical(probs=align).entropy()\n        loss = (entropy / np.log(align.shape[1])).mean()\n        return loss\n\n\nclass BCELossMasked(nn.Module):\n    \"\"\"BCE loss with masking.\n\n    Used mainly for stopnet in autoregressive models.\n\n    Args:\n        pos_weight (float): weight for positive samples. If set < 1, penalize early stopping. Defaults to None.\n    \"\"\"\n\n    def __init__(self, pos_weight: float = None):\n        super().__init__()\n        self.register_buffer(\"pos_weight\", torch.tensor([pos_weight]))\n\n    def forward(self, x, target, length):\n        \"\"\"\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Shapes:\n            x: B x T\n            target: B x T\n            length: B\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        \"\"\"\n        target.requires_grad = False\n        if length is not None:\n            # mask: (batch, max_len, 1)\n            mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n            num_items = mask.sum()\n            loss = functional.binary_cross_entropy_with_logits(\n                x.masked_select(mask),\n                target.masked_select(mask),\n                pos_weight=self.pos_weight.to(x.device),\n                reduction=\"sum\",\n            )\n        else:\n            loss = functional.binary_cross_entropy_with_logits(\n                x, target, pos_weight=self.pos_weight.to(x.device), reduction=\"sum\"\n            )\n            num_items = torch.numel(x)\n        loss = loss / num_items\n        return loss\n\n\nclass DifferentialSpectralLoss(nn.Module):\n    \"\"\"Differential Spectral Loss\n    https://arxiv.org/ftp/arxiv/papers/1909/1909.10302.pdf\"\"\"\n\n    def __init__(self, loss_func):\n        super().__init__()\n        self.loss_func = loss_func\n\n    def forward(self, x, target, length=None):\n        \"\"\"\n         Shapes:\n            x: B x T\n            target: B x T\n            length: B\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        \"\"\"\n        x_diff = x[:, 1:] - x[:, :-1]\n        target_diff = target[:, 1:] - target[:, :-1]\n        if length is None:\n            return self.loss_func(x_diff, target_diff)\n        return self.loss_func(x_diff, target_diff, length - 1)\n\n\nclass GuidedAttentionLoss(torch.nn.Module):\n    def __init__(self, sigma=0.4):\n        super().__init__()\n        self.sigma = sigma\n\n    def _make_ga_masks(self, ilens, olens):\n        B = len(ilens)\n        max_ilen = max(ilens)\n        max_olen = max(olens)\n        ga_masks = torch.zeros((B, max_olen, max_ilen))\n        for idx, (ilen, olen) in enumerate(zip(ilens, olens)):\n            ga_masks[idx, :olen, :ilen] = self._make_ga_mask(ilen, olen, self.sigma)\n        return ga_masks\n\n    def forward(self, att_ws, ilens, olens):\n        ga_masks = self._make_ga_masks(ilens, olens).to(att_ws.device)\n        seq_masks = self._make_masks(ilens, olens).to(att_ws.device)\n        losses = ga_masks * att_ws\n        loss = torch.mean(losses.masked_select(seq_masks))\n        return loss\n\n    @staticmethod\n    def _make_ga_mask(ilen, olen, sigma):\n        grid_x, grid_y = torch.meshgrid(torch.arange(olen).to(olen), torch.arange(ilen).to(ilen))\n        grid_x, grid_y = grid_x.float(), grid_y.float()\n        return 1.0 - torch.exp(-((grid_y / ilen - grid_x / olen) ** 2) / (2 * (sigma**2)))\n\n    @staticmethod\n    def _make_masks(ilens, olens):\n        in_masks = sequence_mask(ilens)\n        out_masks = sequence_mask(olens)\n        return out_masks.unsqueeze(-1) & in_masks.unsqueeze(-2)\n\n\nclass Huber(nn.Module):\n    # pylint: disable=R0201\n    def forward(self, x, y, length=None):\n        \"\"\"\n        Shapes:\n            x: B x T\n            y: B x T\n            length: B\n        \"\"\"\n        mask = sequence_mask(sequence_length=length, max_len=y.size(1)).unsqueeze(2).float()\n        return torch.nn.functional.smooth_l1_loss(x * mask, y * mask, reduction=\"sum\") / mask.sum()\n\n\nclass ForwardSumLoss(nn.Module):\n    def __init__(self, blank_logprob=-1):\n        super().__init__()\n        self.log_softmax = torch.nn.LogSoftmax(dim=3)\n        self.ctc_loss = torch.nn.CTCLoss(zero_infinity=True)\n        self.blank_logprob = blank_logprob\n\n    def forward(self, attn_logprob, in_lens, out_lens):\n        key_lens = in_lens\n        query_lens = out_lens\n        attn_logprob_padded = torch.nn.functional.pad(input=attn_logprob, pad=(1, 0), value=self.blank_logprob)\n\n        total_loss = 0.0\n        for bid in range(attn_logprob.shape[0]):\n            target_seq = torch.arange(1, key_lens[bid] + 1).unsqueeze(0)\n            curr_logprob = attn_logprob_padded[bid].permute(1, 0, 2)[: query_lens[bid], :, : key_lens[bid] + 1]\n\n            curr_logprob = self.log_softmax(curr_logprob[None])[0]\n            loss = self.ctc_loss(\n                curr_logprob,\n                target_seq,\n                input_lengths=query_lens[bid : bid + 1],\n                target_lengths=key_lens[bid : bid + 1],\n            )\n            total_loss = total_loss + loss\n\n        total_loss = total_loss / attn_logprob.shape[0]\n        return total_loss\n\n\n########################\n# MODEL LOSS LAYERS\n########################\n\n\nclass TacotronLoss(torch.nn.Module):\n    \"\"\"Collection of Tacotron set-up based on provided config.\"\"\"\n\n    def __init__(self, c, ga_sigma=0.4):\n        super().__init__()\n        self.stopnet_pos_weight = c.stopnet_pos_weight\n        self.use_capacitron_vae = c.use_capacitron_vae\n        if self.use_capacitron_vae:\n            self.capacitron_capacity = c.capacitron_vae.capacitron_capacity\n            self.capacitron_vae_loss_alpha = c.capacitron_vae.capacitron_VAE_loss_alpha\n        self.ga_alpha = c.ga_alpha\n        self.decoder_diff_spec_alpha = c.decoder_diff_spec_alpha\n        self.postnet_diff_spec_alpha = c.postnet_diff_spec_alpha\n        self.decoder_alpha = c.decoder_loss_alpha\n        self.postnet_alpha = c.postnet_loss_alpha\n        self.decoder_ssim_alpha = c.decoder_ssim_alpha\n        self.postnet_ssim_alpha = c.postnet_ssim_alpha\n        self.config = c\n\n        # postnet and decoder loss\n        if c.loss_masking:\n            self.criterion = L1LossMasked(c.seq_len_norm) if c.model in [\"Tacotron\"] else MSELossMasked(c.seq_len_norm)\n        else:\n            self.criterion = nn.L1Loss() if c.model in [\"Tacotron\"] else nn.MSELoss()\n        # guided attention loss\n        if c.ga_alpha > 0:\n            self.criterion_ga = GuidedAttentionLoss(sigma=ga_sigma)\n        # differential spectral loss\n        if c.postnet_diff_spec_alpha > 0 or c.decoder_diff_spec_alpha > 0:\n            self.criterion_diff_spec = DifferentialSpectralLoss(loss_func=self.criterion)\n        # ssim loss\n        if c.postnet_ssim_alpha > 0 or c.decoder_ssim_alpha > 0:\n            self.criterion_ssim = SSIMLoss()\n        # stopnet loss\n        # pylint: disable=not-callable\n        self.criterion_st = BCELossMasked(pos_weight=torch.tensor(self.stopnet_pos_weight)) if c.stopnet else None\n\n        # For dev pruposes only\n        self.criterion_capacitron_reconstruction_loss = nn.L1Loss(reduction=\"sum\")\n\n    def forward(\n        self,\n        postnet_output,\n        decoder_output,\n        mel_input,\n        linear_input,\n        stopnet_output,\n        stopnet_target,\n        stop_target_length,\n        capacitron_vae_outputs,\n        output_lens,\n        decoder_b_output,\n        alignments,\n        alignment_lens,\n        alignments_backwards,\n        input_lens,\n    ):\n        # decoder outputs linear or mel spectrograms for Tacotron and Tacotron2\n        # the target should be set acccordingly\n        postnet_target = linear_input if self.config.model.lower() in [\"tacotron\"] else mel_input\n\n        return_dict = {}\n        # remove lengths if no masking is applied\n        if not self.config.loss_masking:\n            output_lens = None\n        # decoder and postnet losses\n        if self.config.loss_masking:\n            if self.decoder_alpha > 0:\n                decoder_loss = self.criterion(decoder_output, mel_input, output_lens)\n            if self.postnet_alpha > 0:\n                postnet_loss = self.criterion(postnet_output, postnet_target, output_lens)\n        else:\n            if self.decoder_alpha > 0:\n                decoder_loss = self.criterion(decoder_output, mel_input)\n            if self.postnet_alpha > 0:\n                postnet_loss = self.criterion(postnet_output, postnet_target)\n        loss = self.decoder_alpha * decoder_loss + self.postnet_alpha * postnet_loss\n        return_dict[\"decoder_loss\"] = decoder_loss\n        return_dict[\"postnet_loss\"] = postnet_loss\n\n        if self.use_capacitron_vae:\n            # extract capacitron vae infos\n            posterior_distribution, prior_distribution, beta = capacitron_vae_outputs\n\n            # KL divergence term between the posterior and the prior\n            kl_term = torch.mean(torch.distributions.kl_divergence(posterior_distribution, prior_distribution))\n\n            # Limit the mutual information between the data and latent space by the variational capacity limit\n            kl_capacity = kl_term - self.capacitron_capacity\n\n            # pass beta through softplus to keep it positive\n            beta = torch.nn.functional.softplus(beta)[0]\n\n            # This is the term going to the main ADAM optimiser, we detach beta because\n            # beta is optimised by a separate, SGD optimiser below\n            capacitron_vae_loss = beta.detach() * kl_capacity\n\n            # normalize the capacitron_vae_loss as in L1Loss or MSELoss.\n            # After this, both the standard loss and capacitron_vae_loss will be in the same scale.\n            # For this reason we don't need use L1Loss and MSELoss in \"sum\" reduction mode.\n            # Note: the batch is not considered because the L1Loss was calculated in \"sum\" mode\n            # divided by the batch size, So not dividing the capacitron_vae_loss by B is legitimate.\n\n            # get B T D dimension from input\n            B, T, D = mel_input.size()\n            # normalize\n            if self.config.loss_masking:\n                # if mask loss get T using the mask\n                T = output_lens.sum() / B\n\n            # Only for dev purposes to be able to compare the reconstruction loss with the values in the\n            # original Capacitron paper\n            return_dict[\"capaciton_reconstruction_loss\"] = (\n                self.criterion_capacitron_reconstruction_loss(decoder_output, mel_input) / decoder_output.size(0)\n            ) + kl_capacity\n\n            capacitron_vae_loss = capacitron_vae_loss / (T * D)\n            capacitron_vae_loss = capacitron_vae_loss * self.capacitron_vae_loss_alpha\n\n            # This is the term to purely optimise beta and to pass into the SGD optimizer\n            beta_loss = torch.negative(beta) * kl_capacity.detach()\n\n            loss += capacitron_vae_loss\n\n            return_dict[\"capacitron_vae_loss\"] = capacitron_vae_loss\n            return_dict[\"capacitron_vae_beta_loss\"] = beta_loss\n            return_dict[\"capacitron_vae_kl_term\"] = kl_term\n            return_dict[\"capacitron_beta\"] = beta\n\n        stop_loss = (\n            self.criterion_st(stopnet_output, stopnet_target, stop_target_length)\n            if self.config.stopnet\n            else torch.zeros(1)\n        )\n        loss += stop_loss\n        return_dict[\"stopnet_loss\"] = stop_loss\n\n        # backward decoder loss (if enabled)\n        if self.config.bidirectional_decoder:\n            if self.config.loss_masking:\n                decoder_b_loss = self.criterion(torch.flip(decoder_b_output, dims=(1,)), mel_input, output_lens)\n            else:\n                decoder_b_loss = self.criterion(torch.flip(decoder_b_output, dims=(1,)), mel_input)\n            decoder_c_loss = torch.nn.functional.l1_loss(torch.flip(decoder_b_output, dims=(1,)), decoder_output)\n            loss += self.decoder_alpha * (decoder_b_loss + decoder_c_loss)\n            return_dict[\"decoder_b_loss\"] = decoder_b_loss\n            return_dict[\"decoder_c_loss\"] = decoder_c_loss\n\n        # double decoder consistency loss (if enabled)\n        if self.config.double_decoder_consistency:\n            if self.config.loss_masking:\n                decoder_b_loss = self.criterion(decoder_b_output, mel_input, output_lens)\n            else:\n                decoder_b_loss = self.criterion(decoder_b_output, mel_input)\n            # decoder_c_loss = torch.nn.functional.l1_loss(decoder_b_output, decoder_output)\n            attention_c_loss = torch.nn.functional.l1_loss(alignments, alignments_backwards)\n            loss += self.decoder_alpha * (decoder_b_loss + attention_c_loss)\n            return_dict[\"decoder_coarse_loss\"] = decoder_b_loss\n            return_dict[\"decoder_ddc_loss\"] = attention_c_loss\n\n        # guided attention loss (if enabled)\n        if self.config.ga_alpha > 0:\n            ga_loss = self.criterion_ga(alignments, input_lens, alignment_lens)\n            loss += ga_loss * self.ga_alpha\n            return_dict[\"ga_loss\"] = ga_loss\n\n        # decoder differential spectral loss\n        if self.config.decoder_diff_spec_alpha > 0:\n            decoder_diff_spec_loss = self.criterion_diff_spec(decoder_output, mel_input, output_lens)\n            loss += decoder_diff_spec_loss * self.decoder_diff_spec_alpha\n            return_dict[\"decoder_diff_spec_loss\"] = decoder_diff_spec_loss\n\n        # postnet differential spectral loss\n        if self.config.postnet_diff_spec_alpha > 0:\n            postnet_diff_spec_loss = self.criterion_diff_spec(postnet_output, postnet_target, output_lens)\n            loss += postnet_diff_spec_loss * self.postnet_diff_spec_alpha\n            return_dict[\"postnet_diff_spec_loss\"] = postnet_diff_spec_loss\n\n        # decoder ssim loss\n        if self.config.decoder_ssim_alpha > 0:\n            decoder_ssim_loss = self.criterion_ssim(decoder_output, mel_input, output_lens)\n            loss += decoder_ssim_loss * self.postnet_ssim_alpha\n            return_dict[\"decoder_ssim_loss\"] = decoder_ssim_loss\n\n        # postnet ssim loss\n        if self.config.postnet_ssim_alpha > 0:\n            postnet_ssim_loss = self.criterion_ssim(postnet_output, postnet_target, output_lens)\n            loss += postnet_ssim_loss * self.postnet_ssim_alpha\n            return_dict[\"postnet_ssim_loss\"] = postnet_ssim_loss\n\n        return_dict[\"loss\"] = loss\n        return return_dict\n\n\nclass GlowTTSLoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.constant_factor = 0.5 * math.log(2 * math.pi)\n\n    def forward(self, z, means, scales, log_det, y_lengths, o_dur_log, o_attn_dur, x_lengths):\n        return_dict = {}\n        # flow loss - neg log likelihood\n        pz = torch.sum(scales) + 0.5 * torch.sum(torch.exp(-2 * scales) * (z - means) ** 2)\n        log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[2])\n        # duration loss - MSE\n        loss_dur = torch.sum((o_dur_log - o_attn_dur) ** 2) / torch.sum(x_lengths)\n        # duration loss - huber loss\n        # loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)\n        return_dict[\"loss\"] = log_mle + loss_dur\n        return_dict[\"log_mle\"] = log_mle\n        return_dict[\"loss_dur\"] = loss_dur\n\n        # check if any loss is NaN\n        for key, loss in return_dict.items():\n            if torch.isnan(loss):\n                raise RuntimeError(f\" [!] NaN loss with {key}.\")\n        return return_dict\n\n\ndef mse_loss_custom(x, y):\n    \"\"\"MSE loss using the torch back-end without reduction.\n    It uses less VRAM than the raw code\"\"\"\n    expanded_x, expanded_y = torch.broadcast_tensors(x, y)\n    return torch._C._nn.mse_loss(expanded_x, expanded_y, 0)  # pylint: disable=protected-access, c-extension-no-member\n\n\nclass MDNLoss(nn.Module):\n    \"\"\"Mixture of Density Network Loss as described in https://arxiv.org/pdf/2003.01950.pdf.\"\"\"\n\n    def forward(self, logp, text_lengths, mel_lengths):  # pylint: disable=no-self-use\n        \"\"\"\n        Shapes:\n            mu: [B, D, T]\n            log_sigma: [B, D, T]\n            mel_spec: [B, D, T]\n        \"\"\"\n        B, T_seq, T_mel = logp.shape\n        log_alpha = logp.new_ones(B, T_seq, T_mel) * (-1e4)\n        log_alpha[:, 0, 0] = logp[:, 0, 0]\n        for t in range(1, T_mel):\n            prev_step = torch.cat(\n                [log_alpha[:, :, t - 1 : t], functional.pad(log_alpha[:, :, t - 1 : t], (0, 0, 1, -1), value=-1e4)],\n                dim=-1,\n            )\n            log_alpha[:, :, t] = torch.logsumexp(prev_step + 1e-4, dim=-1) + logp[:, :, t]\n        alpha_last = log_alpha[torch.arange(B), text_lengths - 1, mel_lengths - 1]\n        mdn_loss = -alpha_last.mean() / T_seq\n        return mdn_loss  # , log_prob_matrix\n\n\nclass AlignTTSLoss(nn.Module):\n    \"\"\"Modified AlignTTS Loss.\n    Computes\n        - L1 and SSIM losses from output spectrograms.\n        - Huber loss for duration predictor.\n        - MDNLoss for Mixture of Density Network.\n\n    All loss values are aggregated by a weighted sum of the alpha values.\n\n    Args:\n        c (dict): TTS model configuration.\n    \"\"\"\n\n    def __init__(self, c):\n        super().__init__()\n        self.mdn_loss = MDNLoss()\n        self.spec_loss = MSELossMasked(False)\n        self.ssim = SSIMLoss()\n        self.dur_loss = MSELossMasked(False)\n\n        self.ssim_alpha = c.ssim_alpha\n        self.dur_loss_alpha = c.dur_loss_alpha\n        self.spec_loss_alpha = c.spec_loss_alpha\n        self.mdn_alpha = c.mdn_alpha\n\n    def forward(\n        self, logp, decoder_output, decoder_target, decoder_output_lens, dur_output, dur_target, input_lens, phase\n    ):\n        # ssim_alpha, dur_loss_alpha, spec_loss_alpha, mdn_alpha = self.set_alphas(step)\n        spec_loss, ssim_loss, dur_loss, mdn_loss = 0, 0, 0, 0\n        if phase == 0:\n            mdn_loss = self.mdn_loss(logp, input_lens, decoder_output_lens)\n        elif phase == 1:\n            spec_loss = self.spec_loss(decoder_output, decoder_target, decoder_output_lens)\n            ssim_loss = self.ssim(decoder_output, decoder_target, decoder_output_lens)\n        elif phase == 2:\n            mdn_loss = self.mdn_loss(logp, input_lens, decoder_output_lens)\n            spec_loss = self.spec_lossX(decoder_output, decoder_target, decoder_output_lens)\n            ssim_loss = self.ssim(decoder_output, decoder_target, decoder_output_lens)\n        elif phase == 3:\n            dur_loss = self.dur_loss(dur_output.unsqueeze(2), dur_target.unsqueeze(2), input_lens)\n        else:\n            mdn_loss = self.mdn_loss(logp, input_lens, decoder_output_lens)\n            spec_loss = self.spec_loss(decoder_output, decoder_target, decoder_output_lens)\n            ssim_loss = self.ssim(decoder_output, decoder_target, decoder_output_lens)\n            dur_loss = self.dur_loss(dur_output.unsqueeze(2), dur_target.unsqueeze(2), input_lens)\n        loss = (\n            self.spec_loss_alpha * spec_loss\n            + self.ssim_alpha * ssim_loss\n            + self.dur_loss_alpha * dur_loss\n            + self.mdn_alpha * mdn_loss\n        )\n        return {\"loss\": loss, \"loss_l1\": spec_loss, \"loss_ssim\": ssim_loss, \"loss_dur\": dur_loss, \"mdn_loss\": mdn_loss}\n\n\nclass VitsGeneratorLoss(nn.Module):\n    def __init__(self, c: Coqpit):\n        super().__init__()\n        self.kl_loss_alpha = c.kl_loss_alpha\n        self.gen_loss_alpha = c.gen_loss_alpha\n        self.feat_loss_alpha = c.feat_loss_alpha\n        self.dur_loss_alpha = c.dur_loss_alpha\n        self.mel_loss_alpha = c.mel_loss_alpha\n        self.spk_encoder_loss_alpha = c.speaker_encoder_loss_alpha\n        self.stft = TorchSTFT(\n            c.audio.fft_size,\n            c.audio.hop_length,\n            c.audio.win_length,\n            sample_rate=c.audio.sample_rate,\n            mel_fmin=c.audio.mel_fmin,\n            mel_fmax=c.audio.mel_fmax,\n            n_mels=c.audio.num_mels,\n            use_mel=True,\n            do_amp_to_db=True,\n        )\n\n    @staticmethod\n    def feature_loss(feats_real, feats_generated):\n        loss = 0\n        for dr, dg in zip(feats_real, feats_generated):\n            for rl, gl in zip(dr, dg):\n                rl = rl.float().detach()\n                gl = gl.float()\n                loss += torch.mean(torch.abs(rl - gl))\n        return loss * 2\n\n    @staticmethod\n    def generator_loss(scores_fake):\n        loss = 0\n        gen_losses = []\n        for dg in scores_fake:\n            dg = dg.float()\n            l = torch.mean((1 - dg) ** 2)\n            gen_losses.append(l)\n            loss += l\n\n        return loss, gen_losses\n\n    @staticmethod\n    def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n        \"\"\"\n        z_p, logs_q: [b, h, t_t]\n        m_p, logs_p: [b, h, t_t]\n        \"\"\"\n        z_p = z_p.float()\n        logs_q = logs_q.float()\n        m_p = m_p.float()\n        logs_p = logs_p.float()\n        z_mask = z_mask.float()\n\n        kl = logs_p - logs_q - 0.5\n        kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n        kl = torch.sum(kl * z_mask)\n        l = kl / torch.sum(z_mask)\n        return l\n\n    @staticmethod\n    def cosine_similarity_loss(gt_spk_emb, syn_spk_emb):\n        return -torch.nn.functional.cosine_similarity(gt_spk_emb, syn_spk_emb).mean()\n\n    def forward(\n        self,\n        mel_slice,\n        mel_slice_hat,\n        z_p,\n        logs_q,\n        m_p,\n        logs_p,\n        z_len,\n        scores_disc_fake,\n        feats_disc_fake,\n        feats_disc_real,\n        loss_duration,\n        use_speaker_encoder_as_loss=False,\n        gt_spk_emb=None,\n        syn_spk_emb=None,\n    ):\n        \"\"\"\n        Shapes:\n            - mel_slice : :math:`[B, 1, T]`\n            - mel_slice_hat: :math:`[B, 1, T]`\n            - z_p: :math:`[B, C, T]`\n            - logs_q: :math:`[B, C, T]`\n            - m_p: :math:`[B, C, T]`\n            - logs_p: :math:`[B, C, T]`\n            - z_len: :math:`[B]`\n            - scores_disc_fake[i]: :math:`[B, C]`\n            - feats_disc_fake[i][j]: :math:`[B, C, T', P]`\n            - feats_disc_real[i][j]: :math:`[B, C, T', P]`\n        \"\"\"\n        loss = 0.0\n        return_dict = {}\n        z_mask = sequence_mask(z_len).float()\n        # compute losses\n        loss_kl = (\n            self.kl_loss(z_p=z_p, logs_q=logs_q, m_p=m_p, logs_p=logs_p, z_mask=z_mask.unsqueeze(1))\n            * self.kl_loss_alpha\n        )\n        loss_feat = (\n            self.feature_loss(feats_real=feats_disc_real, feats_generated=feats_disc_fake) * self.feat_loss_alpha\n        )\n        loss_gen = self.generator_loss(scores_fake=scores_disc_fake)[0] * self.gen_loss_alpha\n        loss_mel = torch.nn.functional.l1_loss(mel_slice, mel_slice_hat) * self.mel_loss_alpha\n        loss_duration = torch.sum(loss_duration.float()) * self.dur_loss_alpha\n        loss = loss_kl + loss_feat + loss_mel + loss_gen + loss_duration\n\n        if use_speaker_encoder_as_loss:\n            loss_se = self.cosine_similarity_loss(gt_spk_emb, syn_spk_emb) * self.spk_encoder_loss_alpha\n            loss = loss + loss_se\n            return_dict[\"loss_spk_encoder\"] = loss_se\n        # pass losses to the dict\n        return_dict[\"loss_gen\"] = loss_gen\n        return_dict[\"loss_kl\"] = loss_kl\n        return_dict[\"loss_feat\"] = loss_feat\n        return_dict[\"loss_mel\"] = loss_mel\n        return_dict[\"loss_duration\"] = loss_duration\n        return_dict[\"loss\"] = loss\n        return return_dict\n\n\nclass VitsDiscriminatorLoss(nn.Module):\n    def __init__(self, c: Coqpit):\n        super().__init__()\n        self.disc_loss_alpha = c.disc_loss_alpha\n\n    @staticmethod\n    def discriminator_loss(scores_real, scores_fake):\n        loss = 0\n        real_losses = []\n        fake_losses = []\n        for dr, dg in zip(scores_real, scores_fake):\n            dr = dr.float()\n            dg = dg.float()\n            real_loss = torch.mean((1 - dr) ** 2)\n            fake_loss = torch.mean(dg**2)\n            loss += real_loss + fake_loss\n            real_losses.append(real_loss.item())\n            fake_losses.append(fake_loss.item())\n        return loss, real_losses, fake_losses\n\n    def forward(self, scores_disc_real, scores_disc_fake):\n        loss = 0.0\n        return_dict = {}\n        loss_disc, loss_disc_real, _ = self.discriminator_loss(\n            scores_real=scores_disc_real, scores_fake=scores_disc_fake\n        )\n        return_dict[\"loss_disc\"] = loss_disc * self.disc_loss_alpha\n        loss = loss + return_dict[\"loss_disc\"]\n        return_dict[\"loss\"] = loss\n\n        for i, ldr in enumerate(loss_disc_real):\n            return_dict[f\"loss_disc_real_{i}\"] = ldr\n        return return_dict\n\n\nclass ForwardTTSLoss(nn.Module):\n    \"\"\"Generic configurable ForwardTTS loss.\"\"\"\n\n    def __init__(self, c):\n        super().__init__()\n        if c.spec_loss_type == \"mse\":\n            self.spec_loss = MSELossMasked(False)\n        elif c.spec_loss_type == \"l1\":\n            self.spec_loss = L1LossMasked(False)\n        else:\n            raise ValueError(\" [!] Unknown spec_loss_type {}\".format(c.spec_loss_type))\n\n        if c.duration_loss_type == \"mse\":\n            self.dur_loss = MSELossMasked(False)\n        elif c.duration_loss_type == \"l1\":\n            self.dur_loss = L1LossMasked(False)\n        elif c.duration_loss_type == \"huber\":\n            self.dur_loss = Huber()\n        else:\n            raise ValueError(\" [!] Unknown duration_loss_type {}\".format(c.duration_loss_type))\n\n        if c.model_args.use_aligner:\n            self.aligner_loss = ForwardSumLoss()\n            self.aligner_loss_alpha = c.aligner_loss_alpha\n\n        if c.model_args.use_pitch:\n            self.pitch_loss = MSELossMasked(False)\n            self.pitch_loss_alpha = c.pitch_loss_alpha\n\n        if c.model_args.use_energy:\n            self.energy_loss = MSELossMasked(False)\n            self.energy_loss_alpha = c.energy_loss_alpha\n\n        if c.use_ssim_loss:\n            self.ssim = SSIMLoss() if c.use_ssim_loss else None\n            self.ssim_loss_alpha = c.ssim_loss_alpha\n\n        self.spec_loss_alpha = c.spec_loss_alpha\n        self.dur_loss_alpha = c.dur_loss_alpha\n        self.binary_alignment_loss_alpha = c.binary_align_loss_alpha\n\n    @staticmethod\n    def _binary_alignment_loss(alignment_hard, alignment_soft):\n        \"\"\"Binary loss that forces soft alignments to match the hard alignments as\n        explained in `https://arxiv.org/pdf/2108.10447.pdf`.\n        \"\"\"\n        log_sum = torch.log(torch.clamp(alignment_soft[alignment_hard == 1], min=1e-12)).sum()\n        return -log_sum / alignment_hard.sum()\n\n    def forward(\n        self,\n        decoder_output,\n        decoder_target,\n        decoder_output_lens,\n        dur_output,\n        dur_target,\n        pitch_output,\n        pitch_target,\n        energy_output,\n        energy_target,\n        input_lens,\n        alignment_logprob=None,\n        alignment_hard=None,\n        alignment_soft=None,\n        binary_loss_weight=None,\n    ):\n        loss = 0\n        return_dict = {}\n        if hasattr(self, \"ssim_loss\") and self.ssim_loss_alpha > 0:\n            ssim_loss = self.ssim(decoder_output, decoder_target, decoder_output_lens)\n            loss = loss + self.ssim_loss_alpha * ssim_loss\n            return_dict[\"loss_ssim\"] = self.ssim_loss_alpha * ssim_loss\n\n        if self.spec_loss_alpha > 0:\n            spec_loss = self.spec_loss(decoder_output, decoder_target, decoder_output_lens)\n            loss = loss + self.spec_loss_alpha * spec_loss\n            return_dict[\"loss_spec\"] = self.spec_loss_alpha * spec_loss\n\n        if self.dur_loss_alpha > 0:\n            log_dur_tgt = torch.log(dur_target.float() + 1)\n            dur_loss = self.dur_loss(dur_output[:, :, None], log_dur_tgt[:, :, None], input_lens)\n            loss = loss + self.dur_loss_alpha * dur_loss\n            return_dict[\"loss_dur\"] = self.dur_loss_alpha * dur_loss\n\n        if hasattr(self, \"pitch_loss\") and self.pitch_loss_alpha > 0:\n            pitch_loss = self.pitch_loss(pitch_output.transpose(1, 2), pitch_target.transpose(1, 2), input_lens)\n            loss = loss + self.pitch_loss_alpha * pitch_loss\n            return_dict[\"loss_pitch\"] = self.pitch_loss_alpha * pitch_loss\n\n        if hasattr(self, \"energy_loss\") and self.energy_loss_alpha > 0:\n            energy_loss = self.energy_loss(energy_output.transpose(1, 2), energy_target.transpose(1, 2), input_lens)\n            loss = loss + self.energy_loss_alpha * energy_loss\n            return_dict[\"loss_energy\"] = self.energy_loss_alpha * energy_loss\n\n        if hasattr(self, \"aligner_loss\") and self.aligner_loss_alpha > 0:\n            aligner_loss = self.aligner_loss(alignment_logprob, input_lens, decoder_output_lens)\n            loss = loss + self.aligner_loss_alpha * aligner_loss\n            return_dict[\"loss_aligner\"] = self.aligner_loss_alpha * aligner_loss\n\n        if self.binary_alignment_loss_alpha > 0 and alignment_hard is not None:\n            binary_alignment_loss = self._binary_alignment_loss(alignment_hard, alignment_soft)\n            loss = loss + self.binary_alignment_loss_alpha * binary_alignment_loss\n            if binary_loss_weight:\n                return_dict[\"loss_binary_alignment\"] = (\n                    self.binary_alignment_loss_alpha * binary_alignment_loss * binary_loss_weight\n                )\n            else:\n                return_dict[\"loss_binary_alignment\"] = self.binary_alignment_loss_alpha * binary_alignment_loss\n\n        return_dict[\"loss\"] = loss\n        return return_dict\n", "TTS/tts/layers/__init__.py": "from TTS.tts.layers.losses import *\n", "TTS/tts/layers/overflow/common_layers.py": "from typing import List, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom tqdm.auto import tqdm\n\nfrom TTS.tts.layers.tacotron.common_layers import Linear\nfrom TTS.tts.layers.tacotron.tacotron2 import ConvBNBlock\n\n\nclass Encoder(nn.Module):\n    r\"\"\"Neural HMM Encoder\n\n    Same as Tacotron 2 encoder but increases the input length by states per phone\n\n    Args:\n        num_chars (int): Number of characters in the input.\n        state_per_phone (int): Number of states per phone.\n        in_out_channels (int): number of input and output channels.\n        n_convolutions (int): number of convolutional layers.\n    \"\"\"\n\n    def __init__(self, num_chars, state_per_phone, in_out_channels=512, n_convolutions=3):\n        super().__init__()\n\n        self.state_per_phone = state_per_phone\n        self.in_out_channels = in_out_channels\n\n        self.emb = nn.Embedding(num_chars, in_out_channels)\n        self.convolutions = nn.ModuleList()\n        for _ in range(n_convolutions):\n            self.convolutions.append(ConvBNBlock(in_out_channels, in_out_channels, 5, \"relu\"))\n        self.lstm = nn.LSTM(\n            in_out_channels,\n            int(in_out_channels / 2) * state_per_phone,\n            num_layers=1,\n            batch_first=True,\n            bias=True,\n            bidirectional=True,\n        )\n        self.rnn_state = None\n\n    def forward(self, x: torch.FloatTensor, x_len: torch.LongTensor) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n        \"\"\"Forward pass to the encoder.\n\n        Args:\n            x (torch.FloatTensor): input text indices.\n                - shape: :math:`(b, T_{in})`\n            x_len (torch.LongTensor): input text lengths.\n                - shape: :math:`(b,)`\n\n        Returns:\n            Tuple[torch.FloatTensor, torch.LongTensor]: encoder outputs and output lengths.\n                -shape: :math:`((b, T_{in} * states_per_phone, in_out_channels), (b,))`\n        \"\"\"\n        b, T = x.shape\n        o = self.emb(x).transpose(1, 2)\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        o = nn.utils.rnn.pack_padded_sequence(o, x_len.cpu(), batch_first=True)\n        self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n        o = o.reshape(b, T * self.state_per_phone, self.in_out_channels)\n        x_len = x_len * self.state_per_phone\n        return o, x_len\n\n    def inference(self, x, x_len):\n        \"\"\"Inference to the encoder.\n\n        Args:\n            x (torch.FloatTensor): input text indices.\n                - shape: :math:`(b, T_{in})`\n            x_len (torch.LongTensor): input text lengths.\n                - shape: :math:`(b,)`\n\n        Returns:\n            Tuple[torch.FloatTensor, torch.LongTensor]: encoder outputs and output lengths.\n                -shape: :math:`((b, T_{in} * states_per_phone, in_out_channels), (b,))`\n        \"\"\"\n        b, T = x.shape\n        o = self.emb(x).transpose(1, 2)\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        # self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        o = o.reshape(b, T * self.state_per_phone, self.in_out_channels)\n        x_len = x_len * self.state_per_phone\n        return o, x_len\n\n\nclass ParameterModel(nn.Module):\n    r\"\"\"Main neural network of the outputnet\n\n    Note: Do not put dropout layers here, the model will not converge.\n\n    Args:\n            outputnet_size (List[int]): the architecture of the parameter model\n            input_size (int): size of input for the first layer\n            output_size (int): size of output i.e size of the feature dim\n            frame_channels (int): feature dim to set the flat start bias\n            flat_start_params (dict): flat start parameters to set the bias\n    \"\"\"\n\n    def __init__(\n        self,\n        outputnet_size: List[int],\n        input_size: int,\n        output_size: int,\n        frame_channels: int,\n        flat_start_params: dict,\n    ):\n        super().__init__()\n        self.frame_channels = frame_channels\n\n        self.layers = nn.ModuleList(\n            [Linear(inp, out) for inp, out in zip([input_size] + outputnet_size[:-1], outputnet_size)]\n        )\n        self.last_layer = nn.Linear(outputnet_size[-1], output_size)\n        self.flat_start_output_layer(\n            flat_start_params[\"mean\"], flat_start_params[\"std\"], flat_start_params[\"transition_p\"]\n        )\n\n    def flat_start_output_layer(self, mean, std, transition_p):\n        self.last_layer.weight.data.zero_()\n        self.last_layer.bias.data[0 : self.frame_channels] = mean\n        self.last_layer.bias.data[self.frame_channels : 2 * self.frame_channels] = OverflowUtils.inverse_softplus(std)\n        self.last_layer.bias.data[2 * self.frame_channels :] = OverflowUtils.inverse_sigmod(transition_p)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = F.relu(layer(x))\n        x = self.last_layer(x)\n        return x\n\n\nclass Outputnet(nn.Module):\n    r\"\"\"\n    This network takes current state and previous observed values as input\n    and returns its parameters, mean, standard deviation and probability\n    of transition to the next state\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_dim: int,\n        memory_rnn_dim: int,\n        frame_channels: int,\n        outputnet_size: List[int],\n        flat_start_params: dict,\n        std_floor: float = 1e-2,\n    ):\n        super().__init__()\n\n        self.frame_channels = frame_channels\n        self.flat_start_params = flat_start_params\n        self.std_floor = std_floor\n\n        input_size = memory_rnn_dim + encoder_dim\n        output_size = 2 * frame_channels + 1\n\n        self.parametermodel = ParameterModel(\n            outputnet_size=outputnet_size,\n            input_size=input_size,\n            output_size=output_size,\n            flat_start_params=flat_start_params,\n            frame_channels=frame_channels,\n        )\n\n    def forward(self, ar_mels, inputs):\n        r\"\"\"Inputs observation and returns the means, stds and transition probability for the current state\n\n        Args:\n            ar_mel_inputs (torch.FloatTensor): shape (batch, prenet_dim)\n            states (torch.FloatTensor):  (batch, hidden_states, hidden_state_dim)\n\n        Returns:\n            means: means for the emission observation for each feature\n                - shape: (B, hidden_states, feature_size)\n            stds: standard deviations for the emission observation for each feature\n                - shape: (batch, hidden_states, feature_size)\n            transition_vectors: transition vector for the current hidden state\n                - shape: (batch, hidden_states)\n        \"\"\"\n        batch_size, prenet_dim = ar_mels.shape[0], ar_mels.shape[1]\n        N = inputs.shape[1]\n\n        ar_mels = ar_mels.unsqueeze(1).expand(batch_size, N, prenet_dim)\n        ar_mels = torch.cat((ar_mels, inputs), dim=2)\n        ar_mels = self.parametermodel(ar_mels)\n\n        mean, std, transition_vector = (\n            ar_mels[:, :, 0 : self.frame_channels],\n            ar_mels[:, :, self.frame_channels : 2 * self.frame_channels],\n            ar_mels[:, :, 2 * self.frame_channels :].squeeze(2),\n        )\n        std = F.softplus(std)\n        std = self._floor_std(std)\n        return mean, std, transition_vector\n\n    def _floor_std(self, std):\n        r\"\"\"\n        It clamps the standard deviation to not to go below some level\n        This removes the problem when the model tries to cheat for higher likelihoods by converting\n        one of the gaussians to a point mass.\n\n        Args:\n            std (float Tensor): tensor containing the standard deviation to be\n        \"\"\"\n        original_tensor = std.clone().detach()\n        std = torch.clamp(std, min=self.std_floor)\n        if torch.any(original_tensor != std):\n            print(\n                \"[*] Standard deviation was floored! The model is preventing overfitting, nothing serious to worry about\"\n            )\n        return std\n\n\nclass OverflowUtils:\n    @staticmethod\n    def get_data_parameters_for_flat_start(\n        data_loader: torch.utils.data.DataLoader, out_channels: int, states_per_phone: int\n    ):\n        \"\"\"Generates data parameters for flat starting the HMM.\n\n        Args:\n            data_loader (torch.utils.data.Dataloader): _description_\n            out_channels (int): mel spectrogram channels\n            states_per_phone (_type_): HMM states per phone\n        \"\"\"\n\n        # State related information for transition_p\n        total_state_len = 0\n        total_mel_len = 0\n\n        # Useful for data mean an std\n        total_mel_sum = 0\n        total_mel_sq_sum = 0\n\n        for batch in tqdm(data_loader, leave=False):\n            text_lengths = batch[\"token_id_lengths\"]\n            mels = batch[\"mel\"]\n            mel_lengths = batch[\"mel_lengths\"]\n\n            total_state_len += torch.sum(text_lengths)\n            total_mel_len += torch.sum(mel_lengths)\n            total_mel_sum += torch.sum(mels)\n            total_mel_sq_sum += torch.sum(torch.pow(mels, 2))\n\n        data_mean = total_mel_sum / (total_mel_len * out_channels)\n        data_std = torch.sqrt((total_mel_sq_sum / (total_mel_len * out_channels)) - torch.pow(data_mean, 2))\n        average_num_states = total_state_len / len(data_loader.dataset)\n        average_mel_len = total_mel_len / len(data_loader.dataset)\n        average_duration_each_state = average_mel_len / average_num_states\n        init_transition_prob = 1 / average_duration_each_state\n\n        return data_mean, data_std, (init_transition_prob * states_per_phone)\n\n    @staticmethod\n    @torch.no_grad()\n    def update_flat_start_transition(model, transition_p):\n        model.neural_hmm.output_net.parametermodel.flat_start_output_layer(0.0, 1.0, transition_p)\n\n    @staticmethod\n    def log_clamped(x, eps=1e-04):\n        \"\"\"\n        Avoids the log(0) problem\n\n        Args:\n            x (torch.tensor): input tensor\n            eps (float, optional): lower bound. Defaults to 1e-04.\n\n        Returns:\n            torch.tensor: :math:`log(x)`\n        \"\"\"\n        clamped_x = torch.clamp(x, min=eps)\n        return torch.log(clamped_x)\n\n    @staticmethod\n    def inverse_sigmod(x):\n        r\"\"\"\n        Inverse of the sigmoid function\n        \"\"\"\n        if not torch.is_tensor(x):\n            x = torch.tensor(x)\n        return OverflowUtils.log_clamped(x / (1.0 - x))\n\n    @staticmethod\n    def inverse_softplus(x):\n        r\"\"\"\n        Inverse of the softplus function\n        \"\"\"\n        if not torch.is_tensor(x):\n            x = torch.tensor(x)\n        return OverflowUtils.log_clamped(torch.exp(x) - 1.0)\n\n    @staticmethod\n    def logsumexp(x, dim):\n        r\"\"\"\n        Differentiable LogSumExp: Does not creates nan gradients\n            when all the inputs are -inf yeilds 0 gradients.\n        Args:\n            x : torch.Tensor -  The input tensor\n            dim: int - The dimension on which the log sum exp has to be applied\n        \"\"\"\n\n        m, _ = x.max(dim=dim)\n        mask = m == -float(\"inf\")\n        s = (x - m.masked_fill_(mask, 0).unsqueeze(dim=dim)).exp().sum(dim=dim)\n        return s.masked_fill_(mask, 1).log() + m.masked_fill_(mask, -float(\"inf\"))\n\n    @staticmethod\n    def double_pad(list_of_different_shape_tensors):\n        r\"\"\"\n        Pads the list of tensors in 2 dimensions\n        \"\"\"\n        second_dim_lens = [len(a) for a in [i[0] for i in list_of_different_shape_tensors]]\n        second_dim_max = max(second_dim_lens)\n        padded_x = [F.pad(x, (0, second_dim_max - len(x[0]))) for x in list_of_different_shape_tensors]\n        return nn.utils.rnn.pad_sequence(padded_x, batch_first=True)\n", "TTS/tts/layers/overflow/decoder.py": "import torch\nfrom torch import nn\n\nfrom TTS.tts.layers.glow_tts.decoder import Decoder as GlowDecoder\nfrom TTS.tts.utils.helpers import sequence_mask\n\n\nclass Decoder(nn.Module):\n    \"\"\"Uses glow decoder with some modifications.\n    ::\n\n        Squeeze -> ActNorm -> InvertibleConv1x1 -> AffineCoupling -> Unsqueeze\n\n    Args:\n        in_channels (int): channels of input tensor.\n        hidden_channels (int): hidden decoder channels.\n        kernel_size (int): Coupling block kernel size. (Wavenet filter kernel size.)\n        dilation_rate (int): rate to increase dilation by each layer in a decoder block.\n        num_flow_blocks (int): number of decoder blocks.\n        num_coupling_layers (int): number coupling layers. (number of wavenet layers.)\n        dropout_p (float): wavenet dropout rate.\n        sigmoid_scale (bool): enable/disable sigmoid scaling in coupling layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_flow_blocks,\n        num_coupling_layers,\n        dropout_p=0.0,\n        num_splits=4,\n        num_squeeze=2,\n        sigmoid_scale=False,\n        c_in_channels=0,\n    ):\n        super().__init__()\n\n        self.glow_decoder = GlowDecoder(\n            in_channels,\n            hidden_channels,\n            kernel_size,\n            dilation_rate,\n            num_flow_blocks,\n            num_coupling_layers,\n            dropout_p,\n            num_splits,\n            num_squeeze,\n            sigmoid_scale,\n            c_in_channels,\n        )\n        self.n_sqz = num_squeeze\n\n    def forward(self, x, x_len, g=None, reverse=False):\n        \"\"\"\n        Input shapes:\n            - x:  :math:`[B, C, T]`\n            - x_len :math:`[B]`\n            - g: :math:`[B, C]`\n\n        Output shapes:\n            - x:  :math:`[B, C, T]`\n            - x_len :math:`[B]`\n            - logget_tot :math:`[B]`\n        \"\"\"\n        x, x_len, x_max_len = self.preprocess(x, x_len, x_len.max())\n        x_mask = torch.unsqueeze(sequence_mask(x_len, x_max_len), 1).to(x.dtype)\n        x, logdet_tot = self.glow_decoder(x, x_mask, g, reverse)\n        return x, x_len, logdet_tot\n\n    def preprocess(self, y, y_lengths, y_max_length):\n        if y_max_length is not None:\n            y_max_length = torch.div(y_max_length, self.n_sqz, rounding_mode=\"floor\") * self.n_sqz\n            y = y[:, :, :y_max_length]\n        y_lengths = torch.div(y_lengths, self.n_sqz, rounding_mode=\"floor\") * self.n_sqz\n        return y, y_lengths, y_max_length\n\n    def store_inverse(self):\n        self.glow_decoder.store_inverse()\n", "TTS/tts/layers/overflow/neural_hmm.py": "from typing import List\n\nimport torch\nimport torch.distributions as tdist\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom TTS.tts.layers.overflow.common_layers import Outputnet, OverflowUtils\nfrom TTS.tts.layers.tacotron.common_layers import Prenet\nfrom TTS.tts.utils.helpers import sequence_mask\n\n\nclass NeuralHMM(nn.Module):\n    \"\"\"Autoregressive left to right HMM model primarily used in \"Neural HMMs are all you need (for high-quality attention-free TTS)\"\n\n    Paper::\n        https://arxiv.org/abs/2108.13320\n\n    Paper abstract::\n        Neural sequence-to-sequence TTS has achieved significantly better output quality than statistical speech synthesis using\n        HMMs. However, neural TTS is generally not probabilistic and uses non-monotonic attention. Attention failures increase\n        training time and can make synthesis babble incoherently. This paper describes how the old and new paradigms can be\n        combined to obtain the advantages of both worlds, by replacing attention in neural TTS with an autoregressive left-right\n        no-skip hidden Markov model defined by a neural network. Based on this proposal, we modify Tacotron 2 to obtain an\n        HMM-based neural TTS model with monotonic alignment, trained to maximise the full sequence likelihood without\n        approximation. We also describe how to combine ideas from classical and contemporary TTS for best results. The resulting\n        example system is smaller and simpler than Tacotron 2, and learns to speak with fewer iterations and less data, whilst\n        achieving comparable naturalness prior to the post-net. Our approach also allows easy control over speaking rate.\n\n    Args:\n        frame_channels (int): Output dimension to generate.\n        ar_order (int): Autoregressive order of the model. In ablations of Neural HMM it was found that more autoregression while giving more variation hurts naturalness of the synthesised audio.\n        deterministic_transition (bool): deterministic duration generation based on duration quantiles as defiend in \"S. Ronanki, O. Watts, S. King, and G. E. Henter, \u201cMedianbased generation of synthetic speech durations using a nonparametric approach,\u201d in Proc. SLT, 2016.\". Defaults to True.\n        encoder_dim (int): Channels of encoder input and character embedding tensors. Defaults to 512.\n        prenet_type (str): `original` or `bn`. `original` sets the default Prenet and `bn` uses Batch Normalization version of the Prenet.\n        prenet_dim (int): Dimension of the Prenet.\n        prenet_n_layers (int): Number of layers in the Prenet.\n        prenet_dropout (float): Dropout probability of the Prenet.\n        prenet_dropout_at_inference (bool): If True, dropout is applied at inference time.\n        memory_rnn_dim (int): Size of the memory RNN to process output of prenet.\n        outputnet_size (List[int]): Size of the output network inside the neural HMM.\n        flat_start_params (dict): Parameters for the flat start initialization of the neural HMM.\n        std_floor (float): Floor value for the standard deviation of the neural HMM. Prevents model cheating by putting point mass and getting infinite likelihood at any datapoint.\n        use_grad_checkpointing (bool, optional): Use gradient checkpointing to save memory. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        frame_channels: int,\n        ar_order: int,\n        deterministic_transition: bool,\n        encoder_dim: int,\n        prenet_type: str,\n        prenet_dim: int,\n        prenet_n_layers: int,\n        prenet_dropout: float,\n        prenet_dropout_at_inference: bool,\n        memory_rnn_dim: int,\n        outputnet_size: List[int],\n        flat_start_params: dict,\n        std_floor: float,\n        use_grad_checkpointing: bool = True,\n    ):\n        super().__init__()\n\n        self.frame_channels = frame_channels\n        self.ar_order = ar_order\n        self.deterministic_transition = deterministic_transition\n        self.prenet_dim = prenet_dim\n        self.memory_rnn_dim = memory_rnn_dim\n        self.use_grad_checkpointing = use_grad_checkpointing\n\n        self.transition_model = TransitionModel()\n        self.emission_model = EmissionModel()\n\n        assert ar_order > 0, f\"AR order must be greater than 0 provided {ar_order}\"\n\n        self.ar_order = ar_order\n        self.prenet = Prenet(\n            in_features=frame_channels * ar_order,\n            prenet_type=prenet_type,\n            prenet_dropout=prenet_dropout,\n            dropout_at_inference=prenet_dropout_at_inference,\n            out_features=[self.prenet_dim for _ in range(prenet_n_layers)],\n            bias=False,\n        )\n        self.memory_rnn = nn.LSTMCell(input_size=prenet_dim, hidden_size=memory_rnn_dim)\n        self.output_net = Outputnet(\n            encoder_dim, memory_rnn_dim, frame_channels, outputnet_size, flat_start_params, std_floor\n        )\n        self.register_buffer(\"go_tokens\", torch.zeros(ar_order, 1))\n\n    def forward(self, inputs, inputs_len, mels, mel_lens):\n        r\"\"\"HMM forward algorithm for training uses logarithmic version of Rabiner (1989) forward algorithm.\n\n        Args:\n            inputs (torch.FloatTensor): Encoder outputs\n            inputs_len (torch.LongTensor): Encoder output lengths\n            mels (torch.FloatTensor): Mel inputs\n            mel_lens (torch.LongTensor): Length of mel inputs\n\n        Shapes:\n            - inputs: (B, T, D_out_enc)\n            - inputs_len: (B)\n            - mels: (B, D_mel, T_mel)\n            - mel_lens: (B)\n\n        Returns:\n            log_prob (torch.FloatTensor): Log probability of the sequence\n        \"\"\"\n        # Get dimensions of inputs\n        batch_size, N, _ = inputs.shape\n        T_max = torch.max(mel_lens)\n        mels = mels.permute(0, 2, 1)\n\n        # Intialize forward algorithm\n        log_state_priors = self._initialize_log_state_priors(inputs)\n        log_c, log_alpha_scaled, transition_matrix, means = self._initialize_forward_algorithm_variables(mels, N)\n\n        # Initialize autoregression elements\n        ar_inputs = self._add_go_token(mels)\n        h_memory, c_memory = self._init_lstm_states(batch_size, self.memory_rnn_dim, mels)\n\n        for t in range(T_max):\n            # Process Autoregression\n            h_memory, c_memory = self._process_ar_timestep(t, ar_inputs, h_memory, c_memory)\n            # Get mean, std and transition vector from decoder for this timestep\n            # Note: Gradient checkpointing currently doesn't works with multiple gpus inside a loop\n            if self.use_grad_checkpointing and self.training:\n                mean, std, transition_vector = checkpoint(self.output_net, h_memory, inputs)\n            else:\n                mean, std, transition_vector = self.output_net(h_memory, inputs)\n\n            if t == 0:\n                log_alpha_temp = log_state_priors + self.emission_model(mels[:, 0], mean, std, inputs_len)\n            else:\n                log_alpha_temp = self.emission_model(mels[:, t], mean, std, inputs_len) + self.transition_model(\n                    log_alpha_scaled[:, t - 1, :], transition_vector, inputs_len\n                )\n            log_c[:, t] = torch.logsumexp(log_alpha_temp, dim=1)\n            log_alpha_scaled[:, t, :] = log_alpha_temp - log_c[:, t].unsqueeze(1)\n            transition_matrix[:, t] = transition_vector  # needed for absorption state calculation\n\n            # Save for plotting\n            means.append(mean.detach())\n\n        log_c, log_alpha_scaled = self._mask_lengths(mel_lens, log_c, log_alpha_scaled)\n\n        sum_final_log_c = self.get_absorption_state_scaling_factor(\n            mel_lens, log_alpha_scaled, inputs_len, transition_matrix\n        )\n\n        log_probs = torch.sum(log_c, dim=1) + sum_final_log_c\n\n        return log_probs, log_alpha_scaled, transition_matrix, means\n\n    @staticmethod\n    def _mask_lengths(mel_lens, log_c, log_alpha_scaled):\n        \"\"\"\n        Mask the lengths of the forward variables so that the variable lenghts\n        do not contribute in the loss calculation\n        Args:\n            mel_inputs (torch.FloatTensor): (batch, T, frame_channels)\n            mel_inputs_lengths (torch.IntTensor): (batch)\n            log_c (torch.FloatTensor): (batch, T)\n        Returns:\n            log_c (torch.FloatTensor) : scaled probabilities (batch, T)\n            log_alpha_scaled (torch.FloatTensor): forward probabilities (batch, T, N)\n        \"\"\"\n        mask_log_c = sequence_mask(mel_lens)\n        log_c = log_c * mask_log_c\n        mask_log_alpha_scaled = mask_log_c.unsqueeze(2)\n        log_alpha_scaled = log_alpha_scaled * mask_log_alpha_scaled\n        return log_c, log_alpha_scaled\n\n    def _process_ar_timestep(\n        self,\n        t,\n        ar_inputs,\n        h_memory,\n        c_memory,\n    ):\n        \"\"\"\n        Process autoregression in timestep\n        1. At a specific t timestep\n        2. Perform data dropout if applied (we did not use it)\n        3. Run the autoregressive frame through the prenet (has dropout)\n        4. Run the prenet output through the post prenet rnn\n\n        Args:\n            t (int): mel-spec timestep\n            ar_inputs (torch.FloatTensor): go-token appended mel-spectrograms\n                - shape: (b, D_out, T_out)\n            h_post_prenet (torch.FloatTensor): previous timestep rnn hidden state\n                - shape: (b, memory_rnn_dim)\n            c_post_prenet (torch.FloatTensor): previous timestep rnn cell state\n                - shape: (b, memory_rnn_dim)\n\n        Returns:\n            h_post_prenet (torch.FloatTensor): rnn hidden state of the current timestep\n            c_post_prenet (torch.FloatTensor): rnn cell state of the current timestep\n        \"\"\"\n        prenet_input = ar_inputs[:, t : t + self.ar_order].flatten(1)\n        memory_inputs = self.prenet(prenet_input)\n        h_memory, c_memory = self.memory_rnn(memory_inputs, (h_memory, c_memory))\n        return h_memory, c_memory\n\n    def _add_go_token(self, mel_inputs):\n        \"\"\"Append the go token to create the autoregressive input\n        Args:\n            mel_inputs (torch.FloatTensor): (batch_size, T, n_mel_channel)\n        Returns:\n            ar_inputs (torch.FloatTensor): (batch_size, T, n_mel_channel)\n        \"\"\"\n        batch_size, T, _ = mel_inputs.shape\n        go_tokens = self.go_tokens.unsqueeze(0).expand(batch_size, self.ar_order, self.frame_channels)\n        ar_inputs = torch.cat((go_tokens, mel_inputs), dim=1)[:, :T]\n        return ar_inputs\n\n    @staticmethod\n    def _initialize_forward_algorithm_variables(mel_inputs, N):\n        r\"\"\"Initialize placeholders for forward algorithm variables, to use a stable\n                version we will use log_alpha_scaled and the scaling constant\n\n        Args:\n            mel_inputs (torch.FloatTensor): (b, T_max, frame_channels)\n            N (int): number of states\n        Returns:\n            log_c (torch.FloatTensor): Scaling constant (b, T_max)\n        \"\"\"\n        b, T_max, _ = mel_inputs.shape\n        log_alpha_scaled = mel_inputs.new_zeros((b, T_max, N))\n        log_c = mel_inputs.new_zeros(b, T_max)\n        transition_matrix = mel_inputs.new_zeros((b, T_max, N))\n\n        # Saving for plotting later, will not have gradient tapes\n        means = []\n        return log_c, log_alpha_scaled, transition_matrix, means\n\n    @staticmethod\n    def _init_lstm_states(batch_size, hidden_state_dim, device_tensor):\n        r\"\"\"\n        Initialize Hidden and Cell states for LSTM Cell\n\n        Args:\n            batch_size (Int): batch size\n            hidden_state_dim (Int): dimensions of the h and c\n            device_tensor (torch.FloatTensor): useful for the device and type\n\n        Returns:\n            (torch.FloatTensor): shape (batch_size, hidden_state_dim)\n                can be hidden state for LSTM\n            (torch.FloatTensor): shape (batch_size, hidden_state_dim)\n                can be the cell state for LSTM\n        \"\"\"\n        return (\n            device_tensor.new_zeros(batch_size, hidden_state_dim),\n            device_tensor.new_zeros(batch_size, hidden_state_dim),\n        )\n\n    def get_absorption_state_scaling_factor(self, mels_len, log_alpha_scaled, inputs_len, transition_vector):\n        \"\"\"Returns the final scaling factor of absorption state\n\n        Args:\n            mels_len (torch.IntTensor): Input size of mels to\n                    get the last timestep of log_alpha_scaled\n            log_alpha_scaled (torch.FloatTEnsor): State probabilities\n            text_lengths (torch.IntTensor): length of the states to\n                    mask the values of states lengths\n                (\n                    Useful when the batch has very different lengths,\n                    when the length of an observation is less than\n                    the number of max states, then the log alpha after\n                    the state value is filled with -infs. So we mask\n                    those values so that it only consider the states\n                    which are needed for that length\n                )\n            transition_vector (torch.FloatTensor): transtiion vector for each state per timestep\n\n        Shapes:\n            - mels_len: (batch_size)\n            - log_alpha_scaled: (batch_size, N, T)\n            - text_lengths: (batch_size)\n            - transition_vector: (batch_size, N, T)\n\n        Returns:\n            sum_final_log_c (torch.FloatTensor): (batch_size)\n\n        \"\"\"\n        N = torch.max(inputs_len)\n        max_inputs_len = log_alpha_scaled.shape[2]\n        state_lengths_mask = sequence_mask(inputs_len, max_len=max_inputs_len)\n\n        last_log_alpha_scaled_index = (\n            (mels_len - 1).unsqueeze(-1).expand(-1, N).unsqueeze(1)\n        )  # Batch X Hidden State Size\n        last_log_alpha_scaled = torch.gather(log_alpha_scaled, 1, last_log_alpha_scaled_index).squeeze(1)\n        last_log_alpha_scaled = last_log_alpha_scaled.masked_fill(~state_lengths_mask, -float(\"inf\"))\n\n        last_transition_vector = torch.gather(transition_vector, 1, last_log_alpha_scaled_index).squeeze(1)\n        last_transition_probability = torch.sigmoid(last_transition_vector)\n        log_probability_of_transitioning = OverflowUtils.log_clamped(last_transition_probability)\n\n        last_transition_probability_index = self.get_mask_for_last_item(inputs_len, inputs_len.device)\n        log_probability_of_transitioning = log_probability_of_transitioning.masked_fill(\n            ~last_transition_probability_index, -float(\"inf\")\n        )\n        final_log_c = last_log_alpha_scaled + log_probability_of_transitioning\n\n        # If the length of the mel is less than the number of states it will select the -inf values leading to nan gradients\n        # Ideally, we should clean the dataset otherwise this is a little hack uncomment the line below\n        final_log_c = final_log_c.clamp(min=torch.finfo(final_log_c.dtype).min)\n\n        sum_final_log_c = torch.logsumexp(final_log_c, dim=1)\n        return sum_final_log_c\n\n    @staticmethod\n    def get_mask_for_last_item(lengths, device, out_tensor=None):\n        \"\"\"Returns n-1 mask for the last item in the sequence.\n\n        Args:\n            lengths (torch.IntTensor): lengths in a batch\n            device (str, optional): Defaults to \"cpu\".\n            out_tensor (torch.Tensor, optional): uses the memory of a specific tensor.\n                Defaults to None.\n\n        Returns:\n            - Shape: :math:`(b, max_len)`\n        \"\"\"\n        max_len = torch.max(lengths).item()\n        ids = (\n            torch.arange(0, max_len, device=device) if out_tensor is None else torch.arange(0, max_len, out=out_tensor)\n        )\n        mask = ids == lengths.unsqueeze(1) - 1\n        return mask\n\n    @torch.inference_mode()\n    def inference(\n        self,\n        inputs: torch.FloatTensor,\n        input_lens: torch.LongTensor,\n        sampling_temp: float,\n        max_sampling_time: int,\n        duration_threshold: float,\n    ):\n        \"\"\"Inference from autoregressive neural HMM\n\n        Args:\n            inputs (torch.FloatTensor): input states\n                - shape: :math:`(b, T, d)`\n            input_lens (torch.LongTensor): input state lengths\n                - shape: :math:`(b)`\n            sampling_temp (float): sampling temperature\n            max_sampling_temp (int): max sampling temperature\n            duration_threshold (float): duration threshold to switch to next state\n                - Use this to change the spearking rate of the synthesised audio\n        \"\"\"\n\n        b = inputs.shape[0]\n        outputs = {\n            \"hmm_outputs\": [],\n            \"hmm_outputs_len\": [],\n            \"alignments\": [],\n            \"input_parameters\": [],\n            \"output_parameters\": [],\n        }\n        for i in range(b):\n            neural_hmm_outputs, states_travelled, input_parameters, output_parameters = self.sample(\n                inputs[i : i + 1], input_lens[i], sampling_temp, max_sampling_time, duration_threshold\n            )\n\n            outputs[\"hmm_outputs\"].append(neural_hmm_outputs)\n            outputs[\"hmm_outputs_len\"].append(neural_hmm_outputs.shape[0])\n            outputs[\"alignments\"].append(states_travelled)\n            outputs[\"input_parameters\"].append(input_parameters)\n            outputs[\"output_parameters\"].append(output_parameters)\n\n        outputs[\"hmm_outputs\"] = nn.utils.rnn.pad_sequence(outputs[\"hmm_outputs\"], batch_first=True)\n        outputs[\"hmm_outputs_len\"] = torch.tensor(\n            outputs[\"hmm_outputs_len\"], dtype=input_lens.dtype, device=input_lens.device\n        )\n        return outputs\n\n    @torch.inference_mode()\n    def sample(self, inputs, input_lens, sampling_temp, max_sampling_time, duration_threshold):\n        \"\"\"Samples an output from the parameter models\n\n        Args:\n            inputs (torch.FloatTensor): input states\n                - shape: :math:`(1, T, d)`\n            input_lens (torch.LongTensor): input state lengths\n                - shape: :math:`(1)`\n            sampling_temp (float): sampling temperature\n            max_sampling_time (int): max sampling time\n            duration_threshold (float): duration threshold to switch to next state\n\n        Returns:\n            outputs (torch.FloatTensor): Output Observations\n                - Shape: :math:`(T, output_dim)`\n            states_travelled (list[int]): Hidden states travelled\n                - Shape: :math:`(T)`\n            input_parameters (list[torch.FloatTensor]): Input parameters\n            output_parameters (list[torch.FloatTensor]): Output parameters\n        \"\"\"\n        states_travelled, outputs, t = [], [], 0\n\n        # Sample initial state\n        current_state = 0\n        states_travelled.append(current_state)\n\n        # Prepare autoregression\n        prenet_input = self.go_tokens.unsqueeze(0).expand(1, self.ar_order, self.frame_channels)\n        h_memory, c_memory = self._init_lstm_states(1, self.memory_rnn_dim, prenet_input)\n\n        input_parameter_values = []\n        output_parameter_values = []\n        quantile = 1\n        while True:\n            memory_input = self.prenet(prenet_input.flatten(1).unsqueeze(0))\n            # will be 1 while sampling\n            h_memory, c_memory = self.memory_rnn(memory_input.squeeze(0), (h_memory, c_memory))\n\n            z_t = inputs[:, current_state].unsqueeze(0)  # Add fake time dimension\n            mean, std, transition_vector = self.output_net(h_memory, z_t)\n\n            transition_probability = torch.sigmoid(transition_vector.flatten())\n            staying_probability = torch.sigmoid(-transition_vector.flatten())\n\n            # Save for plotting\n            input_parameter_values.append([prenet_input, current_state])\n            output_parameter_values.append([mean, std, transition_probability])\n\n            x_t = self.emission_model.sample(mean, std, sampling_temp=sampling_temp)\n\n            # Prepare autoregressive input for next iteration\n            prenet_input = torch.cat((prenet_input, x_t), dim=1)[:, 1:]\n\n            outputs.append(x_t.flatten())\n\n            transition_matrix = torch.cat((staying_probability, transition_probability))\n            quantile *= staying_probability\n            if not self.deterministic_transition:\n                switch = transition_matrix.multinomial(1)[0].item()\n            else:\n                switch = quantile < duration_threshold\n\n            if switch:\n                current_state += 1\n                quantile = 1\n\n            states_travelled.append(current_state)\n\n            if (current_state == input_lens) or (max_sampling_time and t == max_sampling_time - 1):\n                break\n\n            t += 1\n\n        return (\n            torch.stack(outputs, dim=0),\n            F.one_hot(input_lens.new_tensor(states_travelled)),\n            input_parameter_values,\n            output_parameter_values,\n        )\n\n    @staticmethod\n    def _initialize_log_state_priors(text_embeddings):\n        \"\"\"Creates the log pi in forward algorithm.\n\n        Args:\n            text_embeddings (torch.FloatTensor): used to create the log pi\n                    on current device\n\n        Shapes:\n            - text_embeddings: (B, T, D_out_enc)\n        \"\"\"\n        N = text_embeddings.shape[1]\n        log_state_priors = text_embeddings.new_full([N], -float(\"inf\"))\n        log_state_priors[0] = 0.0\n        return log_state_priors\n\n\nclass TransitionModel(nn.Module):\n    \"\"\"Transition Model of the HMM, it represents the probability of transitioning\n    form current state to all other states\"\"\"\n\n    def forward(self, log_alpha_scaled, transition_vector, inputs_len):  # pylint: disable=no-self-use\n        r\"\"\"\n        product of the past state with transitional probabilities in log space\n\n        Args:\n            log_alpha_scaled (torch.Tensor): Multiply previous timestep's alphas by\n                        transition matrix (in log domain)\n                - shape: (batch size, N)\n            transition_vector (torch.tensor): transition vector for each state\n                - shape: (N)\n            inputs_len (int tensor): Lengths of states in a batch\n                - shape: (batch)\n\n        Returns:\n            out (torch.FloatTensor): log probability of transitioning to each state\n        \"\"\"\n        transition_p = torch.sigmoid(transition_vector)\n        staying_p = torch.sigmoid(-transition_vector)\n\n        log_staying_probability = OverflowUtils.log_clamped(staying_p)\n        log_transition_probability = OverflowUtils.log_clamped(transition_p)\n\n        staying = log_alpha_scaled + log_staying_probability\n        leaving = log_alpha_scaled + log_transition_probability\n        leaving = leaving.roll(1, dims=1)\n        leaving[:, 0] = -float(\"inf\")\n        inputs_len_mask = sequence_mask(inputs_len)\n        out = OverflowUtils.logsumexp(torch.stack((staying, leaving), dim=2), dim=2)\n        out = out.masked_fill(~inputs_len_mask, -float(\"inf\"))  # There are no states to contribute to the loss\n        return out\n\n\nclass EmissionModel(nn.Module):\n    \"\"\"Emission Model of the HMM, it represents the probability of\n    emitting an observation based on the current state\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.distribution_function: tdist.Distribution = tdist.normal.Normal\n\n    def sample(self, means, stds, sampling_temp):\n        return self.distribution_function(means, stds * sampling_temp).sample() if sampling_temp > 0 else means\n\n    def forward(self, x_t, means, stds, state_lengths):\n        r\"\"\"Calculates the log probability of the the given data (x_t)\n            being observed from states with given means and stds\n        Args:\n            x_t (float tensor) : observation at current time step\n                - shape: (batch, feature_dim)\n            means (float tensor): means of the distributions of hidden states\n                - shape: (batch, hidden_state, feature_dim)\n            stds (float tensor): standard deviations of the distributions of the hidden states\n                - shape: (batch, hidden_state, feature_dim)\n            state_lengths (int tensor): Lengths of states in a batch\n                - shape: (batch)\n\n        Returns:\n            out (float tensor): observation log likelihoods,\n                                    expressing the probability of an observation\n                being generated from a state i\n                shape: (batch, hidden_state)\n        \"\"\"\n        emission_dists = self.distribution_function(means, stds)\n        out = emission_dists.log_prob(x_t.unsqueeze(1))\n        state_lengths_mask = sequence_mask(state_lengths).unsqueeze(2)\n        out = torch.sum(out * state_lengths_mask, dim=2)\n        return out\n", "TTS/tts/layers/overflow/plotting_utils.py": "from typing import Any\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n\ndef validate_numpy_array(value: Any):\n    r\"\"\"\n    Validates the input and makes sure it returns a numpy array (i.e on CPU)\n\n    Args:\n        value (Any): the input value\n\n    Raises:\n        TypeError: if the value is not a numpy array or torch tensor\n\n    Returns:\n        np.ndarray: numpy array of the value\n    \"\"\"\n    if isinstance(value, np.ndarray):\n        pass\n    elif isinstance(value, list):\n        value = np.array(value)\n    elif torch.is_tensor(value):\n        value = value.cpu().numpy()\n    else:\n        raise TypeError(\"Value must be a numpy array, a torch tensor or a list\")\n\n    return value\n\n\ndef get_spec_from_most_probable_state(log_alpha_scaled, means, decoder=None):\n    \"\"\"Get the most probable state means from the log_alpha_scaled.\n\n    Args:\n        log_alpha_scaled (torch.Tensor): Log alpha scaled values.\n            - Shape: :math:`(T, N)`\n        means (torch.Tensor): Means of the states.\n            - Shape: :math:`(N, T, D_out)`\n        decoder (torch.nn.Module): Decoder module to decode the latent to melspectrogram. Defaults to None.\n    \"\"\"\n    max_state_numbers = torch.max(log_alpha_scaled, dim=1)[1]\n    max_len = means.shape[0]\n    n_mel_channels = means.shape[2]\n    max_state_numbers = max_state_numbers.unsqueeze(1).unsqueeze(1).expand(max_len, 1, n_mel_channels)\n    means = torch.gather(means, 1, max_state_numbers).squeeze(1).to(log_alpha_scaled.dtype)\n    if decoder is not None:\n        mel = (\n            decoder(means.T.unsqueeze(0), torch.tensor([means.shape[0]], device=means.device), reverse=True)[0]\n            .squeeze(0)\n            .T\n        )\n    else:\n        mel = means\n    return mel\n\n\ndef plot_transition_probabilities_to_numpy(states, transition_probabilities, output_fig=False):\n    \"\"\"Generates trainsition probabilities plot for the states and the probability of transition.\n\n    Args:\n        states (torch.IntTensor): the states\n        transition_probabilities (torch.FloatTensor): the transition probabilities\n    \"\"\"\n    states = validate_numpy_array(states)\n    transition_probabilities = validate_numpy_array(transition_probabilities)\n\n    fig, ax = plt.subplots(figsize=(30, 3))\n    ax.plot(transition_probabilities, \"o\")\n    ax.set_title(\"Transition probability of state\")\n    ax.set_xlabel(\"hidden state\")\n    ax.set_ylabel(\"probability\")\n    ax.set_xticks([i for i in range(len(transition_probabilities))])  # pylint: disable=unnecessary-comprehension\n    ax.set_xticklabels([int(x) for x in states], rotation=90)\n    plt.tight_layout()\n    if not output_fig:\n        plt.close()\n    return fig\n", "TTS/tts/layers/overflow/__init__.py": "", "TTS/tts/layers/glow_tts/glow.py": "import torch\nfrom packaging.version import Version\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom TTS.tts.layers.generic.wavenet import WN\n\nfrom ..generic.normalization import LayerNorm\n\n\nclass ResidualConv1dLayerNormBlock(nn.Module):\n    \"\"\"Conv1d with Layer Normalization and residual connection as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n\n    ::\n\n        x |-> conv1d -> layer_norm -> relu -> dropout -> + -> o\n          |---------------> conv1d_1x1 ------------------|\n\n    Args:\n        in_channels (int): number of input tensor channels.\n        hidden_channels (int): number of inner layer channels.\n        out_channels (int): number of output tensor channels.\n        kernel_size (int): kernel size of conv1d filter.\n        num_layers (int): number of blocks.\n        dropout_p (float): dropout rate for each block.\n    \"\"\"\n\n    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, num_layers, dropout_p):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.dropout_p = dropout_p\n        assert num_layers > 1, \" [!] number of layers should be > 0.\"\n        assert kernel_size % 2 == 1, \" [!] kernel size should be odd number.\"\n\n        self.conv_layers = nn.ModuleList()\n        self.norm_layers = nn.ModuleList()\n\n        for idx in range(num_layers):\n            self.conv_layers.append(\n                nn.Conv1d(\n                    in_channels if idx == 0 else hidden_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n                )\n            )\n            self.norm_layers.append(LayerNorm(hidden_channels))\n\n        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n        \"\"\"\n        x_res = x\n        for i in range(self.num_layers):\n            x = self.conv_layers[i](x * x_mask)\n            x = self.norm_layers[i](x * x_mask)\n            x = F.dropout(F.relu(x), self.dropout_p, training=self.training)\n        x = x_res + self.proj(x)\n        return x * x_mask\n\n\nclass InvConvNear(nn.Module):\n    \"\"\"Invertible Convolution with input splitting as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n\n    Args:\n        channels (int): input and output channels.\n        num_splits (int): number of splits, also H and W of conv layer.\n        no_jacobian (bool): enable/disable jacobian computations.\n\n    Note:\n        Split the input into groups of size self.num_splits and\n        perform 1x1 convolution separately. Cast 1x1 conv operation\n        to 2d by reshaping the input for efficiency.\n    \"\"\"\n\n    def __init__(self, channels, num_splits=4, no_jacobian=False, **kwargs):  # pylint: disable=unused-argument\n        super().__init__()\n        assert num_splits % 2 == 0\n        self.channels = channels\n        self.num_splits = num_splits\n        self.no_jacobian = no_jacobian\n        self.weight_inv = None\n\n        if Version(torch.__version__) < Version(\"1.9\"):\n            w_init = torch.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_())[0]\n        else:\n            w_init = torch.linalg.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_(), \"complete\")[0]\n\n        if torch.det(w_init) < 0:\n            w_init[:, 0] = -1 * w_init[:, 0]\n        self.weight = nn.Parameter(w_init)\n\n    def forward(self, x, x_mask=None, reverse=False, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n        \"\"\"\n        b, c, t = x.size()\n        assert c % self.num_splits == 0\n        if x_mask is None:\n            x_mask = 1\n            x_len = torch.ones((b,), dtype=x.dtype, device=x.device) * t\n        else:\n            x_len = torch.sum(x_mask, [1, 2])\n\n        x = x.view(b, 2, c // self.num_splits, self.num_splits // 2, t)\n        x = x.permute(0, 1, 3, 2, 4).contiguous().view(b, self.num_splits, c // self.num_splits, t)\n\n        if reverse:\n            if self.weight_inv is not None:\n                weight = self.weight_inv\n            else:\n                weight = torch.inverse(self.weight.float()).to(dtype=self.weight.dtype)\n            logdet = None\n        else:\n            weight = self.weight\n            if self.no_jacobian:\n                logdet = 0\n            else:\n                logdet = torch.logdet(self.weight) * (c / self.num_splits) * x_len  # [b]\n\n        weight = weight.view(self.num_splits, self.num_splits, 1, 1)\n        z = F.conv2d(x, weight)\n\n        z = z.view(b, 2, self.num_splits // 2, c // self.num_splits, t)\n        z = z.permute(0, 1, 3, 2, 4).contiguous().view(b, c, t) * x_mask\n        return z, logdet\n\n    def store_inverse(self):\n        weight_inv = torch.inverse(self.weight.float()).to(dtype=self.weight.dtype)\n        self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)\n\n\nclass CouplingBlock(nn.Module):\n    \"\"\"Glow Affine Coupling block as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n\n    ::\n\n        x --> x0 -> conv1d -> wavenet -> conv1d --> t, s -> concat(s*x1 + t, x0) -> o\n        '-> x1 - - - - - - - - - - - - - - - - - - - - - - - - - ^\n\n    Args:\n         in_channels (int): number of input tensor channels.\n         hidden_channels (int): number of hidden channels.\n         kernel_size (int): WaveNet filter kernel size.\n         dilation_rate (int): rate to increase dilation by each layer in a decoder block.\n         num_layers (int): number of WaveNet layers.\n         c_in_channels (int): number of conditioning input channels.\n         dropout_p (int): wavenet dropout rate.\n         sigmoid_scale (bool): enable/disable sigmoid scaling for output scale.\n\n    Note:\n         It does not use the conditional inputs differently from WaveGlow.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_layers,\n        c_in_channels=0,\n        dropout_p=0,\n        sigmoid_scale=False,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.num_layers = num_layers\n        self.c_in_channels = c_in_channels\n        self.dropout_p = dropout_p\n        self.sigmoid_scale = sigmoid_scale\n        # input layer\n        start = torch.nn.Conv1d(in_channels // 2, hidden_channels, 1)\n        start = torch.nn.utils.parametrizations.weight_norm(start)\n        self.start = start\n        # output layer\n        # Initializing last layer to 0 makes the affine coupling layers\n        # do nothing at first.  This helps with training stability\n        end = torch.nn.Conv1d(hidden_channels, in_channels, 1)\n        end.weight.data.zero_()\n        end.bias.data.zero_()\n        self.end = end\n        # coupling layers\n        self.wn = WN(hidden_channels, hidden_channels, kernel_size, dilation_rate, num_layers, c_in_channels, dropout_p)\n\n    def forward(self, x, x_mask=None, reverse=False, g=None, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n            - g: :math:`[B, C, 1]`\n        \"\"\"\n        if x_mask is None:\n            x_mask = 1\n        x_0, x_1 = x[:, : self.in_channels // 2], x[:, self.in_channels // 2 :]\n\n        x = self.start(x_0) * x_mask\n        x = self.wn(x, x_mask, g)\n        out = self.end(x)\n\n        z_0 = x_0\n        t = out[:, : self.in_channels // 2, :]\n        s = out[:, self.in_channels // 2 :, :]\n        if self.sigmoid_scale:\n            s = torch.log(1e-6 + torch.sigmoid(s + 2))\n\n        if reverse:\n            z_1 = (x_1 - t) * torch.exp(-s) * x_mask\n            logdet = None\n        else:\n            z_1 = (t + torch.exp(s) * x_1) * x_mask\n            logdet = torch.sum(s * x_mask, [1, 2])\n\n        z = torch.cat([z_0, z_1], 1)\n        return z, logdet\n\n    def store_inverse(self):\n        self.wn.remove_weight_norm()\n", "TTS/tts/layers/glow_tts/transformer.py": "import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom TTS.tts.layers.generic.normalization import LayerNorm, LayerNorm2\n\n\nclass RelativePositionMultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention with Relative Positional embedding.\n    https://arxiv.org/pdf/1809.04281.pdf\n\n    It learns positional embeddings for a window of neighbours. For keys and values,\n    it learns different set of embeddings. Key embeddings are agregated with the attention\n    scores and value embeddings are aggregated with the output.\n\n    Note:\n        Example with relative attention window size 2\n\n        - input = [a, b, c, d, e]\n        - rel_attn_embeddings = [e(t-2), e(t-1), e(t+1), e(t+2)]\n\n        So it learns 4 embedding vectors (in total 8) separately for key and value vectors.\n\n        Considering the input c\n\n        - e(t-2) corresponds to c -> a\n        - e(t-2) corresponds to c -> b\n        - e(t-2) corresponds to c -> d\n        - e(t-2) corresponds to c -> e\n\n        These embeddings are shared among different time steps. So input a, b, d and e also uses\n        the same embeddings.\n\n        Embeddings are ignored when the relative window is out of limit for the first and the last\n        n items.\n\n    Args:\n        channels (int): input and inner layer channels.\n        out_channels (int): output channels.\n        num_heads (int): number of attention heads.\n        rel_attn_window_size (int, optional): relation attention window size.\n            If 4, for each time step next and previous 4 time steps are attended.\n            If default, relative encoding is disabled and it is a regular transformer.\n            Defaults to None.\n        heads_share (bool, optional): [description]. Defaults to True.\n        dropout_p (float, optional): dropout rate. Defaults to 0..\n        input_length (int, optional): intput length for positional encoding. Defaults to None.\n        proximal_bias (bool, optional): enable/disable proximal bias as in the paper. Defaults to False.\n        proximal_init (bool, optional): enable/disable poximal init as in the paper.\n            Init key and query layer weights the same. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        out_channels,\n        num_heads,\n        rel_attn_window_size=None,\n        heads_share=True,\n        dropout_p=0.0,\n        input_length=None,\n        proximal_bias=False,\n        proximal_init=False,\n    ):\n        super().__init__()\n        assert channels % num_heads == 0, \" [!] channels should be divisible by num_heads.\"\n        # class attributes\n        self.channels = channels\n        self.out_channels = out_channels\n        self.num_heads = num_heads\n        self.rel_attn_window_size = rel_attn_window_size\n        self.heads_share = heads_share\n        self.input_length = input_length\n        self.proximal_bias = proximal_bias\n        self.dropout_p = dropout_p\n        self.attn = None\n        # query, key, value layers\n        self.k_channels = channels // num_heads\n        self.conv_q = nn.Conv1d(channels, channels, 1)\n        self.conv_k = nn.Conv1d(channels, channels, 1)\n        self.conv_v = nn.Conv1d(channels, channels, 1)\n        # output layers\n        self.conv_o = nn.Conv1d(channels, out_channels, 1)\n        self.dropout = nn.Dropout(dropout_p)\n        # relative positional encoding layers\n        if rel_attn_window_size is not None:\n            n_heads_rel = 1 if heads_share else num_heads\n            rel_stddev = self.k_channels**-0.5\n            emb_rel_k = nn.Parameter(\n                torch.randn(n_heads_rel, rel_attn_window_size * 2 + 1, self.k_channels) * rel_stddev\n            )\n            emb_rel_v = nn.Parameter(\n                torch.randn(n_heads_rel, rel_attn_window_size * 2 + 1, self.k_channels) * rel_stddev\n            )\n            self.register_parameter(\"emb_rel_k\", emb_rel_k)\n            self.register_parameter(\"emb_rel_v\", emb_rel_v)\n\n        # init layers\n        nn.init.xavier_uniform_(self.conv_q.weight)\n        nn.init.xavier_uniform_(self.conv_k.weight)\n        # proximal bias\n        if proximal_init:\n            self.conv_k.weight.data.copy_(self.conv_q.weight.data)\n            self.conv_k.bias.data.copy_(self.conv_q.bias.data)\n        nn.init.xavier_uniform_(self.conv_v.weight)\n\n    def forward(self, x, c, attn_mask=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - c: :math:`[B, C, T]`\n            - attn_mask: :math:`[B, 1, T, T]`\n        \"\"\"\n        q = self.conv_q(x)\n        k = self.conv_k(c)\n        v = self.conv_v(c)\n        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n        x = self.conv_o(x)\n        return x\n\n    def attention(self, query, key, value, mask=None):\n        # reshape [b, d, t] -> [b, n_h, t, d_k]\n        b, d, t_s, t_t = (*key.size(), query.size(2))\n        query = query.view(b, self.num_heads, self.k_channels, t_t).transpose(2, 3)\n        key = key.view(b, self.num_heads, self.k_channels, t_s).transpose(2, 3)\n        value = value.view(b, self.num_heads, self.k_channels, t_s).transpose(2, 3)\n        # compute raw attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.k_channels)\n        # relative positional encoding for scores\n        if self.rel_attn_window_size is not None:\n            assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n            # get relative key embeddings\n            key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n            rel_logits = self._matmul_with_relative_keys(query, key_relative_embeddings)\n            rel_logits = self._relative_position_to_absolute_position(rel_logits)\n            scores_local = rel_logits / math.sqrt(self.k_channels)\n            scores = scores + scores_local\n        # proximan bias\n        if self.proximal_bias:\n            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n            scores = scores + self._attn_proximity_bias(t_s).to(device=scores.device, dtype=scores.dtype)\n        # attention score masking\n        if mask is not None:\n            # add small value to prevent oor error.\n            scores = scores.masked_fill(mask == 0, -1e4)\n            if self.input_length is not None:\n                block_mask = torch.ones_like(scores).triu(-1 * self.input_length).tril(self.input_length)\n                scores = scores * block_mask + -1e4 * (1 - block_mask)\n        # attention score normalization\n        p_attn = F.softmax(scores, dim=-1)  # [b, n_h, t_t, t_s]\n        # apply dropout to attention weights\n        p_attn = self.dropout(p_attn)\n        # compute output\n        output = torch.matmul(p_attn, value)\n        # relative positional encoding for values\n        if self.rel_attn_window_size is not None:\n            relative_weights = self._absolute_position_to_relative_position(p_attn)\n            value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n            output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings)\n        output = output.transpose(2, 3).contiguous().view(b, d, t_t)  # [b, n_h, t_t, d_k] -> [b, d, t_t]\n        return output, p_attn\n\n    @staticmethod\n    def _matmul_with_relative_values(p_attn, re):\n        \"\"\"\n        Args:\n            p_attn (Tensor): attention weights.\n            re (Tensor): relative value embedding vector. (a_(i,j)^V)\n\n        Shapes:\n            -p_attn: :math:`[B, H, T, V]`\n            -re: :math:`[H or 1, V, D]`\n            -logits: :math:`[B, H, T, D]`\n        \"\"\"\n        logits = torch.matmul(p_attn, re.unsqueeze(0))\n        return logits\n\n    @staticmethod\n    def _matmul_with_relative_keys(query, re):\n        \"\"\"\n        Args:\n            query (Tensor): batch of query vectors. (x*W^Q)\n            re (Tensor): relative key embedding vector. (a_(i,j)^K)\n\n        Shapes:\n            - query: :math:`[B, H, T, D]`\n            - re: :math:`[H or 1, V, D]`\n            - logits: :math:`[B, H, T, V]`\n        \"\"\"\n        # logits = torch.einsum('bhld, kmd -> bhlm', [query, re.to(query.dtype)])\n        logits = torch.matmul(query, re.unsqueeze(0).transpose(-2, -1))\n        return logits\n\n    def _get_relative_embeddings(self, relative_embeddings, length):\n        \"\"\"Convert embedding vestors to a tensor of embeddings\"\"\"\n        # Pad first before slice to avoid using cond ops.\n        pad_length = max(length - (self.rel_attn_window_size + 1), 0)\n        slice_start_position = max((self.rel_attn_window_size + 1) - length, 0)\n        slice_end_position = slice_start_position + 2 * length - 1\n        if pad_length > 0:\n            padded_relative_embeddings = F.pad(relative_embeddings, [0, 0, pad_length, pad_length, 0, 0])\n        else:\n            padded_relative_embeddings = relative_embeddings\n        used_relative_embeddings = padded_relative_embeddings[:, slice_start_position:slice_end_position]\n        return used_relative_embeddings\n\n    @staticmethod\n    def _relative_position_to_absolute_position(x):\n        \"\"\"Converts tensor from relative to absolute indexing for local attention.\n        Shapes:\n            x: :math:`[B, C, T, 2 * T - 1]`\n        Returns:\n            A Tensor of shape :math:`[B, C, T, T]`\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # Pad to shift from relative to absolute indexing.\n        x = F.pad(x, [0, 1, 0, 0, 0, 0, 0, 0])\n        # Pad extra elements so to add up to shape (len+1, 2*len-1).\n        x_flat = x.view([batch, heads, length * 2 * length])\n        x_flat = F.pad(x_flat, [0, length - 1, 0, 0, 0, 0])\n        # Reshape and slice out the padded elements.\n        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[:, :, :length, length - 1 :]\n        return x_final\n\n    @staticmethod\n    def _absolute_position_to_relative_position(x):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T, T]`\n            - ret: :math:`[B, C, T, 2*T-1]`\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # padd along column\n        x = F.pad(x, [0, length - 1, 0, 0, 0, 0, 0, 0])\n        x_flat = x.view([batch, heads, length**2 + length * (length - 1)])\n        # add 0's in the beginning that will skew the elements after reshape\n        x_flat = F.pad(x_flat, [length, 0, 0, 0, 0, 0])\n        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n        return x_final\n\n    @staticmethod\n    def _attn_proximity_bias(length):\n        \"\"\"Produce an attention mask that discourages distant\n        attention values.\n        Args:\n            length (int): an integer scalar.\n        Returns:\n            a Tensor with shape :math:`[1, 1, T, T]`\n        \"\"\"\n        # L\n        r = torch.arange(length, dtype=torch.float32)\n        # L x L\n        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n        # scale mask values\n        diff = -torch.log1p(torch.abs(diff))\n        # 1 x 1 x L x L\n        return diff.unsqueeze(0).unsqueeze(0)\n\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"Feed Forward Inner layers for Transformer.\n\n    Args:\n        in_channels (int): input tensor channels.\n        out_channels (int): output tensor channels.\n        hidden_channels (int): inner layers hidden channels.\n        kernel_size (int): conv1d filter kernel size.\n        dropout_p (float, optional): dropout rate. Defaults to 0.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size, dropout_p=0.0, causal=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dropout_p = dropout_p\n\n        if causal:\n            self.padding = self._causal_padding\n        else:\n            self.padding = self._same_padding\n\n        self.conv_1 = nn.Conv1d(in_channels, hidden_channels, kernel_size)\n        self.conv_2 = nn.Conv1d(hidden_channels, out_channels, kernel_size)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x, x_mask):\n        x = self.conv_1(self.padding(x * x_mask))\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.conv_2(self.padding(x * x_mask))\n        return x * x_mask\n\n    def _causal_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = self.kernel_size - 1\n        pad_r = 0\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, self._pad_shape(padding))\n        return x\n\n    def _same_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = (self.kernel_size - 1) // 2\n        pad_r = self.kernel_size // 2\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, self._pad_shape(padding))\n        return x\n\n    @staticmethod\n    def _pad_shape(padding):\n        l = padding[::-1]\n        pad_shape = [item for sublist in l for item in sublist]\n        return pad_shape\n\n\nclass RelativePositionTransformer(nn.Module):\n    \"\"\"Transformer with Relative Potional Encoding.\n    https://arxiv.org/abs/1803.02155\n\n    Args:\n        in_channels (int): number of channels of the input tensor.\n        out_chanels (int): number of channels of the output tensor.\n        hidden_channels (int): model hidden channels.\n        hidden_channels_ffn (int): hidden channels of FeedForwardNetwork.\n        num_heads (int): number of attention heads.\n        num_layers (int): number of transformer layers.\n        kernel_size (int, optional): kernel size of feed-forward inner layers. Defaults to 1.\n        dropout_p (float, optional): dropout rate for self-attention and feed-forward inner layers_per_stack. Defaults to 0.\n        rel_attn_window_size (int, optional): relation attention window size.\n            If 4, for each time step next and previous 4 time steps are attended.\n            If default, relative encoding is disabled and it is a regular transformer.\n            Defaults to None.\n        input_length (int, optional): input lenght to limit position encoding. Defaults to None.\n        layer_norm_type (str, optional): type \"1\" uses torch tensor operations and type \"2\" uses torch layer_norm\n            primitive. Use type \"2\", type \"1: is for backward compat. Defaults to \"1\".\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        hidden_channels_ffn: int,\n        num_heads: int,\n        num_layers: int,\n        kernel_size=1,\n        dropout_p=0.0,\n        rel_attn_window_size: int = None,\n        input_length: int = None,\n        layer_norm_type: str = \"1\",\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.hidden_channels_ffn = hidden_channels_ffn\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        self.kernel_size = kernel_size\n        self.dropout_p = dropout_p\n        self.rel_attn_window_size = rel_attn_window_size\n\n        self.dropout = nn.Dropout(dropout_p)\n        self.attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n\n        for idx in range(self.num_layers):\n            self.attn_layers.append(\n                RelativePositionMultiHeadAttention(\n                    hidden_channels if idx != 0 else in_channels,\n                    hidden_channels,\n                    num_heads,\n                    rel_attn_window_size=rel_attn_window_size,\n                    dropout_p=dropout_p,\n                    input_length=input_length,\n                )\n            )\n            if layer_norm_type == \"1\":\n                self.norm_layers_1.append(LayerNorm(hidden_channels))\n            elif layer_norm_type == \"2\":\n                self.norm_layers_1.append(LayerNorm2(hidden_channels))\n            else:\n                raise ValueError(\" [!] Unknown layer norm type\")\n\n            if hidden_channels != out_channels and (idx + 1) == self.num_layers:\n                self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n\n            self.ffn_layers.append(\n                FeedForwardNetwork(\n                    hidden_channels,\n                    hidden_channels if (idx + 1) != self.num_layers else out_channels,\n                    hidden_channels_ffn,\n                    kernel_size,\n                    dropout_p=dropout_p,\n                )\n            )\n\n            if layer_norm_type == \"1\":\n                self.norm_layers_2.append(LayerNorm(hidden_channels if (idx + 1) != self.num_layers else out_channels))\n            elif layer_norm_type == \"2\":\n                self.norm_layers_2.append(LayerNorm2(hidden_channels if (idx + 1) != self.num_layers else out_channels))\n            else:\n                raise ValueError(\" [!] Unknown layer norm type\")\n\n    def forward(self, x, x_mask):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n        \"\"\"\n        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        for i in range(self.num_layers):\n            x = x * x_mask\n            y = self.attn_layers[i](x, x, attn_mask)\n            y = self.dropout(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.dropout(y)\n\n            if (i + 1) == self.num_layers and hasattr(self, \"proj\"):\n                x = self.proj(x)\n\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x\n", "TTS/tts/layers/glow_tts/decoder.py": "import torch\nfrom torch import nn\n\nfrom TTS.tts.layers.generic.normalization import ActNorm\nfrom TTS.tts.layers.glow_tts.glow import CouplingBlock, InvConvNear\n\n\ndef squeeze(x, x_mask=None, num_sqz=2):\n    \"\"\"GlowTTS squeeze operation\n    Increase number of channels and reduce number of time steps\n    by the same factor.\n\n    Note:\n        each 's' is a n-dimensional vector.\n        ``[s1,s2,s3,s4,s5,s6] --> [[s1, s3, s5], [s2, s4, s6]]``\n    \"\"\"\n    b, c, t = x.size()\n\n    t = (t // num_sqz) * num_sqz\n    x = x[:, :, :t]\n    x_sqz = x.view(b, c, t // num_sqz, num_sqz)\n    x_sqz = x_sqz.permute(0, 3, 1, 2).contiguous().view(b, c * num_sqz, t // num_sqz)\n\n    if x_mask is not None:\n        x_mask = x_mask[:, :, num_sqz - 1 :: num_sqz]\n    else:\n        x_mask = torch.ones(b, 1, t // num_sqz).to(device=x.device, dtype=x.dtype)\n    return x_sqz * x_mask, x_mask\n\n\ndef unsqueeze(x, x_mask=None, num_sqz=2):\n    \"\"\"GlowTTS unsqueeze operation (revert the squeeze)\n\n    Note:\n        each 's' is a n-dimensional vector.\n        ``[[s1, s3, s5], [s2, s4, s6]] --> [[s1, s3, s5, s2, s4, s6]]``\n    \"\"\"\n    b, c, t = x.size()\n\n    x_unsqz = x.view(b, num_sqz, c // num_sqz, t)\n    x_unsqz = x_unsqz.permute(0, 2, 3, 1).contiguous().view(b, c // num_sqz, t * num_sqz)\n\n    if x_mask is not None:\n        x_mask = x_mask.unsqueeze(-1).repeat(1, 1, 1, num_sqz).view(b, 1, t * num_sqz)\n    else:\n        x_mask = torch.ones(b, 1, t * num_sqz).to(device=x.device, dtype=x.dtype)\n    return x_unsqz * x_mask, x_mask\n\n\nclass Decoder(nn.Module):\n    \"\"\"Stack of Glow Decoder Modules.\n\n    ::\n\n        Squeeze -> ActNorm -> InvertibleConv1x1 -> AffineCoupling -> Unsqueeze\n\n    Args:\n        in_channels (int): channels of input tensor.\n        hidden_channels (int): hidden decoder channels.\n        kernel_size (int): Coupling block kernel size. (Wavenet filter kernel size.)\n        dilation_rate (int): rate to increase dilation by each layer in a decoder block.\n        num_flow_blocks (int): number of decoder blocks.\n        num_coupling_layers (int): number coupling layers. (number of wavenet layers.)\n        dropout_p (float): wavenet dropout rate.\n        sigmoid_scale (bool): enable/disable sigmoid scaling in coupling layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_flow_blocks,\n        num_coupling_layers,\n        dropout_p=0.0,\n        num_splits=4,\n        num_squeeze=2,\n        sigmoid_scale=False,\n        c_in_channels=0,\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.num_flow_blocks = num_flow_blocks\n        self.num_coupling_layers = num_coupling_layers\n        self.dropout_p = dropout_p\n        self.num_splits = num_splits\n        self.num_squeeze = num_squeeze\n        self.sigmoid_scale = sigmoid_scale\n        self.c_in_channels = c_in_channels\n\n        self.flows = nn.ModuleList()\n        for _ in range(num_flow_blocks):\n            self.flows.append(ActNorm(channels=in_channels * num_squeeze))\n            self.flows.append(InvConvNear(channels=in_channels * num_squeeze, num_splits=num_splits))\n            self.flows.append(\n                CouplingBlock(\n                    in_channels * num_squeeze,\n                    hidden_channels,\n                    kernel_size=kernel_size,\n                    dilation_rate=dilation_rate,\n                    num_layers=num_coupling_layers,\n                    c_in_channels=c_in_channels,\n                    dropout_p=dropout_p,\n                    sigmoid_scale=sigmoid_scale,\n                )\n            )\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        \"\"\"\n        Shapes:\n            - x:  :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1 ,T]`\n            - g: :math:`[B, C]`\n        \"\"\"\n        if not reverse:\n            flows = self.flows\n            logdet_tot = 0\n        else:\n            flows = reversed(self.flows)\n            logdet_tot = None\n\n        if self.num_squeeze > 1:\n            x, x_mask = squeeze(x, x_mask, self.num_squeeze)\n        for f in flows:\n            if not reverse:\n                x, logdet = f(x, x_mask, g=g, reverse=reverse)\n                logdet_tot += logdet\n            else:\n                x, logdet = f(x, x_mask, g=g, reverse=reverse)\n        if self.num_squeeze > 1:\n            x, x_mask = unsqueeze(x, x_mask, self.num_squeeze)\n        return x, logdet_tot\n\n    def store_inverse(self):\n        for f in self.flows:\n            f.store_inverse()\n", "TTS/tts/layers/glow_tts/duration_predictor.py": "import torch\nfrom torch import nn\n\nfrom ..generic.normalization import LayerNorm\n\n\nclass DurationPredictor(nn.Module):\n    \"\"\"Glow-TTS duration prediction model.\n\n    ::\n\n        [2 x (conv1d_kxk -> relu -> layer_norm -> dropout)] -> conv1d_1x1 -> durs\n\n    Args:\n        in_channels (int): Number of channels of the input tensor.\n        hidden_channels (int): Number of hidden channels of the network.\n        kernel_size (int): Kernel size for the conv layers.\n        dropout_p (float): Dropout rate used after each conv layer.\n    \"\"\"\n\n    def __init__(self, in_channels, hidden_channels, kernel_size, dropout_p, cond_channels=None, language_emb_dim=None):\n        super().__init__()\n\n        # add language embedding dim in the input\n        if language_emb_dim:\n            in_channels += language_emb_dim\n\n        # class arguments\n        self.in_channels = in_channels\n        self.filter_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dropout_p = dropout_p\n        # layers\n        self.drop = nn.Dropout(dropout_p)\n        self.conv_1 = nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2)\n        self.norm_1 = LayerNorm(hidden_channels)\n        self.conv_2 = nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size // 2)\n        self.norm_2 = LayerNorm(hidden_channels)\n        # output layer\n        self.proj = nn.Conv1d(hidden_channels, 1, 1)\n        if cond_channels is not None and cond_channels != 0:\n            self.cond = nn.Conv1d(cond_channels, in_channels, 1)\n\n        if language_emb_dim != 0 and language_emb_dim is not None:\n            self.cond_lang = nn.Conv1d(language_emb_dim, in_channels, 1)\n\n    def forward(self, x, x_mask, g=None, lang_emb=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n            - g: :math:`[B, C, 1]`\n        \"\"\"\n        if g is not None:\n            x = x + self.cond(g)\n\n        if lang_emb is not None:\n            x = x + self.cond_lang(lang_emb)\n\n        x = self.conv_1(x * x_mask)\n        x = torch.relu(x)\n        x = self.norm_1(x)\n        x = self.drop(x)\n        x = self.conv_2(x * x_mask)\n        x = torch.relu(x)\n        x = self.norm_2(x)\n        x = self.drop(x)\n        x = self.proj(x * x_mask)\n        return x * x_mask\n", "TTS/tts/layers/glow_tts/__init__.py": "", "TTS/tts/layers/glow_tts/encoder.py": "import math\n\nimport torch\nfrom torch import nn\n\nfrom TTS.tts.layers.generic.gated_conv import GatedConvBlock\nfrom TTS.tts.layers.generic.res_conv_bn import ResidualConv1dBNBlock\nfrom TTS.tts.layers.generic.time_depth_sep_conv import TimeDepthSeparableConvBlock\nfrom TTS.tts.layers.glow_tts.duration_predictor import DurationPredictor\nfrom TTS.tts.layers.glow_tts.glow import ResidualConv1dLayerNormBlock\nfrom TTS.tts.layers.glow_tts.transformer import RelativePositionTransformer\nfrom TTS.tts.utils.helpers import sequence_mask\n\n\nclass Encoder(nn.Module):\n    \"\"\"Glow-TTS encoder module.\n\n    ::\n\n        embedding -> <prenet> -> encoder_module -> <postnet> --> proj_mean\n                                                             |\n                                                             |-> proj_var\n                                                             |\n                                                             |-> concat -> duration_predictor\n                                                                    \u2191\n                                                              speaker_embed\n\n    Args:\n        num_chars (int): number of characters.\n        out_channels (int): number of output channels.\n        hidden_channels (int): encoder's embedding size.\n        hidden_channels_ffn (int): transformer's feed-forward channels.\n        kernel_size (int): kernel size for conv layers and duration predictor.\n        dropout_p (float): dropout rate for any dropout layer.\n        mean_only (bool): if True, output only mean values and use constant std.\n        use_prenet (bool): if True, use pre-convolutional layers before transformer layers.\n        c_in_channels (int): number of channels in conditional input.\n\n    Shapes:\n        - input: (B, T, C)\n\n    ::\n\n        suggested encoder params...\n\n        for encoder_type == 'rel_pos_transformer'\n            encoder_params={\n                'kernel_size':3,\n                'dropout_p': 0.1,\n                'num_layers': 6,\n                'num_heads': 2,\n                'hidden_channels_ffn': 768,  # 4 times the hidden_channels\n                'input_length': None\n            }\n\n        for encoder_type == 'gated_conv'\n            encoder_params={\n                'kernel_size':5,\n                'dropout_p': 0.1,\n                'num_layers': 9,\n            }\n\n        for encoder_type == 'residual_conv_bn'\n            encoder_params={\n                \"kernel_size\": 4,\n                \"dilations\": [1, 2, 4, 1, 2, 4, 1, 2, 4, 1, 2, 4, 1],\n                \"num_conv_blocks\": 2,\n                \"num_res_blocks\": 13\n            }\n\n         for encoder_type == 'time_depth_separable'\n            encoder_params={\n                \"kernel_size\": 5,\n                'num_layers': 9,\n            }\n    \"\"\"\n\n    def __init__(\n        self,\n        num_chars,\n        out_channels,\n        hidden_channels,\n        hidden_channels_dp,\n        encoder_type,\n        encoder_params,\n        dropout_p_dp=0.1,\n        mean_only=False,\n        use_prenet=True,\n        c_in_channels=0,\n    ):\n        super().__init__()\n        # class arguments\n        self.num_chars = num_chars\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.hidden_channels_dp = hidden_channels_dp\n        self.dropout_p_dp = dropout_p_dp\n        self.mean_only = mean_only\n        self.use_prenet = use_prenet\n        self.c_in_channels = c_in_channels\n        self.encoder_type = encoder_type\n        # embedding layer\n        self.emb = nn.Embedding(num_chars, hidden_channels)\n        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n        # init encoder module\n        if encoder_type.lower() == \"rel_pos_transformer\":\n            if use_prenet:\n                self.prenet = ResidualConv1dLayerNormBlock(\n                    hidden_channels, hidden_channels, hidden_channels, kernel_size=5, num_layers=3, dropout_p=0.5\n                )\n            self.encoder = RelativePositionTransformer(\n                hidden_channels, hidden_channels, hidden_channels, **encoder_params\n            )\n        elif encoder_type.lower() == \"gated_conv\":\n            self.encoder = GatedConvBlock(hidden_channels, **encoder_params)\n        elif encoder_type.lower() == \"residual_conv_bn\":\n            if use_prenet:\n                self.prenet = nn.Sequential(nn.Conv1d(hidden_channels, hidden_channels, 1), nn.ReLU())\n            self.encoder = ResidualConv1dBNBlock(hidden_channels, hidden_channels, hidden_channels, **encoder_params)\n            self.postnet = nn.Sequential(\n                nn.Conv1d(self.hidden_channels, self.hidden_channels, 1), nn.BatchNorm1d(self.hidden_channels)\n            )\n        elif encoder_type.lower() == \"time_depth_separable\":\n            if use_prenet:\n                self.prenet = ResidualConv1dLayerNormBlock(\n                    hidden_channels, hidden_channels, hidden_channels, kernel_size=5, num_layers=3, dropout_p=0.5\n                )\n            self.encoder = TimeDepthSeparableConvBlock(\n                hidden_channels, hidden_channels, hidden_channels, **encoder_params\n            )\n        else:\n            raise ValueError(\" [!] Unkown encoder type.\")\n\n        # final projection layers\n        self.proj_m = nn.Conv1d(hidden_channels, out_channels, 1)\n        if not mean_only:\n            self.proj_s = nn.Conv1d(hidden_channels, out_channels, 1)\n        # duration predictor\n        self.duration_predictor = DurationPredictor(\n            hidden_channels + c_in_channels, hidden_channels_dp, 3, dropout_p_dp\n        )\n\n    def forward(self, x, x_lengths, g=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_lengths: :math:`[B]`\n            - g (optional): :math:`[B, 1, T]`\n        \"\"\"\n        # embedding layer\n        # [B ,T, D]\n        x = self.emb(x) * math.sqrt(self.hidden_channels)\n        # [B, D, T]\n        x = torch.transpose(x, 1, -1)\n        # compute input sequence mask\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        # prenet\n        if hasattr(self, \"prenet\") and self.use_prenet:\n            x = self.prenet(x, x_mask)\n        # encoder\n        x = self.encoder(x, x_mask)\n        # postnet\n        if hasattr(self, \"postnet\"):\n            x = self.postnet(x) * x_mask\n        # set duration predictor input\n        if g is not None:\n            g_exp = g.expand(-1, -1, x.size(-1))\n            x_dp = torch.cat([x.detach(), g_exp], 1)\n        else:\n            x_dp = x.detach()\n        # final projection layer\n        x_m = self.proj_m(x) * x_mask\n        if not self.mean_only:\n            x_logs = self.proj_s(x) * x_mask\n        else:\n            x_logs = torch.zeros_like(x_m)\n        # duration predictor\n        logw = self.duration_predictor(x_dp, x_mask)\n        return x_m, x_logs, logw, x_mask\n", "TTS/tts/layers/xtts/perceiver_encoder.py": "# Adapted from https://github.com/lucidrains/naturalspeech2-pytorch/blob/659bec7f7543e7747e809e950cc2f84242fbeec7/naturalspeech2_pytorch/naturalspeech2_pytorch.py#L532\n\nfrom collections import namedtuple\nfrom functools import wraps\n\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom packaging import version\nfrom torch import einsum, nn\n\n\ndef exists(val):\n    return val is not None\n\n\ndef once(fn):\n    called = False\n\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n\n    return inner\n\n\nprint_once = once(print)\n\n# main class\n\n\nclass Attend(nn.Module):\n    def __init__(self, dropout=0.0, causal=False, use_flash=False):\n        super().__init__()\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n\n        self.causal = causal\n        self.register_buffer(\"mask\", None, persistent=False)\n\n        self.use_flash = use_flash\n        assert not (\n            use_flash and version.parse(torch.__version__) < version.parse(\"2.0.0\")\n        ), \"in order to use flash attention, you must be using pytorch 2.0 or above\"\n\n        # determine efficient attention configs for cuda and cpu\n        self.config = namedtuple(\"EfficientAttentionConfig\", [\"enable_flash\", \"enable_math\", \"enable_mem_efficient\"])\n        self.cpu_config = self.config(True, True, True)\n        self.cuda_config = None\n\n        if not torch.cuda.is_available() or not use_flash:\n            return\n\n        device_properties = torch.cuda.get_device_properties(torch.device(\"cuda\"))\n\n        if device_properties.major == 8 and device_properties.minor == 0:\n            print_once(\"A100 GPU detected, using flash attention if input tensor is on cuda\")\n            self.cuda_config = self.config(True, False, False)\n        else:\n            print_once(\"Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\")\n            self.cuda_config = self.config(False, True, True)\n\n    def get_mask(self, n, device):\n        if exists(self.mask) and self.mask.shape[-1] >= n:\n            return self.mask[:n, :n]\n\n        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)\n        self.register_buffer(\"mask\", mask, persistent=False)\n        return mask\n\n    def flash_attn(self, q, k, v, mask=None):\n        _, heads, q_len, _, k_len, is_cuda = *q.shape, k.shape[-2], q.is_cuda\n\n        # Recommended for multi-query single-key-value attention by Tri Dao\n        # kv shape torch.Size([1, 512, 64]) -> torch.Size([1, 8, 512, 64])\n\n        if k.ndim == 3:\n            k = rearrange(k, \"b ... -> b 1 ...\").expand_as(q)\n\n        if v.ndim == 3:\n            v = rearrange(v, \"b ... -> b 1 ...\").expand_as(q)\n\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b 1 1 j\")\n            mask = mask.expand(-1, heads, q_len, -1)\n\n        # Check if there is a compatible device for flash attention\n\n        config = self.cuda_config if is_cuda else self.cpu_config\n\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n\n        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n            out = F.scaled_dot_product_attention(\n                q, k, v, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0, is_causal=self.causal\n            )\n\n        return out\n\n    def forward(self, q, k, v, mask=None):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, device = q.shape[-2], q.device\n\n        scale = q.shape[-1] ** -0.5\n\n        if self.use_flash:\n            return self.flash_attn(q, k, v, mask=mask)\n\n        kv_einsum_eq = \"b j d\" if k.ndim == 3 else \"b h j d\"\n\n        # similarity\n\n        sim = einsum(f\"b h i d, {kv_einsum_eq} -> b h i j\", q, k) * scale\n\n        # key padding mask\n\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b 1 1 j\")\n            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n\n        # causal mask\n\n        if self.causal:\n            causal_mask = self.get_mask(n, device)\n            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        attn = sim.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        # aggregate values\n\n        out = einsum(f\"b h i j, {kv_einsum_eq} -> b h i d\", attn, v)\n\n        return out\n\n\ndef Sequential(*mods):\n    return nn.Sequential(*filter(exists, mods))\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, scale=True, dim_cond=None):\n        super().__init__()\n        self.cond = exists(dim_cond)\n        self.to_gamma_beta = nn.Linear(dim_cond, dim * 2) if self.cond else None\n\n        self.scale = dim**0.5\n        self.gamma = nn.Parameter(torch.ones(dim)) if scale else None\n\n    def forward(self, x, cond=None):\n        gamma = default(self.gamma, 1)\n        out = F.normalize(x, dim=-1) * self.scale * gamma\n\n        if not self.cond:\n            return out\n\n        assert exists(cond)\n        gamma, beta = self.to_gamma_beta(cond).chunk(2, dim=-1)\n        gamma, beta = map(lambda t: rearrange(t, \"b d -> b 1 d\"), (gamma, beta))\n        return out * gamma + beta\n\n\nclass CausalConv1d(nn.Conv1d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        (kernel_size,) = self.kernel_size\n        (dilation,) = self.dilation\n        (stride,) = self.stride\n\n        assert stride == 1\n        self.causal_padding = dilation * (kernel_size - 1)\n\n    def forward(self, x):\n        causal_padded_x = F.pad(x, (self.causal_padding, 0), value=0.0)\n        return super().forward(causal_padded_x)\n\n\nclass GEGLU(nn.Module):\n    def forward(self, x):\n        x, gate = x.chunk(2, dim=-1)\n        return F.gelu(gate) * x\n\n\ndef FeedForward(dim, mult=4, causal_conv=False):\n    dim_inner = int(dim * mult * 2 / 3)\n\n    conv = None\n    if causal_conv:\n        conv = nn.Sequential(\n            Rearrange(\"b n d -> b d n\"),\n            CausalConv1d(dim_inner, dim_inner, 3),\n            Rearrange(\"b d n -> b n d\"),\n        )\n\n    return Sequential(nn.Linear(dim, dim_inner * 2), GEGLU(), conv, nn.Linear(dim_inner, dim))\n\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth=2,\n        dim_context=None,\n        num_latents=32,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n        use_flash_attn=False,\n    ):\n        super().__init__()\n        dim_context = default(dim_context, dim)\n\n        self.proj_context = nn.Linear(dim_context, dim) if dim_context != dim else nn.Identity()\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n        nn.init.normal_(self.latents, std=0.02)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        Attention(\n                            dim=dim,\n                            dim_head=dim_head,\n                            heads=heads,\n                            use_flash=use_flash_attn,\n                            cross_attn_include_queries=True,\n                        ),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n        self.norm = RMSNorm(dim)\n\n    def forward(self, x, mask=None):\n        batch = x.shape[0]\n\n        x = self.proj_context(x)\n\n        latents = repeat(self.latents, \"n d -> b n d\", b=batch)\n\n        for attn, ff in self.layers:\n            latents = attn(latents, x, mask=mask) + latents\n            latents = ff(latents) + latents\n\n        return self.norm(latents)\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_context=None,\n        causal=False,\n        dim_head=64,\n        heads=8,\n        dropout=0.0,\n        use_flash=False,\n        cross_attn_include_queries=False,\n    ):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.cross_attn_include_queries = cross_attn_include_queries\n\n        dim_inner = dim_head * heads\n        dim_context = default(dim_context, dim)\n\n        self.attend = Attend(causal=causal, dropout=dropout, use_flash=use_flash)\n        self.to_q = nn.Linear(dim, dim_inner, bias=False)\n        self.to_kv = nn.Linear(dim_context, dim_inner * 2, bias=False)\n        self.to_out = nn.Linear(dim_inner, dim, bias=False)\n\n    def forward(self, x, context=None, mask=None):\n        h, has_context = self.heads, exists(context)\n\n        context = default(context, x)\n\n        if has_context and self.cross_attn_include_queries:\n            context = torch.cat((x, context), dim=-2)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim=-1))\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n\n        out = self.attend(q, k, v, mask=mask)\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        return self.to_out(out)\n", "TTS/tts/layers/xtts/xtts_manager.py": "import torch\n\nclass SpeakerManager():\n    def __init__(self, speaker_file_path=None):\n        self.speakers = torch.load(speaker_file_path)\n\n    @property\n    def name_to_id(self):\n        return self.speakers.keys()\n    \n    @property\n    def num_speakers(self):\n        return len(self.name_to_id)\n    \n    @property\n    def speaker_names(self):\n        return list(self.name_to_id.keys())\n    \n\nclass LanguageManager():\n    def __init__(self, config):\n        self.langs = config[\"languages\"]\n\n    @property\n    def name_to_id(self):\n        return self.langs\n    \n    @property\n    def num_languages(self):\n        return len(self.name_to_id)\n    \n    @property\n    def language_names(self):\n        return list(self.name_to_id)\n", "TTS/tts/layers/xtts/zh_num2words.py": "# Authors:\n#   2019.5 Zhiyang Zhou (https://github.com/Joee1995/chn_text_norm.git)\n#   2019.9 - 2022 Jiayu DU\n\nimport argparse\nimport csv\nimport os\nimport re\nimport string\nimport sys\n\n# fmt: off\n\n# ================================================================================ #\n#                                    basic constant\n# ================================================================================ #\nCHINESE_DIGIS = \"\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\"\nBIG_CHINESE_DIGIS_SIMPLIFIED = \"\u96f6\u58f9\u8d30\u53c1\u8086\u4f0d\u9646\u67d2\u634c\u7396\"\nBIG_CHINESE_DIGIS_TRADITIONAL = \"\u96f6\u58f9\u8cb3\u53c3\u8086\u4f0d\u9678\u67d2\u634c\u7396\"\nSMALLER_BIG_CHINESE_UNITS_SIMPLIFIED = \"\u5341\u767e\u5343\u4e07\"\nSMALLER_BIG_CHINESE_UNITS_TRADITIONAL = \"\u62fe\u4f70\u4edf\u842c\"\nLARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED = \"\u4ebf\u5146\u4eac\u5793\u79ed\u7a70\u6c9f\u6da7\u6b63\u8f7d\"\nLARGER_CHINESE_NUMERING_UNITS_TRADITIONAL = \"\u5104\u5146\u4eac\u5793\u79ed\u7a70\u6e9d\u6f97\u6b63\u8f09\"\nSMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED = \"\u5341\u767e\u5343\u4e07\"\nSMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL = \"\u62fe\u4f70\u4edf\u842c\"\n\nZERO_ALT = \"\u3007\"\nONE_ALT = \"\u5e7a\"\nTWO_ALTS = [\"\u4e24\", \"\u5169\"]\n\nPOSITIVE = [\"\u6b63\", \"\u6b63\"]\nNEGATIVE = [\"\u8d1f\", \"\u8ca0\"]\nPOINT = [\"\u70b9\", \"\u9ede\"]\n# PLUS = [u'\u52a0', u'\u52a0']\n# SIL = [u'\u6760', u'\u69d3']\n\nFILLER_CHARS = [\"\u5443\", \"\u554a\"]\n\nER_WHITELIST = (\n    \"(\u513f\u5973|\u513f\u5b50|\u513f\u5b59|\u5973\u513f|\u513f\u5ab3|\u59bb\u513f|\"\n    \"\u80ce\u513f|\u5a74\u513f|\u65b0\u751f\u513f|\u5a74\u5e7c\u513f|\u5e7c\u513f|\u5c11\u513f|\u5c0f\u513f|\u513f\u6b4c|\u513f\u7ae5|\u513f\u79d1|\u6258\u513f\u6240|\u5b64\u513f|\"\n    \"\u513f\u620f|\u513f\u5316|\u53f0\u513f\u5e84|\u9e7f\u513f\u5c9b|\u6b63\u513f\u516b\u7ecf|\u540a\u513f\u90ce\u5f53|\u751f\u513f\u80b2\u5973|\u6258\u513f\u5e26\u5973|\u517b\u513f\u9632\u8001|\u75f4\u513f\u5446\u5973|\"\n    \"\u4f73\u513f\u4f73\u5987|\u513f\u601c\u517d\u6270|\u513f\u65e0\u5e38\u7236|\u513f\u4e0d\u5acc\u6bcd\u4e11|\u513f\u884c\u5343\u91cc\u6bcd\u62c5\u5fe7|\u513f\u5927\u4e0d\u7531\u7237|\u82cf\u4e5e\u513f)\"\n)\nER_WHITELIST_PATTERN = re.compile(ER_WHITELIST)\n\n# \u4e2d\u6587\u6570\u5b57\u7cfb\u7edf\u7c7b\u578b\nNUMBERING_TYPES = [\"low\", \"mid\", \"high\"]\n\nCURRENCY_NAMES = \"(\u4eba\u6c11\u5e01|\u7f8e\u5143|\u65e5\u5143|\u82f1\u9551|\u6b27\u5143|\u9a6c\u514b|\u6cd5\u90ce|\u52a0\u62ff\u5927\u5143|\u6fb3\u5143|\u6e2f\u5e01|\u5148\u4ee4|\u82ac\u5170\u9a6c\u514b|\u7231\u5c14\u5170\u9551|\" \"\u91cc\u62c9|\u8377\u5170\u76fe|\u57c3\u65af\u5e93\u591a|\u6bd4\u585e\u5854|\u5370\u5c3c\u76fe|\u6797\u5409\u7279|\u65b0\u897f\u5170\u5143|\u6bd4\u7d22|\u5362\u5e03|\u65b0\u52a0\u5761\u5143|\u97e9\u5143|\u6cf0\u94e2)\"\nCURRENCY_UNITS = \"((\u4ebf|\u5343\u4e07|\u767e\u4e07|\u4e07|\u5343|\u767e)|(\u4ebf|\u5343\u4e07|\u767e\u4e07|\u4e07|\u5343|\u767e|)\u5143|(\u4ebf|\u5343\u4e07|\u767e\u4e07|\u4e07|\u5343|\u767e|)\u5757|\u89d2|\u6bdb|\u5206)\"\nCOM_QUANTIFIERS = (\n    \"(\u5339|\u5f20|\u5ea7|\u56de|\u573a|\u5c3e|\u6761|\u4e2a|\u9996|\u9619|\u9635|\u7f51|\u70ae|\u9876|\u4e18|\u68f5|\u53ea|\u652f|\u88ad|\u8f86|\u6311|\u62c5|\u9897|\u58f3|\u7aa0|\u66f2|\u5899|\u7fa4|\u8154|\"\n    \"\u7823|\u5ea7|\u5ba2|\u8d2f|\u624e|\u6346|\u5200|\u4ee4|\u6253|\u624b|\u7f57|\u5761|\u5c71|\u5cad|\u6c5f|\u6eaa|\u949f|\u961f|\u5355|\u53cc|\u5bf9|\u51fa|\u53e3|\u5934|\u811a|\u677f|\u8df3|\u679d|\u4ef6|\u8d34|\"\n    \"\u9488|\u7ebf|\u7ba1|\u540d|\u4f4d|\u8eab|\u5802|\u8bfe|\u672c|\u9875|\u5bb6|\u6237|\u5c42|\u4e1d|\u6beb|\u5398|\u5206|\u94b1|\u4e24|\u65a4|\u62c5|\u94e2|\u77f3|\u94a7|\u9531|\u5ffd|(\u5343|\u6beb|\u5fae)\u514b|\"\n    \"\u6beb|\u5398|\u5206|\u5bf8|\u5c3a|\u4e08|\u91cc|\u5bfb|\u5e38|\u94fa|\u7a0b|(\u5343|\u5206|\u5398|\u6beb|\u5fae)\u7c73|\u64ae|\u52fa|\u5408|\u5347|\u6597|\u77f3|\u76d8|\u7897|\u789f|\u53e0|\u6876|\u7b3c|\u76c6|\"\n    \"\u76d2|\u676f|\u949f|\u659b|\u9505|\u7c0b|\u7bee|\u76d8|\u6876|\u7f50|\u74f6|\u58f6|\u536e|\u76cf|\u7ba9|\u7bb1|\u7172|\u5556|\u888b|\u94b5|\u5e74|\u6708|\u65e5|\u5b63|\u523b|\u65f6|\u5468|\u5929|\u79d2|\u5206|\u65ec|\"\n    \"\u7eaa|\u5c81|\u4e16|\u66f4|\u591c|\u6625|\u590f|\u79cb|\u51ac|\u4ee3|\u4f0f|\u8f88|\u4e38|\u6ce1|\u7c92|\u9897|\u5e62|\u5806|\u6761|\u6839|\u652f|\u9053|\u9762|\u7247|\u5f20|\u9897|\u5757)\"\n)\n\n\n# Punctuation information are based on Zhon project (https://github.com/tsroten/zhon.git)\nCN_PUNCS_STOP = \"\uff01\uff1f\uff61\u3002\"\nCN_PUNCS_NONSTOP = \"\uff02\uff03\uff04\uff05\uff06\uff07\uff08\uff09\uff0a\uff0b\uff0c\uff0d\uff0f\uff1a\uff1b\uff1c\uff1d\uff1e\uff20\uff3b\uff3c\uff3d\uff3e\uff3f\uff40\uff5b\uff5c\uff5d\uff5e\uff5f\uff60\uff62\uff63\uff64\u3001\u3003\u300a\u300b\u300c\u300d\u300e\u300f\u3010\u3011\u3014\u3015\u3016\u3017\u3018\u3019\u301a\u301b\u301c\u301d\u301e\u301f\u3030\u303e\u303f\u2013\u2014\u2018\u2019\u201b\u201c\u201d\u201e\u201f\u2026\u2027\ufe4f\u00b7\u3008\u3009-\"\nCN_PUNCS = CN_PUNCS_STOP + CN_PUNCS_NONSTOP\n\nPUNCS = CN_PUNCS + string.punctuation\nPUNCS_TRANSFORM = str.maketrans(PUNCS, \",\" * len(PUNCS), \"\")  # replace puncs with English comma\n\n\n# https://zh.wikipedia.org/wiki/\u5168\u884c\u548c\u534a\u884c\nQJ2BJ = {\n    \"\u3000\": \" \",\n    \"\uff01\": \"!\",\n    \"\uff02\": '\"',\n    \"\uff03\": \"#\",\n    \"\uff04\": \"$\",\n    \"\uff05\": \"%\",\n    \"\uff06\": \"&\",\n    \"\uff07\": \"'\",\n    \"\uff08\": \"(\",\n    \"\uff09\": \")\",\n    \"\uff0a\": \"*\",\n    \"\uff0b\": \"+\",\n    \"\uff0c\": \",\",\n    \"\uff0d\": \"-\",\n    \"\uff0e\": \".\",\n    \"\uff0f\": \"/\",\n    \"\uff10\": \"0\",\n    \"\uff11\": \"1\",\n    \"\uff12\": \"2\",\n    \"\uff13\": \"3\",\n    \"\uff14\": \"4\",\n    \"\uff15\": \"5\",\n    \"\uff16\": \"6\",\n    \"\uff17\": \"7\",\n    \"\uff18\": \"8\",\n    \"\uff19\": \"9\",\n    \"\uff1a\": \":\",\n    \"\uff1b\": \";\",\n    \"\uff1c\": \"<\",\n    \"\uff1d\": \"=\",\n    \"\uff1e\": \">\",\n    \"\uff1f\": \"?\",\n    \"\uff20\": \"@\",\n    \"\uff21\": \"A\",\n    \"\uff22\": \"B\",\n    \"\uff23\": \"C\",\n    \"\uff24\": \"D\",\n    \"\uff25\": \"E\",\n    \"\uff26\": \"F\",\n    \"\uff27\": \"G\",\n    \"\uff28\": \"H\",\n    \"\uff29\": \"I\",\n    \"\uff2a\": \"J\",\n    \"\uff2b\": \"K\",\n    \"\uff2c\": \"L\",\n    \"\uff2d\": \"M\",\n    \"\uff2e\": \"N\",\n    \"\uff2f\": \"O\",\n    \"\uff30\": \"P\",\n    \"\uff31\": \"Q\",\n    \"\uff32\": \"R\",\n    \"\uff33\": \"S\",\n    \"\uff34\": \"T\",\n    \"\uff35\": \"U\",\n    \"\uff36\": \"V\",\n    \"\uff37\": \"W\",\n    \"\uff38\": \"X\",\n    \"\uff39\": \"Y\",\n    \"\uff3a\": \"Z\",\n    \"\uff3b\": \"[\",\n    \"\uff3c\": \"\\\\\",\n    \"\uff3d\": \"]\",\n    \"\uff3e\": \"^\",\n    \"\uff3f\": \"_\",\n    \"\uff40\": \"`\",\n    \"\uff41\": \"a\",\n    \"\uff42\": \"b\",\n    \"\uff43\": \"c\",\n    \"\uff44\": \"d\",\n    \"\uff45\": \"e\",\n    \"\uff46\": \"f\",\n    \"\uff47\": \"g\",\n    \"\uff48\": \"h\",\n    \"\uff49\": \"i\",\n    \"\uff4a\": \"j\",\n    \"\uff4b\": \"k\",\n    \"\uff4c\": \"l\",\n    \"\uff4d\": \"m\",\n    \"\uff4e\": \"n\",\n    \"\uff4f\": \"o\",\n    \"\uff50\": \"p\",\n    \"\uff51\": \"q\",\n    \"\uff52\": \"r\",\n    \"\uff53\": \"s\",\n    \"\uff54\": \"t\",\n    \"\uff55\": \"u\",\n    \"\uff56\": \"v\",\n    \"\uff57\": \"w\",\n    \"\uff58\": \"x\",\n    \"\uff59\": \"y\",\n    \"\uff5a\": \"z\",\n    \"\uff5b\": \"{\",\n    \"\uff5c\": \"|\",\n    \"\uff5d\": \"}\",\n    \"\uff5e\": \"~\",\n}\nQJ2BJ_TRANSFORM = str.maketrans(\"\".join(QJ2BJ.keys()), \"\".join(QJ2BJ.values()), \"\")\n\n\n# 2013 China National Standard: https://zh.wikipedia.org/wiki/\u901a\u7528\u89c4\u8303\u6c49\u5b57\u8868, raw resources:\n#   https://github.com/mozillazg/pinyin-data/blob/master/kMandarin_8105.txt with 8105 chinese chars in total\nCN_CHARS_COMMON = (\n    \"\u4e00\u4e01\u4e03\u4e07\u4e08\u4e09\u4e0a\u4e0b\u4e0d\u4e0e\u4e0f\u4e10\u4e11\u4e13\u4e14\u4e15\u4e16\u4e18\u4e19\u4e1a\u4e1b\u4e1c\u4e1d\u4e1e\u4e22\u4e24\u4e25\u4e27\u4e2a\u4e2b\u4e2d\u4e30\u4e32\u4e34\u4e38\u4e39\u4e3a\u4e3b\u4e3d\u4e3e\"\n    \"\u4e42\u4e43\u4e45\u4e48\u4e49\u4e4b\u4e4c\u4e4d\u4e4e\u4e4f\u4e50\u4e52\u4e53\u4e54\u4e56\u4e58\u4e59\u4e5c\u4e5d\u4e5e\u4e5f\u4e60\u4e61\u4e66\u4e69\u4e70\u4e71\u4e73\u4e78\u4e7e\u4e86\u4e88\u4e89\u4e8b\u4e8c\u4e8d\u4e8e\u4e8f\u4e91\u4e92\"\n    \"\u4e93\u4e94\u4e95\u4e98\u4e9a\u4e9b\u4e9f\u4ea1\u4ea2\u4ea4\u4ea5\u4ea6\u4ea7\u4ea8\u4ea9\u4eab\u4eac\u4ead\u4eae\u4eb2\u4eb3\u4eb5\u4eb6\u4eb8\u4eb9\u4eba\u4ebf\u4ec0\u4ec1\u4ec2\u4ec3\u4ec4\u4ec5\u4ec6\u4ec7\u4ec9\u4eca\u4ecb\u4ecd\u4ece\"\n    \"\u4ed1\u4ed3\u4ed4\u4ed5\u4ed6\u4ed7\u4ed8\u4ed9\u4edd\u4ede\u4edf\u4ee1\u4ee3\u4ee4\u4ee5\u4ee8\u4eea\u4eeb\u4eec\u4ef0\u4ef2\u4ef3\u4ef5\u4ef6\u4ef7\u4efb\u4efd\u4eff\u4f01\u4f08\u4f09\u4f0a\u4f0b\u4f0d\u4f0e\u4f0f\u4f10\u4f11\u4f17\u4f18\"\n    \"\u4f19\u4f1a\u4f1b\u4f1e\u4f1f\u4f20\u4f22\u4f23\u4f24\u4f25\u4f26\u4f27\u4f2a\u4f2b\u4f2d\u4f2f\u4f30\u4f32\u4f34\u4f36\u4f38\u4f3a\u4f3c\u4f3d\u4f3e\u4f41\u4f43\u4f46\u4f4d\u4f4e\u4f4f\u4f50\u4f51\u4f53\u4f55\u4f56\u4f57\u4f58\u4f59\u4f5a\"\n    \"\u4f5b\u4f5c\u4f5d\u4f5e\u4f5f\u4f60\u4f63\u4f64\u4f65\u4f69\u4f6c\u4f6f\u4f70\u4f73\u4f74\u4f76\u4f78\u4f7a\u4f7b\u4f7c\u4f7d\u4f7e\u4f7f\u4f81\u4f82\u4f83\u4f84\u4f88\u4f89\u4f8b\u4f8d\u4f8f\u4f91\u4f94\u4f97\u4f98\u4f9b\u4f9d\u4fa0\u4fa3\"\n    \"\u4fa5\u4fa6\u4fa7\u4fa8\u4fa9\u4faa\u4fac\u4fae\u4faf\u4fb4\u4fb5\u4fb9\u4fbf\u4fc3\u4fc4\u4fc5\u4fca\u4fcd\u4fce\u4fcf\u4fd0\u4fd1\u4fd7\u4fd8\u4fd9\u4fda\u4fdc\u4fdd\u4fde\u4fdf\u4fe1\u4fe3\u4fe6\u4fe8\u4fe9\u4fea\u4feb\u4fed\u4fee\u4fef\"\n    \"\u4ff1\u4ff3\u4ff5\u4ff6\u4ff8\u4ffa\u4ffe\u500c\u500d\u500f\u5012\u5013\u5014\u5015\u5018\u5019\u501a\u501c\u501e\u501f\u5021\u5025\u5026\u5027\u5028\u5029\u502a\u502c\u502d\u502e\u5034\u503a\u503b\u503c\u503e\u5041\u5043\u5047\u5048\u504c\"\n    \"\u504e\u504f\u5053\u5055\u505a\u505c\u5061\u5065\u506c\u506d\u5070\u5072\u5076\u5077\u507b\u507e\u507f\u5080\u5083\u5085\u5088\u5089\u508d\u5092\u5095\u50a3\u50a5\u50a7\u50a8\u50a9\u50ac\u50b2\u50ba\u50bb\u50c7\u50ce\u50cf\u50d4\u50d6\u50da\"\n    \"\u50e6\u50e7\u50ec\u50ed\u50ee\u50f0\u50f3\u50f5\u50fb\u5106\u5107\u510b\u5112\u5121\u5126\u5133\u5134\u513f\u5140\u5141\u5143\u5144\u5145\u5146\u5148\u5149\u514b\u514d\u5151\u5154\u5155\u5156\u515a\u515c\u5162\u5165\u5168\u516b\u516c\u516d\"\n    \"\u516e\u5170\u5171\u5173\u5174\u5175\u5176\u5177\u5178\u5179\u517b\u517c\u517d\u5180\u5181\u5185\u5188\u5189\u518c\u518d\u518f\u5192\u5194\u5195\u5197\u5199\u519b\u519c\u51a0\u51a2\u51a4\u51a5\u51ac\u51ae\u51af\u51b0\u51b1\u51b2\u51b3\u51b5\"\n    \"\u51b6\u51b7\u51bb\u51bc\u51bd\u51c0\u51c4\u51c6\u51c7\u51c9\u51cb\u51cc\u51cf\u51d1\u51d3\u51d8\u51db\u51dd\u51e0\u51e1\u51e4\u51eb\u51ed\u51ef\u51f0\u51f3\u51f6\u51f8\u51f9\u51fa\u51fb\u51fc\u51fd\u51ff\u5200\u5201\u5203\u5206\u5207\u5208\"\n    \"\u520a\u520d\u520e\u5211\u5212\u5216\u5217\u5218\u5219\u521a\u521b\u521d\u5220\u5224\u5228\u5229\u522b\u522c\u522d\u522e\u5230\u5233\u5236\u5237\u5238\u5239\u523a\u523b\u523d\u523f\u5240\u5241\u5242\u5243\u5245\u524a\u524b\u524c\u524d\u5250\"\n    \"\u5251\u5254\u5255\u5256\u525c\u525e\u525f\u5261\u5265\u5267\u5269\u526a\u526f\u5272\u527d\u527f\u5281\u5282\u5284\u5288\u5290\u5293\u529b\u529d\u529e\u529f\u52a0\u52a1\u52a2\u52a3\u52a8\u52a9\u52aa\u52ab\u52ac\u52ad\u52b1\u52b2\u52b3\u52bc\"\n    \"\u52be\u52bf\u52c3\u52c7\u52c9\u52cb\u52cd\u52d0\u52d2\u52d4\u52d6\u52d8\u52da\u52df\u52e0\u52e4\u52f0\u52fa\u52fe\u52ff\u5300\u5305\u5306\u5308\u530d\u530f\u5310\u5315\u5316\u5317\u5319\u531c\u531d\u5320\u5321\u5323\u5326\u532a\u532e\u5339\"\n    \"\u533a\u533b\u533c\u533e\u533f\u5341\u5343\u5345\u5347\u5348\u5349\u534a\u534e\u534f\u5351\u5352\u5353\u5355\u5356\u5357\u535a\u535c\u535e\u535f\u5360\u5361\u5362\u5363\u5364\u5366\u5367\u536b\u536c\u536e\u536f\u5370\u5371\u5373\u5374\u5375\"\n    \"\u5377\u5378\u537a\u537f\u5382\u5384\u5385\u5386\u5389\u538b\u538c\u538d\u5395\u5396\u5398\u539a\u539d\u539f\u53a2\u53a3\u53a5\u53a6\u53a8\u53a9\u53ae\u53bb\u53be\u53bf\u53c1\u53c2\u53c6\u53c7\u53c8\u53c9\u53ca\u53cb\u53cc\u53cd\u53d1\u53d4\"\n    \"\u53d5\u53d6\u53d7\u53d8\u53d9\u53da\u53db\u53df\u53e0\u53e3\u53e4\u53e5\u53e6\u53e8\u53e9\u53ea\u53eb\u53ec\u53ed\u53ee\u53ef\u53f0\u53f1\u53f2\u53f3\u53f5\u53f6\u53f7\u53f8\u53f9\u53fb\u53fc\u53fd\u5401\u5403\u5404\u5406\u5408\u5409\u540a\"\n    \"\u540c\u540d\u540e\u540f\u5410\u5411\u5412\u5413\u5415\u5416\u5417\u541b\u541d\u541e\u541f\u5420\u5421\u5423\u5426\u5427\u5428\u5429\u542b\u542c\u542d\u542e\u542f\u5431\u5432\u5434\u5435\u5438\u5439\u543b\u543c\u543d\u543e\u5440\u5443\u5446\"\n    \"\u5447\u5448\u544a\u544b\u5450\u5452\u5453\u5454\u5455\u5456\u5457\u5458\u5459\u545b\u545c\u5462\u5463\u5464\u5466\u5468\u5471\u5472\u5473\u5475\u5476\u5477\u5478\u547b\u547c\u547d\u5480\u5482\u5484\u5486\u5487\u5489\u548b\u548c\u548d\u548e\"\n    \"\u548f\u5490\u5492\u5494\u5495\u5496\u5499\u549a\u549b\u549d\u54a1\u54a3\u54a4\u54a5\u54a6\u54a7\u54a8\u54a9\u54aa\u54ab\u54ac\u54af\u54b1\u54b3\u54b4\u54b8\u54ba\u54bb\u54bd\u54bf\u54c0\u54c1\u54c2\u54c3\u54c4\u54c6\u54c7\u54c8\u54c9\u54cc\"\n    \"\u54cd\u54ce\u54cf\u54d0\u54d1\u54d2\u54d3\u54d4\u54d5\u54d7\u54d9\u54da\u54dd\u54de\u54df\u54e2\u54e5\u54e6\u54e7\u54e8\u54e9\u54ea\u54ed\u54ee\u54f1\u54f2\u54f3\u54fa\u54fc\u54fd\u54ff\u5501\u5506\u5507\u5509\u550f\u5510\u5511\u5514\u551b\"\n    \"\u551d\u5520\u5522\u5523\u5524\u5527\u552a\u552c\u552e\u552f\u5530\u5531\u5533\u5535\u5537\u553c\u553e\u553f\u5541\u5543\u5544\u5546\u5549\u554a\u5550\u5555\u5556\u555c\u5561\u5564\u5565\u5566\u5567\u556a\u556b\u556c\u556d\u556e\u5570\u5574\"\n    \"\u5575\u5576\u5577\u5578\u557b\u557c\u557e\u5580\u5581\u5582\u5583\u5584\u5586\u5587\u5588\u5589\u558a\u558b\u558f\u5591\u5594\u5598\u5599\u559c\u559d\u559f\u55a4\u55a7\u55b1\u55b3\u55b5\u55b7\u55b9\u55bb\u55bd\u55be\u55c4\u55c5\u55c9\u55cc\"\n    \"\u55cd\u55d0\u55d1\u55d2\u55d3\u55d4\u55d6\u55dc\u55dd\u55de\u55df\u55e1\u55e3\u55e4\u55e5\u55e6\u55e8\u55ea\u55eb\u55ec\u55ef\u55f2\u55f3\u55f5\u55f7\u55fd\u55fe\u5600\u5601\u5608\u5609\u560c\u560e\u560f\u5618\u561a\u561b\u561e\u561f\u5621\"\n    \"\u5623\u5624\u5627\u562c\u562d\u5631\u5632\u5634\u5636\u5639\u563b\u563f\u5640\u5642\u5647\u564c\u564d\u564e\u5654\u5657\u5658\u5659\u565c\u5662\u5664\u5668\u5669\u566a\u566b\u566c\u5671\u5676\u567b\u567c\u5684\u5685\u5686\u568e\u568f\u5693\"\n    \"\u569a\u56a3\u56ad\u56af\u56b7\u56bc\u56ca\u56d4\u56da\u56db\u56de\u56df\u56e0\u56e1\u56e2\u56e4\u56eb\u56ed\u56f0\u56f1\u56f4\u56f5\u56f7\u56f9\u56fa\u56fd\u56fe\u56ff\u5703\u5704\u5706\u5708\u5709\u570a\u570c\u5710\u5719\u571c\u571f\u5722\"\n    \"\u5723\u5728\u5729\u572a\u572b\u572c\u572d\u572e\u572f\u5730\u5732\u5733\u5739\u573a\u573b\u573e\u5740\u5742\u5747\u5749\u574a\u574b\u574c\u574d\u574e\u574f\u5750\u5751\u5752\u5757\u575a\u575b\u575c\u575d\u575e\u575f\u5760\u5761\u5764\u5765\"\n    \"\u5766\u5768\u5769\u576a\u576b\u576c\u576d\u576f\u5770\u5773\u5777\u577b\u577c\u577d\u5782\u5783\u5784\u5786\u5788\u578b\u578c\u578d\u578e\u578f\u5792\u5793\u5795\u5799\u579a\u579b\u579e\u579f\u57a0\u57a1\u57a2\u57a3\u57a4\u57a6\u57a7\u57a9\"\n    \"\u57ab\u57ad\u57ae\u57af\u57b1\u57b2\u57b4\u57b5\u57b8\u57ba\u57be\u57bf\u57c2\u57c3\u57c6\u57c7\u57cb\u57cc\u57ce\u57cf\u57d2\u57d4\u57d5\u57d7\u57d8\u57d9\u57da\u57dd\u57df\u57e0\u57e4\u57ea\u57eb\u57ed\u57ef\u57f4\u57f5\u57f8\u57f9\u57fa\"\n    \"\u57fc\u57fd\u5802\u5803\u5806\u5807\u5809\u580b\u580c\u580d\u580e\u5810\u5811\u5815\u5819\u581e\u5820\u5821\u5824\u5827\u5828\u582a\u5830\u5832\u5835\u583c\u583d\u583e\u5844\u5845\u5846\u584c\u584d\u5851\u5854\u5858\u585d\u585e\u5865\u586b\"\n    \"\u586c\u5871\u587e\u5880\u5881\u5883\u5885\u5888\u5889\u5890\u5892\u5893\u5895\u5898\u5899\u589a\u589e\u589f\u58a1\u58a3\u58a6\u58a8\u58a9\u58bc\u58c1\u58c5\u58d1\u58d5\u58e4\u58eb\u58ec\u58ee\u58f0\u58f3\u58f6\u58f8\u58f9\u5904\u5907\u590d\"\n    \"\u590f\u5910\u5914\u5915\u5916\u5919\u591a\u591c\u591f\u5924\u5925\u5927\u5929\u592a\u592b\u592c\u592d\u592e\u592f\u5931\u5934\u5937\u5938\u5939\u593a\u593c\u5941\u5942\u5944\u5947\u5948\u5949\u594b\u594e\u594f\u5951\u5953\u5954\u5955\u5956\"\n    \"\u5957\u5958\u595a\u5960\u5961\u5962\u5965\u596d\u5973\u5974\u5976\u5978\u5979\u597d\u5981\u5982\u5983\u5984\u5986\u5987\u5988\u598a\u598d\u5992\u5993\u5996\u5997\u5998\u5999\u599e\u59a3\u59a4\u59a5\u59a7\u59a8\u59a9\u59aa\u59ab\u59ad\u59ae\"\n    \"\u59af\u59b2\u59b9\u59bb\u59be\u59c6\u59c8\u59ca\u59cb\u59d0\u59d1\u59d2\u59d3\u59d4\u59d7\u59d8\u59da\u59dc\u59dd\u59de\u59e3\u59e4\u59e5\u59e8\u59ec\u59ee\u59f1\u59f6\u59f9\u59fb\u59fd\u59ff\u5a00\u5a01\u5a03\u5a04\u5a05\u5a06\u5a07\u5a08\"\n    \"\u5a09\u5a0c\u5a11\u5a13\u5a18\u5a1c\u5a1f\u5a20\u5a23\u5a25\u5a29\u5a31\u5a32\u5a34\u5a35\u5a36\u5a3c\u5a40\u5a46\u5a49\u5a4a\u5a4c\u5a4d\u5a55\u5a58\u5a5a\u5a5e\u5a60\u5a62\u5a64\u5a67\u5a6a\u5a6b\u5a73\u5a74\u5a75\u5a76\u5a77\u5a7a\u5a7b\"\n    \"\u5a7c\u5a7f\u5a82\u5a84\u5a86\u5a92\u5a93\u5a96\u5a9a\u5a9b\u5a9e\u5aaa\u5aad\u5ab1\u5ab2\u5ab3\u5ab5\u5ab8\u5abe\u5ac1\u5ac2\u5ac4\u5ac9\u5acc\u5ad2\u5ad4\u5ad5\u5ad6\u5ad8\u5ada\u5adc\u5ae0\u5ae1\u5ae3\u5ae6\u5ae9\u5aea\u5aeb\u5aed\u5af1\"\n    \"\u5afd\u5b09\u5b16\u5b17\u5b1b\u5b25\u5b2c\u5b34\u5b37\u5b3f\u5b40\u5b45\u5b50\u5b51\u5b53\u5b54\u5b55\u5b56\u5b57\u5b58\u5b59\u5b5a\u5b5b\u5b5c\u5b5d\u5b5f\u5b62\u5b63\u5b64\u5b65\u5b66\u5b69\u5b6a\u5b6c\u5b70\u5b71\u5b73\u5b75\u5b7a\u5b7d\"\n    \"\u5b81\u5b83\u5b84\u5b85\u5b87\u5b88\u5b89\u5b8b\u5b8c\u5b8f\u5b93\u5b95\u5b97\u5b98\u5b99\u5b9a\u5b9b\u5b9c\u5b9d\u5b9e\u5ba0\u5ba1\u5ba2\u5ba3\u5ba4\u5ba5\u5ba6\u5ba7\u5baa\u5bab\u5bac\u5bb0\u5bb3\u5bb4\u5bb5\u5bb6\u5bb8\u5bb9\u5bbd\u5bbe\"\n    \"\u5bbf\u5bc1\u5bc2\u5bc4\u5bc5\u5bc6\u5bc7\u5bcc\u5bd0\u5bd2\u5bd3\u5bdd\u5bde\u5bdf\u5be1\u5be4\u5be5\u5be8\u5bee\u5bf0\u5bf8\u5bf9\u5bfa\u5bfb\u5bfc\u5bff\u5c01\u5c04\u5c06\u5c09\u5c0a\u5c0f\u5c11\u5c14\u5c15\u5c16\u5c18\u5c1a\u5c1c\u5c1d\"\n    \"\u5c22\u5c24\u5c25\u5c27\u5c28\u5c2a\u5c2c\u5c31\u5c34\u5c38\u5c39\u5c3a\u5c3b\u5c3c\u5c3d\u5c3e\u5c3f\u5c40\u5c41\u5c42\u5c43\u5c45\u5c48\u5c49\u5c4a\u5c4b\u5c4e\u5c4f\u5c50\u5c51\u5c55\u5c59\u5c5e\u5c60\u5c61\u5c63\u5c65\u5c66\u5c6f\u5c71\"\n    \"\u5c79\u5c7a\u5c7c\u5c7e\u5c7f\u5c81\u5c82\u5c88\u5c8a\u5c8c\u5c8d\u5c90\u5c91\u5c94\u5c96\u5c97\u5c98\u5c99\u5c9a\u5c9b\u5c9c\u5c9e\u5ca0\u5ca2\u5ca3\u5ca8\u5ca9\u5cab\u5cac\u5cad\u5cb1\u5cb3\u5cb5\u5cb7\u5cb8\u5cbd\u5cbf\u5cc1\u5cc2\u5cc3\"\n    \"\u5cc4\u5ccb\u5cd2\u5cd7\u5cd8\u5cd9\u5cdb\u5ce1\u5ce3\u5ce4\u5ce5\u5ce6\u5ce7\u5ce8\u5cea\u5ced\u5cf0\u5cf1\u5cfb\u5cff\u5d00\u5d01\u5d02\u5d03\u5d04\u5d06\u5d07\u5d0c\u5d0e\u5d12\u5d14\u5d16\u5d1a\u5d1b\u5d1e\u5d1f\u5d21\u5d24\u5d26\u5d27\"\n    \"\u5d29\u5d2d\u5d2e\u5d34\u5d36\u5d3d\u5d3e\u5d3f\u5d41\u5d45\u5d47\u5d4a\u5d4b\u5d4c\u5d4e\u5d56\u5d58\u5d5a\u5d5b\u5d5d\u5d69\u5d6b\u5d6c\u5d6f\u5d72\u5d74\u5d82\u5d85\u5d8d\u5d92\u5d93\u5d99\u5d9d\u5d9f\u5da6\u5db2\u5db7\u5dc5\u5dc7\u5dc9\"\n    \"\u5dcd\u5ddd\u5dde\u5de1\u5de2\u5de5\u5de6\u5de7\u5de8\u5de9\u5deb\u5dee\u5def\u5df1\u5df2\u5df3\u5df4\u5df7\u5dfd\u5dfe\u5e01\u5e02\u5e03\u5e05\u5e06\u5e08\u5e0c\u5e0f\u5e10\u5e11\u5e14\u5e15\u5e16\u5e18\u5e19\u5e1a\u5e1b\u5e1c\u5e1d\u5e21\"\n    \"\u5e26\u5e27\u5e28\u5e2d\u5e2e\u5e31\u5e37\u5e38\u5e3b\u5e3c\u5e3d\u5e42\u5e44\u5e45\u5e4c\u5e54\u5e55\u5e56\u5e5b\u5e5e\u5e61\u5e62\u5e6a\u5e72\u5e73\u5e74\u5e76\u5e78\u5e7a\u5e7b\u5e7c\u5e7d\u5e7f\u5e84\u5e86\u5e87\u5e8a\u5e8b\u5e8f\u5e90\"\n    \"\u5e91\u5e93\u5e94\u5e95\u5e96\u5e97\u5e99\u5e9a\u5e9c\u5e9e\u5e9f\u5ea0\u5ea4\u5ea5\u5ea6\u5ea7\u5ead\u5eb1\u5eb3\u5eb5\u5eb6\u5eb7\u5eb8\u5eb9\u5ebc\u5ebe\u5ec6\u5ec9\u5eca\u5ecb\u5ed1\u5ed2\u5ed3\u5ed6\u5ed9\u5edb\u5ee8\u5eea\u5ef6\u5ef7\"\n    \"\u5efa\u5eff\u5f00\u5f01\u5f02\u5f03\u5f04\u5f06\u5f07\u5f08\u5f0a\u5f0b\u5f0f\u5f11\u5f13\u5f15\u5f17\u5f18\u5f1b\u5f1f\u5f20\u5f22\u5f25\u5f26\u5f27\u5f28\u5f29\u5f2d\u5f2f\u5f31\u5f36\u5f38\u5f39\u5f3a\u5f3c\u5f40\u5f52\u5f53\u5f55\u5f56\"\n    \"\u5f57\u5f58\u5f5d\u5f5f\u5f62\u5f64\u5f66\u5f67\u5f69\u5f6a\u5f6c\u5f6d\u5f70\u5f71\u5f73\u5f77\u5f79\u5f7b\u5f7c\u5f80\u5f81\u5f82\u5f84\u5f85\u5f87\u5f88\u5f89\u5f8a\u5f8b\u5f90\u5f92\u5f95\u5f97\u5f98\u5f99\u5f9b\u5f9c\u5fa1\u5fa8\u5faa\"\n    \"\u5fad\u5fae\u5fb5\u5fb7\u5fbc\u5fbd\u5fc3\u5fc5\u5fc6\u5fc9\u5fcc\u5fcd\u5fcf\u5fd0\u5fd1\u5fd2\u5fd6\u5fd7\u5fd8\u5fd9\u5fdd\u5fde\u5fe0\u5fe1\u5fe4\u5fe7\u5fea\u5feb\u5fed\u5fee\u5ff1\u5ff3\u5ff5\u5ff8\u5ffa\u5ffb\u5ffd\u5ffe\u5fff\u6000\"\n    \"\u6001\u6002\u6003\u6004\u6005\u6006\u600a\u600d\u600e\u600f\u6012\u6014\u6015\u6016\u6019\u601b\u601c\u601d\u6020\u6021\u6025\u6026\u6027\u6028\u6029\u602a\u602b\u602f\u6035\u603b\u603c\u603f\u6041\u6042\u6043\u604b\u604d\u6050\u6052\u6053\"\n    \"\u6054\u6055\u6059\u605a\u605d\u6062\u6063\u6064\u6067\u6068\u6069\u606a\u606b\u606c\u606d\u606f\u6070\u6073\u6076\u6078\u6079\u607a\u607b\u607c\u607d\u607f\u6083\u6084\u6086\u6088\u6089\u608c\u608d\u6092\u6094\u6096\u609a\u609b\u609d\u609f\"\n    \"\u60a0\u60a2\u60a3\u60a6\u60a8\u60ab\u60ac\u60ad\u60af\u60b0\u60b1\u60b2\u60b4\u60b8\u60bb\u60bc\u60c5\u60c6\u60c7\u60ca\u60cb\u60ce\u60d1\u60d4\u60d5\u60d8\u60d9\u60da\u60db\u60dc\u60dd\u60df\u60e0\u60e6\u60e7\u60e8\u60e9\u60eb\u60ec\u60ed\"\n    \"\u60ee\u60ef\u60f0\u60f3\u60f4\u60f6\u60f9\u60fa\u6100\u6101\u6103\u6106\u6108\u6109\u610d\u610e\u610f\u6110\u6114\u6115\u611a\u611f\u6120\u6123\u6124\u6126\u6127\u612b\u612d\u613f\u6146\u6148\u614a\u614c\u614e\u6151\u6155\u615d\u6162\u6165\"\n    \"\u6167\u6168\u616c\u616d\u6170\u6175\u6177\u618b\u618e\u6194\u6195\u6199\u61a7\u61a8\u61a9\u61ac\u61ad\u61b7\u61ba\u61be\u61c2\u61c8\u61ca\u61cb\u61d1\u61d2\u61d4\u61e6\u61f5\u61ff\u6206\u6208\u620a\u620b\u620c\u620d\u620e\u620f\u6210\u6211\"\n    \"\u6212\u6215\u6216\u6217\u6218\u621a\u621b\u621f\u6221\u6222\u6223\u6224\u6225\u622a\u622c\u622d\u622e\u6233\u6234\u6237\u623d\u623e\u623f\u6240\u6241\u6242\u6243\u6245\u6246\u6247\u6248\u6249\u624a\u624b\u624d\u624e\u6251\u6252\u6253\u6254\"\n    \"\u6258\u625b\u625e\u6263\u6266\u6267\u6269\u626a\u626b\u626c\u626d\u626e\u626f\u6270\u6273\u6276\u6279\u627a\u627c\u627d\u627e\u627f\u6280\u6283\u6284\u6289\u628a\u6291\u6292\u6293\u6294\u6295\u6296\u6297\u6298\u629a\u629b\u629f\u62a0\u62a1\"\n    \"\u62a2\u62a4\u62a5\u62a8\u62ab\u62ac\u62b1\u62b5\u62b9\u62bb\u62bc\u62bd\u62bf\u62c2\u62c3\u62c4\u62c5\u62c6\u62c7\u62c8\u62c9\u62ca\u62cc\u62cd\u62ce\u62d0\u62d2\u62d3\u62d4\u62d6\u62d7\u62d8\u62d9\u62db\u62dc\u62df\u62e2\u62e3\u62e4\u62e5\"\n    \"\u62e6\u62e7\u62e8\u62e9\u62ec\u62ed\u62ee\u62ef\u62f1\u62f3\u62f4\u62f6\u62f7\u62fc\u62fd\u62fe\u62ff\u6301\u6302\u6307\u6308\u6309\u630e\u6311\u6313\u6316\u631a\u631b\u631d\u631e\u631f\u6320\u6321\u6323\u6324\u6325\u6326\u6328\u632a\u632b\"\n    \"\u632f\u6332\u6339\u633a\u633d\u6342\u6343\u6345\u6346\u6349\u634b\u634c\u634d\u634e\u634f\u6350\u6355\u635e\u635f\u6361\u6362\u6363\u6367\u6369\u636d\u636e\u636f\u6376\u6377\u637a\u637b\u637d\u6380\u6382\u6387\u6388\u6389\u638a\u638c\u638e\"\n    \"\u638f\u6390\u6392\u6396\u6398\u639e\u63a0\u63a2\u63a3\u63a5\u63a7\u63a8\u63a9\u63aa\u63ac\u63ad\u63ae\u63b0\u63b3\u63b4\u63b7\u63b8\u63ba\u63bc\u63be\u63c4\u63c6\u63c9\u63cd\u63cf\u63d0\u63d2\u63d5\u63d6\u63e0\u63e1\u63e3\u63e9\u63ea\u63ed\"\n    \"\u63f3\u63f4\u63f6\u63f8\u63fd\u63ff\u6400\u6401\u6402\u6405\u640b\u640c\u640f\u6410\u6412\u6413\u6414\u641b\u641c\u641e\u6420\u6421\u6426\u642a\u642c\u642d\u6434\u643a\u643d\u6441\u6444\u6445\u6446\u6447\u6448\u644a\u644f\u6452\u6454\u6458\"\n    \"\u645b\u645e\u6467\u6469\u646d\u6474\u6478\u6479\u647d\u6482\u6484\u6485\u6487\u6491\u6492\u6495\u6496\u6499\u649e\u64a4\u64a9\u64ac\u64ad\u64ae\u64b0\u64b5\u64b7\u64b8\u64ba\u64bc\u64c0\u64c2\u64c5\u64cd\u64ce\u64d0\u64d2\u64d8\u64de\u64e2\"\n    \"\u64e4\u64e6\u64ff\u6500\u6509\u6512\u6518\u6525\u652b\u652e\u652f\u6536\u6538\u6539\u653b\u653d\u653e\u653f\u6545\u6548\u6549\u654c\u654f\u6551\u6554\u6555\u6556\u6559\u655b\u655d\u655e\u6562\u6563\u6566\u6569\u656b\u656c\u6570\u6572\u6574\"\n    \"\u6577\u6587\u658b\u658c\u6590\u6591\u6593\u6597\u6599\u659b\u659c\u659d\u659f\u65a0\u65a1\u65a4\u65a5\u65a7\u65a9\u65ab\u65ad\u65af\u65b0\u65b6\u65b9\u65bc\u65bd\u65c1\u65c3\u65c4\u65c5\u65c6\u65cb\u65cc\u65ce\u65cf\u65d0\u65d2\u65d6\u65d7\"\n    \"\u65de\u65e0\u65e2\u65e5\u65e6\u65e7\u65e8\u65e9\u65ec\u65ed\u65ee\u65ef\u65f0\u65f1\u65f4\u65f5\u65f6\u65f7\u65f8\u65fa\u65fb\u65ff\u6600\u6602\u6603\u6604\u6606\u6607\u6608\u6609\u660a\u660c\u660e\u660f\u6612\u6613\u6614\u6615\u6619\u661d\"\n    \"\u661f\u6620\u6621\u6623\u6624\u6625\u6627\u6628\u662a\u662b\u662d\u662f\u6631\u6633\u6634\u6635\u6636\u663a\u663c\u663d\u663e\u6641\u6643\u6645\u664a\u664b\u664c\u664f\u6650\u6652\u6653\u6654\u6655\u6656\u6657\u6659\u665a\u665e\u665f\u6661\"\n    \"\u6662\u6664\u6666\u6668\u666a\u666b\u666e\u666f\u6670\u6671\u6674\u6676\u6677\u667a\u667e\u6682\u6684\u6685\u6687\u668c\u6691\u6695\u6696\u6697\u669d\u66a7\u66a8\u66ae\u66b2\u66b4\u66b5\u66b6\u66b9\u66be\u66bf\u66c8\u66cc\u66d9\u66db\u66dc\"\n    \"\u66dd\u66e6\u66e9\u66f0\u66f2\u66f3\u66f4\u66f7\u66f9\u66fc\u66fe\u66ff\u6700\u6708\u6709\u670b\u670d\u670f\u6710\u6713\u6714\u6715\u6717\u671b\u671d\u671f\u6726\u6728\u672a\u672b\u672c\u672d\u672f\u6731\u6733\u6734\u6735\u6738\u673a\u673d\"\n    \"\u6740\u6742\u6743\u6744\u6746\u6748\u6749\u674c\u674e\u674f\u6750\u6751\u6753\u6755\u6756\u6759\u675c\u675e\u675f\u6760\u6761\u6765\u6767\u6768\u6769\u676a\u676d\u676f\u6770\u6772\u6773\u6775\u6777\u677b\u677c\u677e\u677f\u6781\u6784\u6785\"\n    \"\u6787\u6789\u678b\u678d\u6790\u6795\u6797\u6798\u679a\u679c\u679d\u679e\u67a2\u67a3\u67a5\u67a7\u67a8\u67aa\u67ab\u67ad\u67af\u67b0\u67b2\u67b3\u67b5\u67b6\u67b7\u67b8\u67b9\u67c1\u67c3\u67c4\u67c8\u67ca\u67cf\u67d0\u67d1\u67d2\u67d3\u67d4\"\n    \"\u67d6\u67d8\u67d9\u67da\u67dc\u67dd\u67de\u67e0\u67e2\u67e5\u67e9\u67ec\u67ef\u67f0\u67f1\u67f3\u67f4\u67f7\u67fd\u67ff\u6800\u6805\u6807\u6808\u6809\u680a\u680b\u680c\u680e\u680f\u6810\u6811\u6812\u6813\u6816\u6817\u681d\u681f\u6821\u6829\"\n    \"\u682a\u6832\u6833\u6834\u6837\u6838\u6839\u683b\u683c\u683d\u683e\u6840\u6841\u6842\u6843\u6844\u6845\u6846\u6848\u6849\u684a\u684c\u684e\u6850\u6851\u6853\u6854\u6855\u6860\u6861\u6862\u6863\u6864\u6865\u6866\u6867\u6868\u6869\u686b\u686f\"\n    \"\u6872\u6874\u6876\u6877\u6879\u6881\u6883\u6885\u6886\u688c\u688f\u6893\u6897\u68a0\u68a2\u68a3\u68a6\u68a7\u68a8\u68ad\u68af\u68b0\u68b3\u68b4\u68b5\u68bc\u68bd\u68be\u68bf\u68c0\u68c1\u68c2\u68c9\u68cb\u68cd\u68d0\u68d2\u68d3\u68d5\u68d8\"\n    \"\u68da\u68e0\u68e3\u68e4\u68e8\u68ea\u68eb\u68ec\u68ee\u68f0\u68f1\u68f5\u68f9\u68fa\u68fb\u68fc\u68fd\u6900\u6901\u6905\u6906\u690b\u690d\u690e\u6910\u6911\u6912\u6913\u691f\u6920\u6924\u692a\u692d\u6930\u6934\u6938\u6939\u693d\u693f\u6942\"\n    \"\u6952\u6954\u6957\u6959\u695a\u695d\u695e\u6960\u6963\u6966\u6969\u696a\u696b\u696e\u696f\u6977\u6978\u6979\u697c\u6982\u6983\u6984\u6985\u6986\u6987\u6988\u6989\u698d\u6991\u6994\u6995\u6996\u699b\u699c\u69a7\u69a8\u69ab\u69ad\u69b0\u69b1\"\n    \"\u69b4\u69b7\u69bb\u69c1\u69c3\u69ca\u69cc\u69ce\u69d0\u69d4\u69da\u69db\u69dc\u69df\u69e0\u69ed\u69f1\u69f2\u69fd\u69ff\u6a0a\u6a17\u6a18\u6a1f\u6a21\u6a28\u6a2a\u6a2f\u6a31\u6a35\u6a3d\u6a3e\u6a44\u6a47\u6a50\u6a51\u6a58\u6a59\u6a5b\u6a5e\"\n    \"\u6a61\u6a65\u6a66\u6a71\u6a79\u6a7c\u6a80\u6a84\u6a8e\u6a90\u6a91\u6a97\u6a9e\u6aa0\u6aa9\u6aab\u6aac\u6ac6\u6b02\u6b20\u6b21\u6b22\u6b23\u6b24\u6b27\u6b32\u6b38\u6b39\u6b3a\u6b3b\u6b3e\u6b43\u6b45\u6b46\u6b47\u6b49\u6b4c\u6b59\u6b62\u6b63\"\n    \"\u6b64\u6b65\u6b66\u6b67\u6b6a\u6b79\u6b7b\u6b7c\u6b81\u6b82\u6b83\u6b84\u6b86\u6b87\u6b89\u6b8a\u6b8b\u6b8d\u6b92\u6b93\u6b96\u6b9a\u6b9b\u6ba1\u6ba3\u6baa\u6bb3\u6bb4\u6bb5\u6bb7\u6bbf\u6bc1\u6bc2\u6bc5\u6bcb\u6bcc\u6bcd\u6bcf\u6bd0\u6bd2\"\n    \"\u6bd3\u6bd4\u6bd5\u6bd6\u6bd7\u6bd9\u6bdb\u6be1\u6bea\u6beb\u6bef\u6bf3\u6bf5\u6bf9\u6bfd\u6c05\u6c06\u6c07\u6c0d\u6c0f\u6c10\u6c11\u6c13\u6c14\u6c15\u6c16\u6c18\u6c19\u6c1a\u6c1b\u6c1f\u6c21\u6c22\u6c24\u6c26\u6c27\u6c28\u6c29\u6c2a\u6c2e\"\n    \"\u6c2f\u6c30\u6c32\u6c34\u6c38\u6c3e\u6c3f\u6c40\u6c41\u6c42\u6c46\u6c47\u6c48\u6c49\u6c4a\u6c4b\u6c50\u6c54\u6c55\u6c57\u6c5b\u6c5c\u6c5d\u6c5e\u6c5f\u6c60\u6c61\u6c64\u6c67\u6c68\u6c69\u6c6a\u6c6b\u6c6d\u6c70\u6c72\u6c74\u6c76\u6c79\u6c7d\"\n    \"\u6c7e\u6c81\u6c82\u6c83\u6c84\u6c85\u6c86\u6c87\u6c88\u6c89\u6c8c\u6c8f\u6c90\u6c93\u6c94\u6c98\u6c99\u6c9a\u6c9b\u6c9f\u6ca1\u6ca3\u6ca4\u6ca5\u6ca6\u6ca7\u6ca8\u6ca9\u6caa\u6cab\u6cad\u6cae\u6cb1\u6cb3\u6cb8\u6cb9\u6cba\u6cbb\u6cbc\u6cbd\"\n    \"\u6cbe\u6cbf\u6cc2\u6cc3\u6cc4\u6cc5\u6cc7\u6cc9\u6cca\u6ccc\u6cd0\u6cd3\u6cd4\u6cd5\u6cd6\u6cd7\u6cd9\u6cda\u6cdb\u6cdc\u6cde\u6ce0\u6ce1\u6ce2\u6ce3\u6ce5\u6ce8\u6cea\u6ceb\u6cee\u6cef\u6cf0\u6cf1\u6cf3\u6cf5\u6cf7\u6cf8\u6cfa\u6cfb\u6cfc\"\n    \"\u6cfd\u6cfe\u6d01\u6d04\u6d07\u6d08\u6d0b\u6d0c\u6d0e\u6d11\u6d12\u6d13\u6d17\u6d18\u6d19\u6d1a\u6d1b\u6d1e\u6d22\u6d23\u6d25\u6d27\u6d28\u6d2a\u6d2b\u6d2d\u6d2e\u6d31\u6d32\u6d33\u6d34\u6d35\u6d38\u6d39\u6d3a\u6d3b\u6d3c\u6d3d\u6d3e\u6d3f\"\n    \"\u6d41\u6d43\u6d45\u6d46\u6d47\u6d48\u6d49\u6d4a\u6d4b\u6d4d\u6d4e\u6d4f\u6d50\u6d51\u6d52\u6d53\u6d54\u6d55\u6d59\u6d5a\u6d5b\u6d5c\u6d5e\u6d5f\u6d60\u6d61\u6d63\u6d65\u6d66\u6d69\u6d6a\u6d6c\u6d6d\u6d6e\u6d6f\u6d70\u6d72\u6d74\u6d77\u6d78\"\n    \"\u6d7c\u6d82\u6d84\u6d85\u6d88\u6d89\u6d8c\u6d8d\u6d8e\u6d90\u6d91\u6d93\u6d94\u6d95\u6d98\u6d9b\u6d9d\u6d9e\u6d9f\u6da0\u6da1\u6da2\u6da3\u6da4\u6da6\u6da7\u6da8\u6da9\u6daa\u6dab\u6dae\u6daf\u6db2\u6db4\u6db5\u6db8\u6dbf\u6dc0\u6dc4\u6dc5\"\n    \"\u6dc6\u6dc7\u6dcb\u6dcc\u6dcf\u6dd1\u6dd6\u6dd8\u6dd9\u6ddc\u6ddd\u6dde\u6ddf\u6de0\u6de1\u6de4\u6de6\u6deb\u6dec\u6dee\u6def\u6df1\u6df3\u6df4\u6df7\u6df9\u6dfb\u6dfc\u6e05\u6e0a\u6e0c\u6e0d\u6e0e\u6e10\u6e11\u6e14\u6e17\u6e1a\u6e1d\u6e1f\"\n    \"\u6e20\u6e21\u6e23\u6e24\u6e25\u6e29\u6e2b\u6e2d\u6e2f\u6e30\u6e32\u6e34\u6e38\u6e3a\u6e3c\u6e43\u6e44\u6e49\u6e4d\u6e4e\u6e51\u6e53\u6e54\u6e56\u6e58\u6e5b\u6e5c\u6e5d\u6e5f\u6e63\u6e6b\u6e6e\u6e72\u6e74\u6e7e\u6e7f\u6e81\u6e83\u6e85\u6e86\"\n    \"\u6e87\u6e89\u6e8d\u6e8f\u6e90\u6e98\u6e9a\u6e9c\u6e9e\u6e9f\u6ea0\u6ea2\u6ea5\u6ea6\u6ea7\u6eaa\u6eaf\u6eb1\u6eb2\u6eb4\u6eb5\u6eb6\u6eb7\u6eb9\u6eba\u6ebb\u6ebd\u6ec1\u6ec2\u6ec3\u6ec6\u6ec7\u6ec9\u6ecb\u6ecd\u6ecf\u6ed1\u6ed3\u6ed4\u6ed5\"\n    \"\u6ed7\u6ed8\u6eda\u6ede\u6edf\u6ee0\u6ee1\u6ee2\u6ee4\u6ee5\u6ee6\u6ee7\u6ee8\u6ee9\u6eea\u6eeb\u6ef4\u6ef9\u6f02\u6f06\u6f08\u6f09\u6f0b\u6f0f\u6f13\u6f14\u6f15\u6f16\u6f20\u6f24\u6f26\u6f29\u6f2a\u6f2b\u6f2d\u6f2f\u6f31\u6f33\u6f34\u6f36\"\n    \"\u6f37\u6f39\u6f3b\u6f3c\u6f3e\u6f46\u6f47\u6f4b\u6f4d\u6f4f\u6f56\u6f58\u6f5c\u6f5e\u6f5f\u6f62\u6f66\u6f69\u6f6d\u6f6e\u6f72\u6f74\u6f75\u6f78\u6f7a\u6f7c\u6f7d\u6f7e\u6f82\u6f84\u6f88\u6f89\u6f8c\u6f8d\u6f8e\u6f9b\u6f9c\u6fa1\u6fa5\u6fa7\"\n    \"\u6faa\u6fad\u6fb3\u6fb4\u6fb6\u6fb9\u6fbc\u6fbd\u6fc0\u6fc2\u6fc9\u6fcb\u6fd1\u6fd2\u6fde\u6fe0\u6fe1\u6fe9\u6fee\u6fef\u700c\u700d\u7011\u7014\u701a\u701b\u7023\u7031\u7035\u7039\u703c\u7048\u704c\u704f\u705e\u706b\u706d\u706f\u7070\u7075\"\n    \"\u7076\u7078\u707c\u707e\u707f\u7080\u7085\u7086\u7089\u708a\u708c\u708e\u7092\u7094\u7095\u7096\u7098\u7099\u709c\u709d\u709f\u70a3\u70ab\u70ac\u70ad\u70ae\u70af\u70b1\u70b3\u70b7\u70b8\u70b9\u70bb\u70bc\u70bd\u70c0\u70c1\u70c2\u70c3\u70c8\"\n    \"\u70ca\u70d4\u70d8\u70d9\u70db\u70dc\u70dd\u70df\u70e0\u70e4\u70e6\u70e7\u70e8\u70e9\u70eb\u70ec\u70ed\u70ef\u70f6\u70f7\u70f9\u70fa\u70fb\u70fd\u7106\u7109\u710a\u710c\u7110\u7113\u7115\u7116\u7117\u7118\u7119\u711a\u711c\u711e\u7126\u712f\"\n    \"\u7130\u7131\u7136\u7141\u7143\u7145\u714a\u714b\u714c\u714e\u7153\u715c\u715e\u715f\u7164\u7166\u7167\u7168\u716e\u7172\u7173\u7174\u7178\u717a\u717d\u7184\u7187\u718a\u718f\u7194\u7198\u7199\u719b\u719c\u719f\u71a0\u71a5\u71a8\u71ac\u71b5\"\n    \"\u71b9\u71bb\u71c3\u71ca\u71cb\u71ce\u71cf\u71d4\u71d5\u71da\u71e0\u71e5\u71e7\u71ee\u71f9\u7206\u7207\u7214\u721a\u721d\u721f\u7228\u722a\u722c\u7230\u7231\u7235\u7236\u7237\u7238\u7239\u723b\u723d\u723f\u7241\u7242\u7247\u7248\u724c\u724d\"\n    \"\u7252\u7256\u7259\u725a\u725b\u725d\u725f\u7261\u7262\u7264\u7265\u7266\u7267\u7269\u726e\u726f\u7272\u7275\u7279\u727a\u727b\u727e\u727f\u7280\u7281\u7284\u7287\u728a\u728b\u728d\u728f\u7292\u729f\u72a8\u72ac\u72af\u72b0\u72b4\u72b6\u72b7\"\n    \"\u72b8\u72b9\u72c1\u72c2\u72c3\u72c4\u72c8\u72c9\u72cd\u72ce\u72d0\u72d2\u72d7\u72d9\u72dd\u72de\u72e0\u72e1\u72e8\u72e9\u72ec\u72ed\u72ee\u72ef\u72f0\u72f1\u72f2\u72f3\u72f4\u72f7\u72f8\u72fa\u72fb\u72fc\u7301\u7303\u7304\u7307\u730a\u730e\"\n    \"\u7315\u7316\u7317\u731b\u731c\u731d\u731e\u7321\u7322\u7325\u7329\u732a\u732b\u732c\u732e\u732f\u7330\u7331\u7334\u7337\u7339\u733a\u733e\u733f\u734d\u7350\u7352\u7357\u7360\u736c\u736d\u736f\u7374\u737e\u7383\u7384\u7387\u7389\u738b\u738e\"\n    \"\u7391\u7392\u7393\u7395\u7396\u7398\u7399\u739a\u739b\u739e\u739f\u73a0\u73a1\u73a2\u73a4\u73a5\u73a6\u73a9\u73ab\u73ad\u73ae\u73af\u73b0\u73b1\u73b2\u73b3\u73b6\u73b7\u73b9\u73ba\u73bb\u73bc\u73bf\u73c0\u73c2\u73c5\u73c7\u73c8\u73c9\u73ca\"\n    \"\u73cb\u73cc\u73cd\u73cf\u73d0\u73d1\u73d2\u73d5\u73d6\u73d9\u73db\u73dd\u73de\u73e0\u73e2\u73e3\u73e5\u73e6\u73e7\u73e9\u73ea\u73eb\u73ed\u73f0\u73f2\u73f5\u73f7\u73f8\u73f9\u73fa\u73fd\u7400\u7403\u7404\u7405\u7406\u7407\u7408\u7409\u740a\"\n    \"\u740e\u740f\u7410\u7414\u741a\u741b\u741f\u7421\u7422\u7424\u7425\u7426\u7428\u742a\u742b\u742c\u742d\u742e\u742f\u7430\u7432\u7433\u7434\u7435\u7436\u743c\u7440\u7441\u7442\u7443\u7444\u7445\u7446\u7451\u7453\u7454\u7455\u7456\u7457\u7459\"\n    \"\u745a\u745b\u745c\u745d\u745e\u745f\u7462\u7467\u7468\u746c\u746d\u7470\u7471\u7473\u7476\u7477\u747e\u7480\u7481\u7483\u7486\u7487\u7488\u748b\u748e\u7490\u7492\u7498\u749c\u749e\u749f\u74a0\u74a5\u74a7\u74a8\u74a9\u74aa\u74ac\u74ae\u74b1\"\n    \"\u74b2\u74ba\u74c0\u74d2\u74d6\u74d8\u74dc\u74de\u74e0\u74e2\u74e3\u74e4\u74e6\u74ee\u74ef\u74f4\u74f6\u74f7\u74fb\u74ff\u7504\u750d\u750f\u7511\u7513\u7517\u7518\u751a\u751c\u751f\u7521\u7525\u7526\u7528\u7529\u752a\u752b\u752c\u752d\u752f\"\n    \"\u7530\u7531\u7532\u7533\u7535\u7537\u7538\u753a\u753b\u753e\u7540\u7545\u7548\u754b\u754c\u754e\u754f\u7554\u7556\u7559\u755a\u755b\u755c\u7564\u7565\u7566\u756a\u756c\u756f\u7572\u7574\u7578\u7579\u757f\u7581\u7583\u7586\u758d\u758f\u7590\"\n    \"\u7591\u7594\u7596\u7597\u7599\u759a\u759d\u759f\u75a0\u75a1\u75a2\u75a3\u75a4\u75a5\u75ab\u75ac\u75ad\u75ae\u75af\u75b0\u75b1\u75b2\u75b3\u75b4\u75b5\u75b8\u75b9\u75bc\u75bd\u75be\u75c2\u75c3\u75c4\u75c5\u75c7\u75c8\u75c9\u75ca\u75cd\u75d2\"\n    \"\u75d3\u75d4\u75d5\u75d8\u75db\u75de\u75e2\u75e3\u75e4\u75e6\u75e7\u75e8\u75ea\u75eb\u75f0\u75f1\u75f4\u75f9\u75fc\u75ff\u7600\u7601\u7603\u7605\u7606\u760a\u760c\u7610\u7615\u7617\u7618\u7619\u761b\u761f\u7620\u7622\u7624\u7625\u7626\u7629\"\n    \"\u762a\u762b\u762d\u7630\u7633\u7634\u7635\u7638\u763c\u763e\u763f\u7640\u7643\u764c\u764d\u7654\u7656\u7657\u765c\u765e\u7663\u766b\u766f\u7678\u767b\u767d\u767e\u767f\u7682\u7684\u7686\u7687\u7688\u768b\u768e\u7691\u7693\u7695\u7696\u7699\"\n    \"\u769b\u769e\u76a4\u76a6\u76ad\u76ae\u76b1\u76b2\u76b4\u76bf\u76c2\u76c5\u76c6\u76c8\u76c9\u76ca\u76cd\u76ce\u76cf\u76d0\u76d1\u76d2\u76d4\u76d6\u76d7\u76d8\u76db\u76df\u76e5\u76e6\u76ee\u76ef\u76f1\u76f2\u76f4\u76f7\u76f8\u76f9\u76fc\u76fe\"\n    \"\u7701\u7704\u7707\u7708\u7709\u770a\u770b\u770d\u7719\u771a\u771f\u7720\u7722\u7726\u7728\u7729\u772c\u772d\u772f\u7735\u7736\u7737\u7738\u773a\u773c\u7740\u7741\u7743\u7744\u7747\u774e\u7750\u7751\u775a\u775b\u7761\u7762\u7763\u7765\u7766\"\n    \"\u7768\u776b\u776c\u7779\u777d\u777e\u777f\u7780\u7784\u7785\u778b\u778c\u778d\u778e\u7791\u7792\u779f\u77a0\u77a2\u77a5\u77a7\u77a9\u77aa\u77ab\u77ac\u77ad\u77b0\u77b3\u77b5\u77bb\u77bd\u77bf\u77cd\u77d7\u77db\u77dc\u77de\u77e2\u77e3\u77e5\"\n    \"\u77e7\u77e9\u77eb\u77ec\u77ed\u77ee\u77f0\u77f3\u77f6\u77f8\u77fb\u77fc\u77fe\u77ff\u7800\u7801\u7802\u7804\u7806\u7809\u780c\u780d\u7811\u7812\u7814\u7816\u7817\u7818\u781a\u781c\u781d\u781f\u7820\u7823\u7825\u7827\u782b\u782c\u782d\u782e\"\n    \"\u7830\u7834\u7835\u7837\u7838\u7839\u783a\u783b\u783c\u783e\u7840\u7841\u7845\u7847\u784a\u784c\u784d\u784e\u7850\u7852\u7854\u7855\u7856\u7857\u7859\u785a\u785d\u786a\u786b\u786c\u786d\u786e\u787c\u787f\u7883\u7887\u7888\u7889\u788c\u788d\"\n    \"\u788e\u788f\u7891\u7893\u7897\u7898\u789a\u789b\u789c\u789f\u78a1\u78a3\u78a5\u78a7\u78a8\u78b0\u78b1\u78b2\u78b3\u78b4\u78b6\u78b9\u78be\u78c1\u78c5\u78c9\u78ca\u78cb\u78cf\u78d0\u78d4\u78d5\u78d9\u78dc\u78e1\u78e8\u78ec\u78f2\u78f4\u78f7\"\n    \"\u78f9\u78fb\u7901\u7905\u790c\u7913\u791e\u7934\u7935\u793a\u793c\u793e\u7940\u7941\u7943\u7946\u7947\u7948\u7949\u794a\u794b\u794e\u794f\u7950\u7953\u7955\u7956\u7957\u795a\u795b\u795c\u795d\u795e\u795f\u7960\u7962\u7965\u7967\u7968\u796d\"\n    \"\u796f\u7972\u7977\u7978\u797a\u797c\u797e\u7980\u7981\u7984\u7985\u798a\u798b\u798f\u7992\u7994\u7998\u799a\u799b\u79a4\u79a7\u79b3\u79b9\u79ba\u79bb\u79bd\u79be\u79c0\u79c1\u79c3\u79c6\u79c9\u79cb\u79cd\u79d1\u79d2\u79d5\u79d8\u79df\u79e3\"\n    \"\u79e4\u79e6\u79e7\u79e9\u79eb\u79ec\u79ed\u79ef\u79f0\u79f8\u79fb\u79fd\u79fe\u7a00\u7a02\u7a03\u7a06\u7a0b\u7a0c\u7a0d\u7a0e\u7a11\u7a14\u7a17\u7a19\u7a1a\u7a1e\u7a20\u7a23\u7a33\u7a37\u7a39\u7a3b\u7a3c\u7a3d\u7a3f\u7a44\u7a46\u7a51\u7a57\"\n    \"\u7a59\u7a5c\u7a5f\u7a70\u7a74\u7a76\u7a77\u7a78\u7a79\u7a7a\u7a7f\u7a80\u7a81\u7a83\u7a84\u7a85\u7a88\u7a8a\u7a8d\u7a8e\u7a91\u7a92\u7a95\u7a96\u7a97\u7a98\u7a9c\u7a9d\u7a9f\u7aa0\u7aa3\u7aa5\u7aa6\u7aa8\u7aac\u7aad\u7ab3\u7ab8\u7abf\u7acb\"\n    \"\u7ad1\u7ad6\u7ad8\u7ad9\u7ade\u7adf\u7ae0\u7ae3\u7ae5\u7ae6\u7aeb\u7aed\u7aef\u7af9\u7afa\u7afd\u7aff\u7b03\u7b04\u7b06\u7b08\u7b0a\u7b0b\u7b0f\u7b11\u7b14\u7b15\u7b19\u7b1b\u7b1e\u7b20\u7b24\u7b25\u7b26\u7b28\u7b2a\u7b2b\u7b2c\u7b2e\u7b2f\"\n    \"\u7b31\u7b33\u7b38\u7b3a\u7b3c\u7b3e\u7b40\u7b45\u7b47\u7b49\u7b4b\u7b4c\u7b4f\u7b50\u7b51\u7b52\u7b54\u7b56\u7b58\u7b5a\u7b5b\u7b5c\u7b5d\u7b60\u7b62\u7b64\u7b65\u7b66\u7b6e\u7b71\u7b72\u7b75\u7b76\u7b77\u7b79\u7b7b\u7b7c\u7b7e\u7b80\u7b85\"\n    \"\u7b8d\u7b90\u7b93\u7b94\u7b95\u7b96\u7b97\u7b9c\u7ba1\u7ba2\u7ba6\u7ba7\u7ba8\u7ba9\u7baa\u7bab\u7bac\u7bad\u7bb1\u7bb4\u7bb8\u7bc1\u7bc6\u7bc7\u7bcc\u7bd1\u7bd3\u7bd9\u7bda\u7bdd\u7be1\u7be5\u7be6\u7bea\u7bee\u7bef\u7bf1\u7bf7\u7bfc\u7bfe\"\n    \"\u7c03\u7c07\u7c09\u7c0b\u7c0c\u7c0f\u7c15\u7c16\u7c1d\u7c1f\u7c20\u7c27\u7c2a\u7c30\u7c38\u7c3f\u7c40\u7c41\u7c4d\u7c65\u7c73\u7c74\u7c7b\u7c7c\u7c7d\u7c89\u7c91\u7c92\u7c95\u7c97\u7c98\u7c9c\u7c9d\u7c9e\u7c9f\u7ca2\u7ca4\u7ca5\u7caa\u7cae\"\n    \"\u7cb1\u7cb2\u7cb3\u7cb9\u7cbc\u7cbd\u7cbe\u7cbf\u7cc1\u7cc5\u7cc7\u7cc8\u7cca\u7ccc\u7ccd\u7cd2\u7cd5\u7cd6\u7cd7\u7cd9\u7cdc\u7cdf\u7ce0\u7ce8\u7cef\u7cf5\u7cfb\u7d0a\u7d20\u7d22\u7d27\u7d2b\u7d2f\u7d5c\u7d6e\u7d77\u7da6\u7dae\u7e20\u7e22\"\n    \"\u7e3b\u7e41\u7e44\u7e47\u7e82\u7e9b\u7ea0\u7ea1\u7ea2\u7ea3\u7ea4\u7ea5\u7ea6\u7ea7\u7ea8\u7ea9\u7eaa\u7eab\u7eac\u7ead\u7eae\u7eaf\u7eb0\u7eb1\u7eb2\u7eb3\u7eb4\u7eb5\u7eb6\u7eb7\u7eb8\u7eb9\u7eba\u7ebb\u7ebc\u7ebd\u7ebe\u7ebf\u7ec0\u7ec1\"\n    \"\u7ec2\u7ec3\u7ec4\u7ec5\u7ec6\u7ec7\u7ec8\u7ec9\u7eca\u7ecb\u7ecc\u7ecd\u7ece\u7ecf\u7ed0\u7ed1\u7ed2\u7ed3\u7ed4\u7ed5\u7ed6\u7ed7\u7ed8\u7ed9\u7eda\u7edb\u7edc\u7edd\u7ede\u7edf\u7ee0\u7ee1\u7ee2\u7ee3\u7ee4\u7ee5\u7ee6\u7ee7\u7ee8\u7ee9\"\n    \"\u7eea\u7eeb\u7eed\u7eee\u7eef\u7ef0\u7ef1\u7ef2\u7ef3\u7ef4\u7ef5\u7ef6\u7ef7\u7ef8\u7ef9\u7efa\u7efb\u7efc\u7efd\u7efe\u7eff\u7f00\u7f01\u7f02\u7f03\u7f04\u7f05\u7f06\u7f07\u7f08\u7f09\u7f0a\u7f0c\u7f0e\u7f10\u7f11\u7f12\u7f13\u7f14\u7f15\"\n    \"\u7f16\u7f17\u7f18\u7f19\u7f1a\u7f1b\u7f1c\u7f1d\u7f1e\u7f1f\u7f20\u7f21\u7f22\u7f23\u7f24\u7f25\u7f26\u7f27\u7f28\u7f29\u7f2a\u7f2b\u7f2c\u7f2d\u7f2e\u7f2f\u7f30\u7f31\u7f32\u7f33\u7f34\u7f35\u7f36\u7f38\u7f3a\u7f42\u7f44\u7f45\u7f4d\u7f50\"\n    \"\u7f51\u7f54\u7f55\u7f57\u7f58\u7f5a\u7f5f\u7f61\u7f62\u7f68\u7f69\u7f6a\u7f6e\u7f71\u7f72\u7f74\u7f76\u7f79\u7f7d\u7f7e\u7f81\u7f8a\u7f8c\u7f8e\u7f91\u7f93\u7f94\u7f95\u7f96\u7f9a\u7f9d\u7f9e\u7f9f\u7fa1\u7fa4\u7fa7\u7faf\u7fb0\u7fb1\u7fb2\"\n    \"\u7fb8\u7fb9\u7fbc\u7fbd\u7fbf\u7fc0\u7fc1\u7fc2\u7fc3\u7fc5\u7fc8\u7fca\u7fcc\u7fce\u7fd4\u7fd5\u7fd8\u7fd9\u7fda\u7fdb\u7fdf\u7fe0\u7fe1\u7fe5\u7fe6\u7fe9\u7fee\u7fef\u7ff0\u7ff1\u7ff3\u7ff7\u7ffb\u7ffc\u7ffe\u8000\u8001\u8003\u8004\u8005\"\n    \"\u8006\u8007\u800b\u800c\u800d\u800f\u8010\u8011\u8012\u8014\u8015\u8016\u8017\u8018\u8019\u801c\u8020\u8022\u8024\u8025\u8026\u8027\u8028\u8029\u802a\u8030\u8031\u8033\u8035\u8036\u8037\u8038\u803b\u803d\u803f\u8042\u8043\u8046\u804a\u804b\"\n    \"\u804c\u804d\u8052\u8054\u8058\u805a\u8069\u806a\u8071\u807f\u8083\u8084\u8086\u8087\u8089\u808b\u808c\u8093\u8096\u8098\u809a\u809b\u809d\u809f\u80a0\u80a1\u80a2\u80a4\u80a5\u80a9\u80aa\u80ab\u80ad\u80ae\u80af\u80b1\u80b2\u80b4\u80b7\u80b8\"\n    \"\u80ba\u80bc\u80bd\u80be\u80bf\u80c0\u80c1\u80c2\u80c3\u80c4\u80c6\u80c8\u80cc\u80cd\u80ce\u80d6\u80d7\u80d9\u80da\u80db\u80dc\u80dd\u80de\u80e0\u80e1\u80e3\u80e4\u80e5\u80e7\u80e8\u80e9\u80ea\u80eb\u80ec\u80ed\u80ef\u80f0\u80f1\u80f2\u80f3\"\n    \"\u80f4\u80f6\u80f8\u80fa\u80fc\u80fd\u8102\u8106\u8109\u810a\u810d\u810e\u810f\u8110\u8111\u8112\u8113\u8114\u8116\u8118\u811a\u811e\u811f\u8129\u812c\u812f\u8131\u8132\u8136\u8138\u813e\u813f\u8146\u8148\u814a\u814b\u814c\u8150\u8151\u8152\"\n    \"\u8153\u8154\u8155\u8158\u8159\u815a\u8160\u8165\u8167\u8168\u8169\u816d\u816e\u816f\u8170\u8171\u8174\u8179\u817a\u817b\u817c\u817d\u817e\u817f\u8180\u8182\u8188\u818a\u818f\u8191\u8198\u8199\u819b\u819c\u819d\u81a6\u81a8\u81b3\u81ba\u81bb\"\n    \"\u81c0\u81c2\u81c3\u81c6\u81ca\u81cc\u81d1\u81dc\u81e3\u81e7\u81ea\u81ec\u81ed\u81f3\u81f4\u81fb\u81fc\u81fe\u8200\u8201\u8202\u8204\u8205\u8206\u820c\u820d\u8210\u8212\u8214\u821b\u821c\u821e\u821f\u8220\u8222\u8223\u8225\u822a\u822b\u822c\"\n    \"\u822d\u822f\u8230\u8231\u8232\u8233\u8234\u8235\u8236\u8237\u8238\u8239\u823b\u823e\u8244\u8245\u8247\u8249\u824b\u824e\u824f\u8258\u825a\u825f\u8268\u826e\u826f\u8270\u8272\u8273\u8274\u827a\u827d\u827e\u827f\u8282\u8283\u8284\u8288\u828a\"\n    \"\u828b\u828d\u828e\u828f\u8291\u8292\u8297\u8298\u8299\u829c\u829d\u829f\u82a0\u82a1\u82a3\u82a4\u82a5\u82a6\u82a8\u82a9\u82aa\u82ab\u82ac\u82ad\u82ae\u82af\u82b0\u82b1\u82b3\u82b4\u82b7\u82b8\u82b9\u82bc\u82bd\u82be\u82c1\u82c4\u82c7\u82c8\"\n    \"\u82c9\u82ca\u82cb\u82cc\u82cd\u82ce\u82cf\u82d1\u82d2\u82d3\u82d4\u82d5\u82d7\u82d8\u82db\u82dc\u82de\u82df\u82e0\u82e1\u82e3\u82e4\u82e5\u82e6\u82e7\u82eb\u82ef\u82f1\u82f4\u82f7\u82f9\u82fb\u82fe\u8300\u8301\u8302\u8303\u8304\u8305\u8306\"\n    \"\u8308\u8309\u830b\u830c\u830e\u830f\u8311\u8313\u8314\u8315\u8317\u831a\u831b\u831c\u831d\u8327\u8328\u832b\u832c\u832d\u832f\u8331\u8333\u8334\u8335\u8336\u8338\u8339\u833a\u833c\u833d\u8340\u8341\u8343\u8344\u8346\u8347\u8349\u834f\u8350\"\n    \"\u8351\u8352\u8353\u8354\u8356\u8359\u835a\u835b\u835c\u835e\u835f\u8360\u8361\u8363\u8364\u8365\u8366\u8367\u8368\u8369\u836a\u836b\u836c\u836d\u836e\u836f\u8377\u8378\u837b\u837c\u837d\u8385\u8386\u8389\u838e\u8392\u8393\u8398\u8399\u839b\"\n    \"\u839c\u839d\u839e\u83a0\u83a8\u83a9\u83aa\u83ab\u83b0\u83b1\u83b2\u83b3\u83b4\u83b6\u83b7\u83b8\u83b9\u83ba\u83bc\u83bd\u83bf\u83c0\u83c1\u83c2\u83c5\u83c7\u83c9\u83ca\u83cc\u83cd\u83cf\u83d4\u83d6\u83d8\u83dc\u83dd\u83df\u83e0\u83e1\u83e5\"\n    \"\u83e9\u83ea\u83f0\u83f1\u83f2\u83f9\u83fc\u83fd\u8401\u8403\u8404\u8406\u840b\u840c\u840d\u840e\u840f\u8411\u8418\u841a\u841c\u841d\u8423\u8424\u8425\u8426\u8427\u8428\u8429\u8431\u8433\u8438\u8439\u843c\u843d\u8446\u844e\u8451\u8456\u8457\"\n    \"\u8459\u845a\u845b\u845c\u8461\u8463\u8469\u846b\u846c\u846d\u8470\u8471\u8473\u8474\u8475\u8476\u8478\u847a\u8482\u8484\u8487\u8488\u8489\u848b\u848c\u848e\u8490\u8497\u8499\u849c\u849f\u84a1\u84a8\u84af\u84b1\u84b2\u84b4\u84b8\u84b9\u84ba\"\n    \"\u84bb\u84bd\u84bf\u84c1\u84c2\u84c4\u84c7\u84c9\u84ca\u84cd\u84cf\u84d0\u84d1\u84d3\u84d6\u84dd\u84df\u84e0\u84e2\u84e3\u84e5\u84e6\u84ec\u84f0\u84fc\u84ff\u8500\u8503\u8508\u850a\u850c\u8511\u8513\u8517\u851a\u851f\u8521\u852b\u852c\u8537\"\n    \"\u8538\u8539\u853a\u853b\u853c\u853d\u8543\u8548\u8549\u854a\u8556\u8557\u8559\u855e\u8564\u8568\u8570\u8572\u8574\u8579\u857a\u857b\u857e\u8581\u8584\u8585\u8587\u858f\u859b\u859c\u85a2\u85a4\u85a8\u85aa\u85ae\u85af\u85b0\u85b3\u85b7\u85b8\"\n    \"\u85b9\u85bf\u85c1\u85c9\u85cf\u85d0\u85d3\u85d5\u85dc\u85df\u85e0\u85e4\u85e6\u85e8\u85e9\u85fb\u85ff\u8605\u8611\u8616\u8618\u8627\u8629\u8638\u863c\u864e\u864f\u8650\u8651\u8652\u8653\u8654\u865a\u865e\u8662\u8664\u866b\u866c\u866e\u8671\"\n    \"\u8677\u8678\u8679\u867a\u867b\u867c\u867d\u867e\u867f\u8680\u8681\u8682\u8684\u8686\u868a\u868b\u868c\u868d\u8693\u8695\u869c\u869d\u86a3\u86a4\u86a7\u86a8\u86a9\u86aa\u86ac\u86af\u86b0\u86b1\u86b2\u86b4\u86b6\u86ba\u86c0\u86c3\u86c4\u86c6\"\n    \"\u86c7\u86c9\u86ca\u86cb\u86ce\u86cf\u86d0\u86d1\u86d4\u86d8\u86d9\u86db\u86de\u86df\u86e4\u86e9\u86ed\u86ee\u86f0\u86f1\u86f2\u86f3\u86f4\u86f8\u86f9\u86fe\u8700\u8702\u8703\u8707\u8708\u8709\u870a\u870d\u870e\u8710\u8712\u8713\u8715\u8717\"\n    \"\u8718\u871a\u871c\u871e\u8721\u8722\u8723\u8725\u8729\u872e\u8731\u8734\u8737\u873b\u873e\u873f\u8747\u8748\u8749\u874c\u874e\u8753\u8757\u8758\u8759\u8760\u8763\u8764\u8765\u876e\u8770\u8772\u8774\u8776\u877b\u877c\u877d\u877e\u8782\u8783\"\n    \"\u8785\u8788\u878b\u878d\u8797\u879f\u87a0\u87a3\u87a8\u87ab\u87ac\u87ad\u87af\u87b1\u87b3\u87b5\u87ba\u87bd\u87c0\u87c6\u87ca\u87cb\u87cf\u87d1\u87d2\u87db\u87e0\u87e5\u87ea\u87eb\u87ee\u87f9\u87fe\u8803\u880a\u880b\u8813\u8815\u8816\u8821\"\n    \"\u8822\u8832\u8839\u883c\u8840\u8843\u8844\u8845\u884c\u884d\u884e\u8852\u8854\u8857\u8859\u8860\u8861\u8862\u8863\u8865\u8868\u8869\u886b\u886c\u886e\u8870\u8872\u8877\u887d\u887e\u887f\u8881\u8882\u8884\u8885\u8886\u8888\u888b\u888d\u8892\"\n    \"\u8896\u8897\u889c\u88a2\u88a4\u88aa\u88ab\u88ad\u88af\u88b1\u88b7\u88bc\u88c1\u88c2\u88c5\u88c6\u88c8\u88c9\u88ce\u88d2\u88d4\u88d5\u88d8\u88d9\u88db\u88df\u88e2\u88e3\u88e4\u88e5\u88e8\u88f0\u88f1\u88f3\u88f4\u88f8\u88f9\u88fc\u88fe\u8902\"\n    \"\u890a\u8910\u8912\u8913\u8915\u8919\u891a\u891b\u891f\u8921\u8925\u892a\u892b\u892f\u8930\u8934\u8936\u8941\u8944\u8955\u895a\u895c\u895e\u895f\u8966\u896b\u897b\u897f\u8981\u8983\u8986\u89c1\u89c2\u89c3\u89c4\u89c5\u89c6\u89c7\u89c8\u89c9\"\n    \"\u89ca\u89cb\u89cc\u89ce\u89cf\u89d0\u89d1\u89d2\u89d6\u89da\u89dc\u89de\u89df\u89e3\u89e5\u89e6\u89eb\u89ed\u89ef\u89f1\u89f3\u89ff\u8a00\u8a04\u8a07\u8a1a\u8a3e\u8a48\u8a5f\u8a79\u8a89\u8a8a\u8a93\u8b07\u8b66\u8b6c\u8ba1\u8ba2\u8ba3\u8ba4\"\n    \"\u8ba5\u8ba6\u8ba7\u8ba8\u8ba9\u8baa\u8bab\u8bad\u8bae\u8baf\u8bb0\u8bb1\u8bb2\u8bb3\u8bb4\u8bb5\u8bb6\u8bb7\u8bb8\u8bb9\u8bba\u8bbb\u8bbc\u8bbd\u8bbe\u8bbf\u8bc0\u8bc1\u8bc2\u8bc3\u8bc4\u8bc5\u8bc6\u8bc7\u8bc8\u8bc9\u8bca\u8bcb\u8bcc\u8bcd\"\n    \"\u8bce\u8bcf\u8bd0\u8bd1\u8bd2\u8bd3\u8bd4\u8bd5\u8bd6\u8bd7\u8bd8\u8bd9\u8bda\u8bdb\u8bdc\u8bdd\u8bde\u8bdf\u8be0\u8be1\u8be2\u8be3\u8be4\u8be5\u8be6\u8be7\u8be8\u8be9\u8beb\u8bec\u8bed\u8bee\u8bef\u8bf0\u8bf1\u8bf2\u8bf3\u8bf4\u8bf5\u8bf7\"\n    \"\u8bf8\u8bf9\u8bfa\u8bfb\u8bfc\u8bfd\u8bfe\u8bff\u8c00\u8c01\u8c02\u8c03\u8c04\u8c05\u8c06\u8c07\u8c08\u8c0a\u8c0b\u8c0c\u8c0d\u8c0e\u8c0f\u8c10\u8c11\u8c12\u8c13\u8c14\u8c15\u8c16\u8c17\u8c19\u8c1a\u8c1b\u8c1c\u8c1d\u8c1e\u8c1f\u8c20\u8c21\"\n    \"\u8c22\u8c23\u8c24\u8c25\u8c26\u8c27\u8c28\u8c29\u8c2a\u8c2b\u8c2c\u8c2d\u8c2e\u8c2f\u8c30\u8c31\u8c32\u8c33\u8c34\u8c35\u8c36\u8c37\u8c3c\u8c3f\u8c41\u8c46\u8c47\u8c49\u8c4c\u8c55\u8c5a\u8c61\u8c62\u8c68\u8c6a\u8c6b\u8c6e\u8c73\u8c78\u8c79\"\n    \"\u8c7a\u8c82\u8c85\u8c86\u8c89\u8c8a\u8c8c\u8c94\u8c98\u8d1d\u8d1e\u8d1f\u8d21\u8d22\u8d23\u8d24\u8d25\u8d26\u8d27\u8d28\u8d29\u8d2a\u8d2b\u8d2c\u8d2d\u8d2e\u8d2f\u8d30\u8d31\u8d32\u8d33\u8d34\u8d35\u8d36\u8d37\u8d38\u8d39\u8d3a\u8d3b\u8d3c\"\n    \"\u8d3d\u8d3e\u8d3f\u8d40\u8d41\u8d42\u8d43\u8d44\u8d45\u8d46\u8d47\u8d48\u8d49\u8d4a\u8d4b\u8d4c\u8d4d\u8d4e\u8d4f\u8d50\u8d51\u8d52\u8d53\u8d54\u8d55\u8d56\u8d57\u8d58\u8d59\u8d5a\u8d5b\u8d5c\u8d5d\u8d5e\u8d5f\u8d60\u8d61\u8d62\u8d63\u8d64\"\n    \"\u8d66\u8d67\u8d6a\u8d6b\u8d6d\u8d70\u8d73\u8d74\u8d75\u8d76\u8d77\u8d81\u8d84\u8d85\u8d8a\u8d8b\u8d91\u8d94\u8d9f\u8da3\u8daf\u8db1\u8db3\u8db4\u8db5\u8db8\u8dba\u8dbc\u8dbe\u8dbf\u8dc2\u8dc3\u8dc4\u8dc6\u8dcb\u8dcc\u8dce\u8dcf\u8dd0\u8dd1\"\n    \"\u8dd6\u8dd7\u8dda\u8ddb\u8ddd\u8dde\u8ddf\u8de3\u8de4\u8de8\u8dea\u8dec\u8def\u8df1\u8df3\u8df5\u8df6\u8df7\u8df8\u8df9\u8dfa\u8dfb\u8dfd\u8e05\u8e09\u8e0a\u8e0c\u8e0f\u8e12\u8e14\u8e1d\u8e1e\u8e1f\u8e22\u8e23\u8e26\u8e29\u8e2a\u8e2c\u8e2e\"\n    \"\u8e2f\u8e31\u8e35\u8e36\u8e39\u8e3a\u8e3d\u8e40\u8e41\u8e42\u8e44\u8e45\u8e47\u8e48\u8e49\u8e4a\u8e4b\u8e50\u8e51\u8e52\u8e59\u8e5a\u8e5c\u8e62\u8e66\u8e69\u8e6c\u8e6d\u8e6f\u8e70\u8e72\u8e74\u8e76\u8e7c\u8e7d\u8e7e\u8e7f\u8e81\u8e85\u8e87\"\n    \"\u8e8f\u8e90\u8e94\u8e9c\u8e9e\u8eab\u8eac\u8eaf\u8eb2\u8eba\u8f66\u8f67\u8f68\u8f69\u8f6a\u8f6b\u8f6c\u8f6d\u8f6e\u8f6f\u8f70\u8f71\u8f72\u8f73\u8f74\u8f75\u8f76\u8f77\u8f78\u8f79\u8f7a\u8f7b\u8f7c\u8f7d\u8f7e\u8f7f\u8f80\u8f81\u8f82\u8f83\"\n    \"\u8f84\u8f85\u8f86\u8f87\u8f88\u8f89\u8f8a\u8f8b\u8f8c\u8f8d\u8f8e\u8f8f\u8f90\u8f91\u8f92\u8f93\u8f94\u8f95\u8f96\u8f97\u8f98\u8f99\u8f9a\u8f9b\u8f9c\u8f9e\u8f9f\u8fa3\u8fa8\u8fa9\u8fab\u8fb0\u8fb1\u8fb9\u8fbd\u8fbe\u8fbf\u8fc1\u8fc2\u8fc4\"\n    \"\u8fc5\u8fc7\u8fc8\u8fce\u8fd0\u8fd1\u8fd3\u8fd4\u8fd5\u8fd8\u8fd9\u8fdb\u8fdc\u8fdd\u8fde\u8fdf\u8fe2\u8fe4\u8fe5\u8fe6\u8fe8\u8fe9\u8fea\u8feb\u8fed\u8fee\u8ff0\u8ff3\u8ff7\u8ff8\u8ff9\u8ffa\u8ffd\u9000\u9001\u9002\u9003\u9004\u9005\u9006\"\n    \"\u9009\u900a\u900b\u900d\u900f\u9010\u9011\u9012\u9014\u9016\u9017\u901a\u901b\u901d\u901e\u901f\u9020\u9021\u9022\u9026\u902d\u902e\u902f\u9034\u9035\u9036\u9038\u903b\u903c\u903e\u9041\u9042\u9044\u9046\u9047\u904d\u904f\u9050\u9051\u9052\"\n    \"\u9053\u9057\u9058\u905b\u9062\u9063\u9065\u9068\u906d\u906e\u9074\u9075\u9079\u907d\u907f\u9080\u9082\u9083\u9088\u908b\u9091\u9093\u9095\u9097\u9098\u9099\u909b\u909d\u90a0\u90a1\u90a2\u90a3\u90a6\u90a8\u90aa\u90ac\u90ae\u90af\u90b0\u90b1\"\n    \"\u90b2\u90b3\u90b4\u90b5\u90b6\u90b8\u90b9\u90ba\u90bb\u90bd\u90be\u90bf\u90c1\u90c3\u90c4\u90c5\u90c7\u90c8\u90ca\u90ce\u90cf\u90d0\u90d1\u90d3\u90d7\u90da\u90db\u90dc\u90dd\u90e1\u90e2\u90e4\u90e6\u90e7\u90e8\u90ea\u90eb\u90ed\u90ef\u90f4\"\n    \"\u90f8\u90fd\u90fe\u90ff\u9100\u9102\u9103\u9104\u9105\u910c\u9111\u9117\u9118\u9119\u911a\u911c\u911e\u9120\u9122\u9123\u912b\u912f\u9131\u9139\u9142\u9143\u9145\u9146\u9149\u914a\u914b\u914c\u914d\u914e\u914f\u9150\u9152\u9157\u915a\u915d\"\n    \"\u915e\u9161\u9162\u9163\u9164\u9165\u9166\u9169\u916a\u916c\u916e\u916f\u9170\u9171\u9172\u9174\u9175\u9176\u9177\u9178\u9179\u917a\u917d\u917e\u917f\u9185\u9187\u9189\u918b\u918c\u918d\u9190\u9191\u9192\u919a\u919b\u91a2\u91a8\u91aa\u91ad\"\n    \"\u91ae\u91af\u91b4\u91b5\u91ba\u91be\u91c7\u91c9\u91ca\u91cc\u91cd\u91ce\u91cf\u91d0\u91d1\u91dc\u9274\u928e\u92ae\u92c6\u92c8\u933e\u936a\u938f\u93ca\u93d6\u943e\u946b\u9486\u9487\u9488\u9489\u948a\u948b\u948c\u948d\u948e\u948f\u9490\u9492\"\n    \"\u9493\u9494\u9495\u9496\u9497\u9498\u9499\u949a\u949b\u949c\u949d\u949e\u949f\u94a0\u94a1\u94a2\u94a3\u94a4\u94a5\u94a6\u94a7\u94a8\u94a9\u94aa\u94ab\u94ac\u94ad\u94ae\u94af\u94b0\u94b1\u94b2\u94b3\u94b4\u94b5\u94b7\u94b9\u94ba\u94bb\u94bc\"\n    \"\u94bd\u94be\u94bf\u94c0\u94c1\u94c2\u94c3\u94c4\u94c5\u94c6\u94c8\u94c9\u94ca\u94cb\u94cc\u94cd\u94ce\u94cf\u94d0\u94d1\u94d2\u94d5\u94d6\u94d7\u94d8\u94d9\u94da\u94db\u94dc\u94dd\u94de\u94df\u94e0\u94e1\u94e2\u94e3\u94e4\u94e5\u94e7\u94e8\"\n    \"\u94e9\u94ea\u94eb\u94ec\u94ed\u94ee\u94ef\u94f0\u94f1\u94f2\u94f3\u94f4\u94f5\u94f6\u94f7\u94f8\u94f9\u94fa\u94fb\u94fc\u94fd\u94fe\u94ff\u9500\u9501\u9502\u9503\u9504\u9505\u9506\u9507\u9508\u9509\u950a\u950b\u950c\u950d\u950e\u950f\u9510\"\n    \"\u9511\u9512\u9513\u9514\u9515\u9516\u9517\u9518\u9519\u951a\u951b\u951c\u951d\u951e\u951f\u9521\u9522\u9523\u9524\u9525\u9526\u9527\u9528\u9529\u952a\u952b\u952c\u952d\u952e\u952f\u9530\u9531\u9532\u9533\u9534\u9535\u9536\u9537\u9538\u9539\"\n    \"\u953a\u953b\u953c\u953d\u953e\u953f\u9540\u9541\u9542\u9543\u9544\u9545\u9546\u9547\u9548\u9549\u954a\u954b\u954c\u954d\u954e\u954f\u9550\u9551\u9552\u9553\u9554\u9555\u9556\u9557\u9558\u955a\u955b\u955c\u955d\u955e\u9560\u9561\u9562\u9563\"\n    \"\u9564\u9565\u9566\u9567\u9568\u9569\u956a\u956b\u956c\u956d\u956e\u956f\u9570\u9571\u9572\u9573\u9574\u9575\u9576\u957f\u95e8\u95e9\u95ea\u95eb\u95ed\u95ee\u95ef\u95f0\u95f1\u95f2\u95f3\u95f4\u95f5\u95f6\u95f7\u95f8\u95f9\u95fa\u95fb\u95fc\"\n    \"\u95fd\u95fe\u95ff\u9600\u9601\u9602\u9603\u9604\u9605\u9606\u9607\u9608\u9609\u960a\u960b\u960c\u960d\u960e\u960f\u9610\u9611\u9612\u9614\u9615\u9616\u9617\u9618\u9619\u961a\u961c\u961f\u9621\u962a\u962e\u9631\u9632\u9633\u9634\u9635\u9636\"\n    \"\u963b\u963c\u963d\u963f\u9640\u9642\u9644\u9645\u9646\u9647\u9648\u9649\u964b\u964c\u964d\u964e\u9650\u9651\u9654\u9655\u965b\u965e\u965f\u9661\u9662\u9664\u9667\u9668\u9669\u966a\u966c\u9672\u9674\u9675\u9676\u9677\u9683\u9685\u9686\u9688\"\n    \"\u968b\u968d\u968f\u9690\u9694\u9697\u9698\u9699\u969c\u96a7\u96a9\u96b0\u96b3\u96b6\u96b9\u96ba\u96bc\u96bd\u96be\u96c0\u96c1\u96c4\u96c5\u96c6\u96c7\u96c9\u96ca\u96cc\u96cd\u96ce\u96cf\u96d2\u96d5\u96e0\u96e8\u96e9\u96ea\u96ef\u96f1\u96f3\"\n    \"\u96f6\u96f7\u96f9\u96fe\u9700\u9701\u9704\u9705\u9706\u9707\u9708\u9709\u970d\u970e\u970f\u9713\u9716\u971c\u971e\u9728\u972a\u972d\u9730\u9732\u9738\u9739\u973e\u9752\u9753\u9756\u9759\u975b\u975e\u9760\u9761\u9762\u9765\u9769\u976c\u9770\"\n    \"\u9773\u9774\u9776\u9778\u977a\u977c\u977d\u977f\u9781\u9785\u978b\u978d\u9791\u9792\u9794\u9798\u97a0\u97a1\u97a3\u97a7\u97a8\u97ab\u97ac\u97ad\u97ae\u97af\u97b2\u97b3\u97b4\u97c2\u97e6\u97e7\u97e8\u97e9\u97ea\u97eb\u97ec\u97ed\u97f3\u97f5\"\n    \"\u97f6\u9875\u9876\u9877\u9878\u9879\u987a\u987b\u987c\u987d\u987e\u987f\u9880\u9881\u9882\u9883\u9884\u9885\u9886\u9887\u9888\u9889\u988a\u988b\u988c\u988d\u988e\u988f\u9890\u9891\u9893\u9894\u9896\u9897\u9898\u9899\u989a\u989b\u989c\u989d\"\n    \"\u989e\u989f\u98a0\u98a1\u98a2\u98a4\u98a5\u98a6\u98a7\u98ce\u98cf\u98d0\u98d1\u98d2\u98d3\u98d4\u98d5\u98d7\u98d8\u98d9\u98de\u98df\u98e7\u98e8\u990d\u9910\u992e\u9954\u9955\u9965\u9967\u9968\u9969\u996a\u996b\u996c\u996d\u996e\u996f\u9970\"\n    \"\u9971\u9972\u9973\u9974\u9975\u9976\u9977\u9978\u9979\u997a\u997b\u997c\u997d\u997f\u9981\u9983\u9984\u9985\u9986\u9987\u9988\u9989\u998a\u998b\u998c\u998d\u998f\u9990\u9991\u9992\u9993\u9994\u9995\u9996\u9997\u9998\u9999\u999d\u999e\u99a5\"\n    \"\u99a7\u99a8\u9a6c\u9a6d\u9a6e\u9a6f\u9a70\u9a71\u9a72\u9a73\u9a74\u9a75\u9a76\u9a77\u9a78\u9a79\u9a7a\u9a7b\u9a7c\u9a7d\u9a7e\u9a7f\u9a80\u9a81\u9a82\u9a83\u9a84\u9a85\u9a86\u9a87\u9a88\u9a89\u9a8a\u9a8b\u9a8c\u9a8d\u9a8e\u9a8f\u9a90\u9a91\"\n    \"\u9a92\u9a93\u9a95\u9a96\u9a97\u9a98\u9a99\u9a9a\u9a9b\u9a9c\u9a9d\u9a9e\u9a9f\u9aa0\u9aa1\u9aa2\u9aa3\u9aa4\u9aa5\u9aa6\u9aa7\u9aa8\u9ab0\u9ab1\u9ab6\u9ab7\u9ab8\u9aba\u9abc\u9ac0\u9ac1\u9ac2\u9ac3\u9ac5\u9acb\u9acc\u9ace\u9ad1\u9ad3\u9ad8\"\n    \"\u9ae1\u9ae2\u9ae6\u9aeb\u9aed\u9aef\u9af9\u9afb\u9afd\u9b03\u9b08\u9b0f\u9b12\u9b13\u9b18\u9b1f\u9b23\u9b2f\u9b32\u9b36\u9b37\u9b3b\u9b3c\u9b41\u9b42\u9b43\u9b44\u9b45\u9b46\u9b47\u9b48\u9b49\u9b4b\u9b4d\u9b4f\u9b51\u9b54\u9c7c\u9c7d\u9c7e\"\n    \"\u9c7f\u9c80\u9c81\u9c82\u9c83\u9c85\u9c86\u9c87\u9c88\u9c89\u9c8a\u9c8b\u9c8c\u9c8d\u9c8e\u9c8f\u9c90\u9c91\u9c92\u9c94\u9c95\u9c96\u9c97\u9c98\u9c99\u9c9a\u9c9b\u9c9c\u9c9d\u9c9e\u9c9f\u9ca0\u9ca1\u9ca2\u9ca3\u9ca4\u9ca5\u9ca6\u9ca7\u9ca8\"\n    \"\u9ca9\u9caa\u9cab\u9cac\u9cad\u9cae\u9caf\u9cb0\u9cb1\u9cb2\u9cb3\u9cb4\u9cb5\u9cb7\u9cb8\u9cb9\u9cba\u9cbb\u9cbc\u9cbd\u9cbe\u9cbf\u9cc0\u9cc1\u9cc2\u9cc3\u9cc4\u9cc5\u9cc7\u9cc8\u9cc9\u9cca\u9ccc\u9ccd\u9cce\u9ccf\u9cd0\u9cd1\u9cd2\u9cd3\"\n    \"\u9cd4\u9cd5\u9cd6\u9cd7\u9cd8\u9cd9\u9cda\u9cdb\u9cdc\u9cdd\u9cde\u9cdf\u9ce0\u9ce1\u9ce2\u9ce3\u9ce4\u9e1f\u9e20\u9e21\u9e22\u9e23\u9e24\u9e25\u9e26\u9e27\u9e28\u9e29\u9e2a\u9e2b\u9e2c\u9e2d\u9e2e\u9e2f\u9e30\u9e31\u9e32\u9e33\u9e35\u9e36\"\n    \"\u9e37\u9e38\u9e39\u9e3a\u9e3b\u9e3c\u9e3d\u9e3e\u9e3f\u9e40\u9e41\u9e42\u9e43\u9e44\u9e45\u9e46\u9e47\u9e48\u9e49\u9e4a\u9e4b\u9e4c\u9e4d\u9e4e\u9e4f\u9e50\u9e51\u9e52\u9e54\u9e55\u9e56\u9e57\u9e58\u9e59\u9e5a\u9e5b\u9e5c\u9e5d\u9e5e\u9e5f\"\n    \"\u9e60\u9e61\u9e62\u9e63\u9e64\u9e66\u9e67\u9e68\u9e69\u9e6a\u9e6b\u9e6c\u9e6d\u9e6e\u9e6f\u9e70\u9e71\u9e72\u9e73\u9e74\u9e7e\u9e7f\u9e80\u9e82\u9e87\u9e88\u9e8b\u9e91\u9e92\u9e93\u9e96\u9e9d\u9e9f\u9ea6\u9eb8\u9eb9\u9ebb\u9ebd\u9ebe\u9ec4\"\n    \"\u9ec7\u9ec9\u9ecd\u9ece\u9ecf\u9ed1\u9ed4\u9ed8\u9edb\u9edc\u9edd\u9edf\u9ee0\u9ee1\u9ee2\u9ee5\u9ee7\u9ee9\u9eea\u9eef\u9ef9\u9efb\u9efc\u9efe\u9f0b\u9f0d\u9f0e\u9f10\u9f12\u9f13\u9f17\u9f19\u9f20\u9f22\u9f29\u9f2b\u9f2c\u9f2f\u9f31\u9f37\"\n    \"\u9f39\u9f3b\u9f3d\u9f3e\u9f41\u9f47\u9f49\u9f50\u9f51\u9f7f\u9f80\u9f81\u9f82\u9f83\u9f84\u9f85\u9f86\u9f87\u9f88\u9f89\u9f8a\u9f8b\u9f8c\u9f99\u9f9a\u9f9b\u9f9f\u9fa0\u9fa2\u9fcd\u9fce\u9fcf\u3447\u344a\u356e\u360e\u364d\u3658\u3666\u36c3\"\n    \"\u36da\u36f9\u37c3\u3807\u3813\u3918\u3944\u39d0\u39d1\u39df\u3af0\u3b0a\u3b0e\u3b1a\u3b4e\u3b55\u3bbe\u3c00\u3cc7\u3cd8\u3cda\u3d14\u3d50\u3db2\u3e06\u3e0c\u3e84\u3eec\u3f4f\u3fe0\u4056\u40ae\u40c5\u40ce\u415f\u4339\u4383\u4396\u43dd\u43e1\"\n    \"\u43f2\u4403\u44d6\u44db\u44e8\u44eb\u44ec\u45d6\u45db\u45ea\u45f4\u4723\u4759\u48ba\u48bc\u48d8\u497d\u4983\u4c9f\u4ca0\u4ca2\u4d13\u4d14\u4d15\u4d16\u4d17\u4d18\u4d19\u4dae\ud840\udd64\ud841\ude76\ud843\udcd0\ud844\udf9a\ud845\udc13\ud84d\uddcb\ud84f\udc97\ud84f\udc98\ud84f\ude23\ud852\udddb\ud852\ude7d\"\n    \"\ud852\udec9\ud855\udd32\ud855\udd62\ud855\udda8\ud857\uded7\ud858\ude21\ud859\udc8d\ud859\ude76\ud859\udf7c\ud85a\udf5c\ud85b\udc21\ud85f\udff9\ud861\udc08\ud861\ude78\ud861\ude95\ud861\udfe0\ud862\udf49\ud863\udc47\ud863\udc4f\ud863\udc51\ud863\udc54\ud863\ude99\ud867\udf7e\ud867\udf83\ud867\udf8c\ud869\udfdd\ud86a\udcfb\ud86a\udd17\ud86a\ude30\ud86a\ude36\ud86a\ude58\ud86b\udfa2\ud86c\udd27\ud86c\udd28\ud86c\udd37\ud86c\udd38\ud86c\udded\ud86c\udf00\ud86c\udf63\ud86c\udf6f\"\n    \"\ud86c\udf72\ud86c\udf7d\ud86d\udc04\ud86d\udc10\ud86d\udc13\ud86d\udc61\ud86d\udce7\ud86d\udcef\ud86d\udcf6\ud86d\udcf9\ud86d\udd0d\ud86d\udd0e\ud86d\udd36\ud86d\uddae\ud86d\uddaf\ud86d\uddb3\ud86d\udde7\ud86d\uddf4\ud86d\ude1c\ud86d\ude1d\ud86d\ude26\ud86d\ude27\ud86d\ude28\ud86d\ude2a\ud86d\ude2c\ud86d\ude95\ud86d\ude96\ud86d\udead\ud86d\udeed\ud86d\udfa9\ud86d\udfc5\ud86d\udfe6\ud86d\udff9\ud86d\udffc\ud86e\udc06\ud86e\udc0a\ud86e\udc1c\ud86e\udcb8\ud86e\udec7\ud86e\udf5f\"\n    \"\ud86e\udf62\ud86e\udf7c\ud86e\udf83\ud86f\udc1b\ud86f\udd77\ud86f\udd87\ud86f\uddf7\ud86f\ude29\ud870\udc29\ud870\udc2a\ud870\udca9\ud870\udcca\ud870\uddd5\ud870\uddd9\ud870\uddf9\ud870\ude7c\ud870\ude88\ud870\udea4\ud870\udf17\ud870\udf5b\ud870\udf61\ud870\udf64\ud871\udc88\ud871\udc94\ud871\udc97\ud871\udd42\ud871\ude13\ud871\ude18\ud871\ude21\ud871\ude29\ud871\ude2b\ud871\ude2c\ud871\ude2d\ud871\ude2f\ud871\ude42\ud871\ude4a\ud871\ude4b\ud871\udf2c\ud871\udf2f\ud871\udf9f\"\n    \"\ud871\udfc1\ud871\udffd\ud872\udcd9\ud872\udcde\ud872\udce1\ud872\udcf3\ud872\udd07\ud872\udd0a\ud872\udd1d\ud872\ude02\ud872\ude0e\ud872\ude7d\ud872\udea9\ud872\udf29\ud872\udf2d\ud872\udf2e\ud872\udf31\ud872\udf38\ud872\udf39\ud872\udf3b\ud872\udf3f\ud872\udf41\ud872\udf4a\ud872\udf4e\ud872\udf5a\ud872\udf5b\ud872\udf64\ud872\udf69\ud872\udf6c\ud872\udf6f\ud872\udf73\ud872\udf76\ud872\udf78\ud872\udf7c\ud872\udfb1\ud872\udfbf\ud872\udfc0\ud872\udfce\ud873\udc56\ud873\udc5f\"\n    \"\ud873\udcf5\ud873\udcf6\ud873\udcfd\ud873\udcff\ud873\udd02\ud873\udd03\ud873\udd0a\ud873\udd8b\ud873\udd8d\ud873\udd8f\ud873\udd90\ud873\udd9f\ud873\udda0\ud873\udda8\ud873\uddad\ud873\uddae\ud873\uddd5\ud873\ude18\ud873\ude1a\ud873\ude23\ud873\ude26\ud873\ude2a\ud873\ude7c\ud873\ude88\ud873\ude93\"\n)\nCN_CHARS_EXT = \"\u5436\u8bf6\u5c4c\u56e7\u98da\u5c44\"\n\nCN_CHARS = CN_CHARS_COMMON + CN_CHARS_EXT\nIN_CH_CHARS = {c: True for c in CN_CHARS}\n\nEN_CHARS = string.ascii_letters + string.digits\nIN_EN_CHARS = {c: True for c in EN_CHARS}\n\nVALID_CHARS = CN_CHARS + EN_CHARS + \" \"\nIN_VALID_CHARS = {c: True for c in VALID_CHARS}\n\n\n# ================================================================================ #\n#                                    basic class\n# ================================================================================ #\nclass ChineseChar(object):\n    \"\"\"\n    \u4e2d\u6587\u5b57\u7b26\n    \u6bcf\u4e2a\u5b57\u7b26\u5bf9\u5e94\u7b80\u4f53\u548c\u7e41\u4f53,\n    e.g. \u7b80\u4f53 = '\u8d1f', \u7e41\u4f53 = '\u8ca0'\n    \u8f6c\u6362\u65f6\u53ef\u8f6c\u6362\u4e3a\u7b80\u4f53\u6216\u7e41\u4f53\n    \"\"\"\n\n    def __init__(self, simplified, traditional):\n        self.simplified = simplified\n        self.traditional = traditional\n        # self.__repr__ = self.__str__\n\n    def __str__(self):\n        return self.simplified or self.traditional or None\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass ChineseNumberUnit(ChineseChar):\n    \"\"\"\n    \u4e2d\u6587\u6570\u5b57/\u6570\u4f4d\u5b57\u7b26\n    \u6bcf\u4e2a\u5b57\u7b26\u9664\u7e41\u7b80\u4f53\u5916\u8fd8\u6709\u4e00\u4e2a\u989d\u5916\u7684\u5927\u5199\u5b57\u7b26\n    e.g. '\u9646' \u548c '\u9678'\n    \"\"\"\n\n    def __init__(self, power, simplified, traditional, big_s, big_t):\n        super(ChineseNumberUnit, self).__init__(simplified, traditional)\n        self.power = power\n        self.big_s = big_s\n        self.big_t = big_t\n\n    def __str__(self):\n        return \"10^{}\".format(self.power)\n\n    @classmethod\n    def create(cls, index, value, numbering_type=NUMBERING_TYPES[1], small_unit=False):\n        if small_unit:\n            return ChineseNumberUnit(\n                power=index + 1, simplified=value[0], traditional=value[1], big_s=value[1], big_t=value[1]\n            )\n        elif numbering_type == NUMBERING_TYPES[0]:\n            return ChineseNumberUnit(\n                power=index + 8, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]\n            )\n        elif numbering_type == NUMBERING_TYPES[1]:\n            return ChineseNumberUnit(\n                power=(index + 2) * 4, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]\n            )\n        elif numbering_type == NUMBERING_TYPES[2]:\n            return ChineseNumberUnit(\n                power=pow(2, index + 3), simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]\n            )\n        else:\n            raise ValueError(\"Counting type should be in {0} ({1} provided).\".format(NUMBERING_TYPES, numbering_type))\n\n\nclass ChineseNumberDigit(ChineseChar):\n    \"\"\"\n    \u4e2d\u6587\u6570\u5b57\u5b57\u7b26\n    \"\"\"\n\n    def __init__(self, value, simplified, traditional, big_s, big_t, alt_s=None, alt_t=None):\n        super(ChineseNumberDigit, self).__init__(simplified, traditional)\n        self.value = value\n        self.big_s = big_s\n        self.big_t = big_t\n        self.alt_s = alt_s\n        self.alt_t = alt_t\n\n    def __str__(self):\n        return str(self.value)\n\n    @classmethod\n    def create(cls, i, v):\n        return ChineseNumberDigit(i, v[0], v[1], v[2], v[3])\n\n\nclass ChineseMath(ChineseChar):\n    \"\"\"\n    \u4e2d\u6587\u6570\u4f4d\u5b57\u7b26\n    \"\"\"\n\n    def __init__(self, simplified, traditional, symbol, expression=None):\n        super(ChineseMath, self).__init__(simplified, traditional)\n        self.symbol = symbol\n        self.expression = expression\n        self.big_s = simplified\n        self.big_t = traditional\n\n\nCC, CNU, CND, CM = ChineseChar, ChineseNumberUnit, ChineseNumberDigit, ChineseMath\n\n\nclass NumberSystem(object):\n    \"\"\"\n    \u4e2d\u6587\u6570\u5b57\u7cfb\u7edf\n    \"\"\"\n\n    pass\n\n\nclass MathSymbol(object):\n    \"\"\"\n    \u7528\u4e8e\u4e2d\u6587\u6570\u5b57\u7cfb\u7edf\u7684\u6570\u5b66\u7b26\u53f7 (\u7e41/\u7b80\u4f53), e.g.\n    positive = ['\u6b63', '\u6b63']\n    negative = ['\u8d1f', '\u8ca0']\n    point = ['\u70b9', '\u9ede']\n    \"\"\"\n\n    def __init__(self, positive, negative, point):\n        self.positive = positive\n        self.negative = negative\n        self.point = point\n\n    def __iter__(self):\n        for v in self.__dict__.values():\n            yield v\n\n\n# class OtherSymbol(object):\n#     \"\"\"\n#     \u5176\u4ed6\u7b26\u53f7\n#     \"\"\"\n#\n#     def __init__(self, sil):\n#         self.sil = sil\n#\n#     def __iter__(self):\n#         for v in self.__dict__.values():\n#             yield v\n\n\n# ================================================================================ #\n#                                    basic utils\n# ================================================================================ #\ndef create_system(numbering_type=NUMBERING_TYPES[1]):\n    \"\"\"\n    \u6839\u636e\u6570\u5b57\u7cfb\u7edf\u7c7b\u578b\u8fd4\u56de\u521b\u5efa\u76f8\u5e94\u7684\u6570\u5b57\u7cfb\u7edf\uff0c\u9ed8\u8ba4\u4e3a mid\n    NUMBERING_TYPES = ['low', 'mid', 'high']: \u4e2d\u6587\u6570\u5b57\u7cfb\u7edf\u7c7b\u578b\n        low:  '\u5146' = '\u4ebf' * '\u5341' = $10^{9}$,  '\u4eac' = '\u5146' * '\u5341', etc.\n        mid:  '\u5146' = '\u4ebf' * '\u4e07' = $10^{12}$, '\u4eac' = '\u5146' * '\u4e07', etc.\n        high: '\u5146' = '\u4ebf' * '\u4ebf' = $10^{16}$, '\u4eac' = '\u5146' * '\u5146', etc.\n    \u8fd4\u56de\u5bf9\u5e94\u7684\u6570\u5b57\u7cfb\u7edf\n    \"\"\"\n\n    # chinese number units of '\u4ebf' and larger\n    all_larger_units = zip(LARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED, LARGER_CHINESE_NUMERING_UNITS_TRADITIONAL)\n    larger_units = [CNU.create(i, v, numbering_type, False) for i, v in enumerate(all_larger_units)]\n    # chinese number units of '\u5341, \u767e, \u5343, \u4e07'\n    all_smaller_units = zip(SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED, SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL)\n    smaller_units = [CNU.create(i, v, small_unit=True) for i, v in enumerate(all_smaller_units)]\n    # digis\n    chinese_digis = zip(CHINESE_DIGIS, CHINESE_DIGIS, BIG_CHINESE_DIGIS_SIMPLIFIED, BIG_CHINESE_DIGIS_TRADITIONAL)\n    digits = [CND.create(i, v) for i, v in enumerate(chinese_digis)]\n    digits[0].alt_s, digits[0].alt_t = ZERO_ALT, ZERO_ALT\n    digits[1].alt_s, digits[1].alt_t = ONE_ALT, ONE_ALT\n    digits[2].alt_s, digits[2].alt_t = TWO_ALTS[0], TWO_ALTS[1]\n\n    # symbols\n    positive_cn = CM(POSITIVE[0], POSITIVE[1], \"+\", lambda x: x)\n    negative_cn = CM(NEGATIVE[0], NEGATIVE[1], \"-\", lambda x: -x)\n    point_cn = CM(POINT[0], POINT[1], \".\", lambda x, y: float(str(x) + \".\" + str(y)))\n    # sil_cn = CM(SIL[0], SIL[1], '-', lambda x, y: float(str(x) + '-' + str(y)))\n    system = NumberSystem()\n    system.units = smaller_units + larger_units\n    system.digits = digits\n    system.math = MathSymbol(positive_cn, negative_cn, point_cn)\n    # system.symbols = OtherSymbol(sil_cn)\n    return system\n\n\ndef chn2num(chinese_string, numbering_type=NUMBERING_TYPES[1]):\n    def get_symbol(char, system):\n        for u in system.units:\n            if char in [u.traditional, u.simplified, u.big_s, u.big_t]:\n                return u\n        for d in system.digits:\n            if char in [d.traditional, d.simplified, d.big_s, d.big_t, d.alt_s, d.alt_t]:\n                return d\n        for m in system.math:\n            if char in [m.traditional, m.simplified]:\n                return m\n\n    def string2symbols(chinese_string, system):\n        int_string, dec_string = chinese_string, \"\"\n        for p in [system.math.point.simplified, system.math.point.traditional]:\n            if p in chinese_string:\n                int_string, dec_string = chinese_string.split(p)\n                break\n        return [get_symbol(c, system) for c in int_string], [get_symbol(c, system) for c in dec_string]\n\n    def correct_symbols(integer_symbols, system):\n        \"\"\"\n        \u4e00\u767e\u516b to \u4e00\u767e\u516b\u5341\n        \u4e00\u4ebf\u4e00\u5343\u4e09\u767e\u4e07 to \u4e00\u4ebf \u4e00\u5343\u4e07 \u4e09\u767e\u4e07\n        \"\"\"\n\n        if integer_symbols and isinstance(integer_symbols[0], CNU):\n            if integer_symbols[0].power == 1:\n                integer_symbols = [system.digits[1]] + integer_symbols\n\n        if len(integer_symbols) > 1:\n            if isinstance(integer_symbols[-1], CND) and isinstance(integer_symbols[-2], CNU):\n                integer_symbols.append(CNU(integer_symbols[-2].power - 1, None, None, None, None))\n\n        result = []\n        unit_count = 0\n        for s in integer_symbols:\n            if isinstance(s, CND):\n                result.append(s)\n                unit_count = 0\n            elif isinstance(s, CNU):\n                current_unit = CNU(s.power, None, None, None, None)\n                unit_count += 1\n\n            if unit_count == 1:\n                result.append(current_unit)\n            elif unit_count > 1:\n                for i in range(len(result)):\n                    if isinstance(result[-i - 1], CNU) and result[-i - 1].power < current_unit.power:\n                        result[-i - 1] = CNU(result[-i - 1].power + current_unit.power, None, None, None, None)\n        return result\n\n    def compute_value(integer_symbols):\n        \"\"\"\n        Compute the value.\n        When current unit is larger than previous unit, current unit * all previous units will be used as all previous units.\n        e.g. '\u4e24\u5343\u4e07' = 2000 * 10000 not 2000 + 10000\n        \"\"\"\n        value = [0]\n        last_power = 0\n        for s in integer_symbols:\n            if isinstance(s, CND):\n                value[-1] = s.value\n            elif isinstance(s, CNU):\n                value[-1] *= pow(10, s.power)\n                if s.power > last_power:\n                    value[:-1] = list(map(lambda v: v * pow(10, s.power), value[:-1]))\n                    last_power = s.power\n                value.append(0)\n        return sum(value)\n\n    system = create_system(numbering_type)\n    int_part, dec_part = string2symbols(chinese_string, system)\n    int_part = correct_symbols(int_part, system)\n    int_str = str(compute_value(int_part))\n    dec_str = \"\".join([str(d.value) for d in dec_part])\n    if dec_part:\n        return \"{0}.{1}\".format(int_str, dec_str)\n    else:\n        return int_str\n\n\ndef num2chn(\n    number_string,\n    numbering_type=NUMBERING_TYPES[1],\n    big=False,\n    traditional=False,\n    alt_zero=False,\n    alt_one=False,\n    alt_two=True,\n    use_zeros=True,\n    use_units=True,\n):\n    def get_value(value_string, use_zeros=True):\n        striped_string = value_string.lstrip(\"0\")\n\n        # record nothing if all zeros\n        if not striped_string:\n            return []\n\n        # record one digits\n        elif len(striped_string) == 1:\n            if use_zeros and len(value_string) != len(striped_string):\n                return [system.digits[0], system.digits[int(striped_string)]]\n            else:\n                return [system.digits[int(striped_string)]]\n\n        # recursively record multiple digits\n        else:\n            result_unit = next(u for u in reversed(system.units) if u.power < len(striped_string))\n            result_string = value_string[: -result_unit.power]\n            return get_value(result_string) + [result_unit] + get_value(striped_string[-result_unit.power :])\n\n    system = create_system(numbering_type)\n\n    int_dec = number_string.split(\".\")\n    if len(int_dec) == 1:\n        int_string = int_dec[0]\n        dec_string = \"\"\n    elif len(int_dec) == 2:\n        int_string = int_dec[0]\n        dec_string = int_dec[1]\n    else:\n        raise ValueError(\"invalid input num string with more than one dot: {}\".format(number_string))\n\n    if use_units and len(int_string) > 1:\n        result_symbols = get_value(int_string)\n    else:\n        result_symbols = [system.digits[int(c)] for c in int_string]\n    dec_symbols = [system.digits[int(c)] for c in dec_string]\n    if dec_string:\n        result_symbols += [system.math.point] + dec_symbols\n\n    if alt_two:\n        liang = CND(2, system.digits[2].alt_s, system.digits[2].alt_t, system.digits[2].big_s, system.digits[2].big_t)\n        for i, v in enumerate(result_symbols):\n            if isinstance(v, CND) and v.value == 2:\n                next_symbol = result_symbols[i + 1] if i < len(result_symbols) - 1 else None\n                previous_symbol = result_symbols[i - 1] if i > 0 else None\n                if isinstance(next_symbol, CNU) and isinstance(previous_symbol, (CNU, type(None))):\n                    if next_symbol.power != 1 and ((previous_symbol is None) or (previous_symbol.power != 1)):\n                        result_symbols[i] = liang\n\n    # if big is True, '\u4e24' will not be used and `alt_two` has no impact on output\n    if big:\n        attr_name = \"big_\"\n        if traditional:\n            attr_name += \"t\"\n        else:\n            attr_name += \"s\"\n    else:\n        if traditional:\n            attr_name = \"traditional\"\n        else:\n            attr_name = \"simplified\"\n\n    result = \"\".join([getattr(s, attr_name) for s in result_symbols])\n\n    # if not use_zeros:\n    #     result = result.strip(getattr(system.digits[0], attr_name))\n\n    if alt_zero:\n        result = result.replace(getattr(system.digits[0], attr_name), system.digits[0].alt_s)\n\n    if alt_one:\n        result = result.replace(getattr(system.digits[1], attr_name), system.digits[1].alt_s)\n\n    for i, p in enumerate(POINT):\n        if result.startswith(p):\n            return CHINESE_DIGIS[0] + result\n\n    # ^10, 11, .., 19\n    if (\n        len(result) >= 2\n        and result[1] in [SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED[0], SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL[0]]\n        and result[0] in [CHINESE_DIGIS[1], BIG_CHINESE_DIGIS_SIMPLIFIED[1], BIG_CHINESE_DIGIS_TRADITIONAL[1]]\n    ):\n        result = result[1:]\n\n    return result\n\n\n# ================================================================================ #\n#                          different types of rewriters\n# ================================================================================ #\nclass Cardinal:\n    \"\"\"\n    CARDINAL\u7c7b\n    \"\"\"\n\n    def __init__(self, cardinal=None, chntext=None):\n        self.cardinal = cardinal\n        self.chntext = chntext\n\n    def chntext2cardinal(self):\n        return chn2num(self.chntext)\n\n    def cardinal2chntext(self):\n        return num2chn(self.cardinal)\n\n\nclass Digit:\n    \"\"\"\n    DIGIT\u7c7b\n    \"\"\"\n\n    def __init__(self, digit=None, chntext=None):\n        self.digit = digit\n        self.chntext = chntext\n\n    # def chntext2digit(self):\n    #     return chn2num(self.chntext)\n\n    def digit2chntext(self):\n        return num2chn(self.digit, alt_two=False, use_units=False)\n\n\nclass TelePhone:\n    \"\"\"\n    TELEPHONE\u7c7b\n    \"\"\"\n\n    def __init__(self, telephone=None, raw_chntext=None, chntext=None):\n        self.telephone = telephone\n        self.raw_chntext = raw_chntext\n        self.chntext = chntext\n\n    # def chntext2telephone(self):\n    #     sil_parts = self.raw_chntext.split('<SIL>')\n    #     self.telephone = '-'.join([\n    #         str(chn2num(p)) for p in sil_parts\n    #     ])\n    #     return self.telephone\n\n    def telephone2chntext(self, fixed=False):\n        if fixed:\n            sil_parts = self.telephone.split(\"-\")\n            self.raw_chntext = \"<SIL>\".join([num2chn(part, alt_two=False, use_units=False) for part in sil_parts])\n            self.chntext = self.raw_chntext.replace(\"<SIL>\", \"\")\n        else:\n            sp_parts = self.telephone.strip(\"+\").split()\n            self.raw_chntext = \"<SP>\".join([num2chn(part, alt_two=False, use_units=False) for part in sp_parts])\n            self.chntext = self.raw_chntext.replace(\"<SP>\", \"\")\n        return self.chntext\n\n\nclass Fraction:\n    \"\"\"\n    FRACTION\u7c7b\n    \"\"\"\n\n    def __init__(self, fraction=None, chntext=None):\n        self.fraction = fraction\n        self.chntext = chntext\n\n    def chntext2fraction(self):\n        denominator, numerator = self.chntext.split(\"\u5206\u4e4b\")\n        return chn2num(numerator) + \"/\" + chn2num(denominator)\n\n    def fraction2chntext(self):\n        numerator, denominator = self.fraction.split(\"/\")\n        return num2chn(denominator) + \"\u5206\u4e4b\" + num2chn(numerator)\n\n\nclass Date:\n    \"\"\"\n    DATE\u7c7b\n    \"\"\"\n\n    def __init__(self, date=None, chntext=None):\n        self.date = date\n        self.chntext = chntext\n\n    # def chntext2date(self):\n    #     chntext = self.chntext\n    #     try:\n    #         year, other = chntext.strip().split('\u5e74', maxsplit=1)\n    #         year = Digit(chntext=year).digit2chntext() + '\u5e74'\n    #     except ValueError:\n    #         other = chntext\n    #         year = ''\n    #     if other:\n    #         try:\n    #             month, day = other.strip().split('\u6708', maxsplit=1)\n    #             month = Cardinal(chntext=month).chntext2cardinal() + '\u6708'\n    #         except ValueError:\n    #             day = chntext\n    #             month = ''\n    #         if day:\n    #             day = Cardinal(chntext=day[:-1]).chntext2cardinal() + day[-1]\n    #     else:\n    #         month = ''\n    #         day = ''\n    #     date = year + month + day\n    #     self.date = date\n    #     return self.date\n\n    def date2chntext(self):\n        date = self.date\n        try:\n            year, other = date.strip().split(\"\u5e74\", 1)\n            year = Digit(digit=year).digit2chntext() + \"\u5e74\"\n        except ValueError:\n            other = date\n            year = \"\"\n        if other:\n            try:\n                month, day = other.strip().split(\"\u6708\", 1)\n                month = Cardinal(cardinal=month).cardinal2chntext() + \"\u6708\"\n            except ValueError:\n                day = date\n                month = \"\"\n            if day:\n                day = Cardinal(cardinal=day[:-1]).cardinal2chntext() + day[-1]\n        else:\n            month = \"\"\n            day = \"\"\n        chntext = year + month + day\n        self.chntext = chntext\n        return self.chntext\n\n\nclass Money:\n    \"\"\"\n    MONEY\u7c7b\n    \"\"\"\n\n    def __init__(self, money=None, chntext=None):\n        self.money = money\n        self.chntext = chntext\n\n    # def chntext2money(self):\n    #     return self.money\n\n    def money2chntext(self):\n        money = self.money\n        pattern = re.compile(r\"(\\d+(\\.\\d+)?)\")\n        matchers = pattern.findall(money)\n        if matchers:\n            for matcher in matchers:\n                money = money.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext())\n        self.chntext = money\n        return self.chntext\n\n\nclass Percentage:\n    \"\"\"\n    PERCENTAGE\u7c7b\n    \"\"\"\n\n    def __init__(self, percentage=None, chntext=None):\n        self.percentage = percentage\n        self.chntext = chntext\n\n    def chntext2percentage(self):\n        return chn2num(self.chntext.strip().strip(\"\u767e\u5206\u4e4b\")) + \"%\"\n\n    def percentage2chntext(self):\n        return \"\u767e\u5206\u4e4b\" + num2chn(self.percentage.strip().strip(\"%\"))\n\n\ndef normalize_nsw(raw_text):\n    text = \"^\" + raw_text + \"$\"\n\n    # \u89c4\u8303\u5316\u65e5\u671f\n    pattern = re.compile(r\"\\D+((([089]\\d|(19|20)\\d{2})\u5e74)?(\\d{1,2}\u6708(\\d{1,2}[\u65e5\u53f7])?)?)\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('date')\n        for matcher in matchers:\n            text = text.replace(matcher[0], Date(date=matcher[0]).date2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u91d1\u94b1\n    pattern = re.compile(r\"\\D+((\\d+(\\.\\d+)?)[\u591a\u4f59\u51e0]?\" + CURRENCY_UNITS + r\"(\\d\" + CURRENCY_UNITS + r\"?)?)\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('money')\n        for matcher in matchers:\n            text = text.replace(matcher[0], Money(money=matcher[0]).money2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u56fa\u8bdd/\u624b\u673a\u53f7\u7801\n    # \u624b\u673a\n    # http://www.jihaoba.com/news/show/13680\n    # \u79fb\u52a8\uff1a139\u3001138\u3001137\u3001136\u3001135\u3001134\u3001159\u3001158\u3001157\u3001150\u3001151\u3001152\u3001188\u3001187\u3001182\u3001183\u3001184\u3001178\u3001198\n    # \u8054\u901a\uff1a130\u3001131\u3001132\u3001156\u3001155\u3001186\u3001185\u3001176\n    # \u7535\u4fe1\uff1a133\u3001153\u3001189\u3001180\u3001181\u3001177\n    pattern = re.compile(r\"\\D((\\+?86 ?)?1([38]\\d|5[0-35-9]|7[678]|9[89])\\d{8})\\D\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('telephone')\n        for matcher in matchers:\n            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(), 1)\n    # \u56fa\u8bdd\n    pattern = re.compile(r\"\\D((0(10|2[1-3]|[3-9]\\d{2})-?)?[1-9]\\d{6,7})\\D\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('fixed telephone')\n        for matcher in matchers:\n            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(fixed=True), 1)\n\n    # \u89c4\u8303\u5316\u5206\u6570\n    pattern = re.compile(r\"(\\d+/\\d+)\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('fraction')\n        for matcher in matchers:\n            text = text.replace(matcher, Fraction(fraction=matcher).fraction2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u767e\u5206\u6570\n    text = text.replace(\"\uff05\", \"%\")\n    pattern = re.compile(r\"(\\d+(\\.\\d+)?%)\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('percentage')\n        for matcher in matchers:\n            text = text.replace(matcher[0], Percentage(percentage=matcher[0]).percentage2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u7eaf\u6570+\u91cf\u8bcd\n    pattern = re.compile(r\"(\\d+(\\.\\d+)?)[\u591a\u4f59\u51e0]?\" + COM_QUANTIFIERS)\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('cardinal+quantifier')\n        for matcher in matchers:\n            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u6570\u5b57\u7f16\u53f7\n    pattern = re.compile(r\"(\\d{4,32})\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('digit')\n        for matcher in matchers:\n            text = text.replace(matcher, Digit(digit=matcher).digit2chntext(), 1)\n\n    # \u89c4\u8303\u5316\u7eaf\u6570\n    pattern = re.compile(r\"(\\d+(\\.\\d+)?)\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('cardinal')\n        for matcher in matchers:\n            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)\n\n    # restore P2P, O2O, B2C, B2B etc\n    pattern = re.compile(r\"(([a-zA-Z]+)\u4e8c([a-zA-Z]+))\")\n    matchers = pattern.findall(text)\n    if matchers:\n        # print('particular')\n        for matcher in matchers:\n            text = text.replace(matcher[0], matcher[1] + \"2\" + matcher[2], 1)\n\n    return text.lstrip(\"^\").rstrip(\"$\")\n\n\ndef remove_erhua(text):\n    \"\"\"\n    \u53bb\u9664\u513f\u5316\u97f3\u8bcd\u4e2d\u7684\u513f:\n    \u4ed6\u5973\u513f\u5728\u90a3\u8fb9\u513f -> \u4ed6\u5973\u513f\u5728\u90a3\u8fb9\n    \"\"\"\n\n    new_str = \"\"\n    while re.search(\"\u513f\", text):\n        a = re.search(\"\u513f\", text).span()\n        remove_er_flag = 0\n\n        if ER_WHITELIST_PATTERN.search(text):\n            b = ER_WHITELIST_PATTERN.search(text).span()\n            if b[0] <= a[0]:\n                remove_er_flag = 1\n\n        if remove_er_flag == 0:\n            new_str = new_str + text[0 : a[0]]\n            text = text[a[1] :]\n        else:\n            new_str = new_str + text[0 : b[1]]\n            text = text[b[1] :]\n\n    text = new_str + text\n    return text\n\n\ndef remove_space(text):\n    tokens = text.split()\n    new = []\n    for k, t in enumerate(tokens):\n        if k != 0:\n            if IN_EN_CHARS.get(tokens[k - 1][-1]) and IN_EN_CHARS.get(t[0]):\n                new.append(\" \")\n        new.append(t)\n    return \"\".join(new)\n\n\nclass TextNorm:\n    def __init__(\n        self,\n        to_banjiao: bool = False,\n        to_upper: bool = False,\n        to_lower: bool = False,\n        remove_fillers: bool = False,\n        remove_erhua: bool = False,\n        check_chars: bool = False,\n        remove_space: bool = False,\n        cc_mode: str = \"\",\n    ):\n        self.to_banjiao = to_banjiao\n        self.to_upper = to_upper\n        self.to_lower = to_lower\n        self.remove_fillers = remove_fillers\n        self.remove_erhua = remove_erhua\n        self.check_chars = check_chars\n        self.remove_space = remove_space\n\n        self.cc = None\n        if cc_mode:\n            from opencc import OpenCC  # Open Chinese Convert: pip install opencc\n\n            self.cc = OpenCC(cc_mode)\n\n    def __call__(self, text):\n        if self.cc:\n            text = self.cc.convert(text)\n\n        if self.to_banjiao:\n            text = text.translate(QJ2BJ_TRANSFORM)\n\n        if self.to_upper:\n            text = text.upper()\n\n        if self.to_lower:\n            text = text.lower()\n\n        if self.remove_fillers:\n            for c in FILLER_CHARS:\n                text = text.replace(c, \"\")\n\n        if self.remove_erhua:\n            text = remove_erhua(text)\n\n        text = normalize_nsw(text)\n\n        text = text.translate(PUNCS_TRANSFORM)\n\n        if self.check_chars:\n            for c in text:\n                if not IN_VALID_CHARS.get(c):\n                    print(f\"WARNING: illegal char {c} in: {text}\", file=sys.stderr)\n                    return \"\"\n\n        if self.remove_space:\n            text = remove_space(text)\n\n        return text\n\n\nif __name__ == \"__main__\":\n    p = argparse.ArgumentParser()\n\n    # normalizer options\n    p.add_argument(\"--to_banjiao\", action=\"store_true\", help=\"convert quanjiao chars to banjiao\")\n    p.add_argument(\"--to_upper\", action=\"store_true\", help=\"convert to upper case\")\n    p.add_argument(\"--to_lower\", action=\"store_true\", help=\"convert to lower case\")\n    p.add_argument(\"--remove_fillers\", action=\"store_true\", help='remove filler chars such as \"\u5443, \u554a\"')\n    p.add_argument(\"--remove_erhua\", action=\"store_true\", help='remove erhua chars such as \"\u4ed6\u5973\u513f\u5728\u90a3\u8fb9\u513f -> \u4ed6\u5973\u513f\u5728\u90a3\u8fb9\"')\n    p.add_argument(\"--check_chars\", action=\"store_true\", help=\"skip sentences containing illegal chars\")\n    p.add_argument(\"--remove_space\", action=\"store_true\", help=\"remove whitespace\")\n    p.add_argument(\n        \"--cc_mode\", choices=[\"\", \"t2s\", \"s2t\"], default=\"\", help=\"convert between traditional to simplified\"\n    )\n\n    # I/O options\n    p.add_argument(\"--log_interval\", type=int, default=10000, help=\"log interval in number of processed lines\")\n    p.add_argument(\"--has_key\", action=\"store_true\", help=\"will be deprecated, set --format ark instead\")\n    p.add_argument(\"--format\", type=str, choices=[\"txt\", \"ark\", \"tsv\"], default=\"txt\", help=\"input format\")\n    p.add_argument(\"ifile\", help=\"input filename, assume utf-8 encoding\")\n    p.add_argument(\"ofile\", help=\"output filename\")\n\n    args = p.parse_args()\n\n    if args.has_key:\n        args.format = \"ark\"\n\n    normalizer = TextNorm(\n        to_banjiao=args.to_banjiao,\n        to_upper=args.to_upper,\n        to_lower=args.to_lower,\n        remove_fillers=args.remove_fillers,\n        remove_erhua=args.remove_erhua,\n        check_chars=args.check_chars,\n        remove_space=args.remove_space,\n        cc_mode=args.cc_mode,\n    )\n\n    normalizer = TextNorm(\n        to_banjiao=args.to_banjiao,\n        to_upper=args.to_upper,\n        to_lower=args.to_lower,\n        remove_fillers=args.remove_fillers,\n        remove_erhua=args.remove_erhua,\n        check_chars=args.check_chars,\n        remove_space=args.remove_space,\n        cc_mode=args.cc_mode,\n    )\n\n    ndone = 0\n    with open(args.ifile, \"r\", encoding=\"utf8\") as istream, open(args.ofile, \"w+\", encoding=\"utf8\") as ostream:\n        if args.format == \"tsv\":\n            reader = csv.DictReader(istream, delimiter=\"\\t\")\n            assert \"TEXT\" in reader.fieldnames\n            print(\"\\t\".join(reader.fieldnames), file=ostream)\n\n            for item in reader:\n                text = item[\"TEXT\"]\n\n                if text:\n                    text = normalizer(text)\n\n                if text:\n                    item[\"TEXT\"] = text\n                    print(\"\\t\".join([item[f] for f in reader.fieldnames]), file=ostream)\n\n                ndone += 1\n                if ndone % args.log_interval == 0:\n                    print(f\"text norm: {ndone} lines done.\", file=sys.stderr, flush=True)\n        else:\n            for l in istream:\n                key, text = \"\", \"\"\n                if args.format == \"ark\":  # KALDI archive, line format: \"key text\"\n                    cols = l.strip().split(maxsplit=1)\n                    key, text = cols[0], cols[1] if len(cols) == 2 else \"\"\n                else:\n                    text = l.strip()\n\n                if text:\n                    text = normalizer(text)\n\n                if text:\n                    if args.format == \"ark\":\n                        print(key + \"\\t\" + text, file=ostream)\n                    else:\n                        print(text, file=ostream)\n\n                ndone += 1\n                if ndone % args.log_interval == 0:\n                    print(f\"text norm: {ndone} lines done.\", file=sys.stderr, flush=True)\n    print(f\"text norm: {ndone} lines done in total.\", file=sys.stderr, flush=True)\n", "TTS/tts/layers/xtts/hifigan_decoder.py": "import torch\nimport torchaudio\nfrom torch import nn\nfrom torch.nn import Conv1d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.nn.utils.parametrize import remove_parametrizations\n\nfrom TTS.utils.io import load_fsspec\n\nLRELU_SLOPE = 0.1\n\n\ndef get_padding(k, d):\n    return int((k * d - d) / 2)\n\n\nclass ResBlock1(torch.nn.Module):\n    \"\"\"Residual Block Type 1. It has 3 convolutional layers in each convolutional block.\n\n    Network::\n\n        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o\n        |--------------------------------------------------------------------------------------------------|\n\n\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super().__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n            ]\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input tensor.\n        Returns:\n            Tensor: output tensor.\n        Shapes:\n            x: [B, C, T]\n        \"\"\"\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_parametrizations(l, \"weight\")\n        for l in self.convs2:\n            remove_parametrizations(l, \"weight\")\n\n\nclass ResBlock2(torch.nn.Module):\n    \"\"\"Residual Block Type 2. It has 1 convolutional layers in each convolutional block.\n\n    Network::\n\n        x -> lrelu -> conv1-> -> z -> lrelu -> conv2-> o -> + -> o\n        |---------------------------------------------------|\n\n\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super().__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_parametrizations(l, \"weight\")\n\n\nclass HifiganGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        resblock_type,\n        resblock_dilation_sizes,\n        resblock_kernel_sizes,\n        upsample_kernel_sizes,\n        upsample_initial_channel,\n        upsample_factors,\n        inference_padding=5,\n        cond_channels=0,\n        conv_pre_weight_norm=True,\n        conv_post_weight_norm=True,\n        conv_post_bias=True,\n        cond_in_each_up_layer=False,\n    ):\n        r\"\"\"HiFiGAN Generator with Multi-Receptive Field Fusion (MRF)\n\n        Network:\n            x -> lrelu -> upsampling_layer -> resblock1_k1x1 -> z1 -> + -> z_sum / #resblocks -> lrelu -> conv_post_7x1 -> tanh -> o\n                                                 ..          -> zI ---|\n                                              resblockN_kNx1 -> zN ---'\n\n        Args:\n            in_channels (int): number of input tensor channels.\n            out_channels (int): number of output tensor channels.\n            resblock_type (str): type of the `ResBlock`. '1' or '2'.\n            resblock_dilation_sizes (List[List[int]]): list of dilation values in each layer of a `ResBlock`.\n            resblock_kernel_sizes (List[int]): list of kernel sizes for each `ResBlock`.\n            upsample_kernel_sizes (List[int]): list of kernel sizes for each transposed convolution.\n            upsample_initial_channel (int): number of channels for the first upsampling layer. This is divided by 2\n                for each consecutive upsampling layer.\n            upsample_factors (List[int]): upsampling factors (stride) for each upsampling layer.\n            inference_padding (int): constant padding applied to the input at inference time. Defaults to 5.\n        \"\"\"\n        super().__init__()\n        self.inference_padding = inference_padding\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_factors)\n        self.cond_in_each_up_layer = cond_in_each_up_layer\n\n        # initial upsampling layers\n        self.conv_pre = weight_norm(Conv1d(in_channels, upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if resblock_type == \"1\" else ResBlock2\n        # upsampling layers\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_factors, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n        # MRF blocks\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for _, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n        # post convolution layer\n        self.conv_post = weight_norm(Conv1d(ch, out_channels, 7, 1, padding=3, bias=conv_post_bias))\n        if cond_channels > 0:\n            self.cond_layer = nn.Conv1d(cond_channels, upsample_initial_channel, 1)\n\n        if not conv_pre_weight_norm:\n            remove_parametrizations(self.conv_pre, \"weight\")\n\n        if not conv_post_weight_norm:\n            remove_parametrizations(self.conv_post, \"weight\")\n\n        if self.cond_in_each_up_layer:\n            self.conds = nn.ModuleList()\n            for i in range(len(self.ups)):\n                ch = upsample_initial_channel // (2 ** (i + 1))\n                self.conds.append(nn.Conv1d(cond_channels, ch, 1))\n\n    def forward(self, x, g=None):\n        \"\"\"\n        Args:\n            x (Tensor): feature input tensor.\n            g (Tensor): global conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n        o = self.conv_pre(x)\n        if hasattr(self, \"cond_layer\"):\n            o = o + self.cond_layer(g)\n        for i in range(self.num_upsamples):\n            o = F.leaky_relu(o, LRELU_SLOPE)\n            o = self.ups[i](o)\n\n            if self.cond_in_each_up_layer:\n                o = o + self.conds[i](g)\n\n            z_sum = None\n            for j in range(self.num_kernels):\n                if z_sum is None:\n                    z_sum = self.resblocks[i * self.num_kernels + j](o)\n                else:\n                    z_sum += self.resblocks[i * self.num_kernels + j](o)\n            o = z_sum / self.num_kernels\n        o = F.leaky_relu(o)\n        o = self.conv_post(o)\n        o = torch.tanh(o)\n        return o\n\n    @torch.no_grad()\n    def inference(self, c):\n        \"\"\"\n        Args:\n            x (Tensor): conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n        c = c.to(self.conv_pre.weight.device)\n        c = torch.nn.functional.pad(c, (self.inference_padding, self.inference_padding), \"replicate\")\n        return self.forward(c)\n\n    def remove_weight_norm(self):\n        print(\"Removing weight norm...\")\n        for l in self.ups:\n            remove_parametrizations(l, \"weight\")\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        remove_parametrizations(self.conv_pre, \"weight\")\n        remove_parametrizations(self.conv_post, \"weight\")\n\n    def load_checkpoint(\n        self, config, checkpoint_path, eval=False, cache=False\n    ):  # pylint: disable=unused-argument, redefined-builtin\n        state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n        self.load_state_dict(state[\"model\"])\n        if eval:\n            self.eval()\n            assert not self.training\n            self.remove_weight_norm()\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=8):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.bn1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\n\ndef set_init_dict(model_dict, checkpoint_state, c):\n    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n    for k, v in checkpoint_state.items():\n        if k not in model_dict:\n            print(\" | > Layer missing in the model definition: {}\".format(k))\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in checkpoint_state.items() if k in model_dict}\n    # 2. filter out different size layers\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if v.numel() == model_dict[k].numel()}\n    # 3. skip reinit layers\n    if c.has(\"reinit_layers\") and c.reinit_layers is not None:\n        for reinit_layer_name in c.reinit_layers:\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if reinit_layer_name not in k}\n    # 4. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    print(\" | > {} / {} layers are restored.\".format(len(pretrained_dict), len(model_dict)))\n    return model_dict\n\n\nclass PreEmphasis(nn.Module):\n    def __init__(self, coefficient=0.97):\n        super().__init__()\n        self.coefficient = coefficient\n        self.register_buffer(\"filter\", torch.FloatTensor([-self.coefficient, 1.0]).unsqueeze(0).unsqueeze(0))\n\n    def forward(self, x):\n        assert len(x.size()) == 2\n\n        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), \"reflect\")\n        return torch.nn.functional.conv1d(x, self.filter).squeeze(1)\n\n\nclass ResNetSpeakerEncoder(nn.Module):\n    \"\"\"This is copied from \ud83d\udc38TTS to remove it from the dependencies.\"\"\"\n\n    # pylint: disable=W0102\n    def __init__(\n        self,\n        input_dim=64,\n        proj_dim=512,\n        layers=[3, 4, 6, 3],\n        num_filters=[32, 64, 128, 256],\n        encoder_type=\"ASP\",\n        log_input=False,\n        use_torch_spec=False,\n        audio_config=None,\n    ):\n        super(ResNetSpeakerEncoder, self).__init__()\n\n        self.encoder_type = encoder_type\n        self.input_dim = input_dim\n        self.log_input = log_input\n        self.use_torch_spec = use_torch_spec\n        self.audio_config = audio_config\n        self.proj_dim = proj_dim\n\n        self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn1 = nn.BatchNorm2d(num_filters[0])\n\n        self.inplanes = num_filters[0]\n        self.layer1 = self.create_layer(SEBasicBlock, num_filters[0], layers[0])\n        self.layer2 = self.create_layer(SEBasicBlock, num_filters[1], layers[1], stride=(2, 2))\n        self.layer3 = self.create_layer(SEBasicBlock, num_filters[2], layers[2], stride=(2, 2))\n        self.layer4 = self.create_layer(SEBasicBlock, num_filters[3], layers[3], stride=(2, 2))\n\n        self.instancenorm = nn.InstanceNorm1d(input_dim)\n\n        if self.use_torch_spec:\n            self.torch_spec = torch.nn.Sequential(\n                PreEmphasis(audio_config[\"preemphasis\"]),\n                torchaudio.transforms.MelSpectrogram(\n                    sample_rate=audio_config[\"sample_rate\"],\n                    n_fft=audio_config[\"fft_size\"],\n                    win_length=audio_config[\"win_length\"],\n                    hop_length=audio_config[\"hop_length\"],\n                    window_fn=torch.hamming_window,\n                    n_mels=audio_config[\"num_mels\"],\n                ),\n            )\n\n        else:\n            self.torch_spec = None\n\n        outmap_size = int(self.input_dim / 8)\n\n        self.attention = nn.Sequential(\n            nn.Conv1d(num_filters[3] * outmap_size, 128, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Conv1d(128, num_filters[3] * outmap_size, kernel_size=1),\n            nn.Softmax(dim=2),\n        )\n\n        if self.encoder_type == \"SAP\":\n            out_dim = num_filters[3] * outmap_size\n        elif self.encoder_type == \"ASP\":\n            out_dim = num_filters[3] * outmap_size * 2\n        else:\n            raise ValueError(\"Undefined encoder\")\n\n        self.fc = nn.Linear(out_dim, proj_dim)\n\n        self._init_layers()\n\n    def _init_layers(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def create_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    # pylint: disable=R0201\n    def new_parameter(self, *size):\n        out = nn.Parameter(torch.FloatTensor(*size))\n        nn.init.xavier_normal_(out)\n        return out\n\n    def forward(self, x, l2_norm=False):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x (Tensor): Raw waveform signal or spectrogram frames. If input is a waveform, `torch_spec` must be `True`\n                to compute the spectrogram on-the-fly.\n            l2_norm (bool): Whether to L2-normalize the outputs.\n\n        Shapes:\n            - x: :math:`(N, 1, T_{in})` or :math:`(N, D_{spec}, T_{in})`\n        \"\"\"\n        x.squeeze_(1)\n        # if you torch spec compute it otherwise use the mel spec computed by the AP\n        if self.use_torch_spec:\n            x = self.torch_spec(x)\n\n        if self.log_input:\n            x = (x + 1e-6).log()\n        x = self.instancenorm(x).unsqueeze(1)\n\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.bn1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = x.reshape(x.size()[0], -1, x.size()[-1])\n\n        w = self.attention(x)\n\n        if self.encoder_type == \"SAP\":\n            x = torch.sum(x * w, dim=2)\n        elif self.encoder_type == \"ASP\":\n            mu = torch.sum(x * w, dim=2)\n            sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-5))\n            x = torch.cat((mu, sg), 1)\n\n        x = x.view(x.size()[0], -1)\n        x = self.fc(x)\n\n        if l2_norm:\n            x = torch.nn.functional.normalize(x, p=2, dim=1)\n        return x\n\n    def load_checkpoint(\n        self,\n        checkpoint_path: str,\n        eval: bool = False,\n        use_cuda: bool = False,\n        criterion=None,\n        cache=False,\n    ):\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n        try:\n            self.load_state_dict(state[\"model\"])\n            print(\" > Model fully restored. \")\n        except (KeyError, RuntimeError) as error:\n            # If eval raise the error\n            if eval:\n                raise error\n\n            print(\" > Partial model initialization.\")\n            model_dict = self.state_dict()\n            model_dict = set_init_dict(model_dict, state[\"model\"])\n            self.load_state_dict(model_dict)\n            del model_dict\n\n        # load the criterion for restore_path\n        if criterion is not None and \"criterion\" in state:\n            try:\n                criterion.load_state_dict(state[\"criterion\"])\n            except (KeyError, RuntimeError) as error:\n                print(\" > Criterion load ignored because of:\", error)\n\n        if use_cuda:\n            self.cuda()\n            if criterion is not None:\n                criterion = criterion.cuda()\n\n        if eval:\n            self.eval()\n            assert not self.training\n\n        if not eval:\n            return criterion, state[\"step\"]\n        return criterion\n\n\nclass HifiDecoder(torch.nn.Module):\n    def __init__(\n        self,\n        input_sample_rate=22050,\n        output_sample_rate=24000,\n        output_hop_length=256,\n        ar_mel_length_compression=1024,\n        decoder_input_dim=1024,\n        resblock_type_decoder=\"1\",\n        resblock_dilation_sizes_decoder=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n        resblock_kernel_sizes_decoder=[3, 7, 11],\n        upsample_rates_decoder=[8, 8, 2, 2],\n        upsample_initial_channel_decoder=512,\n        upsample_kernel_sizes_decoder=[16, 16, 4, 4],\n        d_vector_dim=512,\n        cond_d_vector_in_each_upsampling_layer=True,\n        speaker_encoder_audio_config={\n            \"fft_size\": 512,\n            \"win_length\": 400,\n            \"hop_length\": 160,\n            \"sample_rate\": 16000,\n            \"preemphasis\": 0.97,\n            \"num_mels\": 64,\n        },\n    ):\n        super().__init__()\n        self.input_sample_rate = input_sample_rate\n        self.output_sample_rate = output_sample_rate\n        self.output_hop_length = output_hop_length\n        self.ar_mel_length_compression = ar_mel_length_compression\n        self.speaker_encoder_audio_config = speaker_encoder_audio_config\n        self.waveform_decoder = HifiganGenerator(\n            decoder_input_dim,\n            1,\n            resblock_type_decoder,\n            resblock_dilation_sizes_decoder,\n            resblock_kernel_sizes_decoder,\n            upsample_kernel_sizes_decoder,\n            upsample_initial_channel_decoder,\n            upsample_rates_decoder,\n            inference_padding=0,\n            cond_channels=d_vector_dim,\n            conv_pre_weight_norm=False,\n            conv_post_weight_norm=False,\n            conv_post_bias=False,\n            cond_in_each_up_layer=cond_d_vector_in_each_upsampling_layer,\n        )\n        self.speaker_encoder = ResNetSpeakerEncoder(\n            input_dim=64,\n            proj_dim=512,\n            log_input=True,\n            use_torch_spec=True,\n            audio_config=speaker_encoder_audio_config,\n        )\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, latents, g=None):\n        \"\"\"\n        Args:\n            x (Tensor): feature input tensor (GPT latent).\n            g (Tensor): global conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n\n        z = torch.nn.functional.interpolate(\n            latents.transpose(1, 2),\n            scale_factor=[self.ar_mel_length_compression / self.output_hop_length],\n            mode=\"linear\",\n        ).squeeze(1)\n        # upsample to the right sr\n        if self.output_sample_rate != self.input_sample_rate:\n            z = torch.nn.functional.interpolate(\n                z,\n                scale_factor=[self.output_sample_rate / self.input_sample_rate],\n                mode=\"linear\",\n            ).squeeze(0)\n        o = self.waveform_decoder(z, g=g)\n        return o\n\n    @torch.no_grad()\n    def inference(self, c, g):\n        \"\"\"\n        Args:\n            x (Tensor): feature input tensor (GPT latent).\n            g (Tensor): global conditioning input tensor.\n\n        Returns:\n            Tensor: output waveform.\n\n        Shapes:\n            x: [B, C, T]\n            Tensor: [B, 1, T]\n        \"\"\"\n        return self.forward(c, g=g)\n\n    def load_checkpoint(self, checkpoint_path, eval=False):  # pylint: disable=unused-argument, redefined-builtin\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))\n        # remove unused keys\n        state = state[\"model\"]\n        states_keys = list(state.keys())\n        for key in states_keys:\n            if \"waveform_decoder.\" not in key and \"speaker_encoder.\" not in key:\n                del state[key]\n\n        self.load_state_dict(state)\n        if eval:\n            self.eval()\n            assert not self.training\n            self.waveform_decoder.remove_weight_norm()\n", "TTS/tts/layers/xtts/gpt_inference.py": "import math\n\nimport torch\nfrom torch import nn\nfrom transformers import GPT2PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n\n\nclass GPT2InferenceModel(GPT2PreTrainedModel):\n    \"\"\"Override GPT2LMHeadModel to allow for prefix conditioning.\"\"\"\n\n    def __init__(self, config, gpt, pos_emb, embeddings, norm, linear, kv_cache):\n        super().__init__(config)\n        self.transformer = gpt\n        self.pos_embedding = pos_emb\n        self.embeddings = embeddings\n        self.final_norm = norm\n        self.lm_head = nn.Sequential(norm, linear)\n        self.kv_cache = kv_cache\n\n    def store_prefix_emb(self, prefix_emb):\n        self.cached_prefix_emb = prefix_emb\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):\n        token_type_ids = kwargs.get(\"token_type_ids\", None)  # usually None\n        if not self.kv_cache:\n            past_key_values = None\n\n        # only last token for inputs_ids if past is defined in kwargs\n        if past_key_values is not None:\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n            if token_type_ids is not None:\n                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n\n        attention_mask = kwargs.get(\"attention_mask\", None)\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values is not None:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n        else:\n            position_ids = None\n        return {\n            \"input_ids\": input_ids,\n            \"past_key_values\": past_key_values,\n            \"use_cache\": kwargs.get(\"use_cache\"),\n            \"position_ids\": position_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n\n    def forward(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        assert self.cached_prefix_emb is not None\n        assert inputs_embeds is None  # Not supported by this inference model.\n        assert labels is None  # Training not supported by this inference model.\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # assert len(past_key_values) + len(input_ids) == attention_mask.shape[1]\n\n        # Create embedding\n        prefix_len = self.cached_prefix_emb.shape[1]\n        if input_ids.shape[1] != 1:\n            gen_inputs = input_ids[:, prefix_len:]\n            gen_emb = self.embeddings(gen_inputs)\n            gen_emb = gen_emb + self.pos_embedding(gen_emb)\n            if self.cached_prefix_emb.shape[0] != gen_emb.shape[0]:\n                prefix_emb = self.cached_prefix_emb.repeat_interleave(\n                    gen_emb.shape[0] // self.cached_prefix_emb.shape[0], 0\n                )\n            else:\n                prefix_emb = self.cached_prefix_emb.to(gen_emb.dtype)\n            emb = torch.cat([prefix_emb, gen_emb], dim=1)\n        else:\n            emb = self.embeddings(input_ids)\n            emb = emb + self.pos_embedding.get_fixed_embedding(\n                attention_mask.shape[1] - (prefix_len + 1), attention_mask.device\n            )\n        transformer_outputs = self.transformer(\n            inputs_embeds=emb,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + transformer_outputs[1:]\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=None,\n            logits=lm_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n            cross_attentions=transformer_outputs.cross_attentions,\n        )\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        \"\"\"\n        This function is used to re-order the :obj:`past_key_values` cache if\n        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n        \"\"\"\n        return tuple(\n            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n            for layer_past in past\n        )\n", "TTS/tts/layers/xtts/latent_encoder.py": "# ported from: Originally ported from: https://github.com/neonbjb/tortoise-tts\n\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef normalization(channels):\n    groups = 32\n    if channels <= 16:\n        groups = 8\n    elif channels <= 64:\n        groups = 16\n    while channels % groups != 0:\n        groups = int(groups / 2)\n    assert groups > 2\n    return GroupNorm32(groups, channels)\n\n\ndef zero_module(module):\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass QKVAttention(nn.Module):\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv, mask=None, qk_bias=0):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n        weight = weight + qk_bias\n        if mask is not None:\n            mask = mask.repeat(self.n_heads, 1, 1)\n            weight[mask.logical_not()] = -torch.inf\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n\n        return a.reshape(bs, -1, length)\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"An attention block that allows spatial positions to attend to each other.\"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        out_channels=None,\n        do_activation=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        out_channels = channels if out_channels is None else out_channels\n        self.do_activation = do_activation\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, out_channels * 3, 1)\n        self.attention = QKVAttention(self.num_heads)\n\n        self.x_proj = nn.Identity() if out_channels == channels else conv_nd(1, channels, out_channels, 1)\n        self.proj_out = zero_module(conv_nd(1, out_channels, out_channels, 1))\n\n    def forward(self, x, mask=None, qk_bias=0):\n        b, c, *spatial = x.shape\n        if mask is not None:\n            if len(mask.shape) == 2:\n                mask = mask.unsqueeze(0).repeat(x.shape[0], 1, 1)\n            if mask.shape[1] != x.shape[-1]:\n                mask = mask[:, : x.shape[-1], : x.shape[-1]]\n\n        x = x.reshape(b, c, -1)\n        x = self.norm(x)\n        if self.do_activation:\n            x = F.silu(x, inplace=True)\n        qkv = self.qkv(x)\n        h = self.attention(qkv, mask=mask, qk_bias=qk_bias)\n        h = self.proj_out(h)\n        xp = self.x_proj(x)\n        return (xp + h).reshape(b, xp.shape[1], *spatial)\n\n\nclass ConditioningEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        attn_blocks=6,\n        num_attn_heads=4,\n    ):\n        super().__init__()\n        attn = []\n        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)\n        for a in range(attn_blocks):\n            attn.append(AttentionBlock(embedding_dim, num_attn_heads))\n        self.attn = nn.Sequential(*attn)\n        self.dim = embedding_dim\n\n    def forward(self, x):\n        \"\"\"\n        x: (b, 80, s)\n        \"\"\"\n        h = self.init(x)\n        h = self.attn(h)\n        return h\n", "TTS/tts/layers/xtts/gpt.py": "# ported from: https://github.com/neonbjb/tortoise-tts\n\nimport functools\nimport math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Config\n\nfrom TTS.tts.layers.xtts.gpt_inference import GPT2InferenceModel\nfrom TTS.tts.layers.xtts.latent_encoder import ConditioningEncoder\nfrom TTS.tts.layers.xtts.perceiver_encoder import PerceiverResampler\n\n\ndef null_position_embeddings(range, dim):\n    return torch.zeros((range.shape[0], range.shape[1], dim), device=range.device)\n\n\nclass LearnedPositionEmbeddings(nn.Module):\n    def __init__(self, seq_len, model_dim, init=0.02, relative=False):\n        super().__init__()\n        # nn.Embedding\n        self.emb = torch.nn.Embedding(seq_len, model_dim)\n        # Initializing this way is standard for GPT-2\n        self.emb.weight.data.normal_(mean=0.0, std=init)\n        self.relative = relative\n        self.seq_len = seq_len\n\n    def forward(self, x):\n        sl = x.shape[1]\n        if self.relative:\n            start = random.randint(sl, self.seq_len) - sl\n            return self.emb(torch.arange(start, start + sl, device=x.device))\n        else:\n            return self.emb(torch.arange(0, sl, device=x.device))\n\n    def get_fixed_embedding(self, ind, dev):\n        return self.emb(torch.tensor([ind], device=dev)).unsqueeze(0)\n\n\ndef build_hf_gpt_transformer(\n    layers,\n    model_dim,\n    heads,\n    max_mel_seq_len,\n    max_text_seq_len,\n    max_prompt_len,\n    checkpointing,\n):\n    \"\"\"\n    GPT-2 implemented by the HuggingFace library.\n    \"\"\"\n    from transformers import GPT2Config, GPT2Model\n\n    gpt_config = GPT2Config(\n        vocab_size=256,  # Unused.\n        n_positions=max_mel_seq_len + max_text_seq_len + max_prompt_len,\n        n_ctx=max_mel_seq_len + max_text_seq_len + max_prompt_len,\n        n_embd=model_dim,\n        n_layer=layers,\n        n_head=heads,\n        gradient_checkpointing=checkpointing,\n        use_cache=not checkpointing,\n    )\n    gpt = GPT2Model(gpt_config)\n    # Override the built in positional embeddings\n    del gpt.wpe\n    gpt.wpe = functools.partial(null_position_embeddings, dim=model_dim)\n    # Built-in token embeddings are unused.\n    del gpt.wte\n\n    mel_pos_emb = (\n        LearnedPositionEmbeddings(max_mel_seq_len, model_dim)\n        if max_mel_seq_len != -1\n        else functools.partial(null_position_embeddings, dim=model_dim)\n    )\n    text_pos_emb = (\n        LearnedPositionEmbeddings(max_text_seq_len, model_dim)\n        if max_mel_seq_len != -1\n        else functools.partial(null_position_embeddings, dim=model_dim)\n    )\n    # gpt = torch.compile(gpt, mode=\"reduce-overhead\", fullgraph=True)\n    return gpt, mel_pos_emb, text_pos_emb, None, None\n\n\nclass GPT(nn.Module):\n    def __init__(\n        self,\n        start_text_token=261,\n        stop_text_token=0,\n        layers=8,\n        model_dim=512,\n        heads=8,\n        max_text_tokens=120,\n        max_mel_tokens=250,\n        max_prompt_tokens=70,\n        max_conditioning_inputs=1,\n        code_stride_len=1024,\n        number_text_tokens=256,\n        num_audio_tokens=8194,\n        start_audio_token=8192,\n        stop_audio_token=8193,\n        train_solo_embeddings=False,\n        checkpointing=False,\n        average_conditioning_embeddings=False,\n        label_smoothing=0.0,\n        use_perceiver_resampler=False,\n        perceiver_cond_length_compression=256,\n    ):\n        \"\"\"\n        Args:\n\n        \"\"\"\n        super().__init__()\n\n        self.label_smoothing = label_smoothing\n        self.number_text_tokens = number_text_tokens\n        self.start_text_token = start_text_token\n        self.stop_text_token = stop_text_token\n        self.num_audio_tokens = num_audio_tokens\n        self.start_audio_token = start_audio_token\n        self.stop_audio_token = stop_audio_token\n        self.start_prompt_token = start_audio_token\n        self.stop_prompt_token = stop_audio_token\n        self.layers = layers\n        self.heads = heads\n        self.model_dim = model_dim\n        self.max_conditioning_inputs = max_conditioning_inputs\n        self.max_gen_mel_tokens = max_mel_tokens - self.max_conditioning_inputs - 2\n        self.max_mel_tokens = -1 if max_mel_tokens == -1 else max_mel_tokens + 2 + self.max_conditioning_inputs\n        self.max_text_tokens = -1 if max_text_tokens == -1 else max_text_tokens + 2\n        self.max_prompt_tokens = max_prompt_tokens\n        self.code_stride_len = code_stride_len\n        self.conditioning_encoder = ConditioningEncoder(80, model_dim, num_attn_heads=heads)\n        self.conditioning_dropout = nn.Dropout1d(0.1)\n        self.average_conditioning_embeddings = average_conditioning_embeddings\n        self.use_perceiver_resampler = use_perceiver_resampler\n        self.perceiver_cond_length_compression = perceiver_cond_length_compression\n\n        self.text_embedding = nn.Embedding(self.number_text_tokens, model_dim)\n        self.mel_embedding = nn.Embedding(self.num_audio_tokens, model_dim)\n\n        (\n            self.gpt,\n            self.mel_pos_embedding,\n            self.text_pos_embedding,\n            self.mel_layer_pos_embedding,\n            self.text_layer_pos_embedding,\n        ) = build_hf_gpt_transformer(\n            layers,\n            model_dim,\n            heads,\n            self.max_mel_tokens,\n            self.max_text_tokens,\n            self.max_prompt_tokens,\n            checkpointing,\n        )\n        if train_solo_embeddings:\n            self.mel_solo_embedding = nn.Parameter(torch.randn(1, 1, model_dim) * 0.02, requires_grad=True)\n            self.text_solo_embedding = nn.Parameter(torch.randn(1, 1, model_dim) * 0.02, requires_grad=True)\n        else:\n            self.mel_solo_embedding = 0\n            self.text_solo_embedding = 0\n\n        self.final_norm = nn.LayerNorm(model_dim)\n        self.text_head = nn.Linear(model_dim, self.number_text_tokens)\n        self.mel_head = nn.Linear(model_dim, self.num_audio_tokens)\n\n        if self.use_perceiver_resampler:\n            # XTTS v2\n            self.conditioning_perceiver = PerceiverResampler(\n                dim=model_dim,\n                depth=2,\n                dim_context=model_dim,\n                num_latents=32,\n                dim_head=64,\n                heads=8,\n                ff_mult=4,\n                use_flash_attn=False,\n            )\n        else:\n            # XTTS v1\n            self.prompt_embedding = nn.Embedding(self.num_audio_tokens, model_dim)\n            self.prompt_pos_embedding = LearnedPositionEmbeddings(24 * 9, model_dim)\n\n    def get_grad_norm_parameter_groups(self):\n        return {\n            \"conditioning_encoder\": list(self.conditioning_encoder.parameters()),\n            \"conditioning_perceiver\": list(self.conditioning_perceiver.parameters())\n            if self.use_perceiver_resampler\n            else None,\n            \"gpt\": list(self.gpt.parameters()),\n            \"heads\": list(self.text_head.parameters()) + list(self.mel_head.parameters()),\n        }\n\n    def init_gpt_for_inference(self, kv_cache=True, use_deepspeed=False):\n        seq_length = self.max_prompt_tokens + self.max_mel_tokens + self.max_text_tokens + 1\n        gpt_config = GPT2Config(\n            vocab_size=self.max_mel_tokens,\n            n_positions=seq_length,\n            n_ctx=seq_length,\n            n_embd=self.model_dim,\n            n_layer=self.layers,\n            n_head=self.heads,\n            gradient_checkpointing=False,\n            use_cache=True,\n        )\n        self.gpt_inference = GPT2InferenceModel(\n            gpt_config,\n            self.gpt,\n            self.mel_pos_embedding,\n            self.mel_embedding,\n            self.final_norm,\n            self.mel_head,\n            kv_cache=kv_cache,\n        )\n        self.gpt.wte = self.mel_embedding\n\n        if use_deepspeed:\n            import deepspeed\n\n            self.ds_engine = deepspeed.init_inference(\n                model=self.gpt_inference.half(),  # Transformers models\n                mp_size=1,  # Number of GPU\n                dtype=torch.float32,  # desired data type of output\n                replace_method=\"auto\",  # Lets DS autmatically identify the layer to replace\n                replace_with_kernel_inject=True,  # replace the model with the kernel injector\n            )\n            self.gpt_inference = self.ds_engine.module.eval()\n\n    def set_inputs_and_targets(self, input, start_token, stop_token):\n        inp = F.pad(input, (1, 0), value=start_token)\n        tar = F.pad(input, (0, 1), value=stop_token)\n        return inp, tar\n\n    def set_mel_padding(self, mel_input_tokens, code_lengths):\n        \"\"\"\n        Given mel tokens that are derived from a padded audio clip and the actual lengths of each batch element in\n        that audio clip, reformats the tokens with stop_audio_token in place of the zero padding. This is required\n        preformatting to create a working TTS model.\n        \"\"\"\n        # Set padding areas within MEL (currently it is coded with the MEL code for <zero>).\n        for b in range(len(code_lengths)):\n            actual_end = code_lengths[b]\n            if actual_end < mel_input_tokens.shape[-1]:\n                mel_input_tokens[b, actual_end:] = self.stop_audio_token\n        return mel_input_tokens\n\n    def get_logits(\n        self,\n        first_inputs,\n        first_head,\n        second_inputs=None,\n        second_head=None,\n        prompt=None,\n        get_attns=False,\n        return_latent=False,\n        attn_mask_cond=None,\n        attn_mask_text=None,\n        attn_mask_mel=None,\n    ):\n        if prompt is not None:\n            offset = prompt.shape[1]\n            if second_inputs is not None:\n                emb = torch.cat([prompt, first_inputs, second_inputs], dim=1)\n            else:\n                emb = torch.cat([prompt, first_inputs], dim=1)\n\n        # with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n        attn_mask = None\n        if attn_mask_text is not None:\n            attn_mask = torch.cat([attn_mask_text, attn_mask_mel], dim=1)\n            if prompt is not None:\n                attn_mask_cond = torch.ones(prompt.shape[0], offset, dtype=torch.bool, device=emb.device)\n                attn_mask = torch.cat([attn_mask_cond, attn_mask], dim=1)\n\n        gpt_out = self.gpt(\n            inputs_embeds=emb,\n            return_dict=True,\n            output_attentions=get_attns,\n            attention_mask=attn_mask,\n        )\n\n        if get_attns:\n            return gpt_out.attentions\n\n        enc = gpt_out.last_hidden_state[:, offset:]\n        enc = self.final_norm(enc)\n\n        if return_latent:\n            return enc[:, : first_inputs.shape[1]], enc[:, -second_inputs.shape[1] :]\n\n        first_logits = enc[:, : first_inputs.shape[1]]\n        first_logits = first_head(first_logits)\n        first_logits = first_logits.permute(0, 2, 1)\n        if second_inputs is not None:\n            second_logits = enc[:, -second_inputs.shape[1] :]\n            second_logits = second_head(second_logits)\n            second_logits = second_logits.permute(0, 2, 1)\n            return first_logits, second_logits\n        else:\n            return first_logits\n\n    def get_conditioning(self, speech_conditioning_input):\n        speech_conditioning_input = (\n            speech_conditioning_input.unsqueeze(1)\n            if len(speech_conditioning_input.shape) == 3\n            else speech_conditioning_input\n        )\n        conds = []\n        for j in range(speech_conditioning_input.shape[1]):\n            conds.append(self.conditioning_encoder(speech_conditioning_input[:, j]))\n        conds = torch.stack(conds, dim=1)\n        conds = conds.mean(dim=1)\n        return conds\n\n    def get_prompts(self, prompt_codes):\n        \"\"\"\n        Create a prompt from the mel codes. This is used to condition the model on the mel codes.\n        Pad the prompt with start and stop mel tokens.\n        \"\"\"\n        prompt = prompt_codes\n        if self.training:\n            lengths = []\n            # Compute the real prompt length based on the first encounter with the token 83 used for padding\n            for i in range(prompt_codes.shape[0]):\n                length = 0\n                for j in range(prompt_codes.shape[1]):\n                    if prompt_codes[i, j] == 83:\n                        break\n                    else:\n                        length += 1\n                lengths.append(length)\n\n            # prompt_len = random.randint(1, 9)  # in secs\n            prompt_len = 3\n            prompt_len = prompt_len * 24  # in frames\n            if prompt_codes.shape[-1] >= prompt_len:\n                for i in range(prompt_codes.shape[0]):\n                    if lengths[i] < prompt_len:\n                        start = 0\n                    else:\n                        start = random.randint(0, lengths[i] - prompt_len)\n                prompt = prompt_codes[:, start : start + prompt_len]\n\n        # add start and stop tokens\n        prompt = F.pad(prompt, (1, 0), value=self.start_prompt_token)\n        prompt = F.pad(prompt, (0, 1), value=self.stop_prompt_token)\n        return prompt\n\n    def get_style_emb(self, cond_input, return_latent=False):\n        \"\"\"\n        cond_input: (b, 80, s) or (b, 1, 80, s)\n        conds: (b, 1024, s)\n        \"\"\"\n        conds = None\n        if not return_latent:\n            if cond_input.ndim == 4:\n                cond_input = cond_input.squeeze(1)\n            conds = self.conditioning_encoder(cond_input)  # (b, d, s)\n            if self.use_perceiver_resampler:\n                conds = self.conditioning_perceiver(conds.permute(0, 2, 1)).transpose(1, 2)  # (b, d, 32)\n        else:\n            # already computed\n            conds = cond_input.unsqueeze(1)\n        return conds\n\n    def forward(\n        self,\n        text_inputs,\n        text_lengths,\n        audio_codes,\n        wav_lengths,\n        cond_mels=None,\n        cond_idxs=None,\n        cond_lens=None,\n        cond_latents=None,\n        return_attentions=False,\n        return_latent=False,\n    ):\n        \"\"\"\n        Forward pass that uses both text and voice in either text conditioning mode or voice conditioning mode\n        (actuated by `text_first`).\n\n        text_inputs: long tensor, (b,t)\n        text_lengths: long tensor, (b,)\n        mel_inputs:  long tensor, (b,m)\n        wav_lengths: long tensor, (b,)\n        cond_mels: MEL float tensor, (b, 1, 80,s)\n        cond_idxs: cond start and end indexs, (b, 2)\n\n        If return_attentions is specified, only logits are returned.\n        If return_latent is specified, loss & logits are not computed or returned. Only the predicted latents are returned.\n        \"\"\"\n        # \u2757 FIXIT\n        if self.max_conditioning_inputs == 0:\n            assert cond_mels is None, \" \u2757 cond_mels is not None, but max_conditioning_inputs == 0\"\n\n        max_text_len = text_lengths.max()\n        code_lengths = torch.ceil(wav_lengths / self.code_stride_len).long() + 3\n\n        if cond_lens is not None:\n            if self.use_perceiver_resampler:\n                cond_lens = cond_lens // self.perceiver_cond_length_compression\n            else:\n                cond_lens = cond_lens // self.code_stride_len\n\n        if cond_idxs is not None:\n            # recompute cond idxs for mel lengths\n            for idx in range(cond_idxs.size(0)):\n                if self.use_perceiver_resampler:\n                    cond_idxs[idx] = cond_idxs[idx] // self.perceiver_cond_length_compression\n                else:\n                    cond_idxs[idx] = cond_idxs[idx] // self.code_stride_len\n\n        # ensure that the cond_mel does not have padding\n        # if cond_lens is not None and cond_idxs is None:\n        #     min_cond_len = torch.min(cond_lens)\n        #     cond_mels = cond_mels[:, :, :, :min_cond_len]\n\n        # If len(codes) + 3 is larger than maxiumum allowed length, we truncate the codes.\n        max_mel_len = code_lengths.max()\n\n        if max_mel_len > audio_codes.shape[-1]:\n            audio_codes = F.pad(audio_codes, (0, max_mel_len - audio_codes.shape[-1]))\n\n        # \ud83d\udc96 Lovely assertions\n        assert (\n            max_mel_len <= audio_codes.shape[-1]\n        ), f\" \u2757 max_mel_len ({max_mel_len}) > audio_codes.shape[-1] ({audio_codes.shape[-1]})\"\n        assert (\n            max_text_len <= text_inputs.shape[-1]\n        ), f\" \u2757 max_text_len ({max_text_len}) > text_inputs.shape[-1] ({text_inputs.shape[-1]})\"\n\n        # Append stop token to text inputs\n        text_inputs = F.pad(text_inputs[:, :max_text_len], (0, 1), value=self.stop_text_token)\n\n        # Append silence token to mel codes\n        audio_codes = F.pad(audio_codes[:, :max_mel_len], (0, 1), value=self.stop_audio_token)\n\n        # Pad mel codes with stop_audio_token\n        audio_codes = self.set_mel_padding(\n            audio_codes, code_lengths - 3\n        )  # -3 to get the real code lengths without consider start and stop tokens that was not added yet\n\n        # Build input and target tensors\n        # Prepend start token to inputs and append stop token to targets\n        text_inputs, text_targets = self.set_inputs_and_targets(\n            text_inputs, self.start_text_token, self.stop_text_token\n        )\n        audio_codes, mel_targets = self.set_inputs_and_targets(\n            audio_codes, self.start_audio_token, self.stop_audio_token\n        )\n\n        # Set attn_mask\n        attn_mask_cond = None\n        attn_mask_text = None\n        attn_mask_mel = None\n        if not return_latent:\n            attn_mask_cond = torch.ones(\n                cond_mels.shape[0],\n                cond_mels.shape[-1],\n                dtype=torch.bool,\n                device=text_inputs.device,\n            )\n            attn_mask_text = torch.ones(\n                text_inputs.shape[0],\n                text_inputs.shape[1],\n                dtype=torch.bool,\n                device=text_inputs.device,\n            )\n            attn_mask_mel = torch.ones(\n                audio_codes.shape[0],\n                audio_codes.shape[1],\n                dtype=torch.bool,\n                device=audio_codes.device,\n            )\n\n            if cond_idxs is not None:\n                # use masking approach\n                for idx, r in enumerate(cond_idxs):\n                    l = r[1] - r[0]\n                    attn_mask_cond[idx, l:] = 0.0\n            elif cond_lens is not None:\n                for idx, l in enumerate(cond_lens):\n                    attn_mask_cond[idx, l:] = 0.0\n\n            for idx, l in enumerate(text_lengths):\n                attn_mask_text[idx, l + 1 :] = 0.0\n\n            for idx, l in enumerate(code_lengths):\n                attn_mask_mel[idx, l + 1 :] = 0.0\n\n        # Compute text embeddings + positional embeddings\n        text_emb = self.text_embedding(text_inputs) + self.text_pos_embedding(text_inputs)\n\n        # Compute mel embeddings + positional embeddings\n        mel_emb = self.mel_embedding(audio_codes) + self.mel_pos_embedding(audio_codes)\n\n        # Compute speech conditioning input\n        if cond_latents is None:\n            cond_latents = self.get_style_emb(cond_mels).transpose(1, 2)\n\n        # Get logits\n        sub = -5  # don't ask me why \ud83d\ude04\n        if self.training:\n            sub = -1\n\n        text_logits, mel_logits = self.get_logits(\n            text_emb,\n            self.text_head,\n            mel_emb,\n            self.mel_head,\n            prompt=cond_latents,\n            get_attns=return_attentions,\n            return_latent=return_latent,\n            attn_mask_cond=attn_mask_cond,\n            attn_mask_text=attn_mask_text,\n            attn_mask_mel=attn_mask_mel,\n        )\n        if return_latent:\n            return mel_logits[:, :sub]  # sub to prevent bla.\n\n        if return_attentions:\n            return mel_logits\n\n        # Set paddings to -1 to ignore them in loss\n        for idx, l in enumerate(text_lengths):\n            text_targets[idx, l + 1 :] = -1\n\n        for idx, l in enumerate(code_lengths):\n            mel_targets[idx, l + 1 :] = -1\n\n        # check if stoptoken is in every row of mel_targets\n        assert (mel_targets == self.stop_audio_token).sum() >= mel_targets.shape[\n            0\n        ], f\" \u2757 mel_targets does not contain stop token ({self.stop_audio_token}) in every row.\"\n\n        # ignore the loss for the segment used for conditioning\n        # coin flip for the segment to be ignored\n        if cond_idxs is not None:\n            cond_start = cond_idxs[idx, 0]\n            cond_end = cond_idxs[idx, 1]\n            mel_targets[idx, cond_start:cond_end] = -1\n\n        # Compute losses\n        loss_text = F.cross_entropy(\n            text_logits, text_targets.long(), ignore_index=-1, label_smoothing=self.label_smoothing\n        )\n        loss_mel = F.cross_entropy(\n            mel_logits, mel_targets.long(), ignore_index=-1, label_smoothing=self.label_smoothing\n        )\n        return loss_text.mean(), loss_mel.mean(), mel_logits\n\n    def inference(self, cond_latents, text_inputs, **hf_generate_kwargs):\n        self.compute_embeddings(cond_latents, text_inputs)\n        return self.generate(cond_latents, text_inputs, **hf_generate_kwargs)\n\n    def compute_embeddings(\n        self,\n        cond_latents,\n        text_inputs,\n    ):\n        text_inputs = F.pad(text_inputs, (0, 1), value=self.stop_text_token)\n        text_inputs = F.pad(text_inputs, (1, 0), value=self.start_text_token)\n        emb = self.text_embedding(text_inputs) + self.text_pos_embedding(text_inputs)\n        emb = torch.cat([cond_latents, emb], dim=1)\n        self.gpt_inference.store_prefix_emb(emb)\n        gpt_inputs = torch.full(\n            (\n                emb.shape[0],\n                emb.shape[1] + 1,  # +1 for the start_audio_token\n            ),\n            fill_value=1,\n            dtype=torch.long,\n            device=text_inputs.device,\n        )\n        gpt_inputs[:, -1] = self.start_audio_token\n        return gpt_inputs\n\n    def generate(\n        self,\n        cond_latents,\n        text_inputs,\n        **hf_generate_kwargs,\n    ):\n        gpt_inputs = self.compute_embeddings(cond_latents, text_inputs)\n        gen = self.gpt_inference.generate(\n            gpt_inputs,\n            bos_token_id=self.start_audio_token,\n            pad_token_id=self.stop_audio_token,\n            eos_token_id=self.stop_audio_token,\n            max_length=self.max_gen_mel_tokens + gpt_inputs.shape[-1],\n            **hf_generate_kwargs,\n        )\n        if \"return_dict_in_generate\" in hf_generate_kwargs:\n            return gen.sequences[:, gpt_inputs.shape[1] :], gen\n        return gen[:, gpt_inputs.shape[1] :]\n\n    def get_generator(self, fake_inputs, **hf_generate_kwargs):\n        return self.gpt_inference.generate_stream(\n            fake_inputs,\n            bos_token_id=self.start_audio_token,\n            pad_token_id=self.stop_audio_token,\n            eos_token_id=self.stop_audio_token,\n            max_length=self.max_gen_mel_tokens + fake_inputs.shape[-1],\n            do_stream=True,\n            **hf_generate_kwargs,\n        )\n", "TTS/tts/layers/xtts/stream_generator.py": "# Adapted from: https://github.com/LowinLi/transformers-stream-generator\n\nimport copy\nimport inspect\nimport random\nimport warnings\nfrom typing import Callable, List, Optional, Union\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom torch import nn\nfrom transformers import (\n    BeamSearchScorer,\n    ConstrainedBeamSearchScorer,\n    DisjunctiveConstraint,\n    GenerationConfig,\n    GenerationMixin,\n    LogitsProcessorList,\n    PhrasalConstraint,\n    PreTrainedModel,\n    StoppingCriteriaList,\n)\nfrom transformers.generation.utils import GenerateOutput, SampleOutput, logger\n\n\ndef setup_seed(seed):\n    if seed == -1:\n        return\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nclass StreamGenerationConfig(GenerationConfig):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.do_stream = kwargs.pop(\"do_stream\", False)\n\n\nclass NewGenerationMixin(GenerationMixin):\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        generation_config: Optional[StreamGenerationConfig] = None,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n        synced_gpus: Optional[bool] = False,\n        seed=0,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        r\"\"\"\n\n        Generates sequences of token ids for models with a language modeling head.\n\n        <Tip warning={true}>\n\n        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n                should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n            generation_config (`~generation.GenerationConfig`, *optional*):\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                passed to generate matching the attributes of `generation_config` will override them. If\n                `generation_config` is not provided, the default will be used, which had the following loading\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n                default values, whose documentation should be checked to parameterize generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                Custom logits processors that complement the default logits processors built from arguments and\n                generation config. If a logit processor is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                generation config. If a stopping criteria is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            kwargs:\n                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n\n        Return:\n            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n\n                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n                [`~utils.ModelOutput`] types are:\n\n                    - [`~generation.GreedySearchDecoderOnlyOutput`],\n                    - [`~generation.SampleDecoderOnlyOutput`],\n                    - [`~generation.BeamSearchDecoderOnlyOutput`],\n                    - [`~generation.BeamSampleDecoderOnlyOutput`]\n\n                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n                [`~utils.ModelOutput`] types are:\n\n                    - [`~generation.GreedySearchEncoderDecoderOutput`],\n                    - [`~generation.SampleEncoderDecoderOutput`],\n                    - [`~generation.BeamSearchEncoderDecoderOutput`],\n                    - [`~generation.BeamSampleEncoderDecoderOutput`]\n        \"\"\"\n        # setup_seed(seed)\n        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n        self._validate_model_class()\n\n        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n        if generation_config is None:\n            # legacy: users may modify the model configuration to control generation -- update the generation config\n            # model attribute accordingly, if it was created from the model config\n            if self.generation_config._from_model_config:\n                new_generation_config = StreamGenerationConfig.from_model_config(self.config)\n                if new_generation_config != self.generation_config:\n                    warnings.warn(\n                        \"You have modified the pretrained model configuration to control generation. This is a\"\n                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n                        \" Please use a generation configuration file (see\"\n                        \" https://huggingface.co/docs/transformers/main_classes/text_generation)\"\n                    )\n                    self.generation_config = new_generation_config\n            generation_config = self.generation_config\n\n        generation_config = copy.deepcopy(generation_config)\n        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n        # self._validate_model_kwargs(model_kwargs.copy())\n\n        # 2. Set generation parameters if not already defined\n        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n\n        if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n            if model_kwargs.get(\"attention_mask\", None) is None:\n                logger.warning(\n                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n                )\n            eos_token_id = generation_config.eos_token_id\n            if isinstance(eos_token_id, list):\n                eos_token_id = eos_token_id[0]\n            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n            generation_config.pad_token_id = eos_token_id\n\n        # 3. Define model inputs\n        # inputs_tensor has to be defined\n        # model_input_name is defined if model-specific keyword input is passed\n        # otherwise model_input_name is None\n        # all model-specific keyword inputs are removed from `model_kwargs`\n        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n            inputs, generation_config.bos_token_id, model_kwargs\n        )\n        batch_size = inputs_tensor.shape[0]\n\n        # 4. Define other model kwargs\n        model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n        model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n        model_kwargs[\"use_cache\"] = generation_config.use_cache\n\n        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n\n        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n                inputs_tensor,\n                generation_config.pad_token_id,\n                generation_config.eos_token_id,\n            )\n\n        # decoder-only models should use left-padding for generation\n        if not self.config.is_encoder_decoder:\n            if (\n                generation_config.pad_token_id is not None\n                and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\n            ):\n                logger.warning(\n                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n                )\n\n        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n            # if model is encoder decoder encoder_outputs are created\n            # and added to `model_kwargs`\n            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n                inputs_tensor, model_kwargs, model_input_name\n            )\n\n        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n        if self.config.is_encoder_decoder:\n            input_ids = self._prepare_decoder_input_ids_for_generation(\n                batch_size,\n                decoder_start_token_id=generation_config.decoder_start_token_id,\n                bos_token_id=generation_config.bos_token_id,\n                model_kwargs=model_kwargs,\n                device=inputs_tensor.device,\n            )\n        else:\n            # if decoder-only then inputs_tensor has to be `input_ids`\n            input_ids = inputs_tensor\n\n        # 6. Prepare `max_length` depending on other stopping criteria.\n        input_ids_seq_length = input_ids.shape[-1]\n        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n        if has_default_max_length and generation_config.max_new_tokens is None:\n            warnings.warn(\n                \"Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to\"\n                f\" {generation_config.max_length} (`generation_config.max_length`). Controlling `max_length` via the\"\n                \" config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we\"\n                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n                UserWarning,\n            )\n        elif has_default_max_length and generation_config.max_new_tokens is not None:\n            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n        elif not has_default_max_length and generation_config.max_new_tokens is not None:\n            raise ValueError(\n                \"Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a\"\n                \" limit to the generated output length. Remove one of those arguments. Please refer to the\"\n                \" documentation for more information. \"\n                \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n            )\n\n        if generation_config.min_length is not None and generation_config.min_length > generation_config.max_length:\n            raise ValueError(\n                f\"Unfeasible length constraints: the minimum length ({generation_config.min_length}) is larger than\"\n                f\" the maximum length ({generation_config.max_length})\"\n            )\n        if input_ids_seq_length >= generation_config.max_length:\n            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n            logger.warning(\n                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n                \" increasing `max_new_tokens`.\"\n            )\n\n        # 7. determine generation mode\n        is_constraint_gen_mode = (\n            generation_config.constraints is not None or generation_config.force_words_ids is not None\n        )\n\n        is_contrastive_search_gen_mode = (\n            generation_config.top_k is not None\n            and generation_config.top_k > 1\n            and generation_config.do_sample is False\n            and generation_config.penalty_alpha is not None\n            and generation_config.penalty_alpha > 0\n        )\n\n        is_greedy_gen_mode = (\n            (generation_config.num_beams == 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is False\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_sample_gen_mode = (\n            (generation_config.num_beams == 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is True\n            and generation_config.do_stream is False\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_sample_gen_stream_mode = (\n            (generation_config.num_beams == 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_stream is True\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_beam_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is False\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_beam_sample_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is True\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_group_beam_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups > 1)\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n\n        if generation_config.num_beam_groups > generation_config.num_beams:\n            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n        if is_group_beam_gen_mode and generation_config.do_sample is True:\n            raise ValueError(\n                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n            )\n\n        if self.device.type != input_ids.device.type:\n            warnings.warn(\n                \"You are calling .generate() with the `input_ids` being on a device type different\"\n                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n                \" Please make sure that you have put `input_ids` to the\"\n                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n                \" running `.generate()`.\",\n                UserWarning,\n            )\n        # 8. prepare distribution pre_processing samplers\n        logits_processor = self._get_logits_processor(\n            generation_config=generation_config,\n            input_ids_seq_length=input_ids_seq_length,\n            encoder_input_ids=inputs_tensor,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            logits_processor=logits_processor,\n        )\n\n        # 9. prepare stopping criteria\n        stopping_criteria = self._get_stopping_criteria(\n            generation_config=generation_config, stopping_criteria=stopping_criteria\n        )\n        # 10. go into different generation modes\n        if is_greedy_gen_mode:\n            if generation_config.num_return_sequences > 1:\n                raise ValueError(\n                    f\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\n                    \" greedy search.\"\n                )\n\n            # 11. run greedy search\n            return self.greedy_search(\n                input_ids,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n        elif is_contrastive_search_gen_mode:\n            if generation_config.num_return_sequences > 1:\n                raise ValueError(\n                    f\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\n                    \" contrastive search.\"\n                )\n\n            return self.contrastive_search(\n                input_ids,\n                top_k=generation_config.top_k,\n                penalty_alpha=generation_config.penalty_alpha,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n        elif is_sample_gen_mode:\n            # 11. prepare logits warper\n            logits_warper = self._get_logits_warper(generation_config)\n\n            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_return_sequences,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n\n            # 13. run sample\n            return self.sample(\n                input_ids,\n                logits_processor=logits_processor,\n                logits_warper=logits_warper,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n        elif is_sample_gen_stream_mode:\n            # 11. prepare logits warper\n            logits_warper = self._get_logits_warper(generation_config)\n\n            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_return_sequences,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n\n            # 13. run sample\n            return self.sample_stream(\n                input_ids,\n                logits_processor=logits_processor,\n                logits_warper=logits_warper,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n        elif is_beam_gen_mode:\n            if generation_config.num_return_sequences > generation_config.num_beams:\n                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n\n            if stopping_criteria.max_length is None:\n                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n\n            # 11. prepare beam search scorer\n            beam_scorer = BeamSearchScorer(\n                batch_size=batch_size,\n                num_beams=generation_config.num_beams,\n                device=inputs_tensor.device,\n                length_penalty=generation_config.length_penalty,\n                do_early_stopping=generation_config.early_stopping,\n                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n            )\n            # 12. interleave input_ids with `num_beams` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_beams,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n            # 13. run beam search\n            return self.beam_search(\n                input_ids,\n                beam_scorer,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n        elif is_beam_sample_gen_mode:\n            # 11. prepare logits warper\n            logits_warper = self._get_logits_warper(generation_config)\n\n            if stopping_criteria.max_length is None:\n                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n            # 12. prepare beam search scorer\n            beam_scorer = BeamSearchScorer(\n                batch_size=batch_size * generation_config.num_return_sequences,\n                num_beams=generation_config.num_beams,\n                device=inputs_tensor.device,\n                length_penalty=generation_config.length_penalty,\n                do_early_stopping=generation_config.early_stopping,\n            )\n\n            # 13. interleave input_ids with `num_beams` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_beams * generation_config.num_return_sequences,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n\n            # 14. run beam sample\n            return self.beam_sample(\n                input_ids,\n                beam_scorer,\n                logits_processor=logits_processor,\n                logits_warper=logits_warper,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n        elif is_group_beam_gen_mode:\n            if generation_config.num_return_sequences > generation_config.num_beams:\n                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n\n            if generation_config.num_beams % generation_config.num_beam_groups != 0:\n                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n\n            if stopping_criteria.max_length is None:\n                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n\n            has_default_typical_p = kwargs.get(\"typical_p\") is None and generation_config.typical_p == 1.0\n            if not has_default_typical_p:\n                raise ValueError(\"Decoder argument `typical_p` is not supported with beam groups.\")\n\n            # 11. prepare beam search scorer\n            beam_scorer = BeamSearchScorer(\n                batch_size=batch_size,\n                num_beams=generation_config.num_beams,\n                max_length=stopping_criteria.max_length,\n                device=inputs_tensor.device,\n                length_penalty=generation_config.length_penalty,\n                do_early_stopping=generation_config.early_stopping,\n                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n                num_beam_groups=generation_config.num_beam_groups,\n            )\n            # 12. interleave input_ids with `num_beams` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_beams,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n            # 13. run beam search\n            return self.group_beam_search(\n                input_ids,\n                beam_scorer,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n        elif is_constraint_gen_mode:\n            if generation_config.num_return_sequences > generation_config.num_beams:\n                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n\n            if stopping_criteria.max_length is None:\n                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n\n            if generation_config.num_beams <= 1:\n                raise ValueError(\"`num_beams` needs to be greater than 1 for constrained generation.\")\n\n            if generation_config.do_sample:\n                raise ValueError(\"`do_sample` needs to be false for constrained generation.\")\n\n            if generation_config.num_beam_groups is not None and generation_config.num_beam_groups > 1:\n                raise ValueError(\"`num_beam_groups` not supported yet for constrained generation.\")\n\n            final_constraints = []\n            if generation_config.constraints is not None:\n                final_constraints = generation_config.constraints\n\n            if generation_config.force_words_ids is not None:\n\n                def typeerror():\n                    raise ValueError(\n                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]`\"\n                        f\"of positive integers, but is {generation_config.force_words_ids}.\"\n                    )\n\n                if (\n                    not isinstance(generation_config.force_words_ids, list)\n                    or len(generation_config.force_words_ids) == 0\n                ):\n                    typeerror()\n\n                for word_ids in generation_config.force_words_ids:\n                    if isinstance(word_ids[0], list):\n                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n                            typeerror()\n                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n                            typeerror()\n                        if any(\n                            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n                            for token_ids in word_ids\n                        ):\n                            typeerror()\n\n                        constraint = DisjunctiveConstraint(word_ids)\n                    else:\n                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n                            typeerror()\n                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n                            typeerror()\n\n                        constraint = PhrasalConstraint(word_ids)\n                    final_constraints.append(constraint)\n\n            # 11. prepare beam search scorer\n            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n                constraints=final_constraints,\n                batch_size=batch_size,\n                num_beams=generation_config.num_beams,\n                device=inputs_tensor.device,\n                length_penalty=generation_config.length_penalty,\n                do_early_stopping=generation_config.early_stopping,\n                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n            )\n            # 12. interleave input_ids with `num_beams` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_beams,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n            # 13. run beam search\n            return self.constrained_beam_search(\n                input_ids,\n                constrained_beam_scorer=constrained_beam_scorer,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                pad_token_id=generation_config.pad_token_id,\n                eos_token_id=generation_config.eos_token_id,\n                output_scores=generation_config.output_scores,\n                return_dict_in_generate=generation_config.return_dict_in_generate,\n                synced_gpus=synced_gpus,\n                **model_kwargs,\n            )\n\n    @torch.no_grad()\n    def sample_stream(\n        self,\n        input_ids: torch.LongTensor,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        logits_warper: Optional[LogitsProcessorList] = None,\n        max_length: Optional[int] = None,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[Union[int, List[int]]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_scores: Optional[bool] = None,\n        return_dict_in_generate: Optional[bool] = None,\n        synced_gpus: Optional[bool] = False,\n        **model_kwargs,\n    ) -> Union[SampleOutput, torch.LongTensor]:\n        r\"\"\"\n        Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n\n        <Tip warning={true}>\n\n        In most cases, you do not need to call [`~generation.GenerationMixin.sample`] directly. Use generate() instead.\n        For an overview of generation strategies and code examples, check the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                The sequence used as a prompt for the generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n                used to modify the prediction scores of the language modeling head applied at each generation step.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n                used to tell if the generation loop should stop.\n            logits_warper (`LogitsProcessorList`, *optional*):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n                to warp the prediction score distribution of the language modeling head applied before multinomial\n                sampling at each generation step.\n            max_length (`int`, *optional*, defaults to 20):\n                **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n                tokens. The maximum length of the sequence to be generated.\n            pad_token_id (`int`, *optional*):\n                The id of the *padding* token.\n            eos_token_id (`int`, *optional*):\n                The id of the *end-of-sequence* token.\n            output_attentions (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more details.\n            output_hidden_states (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more details.\n            output_scores (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            model_kwargs:\n                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n                an encoder-decoder model the kwargs should include `encoder_outputs`.\n\n        Return:\n            [`~generation.SampleDecoderOnlyOutput`], [`~generation.SampleEncoderDecoderOutput`] or `torch.LongTensor`:\n            A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n            [`~generation.SampleDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n            `return_dict_in_generate=True` or a [`~generation.SampleEncoderDecoderOutput`] if\n            `model.config.is_encoder_decoder=True`.\n\n        Examples:\n\n        ```python\n        >>> from transformers import (\n        ...     AutoTokenizer,\n        ...     AutoModelForCausalLM,\n        ...     LogitsProcessorList,\n        ...     MinLengthLogitsProcessor,\n        ...     TopKLogitsWarper,\n        ...     TemperatureLogitsWarper,\n        ...     StoppingCriteriaList,\n        ...     MaxLengthCriteria,\n        ... )\n        >>> import torch\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n        >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n        >>> model.config.pad_token_id = model.config.eos_token_id\n        >>> model.generation_config.pad_token_id = model.config.eos_token_id\n\n        >>> input_prompt = \"Today is a beautiful day, and\"\n        >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n\n        >>> # instantiate logits processors\n        >>> logits_processor = LogitsProcessorList(\n        ...     [\n        ...         MinLengthLogitsProcessor(15, eos_token_id=model.generation_config.eos_token_id),\n        ...     ]\n        ... )\n        >>> # instantiate logits processors\n        >>> logits_warper = LogitsProcessorList(\n        ...     [\n        ...         TopKLogitsWarper(50),\n        ...         TemperatureLogitsWarper(0.7),\n        ...     ]\n        ... )\n\n        >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n\n        >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n        >>> outputs = model.sample(\n        ...     input_ids,\n        ...     logits_processor=logits_processor,\n        ...     logits_warper=logits_warper,\n        ...     stopping_criteria=stopping_criteria,\n        ... )\n\n        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        ['Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the']\n        ```\"\"\"\n        # init values\n        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n        if max_length is not None:\n            warnings.warn(\n                \"`max_length` is deprecated in this function, use\"\n                \" `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\",\n                UserWarning,\n            )\n            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()\n        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n        output_attentions = (\n            output_attentions if output_attentions is not None else self.generation_config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n        )\n        return_dict_in_generate = (\n            return_dict_in_generate\n            if return_dict_in_generate is not None\n            else self.generation_config.return_dict_in_generate\n        )\n\n        # init attention / hidden states / scores tuples\n        scores = () if (return_dict_in_generate and output_scores) else None\n        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n\n        # keep track of which sequences are already finished\n        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n\n        this_peer_finished = False  # used by synced_gpus only\n        # auto-regressive generation\n        while True:\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n\n            # prepare model inputs\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\n            # forward pass to get next token\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n\n            if synced_gpus and this_peer_finished:\n                continue  # don't waste resources running the code we don't need\n\n            next_token_logits = outputs.logits[:, -1, :]\n\n            # pre-process distribution\n            next_token_scores = logits_processor(input_ids, next_token_logits)\n            next_token_scores = logits_warper(input_ids, next_token_scores)\n\n            # Store scores, attentions and hidden_states when required\n            if return_dict_in_generate:\n                if output_scores:\n                    scores += (next_token_scores,)\n                if output_attentions:\n                    decoder_attentions += (\n                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n                    )\n                    if self.config.is_encoder_decoder:\n                        cross_attentions += (outputs.cross_attentions,)\n\n                if output_hidden_states:\n                    decoder_hidden_states += (\n                        (outputs.decoder_hidden_states,) if self.config.is_encoder_decoder else (outputs.hidden_states,)\n                    )\n\n            # sample\n            probs = nn.functional.softmax(next_token_scores, dim=-1)\n            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n            # finished sentences should have their next token be a padding token\n            if eos_token_id is not None:\n                if pad_token_id is None:\n                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n            yield next_tokens, self.final_norm(outputs.hidden_states[-1][:, -1])\n            # update generated ids, model inputs, and length for next step\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n\n            # if eos_token was found in one sentence, set sentence to finished\n            if eos_token_id is not None:\n                unfinished_sequences = unfinished_sequences.mul((sum(next_tokens != i for i in eos_token_id)).long())\n\n            # stop when each sentence is finished, or if we exceed the maximum length\n            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n\n\ndef init_stream_support():\n    \"\"\"Overload PreTrainedModel for streaming.\"\"\"\n    PreTrainedModel.generate_stream = NewGenerationMixin.generate\n    PreTrainedModel.sample_stream = NewGenerationMixin.sample_stream\n\n\nif __name__ == \"__main__\":\n    from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n\n    PreTrainedModel.generate = NewGenerationMixin.generate\n    PreTrainedModel.sample_stream = NewGenerationMixin.sample_stream\n    model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", torch_dtype=torch.float16)\n\n    tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n    model = model.to(\"cuda:0\")\n    model = model.eval()\n    prompt_text = \"hello? \\n\"\n    input_ids = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).input_ids\n    input_ids = input_ids.to(\"cuda:0\")\n\n    with torch.no_grad():\n        result = model.generate(\n            input_ids,\n            max_new_tokens=200,\n            do_sample=True,\n            top_k=30,\n            top_p=0.85,\n            temperature=0.35,\n            repetition_penalty=1.2,\n            early_stopping=True,\n            seed=0,\n        )\n        print(tokenizer.decode(result, skip_special_tokens=True))\n        generator = model.generate(\n            input_ids,\n            max_new_tokens=200,\n            do_sample=True,\n            top_k=30,\n            top_p=0.85,\n            temperature=0.35,\n            repetition_penalty=1.2,\n            early_stopping=True,\n            seed=0,\n            do_stream=True,\n        )\n        stream_result = \"\"\n        for x in generator:\n            chunk = tokenizer.decode(x, skip_special_tokens=True)\n            stream_result += chunk\n        print(stream_result)\n", "TTS/tts/layers/xtts/dvae.py": "import functools\nfrom math import sqrt\n\nimport torch\nimport torch.distributed as distributed\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom einops import rearrange\n\n\ndef default(val, d):\n    return val if val is not None else d\n\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n\n    return inner\n\n\ndef dvae_wav_to_mel(\n    wav, mel_norms_file=\"../experiments/clips_mel_norms.pth\", mel_norms=None, device=torch.device(\"cpu\")\n):\n    mel_stft = torchaudio.transforms.MelSpectrogram(\n        n_fft=1024,\n        hop_length=256,\n        win_length=1024,\n        power=2,\n        normalized=False,\n        sample_rate=22050,\n        f_min=0,\n        f_max=8000,\n        n_mels=80,\n        norm=\"slaney\",\n    ).to(device)\n    wav = wav.to(device)\n    mel = mel_stft(wav)\n    mel = torch.log(torch.clamp(mel, min=1e-5))\n    if mel_norms is None:\n        mel_norms = torch.load(mel_norms_file, map_location=device)\n    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)\n    return mel\n\n\nclass Quantize(nn.Module):\n    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5, balancing_heuristic=False, new_return_order=False):\n        super().__init__()\n\n        self.dim = dim\n        self.n_embed = n_embed\n        self.decay = decay\n        self.eps = eps\n\n        self.balancing_heuristic = balancing_heuristic\n        self.codes = None\n        self.max_codes = 64000\n        self.codes_full = False\n        self.new_return_order = new_return_order\n\n        embed = torch.randn(dim, n_embed)\n        self.register_buffer(\"embed\", embed)\n        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n        self.register_buffer(\"embed_avg\", embed.clone())\n\n    def forward(self, input, return_soft_codes=False):\n        if self.balancing_heuristic and self.codes_full:\n            h = torch.histc(self.codes, bins=self.n_embed, min=0, max=self.n_embed) / len(self.codes)\n            mask = torch.logical_or(h > 0.9, h < 0.01).unsqueeze(1)\n            ep = self.embed.permute(1, 0)\n            ea = self.embed_avg.permute(1, 0)\n            rand_embed = torch.randn_like(ep) * mask\n            self.embed = (ep * ~mask + rand_embed).permute(1, 0)\n            self.embed_avg = (ea * ~mask + rand_embed).permute(1, 0)\n            self.cluster_size = self.cluster_size * ~mask.squeeze()\n            if torch.any(mask):\n                print(f\"Reset {torch.sum(mask)} embedding codes.\")\n                self.codes = None\n                self.codes_full = False\n\n        flatten = input.reshape(-1, self.dim)\n        dist = flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True)\n        soft_codes = -dist\n        _, embed_ind = soft_codes.max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n        quantize = self.embed_code(embed_ind)\n\n        if self.balancing_heuristic:\n            if self.codes is None:\n                self.codes = embed_ind.flatten()\n            else:\n                self.codes = torch.cat([self.codes, embed_ind.flatten()])\n                if len(self.codes) > self.max_codes:\n                    self.codes = self.codes[-self.max_codes :]\n                    self.codes_full = True\n\n        if self.training:\n            embed_onehot_sum = embed_onehot.sum(0)\n            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n\n            if distributed.is_initialized() and distributed.get_world_size() > 1:\n                distributed.all_reduce(embed_onehot_sum)\n                distributed.all_reduce(embed_sum)\n\n            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n            n = self.cluster_size.sum()\n            cluster_size = (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n            self.embed.data.copy_(embed_normalized)\n\n        diff = (quantize.detach() - input).pow(2).mean()\n        quantize = input + (quantize - input).detach()\n\n        if return_soft_codes:\n            return quantize, diff, embed_ind, soft_codes.view(input.shape[:-1] + (-1,))\n        elif self.new_return_order:\n            return quantize, embed_ind, diff\n        else:\n            return quantize, diff, embed_ind\n\n    def embed_code(self, embed_id):\n        return F.embedding(embed_id, self.embed.transpose(0, 1))\n\n\n# Fits a soft-discretized input to a normal-PDF across the specified dimension.\n# In other words, attempts to force the discretization function to have a mean equal utilization across all discrete\n# values with the specified expected variance.\nclass DiscretizationLoss(nn.Module):\n    def __init__(self, discrete_bins, dim, expected_variance, store_past=0):\n        super().__init__()\n        self.discrete_bins = discrete_bins\n        self.dim = dim\n        self.dist = torch.distributions.Normal(0, scale=expected_variance)\n        if store_past > 0:\n            self.record_past = True\n            self.register_buffer(\"accumulator_index\", torch.zeros(1, dtype=torch.long, device=\"cpu\"))\n            self.register_buffer(\"accumulator_filled\", torch.zeros(1, dtype=torch.long, device=\"cpu\"))\n            self.register_buffer(\"accumulator\", torch.zeros(store_past, discrete_bins))\n        else:\n            self.record_past = False\n\n    def forward(self, x):\n        other_dims = set(range(len(x.shape))) - set([self.dim])\n        averaged = x.sum(dim=tuple(other_dims)) / x.sum()\n        averaged = averaged - averaged.mean()\n\n        if self.record_past:\n            acc_count = self.accumulator.shape[0]\n            avg = averaged.detach().clone()\n            if self.accumulator_filled > 0:\n                averaged = torch.mean(self.accumulator, dim=0) * (acc_count - 1) / acc_count + averaged / acc_count\n\n            # Also push averaged into the accumulator.\n            self.accumulator[self.accumulator_index] = avg\n            self.accumulator_index += 1\n            if self.accumulator_index >= acc_count:\n                self.accumulator_index *= 0\n                if self.accumulator_filled <= 0:\n                    self.accumulator_filled += 1\n\n        return torch.sum(-self.dist.log_prob(averaged))\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, chan, conv, activation):\n        super().__init__()\n        self.net = nn.Sequential(\n            conv(chan, chan, 3, padding=1),\n            activation(),\n            conv(chan, chan, 3, padding=1),\n            activation(),\n            conv(chan, chan, 1),\n        )\n\n    def forward(self, x):\n        return self.net(x) + x\n\n\nclass UpsampledConv(nn.Module):\n    def __init__(self, conv, *args, **kwargs):\n        super().__init__()\n        assert \"stride\" in kwargs.keys()\n        self.stride = kwargs[\"stride\"]\n        del kwargs[\"stride\"]\n        self.conv = conv(*args, **kwargs)\n\n    def forward(self, x):\n        up = nn.functional.interpolate(x, scale_factor=self.stride, mode=\"nearest\")\n        return self.conv(up)\n\n\n# DiscreteVAE partially derived from lucidrains DALLE implementation\n# Credit: https://github.com/lucidrains/DALLE-pytorch\nclass DiscreteVAE(nn.Module):\n    def __init__(\n        self,\n        positional_dims=2,\n        num_tokens=512,\n        codebook_dim=512,\n        num_layers=3,\n        num_resnet_blocks=0,\n        hidden_dim=64,\n        channels=3,\n        stride=2,\n        kernel_size=4,\n        use_transposed_convs=True,\n        encoder_norm=False,\n        activation=\"relu\",\n        smooth_l1_loss=False,\n        straight_through=False,\n        normalization=None,  # ((0.5,) * 3, (0.5,) * 3),\n        record_codes=False,\n        discretization_loss_averaging_steps=100,\n        lr_quantizer_args={},\n    ):\n        super().__init__()\n        has_resblocks = num_resnet_blocks > 0\n\n        self.num_tokens = num_tokens\n        self.num_layers = num_layers\n        self.straight_through = straight_through\n        self.positional_dims = positional_dims\n        self.discrete_loss = DiscretizationLoss(\n            num_tokens, 2, 1 / (num_tokens * 2), discretization_loss_averaging_steps\n        )\n\n        assert positional_dims > 0 and positional_dims < 3  # This VAE only supports 1d and 2d inputs for now.\n        if positional_dims == 2:\n            conv = nn.Conv2d\n            conv_transpose = nn.ConvTranspose2d\n        else:\n            conv = nn.Conv1d\n            conv_transpose = nn.ConvTranspose1d\n        if not use_transposed_convs:\n            conv_transpose = functools.partial(UpsampledConv, conv)\n\n        if activation == \"relu\":\n            act = nn.ReLU\n        elif activation == \"silu\":\n            act = nn.SiLU\n        else:\n            assert NotImplementedError()\n\n        enc_layers = []\n        dec_layers = []\n\n        if num_layers > 0:\n            enc_chans = [hidden_dim * 2**i for i in range(num_layers)]\n            dec_chans = list(reversed(enc_chans))\n\n            enc_chans = [channels, *enc_chans]\n\n            dec_init_chan = codebook_dim if not has_resblocks else dec_chans[0]\n            dec_chans = [dec_init_chan, *dec_chans]\n\n            enc_chans_io, dec_chans_io = map(lambda t: list(zip(t[:-1], t[1:])), (enc_chans, dec_chans))\n\n            pad = (kernel_size - 1) // 2\n            for (enc_in, enc_out), (dec_in, dec_out) in zip(enc_chans_io, dec_chans_io):\n                enc_layers.append(nn.Sequential(conv(enc_in, enc_out, kernel_size, stride=stride, padding=pad), act()))\n                if encoder_norm:\n                    enc_layers.append(nn.GroupNorm(8, enc_out))\n                dec_layers.append(\n                    nn.Sequential(conv_transpose(dec_in, dec_out, kernel_size, stride=stride, padding=pad), act())\n                )\n            dec_out_chans = dec_chans[-1]\n            innermost_dim = dec_chans[0]\n        else:\n            enc_layers.append(nn.Sequential(conv(channels, hidden_dim, 1), act()))\n            dec_out_chans = hidden_dim\n            innermost_dim = hidden_dim\n\n        for _ in range(num_resnet_blocks):\n            dec_layers.insert(0, ResBlock(innermost_dim, conv, act))\n            enc_layers.append(ResBlock(innermost_dim, conv, act))\n\n        if num_resnet_blocks > 0:\n            dec_layers.insert(0, conv(codebook_dim, innermost_dim, 1))\n\n        enc_layers.append(conv(innermost_dim, codebook_dim, 1))\n        dec_layers.append(conv(dec_out_chans, channels, 1))\n\n        self.encoder = nn.Sequential(*enc_layers)\n        self.decoder = nn.Sequential(*dec_layers)\n\n        self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss\n        self.codebook = Quantize(codebook_dim, num_tokens, new_return_order=True)\n\n        # take care of normalization within class\n        self.normalization = normalization\n        self.record_codes = record_codes\n        if record_codes:\n            self.codes = torch.zeros((1228800,), dtype=torch.long)\n            self.code_ind = 0\n            self.total_codes = 0\n        self.internal_step = 0\n\n    def norm(self, images):\n        if not self.normalization is not None:\n            return images\n\n        means, stds = map(lambda t: torch.as_tensor(t).to(images), self.normalization)\n        arrange = \"c -> () c () ()\" if self.positional_dims == 2 else \"c -> () c ()\"\n        means, stds = map(lambda t: rearrange(t, arrange), (means, stds))\n        images = images.clone()\n        images.sub_(means).div_(stds)\n        return images\n\n    def get_debug_values(self, step, __):\n        if self.record_codes and self.total_codes > 0:\n            # Report annealing schedule\n            return {\"histogram_codes\": self.codes[: self.total_codes]}\n        else:\n            return {}\n\n    @torch.no_grad()\n    @eval_decorator\n    def get_codebook_indices(self, images):\n        img = self.norm(images)\n        logits = self.encoder(img).permute((0, 2, 3, 1) if len(img.shape) == 4 else (0, 2, 1))\n        sampled, codes, _ = self.codebook(logits)\n        self.log_codes(codes)\n        return codes\n\n    def decode(self, img_seq):\n        self.log_codes(img_seq)\n        if hasattr(self.codebook, \"embed_code\"):\n            image_embeds = self.codebook.embed_code(img_seq)\n        else:\n            image_embeds = F.embedding(img_seq, self.codebook.codebook)\n        b, n, d = image_embeds.shape\n\n        kwargs = {}\n        if self.positional_dims == 1:\n            arrange = \"b n d -> b d n\"\n        else:\n            h = w = int(sqrt(n))\n            arrange = \"b (h w) d -> b d h w\"\n            kwargs = {\"h\": h, \"w\": w}\n        image_embeds = rearrange(image_embeds, arrange, **kwargs)\n        images = [image_embeds]\n        for layer in self.decoder:\n            images.append(layer(images[-1]))\n        return images[-1], images[-2]\n\n    def infer(self, img):\n        img = self.norm(img)\n        logits = self.encoder(img).permute((0, 2, 3, 1) if len(img.shape) == 4 else (0, 2, 1))\n        sampled, codes, commitment_loss = self.codebook(logits)\n        return self.decode(codes)\n\n    # Note: This module is not meant to be run in forward() except while training. It has special logic which performs\n    # evaluation using quantized values when it detects that it is being run in eval() mode, which will be substantially\n    # more lossy (but useful for determining network performance).\n    def forward(self, img):\n        img = self.norm(img)\n        logits = self.encoder(img).permute((0, 2, 3, 1) if len(img.shape) == 4 else (0, 2, 1))\n        sampled, codes, commitment_loss = self.codebook(logits)\n        sampled = sampled.permute((0, 3, 1, 2) if len(img.shape) == 4 else (0, 2, 1))\n\n        if self.training:\n            out = sampled\n            for d in self.decoder:\n                out = d(out)\n            self.log_codes(codes)\n        else:\n            # This is non-differentiable, but gives a better idea of how the network is actually performing.\n            out, _ = self.decode(codes)\n\n        # reconstruction loss\n        recon_loss = self.loss_fn(img, out, reduction=\"none\")\n\n        return recon_loss, commitment_loss, out\n\n    def log_codes(self, codes):\n        # This is so we can debug the distribution of codes being learned.\n        if self.record_codes and self.internal_step % 10 == 0:\n            codes = codes.flatten()\n            l = codes.shape[0]\n            i = self.code_ind if (self.codes.shape[0] - self.code_ind) > l else self.codes.shape[0] - l\n            self.codes[i : i + l] = codes.cpu()\n            self.code_ind = self.code_ind + l\n            if self.code_ind >= self.codes.shape[0]:\n                self.code_ind = 0\n            self.total_codes += 1\n        self.internal_step += 1\n", "TTS/tts/layers/xtts/tokenizer.py": "import os\nimport re\nimport textwrap\nfrom functools import cached_property\n\nimport pypinyin\nimport torch\nfrom hangul_romanize import Transliter\nfrom hangul_romanize.rule import academic\nfrom num2words import num2words\nfrom spacy.lang.ar import Arabic\nfrom spacy.lang.en import English\nfrom spacy.lang.es import Spanish\nfrom spacy.lang.ja import Japanese\nfrom spacy.lang.zh import Chinese\nfrom tokenizers import Tokenizer\n\nfrom TTS.tts.layers.xtts.zh_num2words import TextNorm as zh_num2words\n\n\ndef get_spacy_lang(lang):\n    if lang == \"zh\":\n        return Chinese()\n    elif lang == \"ja\":\n        return Japanese()\n    elif lang == \"ar\":\n        return Arabic()\n    elif lang == \"es\":\n        return Spanish()\n    else:\n        # For most languages, Enlish does the job\n        return English()\n\n\ndef split_sentence(text, lang, text_split_length=250):\n    \"\"\"Preprocess the input text\"\"\"\n    text_splits = []\n    if text_split_length is not None and len(text) >= text_split_length:\n        text_splits.append(\"\")\n        nlp = get_spacy_lang(lang)\n        nlp.add_pipe(\"sentencizer\")\n        doc = nlp(text)\n        for sentence in doc.sents:\n            if len(text_splits[-1]) + len(str(sentence)) <= text_split_length:\n                # if the last sentence + the current sentence is less than the text_split_length\n                # then add the current sentence to the last sentence\n                text_splits[-1] += \" \" + str(sentence)\n                text_splits[-1] = text_splits[-1].lstrip()\n            elif len(str(sentence)) > text_split_length:\n                # if the current sentence is greater than the text_split_length\n                for line in textwrap.wrap(\n                    str(sentence),\n                    width=text_split_length,\n                    drop_whitespace=True,\n                    break_on_hyphens=False,\n                    tabsize=1,\n                ):\n                    text_splits.append(str(line))\n            else:\n                text_splits.append(str(sentence))\n\n        if len(text_splits) > 1:\n            if text_splits[0] == \"\":\n                del text_splits[0]\n    else:\n        text_splits = [text.lstrip()]\n\n    return text_splits\n\n\n_whitespace_re = re.compile(r\"\\s+\")\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = {\n    \"en\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"mrs\", \"misess\"),\n            (\"mr\", \"mister\"),\n            (\"dr\", \"doctor\"),\n            (\"st\", \"saint\"),\n            (\"co\", \"company\"),\n            (\"jr\", \"junior\"),\n            (\"maj\", \"major\"),\n            (\"gen\", \"general\"),\n            (\"drs\", \"doctors\"),\n            (\"rev\", \"reverend\"),\n            (\"lt\", \"lieutenant\"),\n            (\"hon\", \"honorable\"),\n            (\"sgt\", \"sergeant\"),\n            (\"capt\", \"captain\"),\n            (\"esq\", \"esquire\"),\n            (\"ltd\", \"limited\"),\n            (\"col\", \"colonel\"),\n            (\"ft\", \"fort\"),\n        ]\n    ],\n    \"es\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"sra\", \"se\u00f1ora\"),\n            (\"sr\", \"se\u00f1or\"),\n            (\"dr\", \"doctor\"),\n            (\"dra\", \"doctora\"),\n            (\"st\", \"santo\"),\n            (\"co\", \"compa\u00f1\u00eda\"),\n            (\"jr\", \"junior\"),\n            (\"ltd\", \"limitada\"),\n        ]\n    ],\n    \"fr\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"mme\", \"madame\"),\n            (\"mr\", \"monsieur\"),\n            (\"dr\", \"docteur\"),\n            (\"st\", \"saint\"),\n            (\"co\", \"compagnie\"),\n            (\"jr\", \"junior\"),\n            (\"ltd\", \"limit\u00e9e\"),\n        ]\n    ],\n    \"de\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"fr\", \"frau\"),\n            (\"dr\", \"doktor\"),\n            (\"st\", \"sankt\"),\n            (\"co\", \"firma\"),\n            (\"jr\", \"junior\"),\n        ]\n    ],\n    \"pt\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"sra\", \"senhora\"),\n            (\"sr\", \"senhor\"),\n            (\"dr\", \"doutor\"),\n            (\"dra\", \"doutora\"),\n            (\"st\", \"santo\"),\n            (\"co\", \"companhia\"),\n            (\"jr\", \"j\u00fanior\"),\n            (\"ltd\", \"limitada\"),\n        ]\n    ],\n    \"it\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            # (\"sig.ra\", \"signora\"),\n            (\"sig\", \"signore\"),\n            (\"dr\", \"dottore\"),\n            (\"st\", \"santo\"),\n            (\"co\", \"compagnia\"),\n            (\"jr\", \"junior\"),\n            (\"ltd\", \"limitata\"),\n        ]\n    ],\n    \"pl\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"p\", \"pani\"),\n            (\"m\", \"pan\"),\n            (\"dr\", \"doktor\"),\n            (\"sw\", \"\u015bwi\u0119ty\"),\n            (\"jr\", \"junior\"),\n        ]\n    ],\n    \"ar\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            # There are not many common abbreviations in Arabic as in English.\n        ]\n    ],\n    \"zh\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            # Chinese doesn't typically use abbreviations in the same way as Latin-based scripts.\n        ]\n    ],\n    \"cs\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"dr\", \"doktor\"),  # doctor\n            (\"ing\", \"in\u017een\u00fdr\"),  # engineer\n            (\"p\", \"pan\"),  # Could also map to pani for woman but no easy way to do it\n            # Other abbreviations would be specialized and not as common.\n        ]\n    ],\n    \"ru\": [\n        (re.compile(\"\\\\b%s\\\\b\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"\u0433-\u0436\u0430\", \"\u0433\u043e\u0441\u043f\u043e\u0436\u0430\"),  # Mrs.\n            (\"\u0433-\u043d\", \"\u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d\"),  # Mr.\n            (\"\u0434-\u0440\", \"\u0434\u043e\u043a\u0442\u043e\u0440\"),  # doctor\n            # Other abbreviations are less common or specialized.\n        ]\n    ],\n    \"nl\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"dhr\", \"de heer\"),  # Mr.\n            (\"mevr\", \"mevrouw\"),  # Mrs.\n            (\"dr\", \"dokter\"),  # doctor\n            (\"jhr\", \"jonkheer\"),  # young lord or nobleman\n            # Dutch uses more abbreviations, but these are the most common ones.\n        ]\n    ],\n    \"tr\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"b\", \"bay\"),  # Mr.\n            (\"byk\", \"b\u00fcy\u00fck\"),  # b\u00fcy\u00fck\n            (\"dr\", \"doktor\"),  # doctor\n            # Add other Turkish abbreviations here if needed.\n        ]\n    ],\n    \"hu\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            (\"dr\", \"doktor\"),  # doctor\n            (\"b\", \"b\u00e1csi\"),  # Mr.\n            (\"n\u0151v\", \"n\u0151v\u00e9r\"),  # nurse\n            # Add other Hungarian abbreviations here if needed.\n        ]\n    ],\n    \"ko\": [\n        (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n        for x in [\n            # Korean doesn't typically use abbreviations in the same way as Latin-based scripts.\n        ]\n    ],\n}\n\n\ndef expand_abbreviations_multilingual(text, lang=\"en\"):\n    for regex, replacement in _abbreviations[lang]:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\n_symbols_multilingual = {\n    \"en\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" and \"),\n            (\"@\", \" at \"),\n            (\"%\", \" percent \"),\n            (\"#\", \" hash \"),\n            (\"$\", \" dollar \"),\n            (\"\u00a3\", \" pound \"),\n            (\"\u00b0\", \" degree \"),\n        ]\n    ],\n    \"es\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" y \"),\n            (\"@\", \" arroba \"),\n            (\"%\", \" por ciento \"),\n            (\"#\", \" numeral \"),\n            (\"$\", \" dolar \"),\n            (\"\u00a3\", \" libra \"),\n            (\"\u00b0\", \" grados \"),\n        ]\n    ],\n    \"fr\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" et \"),\n            (\"@\", \" arobase \"),\n            (\"%\", \" pour cent \"),\n            (\"#\", \" di\u00e8se \"),\n            (\"$\", \" dollar \"),\n            (\"\u00a3\", \" livre \"),\n            (\"\u00b0\", \" degr\u00e9s \"),\n        ]\n    ],\n    \"de\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" und \"),\n            (\"@\", \" at \"),\n            (\"%\", \" prozent \"),\n            (\"#\", \" raute \"),\n            (\"$\", \" dollar \"),\n            (\"\u00a3\", \" pfund \"),\n            (\"\u00b0\", \" grad \"),\n        ]\n    ],\n    \"pt\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" e \"),\n            (\"@\", \" arroba \"),\n            (\"%\", \" por cento \"),\n            (\"#\", \" cardinal \"),\n            (\"$\", \" d\u00f3lar \"),\n            (\"\u00a3\", \" libra \"),\n            (\"\u00b0\", \" graus \"),\n        ]\n    ],\n    \"it\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" e \"),\n            (\"@\", \" chiocciola \"),\n            (\"%\", \" per cento \"),\n            (\"#\", \" cancelletto \"),\n            (\"$\", \" dollaro \"),\n            (\"\u00a3\", \" sterlina \"),\n            (\"\u00b0\", \" gradi \"),\n        ]\n    ],\n    \"pl\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" i \"),\n            (\"@\", \" ma\u0142pa \"),\n            (\"%\", \" procent \"),\n            (\"#\", \" krzy\u017cyk \"),\n            (\"$\", \" dolar \"),\n            (\"\u00a3\", \" funt \"),\n            (\"\u00b0\", \" stopnie \"),\n        ]\n    ],\n    \"ar\": [\n        # Arabic\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" \u0648 \"),\n            (\"@\", \" \u0639\u0644\u0649 \"),\n            (\"%\", \" \u0641\u064a \u0627\u0644\u0645\u0626\u0629 \"),\n            (\"#\", \" \u0631\u0642\u0645 \"),\n            (\"$\", \" \u062f\u0648\u0644\u0627\u0631 \"),\n            (\"\u00a3\", \" \u062c\u0646\u064a\u0647 \"),\n            (\"\u00b0\", \" \u062f\u0631\u062c\u0629 \"),\n        ]\n    ],\n    \"zh\": [\n        # Chinese\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" \u548c \"),\n            (\"@\", \" \u5728 \"),\n            (\"%\", \" \u767e\u5206\u4e4b \"),\n            (\"#\", \" \u53f7 \"),\n            (\"$\", \" \u7f8e\u5143 \"),\n            (\"\u00a3\", \" \u82f1\u9551 \"),\n            (\"\u00b0\", \" \u5ea6 \"),\n        ]\n    ],\n    \"cs\": [\n        # Czech\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" a \"),\n            (\"@\", \" na \"),\n            (\"%\", \" procento \"),\n            (\"#\", \" k\u0159\u00ed\u017eek \"),\n            (\"$\", \" dolar \"),\n            (\"\u00a3\", \" libra \"),\n            (\"\u00b0\", \" stupn\u011b \"),\n        ]\n    ],\n    \"ru\": [\n        # Russian\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" \u0438 \"),\n            (\"@\", \" \u0441\u043e\u0431\u0430\u043a\u0430 \"),\n            (\"%\", \" \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \"),\n            (\"#\", \" \u043d\u043e\u043c\u0435\u0440 \"),\n            (\"$\", \" \u0434\u043e\u043b\u043b\u0430\u0440 \"),\n            (\"\u00a3\", \" \u0444\u0443\u043d\u0442 \"),\n            (\"\u00b0\", \" \u0433\u0440\u0430\u0434\u0443\u0441 \"),\n        ]\n    ],\n    \"nl\": [\n        # Dutch\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" en \"),\n            (\"@\", \" bij \"),\n            (\"%\", \" procent \"),\n            (\"#\", \" hekje \"),\n            (\"$\", \" dollar \"),\n            (\"\u00a3\", \" pond \"),\n            (\"\u00b0\", \" graden \"),\n        ]\n    ],\n    \"tr\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" ve \"),\n            (\"@\", \" at \"),\n            (\"%\", \" y\u00fczde \"),\n            (\"#\", \" diyez \"),\n            (\"$\", \" dolar \"),\n            (\"\u00a3\", \" sterlin \"),\n            (\"\u00b0\", \" derece \"),\n        ]\n    ],\n    \"hu\": [\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" \u00e9s \"),\n            (\"@\", \" kukac \"),\n            (\"%\", \" sz\u00e1zal\u00e9k \"),\n            (\"#\", \" kett\u0151skereszt \"),\n            (\"$\", \" doll\u00e1r \"),\n            (\"\u00a3\", \" font \"),\n            (\"\u00b0\", \" fok \"),\n        ]\n    ],\n    \"ko\": [\n        # Korean\n        (re.compile(r\"%s\" % re.escape(x[0]), re.IGNORECASE), x[1])\n        for x in [\n            (\"&\", \" \uadf8\ub9ac\uace0 \"),\n            (\"@\", \" \uc5d0 \"),\n            (\"%\", \" \ud37c\uc13c\ud2b8 \"),\n            (\"#\", \" \ubc88\ud638 \"),\n            (\"$\", \" \ub2ec\ub7ec \"),\n            (\"\u00a3\", \" \ud30c\uc6b4\ub4dc \"),\n            (\"\u00b0\", \" \ub3c4 \"),\n        ]\n    ],\n}\n\n\ndef expand_symbols_multilingual(text, lang=\"en\"):\n    for regex, replacement in _symbols_multilingual[lang]:\n        text = re.sub(regex, replacement, text)\n        text = text.replace(\"  \", \" \")  # Ensure there are no double spaces\n    return text.strip()\n\n\n_ordinal_re = {\n    \"en\": re.compile(r\"([0-9]+)(st|nd|rd|th)\"),\n    \"es\": re.compile(r\"([0-9]+)(\u00ba|\u00aa|er|o|a|os|as)\"),\n    \"fr\": re.compile(r\"([0-9]+)(\u00ba|\u00aa|er|re|e|\u00e8me)\"),\n    \"de\": re.compile(r\"([0-9]+)(st|nd|rd|th|\u00ba|\u00aa|\\.(?=\\s|$))\"),\n    \"pt\": re.compile(r\"([0-9]+)(\u00ba|\u00aa|o|a|os|as)\"),\n    \"it\": re.compile(r\"([0-9]+)(\u00ba|\u00b0|\u00aa|o|a|i|e)\"),\n    \"pl\": re.compile(r\"([0-9]+)(\u00ba|\u00aa|st|nd|rd|th)\"),\n    \"ar\": re.compile(r\"([0-9]+)(\u0648\u0646|\u064a\u0646|\u062b|\u0631|\u0649)\"),\n    \"cs\": re.compile(r\"([0-9]+)\\.(?=\\s|$)\"),  # In Czech, a dot is often used after the number to indicate ordinals.\n    \"ru\": re.compile(r\"([0-9]+)(-\u0439|-\u044f|-\u0435|-\u043e\u0435|-\u044c\u0435|-\u0433\u043e)\"),\n    \"nl\": re.compile(r\"([0-9]+)(de|ste|e)\"),\n    \"tr\": re.compile(r\"([0-9]+)(\\.|inci|nci|uncu|\u00fcnc\u00fc|\\.)\"),\n    \"hu\": re.compile(r\"([0-9]+)(\\.|adik|edik|odik|edik|\u00f6dik|\u00f6dike|ik)\"),\n    \"ko\": re.compile(r\"([0-9]+)(\ubc88\uc9f8|\ubc88|\ucc28|\uc9f8)\"),\n}\n_number_re = re.compile(r\"[0-9]+\")\n_currency_re = {\n    \"USD\": re.compile(r\"((\\$[0-9\\.\\,]*[0-9]+)|([0-9\\.\\,]*[0-9]+\\$))\"),\n    \"GBP\": re.compile(r\"((\u00a3[0-9\\.\\,]*[0-9]+)|([0-9\\.\\,]*[0-9]+\u00a3))\"),\n    \"EUR\": re.compile(r\"(([0-9\\.\\,]*[0-9]+\u20ac)|((\u20ac[0-9\\.\\,]*[0-9]+)))\"),\n}\n\n_comma_number_re = re.compile(r\"\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b\")\n_dot_number_re = re.compile(r\"\\b\\d{1,3}(.\\d{3})*(\\,\\d+)?\\b\")\n_decimal_number_re = re.compile(r\"([0-9]+[.,][0-9]+)\")\n\n\ndef _remove_commas(m):\n    text = m.group(0)\n    if \",\" in text:\n        text = text.replace(\",\", \"\")\n    return text\n\n\ndef _remove_dots(m):\n    text = m.group(0)\n    if \".\" in text:\n        text = text.replace(\".\", \"\")\n    return text\n\n\ndef _expand_decimal_point(m, lang=\"en\"):\n    amount = m.group(1).replace(\",\", \".\")\n    return num2words(float(amount), lang=lang if lang != \"cs\" else \"cz\")\n\n\ndef _expand_currency(m, lang=\"en\", currency=\"USD\"):\n    amount = float((re.sub(r\"[^\\d.]\", \"\", m.group(0).replace(\",\", \".\"))))\n    full_amount = num2words(amount, to=\"currency\", currency=currency, lang=lang if lang != \"cs\" else \"cz\")\n\n    and_equivalents = {\n        \"en\": \", \",\n        \"es\": \" con \",\n        \"fr\": \" et \",\n        \"de\": \" und \",\n        \"pt\": \" e \",\n        \"it\": \" e \",\n        \"pl\": \", \",\n        \"cs\": \", \",\n        \"ru\": \", \",\n        \"nl\": \", \",\n        \"ar\": \", \",\n        \"tr\": \", \",\n        \"hu\": \", \",\n        \"ko\": \", \",\n    }\n\n    if amount.is_integer():\n        last_and = full_amount.rfind(and_equivalents[lang])\n        if last_and != -1:\n            full_amount = full_amount[:last_and]\n\n    return full_amount\n\n\ndef _expand_ordinal(m, lang=\"en\"):\n    return num2words(int(m.group(1)), ordinal=True, lang=lang if lang != \"cs\" else \"cz\")\n\n\ndef _expand_number(m, lang=\"en\"):\n    return num2words(int(m.group(0)), lang=lang if lang != \"cs\" else \"cz\")\n\n\ndef expand_numbers_multilingual(text, lang=\"en\"):\n    if lang == \"zh\":\n        text = zh_num2words()(text)\n    else:\n        if lang in [\"en\", \"ru\"]:\n            text = re.sub(_comma_number_re, _remove_commas, text)\n        else:\n            text = re.sub(_dot_number_re, _remove_dots, text)\n        try:\n            text = re.sub(_currency_re[\"GBP\"], lambda m: _expand_currency(m, lang, \"GBP\"), text)\n            text = re.sub(_currency_re[\"USD\"], lambda m: _expand_currency(m, lang, \"USD\"), text)\n            text = re.sub(_currency_re[\"EUR\"], lambda m: _expand_currency(m, lang, \"EUR\"), text)\n        except:\n            pass\n        if lang != \"tr\":\n            text = re.sub(_decimal_number_re, lambda m: _expand_decimal_point(m, lang), text)\n        text = re.sub(_ordinal_re[lang], lambda m: _expand_ordinal(m, lang), text)\n        text = re.sub(_number_re, lambda m: _expand_number(m, lang), text)\n    return text\n\n\ndef lowercase(text):\n    return text.lower()\n\n\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)\n\n\ndef multilingual_cleaners(text, lang):\n    text = text.replace('\"', \"\")\n    if lang == \"tr\":\n        text = text.replace(\"\u0130\", \"i\")\n        text = text.replace(\"\u00d6\", \"\u00f6\")\n        text = text.replace(\"\u00dc\", \"\u00fc\")\n    text = lowercase(text)\n    text = expand_numbers_multilingual(text, lang)\n    text = expand_abbreviations_multilingual(text, lang)\n    text = expand_symbols_multilingual(text, lang=lang)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef chinese_transliterate(text):\n    return \"\".join(\n        [p[0] for p in pypinyin.pinyin(text, style=pypinyin.Style.TONE3, heteronym=False, neutral_tone_with_five=True)]\n    )\n\n\ndef japanese_cleaners(text, katsu):\n    text = katsu.romaji(text)\n    text = lowercase(text)\n    return text\n\n\ndef korean_transliterate(text):\n    r = Transliter(academic)\n    return r.translit(text)\n\n\nDEFAULT_VOCAB_FILE = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../data/tokenizer.json\")\n\n\nclass VoiceBpeTokenizer:\n    def __init__(self, vocab_file=None):\n        self.tokenizer = None\n        if vocab_file is not None:\n            self.tokenizer = Tokenizer.from_file(vocab_file)\n        self.char_limits = {\n            \"en\": 250,\n            \"de\": 253,\n            \"fr\": 273,\n            \"es\": 239,\n            \"it\": 213,\n            \"pt\": 203,\n            \"pl\": 224,\n            \"zh\": 82,\n            \"ar\": 166,\n            \"cs\": 186,\n            \"ru\": 182,\n            \"nl\": 251,\n            \"tr\": 226,\n            \"ja\": 71,\n            \"hu\": 224,\n            \"ko\": 95,\n        }\n\n    @cached_property\n    def katsu(self):\n        import cutlet\n\n        return cutlet.Cutlet()\n\n    def check_input_length(self, txt, lang):\n        lang = lang.split(\"-\")[0]  # remove the region\n        limit = self.char_limits.get(lang, 250)\n        if len(txt) > limit:\n            print(\n                f\"[!] Warning: The text length exceeds the character limit of {limit} for language '{lang}', this might cause truncated audio.\"\n            )\n\n    def preprocess_text(self, txt, lang):\n        if lang in {\"ar\", \"cs\", \"de\", \"en\", \"es\", \"fr\", \"hu\", \"it\", \"nl\", \"pl\", \"pt\", \"ru\", \"tr\", \"zh\", \"ko\"}:\n            txt = multilingual_cleaners(txt, lang)\n            if lang == \"zh\":\n                txt = chinese_transliterate(txt)\n            if lang == \"ko\":\n                txt = korean_transliterate(txt)\n        elif lang == \"ja\":\n            txt = japanese_cleaners(txt, self.katsu)\n        elif lang == \"hi\":\n            # @manmay will implement this\n            txt = basic_cleaners(txt)\n        else:\n            raise NotImplementedError(f\"Language '{lang}' is not supported.\")\n        return txt\n\n    def encode(self, txt, lang):\n        lang = lang.split(\"-\")[0]  # remove the region\n        self.check_input_length(txt, lang)\n        txt = self.preprocess_text(txt, lang)\n        lang = \"zh-cn\" if lang == \"zh\" else lang\n        txt = f\"[{lang}]{txt}\"\n        txt = txt.replace(\" \", \"[SPACE]\")\n        return self.tokenizer.encode(txt).ids\n\n    def decode(self, seq):\n        if isinstance(seq, torch.Tensor):\n            seq = seq.cpu().numpy()\n        txt = self.tokenizer.decode(seq, skip_special_tokens=False).replace(\" \", \"\")\n        txt = txt.replace(\"[SPACE]\", \" \")\n        txt = txt.replace(\"[STOP]\", \"\")\n        txt = txt.replace(\"[UNK]\", \"\")\n        return txt\n\n    def __len__(self):\n        return self.tokenizer.get_vocab_size()\n\n    def get_number_tokens(self):\n        return max(self.tokenizer.get_vocab().values()) + 1\n\n\ndef test_expand_numbers_multilingual():\n    test_cases = [\n        # English\n        (\"In 12.5 seconds.\", \"In twelve point five seconds.\", \"en\"),\n        (\"There were 50 soldiers.\", \"There were fifty soldiers.\", \"en\"),\n        (\"This is a 1st test\", \"This is a first test\", \"en\"),\n        (\"That will be $20 sir.\", \"That will be twenty dollars sir.\", \"en\"),\n        (\"That will be 20\u20ac sir.\", \"That will be twenty euro sir.\", \"en\"),\n        (\"That will be 20.15\u20ac sir.\", \"That will be twenty euro, fifteen cents sir.\", \"en\"),\n        (\"That's 100,000.5.\", \"That's one hundred thousand point five.\", \"en\"),\n        # French\n        (\"En 12,5 secondes.\", \"En douze virgule cinq secondes.\", \"fr\"),\n        (\"Il y avait 50 soldats.\", \"Il y avait cinquante soldats.\", \"fr\"),\n        (\"Ceci est un 1er test\", \"Ceci est un premier test\", \"fr\"),\n        (\"Cela vous fera $20 monsieur.\", \"Cela vous fera vingt dollars monsieur.\", \"fr\"),\n        (\"Cela vous fera 20\u20ac monsieur.\", \"Cela vous fera vingt euros monsieur.\", \"fr\"),\n        (\"Cela vous fera 20,15\u20ac monsieur.\", \"Cela vous fera vingt euros et quinze centimes monsieur.\", \"fr\"),\n        (\"Ce sera 100.000,5.\", \"Ce sera cent mille virgule cinq.\", \"fr\"),\n        # German\n        (\"In 12,5 Sekunden.\", \"In zw\u00f6lf Komma f\u00fcnf Sekunden.\", \"de\"),\n        (\"Es gab 50 Soldaten.\", \"Es gab f\u00fcnfzig Soldaten.\", \"de\"),\n        (\"Dies ist ein 1. Test\", \"Dies ist ein erste Test\", \"de\"),  # Issue with gender\n        (\"Das macht $20 Herr.\", \"Das macht zwanzig Dollar Herr.\", \"de\"),\n        (\"Das macht 20\u20ac Herr.\", \"Das macht zwanzig Euro Herr.\", \"de\"),\n        (\"Das macht 20,15\u20ac Herr.\", \"Das macht zwanzig Euro und f\u00fcnfzehn Cent Herr.\", \"de\"),\n        # Spanish\n        (\"En 12,5 segundos.\", \"En doce punto cinco segundos.\", \"es\"),\n        (\"Hab\u00eda 50 soldados.\", \"Hab\u00eda cincuenta soldados.\", \"es\"),\n        (\"Este es un 1er test\", \"Este es un primero test\", \"es\"),\n        (\"Eso le costar\u00e1 $20 se\u00f1or.\", \"Eso le costar\u00e1 veinte d\u00f3lares se\u00f1or.\", \"es\"),\n        (\"Eso le costar\u00e1 20\u20ac se\u00f1or.\", \"Eso le costar\u00e1 veinte euros se\u00f1or.\", \"es\"),\n        (\"Eso le costar\u00e1 20,15\u20ac se\u00f1or.\", \"Eso le costar\u00e1 veinte euros con quince c\u00e9ntimos se\u00f1or.\", \"es\"),\n        # Italian\n        (\"In 12,5 secondi.\", \"In dodici virgola cinque secondi.\", \"it\"),\n        (\"C'erano 50 soldati.\", \"C'erano cinquanta soldati.\", \"it\"),\n        (\"Questo \u00e8 un 1\u00b0 test\", \"Questo \u00e8 un primo test\", \"it\"),\n        (\"Ti coster\u00e0 $20 signore.\", \"Ti coster\u00e0 venti dollari signore.\", \"it\"),\n        (\"Ti coster\u00e0 20\u20ac signore.\", \"Ti coster\u00e0 venti euro signore.\", \"it\"),\n        (\"Ti coster\u00e0 20,15\u20ac signore.\", \"Ti coster\u00e0 venti euro e quindici centesimi signore.\", \"it\"),\n        # Portuguese\n        (\"Em 12,5 segundos.\", \"Em doze v\u00edrgula cinco segundos.\", \"pt\"),\n        (\"Havia 50 soldados.\", \"Havia cinquenta soldados.\", \"pt\"),\n        (\"Este \u00e9 um 1\u00ba teste\", \"Este \u00e9 um primeiro teste\", \"pt\"),\n        (\"Isso custar\u00e1 $20 senhor.\", \"Isso custar\u00e1 vinte d\u00f3lares senhor.\", \"pt\"),\n        (\"Isso custar\u00e1 20\u20ac senhor.\", \"Isso custar\u00e1 vinte euros senhor.\", \"pt\"),\n        (\n            \"Isso custar\u00e1 20,15\u20ac senhor.\",\n            \"Isso custar\u00e1 vinte euros e quinze c\u00eantimos senhor.\",\n            \"pt\",\n        ),  # \"c\u00eantimos\" should be \"centavos\" num2words issue\n        # Polish\n        (\"W 12,5 sekundy.\", \"W dwana\u015bcie przecinek pi\u0119\u0107 sekundy.\", \"pl\"),\n        (\"By\u0142o 50 \u017co\u0142nierzy.\", \"By\u0142o pi\u0119\u0107dziesi\u0105t \u017co\u0142nierzy.\", \"pl\"),\n        (\"To b\u0119dzie kosztowa\u0107 20\u20ac panie.\", \"To b\u0119dzie kosztowa\u0107 dwadzie\u015bcia euro panie.\", \"pl\"),\n        (\"To b\u0119dzie kosztowa\u0107 20,15\u20ac panie.\", \"To b\u0119dzie kosztowa\u0107 dwadzie\u015bcia euro, pi\u0119tna\u015bcie cent\u00f3w panie.\", \"pl\"),\n        # Arabic\n        (\"\u0641\u064a \u0627\u0644\u0640 12,5 \u062b\u0627\u0646\u064a\u0629.\", \"\u0641\u064a \u0627\u0644\u0640 \u0627\u062b\u0646\u0627 \u0639\u0634\u0631  , \u062e\u0645\u0633\u0648\u0646 \u062b\u0627\u0646\u064a\u0629.\", \"ar\"),\n        (\"\u0643\u0627\u0646 \u0647\u0646\u0627\u0643 50 \u062c\u0646\u062f\u064a\u064b\u0627.\", \"\u0643\u0627\u0646 \u0647\u0646\u0627\u0643 \u062e\u0645\u0633\u0648\u0646 \u062c\u0646\u062f\u064a\u064b\u0627.\", \"ar\"),\n        # (\"\u0633\u062a\u0643\u0648\u0646 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 $20 \u064a\u0627 \u0633\u064a\u062f.\", '\u0633\u062a\u0643\u0648\u0646 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0639\u0634\u0631\u0648\u0646 \u062f\u0648\u0644\u0627\u0631 \u064a\u0627 \u0633\u064a\u062f.', 'ar'), # $ and \u20ac are mising from num2words\n        # (\"\u0633\u062a\u0643\u0648\u0646 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 20\u20ac \u064a\u0627 \u0633\u064a\u062f.\", '\u0633\u062a\u0643\u0648\u0646 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0639\u0634\u0631\u0648\u0646 \u064a\u0648\u0631\u0648 \u064a\u0627 \u0633\u064a\u062f.', 'ar'),\n        # Czech\n        (\"Za 12,5 vte\u0159iny.\", \"Za dvan\u00e1ct cel\u00e1 p\u011bt vte\u0159iny.\", \"cs\"),\n        (\"Bylo tam 50 voj\u00e1k\u016f.\", \"Bylo tam pades\u00e1t voj\u00e1k\u016f.\", \"cs\"),\n        (\"To bude st\u00e1t 20\u20ac pane.\", \"To bude st\u00e1t dvacet euro pane.\", \"cs\"),\n        (\"To bude 20.15\u20ac pane.\", \"To bude dvacet euro, patn\u00e1ct cent\u016f pane.\", \"cs\"),\n        # Russian\n        (\"\u0427\u0435\u0440\u0435\u0437 12.5 \u0441\u0435\u043a\u0443\u043d\u0434\u044b.\", \"\u0427\u0435\u0440\u0435\u0437 \u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u044c \u0437\u0430\u043f\u044f\u0442\u0430\u044f \u043f\u044f\u0442\u044c \u0441\u0435\u043a\u0443\u043d\u0434\u044b.\", \"ru\"),\n        (\"\u0422\u0430\u043c \u0431\u044b\u043b\u043e 50 \u0441\u043e\u043b\u0434\u0430\u0442.\", \"\u0422\u0430\u043c \u0431\u044b\u043b\u043e \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0441\u043e\u043b\u0434\u0430\u0442.\", \"ru\"),\n        (\"\u042d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 20.15\u20ac \u0441\u044d\u0440.\", \"\u042d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0435\u0432\u0440\u043e, \u043f\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044c \u0446\u0435\u043d\u0442\u043e\u0432 \u0441\u044d\u0440.\", \"ru\"),\n        (\"\u042d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0441\u0442\u043e\u0438\u0442\u044c 20\u20ac \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d.\", \"\u042d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0441\u0442\u043e\u0438\u0442\u044c \u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0435\u0432\u0440\u043e \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d.\", \"ru\"),\n        # Dutch\n        (\"In 12,5 seconden.\", \"In twaalf komma vijf seconden.\", \"nl\"),\n        (\"Er waren 50 soldaten.\", \"Er waren vijftig soldaten.\", \"nl\"),\n        (\"Dat wordt dan $20 meneer.\", \"Dat wordt dan twintig dollar meneer.\", \"nl\"),\n        (\"Dat wordt dan 20\u20ac meneer.\", \"Dat wordt dan twintig euro meneer.\", \"nl\"),\n        # Chinese (Simplified)\n        (\"\u572812.5\u79d2\u5185\", \"\u5728\u5341\u4e8c\u70b9\u4e94\u79d2\u5185\", \"zh\"),\n        (\"\u670950\u540d\u58eb\u5175\", \"\u6709\u4e94\u5341\u540d\u58eb\u5175\", \"zh\"),\n        # (\"\u90a3\u5c06\u662f$20\u5148\u751f\", '\u90a3\u5c06\u662f\u4e8c\u5341\u7f8e\u5143\u5148\u751f', 'zh'), currency doesn't work\n        # (\"\u90a3\u5c06\u662f20\u20ac\u5148\u751f\", '\u90a3\u5c06\u662f\u4e8c\u5341\u6b27\u5143\u5148\u751f', 'zh'),\n        # Turkish\n        # (\"12,5 saniye i\u00e7inde.\", 'On iki virg\u00fcl be\u015f saniye i\u00e7inde.', 'tr'), # decimal doesn't work for TR\n        (\"50 asker vard\u0131.\", \"elli asker vard\u0131.\", \"tr\"),\n        (\"Bu 1. test\", \"Bu birinci test\", \"tr\"),\n        # (\"Bu 100.000,5.\", 'Bu y\u00fcz bin virg\u00fcl be\u015f.', 'tr'),\n        # Hungarian\n        (\"12,5 m\u00e1sodperc alatt.\", \"tizenkett\u0151 eg\u00e9sz \u00f6t tized m\u00e1sodperc alatt.\", \"hu\"),\n        (\"50 katona volt.\", \"\u00f6tven katona volt.\", \"hu\"),\n        (\"Ez az 1. teszt\", \"Ez az els\u0151 teszt\", \"hu\"),\n        # Korean\n        (\"12.5 \ucd08 \uc548\uc5d0.\", \"\uc2ed\uc774 \uc810 \ub2e4\uc12f \ucd08 \uc548\uc5d0.\", \"ko\"),\n        (\"50 \uba85\uc758 \ubcd1\uc0ac\uac00 \uc788\uc5c8\ub2e4.\", \"\uc624\uc2ed \uba85\uc758 \ubcd1\uc0ac\uac00 \uc788\uc5c8\ub2e4.\", \"ko\"),\n        (\"\uc774\uac83\uc740 1 \ubc88\uc9f8 \ud14c\uc2a4\ud2b8\uc785\ub2c8\ub2e4\", \"\uc774\uac83\uc740 \uccab \ubc88\uc9f8 \ud14c\uc2a4\ud2b8\uc785\ub2c8\ub2e4\", \"ko\"),\n    ]\n    for a, b, lang in test_cases:\n        out = expand_numbers_multilingual(a, lang=lang)\n        assert out == b, f\"'{out}' vs '{b}'\"\n\n\ndef test_abbreviations_multilingual():\n    test_cases = [\n        # English\n        (\"Hello Mr. Smith.\", \"Hello mister Smith.\", \"en\"),\n        (\"Dr. Jones is here.\", \"doctor Jones is here.\", \"en\"),\n        # Spanish\n        (\"Hola Sr. Garcia.\", \"Hola se\u00f1or Garcia.\", \"es\"),\n        (\"La Dra. Martinez es muy buena.\", \"La doctora Martinez es muy buena.\", \"es\"),\n        # French\n        (\"Bonjour Mr. Dupond.\", \"Bonjour monsieur Dupond.\", \"fr\"),\n        (\"Mme. Moreau est absente aujourd'hui.\", \"madame Moreau est absente aujourd'hui.\", \"fr\"),\n        # German\n        (\"Frau Dr. M\u00fcller ist sehr klug.\", \"Frau doktor M\u00fcller ist sehr klug.\", \"de\"),\n        # Portuguese\n        (\"Ol\u00e1 Sr. Silva.\", \"Ol\u00e1 senhor Silva.\", \"pt\"),\n        (\"Dra. Costa, voc\u00ea est\u00e1 dispon\u00edvel?\", \"doutora Costa, voc\u00ea est\u00e1 dispon\u00edvel?\", \"pt\"),\n        # Italian\n        (\"Buongiorno, Sig. Rossi.\", \"Buongiorno, signore Rossi.\", \"it\"),\n        # (\"Sig.ra Bianchi, posso aiutarti?\", 'signora Bianchi, posso aiutarti?', 'it'), # Issue with matching that pattern\n        # Polish\n        (\"Dzie\u0144 dobry, P. Kowalski.\", \"Dzie\u0144 dobry, pani Kowalski.\", \"pl\"),\n        (\"M. Nowak, czy mog\u0119 zada\u0107 pytanie?\", \"pan Nowak, czy mog\u0119 zada\u0107 pytanie?\", \"pl\"),\n        # Czech\n        (\"P. Nov\u00e1k\", \"pan Nov\u00e1k\", \"cs\"),\n        (\"Dr. Vojt\u011bch\", \"doktor Vojt\u011bch\", \"cs\"),\n        # Dutch\n        (\"Dhr. Jansen\", \"de heer Jansen\", \"nl\"),\n        (\"Mevr. de Vries\", \"mevrouw de Vries\", \"nl\"),\n        # Russian\n        (\"\u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435 \u0413-\u043d \u0418\u0432\u0430\u043d\u043e\u0432.\", \"\u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435 \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d \u0418\u0432\u0430\u043d\u043e\u0432.\", \"ru\"),\n        (\"\u0414-\u0440 \u0421\u043c\u0438\u0440\u043d\u043e\u0432 \u0437\u0434\u0435\u0441\u044c, \u0447\u0442\u043e\u0431\u044b \u0443\u0432\u0438\u0434\u0435\u0442\u044c \u0432\u0430\u0441.\", \"\u0434\u043e\u043a\u0442\u043e\u0440 \u0421\u043c\u0438\u0440\u043d\u043e\u0432 \u0437\u0434\u0435\u0441\u044c, \u0447\u0442\u043e\u0431\u044b \u0443\u0432\u0438\u0434\u0435\u0442\u044c \u0432\u0430\u0441.\", \"ru\"),\n        # Turkish\n        (\"Merhaba B. Y\u0131lmaz.\", \"Merhaba bay Y\u0131lmaz.\", \"tr\"),\n        (\"Dr. Ay\u015fe burada.\", \"doktor Ay\u015fe burada.\", \"tr\"),\n        # Hungarian\n        (\"Dr. Szab\u00f3 itt van.\", \"doktor Szab\u00f3 itt van.\", \"hu\"),\n    ]\n\n    for a, b, lang in test_cases:\n        out = expand_abbreviations_multilingual(a, lang=lang)\n        assert out == b, f\"'{out}' vs '{b}'\"\n\n\ndef test_symbols_multilingual():\n    test_cases = [\n        (\"I have 14% battery\", \"I have 14 percent battery\", \"en\"),\n        (\"Te veo @ la fiesta\", \"Te veo arroba la fiesta\", \"es\"),\n        (\"J'ai 14\u00b0 de fi\u00e8vre\", \"J'ai 14 degr\u00e9s de fi\u00e8vre\", \"fr\"),\n        (\"Die Rechnung betr\u00e4gt \u00a3 20\", \"Die Rechnung betr\u00e4gt pfund 20\", \"de\"),\n        (\"O meu email \u00e9 ana&joao@gmail.com\", \"O meu email \u00e9 ana e joao arroba gmail.com\", \"pt\"),\n        (\"linguaggio di programmazione C#\", \"linguaggio di programmazione C cancelletto\", \"it\"),\n        (\"Moja temperatura to 36.6\u00b0\", \"Moja temperatura to 36.6 stopnie\", \"pl\"),\n        (\"M\u00e1m 14% baterie\", \"M\u00e1m 14 procento baterie\", \"cs\"),\n        (\"T\u011b\u0161\u00edm se na tebe @ party\", \"T\u011b\u0161\u00edm se na tebe na party\", \"cs\"),\n        (\"\u0423 \u043c\u0435\u043d\u044f 14% \u0437\u0430\u0440\u044f\u0434\u0430\", \"\u0423 \u043c\u0435\u043d\u044f 14 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \u0437\u0430\u0440\u044f\u0434\u0430\", \"ru\"),\n        (\"\u042f \u0431\u0443\u0434\u0443 @ \u0434\u043e\u043c\u0430\", \"\u042f \u0431\u0443\u0434\u0443 \u0441\u043e\u0431\u0430\u043a\u0430 \u0434\u043e\u043c\u0430\", \"ru\"),\n        (\"Ik heb 14% batterij\", \"Ik heb 14 procent batterij\", \"nl\"),\n        (\"Ik zie je @ het feest\", \"Ik zie je bij het feest\", \"nl\"),\n        (\"\u0644\u062f\u064a 14% \u0641\u064a \u0627\u0644\u0628\u0637\u0627\u0631\u064a\u0629\", \"\u0644\u062f\u064a 14 \u0641\u064a \u0627\u0644\u0645\u0626\u0629 \u0641\u064a \u0627\u0644\u0628\u0637\u0627\u0631\u064a\u0629\", \"ar\"),\n        (\"\u6211\u7684\u7535\u91cf\u4e3a 14%\", \"\u6211\u7684\u7535\u91cf\u4e3a 14 \u767e\u5206\u4e4b\", \"zh\"),\n        (\"Pilim %14 dolu.\", \"Pilim y\u00fczde 14 dolu.\", \"tr\"),\n        (\"Az akkumul\u00e1torom t\u00f6lt\u00f6tts\u00e9ge 14%\", \"Az akkumul\u00e1torom t\u00f6lt\u00f6tts\u00e9ge 14 sz\u00e1zal\u00e9k\", \"hu\"),\n        (\"\ubc30\ud130\ub9ac \uc794\ub7c9\uc774 14%\uc785\ub2c8\ub2e4.\", \"\ubc30\ud130\ub9ac \uc794\ub7c9\uc774 14 \ud37c\uc13c\ud2b8\uc785\ub2c8\ub2e4.\", \"ko\"),\n    ]\n\n    for a, b, lang in test_cases:\n        out = expand_symbols_multilingual(a, lang=lang)\n        assert out == b, f\"'{out}' vs '{b}'\"\n\n\nif __name__ == \"__main__\":\n    test_expand_numbers_multilingual()\n    test_abbreviations_multilingual()\n    test_symbols_multilingual()\n", "TTS/tts/layers/xtts/trainer/gpt_trainer.py": "from dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torchaudio\nfrom coqpit import Coqpit\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom trainer.torch import DistributedSampler\nfrom trainer.trainer_utils import get_optimizer, get_scheduler\n\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.datasets.dataset import TTSDataset\nfrom TTS.tts.layers.tortoise.arch_utils import TorchMelSpectrogram\nfrom TTS.tts.layers.xtts.dvae import DiscreteVAE\nfrom TTS.tts.layers.xtts.tokenizer import VoiceBpeTokenizer\nfrom TTS.tts.layers.xtts.trainer.dataset import XTTSDataset\nfrom TTS.tts.models.base_tts import BaseTTS\nfrom TTS.tts.models.xtts import Xtts, XttsArgs, XttsAudioConfig\nfrom TTS.utils.io import load_fsspec\n\n\n@dataclass\nclass GPTTrainerConfig(XttsConfig):\n    lr: float = 5e-06\n    training_seed: int = 1\n    optimizer_wd_only_on_weights: bool = False\n    weighted_loss_attrs: dict = field(default_factory=lambda: {})\n    weighted_loss_multipliers: dict = field(default_factory=lambda: {})\n    test_sentences: List[dict] = field(default_factory=lambda: [])\n\n\n@dataclass\nclass XttsAudioConfig(XttsAudioConfig):\n    dvae_sample_rate: int = 22050\n\n\n@dataclass\nclass GPTArgs(XttsArgs):\n    min_conditioning_length: int = 66150\n    max_conditioning_length: int = 132300\n    gpt_loss_text_ce_weight: float = 0.01\n    gpt_loss_mel_ce_weight: float = 1.0\n    gpt_num_audio_tokens: int = 8194\n    debug_loading_failures: bool = False\n    max_wav_length: int = 255995  # ~11.6 seconds\n    max_text_length: int = 200\n    tokenizer_file: str = \"\"\n    mel_norm_file: str = \"https://coqui.gateway.scarf.sh/v0.14.0_models/mel_norms.pth\"\n    dvae_checkpoint: str = \"\"\n    xtts_checkpoint: str = \"\"\n    gpt_checkpoint: str = \"\"  # if defined it will replace the gpt weights on xtts model\n    vocoder: str = \"\"  # overide vocoder key on the config to avoid json write issues\n\n\ndef callback_clearml_load_save(operation_type, model_info):\n    # return None means skip the file upload/log, returning model_info will continue with the log/upload\n    # you can also change the upload destination file name model_info.upload_filename or check the local file size with Path(model_info.local_model_path).stat().st_size\n    assert operation_type in (\"load\", \"save\")\n    # print(operation_type, model_info.__dict__)\n\n    if \"similarities.pth\" in model_info.__dict__[\"local_model_path\"]:\n        return None\n\n    return model_info\n\n\nclass GPTTrainer(BaseTTS):\n    def __init__(self, config: Coqpit):\n        \"\"\"\n        Tortoise GPT training class\n        \"\"\"\n        super().__init__(config, ap=None, tokenizer=None)\n        self.config = config\n        # init XTTS model\n        self.xtts = Xtts(self.config)\n        # create the tokenizer with the target vocabulary\n        self.xtts.tokenizer = VoiceBpeTokenizer(self.args.tokenizer_file)\n        # init gpt encoder and hifigan decoder\n        self.xtts.init_models()\n\n        if self.args.xtts_checkpoint:\n            self.load_checkpoint(self.config, self.args.xtts_checkpoint, eval=False, strict=False)\n\n        # set mel stats\n        if self.args.mel_norm_file:\n            self.xtts.mel_stats = load_fsspec(self.args.mel_norm_file)\n\n        # load GPT if available\n        if self.args.gpt_checkpoint:\n            gpt_checkpoint = torch.load(self.args.gpt_checkpoint, map_location=torch.device(\"cpu\"))\n            # deal with coqui Trainer exported model\n            if \"model\" in gpt_checkpoint.keys() and \"config\" in gpt_checkpoint.keys():\n                print(\"Coqui Trainer checkpoint detected! Converting it!\")\n                gpt_checkpoint = gpt_checkpoint[\"model\"]\n                states_keys = list(gpt_checkpoint.keys())\n                for key in states_keys:\n                    if \"gpt.\" in key:\n                        new_key = key.replace(\"gpt.\", \"\")\n                        gpt_checkpoint[new_key] = gpt_checkpoint[key]\n                        del gpt_checkpoint[key]\n                    else:\n                        del gpt_checkpoint[key]\n\n            # edit checkpoint if the number of tokens is changed to ensures the better transfer learning possible\n            if (\n                \"text_embedding.weight\" in gpt_checkpoint\n                and gpt_checkpoint[\"text_embedding.weight\"].shape != self.xtts.gpt.text_embedding.weight.shape\n            ):\n                num_new_tokens = (\n                    self.xtts.gpt.text_embedding.weight.shape[0] - gpt_checkpoint[\"text_embedding.weight\"].shape[0]\n                )\n                print(f\" > Loading checkpoint with {num_new_tokens} additional tokens.\")\n\n                # add new tokens to a linear layer (text_head)\n                emb_g = gpt_checkpoint[\"text_embedding.weight\"]\n                new_row = torch.randn(num_new_tokens, emb_g.shape[1])\n                start_token_row = emb_g[-1, :]\n                emb_g = torch.cat([emb_g, new_row], axis=0)\n                emb_g[-1, :] = start_token_row\n                gpt_checkpoint[\"text_embedding.weight\"] = emb_g\n\n                # add new weights to the linear layer (text_head)\n                text_head_weight = gpt_checkpoint[\"text_head.weight\"]\n                start_token_row = text_head_weight[-1, :]\n                new_entry = torch.randn(num_new_tokens, self.xtts.gpt.text_head.weight.shape[1])\n                text_head_weight = torch.cat([text_head_weight, new_entry], axis=0)\n                text_head_weight[-1, :] = start_token_row\n                gpt_checkpoint[\"text_head.weight\"] = text_head_weight\n\n                # add new biases to the linear layer (text_head)\n                text_head_bias = gpt_checkpoint[\"text_head.bias\"]\n                start_token_row = text_head_bias[-1]\n                new_bias_entry = torch.zeros(num_new_tokens)\n                text_head_bias = torch.cat([text_head_bias, new_bias_entry], axis=0)\n                text_head_bias[-1] = start_token_row\n                gpt_checkpoint[\"text_head.bias\"] = text_head_bias\n\n            self.xtts.gpt.load_state_dict(gpt_checkpoint, strict=True)\n            print(\">> GPT weights restored from:\", self.args.gpt_checkpoint)\n\n        # Mel spectrogram extractor for conditioning\n        if self.args.gpt_use_perceiver_resampler:\n            self.torch_mel_spectrogram_style_encoder = TorchMelSpectrogram(\n                filter_length=2048,\n                hop_length=256,\n                win_length=1024,\n                normalize=False,\n                sampling_rate=config.audio.sample_rate,\n                mel_fmin=0,\n                mel_fmax=8000,\n                n_mel_channels=80,\n                mel_norm_file=self.args.mel_norm_file,\n            )\n        else:\n            self.torch_mel_spectrogram_style_encoder = TorchMelSpectrogram(\n                filter_length=4096,\n                hop_length=1024,\n                win_length=4096,\n                normalize=False,\n                sampling_rate=config.audio.sample_rate,\n                mel_fmin=0,\n                mel_fmax=8000,\n                n_mel_channels=80,\n                mel_norm_file=self.args.mel_norm_file,\n            )\n\n        # Load DVAE\n        self.dvae = DiscreteVAE(\n            channels=80,\n            normalization=None,\n            positional_dims=1,\n            num_tokens=self.args.gpt_num_audio_tokens - 2,\n            codebook_dim=512,\n            hidden_dim=512,\n            num_resnet_blocks=3,\n            kernel_size=3,\n            num_layers=2,\n            use_transposed_convs=False,\n        )\n\n        self.dvae.eval()\n        if self.args.dvae_checkpoint:\n            dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n            self.dvae.load_state_dict(dvae_checkpoint, strict=False)\n            print(\">> DVAE weights restored from:\", self.args.dvae_checkpoint)\n        else:\n            raise RuntimeError(\n                \"You need to specify config.model_args.dvae_checkpoint path to be able to train the GPT decoder!!\"\n            )\n\n        # Mel spectrogram extractor for DVAE\n        self.torch_mel_spectrogram_dvae = TorchMelSpectrogram(\n            mel_norm_file=self.args.mel_norm_file, sampling_rate=config.audio.dvae_sample_rate\n        )\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_mels, cond_idxs, cond_lens):\n        \"\"\"\n        Forward pass that uses both text and voice in either text conditioning mode or voice conditioning mode\n        (actuated by `text_first`).\n\n        text_inputs: long tensor, (b,t)\n        text_lengths: long tensor, (b,)\n        mel_inputs:  long tensor, (b,m)\n        wav_lengths: long tensor, (b,)\n        cond_mels: MEL float tensor, (b, num_samples, 80,t_m)\n        cond_idxs: cond start and end indexs, (b, 2)\n        cond_lens: long tensor, (b,)\n        \"\"\"\n        losses = self.xtts.gpt(\n            text_inputs,\n            text_lengths,\n            audio_codes,\n            wav_lengths,\n            cond_mels=cond_mels,\n            cond_idxs=cond_idxs,\n            cond_lens=cond_lens,\n        )\n        return losses\n\n    @torch.no_grad()\n    def test_run(self, assets) -> Tuple[Dict, Dict]:  # pylint: disable=W0613\n        test_audios = {}\n        if self.config.test_sentences:\n            # init gpt for inference mode\n            self.xtts.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache, use_deepspeed=False)\n            self.xtts.gpt.eval()\n            print(\" | > Synthesizing test sentences.\")\n            for idx, s_info in enumerate(self.config.test_sentences):\n                wav = self.xtts.synthesize(\n                    s_info[\"text\"],\n                    self.config,\n                    s_info[\"speaker_wav\"],\n                    s_info[\"language\"],\n                    gpt_cond_len=3,\n                )[\"wav\"]\n                test_audios[\"{}-audio\".format(idx)] = wav\n\n            # delete inference layers\n            del self.xtts.gpt.gpt_inference\n            del self.xtts.gpt.gpt.wte\n        return {\"audios\": test_audios}\n\n    def test_log(\n        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n    ) -> None:\n        logger.test_audios(steps, outputs[\"audios\"], self.args.output_sample_rate)\n\n    def format_batch(self, batch: Dict) -> Dict:\n        return batch\n\n    @torch.no_grad()  # torch no grad to avoid gradients from the pre-processing and DVAE codes extraction\n    def format_batch_on_device(self, batch):\n        \"\"\"Compute spectrograms on the device.\"\"\"\n        batch[\"text_lengths\"] = batch[\"text_lengths\"]\n        batch[\"wav_lengths\"] = batch[\"wav_lengths\"]\n        batch[\"text_inputs\"] = batch[\"padded_text\"]\n        batch[\"cond_idxs\"] = batch[\"cond_idxs\"]\n        # compute conditioning mel specs\n        # transform waves from torch.Size([B, num_cond_samples, 1, T] to torch.Size([B * num_cond_samples, 1, T] because if is faster than iterate the tensor\n        B, num_cond_samples, C, T = batch[\"conditioning\"].size()\n        conditioning_reshaped = batch[\"conditioning\"].view(B * num_cond_samples, C, T)\n        paired_conditioning_mel = self.torch_mel_spectrogram_style_encoder(conditioning_reshaped)\n        # transform torch.Size([B * num_cond_samples, n_mel, T_mel]) in torch.Size([B, num_cond_samples, n_mel, T_mel])\n        n_mel = self.torch_mel_spectrogram_style_encoder.n_mel_channels  # paired_conditioning_mel.size(1)\n        T_mel = paired_conditioning_mel.size(2)\n        paired_conditioning_mel = paired_conditioning_mel.view(B, num_cond_samples, n_mel, T_mel)\n        # get the conditioning embeddings\n        batch[\"cond_mels\"] = paired_conditioning_mel\n        # compute codes using DVAE\n        if self.config.audio.sample_rate != self.config.audio.dvae_sample_rate:\n            dvae_wav = torchaudio.functional.resample(\n                batch[\"wav\"],\n                orig_freq=self.config.audio.sample_rate,\n                new_freq=self.config.audio.dvae_sample_rate,\n                lowpass_filter_width=64,\n                rolloff=0.9475937167399596,\n                resampling_method=\"kaiser_window\",\n                beta=14.769656459379492,\n            )\n        else:\n            dvae_wav = batch[\"wav\"]\n        dvae_mel_spec = self.torch_mel_spectrogram_dvae(dvae_wav)\n        codes = self.dvae.get_codebook_indices(dvae_mel_spec)\n\n        batch[\"audio_codes\"] = codes\n        # delete useless batch tensors\n        del batch[\"padded_text\"]\n        del batch[\"wav\"]\n        del batch[\"conditioning\"]\n        return batch\n\n    def train_step(self, batch, criterion):\n        loss_dict = {}\n        cond_mels = batch[\"cond_mels\"]\n        text_inputs = batch[\"text_inputs\"]\n        text_lengths = batch[\"text_lengths\"]\n        audio_codes = batch[\"audio_codes\"]\n        wav_lengths = batch[\"wav_lengths\"]\n        cond_idxs = batch[\"cond_idxs\"]\n        cond_lens = batch[\"cond_lens\"]\n\n        loss_text, loss_mel, _ = self.forward(\n            text_inputs, text_lengths, audio_codes, wav_lengths, cond_mels, cond_idxs, cond_lens\n        )\n        loss_dict[\"loss_text_ce\"] = loss_text * self.args.gpt_loss_text_ce_weight\n        loss_dict[\"loss_mel_ce\"] = loss_mel * self.args.gpt_loss_mel_ce_weight\n        loss_dict[\"loss\"] = loss_dict[\"loss_text_ce\"] + loss_dict[\"loss_mel_ce\"]\n        return {\"model_outputs\": None}, loss_dict\n\n    def eval_step(self, batch, criterion):\n        # ignore masking for more consistent evaluation\n        batch[\"cond_idxs\"] = None\n        return self.train_step(batch, criterion)\n\n    def on_train_epoch_start(self, trainer):\n        trainer.model.eval()  # the whole model to eval\n        # put gpt model in training mode\n        if hasattr(trainer.model, \"module\") and hasattr(trainer.model.module, \"xtts\"):\n            trainer.model.module.xtts.gpt.train()\n        else:\n            trainer.model.xtts.gpt.train()\n\n    def on_init_end(self, trainer):  # pylint: disable=W0613\n        # ignore similarities.pth on clearml save/upload\n        if self.config.dashboard_logger.lower() == \"clearml\":\n            from clearml.binding.frameworks import WeightsFileHandler\n\n            WeightsFileHandler.add_pre_callback(callback_clearml_load_save)\n\n    @torch.no_grad()\n    def inference(\n        self,\n        x,\n        aux_input=None,\n    ):  # pylint: disable=dangerous-default-value\n        return None\n\n    @staticmethod\n    def get_criterion():\n        return None\n\n    def get_sampler(self, dataset: TTSDataset, num_gpus=1):\n        # sampler for DDP\n        batch_sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        return batch_sampler\n\n    def get_data_loader(\n        self,\n        config: Coqpit,\n        assets: Dict,\n        is_eval: bool,\n        samples: Union[List[Dict], List[List]],\n        verbose: bool,\n        num_gpus: int,\n        rank: int = None,\n    ) -> \"DataLoader\":  # pylint: disable=W0613\n        if is_eval and not config.run_eval:\n            loader = None\n        else:\n            # init dataloader\n            dataset = XTTSDataset(self.config, samples, self.xtts.tokenizer, config.audio.sample_rate, is_eval)\n\n            # wait all the DDP process to be ready\n            if num_gpus > 1:\n                torch.distributed.barrier()\n\n            # sort input sequences from short to long\n            # dataset.preprocess_samples()\n\n            # get samplers\n            sampler = self.get_sampler(dataset, num_gpus)\n\n            # ignore sampler when is eval because if we changed the sampler parameter we will not be able to compare previous runs\n            if sampler is None or is_eval:\n                loader = DataLoader(\n                    dataset,\n                    batch_size=config.eval_batch_size if is_eval else config.batch_size,\n                    shuffle=False,\n                    drop_last=False,\n                    collate_fn=dataset.collate_fn,\n                    num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                    pin_memory=False,\n                )\n            else:\n                loader = DataLoader(\n                    dataset,\n                    sampler=sampler,\n                    batch_size = config.eval_batch_size if is_eval else config.batch_size,\n                    collate_fn=dataset.collate_fn,\n                    num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n                    pin_memory=False,\n                )\n        return loader\n\n    def get_optimizer(self) -> List:\n        \"\"\"Initiate and return the optimizer based on the config parameters.\"\"\"\n        # ToDo: deal with multi GPU training\n        if self.config.optimizer_wd_only_on_weights:\n            # parameters to only GPT model\n            net = self.xtts.gpt\n\n            # normalizations\n            norm_modules = (\n                nn.BatchNorm2d,\n                nn.InstanceNorm2d,\n                nn.BatchNorm1d,\n                nn.InstanceNorm1d,\n                nn.BatchNorm3d,\n                nn.InstanceNorm3d,\n                nn.GroupNorm,\n                nn.LayerNorm,\n            )\n            # nn.Embedding\n            emb_modules = (nn.Embedding, nn.EmbeddingBag)\n\n            param_names_notweights = set()\n            all_param_names = set()\n            param_map = {}\n            for mn, m in net.named_modules():\n                for k, v in m.named_parameters():\n                    v.is_bias = k.endswith(\".bias\")\n                    v.is_weight = k.endswith(\".weight\")\n                    v.is_norm = isinstance(m, norm_modules)\n                    v.is_emb = isinstance(m, emb_modules)\n\n                    fpn = \"%s.%s\" % (mn, k) if mn else k  # full param name\n                    all_param_names.add(fpn)\n                    param_map[fpn] = v\n                    if v.is_bias or v.is_norm or v.is_emb:\n                        param_names_notweights.add(fpn)\n\n            params_names_notweights = sorted(list(param_names_notweights))\n            params_notweights = [param_map[k] for k in params_names_notweights]\n            params_names_weights = sorted(list(all_param_names ^ param_names_notweights))\n            params_weights = [param_map[k] for k in params_names_weights]\n\n            groups = [\n                {\"params\": params_weights, \"weight_decay\": self.config.optimizer_params[\"weight_decay\"]},\n                {\"params\": params_notweights, \"weight_decay\": 0},\n            ]\n            # torch.optim.AdamW\n            opt = get_optimizer(\n                self.config.optimizer,\n                self.config.optimizer_params,\n                self.config.lr,\n                parameters=groups,\n            )\n            opt._group_names = [params_names_weights, params_names_notweights]\n            return opt\n\n        return get_optimizer(\n            self.config.optimizer,\n            self.config.optimizer_params,\n            self.config.lr,\n            # optimize only for the GPT model\n            parameters=self.xtts.gpt.parameters(),\n        )\n\n    def get_scheduler(self, optimizer) -> List:\n        \"\"\"Set the scheduler for the optimizer.\n\n        Args:\n            optimizer: `torch.optim.Optimizer`.\n        \"\"\"\n        return get_scheduler(self.config.lr_scheduler, self.config.lr_scheduler_params, optimizer)\n\n    def load_checkpoint(\n        self,\n        config,\n        checkpoint_path,\n        eval=False,\n        strict=True,\n        cache_storage=\"/tmp/tts_cache\",\n        target_protocol=\"s3\",\n        target_options={\"anon\": True},\n    ):  # pylint: disable=unused-argument, disable=W0201, disable=W0102, redefined-builtin\n        \"\"\"Load the model checkpoint and setup for training or inference\"\"\"\n\n        state = self.xtts.get_compatible_checkpoint_state_dict(checkpoint_path)\n\n        # load the model weights\n        self.xtts.load_state_dict(state, strict=strict)\n\n        if eval:\n            self.xtts.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache, use_deepspeed=False)\n            self.eval()\n            assert not self.training\n\n    @staticmethod\n    def init_from_config(config: \"GPTTrainerConfig\", samples: Union[List[List], List[Dict]] = None):\n        \"\"\"Initiate model from config\n\n        Args:\n            config (GPTTrainerConfig): Model config.\n            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n                Defaults to None.\n        \"\"\"\n        return GPTTrainer(config)\n", "TTS/tts/layers/vits/stochastic_duration_predictor.py": "import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom TTS.tts.layers.generic.normalization import LayerNorm2\nfrom TTS.tts.layers.vits.transforms import piecewise_rational_quadratic_transform\n\n\nclass DilatedDepthSeparableConv(nn.Module):\n    def __init__(self, channels, kernel_size, num_layers, dropout_p=0.0) -> torch.tensor:\n        \"\"\"Dilated Depth-wise Separable Convolution module.\n\n        ::\n            x |-> DDSConv(x) -> LayerNorm(x) -> GeLU(x) -> Conv1x1(x) -> LayerNorm(x) -> GeLU(x) -> + -> o\n              |-------------------------------------------------------------------------------------^\n\n        Args:\n            channels ([type]): [description]\n            kernel_size ([type]): [description]\n            num_layers ([type]): [description]\n            dropout_p (float, optional): [description]. Defaults to 0.0.\n\n        Returns:\n            torch.tensor: Network output masked by the input sequence mask.\n        \"\"\"\n        super().__init__()\n        self.num_layers = num_layers\n\n        self.convs_sep = nn.ModuleList()\n        self.convs_1x1 = nn.ModuleList()\n        self.norms_1 = nn.ModuleList()\n        self.norms_2 = nn.ModuleList()\n        for i in range(num_layers):\n            dilation = kernel_size**i\n            padding = (kernel_size * dilation - dilation) // 2\n            self.convs_sep.append(\n                nn.Conv1d(channels, channels, kernel_size, groups=channels, dilation=dilation, padding=padding)\n            )\n            self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n            self.norms_1.append(LayerNorm2(channels))\n            self.norms_2.append(LayerNorm2(channels))\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x, x_mask, g=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n        \"\"\"\n        if g is not None:\n            x = x + g\n        for i in range(self.num_layers):\n            y = self.convs_sep[i](x * x_mask)\n            y = self.norms_1[i](y)\n            y = F.gelu(y)\n            y = self.convs_1x1[i](y)\n            y = self.norms_2[i](y)\n            y = F.gelu(y)\n            y = self.dropout(y)\n            x = x + y\n        return x * x_mask\n\n\nclass ElementwiseAffine(nn.Module):\n    \"\"\"Element-wise affine transform like no-population stats BatchNorm alternative.\n\n    Args:\n        channels (int): Number of input tensor channels.\n    \"\"\"\n\n    def __init__(self, channels):\n        super().__init__()\n        self.translation = nn.Parameter(torch.zeros(channels, 1))\n        self.log_scale = nn.Parameter(torch.zeros(channels, 1))\n\n    def forward(self, x, x_mask, reverse=False, **kwargs):  # pylint: disable=unused-argument\n        if not reverse:\n            y = (x * torch.exp(self.log_scale) + self.translation) * x_mask\n            logdet = torch.sum(self.log_scale * x_mask, [1, 2])\n            return y, logdet\n        x = (x - self.translation) * torch.exp(-self.log_scale) * x_mask\n        return x\n\n\nclass ConvFlow(nn.Module):\n    \"\"\"Dilated depth separable convolutional based spline flow.\n\n    Args:\n        in_channels (int): Number of input tensor channels.\n        hidden_channels (int): Number of in network channels.\n        kernel_size (int): Convolutional kernel size.\n        num_layers (int): Number of convolutional layers.\n        num_bins (int, optional): Number of spline bins. Defaults to 10.\n        tail_bound (float, optional): Tail bound for PRQT. Defaults to 5.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        num_layers: int,\n        num_bins=10,\n        tail_bound=5.0,\n    ):\n        super().__init__()\n        self.num_bins = num_bins\n        self.tail_bound = tail_bound\n        self.hidden_channels = hidden_channels\n        self.half_channels = in_channels // 2\n\n        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n        self.convs = DilatedDepthSeparableConv(hidden_channels, kernel_size, num_layers, dropout_p=0.0)\n        self.proj = nn.Conv1d(hidden_channels, self.half_channels * (num_bins * 3 - 1), 1)\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n        h = self.pre(x0)\n        h = self.convs(h, x_mask, g=g)\n        h = self.proj(h) * x_mask\n\n        b, c, t = x0.shape\n        h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n\n        unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.hidden_channels)\n        unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(self.hidden_channels)\n        unnormalized_derivatives = h[..., 2 * self.num_bins :]\n\n        x1, logabsdet = piecewise_rational_quadratic_transform(\n            x1,\n            unnormalized_widths,\n            unnormalized_heights,\n            unnormalized_derivatives,\n            inverse=reverse,\n            tails=\"linear\",\n            tail_bound=self.tail_bound,\n        )\n\n        x = torch.cat([x0, x1], 1) * x_mask\n        logdet = torch.sum(logabsdet * x_mask, [1, 2])\n        if not reverse:\n            return x, logdet\n        return x\n\n\nclass StochasticDurationPredictor(nn.Module):\n    \"\"\"Stochastic duration predictor with Spline Flows.\n\n    It applies Variational Dequantization and Variational Data Augmentation.\n\n    Paper:\n        SDP: https://arxiv.org/pdf/2106.06103.pdf\n        Spline Flow: https://arxiv.org/abs/1906.04032\n\n    ::\n        ## Inference\n\n        x -> TextCondEncoder() -> Flow() -> dr_hat\n        noise ----------------------^\n\n        ## Training\n                                                                              |---------------------|\n        x -> TextCondEncoder() -> + -> PosteriorEncoder() -> split() -> z_u, z_v -> (d - z_u) -> concat() -> Flow() -> noise\n        d -> DurCondEncoder()  -> ^                                                    |\n        |------------------------------------------------------------------------------|\n\n    Args:\n        in_channels (int): Number of input tensor channels.\n        hidden_channels (int): Number of hidden channels.\n        kernel_size (int): Kernel size of convolutional layers.\n        dropout_p (float): Dropout rate.\n        num_flows (int, optional): Number of flow blocks. Defaults to 4.\n        cond_channels (int, optional): Number of channels of conditioning tensor. Defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        dropout_p: float,\n        num_flows=4,\n        cond_channels=0,\n        language_emb_dim=0,\n    ):\n        super().__init__()\n\n        # add language embedding dim in the input\n        if language_emb_dim:\n            in_channels += language_emb_dim\n\n        # condition encoder text\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.convs = DilatedDepthSeparableConv(hidden_channels, kernel_size, num_layers=3, dropout_p=dropout_p)\n        self.proj = nn.Conv1d(hidden_channels, hidden_channels, 1)\n\n        # posterior encoder\n        self.flows = nn.ModuleList()\n        self.flows.append(ElementwiseAffine(2))\n        self.flows += [ConvFlow(2, hidden_channels, kernel_size, num_layers=3) for _ in range(num_flows)]\n\n        # condition encoder duration\n        self.post_pre = nn.Conv1d(1, hidden_channels, 1)\n        self.post_convs = DilatedDepthSeparableConv(hidden_channels, kernel_size, num_layers=3, dropout_p=dropout_p)\n        self.post_proj = nn.Conv1d(hidden_channels, hidden_channels, 1)\n\n        # flow layers\n        self.post_flows = nn.ModuleList()\n        self.post_flows.append(ElementwiseAffine(2))\n        self.post_flows += [ConvFlow(2, hidden_channels, kernel_size, num_layers=3) for _ in range(num_flows)]\n\n        if cond_channels != 0 and cond_channels is not None:\n            self.cond = nn.Conv1d(cond_channels, hidden_channels, 1)\n\n        if language_emb_dim != 0 and language_emb_dim is not None:\n            self.cond_lang = nn.Conv1d(language_emb_dim, hidden_channels, 1)\n\n    def forward(self, x, x_mask, dr=None, g=None, lang_emb=None, reverse=False, noise_scale=1.0):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n            - dr: :math:`[B, 1, T]`\n            - g: :math:`[B, C]`\n        \"\"\"\n        # condition encoder text\n        x = self.pre(x)\n        if g is not None:\n            x = x + self.cond(g)\n\n        if lang_emb is not None:\n            x = x + self.cond_lang(lang_emb)\n\n        x = self.convs(x, x_mask)\n        x = self.proj(x) * x_mask\n\n        if not reverse:\n            flows = self.flows\n            assert dr is not None\n\n            # condition encoder duration\n            h = self.post_pre(dr)\n            h = self.post_convs(h, x_mask)\n            h = self.post_proj(h) * x_mask\n            noise = torch.randn(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n            z_q = noise\n\n            # posterior encoder\n            logdet_tot_q = 0.0\n            for idx, flow in enumerate(self.post_flows):\n                z_q, logdet_q = flow(z_q, x_mask, g=(x + h))\n                logdet_tot_q = logdet_tot_q + logdet_q\n                if idx > 0:\n                    z_q = torch.flip(z_q, [1])\n\n            z_u, z_v = torch.split(z_q, [1, 1], 1)\n            u = torch.sigmoid(z_u) * x_mask\n            z0 = (dr - u) * x_mask\n\n            # posterior encoder - neg log likelihood\n            logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1, 2])\n            nll_posterior_encoder = (\n                torch.sum(-0.5 * (math.log(2 * math.pi) + (noise**2)) * x_mask, [1, 2]) - logdet_tot_q\n            )\n\n            z0 = torch.log(torch.clamp_min(z0, 1e-5)) * x_mask\n            logdet_tot = torch.sum(-z0, [1, 2])\n            z = torch.cat([z0, z_v], 1)\n\n            # flow layers\n            for idx, flow in enumerate(flows):\n                z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n                logdet_tot = logdet_tot + logdet\n                if idx > 0:\n                    z = torch.flip(z, [1])\n\n            # flow layers - neg log likelihood\n            nll_flow_layers = torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2]) - logdet_tot\n            return nll_flow_layers + nll_posterior_encoder\n\n        flows = list(reversed(self.flows))\n        flows = flows[:-2] + [flows[-1]]  # remove a useless vflow\n        z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n        for flow in flows:\n            z = torch.flip(z, [1])\n            z = flow(z, x_mask, g=x, reverse=reverse)\n\n        z0, _ = torch.split(z, [1, 1], 1)\n        logw = z0\n        return logw\n", "TTS/tts/layers/vits/transforms.py": "# adopted from https://github.com/bayesiains/nflows\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\n\nDEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\n\n\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\"tails\": tails, \"tail_bound\": tail_bound}\n\n    outputs, logabsdet = spline_fn(\n        inputs=inputs,\n        unnormalized_widths=unnormalized_widths,\n        unnormalized_heights=unnormalized_heights,\n        unnormalized_derivatives=unnormalized_derivatives,\n        inverse=inverse,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative,\n        **spline_kwargs,\n    )\n    return outputs, logabsdet\n\n\ndef searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n\n\ndef unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == \"linear\":\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError(\"{} tails are not implemented.\".format(tails))\n\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound,\n        right=tail_bound,\n        bottom=-tail_bound,\n        top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative,\n    )\n\n    return outputs, logabsdet\n\n\ndef rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    left=0.0,\n    right=1.0,\n    bottom=0.0,\n    top=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    if torch.min(inputs) < left or torch.max(inputs) > right:\n        raise ValueError(\"Input to a transform is not within its domain\")\n\n    num_bins = unnormalized_widths.shape[-1]\n\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError(\"Minimal bin width too large for the number of bins\")\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError(\"Minimal bin height too large for the number of bins\")\n\n    widths = F.softmax(unnormalized_widths, dim=-1)\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n    cumwidths = torch.cumsum(widths, dim=-1)\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n    cumwidths = (right - left) * cumwidths + left\n    cumwidths[..., 0] = left\n    cumwidths[..., -1] = right\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\n    heights = F.softmax(unnormalized_heights, dim=-1)\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n    cumheights = torch.cumsum(heights, dim=-1)\n    cumheights = F.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n    cumheights = (top - bottom) * cumheights + bottom\n    cumheights[..., 0] = bottom\n    cumheights[..., -1] = top\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\n\n    if inverse:\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\n    else:\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n    delta = heights / widths\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\n    if inverse:\n        a = (inputs - input_cumheights) * (\n            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n        ) + input_heights * (input_delta - input_derivatives)\n        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n        )\n        c = -input_delta * (inputs - input_cumheights)\n\n        discriminant = b.pow(2) - 4 * a * c\n        assert (discriminant >= 0).all()\n\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\n        outputs = root * input_bin_widths + input_cumwidths\n\n        theta_one_minus_theta = root * (1 - root)\n        denominator = input_delta + (\n            (input_derivatives + input_derivatives_plus_one - 2 * input_delta) * theta_one_minus_theta\n        )\n        derivative_numerator = input_delta.pow(2) * (\n            input_derivatives_plus_one * root.pow(2)\n            + 2 * input_delta * theta_one_minus_theta\n            + input_derivatives * (1 - root).pow(2)\n        )\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, -logabsdet\n    else:\n        theta = (inputs - input_cumwidths) / input_bin_widths\n        theta_one_minus_theta = theta * (1 - theta)\n\n        numerator = input_heights * (input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta)\n        denominator = input_delta + (\n            (input_derivatives + input_derivatives_plus_one - 2 * input_delta) * theta_one_minus_theta\n        )\n        outputs = input_cumheights + numerator / denominator\n\n        derivative_numerator = input_delta.pow(2) * (\n            input_derivatives_plus_one * theta.pow(2)\n            + 2 * input_delta * theta_one_minus_theta\n            + input_derivatives * (1 - theta).pow(2)\n        )\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, logabsdet\n", "TTS/tts/layers/vits/discriminator.py": "import torch\nfrom torch import nn\nfrom torch.nn.modules.conv import Conv1d\n\nfrom TTS.vocoder.models.hifigan_discriminator import DiscriminatorP, MultiPeriodDiscriminator\n\n\nclass DiscriminatorS(torch.nn.Module):\n    \"\"\"HiFiGAN Scale Discriminator. Channel sizes are different from the original HiFiGAN.\n\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n    \"\"\"\n\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        norm_f = nn.utils.spectral_norm if use_spectral_norm else nn.utils.parametrizations.weight_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n            ]\n        )\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.\n\n        Returns:\n            Tensor: discriminator scores.\n            List[Tensor]: list of features from the convolutiona layers.\n        \"\"\"\n        feat = []\n        for l in self.convs:\n            x = l(x)\n            x = torch.nn.functional.leaky_relu(x, 0.1)\n            feat.append(x)\n        x = self.conv_post(x)\n        feat.append(x)\n        x = torch.flatten(x, 1, -1)\n        return x, feat\n\n\nclass VitsDiscriminator(nn.Module):\n    \"\"\"VITS discriminator wrapping one Scale Discriminator and a stack of Period Discriminator.\n\n    ::\n        waveform -> ScaleDiscriminator() -> scores_sd, feats_sd --> append() -> scores, feats\n               |--> MultiPeriodDiscriminator() -> scores_mpd, feats_mpd ^\n\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n    \"\"\"\n\n    def __init__(self, periods=(2, 3, 5, 7, 11), use_spectral_norm=False):\n        super().__init__()\n        self.nets = nn.ModuleList()\n        self.nets.append(DiscriminatorS(use_spectral_norm=use_spectral_norm))\n        self.nets.extend([DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods])\n\n    def forward(self, x, x_hat=None):\n        \"\"\"\n        Args:\n            x (Tensor): ground truth waveform.\n            x_hat (Tensor): predicted waveform.\n\n        Returns:\n            List[Tensor]: discriminator scores.\n            List[List[Tensor]]: list of list of features from each layers of each discriminator.\n        \"\"\"\n        x_scores = []\n        x_hat_scores = [] if x_hat is not None else None\n        x_feats = []\n        x_hat_feats = [] if x_hat is not None else None\n        for net in self.nets:\n            x_score, x_feat = net(x)\n            x_scores.append(x_score)\n            x_feats.append(x_feat)\n            if x_hat is not None:\n                x_hat_score, x_hat_feat = net(x_hat)\n                x_hat_scores.append(x_hat_score)\n                x_hat_feats.append(x_hat_feat)\n        return x_scores, x_feats, x_hat_scores, x_hat_feats\n", "TTS/tts/layers/vits/networks.py": "import math\n\nimport torch\nfrom torch import nn\n\nfrom TTS.tts.layers.glow_tts.glow import WN\nfrom TTS.tts.layers.glow_tts.transformer import RelativePositionTransformer\nfrom TTS.tts.utils.helpers import sequence_mask\n\nLRELU_SLOPE = 0.1\n\n\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\n\n\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\n\n\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\n\n\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        n_vocab: int,\n        out_channels: int,\n        hidden_channels: int,\n        hidden_channels_ffn: int,\n        num_heads: int,\n        num_layers: int,\n        kernel_size: int,\n        dropout_p: float,\n        language_emb_dim: int = None,\n    ):\n        \"\"\"Text Encoder for VITS model.\n\n        Args:\n            n_vocab (int): Number of characters for the embedding layer.\n            out_channels (int): Number of channels for the output.\n            hidden_channels (int): Number of channels for the hidden layers.\n            hidden_channels_ffn (int): Number of channels for the convolutional layers.\n            num_heads (int): Number of attention heads for the Transformer layers.\n            num_layers (int): Number of Transformer layers.\n            kernel_size (int): Kernel size for the FFN layers in Transformer network.\n            dropout_p (float): Dropout rate for the Transformer layers.\n        \"\"\"\n        super().__init__()\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n\n        self.emb = nn.Embedding(n_vocab, hidden_channels)\n\n        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n        if language_emb_dim:\n            hidden_channels += language_emb_dim\n\n        self.encoder = RelativePositionTransformer(\n            in_channels=hidden_channels,\n            out_channels=hidden_channels,\n            hidden_channels=hidden_channels,\n            hidden_channels_ffn=hidden_channels_ffn,\n            num_heads=num_heads,\n            num_layers=num_layers,\n            kernel_size=kernel_size,\n            dropout_p=dropout_p,\n            layer_norm_type=\"2\",\n            rel_attn_window_size=4,\n        )\n\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, lang_emb=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, T]`\n            - x_length: :math:`[B]`\n        \"\"\"\n        assert x.shape[0] == x_lengths.shape[0]\n        x = self.emb(x) * math.sqrt(self.hidden_channels)  # [b, t, h]\n\n        # concat the lang emb in embedding chars\n        if lang_emb is not None:\n            x = torch.cat((x, lang_emb.transpose(2, 1).expand(x.size(0), x.size(1), -1)), dim=-1)\n\n        x = torch.transpose(x, 1, -1)  # [b, h, t]\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)  # [b, 1, t]\n\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return x, m, logs, x_mask\n\n\nclass ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_layers,\n        dropout_p=0,\n        cond_channels=0,\n        mean_only=False,\n    ):\n        assert channels % 2 == 0, \"channels should be divisible by 2\"\n        super().__init__()\n        self.half_channels = channels // 2\n        self.mean_only = mean_only\n        # input layer\n        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n        # coupling layers\n        self.enc = WN(\n            hidden_channels,\n            hidden_channels,\n            kernel_size,\n            dilation_rate,\n            num_layers,\n            dropout_p=dropout_p,\n            c_in_channels=cond_channels,\n        )\n        # output layer\n        # Initializing last layer to 0 makes the affine coupling layers\n        # do nothing at first.  This helps with training stability\n        self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n        self.post.weight.data.zero_()\n        self.post.bias.data.zero_()\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        \"\"\"\n        Note:\n            Set `reverse` to True for inference.\n\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n            - g: :math:`[B, C, 1]`\n        \"\"\"\n        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n        h = self.pre(x0) * x_mask\n        h = self.enc(h, x_mask, g=g)\n        stats = self.post(h) * x_mask\n        if not self.mean_only:\n            m, log_scale = torch.split(stats, [self.half_channels] * 2, 1)\n        else:\n            m = stats\n            log_scale = torch.zeros_like(m)\n\n        if not reverse:\n            x1 = m + x1 * torch.exp(log_scale) * x_mask\n            x = torch.cat([x0, x1], 1)\n            logdet = torch.sum(log_scale, [1, 2])\n            return x, logdet\n        else:\n            x1 = (x1 - m) * torch.exp(-log_scale) * x_mask\n            x = torch.cat([x0, x1], 1)\n            return x\n\n\nclass ResidualCouplingBlocks(nn.Module):\n    def __init__(\n        self,\n        channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        dilation_rate: int,\n        num_layers: int,\n        num_flows=4,\n        cond_channels=0,\n    ):\n        \"\"\"Redisual Coupling blocks for VITS flow layers.\n\n        Args:\n            channels (int): Number of input and output tensor channels.\n            hidden_channels (int): Number of hidden network channels.\n            kernel_size (int): Kernel size of the WaveNet layers.\n            dilation_rate (int): Dilation rate of the WaveNet layers.\n            num_layers (int): Number of the WaveNet layers.\n            num_flows (int, optional): Number of Residual Coupling blocks. Defaults to 4.\n            cond_channels (int, optional): Number of channels of the conditioning tensor. Defaults to 0.\n        \"\"\"\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.num_layers = num_layers\n        self.num_flows = num_flows\n        self.cond_channels = cond_channels\n\n        self.flows = nn.ModuleList()\n        for _ in range(num_flows):\n            self.flows.append(\n                ResidualCouplingBlock(\n                    channels,\n                    hidden_channels,\n                    kernel_size,\n                    dilation_rate,\n                    num_layers,\n                    cond_channels=cond_channels,\n                    mean_only=True,\n                )\n            )\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        \"\"\"\n        Note:\n            Set `reverse` to True for inference.\n\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_mask: :math:`[B, 1, T]`\n            - g: :math:`[B, C, 1]`\n        \"\"\"\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n                x = torch.flip(x, [1])\n        else:\n            for flow in reversed(self.flows):\n                x = torch.flip(x, [1])\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n\n\nclass PosteriorEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        dilation_rate: int,\n        num_layers: int,\n        cond_channels=0,\n    ):\n        \"\"\"Posterior Encoder of VITS model.\n\n        ::\n            x -> conv1x1() -> WaveNet() (non-causal) -> conv1x1() -> split() -> [m, s] -> sample(m, s) -> z\n\n        Args:\n            in_channels (int): Number of input tensor channels.\n            out_channels (int): Number of output tensor channels.\n            hidden_channels (int): Number of hidden channels.\n            kernel_size (int): Kernel size of the WaveNet convolution layers.\n            dilation_rate (int): Dilation rate of the WaveNet layers.\n            num_layers (int): Number of the WaveNet layers.\n            cond_channels (int, optional): Number of conditioning tensor channels. Defaults to 0.\n        \"\"\"\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.num_layers = num_layers\n        self.cond_channels = cond_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = WN(\n            hidden_channels, hidden_channels, kernel_size, dilation_rate, num_layers, c_in_channels=cond_channels\n        )\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - x_lengths: :math:`[B, 1]`\n            - g: :math:`[B, C, 1]`\n        \"\"\"\n        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        mean, log_scale = torch.split(stats, self.out_channels, dim=1)\n        z = (mean + torch.randn_like(mean) * torch.exp(log_scale)) * x_mask\n        return z, mean, log_scale, x_mask\n", "TTS/tts/layers/tortoise/xtransformers.py": "import math\nfrom collections import namedtuple\nfrom functools import partial\nfrom inspect import isfunction\n\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom torch import einsum, nn\n\nDEFAULT_DIM_HEAD = 64\n\nIntermediates = namedtuple(\"Intermediates\", [\"pre_softmax_attn\", \"post_softmax_attn\"])\n\nLayerIntermediates = namedtuple(\n    \"Intermediates\",\n    [\n        \"hiddens\",\n        \"attn_intermediates\",\n        \"past_key_values\",\n    ],\n)\n\n\n# helpers\n\n\ndef exists(val):\n    return val is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\n\n\nclass always:\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\n\nclass not_equals:\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, x, *args, **kwargs):\n        return x != self.val\n\n\nclass equals:\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, x, *args, **kwargs):\n        return x == self.val\n\n\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n\n\ndef l2norm(t):\n    return F.normalize(t, p=2, dim=-1)\n\n\n# init helpers\n\n\ndef init_zero_(layer):\n    nn.init.constant_(layer.weight, 0.0)\n    if exists(layer.bias):\n        nn.init.constant_(layer.bias, 0.0)\n\n\n# keyword argument helpers\n\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\n\n# activations\n\n\nclass ReluSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2\n\n\n# positional embeddings\n\n\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(self, x):\n        n = torch.arange(x.shape[1], device=x.device)\n        pos_emb = self.emb(n)\n        pos_emb = rearrange(pos_emb, \"n d -> () n d\")\n        return pos_emb * self.scale\n\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, x, seq_dim=1, offset=0):\n        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq) + offset\n        sinusoid_inp = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        return rearrange(emb, \"n d -> () n d\")\n\n\nclass RelativePositionBias(nn.Module):\n    def __init__(self, scale, causal=False, num_buckets=32, max_distance=128, heads=8):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal=True, num_buckets=32, max_distance=128):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = (\n            max_exact\n            + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n        )\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, qk_dots):\n        i, j, device = *qk_dots.shape[-2:], qk_dots.device\n        q_pos = torch.arange(i, dtype=torch.long, device=device)\n        k_pos = torch.arange(j, dtype=torch.long, device=device)\n        rel_pos = k_pos[None, :] - q_pos[:, None]\n        rp_bucket = self._relative_position_bucket(\n            rel_pos, causal=self.causal, num_buckets=self.num_buckets, max_distance=self.max_distance\n        )\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, \"i j h -> () h i j\")\n        return qk_dots + (bias * self.scale)\n\n\nclass AlibiPositionalBias(nn.Module):\n    def __init__(self, heads, **kwargs):\n        super().__init__()\n        self.heads = heads\n        slopes = torch.Tensor(self._get_slopes(heads))\n        slopes = rearrange(slopes, \"h -> () h () ()\")\n        self.register_buffer(\"slopes\", slopes, persistent=False)\n        self.register_buffer(\"bias\", None, persistent=False)\n\n    @staticmethod\n    def _get_slopes(heads):\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio**i for i in range(n)]\n\n        if math.log2(heads).is_integer():\n            return get_slopes_power_of_2(heads)\n\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n        return (\n            get_slopes_power_of_2(closest_power_of_2)\n            + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][: heads - closest_power_of_2]\n        )\n\n    def forward(self, qk_dots):\n        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device\n\n        if exists(self.bias) and self.bias.shape[-1] >= j:\n            return qk_dots + self.bias[..., :j]\n\n        bias = torch.arange(j, device=device)\n        bias = rearrange(bias, \"j -> () () () j\")\n        bias = bias * self.slopes\n\n        num_heads_unalibied = h - bias.shape[1]\n        bias = F.pad(bias, (0, 0, 0, 0, 0, num_heads_unalibied))\n\n        self.register_buffer(\"bias\", bias, persistent=False)\n        return qk_dots + self.bias\n\n\nclass LearnedAlibiPositionalBias(AlibiPositionalBias):\n    def __init__(self, heads, bidirectional=False):\n        super().__init__(heads)\n        los_slopes = torch.log(self.slopes)\n        self.learned_logslopes = nn.Parameter(los_slopes)\n\n        self.bidirectional = bidirectional\n        if self.bidirectional:\n            self.learned_logslopes_future = nn.Parameter(los_slopes)\n\n    def forward(self, qk_dots):\n        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device\n\n        def get_slopes(param):\n            return F.pad(param.exp(), (0, 0, 0, 0, 0, h - param.shape[1]))\n\n        if exists(self.bias) and self.bias.shape[-1] >= j:\n            bias = self.bias[..., :i, :j]\n        else:\n            i_arange = torch.arange(i, device=device)\n            j_arange = torch.arange(j, device=device)\n            bias = rearrange(j_arange, \"j -> 1 1 1 j\") - rearrange(i_arange, \"i -> 1 1 i 1\")\n            self.register_buffer(\"bias\", bias, persistent=False)\n\n        if self.bidirectional:\n            past_slopes = get_slopes(self.learned_logslopes)\n            future_slopes = get_slopes(self.learned_logslopes_future)\n            bias = torch.tril(bias * past_slopes) + torch.triu(bias * future_slopes)\n        else:\n            slopes = get_slopes(self.learned_logslopes)\n            bias = bias * slopes\n\n        return qk_dots + bias\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, max_seq_len, device):\n        t = torch.arange(max_seq_len, device=device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        return rearrange(emb, \"n d -> () () n d\")\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n    x1, x2 = x.unbind(dim=-2)\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(t, freqs):\n    seq_len = t.shape[-2]\n    freqs = freqs[:, :, -seq_len:]\n    return (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n\n\n# norms\n\n\nclass Scale(nn.Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        scale_fn = lambda t: t * self.value\n\n        if not isinstance(out, tuple):\n            return scale_fn(out)\n\n        return (scale_fn(out[0]), *out[1:])\n\n\nclass Rezero(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        self.g = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        rezero_fn = lambda t: t * self.g\n\n        if not isinstance(out, tuple):\n            return rezero_fn(out)\n\n        return (rezero_fn(out[0]), *out[1:])\n\n\nclass ScaleNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1))\n\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\n\n\nclass RMSScaleShiftNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.scale_shift_process = nn.Linear(dim * 2, dim * 2)\n\n    def forward(self, x, norm_scale_shift_inp):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        norm = x / norm.clamp(min=self.eps) * self.g\n\n        ss_emb = self.scale_shift_process(norm_scale_shift_inp)\n        scale, shift = torch.chunk(ss_emb, 2, dim=1)\n        h = norm * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n        return h\n\n\n# residual and residual gates\n\n\nclass Residual(nn.Module):\n    def __init__(self, dim, scale_residual=False):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        return x + residual\n\n\nclass GRUGating(nn.Module):\n    def __init__(self, dim, scale_residual=False):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        gated_output = self.gru(rearrange(x, \"b n d -> (b n) d\"), rearrange(residual, \"b n d -> (b n) d\"))\n\n        return gated_output.reshape_as(x)\n\n\n# token shifting\n\n\ndef shift(t, amount, mask=None):\n    if amount == 0:\n        return t\n\n    if exists(mask):\n        t = t.masked_fill(~mask[..., None], 0.0)\n\n    return F.pad(t, (0, 0, amount, -amount), value=0.0)\n\n\nclass ShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        mask = kwargs.get(\"mask\", None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim=-1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = list(map(lambda args: shift(*args, mask=mask), zip(segments_to_shift, shifts)))\n        x = torch.cat((*segments_to_shift, *rest), dim=-1)\n        return self.fn(x, **kwargs)\n\n\n# feedforward\n\n\nclass GLU(nn.Module):\n    def __init__(self, dim_in, dim_out, activation):\n        super().__init__()\n        self.act = activation\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * self.act(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out=None,\n        mult=4,\n        glu=False,\n        relu_squared=False,\n        post_act_ln=False,\n        dropout=0.0,\n        zero_init_output=False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        activation = ReluSquared() if relu_squared else nn.GELU()\n\n        project_in = (\n            nn.Sequential(nn.Linear(dim, inner_dim), activation) if not glu else GLU(dim, inner_dim, activation)\n        )\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.LayerNorm(inner_dim) if post_act_ln else nn.Identity(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out),\n        )\n\n        # init last linear layer to 0\n        if zero_init_output:\n            init_zero_(self.net[-1])\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# attention.\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=DEFAULT_DIM_HEAD,\n        heads=8,\n        causal=False,\n        talking_heads=False,\n        head_scale=False,\n        collab_heads=False,\n        collab_compression=0.3,\n        sparse_topk=None,\n        use_entmax15=False,\n        num_mem_kv=0,\n        dropout=0.0,\n        on_attn=False,\n        gate_values=False,\n        zero_init_output=False,\n        max_attend_past=None,\n        qk_norm=False,\n        scale_init_value=None,\n        rel_pos_bias=False,\n        rel_pos_num_buckets=32,\n        rel_pos_max_distance=128,\n    ):\n        super().__init__()\n        self.scale = dim_head**-0.5\n\n        self.heads = heads\n        self.causal = causal\n        self.max_attend_past = max_attend_past\n\n        qk_dim = v_dim = dim_head * heads\n\n        # collaborative heads\n        self.collab_heads = collab_heads\n        if self.collab_heads:\n            qk_dim = int(collab_compression * qk_dim)\n            self.collab_mixing = nn.Parameter(torch.randn(heads, qk_dim))\n\n        self.to_q = nn.Linear(dim, qk_dim, bias=False)\n        self.to_k = nn.Linear(dim, qk_dim, bias=False)\n        self.to_v = nn.Linear(dim, v_dim, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n        # add GLU gating for aggregated values, from alphafold2\n        self.to_v_gate = None\n        if gate_values:\n            self.to_v_gate = nn.Linear(dim, v_dim)\n            nn.init.constant_(self.to_v_gate.weight, 0)\n            nn.init.constant_(self.to_v_gate.bias, 1)\n\n        # cosine sim attention\n        self.qk_norm = qk_norm\n        if qk_norm:\n            scale_init_value = default(\n                scale_init_value, -3\n            )  # if not provided, initialize as though it were sequence length of 1024\n            self.scale = nn.Parameter(torch.ones(1, heads, 1, 1) * scale_init_value)\n\n        # talking heads\n        self.talking_heads = talking_heads\n        if talking_heads:\n            self.pre_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n            self.post_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n\n        # head scaling\n        self.head_scale = head_scale\n        if head_scale:\n            self.head_scale_params = nn.Parameter(torch.ones(1, heads, 1, 1))\n\n        # explicit topk sparse attention\n        self.sparse_topk = sparse_topk\n\n        # entmax\n        self.attn_fn = F.softmax\n\n        # add memory key / values\n        self.num_mem_kv = num_mem_kv\n        if num_mem_kv > 0:\n            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n\n        # attention on attention\n        self.attn_on_attn = on_attn\n        self.to_out = nn.Sequential(nn.Linear(v_dim, dim * 2), nn.GLU()) if on_attn else nn.Linear(v_dim, dim)\n\n        self.rel_pos_bias = rel_pos_bias\n        if rel_pos_bias:\n            assert (\n                rel_pos_num_buckets <= rel_pos_max_distance\n            ), \"number of relative position buckets must be less than the relative position max distance\"\n            self.rel_pos = RelativePositionBias(\n                scale=dim_head**0.5,\n                causal=causal,\n                heads=heads,\n                num_buckets=rel_pos_num_buckets,\n                max_distance=rel_pos_max_distance,\n            )\n\n        # init output projection 0\n        if zero_init_output:\n            init_zero_(self.to_out)\n\n    def forward(\n        self,\n        x,\n        context=None,\n        mask=None,\n        context_mask=None,\n        attn_mask=None,\n        sinusoidal_emb=None,\n        rotary_pos_emb=None,\n        prev_attn=None,\n        mem=None,\n        layer_past=None,\n    ):\n        b, n, _, h, talking_heads, collab_heads, head_scale, scale, device, has_context = (\n            *x.shape,\n            self.heads,\n            self.talking_heads,\n            self.collab_heads,\n            self.head_scale,\n            self.scale,\n            x.device,\n            exists(context),\n        )\n        kv_input = default(context, x)\n\n        q_input = x\n        k_input = kv_input\n        v_input = kv_input\n\n        if exists(mem):\n            k_input = torch.cat((mem, k_input), dim=-2)\n            v_input = torch.cat((mem, v_input), dim=-2)\n\n        if exists(sinusoidal_emb):\n            # in shortformer, the query would start at a position offset depending on the past cached memory\n            offset = k_input.shape[-2] - q_input.shape[-2]\n            q_input = q_input + sinusoidal_emb(q_input, offset=offset)\n            k_input = k_input + sinusoidal_emb(k_input)\n\n        q = self.to_q(q_input)\n        k = self.to_k(k_input)\n        v = self.to_v(v_input)\n\n        if not collab_heads:\n            q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n        else:\n            q = einsum(\"b i d, h d -> b h i d\", q, self.collab_mixing)\n            k = rearrange(k, \"b n d -> b () n d\")\n            v = rearrange(v, \"b n (h d) -> b h n d\", h=h)\n\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            k = torch.cat([past_key, k], dim=-2)\n            v = torch.cat([past_value, v], dim=-2)\n        k_cache = k\n        v_cache = v\n\n        if exists(rotary_pos_emb) and not has_context:\n            l = rotary_pos_emb.shape[-1]\n            (ql, qr), (kl, kr), (vl, vr) = map(lambda t: (t[..., :l], t[..., l:]), (q, k, v))\n            ql, kl, vl = map(lambda t: apply_rotary_pos_emb(t, rotary_pos_emb), (ql, kl, vl))\n            q, k, v = map(lambda t: torch.cat(t, dim=-1), ((ql, qr), (kl, kr), (vl, vr)))\n\n        input_mask = None\n        if any(map(exists, (mask, context_mask))):\n            q_mask = default(mask, lambda: torch.ones((b, n), device=device).bool())\n            k_mask = q_mask if not exists(context) else context_mask\n            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device=device).bool())\n            q_mask = rearrange(q_mask, \"b i -> b () i ()\")\n            k_mask = rearrange(k_mask, \"b j -> b () () j\")\n            input_mask = q_mask * k_mask\n\n        if self.num_mem_kv > 0:\n            mem_k, mem_v = map(lambda t: repeat(t, \"h n d -> b h n d\", b=b), (self.mem_k, self.mem_v))\n            k = torch.cat((mem_k, k), dim=-2)\n            v = torch.cat((mem_v, v), dim=-2)\n            if exists(input_mask):\n                input_mask = F.pad(input_mask, (self.num_mem_kv, 0), value=True)\n\n        if collab_heads:\n            k = k.expand(-1, h, -1, -1)\n\n        if self.qk_norm:\n            q, k = map(l2norm, (q, k))\n            scale = 1 / (self.scale.exp().clamp(min=1e-2))\n\n        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * scale\n        mask_value = max_neg_value(dots)\n\n        if exists(prev_attn):\n            dots = dots + prev_attn\n\n        pre_softmax_attn = dots.clone()\n\n        if talking_heads:\n            dots = einsum(\"b h i j, h k -> b k i j\", dots, self.pre_softmax_proj).contiguous()\n\n        if self.rel_pos_bias:\n            dots = self.rel_pos(dots)\n\n        if exists(input_mask):\n            dots.masked_fill_(~input_mask, mask_value)\n            del input_mask\n\n        if exists(attn_mask):\n            assert (\n                2 <= attn_mask.ndim <= 4\n            ), \"attention mask must have greater than 2 dimensions but less than or equal to 4\"\n            if attn_mask.ndim == 2:\n                attn_mask = rearrange(attn_mask, \"i j -> () () i j\")\n            elif attn_mask.ndim == 3:\n                attn_mask = rearrange(attn_mask, \"h i j -> () h i j\")\n            dots.masked_fill_(~attn_mask, mask_value)\n\n        if exists(self.max_attend_past):\n            i, j = dots.shape[-2:]\n            range_q = torch.arange(j - i, j, device=device)\n            range_k = torch.arange(j, device=device)\n            dist = rearrange(range_q, \"i -> () () i ()\") - rearrange(range_k, \"j -> () () () j\")\n            mask = dist > self.max_attend_past\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        if self.causal:\n            i, j = dots.shape[-2:]\n            r = torch.arange(i, device=device)\n            mask = rearrange(r, \"i -> () () i ()\") < rearrange(r, \"j -> () () () j\")\n            mask = F.pad(mask, (j - i, 0), value=False)\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n            top, _ = dots.topk(self.sparse_topk, dim=-1)\n            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n            mask = dots < vk\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        attn = self.attn_fn(dots, dim=-1)\n        post_softmax_attn = attn.clone()\n\n        attn = self.dropout(attn)\n\n        if talking_heads:\n            attn = einsum(\"b h i j, h k -> b k i j\", attn, self.post_softmax_proj).contiguous()\n\n        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n\n        if head_scale:\n            out = out * self.head_scale_params\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n\n        if exists(self.to_v_gate):\n            gates = self.to_v_gate(x)\n            out = out * gates.sigmoid()\n\n        intermediates = Intermediates(pre_softmax_attn=pre_softmax_attn, post_softmax_attn=post_softmax_attn)\n\n        return self.to_out(out), intermediates, k_cache, v_cache\n\n\nclass AttentionLayers(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads=8,\n        causal=False,\n        cross_attend=False,\n        only_cross=False,\n        use_scalenorm=False,\n        use_rms_scaleshift_norm=False,\n        use_rmsnorm=False,\n        use_rezero=False,\n        alibi_pos_bias=False,\n        alibi_num_heads=None,\n        alibi_learned=False,\n        position_infused_attn=False,\n        rotary_pos_emb=False,\n        rotary_emb_dim=None,\n        custom_layers=None,\n        sandwich_coef=None,\n        par_ratio=None,\n        residual_attn=False,\n        cross_residual_attn=False,\n        macaron=False,\n        pre_norm=True,\n        gate_residual=False,\n        scale_residual=False,\n        shift_tokens=0,\n        sandwich_norm=False,\n        use_qk_norm_attn=False,\n        qk_norm_attn_seq_len=None,\n        zero_init_branch_output=False,\n        **kwargs,\n    ):\n        super().__init__()\n        ff_kwargs, kwargs = groupby_prefix_and_trim(\"ff_\", kwargs)\n        attn_kwargs, _ = groupby_prefix_and_trim(\"attn_\", kwargs)\n\n        dim_head = attn_kwargs.get(\"dim_head\", DEFAULT_DIM_HEAD)\n\n        self.dim = dim\n        self.depth = depth\n        self.layers = nn.ModuleList([])\n        self.causal = causal\n\n        rel_pos_bias = \"rel_pos_bias\" in attn_kwargs\n        self.has_pos_emb = position_infused_attn or rel_pos_bias or rotary_pos_emb\n        self.pia_pos_emb = FixedPositionalEmbedding(dim) if position_infused_attn else None\n\n        rotary_emb_dim = max(default(rotary_emb_dim, dim_head // 2), 32)\n        self.rotary_pos_emb = RotaryEmbedding(rotary_emb_dim) if rotary_pos_emb else None\n\n        assert not (\n            alibi_pos_bias and rel_pos_bias\n        ), \"you can only choose Alibi positional bias or T5 relative positional bias, not both\"\n\n        if alibi_pos_bias:\n            alibi_num_heads = default(alibi_num_heads, heads)\n            assert alibi_num_heads <= heads, \"number of ALiBi heads must be less than the total number of heads\"\n            alibi_pos_klass = LearnedAlibiPositionalBias if alibi_learned or not causal else AlibiPositionalBias\n            self.rel_pos = alibi_pos_klass(heads=alibi_num_heads, bidirectional=not causal)\n        else:\n            self.rel_pos = None\n\n        assert not (not pre_norm and sandwich_norm), \"sandwich norm cannot be used when not using prenorm\"\n        self.pre_norm = pre_norm\n        self.sandwich_norm = sandwich_norm\n\n        self.residual_attn = residual_attn\n        self.cross_residual_attn = cross_residual_attn\n        self.cross_attend = cross_attend\n\n        norm_class = ScaleNorm if use_scalenorm else nn.LayerNorm\n        norm_class = RMSNorm if use_rmsnorm else norm_class\n        norm_class = RMSScaleShiftNorm if use_rms_scaleshift_norm else norm_class\n        norm_fn = partial(norm_class, dim)\n\n        norm_fn = nn.Identity if use_rezero else norm_fn\n        branch_fn = Rezero if use_rezero else None\n\n        if cross_attend and not only_cross:\n            default_block = (\"a\", \"c\", \"f\")\n        elif cross_attend and only_cross:\n            default_block = (\"c\", \"f\")\n        else:\n            default_block = (\"a\", \"f\")\n\n        if macaron:\n            default_block = (\"f\",) + default_block\n\n        # qk normalization\n\n        if use_qk_norm_attn:\n            attn_scale_init_value = (\n                -math.log(math.log2(qk_norm_attn_seq_len**2 - qk_norm_attn_seq_len))\n                if exists(qk_norm_attn_seq_len)\n                else None\n            )\n            attn_kwargs = {**attn_kwargs, \"qk_norm\": True, \"scale_init_value\": attn_scale_init_value}\n\n        # zero init\n\n        if zero_init_branch_output:\n            attn_kwargs = {**attn_kwargs, \"zero_init_output\": True}\n            ff_kwargs = {**ff_kwargs, \"zero_init_output\": True}\n\n        # calculate layer block order\n\n        if exists(custom_layers):\n            layer_types = custom_layers\n        elif exists(par_ratio):\n            par_depth = depth * len(default_block)\n            assert 1 < par_ratio <= par_depth, \"par ratio out of range\"\n            default_block = tuple(filter(not_equals(\"f\"), default_block))\n            par_attn = par_depth // par_ratio\n            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n            assert len(default_block) <= par_width, \"default block is too large for par_ratio\"\n            par_block = default_block + (\"f\",) * (par_width - len(default_block))\n            par_head = par_block * par_attn\n            layer_types = par_head + (\"f\",) * (par_depth - len(par_head))\n        elif exists(sandwich_coef):\n            assert sandwich_coef > 0 and sandwich_coef <= depth, \"sandwich coefficient should be less than the depth\"\n            layer_types = (\"a\",) * sandwich_coef + default_block * (depth - sandwich_coef) + (\"f\",) * sandwich_coef\n        else:\n            layer_types = default_block * depth\n\n        self.layer_types = layer_types\n        self.num_attn_layers = len(list(filter(equals(\"a\"), layer_types)))\n\n        # calculate token shifting\n\n        shift_tokens = cast_tuple(shift_tokens, len(layer_types))\n\n        # iterate and construct layers\n\n        for ind, (layer_type, layer_shift_tokens) in enumerate(zip(self.layer_types, shift_tokens)):\n            is_last_layer = ind == (len(self.layer_types) - 1)\n\n            if layer_type == \"a\":\n                layer = Attention(dim, heads=heads, causal=causal, **attn_kwargs)\n            elif layer_type == \"c\":\n                layer = Attention(dim, heads=heads, **attn_kwargs)\n            elif layer_type == \"f\":\n                layer = FeedForward(dim, **ff_kwargs)\n                layer = layer if not macaron else Scale(0.5, layer)\n            else:\n                raise Exception(f\"invalid layer type {layer_type}\")\n\n            if layer_shift_tokens > 0:\n                shift_range_upper = layer_shift_tokens + 1\n                shift_range_lower = -layer_shift_tokens if not causal else 0\n                layer = ShiftTokens(range(shift_range_lower, shift_range_upper), layer)\n\n            if exists(branch_fn):\n                layer = branch_fn(layer)\n\n            residual_fn = GRUGating if gate_residual else Residual\n            residual = residual_fn(dim, scale_residual=scale_residual)\n\n            layer_uses_qk_norm = use_qk_norm_attn and layer_type in (\"a\", \"c\")\n\n            pre_branch_norm = norm_fn() if pre_norm and not layer_uses_qk_norm else None\n            post_branch_norm = norm_fn() if sandwich_norm or layer_uses_qk_norm else None\n            post_main_norm = norm_fn() if not pre_norm and not is_last_layer else None\n\n            norms = nn.ModuleList([pre_branch_norm, post_branch_norm, post_main_norm])\n\n            self.layers.append(nn.ModuleList([norms, layer, residual]))\n\n    def forward(\n        self,\n        x,\n        context=None,\n        full_context=None,  # for passing a list of hidden states from an encoder\n        mask=None,\n        context_mask=None,\n        attn_mask=None,\n        mems=None,\n        return_hiddens=False,\n        norm_scale_shift_inp=None,\n        past_key_values=None,\n        expected_seq_len=None,\n    ):\n        assert not (\n            self.cross_attend ^ (exists(context) or exists(full_context))\n        ), \"context must be passed in if cross_attend is set to True\"\n        assert context is None or full_context is None, \"only one of full_context or context can be provided\"\n\n        hiddens = []\n        intermediates = []\n        prev_attn = None\n        prev_cross_attn = None\n\n        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n        norm_args = {}\n        if exists(norm_scale_shift_inp):\n            norm_args[\"norm_scale_shift_inp\"] = norm_scale_shift_inp\n\n        rotary_pos_emb = None\n        if exists(self.rotary_pos_emb):\n            if not self.training and self.causal:\n                assert (\n                    expected_seq_len is not None\n                ), \"To decode a transformer with rotary embeddings, you must specify an `expected_seq_len`\"\n            elif expected_seq_len is None:\n                expected_seq_len = 0\n            seq_len = x.shape[1]\n            if past_key_values is not None:\n                seq_len += past_key_values[0][0].shape[-2]\n            max_rotary_emb_length = max(\n                list(map(lambda m: (m.shape[1] if exists(m) else 0) + seq_len, mems)) + [expected_seq_len]\n            )\n            rotary_pos_emb = self.rotary_pos_emb(max_rotary_emb_length, x.device)\n\n        present_key_values = []\n        cross_attn_count = 0\n        for ind, (layer_type, (norm, block, residual_fn)) in enumerate(zip(self.layer_types, self.layers)):\n            if layer_type == \"a\":\n                layer_mem = mems.pop(0) if mems else None\n\n            residual = x\n\n            pre_branch_norm, post_branch_norm, post_main_norm = norm\n\n            if exists(pre_branch_norm):\n                x = pre_branch_norm(x, **norm_args)\n\n            if layer_type == \"a\" or layer_type == \"c\":\n                if past_key_values is not None:\n                    layer_kv = past_key_values.pop(0)\n                    layer_past = tuple(s.to(x.device) for s in layer_kv)\n                else:\n                    layer_past = None\n\n            if layer_type == \"a\":\n                out, inter, k, v = block(\n                    x, None, mask, None, attn_mask, self.pia_pos_emb, rotary_pos_emb, prev_attn, layer_mem, layer_past\n                )\n            elif layer_type == \"c\":\n                if exists(full_context):\n                    out, inter, k, v = block(\n                        x,\n                        full_context[cross_attn_count],\n                        mask,\n                        context_mask,\n                        None,\n                        None,\n                        None,\n                        prev_attn,\n                        None,\n                        layer_past,\n                    )\n                else:\n                    out, inter, k, v = block(\n                        x, context, mask, context_mask, None, None, None, prev_attn, None, layer_past\n                    )\n            elif layer_type == \"f\":\n                out = block(x)\n\n            if layer_type == \"a\" or layer_type == \"c\" and present_key_values is not None:\n                present_key_values.append((k.detach(), v.detach()))\n\n            if exists(post_branch_norm):\n                out = post_branch_norm(out, **norm_args)\n\n            x = residual_fn(out, residual)\n\n            if layer_type in (\"a\", \"c\"):\n                intermediates.append(inter)\n\n            if layer_type == \"a\" and self.residual_attn:\n                prev_attn = inter.pre_softmax_attn\n            elif layer_type == \"c\" and self.cross_residual_attn:\n                prev_cross_attn = inter.pre_softmax_attn\n\n            if exists(post_main_norm):\n                x = post_main_norm(x, **norm_args)\n\n            if layer_type == \"c\":\n                cross_attn_count += 1\n\n            if layer_type == \"f\":\n                hiddens.append(x)\n\n        if return_hiddens:\n            intermediates = LayerIntermediates(\n                hiddens=hiddens, attn_intermediates=intermediates, past_key_values=present_key_values\n            )\n\n            return x, intermediates\n\n        return x\n\n\nclass Encoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on encoder\"\n        super().__init__(causal=False, **kwargs)\n\n\nclass Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\n\n\nclass CrossAttender(AttentionLayers):\n    def __init__(self, **kwargs):\n        super().__init__(cross_attend=True, only_cross=True, **kwargs)\n\n\nclass ViTransformerWrapper(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, num_classes=None, dropout=0.0, emb_dropout=0.0):\n        super().__init__()\n        assert isinstance(attn_layers, Encoder), \"attention layers must be an Encoder\"\n        assert image_size % patch_size == 0, \"image dimensions must be divisible by the patch size\"\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = 3 * patch_size**2\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.attn_layers = attn_layers\n        self.norm = nn.LayerNorm(dim)\n        self.mlp_head = FeedForward(dim, dim_out=num_classes, dropout=dropout) if exists(num_classes) else None\n\n    def forward(self, img, return_embeddings=False):\n        p = self.patch_size\n\n        x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n        x = self.patch_to_embedding(x)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embedding[:, : (n + 1)]\n        x = self.dropout(x)\n\n        x = self.attn_layers(x)\n        x = self.norm(x)\n\n        if not exists(self.mlp_head) or return_embeddings:\n            return x\n\n        return self.mlp_head(x[:, 0])\n\n\nclass TransformerWrapper(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers,\n        emb_dim=None,\n        max_mem_len=0.0,\n        shift_mem_down=0,\n        emb_dropout=0.0,\n        num_memory_tokens=None,\n        tie_embedding=False,\n        use_pos_emb=True,\n    ):\n        super().__init__()\n        assert isinstance(attn_layers, AttentionLayers), \"attention layers must be one of Encoder or Decoder\"\n\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n        self.pos_emb = (\n            AbsolutePositionalEmbedding(emb_dim, max_seq_len)\n            if (use_pos_emb and not attn_layers.has_pos_emb)\n            else always(0)\n        )\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n        self.norm = nn.LayerNorm(dim)\n\n        self.init_()\n\n        self.to_logits = nn.Linear(dim, num_tokens) if not tie_embedding else lambda t: t @ self.token_emb.weight.t()\n\n        # memory tokens (like [cls]) from Memory Transformers paper\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n    def init_(self):\n        nn.init.kaiming_normal_(self.token_emb.weight)\n\n    def forward(\n        self,\n        x,\n        return_embeddings=False,\n        mask=None,\n        return_hiddens=False,\n        return_attn=False,\n        mems=None,\n        use_cache=False,\n        **kwargs,\n    ):\n        b, n, device, num_mem = *x.shape, x.device, self.num_memory_tokens\n        x = self.token_emb(x)\n        x = x + self.pos_emb(x)\n        x = self.emb_dropout(x)\n\n        x = self.project_emb(x)\n\n        if num_mem > 0:\n            mem = repeat(self.memory_tokens, \"n d -> b n d\", b=b)\n            x = torch.cat((mem, x), dim=1)\n\n            # auto-handle masking after appending memory tokens\n            if exists(mask):\n                mask = F.pad(mask, (num_mem, 0), value=True)\n\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[: self.shift_mem_down], mems[self.shift_mem_down :]\n            mems = [*mems_r, *mems_l]\n\n        x, intermediates = self.attn_layers(x, mask=mask, mems=mems, return_hiddens=True, **kwargs)\n        x = self.norm(x)\n\n        mem, x = x[:, :num_mem], x[:, num_mem:]\n\n        out = self.to_logits(x) if not return_embeddings else x\n\n        if return_hiddens:\n            hiddens = intermediates.hiddens\n            return out, hiddens\n\n        res = [out]\n        if return_attn:\n            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n            res.append(attn_maps)\n        if use_cache:\n            res.append(intermediates.past_key_values)\n\n        if len(res) > 1:\n            return tuple(res)\n        return res[0]\n\n\nclass ContinuousTransformerWrapper(nn.Module):\n    def __init__(\n        self, *, max_seq_len, attn_layers, dim_in=None, dim_out=None, emb_dim=None, emb_dropout=0.0, use_pos_emb=True\n    ):\n        super().__init__()\n        assert isinstance(attn_layers, AttentionLayers), \"attention layers must be one of Encoder or Decoder\"\n\n        dim = attn_layers.dim\n\n        self.max_seq_len = max_seq_len\n\n        self.pos_emb = (\n            AbsolutePositionalEmbedding(dim, max_seq_len)\n            if (use_pos_emb and not attn_layers.has_pos_emb)\n            else always(0)\n        )\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_in = nn.Linear(dim_in, dim) if exists(dim_in) else nn.Identity()\n\n        self.attn_layers = attn_layers\n        self.norm = nn.LayerNorm(dim)\n\n        self.project_out = nn.Linear(dim, dim_out) if exists(dim_out) else nn.Identity()\n\n    def forward(self, x, return_embeddings=False, mask=None, return_attn=False, mems=None, use_cache=False, **kwargs):\n        b, n, _, device = *x.shape, x.device\n\n        x = self.project_in(x)\n        x = x + self.pos_emb(x)\n        x = self.emb_dropout(x)\n\n        x, intermediates = self.attn_layers(x, mask=mask, mems=mems, return_hiddens=True, **kwargs)\n        x = self.norm(x)\n\n        out = self.project_out(x) if not return_embeddings else x\n\n        res = [out]\n        if return_attn:\n            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n            res.append(attn_maps)\n        if use_cache:\n            res.append(intermediates.past_key_values)\n\n        if len(res) > 1:\n            return tuple(res)\n        return res[0]\n", "TTS/tts/layers/tortoise/clvp.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import einsum\n\nfrom TTS.tts.layers.tortoise.arch_utils import CheckpointedXTransformerEncoder\nfrom TTS.tts.layers.tortoise.transformer import Transformer\nfrom TTS.tts.layers.tortoise.xtransformers import Encoder\n\n\ndef exists(val):\n    return val is not None\n\n\ndef masked_mean(t, mask, dim=1):\n    t = t.masked_fill(~mask[:, :, None], 0.0)\n    return t.sum(dim=1) / mask.sum(dim=1)[..., None]\n\n\nclass CLVP(nn.Module):\n    \"\"\"\n    CLIP model retrofitted for performing contrastive evaluation between tokenized audio data and the corresponding\n    transcribed text.\n\n    Originally from https://github.com/lucidrains/DALLE-pytorch/blob/main/dalle_pytorch/dalle_pytorch.py\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_text=512,\n        dim_speech=512,\n        dim_latent=512,\n        num_text_tokens=256,\n        text_enc_depth=6,\n        text_seq_len=120,\n        text_heads=8,\n        num_speech_tokens=8192,\n        speech_enc_depth=6,\n        speech_heads=8,\n        speech_seq_len=250,\n        text_mask_percentage=0,\n        voice_mask_percentage=0,\n        wav_token_compression=1024,\n        use_xformers=False,\n    ):\n        super().__init__()\n        self.text_emb = nn.Embedding(num_text_tokens, dim_text)\n        self.to_text_latent = nn.Linear(dim_text, dim_latent, bias=False)\n\n        self.speech_emb = nn.Embedding(num_speech_tokens, dim_speech)\n        self.to_speech_latent = nn.Linear(dim_speech, dim_latent, bias=False)\n\n        if use_xformers:\n            self.text_transformer = CheckpointedXTransformerEncoder(\n                needs_permute=False,\n                exit_permute=False,\n                max_seq_len=-1,\n                attn_layers=Encoder(\n                    dim=dim_text,\n                    depth=text_enc_depth,\n                    heads=text_heads,\n                    ff_dropout=0.1,\n                    ff_mult=2,\n                    attn_dropout=0.1,\n                    use_rmsnorm=True,\n                    ff_glu=True,\n                    rotary_pos_emb=True,\n                ),\n            )\n            self.speech_transformer = CheckpointedXTransformerEncoder(\n                needs_permute=False,\n                exit_permute=False,\n                max_seq_len=-1,\n                attn_layers=Encoder(\n                    dim=dim_speech,\n                    depth=speech_enc_depth,\n                    heads=speech_heads,\n                    ff_dropout=0.1,\n                    ff_mult=2,\n                    attn_dropout=0.1,\n                    use_rmsnorm=True,\n                    ff_glu=True,\n                    rotary_pos_emb=True,\n                ),\n            )\n        else:\n            self.text_transformer = Transformer(\n                causal=False, seq_len=text_seq_len, dim=dim_text, depth=text_enc_depth, heads=text_heads\n            )\n            self.speech_transformer = Transformer(\n                causal=False, seq_len=speech_seq_len, dim=dim_speech, depth=speech_enc_depth, heads=speech_heads\n            )\n\n        self.temperature = nn.Parameter(torch.tensor(1.0))\n        self.text_mask_percentage = text_mask_percentage\n        self.voice_mask_percentage = voice_mask_percentage\n        self.wav_token_compression = wav_token_compression\n        self.xformers = use_xformers\n        if not use_xformers:\n            self.text_pos_emb = nn.Embedding(text_seq_len, dim_text)\n            self.speech_pos_emb = nn.Embedding(num_speech_tokens, dim_speech)\n\n    def forward(self, text, speech_tokens, return_loss=False):\n        b, device = text.shape[0], text.device\n        if self.training:\n            text_mask = torch.rand_like(text.float()) > self.text_mask_percentage\n            voice_mask = torch.rand_like(speech_tokens.float()) > self.voice_mask_percentage\n        else:\n            text_mask = torch.ones_like(text.float()).bool()\n            voice_mask = torch.ones_like(speech_tokens.float()).bool()\n\n        text_emb = self.text_emb(text)\n        speech_emb = self.speech_emb(speech_tokens)\n\n        if not self.xformers:\n            text_emb += self.text_pos_emb(torch.arange(text.shape[1], device=device))\n            speech_emb += self.speech_pos_emb(torch.arange(speech_emb.shape[1], device=device))\n\n        enc_text = self.text_transformer(text_emb, mask=text_mask)\n        enc_speech = self.speech_transformer(speech_emb, mask=voice_mask)\n\n        text_latents = masked_mean(enc_text, text_mask, dim=1)\n        speech_latents = masked_mean(enc_speech, voice_mask, dim=1)\n\n        text_latents = self.to_text_latent(text_latents)\n        speech_latents = self.to_speech_latent(speech_latents)\n\n        text_latents, speech_latents = map(lambda t: F.normalize(t, p=2, dim=-1), (text_latents, speech_latents))\n\n        temp = self.temperature.exp()\n\n        if not return_loss:\n            sim = einsum(\"n d, n d -> n\", text_latents, speech_latents) * temp\n            return sim\n\n        sim = einsum(\"i d, j d -> i j\", text_latents, speech_latents) * temp\n        labels = torch.arange(b, device=device)\n        loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n        return loss\n\n\nif __name__ == \"__main__\":\n    clip = CLVP(text_mask_percentage=0.2, voice_mask_percentage=0.2)\n    clip(\n        torch.randint(0, 256, (2, 120)),\n        torch.tensor([50, 100]),\n        torch.randint(0, 8192, (2, 250)),\n        torch.tensor([101, 102]),\n        return_loss=True,\n    )\n    nonloss = clip(\n        torch.randint(0, 256, (2, 120)),\n        torch.tensor([50, 100]),\n        torch.randint(0, 8192, (2, 250)),\n        torch.tensor([101, 102]),\n        return_loss=False,\n    )\n    print(nonloss.shape)\n", "TTS/tts/layers/tortoise/arch_utils.py": "import functools\nimport math\nimport os\n\nimport fsspec\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom transformers import LogitsWarper\n\nfrom TTS.tts.layers.tortoise.xtransformers import ContinuousTransformerWrapper, RelativePositionBias\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    groups = 32\n    if channels <= 16:\n        groups = 8\n    elif channels <= 64:\n        groups = 16\n    while channels % groups != 0:\n        groups = int(groups / 2)\n    assert groups > 2\n    return GroupNorm32(groups, channels)\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/output heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv, mask=None, rel_pos=None):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)  # More stable with f16 than dividing afterwards\n        if rel_pos is not None:\n            weight = rel_pos(weight.reshape(bs, self.n_heads, weight.shape[-2], weight.shape[-1])).reshape(\n                bs * self.n_heads, weight.shape[-2], weight.shape[-1]\n            )\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        if mask is not None:\n            # The proper way to do this is to mask before the softmax using -inf, but that doesn't work properly on CPUs.\n            mask = mask.repeat(self.n_heads, 1).unsqueeze(1)\n            weight = weight * mask\n        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n\n        return a.reshape(bs, -1, length)\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        do_checkpoint=True,\n        relative_pos_embeddings=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.do_checkpoint = do_checkpoint\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.norm = normalization(channels)\n        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n        # split heads before split qkv\n        self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n        if relative_pos_embeddings:\n            self.relative_pos_embeddings = RelativePositionBias(\n                scale=(channels // self.num_heads) ** 0.5,\n                causal=False,\n                heads=num_heads,\n                num_buckets=32,\n                max_distance=64,\n            )\n        else:\n            self.relative_pos_embeddings = None\n\n    def forward(self, x, mask=None):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv, mask, self.relative_pos_embeddings)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, out_channels=None, factor=4):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.factor = factor\n        if use_conv:\n            ksize = 5\n            pad = 2\n            self.conv = nn.Conv1d(self.channels, self.out_channels, ksize, padding=pad)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        x = F.interpolate(x, scale_factor=self.factor, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, out_channels=None, factor=4, ksize=5, pad=2):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n\n        stride = factor\n        if use_conv:\n            self.op = nn.Conv1d(self.channels, self.out_channels, ksize, stride=stride, padding=pad)\n        else:\n            assert self.channels == self.out_channels\n            self.op = nn.AvgPool1d(kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        up=False,\n        down=False,\n        kernel_size=3,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_scale_shift_norm = use_scale_shift_norm\n        padding = 1 if kernel_size == 3 else 2\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            nn.Conv1d(channels, self.out_channels, kernel_size, padding=padding),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False)\n            self.x_upd = Upsample(channels, False)\n        elif down:\n            self.h_upd = Downsample(channels, False)\n            self.x_upd = Downsample(channels, False)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Conv1d(self.out_channels, self.out_channels, kernel_size, padding=padding)),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size, padding=padding)\n        else:\n            self.skip_connection = nn.Conv1d(channels, self.out_channels, 1)\n\n    def forward(self, x):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AudioMiniEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        base_channels=128,\n        depth=2,\n        resnet_blocks=2,\n        attn_blocks=4,\n        num_attn_heads=4,\n        dropout=0,\n        downsample_factor=2,\n        kernel_size=3,\n    ):\n        super().__init__()\n        self.init = nn.Sequential(nn.Conv1d(spec_dim, base_channels, 3, padding=1))\n        ch = base_channels\n        res = []\n        for l in range(depth):\n            for r in range(resnet_blocks):\n                res.append(ResBlock(ch, dropout, kernel_size=kernel_size))\n            res.append(Downsample(ch, use_conv=True, out_channels=ch * 2, factor=downsample_factor))\n            ch *= 2\n        self.res = nn.Sequential(*res)\n        self.final = nn.Sequential(normalization(ch), nn.SiLU(), nn.Conv1d(ch, embedding_dim, 1))\n        attn = []\n        for a in range(attn_blocks):\n            attn.append(\n                AttentionBlock(\n                    embedding_dim,\n                    num_attn_heads,\n                )\n            )\n        self.attn = nn.Sequential(*attn)\n        self.dim = embedding_dim\n\n    def forward(self, x):\n        h = self.init(x)\n        h = self.res(h)\n        h = self.final(h)\n        h = self.attn(h)\n        return h[:, :, 0]\n\n\nDEFAULT_MEL_NORM_FILE = \"https://coqui.gateway.scarf.sh/v0.14.1_models/mel_norms.pth\"\n\n\nclass TorchMelSpectrogram(nn.Module):\n    def __init__(\n        self,\n        filter_length=1024,\n        hop_length=256,\n        win_length=1024,\n        n_mel_channels=80,\n        mel_fmin=0,\n        mel_fmax=8000,\n        sampling_rate=22050,\n        normalize=False,\n        mel_norm_file=DEFAULT_MEL_NORM_FILE,\n    ):\n        super().__init__()\n        # These are the default tacotron values for the MEL spectrogram.\n        self.filter_length = filter_length\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.n_mel_channels = n_mel_channels\n        self.mel_fmin = mel_fmin\n        self.mel_fmax = mel_fmax\n        self.sampling_rate = sampling_rate\n        self.mel_stft = torchaudio.transforms.MelSpectrogram(\n            n_fft=self.filter_length,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            power=2,\n            normalized=normalize,\n            sample_rate=self.sampling_rate,\n            f_min=self.mel_fmin,\n            f_max=self.mel_fmax,\n            n_mels=self.n_mel_channels,\n            norm=\"slaney\",\n        )\n        self.mel_norm_file = mel_norm_file\n        if self.mel_norm_file is not None:\n            with fsspec.open(self.mel_norm_file) as f:\n                self.mel_norms = torch.load(f)\n        else:\n            self.mel_norms = None\n\n    def forward(self, inp):\n        if (\n            len(inp.shape) == 3\n        ):  # Automatically squeeze out the channels dimension if it is present (assuming mono-audio)\n            inp = inp.squeeze(1)\n        assert len(inp.shape) == 2\n        self.mel_stft = self.mel_stft.to(inp.device)\n        mel = self.mel_stft(inp)\n        # Perform dynamic range compression\n        mel = torch.log(torch.clamp(mel, min=1e-5))\n        if self.mel_norms is not None:\n            self.mel_norms = self.mel_norms.to(mel.device)\n            mel = mel / self.mel_norms.unsqueeze(0).unsqueeze(-1)\n        return mel\n\n\nclass CheckpointedLayer(nn.Module):\n    \"\"\"\n    Wraps a module. When forward() is called, passes kwargs that require_grad through torch.checkpoint() and bypasses\n    checkpoint for all other args.\n    \"\"\"\n\n    def __init__(self, wrap):\n        super().__init__()\n        self.wrap = wrap\n\n    def forward(self, x, *args, **kwargs):\n        for k, v in kwargs.items():\n            assert not (isinstance(v, torch.Tensor) and v.requires_grad)  # This would screw up checkpointing.\n        partial = functools.partial(self.wrap, **kwargs)\n        return partial(x, *args)\n\n\nclass CheckpointedXTransformerEncoder(nn.Module):\n    \"\"\"\n    Wraps a ContinuousTransformerWrapper and applies CheckpointedLayer to each layer and permutes from channels-mid\n    to channels-last that XTransformer expects.\n    \"\"\"\n\n    def __init__(self, needs_permute=True, exit_permute=True, checkpoint=True, **xtransformer_kwargs):\n        super().__init__()\n        self.transformer = ContinuousTransformerWrapper(**xtransformer_kwargs)\n        self.needs_permute = needs_permute\n        self.exit_permute = exit_permute\n\n        if not checkpoint:\n            return\n        for i in range(len(self.transformer.attn_layers.layers)):\n            n, b, r = self.transformer.attn_layers.layers[i]\n            self.transformer.attn_layers.layers[i] = nn.ModuleList([n, CheckpointedLayer(b), r])\n\n    def forward(self, x, **kwargs):\n        if self.needs_permute:\n            x = x.permute(0, 2, 1)\n        h = self.transformer(x, **kwargs)\n        if self.exit_permute:\n            h = h.permute(0, 2, 1)\n        return h\n\n\nclass TypicalLogitsWarper(LogitsWarper):\n    def __init__(\n        self,\n        mass: float = 0.9,\n        filter_value: float = -float(\"Inf\"),\n        min_tokens_to_keep: int = 1,\n    ):\n        self.filter_value = filter_value\n        self.mass = mass\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        # calculate entropy\n        normalized = torch.nn.functional.log_softmax(scores, dim=-1)\n        p = torch.exp(normalized)\n        ent = -(normalized * p).nansum(-1, keepdim=True)\n\n        # shift and sort\n        shifted_scores = torch.abs((-normalized) - ent)\n        sorted_scores, sorted_indices = torch.sort(shifted_scores, descending=False)\n        sorted_logits = scores.gather(-1, sorted_indices)\n        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n\n        # Remove tokens with cumulative mass above the threshold\n        last_ind = (cumulative_probs < self.mass).sum(dim=1)\n        last_ind[last_ind < 0] = 0\n        sorted_indices_to_remove = sorted_scores > sorted_scores.gather(1, last_ind.view(-1, 1))\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n", "TTS/tts/layers/tortoise/audio_utils.py": "import os\nfrom glob import glob\nfrom typing import Dict, List\n\nimport librosa\nimport numpy as np\nimport torch\nimport torchaudio\nfrom scipy.io.wavfile import read\n\nfrom TTS.utils.audio.torch_transforms import TorchSTFT\n\n\ndef load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    if data.dtype == np.int32:\n        norm_fix = 2**31\n    elif data.dtype == np.int16:\n        norm_fix = 2**15\n    elif data.dtype == np.float16 or data.dtype == np.float32:\n        norm_fix = 1.0\n    else:\n        raise NotImplementedError(f\"Provided data dtype not supported: {data.dtype}\")\n    return (torch.FloatTensor(data.astype(np.float32)) / norm_fix, sampling_rate)\n\n\ndef check_audio(audio, audiopath: str):\n    # Check some assumptions about audio range. This should be automatically fixed in load_wav_to_torch, but might not be in some edge cases, where we should squawk.\n    # '2' is arbitrarily chosen since it seems like audio will often \"overdrive\" the [-1,1] bounds.\n    if torch.any(audio > 2) or not torch.any(audio < 0):\n        print(f\"Error with {audiopath}. Max={audio.max()} min={audio.min()}\")\n    audio.clip_(-1, 1)\n\n\ndef read_audio_file(audiopath: str):\n    if audiopath[-4:] == \".wav\":\n        audio, lsr = load_wav_to_torch(audiopath)\n    elif audiopath[-4:] == \".mp3\":\n        audio, lsr = librosa.load(audiopath, sr=None)\n        audio = torch.FloatTensor(audio)\n    else:\n        assert False, f\"Unsupported audio format provided: {audiopath[-4:]}\"\n\n    # Remove any channel data.\n    if len(audio.shape) > 1:\n        if audio.shape[0] < 5:\n            audio = audio[0]\n        else:\n            assert audio.shape[1] < 5\n            audio = audio[:, 0]\n\n    return audio, lsr\n\n\ndef load_required_audio(audiopath: str):\n    audio, lsr = read_audio_file(audiopath)\n\n    audios = [torchaudio.functional.resample(audio, lsr, sampling_rate) for sampling_rate in (22050, 24000)]\n    for audio in audios:\n        check_audio(audio, audiopath)\n\n    return [audio.unsqueeze(0) for audio in audios]\n\n\ndef load_audio(audiopath, sampling_rate):\n    audio, lsr = read_audio_file(audiopath)\n\n    if lsr != sampling_rate:\n        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)\n    check_audio(audio, audiopath)\n\n    return audio.unsqueeze(0)\n\n\nTACOTRON_MEL_MAX = 2.3143386840820312\nTACOTRON_MEL_MIN = -11.512925148010254\n\n\ndef denormalize_tacotron_mel(norm_mel):\n    return ((norm_mel + 1) / 2) * (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN) + TACOTRON_MEL_MIN\n\n\ndef normalize_tacotron_mel(mel):\n    return 2 * ((mel - TACOTRON_MEL_MIN) / (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN)) - 1\n\n\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\n\n\ndef get_voices(extra_voice_dirs: List[str] = []):\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}\n    for d in dirs:\n        subs = os.listdir(d)\n        for sub in subs:\n            subj = os.path.join(d, sub)\n            if os.path.isdir(subj):\n                voices[sub] = list(glob(f\"{subj}/*.wav\")) + list(glob(f\"{subj}/*.mp3\")) + list(glob(f\"{subj}/*.pth\"))\n    return voices\n\n\ndef load_voice(voice: str, extra_voice_dirs: List[str] = []):\n    if voice == \"random\":\n        return None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n    if len(paths) == 1 and paths[0].endswith(\".pth\"):\n        return None, torch.load(paths[0])\n    else:\n        conds = []\n        for cond_path in paths:\n            c = load_required_audio(cond_path)\n            conds.append(c)\n        return conds, None\n\n\ndef load_voices(voices: List[str], extra_voice_dirs: List[str] = []):\n    latents = []\n    clips = []\n    for voice in voices:\n        if voice == \"random\":\n            if len(voices) > 1:\n                print(\"Cannot combine a random voice with a non-random voice. Just using a random voice.\")\n            return None, None\n        clip, latent = load_voice(voice, extra_voice_dirs)\n        if latent is None:\n            assert (\n                len(latents) == 0\n            ), \"Can only combine raw audio voices or latent voices, not both. Do it yourself if you want this.\"\n            clips.extend(clip)\n        elif clip is None:\n            assert (\n                len(clips) == 0\n            ), \"Can only combine raw audio voices or latent voices, not both. Do it yourself if you want this.\"\n            latents.append(latent)\n    if len(latents) == 0:\n        return clips, None\n    else:\n        latents_0 = torch.stack([l[0] for l in latents], dim=0).mean(dim=0)\n        latents_1 = torch.stack([l[1] for l in latents], dim=0).mean(dim=0)\n        latents = (latents_0, latents_1)\n        return None, latents\n\n\ndef wav_to_univnet_mel(wav, do_normalization=False, device=\"cuda\"):\n    stft = TorchSTFT(\n        n_fft=1024,\n        hop_length=256,\n        win_length=1024,\n        use_mel=True,\n        n_mels=100,\n        sample_rate=24000,\n        mel_fmin=0,\n        mel_fmax=12000,\n    )\n    stft = stft.to(device)\n    mel = stft(wav)\n    mel = dynamic_range_compression(mel)\n    if do_normalization:\n        mel = normalize_tacotron_mel(mel)\n    return mel\n", "TTS/tts/layers/tortoise/utils.py": "import os\nfrom urllib import request\n\nfrom tqdm import tqdm\n\nDEFAULT_MODELS_DIR = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"tortoise\", \"models\")\nMODELS_DIR = os.environ.get(\"TORTOISE_MODELS_DIR\", DEFAULT_MODELS_DIR)\nMODELS_DIR = \"/data/speech_synth/models/\"\nMODELS = {\n    \"autoregressive.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth\",\n    \"classifier.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/classifier.pth\",\n    \"clvp2.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/clvp2.pth\",\n    \"diffusion_decoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/diffusion_decoder.pth\",\n    \"vocoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/vocoder.pth\",\n    \"rlg_auto.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_auto.pth\",\n    \"rlg_diffuser.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_diffuser.pth\",\n}\n\n\ndef download_models(specific_models=None):\n    \"\"\"\n    Call to download all the models that Tortoise uses.\n    \"\"\"\n    os.makedirs(MODELS_DIR, exist_ok=True)\n    for model_name, url in MODELS.items():\n        if specific_models is not None and model_name not in specific_models:\n            continue\n        model_path = os.path.join(MODELS_DIR, model_name)\n        if os.path.exists(model_path):\n            continue\n        print(f\"Downloading {model_name} from {url}...\")\n        with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1) as t:\n            request.urlretrieve(url, model_path, lambda nb, bs, fs, t=t: t.update(nb * bs - t.n))\n        print(\"Done.\")\n\n\ndef get_model_path(model_name, models_dir=MODELS_DIR):\n    \"\"\"\n    Get path to given model, download it if it doesn't exist.\n    \"\"\"\n    if model_name not in MODELS:\n        raise ValueError(f\"Model {model_name} not found in available models.\")\n    model_path = os.path.join(models_dir, model_name)\n    if not os.path.exists(model_path) and models_dir == MODELS_DIR:\n        download_models([model_name])\n    return model_path\n", "TTS/tts/layers/tortoise/diffusion.py": "\"\"\"\nThis is an almost carbon copy of gaussian_diffusion.py from OpenAI's ImprovedDiffusion repo, which itself:\n\nThis code started out as a PyTorch port of Ho et al's diffusion models:\nhttps://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py\n\nDocstrings have been added, as well as DDIM sampling and a new collection of beta schedules.\n\"\"\"\n\nimport enum\nimport math\n\nimport numpy as np\nimport torch\nimport torch as th\nfrom tqdm import tqdm\n\nfrom TTS.tts.layers.tortoise.dpm_solver import DPM_Solver, NoiseScheduleVP, model_wrapper\n\ntry:\n    from k_diffusion.sampling import sample_dpmpp_2m, sample_euler_ancestral\n\n    K_DIFFUSION_SAMPLERS = {\"k_euler_a\": sample_euler_ancestral, \"dpm++2m\": sample_dpmpp_2m}\nexcept ImportError:\n    K_DIFFUSION_SAMPLERS = None\n\n\nSAMPLERS = [\"dpm++2m\", \"p\", \"ddim\"]\n\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, th.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for th.exp().\n    logvar1, logvar2 = [x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (-1.0 + logvar2 - logvar1 + th.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * th.exp(-logvar2))\n\n\ndef approx_standard_normal_cdf(x):\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))\n\n\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales):\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = th.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = th.where(\n        x < -0.999,\n        log_cdf_plus,\n        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    elif schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    else:\n        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\nclass ModelMeanType(enum.Enum):\n    \"\"\"\n    Which type of output the model predicts.\n    \"\"\"\n\n    PREVIOUS_X = \"previous_x\"  # the model predicts x_{t-1}\n    START_X = \"start_x\"  # the model predicts x_0\n    EPSILON = \"epsilon\"  # the model predicts epsilon\n\n\nclass ModelVarType(enum.Enum):\n    \"\"\"\n    What is used as the model's output variance.\n\n    The LEARNED_RANGE option has been added to allow the model to predict\n    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n    \"\"\"\n\n    LEARNED = \"learned\"\n    FIXED_SMALL = \"fixed_small\"\n    FIXED_LARGE = \"fixed_large\"\n    LEARNED_RANGE = \"learned_range\"\n\n\nclass LossType(enum.Enum):\n    MSE = \"mse\"  # use raw MSE loss (and KL when learning variances)\n    RESCALED_MSE = \"rescaled_mse\"  # use raw MSE loss (with RESCALED_KL when learning variances)\n    KL = \"kl\"  # use the variational lower-bound\n    RESCALED_KL = \"rescaled_kl\"  # like KL, but rescale to estimate the full VLB\n\n    def is_vb(self):\n        return self == LossType.KL or self == LossType.RESCALED_KL\n\n\nclass GaussianDiffusion:\n    \"\"\"\n    Utilities for training and sampling diffusion models.\n\n    Ported directly from here, and then adapted over time to further experimentation.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n\n    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n                  starting at T and going to 1.\n    :param model_mean_type: a ModelMeanType determining what the model outputs.\n    :param model_var_type: a ModelVarType determining how variance is output.\n    :param loss_type: a LossType determining the loss function to use.\n    :param rescale_timesteps: if True, pass floating point timesteps into the\n                              model so that they are always scaled like in the\n                              original paper (0 to 1000).\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        betas,\n        model_mean_type,\n        model_var_type,\n        loss_type,\n        rescale_timesteps=False,\n        conditioning_free=False,\n        conditioning_free_k=1,\n        ramp_conditioning_free=True,\n        sampler=\"p\",\n    ):\n        self.sampler = sampler\n        self.model_mean_type = ModelMeanType(model_mean_type)\n        self.model_var_type = ModelVarType(model_var_type)\n        self.loss_type = LossType(loss_type)\n        self.rescale_timesteps = rescale_timesteps\n        self.conditioning_free = conditioning_free\n        self.conditioning_free_k = conditioning_free_k\n        self.ramp_conditioning_free = ramp_conditioning_free\n\n        # Use float64 for accuracy.\n        betas = np.array(betas, dtype=np.float64)\n        self.betas = betas\n        assert len(betas.shape) == 1, \"betas must be 1-D\"\n        assert (betas > 0).all() and (betas <= 1).all()\n\n        self.num_timesteps = int(betas.shape[0])\n\n        alphas = 1.0 - betas\n        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        # log calculation clipped because the posterior variance is 0 at the\n        # beginning of the diffusion chain.\n        self.posterior_log_variance_clipped = np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:]))\n        self.posterior_mean_coef1 = betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (1.0 - self.alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - self.alphas_cumprod)\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = _extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Diffuse the data for a given number of diffusion steps.\n\n        In other words, sample from q(x_t | x_0).\n\n        :param x_start: the initial data batch.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :param noise: if specified, the split-out normal noise.\n        :return: A noisy version of x_start.\n        \"\"\"\n        if noise is None:\n            noise = th.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean and variance of the diffusion posterior:\n\n            q(x_{t-1} | x_t, x_0)\n\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None):\n        \"\"\"\n        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n        the initial x, x_0.\n\n        :param model: the model, which takes a signal and a batch of timesteps\n                      as input.\n        :param x: the [N x C x ...] tensor at time t.\n        :param t: a 1-D Tensor of timesteps.\n        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample. Applies before\n            clip_denoised.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict with the following keys:\n                 - 'mean': the model mean output.\n                 - 'variance': the model variance output.\n                 - 'log_variance': the log of 'variance'.\n                 - 'pred_xstart': the prediction for x_0.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n        if self.conditioning_free:\n            model_output_no_conditioning = model(x, self._scale_timesteps(t), conditioning_free=True, **model_kwargs)\n\n        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n            assert model_output.shape == (B, C * 2, *x.shape[2:])\n            model_output, model_var_values = th.split(model_output, C, dim=1)\n            if self.conditioning_free:\n                model_output_no_conditioning, _ = th.split(model_output_no_conditioning, C, dim=1)\n            if self.model_var_type == ModelVarType.LEARNED:\n                model_log_variance = model_var_values\n                model_variance = th.exp(model_log_variance)\n            else:\n                min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n                max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n                # The model_var_values is [-1, 1] for [min_var, max_var].\n                frac = (model_var_values + 1) / 2\n                model_log_variance = frac * max_log + (1 - frac) * min_log\n                model_variance = th.exp(model_log_variance)\n        else:\n            model_variance, model_log_variance = {\n                # for fixedlarge, we set the initial (log-)variance like so\n                # to get a better decoder log likelihood.\n                ModelVarType.FIXED_LARGE: (\n                    np.append(self.posterior_variance[1], self.betas[1:]),\n                    np.log(np.append(self.posterior_variance[1], self.betas[1:])),\n                ),\n                ModelVarType.FIXED_SMALL: (\n                    self.posterior_variance,\n                    self.posterior_log_variance_clipped,\n                ),\n            }[self.model_var_type]\n            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n\n        if self.conditioning_free:\n            if self.ramp_conditioning_free:\n                assert t.shape[0] == 1  # This should only be used in inference.\n                cfk = self.conditioning_free_k * (1 - self._scale_timesteps(t)[0].item() / self.num_timesteps)\n            else:\n                cfk = self.conditioning_free_k\n            model_output = (1 + cfk) * model_output - cfk * model_output_no_conditioning\n\n        def process_xstart(x):\n            if denoised_fn is not None:\n                x = denoised_fn(x)\n            if clip_denoised:\n                return x.clamp(-1, 1)\n            return x\n\n        if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n            pred_xstart = process_xstart(self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output))\n            model_mean = model_output\n        elif self.model_mean_type in [ModelMeanType.START_X, ModelMeanType.EPSILON]:\n            if self.model_mean_type == ModelMeanType.START_X:\n                pred_xstart = process_xstart(model_output)\n            else:\n                pred_xstart = process_xstart(self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output))\n            model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n        else:\n            raise NotImplementedError(self.model_mean_type)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _predict_xstart_from_eps(self, x_t, t, eps):\n        assert x_t.shape == eps.shape\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_xstart_from_xprev(self, x_t, t, xprev):\n        assert x_t.shape == xprev.shape\n        return (  # (xprev - coef2*x_t) / coef1\n            _extract_into_tensor(1.0 / self.posterior_mean_coef1, t, x_t.shape) * xprev\n            - _extract_into_tensor(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t\n        )\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _scale_timesteps(self, t):\n        if self.rescale_timesteps:\n            return t.float() * (1000.0 / self.num_timesteps)\n        return t\n\n    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        gradient = cond_fn(x, self._scale_timesteps(t), **model_kwargs)\n        new_mean = p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n        return new_mean\n\n    def condition_score(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, self._scale_timesteps(t), **model_kwargs)\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def k_diffusion_sample_loop(\n        self,\n        k_sampler,\n        pbar,\n        model,\n        shape,\n        noise=None,  # all given\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        device=None,  # ALL UNUSED\n        model_kwargs=None,  # {'precomputed_aligned_embeddings': precomputed_embeddings},\n        progress=False,  # unused as well\n    ):\n        assert isinstance(model_kwargs, dict)\n        if device is None:\n            device = next(model.parameters()).device\n        s_in = noise.new_ones([noise.shape[0]])\n\n        def model_split(*args, **kwargs):\n            model_output = model(*args, **kwargs)\n            model_epsilon, model_var = th.split(model_output, model_output.shape[1] // 2, dim=1)\n            return model_epsilon, model_var\n\n        #\n        \"\"\"\n        print(self.betas)\n        print(th.tensor(self.betas))\n        noise_schedule = NoiseScheduleVP(schedule='discrete', betas=th.tensor(self.betas))\n        \"\"\"\n        noise_schedule = NoiseScheduleVP(schedule=\"linear\", continuous_beta_0=0.1 / 4, continuous_beta_1=20.0 / 4)\n\n        def model_fn_prewrap(x, t, *args, **kwargs):\n            \"\"\"\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n            print(t)\n            print(self.timestep_map)\n            exit()\n            \"\"\"\n            \"\"\"\n            model_output = model(x, self._scale_timesteps(t*4000), **model_kwargs)\n            out = self.p_mean_variance(model, x, t*4000, model_kwargs=model_kwargs)\n            return out['pred_xstart']\n            \"\"\"\n            x, _ = x.chunk(2)\n            t, _ = (t * 1000).chunk(2)\n            res = torch.cat(\n                [\n                    model_split(x, t, conditioning_free=True, **model_kwargs)[0],\n                    model_split(x, t, **model_kwargs)[0],\n                ]\n            )\n            pbar.update(1)\n            return res\n\n        model_fn = model_wrapper(\n            model_fn_prewrap,\n            noise_schedule,\n            model_type=\"noise\",  # \"noise\" or \"x_start\" or \"v\" or \"score\"\n            model_kwargs=model_kwargs,\n            guidance_type=\"classifier-free\",\n            condition=th.Tensor(1),\n            unconditional_condition=th.Tensor(1),\n            guidance_scale=self.conditioning_free_k,\n        )\n        dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type=\"dpmsolver++\")\n        x_sample = dpm_solver.sample(\n            noise,\n            steps=self.num_timesteps,\n            order=2,\n            skip_type=\"time_uniform\",\n            method=\"multistep\",\n        )\n        #'''\n        return x_sample\n\n    def sample_loop(self, *args, **kwargs):\n        s = self.sampler\n        if s == \"p\":\n            return self.p_sample_loop(*args, **kwargs)\n        elif s == \"ddim\":\n            return self.ddim_sample_loop(*args, **kwargs)\n        elif s == \"dpm++2m\":\n            if self.conditioning_free is not True:\n                raise RuntimeError(\"cond_free must be true\")\n            with tqdm(total=self.num_timesteps) as pbar:\n                if K_DIFFUSION_SAMPLERS is None:\n                    raise ModuleNotFoundError(\"Install k_diffusion for using k_diffusion samplers\")\n                return self.k_diffusion_sample_loop(K_DIFFUSION_SAMPLERS[s], pbar, *args, **kwargs)\n        else:\n            raise RuntimeError(\"sampler not impl\")\n\n    def p_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model at the given timestep.\n\n        :param model: the model to sample from.\n        :param x: the current tensor at x_{t-1}.\n        :param t: the value of t, starting at 0 for the first diffusion step.\n        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict containing the following keys:\n                 - 'sample': a random sample from the model.\n                 - 'pred_xstart': a prediction of x_0.\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = th.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def p_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model.\n\n        :param model: the model module.\n        :param shape: the shape of the samples, (N, C, H, W).\n        :param noise: if specified, the noise from the encoder to sample.\n                      Should be of the same shape as `shape`.\n        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param device: if specified, the device to create the samples on.\n                       If not specified, use a model parameter's device.\n        :param progress: if True, show a tqdm progress bar.\n        :return: a non-differentiable batch of samples.\n        \"\"\"\n        final = None\n        for sample in self.p_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def p_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model and yield intermediate samples from\n        each timestep of diffusion.\n\n        Arguments are the same as p_sample_loop().\n        Returns a generator over dicts, where each dict is the return value of\n        p_sample().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        for i in tqdm(indices, disable=not progress):\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def ddim_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model using DDIM.\n\n        Same usage as p_sample().\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n        # Equation 12.\n        noise = th.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev) + th.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        sample = mean_pred + nonzero_mask * sigma * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_reverse_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t+1} from the model using DDIM reverse ODE.\n        \"\"\"\n        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n\n        # Equation 12. reversed\n        mean_pred = out[\"pred_xstart\"] * th.sqrt(alpha_bar_next) + th.sqrt(1 - alpha_bar_next) * eps\n\n        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Generate samples from the model using DDIM.\n\n        Same usage as p_sample_loop().\n        \"\"\"\n        final = None\n        for sample in self.ddim_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            eta=eta,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def ddim_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Use DDIM to sample from the model and yield intermediate samples from\n        each timestep of DDIM.\n\n        Same usage as p_sample_loop_progressive().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices, disable=not progress)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.ddim_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                    eta=eta,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def _vb_terms_bpd(self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None):\n        \"\"\"\n        Get a term for the variational lower-bound.\n\n        The resulting units are bits (rather than nats, as one might expect).\n        This allows for comparison to other papers.\n\n        :return: a dict with the following keys:\n                 - 'output': a shape [N] tensor of NLLs or KLs.\n                 - 'pred_xstart': the x_0 predictions.\n        \"\"\"\n        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.p_mean_variance(model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs)\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = th.where((t == 0), decoder_nll, kl)\n        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def training_losses(self, model, x_start, t, model_kwargs=None, noise=None):\n        \"\"\"\n        Compute training losses for a single timestep.\n\n        :param model: the model to evaluate loss on.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :param t: a batch of timestep indices.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param noise: if specified, the specific Gaussian noise to try to remove.\n        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n                 Some mean or variance settings may also have other keys.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n        if noise is None:\n            noise = th.randn_like(x_start)\n        x_t = self.q_sample(x_start, t, noise=noise)\n\n        terms = {}\n\n        if self.loss_type == LossType.KL or self.loss_type == LossType.RESCALED_KL:\n            # TODO: support multiple model outputs for this mode.\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model=model,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n            if self.loss_type == LossType.RESCALED_KL:\n                terms[\"loss\"] *= self.num_timesteps\n        elif self.loss_type == LossType.MSE or self.loss_type == LossType.RESCALED_MSE:\n            model_outputs = model(x_t, self._scale_timesteps(t), **model_kwargs)\n            if isinstance(model_outputs, tuple):\n                model_output = model_outputs[0]\n                terms[\"extra_outputs\"] = model_outputs[1:]\n            else:\n                model_output = model_outputs\n\n            if self.model_var_type in [\n                ModelVarType.LEARNED,\n                ModelVarType.LEARNED_RANGE,\n            ]:\n                B, C = x_t.shape[:2]\n                assert model_output.shape == (B, C * 2, *x_t.shape[2:])\n                model_output, model_var_values = th.split(model_output, C, dim=1)\n                # Learn the variance using the variational bound, but don't let\n                # it affect our mean prediction.\n                frozen_out = th.cat([model_output.detach(), model_var_values], dim=1)\n                terms[\"vb\"] = self._vb_terms_bpd(\n                    model=lambda *args, r=frozen_out: r,\n                    x_start=x_start,\n                    x_t=x_t,\n                    t=t,\n                    clip_denoised=False,\n                )[\"output\"]\n                if self.loss_type == LossType.RESCALED_MSE:\n                    # Divide by 1000 for equivalence with initial implementation.\n                    # Without a factor of 1/1000, the VB term hurts the MSE term.\n                    terms[\"vb\"] *= self.num_timesteps / 1000.0\n\n            if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n                target = self.q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)[0]\n                x_start_pred = torch.zeros(x_start)  # Not supported.\n            elif self.model_mean_type == ModelMeanType.START_X:\n                target = x_start\n                x_start_pred = model_output\n            elif self.model_mean_type == ModelMeanType.EPSILON:\n                target = noise\n                x_start_pred = self._predict_xstart_from_eps(x_t, t, model_output)\n            else:\n                raise NotImplementedError(self.model_mean_type)\n            assert model_output.shape == target.shape == x_start.shape\n            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n            terms[\"x_start_predicted\"] = x_start_pred\n            if \"vb\" in terms:\n                terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n            else:\n                terms[\"loss\"] = terms[\"mse\"]\n        else:\n            raise NotImplementedError(self.loss_type)\n\n        return terms\n\n    def autoregressive_training_losses(\n        self, model, x_start, t, model_output_keys, gd_out_key, model_kwargs=None, noise=None\n    ):\n        \"\"\"\n        Compute training losses for a single timestep.\n\n        :param model: the model to evaluate loss on.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :param t: a batch of timestep indices.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param noise: if specified, the specific Gaussian noise to try to remove.\n        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n                 Some mean or variance settings may also have other keys.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n        if noise is None:\n            noise = th.randn_like(x_start)\n        x_t = self.q_sample(x_start, t, noise=noise)\n        terms = {}\n        if self.loss_type == LossType.KL or self.loss_type == LossType.RESCALED_KL:\n            assert False  # not currently supported for this type of diffusion.\n        elif self.loss_type == LossType.MSE or self.loss_type == LossType.RESCALED_MSE:\n            model_outputs = model(x_t, x_start, self._scale_timesteps(t), **model_kwargs)\n            terms.update({k: o for k, o in zip(model_output_keys, model_outputs)})\n            model_output = terms[gd_out_key]\n            if self.model_var_type in [\n                ModelVarType.LEARNED,\n                ModelVarType.LEARNED_RANGE,\n            ]:\n                B, C = x_t.shape[:2]\n                assert model_output.shape == (B, C, 2, *x_t.shape[2:])\n                model_output, model_var_values = model_output[:, :, 0], model_output[:, :, 1]\n                # Learn the variance using the variational bound, but don't let\n                # it affect our mean prediction.\n                frozen_out = th.cat([model_output.detach(), model_var_values], dim=1)\n                terms[\"vb\"] = self._vb_terms_bpd(\n                    model=lambda *args, r=frozen_out: r,\n                    x_start=x_start,\n                    x_t=x_t,\n                    t=t,\n                    clip_denoised=False,\n                )[\"output\"]\n                if self.loss_type == LossType.RESCALED_MSE:\n                    # Divide by 1000 for equivalence with initial implementation.\n                    # Without a factor of 1/1000, the VB term hurts the MSE term.\n                    terms[\"vb\"] *= self.num_timesteps / 1000.0\n\n            if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n                target = self.q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)[0]\n                x_start_pred = torch.zeros(x_start)  # Not supported.\n            elif self.model_mean_type == ModelMeanType.START_X:\n                target = x_start\n                x_start_pred = model_output\n            elif self.model_mean_type == ModelMeanType.EPSILON:\n                target = noise\n                x_start_pred = self._predict_xstart_from_eps(x_t, t, model_output)\n            else:\n                raise NotImplementedError(self.model_mean_type)\n            assert model_output.shape == target.shape == x_start.shape\n            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n            terms[\"x_start_predicted\"] = x_start_pred\n            if \"vb\" in terms:\n                terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n            else:\n                terms[\"loss\"] = terms[\"mse\"]\n        else:\n            raise NotImplementedError(self.loss_type)\n\n        return terms\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = th.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):\n        \"\"\"\n        Compute the entire variational lower-bound, measured in bits-per-dim,\n        as well as other related quantities.\n\n        :param model: the model to evaluate loss on.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :param clip_denoised: if True, clip denoised samples.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n\n        :return: a dict containing the following keys:\n                 - total_bpd: the total variational lower-bound, per batch element.\n                 - prior_bpd: the prior term in the lower-bound.\n                 - vb: an [N x T] tensor of terms in the lower-bound.\n                 - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.\n                 - mse: an [N x T] tensor of epsilon MSEs for each timestep.\n        \"\"\"\n        device = x_start.device\n        batch_size = x_start.shape[0]\n\n        vb = []\n        xstart_mse = []\n        mse = []\n        for t in list(range(self.num_timesteps))[::-1]:\n            t_batch = th.tensor([t] * batch_size, device=device)\n            noise = th.randn_like(x_start)\n            x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)\n            # Calculate VLB term at the current timestep\n            with th.no_grad():\n                out = self._vb_terms_bpd(\n                    model,\n                    x_start=x_start,\n                    x_t=x_t,\n                    t=t_batch,\n                    clip_denoised=clip_denoised,\n                    model_kwargs=model_kwargs,\n                )\n            vb.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_start) ** 2))\n            eps = self._predict_eps_from_xstart(x_t, t_batch, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        vb = th.stack(vb, dim=1)\n        xstart_mse = th.stack(xstart_mse, dim=1)\n        mse = th.stack(mse, dim=1)\n\n        prior_bpd = self._prior_bpd(x_start)\n        total_bpd = vb.sum(dim=1) + prior_bpd\n        return {\n            \"total_bpd\": total_bpd,\n            \"prior_bpd\": prior_bpd,\n            \"vb\": vb,\n            \"xstart_mse\": xstart_mse,\n            \"mse\": mse,\n        }\n\n\nclass SpacedDiffusion(GaussianDiffusion):\n    \"\"\"\n    A diffusion process which can skip steps in a base diffusion process.\n\n    :param use_timesteps: a collection (sequence or set) of timesteps from the\n                          original diffusion process to retain.\n    :param kwargs: the kwargs to create the base diffusion process.\n    \"\"\"\n\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.timestep_map = []\n        self.original_num_steps = len(kwargs[\"betas\"])\n        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n        last_alpha_cumprod = 1.0\n        new_betas = []\n        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n            if i in self.use_timesteps:\n                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n                last_alpha_cumprod = alpha_cumprod\n                self.timestep_map.append(i)\n        kwargs[\"betas\"] = np.array(new_betas)\n        super().__init__(**kwargs)\n\n    def p_mean_variance(self, model, *args, **kwargs):  # pylint: disable=signature-differs\n        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n\n    def training_losses(self, model, *args, **kwargs):  # pylint: disable=signature-differs\n        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n\n    def autoregressive_training_losses(self, model, *args, **kwargs):  # pylint: disable=signature-differs\n        return super().autoregressive_training_losses(self._wrap_model(model, True), *args, **kwargs)\n\n    def condition_mean(self, cond_fn, *args, **kwargs):\n        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def condition_score(self, cond_fn, *args, **kwargs):\n        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def _wrap_model(self, model, autoregressive=False):\n        if isinstance(model, _WrappedModel) or isinstance(model, _WrappedAutoregressiveModel):\n            return model\n        mod = _WrappedAutoregressiveModel if autoregressive else _WrappedModel\n        return mod(model, self.timestep_map, self.rescale_timesteps, self.original_num_steps)\n\n    def _scale_timesteps(self, t):\n        # Scaling is done by the wrapped model.\n        return t\n\n\ndef space_timesteps(num_timesteps, section_counts):\n    \"\"\"\n    Create a list of timesteps to use from an original diffusion process,\n    given the number of timesteps we want to take from equally-sized portions\n    of the original process.\n\n    For example, if there's 300 timesteps and the section counts are [10,15,20]\n    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n\n    If the stride is a string starting with \"ddim\", then the fixed striding\n    from the DDIM paper is used, and only one section is allowed.\n\n    :param num_timesteps: the number of diffusion steps in the original\n                          process to divide up.\n    :param section_counts: either a list of numbers, or a string containing\n                           comma-separated numbers, indicating the step count\n                           per section. As a special case, use \"ddimN\" where N\n                           is a number of steps to use the striding from the\n                           DDIM paper.\n    :return: a set of diffusion steps from the original process to use.\n    \"\"\"\n    if isinstance(section_counts, str):\n        if section_counts.startswith(\"ddim\"):\n            desired_count = int(section_counts[len(\"ddim\") :])\n            for i in range(1, num_timesteps):\n                if len(range(0, num_timesteps, i)) == desired_count:\n                    return set(range(0, num_timesteps, i))\n            raise ValueError(f\"cannot create exactly {num_timesteps} steps with an integer stride\")\n        section_counts = [int(x) for x in section_counts.split(\",\")]\n    size_per = num_timesteps // len(section_counts)\n    extra = num_timesteps % len(section_counts)\n    start_idx = 0\n    all_steps = []\n    for i, section_count in enumerate(section_counts):\n        size = size_per + (1 if i < extra else 0)\n        if size < section_count:\n            raise ValueError(f\"cannot divide section of {size} steps into {section_count}\")\n        if section_count <= 1:\n            frac_stride = 1\n        else:\n            frac_stride = (size - 1) / (section_count - 1)\n        cur_idx = 0.0\n        taken_steps = []\n        for _ in range(section_count):\n            taken_steps.append(start_idx + round(cur_idx))\n            cur_idx += frac_stride\n        all_steps += taken_steps\n        start_idx += size\n    return set(all_steps)\n\n\nclass _WrappedModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n\n    def __call__(self, x, ts, **kwargs):\n        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:\n            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n        model_output = self.model(x, new_ts, **kwargs)\n        return model_output\n\n\nclass _WrappedAutoregressiveModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n\n    def __call__(self, x, x0, ts, **kwargs):\n        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:\n            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n        return self.model(x, x0, new_ts, **kwargs)\n\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)\n", "TTS/tts/layers/tortoise/wav2vec_alignment.py": "import torch\nimport torchaudio\nfrom transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC\n\n\ndef max_alignment(s1, s2, skip_character=\"~\", record=None):\n    \"\"\"\n    A clever function that aligns s1 to s2 as best it can. Wherever a character from s1 is not found in s2, a '~' is\n    used to replace that character.\n\n    Finally got to use my DP skills!\n    \"\"\"\n    if record is None:\n        record = {}\n    assert skip_character not in s1, f\"Found the skip character {skip_character} in the provided string, {s1}\"\n    if len(s1) == 0:\n        return \"\"\n    if len(s2) == 0:\n        return skip_character * len(s1)\n    if s1 == s2:\n        return s1\n    if s1[0] == s2[0]:\n        return s1[0] + max_alignment(s1[1:], s2[1:], skip_character, record)\n\n    take_s1_key = (len(s1), len(s2) - 1)\n    if take_s1_key in record:\n        take_s1, take_s1_score = record[take_s1_key]\n    else:\n        take_s1 = max_alignment(s1, s2[1:], skip_character, record)\n        take_s1_score = len(take_s1.replace(skip_character, \"\"))\n        record[take_s1_key] = (take_s1, take_s1_score)\n\n    take_s2_key = (len(s1) - 1, len(s2))\n    if take_s2_key in record:\n        take_s2, take_s2_score = record[take_s2_key]\n    else:\n        take_s2 = max_alignment(s1[1:], s2, skip_character, record)\n        take_s2_score = len(take_s2.replace(skip_character, \"\"))\n        record[take_s2_key] = (take_s2, take_s2_score)\n\n    return take_s1 if take_s1_score > take_s2_score else skip_character + take_s2\n\n\nclass Wav2VecAlignment:\n    \"\"\"\n    Uses wav2vec2 to perform audio<->text alignment.\n    \"\"\"\n\n    def __init__(self, device=\"cuda\"):\n        self.model = Wav2Vec2ForCTC.from_pretrained(\"jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\").cpu()\n        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"jbetker/tacotron-symbols\")\n        self.device = device\n\n    def align(self, audio, expected_text, audio_sample_rate=24000):\n        orig_len = audio.shape[-1]\n\n        with torch.no_grad():\n            self.model = self.model.to(self.device)\n            audio = audio.to(self.device)\n            audio = torchaudio.functional.resample(audio, audio_sample_rate, 16000)\n            clip_norm = (audio - audio.mean()) / torch.sqrt(audio.var() + 1e-7)\n            logits = self.model(clip_norm).logits\n            self.model = self.model.cpu()\n\n        logits = logits[0]\n        pred_string = self.tokenizer.decode(logits.argmax(-1).tolist())\n\n        fixed_expectation = max_alignment(expected_text.lower(), pred_string)\n        w2v_compression = orig_len // logits.shape[0]\n        expected_tokens = self.tokenizer.encode(fixed_expectation)\n        expected_chars = list(fixed_expectation)\n        if len(expected_tokens) == 1:\n            return [0]  # The alignment is simple; there is only one token.\n        expected_tokens.pop(0)  # The first token is a given.\n        expected_chars.pop(0)\n\n        alignments = [0]\n\n        def pop_till_you_win():\n            if len(expected_tokens) == 0:\n                return None\n            popped = expected_tokens.pop(0)\n            popped_char = expected_chars.pop(0)\n            while popped_char == \"~\":\n                alignments.append(-1)\n                if len(expected_tokens) == 0:\n                    return None\n                popped = expected_tokens.pop(0)\n                popped_char = expected_chars.pop(0)\n            return popped\n\n        next_expected_token = pop_till_you_win()\n        for i, logit in enumerate(logits):\n            top = logit.argmax()\n            if next_expected_token == top:\n                alignments.append(i * w2v_compression)\n                if len(expected_tokens) > 0:\n                    next_expected_token = pop_till_you_win()\n                else:\n                    break\n\n        pop_till_you_win()\n        if not (len(expected_tokens) == 0 and len(alignments) == len(expected_text)):\n            torch.save([audio, expected_text], \"alignment_debug.pth\")\n            assert False, (\n                \"Something went wrong with the alignment algorithm. I've dumped a file, 'alignment_debug.pth' to\"\n                \"your current working directory. Please report this along with the file so it can get fixed.\"\n            )\n\n        # Now fix up alignments. Anything with -1 should be interpolated.\n        alignments.append(orig_len)  # This'll get removed but makes the algorithm below more readable.\n        for i in range(len(alignments)):\n            if alignments[i] == -1:\n                for j in range(i + 1, len(alignments)):\n                    if alignments[j] != -1:\n                        next_found_token = j\n                        break\n                for j in range(i, next_found_token):\n                    gap = alignments[next_found_token] - alignments[i - 1]\n                    alignments[j] = (j - i + 1) * gap // (next_found_token - i + 1) + alignments[i - 1]\n\n        return alignments[:-1]\n\n    def redact(self, audio, expected_text, audio_sample_rate=24000):\n        if \"[\" not in expected_text:\n            return audio\n        splitted = expected_text.split(\"[\")\n        fully_split = [splitted[0]]\n        for spl in splitted[1:]:\n            assert \"]\" in spl, 'Every \"[\" character must be paired with a \"]\" with no nesting.'\n            fully_split.extend(spl.split(\"]\"))\n\n        # At this point, fully_split is a list of strings, with every other string being something that should be redacted.\n        non_redacted_intervals = []\n        last_point = 0\n        for i in range(len(fully_split)):\n            if i % 2 == 0:\n                end_interval = max(0, last_point + len(fully_split[i]) - 1)\n                non_redacted_intervals.append((last_point, end_interval))\n            last_point += len(fully_split[i])\n\n        bare_text = \"\".join(fully_split)\n        alignments = self.align(audio, bare_text, audio_sample_rate)\n\n        output_audio = []\n        for nri in non_redacted_intervals:\n            start, stop = nri\n            output_audio.append(audio[:, alignments[start] : alignments[stop]])\n        return torch.cat(output_audio, dim=-1)\n", "TTS/tts/layers/tortoise/diffusion_decoder.py": "import math\nimport random\nfrom abc import abstractmethod\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import autocast\n\nfrom TTS.tts.layers.tortoise.arch_utils import AttentionBlock, normalization\n\n\ndef is_latent(t):\n    return t.dtype == torch.float\n\n\ndef is_sequence(t):\n    return t.dtype == torch.long\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\nclass TimestepBlock(nn.Module):\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\n\n\nclass ResBlock(TimestepBlock):\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        dims=2,\n        kernel_size=3,\n        efficient_config=True,\n        use_scale_shift_norm=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_scale_shift_norm = use_scale_shift_norm\n        padding = {1: 0, 3: 1, 5: 2}[kernel_size]\n        eff_kernel = 1 if efficient_config else 3\n        eff_padding = 0 if efficient_config else 1\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            nn.Conv1d(channels, self.out_channels, eff_kernel, padding=eff_padding),\n        )\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            nn.Conv1d(self.out_channels, self.out_channels, kernel_size, padding=padding),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        else:\n            self.skip_connection = nn.Conv1d(channels, self.out_channels, eff_kernel, padding=eff_padding)\n\n    def forward(self, x, emb):\n        h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = torch.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass DiffusionLayer(TimestepBlock):\n    def __init__(self, model_channels, dropout, num_heads):\n        super().__init__()\n        self.resblk = ResBlock(\n            model_channels,\n            model_channels,\n            dropout,\n            model_channels,\n            dims=1,\n            use_scale_shift_norm=True,\n        )\n        self.attn = AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True)\n\n    def forward(self, x, time_emb):\n        y = self.resblk(x, time_emb)\n        return self.attn(y)\n\n\nclass DiffusionTts(nn.Module):\n    def __init__(\n        self,\n        model_channels=512,\n        num_layers=8,\n        in_channels=100,\n        in_latent_channels=512,\n        in_tokens=8193,\n        out_channels=200,  # mean and variance\n        dropout=0,\n        use_fp16=False,\n        num_heads=16,\n        # Parameters for regularization.\n        layer_drop=0.1,\n        unconditioned_percentage=0.1,  # This implements a mechanism similar to what is used in classifier-free training.\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.dropout = dropout\n        self.num_heads = num_heads\n        self.unconditioned_percentage = unconditioned_percentage\n        self.enable_fp16 = use_fp16\n        self.layer_drop = layer_drop\n\n        self.inp_block = nn.Conv1d(in_channels, model_channels, 3, 1, 1)\n        self.time_embed = nn.Sequential(\n            nn.Linear(model_channels, model_channels),\n            nn.SiLU(),\n            nn.Linear(model_channels, model_channels),\n        )\n\n        # Either code_converter or latent_converter is used, depending on what type of conditioning data is fed.\n        # This model is meant to be able to be trained on both for efficiency purposes - it is far less computationally\n        # complex to generate tokens, while generating latents will normally mean propagating through a deep autoregressive\n        # transformer network.\n        self.code_embedding = nn.Embedding(in_tokens, model_channels)\n        self.code_converter = nn.Sequential(\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n        )\n        self.code_norm = normalization(model_channels)\n        self.latent_conditioner = nn.Sequential(\n            nn.Conv1d(in_latent_channels, model_channels, 3, padding=1),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n            AttentionBlock(model_channels, num_heads, relative_pos_embeddings=True),\n        )\n        self.contextual_embedder = nn.Sequential(\n            nn.Conv1d(in_channels, model_channels, 3, padding=1, stride=2),\n            nn.Conv1d(model_channels, model_channels * 2, 3, padding=1, stride=2),\n            AttentionBlock(\n                model_channels * 2,\n                num_heads,\n                relative_pos_embeddings=True,\n                do_checkpoint=False,\n            ),\n            AttentionBlock(\n                model_channels * 2,\n                num_heads,\n                relative_pos_embeddings=True,\n                do_checkpoint=False,\n            ),\n            AttentionBlock(\n                model_channels * 2,\n                num_heads,\n                relative_pos_embeddings=True,\n                do_checkpoint=False,\n            ),\n            AttentionBlock(\n                model_channels * 2,\n                num_heads,\n                relative_pos_embeddings=True,\n                do_checkpoint=False,\n            ),\n            AttentionBlock(\n                model_channels * 2,\n                num_heads,\n                relative_pos_embeddings=True,\n                do_checkpoint=False,\n            ),\n        )\n        self.unconditioned_embedding = nn.Parameter(torch.randn(1, model_channels, 1))\n        self.conditioning_timestep_integrator = TimestepEmbedSequential(\n            DiffusionLayer(model_channels, dropout, num_heads),\n            DiffusionLayer(model_channels, dropout, num_heads),\n            DiffusionLayer(model_channels, dropout, num_heads),\n        )\n\n        self.integrating_conv = nn.Conv1d(model_channels * 2, model_channels, kernel_size=1)\n        self.mel_head = nn.Conv1d(model_channels, in_channels, kernel_size=3, padding=1)\n\n        self.layers = nn.ModuleList(\n            [DiffusionLayer(model_channels, dropout, num_heads) for _ in range(num_layers)]\n            + [\n                ResBlock(\n                    model_channels,\n                    model_channels,\n                    dropout,\n                    dims=1,\n                    use_scale_shift_norm=True,\n                )\n                for _ in range(3)\n            ]\n        )\n\n        self.out = nn.Sequential(\n            normalization(model_channels),\n            nn.SiLU(),\n            nn.Conv1d(model_channels, out_channels, 3, padding=1),\n        )\n\n    def get_grad_norm_parameter_groups(self):\n        groups = {\n            \"minicoder\": list(self.contextual_embedder.parameters()),\n            \"layers\": list(self.layers.parameters()),\n            \"code_converters\": list(self.code_embedding.parameters())\n            + list(self.code_converter.parameters())\n            + list(self.latent_conditioner.parameters())\n            + list(self.latent_conditioner.parameters()),\n            \"timestep_integrator\": list(self.conditioning_timestep_integrator.parameters())\n            + list(self.integrating_conv.parameters()),\n            \"time_embed\": list(self.time_embed.parameters()),\n        }\n        return groups\n\n    def get_conditioning(self, conditioning_input):\n        speech_conditioning_input = (\n            conditioning_input.unsqueeze(1) if len(conditioning_input.shape) == 3 else conditioning_input\n        )\n        conds = []\n        for j in range(speech_conditioning_input.shape[1]):\n            conds.append(self.contextual_embedder(speech_conditioning_input[:, j]))\n        conds = torch.cat(conds, dim=-1)\n        conds = conds.mean(dim=-1)\n        return conds\n\n    def timestep_independent(\n        self,\n        aligned_conditioning,\n        conditioning_latent,\n        expected_seq_len,\n        return_code_pred,\n    ):\n        # Shuffle aligned_latent to BxCxS format\n        if is_latent(aligned_conditioning):\n            aligned_conditioning = aligned_conditioning.permute(0, 2, 1)\n\n        cond_scale, cond_shift = torch.chunk(conditioning_latent, 2, dim=1)\n        if is_latent(aligned_conditioning):\n            code_emb = self.latent_conditioner(aligned_conditioning)\n        else:\n            code_emb = self.code_embedding(aligned_conditioning).permute(0, 2, 1)\n            code_emb = self.code_converter(code_emb)\n        code_emb = self.code_norm(code_emb) * (1 + cond_scale.unsqueeze(-1)) + cond_shift.unsqueeze(-1)\n\n        unconditioned_batches = torch.zeros((code_emb.shape[0], 1, 1), device=code_emb.device)\n        # Mask out the conditioning branch for whole batch elements, implementing something similar to classifier-free guidance.\n        if self.training and self.unconditioned_percentage > 0:\n            unconditioned_batches = (\n                torch.rand((code_emb.shape[0], 1, 1), device=code_emb.device) < self.unconditioned_percentage\n            )\n            code_emb = torch.where(\n                unconditioned_batches,\n                self.unconditioned_embedding.repeat(aligned_conditioning.shape[0], 1, 1),\n                code_emb,\n            )\n        expanded_code_emb = F.interpolate(code_emb, size=expected_seq_len, mode=\"nearest\")\n\n        if not return_code_pred:\n            return expanded_code_emb\n        else:\n            mel_pred = self.mel_head(expanded_code_emb)\n            # Multiply mel_pred by !unconditioned_branches, which drops the gradient on unconditioned branches. This is because we don't want that gradient being used to train parameters through the codes_embedder as it unbalances contributions to that network from the MSE loss.\n            mel_pred = mel_pred * unconditioned_batches.logical_not()\n            return expanded_code_emb, mel_pred\n\n    def forward(\n        self,\n        x,\n        timesteps,\n        aligned_conditioning=None,\n        conditioning_latent=None,\n        precomputed_aligned_embeddings=None,\n        conditioning_free=False,\n        return_code_pred=False,\n    ):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param aligned_conditioning: an aligned latent or sequence of tokens providing useful data about the sample to be produced.\n        :param conditioning_latent: a pre-computed conditioning latent; see get_conditioning().\n        :param precomputed_aligned_embeddings: Embeddings returned from self.timestep_independent()\n        :param conditioning_free: When set, all conditioning inputs (including tokens and conditioning_input) will not be considered.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert precomputed_aligned_embeddings is not None or (\n            aligned_conditioning is not None and conditioning_latent is not None\n        )\n        assert not (\n            return_code_pred and precomputed_aligned_embeddings is not None\n        )  # These two are mutually exclusive.\n\n        unused_params = []\n        if conditioning_free:\n            code_emb = self.unconditioned_embedding.repeat(x.shape[0], 1, x.shape[-1])\n            unused_params.extend(list(self.code_converter.parameters()) + list(self.code_embedding.parameters()))\n            unused_params.extend(list(self.latent_conditioner.parameters()))\n        else:\n            if precomputed_aligned_embeddings is not None:\n                code_emb = precomputed_aligned_embeddings\n            else:\n                code_emb, mel_pred = self.timestep_independent(\n                    aligned_conditioning, conditioning_latent, x.shape[-1], True\n                )\n                if is_latent(aligned_conditioning):\n                    unused_params.extend(\n                        list(self.code_converter.parameters()) + list(self.code_embedding.parameters())\n                    )\n                else:\n                    unused_params.extend(list(self.latent_conditioner.parameters()))\n\n            unused_params.append(self.unconditioned_embedding)\n\n        time_emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n        code_emb = self.conditioning_timestep_integrator(code_emb, time_emb)\n        x = self.inp_block(x)\n        x = torch.cat([x, code_emb], dim=1)\n        x = self.integrating_conv(x)\n        for i, lyr in enumerate(self.layers):\n            # Do layer drop where applicable. Do not drop first and last layers.\n            if (\n                self.training\n                and self.layer_drop > 0\n                and i != 0\n                and i != (len(self.layers) - 1)\n                and random.random() < self.layer_drop\n            ):\n                unused_params.extend(list(lyr.parameters()))\n            else:\n                # First and last blocks will have autocast disabled for improved precision.\n                with autocast(x.device.type, enabled=self.enable_fp16 and i != 0):\n                    x = lyr(x, time_emb)\n\n        x = x.float()\n        out = self.out(x)\n\n        # Involve probabilistic or possibly unused parameters in loss so we don't get DDP errors.\n        extraneous_addition = 0\n        for p in unused_params:\n            extraneous_addition = extraneous_addition + p.mean()\n        out = out + extraneous_addition * 0\n\n        if return_code_pred:\n            return out, mel_pred\n        return out\n\n\nif __name__ == \"__main__\":\n    clip = torch.randn(2, 100, 400)\n    aligned_latent = torch.randn(2, 388, 512)\n    aligned_sequence = torch.randint(0, 8192, (2, 100))\n    cond = torch.randn(2, 100, 400)\n    ts = torch.LongTensor([600, 600])\n    model = DiffusionTts(512, layer_drop=0.3, unconditioned_percentage=0.5)\n    # Test with latent aligned conditioning\n    # o = model(clip, ts, aligned_latent, cond)\n    # Test with sequence aligned conditioning\n    o = model(clip, ts, aligned_sequence, cond)\n", "TTS/tts/layers/tortoise/classifier.py": "import torch\nimport torch.nn as nn\n\nfrom TTS.tts.layers.tortoise.arch_utils import AttentionBlock, Downsample, Upsample, normalization, zero_module\n\n\nclass ResBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        up=False,\n        down=False,\n        kernel_size=3,\n        do_checkpoint=True,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.do_checkpoint = do_checkpoint\n        padding = 1 if kernel_size == 3 else 2\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            nn.Conv1d(channels, self.out_channels, kernel_size, padding=padding),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Conv1d(self.out_channels, self.out_channels, kernel_size, padding=padding)),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = nn.Conv1d(dims, channels, self.out_channels, kernel_size, padding=padding)\n        else:\n            self.skip_connection = nn.Conv1d(dims, channels, self.out_channels, 1)\n\n    def forward(self, x):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AudioMiniEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        base_channels=128,\n        depth=2,\n        resnet_blocks=2,\n        attn_blocks=4,\n        num_attn_heads=4,\n        dropout=0,\n        downsample_factor=2,\n        kernel_size=3,\n    ):\n        super().__init__()\n        self.init = nn.Sequential(nn.Conv1d(spec_dim, base_channels, 3, padding=1))\n        ch = base_channels\n        res = []\n        self.layers = depth\n        for l in range(depth):\n            for r in range(resnet_blocks):\n                res.append(ResBlock(ch, dropout, do_checkpoint=False, kernel_size=kernel_size))\n            res.append(Downsample(ch, use_conv=True, out_channels=ch * 2, factor=downsample_factor))\n            ch *= 2\n        self.res = nn.Sequential(*res)\n        self.final = nn.Sequential(normalization(ch), nn.SiLU(), nn.Conv1d(ch, embedding_dim, 1))\n        attn = []\n        for a in range(attn_blocks):\n            attn.append(AttentionBlock(embedding_dim, num_attn_heads, do_checkpoint=False))\n        self.attn = nn.Sequential(*attn)\n        self.dim = embedding_dim\n\n    def forward(self, x):\n        h = self.init(x)\n        h = self.res(h)\n        h = self.final(h)\n        for blk in self.attn:\n            h = blk(h)\n        return h[:, :, 0]\n\n\nclass AudioMiniEncoderWithClassifierHead(nn.Module):\n    def __init__(self, classes, distribute_zero_label=True, **kwargs):\n        super().__init__()\n        self.enc = AudioMiniEncoder(**kwargs)\n        self.head = nn.Linear(self.enc.dim, classes)\n        self.num_classes = classes\n        self.distribute_zero_label = distribute_zero_label\n\n    def forward(self, x, labels=None):\n        h = self.enc(x)\n        logits = self.head(h)\n        if labels is None:\n            return logits\n        else:\n            if self.distribute_zero_label:\n                oh_labels = nn.functional.one_hot(labels, num_classes=self.num_classes)\n                zeros_indices = (labels == 0).unsqueeze(-1)\n                # Distribute 20% of the probability mass on all classes when zero is specified, to compensate for dataset noise.\n                zero_extra_mass = torch.full_like(\n                    oh_labels,\n                    dtype=torch.float,\n                    fill_value=0.2 / (self.num_classes - 1),\n                )\n                zero_extra_mass[:, 0] = -0.2\n                zero_extra_mass = zero_extra_mass * zeros_indices\n                oh_labels = oh_labels + zero_extra_mass\n            else:\n                oh_labels = labels\n            loss = nn.functional.cross_entropy(logits, oh_labels)\n            return loss\n", "TTS/tts/layers/tortoise/dpm_solver.py": "import math\n\nimport torch\n\n\nclass NoiseScheduleVP:\n    def __init__(\n        self,\n        schedule=\"discrete\",\n        betas=None,\n        alphas_cumprod=None,\n        continuous_beta_0=0.1,\n        continuous_beta_1=20.0,\n        dtype=torch.float32,\n    ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n\n            t = self.inverse_lambda(lambda_t)\n\n        ===============================================================\n\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n\n        1. For discrete-time DPMs:\n\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(1 - betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \"\"\"\n\n        if schedule not in [\"discrete\", \"linear\", \"cosine\"]:\n            raise ValueError(\n                \"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(\n                    schedule\n                )\n            )\n\n        self.schedule = schedule\n        if schedule == \"discrete\":\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.0\n            self.t_array = torch.linspace(0.0, 1.0, self.total_N + 1)[1:].reshape((1, -1)).to(dtype=dtype)\n            self.log_alpha_array = log_alphas.reshape(\n                (\n                    1,\n                    -1,\n                )\n            ).to(dtype=dtype)\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.0\n            self.cosine_t_max = (\n                math.atan(self.cosine_beta_max * (1.0 + self.cosine_s) / math.pi)\n                * 2.0\n                * (1.0 + self.cosine_s)\n                / math.pi\n                - self.cosine_s\n            )\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1.0 + self.cosine_s) * math.pi / 2.0))\n            self.schedule = schedule\n            if schedule == \"cosine\":\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.0\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == \"discrete\":\n            return interpolate_fn(\n                t.reshape((-1, 1)),\n                self.t_array.to(t.device),\n                self.log_alpha_array.to(t.device),\n            ).reshape((-1))\n        elif self.schedule == \"linear\":\n            return -0.25 * t**2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == \"cosine\":\n\n            def log_alpha_fn(s):\n                return torch.log(torch.cos((s + self.cosine_s) / (1.0 + self.cosine_s) * math.pi / 2.0))\n\n            log_alpha_t = log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1.0 - torch.exp(2.0 * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1.0 - torch.exp(2.0 * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == \"linear\":\n            tmp = 2.0 * (self.beta_1 - self.beta_0) * torch.logaddexp(-2.0 * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == \"discrete\":\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2.0 * lamb)\n            t = interpolate_fn(\n                log_alpha.reshape((-1, 1)),\n                torch.flip(self.log_alpha_array.to(lamb.device), [1]),\n                torch.flip(self.t_array.to(lamb.device), [1]),\n            )\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2.0 * lamb, torch.zeros((1,)).to(lamb))\n\n            def t_fn(log_alpha_t):\n                return (\n                    torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0))\n                    * 2.0\n                    * (1.0 + self.cosine_s)\n                    / math.pi\n                    - self.cosine_s\n                )\n\n            t = t_fn(log_alpha)\n            return t\n\n\ndef model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.0,\n    classifier_fn=None,\n    classifier_kwargs={},\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n\n    We support four types of the diffusion model by setting `model_type`:\n\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n\n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == \"discrete\":\n            return (t_continuous - 1.0 / noise_schedule.total_N) * 1000.0\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        t_input = get_model_input_time(t_continuous)\n        if cond is None:\n            output = model(x, t_input, **model_kwargs)\n        else:\n            output = model(x, t_input, cond, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            return (x - alpha_t * output) / sigma_t\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            return alpha_t * output + sigma_t * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            return -sigma_t * output\n\n    def cond_grad_fn(x, t_input):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * sigma_t * cond_grad\n        elif guidance_type == \"classifier-free\":\n            if guidance_scale == 1.0 or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\", \"score\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn\n\n\nclass DPM_Solver:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        algorithm_type=\"dpmsolver++\",\n        correcting_x0_fn=None,\n        correcting_xt_fn=None,\n        thresholding_max_val=1.0,\n        dynamic_thresholding_ratio=0.995,\n    ):\n        \"\"\"Construct a DPM-Solver.\n\n        We support both DPM-Solver (`algorithm_type=\"dpmsolver\"`) and DPM-Solver++ (`algorithm_type=\"dpmsolver++\"`).\n\n        We also support the \"dynamic thresholding\" method in Imagen[1]. For pixel-space diffusion models, you\n        can set both `algorithm_type=\"dpmsolver++\"` and `correcting_x0_fn=\"dynamic_thresholding\"` to use the\n        dynamic thresholding. The \"dynamic thresholding\" can greatly improve the sample quality for pixel-space\n        DPMs with large guidance scales. Note that the thresholding method is **unsuitable** for latent-space\n        DPMs (such as stable-diffusion).\n\n        To support advanced algorithms in image-to-image applications, we also support corrector functions for\n        both x0 and xt.\n\n        Args:\n            model_fn: A noise prediction model function which accepts the continuous-time input (t in [epsilon, T]):\n                ``\n                def model_fn(x, t_continuous):\n                    return noise\n                ``\n                The shape of `x` is `(batch_size, **shape)`, and the shape of `t_continuous` is `(batch_size,)`.\n            noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n            algorithm_type: A `str`. Either \"dpmsolver\" or \"dpmsolver++\".\n            correcting_x0_fn: A `str` or a function with the following format:\n                ```\n                def correcting_x0_fn(x0, t):\n                    x0_new = ...\n                    return x0_new\n                ```\n                This function is to correct the outputs of the data prediction model at each sampling step. e.g.,\n                ```\n                x0_pred = data_pred_model(xt, t)\n                if correcting_x0_fn is not None:\n                    x0_pred = correcting_x0_fn(x0_pred, t)\n                xt_1 = update(x0_pred, xt, t)\n                ```\n                If `correcting_x0_fn=\"dynamic_thresholding\"`, we use the dynamic thresholding proposed in Imagen[1].\n            correcting_xt_fn: A function with the following format:\n                ```\n                def correcting_xt_fn(xt, t, step):\n                    x_new = ...\n                    return x_new\n                ```\n                This function is to correct the intermediate samples xt at each sampling step. e.g.,\n                ```\n                xt = ...\n                xt = correcting_xt_fn(xt, t, step)\n                ```\n            thresholding_max_val: A `float`. The max value for thresholding.\n                Valid only when use `dpmsolver++` and `correcting_x0_fn=\"dynamic_thresholding\"`.\n            dynamic_thresholding_ratio: A `float`. The ratio for dynamic thresholding (see Imagen[1] for details).\n                Valid only when use `dpmsolver++` and `correcting_x0_fn=\"dynamic_thresholding\"`.\n\n        [1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\n            Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models\n            with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n        \"\"\"\n        self.model = lambda x, t: model_fn(x, t.expand((x.shape[0])))\n        self.noise_schedule = noise_schedule\n        assert algorithm_type in [\"dpmsolver\", \"dpmsolver++\"]\n        self.algorithm_type = algorithm_type\n        if correcting_x0_fn == \"dynamic_thresholding\":\n            self.correcting_x0_fn = self.dynamic_thresholding_fn\n        else:\n            self.correcting_x0_fn = correcting_x0_fn\n        self.correcting_xt_fn = correcting_xt_fn\n        self.dynamic_thresholding_ratio = dynamic_thresholding_ratio\n        self.thresholding_max_val = thresholding_max_val\n\n    def dynamic_thresholding_fn(self, x0, t):\n        \"\"\"\n        The dynamic thresholding method.\n        \"\"\"\n        dims = x0.dim()\n        p = self.dynamic_thresholding_ratio\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(\n            torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)),\n            dims,\n        )\n        x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with corrector).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - sigma_t * noise) / alpha_t\n        if self.correcting_x0_fn is not None:\n            x0 = self.correcting_x0_fn(x0, t)\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model.\n        \"\"\"\n        if self.algorithm_type == \"dpmsolver++\":\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n\n        Args:\n            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\n                - 'logSNR': uniform logSNR for the time steps.\n                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)\n                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            N: A `int`. The total number of the spacing of the time steps.\n            device: A torch device.\n        Returns:\n            A pytorch tensor of the time steps, with the shape (N + 1,).\n        \"\"\"\n        if skip_type == \"logSNR\":\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == \"time_uniform\":\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == \"time_quadratic\":\n            t_order = 2\n            t = torch.linspace(t_T ** (1.0 / t_order), t_0 ** (1.0 / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(\n                \"Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\".format(skip_type)\n            )\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n\n        We combine both DPM-Solver-1,2,3 to use all the function evaluations, which is named as \"DPM-Solver-fast\".\n        Given a fixed number of function evaluations by `steps`, the sampling procedure by DPM-Solver-fast is:\n            - If order == 1:\n                We take `steps` of DPM-Solver-1 (i.e. DDIM).\n            - If order == 2:\n                - Denote K = (steps // 2). We take K or (K + 1) intermediate time steps for sampling.\n                - If steps % 2 == 0, we use K steps of DPM-Solver-2.\n                - If steps % 2 == 1, we use K steps of DPM-Solver-2 and 1 step of DPM-Solver-1.\n            - If order == 3:\n                - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.\n                - If steps % 3 == 0, we use (K - 2) steps of DPM-Solver-3, and 1 step of DPM-Solver-2 and 1 step of DPM-Solver-1.\n                - If steps % 3 == 1, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-1.\n                - If steps % 3 == 2, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-2.\n\n        ============================================\n        Args:\n            order: A `int`. The max order for the solver (2 or 3).\n            steps: A `int`. The total number of function evaluations (NFE).\n            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\n                - 'logSNR': uniform logSNR for the time steps.\n                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)\n                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            device: A torch device.\n        Returns:\n            orders: A list of the solver order of each step.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [\n                    3,\n                ] * (\n                    K - 2\n                ) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [\n                    3,\n                ] * (\n                    K - 1\n                ) + [1]\n            else:\n                orders = [\n                    3,\n                ] * (\n                    K - 1\n                ) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [\n                    2,\n                ] * K\n            else:\n                K = steps // 2 + 1\n                orders = [\n                    2,\n                ] * (\n                    K - 1\n                ) + [1]\n        elif order == 1:\n            K = 1\n            orders = [\n                1,\n            ] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == \"logSNR\":\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[\n                torch.cumsum(\n                    torch.tensor(\n                        [\n                            0,\n                        ]\n                        + orders\n                    ),\n                    0,\n                ).to(device)\n            ]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization.\n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def dpm_solver_first_update(self, x, s, t, model_s=None, return_intermediate=False):\n        \"\"\"\n        DPM-Solver-1 (equivalent to DDIM) from time `s` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (1,).\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        ns = self.noise_schedule\n        dims = x.dim()\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        log_alpha_s, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(t)\n        sigma_s, sigma_t = ns.marginal_std(s), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        if self.algorithm_type == \"dpmsolver++\":\n            phi_1 = torch.expm1(-h)\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_t = sigma_t / sigma_s * x - alpha_t * phi_1 * model_s\n            if return_intermediate:\n                return x_t, {\"model_s\": model_s}\n            else:\n                return x_t\n        else:\n            phi_1 = torch.expm1(h)\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_t = torch.exp(log_alpha_t - log_alpha_s) * x - (sigma_t * phi_1) * model_s\n            if return_intermediate:\n                return x_t, {\"model_s\": model_s}\n            else:\n                return x_t\n\n    def singlestep_dpm_solver_second_update(\n        self,\n        x,\n        s,\n        t,\n        r1=0.5,\n        model_s=None,\n        return_intermediate=False,\n        solver_type=\"dpmsolver\",\n    ):\n        \"\"\"\n        Singlestep solver DPM-Solver-2 from time `s` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (1,).\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            r1: A `float`. The hyperparameter of the second-order solver.\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s` and `s1` (the intermediate time).\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in [\"dpmsolver\", \"taylor\"]:\n            raise ValueError(\"'solver_type' must be either 'dpmsolver' or 'taylor', got {}\".format(solver_type))\n        if r1 is None:\n            r1 = 0.5\n        ns = self.noise_schedule\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        lambda_s1 = lambda_s + r1 * h\n        s1 = ns.inverse_lambda(lambda_s1)\n        log_alpha_s, log_alpha_s1, log_alpha_t = (\n            ns.marginal_log_mean_coeff(s),\n            ns.marginal_log_mean_coeff(s1),\n            ns.marginal_log_mean_coeff(t),\n        )\n        sigma_s, sigma_s1, sigma_t = (\n            ns.marginal_std(s),\n            ns.marginal_std(s1),\n            ns.marginal_std(t),\n        )\n        alpha_s1, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_t)\n\n        if self.algorithm_type == \"dpmsolver++\":\n            phi_11 = torch.expm1(-r1 * h)\n            phi_1 = torch.expm1(-h)\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_s1 = (sigma_s1 / sigma_s) * x - (alpha_s1 * phi_11) * model_s\n            model_s1 = self.model_fn(x_s1, s1)\n            if solver_type == \"dpmsolver\":\n                x_t = (\n                    (sigma_t / sigma_s) * x\n                    - (alpha_t * phi_1) * model_s\n                    - (0.5 / r1) * (alpha_t * phi_1) * (model_s1 - model_s)\n                )\n            elif solver_type == \"taylor\":\n                x_t = (\n                    (sigma_t / sigma_s) * x\n                    - (alpha_t * phi_1) * model_s\n                    + (1.0 / r1) * (alpha_t * (phi_1 / h + 1.0)) * (model_s1 - model_s)\n                )\n        else:\n            phi_11 = torch.expm1(r1 * h)\n            phi_1 = torch.expm1(h)\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_s1 = torch.exp(log_alpha_s1 - log_alpha_s) * x - (sigma_s1 * phi_11) * model_s\n            model_s1 = self.model_fn(x_s1, s1)\n            if solver_type == \"dpmsolver\":\n                x_t = (\n                    torch.exp(log_alpha_t - log_alpha_s) * x\n                    - (sigma_t * phi_1) * model_s\n                    - (0.5 / r1) * (sigma_t * phi_1) * (model_s1 - model_s)\n                )\n            elif solver_type == \"taylor\":\n                x_t = (\n                    torch.exp(log_alpha_t - log_alpha_s) * x\n                    - (sigma_t * phi_1) * model_s\n                    - (1.0 / r1) * (sigma_t * (phi_1 / h - 1.0)) * (model_s1 - model_s)\n                )\n        if return_intermediate:\n            return x_t, {\"model_s\": model_s, \"model_s1\": model_s1}\n        else:\n            return x_t\n\n    def singlestep_dpm_solver_third_update(\n        self,\n        x,\n        s,\n        t,\n        r1=1.0 / 3.0,\n        r2=2.0 / 3.0,\n        model_s=None,\n        model_s1=None,\n        return_intermediate=False,\n        solver_type=\"dpmsolver\",\n    ):\n        \"\"\"\n        Singlestep solver DPM-Solver-3 from time `s` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (1,).\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            r1: A `float`. The hyperparameter of the third-order solver.\n            r2: A `float`. The hyperparameter of the third-order solver.\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            model_s1: A pytorch tensor. The model function evaluated at time `s1` (the intermediate time given by `r1`).\n                If `model_s1` is None, we evaluate the model at `s1`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in [\"dpmsolver\", \"taylor\"]:\n            raise ValueError(\"'solver_type' must be either 'dpmsolver' or 'taylor', got {}\".format(solver_type))\n        if r1 is None:\n            r1 = 1.0 / 3.0\n        if r2 is None:\n            r2 = 2.0 / 3.0\n        ns = self.noise_schedule\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        lambda_s1 = lambda_s + r1 * h\n        lambda_s2 = lambda_s + r2 * h\n        s1 = ns.inverse_lambda(lambda_s1)\n        s2 = ns.inverse_lambda(lambda_s2)\n        log_alpha_s, log_alpha_s1, log_alpha_s2, log_alpha_t = (\n            ns.marginal_log_mean_coeff(s),\n            ns.marginal_log_mean_coeff(s1),\n            ns.marginal_log_mean_coeff(s2),\n            ns.marginal_log_mean_coeff(t),\n        )\n        sigma_s, sigma_s1, sigma_s2, sigma_t = (\n            ns.marginal_std(s),\n            ns.marginal_std(s1),\n            ns.marginal_std(s2),\n            ns.marginal_std(t),\n        )\n        alpha_s1, alpha_s2, alpha_t = (\n            torch.exp(log_alpha_s1),\n            torch.exp(log_alpha_s2),\n            torch.exp(log_alpha_t),\n        )\n\n        if self.algorithm_type == \"dpmsolver++\":\n            phi_11 = torch.expm1(-r1 * h)\n            phi_12 = torch.expm1(-r2 * h)\n            phi_1 = torch.expm1(-h)\n            phi_22 = torch.expm1(-r2 * h) / (r2 * h) + 1.0\n            phi_2 = phi_1 / h + 1.0\n            phi_3 = phi_2 / h - 0.5\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            if model_s1 is None:\n                x_s1 = (sigma_s1 / sigma_s) * x - (alpha_s1 * phi_11) * model_s\n                model_s1 = self.model_fn(x_s1, s1)\n            x_s2 = (\n                (sigma_s2 / sigma_s) * x\n                - (alpha_s2 * phi_12) * model_s\n                + r2 / r1 * (alpha_s2 * phi_22) * (model_s1 - model_s)\n            )\n            model_s2 = self.model_fn(x_s2, s2)\n            if solver_type == \"dpmsolver\":\n                x_t = (\n                    (sigma_t / sigma_s) * x\n                    - (alpha_t * phi_1) * model_s\n                    + (1.0 / r2) * (alpha_t * phi_2) * (model_s2 - model_s)\n                )\n            elif solver_type == \"taylor\":\n                D1_0 = (1.0 / r1) * (model_s1 - model_s)\n                D1_1 = (1.0 / r2) * (model_s2 - model_s)\n                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\n                D2 = 2.0 * (D1_1 - D1_0) / (r2 - r1)\n                x_t = (\n                    (sigma_t / sigma_s) * x\n                    - (alpha_t * phi_1) * model_s\n                    + (alpha_t * phi_2) * D1\n                    - (alpha_t * phi_3) * D2\n                )\n        else:\n            phi_11 = torch.expm1(r1 * h)\n            phi_12 = torch.expm1(r2 * h)\n            phi_1 = torch.expm1(h)\n            phi_22 = torch.expm1(r2 * h) / (r2 * h) - 1.0\n            phi_2 = phi_1 / h - 1.0\n            phi_3 = phi_2 / h - 0.5\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            if model_s1 is None:\n                x_s1 = (torch.exp(log_alpha_s1 - log_alpha_s)) * x - (sigma_s1 * phi_11) * model_s\n                model_s1 = self.model_fn(x_s1, s1)\n            x_s2 = (\n                (torch.exp(log_alpha_s2 - log_alpha_s)) * x\n                - (sigma_s2 * phi_12) * model_s\n                - r2 / r1 * (sigma_s2 * phi_22) * (model_s1 - model_s)\n            )\n            model_s2 = self.model_fn(x_s2, s2)\n            if solver_type == \"dpmsolver\":\n                x_t = (\n                    (torch.exp(log_alpha_t - log_alpha_s)) * x\n                    - (sigma_t * phi_1) * model_s\n                    - (1.0 / r2) * (sigma_t * phi_2) * (model_s2 - model_s)\n                )\n            elif solver_type == \"taylor\":\n                D1_0 = (1.0 / r1) * (model_s1 - model_s)\n                D1_1 = (1.0 / r2) * (model_s2 - model_s)\n                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\n                D2 = 2.0 * (D1_1 - D1_0) / (r2 - r1)\n                x_t = (\n                    (torch.exp(log_alpha_t - log_alpha_s)) * x\n                    - (sigma_t * phi_1) * model_s\n                    - (sigma_t * phi_2) * D1\n                    - (sigma_t * phi_3) * D2\n                )\n\n        if return_intermediate:\n            return x_t, {\"model_s\": model_s, \"model_s1\": model_s1, \"model_s2\": model_s2}\n        else:\n            return x_t\n\n    def multistep_dpm_solver_second_update(self, x, model_prev_list, t_prev_list, t, solver_type=\"dpmsolver\"):\n        \"\"\"\n        Multistep solver DPM-Solver-2 from time `t_prev_list[-1]` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in [\"dpmsolver\", \"taylor\"]:\n            raise ValueError(\"'solver_type' must be either 'dpmsolver' or 'taylor', got {}\".format(solver_type))\n        ns = self.noise_schedule\n        model_prev_1, model_prev_0 = model_prev_list[-2], model_prev_list[-1]\n        t_prev_1, t_prev_0 = t_prev_list[-2], t_prev_list[-1]\n        lambda_prev_1, lambda_prev_0, lambda_t = (\n            ns.marginal_lambda(t_prev_1),\n            ns.marginal_lambda(t_prev_0),\n            ns.marginal_lambda(t),\n        )\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h_0 = lambda_prev_0 - lambda_prev_1\n        h = lambda_t - lambda_prev_0\n        r0 = h_0 / h\n        D1_0 = (1.0 / r0) * (model_prev_0 - model_prev_1)\n        if self.algorithm_type == \"dpmsolver++\":\n            phi_1 = torch.expm1(-h)\n            if solver_type == \"dpmsolver\":\n                x_t = (sigma_t / sigma_prev_0) * x - (alpha_t * phi_1) * model_prev_0 - 0.5 * (alpha_t * phi_1) * D1_0\n            elif solver_type == \"taylor\":\n                x_t = (\n                    (sigma_t / sigma_prev_0) * x\n                    - (alpha_t * phi_1) * model_prev_0\n                    + (alpha_t * (phi_1 / h + 1.0)) * D1_0\n                )\n        else:\n            phi_1 = torch.expm1(h)\n            if solver_type == \"dpmsolver\":\n                x_t = (\n                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                    - (sigma_t * phi_1) * model_prev_0\n                    - 0.5 * (sigma_t * phi_1) * D1_0\n                )\n            elif solver_type == \"taylor\":\n                x_t = (\n                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                    - (sigma_t * phi_1) * model_prev_0\n                    - (sigma_t * (phi_1 / h - 1.0)) * D1_0\n                )\n        return x_t\n\n    def multistep_dpm_solver_third_update(self, x, model_prev_list, t_prev_list, t, solver_type=\"dpmsolver\"):\n        \"\"\"\n        Multistep solver DPM-Solver-3 from time `t_prev_list[-1]` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        ns = self.noise_schedule\n        model_prev_2, model_prev_1, model_prev_0 = model_prev_list\n        t_prev_2, t_prev_1, t_prev_0 = t_prev_list\n        lambda_prev_2, lambda_prev_1, lambda_prev_0, lambda_t = (\n            ns.marginal_lambda(t_prev_2),\n            ns.marginal_lambda(t_prev_1),\n            ns.marginal_lambda(t_prev_0),\n            ns.marginal_lambda(t),\n        )\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h_1 = lambda_prev_1 - lambda_prev_2\n        h_0 = lambda_prev_0 - lambda_prev_1\n        h = lambda_t - lambda_prev_0\n        r0, r1 = h_0 / h, h_1 / h\n        D1_0 = (1.0 / r0) * (model_prev_0 - model_prev_1)\n        D1_1 = (1.0 / r1) * (model_prev_1 - model_prev_2)\n        D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)\n        D2 = (1.0 / (r0 + r1)) * (D1_0 - D1_1)\n        if self.algorithm_type == \"dpmsolver++\":\n            phi_1 = torch.expm1(-h)\n            phi_2 = phi_1 / h + 1.0\n            phi_3 = phi_2 / h - 0.5\n            x_t = (\n                (sigma_t / sigma_prev_0) * x\n                - (alpha_t * phi_1) * model_prev_0\n                + (alpha_t * phi_2) * D1\n                - (alpha_t * phi_3) * D2\n            )\n        else:\n            phi_1 = torch.expm1(h)\n            phi_2 = phi_1 / h - 1.0\n            phi_3 = phi_2 / h - 0.5\n            x_t = (\n                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                - (sigma_t * phi_1) * model_prev_0\n                - (sigma_t * phi_2) * D1\n                - (sigma_t * phi_3) * D2\n            )\n        return x_t\n\n    def singlestep_dpm_solver_update(\n        self,\n        x,\n        s,\n        t,\n        order,\n        return_intermediate=False,\n        solver_type=\"dpmsolver\",\n        r1=None,\n        r2=None,\n    ):\n        \"\"\"\n        Singlestep DPM-Solver with the order `order` from time `s` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (1,).\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n            r1: A `float`. The hyperparameter of the second-order or third-order solver.\n            r2: A `float`. The hyperparameter of the third-order solver.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if order == 1:\n            return self.dpm_solver_first_update(x, s, t, return_intermediate=return_intermediate)\n        elif order == 2:\n            return self.singlestep_dpm_solver_second_update(\n                x,\n                s,\n                t,\n                return_intermediate=return_intermediate,\n                solver_type=solver_type,\n                r1=r1,\n            )\n        elif order == 3:\n            return self.singlestep_dpm_solver_third_update(\n                x,\n                s,\n                t,\n                return_intermediate=return_intermediate,\n                solver_type=solver_type,\n                r1=r1,\n                r2=r2,\n            )\n        else:\n            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n\n    def multistep_dpm_solver_update(self, x, model_prev_list, t_prev_list, t, order, solver_type=\"dpmsolver\"):\n        \"\"\"\n        Multistep DPM-Solver with the order `order` from time `t_prev_list[-1]` to time `t`.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\n            t: A pytorch tensor. The ending time, with the shape (1,).\n            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if order == 1:\n            return self.dpm_solver_first_update(x, t_prev_list[-1], t, model_s=model_prev_list[-1])\n        elif order == 2:\n            return self.multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\n        elif order == 3:\n            return self.multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\n        else:\n            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n\n    def dpm_solver_adaptive(\n        self,\n        x,\n        order,\n        t_T,\n        t_0,\n        h_init=0.05,\n        atol=0.0078,\n        rtol=0.05,\n        theta=0.9,\n        t_err=1e-5,\n        solver_type=\"dpmsolver\",\n    ):\n        \"\"\"\n        The adaptive step size solver based on singlestep DPM-Solver.\n\n        Args:\n            x: A pytorch tensor. The initial value at time `t_T`.\n            order: A `int`. The (higher) order of the solver. We only support order == 2 or 3.\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            h_init: A `float`. The initial step size (for logSNR).\n            atol: A `float`. The absolute tolerance of the solver. For image data, the default setting is 0.0078, followed [1].\n            rtol: A `float`. The relative tolerance of the solver. The default setting is 0.05.\n            theta: A `float`. The safety hyperparameter for adapting the step size. The default setting is 0.9, followed [1].\n            t_err: A `float`. The tolerance for the time. We solve the diffusion ODE until the absolute error between the\n                current time and `t_0` is less than `t_err`. The default setting is 1e-5.\n            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.\n        Returns:\n            x_0: A pytorch tensor. The approximated solution at time `t_0`.\n\n        [1] A. Jolicoeur-Martineau, K. Li, R. Pich\u00e9-Taillefer, T. Kachman, and I. Mitliagkas, \"Gotta go fast when generating data with score-based models,\" arXiv preprint arXiv:2105.14080, 2021.\n        \"\"\"\n        ns = self.noise_schedule\n        s = t_T * torch.ones((1,)).to(x)\n        lambda_s = ns.marginal_lambda(s)\n        lambda_0 = ns.marginal_lambda(t_0 * torch.ones_like(s).to(x))\n        h = h_init * torch.ones_like(s).to(x)\n        x_prev = x\n        nfe = 0\n        if order == 2:\n            r1 = 0.5\n\n            def lower_update(x, s, t):\n                return self.dpm_solver_first_update(x, s, t, return_intermediate=True)\n\n            def higher_update(x, s, t, **kwargs):\n                return self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, solver_type=solver_type, **kwargs)\n\n        elif order == 3:\n            r1, r2 = 1.0 / 3.0, 2.0 / 3.0\n\n            def lower_update(x, s, t):\n                return self.singlestep_dpm_solver_second_update(\n                    x, s, t, r1=r1, return_intermediate=True, solver_type=solver_type\n                )\n\n            def higher_update(x, s, t, **kwargs):\n                return self.singlestep_dpm_solver_third_update(x, s, t, r1=r1, r2=r2, solver_type=solver_type, **kwargs)\n\n        else:\n            raise ValueError(\"For adaptive step size solver, order must be 2 or 3, got {}\".format(order))\n        while torch.abs((s - t_0)).mean() > t_err:\n            t = ns.inverse_lambda(lambda_s + h)\n            x_lower, lower_noise_kwargs = lower_update(x, s, t)\n            x_higher = higher_update(x, s, t, **lower_noise_kwargs)\n            delta = torch.max(\n                torch.ones_like(x).to(x) * atol,\n                rtol * torch.max(torch.abs(x_lower), torch.abs(x_prev)),\n            )\n\n            def norm_fn(v):\n                return torch.sqrt(torch.square(v.reshape((v.shape[0], -1))).mean(dim=-1, keepdim=True))\n\n            E = norm_fn((x_higher - x_lower) / delta).max()\n            if torch.all(E <= 1.0):\n                x = x_higher\n                s = t\n                x_prev = x_lower\n                lambda_s = ns.marginal_lambda(s)\n            h = torch.min(\n                theta * h * torch.float_power(E, -1.0 / order).float(),\n                lambda_0 - lambda_s,\n            )\n            nfe += order\n        print(\"adaptive solver nfe\", nfe)\n        return x\n\n    def add_noise(self, x, t, noise=None):\n        \"\"\"\n        Compute the noised input xt = alpha_t * x + sigma_t * noise.\n\n        Args:\n            x: A `torch.Tensor` with shape `(batch_size, *shape)`.\n            t: A `torch.Tensor` with shape `(t_size,)`.\n        Returns:\n            xt with shape `(t_size, batch_size, *shape)`.\n        \"\"\"\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        if noise is None:\n            noise = torch.randn((t.shape[0], *x.shape), device=x.device)\n        x = x.reshape((-1, *x.shape))\n        xt = expand_dims(alpha_t, x.dim()) * x + expand_dims(sigma_t, x.dim()) * noise\n        if t.shape[0] == 1:\n            return xt.squeeze(0)\n        else:\n            return xt\n\n    def inverse(\n        self,\n        x,\n        steps=20,\n        t_start=None,\n        t_end=None,\n        order=2,\n        skip_type=\"time_uniform\",\n        method=\"multistep\",\n        lower_order_final=True,\n        denoise_to_zero=False,\n        solver_type=\"dpmsolver\",\n        atol=0.0078,\n        rtol=0.05,\n        return_intermediate=False,\n    ):\n        \"\"\"\n        Inverse the sample `x` from time `t_start` to `t_end` by DPM-Solver.\n        For discrete-time DPMs, we use `t_start=1/N`, where `N` is the total time steps during training.\n        \"\"\"\n        t_0 = 1.0 / self.noise_schedule.total_N if t_start is None else t_start\n        t_T = self.noise_schedule.T if t_end is None else t_end\n        assert (\n            t_0 > 0 and t_T > 0\n        ), \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\n        return self.sample(\n            x,\n            steps=steps,\n            t_start=t_0,\n            t_end=t_T,\n            order=order,\n            skip_type=skip_type,\n            method=method,\n            lower_order_final=lower_order_final,\n            denoise_to_zero=denoise_to_zero,\n            solver_type=solver_type,\n            atol=atol,\n            rtol=rtol,\n            return_intermediate=return_intermediate,\n        )\n\n    def sample(\n        self,\n        x,\n        steps=20,\n        t_start=None,\n        t_end=None,\n        order=2,\n        skip_type=\"time_uniform\",\n        method=\"multistep\",\n        lower_order_final=True,\n        denoise_to_zero=False,\n        solver_type=\"dpmsolver\",\n        atol=0.0078,\n        rtol=0.05,\n        return_intermediate=False,\n    ):\n        \"\"\"\n        Compute the sample at time `t_end` by DPM-Solver, given the initial `x` at time `t_start`.\n\n        =====================================================\n\n        We support the following algorithms for both noise prediction model and data prediction model:\n            - 'singlestep':\n                Singlestep DPM-Solver (i.e. \"DPM-Solver-fast\" in the paper), which combines different orders of singlestep DPM-Solver.\n                We combine all the singlestep solvers with order <= `order` to use up all the function evaluations (steps).\n                The total number of function evaluations (NFE) == `steps`.\n                Given a fixed NFE == `steps`, the sampling procedure is:\n                    - If `order` == 1:\n                        - Denote K = steps. We use K steps of DPM-Solver-1 (i.e. DDIM).\n                    - If `order` == 2:\n                        - Denote K = (steps // 2) + (steps % 2). We take K intermediate time steps for sampling.\n                        - If steps % 2 == 0, we use K steps of singlestep DPM-Solver-2.\n                        - If steps % 2 == 1, we use (K - 1) steps of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.\n                    - If `order` == 3:\n                        - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.\n                        - If steps % 3 == 0, we use (K - 2) steps of singlestep DPM-Solver-3, and 1 step of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.\n                        - If steps % 3 == 1, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of DPM-Solver-1.\n                        - If steps % 3 == 2, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of singlestep DPM-Solver-2.\n            - 'multistep':\n                Multistep DPM-Solver with the order of `order`. The total number of function evaluations (NFE) == `steps`.\n                We initialize the first `order` values by lower order multistep solvers.\n                Given a fixed NFE == `steps`, the sampling procedure is:\n                    Denote K = steps.\n                    - If `order` == 1:\n                        - We use K steps of DPM-Solver-1 (i.e. DDIM).\n                    - If `order` == 2:\n                        - We firstly use 1 step of DPM-Solver-1, then use (K - 1) step of multistep DPM-Solver-2.\n                    - If `order` == 3:\n                        - We firstly use 1 step of DPM-Solver-1, then 1 step of multistep DPM-Solver-2, then (K - 2) step of multistep DPM-Solver-3.\n            - 'singlestep_fixed':\n                Fixed order singlestep DPM-Solver (i.e. DPM-Solver-1 or singlestep DPM-Solver-2 or singlestep DPM-Solver-3).\n                We use singlestep DPM-Solver-`order` for `order`=1 or 2 or 3, with total [`steps` // `order`] * `order` NFE.\n            - 'adaptive':\n                Adaptive step size DPM-Solver (i.e. \"DPM-Solver-12\" and \"DPM-Solver-23\" in the paper).\n                We ignore `steps` and use adaptive step size DPM-Solver with a higher order of `order`.\n                You can adjust the absolute tolerance `atol` and the relative tolerance `rtol` to balance the computatation costs\n                (NFE) and the sample quality.\n                    - If `order` == 2, we use DPM-Solver-12 which combines DPM-Solver-1 and singlestep DPM-Solver-2.\n                    - If `order` == 3, we use DPM-Solver-23 which combines singlestep DPM-Solver-2 and singlestep DPM-Solver-3.\n\n        =====================================================\n\n        Some advices for choosing the algorithm:\n            - For **unconditional sampling** or **guided sampling with small guidance scale** by DPMs:\n                Use singlestep DPM-Solver or DPM-Solver++ (\"DPM-Solver-fast\" in the paper) with `order = 3`.\n                e.g., DPM-Solver:\n                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type=\"dpmsolver\")\n                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=3,\n                            skip_type='time_uniform', method='singlestep')\n                e.g., DPM-Solver++:\n                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type=\"dpmsolver++\")\n                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=3,\n                            skip_type='time_uniform', method='singlestep')\n            - For **guided sampling with large guidance scale** by DPMs:\n                Use multistep DPM-Solver with `algorithm_type=\"dpmsolver++\"` and `order = 2`.\n                e.g.\n                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type=\"dpmsolver++\")\n                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=2,\n                            skip_type='time_uniform', method='multistep')\n\n        We support three types of `skip_type`:\n            - 'logSNR': uniform logSNR for the time steps. **Recommended for low-resolutional images**\n            - 'time_uniform': uniform time for the time steps. **Recommended for high-resolutional images**.\n            - 'time_quadratic': quadratic time for the time steps.\n\n        =====================================================\n        Args:\n            x: A pytorch tensor. The initial value at time `t_start`\n                e.g. if `t_start` == T, then `x` is a sample from the standard normal distribution.\n            steps: A `int`. The total number of function evaluations (NFE).\n            t_start: A `float`. The starting time of the sampling.\n                If `T` is None, we use self.noise_schedule.T (default is 1.0).\n            t_end: A `float`. The ending time of the sampling.\n                If `t_end` is None, we use 1. / self.noise_schedule.total_N.\n                e.g. if total_N == 1000, we have `t_end` == 1e-3.\n                For discrete-time DPMs:\n                    - We recommend `t_end` == 1. / self.noise_schedule.total_N.\n                For continuous-time DPMs:\n                    - We recommend `t_end` == 1e-3 when `steps` <= 15; and `t_end` == 1e-4 when `steps` > 15.\n            order: A `int`. The order of DPM-Solver.\n            skip_type: A `str`. The type for the spacing of the time steps. 'time_uniform' or 'logSNR' or 'time_quadratic'.\n            method: A `str`. The method for sampling. 'singlestep' or 'multistep' or 'singlestep_fixed' or 'adaptive'.\n            denoise_to_zero: A `bool`. Whether to denoise to time 0 at the final step.\n                Default is `False`. If `denoise_to_zero` is `True`, the total NFE is (`steps` + 1).\n\n                This trick is firstly proposed by DDPM (https://arxiv.org/abs/2006.11239) and\n                score_sde (https://arxiv.org/abs/2011.13456). Such trick can improve the FID\n                for diffusion models sampling by diffusion SDEs for low-resolutional images\n                (such as CIFAR-10). However, we observed that such trick does not matter for\n                high-resolutional images. As it needs an additional NFE, we do not recommend\n                it for high-resolutional images.\n            lower_order_final: A `bool`. Whether to use lower order solvers at the final steps.\n                Only valid for `method=multistep` and `steps < 15`. We empirically find that\n                this trick is a key to stabilizing the sampling by DPM-Solver with very few steps\n                (especially for steps <= 10). So we recommend to set it to be `True`.\n            solver_type: A `str`. The taylor expansion type for the solver. `dpmsolver` or `taylor`. We recommend `dpmsolver`.\n            atol: A `float`. The absolute tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.\n            rtol: A `float`. The relative tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.\n            return_intermediate: A `bool`. Whether to save the xt at each step.\n                When set to `True`, method returns a tuple (x0, intermediates); when set to False, method returns only x0.\n        Returns:\n            x_end: A pytorch tensor. The approximated solution at time `t_end`.\n\n        \"\"\"\n        t_0 = 1.0 / self.noise_schedule.total_N if t_end is None else t_end\n        t_T = self.noise_schedule.T if t_start is None else t_start\n        assert (\n            t_0 > 0 and t_T > 0\n        ), \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\n        if return_intermediate:\n            assert method in [\n                \"multistep\",\n                \"singlestep\",\n                \"singlestep_fixed\",\n            ], \"Cannot use adaptive solver when saving intermediate values\"\n        if self.correcting_xt_fn is not None:\n            assert method in [\n                \"multistep\",\n                \"singlestep\",\n                \"singlestep_fixed\",\n            ], \"Cannot use adaptive solver when correcting_xt_fn is not None\"\n        device = x.device\n        intermediates = []\n        with torch.no_grad():\n            if method == \"adaptive\":\n                x = self.dpm_solver_adaptive(\n                    x,\n                    order=order,\n                    t_T=t_T,\n                    t_0=t_0,\n                    atol=atol,\n                    rtol=rtol,\n                    solver_type=solver_type,\n                )\n            elif method == \"multistep\":\n                assert steps >= order\n                timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n                assert timesteps.shape[0] - 1 == steps\n                # Init the initial values.\n                step = 0\n                t = timesteps[step]\n                t_prev_list = [t]\n                model_prev_list = [self.model_fn(x, t)]\n                if self.correcting_xt_fn is not None:\n                    x = self.correcting_xt_fn(x, t, step)\n                if return_intermediate:\n                    intermediates.append(x)\n                # Init the first `order` values by lower order multistep DPM-Solver.\n                for step in range(1, order):\n                    t = timesteps[step]\n                    x = self.multistep_dpm_solver_update(\n                        x,\n                        model_prev_list,\n                        t_prev_list,\n                        t,\n                        step,\n                        solver_type=solver_type,\n                    )\n                    if self.correcting_xt_fn is not None:\n                        x = self.correcting_xt_fn(x, t, step)\n                    if return_intermediate:\n                        intermediates.append(x)\n                    t_prev_list.append(t)\n                    model_prev_list.append(self.model_fn(x, t))\n                # Compute the remaining values by `order`-th order multistep DPM-Solver.\n                for step in range(order, steps + 1):\n                    t = timesteps[step]\n                    # We only use lower order for steps < 10\n                    if lower_order_final and steps < 10:\n                        step_order = min(order, steps + 1 - step)\n                    else:\n                        step_order = order\n                    x = self.multistep_dpm_solver_update(\n                        x,\n                        model_prev_list,\n                        t_prev_list,\n                        t,\n                        step_order,\n                        solver_type=solver_type,\n                    )\n                    if self.correcting_xt_fn is not None:\n                        x = self.correcting_xt_fn(x, t, step)\n                    if return_intermediate:\n                        intermediates.append(x)\n                    for i in range(order - 1):\n                        t_prev_list[i] = t_prev_list[i + 1]\n                        model_prev_list[i] = model_prev_list[i + 1]\n                    t_prev_list[-1] = t\n                    # We do not need to evaluate the final model value.\n                    if step < steps:\n                        model_prev_list[-1] = self.model_fn(x, t)\n            elif method in [\"singlestep\", \"singlestep_fixed\"]:\n                if method == \"singlestep\":\n                    (\n                        timesteps_outer,\n                        orders,\n                    ) = self.get_orders_and_timesteps_for_singlestep_solver(\n                        steps=steps,\n                        order=order,\n                        skip_type=skip_type,\n                        t_T=t_T,\n                        t_0=t_0,\n                        device=device,\n                    )\n                elif method == \"singlestep_fixed\":\n                    K = steps // order\n                    orders = [\n                        order,\n                    ] * K\n                    timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)\n                for step, order in enumerate(orders):\n                    s, t = timesteps_outer[step], timesteps_outer[step + 1]\n                    timesteps_inner = self.get_time_steps(\n                        skip_type=skip_type,\n                        t_T=s.item(),\n                        t_0=t.item(),\n                        N=order,\n                        device=device,\n                    )\n                    lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\n                    h = lambda_inner[-1] - lambda_inner[0]\n                    r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\n                    r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\n                    x = self.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)\n                    if self.correcting_xt_fn is not None:\n                        x = self.correcting_xt_fn(x, t, step)\n                    if return_intermediate:\n                        intermediates.append(x)\n            else:\n                raise ValueError(\"Got wrong method {}\".format(method))\n            if denoise_to_zero:\n                t = torch.ones((1,)).to(device) * t_0\n                x = self.denoise_to_zero_fn(x, t)\n                if self.correcting_xt_fn is not None:\n                    x = self.correcting_xt_fn(x, t, step + 1)\n                if return_intermediate:\n                    intermediates.append(x)\n        if return_intermediate:\n            return x, intermediates\n        else:\n            return x\n\n\n#############################################################\n# other utility functions\n#############################################################\n\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K),\n            torch.tensor(K - 2, device=x.device),\n            cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K),\n            torch.tensor(K - 2, device=x.device),\n            cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand\n\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,) * (dims - 1)]\n", "TTS/tts/layers/tortoise/autoregressive.py": "# AGPL: a notification must be added stating that changes have been made to that file.\nimport functools\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Config, GPT2PreTrainedModel, LogitsProcessorList\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n\nfrom TTS.tts.layers.tortoise.arch_utils import AttentionBlock, TypicalLogitsWarper\n\n\ndef null_position_embeddings(range, dim):\n    return torch.zeros((range.shape[0], range.shape[1], dim), device=range.device)\n\n\ndef _p(t):\n    return t and (len(t), len(t[0]), t[0][0].shape)  # kv_cache debug\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Basic residual convolutional block that uses GroupNorm.\n    \"\"\"\n\n    def __init__(self, chan):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(chan, chan, kernel_size=3, padding=1),\n            nn.GroupNorm(chan // 8, chan),\n            nn.ReLU(),\n            nn.Conv1d(chan, chan, kernel_size=3, padding=1),\n            nn.GroupNorm(chan // 8, chan),\n        )\n\n    def forward(self, x):\n        return F.relu(self.net(x) + x)\n\n\nclass GPT2InferenceModel(GPT2PreTrainedModel):\n    def __init__(self, config, gpt, text_pos_emb, embeddings, norm, linear, kv_cache):\n        super().__init__(config)\n        self.transformer = gpt\n        self.text_pos_embedding = text_pos_emb\n        self.embeddings = embeddings\n        self.lm_head = nn.Sequential(norm, linear)\n        self.kv_cache = kv_cache\n\n    def store_mel_emb(self, mel_emb):\n        self.cached_mel_emb = mel_emb\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):\n        token_type_ids = kwargs.get(\"token_type_ids\", None)  # usually None\n        if not self.kv_cache:\n            past_key_values = None\n        # only last token for inputs_ids if past is defined in kwargs\n        if past_key_values:\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n            if token_type_ids is not None:\n                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n\n        attention_mask = kwargs.get(\"attention_mask\", None)\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n        else:\n            position_ids = None\n        return {\n            \"input_ids\": input_ids,\n            \"past_key_values\": past_key_values,\n            \"use_cache\": kwargs.get(\"use_cache\"),\n            \"position_ids\": position_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n\n    def forward(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        assert self.cached_mel_emb is not None\n        assert inputs_embeds is None  # Not supported by this inference model.\n        assert labels is None  # Training not supported by this inference model.\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # Create embedding\n        mel_len = self.cached_mel_emb.shape[1]\n        if input_ids.shape[1] != 1:\n            text_inputs = input_ids[:, mel_len:]\n            text_emb = self.embeddings(text_inputs)\n            text_emb = text_emb + self.text_pos_embedding(text_emb)\n            if self.cached_mel_emb.shape[0] != text_emb.shape[0]:\n                mel_emb = self.cached_mel_emb.repeat_interleave(text_emb.shape[0] // self.cached_mel_emb.shape[0], 0)\n            else:  # this outcome only occurs once per loop in most cases\n                mel_emb = self.cached_mel_emb\n            emb = torch.cat([mel_emb, text_emb], dim=1)\n        else:\n            emb = self.embeddings(input_ids)\n            emb = emb + self.text_pos_embedding.get_fixed_embedding(\n                attention_mask.shape[1] - mel_len, attention_mask.device\n            )\n\n        transformer_outputs = self.transformer(\n            inputs_embeds=emb,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + transformer_outputs[1:]\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=None,\n            logits=lm_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n            cross_attentions=transformer_outputs.cross_attentions,\n        )\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        \"\"\"\n        This function is used to re-order the :obj:`past_key_values` cache if\n        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n        \"\"\"\n        return tuple(\n            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n            for layer_past in past\n        )\n\n\nclass ConditioningEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        attn_blocks=6,\n        num_attn_heads=4,\n        do_checkpointing=False,\n        mean=False,\n    ):\n        super().__init__()\n        attn = []\n        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)\n        for a in range(attn_blocks):\n            attn.append(AttentionBlock(embedding_dim, num_attn_heads))\n        self.attn = nn.Sequential(*attn)\n        self.dim = embedding_dim\n        self.do_checkpointing = do_checkpointing\n        self.mean = mean\n\n    def forward(self, x):\n        h = self.init(x)\n        h = self.attn(h)\n        if self.mean:\n            return h.mean(dim=2)\n        else:\n            return h[:, :, 0]\n\n\nclass LearnedPositionEmbeddings(nn.Module):\n    def __init__(self, seq_len, model_dim, init=0.02):\n        super().__init__()\n        self.emb = nn.Embedding(seq_len, model_dim)\n        # Initializing this way is standard for GPT-2\n        self.emb.weight.data.normal_(mean=0.0, std=init)\n\n    def forward(self, x):\n        sl = x.shape[1]\n        return self.emb(torch.arange(0, sl, device=x.device))\n\n    def get_fixed_embedding(self, ind, dev):\n        return self.emb(torch.arange(0, ind, device=dev))[ind - 1 : ind]\n\n\ndef build_hf_gpt_transformer(layers, model_dim, heads, max_mel_seq_len, max_text_seq_len, checkpointing):\n    \"\"\"\n    GPT-2 implemented by the HuggingFace library.\n    \"\"\"\n    from transformers import GPT2Config, GPT2Model\n\n    gpt_config = GPT2Config(\n        vocab_size=256,  # Unused.\n        n_positions=max_mel_seq_len + max_text_seq_len,\n        n_ctx=max_mel_seq_len + max_text_seq_len,\n        n_embd=model_dim,\n        n_layer=layers,\n        n_head=heads,\n        gradient_checkpointing=checkpointing,\n        use_cache=not checkpointing,\n    )\n    gpt = GPT2Model(gpt_config)\n    # Override the built in positional embeddings\n    del gpt.wpe  # TODO: figure out relevance in fixing exported model definition: Embedding(1012, 1024)\n    gpt.wpe = functools.partial(null_position_embeddings, dim=model_dim)\n    # Built-in token embeddings are unused.\n    del gpt.wte\n    return (\n        gpt,\n        LearnedPositionEmbeddings(max_mel_seq_len, model_dim),\n        LearnedPositionEmbeddings(max_text_seq_len, model_dim),\n        None,\n        None,\n    )\n\n\nclass MelEncoder(nn.Module):\n    def __init__(self, channels, mel_channels=80, resblocks_per_reduction=2):\n        super().__init__()\n        self.channels = channels\n        self.encoder = nn.Sequential(\n            nn.Conv1d(mel_channels, channels // 4, kernel_size=3, padding=1),\n            nn.Sequential(*[ResBlock(channels // 4) for _ in range(resblocks_per_reduction)]),\n            nn.Conv1d(channels // 4, channels // 2, kernel_size=3, stride=2, padding=1),\n            nn.GroupNorm(channels // 16, channels // 2),\n            nn.ReLU(),\n            nn.Sequential(*[ResBlock(channels // 2) for _ in range(resblocks_per_reduction)]),\n            nn.Conv1d(channels // 2, channels, kernel_size=3, stride=2, padding=1),\n            nn.GroupNorm(channels // 8, channels),\n            nn.ReLU(),\n            nn.Sequential(*[ResBlock(channels) for _ in range(resblocks_per_reduction)]),\n        )\n        self.reduction = 4\n\n    def forward(self, x):\n        for e in self.encoder:\n            x = e(x)\n        return x.permute(0, 2, 1)\n\n\nclass UnifiedVoice(nn.Module):\n    def __init__(\n        self,\n        layers=8,\n        model_dim=512,\n        heads=8,\n        max_text_tokens=120,\n        max_mel_tokens=250,\n        max_conditioning_inputs=1,\n        mel_length_compression=1024,\n        number_text_tokens=256,\n        start_text_token=None,\n        number_mel_codes=8194,\n        start_mel_token=8192,\n        stop_mel_token=8193,\n        train_solo_embeddings=False,\n        use_mel_codes_as_input=True,\n        checkpointing=True,\n        types=1,\n    ):\n        \"\"\"\n        Args:\n            layers: Number of layers in transformer stack.\n            model_dim: Operating dimensions of the transformer\n            heads: Number of transformer heads. Must be divisible by model_dim. Recommend model_dim//64\n            max_text_tokens: Maximum number of text tokens that will be encountered by model.\n            max_mel_tokens: Maximum number of MEL tokens that will be encountered by model.\n            max_conditioning_inputs: Maximum number of conditioning inputs provided to the model. If (1), conditioning input can be of format (b,80,s), otherwise (b,n,80,s).\n            mel_length_compression: The factor between <number_input_samples> and <mel_tokens>. Used to compute MEL code padding given wav input length.\n            number_text_tokens:\n            start_text_token:\n            stop_text_token:\n            number_mel_codes:\n            start_mel_token:\n            stop_mel_token:\n            train_solo_embeddings:\n            use_mel_codes_as_input:\n            checkpointing:\n        \"\"\"\n        super().__init__()\n\n        self.number_text_tokens = number_text_tokens\n        self.start_text_token = number_text_tokens * types if start_text_token is None else start_text_token\n        self.stop_text_token = 0\n        self.number_mel_codes = number_mel_codes\n        self.start_mel_token = start_mel_token\n        self.stop_mel_token = stop_mel_token\n        self.layers = layers\n        self.heads = heads\n        self.max_mel_tokens = max_mel_tokens\n        self.max_text_tokens = max_text_tokens\n        self.model_dim = model_dim\n        self.max_conditioning_inputs = max_conditioning_inputs\n        self.mel_length_compression = mel_length_compression\n        self.conditioning_encoder = ConditioningEncoder(80, model_dim, num_attn_heads=heads)\n        self.text_embedding = nn.Embedding(self.number_text_tokens * types + 1, model_dim)\n        if use_mel_codes_as_input:\n            self.mel_embedding = nn.Embedding(self.number_mel_codes, model_dim)\n        else:\n            self.mel_embedding = MelEncoder(model_dim, resblocks_per_reduction=1)\n        (\n            self.gpt,\n            self.mel_pos_embedding,\n            self.text_pos_embedding,\n            self.mel_layer_pos_embedding,\n            self.text_layer_pos_embedding,\n        ) = build_hf_gpt_transformer(\n            layers,\n            model_dim,\n            heads,\n            self.max_mel_tokens + 2 + self.max_conditioning_inputs,\n            self.max_text_tokens + 2,\n            checkpointing,\n        )\n        if train_solo_embeddings:\n            self.mel_solo_embedding = nn.Parameter(torch.randn(1, 1, model_dim) * 0.02, requires_grad=True)\n            self.text_solo_embedding = nn.Parameter(torch.randn(1, 1, model_dim) * 0.02, requires_grad=True)\n        else:\n            self.mel_solo_embedding = 0\n            self.text_solo_embedding = 0\n\n        self.final_norm = nn.LayerNorm(model_dim)\n        self.text_head = nn.Linear(model_dim, self.number_text_tokens * types + 1)\n        self.mel_head = nn.Linear(model_dim, self.number_mel_codes)\n\n        # Initialize the embeddings per the GPT-2 scheme\n        embeddings = [self.text_embedding]\n        if use_mel_codes_as_input:\n            embeddings.append(self.mel_embedding)\n        for module in embeddings:\n            module.weight.data.normal_(mean=0.0, std=0.02)\n\n    def post_init_gpt2_config(self, kv_cache=True):\n        seq_length = self.max_mel_tokens + self.max_text_tokens + 2\n        gpt_config = GPT2Config(\n            vocab_size=self.max_mel_tokens,\n            n_positions=seq_length,\n            n_ctx=seq_length,\n            n_embd=self.model_dim,\n            n_layer=self.layers,\n            n_head=self.heads,\n            gradient_checkpointing=False,\n            use_cache=True,\n        )\n        self.inference_model = GPT2InferenceModel(\n            gpt_config,\n            self.gpt,\n            self.mel_pos_embedding,\n            self.mel_embedding,\n            self.final_norm,\n            self.mel_head,\n            kv_cache=kv_cache,\n        )\n        # self.inference_model = PrunedGPT2InferenceModel(gpt_config, self.gpt, self.mel_pos_embedding, self.mel_embedding, self.final_norm, self.mel_head)\n        self.gpt.wte = self.mel_embedding\n        # self.inference_model.save_pretrained(\"\")\n\n    def build_aligned_inputs_and_targets(self, input, start_token, stop_token):\n        inp = F.pad(input, (1, 0), value=start_token)\n        tar = F.pad(input, (0, 1), value=stop_token)\n        return inp, tar\n\n    def set_mel_padding(self, mel_input_tokens, wav_lengths):\n        \"\"\"\n        Given mel tokens that are derived from a padded audio clip and the actual lengths of each batch element in\n        that audio clip, reformats the tokens with STOP_MEL_TOKEN in place of the zero padding. This is required\n        preformatting to create a working TTS model.\n        \"\"\"\n        # Set padding areas within MEL (currently it is coded with the MEL code for <zero>).\n        mel_lengths = torch.div(wav_lengths, self.mel_length_compression, rounding_mode=\"trunc\")\n        for b in range(len(mel_lengths)):\n            actual_end = (\n                mel_lengths[b] + 1\n            )  # Due to the convolutional nature of how these tokens are generated, it would be best if the model predicts a token past the actual last token.\n            if actual_end < mel_input_tokens.shape[-1]:\n                mel_input_tokens[b, actual_end:] = self.stop_mel_token\n        return mel_input_tokens\n\n    def get_logits(\n        self,\n        speech_conditioning_inputs,\n        first_inputs,\n        first_head,\n        second_inputs=None,\n        second_head=None,\n        get_attns=False,\n        return_latent=False,\n    ):\n        if second_inputs is not None:\n            emb = torch.cat([speech_conditioning_inputs, first_inputs, second_inputs], dim=1)\n        else:\n            emb = torch.cat([speech_conditioning_inputs, first_inputs], dim=1)\n\n        gpt_out = self.gpt(inputs_embeds=emb, return_dict=True, output_attentions=get_attns)\n        if get_attns:\n            return gpt_out.attentions\n\n        enc = gpt_out.last_hidden_state[:, 1:]  # The first logit is tied to the speech_conditioning_input\n        enc = self.final_norm(enc)\n\n        if return_latent:\n            return (\n                enc[\n                    :,\n                    speech_conditioning_inputs.shape[1] : speech_conditioning_inputs.shape[1] + first_inputs.shape[1],\n                ],\n                enc[:, -second_inputs.shape[1] :],\n            )\n\n        first_logits = enc[:, : first_inputs.shape[1]]\n        first_logits = first_head(first_logits)\n        first_logits = first_logits.permute(0, 2, 1)\n        if second_inputs is not None:\n            second_logits = enc[:, -second_inputs.shape[1] :]\n            second_logits = second_head(second_logits)\n            second_logits = second_logits.permute(0, 2, 1)\n            return first_logits, second_logits\n        else:\n            return first_logits\n\n    def get_conditioning(self, speech_conditioning_input):\n        speech_conditioning_input = (\n            speech_conditioning_input.unsqueeze(1)\n            if len(speech_conditioning_input.shape) == 3\n            else speech_conditioning_input\n        )\n        conds = []\n        for j in range(speech_conditioning_input.shape[1]):\n            conds.append(self.conditioning_encoder(speech_conditioning_input[:, j]))\n        conds = torch.stack(conds, dim=1)\n        conds = conds.mean(dim=1)\n        return conds\n\n    def forward(\n        self,\n        speech_conditioning_latent,\n        text_inputs,\n        text_lengths,\n        mel_codes,\n        wav_lengths,\n        types=None,\n        text_first=True,\n        raw_mels=None,\n        return_attentions=False,\n        return_latent=False,\n        clip_inputs=True,\n    ):\n        \"\"\"\n        Forward pass that uses both text and voice in either text conditioning mode or voice conditioning mode\n        (actuated by `text_first`).\n\n        speech_conditioning_input: MEL float tensor, (b,1024)\n        text_inputs: long tensor, (b,t)\n        text_lengths: long tensor, (b,)\n        mel_inputs:  long tensor, (b,m)\n        wav_lengths: long tensor, (b,)\n        raw_mels: MEL float tensor (b,80,s)\n\n        If return_attentions is specified, only logits are returned.\n        If return_latent is specified, loss & logits are not computed or returned. Only the predicted latents are returned.\n        If clip_inputs is True, the inputs will be clipped to the smallest input size across each input modality.\n        \"\"\"\n        # Types are expressed by expanding the text embedding space.\n        if types is not None:\n            text_inputs = text_inputs * (1 + types).unsqueeze(-1)\n\n        if clip_inputs:\n            # This model will receive micro-batches with a ton of padding for both the text and MELs. Ameliorate this by\n            # chopping the inputs by the maximum actual length.\n            max_text_len = text_lengths.max()\n            text_inputs = text_inputs[:, :max_text_len]\n            max_mel_len = wav_lengths.max() // self.mel_length_compression\n            mel_codes = mel_codes[:, :max_mel_len]\n            if raw_mels is not None:\n                raw_mels = raw_mels[:, :, : max_mel_len * 4]\n        mel_codes = self.set_mel_padding(mel_codes, wav_lengths)\n        text_inputs = F.pad(text_inputs, (0, 1), value=self.stop_text_token)\n        mel_codes = F.pad(mel_codes, (0, 1), value=self.stop_mel_token)\n\n        conds = speech_conditioning_latent.unsqueeze(1)\n        text_inputs, text_targets = self.build_aligned_inputs_and_targets(\n            text_inputs, self.start_text_token, self.stop_text_token\n        )\n        text_emb = self.text_embedding(text_inputs) + self.text_pos_embedding(text_inputs)\n        mel_codes, mel_targets = self.build_aligned_inputs_and_targets(\n            mel_codes, self.start_mel_token, self.stop_mel_token\n        )\n        if raw_mels is not None:\n            mel_inp = F.pad(raw_mels, (0, 8))\n        else:\n            mel_inp = mel_codes\n        mel_emb = self.mel_embedding(mel_inp)\n        mel_emb = mel_emb + self.mel_pos_embedding(mel_codes)\n\n        if text_first:\n            text_logits, mel_logits = self.get_logits(\n                conds,\n                text_emb,\n                self.text_head,\n                mel_emb,\n                self.mel_head,\n                get_attns=return_attentions,\n                return_latent=return_latent,\n            )\n            if return_latent:\n                return mel_logits[\n                    :, :-2\n                ]  # Despite the name, these are not logits. Strip off the two tokens added by this forward pass.\n        else:\n            mel_logits, text_logits = self.get_logits(\n                conds,\n                mel_emb,\n                self.mel_head,\n                text_emb,\n                self.text_head,\n                get_attns=return_attentions,\n                return_latent=return_latent,\n            )\n            if return_latent:\n                return text_logits[\n                    :, :-2\n                ]  # Despite the name, these are not logits. Strip off the two tokens added by this forward pass.\n\n        if return_attentions:\n            return mel_logits\n        loss_text = F.cross_entropy(text_logits, text_targets.long())\n        loss_mel = F.cross_entropy(mel_logits, mel_targets.long())\n        return loss_text.mean(), loss_mel.mean(), mel_logits\n\n    def inference_speech(\n        self,\n        speech_conditioning_latent,\n        text_inputs,\n        input_tokens=None,\n        num_return_sequences=1,\n        max_generate_length=None,\n        typical_sampling=False,\n        typical_mass=0.9,\n        **hf_generate_kwargs,\n    ):\n        text_inputs = F.pad(text_inputs, (0, 1), value=self.stop_text_token)\n        text_inputs, text_targets = self.build_aligned_inputs_and_targets(\n            text_inputs, self.start_text_token, self.stop_text_token\n        )\n        text_emb = self.text_embedding(text_inputs) + self.text_pos_embedding(text_inputs)\n\n        conds = speech_conditioning_latent.unsqueeze(1)\n        emb = torch.cat([conds, text_emb], dim=1)\n        self.inference_model.store_mel_emb(emb)\n\n        fake_inputs = torch.full(\n            (\n                emb.shape[0],\n                conds.shape[1] + emb.shape[1],\n            ),\n            fill_value=1,\n            dtype=torch.long,\n            device=text_inputs.device,\n        )\n        fake_inputs[:, -1] = self.start_mel_token\n        trunc_index = fake_inputs.shape[1]\n        if input_tokens is None:\n            inputs = fake_inputs\n        else:\n            assert (\n                num_return_sequences % input_tokens.shape[0] == 0\n            ), \"The number of return sequences must be divisible by the number of input sequences\"\n            fake_inputs = fake_inputs.repeat(num_return_sequences, 1)\n            input_tokens = input_tokens.repeat(num_return_sequences // input_tokens.shape[0], 1)\n            inputs = torch.cat([fake_inputs, input_tokens], dim=1)\n\n        logits_processor = (\n            LogitsProcessorList([TypicalLogitsWarper(mass=typical_mass)]) if typical_sampling else LogitsProcessorList()\n        )  # TODO disable this\n        max_length = (\n            trunc_index + self.max_mel_tokens - 1 if max_generate_length is None else trunc_index + max_generate_length\n        )\n        gen = self.inference_model.generate(\n            inputs,\n            bos_token_id=self.start_mel_token,\n            pad_token_id=self.stop_mel_token,\n            eos_token_id=self.stop_mel_token,\n            max_length=max_length,\n            logits_processor=logits_processor,\n            num_return_sequences=num_return_sequences,\n            **hf_generate_kwargs,\n        )\n        return gen[:, trunc_index:]\n\n\nif __name__ == \"__main__\":\n    gpt = UnifiedVoice(\n        model_dim=256,\n        heads=4,\n        train_solo_embeddings=True,\n        use_mel_codes_as_input=True,\n        max_conditioning_inputs=4,\n    )\n    l = gpt(\n        torch.randn(2, 3, 80, 800),\n        torch.randint(high=120, size=(2, 120)),\n        torch.tensor([32, 120]),\n        torch.randint(high=8192, size=(2, 250)),\n        torch.tensor([250 * 256, 195 * 256]),\n    )\n    gpt.text_forward(\n        torch.randn(2, 80, 800),\n        torch.randint(high=50, size=(2, 80)),\n        torch.tensor([32, 80]),\n    )\n", "TTS/tts/layers/tortoise/transformer.py": "import torch\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom torch import nn\n\n# helpers\n\n\ndef exists(val):\n    return val is not None\n\n\ndef default(val, d):\n    return val if exists(val) else d\n\n\ndef cast_tuple(val, depth=1):\n    if isinstance(val, list):\n        val = tuple(val)\n    return val if isinstance(val, tuple) else (val,) * depth\n\n\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\n\n\ndef stable_softmax(t, dim=-1, alpha=32**2):\n    t = t / alpha\n    t = t - torch.amax(t, dim=dim, keepdim=True).detach()\n    return (t * alpha).softmax(dim=dim)\n\n\ndef route_args(router, args, depth):\n    routed_args = [(dict(), dict()) for _ in range(depth)]\n    matched_keys = [key for key in args.keys() if key in router]\n\n    for key in matched_keys:\n        val = args[key]\n        for depth, ((f_args, g_args), routes) in enumerate(zip(routed_args, router[key])):\n            new_f_args, new_g_args = map(lambda route: ({key: val} if route else {}), routes)\n            routed_args[depth] = ({**f_args, **new_f_args}, {**g_args, **new_g_args})\n    return routed_args\n\n\n# classes\nclass SequentialSequence(nn.Module):\n    def __init__(self, layers, args_route={}, layer_dropout=0.0):\n        super().__init__()\n        assert all(\n            len(route) == len(layers) for route in args_route.values()\n        ), \"each argument route map must have the same depth as the number of sequential layers\"\n        self.layers = layers\n        self.args_route = args_route\n        self.layer_dropout = layer_dropout\n\n    def forward(self, x, **kwargs):\n        args = route_args(self.args_route, kwargs, len(self.layers))\n        layers_and_args = list(zip(self.layers, args))\n\n        for (f, g), (f_args, g_args) in layers_and_args:\n            x = x + f(x, **f_args)\n            x = x + g(x, **g_args)\n        return x\n\n\nclass DivideMax(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        maxes = x.amax(dim=self.dim, keepdim=True).detach()\n        return x / maxes\n\n\n# https://arxiv.org/abs/2103.17239\nclass LayerScale(nn.Module):\n    def __init__(self, dim, depth, fn):\n        super().__init__()\n        if depth <= 18:\n            init_eps = 0.1\n        elif depth > 18 and depth <= 24:\n            init_eps = 1e-5\n        else:\n            init_eps = 1e-6\n\n        scale = torch.zeros(1, 1, dim).fill_(init_eps)\n        self.scale = nn.Parameter(scale)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) * self.scale\n\n\n# layer norm\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn, sandwich=False):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.norm_out = nn.LayerNorm(dim) if sandwich else nn.Identity()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        x = self.fn(x, **kwargs)\n        return self.norm_out(x)\n\n\n# feed forward\n\n\nclass GEGLU(nn.Module):\n    def forward(self, x):\n        x, gates = x.chunk(2, dim=-1)\n        return x * F.gelu(gates)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dropout=0.0, mult=4.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult * 2),\n            GEGLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * mult, dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Attention\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, seq_len, causal=True, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.seq_len = seq_len\n        self.scale = dim_head**-0.5\n\n        self.causal = causal\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))\n\n    def forward(self, x, mask=None):\n        b, n, _, h, device = *x.shape, self.heads, x.device\n        softmax = torch.softmax\n\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n\n        q = q * self.scale\n\n        dots = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k)\n        mask_value = max_neg_value(dots)\n\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b () () j\")\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        if self.causal:\n            i, j = dots.shape[-2:]\n            mask = torch.ones(i, j, device=device).triu_(j - i + 1).bool()\n            dots.masked_fill_(mask, mask_value)\n\n        attn = softmax(dots, dim=-1)\n\n        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        out = self.to_out(out)\n        return out\n\n\n# main transformer class\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        seq_len,\n        causal=True,\n        heads=8,\n        dim_head=64,\n        ff_mult=4,\n        attn_dropout=0.0,\n        ff_dropout=0.0,\n        sparse_attn=False,\n        sandwich_norm=False,\n    ):\n        super().__init__()\n        layers = nn.ModuleList([])\n        sparse_layer = cast_tuple(sparse_attn, depth)\n\n        for ind, sparse_attn in zip(range(depth), sparse_layer):\n            attn = Attention(\n                dim,\n                causal=causal,\n                seq_len=seq_len,\n                heads=heads,\n                dim_head=dim_head,\n                dropout=attn_dropout,\n            )\n\n            ff = FeedForward(dim, mult=ff_mult, dropout=ff_dropout)\n\n            layers.append(\n                nn.ModuleList(\n                    [\n                        LayerScale(dim, ind + 1, PreNorm(dim, attn, sandwich=sandwich_norm)),\n                        LayerScale(dim, ind + 1, PreNorm(dim, ff, sandwich=sandwich_norm)),\n                    ]\n                )\n            )\n\n        execute_type = SequentialSequence\n        route_attn = ((True, False),) * depth\n        attn_route_map = {\"mask\": route_attn}\n\n        self.layers = execute_type(layers, args_route=attn_route_map)\n\n    def forward(self, x, **kwargs):\n        return self.layers(x, **kwargs)\n", "TTS/tts/layers/tortoise/tokenizer.py": "import os\n\nimport torch\nfrom tokenizers import Tokenizer\n\nfrom TTS.tts.utils.text.cleaners import english_cleaners\n\nDEFAULT_VOCAB_FILE = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), \"../../utils/assets/tortoise/tokenizer.json\"\n)\n\n\nclass VoiceBpeTokenizer:\n    def __init__(self, vocab_file=DEFAULT_VOCAB_FILE, vocab_str=None):\n        self.tokenizer = None\n        if vocab_file is not None:\n            self.tokenizer = Tokenizer.from_file(vocab_file)\n        if vocab_str is not None:\n            self.tokenizer = Tokenizer.from_str(vocab_str)\n\n    def preprocess_text(self, txt):\n        txt = english_cleaners(txt)\n        return txt\n\n    def encode(self, txt):\n        txt = self.preprocess_text(txt)\n        txt = txt.replace(\" \", \"[SPACE]\")\n        return self.tokenizer.encode(txt).ids\n\n    def decode(self, seq):\n        if isinstance(seq, torch.Tensor):\n            seq = seq.cpu().numpy()\n        txt = self.tokenizer.decode(seq, skip_special_tokens=False).replace(\" \", \"\")\n        txt = txt.replace(\"[SPACE]\", \" \")\n        txt = txt.replace(\"[STOP]\", \"\")\n        txt = txt.replace(\"[UNK]\", \"\")\n        return txt\n", "TTS/tts/layers/tortoise/random_latent_generator.py": "import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2**0.5):\n    if bias is not None:\n        rest_dim = [1] * (input.ndim - bias.ndim - 1)\n        return (\n            F.leaky_relu(\n                input + bias.view(1, bias.shape[0], *rest_dim),\n                negative_slope=negative_slope,\n            )\n            * scale\n        )\n    else:\n        return F.leaky_relu(input, negative_slope=0.2) * scale\n\n\nclass EqualLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n        else:\n            self.bias = None\n        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n        self.lr_mul = lr_mul\n\n    def forward(self, input):\n        out = F.linear(input, self.weight * self.scale)\n        out = fused_leaky_relu(out, self.bias * self.lr_mul)\n        return out\n\n\nclass RandomLatentConverter(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.layers = nn.Sequential(\n            *[EqualLinear(channels, channels, lr_mul=0.1) for _ in range(5)], nn.Linear(channels, channels)\n        )\n        self.channels = channels\n\n    def forward(self, ref):\n        r = torch.randn(ref.shape[0], self.channels, device=ref.device)\n        y = self.layers(r)\n        return y\n\n\nif __name__ == \"__main__\":\n    model = RandomLatentConverter(512)\n    model(torch.randn(5, 512))\n", "TTS/tts/layers/tortoise/vocoder.py": "from dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Callable, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils.parametrize as parametrize\n\nMAX_WAV_VALUE = 32768.0\n\n\nclass KernelPredictor(torch.nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\"\"\"\n\n    def __init__(\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n        kpnet_nonlinear_activation=\"LeakyReLU\",\n        kpnet_nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        \"\"\"\n        Args:\n            cond_channels (int): number of channel for the conditioning sequence,\n            conv_in_channels (int): number of channel for the input sequence,\n            conv_out_channels (int): number of channel for the output sequence,\n            conv_layers (int): number of layers\n        \"\"\"\n        super().__init__()\n\n        self.conv_in_channels = conv_in_channels\n        self.conv_out_channels = conv_out_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_layers = conv_layers\n\n        kpnet_kernel_channels = conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers  # l_w\n        kpnet_bias_channels = conv_out_channels * conv_layers  # l_b\n\n        self.input_conv = nn.Sequential(\n            nn.utils.parametrizations.weight_norm(\n                nn.Conv1d(cond_channels, kpnet_hidden_channels, 5, padding=2, bias=True)\n            ),\n            getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.residual_convs = nn.ModuleList()\n        padding = (kpnet_conv_size - 1) // 2\n        for _ in range(3):\n            self.residual_convs.append(\n                nn.Sequential(\n                    nn.Dropout(kpnet_dropout),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        )\n                    ),\n                    getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        )\n                    ),\n                    getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n                )\n            )\n        self.kernel_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_kernel_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            )\n        )\n        self.bias_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_bias_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            )\n        )\n\n    def forward(self, c):\n        \"\"\"\n        Args:\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n        \"\"\"\n        batch, _, cond_length = c.shape\n        c = self.input_conv(c)\n        for residual_conv in self.residual_convs:\n            residual_conv.to(c.device)\n            c = c + residual_conv(c)\n        k = self.kernel_conv(c)\n        b = self.bias_conv(c)\n        kernels = k.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_in_channels,\n            self.conv_out_channels,\n            self.conv_kernel_size,\n            cond_length,\n        )\n        bias = b.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_out_channels,\n            cond_length,\n        )\n\n        return kernels, bias\n\n    def remove_weight_norm(self):\n        parametrize.remove_parametrizations(self.input_conv[0], \"weight\")\n        parametrize.remove_parametrizations(self.kernel_conv, \"weight\")\n        parametrize.remove_parametrizations(self.bias_conv)\n        for block in self.residual_convs:\n            parametrize.remove_parametrizations(block[1], \"weight\")\n            parametrize.remove_parametrizations(block[3], \"weight\")\n\n\nclass LVCBlock(torch.nn.Module):\n    \"\"\"the location-variable convolutions\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        cond_channels,\n        stride,\n        dilations=[1, 3, 9, 27],\n        lReLU_slope=0.2,\n        conv_kernel_size=3,\n        cond_hop_length=256,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n    ):\n        super().__init__()\n\n        self.cond_hop_length = cond_hop_length\n        self.conv_layers = len(dilations)\n        self.conv_kernel_size = conv_kernel_size\n\n        self.kernel_predictor = KernelPredictor(\n            cond_channels=cond_channels,\n            conv_in_channels=in_channels,\n            conv_out_channels=2 * in_channels,\n            conv_layers=len(dilations),\n            conv_kernel_size=conv_kernel_size,\n            kpnet_hidden_channels=kpnet_hidden_channels,\n            kpnet_conv_size=kpnet_conv_size,\n            kpnet_dropout=kpnet_dropout,\n            kpnet_nonlinear_activation_params={\"negative_slope\": lReLU_slope},\n        )\n\n        self.convt_pre = nn.Sequential(\n            nn.LeakyReLU(lReLU_slope),\n            nn.utils.parametrizations.weight_norm(\n                nn.ConvTranspose1d(\n                    in_channels,\n                    in_channels,\n                    2 * stride,\n                    stride=stride,\n                    padding=stride // 2 + stride % 2,\n                    output_padding=stride % 2,\n                )\n            ),\n        )\n\n        self.conv_blocks = nn.ModuleList()\n        for dilation in dilations:\n            self.conv_blocks.append(\n                nn.Sequential(\n                    nn.LeakyReLU(lReLU_slope),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            in_channels,\n                            in_channels,\n                            conv_kernel_size,\n                            padding=dilation * (conv_kernel_size - 1) // 2,\n                            dilation=dilation,\n                        )\n                    ),\n                    nn.LeakyReLU(lReLU_slope),\n                )\n            )\n\n    def forward(self, x, c):\n        \"\"\"forward propagation of the location-variable convolutions.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length)\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n\n        Returns:\n            Tensor: the output sequence (batch, in_channels, in_length)\n        \"\"\"\n        _, in_channels, _ = x.shape  # (B, c_g, L')\n\n        x = self.convt_pre(x)  # (B, c_g, stride * L')\n        kernels, bias = self.kernel_predictor(c)\n\n        for i, conv in enumerate(self.conv_blocks):\n            output = conv(x)  # (B, c_g, stride * L')\n\n            k = kernels[:, i, :, :, :, :]  # (B, 2 * c_g, c_g, kernel_size, cond_length)\n            b = bias[:, i, :, :]  # (B, 2 * c_g, cond_length)\n\n            output = self.location_variable_convolution(\n                output, k, b, hop_size=self.cond_hop_length\n            )  # (B, 2 * c_g, stride * L'): LVC\n            x = x + torch.sigmoid(output[:, :in_channels, :]) * torch.tanh(\n                output[:, in_channels:, :]\n            )  # (B, c_g, stride * L'): GAU\n\n        return x\n\n    def location_variable_convolution(self, x, kernel, bias, dilation=1, hop_size=256):\n        \"\"\"perform location-variable convolution operation on the input sequence (x) using the local convolution kernl.\n        Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length).\n            kernel (Tensor): the local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length)\n            bias (Tensor): the bias for the local convolution (batch, out_channels, kernel_length)\n            dilation (int): the dilation of convolution.\n            hop_size (int): the hop_size of the conditioning sequence.\n        Returns:\n            (Tensor): the output sequence after performing local convolution. (batch, out_channels, in_length).\n        \"\"\"\n        batch, _, in_length = x.shape\n        batch, _, out_channels, kernel_size, kernel_length = kernel.shape\n        assert in_length == (kernel_length * hop_size), \"length of (x, kernel) is not matched\"\n\n        padding = dilation * int((kernel_size - 1) / 2)\n        x = F.pad(x, (padding, padding), \"constant\", 0)  # (batch, in_channels, in_length + 2*padding)\n        x = x.unfold(2, hop_size + 2 * padding, hop_size)  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n        if hop_size < dilation:\n            x = F.pad(x, (0, dilation), \"constant\", 0)\n        x = x.unfold(\n            3, dilation, dilation\n        )  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n        x = x[:, :, :, :, :hop_size]\n        x = x.transpose(3, 4)  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n        x = x.unfold(4, kernel_size, 1)  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n        o = torch.einsum(\"bildsk,biokl->bolsd\", x, kernel)\n        o = o.to(memory_format=torch.channels_last_3d)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).to(memory_format=torch.channels_last_3d)\n        o = o + bias\n        o = o.contiguous().view(batch, out_channels, -1)\n\n        return o\n\n    def remove_weight_norm(self):\n        self.kernel_predictor.remove_weight_norm()\n        parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n        for block in self.conv_blocks:\n            parametrize.remove_parametrizations(block[1], \"weight\")\n\n\nclass UnivNetGenerator(nn.Module):\n    \"\"\"\n    UnivNet Generator\n\n    Originally from https://github.com/mindslab-ai/univnet/blob/master/model/generator.py.\n    \"\"\"\n\n    def __init__(\n        self,\n        noise_dim=64,\n        channel_size=32,\n        dilations=[1, 3, 9, 27],\n        strides=[8, 8, 4],\n        lReLU_slope=0.2,\n        kpnet_conv_size=3,\n        # Below are MEL configurations options that this generator requires.\n        hop_length=256,\n        n_mel_channels=100,\n    ):\n        super(UnivNetGenerator, self).__init__()\n        self.mel_channel = n_mel_channels\n        self.noise_dim = noise_dim\n        self.hop_length = hop_length\n        channel_size = channel_size\n        kpnet_conv_size = kpnet_conv_size\n\n        self.res_stack = nn.ModuleList()\n        hop_length = 1\n        for stride in strides:\n            hop_length = stride * hop_length\n            self.res_stack.append(\n                LVCBlock(\n                    channel_size,\n                    n_mel_channels,\n                    stride=stride,\n                    dilations=dilations,\n                    lReLU_slope=lReLU_slope,\n                    cond_hop_length=hop_length,\n                    kpnet_conv_size=kpnet_conv_size,\n                )\n            )\n\n        self.conv_pre = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(noise_dim, channel_size, 7, padding=3, padding_mode=\"reflect\")\n        )\n\n        self.conv_post = nn.Sequential(\n            nn.LeakyReLU(lReLU_slope),\n            nn.utils.parametrizations.weight_norm(nn.Conv1d(channel_size, 1, 7, padding=3, padding_mode=\"reflect\")),\n            nn.Tanh(),\n        )\n\n    def forward(self, c, z):\n        \"\"\"\n        Args:\n            c (Tensor): the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)\n            z (Tensor): the noise sequence (batch, noise_dim, in_length)\n\n        \"\"\"\n        z = self.conv_pre(z)  # (B, c_g, L)\n\n        for res_block in self.res_stack:\n            res_block.to(z.device)\n            z = res_block(z, c)  # (B, c_g, L * s_0 * ... * s_i)\n\n        z = self.conv_post(z)  # (B, 1, L * 256)\n\n        return z\n\n    def eval(self, inference=False):\n        super(UnivNetGenerator, self).eval()\n        # don't remove weight norm while validation in training loop\n        if inference:\n            self.remove_weight_norm()\n\n    def remove_weight_norm(self):\n        parametrize.remove_parametrizations(self.conv_pre, \"weight\")\n\n        for layer in self.conv_post:\n            if len(layer.state_dict()) != 0:\n                parametrize.remove_parametrizations(layer, \"weight\")\n\n        for res_block in self.res_stack:\n            res_block.remove_weight_norm()\n\n    def inference(self, c, z=None):\n        # pad input mel with zeros to cut artifact\n        # see https://github.com/seungwonpark/melgan/issues/8\n        zero = torch.full((c.shape[0], self.mel_channel, 10), -11.5129).to(c.device)\n        mel = torch.cat((c, zero), dim=2)\n\n        if z is None:\n            z = torch.randn(c.shape[0], self.noise_dim, mel.size(2)).to(mel.device)\n\n        audio = self.forward(mel, z)\n        audio = audio[:, :, : -(self.hop_length * 10)]\n        audio = audio.clamp(min=-1, max=1)\n        return audio\n\n\n@dataclass\nclass VocType:\n    constructor: Callable[[], nn.Module]\n    model_path: str\n    subkey: Optional[str] = None\n\n    def optionally_index(self, model_dict):\n        if self.subkey is not None:\n            return model_dict[self.subkey]\n        return model_dict\n\n\nclass VocConf(Enum):\n    Univnet = VocType(UnivNetGenerator, \"vocoder.pth\", \"model_g\")\n\n\nif __name__ == \"__main__\":\n    model = UnivNetGenerator()\n\n    c = torch.randn(3, 100, 10)\n    z = torch.randn(3, 64, 10)\n    print(c.shape)\n\n    y = model(c, z)\n    print(y.shape)\n    assert y.shape == torch.Size([3, 1, 2560])\n\n    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(pytorch_total_params)\n", "TTS/tts/layers/generic/time_depth_sep_conv.py": "import torch\nfrom torch import nn\n\n\nclass TimeDepthSeparableConv(nn.Module):\n    \"\"\"Time depth separable convolution as in https://arxiv.org/pdf/1904.02619.pdf\n    It shows competative results with less computation and memory footprint.\"\"\"\n\n    def __init__(self, in_channels, hid_channels, out_channels, kernel_size, bias=True):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hid_channels = hid_channels\n        self.kernel_size = kernel_size\n\n        self.time_conv = nn.Conv1d(\n            in_channels,\n            2 * hid_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.norm1 = nn.BatchNorm1d(2 * hid_channels)\n        self.depth_conv = nn.Conv1d(\n            hid_channels,\n            hid_channels,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n            groups=hid_channels,\n            bias=bias,\n        )\n        self.norm2 = nn.BatchNorm1d(hid_channels)\n        self.time_conv2 = nn.Conv1d(\n            hid_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.norm3 = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):\n        x_res = x\n        x = self.time_conv(x)\n        x = self.norm1(x)\n        x = nn.functional.glu(x, dim=1)\n        x = self.depth_conv(x)\n        x = self.norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.time_conv2(x)\n        x = self.norm3(x)\n        x = x_res + x\n        return x\n\n\nclass TimeDepthSeparableConvBlock(nn.Module):\n    def __init__(self, in_channels, hid_channels, out_channels, num_layers, kernel_size, bias=True):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0\n        assert num_layers > 1\n\n        self.layers = nn.ModuleList()\n        layer = TimeDepthSeparableConv(\n            in_channels, hid_channels, out_channels if num_layers == 1 else hid_channels, kernel_size, bias\n        )\n        self.layers.append(layer)\n        for idx in range(num_layers - 1):\n            layer = TimeDepthSeparableConv(\n                hid_channels,\n                hid_channels,\n                out_channels if (idx + 1) == (num_layers - 1) else hid_channels,\n                kernel_size,\n                bias,\n            )\n            self.layers.append(layer)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x * mask)\n        return x\n", "TTS/tts/layers/generic/normalization.py": "import torch\nfrom torch import nn\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-4):\n        \"\"\"Layer norm for the 2nd dimension of the input.\n        Args:\n            channels (int): number of channels (2nd dimension) of the input.\n            eps (float): to prevent 0 division\n\n        Shapes:\n            - input: (B, C, T)\n            - output: (B, C, T)\n        \"\"\"\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(1, channels, 1) * 0.1)\n        self.beta = nn.Parameter(torch.zeros(1, channels, 1))\n\n    def forward(self, x):\n        mean = torch.mean(x, 1, keepdim=True)\n        variance = torch.mean((x - mean) ** 2, 1, keepdim=True)\n        x = (x - mean) * torch.rsqrt(variance + self.eps)\n        x = x * self.gamma + self.beta\n        return x\n\n\nclass LayerNorm2(nn.Module):\n    \"\"\"Layer norm for the 2nd dimension of the input using torch primitive.\n    Args:\n        channels (int): number of channels (2nd dimension) of the input.\n        eps (float): to prevent 0 division\n\n    Shapes:\n        - input: (B, C, T)\n        - output: (B, C, T)\n    \"\"\"\n\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = torch.nn.functional.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n        return x.transpose(1, -1)\n\n\nclass TemporalBatchNorm1d(nn.BatchNorm1d):\n    \"\"\"Normalize each channel separately over time and batch.\"\"\"\n\n    def __init__(self, channels, affine=True, track_running_stats=True, momentum=0.1):\n        super().__init__(channels, affine=affine, track_running_stats=track_running_stats, momentum=momentum)\n\n    def forward(self, x):\n        return super().forward(x.transpose(2, 1)).transpose(2, 1)\n\n\nclass ActNorm(nn.Module):\n    \"\"\"Activation Normalization bijector as an alternative to Batch Norm. It computes\n    mean and std from a sample data in advance and it uses these values\n    for normalization at training.\n\n    Args:\n        channels (int): input channels.\n        ddi (False): data depended initialization flag.\n\n    Shapes:\n        - inputs: (B, C, T)\n        - outputs: (B, C, T)\n    \"\"\"\n\n    def __init__(self, channels, ddi=False, **kwargs):  # pylint: disable=unused-argument\n        super().__init__()\n        self.channels = channels\n        self.initialized = not ddi\n\n        self.logs = nn.Parameter(torch.zeros(1, channels, 1))\n        self.bias = nn.Parameter(torch.zeros(1, channels, 1))\n\n    def forward(self, x, x_mask=None, reverse=False, **kwargs):  # pylint: disable=unused-argument\n        if x_mask is None:\n            x_mask = torch.ones(x.size(0), 1, x.size(2)).to(device=x.device, dtype=x.dtype)\n        x_len = torch.sum(x_mask, [1, 2])\n        if not self.initialized:\n            self.initialize(x, x_mask)\n            self.initialized = True\n\n        if reverse:\n            z = (x - self.bias) * torch.exp(-self.logs) * x_mask\n            logdet = None\n        else:\n            z = (self.bias + torch.exp(self.logs) * x) * x_mask\n            logdet = torch.sum(self.logs) * x_len  # [b]\n\n        return z, logdet\n\n    def store_inverse(self):\n        pass\n\n    def set_ddi(self, ddi):\n        self.initialized = not ddi\n\n    def initialize(self, x, x_mask):\n        with torch.no_grad():\n            denom = torch.sum(x_mask, [0, 2])\n            m = torch.sum(x * x_mask, [0, 2]) / denom\n            m_sq = torch.sum(x * x * x_mask, [0, 2]) / denom\n            v = m_sq - (m**2)\n            logs = 0.5 * torch.log(torch.clamp_min(v, 1e-6))\n\n            bias_init = (-m * torch.exp(-logs)).view(*self.bias.shape).to(dtype=self.bias.dtype)\n            logs_init = (-logs).view(*self.logs.shape).to(dtype=self.logs.dtype)\n\n            self.bias.data.copy_(bias_init)\n            self.logs.data.copy_(logs_init)\n", "TTS/tts/layers/generic/gated_conv.py": "from torch import nn\n\nfrom .normalization import LayerNorm\n\n\nclass GatedConvBlock(nn.Module):\n    \"\"\"Gated convolutional block as in https://arxiv.org/pdf/1612.08083.pdf\n    Args:\n        in_out_channels (int): number of input/output channels.\n        kernel_size (int): convolution kernel size.\n        dropout_p (float): dropout rate.\n    \"\"\"\n\n    def __init__(self, in_out_channels, kernel_size, dropout_p, num_layers):\n        super().__init__()\n        # class arguments\n        self.dropout_p = dropout_p\n        self.num_layers = num_layers\n        # define layers\n        self.conv_layers = nn.ModuleList()\n        self.norm_layers = nn.ModuleList()\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.conv_layers += [nn.Conv1d(in_out_channels, 2 * in_out_channels, kernel_size, padding=kernel_size // 2)]\n            self.norm_layers += [LayerNorm(2 * in_out_channels)]\n\n    def forward(self, x, x_mask):\n        o = x\n        res = x\n        for idx in range(self.num_layers):\n            o = nn.functional.dropout(o, p=self.dropout_p, training=self.training)\n            o = self.conv_layers[idx](o * x_mask)\n            o = self.norm_layers[idx](o)\n            o = nn.functional.glu(o, dim=1)\n            o = res + o\n            res = o\n        return o\n", "TTS/tts/layers/generic/pos_encoding.py": "import math\n\nimport torch\nfrom torch import nn\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding for non-recurrent neural networks.\n    Implementation based on \"Attention Is All You Need\"\n\n    Args:\n       channels (int): embedding size\n       dropout_p (float): dropout rate applied to the output.\n       max_len (int): maximum sequence length.\n       use_scale (bool): whether to use a learnable scaling coefficient.\n    \"\"\"\n\n    def __init__(self, channels, dropout_p=0.0, max_len=5000, use_scale=False):\n        super().__init__()\n        if channels % 2 != 0:\n            raise ValueError(\n                \"Cannot use sin/cos positional encoding with \" \"odd channels (got channels={:d})\".format(channels)\n            )\n        self.use_scale = use_scale\n        if use_scale:\n            self.scale = torch.nn.Parameter(torch.ones(1))\n        pe = torch.zeros(max_len, channels)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.pow(10000, torch.arange(0, channels, 2).float() / channels)\n        pe[:, 0::2] = torch.sin(position.float() * div_term)\n        pe[:, 1::2] = torch.cos(position.float() * div_term)\n        pe = pe.unsqueeze(0).transpose(1, 2)\n        self.register_buffer(\"pe\", pe)\n        if dropout_p > 0:\n            self.dropout = nn.Dropout(p=dropout_p)\n        self.channels = channels\n\n    def forward(self, x, mask=None, first_idx=None, last_idx=None):\n        \"\"\"\n        Shapes:\n            x: [B, C, T]\n            mask: [B, 1, T]\n            first_idx: int\n            last_idx: int\n        \"\"\"\n\n        x = x * math.sqrt(self.channels)\n        if first_idx is None:\n            if self.pe.size(2) < x.size(2):\n                raise RuntimeError(\n                    f\"Sequence is {x.size(2)} but PositionalEncoding is\"\n                    f\" limited to {self.pe.size(2)}. See max_len argument.\"\n                )\n            if mask is not None:\n                pos_enc = self.pe[:, :, : x.size(2)] * mask\n            else:\n                pos_enc = self.pe[:, :, : x.size(2)]\n            if self.use_scale:\n                x = x + self.scale * pos_enc\n            else:\n                x = x + pos_enc\n        else:\n            if self.use_scale:\n                x = x + self.scale * self.pe[:, :, first_idx:last_idx]\n            else:\n                x = x + self.pe[:, :, first_idx:last_idx]\n        if hasattr(self, \"dropout\"):\n            x = self.dropout(x)\n        return x\n", "TTS/tts/layers/generic/transformer.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass FFTransformer(nn.Module):\n    def __init__(self, in_out_channels, num_heads, hidden_channels_ffn=1024, kernel_size_fft=3, dropout_p=0.1):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(in_out_channels, num_heads, dropout=dropout_p)\n\n        padding = (kernel_size_fft - 1) // 2\n        self.conv1 = nn.Conv1d(in_out_channels, hidden_channels_ffn, kernel_size=kernel_size_fft, padding=padding)\n        self.conv2 = nn.Conv1d(hidden_channels_ffn, in_out_channels, kernel_size=kernel_size_fft, padding=padding)\n\n        self.norm1 = nn.LayerNorm(in_out_channels)\n        self.norm2 = nn.LayerNorm(in_out_channels)\n\n        self.dropout1 = nn.Dropout(dropout_p)\n        self.dropout2 = nn.Dropout(dropout_p)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        \"\"\"\ud83d\ude26 ugly looking with all the transposing\"\"\"\n        src = src.permute(2, 0, 1)\n        src2, enc_align = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n        src = src + self.dropout1(src2)\n        src = self.norm1(src + src2)\n        # T x B x D -> B x D x T\n        src = src.permute(1, 2, 0)\n        src2 = self.conv2(F.relu(self.conv1(src)))\n        src2 = self.dropout2(src2)\n        src = src + src2\n        src = src.transpose(1, 2)\n        src = self.norm2(src)\n        src = src.transpose(1, 2)\n        return src, enc_align\n\n\nclass FFTransformerBlock(nn.Module):\n    def __init__(self, in_out_channels, num_heads, hidden_channels_ffn, num_layers, dropout_p):\n        super().__init__()\n        self.fft_layers = nn.ModuleList(\n            [\n                FFTransformer(\n                    in_out_channels=in_out_channels,\n                    num_heads=num_heads,\n                    hidden_channels_ffn=hidden_channels_ffn,\n                    dropout_p=dropout_p,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n    def forward(self, x, mask=None, g=None):  # pylint: disable=unused-argument\n        \"\"\"\n        TODO: handle multi-speaker\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - mask:  :math:`[B, 1, T] or [B, T]`\n        \"\"\"\n        if mask is not None and mask.ndim == 3:\n            mask = mask.squeeze(1)\n            # mask is negated, torch uses 1s and 0s reversely.\n            mask = ~mask.bool()\n        alignments = []\n        for layer in self.fft_layers:\n            x, align = layer(x, src_key_padding_mask=mask)\n            alignments.append(align.unsqueeze(1))\n        alignments = torch.cat(alignments, 1)\n        return x\n\n\nclass FFTDurationPredictor:\n    def __init__(\n        self, in_channels, hidden_channels, num_heads, num_layers, dropout_p=0.1, cond_channels=None\n    ):  # pylint: disable=unused-argument\n        self.fft = FFTransformerBlock(in_channels, num_heads, hidden_channels, num_layers, dropout_p)\n        self.proj = nn.Linear(in_channels, 1)\n\n    def forward(self, x, mask=None, g=None):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`\n            - mask:  :math:`[B, 1, T]`\n\n        TODO: Handle the cond input\n        \"\"\"\n        x = self.fft(x, mask=mask)\n        x = self.proj(x)\n        return x\n", "TTS/tts/layers/generic/aligner.py": "from typing import Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass AlignmentNetwork(torch.nn.Module):\n    \"\"\"Aligner Network for learning alignment between the input text and the model output with Gaussian Attention.\n\n    ::\n\n        query -> conv1d -> relu -> conv1d -> relu -> conv1d -> L2_dist -> softmax -> alignment\n        key   -> conv1d -> relu -> conv1d -----------------------^\n\n    Args:\n        in_query_channels (int): Number of channels in the query network. Defaults to 80.\n        in_key_channels (int): Number of channels in the key network. Defaults to 512.\n        attn_channels (int): Number of inner channels in the attention layers. Defaults to 80.\n        temperature (float): Temperature for the softmax. Defaults to 0.0005.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_query_channels=80,\n        in_key_channels=512,\n        attn_channels=80,\n        temperature=0.0005,\n    ):\n        super().__init__()\n        self.temperature = temperature\n        self.softmax = torch.nn.Softmax(dim=3)\n        self.log_softmax = torch.nn.LogSoftmax(dim=3)\n\n        self.key_layer = nn.Sequential(\n            nn.Conv1d(\n                in_key_channels,\n                in_key_channels * 2,\n                kernel_size=3,\n                padding=1,\n                bias=True,\n            ),\n            torch.nn.ReLU(),\n            nn.Conv1d(in_key_channels * 2, attn_channels, kernel_size=1, padding=0, bias=True),\n        )\n\n        self.query_layer = nn.Sequential(\n            nn.Conv1d(\n                in_query_channels,\n                in_query_channels * 2,\n                kernel_size=3,\n                padding=1,\n                bias=True,\n            ),\n            torch.nn.ReLU(),\n            nn.Conv1d(in_query_channels * 2, in_query_channels, kernel_size=1, padding=0, bias=True),\n            torch.nn.ReLU(),\n            nn.Conv1d(in_query_channels, attn_channels, kernel_size=1, padding=0, bias=True),\n        )\n\n        self.init_layers()\n\n    def init_layers(self):\n        torch.nn.init.xavier_uniform_(self.key_layer[0].weight, gain=torch.nn.init.calculate_gain(\"relu\"))\n        torch.nn.init.xavier_uniform_(self.key_layer[2].weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n        torch.nn.init.xavier_uniform_(self.query_layer[0].weight, gain=torch.nn.init.calculate_gain(\"relu\"))\n        torch.nn.init.xavier_uniform_(self.query_layer[2].weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n        torch.nn.init.xavier_uniform_(self.query_layer[4].weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n\n    def forward(\n        self, queries: torch.tensor, keys: torch.tensor, mask: torch.tensor = None, attn_prior: torch.tensor = None\n    ) -> Tuple[torch.tensor, torch.tensor]:\n        \"\"\"Forward pass of the aligner encoder.\n        Shapes:\n            - queries: :math:`[B, C, T_de]`\n            - keys: :math:`[B, C_emb, T_en]`\n            - mask: :math:`[B, T_de]`\n        Output:\n            attn (torch.tensor): :math:`[B, 1, T_en, T_de]` soft attention mask.\n            attn_logp (torch.tensor): :math:`[\u00dfB, 1, T_en , T_de]` log probabilities.\n        \"\"\"\n        key_out = self.key_layer(keys)\n        query_out = self.query_layer(queries)\n        attn_factor = (query_out[:, :, :, None] - key_out[:, :, None]) ** 2\n        attn_logp = -self.temperature * attn_factor.sum(1, keepdim=True)\n        if attn_prior is not None:\n            attn_logp = self.log_softmax(attn_logp) + torch.log(attn_prior[:, None] + 1e-8)\n\n        if mask is not None:\n            attn_logp.data.masked_fill_(~mask.bool().unsqueeze(2), -float(\"inf\"))\n\n        attn = self.softmax(attn_logp)\n        return attn, attn_logp\n", "TTS/tts/layers/generic/res_conv_bn.py": "from torch import nn\n\n\nclass ZeroTemporalPad(nn.Module):\n    \"\"\"Pad sequences to equal lentgh in the temporal dimension\"\"\"\n\n    def __init__(self, kernel_size, dilation):\n        super().__init__()\n        total_pad = dilation * (kernel_size - 1)\n        begin = total_pad // 2\n        end = total_pad - begin\n        self.pad_layer = nn.ZeroPad2d((0, 0, begin, end))\n\n    def forward(self, x):\n        return self.pad_layer(x)\n\n\nclass Conv1dBN(nn.Module):\n    \"\"\"1d convolutional with batch norm.\n    conv1d -> relu -> BN blocks.\n\n    Note:\n        Batch normalization is applied after ReLU regarding the original implementation.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): kernel size for convolutional filters.\n        dilation (int): dilation for convolution layers.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n        super().__init__()\n        padding = dilation * (kernel_size - 1)\n        pad_s = padding // 2\n        pad_e = padding - pad_s\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation)\n        self.pad = nn.ZeroPad2d((pad_s, pad_e, 0, 0))  # uneven left and right padding\n        self.norm = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):\n        o = self.conv1d(x)\n        o = self.pad(o)\n        o = nn.functional.relu(o)\n        o = self.norm(o)\n        return o\n\n\nclass Conv1dBNBlock(nn.Module):\n    \"\"\"1d convolutional block with batch norm. It is a set of conv1d -> relu -> BN blocks.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of inner convolution channels.\n        kernel_size (int): kernel size for convolutional filters.\n        dilation (int): dilation for convolution layers.\n        num_conv_blocks (int, optional): number of convolutional blocks. Defaults to 2.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size, dilation, num_conv_blocks=2):\n        super().__init__()\n        self.conv_bn_blocks = []\n        for idx in range(num_conv_blocks):\n            layer = Conv1dBN(\n                in_channels if idx == 0 else hidden_channels,\n                out_channels if idx == (num_conv_blocks - 1) else hidden_channels,\n                kernel_size,\n                dilation,\n            )\n            self.conv_bn_blocks.append(layer)\n        self.conv_bn_blocks = nn.Sequential(*self.conv_bn_blocks)\n\n    def forward(self, x):\n        \"\"\"\n        Shapes:\n            x: (B, D, T)\n        \"\"\"\n        return self.conv_bn_blocks(x)\n\n\nclass ResidualConv1dBNBlock(nn.Module):\n    \"\"\"Residual Convolutional Blocks with BN\n    Each block has 'num_conv_block' conv layers and 'num_res_blocks' such blocks are connected\n    with residual connections.\n\n    conv_block = (conv1d -> relu -> bn) x 'num_conv_blocks'\n    residuak_conv_block =  (x -> conv_block ->  + ->) x 'num_res_blocks'\n                            ' - - - - - - - - - ^\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of inner convolution channels.\n        kernel_size (int): kernel size for convolutional filters.\n        dilations (list): dilations for each convolution layer.\n        num_res_blocks (int, optional): number of residual blocks. Defaults to 13.\n        num_conv_blocks (int, optional): number of convolutional blocks in each residual block. Defaults to 2.\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, hidden_channels, kernel_size, dilations, num_res_blocks=13, num_conv_blocks=2\n    ):\n        super().__init__()\n        assert len(dilations) == num_res_blocks\n        self.res_blocks = nn.ModuleList()\n        for idx, dilation in enumerate(dilations):\n            block = Conv1dBNBlock(\n                in_channels if idx == 0 else hidden_channels,\n                out_channels if (idx + 1) == len(dilations) else hidden_channels,\n                hidden_channels,\n                kernel_size,\n                dilation,\n                num_conv_blocks,\n            )\n            self.res_blocks.append(block)\n\n    def forward(self, x, x_mask=None):\n        if x_mask is None:\n            x_mask = 1.0\n        o = x * x_mask\n        for block in self.res_blocks:\n            res = o\n            o = block(o)\n            o = o + res\n            if x_mask is not None:\n                o = o * x_mask\n        return o\n", "TTS/tts/layers/generic/wavenet.py": "import torch\nfrom torch import nn\nfrom torch.nn.utils import parametrize\n\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\n\n\nclass WN(torch.nn.Module):\n    \"\"\"Wavenet layers with weight norm and no input conditioning.\n\n         |-----------------------------------------------------------------------------|\n         |                                    |-> tanh    -|                           |\n    res -|- conv1d(dilation) -> dropout -> + -|            * -> conv1d1x1 -> split -|- + -> res\n    g -------------------------------------|  |-> sigmoid -|                        |\n    o --------------------------------------------------------------------------- + --------- o\n\n    Args:\n        in_channels (int): number of input channels.\n        hidden_channes (int): number of hidden channels.\n        kernel_size (int): filter kernel size for the first conv layer.\n        dilation_rate (int): dilations rate to increase dilation per layer.\n            If it is 2, dilations are 1, 2, 4, 8 for the next 4 layers.\n        num_layers (int): number of wavenet layers.\n        c_in_channels (int): number of channels of conditioning input.\n        dropout_p (float): dropout rate.\n        weight_norm (bool): enable/disable weight norm for convolution layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_layers,\n        c_in_channels=0,\n        dropout_p=0,\n        weight_norm=True,\n    ):\n        super().__init__()\n        assert kernel_size % 2 == 1\n        assert hidden_channels % 2 == 0\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.num_layers = num_layers\n        self.c_in_channels = c_in_channels\n        self.dropout_p = dropout_p\n\n        self.in_layers = torch.nn.ModuleList()\n        self.res_skip_layers = torch.nn.ModuleList()\n        self.dropout = nn.Dropout(dropout_p)\n\n        # init conditioning layer\n        if c_in_channels > 0:\n            cond_layer = torch.nn.Conv1d(c_in_channels, 2 * hidden_channels * num_layers, 1)\n            self.cond_layer = torch.nn.utils.parametrizations.weight_norm(cond_layer, name=\"weight\")\n        # intermediate layers\n        for i in range(num_layers):\n            dilation = dilation_rate**i\n            padding = int((kernel_size * dilation - dilation) / 2)\n            if i == 0:\n                in_layer = torch.nn.Conv1d(\n                    in_channels, 2 * hidden_channels, kernel_size, dilation=dilation, padding=padding\n                )\n            else:\n                in_layer = torch.nn.Conv1d(\n                    hidden_channels, 2 * hidden_channels, kernel_size, dilation=dilation, padding=padding\n                )\n            in_layer = torch.nn.utils.parametrizations.weight_norm(in_layer, name=\"weight\")\n            self.in_layers.append(in_layer)\n\n            if i < num_layers - 1:\n                res_skip_channels = 2 * hidden_channels\n            else:\n                res_skip_channels = hidden_channels\n\n            res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n            res_skip_layer = torch.nn.utils.parametrizations.weight_norm(res_skip_layer, name=\"weight\")\n            self.res_skip_layers.append(res_skip_layer)\n        # setup weight norm\n        if not weight_norm:\n            self.remove_weight_norm()\n\n    def forward(self, x, x_mask=None, g=None, **kwargs):  # pylint: disable=unused-argument\n        output = torch.zeros_like(x)\n        n_channels_tensor = torch.IntTensor([self.hidden_channels])\n        x_mask = 1.0 if x_mask is None else x_mask\n        if g is not None:\n            g = self.cond_layer(g)\n        for i in range(self.num_layers):\n            x_in = self.in_layers[i](x)\n            x_in = self.dropout(x_in)\n            if g is not None:\n                cond_offset = i * 2 * self.hidden_channels\n                g_l = g[:, cond_offset : cond_offset + 2 * self.hidden_channels, :]\n            else:\n                g_l = torch.zeros_like(x_in)\n            acts = fused_add_tanh_sigmoid_multiply(x_in, g_l, n_channels_tensor)\n            res_skip_acts = self.res_skip_layers[i](acts)\n            if i < self.num_layers - 1:\n                x = (x + res_skip_acts[:, : self.hidden_channels, :]) * x_mask\n                output = output + res_skip_acts[:, self.hidden_channels :, :]\n            else:\n                output = output + res_skip_acts\n        return output * x_mask\n\n    def remove_weight_norm(self):\n        if self.c_in_channels != 0:\n            parametrize.remove_parametrizations(self.cond_layer, \"weight\")\n        for l in self.in_layers:\n            parametrize.remove_parametrizations(l, \"weight\")\n        for l in self.res_skip_layers:\n            parametrize.remove_parametrizations(l, \"weight\")\n\n\nclass WNBlocks(nn.Module):\n    \"\"\"Wavenet blocks.\n\n    Note: After each block dilation resets to 1 and it increases in each block\n        along the dilation rate.\n\n    Args:\n        in_channels (int): number of input channels.\n        hidden_channes (int): number of hidden channels.\n        kernel_size (int): filter kernel size for the first conv layer.\n        dilation_rate (int): dilations rate to increase dilation per layer.\n            If it is 2, dilations are 1, 2, 4, 8 for the next 4 layers.\n        num_blocks (int): number of wavenet blocks.\n        num_layers (int): number of wavenet layers.\n        c_in_channels (int): number of channels of conditioning input.\n        dropout_p (float): dropout rate.\n        weight_norm (bool): enable/disable weight norm for convolution layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_blocks,\n        num_layers,\n        c_in_channels=0,\n        dropout_p=0,\n        weight_norm=True,\n    ):\n        super().__init__()\n        self.wn_blocks = nn.ModuleList()\n        for idx in range(num_blocks):\n            layer = WN(\n                in_channels=in_channels if idx == 0 else hidden_channels,\n                hidden_channels=hidden_channels,\n                kernel_size=kernel_size,\n                dilation_rate=dilation_rate,\n                num_layers=num_layers,\n                c_in_channels=c_in_channels,\n                dropout_p=dropout_p,\n                weight_norm=weight_norm,\n            )\n            self.wn_blocks.append(layer)\n\n    def forward(self, x, x_mask=None, g=None):\n        o = x\n        for layer in self.wn_blocks:\n            o = layer(o, x_mask, g)\n        return o\n", "TTS/tts/layers/generic/__init__.py": "", "TTS/tts/layers/align_tts/mdn.py": "from torch import nn\n\n\nclass MDNBlock(nn.Module):\n    \"\"\"Mixture of Density Network implementation\n    https://arxiv.org/pdf/2003.01950.pdf\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.out_channels = out_channels\n        self.conv1 = nn.Conv1d(in_channels, in_channels, 1)\n        self.norm = nn.LayerNorm(in_channels)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.conv2 = nn.Conv1d(in_channels, out_channels, 1)\n\n    def forward(self, x):\n        o = self.conv1(x)\n        o = o.transpose(1, 2)\n        o = self.norm(o)\n        o = o.transpose(1, 2)\n        o = self.relu(o)\n        o = self.dropout(o)\n        mu_sigma = self.conv2(o)\n        # TODO: check this sigmoid\n        # mu = torch.sigmoid(mu_sigma[:, :self.out_channels//2, :])\n        mu = mu_sigma[:, : self.out_channels // 2, :]\n        log_sigma = mu_sigma[:, self.out_channels // 2 :, :]\n        return mu, log_sigma\n", "TTS/tts/layers/align_tts/duration_predictor.py": "from torch import nn\n\nfrom TTS.tts.layers.generic.pos_encoding import PositionalEncoding\nfrom TTS.tts.layers.generic.transformer import FFTransformerBlock\n\n\nclass DurationPredictor(nn.Module):\n    def __init__(self, num_chars, hidden_channels, hidden_channels_ffn, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(num_chars, hidden_channels)\n        self.pos_enc = PositionalEncoding(hidden_channels, dropout_p=0.1)\n        self.FFT = FFTransformerBlock(hidden_channels, num_heads, hidden_channels_ffn, 2, 0.1)\n        self.out_layer = nn.Conv1d(hidden_channels, 1, 1)\n\n    def forward(self, text, text_lengths):\n        # B, L -> B, L\n        emb = self.embed(text)\n        emb = self.pos_enc(emb.transpose(1, 2))\n        x = self.FFT(emb, text_lengths)\n        x = self.out_layer(x).squeeze(-1)\n        return x\n", "TTS/tts/layers/align_tts/__init__.py": "", "TTS/tts/layers/tacotron/common_layers.py": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Linear(nn.Module):\n    \"\"\"Linear layer with a specific initialization.\n\n    Args:\n        in_features (int): number of channels in the input tensor.\n        out_features (int): number of channels in the output tensor.\n        bias (bool, optional): enable/disable bias in the layer. Defaults to True.\n        init_gain (str, optional): method to compute the gain in the weight initializtion based on the nonlinear activation used afterwards. Defaults to 'linear'.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True, init_gain=\"linear\"):\n        super().__init__()\n        self.linear_layer = torch.nn.Linear(in_features, out_features, bias=bias)\n        self._init_w(init_gain)\n\n    def _init_w(self, init_gain):\n        torch.nn.init.xavier_uniform_(self.linear_layer.weight, gain=torch.nn.init.calculate_gain(init_gain))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass LinearBN(nn.Module):\n    \"\"\"Linear layer with Batch Normalization.\n\n    x -> linear -> BN -> o\n\n    Args:\n        in_features (int): number of channels in the input tensor.\n        out_features (int ): number of channels in the output tensor.\n        bias (bool, optional): enable/disable bias in the linear layer. Defaults to True.\n        init_gain (str, optional): method to set the gain for weight initialization. Defaults to 'linear'.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True, init_gain=\"linear\"):\n        super().__init__()\n        self.linear_layer = torch.nn.Linear(in_features, out_features, bias=bias)\n        self.batch_normalization = nn.BatchNorm1d(out_features, momentum=0.1, eps=1e-5)\n        self._init_w(init_gain)\n\n    def _init_w(self, init_gain):\n        torch.nn.init.xavier_uniform_(self.linear_layer.weight, gain=torch.nn.init.calculate_gain(init_gain))\n\n    def forward(self, x):\n        \"\"\"\n        Shapes:\n            x: [T, B, C] or [B, C]\n        \"\"\"\n        out = self.linear_layer(x)\n        if len(out.shape) == 3:\n            out = out.permute(1, 2, 0)\n        out = self.batch_normalization(out)\n        if len(out.shape) == 3:\n            out = out.permute(2, 0, 1)\n        return out\n\n\nclass Prenet(nn.Module):\n    \"\"\"Tacotron specific Prenet with an optional Batch Normalization.\n\n    Note:\n        Prenet with BN improves the model performance significantly especially\n    if it is enabled after learning a diagonal attention alignment with the original\n    prenet. However, if the target dataset is high quality then it also works from\n    the start. It is also suggested to disable dropout if BN is in use.\n\n        prenet_type == \"original\"\n            x -> [linear -> ReLU -> Dropout]xN -> o\n\n        prenet_type == \"bn\"\n            x -> [linear -> BN -> ReLU -> Dropout]xN -> o\n\n    Args:\n        in_features (int): number of channels in the input tensor and the inner layers.\n        prenet_type (str, optional): prenet type \"original\" or \"bn\". Defaults to \"original\".\n        prenet_dropout (bool, optional): dropout rate. Defaults to True.\n        dropout_at_inference (bool, optional): use dropout at inference. It leads to a better quality for some models.\n        out_features (list, optional): List of output channels for each prenet block.\n            It also defines number of the prenet blocks based on the length of argument list.\n            Defaults to [256, 256].\n        bias (bool, optional): enable/disable bias in prenet linear layers. Defaults to True.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_features,\n        prenet_type=\"original\",\n        prenet_dropout=True,\n        dropout_at_inference=False,\n        out_features=[256, 256],\n        bias=True,\n    ):\n        super().__init__()\n        self.prenet_type = prenet_type\n        self.prenet_dropout = prenet_dropout\n        self.dropout_at_inference = dropout_at_inference\n        in_features = [in_features] + out_features[:-1]\n        if prenet_type == \"bn\":\n            self.linear_layers = nn.ModuleList(\n                [LinearBN(in_size, out_size, bias=bias) for (in_size, out_size) in zip(in_features, out_features)]\n            )\n        elif prenet_type == \"original\":\n            self.linear_layers = nn.ModuleList(\n                [Linear(in_size, out_size, bias=bias) for (in_size, out_size) in zip(in_features, out_features)]\n            )\n\n    def forward(self, x):\n        for linear in self.linear_layers:\n            if self.prenet_dropout:\n                x = F.dropout(F.relu(linear(x)), p=0.5, training=self.training or self.dropout_at_inference)\n            else:\n                x = F.relu(linear(x))\n        return x\n", "TTS/tts/layers/tacotron/gst_layers.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass GST(nn.Module):\n    \"\"\"Global Style Token Module for factorizing prosody in speech.\n\n    See https://arxiv.org/pdf/1803.09017\"\"\"\n\n    def __init__(self, num_mel, num_heads, num_style_tokens, gst_embedding_dim, embedded_speaker_dim=None):\n        super().__init__()\n        self.encoder = ReferenceEncoder(num_mel, gst_embedding_dim)\n        self.style_token_layer = StyleTokenLayer(num_heads, num_style_tokens, gst_embedding_dim, embedded_speaker_dim)\n\n    def forward(self, inputs, speaker_embedding=None):\n        enc_out = self.encoder(inputs)\n        # concat speaker_embedding\n        if speaker_embedding is not None:\n            enc_out = torch.cat([enc_out, speaker_embedding], dim=-1)\n        style_embed = self.style_token_layer(enc_out)\n\n        return style_embed\n\n\nclass ReferenceEncoder(nn.Module):\n    \"\"\"NN module creating a fixed size prosody embedding from a spectrogram.\n\n    inputs: mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    \"\"\"\n\n    def __init__(self, num_mel, embedding_dim):\n        super().__init__()\n        self.num_mel = num_mel\n        filters = [1] + [32, 32, 64, 64, 128, 128]\n        num_layers = len(filters) - 1\n        convs = [\n            nn.Conv2d(\n                in_channels=filters[i], out_channels=filters[i + 1], kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n            )\n            for i in range(num_layers)\n        ]\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList([nn.BatchNorm2d(num_features=filter_size) for filter_size in filters[1:]])\n\n        post_conv_height = self.calculate_post_conv_height(num_mel, 3, 2, 1, num_layers)\n        self.recurrence = nn.GRU(\n            input_size=filters[-1] * post_conv_height, hidden_size=embedding_dim // 2, batch_first=True\n        )\n\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        x = inputs.view(batch_size, 1, -1, self.num_mel)\n        # x: 4D tensor [batch_size, num_channels==1, num_frames, num_mel]\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x)\n            x = bn(x)\n            x = F.relu(x)\n\n        x = x.transpose(1, 2)\n        # x: 4D tensor [batch_size, post_conv_width,\n        #               num_channels==128, post_conv_height]\n        post_conv_width = x.size(1)\n        x = x.contiguous().view(batch_size, post_conv_width, -1)\n        # x: 3D tensor [batch_size, post_conv_width,\n        #               num_channels*post_conv_height]\n        self.recurrence.flatten_parameters()\n        _, out = self.recurrence(x)\n        # out: 3D tensor [seq_len==1, batch_size, encoding_size=128]\n\n        return out.squeeze(0)\n\n    @staticmethod\n    def calculate_post_conv_height(height, kernel_size, stride, pad, n_convs):\n        \"\"\"Height of spec after n convolutions with fixed kernel/stride/pad.\"\"\"\n        for _ in range(n_convs):\n            height = (height - kernel_size + 2 * pad) // stride + 1\n        return height\n\n\nclass StyleTokenLayer(nn.Module):\n    \"\"\"NN Module attending to style tokens based on prosody encodings.\"\"\"\n\n    def __init__(self, num_heads, num_style_tokens, gst_embedding_dim, d_vector_dim=None):\n        super().__init__()\n\n        self.query_dim = gst_embedding_dim // 2\n\n        if d_vector_dim:\n            self.query_dim += d_vector_dim\n\n        self.key_dim = gst_embedding_dim // num_heads\n        self.style_tokens = nn.Parameter(torch.FloatTensor(num_style_tokens, self.key_dim))\n        nn.init.normal_(self.style_tokens, mean=0, std=0.5)\n        self.attention = MultiHeadAttention(\n            query_dim=self.query_dim, key_dim=self.key_dim, num_units=gst_embedding_dim, num_heads=num_heads\n        )\n\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        prosody_encoding = inputs.unsqueeze(1)\n        # prosody_encoding: 3D tensor [batch_size, 1, encoding_size==128]\n        tokens = torch.tanh(self.style_tokens).unsqueeze(0).expand(batch_size, -1, -1)\n        # tokens: 3D tensor [batch_size, num tokens, token embedding size]\n        style_embed = self.attention(prosody_encoding, tokens)\n\n        return style_embed\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    input:\n        query --- [N, T_q, query_dim]\n        key --- [N, T_k, key_dim]\n    output:\n        out --- [N, T_q, num_units]\n    \"\"\"\n\n    def __init__(self, query_dim, key_dim, num_units, num_heads):\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n\n    def forward(self, query, key):\n        queries = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key)  # [N, T_k, num_units]\n        values = self.W_value(key)\n\n        split_size = self.num_units // self.num_heads\n        queries = torch.stack(torch.split(queries, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n\n        # score = softmax(QK^T / (d_k**0.5))\n        scores = torch.matmul(queries, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores = scores / (self.key_dim**0.5)\n        scores = F.softmax(scores, dim=3)\n\n        # out = score * V\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n\n        return out\n", "TTS/tts/layers/tacotron/attentions.py": "import torch\nfrom scipy.stats import betabinom\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom TTS.tts.layers.tacotron.common_layers import Linear\n\n\nclass LocationLayer(nn.Module):\n    \"\"\"Layers for Location Sensitive Attention\n\n    Args:\n        attention_dim (int): number of channels in the input tensor.\n        attention_n_filters (int, optional): number of filters in convolution. Defaults to 32.\n        attention_kernel_size (int, optional): kernel size of convolution filter. Defaults to 31.\n    \"\"\"\n\n    def __init__(self, attention_dim, attention_n_filters=32, attention_kernel_size=31):\n        super().__init__()\n        self.location_conv1d = nn.Conv1d(\n            in_channels=2,\n            out_channels=attention_n_filters,\n            kernel_size=attention_kernel_size,\n            stride=1,\n            padding=(attention_kernel_size - 1) // 2,\n            bias=False,\n        )\n        self.location_dense = Linear(attention_n_filters, attention_dim, bias=False, init_gain=\"tanh\")\n\n    def forward(self, attention_cat):\n        \"\"\"\n        Shapes:\n            attention_cat: [B, 2, C]\n        \"\"\"\n        processed_attention = self.location_conv1d(attention_cat)\n        processed_attention = self.location_dense(processed_attention.transpose(1, 2))\n        return processed_attention\n\n\nclass GravesAttention(nn.Module):\n    \"\"\"Graves Attention as is ref1 with updates from ref2.\n    ref1: https://arxiv.org/abs/1910.10288\n    ref2: https://arxiv.org/pdf/1906.01083.pdf\n\n    Args:\n        query_dim (int): number of channels in query tensor.\n        K (int): number of Gaussian heads to be used for computing attention.\n    \"\"\"\n\n    COEF = 0.3989422917366028  # numpy.sqrt(1/(2*numpy.pi))\n\n    def __init__(self, query_dim, K):\n        super().__init__()\n        self._mask_value = 1e-8\n        self.K = K\n        # self.attention_alignment = 0.05\n        self.eps = 1e-5\n        self.J = None\n        self.N_a = nn.Sequential(\n            nn.Linear(query_dim, query_dim, bias=True), nn.ReLU(), nn.Linear(query_dim, 3 * K, bias=True)\n        )\n        self.attention_weights = None\n        self.mu_prev = None\n        self.init_layers()\n\n    def init_layers(self):\n        torch.nn.init.constant_(self.N_a[2].bias[(2 * self.K) : (3 * self.K)], 1.0)  # bias mean\n        torch.nn.init.constant_(self.N_a[2].bias[self.K : (2 * self.K)], 10)  # bias std\n\n    def init_states(self, inputs):\n        if self.J is None or inputs.shape[1] + 1 > self.J.shape[-1]:\n            self.J = torch.arange(0, inputs.shape[1] + 2.0).to(inputs.device) + 0.5\n        self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)\n        self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)\n\n    # pylint: disable=R0201\n    # pylint: disable=unused-argument\n    def preprocess_inputs(self, inputs):\n        return None\n\n    def forward(self, query, inputs, processed_inputs, mask):\n        \"\"\"\n        Shapes:\n            query: [B, C_attention_rnn]\n            inputs: [B, T_in, C_encoder]\n            processed_inputs: place_holder\n            mask: [B, T_in]\n        \"\"\"\n        gbk_t = self.N_a(query)\n        gbk_t = gbk_t.view(gbk_t.size(0), -1, self.K)\n\n        # attention model parameters\n        # each B x K\n        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # dropout to decorrelate attention heads\n        g_t = torch.nn.functional.dropout(g_t, p=0.5, training=self.training)\n\n        # attention GMM parameters\n        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) + self.eps\n\n        j = self.J[: inputs.size(1) + 1]\n\n        # attention weights\n        phi_t = g_t.unsqueeze(-1) * (1 / (1 + torch.sigmoid((mu_t.unsqueeze(-1) - j) / sig_t.unsqueeze(-1))))\n\n        # discritize attention weights\n        alpha_t = torch.sum(phi_t, 1)\n        alpha_t = alpha_t[:, 1:] - alpha_t[:, :-1]\n        alpha_t[alpha_t == 0] = 1e-8\n\n        # apply masking\n        if mask is not None:\n            alpha_t.data.masked_fill_(~mask, self._mask_value)\n\n        context = torch.bmm(alpha_t.unsqueeze(1), inputs).squeeze(1)\n        self.attention_weights = alpha_t\n        self.mu_prev = mu_t\n        return context\n\n\nclass OriginalAttention(nn.Module):\n    \"\"\"Bahdanau Attention with various optional modifications.\n    - Location sensitive attnetion: https://arxiv.org/abs/1712.05884\n    - Forward Attention: https://arxiv.org/abs/1807.06736 + state masking at inference\n    - Using sigmoid instead of softmax normalization\n    - Attention windowing at inference time\n\n    Note:\n        Location Sensitive Attention extends the additive attention mechanism\n    to use cumulative attention weights from previous decoder time steps with the current time step features.\n\n        Forward attention computes most probable monotonic alignment. The modified attention probabilities at each\n    timestep are computed recursively by the forward algorithm.\n\n        Transition agent in the forward attention explicitly gates the attention mechanism whether to move forward or\n    stay at each decoder timestep.\n\n        Attention windowing is a inductive prior that prevents the model from attending to previous and future timesteps\n    beyond a certain window.\n\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the vakue tensor. In general, the value tensor is the output of the encoder layer.\n        attention_dim (int): number of channels of the inner attention layers.\n        location_attention (bool): enable/disable location sensitive attention.\n        attention_location_n_filters (int): number of location attention filters.\n        attention_location_kernel_size (int): filter size of location attention convolution layer.\n        windowing (int): window size for attention windowing. if it is 5, for computing the attention, it only considers the time steps [(t-5), ..., (t+5)] of the input.\n        norm (str): normalization method applied to the attention weights. 'softmax' or 'sigmoid'\n        forward_attn (bool): enable/disable forward attention.\n        trans_agent (bool): enable/disable transition agent in the forward attention.\n        forward_attn_mask (int): enable/disable an explicit masking in forward attention. It is useful to set at especially inference time.\n    \"\"\"\n\n    # Pylint gets confused by PyTorch conventions here\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(\n        self,\n        query_dim,\n        embedding_dim,\n        attention_dim,\n        location_attention,\n        attention_location_n_filters,\n        attention_location_kernel_size,\n        windowing,\n        norm,\n        forward_attn,\n        trans_agent,\n        forward_attn_mask,\n    ):\n        super().__init__()\n        self.query_layer = Linear(query_dim, attention_dim, bias=False, init_gain=\"tanh\")\n        self.inputs_layer = Linear(embedding_dim, attention_dim, bias=False, init_gain=\"tanh\")\n        self.v = Linear(attention_dim, 1, bias=True)\n        if trans_agent:\n            self.ta = nn.Linear(query_dim + embedding_dim, 1, bias=True)\n        if location_attention:\n            self.location_layer = LocationLayer(\n                attention_dim,\n                attention_location_n_filters,\n                attention_location_kernel_size,\n            )\n        self._mask_value = -float(\"inf\")\n        self.windowing = windowing\n        self.win_idx = None\n        self.norm = norm\n        self.forward_attn = forward_attn\n        self.trans_agent = trans_agent\n        self.forward_attn_mask = forward_attn_mask\n        self.location_attention = location_attention\n\n    def init_win_idx(self):\n        self.win_idx = -1\n        self.win_back = 2\n        self.win_front = 6\n\n    def init_forward_attn(self, inputs):\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        self.alpha = torch.cat([torch.ones([B, 1]), torch.zeros([B, T])[:, :-1] + 1e-7], dim=1).to(inputs.device)\n        self.u = (0.5 * torch.ones([B, 1])).to(inputs.device)\n\n    def init_location_attention(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        self.attention_weights_cum = torch.zeros([B, T], device=inputs.device)\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        self.attention_weights = torch.zeros([B, T], device=inputs.device)\n        if self.location_attention:\n            self.init_location_attention(inputs)\n        if self.forward_attn:\n            self.init_forward_attn(inputs)\n        if self.windowing:\n            self.init_win_idx()\n\n    def preprocess_inputs(self, inputs):\n        return self.inputs_layer(inputs)\n\n    def update_location_attention(self, alignments):\n        self.attention_weights_cum += alignments\n\n    def get_location_attention(self, query, processed_inputs):\n        attention_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)\n        processed_query = self.query_layer(query.unsqueeze(1))\n        processed_attention_weights = self.location_layer(attention_cat)\n        energies = self.v(torch.tanh(processed_query + processed_attention_weights + processed_inputs))\n        energies = energies.squeeze(-1)\n        return energies, processed_query\n\n    def get_attention(self, query, processed_inputs):\n        processed_query = self.query_layer(query.unsqueeze(1))\n        energies = self.v(torch.tanh(processed_query + processed_inputs))\n        energies = energies.squeeze(-1)\n        return energies, processed_query\n\n    def apply_windowing(self, attention, inputs):\n        back_win = self.win_idx - self.win_back\n        front_win = self.win_idx + self.win_front\n        if back_win > 0:\n            attention[:, :back_win] = -float(\"inf\")\n        if front_win < inputs.shape[1]:\n            attention[:, front_win:] = -float(\"inf\")\n        # this is a trick to solve a special problem.\n        # but it does not hurt.\n        if self.win_idx == -1:\n            attention[:, 0] = attention.max()\n        # Update the window\n        self.win_idx = torch.argmax(attention, 1).long()[0].item()\n        return attention\n\n    def apply_forward_attention(self, alignment):\n        # forward attention\n        fwd_shifted_alpha = F.pad(self.alpha[:, :-1].clone().to(alignment.device), (1, 0, 0, 0))\n        # compute transition potentials\n        alpha = ((1 - self.u) * self.alpha + self.u * fwd_shifted_alpha + 1e-8) * alignment\n        # force incremental alignment\n        if not self.training and self.forward_attn_mask:\n            _, n = fwd_shifted_alpha.max(1)\n            val, _ = alpha.max(1)\n            for b in range(alignment.shape[0]):\n                alpha[b, n[b] + 3 :] = 0\n                alpha[b, : (n[b] - 1)] = 0  # ignore all previous states to prevent repetition.\n                alpha[b, (n[b] - 2)] = 0.01 * val[b]  # smoothing factor for the prev step\n        # renormalize attention weights\n        alpha = alpha / alpha.sum(dim=1, keepdim=True)\n        return alpha\n\n    def forward(self, query, inputs, processed_inputs, mask):\n        \"\"\"\n        shapes:\n            query: [B, C_attn_rnn]\n            inputs: [B, T_en, D_en]\n            processed_inputs: [B, T_en, D_attn]\n            mask: [B, T_en]\n        \"\"\"\n        if self.location_attention:\n            attention, _ = self.get_location_attention(query, processed_inputs)\n        else:\n            attention, _ = self.get_attention(query, processed_inputs)\n        # apply masking\n        if mask is not None:\n            attention.data.masked_fill_(~mask, self._mask_value)\n        # apply windowing - only in eval mode\n        if not self.training and self.windowing:\n            attention = self.apply_windowing(attention, inputs)\n\n        # normalize attention values\n        if self.norm == \"softmax\":\n            alignment = torch.softmax(attention, dim=-1)\n        elif self.norm == \"sigmoid\":\n            alignment = torch.sigmoid(attention) / torch.sigmoid(attention).sum(dim=1, keepdim=True)\n        else:\n            raise ValueError(\"Unknown value for attention norm type\")\n\n        if self.location_attention:\n            self.update_location_attention(alignment)\n\n        # apply forward attention if enabled\n        if self.forward_attn:\n            alignment = self.apply_forward_attention(alignment)\n            self.alpha = alignment\n\n        context = torch.bmm(alignment.unsqueeze(1), inputs)\n        context = context.squeeze(1)\n        self.attention_weights = alignment\n\n        # compute transition agent\n        if self.forward_attn and self.trans_agent:\n            ta_input = torch.cat([context, query.squeeze(1)], dim=-1)\n            self.u = torch.sigmoid(self.ta(ta_input))\n        return context\n\n\nclass MonotonicDynamicConvolutionAttention(nn.Module):\n    \"\"\"Dynamic convolution attention from\n    https://arxiv.org/pdf/1910.10288.pdf\n\n\n    query -> linear -> tanh -> linear ->|\n                                        |                                            mask values\n                                        v                                              |    |\n               atten_w(t-1) -|-> conv1d_dynamic -> linear -|-> tanh -> + -> softmax -> * -> * -> context\n                             |-> conv1d_static  -> linear -|           |\n                             |-> conv1d_prior   -> log ----------------|\n\n    query: attention rnn output.\n\n    Note:\n        Dynamic convolution attention is an alternation of the location senstive attention with\n    dynamically computed convolution filters from the previous attention scores and a set of\n    constraints to keep the attention alignment diagonal.\n        DCA is sensitive to mixed precision training and might cause instable training.\n\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the value tensor.\n        static_filter_dim (int): number of channels in the convolution layer computing the static filters.\n        static_kernel_size (int): kernel size for the convolution layer computing the static filters.\n        dynamic_filter_dim (int): number of channels in the convolution layer computing the dynamic filters.\n        dynamic_kernel_size (int): kernel size for the convolution layer computing the dynamic filters.\n        prior_filter_len (int, optional): [description]. Defaults to 11 from the paper.\n        alpha (float, optional): [description]. Defaults to 0.1 from the paper.\n        beta (float, optional): [description]. Defaults to 0.9 from the paper.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim,\n        embedding_dim,  # pylint: disable=unused-argument\n        attention_dim,\n        static_filter_dim,\n        static_kernel_size,\n        dynamic_filter_dim,\n        dynamic_kernel_size,\n        prior_filter_len=11,\n        alpha=0.1,\n        beta=0.9,\n    ):\n        super().__init__()\n        self._mask_value = 1e-8\n        self.dynamic_filter_dim = dynamic_filter_dim\n        self.dynamic_kernel_size = dynamic_kernel_size\n        self.prior_filter_len = prior_filter_len\n        self.attention_weights = None\n        # setup key and query layers\n        self.query_layer = nn.Linear(query_dim, attention_dim)\n        self.key_layer = nn.Linear(attention_dim, dynamic_filter_dim * dynamic_kernel_size, bias=False)\n        self.static_filter_conv = nn.Conv1d(\n            1,\n            static_filter_dim,\n            static_kernel_size,\n            padding=(static_kernel_size - 1) // 2,\n            bias=False,\n        )\n        self.static_filter_layer = nn.Linear(static_filter_dim, attention_dim, bias=False)\n        self.dynamic_filter_layer = nn.Linear(dynamic_filter_dim, attention_dim)\n        self.v = nn.Linear(attention_dim, 1, bias=False)\n\n        prior = betabinom.pmf(range(prior_filter_len), prior_filter_len - 1, alpha, beta)\n        self.register_buffer(\"prior\", torch.FloatTensor(prior).flip(0))\n\n    # pylint: disable=unused-argument\n    def forward(self, query, inputs, processed_inputs, mask):\n        \"\"\"\n        query: [B, C_attn_rnn]\n        inputs: [B, T_en, D_en]\n        processed_inputs: place holder.\n        mask: [B, T_en]\n        \"\"\"\n        # compute prior filters\n        prior_filter = F.conv1d(\n            F.pad(self.attention_weights.unsqueeze(1), (self.prior_filter_len - 1, 0)), self.prior.view(1, 1, -1)\n        )\n        prior_filter = torch.log(prior_filter.clamp_min_(1e-6)).squeeze(1)\n        G = self.key_layer(torch.tanh(self.query_layer(query)))\n        # compute dynamic filters\n        dynamic_filter = F.conv1d(\n            self.attention_weights.unsqueeze(0),\n            G.view(-1, 1, self.dynamic_kernel_size),\n            padding=(self.dynamic_kernel_size - 1) // 2,\n            groups=query.size(0),\n        )\n        dynamic_filter = dynamic_filter.view(query.size(0), self.dynamic_filter_dim, -1).transpose(1, 2)\n        # compute static filters\n        static_filter = self.static_filter_conv(self.attention_weights.unsqueeze(1)).transpose(1, 2)\n        alignment = (\n            self.v(\n                torch.tanh(self.static_filter_layer(static_filter) + self.dynamic_filter_layer(dynamic_filter))\n            ).squeeze(-1)\n            + prior_filter\n        )\n        # compute attention weights\n        attention_weights = F.softmax(alignment, dim=-1)\n        # apply masking\n        if mask is not None:\n            attention_weights.data.masked_fill_(~mask, self._mask_value)\n        self.attention_weights = attention_weights\n        # compute context\n        context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n        return context\n\n    def preprocess_inputs(self, inputs):  # pylint: disable=no-self-use\n        return None\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        self.attention_weights = torch.zeros([B, T], device=inputs.device)\n        self.attention_weights[:, 0] = 1.0\n\n\ndef init_attn(\n    attn_type,\n    query_dim,\n    embedding_dim,\n    attention_dim,\n    location_attention,\n    attention_location_n_filters,\n    attention_location_kernel_size,\n    windowing,\n    norm,\n    forward_attn,\n    trans_agent,\n    forward_attn_mask,\n    attn_K,\n):\n    if attn_type == \"original\":\n        return OriginalAttention(\n            query_dim,\n            embedding_dim,\n            attention_dim,\n            location_attention,\n            attention_location_n_filters,\n            attention_location_kernel_size,\n            windowing,\n            norm,\n            forward_attn,\n            trans_agent,\n            forward_attn_mask,\n        )\n    if attn_type == \"graves\":\n        return GravesAttention(query_dim, attn_K)\n    if attn_type == \"dynamic_convolution\":\n        return MonotonicDynamicConvolutionAttention(\n            query_dim,\n            embedding_dim,\n            attention_dim,\n            static_filter_dim=8,\n            static_kernel_size=21,\n            dynamic_filter_dim=8,\n            dynamic_kernel_size=21,\n            prior_filter_len=11,\n            alpha=0.1,\n            beta=0.9,\n        )\n\n    raise RuntimeError(f\" [!] Given Attention Type '{attn_type}' is not exist.\")\n", "TTS/tts/layers/tacotron/tacotron.py": "# coding: utf-8\n# adapted from https://github.com/r9y9/tacotron_pytorch\n\nimport torch\nfrom torch import nn\n\nfrom .attentions import init_attn\nfrom .common_layers import Prenet\n\n\nclass BatchNormConv1d(nn.Module):\n    r\"\"\"A wrapper for Conv1d with BatchNorm. It sets the activation\n    function between Conv and BatchNorm layers. BatchNorm layer\n    is initialized with the TF default values for momentum and eps.\n\n    Args:\n        in_channels: size of each input sample\n        out_channels: size of each output samples\n        kernel_size: kernel size of conv filters\n        stride: stride of conv filters\n        padding: padding of conv filters\n        activation: activation function set b/w Conv1d and BatchNorm\n\n    Shapes:\n        - input: (B, D)\n        - output: (B, D)\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, activation=None):\n        super().__init__()\n        self.padding = padding\n        self.padder = nn.ConstantPad1d(padding, 0)\n        self.conv1d = nn.Conv1d(\n            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0, bias=False\n        )\n        # Following tensorflow's default parameters\n        self.bn = nn.BatchNorm1d(out_channels, momentum=0.99, eps=1e-3)\n        self.activation = activation\n        # self.init_layers()\n\n    def init_layers(self):\n        if isinstance(self.activation, torch.nn.ReLU):\n            w_gain = \"relu\"\n        elif isinstance(self.activation, torch.nn.Tanh):\n            w_gain = \"tanh\"\n        elif self.activation is None:\n            w_gain = \"linear\"\n        else:\n            raise RuntimeError(\"Unknown activation function\")\n        torch.nn.init.xavier_uniform_(self.conv1d.weight, gain=torch.nn.init.calculate_gain(w_gain))\n\n    def forward(self, x):\n        x = self.padder(x)\n        x = self.conv1d(x)\n        x = self.bn(x)\n        if self.activation is not None:\n            x = self.activation(x)\n        return x\n\n\nclass Highway(nn.Module):\n    r\"\"\"Highway layers as explained in https://arxiv.org/abs/1505.00387\n\n    Args:\n        in_features (int): size of each input sample\n        out_feature (int): size of each output sample\n\n    Shapes:\n        - input: (B, *, H_in)\n        - output: (B, *, H_out)\n    \"\"\"\n\n    # TODO: Try GLU layer\n    def __init__(self, in_features, out_feature):\n        super().__init__()\n        self.H = nn.Linear(in_features, out_feature)\n        self.H.bias.data.zero_()\n        self.T = nn.Linear(in_features, out_feature)\n        self.T.bias.data.fill_(-1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        # self.init_layers()\n\n    def init_layers(self):\n        torch.nn.init.xavier_uniform_(self.H.weight, gain=torch.nn.init.calculate_gain(\"relu\"))\n        torch.nn.init.xavier_uniform_(self.T.weight, gain=torch.nn.init.calculate_gain(\"sigmoid\"))\n\n    def forward(self, inputs):\n        H = self.relu(self.H(inputs))\n        T = self.sigmoid(self.T(inputs))\n        return H * T + inputs * (1.0 - T)\n\n\nclass CBHG(nn.Module):\n    \"\"\"CBHG module: a recurrent neural network composed of:\n    - 1-d convolution banks\n    - Highway networks + residual connections\n    - Bidirectional gated recurrent units\n\n    Args:\n        in_features (int): sample size\n        K (int): max filter size in conv bank\n        projections (list): conv channel sizes for conv projections\n        num_highways (int): number of highways layers\n\n    Shapes:\n        - input: (B, C, T_in)\n        - output: (B, T_in, C*2)\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_features,\n        K=16,\n        conv_bank_features=128,\n        conv_projections=[128, 128],\n        highway_features=128,\n        gru_features=128,\n        num_highways=4,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.conv_bank_features = conv_bank_features\n        self.highway_features = highway_features\n        self.gru_features = gru_features\n        self.conv_projections = conv_projections\n        self.relu = nn.ReLU()\n        # list of conv1d bank with filter size k=1...K\n        # TODO: try dilational layers instead\n        self.conv1d_banks = nn.ModuleList(\n            [\n                BatchNormConv1d(\n                    in_features,\n                    conv_bank_features,\n                    kernel_size=k,\n                    stride=1,\n                    padding=[(k - 1) // 2, k // 2],\n                    activation=self.relu,\n                )\n                for k in range(1, K + 1)\n            ]\n        )\n        # max pooling of conv bank, with padding\n        # TODO: try average pooling OR larger kernel size\n        out_features = [K * conv_bank_features] + conv_projections[:-1]\n        activations = [self.relu] * (len(conv_projections) - 1)\n        activations += [None]\n        # setup conv1d projection layers\n        layer_set = []\n        for in_size, out_size, ac in zip(out_features, conv_projections, activations):\n            layer = BatchNormConv1d(in_size, out_size, kernel_size=3, stride=1, padding=[1, 1], activation=ac)\n            layer_set.append(layer)\n        self.conv1d_projections = nn.ModuleList(layer_set)\n        # setup Highway layers\n        if self.highway_features != conv_projections[-1]:\n            self.pre_highway = nn.Linear(conv_projections[-1], highway_features, bias=False)\n        self.highways = nn.ModuleList([Highway(highway_features, highway_features) for _ in range(num_highways)])\n        # bi-directional GPU layer\n        self.gru = nn.GRU(gru_features, gru_features, 1, batch_first=True, bidirectional=True)\n\n    def forward(self, inputs):\n        # (B, in_features, T_in)\n        x = inputs\n        # (B, hid_features*K, T_in)\n        # Concat conv1d bank outputs\n        outs = []\n        for conv1d in self.conv1d_banks:\n            out = conv1d(x)\n            outs.append(out)\n        x = torch.cat(outs, dim=1)\n        assert x.size(1) == self.conv_bank_features * len(self.conv1d_banks)\n        for conv1d in self.conv1d_projections:\n            x = conv1d(x)\n        x += inputs\n        x = x.transpose(1, 2)\n        if self.highway_features != self.conv_projections[-1]:\n            x = self.pre_highway(x)\n        # Residual connection\n        # TODO: try residual scaling as in Deep Voice 3\n        # TODO: try plain residual layers\n        for highway in self.highways:\n            x = highway(x)\n        # (B, T_in, hid_features*2)\n        # TODO: replace GRU with convolution as in Deep Voice 3\n        self.gru.flatten_parameters()\n        outputs, _ = self.gru(x)\n        return outputs\n\n\nclass EncoderCBHG(nn.Module):\n    r\"\"\"CBHG module with Encoder specific arguments\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.cbhg = CBHG(\n            128,\n            K=16,\n            conv_bank_features=128,\n            conv_projections=[128, 128],\n            highway_features=128,\n            gru_features=128,\n            num_highways=4,\n        )\n\n    def forward(self, x):\n        return self.cbhg(x)\n\n\nclass Encoder(nn.Module):\n    r\"\"\"Stack Prenet and CBHG module for encoder\n    Args:\n        inputs (FloatTensor): embedding features\n\n    Shapes:\n        - inputs: (B, T, D_in)\n        - outputs: (B, T, 128 * 2)\n    \"\"\"\n\n    def __init__(self, in_features):\n        super().__init__()\n        self.prenet = Prenet(in_features, out_features=[256, 128])\n        self.cbhg = EncoderCBHG()\n\n    def forward(self, inputs):\n        # B x T x prenet_dim\n        outputs = self.prenet(inputs)\n        outputs = self.cbhg(outputs.transpose(1, 2))\n        return outputs\n\n\nclass PostCBHG(nn.Module):\n    def __init__(self, mel_dim):\n        super().__init__()\n        self.cbhg = CBHG(\n            mel_dim,\n            K=8,\n            conv_bank_features=128,\n            conv_projections=[256, mel_dim],\n            highway_features=128,\n            gru_features=128,\n            num_highways=4,\n        )\n\n    def forward(self, x):\n        return self.cbhg(x)\n\n\nclass Decoder(nn.Module):\n    \"\"\"Tacotron decoder.\n\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        r (int): number of outputs per time step (reduction rate).\n        memory_size (int): size of the past window. if <= 0 memory_size = r\n        attn_type (string): type of attention used in decoder.\n        attn_windowing (bool): if true, define an attention window centered to maximum\n            attention response. It provides more robust attention alignment especially\n            at interence time.\n        attn_norm (string): attention normalization function. 'sigmoid' or 'softmax'.\n        prenet_type (string): 'original' or 'bn'.\n        prenet_dropout (float): prenet dropout rate.\n        forward_attn (bool): if true, use forward attention method. https://arxiv.org/abs/1807.06736\n        trans_agent (bool): if true, use transition agent. https://arxiv.org/abs/1807.06736\n        forward_attn_mask (bool): if true, mask attention values smaller than a threshold.\n        location_attn (bool): if true, use location sensitive attention.\n        attn_K (int): number of attention heads for GravesAttention.\n        separate_stopnet (bool): if true, detach stopnet input to prevent gradient flow.\n        d_vector_dim (int): size of speaker embedding vector, for multi-speaker training.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 500.\n    \"\"\"\n\n    # Pylint gets confused by PyTorch conventions here\n    # pylint: disable=attribute-defined-outside-init\n\n    def __init__(\n        self,\n        in_channels,\n        frame_channels,\n        r,\n        memory_size,\n        attn_type,\n        attn_windowing,\n        attn_norm,\n        prenet_type,\n        prenet_dropout,\n        forward_attn,\n        trans_agent,\n        forward_attn_mask,\n        location_attn,\n        attn_K,\n        separate_stopnet,\n        max_decoder_steps,\n    ):\n        super().__init__()\n        self.r_init = r\n        self.r = r\n        self.in_channels = in_channels\n        self.max_decoder_steps = max_decoder_steps\n        self.use_memory_queue = memory_size > 0\n        self.memory_size = memory_size if memory_size > 0 else r\n        self.frame_channels = frame_channels\n        self.separate_stopnet = separate_stopnet\n        self.query_dim = 256\n        # memory -> |Prenet| -> processed_memory\n        prenet_dim = frame_channels * self.memory_size if self.use_memory_queue else frame_channels\n        self.prenet = Prenet(prenet_dim, prenet_type, prenet_dropout, out_features=[256, 128])\n        # processed_inputs, processed_memory -> |Attention| -> Attention, attention, RNN_State\n        # attention_rnn generates queries for the attention mechanism\n        self.attention_rnn = nn.GRUCell(in_channels + 128, self.query_dim)\n        self.attention = init_attn(\n            attn_type=attn_type,\n            query_dim=self.query_dim,\n            embedding_dim=in_channels,\n            attention_dim=128,\n            location_attention=location_attn,\n            attention_location_n_filters=32,\n            attention_location_kernel_size=31,\n            windowing=attn_windowing,\n            norm=attn_norm,\n            forward_attn=forward_attn,\n            trans_agent=trans_agent,\n            forward_attn_mask=forward_attn_mask,\n            attn_K=attn_K,\n        )\n        # (processed_memory | attention context) -> |Linear| -> decoder_RNN_input\n        self.project_to_decoder_in = nn.Linear(256 + in_channels, 256)\n        # decoder_RNN_input -> |RNN| -> RNN_state\n        self.decoder_rnns = nn.ModuleList([nn.GRUCell(256, 256) for _ in range(2)])\n        # RNN_state -> |Linear| -> mel_spec\n        self.proj_to_mel = nn.Linear(256, frame_channels * self.r_init)\n        # learn init values instead of zero init.\n        self.stopnet = StopNet(256 + frame_channels * self.r_init)\n\n    def set_r(self, new_r):\n        self.r = new_r\n\n    def _reshape_memory(self, memory):\n        \"\"\"\n        Reshape the spectrograms for given 'r'\n        \"\"\"\n        # Grouping multiple frames if necessary\n        if memory.size(-1) == self.frame_channels:\n            memory = memory.view(memory.shape[0], memory.size(1) // self.r, -1)\n        # Time first (T_decoder, B, frame_channels)\n        memory = memory.transpose(0, 1)\n        return memory\n\n    def _init_states(self, inputs):\n        \"\"\"\n        Initialization of decoder states\n        \"\"\"\n        B = inputs.size(0)\n        # go frame as zeros matrix\n        if self.use_memory_queue:\n            self.memory_input = torch.zeros(1, device=inputs.device).repeat(B, self.frame_channels * self.memory_size)\n        else:\n            self.memory_input = torch.zeros(1, device=inputs.device).repeat(B, self.frame_channels)\n        # decoder states\n        self.attention_rnn_hidden = torch.zeros(1, device=inputs.device).repeat(B, 256)\n        self.decoder_rnn_hiddens = [\n            torch.zeros(1, device=inputs.device).repeat(B, 256) for idx in range(len(self.decoder_rnns))\n        ]\n        self.context_vec = inputs.data.new(B, self.in_channels).zero_()\n        # cache attention inputs\n        self.processed_inputs = self.attention.preprocess_inputs(inputs)\n\n    def _parse_outputs(self, outputs, attentions, stop_tokens):\n        # Back to batch first\n        attentions = torch.stack(attentions).transpose(0, 1)\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        outputs = outputs.view(outputs.size(0), -1, self.frame_channels)\n        outputs = outputs.transpose(1, 2)\n        return outputs, attentions, stop_tokens\n\n    def decode(self, inputs, mask=None):\n        # Prenet\n        processed_memory = self.prenet(self.memory_input)\n        # Attention RNN\n        self.attention_rnn_hidden = self.attention_rnn(\n            torch.cat((processed_memory, self.context_vec), -1), self.attention_rnn_hidden\n        )\n        self.context_vec = self.attention(self.attention_rnn_hidden, inputs, self.processed_inputs, mask)\n        # Concat RNN output and attention context vector\n        decoder_input = self.project_to_decoder_in(torch.cat((self.attention_rnn_hidden, self.context_vec), -1))\n\n        # Pass through the decoder RNNs\n        for idx, decoder_rnn in enumerate(self.decoder_rnns):\n            self.decoder_rnn_hiddens[idx] = decoder_rnn(decoder_input, self.decoder_rnn_hiddens[idx])\n            # Residual connection\n            decoder_input = self.decoder_rnn_hiddens[idx] + decoder_input\n        decoder_output = decoder_input\n\n        # predict mel vectors from decoder vectors\n        output = self.proj_to_mel(decoder_output)\n        # output = torch.sigmoid(output)\n        # predict stop token\n        stopnet_input = torch.cat([decoder_output, output], -1)\n        if self.separate_stopnet:\n            stop_token = self.stopnet(stopnet_input.detach())\n        else:\n            stop_token = self.stopnet(stopnet_input)\n        output = output[:, : self.r * self.frame_channels]\n        return output, stop_token, self.attention.attention_weights\n\n    def _update_memory_input(self, new_memory):\n        if self.use_memory_queue:\n            if self.memory_size > self.r:\n                # memory queue size is larger than number of frames per decoder iter\n                self.memory_input = torch.cat(\n                    [new_memory, self.memory_input[:, : (self.memory_size - self.r) * self.frame_channels].clone()],\n                    dim=-1,\n                )\n            else:\n                # memory queue size smaller than number of frames per decoder iter\n                self.memory_input = new_memory[:, : self.memory_size * self.frame_channels]\n        else:\n            # use only the last frame prediction\n            # assert new_memory.shape[-1] == self.r * self.frame_channels\n            self.memory_input = new_memory[:, self.frame_channels * (self.r - 1) :]\n\n    def forward(self, inputs, memory, mask):\n        \"\"\"\n        Args:\n            inputs: Encoder outputs.\n            memory: Decoder memory (autoregression. If None (at eval-time),\n              decoder outputs are used as decoder inputs. If None, it uses the last\n              output as the input.\n            mask: Attention mask for sequence padding.\n\n        Shapes:\n            - inputs: (B, T, D_out_enc)\n            - memory: (B, T_mel, D_mel)\n        \"\"\"\n        # Run greedy decoding if memory is None\n        memory = self._reshape_memory(memory)\n        outputs = []\n        attentions = []\n        stop_tokens = []\n        t = 0\n        self._init_states(inputs)\n        self.attention.init_states(inputs)\n        while len(outputs) < memory.size(0):\n            if t > 0:\n                new_memory = memory[t - 1]\n                self._update_memory_input(new_memory)\n\n            output, stop_token, attention = self.decode(inputs, mask)\n            outputs += [output]\n            attentions += [attention]\n            stop_tokens += [stop_token.squeeze(1)]\n            t += 1\n        return self._parse_outputs(outputs, attentions, stop_tokens)\n\n    def inference(self, inputs):\n        \"\"\"\n        Args:\n            inputs: encoder outputs.\n        Shapes:\n            - inputs: batch x time x encoder_out_dim\n        \"\"\"\n        outputs = []\n        attentions = []\n        stop_tokens = []\n        t = 0\n        self._init_states(inputs)\n        self.attention.init_states(inputs)\n        while True:\n            if t > 0:\n                new_memory = outputs[-1]\n                self._update_memory_input(new_memory)\n            output, stop_token, attention = self.decode(inputs, None)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [output]\n            attentions += [attention]\n            stop_tokens += [stop_token]\n            t += 1\n            if t > inputs.shape[1] / 4 and (stop_token > 0.6 or attention[:, -1].item() > 0.6):\n                break\n            if t > self.max_decoder_steps:\n                print(\"   | > Decoder stopped with 'max_decoder_steps\")\n                break\n        return self._parse_outputs(outputs, attentions, stop_tokens)\n\n\nclass StopNet(nn.Module):\n    r\"\"\"Stopnet signalling decoder to stop inference.\n    Args:\n        in_features (int): feature dimension of input.\n    \"\"\"\n\n    def __init__(self, in_features):\n        super().__init__()\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(in_features, 1)\n        torch.nn.init.xavier_uniform_(self.linear.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n\n    def forward(self, inputs):\n        outputs = self.dropout(inputs)\n        outputs = self.linear(outputs)\n        return outputs\n", "TTS/tts/layers/tacotron/capacitron_layers.py": "import torch\nfrom torch import nn\nfrom torch.distributions.multivariate_normal import MultivariateNormal as MVN\nfrom torch.nn import functional as F\n\n\nclass CapacitronVAE(nn.Module):\n    \"\"\"Effective Use of Variational Embedding Capacity for prosody transfer.\n\n    See https://arxiv.org/abs/1906.03402\"\"\"\n\n    def __init__(\n        self,\n        num_mel,\n        capacitron_VAE_embedding_dim,\n        encoder_output_dim=256,\n        reference_encoder_out_dim=128,\n        speaker_embedding_dim=None,\n        text_summary_embedding_dim=None,\n    ):\n        super().__init__()\n        # Init distributions\n        self.prior_distribution = MVN(\n            torch.zeros(capacitron_VAE_embedding_dim), torch.eye(capacitron_VAE_embedding_dim)\n        )\n        self.approximate_posterior_distribution = None\n        # define output ReferenceEncoder dim to the capacitron_VAE_embedding_dim\n        self.encoder = ReferenceEncoder(num_mel, out_dim=reference_encoder_out_dim)\n\n        # Init beta, the lagrange-like term for the KL distribution\n        self.beta = torch.nn.Parameter(torch.log(torch.exp(torch.Tensor([1.0])) - 1), requires_grad=True)\n        mlp_input_dimension = reference_encoder_out_dim\n\n        if text_summary_embedding_dim is not None:\n            self.text_summary_net = TextSummary(text_summary_embedding_dim, encoder_output_dim=encoder_output_dim)\n            mlp_input_dimension += text_summary_embedding_dim\n        if speaker_embedding_dim is not None:\n            # TODO: Test a multispeaker model!\n            mlp_input_dimension += speaker_embedding_dim\n        self.post_encoder_mlp = PostEncoderMLP(mlp_input_dimension, capacitron_VAE_embedding_dim)\n\n    def forward(self, reference_mel_info=None, text_info=None, speaker_embedding=None):\n        # Use reference\n        if reference_mel_info is not None:\n            reference_mels = reference_mel_info[0]  # [batch_size, num_frames, num_mels]\n            mel_lengths = reference_mel_info[1]  # [batch_size]\n            enc_out = self.encoder(reference_mels, mel_lengths)\n\n            # concat speaker_embedding and/or text summary embedding\n            if text_info is not None:\n                text_inputs = text_info[0]  # [batch_size, num_characters, num_embedding]\n                input_lengths = text_info[1]\n                text_summary_out = self.text_summary_net(text_inputs, input_lengths).to(reference_mels.device)\n                enc_out = torch.cat([enc_out, text_summary_out], dim=-1)\n            if speaker_embedding is not None:\n                speaker_embedding = torch.squeeze(speaker_embedding)\n                enc_out = torch.cat([enc_out, speaker_embedding], dim=-1)\n\n            # Feed the output of the ref encoder and information about text/speaker into\n            # an MLP to produce the parameteres for the approximate poterior distributions\n            mu, sigma = self.post_encoder_mlp(enc_out)\n            # convert to cpu because prior_distribution was created on cpu\n            mu = mu.cpu()\n            sigma = sigma.cpu()\n\n            # Sample from the posterior: z ~ q(z|x)\n            self.approximate_posterior_distribution = MVN(mu, torch.diag_embed(sigma))\n            VAE_embedding = self.approximate_posterior_distribution.rsample()\n        # Infer from the model, bypasses encoding\n        else:\n            # Sample from the prior: z ~ p(z)\n            VAE_embedding = self.prior_distribution.sample().unsqueeze(0)\n\n        # reshape to [batch_size, 1, capacitron_VAE_embedding_dim]\n        return VAE_embedding.unsqueeze(1), self.approximate_posterior_distribution, self.prior_distribution, self.beta\n\n\nclass ReferenceEncoder(nn.Module):\n    \"\"\"NN module creating a fixed size prosody embedding from a spectrogram.\n\n    inputs: mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    \"\"\"\n\n    def __init__(self, num_mel, out_dim):\n        super().__init__()\n        self.num_mel = num_mel\n        filters = [1] + [32, 32, 64, 64, 128, 128]\n        num_layers = len(filters) - 1\n        convs = [\n            nn.Conv2d(\n                in_channels=filters[i], out_channels=filters[i + 1], kernel_size=(3, 3), stride=(2, 2), padding=(2, 2)\n            )\n            for i in range(num_layers)\n        ]\n        self.convs = nn.ModuleList(convs)\n        self.training = False\n        self.bns = nn.ModuleList([nn.BatchNorm2d(num_features=filter_size) for filter_size in filters[1:]])\n\n        post_conv_height = self.calculate_post_conv_height(num_mel, 3, 2, 2, num_layers)\n        self.recurrence = nn.LSTM(\n            input_size=filters[-1] * post_conv_height, hidden_size=out_dim, batch_first=True, bidirectional=False\n        )\n\n    def forward(self, inputs, input_lengths):\n        batch_size = inputs.size(0)\n        x = inputs.view(batch_size, 1, -1, self.num_mel)  # [batch_size, num_channels==1, num_frames, num_mel]\n        valid_lengths = input_lengths.float()  # [batch_size]\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x)\n            x = bn(x)\n            x = F.relu(x)\n\n            # Create the post conv width mask based on the valid lengths of the output of the convolution.\n            # The valid lengths for the output of a convolution on varying length inputs is\n            # ceil(input_length/stride) + 1 for stride=3 and padding=2\n            # For example (kernel_size=3, stride=2, padding=2):\n            # 0 0 x x x x x 0 0 -> Input = 5, 0 is zero padding, x is valid values coming from padding=2 in conv2d\n            # _____\n            #   x _____\n            #       x _____\n            #           x  ____\n            #               x\n            # x x x x -> Output valid length = 4\n            # Since every example in te batch is zero padded and therefore have separate valid_lengths,\n            # we need to mask off all the values AFTER the valid length for each example in the batch.\n            # Otherwise, the convolutions create noise and a lot of not real information\n            valid_lengths = (valid_lengths / 2).float()\n            valid_lengths = torch.ceil(valid_lengths).to(dtype=torch.int64) + 1  # 2 is stride -- size: [batch_size]\n            post_conv_max_width = x.size(2)\n\n            mask = torch.arange(post_conv_max_width).to(inputs.device).expand(\n                len(valid_lengths), post_conv_max_width\n            ) < valid_lengths.unsqueeze(1)\n            mask = mask.expand(1, 1, -1, -1).transpose(2, 0).transpose(-1, 2)  # [batch_size, 1, post_conv_max_width, 1]\n            x = x * mask\n\n        x = x.transpose(1, 2)\n        # x: 4D tensor [batch_size, post_conv_width,\n        #               num_channels==128, post_conv_height]\n\n        post_conv_width = x.size(1)\n        x = x.contiguous().view(batch_size, post_conv_width, -1)\n        # x: 3D tensor [batch_size, post_conv_width,\n        #               num_channels*post_conv_height]\n\n        # Routine for fetching the last valid output of a dynamic LSTM with varying input lengths and padding\n        post_conv_input_lengths = valid_lengths\n        packed_seqs = nn.utils.rnn.pack_padded_sequence(\n            x, post_conv_input_lengths.tolist(), batch_first=True, enforce_sorted=False\n        )  # dynamic rnn sequence padding\n        self.recurrence.flatten_parameters()\n        _, (ht, _) = self.recurrence(packed_seqs)\n        last_output = ht[-1]\n\n        return last_output.to(inputs.device)  # [B, 128]\n\n    @staticmethod\n    def calculate_post_conv_height(height, kernel_size, stride, pad, n_convs):\n        \"\"\"Height of spec after n convolutions with fixed kernel/stride/pad.\"\"\"\n        for _ in range(n_convs):\n            height = (height - kernel_size + 2 * pad) // stride + 1\n        return height\n\n\nclass TextSummary(nn.Module):\n    def __init__(self, embedding_dim, encoder_output_dim):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            encoder_output_dim,  # text embedding dimension from the text encoder\n            embedding_dim,  # fixed length output summary the lstm creates from the input\n            batch_first=True,\n            bidirectional=False,\n        )\n\n    def forward(self, inputs, input_lengths):\n        # Routine for fetching the last valid output of a dynamic LSTM with varying input lengths and padding\n        packed_seqs = nn.utils.rnn.pack_padded_sequence(\n            inputs, input_lengths.tolist(), batch_first=True, enforce_sorted=False\n        )  # dynamic rnn sequence padding\n        self.lstm.flatten_parameters()\n        _, (ht, _) = self.lstm(packed_seqs)\n        last_output = ht[-1]\n        return last_output\n\n\nclass PostEncoderMLP(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        modules = [\n            nn.Linear(input_size, hidden_size),  # Hidden Layer\n            nn.Tanh(),\n            nn.Linear(hidden_size, hidden_size * 2),\n        ]  # Output layer twice the size for mean and variance\n        self.net = nn.Sequential(*modules)\n        self.softplus = nn.Softplus()\n\n    def forward(self, _input):\n        mlp_output = self.net(_input)\n        # The mean parameter is unconstrained\n        mu = mlp_output[:, : self.hidden_size]\n        # The standard deviation must be positive. Parameterise with a softplus\n        sigma = self.softplus(mlp_output[:, self.hidden_size :])\n        return mu, sigma\n", "TTS/tts/layers/tacotron/__init__.py": "", "TTS/tts/layers/tacotron/tacotron2.py": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .attentions import init_attn\nfrom .common_layers import Linear, Prenet\n\n\n# pylint: disable=no-value-for-parameter\n# pylint: disable=unexpected-keyword-arg\nclass ConvBNBlock(nn.Module):\n    r\"\"\"Convolutions with Batch Normalization and non-linear activation.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): convolution kernel size.\n        activation (str): 'relu', 'tanh', None (linear).\n\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_out, T)\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, activation=None):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0\n        padding = (kernel_size - 1) // 2\n        self.convolution1d = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n        self.batch_normalization = nn.BatchNorm1d(out_channels, momentum=0.1, eps=1e-5)\n        self.dropout = nn.Dropout(p=0.5)\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.Identity()\n\n    def forward(self, x):\n        o = self.convolution1d(x)\n        o = self.batch_normalization(o)\n        o = self.activation(o)\n        o = self.dropout(o)\n        return o\n\n\nclass Postnet(nn.Module):\n    r\"\"\"Tacotron2 Postnet\n\n    Args:\n        in_out_channels (int): number of output channels.\n\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_in, T)\n    \"\"\"\n\n    def __init__(self, in_out_channels, num_convs=5):\n        super().__init__()\n        self.convolutions = nn.ModuleList()\n        self.convolutions.append(ConvBNBlock(in_out_channels, 512, kernel_size=5, activation=\"tanh\"))\n        for _ in range(1, num_convs - 1):\n            self.convolutions.append(ConvBNBlock(512, 512, kernel_size=5, activation=\"tanh\"))\n        self.convolutions.append(ConvBNBlock(512, in_out_channels, kernel_size=5, activation=None))\n\n    def forward(self, x):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        return o\n\n\nclass Encoder(nn.Module):\n    r\"\"\"Tacotron2 Encoder\n\n    Args:\n        in_out_channels (int): number of input and output channels.\n\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_in, T)\n    \"\"\"\n\n    def __init__(self, in_out_channels=512):\n        super().__init__()\n        self.convolutions = nn.ModuleList()\n        for _ in range(3):\n            self.convolutions.append(ConvBNBlock(in_out_channels, in_out_channels, 5, \"relu\"))\n        self.lstm = nn.LSTM(\n            in_out_channels, int(in_out_channels / 2), num_layers=1, batch_first=True, bias=True, bidirectional=True\n        )\n        self.rnn_state = None\n\n    def forward(self, x, input_lengths):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        o = nn.utils.rnn.pack_padded_sequence(o, input_lengths.cpu(), batch_first=True)\n        self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n        return o\n\n    def inference(self, x):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        # self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        return o\n\n\n# adapted from https://github.com/NVIDIA/tacotron2/\nclass Decoder(nn.Module):\n    \"\"\"Tacotron2 decoder. We don't use Zoneout but Dropout between RNN layers.\n\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        r (int): number of outputs per time step (reduction rate).\n        memory_size (int): size of the past window. if <= 0 memory_size = r\n        attn_type (string): type of attention used in decoder.\n        attn_win (bool): if true, define an attention window centered to maximum\n            attention response. It provides more robust attention alignment especially\n            at interence time.\n        attn_norm (string): attention normalization function. 'sigmoid' or 'softmax'.\n        prenet_type (string): 'original' or 'bn'.\n        prenet_dropout (float): prenet dropout rate.\n        forward_attn (bool): if true, use forward attention method. https://arxiv.org/abs/1807.06736\n        trans_agent (bool): if true, use transition agent. https://arxiv.org/abs/1807.06736\n        forward_attn_mask (bool): if true, mask attention values smaller than a threshold.\n        location_attn (bool): if true, use location sensitive attention.\n        attn_K (int): number of attention heads for GravesAttention.\n        separate_stopnet (bool): if true, detach stopnet input to prevent gradient flow.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n    \"\"\"\n\n    # Pylint gets confused by PyTorch conventions here\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(\n        self,\n        in_channels,\n        frame_channels,\n        r,\n        attn_type,\n        attn_win,\n        attn_norm,\n        prenet_type,\n        prenet_dropout,\n        forward_attn,\n        trans_agent,\n        forward_attn_mask,\n        location_attn,\n        attn_K,\n        separate_stopnet,\n        max_decoder_steps,\n    ):\n        super().__init__()\n        self.frame_channels = frame_channels\n        self.r_init = r\n        self.r = r\n        self.encoder_embedding_dim = in_channels\n        self.separate_stopnet = separate_stopnet\n        self.max_decoder_steps = max_decoder_steps\n        self.stop_threshold = 0.5\n\n        # model dimensions\n        self.query_dim = 1024\n        self.decoder_rnn_dim = 1024\n        self.prenet_dim = 256\n        self.attn_dim = 128\n        self.p_attention_dropout = 0.1\n        self.p_decoder_dropout = 0.1\n\n        # memory -> |Prenet| -> processed_memory\n        prenet_dim = self.frame_channels\n        self.prenet = Prenet(\n            prenet_dim, prenet_type, prenet_dropout, out_features=[self.prenet_dim, self.prenet_dim], bias=False\n        )\n\n        self.attention_rnn = nn.LSTMCell(self.prenet_dim + in_channels, self.query_dim, bias=True)\n\n        self.attention = init_attn(\n            attn_type=attn_type,\n            query_dim=self.query_dim,\n            embedding_dim=in_channels,\n            attention_dim=128,\n            location_attention=location_attn,\n            attention_location_n_filters=32,\n            attention_location_kernel_size=31,\n            windowing=attn_win,\n            norm=attn_norm,\n            forward_attn=forward_attn,\n            trans_agent=trans_agent,\n            forward_attn_mask=forward_attn_mask,\n            attn_K=attn_K,\n        )\n\n        self.decoder_rnn = nn.LSTMCell(self.query_dim + in_channels, self.decoder_rnn_dim, bias=True)\n\n        self.linear_projection = Linear(self.decoder_rnn_dim + in_channels, self.frame_channels * self.r_init)\n\n        self.stopnet = nn.Sequential(\n            nn.Dropout(0.1),\n            Linear(self.decoder_rnn_dim + self.frame_channels * self.r_init, 1, bias=True, init_gain=\"sigmoid\"),\n        )\n        self.memory_truncated = None\n\n    def set_r(self, new_r):\n        self.r = new_r\n\n    def get_go_frame(self, inputs):\n        B = inputs.size(0)\n        memory = torch.zeros(1, device=inputs.device).repeat(B, self.frame_channels * self.r)\n        return memory\n\n    def _init_states(self, inputs, mask, keep_states=False):\n        B = inputs.size(0)\n        # T = inputs.size(1)\n        if not keep_states:\n            self.query = torch.zeros(1, device=inputs.device).repeat(B, self.query_dim)\n            self.attention_rnn_cell_state = torch.zeros(1, device=inputs.device).repeat(B, self.query_dim)\n            self.decoder_hidden = torch.zeros(1, device=inputs.device).repeat(B, self.decoder_rnn_dim)\n            self.decoder_cell = torch.zeros(1, device=inputs.device).repeat(B, self.decoder_rnn_dim)\n            self.context = torch.zeros(1, device=inputs.device).repeat(B, self.encoder_embedding_dim)\n        self.inputs = inputs\n        self.processed_inputs = self.attention.preprocess_inputs(inputs)\n        self.mask = mask\n\n    def _reshape_memory(self, memory):\n        \"\"\"\n        Reshape the spectrograms for given 'r'\n        \"\"\"\n        # Grouping multiple frames if necessary\n        if memory.size(-1) == self.frame_channels:\n            memory = memory.view(memory.shape[0], memory.size(1) // self.r, -1)\n        # Time first (T_decoder, B, frame_channels)\n        memory = memory.transpose(0, 1)\n        return memory\n\n    def _parse_outputs(self, outputs, stop_tokens, alignments):\n        alignments = torch.stack(alignments).transpose(0, 1)\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        outputs = outputs.view(outputs.size(0), -1, self.frame_channels)\n        outputs = outputs.transpose(1, 2)\n        return outputs, stop_tokens, alignments\n\n    def _update_memory(self, memory):\n        if len(memory.shape) == 2:\n            return memory[:, self.frame_channels * (self.r - 1) :]\n        return memory[:, :, self.frame_channels * (self.r - 1) :]\n\n    def decode(self, memory):\n        \"\"\"\n        shapes:\n           - memory: B x r * self.frame_channels\n        \"\"\"\n        # self.context: B x D_en\n        # query_input: B x D_en + (r * self.frame_channels)\n        query_input = torch.cat((memory, self.context), -1)\n        # self.query and self.attention_rnn_cell_state : B x D_attn_rnn\n        self.query, self.attention_rnn_cell_state = self.attention_rnn(\n            query_input, (self.query, self.attention_rnn_cell_state)\n        )\n        self.query = F.dropout(self.query, self.p_attention_dropout, self.training)\n        self.attention_rnn_cell_state = F.dropout(\n            self.attention_rnn_cell_state, self.p_attention_dropout, self.training\n        )\n        # B x D_en\n        self.context = self.attention(self.query, self.inputs, self.processed_inputs, self.mask)\n        # B x (D_en + D_attn_rnn)\n        decoder_rnn_input = torch.cat((self.query, self.context), -1)\n        # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n            decoder_rnn_input, (self.decoder_hidden, self.decoder_cell)\n        )\n        self.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)\n        # B x (D_decoder_rnn + D_en)\n        decoder_hidden_context = torch.cat((self.decoder_hidden, self.context), dim=1)\n        # B x (self.r * self.frame_channels)\n        decoder_output = self.linear_projection(decoder_hidden_context)\n        # B x (D_decoder_rnn + (self.r * self.frame_channels))\n        stopnet_input = torch.cat((self.decoder_hidden, decoder_output), dim=1)\n        if self.separate_stopnet:\n            stop_token = self.stopnet(stopnet_input.detach())\n        else:\n            stop_token = self.stopnet(stopnet_input)\n        # select outputs for the reduction rate self.r\n        decoder_output = decoder_output[:, : self.r * self.frame_channels]\n        return decoder_output, self.attention.attention_weights, stop_token\n\n    def forward(self, inputs, memories, mask):\n        r\"\"\"Train Decoder with teacher forcing.\n        Args:\n            inputs: Encoder outputs.\n            memories: Feature frames for teacher-forcing.\n            mask: Attention mask for sequence padding.\n\n        Shapes:\n            - inputs: (B, T, D_out_enc)\n            - memory: (B, T_mel, D_mel)\n            - outputs: (B, T_mel, D_mel)\n            - alignments: (B, T_in, T_out)\n            - stop_tokens: (B, T_out)\n        \"\"\"\n        memory = self.get_go_frame(inputs).unsqueeze(0)\n        memories = self._reshape_memory(memories)\n        memories = torch.cat((memory, memories), dim=0)\n        memories = self._update_memory(memories)\n        memories = self.prenet(memories)\n\n        self._init_states(inputs, mask=mask)\n        self.attention.init_states(inputs)\n\n        outputs, stop_tokens, alignments = [], [], []\n        while len(outputs) < memories.size(0) - 1:\n            memory = memories[len(outputs)]\n            decoder_output, attention_weights, stop_token = self.decode(memory)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token.squeeze(1)]\n            alignments += [attention_weights]\n\n        outputs, stop_tokens, alignments = self._parse_outputs(outputs, stop_tokens, alignments)\n        return outputs, alignments, stop_tokens\n\n    def inference(self, inputs):\n        r\"\"\"Decoder inference without teacher forcing and use\n        Stopnet to stop decoder.\n        Args:\n            inputs: Encoder outputs.\n\n        Shapes:\n            - inputs: (B, T, D_out_enc)\n            - outputs: (B, T_mel, D_mel)\n            - alignments: (B, T_in, T_out)\n            - stop_tokens: (B, T_out)\n        \"\"\"\n        memory = self.get_go_frame(inputs)\n        memory = self._update_memory(memory)\n\n        self._init_states(inputs, mask=None)\n        self.attention.init_states(inputs)\n\n        outputs, stop_tokens, alignments, t = [], [], [], 0\n        while True:\n            memory = self.prenet(memory)\n            decoder_output, alignment, stop_token = self.decode(memory)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token]\n            alignments += [alignment]\n\n            if stop_token > self.stop_threshold and t > inputs.shape[0] // 2:\n                break\n            if len(outputs) == self.max_decoder_steps:\n                print(f\"   > Decoder stopped with `max_decoder_steps` {self.max_decoder_steps}\")\n                break\n\n            memory = self._update_memory(decoder_output)\n            t += 1\n\n        outputs, stop_tokens, alignments = self._parse_outputs(outputs, stop_tokens, alignments)\n\n        return outputs, alignments, stop_tokens\n\n    def inference_truncated(self, inputs):\n        \"\"\"\n        Preserve decoder states for continuous inference\n        \"\"\"\n        if self.memory_truncated is None:\n            self.memory_truncated = self.get_go_frame(inputs)\n            self._init_states(inputs, mask=None, keep_states=False)\n        else:\n            self._init_states(inputs, mask=None, keep_states=True)\n\n        self.attention.init_states(inputs)\n        outputs, stop_tokens, alignments, t = [], [], [], 0\n        while True:\n            memory = self.prenet(self.memory_truncated)\n            decoder_output, alignment, stop_token = self.decode(memory)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token]\n            alignments += [alignment]\n\n            if stop_token > 0.7:\n                break\n            if len(outputs) == self.max_decoder_steps:\n                print(\"   | > Decoder stopped with 'max_decoder_steps\")\n                break\n\n            self.memory_truncated = decoder_output\n            t += 1\n\n        outputs, stop_tokens, alignments = self._parse_outputs(outputs, stop_tokens, alignments)\n\n        return outputs, alignments, stop_tokens\n\n    def inference_step(self, inputs, t, memory=None):\n        \"\"\"\n        For debug purposes\n        \"\"\"\n        if t == 0:\n            memory = self.get_go_frame(inputs)\n            self._init_states(inputs, mask=None)\n\n        memory = self.prenet(memory)\n        decoder_output, stop_token, alignment = self.decode(memory)\n        stop_token = torch.sigmoid(stop_token.data)\n        memory = decoder_output\n        return decoder_output, stop_token, alignment\n", "TTS/tts/layers/bark/load_model.py": "import contextlib\nimport functools\nimport hashlib\nimport logging\nimport os\n\nimport requests\nimport torch\nimport tqdm\n\nfrom TTS.tts.layers.bark.model import GPT, GPTConfig\nfrom TTS.tts.layers.bark.model_fine import FineGPT, FineGPTConfig\n\nif (\n    torch.cuda.is_available()\n    and hasattr(torch.cuda, \"amp\")\n    and hasattr(torch.cuda.amp, \"autocast\")\n    and torch.cuda.is_bf16_supported()\n):\n    autocast = functools.partial(torch.cuda.amp.autocast, dtype=torch.bfloat16)\nelse:\n\n    @contextlib.contextmanager\n    def autocast():\n        yield\n\n\n# hold models in global scope to lazy load\n\nlogger = logging.getLogger(__name__)\n\n\nif not hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n    logger.warning(\n        \"torch version does not support flash attention. You will get significantly faster\"\n        + \" inference speed by upgrade torch to newest version / nightly.\"\n    )\n\n\ndef _md5(fname):\n    hash_md5 = hashlib.md5()\n    with open(fname, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\n\ndef _download(from_s3_path, to_local_path, CACHE_DIR):\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    response = requests.get(from_s3_path, stream=True)\n    total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n    progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n    with open(to_local_path, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes not in [0, progress_bar.n]:\n        raise ValueError(\"ERROR, something went wrong\")\n\n\nclass InferenceContext:\n    def __init__(self, benchmark=False):\n        # we can't expect inputs to be the same length, so disable benchmarking by default\n        self._chosen_cudnn_benchmark = benchmark\n        self._cudnn_benchmark = None\n\n    def __enter__(self):\n        self._cudnn_benchmark = torch.backends.cudnn.benchmark\n        torch.backends.cudnn.benchmark = self._chosen_cudnn_benchmark\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        torch.backends.cudnn.benchmark = self._cudnn_benchmark\n\n\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n\n\n@contextlib.contextmanager\ndef inference_mode():\n    with InferenceContext(), torch.inference_mode(), torch.no_grad(), autocast():\n        yield\n\n\ndef clear_cuda_cache():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n\ndef load_model(ckpt_path, device, config, model_type=\"text\"):\n    logger.info(f\"loading {model_type} model from {ckpt_path}...\")\n\n    if device == \"cpu\":\n        logger.warning(\"No GPU being used. Careful, Inference might be extremely slow!\")\n    if model_type == \"text\":\n        ConfigClass = GPTConfig\n        ModelClass = GPT\n    elif model_type == \"coarse\":\n        ConfigClass = GPTConfig\n        ModelClass = GPT\n    elif model_type == \"fine\":\n        ConfigClass = FineGPTConfig\n        ModelClass = FineGPT\n    else:\n        raise NotImplementedError()\n    if (\n        not config.USE_SMALLER_MODELS\n        and os.path.exists(ckpt_path)\n        and _md5(ckpt_path) != config.REMOTE_MODEL_PATHS[model_type][\"checksum\"]\n    ):\n        logger.warning(f\"found outdated {model_type} model, removing...\")\n        os.remove(ckpt_path)\n    if not os.path.exists(ckpt_path):\n        logger.info(f\"{model_type} model not found, downloading...\")\n        _download(config.REMOTE_MODEL_PATHS[model_type][\"path\"], ckpt_path, config.CACHE_DIR)\n\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    # this is a hack\n    model_args = checkpoint[\"model_args\"]\n    if \"input_vocab_size\" not in model_args:\n        model_args[\"input_vocab_size\"] = model_args[\"vocab_size\"]\n        model_args[\"output_vocab_size\"] = model_args[\"vocab_size\"]\n        del model_args[\"vocab_size\"]\n\n    gptconf = ConfigClass(**checkpoint[\"model_args\"])\n    if model_type == \"text\":\n        config.semantic_config = gptconf\n    elif model_type == \"coarse\":\n        config.coarse_config = gptconf\n    elif model_type == \"fine\":\n        config.fine_config = gptconf\n\n    model = ModelClass(gptconf)\n    state_dict = checkpoint[\"model\"]\n    # fixup checkpoint\n    unwanted_prefix = \"_orig_mod.\"\n    for k, _ in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n    extra_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n    extra_keys = set(k for k in extra_keys if not k.endswith(\".attn.bias\"))\n    missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n    missing_keys = set(k for k in missing_keys if not k.endswith(\".attn.bias\"))\n    if len(extra_keys) != 0:\n        raise ValueError(f\"extra keys found: {extra_keys}\")\n    if len(missing_keys) != 0:\n        raise ValueError(f\"missing keys: {missing_keys}\")\n    model.load_state_dict(state_dict, strict=False)\n    n_params = model.get_num_params()\n    val_loss = checkpoint[\"best_val_loss\"].item()\n    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params, {round(val_loss,3)} loss\")\n    model.eval()\n    model.to(device)\n    del checkpoint, state_dict\n    clear_cuda_cache()\n    return model, config\n", "TTS/tts/layers/bark/model.py": "\"\"\"\nMuch of this code is adapted from Andrej Karpathy's NanoGPT\n(https://github.com/karpathy/nanoGPT)\n\"\"\"\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nfrom coqpit import Coqpit\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass LayerNorm(nn.Module):\n    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch nightly and still a bit scary\n        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n        if not self.flash:\n            # print(\"WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\n                \"bias\",\n                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n                    1, 1, config.block_size, config.block_size\n                ),\n            )\n\n    def forward(self, x, past_kv=None, use_cache=False):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n        if past_kv is not None:\n            past_key = past_kv[0]\n            past_value = past_kv[1]\n            k = torch.cat((past_key, k), dim=-2)\n            v = torch.cat((past_value, v), dim=-2)\n\n        FULL_T = k.shape[-2]\n\n        if use_cache is True:\n            present = (k, v)\n        else:\n            present = None\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            if past_kv is not None:\n                # When `past_kv` is provided, we're doing incremental decoding and `q.shape[2] == 1`: q only contains\n                # the query for the last token. scaled_dot_product_attention interprets this as the first token in the\n                # sequence, so if is_causal=True it will mask out all attention from it. This is not what we want, so\n                # to work around this we set is_causal=False.\n                is_causal = False\n            else:\n                is_causal = True\n\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout, is_causal=is_causal)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, FULL_T - T : FULL_T, :FULL_T] == 0, float(\"-inf\"))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return (y, present)\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n        self.layer_idx = layer_idx\n\n    def forward(self, x, past_kv=None, use_cache=False):\n        attn_output, prev_kvs = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache)\n        x = x + attn_output\n        x = x + self.mlp(self.ln_2(x))\n        return (x, prev_kvs)\n\n\n@dataclass\nclass GPTConfig(Coqpit):\n    block_size: int = 1024\n    input_vocab_size: int = 10_048\n    output_vocab_size: int = 10_048\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.input_vocab_size is not None\n        assert config.output_vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.input_vocab_size, config.n_embd),\n                wpe=nn.Embedding(config.block_size, config.n_embd),\n                drop=nn.Dropout(config.dropout),\n                h=nn.ModuleList([Block(config, idx) for idx in range(config.n_layer)]),\n                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.output_vocab_size, bias=False)\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wte.weight.numel()\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def forward(self, idx, merge_context=False, past_kv=None, position_ids=None, use_cache=False):\n        device = idx.device\n        _, t = idx.size()\n        if past_kv is not None:\n            assert t == 1\n            tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n        else:\n            if merge_context:\n                assert idx.shape[1] >= 256 + 256 + 1\n                t = idx.shape[1] - 256\n            else:\n                assert (\n                    t <= self.config.block_size\n                ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\n            # forward the GPT model itself\n            if merge_context:\n                tok_emb = torch.cat(\n                    [\n                        self.transformer.wte(idx[:, :256]) + self.transformer.wte(idx[:, 256 : 256 + 256]),\n                        self.transformer.wte(idx[:, 256 + 256 :]),\n                    ],\n                    dim=1,\n                )\n            else:\n                tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n\n        if past_kv is None:\n            past_length = 0\n            past_kv = tuple([None] * len(self.transformer.h))\n        else:\n            past_length = past_kv[0][0].size(-2)\n\n        if position_ids is None:\n            position_ids = torch.arange(past_length, t + past_length, dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0)  # shape (1, t)\n            assert position_ids.shape == (1, t)\n\n        pos_emb = self.transformer.wpe(position_ids)  # position embeddings of shape (1, t, n_embd)\n\n        x = self.transformer.drop(tok_emb + pos_emb)\n\n        new_kv = () if use_cache else None\n\n        for _, (block, past_layer_kv) in enumerate(zip(self.transformer.h, past_kv)):\n            x, kv = block(x, past_kv=past_layer_kv, use_cache=use_cache)\n\n            if use_cache:\n                new_kv = new_kv + (kv,)\n\n        x = self.transformer.ln_f(x)\n\n        # inference-time mini-optimization: only forward the lm_head on the very last position\n        logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n\n        return (logits, new_kv)\n", "TTS/tts/layers/bark/inference_funcs.py": "import logging\nimport os\nimport re\nfrom glob import glob\nfrom typing import Dict, List\n\nimport librosa\nimport numpy as np\nimport torch\nimport torchaudio\nimport tqdm\nfrom encodec.utils import convert_audio\nfrom scipy.special import softmax\nfrom torch.nn import functional as F\n\nfrom TTS.tts.layers.bark.hubert.hubert_manager import HubertManager\nfrom TTS.tts.layers.bark.hubert.kmeans_hubert import CustomHubert\nfrom TTS.tts.layers.bark.hubert.tokenizer import HubertTokenizer\nfrom TTS.tts.layers.bark.load_model import clear_cuda_cache, inference_mode\n\nlogger = logging.getLogger(__name__)\n\n\ndef _tokenize(tokenizer, text):\n    return tokenizer.encode(text, add_special_tokens=False)\n\n\ndef _detokenize(tokenizer, enc_text):\n    return tokenizer.decode(enc_text)\n\n\ndef _normalize_whitespace(text):\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n\ndef get_voices(extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}\n    for d in dirs:\n        subs = os.listdir(d)\n        for sub in subs:\n            subj = os.path.join(d, sub)\n            if os.path.isdir(subj):\n                voices[sub] = list(glob(f\"{subj}/*.npz\"))\n                # fetch audio files if no npz files are found\n                if len(voices[sub]) == 0:\n                    voices[sub] = list(glob(f\"{subj}/*.wav\")) + list(glob(f\"{subj}/*.mp3\"))\n    return voices\n\n\ndef load_npz(npz_file):\n    x_history = np.load(npz_file)\n    semantic = x_history[\"semantic_prompt\"]\n    coarse = x_history[\"coarse_prompt\"]\n    fine = x_history[\"fine_prompt\"]\n    return semantic, coarse, fine\n\n\ndef load_voice(model, voice: str, extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n\ndef zero_crossing_rate(audio, frame_length=1024, hop_length=512):\n    zero_crossings = np.sum(np.abs(np.diff(np.sign(audio))) / 2)\n    total_frames = 1 + int((len(audio) - frame_length) / hop_length)\n    return zero_crossings / total_frames\n\n\ndef compute_spectral_contrast(audio_data, sample_rate, n_bands=6, fmin=200.0):\n    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate, n_bands=n_bands, fmin=fmin)\n    return np.mean(spectral_contrast)\n\n\ndef compute_average_bass_energy(audio_data, sample_rate, max_bass_freq=250):\n    stft = librosa.stft(audio_data)\n    power_spectrogram = np.abs(stft) ** 2\n    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=stft.shape[0])\n    bass_mask = frequencies <= max_bass_freq\n    bass_energy = power_spectrogram[np.ix_(bass_mask, np.arange(power_spectrogram.shape[1]))].mean()\n    return bass_energy\n\n\ndef generate_voice(\n    audio,\n    model,\n    output_path,\n):\n    \"\"\"Generate a new voice from a given audio and text prompt.\n\n    Args:\n        audio (np.ndarray): The audio to use as a base for the new voice.\n        text (str): Transcription of the audio you are clonning.\n        model (BarkModel): The BarkModel to use for generating the new voice.\n        output_path (str): The path to save the generated voice to.\n    \"\"\"\n    if isinstance(audio, str):\n        audio, sr = torchaudio.load(audio)\n        audio = convert_audio(audio, sr, model.config.sample_rate, model.encodec.channels)\n        audio = audio.unsqueeze(0).to(model.device)\n\n    with torch.no_grad():\n        encoded_frames = model.encodec.encode(audio)\n    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n\n    # move codes to cpu\n    codes = codes.cpu().numpy()\n\n    # generate semantic tokens\n    # Load the HuBERT model\n    hubert_manager = HubertManager()\n    # hubert_manager.make_sure_hubert_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"])\n    hubert_manager.make_sure_tokenizer_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"])\n\n    hubert_model = CustomHubert(checkpoint_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"]).to(model.device)\n\n    # Load the CustomTokenizer model\n    tokenizer = HubertTokenizer.load_from_checkpoint(\n        model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"], map_location=model.device\n    )\n    # semantic_tokens = model.text_to_semantic(\n    #     text, max_gen_duration_s=seconds, top_k=50, top_p=0.95, temp=0.7\n    # )  # not 100%\n    semantic_vectors = hubert_model.forward(audio[0], input_sample_hz=model.config.sample_rate)\n    semantic_tokens = tokenizer.get_token(semantic_vectors)\n    semantic_tokens = semantic_tokens.cpu().numpy()\n\n    np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)\n\n\ndef generate_text_semantic(\n    text,\n    model,\n    history_prompt=None,\n    temp=0.7,\n    top_k=None,\n    top_p=None,\n    silent=False,\n    min_eos_p=0.2,\n    max_gen_duration_s=None,\n    allow_early_stop=True,\n    base=None,\n    use_kv_caching=True,\n    **kwargs,  # pylint: disable=unused-argument\n):\n    \"\"\"Generate semantic tokens from text.\n\n    Args:\n        text (str): The text to generate semantic tokens from.\n        model (BarkModel): The BarkModel to use for generating the semantic tokens.\n        history_prompt (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a prompt for the generation.\n        temp (float): The temperature to use for the generation.\n        top_k (int): The number of top tokens to consider for the generation.\n        top_p (float): The cumulative probability to consider for the generation.\n        silent (bool): Whether to silence the tqdm progress bar.\n        min_eos_p (float): The minimum probability to consider for the end of sentence token.\n        max_gen_duration_s (float): The maximum duration in seconds to generate for.\n        allow_early_stop (bool): Whether to allow the generation to stop early.\n        base (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a base for the generation.\n        use_kv_caching (bool): Whether to use key-value caching for the generation.\n        **kwargs: Additional keyword arguments. They are ignored.\n\n    Returns:\n        np.ndarray: The generated semantic tokens.\n    \"\"\"\n    assert isinstance(text, str)\n    text = _normalize_whitespace(text)\n    assert len(text.strip()) > 0\n    if all(v is not None for v in history_prompt) or base is not None:\n        if history_prompt is not None:\n            semantic_history = history_prompt[0]\n        if base is not None:\n            semantic_history = base[0]\n        assert (\n            isinstance(semantic_history, np.ndarray)\n            and len(semantic_history.shape) == 1\n            and len(semantic_history) > 0\n            and semantic_history.min() >= 0\n            and semantic_history.max() <= model.config.SEMANTIC_VOCAB_SIZE - 1\n        )\n    else:\n        semantic_history = None\n    encoded_text = np.array(_tokenize(model.tokenizer, text)) + model.config.TEXT_ENCODING_OFFSET\n    if len(encoded_text) > 256:\n        p = round((len(encoded_text) - 256) / len(encoded_text) * 100, 1)\n        logger.warning(f\"warning, text too long, lopping of last {p}%\")\n        encoded_text = encoded_text[:256]\n    encoded_text = np.pad(\n        encoded_text,\n        (0, 256 - len(encoded_text)),\n        constant_values=model.config.TEXT_PAD_TOKEN,\n        mode=\"constant\",\n    )\n    if semantic_history is not None:\n        semantic_history = semantic_history.astype(np.int64)\n        # lop off if history is too long, pad if needed\n        semantic_history = semantic_history[-256:]\n        semantic_history = np.pad(\n            semantic_history,\n            (0, 256 - len(semantic_history)),\n            constant_values=model.config.SEMANTIC_PAD_TOKEN,\n            mode=\"constant\",\n        )\n    else:\n        semantic_history = np.array([model.config.SEMANTIC_PAD_TOKEN] * 256)\n    x = torch.from_numpy(\n        np.hstack([encoded_text, semantic_history, np.array([model.config.SEMANTIC_INFER_TOKEN])]).astype(np.int64)\n    )[None]\n    assert x.shape[1] == 256 + 256 + 1\n    with inference_mode():\n        x = x.to(model.device)\n        n_tot_steps = 768\n        # custom tqdm updates since we don't know when eos will occur\n        pbar = tqdm.tqdm(disable=silent, total=100)\n        pbar_state = 0\n        tot_generated_duration_s = 0\n        kv_cache = None\n        for n in range(n_tot_steps):\n            if use_kv_caching and kv_cache is not None:\n                x_input = x[:, [-1]]\n            else:\n                x_input = x\n            logits, kv_cache = model.semantic_model(\n                x_input, merge_context=True, use_cache=use_kv_caching, past_kv=kv_cache\n            )\n            relevant_logits = logits[0, 0, : model.config.SEMANTIC_VOCAB_SIZE]\n            if allow_early_stop:\n                relevant_logits = torch.hstack(\n                    (relevant_logits, logits[0, 0, [model.config.SEMANTIC_PAD_TOKEN]])\n                )  # eos\n            if top_p is not None:\n                # faster to convert to numpy\n                logits_device = relevant_logits.device\n                logits_dtype = relevant_logits.type()\n                relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n                sorted_indices = np.argsort(relevant_logits)[::-1]\n                sorted_logits = relevant_logits[sorted_indices]\n                cumulative_probs = np.cumsum(softmax(sorted_logits))\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n                sorted_indices_to_remove[0] = False\n                relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n                relevant_logits = torch.from_numpy(relevant_logits)\n                relevant_logits = relevant_logits.to(logits_device).type(logits_dtype)\n            if top_k is not None:\n                v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n                relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n            probs = torch.softmax(relevant_logits / temp, dim=-1)\n            item_next = torch.multinomial(probs, num_samples=1)\n            if allow_early_stop and (\n                item_next == model.config.SEMANTIC_VOCAB_SIZE or (min_eos_p is not None and probs[-1] >= min_eos_p)\n            ):\n                # eos found, so break\n                pbar.update(100 - pbar_state)\n                break\n            x = torch.cat((x, item_next[None]), dim=1)\n            tot_generated_duration_s += 1 / model.config.SEMANTIC_RATE_HZ\n            if max_gen_duration_s is not None and tot_generated_duration_s > max_gen_duration_s:\n                pbar.update(100 - pbar_state)\n                break\n            if n == n_tot_steps - 1:\n                pbar.update(100 - pbar_state)\n                break\n            del logits, relevant_logits, probs, item_next\n            req_pbar_state = np.min([100, int(round(100 * n / n_tot_steps))])\n            if req_pbar_state > pbar_state:\n                pbar.update(req_pbar_state - pbar_state)\n            pbar_state = req_pbar_state\n        pbar.close()\n        out = x.detach().cpu().numpy().squeeze()[256 + 256 + 1 :]\n    assert all(out >= 0) and all(out < model.config.SEMANTIC_VOCAB_SIZE)\n    clear_cuda_cache()\n    return out\n\n\ndef _flatten_codebooks(arr, offset_size):\n    assert len(arr.shape) == 2\n    arr = arr.copy()\n    if offset_size is not None:\n        for n in range(1, arr.shape[0]):\n            arr[n, :] += offset_size * n\n    flat_arr = arr.ravel(\"F\")\n    return flat_arr\n\n\ndef generate_coarse(\n    x_semantic,\n    model,\n    history_prompt=None,\n    temp=0.7,\n    top_k=None,\n    top_p=None,\n    silent=False,\n    max_coarse_history=630,  # min 60 (faster), max 630 (more context)\n    sliding_window_len=60,\n    base=None,\n    use_kv_caching=True,\n):\n    \"\"\"Generate coarse audio codes from semantic tokens.\n\n    Args:\n        x_semantic (np.ndarray): The semantic tokens to generate coarse audio codes from.\n        model (BarkModel): The BarkModel to use for generating the coarse audio codes.\n        history_prompt (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a prompt for the generation.\n        temp (float): The temperature to use for the generation.\n        top_k (int): The number of top tokens to consider for the generation.\n        top_p (float): The cumulative probability to consider for the generation.\n        silent (bool): Whether to silence the tqdm progress bar.\n        max_coarse_history (int): The maximum number of coarse audio codes to use as history.\n        sliding_window_len (int): The length of the sliding window to use for the generation.\n        base (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a base for the generation.\n        use_kv_caching (bool): Whether to use key-value caching for the generation.\n\n    Returns:\n        np.ndarray: The generated coarse audio codes.\n    \"\"\"\n    assert (\n        isinstance(x_semantic, np.ndarray)\n        and len(x_semantic.shape) == 1\n        and len(x_semantic) > 0\n        and x_semantic.min() >= 0\n        and x_semantic.max() <= model.config.SEMANTIC_VOCAB_SIZE - 1\n    )\n    assert 60 <= max_coarse_history <= 630\n    assert max_coarse_history + sliding_window_len <= 1024 - 256\n    semantic_to_coarse_ratio = (\n        model.config.COARSE_RATE_HZ / model.config.SEMANTIC_RATE_HZ * model.config.N_COARSE_CODEBOOKS\n    )\n    max_semantic_history = int(np.floor(max_coarse_history / semantic_to_coarse_ratio))\n    if all(v is not None for v in history_prompt) or base is not None:\n        if history_prompt is not None:\n            x_history = history_prompt\n            x_semantic_history = x_history[0]\n            x_coarse_history = x_history[1]\n        if base is not None:\n            x_semantic_history = base[0]\n            x_coarse_history = base[1]\n        assert (\n            isinstance(x_semantic_history, np.ndarray)\n            and len(x_semantic_history.shape) == 1\n            and len(x_semantic_history) > 0\n            and x_semantic_history.min() >= 0\n            and x_semantic_history.max() <= model.config.SEMANTIC_VOCAB_SIZE - 1\n            and isinstance(x_coarse_history, np.ndarray)\n            and len(x_coarse_history.shape) == 2\n            and x_coarse_history.shape[0] == model.config.N_COARSE_CODEBOOKS\n            and x_coarse_history.shape[-1] >= 0\n            and x_coarse_history.min() >= 0\n            and x_coarse_history.max() <= model.config.CODEBOOK_SIZE - 1\n            and (\n                round(x_coarse_history.shape[-1] / len(x_semantic_history), 1)\n                == round(semantic_to_coarse_ratio / model.config.N_COARSE_CODEBOOKS, 1)\n            )\n        )\n        x_coarse_history = (\n            _flatten_codebooks(x_coarse_history, model.config.CODEBOOK_SIZE) + model.config.SEMANTIC_VOCAB_SIZE\n        )\n        # trim histories correctly\n        n_semantic_hist_provided = np.min(\n            [\n                max_semantic_history,\n                len(x_semantic_history) - len(x_semantic_history) % 2,\n                int(np.floor(len(x_coarse_history) / semantic_to_coarse_ratio)),\n            ]\n        )\n        n_coarse_hist_provided = int(round(n_semantic_hist_provided * semantic_to_coarse_ratio))\n        x_semantic_history = x_semantic_history[-n_semantic_hist_provided:].astype(np.int32)\n        x_coarse_history = x_coarse_history[-n_coarse_hist_provided:].astype(np.int32)\n        # TODO: bit of a hack for time alignment (sounds better)\n        x_coarse_history = x_coarse_history[:-2]\n    else:\n        x_semantic_history = np.array([], dtype=np.int32)\n        x_coarse_history = np.array([], dtype=np.int32)\n    # start loop\n    n_steps = int(\n        round(\n            np.floor(len(x_semantic) * semantic_to_coarse_ratio / model.config.N_COARSE_CODEBOOKS)\n            * model.config.N_COARSE_CODEBOOKS\n        )\n    )\n    assert n_steps > 0 and n_steps % model.config.N_COARSE_CODEBOOKS == 0\n    x_semantic = np.hstack([x_semantic_history, x_semantic]).astype(np.int32)\n    x_coarse = x_coarse_history.astype(np.int32)\n    base_semantic_idx = len(x_semantic_history)\n    with inference_mode():\n        x_semantic_in = torch.from_numpy(x_semantic)[None].to(model.device)\n        x_coarse_in = torch.from_numpy(x_coarse)[None].to(model.device)\n        n_window_steps = int(np.ceil(n_steps / sliding_window_len))\n        n_step = 0\n        for _ in tqdm.tqdm(range(n_window_steps), total=n_window_steps, disable=silent):\n            semantic_idx = base_semantic_idx + int(round(n_step / semantic_to_coarse_ratio))\n            # pad from right side\n            x_in = x_semantic_in[:, np.max([0, semantic_idx - max_semantic_history]) :]\n            x_in = x_in[:, :256]\n            x_in = F.pad(\n                x_in,\n                (0, 256 - x_in.shape[-1]),\n                \"constant\",\n                model.config.COARSE_SEMANTIC_PAD_TOKEN,\n            )\n            x_in = torch.hstack(\n                [\n                    x_in,\n                    torch.tensor([model.config.COARSE_INFER_TOKEN])[None].to(model.device),\n                    x_coarse_in[:, -max_coarse_history:],\n                ]\n            )\n            kv_cache = None\n            for _ in range(sliding_window_len):\n                if n_step >= n_steps:\n                    continue\n                is_major_step = n_step % model.config.N_COARSE_CODEBOOKS == 0\n\n                if use_kv_caching and kv_cache is not None:\n                    x_input = x_in[:, [-1]]\n                else:\n                    x_input = x_in\n\n                logits, kv_cache = model.coarse_model(x_input, use_cache=use_kv_caching, past_kv=kv_cache)\n                logit_start_idx = (\n                    model.config.SEMANTIC_VOCAB_SIZE + (1 - int(is_major_step)) * model.config.CODEBOOK_SIZE\n                )\n                logit_end_idx = model.config.SEMANTIC_VOCAB_SIZE + (2 - int(is_major_step)) * model.config.CODEBOOK_SIZE\n                relevant_logits = logits[0, 0, logit_start_idx:logit_end_idx]\n                if top_p is not None:\n                    # faster to convert to numpy\n                    logits_device = relevant_logits.device\n                    logits_dtype = relevant_logits.type()\n                    relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n                    sorted_indices = np.argsort(relevant_logits)[::-1]\n                    sorted_logits = relevant_logits[sorted_indices]\n                    cumulative_probs = np.cumsum(torch.nn.functional.softmax(sorted_logits))\n                    sorted_indices_to_remove = cumulative_probs > top_p\n                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n                    sorted_indices_to_remove[0] = False\n                    relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n                    relevant_logits = torch.from_numpy(relevant_logits)\n                    relevant_logits = relevant_logits.to(logits_device).type(logits_dtype)\n                if top_k is not None:\n                    v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n                    relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n                probs = torch.nn.functional.softmax(relevant_logits / temp, dim=-1)\n                item_next = torch.multinomial(probs, num_samples=1)\n                item_next += logit_start_idx\n                x_coarse_in = torch.cat((x_coarse_in, item_next[None]), dim=1)\n                x_in = torch.cat((x_in, item_next[None]), dim=1)\n                del logits, relevant_logits, probs, item_next\n                n_step += 1\n            del x_in\n        del x_semantic_in\n    gen_coarse_arr = x_coarse_in.detach().cpu().numpy().squeeze()[len(x_coarse_history) :]\n    del x_coarse_in\n    assert len(gen_coarse_arr) == n_steps\n    gen_coarse_audio_arr = (\n        gen_coarse_arr.reshape(-1, model.config.N_COARSE_CODEBOOKS).T - model.config.SEMANTIC_VOCAB_SIZE\n    )\n    for n in range(1, model.config.N_COARSE_CODEBOOKS):\n        gen_coarse_audio_arr[n, :] -= n * model.config.CODEBOOK_SIZE\n    clear_cuda_cache()\n    return gen_coarse_audio_arr\n\n\ndef generate_fine(\n    x_coarse_gen,\n    model,\n    history_prompt=None,\n    temp=0.5,\n    silent=True,\n    base=None,\n):\n    \"\"\"Generate full audio codes from coarse audio codes.\n\n    Args:\n        x_coarse_gen (np.ndarray): The coarse audio codes to generate full audio codes from.\n        model (BarkModel): The BarkModel to use for generating the full audio codes.\n        history_prompt (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a prompt for the generation.\n        temp (float): The temperature to use for the generation.\n        silent (bool): Whether to silence the tqdm progress bar.\n        base (tuple): A tuple of (semantic_history, coarse_history, fine_history) to use as a base for the generation.\n\n    Returns:\n        np.ndarray: The generated full audio codes.\n    \"\"\"\n    assert (\n        isinstance(x_coarse_gen, np.ndarray)\n        and len(x_coarse_gen.shape) == 2\n        and 1 <= x_coarse_gen.shape[0] <= model.config.N_FINE_CODEBOOKS - 1\n        and x_coarse_gen.shape[1] > 0\n        and x_coarse_gen.min() >= 0\n        and x_coarse_gen.max() <= model.config.CODEBOOK_SIZE - 1\n    )\n    if all(v is not None for v in history_prompt) or base is not None:\n        if history_prompt is not None:\n            x_fine_history = history_prompt[2]\n        if base is not None:\n            x_fine_history = base[2]\n        assert (\n            isinstance(x_fine_history, np.ndarray)\n            and len(x_fine_history.shape) == 2\n            and x_fine_history.shape[0] == model.config.N_FINE_CODEBOOKS\n            and x_fine_history.shape[1] >= 0\n            and x_fine_history.min() >= 0\n            and x_fine_history.max() <= model.config.CODEBOOK_SIZE - 1\n        )\n    else:\n        x_fine_history = None\n    n_coarse = x_coarse_gen.shape[0]\n    # make input arr\n    in_arr = np.vstack(\n        [\n            x_coarse_gen,\n            np.zeros((model.config.N_FINE_CODEBOOKS - n_coarse, x_coarse_gen.shape[1]))\n            + model.config.CODEBOOK_SIZE,  # padding\n        ]\n    ).astype(np.int32)\n    # prepend history if available (max 512)\n    if x_fine_history is not None:\n        x_fine_history = x_fine_history.astype(np.int32)\n        in_arr = np.hstack(\n            [\n                x_fine_history[:, -512:].astype(np.int32),\n                in_arr,\n            ]\n        )\n        n_history = x_fine_history[:, -512:].shape[1]\n    else:\n        n_history = 0\n    n_remove_from_end = 0\n    # need to pad if too short (since non-causal model)\n    if in_arr.shape[1] < 1024:\n        n_remove_from_end = 1024 - in_arr.shape[1]\n        in_arr = np.hstack(\n            [\n                in_arr,\n                np.zeros((model.config.N_FINE_CODEBOOKS, n_remove_from_end), dtype=np.int32)\n                + model.config.CODEBOOK_SIZE,\n            ]\n        )\n    # we can be lazy about fractional loop and just keep overwriting codebooks\n    n_loops = np.max([0, int(np.ceil((x_coarse_gen.shape[1] - (1024 - n_history)) / 512))]) + 1\n    with inference_mode():\n        in_arr = torch.tensor(in_arr.T).to(model.device)\n        for n in tqdm.tqdm(range(n_loops), disable=silent):\n            start_idx = np.min([n * 512, in_arr.shape[0] - 1024])\n            start_fill_idx = np.min([n_history + n * 512, in_arr.shape[0] - 512])\n            rel_start_fill_idx = start_fill_idx - start_idx\n            in_buffer = in_arr[start_idx : start_idx + 1024, :][None]\n            for nn in range(n_coarse, model.config.N_FINE_CODEBOOKS):\n                logits = model.fine_model(nn, in_buffer)\n                if temp is None:\n                    relevant_logits = logits[0, rel_start_fill_idx:, : model.config.CODEBOOK_SIZE]\n                    codebook_preds = torch.argmax(relevant_logits, -1)\n                else:\n                    relevant_logits = logits[0, :, : model.config.CODEBOOK_SIZE] / temp\n                    probs = F.softmax(relevant_logits, dim=-1)\n                    codebook_preds = torch.hstack(\n                        [torch.multinomial(probs[n], num_samples=1) for n in range(rel_start_fill_idx, 1024)]\n                    )\n                in_buffer[0, rel_start_fill_idx:, nn] = codebook_preds\n                del logits, codebook_preds\n            # transfer over info into model_in and convert to numpy\n            for nn in range(n_coarse, model.config.N_FINE_CODEBOOKS):\n                in_arr[start_fill_idx : start_fill_idx + (1024 - rel_start_fill_idx), nn] = in_buffer[\n                    0, rel_start_fill_idx:, nn\n                ]\n            del in_buffer\n        gen_fine_arr = in_arr.detach().cpu().numpy().squeeze().T\n        del in_arr\n    gen_fine_arr = gen_fine_arr[:, n_history:]\n    if n_remove_from_end > 0:\n        gen_fine_arr = gen_fine_arr[:, :-n_remove_from_end]\n    assert gen_fine_arr.shape[-1] == x_coarse_gen.shape[-1]\n    clear_cuda_cache()\n    return gen_fine_arr\n\n\ndef codec_decode(fine_tokens, model):\n    \"\"\"Turn quantized audio codes into audio array using encodec.\"\"\"\n    arr = torch.from_numpy(fine_tokens)[None]\n    arr = arr.to(model.device)\n    arr = arr.transpose(0, 1)\n    emb = model.encodec.quantizer.decode(arr)\n    out = model.encodec.decoder(emb)\n    audio_arr = out.detach().cpu().numpy().squeeze()\n    return audio_arr\n", "TTS/tts/layers/bark/model_fine.py": "\"\"\"\nMuch of this code is adapted from Andrej Karpathy's NanoGPT\n(https://github.com/karpathy/nanoGPT)\n\"\"\"\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .model import GPT, MLP, GPTConfig\n\n\nclass NonCausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch nightly and still a bit scary\n        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") and self.dropout == 0.0\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=False\n            )\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nclass FineBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = NonCausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass FineGPT(GPT):\n    def __init__(self, config):\n        super().__init__(config)\n        del self.lm_head\n        self.config = config\n        self.n_codes_total = config.n_codes_total\n        self.transformer = nn.ModuleDict(\n            dict(\n                wtes=nn.ModuleList(\n                    [nn.Embedding(config.input_vocab_size, config.n_embd) for _ in range(config.n_codes_total)]\n                ),\n                wpe=nn.Embedding(config.block_size, config.n_embd),\n                drop=nn.Dropout(config.dropout),\n                h=nn.ModuleList([FineBlock(config) for _ in range(config.n_layer)]),\n                ln_f=nn.LayerNorm(config.n_embd),\n            )\n        )\n        self.lm_heads = nn.ModuleList(\n            [\n                nn.Linear(config.n_embd, config.output_vocab_size, bias=False)\n                for _ in range(config.n_codes_given, self.n_codes_total)\n            ]\n        )\n        for i in range(self.n_codes_total - config.n_codes_given):\n            self.transformer.wtes[i + 1].weight = self.lm_heads[i].weight\n\n    def forward(self, pred_idx, idx):\n        device = idx.device\n        b, t, codes = idx.size()\n        assert (\n            t <= self.config.block_size\n        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        assert pred_idx > 0, \"cannot predict 0th codebook\"\n        assert codes == self.n_codes_total, (b, t, codes)\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, t)\n\n        # forward the GPT model itself\n        tok_embs = [\n            wte(idx[:, :, i]).unsqueeze(-1) for i, wte in enumerate(self.transformer.wtes)\n        ]  # token embeddings of shape (b, t, n_embd)\n        tok_emb = torch.cat(tok_embs, dim=-1)\n        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n        x = tok_emb[:, :, :, : pred_idx + 1].sum(dim=-1)\n        x = self.transformer.drop(x + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_heads[pred_idx - self.config.n_codes_given](x)\n        return logits\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            for wte in self.transformer.wtes:\n                n_params -= wte.weight.numel()\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n\n@dataclass\nclass FineGPTConfig(GPTConfig):\n    n_codes_total: int = 8\n    n_codes_given: int = 1\n", "TTS/tts/layers/bark/__init__.py": "", "TTS/tts/layers/bark/hubert/kmeans_hubert.py": "\"\"\"\nModified HuBERT model without kmeans.\nOriginal author: https://github.com/lucidrains/\nModified by: https://www.github.com/gitmylo/\nLicense: MIT\n\"\"\"\n\n# Modified code from https://github.com/lucidrains/audiolm-pytorch/blob/main/audiolm_pytorch/hubert_kmeans.py\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nfrom einops import pack, unpack\nfrom torch import nn\nfrom torchaudio.functional import resample\nfrom transformers import HubertModel\n\n\ndef round_down_nearest_multiple(num, divisor):\n    return num // divisor * divisor\n\n\ndef curtail_to_multiple(t, mult, from_left=False):\n    data_len = t.shape[-1]\n    rounded_seq_len = round_down_nearest_multiple(data_len, mult)\n    seq_slice = slice(None, rounded_seq_len) if not from_left else slice(-rounded_seq_len, None)\n    return t[..., seq_slice]\n\n\ndef exists(val):\n    return val is not None\n\n\ndef default(val, d):\n    return val if exists(val) else d\n\n\nclass CustomHubert(nn.Module):\n    \"\"\"\n    checkpoint and kmeans can be downloaded at https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n    or you can train your own\n    \"\"\"\n\n    def __init__(self, checkpoint_path, target_sample_hz=16000, seq_len_multiple_of=None, output_layer=9, device=None):\n        super().__init__()\n        self.target_sample_hz = target_sample_hz\n        self.seq_len_multiple_of = seq_len_multiple_of\n        self.output_layer = output_layer\n        if device is not None:\n            self.to(device)\n        self.model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        if device is not None:\n            self.model.to(device)\n        self.model.eval()\n\n    @property\n    def groups(self):\n        return 1\n\n    @torch.no_grad()\n    def forward(self, wav_input, flatten=True, input_sample_hz=None):\n        device = wav_input.device\n\n        if exists(input_sample_hz):\n            wav_input = resample(wav_input, input_sample_hz, self.target_sample_hz)\n\n        if exists(self.seq_len_multiple_of):\n            wav_input = curtail_to_multiple(wav_input, self.seq_len_multiple_of)\n\n        outputs = self.model.forward(\n            wav_input,\n            output_hidden_states=True,\n        )\n        embed = outputs[\"hidden_states\"][self.output_layer]\n        embed, packed_shape = pack([embed], \"* d\")\n        codebook_indices = torch.from_numpy(embed.cpu().detach().numpy()).to(device)\n        if flatten:\n            return codebook_indices\n\n        (codebook_indices,) = unpack(codebook_indices, packed_shape, \"*\")\n        return codebook_indices\n", "TTS/tts/layers/bark/hubert/hubert_manager.py": "# From https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer\n\nimport os.path\nimport shutil\nimport urllib.request\n\nimport huggingface_hub\n\n\nclass HubertManager:\n    @staticmethod\n    def make_sure_hubert_installed(\n        download_url: str = \"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\", model_path: str = \"\"\n    ):\n        if not os.path.isfile(model_path):\n            print(\"Downloading HuBERT base model\")\n            urllib.request.urlretrieve(download_url, model_path)\n            print(\"Downloaded HuBERT\")\n            return model_path\n        return None\n\n    @staticmethod\n    def make_sure_tokenizer_installed(\n        model: str = \"quantifier_hubert_base_ls960_14.pth\",\n        repo: str = \"GitMylo/bark-voice-cloning\",\n        model_path: str = \"\",\n    ):\n        model_dir = os.path.dirname(model_path)\n        if not os.path.isfile(model_path):\n            print(\"Downloading HuBERT custom tokenizer\")\n            huggingface_hub.hf_hub_download(repo, model, local_dir=model_dir, local_dir_use_symlinks=False)\n            shutil.move(os.path.join(model_dir, model), model_path)\n            print(\"Downloaded tokenizer\")\n            return model_path\n        return None\n", "TTS/tts/layers/bark/hubert/tokenizer.py": "\"\"\"\nCustom tokenizer model.\nAuthor: https://www.github.com/gitmylo/\nLicense: MIT\n\"\"\"\n\nimport json\nimport os.path\nfrom zipfile import ZipFile\n\nimport numpy\nimport torch\nfrom torch import nn, optim\n\n\nclass HubertTokenizer(nn.Module):\n    def __init__(self, hidden_size=1024, input_size=768, output_size=10000, version=0):\n        super().__init__()\n        next_size = input_size\n        if version == 0:\n            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n            next_size = hidden_size\n        if version == 1:\n            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n            self.intermediate = nn.Linear(hidden_size, 4096)\n            next_size = 4096\n\n        self.fc = nn.Linear(next_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n        self.optimizer: optim.Optimizer = None\n        self.lossfunc = nn.CrossEntropyLoss()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.version = version\n\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        if self.version == 1:\n            x = self.intermediate(x)\n        x = self.fc(x)\n        x = self.softmax(x)\n        return x\n\n    @torch.no_grad()\n    def get_token(self, x):\n        \"\"\"\n        Used to get the token for the first\n        :param x: An array with shape (N, input_size) where N is a whole number greater or equal to 1, and input_size is the input size used when creating the model.\n        :return: An array with shape (N,) where N is the same as N from the input. Every number in the array is a whole number in range 0...output_size - 1 where output_size is the output size used when creating the model.\n        \"\"\"\n        return torch.argmax(self(x), dim=1)\n\n    def prepare_training(self):\n        self.optimizer = optim.Adam(self.parameters(), 0.001)\n\n    def train_step(self, x_train, y_train, log_loss=False):\n        # y_train = y_train[:-1]\n        # y_train = y_train[1:]\n\n        optimizer = self.optimizer\n        lossfunc = self.lossfunc\n        # Zero the gradients\n        self.zero_grad()\n\n        # Forward pass\n        y_pred = self(x_train)\n\n        y_train_len = len(y_train)\n        y_pred_len = y_pred.shape[0]\n\n        if y_train_len > y_pred_len:\n            diff = y_train_len - y_pred_len\n            y_train = y_train[diff:]\n        elif y_train_len < y_pred_len:\n            diff = y_pred_len - y_train_len\n            y_pred = y_pred[:-diff, :]\n\n        y_train_hot = torch.zeros(len(y_train), self.output_size)\n        y_train_hot[range(len(y_train)), y_train] = 1\n        y_train_hot = y_train_hot.to(\"cuda\")\n\n        # Calculate the loss\n        loss = lossfunc(y_pred, y_train_hot)\n\n        # Print loss\n        if log_loss:\n            print(\"Loss\", loss.item())\n\n        # Backward pass\n        loss.backward()\n\n        # Update the weights\n        optimizer.step()\n\n    def save(self, path):\n        info_path = \".\".join(os.path.basename(path).split(\".\")[:-1]) + \"/.info\"\n        torch.save(self.state_dict(), path)\n        data_from_model = Data(self.input_size, self.hidden_size, self.output_size, self.version)\n        with ZipFile(path, \"a\") as model_zip:\n            model_zip.writestr(info_path, data_from_model.save())\n            model_zip.close()\n\n    @staticmethod\n    def load_from_checkpoint(path, map_location=None):\n        old = True\n        with ZipFile(path) as model_zip:\n            filesMatch = [file for file in model_zip.namelist() if file.endswith(\"/.info\")]\n            file = filesMatch[0] if filesMatch else None\n            if file:\n                old = False\n                data_from_model = Data.load(model_zip.read(file).decode(\"utf-8\"))\n            model_zip.close()\n        if old:\n            model = HubertTokenizer()\n        else:\n            model = HubertTokenizer(\n                data_from_model.hidden_size,\n                data_from_model.input_size,\n                data_from_model.output_size,\n                data_from_model.version,\n            )\n        model.load_state_dict(torch.load(path, map_location=map_location))\n        if map_location:\n            model = model.to(map_location)\n        return model\n\n\nclass Data:\n    input_size: int\n    hidden_size: int\n    output_size: int\n    version: int\n\n    def __init__(self, input_size=768, hidden_size=1024, output_size=10000, version=0):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.version = version\n\n    @staticmethod\n    def load(string):\n        data = json.loads(string)\n        return Data(data[\"input_size\"], data[\"hidden_size\"], data[\"output_size\"], data[\"version\"])\n\n    def save(self):\n        data = {\n            \"input_size\": self.input_size,\n            \"hidden_size\": self.hidden_size,\n            \"output_size\": self.output_size,\n            \"version\": self.version,\n        }\n        return json.dumps(data)\n\n\ndef auto_train(data_path, save_path=\"model.pth\", load_model: str = None, save_epochs=1):\n    data_x, data_y = [], []\n\n    if load_model and os.path.isfile(load_model):\n        print(\"Loading model from\", load_model)\n        model_training = HubertTokenizer.load_from_checkpoint(load_model, \"cuda\")\n    else:\n        print(\"Creating new model.\")\n        model_training = HubertTokenizer(version=1).to(\"cuda\")  # Settings for the model to run without lstm\n    save_path = os.path.join(data_path, save_path)\n    base_save_path = \".\".join(save_path.split(\".\")[:-1])\n\n    sem_string = \"_semantic.npy\"\n    feat_string = \"_semantic_features.npy\"\n\n    ready = os.path.join(data_path, \"ready\")\n    for input_file in os.listdir(ready):\n        full_path = os.path.join(ready, input_file)\n        if input_file.endswith(sem_string):\n            data_y.append(numpy.load(full_path))\n        elif input_file.endswith(feat_string):\n            data_x.append(numpy.load(full_path))\n    model_training.prepare_training()\n\n    epoch = 1\n\n    while 1:\n        for _ in range(save_epochs):\n            j = 0\n            for x, y in zip(data_x, data_y):\n                model_training.train_step(\n                    torch.tensor(x).to(\"cuda\"), torch.tensor(y).to(\"cuda\"), j % 50 == 0\n                )  # Print loss every 50 steps\n                j += 1\n        save_p = save_path\n        save_p_2 = f\"{base_save_path}_epoch_{epoch}.pth\"\n        model_training.save(save_p)\n        model_training.save(save_p_2)\n        print(f\"Epoch {epoch} completed\")\n        epoch += 1\n", "TTS/tts/layers/bark/hubert/__init__.py": "", "TTS/tts/layers/feed_forward/decoder.py": "import torch\nfrom torch import nn\n\nfrom TTS.tts.layers.generic.res_conv_bn import Conv1dBN, Conv1dBNBlock, ResidualConv1dBNBlock\nfrom TTS.tts.layers.generic.transformer import FFTransformerBlock\nfrom TTS.tts.layers.generic.wavenet import WNBlocks\nfrom TTS.tts.layers.glow_tts.transformer import RelativePositionTransformer\n\n\nclass WaveNetDecoder(nn.Module):\n    \"\"\"WaveNet based decoder with a prenet and a postnet.\n\n    prenet: conv1d_1x1\n    postnet: 3 x [conv1d_1x1 -> relu] -> conv1d_1x1\n\n    TODO: Integrate speaker conditioning vector.\n\n    Note:\n        default wavenet parameters;\n            params = {\n                \"num_blocks\": 12,\n                \"hidden_channels\":192,\n                \"kernel_size\": 5,\n                \"dilation_rate\": 1,\n                \"num_layers\": 4,\n                \"dropout_p\": 0.05\n            }\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels for prenet and postnet.\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, c_in_channels, params):\n        super().__init__()\n        # prenet\n        self.prenet = torch.nn.Conv1d(in_channels, params[\"hidden_channels\"], 1)\n        # wavenet layers\n        self.wn = WNBlocks(params[\"hidden_channels\"], c_in_channels=c_in_channels, **params)\n        # postnet\n        self.postnet = [\n            torch.nn.Conv1d(params[\"hidden_channels\"], hidden_channels, 1),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(hidden_channels, hidden_channels, 1),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(hidden_channels, hidden_channels, 1),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(hidden_channels, out_channels, 1),\n        ]\n        self.postnet = nn.Sequential(*self.postnet)\n\n    def forward(self, x, x_mask=None, g=None):\n        x = self.prenet(x) * x_mask\n        x = self.wn(x, x_mask, g)\n        o = self.postnet(x) * x_mask\n        return o\n\n\nclass RelativePositionTransformerDecoder(nn.Module):\n    \"\"\"Decoder with Relative Positional Transformer.\n\n    Note:\n        Default params\n            params={\n                'hidden_channels_ffn': 128,\n                'num_heads': 2,\n                \"kernel_size\": 3,\n                \"dropout_p\": 0.1,\n                \"num_layers\": 8,\n                \"rel_attn_window_size\": 4,\n                \"input_length\": None\n            }\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels including Transformer layers.\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, params):\n        super().__init__()\n        self.prenet = Conv1dBN(in_channels, hidden_channels, 1, 1)\n        self.rel_pos_transformer = RelativePositionTransformer(in_channels, out_channels, hidden_channels, **params)\n\n    def forward(self, x, x_mask=None, g=None):  # pylint: disable=unused-argument\n        o = self.prenet(x) * x_mask\n        o = self.rel_pos_transformer(o, x_mask)\n        return o\n\n\nclass FFTransformerDecoder(nn.Module):\n    \"\"\"Decoder with FeedForwardTransformer.\n\n    Default params\n            params={\n                'hidden_channels_ffn': 1024,\n                'num_heads': 2,\n                \"dropout_p\": 0.1,\n                \"num_layers\": 6,\n            }\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels including Transformer layers.\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, params):\n        super().__init__()\n        self.transformer_block = FFTransformerBlock(in_channels, **params)\n        self.postnet = nn.Conv1d(in_channels, out_channels, 1)\n\n    def forward(self, x, x_mask=None, g=None):  # pylint: disable=unused-argument\n        # TODO: handle multi-speaker\n        x_mask = 1 if x_mask is None else x_mask\n        o = self.transformer_block(x) * x_mask\n        o = self.postnet(o) * x_mask\n        return o\n\n\nclass ResidualConv1dBNDecoder(nn.Module):\n    \"\"\"Residual Convolutional Decoder as in the original Speedy Speech paper\n\n    TODO: Integrate speaker conditioning vector.\n\n    Note:\n        Default params\n                params = {\n                    \"kernel_size\": 4,\n                    \"dilations\": 4 * [1, 2, 4, 8] + [1],\n                    \"num_conv_blocks\": 2,\n                    \"num_res_blocks\": 17\n                }\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels including ResidualConv1dBNBlock layers.\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, params):\n        super().__init__()\n        self.res_conv_block = ResidualConv1dBNBlock(in_channels, hidden_channels, hidden_channels, **params)\n        self.post_conv = nn.Conv1d(hidden_channels, hidden_channels, 1)\n        self.postnet = nn.Sequential(\n            Conv1dBNBlock(\n                hidden_channels, hidden_channels, hidden_channels, params[\"kernel_size\"], 1, num_conv_blocks=2\n            ),\n            nn.Conv1d(hidden_channels, out_channels, 1),\n        )\n\n    def forward(self, x, x_mask=None, g=None):  # pylint: disable=unused-argument\n        o = self.res_conv_block(x, x_mask)\n        o = self.post_conv(o) + x\n        return self.postnet(o) * x_mask\n\n\nclass Decoder(nn.Module):\n    \"\"\"Decodes the expanded phoneme encoding into spectrograms\n    Args:\n        out_channels (int): number of output channels.\n        in_hidden_channels (int): input and hidden channels. Model keeps the input channels for the intermediate layers.\n        decoder_type (str): decoder layer types. 'transformers' or 'residual_conv_bn'. Default 'residual_conv_bn'.\n        decoder_params (dict): model parameters for specified decoder type.\n        c_in_channels (int): number of channels for conditional input.\n\n    Shapes:\n        - input: (B, C, T)\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        out_channels,\n        in_hidden_channels,\n        decoder_type=\"residual_conv_bn\",\n        decoder_params={\n            \"kernel_size\": 4,\n            \"dilations\": 4 * [1, 2, 4, 8] + [1],\n            \"num_conv_blocks\": 2,\n            \"num_res_blocks\": 17,\n        },\n        c_in_channels=0,\n    ):\n        super().__init__()\n\n        if decoder_type.lower() == \"relative_position_transformer\":\n            self.decoder = RelativePositionTransformerDecoder(\n                in_channels=in_hidden_channels,\n                out_channels=out_channels,\n                hidden_channels=in_hidden_channels,\n                params=decoder_params,\n            )\n        elif decoder_type.lower() == \"residual_conv_bn\":\n            self.decoder = ResidualConv1dBNDecoder(\n                in_channels=in_hidden_channels,\n                out_channels=out_channels,\n                hidden_channels=in_hidden_channels,\n                params=decoder_params,\n            )\n        elif decoder_type.lower() == \"wavenet\":\n            self.decoder = WaveNetDecoder(\n                in_channels=in_hidden_channels,\n                out_channels=out_channels,\n                hidden_channels=in_hidden_channels,\n                c_in_channels=c_in_channels,\n                params=decoder_params,\n            )\n        elif decoder_type.lower() == \"fftransformer\":\n            self.decoder = FFTransformerDecoder(in_hidden_channels, out_channels, decoder_params)\n        else:\n            raise ValueError(f\"[!] Unknown decoder type - {decoder_type}\")\n\n    def forward(self, x, x_mask, g=None):  # pylint: disable=unused-argument\n        \"\"\"\n        Args:\n            x: [B, C, T]\n            x_mask: [B, 1, T]\n            g: [B, C_g, 1]\n        \"\"\"\n        # TODO: implement multi-speaker\n        o = self.decoder(x, x_mask, g)\n        return o\n", "TTS/tts/layers/feed_forward/duration_predictor.py": "from torch import nn\n\nfrom TTS.tts.layers.generic.res_conv_bn import Conv1dBN\n\n\nclass DurationPredictor(nn.Module):\n    \"\"\"Speedy Speech duration predictor model.\n    Predicts phoneme durations from encoder outputs.\n\n    Note:\n        Outputs interpreted as log(durations)\n        To get actual durations, do exp transformation\n\n    conv_BN_4x1 -> conv_BN_3x1 -> conv_BN_1x1 -> conv_1x1\n\n    Args:\n        hidden_channels (int): number of channels in the inner layers.\n    \"\"\"\n\n    def __init__(self, hidden_channels):\n        super().__init__()\n\n        self.layers = nn.ModuleList(\n            [\n                Conv1dBN(hidden_channels, hidden_channels, 4, 1),\n                Conv1dBN(hidden_channels, hidden_channels, 3, 1),\n                Conv1dBN(hidden_channels, hidden_channels, 1, 1),\n                nn.Conv1d(hidden_channels, 1, 1),\n            ]\n        )\n\n    def forward(self, x, x_mask):\n        \"\"\"\n        Shapes:\n            x: [B, C, T]\n            x_mask: [B, 1, T]\n        \"\"\"\n        o = x\n        for layer in self.layers:\n            o = layer(o) * x_mask\n        return o\n", "TTS/tts/layers/feed_forward/__init__.py": "", "TTS/tts/layers/feed_forward/encoder.py": "from torch import nn\n\nfrom TTS.tts.layers.generic.res_conv_bn import ResidualConv1dBNBlock\nfrom TTS.tts.layers.generic.transformer import FFTransformerBlock\nfrom TTS.tts.layers.glow_tts.transformer import RelativePositionTransformer\n\n\nclass RelativePositionTransformerEncoder(nn.Module):\n    \"\"\"Speedy speech encoder built on Transformer with Relative Position encoding.\n\n    TODO: Integrate speaker conditioning vector.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, params):\n        super().__init__()\n        self.prenet = ResidualConv1dBNBlock(\n            in_channels,\n            hidden_channels,\n            hidden_channels,\n            kernel_size=5,\n            num_res_blocks=3,\n            num_conv_blocks=1,\n            dilations=[1, 1, 1],\n        )\n        self.rel_pos_transformer = RelativePositionTransformer(hidden_channels, out_channels, hidden_channels, **params)\n\n    def forward(self, x, x_mask=None, g=None):  # pylint: disable=unused-argument\n        if x_mask is None:\n            x_mask = 1\n        o = self.prenet(x) * x_mask\n        o = self.rel_pos_transformer(o, x_mask)\n        return o\n\n\nclass ResidualConv1dBNEncoder(nn.Module):\n    \"\"\"Residual Convolutional Encoder as in the original Speedy Speech paper\n\n    TODO: Integrate speaker conditioning vector.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, hidden_channels, params):\n        super().__init__()\n        self.prenet = nn.Sequential(nn.Conv1d(in_channels, hidden_channels, 1), nn.ReLU())\n        self.res_conv_block = ResidualConv1dBNBlock(hidden_channels, hidden_channels, hidden_channels, **params)\n\n        self.postnet = nn.Sequential(\n            *[\n                nn.Conv1d(hidden_channels, hidden_channels, 1),\n                nn.ReLU(),\n                nn.BatchNorm1d(hidden_channels),\n                nn.Conv1d(hidden_channels, out_channels, 1),\n            ]\n        )\n\n    def forward(self, x, x_mask=None, g=None):  # pylint: disable=unused-argument\n        if x_mask is None:\n            x_mask = 1\n        o = self.prenet(x) * x_mask\n        o = self.res_conv_block(o, x_mask)\n        o = self.postnet(o + x) * x_mask\n        return o * x_mask\n\n\nclass Encoder(nn.Module):\n    # pylint: disable=dangerous-default-value\n    \"\"\"Factory class for Speedy Speech encoder enables different encoder types internally.\n\n    Args:\n        num_chars (int): number of characters.\n        out_channels (int): number of output channels.\n        in_hidden_channels (int): input and hidden channels. Model keeps the input channels for the intermediate layers.\n        encoder_type (str): encoder layer types. 'transformers' or 'residual_conv_bn'. Default 'residual_conv_bn'.\n        encoder_params (dict): model parameters for specified encoder type.\n        c_in_channels (int): number of channels for conditional input.\n\n    Note:\n        Default encoder_params to be set in config.json...\n\n        ```python\n        # for 'relative_position_transformer'\n        encoder_params={\n            'hidden_channels_ffn': 128,\n            'num_heads': 2,\n            \"kernel_size\": 3,\n            \"dropout_p\": 0.1,\n            \"num_layers\": 6,\n            \"rel_attn_window_size\": 4,\n            \"input_length\": None\n        },\n\n        # for 'residual_conv_bn'\n        encoder_params = {\n            \"kernel_size\": 4,\n            \"dilations\": 4 * [1, 2, 4] + [1],\n            \"num_conv_blocks\": 2,\n            \"num_res_blocks\": 13\n        }\n\n        # for 'fftransformer'\n        encoder_params = {\n            \"hidden_channels_ffn\": 1024 ,\n            \"num_heads\": 2,\n            \"num_layers\": 6,\n            \"dropout_p\": 0.1\n        }\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        in_hidden_channels,\n        out_channels,\n        encoder_type=\"residual_conv_bn\",\n        encoder_params={\"kernel_size\": 4, \"dilations\": 4 * [1, 2, 4] + [1], \"num_conv_blocks\": 2, \"num_res_blocks\": 13},\n        c_in_channels=0,\n    ):\n        super().__init__()\n        self.out_channels = out_channels\n        self.in_channels = in_hidden_channels\n        self.hidden_channels = in_hidden_channels\n        self.encoder_type = encoder_type\n        self.c_in_channels = c_in_channels\n\n        # init encoder\n        if encoder_type.lower() == \"relative_position_transformer\":\n            # text encoder\n            # pylint: disable=unexpected-keyword-arg\n            self.encoder = RelativePositionTransformerEncoder(\n                in_hidden_channels, out_channels, in_hidden_channels, encoder_params\n            )\n        elif encoder_type.lower() == \"residual_conv_bn\":\n            self.encoder = ResidualConv1dBNEncoder(in_hidden_channels, out_channels, in_hidden_channels, encoder_params)\n        elif encoder_type.lower() == \"fftransformer\":\n            assert (\n                in_hidden_channels == out_channels\n            ), \"[!] must be `in_channels` == `out_channels` when encoder type is 'fftransformer'\"\n            # pylint: disable=unexpected-keyword-arg\n            self.encoder = FFTransformerBlock(in_hidden_channels, **encoder_params)\n        else:\n            raise NotImplementedError(\" [!] unknown encoder type.\")\n\n    def forward(self, x, x_mask, g=None):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            x: [B, C, T]\n            x_mask: [B, 1, T]\n            g: [B, C, 1]\n        \"\"\"\n        o = self.encoder(x, x_mask)\n        return o * x_mask\n", "TTS/tts/layers/delightful_tts/energy_adaptor.py": "from typing import Callable, Tuple\n\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\n\nfrom TTS.tts.layers.delightful_tts.variance_predictor import VariancePredictor\nfrom TTS.tts.utils.helpers import average_over_durations\n\n\nclass EnergyAdaptor(nn.Module):  # pylint: disable=abstract-method\n    \"\"\"Variance Adaptor with an added 1D conv layer. Used to\n    get energy embeddings.\n\n    Args:\n        channels_in (int): Number of in channels for conv layers.\n        channels_out (int): Number of out channels.\n        kernel_size (int): Size the kernel for the conv layers.\n        dropout (float): Probability of dropout.\n        lrelu_slope (float): Slope for the leaky relu.\n        emb_kernel_size (int): Size the kernel for the pitch embedding.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time1, dim): Tensor containing input vector\n        - **target** (batch, 1, time2): Tensor containing the energy target\n        - **dr** (batch, time1): Tensor containing aligner durations vector\n        - **mask** (batch, time1): Tensor containing indices to be masked\n    Returns:\n        - **energy prediction** (batch, 1, time1): Tensor produced by energy predictor\n        - **energy embedding** (batch, channels, time1): Tensor produced energy adaptor\n        - **average energy target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels_hidden: int,\n        channels_out: int,\n        kernel_size: int,\n        dropout: float,\n        lrelu_slope: float,\n        emb_kernel_size: int,\n    ):\n        super().__init__()\n        self.energy_predictor = VariancePredictor(\n            channels_in=channels_in,\n            channels=channels_hidden,\n            channels_out=channels_out,\n            kernel_size=kernel_size,\n            p_dropout=dropout,\n            lrelu_slope=lrelu_slope,\n        )\n        self.energy_emb = nn.Conv1d(\n            1,\n            channels_hidden,\n            kernel_size=emb_kernel_size,\n            padding=int((emb_kernel_size - 1) / 2),\n        )\n\n    def get_energy_embedding_train(\n        self, x: torch.Tensor, target: torch.Tensor, dr: torch.IntTensor, mask: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T_src, C]`\n            target: :math: `[B, 1, T_max2]`\n            dr: :math: `[B, T_src]`\n            mask: :math: `[B, T_src]`\n        \"\"\"\n        energy_pred = self.energy_predictor(x, mask)\n        energy_pred.unsqueeze_(1)\n        avg_energy_target = average_over_durations(target, dr)\n        energy_emb = self.energy_emb(avg_energy_target)\n        return energy_pred, avg_energy_target, energy_emb\n\n    def get_energy_embedding(self, x: torch.Tensor, mask: torch.Tensor, energy_transform: Callable) -> torch.Tensor:\n        energy_pred = self.energy_predictor(x, mask)\n        energy_pred.unsqueeze_(1)\n        if energy_transform is not None:\n            energy_pred = energy_transform(energy_pred, (~mask).sum(dim=(1, 2)), self.pitch_mean, self.pitch_std)\n        energy_emb_pred = self.energy_emb(energy_pred)\n        return energy_emb_pred, energy_pred\n", "TTS/tts/layers/delightful_tts/acoustic_model.py": "### credit: https://github.com/dunky11/voicesmith\nfrom typing import Callable, Dict, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom coqpit import Coqpit\nfrom torch import nn\n\nfrom TTS.tts.layers.delightful_tts.conformer import Conformer\nfrom TTS.tts.layers.delightful_tts.encoders import (\n    PhonemeLevelProsodyEncoder,\n    UtteranceLevelProsodyEncoder,\n    get_mask_from_lengths,\n)\nfrom TTS.tts.layers.delightful_tts.energy_adaptor import EnergyAdaptor\nfrom TTS.tts.layers.delightful_tts.networks import EmbeddingPadded, positional_encoding\nfrom TTS.tts.layers.delightful_tts.phoneme_prosody_predictor import PhonemeProsodyPredictor\nfrom TTS.tts.layers.delightful_tts.pitch_adaptor import PitchAdaptor\nfrom TTS.tts.layers.delightful_tts.variance_predictor import VariancePredictor\nfrom TTS.tts.layers.generic.aligner import AlignmentNetwork\nfrom TTS.tts.utils.helpers import generate_path, maximum_path, sequence_mask\n\n\nclass AcousticModel(torch.nn.Module):\n    def __init__(\n        self,\n        args: \"ModelArgs\",\n        tokenizer: \"TTSTokenizer\" = None,\n        speaker_manager: \"SpeakerManager\" = None,\n    ):\n        super().__init__()\n        self.args = args\n        self.tokenizer = tokenizer\n        self.speaker_manager = speaker_manager\n\n        self.init_multispeaker(args)\n        # self.set_embedding_dims()\n\n        self.length_scale = (\n            float(self.args.length_scale) if isinstance(self.args.length_scale, int) else self.args.length_scale\n        )\n\n        self.emb_dim = args.n_hidden_conformer_encoder\n        self.encoder = Conformer(\n            dim=self.args.n_hidden_conformer_encoder,\n            n_layers=self.args.n_layers_conformer_encoder,\n            n_heads=self.args.n_heads_conformer_encoder,\n            speaker_embedding_dim=self.embedded_speaker_dim,\n            p_dropout=self.args.dropout_conformer_encoder,\n            kernel_size_conv_mod=self.args.kernel_size_conv_mod_conformer_encoder,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n        self.pitch_adaptor = PitchAdaptor(\n            n_input=self.args.n_hidden_conformer_encoder,\n            n_hidden=self.args.n_hidden_variance_adaptor,\n            n_out=1,\n            kernel_size=self.args.kernel_size_variance_adaptor,\n            emb_kernel_size=self.args.emb_kernel_size_variance_adaptor,\n            p_dropout=self.args.dropout_variance_adaptor,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n        self.energy_adaptor = EnergyAdaptor(\n            channels_in=self.args.n_hidden_conformer_encoder,\n            channels_hidden=self.args.n_hidden_variance_adaptor,\n            channels_out=1,\n            kernel_size=self.args.kernel_size_variance_adaptor,\n            emb_kernel_size=self.args.emb_kernel_size_variance_adaptor,\n            dropout=self.args.dropout_variance_adaptor,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n\n        self.aligner = AlignmentNetwork(\n            in_query_channels=self.args.out_channels,\n            in_key_channels=self.args.n_hidden_conformer_encoder,\n        )\n\n        self.duration_predictor = VariancePredictor(\n            channels_in=self.args.n_hidden_conformer_encoder,\n            channels=self.args.n_hidden_variance_adaptor,\n            channels_out=1,\n            kernel_size=self.args.kernel_size_variance_adaptor,\n            p_dropout=self.args.dropout_variance_adaptor,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n\n        self.utterance_prosody_encoder = UtteranceLevelProsodyEncoder(\n            num_mels=self.args.num_mels,\n            ref_enc_filters=self.args.ref_enc_filters_reference_encoder,\n            ref_enc_size=self.args.ref_enc_size_reference_encoder,\n            ref_enc_gru_size=self.args.ref_enc_gru_size_reference_encoder,\n            ref_enc_strides=self.args.ref_enc_strides_reference_encoder,\n            n_hidden=self.args.n_hidden_conformer_encoder,\n            dropout=self.args.dropout_conformer_encoder,\n            bottleneck_size_u=self.args.bottleneck_size_u_reference_encoder,\n            token_num=self.args.token_num_reference_encoder,\n        )\n\n        self.utterance_prosody_predictor = PhonemeProsodyPredictor(\n            hidden_size=self.args.n_hidden_conformer_encoder,\n            kernel_size=self.args.predictor_kernel_size_reference_encoder,\n            dropout=self.args.dropout_conformer_encoder,\n            bottleneck_size=self.args.bottleneck_size_u_reference_encoder,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n\n        self.phoneme_prosody_encoder = PhonemeLevelProsodyEncoder(\n            num_mels=self.args.num_mels,\n            ref_enc_filters=self.args.ref_enc_filters_reference_encoder,\n            ref_enc_size=self.args.ref_enc_size_reference_encoder,\n            ref_enc_gru_size=self.args.ref_enc_gru_size_reference_encoder,\n            ref_enc_strides=self.args.ref_enc_strides_reference_encoder,\n            n_hidden=self.args.n_hidden_conformer_encoder,\n            dropout=self.args.dropout_conformer_encoder,\n            bottleneck_size_p=self.args.bottleneck_size_p_reference_encoder,\n            n_heads=self.args.n_heads_conformer_encoder,\n        )\n\n        self.phoneme_prosody_predictor = PhonemeProsodyPredictor(\n            hidden_size=self.args.n_hidden_conformer_encoder,\n            kernel_size=self.args.predictor_kernel_size_reference_encoder,\n            dropout=self.args.dropout_conformer_encoder,\n            bottleneck_size=self.args.bottleneck_size_p_reference_encoder,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n\n        self.u_bottle_out = nn.Linear(\n            self.args.bottleneck_size_u_reference_encoder,\n            self.args.n_hidden_conformer_encoder,\n        )\n\n        self.u_norm = nn.InstanceNorm1d(self.args.bottleneck_size_u_reference_encoder)\n        self.p_bottle_out = nn.Linear(\n            self.args.bottleneck_size_p_reference_encoder,\n            self.args.n_hidden_conformer_encoder,\n        )\n        self.p_norm = nn.InstanceNorm1d(\n            self.args.bottleneck_size_p_reference_encoder,\n        )\n        self.decoder = Conformer(\n            dim=self.args.n_hidden_conformer_decoder,\n            n_layers=self.args.n_layers_conformer_decoder,\n            n_heads=self.args.n_heads_conformer_decoder,\n            speaker_embedding_dim=self.embedded_speaker_dim,\n            p_dropout=self.args.dropout_conformer_decoder,\n            kernel_size_conv_mod=self.args.kernel_size_conv_mod_conformer_decoder,\n            lrelu_slope=self.args.lrelu_slope,\n        )\n\n        padding_idx = self.tokenizer.characters.pad_id\n        self.src_word_emb = EmbeddingPadded(\n            self.args.num_chars, self.args.n_hidden_conformer_encoder, padding_idx=padding_idx\n        )\n        self.to_mel = nn.Linear(\n            self.args.n_hidden_conformer_decoder,\n            self.args.num_mels,\n        )\n\n        self.energy_scaler = torch.nn.BatchNorm1d(1, affine=False, track_running_stats=True, momentum=None)\n        self.energy_scaler.requires_grad_(False)\n\n    def init_multispeaker(self, args: Coqpit):  # pylint: disable=unused-argument\n        \"\"\"Init for multi-speaker training.\"\"\"\n        self.embedded_speaker_dim = 0\n        self.num_speakers = self.args.num_speakers\n        self.audio_transform = None\n\n        if self.speaker_manager:\n            self.num_speakers = self.speaker_manager.num_speakers\n\n        if self.args.use_speaker_embedding:\n            self._init_speaker_embedding()\n\n        if self.args.use_d_vector_file:\n            self._init_d_vector()\n\n    @staticmethod\n    def _set_cond_input(aux_input: Dict):\n        \"\"\"Set the speaker conditioning input based on the multi-speaker mode.\"\"\"\n        sid, g, lid, durations = None, None, None, None\n        if \"speaker_ids\" in aux_input and aux_input[\"speaker_ids\"] is not None:\n            sid = aux_input[\"speaker_ids\"]\n            if sid.ndim == 0:\n                sid = sid.unsqueeze_(0)\n        if \"d_vectors\" in aux_input and aux_input[\"d_vectors\"] is not None:\n            g = F.normalize(aux_input[\"d_vectors\"])  # .unsqueeze_(-1)\n            if g.ndim == 2:\n                g = g  #  .unsqueeze_(0) # pylint: disable=self-assigning-variable\n\n        if \"durations\" in aux_input and aux_input[\"durations\"] is not None:\n            durations = aux_input[\"durations\"]\n\n        return sid, g, lid, durations\n\n    def get_aux_input(self, aux_input: Dict):\n        sid, g, lid, _ = self._set_cond_input(aux_input)\n        return {\"speaker_ids\": sid, \"style_wav\": None, \"d_vectors\": g, \"language_ids\": lid}\n\n    def _set_speaker_input(self, aux_input: Dict):\n        d_vectors = aux_input.get(\"d_vectors\", None)\n        speaker_ids = aux_input.get(\"speaker_ids\", None)\n\n        if d_vectors is not None and speaker_ids is not None:\n            raise ValueError(\"[!] Cannot use d-vectors and speaker-ids together.\")\n\n        if speaker_ids is not None and not hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Cannot use speaker-ids without enabling speaker embedding.\")\n\n        g = speaker_ids if speaker_ids is not None else d_vectors\n        return g\n\n    # def set_embedding_dims(self):\n    #     if self.embedded_speaker_dim > 0:\n    #         self.embedding_dims = self.embedded_speaker_dim\n    #     else:\n    #         self.embedding_dims = 0\n\n    def _init_speaker_embedding(self):\n        # pylint: disable=attribute-defined-outside-init\n        if self.num_speakers > 0:\n            print(\" > initialization of speaker-embedding layers.\")\n            self.embedded_speaker_dim = self.args.speaker_embedding_channels\n            self.emb_g = nn.Embedding(self.num_speakers, self.embedded_speaker_dim)\n\n    def _init_d_vector(self):\n        # pylint: disable=attribute-defined-outside-init\n        if hasattr(self, \"emb_g\"):\n            raise ValueError(\"[!] Speaker embedding layer already initialized before d_vector settings.\")\n        self.embedded_speaker_dim = self.args.d_vector_dim\n\n    @staticmethod\n    def generate_attn(dr, x_mask, y_mask=None):\n        \"\"\"Generate an attention mask from the linear scale durations.\n\n        Args:\n            dr (Tensor): Linear scale durations.\n            x_mask (Tensor): Mask for the input (character) sequence.\n            y_mask (Tensor): Mask for the output (spectrogram) sequence. Compute it from the predicted durations\n                if None. Defaults to None.\n\n        Shapes\n           - dr: :math:`(B, T_{en})`\n           - x_mask: :math:`(B, T_{en})`\n           - y_mask: :math:`(B, T_{de})`\n        \"\"\"\n        # compute decode mask from the durations\n        if y_mask is None:\n            y_lengths = dr.sum(1).long()\n            y_lengths[y_lengths < 1] = 1\n            y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(dr.dtype)\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n        attn = generate_path(dr, attn_mask.squeeze(1)).to(dr.dtype)\n        return attn\n\n    def _expand_encoder_with_durations(\n        self,\n        o_en: torch.FloatTensor,\n        dr: torch.IntTensor,\n        x_mask: torch.IntTensor,\n        y_lengths: torch.IntTensor,\n    ):\n        y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(o_en.dtype)\n        attn = self.generate_attn(dr, x_mask, y_mask)\n        o_en_ex = torch.einsum(\"kmn, kjm -> kjn\", [attn.float(), o_en])\n        return y_mask, o_en_ex, attn.transpose(1, 2)\n\n    def _forward_aligner(\n        self,\n        x: torch.FloatTensor,\n        y: torch.FloatTensor,\n        x_mask: torch.IntTensor,\n        y_mask: torch.IntTensor,\n        attn_priors: torch.FloatTensor,\n    ) -> Tuple[torch.IntTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Aligner forward pass.\n\n        1. Compute a mask to apply to the attention map.\n        2. Run the alignment network.\n        3. Apply MAS to compute the hard alignment map.\n        4. Compute the durations from the hard alignment map.\n\n        Args:\n            x (torch.FloatTensor): Input sequence.\n            y (torch.FloatTensor): Output sequence.\n            x_mask (torch.IntTensor): Input sequence mask.\n            y_mask (torch.IntTensor): Output sequence mask.\n            attn_priors (torch.FloatTensor): Prior for the aligner network map.\n\n        Returns:\n            Tuple[torch.IntTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n                Durations from the hard alignment map, soft alignment potentials, log scale alignment potentials,\n                hard alignment map.\n\n        Shapes:\n            - x: :math:`[B, T_en, C_en]`\n            - y: :math:`[B, T_de, C_de]`\n            - x_mask: :math:`[B, 1, T_en]`\n            - y_mask: :math:`[B, 1, T_de]`\n            - attn_priors: :math:`[B, T_de, T_en]`\n\n            - aligner_durations: :math:`[B, T_en]`\n            - aligner_soft: :math:`[B, T_de, T_en]`\n            - aligner_logprob: :math:`[B, 1, T_de, T_en]`\n            - aligner_mas: :math:`[B, T_de, T_en]`\n        \"\"\"\n        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)  # [B, 1, T_en, T_de]\n        aligner_soft, aligner_logprob = self.aligner(y.transpose(1, 2), x.transpose(1, 2), x_mask, attn_priors)\n        aligner_mas = maximum_path(\n            aligner_soft.squeeze(1).transpose(1, 2).contiguous(), attn_mask.squeeze(1).contiguous()\n        )\n        aligner_durations = torch.sum(aligner_mas, -1).int()\n        aligner_soft = aligner_soft.squeeze(1)  # [B, T_max2, T_max]\n        aligner_mas = aligner_mas.transpose(1, 2)  # [B, T_max, T_max2] -> [B, T_max2, T_max]\n        return aligner_durations, aligner_soft, aligner_logprob, aligner_mas\n\n    def average_utterance_prosody(  # pylint: disable=no-self-use\n        self, u_prosody_pred: torch.Tensor, src_mask: torch.Tensor\n    ) -> torch.Tensor:\n        lengths = ((~src_mask) * 1.0).sum(1)\n        u_prosody_pred = u_prosody_pred.sum(1, keepdim=True) / lengths.view(-1, 1, 1)\n        return u_prosody_pred\n\n    def forward(\n        self,\n        tokens: torch.Tensor,\n        src_lens: torch.Tensor,\n        mels: torch.Tensor,\n        mel_lens: torch.Tensor,\n        pitches: torch.Tensor,\n        energies: torch.Tensor,\n        attn_priors: torch.Tensor,\n        use_ground_truth: bool = True,\n        d_vectors: torch.Tensor = None,\n        speaker_idx: torch.Tensor = None,\n    ) -> Dict[str, torch.Tensor]:\n        sid, g, lid, _ = self._set_cond_input(  # pylint: disable=unused-variable\n            {\"d_vectors\": d_vectors, \"speaker_ids\": speaker_idx}\n        )  # pylint: disable=unused-variable\n\n        src_mask = get_mask_from_lengths(src_lens)  # [B, T_src]\n        mel_mask = get_mask_from_lengths(mel_lens)  # [B, T_mel]\n\n        # Token embeddings\n        token_embeddings = self.src_word_emb(tokens)  # [B, T_src, C_hidden]\n        token_embeddings = token_embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n\n        # Alignment network and durations\n        aligner_durations, aligner_soft, aligner_logprob, aligner_mas = self._forward_aligner(\n            x=token_embeddings,\n            y=mels.transpose(1, 2),\n            x_mask=~src_mask[:, None],\n            y_mask=~mel_mask[:, None],\n            attn_priors=attn_priors,\n        )\n        dr = aligner_durations  # [B, T_en]\n\n        # Embeddings\n        speaker_embedding = None\n        if d_vectors is not None:\n            speaker_embedding = g\n        elif speaker_idx is not None:\n            speaker_embedding = F.normalize(self.emb_g(sid))\n\n        pos_encoding = positional_encoding(\n            self.emb_dim,\n            max(token_embeddings.shape[1], max(mel_lens)),\n            device=token_embeddings.device,\n        )\n        encoder_outputs = self.encoder(\n            token_embeddings,\n            src_mask,\n            speaker_embedding=speaker_embedding,\n            encoding=pos_encoding,\n        )\n\n        u_prosody_ref = self.u_norm(self.utterance_prosody_encoder(mels=mels, mel_lens=mel_lens))\n        u_prosody_pred = self.u_norm(\n            self.average_utterance_prosody(\n                u_prosody_pred=self.utterance_prosody_predictor(x=encoder_outputs, mask=src_mask),\n                src_mask=src_mask,\n            )\n        )\n\n        if use_ground_truth:\n            encoder_outputs = encoder_outputs + self.u_bottle_out(u_prosody_ref)\n        else:\n            encoder_outputs = encoder_outputs + self.u_bottle_out(u_prosody_pred)\n\n        p_prosody_ref = self.p_norm(\n            self.phoneme_prosody_encoder(\n                x=encoder_outputs, src_mask=src_mask, mels=mels, mel_lens=mel_lens, encoding=pos_encoding\n            )\n        )\n        p_prosody_pred = self.p_norm(self.phoneme_prosody_predictor(x=encoder_outputs, mask=src_mask))\n\n        if use_ground_truth:\n            encoder_outputs = encoder_outputs + self.p_bottle_out(p_prosody_ref)\n        else:\n            encoder_outputs = encoder_outputs + self.p_bottle_out(p_prosody_pred)\n\n        encoder_outputs_res = encoder_outputs\n\n        pitch_pred, avg_pitch_target, pitch_emb = self.pitch_adaptor.get_pitch_embedding_train(\n            x=encoder_outputs,\n            target=pitches,\n            dr=dr,\n            mask=src_mask,\n        )\n\n        energy_pred, avg_energy_target, energy_emb = self.energy_adaptor.get_energy_embedding_train(\n            x=encoder_outputs,\n            target=energies,\n            dr=dr,\n            mask=src_mask,\n        )\n\n        encoder_outputs = encoder_outputs.transpose(1, 2) + pitch_emb + energy_emb\n        log_duration_prediction = self.duration_predictor(x=encoder_outputs_res.detach(), mask=src_mask)\n\n        mel_pred_mask, encoder_outputs_ex, alignments = self._expand_encoder_with_durations(\n            o_en=encoder_outputs, y_lengths=mel_lens, dr=dr, x_mask=~src_mask[:, None]\n        )\n\n        x = self.decoder(\n            encoder_outputs_ex.transpose(1, 2),\n            mel_mask,\n            speaker_embedding=speaker_embedding,\n            encoding=pos_encoding,\n        )\n        x = self.to_mel(x)\n\n        dr = torch.log(dr + 1)\n\n        dr_pred = torch.exp(log_duration_prediction) - 1\n        alignments_dp = self.generate_attn(dr_pred, src_mask.unsqueeze(1), mel_pred_mask)  # [B, T_max, T_max2']\n\n        return {\n            \"model_outputs\": x,\n            \"pitch_pred\": pitch_pred,\n            \"pitch_target\": avg_pitch_target,\n            \"energy_pred\": energy_pred,\n            \"energy_target\": avg_energy_target,\n            \"u_prosody_pred\": u_prosody_pred,\n            \"u_prosody_ref\": u_prosody_ref,\n            \"p_prosody_pred\": p_prosody_pred,\n            \"p_prosody_ref\": p_prosody_ref,\n            \"alignments_dp\": alignments_dp,\n            \"alignments\": alignments,  # [B, T_de, T_en]\n            \"aligner_soft\": aligner_soft,\n            \"aligner_mas\": aligner_mas,\n            \"aligner_durations\": aligner_durations,\n            \"aligner_logprob\": aligner_logprob,\n            \"dr_log_pred\": log_duration_prediction.squeeze(1),  # [B, T]\n            \"dr_log_target\": dr.squeeze(1),  # [B, T]\n            \"spk_emb\": speaker_embedding,\n        }\n\n    @torch.no_grad()\n    def inference(\n        self,\n        tokens: torch.Tensor,\n        speaker_idx: torch.Tensor,\n        p_control: float = None,  # TODO # pylint: disable=unused-argument\n        d_control: float = None,  # TODO # pylint: disable=unused-argument\n        d_vectors: torch.Tensor = None,\n        pitch_transform: Callable = None,\n        energy_transform: Callable = None,\n    ) -> torch.Tensor:\n        src_mask = get_mask_from_lengths(torch.tensor([tokens.shape[1]], dtype=torch.int64, device=tokens.device))\n        src_lens = torch.tensor(tokens.shape[1:2]).to(tokens.device)  # pylint: disable=unused-variable\n        sid, g, lid, _ = self._set_cond_input(  # pylint: disable=unused-variable\n            {\"d_vectors\": d_vectors, \"speaker_ids\": speaker_idx}\n        )  # pylint: disable=unused-variable\n\n        token_embeddings = self.src_word_emb(tokens)\n        token_embeddings = token_embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n\n        # Embeddings\n        speaker_embedding = None\n        if d_vectors is not None:\n            speaker_embedding = g\n        elif speaker_idx is not None:\n            speaker_embedding = F.normalize(self.emb_g(sid))\n\n        pos_encoding = positional_encoding(\n            self.emb_dim,\n            token_embeddings.shape[1],\n            device=token_embeddings.device,\n        )\n        encoder_outputs = self.encoder(\n            token_embeddings,\n            src_mask,\n            speaker_embedding=speaker_embedding,\n            encoding=pos_encoding,\n        )\n\n        u_prosody_pred = self.u_norm(\n            self.average_utterance_prosody(\n                u_prosody_pred=self.utterance_prosody_predictor(x=encoder_outputs, mask=src_mask),\n                src_mask=src_mask,\n            )\n        )\n        encoder_outputs = encoder_outputs + self.u_bottle_out(u_prosody_pred).expand_as(encoder_outputs)\n\n        p_prosody_pred = self.p_norm(\n            self.phoneme_prosody_predictor(\n                x=encoder_outputs,\n                mask=src_mask,\n            )\n        )\n        encoder_outputs = encoder_outputs + self.p_bottle_out(p_prosody_pred).expand_as(encoder_outputs)\n\n        encoder_outputs_res = encoder_outputs\n\n        pitch_emb_pred, pitch_pred = self.pitch_adaptor.get_pitch_embedding(\n            x=encoder_outputs,\n            mask=src_mask,\n            pitch_transform=pitch_transform,\n            pitch_mean=self.pitch_mean if hasattr(self, \"pitch_mean\") else None,\n            pitch_std=self.pitch_std if hasattr(self, \"pitch_std\") else None,\n        )\n\n        energy_emb_pred, energy_pred = self.energy_adaptor.get_energy_embedding(\n            x=encoder_outputs, mask=src_mask, energy_transform=energy_transform\n        )\n        encoder_outputs = encoder_outputs.transpose(1, 2) + pitch_emb_pred + energy_emb_pred\n\n        log_duration_pred = self.duration_predictor(\n            x=encoder_outputs_res.detach(), mask=src_mask\n        )  # [B, C_hidden, T_src] -> [B, T_src]\n        duration_pred = (torch.exp(log_duration_pred) - 1) * (~src_mask) * self.length_scale  # -> [B, T_src]\n        duration_pred[duration_pred < 1] = 1.0  # -> [B, T_src]\n        duration_pred = torch.round(duration_pred)  # -> [B, T_src]\n        mel_lens = duration_pred.sum(1)  # -> [B,]\n\n        _, encoder_outputs_ex, alignments = self._expand_encoder_with_durations(\n            o_en=encoder_outputs, y_lengths=mel_lens, dr=duration_pred.squeeze(1), x_mask=~src_mask[:, None]\n        )\n\n        mel_mask = get_mask_from_lengths(\n            torch.tensor([encoder_outputs_ex.shape[2]], dtype=torch.int64, device=encoder_outputs_ex.device)\n        )\n\n        if encoder_outputs_ex.shape[1] > pos_encoding.shape[1]:\n            encoding = positional_encoding(self.emb_dim, encoder_outputs_ex.shape[2], device=tokens.device)\n\n        # [B, C_hidden, T_src], [B, 1, T_src], [B, C_emb], [B, T_src, C_hidden] -> [B, C_hidden, T_src]\n        x = self.decoder(\n            encoder_outputs_ex.transpose(1, 2),\n            mel_mask,\n            speaker_embedding=speaker_embedding,\n            encoding=encoding,\n        )\n        x = self.to_mel(x)\n        outputs = {\n            \"model_outputs\": x,\n            \"alignments\": alignments,\n            # \"pitch\": pitch_emb_pred,\n            \"durations\": duration_pred,\n            \"pitch\": pitch_pred,\n            \"energy\": energy_pred,\n            \"spk_emb\": speaker_embedding,\n        }\n        return outputs\n", "TTS/tts/layers/delightful_tts/phoneme_prosody_predictor.py": "import torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\n\nfrom TTS.tts.layers.delightful_tts.conv_layers import ConvTransposed\n\n\nclass PhonemeProsodyPredictor(nn.Module):\n    \"\"\"Non-parallel Prosody Predictor inspired by: https://arxiv.org/pdf/2102.00851.pdf\n    It consists of 2 layers of  1D convolutions each followed by a relu activation, layer norm\n    and dropout, then finally a linear layer.\n\n    Args:\n        hidden_size (int): Size of hidden channels.\n        kernel_size (int): Kernel size for the conv layers.\n        dropout: (float): Probability of dropout.\n        bottleneck_size (int): bottleneck size for last linear layer.\n        lrelu_slope (float): Slope of the leaky relu.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        kernel_size: int,\n        dropout: float,\n        bottleneck_size: int,\n        lrelu_slope: float,\n    ):\n        super().__init__()\n        self.d_model = hidden_size\n        self.layers = nn.ModuleList(\n            [\n                ConvTransposed(\n                    self.d_model,\n                    self.d_model,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(lrelu_slope),\n                nn.LayerNorm(self.d_model),\n                nn.Dropout(dropout),\n                ConvTransposed(\n                    self.d_model,\n                    self.d_model,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(lrelu_slope),\n                nn.LayerNorm(self.d_model),\n                nn.Dropout(dropout),\n            ]\n        )\n        self.predictor_bottleneck = nn.Linear(self.d_model, bottleneck_size)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T, D]`\n            mask: :math: `[B, T]`\n        \"\"\"\n        mask = mask.unsqueeze(2)\n        for layer in self.layers:\n            x = layer(x)\n        x = x.masked_fill(mask, 0.0)\n        x = self.predictor_bottleneck(x)\n        return x\n", "TTS/tts/layers/delightful_tts/conformer.py": "### credit: https://github.com/dunky11/voicesmith\nimport math\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\nimport torch.nn.functional as F\n\nfrom TTS.tts.layers.delightful_tts.conv_layers import Conv1dGLU, DepthWiseConv1d, PointwiseConv1d\nfrom TTS.tts.layers.delightful_tts.networks import GLUActivation\n\n\ndef calc_same_padding(kernel_size: int) -> Tuple[int, int]:\n    pad = kernel_size // 2\n    return (pad, pad - (kernel_size + 1) % 2)\n\n\nclass Conformer(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        n_layers: int,\n        n_heads: int,\n        speaker_embedding_dim: int,\n        p_dropout: float,\n        kernel_size_conv_mod: int,\n        lrelu_slope: float,\n    ):\n        \"\"\"\n        A Transformer variant that integrates both CNNs and Transformers components.\n        Conformer proposes a novel combination of self-attention and convolution, in which self-attention\n        learns the global interaction while the convolutions efficiently capture the local correlations.\n\n        Args:\n            dim (int): Number of the dimensions for the model.\n            n_layers (int): Number of model layers.\n            n_heads (int): The number of attention heads.\n            speaker_embedding_dim (int): Number of speaker embedding dimensions.\n            p_dropout (float): Probabilty of dropout.\n            kernel_size_conv_mod (int): Size of kernels for convolution modules.\n\n        Inputs: inputs, mask\n            - **inputs** (batch, time, dim): Tensor containing input vector\n            - **encoding** (batch, time, dim): Positional embedding tensor\n            - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n        Returns:\n            - **outputs** (batch, time, dim): Tensor produced by Conformer Encoder.\n        \"\"\"\n        super().__init__()\n        d_k = d_v = dim // n_heads\n        self.layer_stack = nn.ModuleList(\n            [\n                ConformerBlock(\n                    dim,\n                    n_heads,\n                    d_k,\n                    d_v,\n                    kernel_size_conv_mod=kernel_size_conv_mod,\n                    dropout=p_dropout,\n                    speaker_embedding_dim=speaker_embedding_dim,\n                    lrelu_slope=lrelu_slope,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n        speaker_embedding: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, T_src, C]`\n            - mask: :math: `[B]`\n            - speaker_embedding: :math: `[B, C]`\n            - encoding: :math: `[B, T_max2, C]`\n        \"\"\"\n\n        attn_mask = mask.view((mask.shape[0], 1, 1, mask.shape[1]))\n        for enc_layer in self.layer_stack:\n            x = enc_layer(\n                x,\n                mask=mask,\n                slf_attn_mask=attn_mask,\n                speaker_embedding=speaker_embedding,\n                encoding=encoding,\n            )\n        return x\n\n\nclass ConformerBlock(torch.nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        d_k: int,  # pylint: disable=unused-argument\n        d_v: int,  # pylint: disable=unused-argument\n        kernel_size_conv_mod: int,\n        speaker_embedding_dim: int,\n        dropout: float,\n        lrelu_slope: float = 0.3,\n    ):\n        \"\"\"\n        A Conformer block is composed of four modules stacked together,\n        A feed-forward module, a self-attention module, a convolution module,\n        and a second feed-forward module in the end. The block starts with two Feed forward\n        modules sandwiching the Multi-Headed Self-Attention module and the Conv module.\n\n        Args:\n            d_model (int): The dimension of model\n            n_head (int): The number of attention heads.\n            kernel_size_conv_mod (int): Size of kernels for convolution modules.\n            speaker_embedding_dim (int): Number of speaker embedding dimensions.\n            emotion_embedding_dim (int): Number of emotion embedding dimensions.\n            dropout (float): Probabilty of dropout.\n\n        Inputs: inputs, mask\n            - **inputs** (batch, time, dim): Tensor containing input vector\n            - **encoding** (batch, time, dim): Positional embedding tensor\n            - **slf_attn_mask** (batch, 1, 1, time1): Tensor containing indices to be masked in self attention module\n            - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n        Returns:\n            - **outputs** (batch, time, dim): Tensor produced by the Conformer Block.\n        \"\"\"\n        super().__init__()\n        if isinstance(speaker_embedding_dim, int):\n            self.conditioning = Conv1dGLU(\n                d_model=d_model,\n                kernel_size=kernel_size_conv_mod,\n                padding=kernel_size_conv_mod // 2,\n                embedding_dim=speaker_embedding_dim,\n            )\n\n        self.ff = FeedForward(d_model=d_model, dropout=dropout, kernel_size=3, lrelu_slope=lrelu_slope)\n        self.conformer_conv_1 = ConformerConvModule(\n            d_model, kernel_size=kernel_size_conv_mod, dropout=dropout, lrelu_slope=lrelu_slope\n        )\n        self.ln = nn.LayerNorm(d_model)\n        self.slf_attn = ConformerMultiHeadedSelfAttention(d_model=d_model, num_heads=n_head, dropout_p=dropout)\n        self.conformer_conv_2 = ConformerConvModule(\n            d_model, kernel_size=kernel_size_conv_mod, dropout=dropout, lrelu_slope=lrelu_slope\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        speaker_embedding: torch.Tensor,\n        mask: torch.Tensor,\n        slf_attn_mask: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, T_src, C]`\n            - mask: :math: `[B]`\n            - slf_attn_mask: :math: `[B, 1, 1, T_src]`\n            - speaker_embedding: :math: `[B, C]`\n            - emotion_embedding: :math: `[B, C]`\n            - encoding: :math: `[B, T_max2, C]`\n        \"\"\"\n        if speaker_embedding is not None:\n            x = self.conditioning(x, embeddings=speaker_embedding)\n        x = self.ff(x) + x\n        x = self.conformer_conv_1(x) + x\n        res = x\n        x = self.ln(x)\n        x, _ = self.slf_attn(query=x, key=x, value=x, mask=slf_attn_mask, encoding=encoding)\n        x = x + res\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n\n        x = self.conformer_conv_2(x) + x\n        return x\n\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        kernel_size: int,\n        dropout: float,\n        lrelu_slope: float,\n        expansion_factor: int = 4,\n    ):\n        \"\"\"\n        Feed Forward module for conformer block.\n\n        Args:\n            d_model (int): The dimension of model.\n            kernel_size (int): Size of the kernels for conv layers.\n            dropout (float): probability of dropout.\n            expansion_factor (int): The factor by which to project the number of channels.\n            lrelu_slope (int): the negative slope factor for the leaky relu activation.\n\n        Inputs: inputs\n            - **inputs** (batch, time, dim): Tensor containing input vector\n        Returns:\n            - **outputs** (batch, time, dim): Tensor produced by the feed forward module.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(d_model)\n        self.conv_1 = nn.Conv1d(\n            d_model,\n            d_model * expansion_factor,\n            kernel_size=kernel_size,\n            padding=kernel_size // 2,\n        )\n        self.act = nn.LeakyReLU(lrelu_slope)\n        self.conv_2 = nn.Conv1d(d_model * expansion_factor, d_model, kernel_size=1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \"\"\"\n        x = self.ln(x)\n        x = x.permute((0, 2, 1))\n        x = self.conv_1(x)\n        x = x.permute((0, 2, 1))\n        x = self.act(x)\n        x = self.dropout(x)\n        x = x.permute((0, 2, 1))\n        x = self.conv_2(x)\n        x = x.permute((0, 2, 1))\n        x = self.dropout(x)\n        x = 0.5 * x\n        return x\n\n\nclass ConformerConvModule(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        expansion_factor: int = 2,\n        kernel_size: int = 7,\n        dropout: float = 0.1,\n        lrelu_slope: float = 0.3,\n    ):\n        \"\"\"\n        Convolution module for conformer. Starts with a gating machanism.\n        a pointwise convolution and a gated linear unit (GLU). This is followed\n        by a single 1-D depthwise convolution layer. Batchnorm is deployed just after the convolution\n        to help with training. it also contains an expansion factor to project the number of channels.\n\n        Args:\n            d_model (int): The dimension of model.\n            expansion_factor (int): The factor by which to project the number of channels.\n            kernel_size (int): Size of kernels for convolution modules.\n            dropout (float): Probabilty of dropout.\n            lrelu_slope (float): The slope coefficient for leaky relu activation.\n\n        Inputs: inputs\n            - **inputs** (batch, time, dim): Tensor containing input vector\n        Returns:\n            - **outputs** (batch, time, dim): Tensor produced by the conv module.\n\n        \"\"\"\n        super().__init__()\n        inner_dim = d_model * expansion_factor\n        self.ln_1 = nn.LayerNorm(d_model)\n        self.conv_1 = PointwiseConv1d(d_model, inner_dim * 2)\n        self.conv_act = GLUActivation(slope=lrelu_slope)\n        self.depthwise = DepthWiseConv1d(\n            inner_dim,\n            inner_dim,\n            kernel_size=kernel_size,\n            padding=calc_same_padding(kernel_size)[0],\n        )\n        self.ln_2 = nn.GroupNorm(1, inner_dim)\n        self.activation = nn.LeakyReLU(lrelu_slope)\n        self.conv_2 = PointwiseConv1d(inner_dim, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T, C]`\n        \"\"\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        return x\n\n\nclass ConformerMultiHeadedSelfAttention(nn.Module):\n    \"\"\"\n    Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL,\n    the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention\n    module to generalize better on different input length and the resulting encoder is more robust to the variance of\n    the utterance length. Conformer use prenorm residual units with dropout which helps training\n    and regularizing deeper models.\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.\n        dropout_p (float): probability of dropout\n    Inputs: inputs, mask\n        - **inputs** (batch, time, dim): Tensor containing input vector\n        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n    Returns:\n        - **outputs** (batch, time, dim): Tensor produces by relative multi headed self attention module.\n    \"\"\"\n\n    def __init__(self, d_model: int, num_heads: int, dropout_p: float):\n        super().__init__()\n        self.attention = RelativeMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.dropout = nn.Dropout(p=dropout_p)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        batch_size, seq_length, _ = key.size()  # pylint: disable=unused-variable\n        encoding = encoding[:, : key.shape[1]]\n        encoding = encoding.repeat(batch_size, 1, 1)\n        outputs, attn = self.attention(query, key, value, pos_embedding=encoding, mask=mask)\n        outputs = self.dropout(outputs)\n        return outputs, attn\n\n\nclass RelativeMultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention with relative positional encoding.\n    This concept was proposed in the \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.\n    Inputs: query, key, value, pos_embedding, mask\n        - **query** (batch, time, dim): Tensor containing query vector\n        - **key** (batch, time, dim): Tensor containing key vector\n        - **value** (batch, time, dim): Tensor containing value vector\n        - **pos_embedding** (batch, time, dim): Positional embedding tensor\n        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n    Returns:\n        - **outputs**: Tensor produces by relative multi head attention module.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 512,\n        num_heads: int = 16,\n    ):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n        self.d_model = d_model\n        self.d_head = int(d_model / num_heads)\n        self.num_heads = num_heads\n        self.sqrt_dim = math.sqrt(d_model)\n\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.pos_proj = nn.Linear(d_model, d_model, bias=False)\n\n        self.u_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n        self.v_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n        torch.nn.init.xavier_uniform_(self.u_bias)\n        torch.nn.init.xavier_uniform_(self.v_bias)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        pos_embedding: torch.Tensor,\n        mask: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        batch_size = query.shape[0]\n        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)\n        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n        pos_embedding = self.pos_proj(pos_embedding).view(batch_size, -1, self.num_heads, self.d_head)\n        u_bias = self.u_bias.expand_as(query)\n        v_bias = self.v_bias.expand_as(query)\n        a = (query + u_bias).transpose(1, 2)\n        content_score = a @ key.transpose(2, 3)\n        b = (query + v_bias).transpose(1, 2)\n        pos_score = b @ pos_embedding.permute(0, 2, 3, 1)\n        pos_score = self._relative_shift(pos_score)\n\n        score = content_score + pos_score\n        score = score * (1.0 / self.sqrt_dim)\n\n        score.masked_fill_(mask, -1e9)\n\n        attn = F.softmax(score, -1)\n\n        context = (attn @ value).transpose(1, 2)\n        context = context.contiguous().view(batch_size, -1, self.d_model)\n\n        return self.out_proj(context), attn\n\n    def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:  # pylint: disable=no-self-use\n        batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n        zeros = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device)\n        padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n        padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n        pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n        return pos_score\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    input:\n        query --- [N, T_q, query_dim]\n        key --- [N, T_k, key_dim]\n    output:\n        out --- [N, T_q, num_units]\n    \"\"\"\n\n    def __init__(self, query_dim: int, key_dim: int, num_units: int, num_heads: int):\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor) -> torch.Tensor:\n        querys = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key)  # [N, T_k, num_units]\n        values = self.W_value(key)\n        split_size = self.num_units // self.num_heads\n        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n        # score = softmax(QK^T / (d_k ** 0.5))\n        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores = scores / (self.key_dim**0.5)\n        scores = F.softmax(scores, dim=3)\n        # out = score * V\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n        return out\n", "TTS/tts/layers/delightful_tts/variance_predictor.py": "import torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\n\nfrom TTS.tts.layers.delightful_tts.conv_layers import ConvTransposed\n\n\nclass VariancePredictor(nn.Module):\n    \"\"\"\n    Network is 2-layer 1D convolutions with leaky relu activation and then\n    followed by layer normalization then a dropout layer and finally an\n    extra linear layer to project the hidden states into the output sequence.\n\n    Args:\n        channels_in (int): Number of in channels for conv layers.\n        channels_out (int): Number of out channels for the last linear layer.\n        kernel_size (int): Size the kernel for the conv layers.\n        p_dropout (float): Probability of dropout.\n        lrelu_slope (float): Slope for the leaky relu.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time, dim): Tensor containing input vector\n        - **mask** (batch, time): Tensor containing indices to be masked\n    Returns:\n        - **outputs** (batch, time): Tensor produced by last linear layer.\n    \"\"\"\n\n    def __init__(\n        self, channels_in: int, channels: int, channels_out: int, kernel_size: int, p_dropout: float, lrelu_slope: float\n    ):\n        super().__init__()\n\n        self.layers = nn.ModuleList(\n            [\n                ConvTransposed(\n                    channels_in,\n                    channels,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(lrelu_slope),\n                nn.LayerNorm(channels),\n                nn.Dropout(p_dropout),\n                ConvTransposed(\n                    channels,\n                    channels,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(lrelu_slope),\n                nn.LayerNorm(channels),\n                nn.Dropout(p_dropout),\n            ]\n        )\n\n        self.linear_layer = nn.Linear(channels, channels_out)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T_src, C]`\n            mask: :math: `[B, T_src]`\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x)\n        x = self.linear_layer(x)\n        x = x.squeeze(-1)\n        x = x.masked_fill(mask, 0.0)\n        return x\n", "TTS/tts/layers/delightful_tts/pitch_adaptor.py": "from typing import Callable, Tuple\n\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\n\nfrom TTS.tts.layers.delightful_tts.variance_predictor import VariancePredictor\nfrom TTS.tts.utils.helpers import average_over_durations\n\n\nclass PitchAdaptor(nn.Module):  # pylint: disable=abstract-method\n    \"\"\"Module to get pitch embeddings via pitch predictor\n\n    Args:\n        n_input (int): Number of pitch predictor input channels.\n        n_hidden (int): Number of pitch predictor hidden channels.\n        n_out (int): Number of pitch predictor out channels.\n        kernel size (int): Size of the kernel for conv layers.\n        emb_kernel_size (int): Size the kernel for the pitch embedding.\n        p_dropout (float): Probability of dropout.\n        lrelu_slope (float): Slope for the leaky relu.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time1, dim): Tensor containing input vector\n        - **target** (batch, 1, time2): Tensor containing the pitch target\n        - **dr** (batch, time1): Tensor containing aligner durations vector\n        - **mask** (batch, time1): Tensor containing indices to be masked\n    Returns:\n        - **pitch prediction** (batch, 1, time1): Tensor produced by pitch predictor\n        - **pitch embedding** (batch, channels, time1): Tensor produced pitch pitch adaptor\n        - **average pitch target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n    \"\"\"\n\n    def __init__(\n        self,\n        n_input: int,\n        n_hidden: int,\n        n_out: int,\n        kernel_size: int,\n        emb_kernel_size: int,\n        p_dropout: float,\n        lrelu_slope: float,\n    ):\n        super().__init__()\n        self.pitch_predictor = VariancePredictor(\n            channels_in=n_input,\n            channels=n_hidden,\n            channels_out=n_out,\n            kernel_size=kernel_size,\n            p_dropout=p_dropout,\n            lrelu_slope=lrelu_slope,\n        )\n        self.pitch_emb = nn.Conv1d(\n            1,\n            n_input,\n            kernel_size=emb_kernel_size,\n            padding=int((emb_kernel_size - 1) / 2),\n        )\n\n    def get_pitch_embedding_train(\n        self, x: torch.Tensor, target: torch.Tensor, dr: torch.IntTensor, mask: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Shapes:\n            x: :math: `[B, T_src, C]`\n            target: :math: `[B, 1, T_max2]`\n            dr: :math: `[B, T_src]`\n            mask: :math: `[B, T_src]`\n        \"\"\"\n        pitch_pred = self.pitch_predictor(x, mask)  # [B, T_src, C_hidden], [B, T_src] --> [B, T_src]\n        pitch_pred.unsqueeze_(1)  # --> [B, 1, T_src]\n        avg_pitch_target = average_over_durations(target, dr)  # [B, 1, T_mel], [B, T_src] --> [B, 1, T_src]\n        pitch_emb = self.pitch_emb(avg_pitch_target)  # [B, 1, T_src] --> [B, C_hidden, T_src]\n        return pitch_pred, avg_pitch_target, pitch_emb\n\n    def get_pitch_embedding(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n        pitch_transform: Callable,\n        pitch_mean: torch.Tensor,\n        pitch_std: torch.Tensor,\n    ) -> torch.Tensor:\n        pitch_pred = self.pitch_predictor(x, mask)\n        if pitch_transform is not None:\n            pitch_pred = pitch_transform(pitch_pred, (~mask).sum(), pitch_mean, pitch_std)\n        pitch_pred.unsqueeze_(1)\n        pitch_emb_pred = self.pitch_emb(pitch_pred)\n        return pitch_emb_pred, pitch_pred\n", "TTS/tts/layers/delightful_tts/__init__.py": "", "TTS/tts/layers/delightful_tts/networks.py": "import math\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\nimport torch.nn.functional as F\n\nfrom TTS.tts.layers.delightful_tts.conv_layers import ConvNorm\n\n\ndef initialize_embeddings(shape: Tuple[int]) -> torch.Tensor:\n    assert len(shape) == 2, \"Can only initialize 2-D embedding matrices ...\"\n    # Kaiming initialization\n    return torch.randn(shape) * np.sqrt(2 / shape[1])\n\n\ndef positional_encoding(d_model: int, length: int, device: torch.device) -> torch.Tensor:\n    pe = torch.zeros(length, d_model, device=device)\n    position = torch.arange(0, length, dtype=torch.float, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * -(math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    pe = pe.unsqueeze(0)\n    return pe\n\n\nclass BottleneckLayer(nn.Module):\n    \"\"\"\n    Bottleneck layer for reducing the dimensionality of a tensor.\n\n    Args:\n        in_dim: The number of input dimensions.\n        reduction_factor: The factor by which to reduce the number of dimensions.\n        norm: The normalization method to use. Can be \"weightnorm\" or \"instancenorm\".\n        non_linearity: The non-linearity to use. Can be \"relu\" or \"leakyrelu\".\n        kernel_size: The size of the convolutional kernel.\n        use_partial_padding: Whether to use partial padding with the convolutional kernel.\n\n    Shape:\n        - Input: :math:`[N, in_dim]` where `N` is the batch size and `in_dim` is the number of input dimensions.\n\n        - Output: :math:`[N, out_dim]` where `out_dim` is the number of output dimensions.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim,\n        reduction_factor,\n        norm=\"weightnorm\",\n        non_linearity=\"relu\",\n        kernel_size=3,\n        use_partial_padding=False,  # pylint: disable=unused-argument\n    ):\n        super(BottleneckLayer, self).__init__()  # pylint: disable=super-with-arguments\n\n        self.reduction_factor = reduction_factor\n        reduced_dim = int(in_dim / reduction_factor)\n        self.out_dim = reduced_dim\n        if self.reduction_factor > 1:\n            fn = ConvNorm(in_dim, reduced_dim, kernel_size=kernel_size, use_weight_norm=(norm == \"weightnorm\"))\n            if norm == \"instancenorm\":\n                fn = nn.Sequential(fn, nn.InstanceNorm1d(reduced_dim, affine=True))\n\n            self.projection_fn = fn\n            self.non_linearity = nn.ReLU()\n            if non_linearity == \"leakyrelu\":\n                self.non_linearity = nn.LeakyReLU()\n\n    def forward(self, x):\n        if self.reduction_factor > 1:\n            x = self.projection_fn(x)\n            x = self.non_linearity(x)\n        return x\n\n\nclass GLUActivation(nn.Module):\n    \"\"\"Class that implements the Gated Linear Unit (GLU) activation function.\n\n    The GLU activation function is a variant of the Leaky ReLU activation function,\n    where the output of the activation function is gated by an input tensor.\n\n    \"\"\"\n\n    def __init__(self, slope: float):\n        super().__init__()\n        self.lrelu = nn.LeakyReLU(slope)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out, gate = x.chunk(2, dim=1)\n        x = out * self.lrelu(gate)\n        return x\n\n\nclass StyleEmbedAttention(nn.Module):\n    def __init__(self, query_dim: int, key_dim: int, num_units: int, num_heads: int):\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n\n    def forward(self, query: torch.Tensor, key_soft: torch.Tensor) -> torch.Tensor:\n        values = self.W_value(key_soft)\n        split_size = self.num_units // self.num_heads\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)\n\n        out_soft = scores_soft = None\n        querys = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key_soft)  # [N, T_k, num_units]\n\n        # [h, N, T_q, num_units/h]\n        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)\n        # [h, N, T_k, num_units/h]\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)\n        # [h, N, T_k, num_units/h]\n\n        # score = softmax(QK^T / (d_k ** 0.5))\n        scores_soft = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores_soft = scores_soft / (self.key_dim**0.5)\n        scores_soft = F.softmax(scores_soft, dim=3)\n\n        # out = score * V\n        # [h, N, T_q, num_units/h]\n        out_soft = torch.matmul(scores_soft, values)\n        out_soft = torch.cat(torch.split(out_soft, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n\n        return out_soft  # , scores_soft\n\n\nclass EmbeddingPadded(nn.Module):\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n        super().__init__()\n        padding_mult = torch.ones((num_embeddings, 1), dtype=torch.int64)\n        padding_mult[padding_idx] = 0\n        self.register_buffer(\"padding_mult\", padding_mult)\n        self.embeddings = nn.parameter.Parameter(initialize_embeddings((num_embeddings, embedding_dim)))\n\n    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n        embeddings_zeroed = self.embeddings * self.padding_mult\n        x = F.embedding(idx, embeddings_zeroed)\n        return x\n\n\nclass EmbeddingProjBlock(nn.Module):\n    def __init__(self, embedding_dim: int):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [\n                nn.Linear(embedding_dim, embedding_dim),\n                nn.LeakyReLU(0.3),\n                nn.Linear(embedding_dim, embedding_dim),\n                nn.LeakyReLU(0.3),\n            ]\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        res = x\n        for layer in self.layers:\n            x = layer(x)\n        x = x + res\n        return x\n\n\nclass LinearNorm(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n\n        nn.init.xavier_uniform_(self.linear.weight)\n        if bias:\n            nn.init.constant_(self.linear.bias, 0.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.linear(x)\n        return x\n\n\nclass STL(nn.Module):\n    \"\"\"\n    A PyTorch module for the Style Token Layer (STL) as described in\n    \"A Style-Based Generator Architecture for Generative Adversarial Networks\"\n    (https://arxiv.org/abs/1812.04948)\n\n    The STL applies a multi-headed attention mechanism over the learned style tokens,\n    using the text input as the query and the style tokens as the keys and values.\n    The output of the attention mechanism is used as the text's style embedding.\n\n    Args:\n        token_num (int): The number of style tokens.\n        n_hidden (int): Number of hidden dimensions.\n    \"\"\"\n\n    def __init__(self, n_hidden: int, token_num: int):\n        super(STL, self).__init__()  # pylint: disable=super-with-arguments\n\n        num_heads = 1\n        E = n_hidden\n        self.token_num = token_num\n        self.embed = nn.Parameter(torch.FloatTensor(self.token_num, E // num_heads))\n        d_q = E // 2\n        d_k = E // num_heads\n        self.attention = StyleEmbedAttention(query_dim=d_q, key_dim=d_k, num_units=E, num_heads=num_heads)\n\n        torch.nn.init.normal_(self.embed, mean=0, std=0.5)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        N = x.size(0)\n        query = x.unsqueeze(1)  # [N, 1, E//2]\n\n        keys_soft = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]\n\n        # Weighted sum\n        emotion_embed_soft = self.attention(query, keys_soft)\n\n        return emotion_embed_soft\n", "TTS/tts/layers/delightful_tts/conv_layers.py": "from typing import Tuple\n\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\nimport torch.nn.functional as F\nfrom torch.nn.utils import parametrize\n\nfrom TTS.tts.layers.delightful_tts.kernel_predictor import KernelPredictor\n\n\ndef calc_same_padding(kernel_size: int) -> Tuple[int, int]:\n    pad = kernel_size // 2\n    return (pad, pad - (kernel_size + 1) % 2)\n\n\nclass ConvNorm(nn.Module):\n    \"\"\"A 1-dimensional convolutional layer with optional weight normalization.\n\n    This layer wraps a 1D convolutional layer from PyTorch and applies\n    optional weight normalization. The layer can be used in a similar way to\n    the convolutional layers in PyTorch's `torch.nn` module.\n\n    Args:\n        in_channels (int): The number of channels in the input signal.\n        out_channels (int): The number of channels in the output signal.\n        kernel_size (int, optional): The size of the convolving kernel.\n            Defaults to 1.\n        stride (int, optional): The stride of the convolution. Defaults to 1.\n        padding (int, optional): Zero-padding added to both sides of the input.\n            If `None`, the padding will be calculated so that the output has\n            the same length as the input. Defaults to `None`.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, add bias after convolution. Defaults to `True`.\n        w_init_gain (str, optional): The weight initialization function to use.\n            Can be either 'linear' or 'relu'. Defaults to 'linear'.\n        use_weight_norm (bool, optional): If `True`, apply weight normalization\n            to the convolutional weights. Defaults to `False`.\n\n    Shapes:\n     - Input: :math:`[N, D, T]`\n\n    - Output: :math:`[N, out_dim, T]` where `out_dim` is the number of output dimensions.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        stride=1,\n        padding=None,\n        dilation=1,\n        bias=True,\n        w_init_gain=\"linear\",\n        use_weight_norm=False,\n    ):\n        super(ConvNorm, self).__init__()  # pylint: disable=super-with-arguments\n        if padding is None:\n            assert kernel_size % 2 == 1\n            padding = int(dilation * (kernel_size - 1) / 2)\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.use_weight_norm = use_weight_norm\n        conv_fn = nn.Conv1d\n        self.conv = conv_fn(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n        nn.init.xavier_uniform_(self.conv.weight, gain=nn.init.calculate_gain(w_init_gain))\n        if self.use_weight_norm:\n            self.conv = nn.utils.parametrizations.weight_norm(self.conv)\n\n    def forward(self, signal, mask=None):\n        conv_signal = self.conv(signal)\n        if mask is not None:\n            # always re-zero output if mask is\n            # available to match zero-padding\n            conv_signal = conv_signal * mask\n        return conv_signal\n\n\nclass ConvLSTMLinear(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        out_dim,\n        n_layers=2,\n        n_channels=256,\n        kernel_size=3,\n        p_dropout=0.1,\n        lstm_type=\"bilstm\",\n        use_linear=True,\n    ):\n        super(ConvLSTMLinear, self).__init__()  # pylint: disable=super-with-arguments\n        self.out_dim = out_dim\n        self.lstm_type = lstm_type\n        self.use_linear = use_linear\n        self.dropout = nn.Dropout(p=p_dropout)\n\n        convolutions = []\n        for i in range(n_layers):\n            conv_layer = ConvNorm(\n                in_dim if i == 0 else n_channels,\n                n_channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=int((kernel_size - 1) / 2),\n                dilation=1,\n                w_init_gain=\"relu\",\n            )\n            conv_layer = nn.utils.parametrizations.weight_norm(conv_layer.conv, name=\"weight\")\n            convolutions.append(conv_layer)\n\n        self.convolutions = nn.ModuleList(convolutions)\n\n        if not self.use_linear:\n            n_channels = out_dim\n\n        if self.lstm_type != \"\":\n            use_bilstm = False\n            lstm_channels = n_channels\n            if self.lstm_type == \"bilstm\":\n                use_bilstm = True\n                lstm_channels = int(n_channels // 2)\n\n            self.bilstm = nn.LSTM(n_channels, lstm_channels, 1, batch_first=True, bidirectional=use_bilstm)\n            lstm_norm_fn_pntr = nn.utils.spectral_norm\n            self.bilstm = lstm_norm_fn_pntr(self.bilstm, \"weight_hh_l0\")\n            if self.lstm_type == \"bilstm\":\n                self.bilstm = lstm_norm_fn_pntr(self.bilstm, \"weight_hh_l0_reverse\")\n\n        if self.use_linear:\n            self.dense = nn.Linear(n_channels, out_dim)\n\n    def run_padded_sequence(self, context, lens):\n        context_embedded = []\n        for b_ind in range(context.size()[0]):  # TODO: speed up\n            curr_context = context[b_ind : b_ind + 1, :, : lens[b_ind]].clone()\n            for conv in self.convolutions:\n                curr_context = self.dropout(F.relu(conv(curr_context)))\n            context_embedded.append(curr_context[0].transpose(0, 1))\n        context = nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n        return context\n\n    def run_unsorted_inputs(self, fn, context, lens):  # pylint: disable=no-self-use\n        lens_sorted, ids_sorted = torch.sort(lens, descending=True)\n        unsort_ids = [0] * lens.size(0)\n        for i in range(len(ids_sorted)):  # pylint: disable=consider-using-enumerate\n            unsort_ids[ids_sorted[i]] = i\n        lens_sorted = lens_sorted.long().cpu()\n\n        context = context[ids_sorted]\n        context = nn.utils.rnn.pack_padded_sequence(context, lens_sorted, batch_first=True)\n        context = fn(context)[0]\n        context = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n\n        # map back to original indices\n        context = context[unsort_ids]\n        return context\n\n    def forward(self, context, lens):\n        if context.size()[0] > 1:\n            context = self.run_padded_sequence(context, lens)\n            # to B, D, T\n            context = context.transpose(1, 2)\n        else:\n            for conv in self.convolutions:\n                context = self.dropout(F.relu(conv(context)))\n\n        if self.lstm_type != \"\":\n            context = context.transpose(1, 2)\n            self.bilstm.flatten_parameters()\n            if lens is not None:\n                context = self.run_unsorted_inputs(self.bilstm, context, lens)\n            else:\n                context = self.bilstm(context)[0]\n            context = context.transpose(1, 2)\n\n        x_hat = context\n        if self.use_linear:\n            x_hat = self.dense(context.transpose(1, 2)).transpose(1, 2)\n\n        return x_hat\n\n\nclass DepthWiseConv1d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, groups=in_channels)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.conv(x)\n\n\nclass PointwiseConv1d(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        padding: int = 0,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.conv = nn.Conv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.conv(x)\n\n\nclass BSConv1d(nn.Module):\n    \"\"\"https://arxiv.org/pdf/2003.13549.pdf\"\"\"\n\n    def __init__(self, channels_in: int, channels_out: int, kernel_size: int, padding: int):\n        super().__init__()\n        self.pointwise = nn.Conv1d(channels_in, channels_out, kernel_size=1)\n        self.depthwise = nn.Conv1d(\n            channels_out,\n            channels_out,\n            kernel_size=kernel_size,\n            padding=padding,\n            groups=channels_out,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x1 = self.pointwise(x)\n        x2 = self.depthwise(x1)\n        return x2\n\n\nclass BSConv2d(nn.Module):\n    \"\"\"https://arxiv.org/pdf/2003.13549.pdf\"\"\"\n\n    def __init__(self, channels_in: int, channels_out: int, kernel_size: int, padding: int):\n        super().__init__()\n        self.pointwise = nn.Conv2d(channels_in, channels_out, kernel_size=1)\n        self.depthwise = nn.Conv2d(\n            channels_out,\n            channels_out,\n            kernel_size=kernel_size,\n            padding=padding,\n            groups=channels_out,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x1 = self.pointwise(x)\n        x2 = self.depthwise(x1)\n        return x2\n\n\nclass Conv1dGLU(nn.Module):\n    \"\"\"From DeepVoice 3\"\"\"\n\n    def __init__(self, d_model: int, kernel_size: int, padding: int, embedding_dim: int):\n        super().__init__()\n        self.conv = BSConv1d(d_model, 2 * d_model, kernel_size=kernel_size, padding=padding)\n        self.embedding_proj = nn.Linear(embedding_dim, d_model)\n        self.register_buffer(\"sqrt\", torch.sqrt(torch.FloatTensor([0.5])).squeeze(0))\n        self.softsign = torch.nn.Softsign()\n\n    def forward(self, x: torch.Tensor, embeddings: torch.Tensor) -> torch.Tensor:\n        x = x.permute((0, 2, 1))\n        residual = x\n        x = self.conv(x)\n        splitdim = 1\n        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n        embeddings = self.embedding_proj(embeddings).unsqueeze(2)\n        softsign = self.softsign(embeddings)\n        softsign = softsign.expand_as(a)\n        a = a + softsign\n        x = a * torch.sigmoid(b)\n        x = x + residual\n        x = x * self.sqrt\n        x = x.permute((0, 2, 1))\n        return x\n\n\nclass ConvTransposed(nn.Module):\n    \"\"\"\n    A 1D convolutional transposed layer for PyTorch.\n    This layer applies a 1D convolutional transpose operation to its input tensor,\n    where the number of channels of the input tensor is the same as the number of channels of the output tensor.\n\n    Attributes:\n        in_channels (int): The number of channels in the input tensor.\n        out_channels (int): The number of channels in the output tensor.\n        kernel_size (int): The size of the convolutional kernel. Default: 1.\n        padding (int): The number of padding elements to add to the input tensor. Default: 0.\n        conv (BSConv1d): The 1D convolutional transpose layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 1,\n        padding: int = 0,\n    ):\n        super().__init__()\n        self.conv = BSConv1d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous().transpose(1, 2)\n        x = self.conv(x)\n        x = x.contiguous().transpose(1, 2)\n        return x\n\n\nclass DepthwiseConvModule(nn.Module):\n    def __init__(self, dim: int, kernel_size: int = 7, expansion: int = 4, lrelu_slope: float = 0.3):\n        super().__init__()\n        padding = calc_same_padding(kernel_size)\n        self.depthwise = nn.Conv1d(\n            dim,\n            dim * expansion,\n            kernel_size=kernel_size,\n            padding=padding[0],\n            groups=dim,\n        )\n        self.act = nn.LeakyReLU(lrelu_slope)\n        self.out = nn.Conv1d(dim * expansion, dim, 1, 1, 0)\n        self.ln = nn.LayerNorm(dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.ln(x)\n        x = x.permute((0, 2, 1))\n        x = self.depthwise(x)\n        x = self.act(x)\n        x = self.out(x)\n        x = x.permute((0, 2, 1))\n        return x\n\n\nclass AddCoords(nn.Module):\n    def __init__(self, rank: int, with_r: bool = False):\n        super().__init__()\n        self.rank = rank\n        self.with_r = with_r\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.rank == 1:\n            batch_size_shape, channel_in_shape, dim_x = x.shape  # pylint: disable=unused-variable\n            xx_range = torch.arange(dim_x, dtype=torch.int32)\n            xx_channel = xx_range[None, None, :]\n\n            xx_channel = xx_channel.float() / (dim_x - 1)\n            xx_channel = xx_channel * 2 - 1\n            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1)\n\n            xx_channel = xx_channel.to(x.device)\n            out = torch.cat([x, xx_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2))\n                out = torch.cat([out, rr], dim=1)\n\n        elif self.rank == 2:\n            batch_size_shape, channel_in_shape, dim_y, dim_x = x.shape\n            xx_ones = torch.ones([1, 1, 1, dim_x], dtype=torch.int32)\n            yy_ones = torch.ones([1, 1, 1, dim_y], dtype=torch.int32)\n\n            xx_range = torch.arange(dim_y, dtype=torch.int32)\n            yy_range = torch.arange(dim_x, dtype=torch.int32)\n            xx_range = xx_range[None, None, :, None]\n            yy_range = yy_range[None, None, :, None]\n\n            xx_channel = torch.matmul(xx_range, xx_ones)\n            yy_channel = torch.matmul(yy_range, yy_ones)\n\n            # transpose y\n            yy_channel = yy_channel.permute(0, 1, 3, 2)\n\n            xx_channel = xx_channel.float() / (dim_y - 1)\n            yy_channel = yy_channel.float() / (dim_x - 1)\n\n            xx_channel = xx_channel * 2 - 1\n            yy_channel = yy_channel * 2 - 1\n\n            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1)\n            yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1)\n\n            xx_channel = xx_channel.to(x.device)\n            yy_channel = yy_channel.to(x.device)\n\n            out = torch.cat([x, xx_channel, yy_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n                out = torch.cat([out, rr], dim=1)\n\n        elif self.rank == 3:\n            batch_size_shape, channel_in_shape, dim_z, dim_y, dim_x = x.shape\n            xx_ones = torch.ones([1, 1, 1, 1, dim_x], dtype=torch.int32)\n            yy_ones = torch.ones([1, 1, 1, 1, dim_y], dtype=torch.int32)\n            zz_ones = torch.ones([1, 1, 1, 1, dim_z], dtype=torch.int32)\n\n            xy_range = torch.arange(dim_y, dtype=torch.int32)\n            xy_range = xy_range[None, None, None, :, None]\n\n            yz_range = torch.arange(dim_z, dtype=torch.int32)\n            yz_range = yz_range[None, None, None, :, None]\n\n            zx_range = torch.arange(dim_x, dtype=torch.int32)\n            zx_range = zx_range[None, None, None, :, None]\n\n            xy_channel = torch.matmul(xy_range, xx_ones)\n            xx_channel = torch.cat([xy_channel + i for i in range(dim_z)], dim=2)\n\n            yz_channel = torch.matmul(yz_range, yy_ones)\n            yz_channel = yz_channel.permute(0, 1, 3, 4, 2)\n            yy_channel = torch.cat([yz_channel + i for i in range(dim_x)], dim=4)\n\n            zx_channel = torch.matmul(zx_range, zz_ones)\n            zx_channel = zx_channel.permute(0, 1, 4, 2, 3)\n            zz_channel = torch.cat([zx_channel + i for i in range(dim_y)], dim=3)\n\n            xx_channel = xx_channel.to(x.device)\n            yy_channel = yy_channel.to(x.device)\n            zz_channel = zz_channel.to(x.device)\n            out = torch.cat([x, xx_channel, yy_channel, zz_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(\n                    torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2) + torch.pow(zz_channel - 0.5, 2)\n                )\n                out = torch.cat([out, rr], dim=1)\n        else:\n            raise NotImplementedError\n\n        return out\n\n\nclass CoordConv1d(nn.modules.conv.Conv1d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n        with_r: bool = False,\n    ):\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n        self.rank = 1\n        self.addcoords = AddCoords(self.rank, with_r)\n        self.conv = nn.Conv1d(\n            in_channels + self.rank + int(with_r),\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n\nclass CoordConv2d(nn.modules.conv.Conv2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n        with_r: bool = False,\n    ):\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n        self.rank = 2\n        self.addcoords = AddCoords(self.rank, with_r)\n        self.conv = nn.Conv2d(\n            in_channels + self.rank + int(with_r),\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.addcoords(x)\n        x = self.conv(x)\n        return x\n\n\nclass LVCBlock(torch.nn.Module):\n    \"\"\"the location-variable convolutions\"\"\"\n\n    def __init__(  # pylint: disable=dangerous-default-value\n        self,\n        in_channels,\n        cond_channels,\n        stride,\n        dilations=[1, 3, 9, 27],\n        lReLU_slope=0.2,\n        conv_kernel_size=3,\n        cond_hop_length=256,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n    ):\n        super().__init__()\n\n        self.cond_hop_length = cond_hop_length\n        self.conv_layers = len(dilations)\n        self.conv_kernel_size = conv_kernel_size\n\n        self.kernel_predictor = KernelPredictor(\n            cond_channels=cond_channels,\n            conv_in_channels=in_channels,\n            conv_out_channels=2 * in_channels,\n            conv_layers=len(dilations),\n            conv_kernel_size=conv_kernel_size,\n            kpnet_hidden_channels=kpnet_hidden_channels,\n            kpnet_conv_size=kpnet_conv_size,\n            kpnet_dropout=kpnet_dropout,\n            kpnet_nonlinear_activation_params={\"negative_slope\": lReLU_slope},\n        )\n\n        self.convt_pre = nn.Sequential(\n            nn.LeakyReLU(lReLU_slope),\n            nn.utils.parametrizations.weight_norm(\n                nn.ConvTranspose1d(\n                    in_channels,\n                    in_channels,\n                    2 * stride,\n                    stride=stride,\n                    padding=stride // 2 + stride % 2,\n                    output_padding=stride % 2,\n                )\n            ),\n        )\n\n        self.conv_blocks = nn.ModuleList()\n        for dilation in dilations:\n            self.conv_blocks.append(\n                nn.Sequential(\n                    nn.LeakyReLU(lReLU_slope),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            in_channels,\n                            in_channels,\n                            conv_kernel_size,\n                            padding=dilation * (conv_kernel_size - 1) // 2,\n                            dilation=dilation,\n                        )\n                    ),\n                    nn.LeakyReLU(lReLU_slope),\n                )\n            )\n\n    def forward(self, x, c):\n        \"\"\"forward propagation of the location-variable convolutions.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length)\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n\n        Returns:\n            Tensor: the output sequence (batch, in_channels, in_length)\n        \"\"\"\n        _, in_channels, _ = x.shape  # (B, c_g, L')\n\n        x = self.convt_pre(x)  # (B, c_g, stride * L')\n        kernels, bias = self.kernel_predictor(c)\n\n        for i, conv in enumerate(self.conv_blocks):\n            output = conv(x)  # (B, c_g, stride * L')\n\n            k = kernels[:, i, :, :, :, :]  # (B, 2 * c_g, c_g, kernel_size, cond_length)\n            b = bias[:, i, :, :]  # (B, 2 * c_g, cond_length)\n\n            output = self.location_variable_convolution(\n                output, k, b, hop_size=self.cond_hop_length\n            )  # (B, 2 * c_g, stride * L'): LVC\n            x = x + torch.sigmoid(output[:, :in_channels, :]) * torch.tanh(\n                output[:, in_channels:, :]\n            )  # (B, c_g, stride * L'): GAU\n\n        return x\n\n    def location_variable_convolution(self, x, kernel, bias, dilation=1, hop_size=256):  # pylint: disable=no-self-use\n        \"\"\"perform location-variable convolution operation on the input sequence (x) using the local convolution kernl.\n        Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n        Args:\n            x (Tensor): the input sequence (batch, in_channels, in_length).\n            kernel (Tensor): the local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length)\n            bias (Tensor): the bias for the local convolution (batch, out_channels, kernel_length)\n            dilation (int): the dilation of convolution.\n            hop_size (int): the hop_size of the conditioning sequence.\n        Returns:\n            (Tensor): the output sequence after performing local convolution. (batch, out_channels, in_length).\n        \"\"\"\n        batch, _, in_length = x.shape\n        batch, _, out_channels, kernel_size, kernel_length = kernel.shape\n        assert in_length == (kernel_length * hop_size), \"length of (x, kernel) is not matched\"\n\n        padding = dilation * int((kernel_size - 1) / 2)\n        x = F.pad(x, (padding, padding), \"constant\", 0)  # (batch, in_channels, in_length + 2*padding)\n        x = x.unfold(2, hop_size + 2 * padding, hop_size)  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n        if hop_size < dilation:\n            x = F.pad(x, (0, dilation), \"constant\", 0)\n        x = x.unfold(\n            3, dilation, dilation\n        )  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n        x = x[:, :, :, :, :hop_size]\n        x = x.transpose(3, 4)  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n        x = x.unfold(4, kernel_size, 1)  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n        o = torch.einsum(\"bildsk,biokl->bolsd\", x, kernel)\n        o = o.to(memory_format=torch.channels_last_3d)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).to(memory_format=torch.channels_last_3d)\n        o = o + bias\n        o = o.contiguous().view(batch, out_channels, -1)\n\n        return o\n\n    def remove_weight_norm(self):\n        self.kernel_predictor.remove_weight_norm()\n        parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n        for block in self.conv_blocks:\n            parametrize.remove_parametrizations(block[1], \"weight\")\n", "TTS/tts/layers/delightful_tts/encoders.py": "from typing import List, Tuple, Union\n\nimport torch\nimport torch.nn as nn  # pylint: disable=consider-using-from-import\nimport torch.nn.functional as F\n\nfrom TTS.tts.layers.delightful_tts.conformer import ConformerMultiHeadedSelfAttention\nfrom TTS.tts.layers.delightful_tts.conv_layers import CoordConv1d\nfrom TTS.tts.layers.delightful_tts.networks import STL\n\n\ndef get_mask_from_lengths(lengths: torch.Tensor) -> torch.Tensor:\n    batch_size = lengths.shape[0]\n    max_len = torch.max(lengths).item()\n    ids = torch.arange(0, max_len, device=lengths.device).unsqueeze(0).expand(batch_size, -1)\n    mask = ids >= lengths.unsqueeze(1).expand(-1, max_len)\n    return mask\n\n\ndef stride_lens(lens: torch.Tensor, stride: int = 2) -> torch.Tensor:\n    return torch.ceil(lens / stride).int()\n\n\nclass ReferenceEncoder(nn.Module):\n    \"\"\"\n    Referance encoder for utterance and phoneme prosody encoders. Reference encoder\n    made up of convolution and RNN layers.\n\n    Args:\n        num_mels (int): Number of mel frames to produce.\n        ref_enc_filters (list[int]): List of channel sizes for encoder layers.\n        ref_enc_size (int): Size of the kernel for the conv layers.\n        ref_enc_strides (List[int]): List of strides to use for conv layers.\n        ref_enc_gru_size (int): Number of hidden features for the gated recurrent unit.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, dim, time): Tensor containing mel vector\n        - **lengths** (batch): Tensor containing the mel lengths.\n    Returns:\n        - **outputs** (batch, time, dim): Tensor produced by Reference Encoder.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_mels: int,\n        ref_enc_filters: List[Union[int, int, int, int, int, int]],\n        ref_enc_size: int,\n        ref_enc_strides: List[Union[int, int, int, int, int]],\n        ref_enc_gru_size: int,\n    ):\n        super().__init__()\n\n        n_mel_channels = num_mels\n        self.n_mel_channels = n_mel_channels\n        K = len(ref_enc_filters)\n        filters = [self.n_mel_channels] + ref_enc_filters\n        strides = [1] + ref_enc_strides\n        # Use CoordConv at the first layer to better preserve positional information: https://arxiv.org/pdf/1811.02122.pdf\n        convs = [\n            CoordConv1d(\n                in_channels=filters[0],\n                out_channels=filters[0 + 1],\n                kernel_size=ref_enc_size,\n                stride=strides[0],\n                padding=ref_enc_size // 2,\n                with_r=True,\n            )\n        ]\n        convs2 = [\n            nn.Conv1d(\n                in_channels=filters[i],\n                out_channels=filters[i + 1],\n                kernel_size=ref_enc_size,\n                stride=strides[i],\n                padding=ref_enc_size // 2,\n            )\n            for i in range(1, K)\n        ]\n        convs.extend(convs2)\n        self.convs = nn.ModuleList(convs)\n\n        self.norms = nn.ModuleList([nn.InstanceNorm1d(num_features=ref_enc_filters[i], affine=True) for i in range(K)])\n\n        self.gru = nn.GRU(\n            input_size=ref_enc_filters[-1],\n            hidden_size=ref_enc_gru_size,\n            batch_first=True,\n        )\n\n    def forward(self, x: torch.Tensor, mel_lens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        inputs --- [N,  n_mels, timesteps]\n        outputs --- [N, E//2]\n        \"\"\"\n\n        mel_masks = get_mask_from_lengths(mel_lens).unsqueeze(1)\n        x = x.masked_fill(mel_masks, 0)\n        for conv, norm in zip(self.convs, self.norms):\n            x = conv(x)\n            x = F.leaky_relu(x, 0.3)  # [N, 128, Ty//2^K, n_mels//2^K]\n            x = norm(x)\n\n        for _ in range(2):\n            mel_lens = stride_lens(mel_lens)\n\n        mel_masks = get_mask_from_lengths(mel_lens)\n\n        x = x.masked_fill(mel_masks.unsqueeze(1), 0)\n        x = x.permute((0, 2, 1))\n        x = torch.nn.utils.rnn.pack_padded_sequence(x, mel_lens.cpu().int(), batch_first=True, enforce_sorted=False)\n\n        self.gru.flatten_parameters()\n        x, memory = self.gru(x)  # memory --- [N, Ty, E//2], out --- [1, N, E//2]\n        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n\n        return x, memory, mel_masks\n\n    def calculate_channels(  # pylint: disable=no-self-use\n        self, L: int, kernel_size: int, stride: int, pad: int, n_convs: int\n    ) -> int:\n        for _ in range(n_convs):\n            L = (L - kernel_size + 2 * pad) // stride + 1\n        return L\n\n\nclass UtteranceLevelProsodyEncoder(nn.Module):\n    def __init__(\n        self,\n        num_mels: int,\n        ref_enc_filters: List[Union[int, int, int, int, int, int]],\n        ref_enc_size: int,\n        ref_enc_strides: List[Union[int, int, int, int, int]],\n        ref_enc_gru_size: int,\n        dropout: float,\n        n_hidden: int,\n        bottleneck_size_u: int,\n        token_num: int,\n    ):\n        \"\"\"\n        Encoder to extract prosody from utterance. it is made up of a reference encoder\n        with a couple of linear layers and style token layer with dropout.\n\n        Args:\n            num_mels (int): Number of mel frames to produce.\n            ref_enc_filters (list[int]): List of channel sizes for ref encoder layers.\n            ref_enc_size (int): Size of the kernel for the ref encoder conv layers.\n            ref_enc_strides (List[int]): List of strides to use for teh ref encoder conv layers.\n            ref_enc_gru_size (int): Number of hidden features for the gated recurrent unit.\n            dropout (float): Probability of dropout.\n            n_hidden (int): Size of hidden layers.\n            bottleneck_size_u (int): Size of the bottle neck layer.\n\n        Inputs: inputs, mask\n            - **inputs** (batch, dim, time): Tensor containing mel vector\n            - **lengths** (batch): Tensor containing the mel lengths.\n        Returns:\n            - **outputs** (batch, 1, dim): Tensor produced by Utterance Level Prosody Encoder.\n        \"\"\"\n        super().__init__()\n\n        self.E = n_hidden\n        self.d_q = self.d_k = n_hidden\n        bottleneck_size = bottleneck_size_u\n\n        self.encoder = ReferenceEncoder(\n            ref_enc_filters=ref_enc_filters,\n            ref_enc_gru_size=ref_enc_gru_size,\n            ref_enc_size=ref_enc_size,\n            ref_enc_strides=ref_enc_strides,\n            num_mels=num_mels,\n        )\n        self.encoder_prj = nn.Linear(ref_enc_gru_size, self.E // 2)\n        self.stl = STL(n_hidden=n_hidden, token_num=token_num)\n        self.encoder_bottleneck = nn.Linear(self.E, bottleneck_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, mels: torch.Tensor, mel_lens: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Shapes:\n            mels: :math: `[B, C, T]`\n            mel_lens: :math: `[B]`\n\n        out --- [N, seq_len, E]\n        \"\"\"\n        _, embedded_prosody, _ = self.encoder(mels, mel_lens)\n\n        # Bottleneck\n        embedded_prosody = self.encoder_prj(embedded_prosody)\n\n        # Style Token\n        out = self.encoder_bottleneck(self.stl(embedded_prosody))\n        out = self.dropout(out)\n\n        out = out.view((-1, 1, out.shape[3]))\n        return out\n\n\nclass PhonemeLevelProsodyEncoder(nn.Module):\n    def __init__(\n        self,\n        num_mels: int,\n        ref_enc_filters: List[Union[int, int, int, int, int, int]],\n        ref_enc_size: int,\n        ref_enc_strides: List[Union[int, int, int, int, int]],\n        ref_enc_gru_size: int,\n        dropout: float,\n        n_hidden: int,\n        n_heads: int,\n        bottleneck_size_p: int,\n    ):\n        super().__init__()\n\n        self.E = n_hidden\n        self.d_q = self.d_k = n_hidden\n        bottleneck_size = bottleneck_size_p\n\n        self.encoder = ReferenceEncoder(\n            ref_enc_filters=ref_enc_filters,\n            ref_enc_gru_size=ref_enc_gru_size,\n            ref_enc_size=ref_enc_size,\n            ref_enc_strides=ref_enc_strides,\n            num_mels=num_mels,\n        )\n        self.encoder_prj = nn.Linear(ref_enc_gru_size, n_hidden)\n        self.attention = ConformerMultiHeadedSelfAttention(\n            d_model=n_hidden,\n            num_heads=n_heads,\n            dropout_p=dropout,\n        )\n        self.encoder_bottleneck = nn.Linear(n_hidden, bottleneck_size)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        src_mask: torch.Tensor,\n        mels: torch.Tensor,\n        mel_lens: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        x --- [N, seq_len, encoder_embedding_dim]\n        mels --- [N, Ty/r, n_mels*r], r=1\n        out --- [N, seq_len, bottleneck_size]\n        attn --- [N, seq_len, ref_len], Ty/r = ref_len\n        \"\"\"\n        embedded_prosody, _, mel_masks = self.encoder(mels, mel_lens)\n\n        # Bottleneck\n        embedded_prosody = self.encoder_prj(embedded_prosody)\n\n        attn_mask = mel_masks.view((mel_masks.shape[0], 1, 1, -1))\n        x, _ = self.attention(\n            query=x,\n            key=embedded_prosody,\n            value=embedded_prosody,\n            mask=attn_mask,\n            encoding=encoding,\n        )\n        x = self.encoder_bottleneck(x)\n        x = x.masked_fill(src_mask.unsqueeze(-1), 0.0)\n        return x\n", "TTS/tts/layers/delightful_tts/kernel_predictor.py": "import torch.nn as nn  # pylint: disable=consider-using-from-import\nfrom torch.nn.utils import parametrize\n\n\nclass KernelPredictor(nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\n\n    Args:\n            cond_channels (int): number of channel for the conditioning sequence,\n            conv_in_channels (int): number of channel for the input sequence,\n            conv_out_channels (int): number of channel for the output sequence,\n            conv_layers (int): number of layers\n\n    \"\"\"\n\n    def __init__(  # pylint: disable=dangerous-default-value\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,\n        kpnet_hidden_channels=64,\n        kpnet_conv_size=3,\n        kpnet_dropout=0.0,\n        kpnet_nonlinear_activation=\"LeakyReLU\",\n        kpnet_nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        super().__init__()\n\n        self.conv_in_channels = conv_in_channels\n        self.conv_out_channels = conv_out_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_layers = conv_layers\n\n        kpnet_kernel_channels = conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers  # l_w\n        kpnet_bias_channels = conv_out_channels * conv_layers  # l_b\n\n        self.input_conv = nn.Sequential(\n            nn.utils.parametrizations.weight_norm(\n                nn.Conv1d(cond_channels, kpnet_hidden_channels, 5, padding=2, bias=True)\n            ),\n            getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n        )\n\n        self.residual_convs = nn.ModuleList()\n        padding = (kpnet_conv_size - 1) // 2\n        for _ in range(3):\n            self.residual_convs.append(\n                nn.Sequential(\n                    nn.Dropout(kpnet_dropout),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        )\n                    ),\n                    getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        )\n                    ),\n                    getattr(nn, kpnet_nonlinear_activation)(**kpnet_nonlinear_activation_params),\n                )\n            )\n        self.kernel_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_kernel_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            )\n        )\n        self.bias_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_bias_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            )\n        )\n\n    def forward(self, c):\n        \"\"\"\n        Args:\n            c (Tensor): the conditioning sequence (batch, cond_channels, cond_length)\n        \"\"\"\n        batch, _, cond_length = c.shape\n        c = self.input_conv(c)\n        for residual_conv in self.residual_convs:\n            residual_conv.to(c.device)\n            c = c + residual_conv(c)\n        k = self.kernel_conv(c)\n        b = self.bias_conv(c)\n        kernels = k.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_in_channels,\n            self.conv_out_channels,\n            self.conv_kernel_size,\n            cond_length,\n        )\n        bias = b.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_out_channels,\n            cond_length,\n        )\n\n        return kernels, bias\n\n    def remove_weight_norm(self):\n        parametrize.remove_parametrizations(self.input_conv[0], \"weight\")\n        parametrize.remove_parametrizations(self.kernel_conv, \"weight\")\n        parametrize.remove_parametrizations(self.bias_conv, \"weight\")\n        for block in self.residual_convs:\n            parametrize.remove_parametrizations(block[1], \"weight\")\n            parametrize.remove_parametrizations(block[3], \"weight\")\n", "TTS/tts/configs/vits_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.vits import VitsArgs, VitsAudioConfig\n\n\n@dataclass\nclass VitsConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for VITS End2End TTS model.\n\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n\n        model_args (VitsArgs):\n            Model architecture arguments. Defaults to `VitsArgs()`.\n\n        audio (VitsAudioConfig):\n            Audio processing configuration. Defaults to `VitsAudioConfig()`.\n\n        grad_clip (List):\n            Gradient clipping thresholds for each optimizer. Defaults to `[1000.0, 1000.0]`.\n\n        lr_gen (float):\n            Initial learning rate for the generator. Defaults to 0.0002.\n\n        lr_disc (float):\n            Initial learning rate for the discriminator. Defaults to 0.0002.\n\n        lr_scheduler_gen (str):\n            Name of the learning rate scheduler for the generator. One of the `torch.optim.lr_scheduler.*`. Defaults to\n            `ExponentialLR`.\n\n        lr_scheduler_gen_params (dict):\n            Parameters for the learning rate scheduler of the generator. Defaults to `{'gamma': 0.999875, \"last_epoch\":-1}`.\n\n        lr_scheduler_disc (str):\n            Name of the learning rate scheduler for the discriminator. One of the `torch.optim.lr_scheduler.*`. Defaults to\n            `ExponentialLR`.\n\n        lr_scheduler_disc_params (dict):\n            Parameters for the learning rate scheduler of the discriminator. Defaults to `{'gamma': 0.999875, \"last_epoch\":-1}`.\n\n        scheduler_after_epoch (bool):\n            If true, step the schedulers after each epoch else after each step. Defaults to `False`.\n\n        optimizer (str):\n            Name of the optimizer to use with both the generator and the discriminator networks. One of the\n            `torch.optim.*`. Defaults to `AdamW`.\n\n        kl_loss_alpha (float):\n            Loss weight for KL loss. Defaults to 1.0.\n\n        disc_loss_alpha (float):\n            Loss weight for the discriminator loss. Defaults to 1.0.\n\n        gen_loss_alpha (float):\n            Loss weight for the generator loss. Defaults to 1.0.\n\n        feat_loss_alpha (float):\n            Loss weight for the feature matching loss. Defaults to 1.0.\n\n        mel_loss_alpha (float):\n            Loss weight for the mel loss. Defaults to 45.0.\n\n        return_wav (bool):\n            If true, data loader returns the waveform as well as the other outputs. Do not change. Defaults to `True`.\n\n        compute_linear_spec (bool):\n            If true, the linear spectrogram is computed and returned alongside the mel output. Do not change. Defaults to `True`.\n\n        use_weighted_sampler (bool):\n            If true, use weighted sampler with bucketing for balancing samples between datasets used in training. Defaults to `False`.\n\n        weighted_sampler_attrs (dict):\n            Key retuned by the formatter to be used for weighted sampler. For example `{\"root_path\": 2.0, \"speaker_name\": 1.0}` sets sample probabilities\n            by overweighting `root_path` by 2.0. Defaults to `{}`.\n\n        weighted_sampler_multipliers (dict):\n            Weight each unique value of a key returned by the formatter for weighted sampling.\n            For example `{\"root_path\":{\"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-100/\":1.0, \"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-360/\": 0.5}`.\n            It will sample instances from `train-clean-100` 2 times more than `train-clean-360`. Defaults to `{}`.\n\n        r (int):\n            Number of spectrogram frames to be generated at a time. Do not change. Defaults to `1`.\n\n        add_blank (bool):\n            If true, a blank token is added in between every character. Defaults to `True`.\n\n        test_sentences (List[List]):\n            List of sentences with speaker and language information to be used for testing.\n\n        language_ids_file (str):\n            Path to the language ids file.\n\n        use_language_embedding (bool):\n            If true, language embedding is used. Defaults to `False`.\n\n    Note:\n        Check :class:`TTS.tts.configs.shared_configs.BaseTTSConfig` for the inherited parameters.\n\n    Example:\n\n        >>> from TTS.tts.configs.vits_config import VitsConfig\n        >>> config = VitsConfig()\n    \"\"\"\n\n    model: str = \"vits\"\n    # model specific params\n    model_args: VitsArgs = field(default_factory=VitsArgs)\n    audio: VitsAudioConfig = field(default_factory=VitsAudioConfig)\n\n    # optimizer\n    grad_clip: List[float] = field(default_factory=lambda: [1000, 1000])\n    lr_gen: float = 0.0002\n    lr_disc: float = 0.0002\n    lr_scheduler_gen: str = \"ExponentialLR\"\n    lr_scheduler_gen_params: dict = field(default_factory=lambda: {\"gamma\": 0.999875, \"last_epoch\": -1})\n    lr_scheduler_disc: str = \"ExponentialLR\"\n    lr_scheduler_disc_params: dict = field(default_factory=lambda: {\"gamma\": 0.999875, \"last_epoch\": -1})\n    scheduler_after_epoch: bool = True\n    optimizer: str = \"AdamW\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.8, 0.99], \"eps\": 1e-9, \"weight_decay\": 0.01})\n\n    # loss params\n    kl_loss_alpha: float = 1.0\n    disc_loss_alpha: float = 1.0\n    gen_loss_alpha: float = 1.0\n    feat_loss_alpha: float = 1.0\n    mel_loss_alpha: float = 45.0\n    dur_loss_alpha: float = 1.0\n    speaker_encoder_loss_alpha: float = 1.0\n\n    # data loader params\n    return_wav: bool = True\n    compute_linear_spec: bool = True\n\n    # sampler params\n    use_weighted_sampler: bool = False  # TODO: move it to the base config\n    weighted_sampler_attrs: dict = field(default_factory=lambda: {})\n    weighted_sampler_multipliers: dict = field(default_factory=lambda: {})\n\n    # overrides\n    r: int = 1  # DO NOT CHANGE\n    add_blank: bool = True\n\n    # testing\n    test_sentences: List[List] = field(\n        default_factory=lambda: [\n            [\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"],\n            [\"Be a voice, not an echo.\"],\n            [\"I'm sorry Dave. I'm afraid I can't do that.\"],\n            [\"This cake is great. It's so delicious and moist.\"],\n            [\"Prior to November 22, 1963.\"],\n        ]\n    )\n\n    # multi-speaker settings\n    # use speaker embedding layer\n    num_speakers: int = 0\n    use_speaker_embedding: bool = False\n    speakers_file: str = None\n    speaker_embedding_channels: int = 256\n    language_ids_file: str = None\n    use_language_embedding: bool = False\n\n    # use d-vectors\n    use_d_vector_file: bool = False\n    d_vector_file: List[str] = None\n    d_vector_dim: int = None\n\n    def __post_init__(self):\n        for key, val in self.model_args.items():\n            if hasattr(self, key):\n                self[key] = val\n", "TTS/tts/configs/glow_tts_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\n\n\n@dataclass\nclass GlowTTSConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for GlowTTS model.\n\n    Example:\n\n        >>> from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n        >>> config = GlowTTSConfig()\n\n    Args:\n        model(str):\n            Model name used for selecting the right model at initialization. Defaults to `glow_tts`.\n        encoder_type (str):\n            Type of the encoder used by the model. Look at `TTS.tts.layers.glow_tts.encoder` for more details.\n            Defaults to `rel_pos_transformers`.\n        encoder_params (dict):\n            Parameters used to define the encoder network. Look at `TTS.tts.layers.glow_tts.encoder` for more details.\n            Defaults to `{\"kernel_size\": 3, \"dropout_p\": 0.1, \"num_layers\": 6, \"num_heads\": 2, \"hidden_channels_ffn\": 768}`\n        use_encoder_prenet (bool):\n            enable / disable the use of a prenet for the encoder. Defaults to True.\n        hidden_channels_enc (int):\n            Number of base hidden channels used by the encoder network. It defines the input and the output channel sizes,\n            and for some encoder types internal hidden channels sizes too. Defaults to 192.\n        hidden_channels_dec (int):\n            Number of base hidden channels used by the decoder WaveNet network. Defaults to 192 as in the original work.\n        hidden_channels_dp (int):\n            Number of layer channels of the duration predictor network. Defaults to 256 as in the original work.\n        mean_only (bool):\n            If true predict only the mean values by the decoder flow. Defaults to True.\n        out_channels (int):\n            Number of channels of the model output tensor. Defaults to 80.\n        num_flow_blocks_dec (int):\n            Number of decoder blocks. Defaults to 12.\n        inference_noise_scale (float):\n            Noise scale used at inference. Defaults to 0.33.\n        kernel_size_dec (int):\n            Decoder kernel size. Defaults to 5\n        dilation_rate (int):\n            Rate to increase dilation by each layer in a decoder block. Defaults to 1.\n        num_block_layers (int):\n            Number of decoder layers in each decoder block.  Defaults to 4.\n        dropout_p_dec (float):\n            Dropout rate for decoder. Defaults to 0.1.\n        num_speaker (int):\n            Number of speaker to define the size of speaker embedding layer. Defaults to 0.\n        c_in_channels (int):\n            Number of speaker embedding channels. It is set to 512 if embeddings are learned. Defaults to 0.\n        num_splits (int):\n            Number of split levels in inversible conv1x1 operation. Defaults to 4.\n        num_squeeze (int):\n            Number of squeeze levels. When squeezing channels increases and time steps reduces by the factor\n            'num_squeeze'. Defaults to 2.\n        sigmoid_scale (bool):\n            enable/disable sigmoid scaling in decoder. Defaults to False.\n        mean_only (bool):\n            If True, encoder only computes mean value and uses constant variance for each time step. Defaults to true.\n        encoder_type (str):\n            Encoder module type. Possible values are`[\"rel_pos_transformer\", \"gated_conv\", \"residual_conv_bn\", \"time_depth_separable\"]`\n            Check `TTS.tts.layers.glow_tts.encoder` for more details. Defaults to `rel_pos_transformers` as in the original paper.\n        encoder_params (dict):\n            Encoder module parameters. Defaults to None.\n        d_vector_dim (int):\n            Channels of external speaker embedding vectors. Defaults to 0.\n        data_dep_init_steps (int):\n            Number of steps used for computing normalization parameters at the beginning of the training. GlowTTS uses\n            Activation Normalization that pre-computes normalization stats at the beginning and use the same values\n            for the rest. Defaults to 10.\n        style_wav_for_test (str):\n            Path to the wav file used for changing the style of the speech. Defaults to None.\n        inference_noise_scale (float):\n            Variance used for sampling the random noise added to the decoder's input at inference. Defaults to 0.0.\n        length_scale (float):\n            Multiply the predicted durations with this value to change the speech speed. Defaults to 1.\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n        noam_schedule (bool):\n            enable / disable the use of Noam LR scheduler. Defaults to False.\n        warmup_steps (int):\n            Number of warm-up steps for the Noam scheduler. Defaults 4000.\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n    \"\"\"\n\n    model: str = \"glow_tts\"\n\n    # model params\n    num_chars: int = None\n    encoder_type: str = \"rel_pos_transformer\"\n    encoder_params: dict = field(\n        default_factory=lambda: {\n            \"kernel_size\": 3,\n            \"dropout_p\": 0.1,\n            \"num_layers\": 6,\n            \"num_heads\": 2,\n            \"hidden_channels_ffn\": 768,\n        }\n    )\n    use_encoder_prenet: bool = True\n    hidden_channels_enc: int = 192\n    hidden_channels_dec: int = 192\n    hidden_channels_dp: int = 256\n    dropout_p_dp: float = 0.1\n    dropout_p_dec: float = 0.05\n    mean_only: bool = True\n    out_channels: int = 80\n    num_flow_blocks_dec: int = 12\n    inference_noise_scale: float = 0.33\n    kernel_size_dec: int = 5\n    dilation_rate: int = 1\n    num_block_layers: int = 4\n    num_speakers: int = 0\n    c_in_channels: int = 0\n    num_splits: int = 4\n    num_squeeze: int = 2\n    sigmoid_scale: bool = False\n    encoder_type: str = \"rel_pos_transformer\"\n    encoder_params: dict = field(\n        default_factory=lambda: {\n            \"kernel_size\": 3,\n            \"dropout_p\": 0.1,\n            \"num_layers\": 6,\n            \"num_heads\": 2,\n            \"hidden_channels_ffn\": 768,\n            \"input_length\": None,\n        }\n    )\n    d_vector_dim: int = 0\n\n    # training params\n    data_dep_init_steps: int = 10\n\n    # inference params\n    style_wav_for_test: str = None\n    inference_noise_scale: float = 0.0\n    length_scale: float = 1.0\n\n    # multi-speaker settings\n    use_speaker_embedding: bool = False\n    speakers_file: str = None\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n\n    # optimizer parameters\n    optimizer: str = \"RAdam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    grad_clip: float = 5.0\n    lr: float = 1e-3\n\n    # overrides\n    min_seq_len: int = 3\n    max_seq_len: int = 500\n    r: int = 1  # DO NOT CHANGE - TODO: make this immutable once coqpit implements it.\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n", "TTS/tts/configs/delightful_tts_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.delightful_tts import DelightfulTtsArgs, DelightfulTtsAudioConfig, VocoderConfig\n\n\n@dataclass\nclass DelightfulTTSConfig(BaseTTSConfig):\n    \"\"\"\n    Configuration class for the DelightfulTTS model.\n\n    Attributes:\n        model (str): Name of the model (\"delightful_tts\").\n        audio (DelightfulTtsAudioConfig): Configuration for audio settings.\n        model_args (DelightfulTtsArgs): Configuration for model arguments.\n        use_attn_priors (bool): Whether to use attention priors.\n        vocoder (VocoderConfig): Configuration for the vocoder.\n        init_discriminator (bool): Whether to initialize the discriminator.\n        steps_to_start_discriminator (int): Number of steps to start the discriminator.\n        grad_clip (List[float]): Gradient clipping values.\n        lr_gen (float): Learning rate for the  gan generator.\n        lr_disc (float): Learning rate for the gan discriminator.\n        lr_scheduler_gen (str): Name of the learning rate scheduler for the generator.\n        lr_scheduler_gen_params (dict): Parameters for the learning rate scheduler for the generator.\n        lr_scheduler_disc (str): Name of the learning rate scheduler for the discriminator.\n        lr_scheduler_disc_params (dict): Parameters for the learning rate scheduler for the discriminator.\n        scheduler_after_epoch (bool): Whether to schedule after each epoch.\n        optimizer (str): Name of the optimizer.\n        optimizer_params (dict): Parameters for the optimizer.\n        ssim_loss_alpha (float): Alpha value for the SSIM loss.\n        mel_loss_alpha (float): Alpha value for the mel loss.\n        aligner_loss_alpha (float): Alpha value for the aligner loss.\n        pitch_loss_alpha (float): Alpha value for the pitch loss.\n        energy_loss_alpha (float): Alpha value for the energy loss.\n        u_prosody_loss_alpha (float): Alpha value for the utterance prosody loss.\n        p_prosody_loss_alpha (float): Alpha value for the phoneme prosody loss.\n        dur_loss_alpha (float): Alpha value for the duration loss.\n        char_dur_loss_alpha (float): Alpha value for the character duration loss.\n        binary_align_loss_alpha (float): Alpha value for the binary alignment loss.\n        binary_loss_warmup_epochs (int): Number of warm-up epochs for the binary loss.\n        disc_loss_alpha (float): Alpha value for the discriminator loss.\n        gen_loss_alpha (float): Alpha value for the generator loss.\n        feat_loss_alpha (float): Alpha value for the feature loss.\n        vocoder_mel_loss_alpha (float): Alpha value for the vocoder mel loss.\n        multi_scale_stft_loss_alpha (float): Alpha value for the multi-scale STFT loss.\n        multi_scale_stft_loss_params (dict): Parameters for the multi-scale STFT loss.\n        return_wav (bool): Whether to return audio waveforms.\n        use_weighted_sampler (bool): Whether to use a weighted sampler.\n        weighted_sampler_attrs (dict): Attributes for the weighted sampler.\n        weighted_sampler_multipliers (dict): Multipliers for the weighted sampler.\n        r (int): Value for the `r` override.\n        compute_f0 (bool): Whether to compute F0 values.\n        f0_cache_path (str): Path to the F0 cache.\n        attn_prior_cache_path (str): Path to the attention prior cache.\n        num_speakers (int): Number of speakers.\n        use_speaker_embedding (bool): Whether to use speaker embedding.\n        speakers_file (str): Path to the speaker file.\n        speaker_embedding_channels (int): Number of channels for the speaker embedding.\n        language_ids_file (str): Path to the language IDs file.\n    \"\"\"\n\n    model: str = \"delightful_tts\"\n\n    # model specific params\n    audio: DelightfulTtsAudioConfig = field(default_factory=DelightfulTtsAudioConfig)\n    model_args: DelightfulTtsArgs = field(default_factory=DelightfulTtsArgs)\n    use_attn_priors: bool = True\n\n    # vocoder\n    vocoder: VocoderConfig = field(default_factory=VocoderConfig)\n    init_discriminator: bool = True\n\n    # optimizer\n    steps_to_start_discriminator: int = 200000\n    grad_clip: List[float] = field(default_factory=lambda: [1000, 1000])\n    lr_gen: float = 0.0002\n    lr_disc: float = 0.0002\n    lr_scheduler_gen: str = \"ExponentialLR\"\n    lr_scheduler_gen_params: dict = field(default_factory=lambda: {\"gamma\": 0.999875, \"last_epoch\": -1})\n    lr_scheduler_disc: str = \"ExponentialLR\"\n    lr_scheduler_disc_params: dict = field(default_factory=lambda: {\"gamma\": 0.999875, \"last_epoch\": -1})\n    scheduler_after_epoch: bool = True\n    optimizer: str = \"AdamW\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.8, 0.99], \"eps\": 1e-9, \"weight_decay\": 0.01})\n\n    # acoustic model loss params\n    ssim_loss_alpha: float = 1.0\n    mel_loss_alpha: float = 1.0\n    aligner_loss_alpha: float = 1.0\n    pitch_loss_alpha: float = 1.0\n    energy_loss_alpha: float = 1.0\n    u_prosody_loss_alpha: float = 0.5\n    p_prosody_loss_alpha: float = 0.5\n    dur_loss_alpha: float = 1.0\n    char_dur_loss_alpha: float = 0.01\n    binary_align_loss_alpha: float = 0.1\n    binary_loss_warmup_epochs: int = 10\n\n    # vocoder loss params\n    disc_loss_alpha: float = 1.0\n    gen_loss_alpha: float = 1.0\n    feat_loss_alpha: float = 1.0\n    vocoder_mel_loss_alpha: float = 10.0\n    multi_scale_stft_loss_alpha: float = 2.5\n    multi_scale_stft_loss_params: dict = field(\n        default_factory=lambda: {\n            \"n_ffts\": [1024, 2048, 512],\n            \"hop_lengths\": [120, 240, 50],\n            \"win_lengths\": [600, 1200, 240],\n        }\n    )\n\n    # data loader params\n    return_wav: bool = True\n    use_weighted_sampler: bool = False\n    weighted_sampler_attrs: dict = field(default_factory=lambda: {})\n    weighted_sampler_multipliers: dict = field(default_factory=lambda: {})\n\n    # overrides\n    r: int = 1\n\n    # dataset configs\n    compute_f0: bool = True\n    f0_cache_path: str = None\n    attn_prior_cache_path: str = None\n\n    # multi-speaker settings\n    # use speaker embedding layer\n    num_speakers: int = 0\n    use_speaker_embedding: bool = False\n    speakers_file: str = None\n    speaker_embedding_channels: int = 256\n    language_ids_file: str = None\n    use_language_embedding: bool = False\n\n    # use d-vectors\n    use_d_vector_file: bool = False\n    d_vector_file: str = None\n    d_vector_dim: int = None\n\n    # testing\n    test_sentences: List[List[str]] = field(\n        default_factory=lambda: [\n            [\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"],\n            [\"Be a voice, not an echo.\"],\n            [\"I'm sorry Dave. I'm afraid I can't do that.\"],\n            [\"This cake is great. It's so delicious and moist.\"],\n            [\"Prior to November 22, 1963.\"],\n        ]\n    )\n\n    def __post_init__(self):\n        # Pass multi-speaker parameters to the model args as `model.init_multispeaker()` looks for it there.\n        if self.num_speakers > 0:\n            self.model_args.num_speakers = self.num_speakers\n\n        # speaker embedding settings\n        if self.use_speaker_embedding:\n            self.model_args.use_speaker_embedding = True\n        if self.speakers_file:\n            self.model_args.speakers_file = self.speakers_file\n\n        # d-vector settings\n        if self.use_d_vector_file:\n            self.model_args.use_d_vector_file = True\n        if self.d_vector_dim is not None and self.d_vector_dim > 0:\n            self.model_args.d_vector_dim = self.d_vector_dim\n        if self.d_vector_file:\n            self.model_args.d_vector_file = self.d_vector_file\n", "TTS/tts/configs/speedy_speech_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.forward_tts import ForwardTTSArgs\n\n\n@dataclass\nclass SpeedySpeechConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as SpeedySpeech model.\n\n    Example:\n\n        >>> from TTS.tts.configs.speedy_speech_config import SpeedySpeechConfig\n        >>> config = SpeedySpeechConfig()\n\n     Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `speedy_speech`.\n\n        base_model (str):\n            Name of the base model being configured as this model so that \ud83d\udc38 TTS knows it needs to initiate\n            the base model rather than searching for the `model` implementation. Defaults to `forward_tts`.\n\n        model_args (Coqpit):\n            Model class arguments. Check `FastPitchArgs` for more details. Defaults to `FastPitchArgs()`.\n\n        data_dep_init_steps (int):\n            Number of steps used for computing normalization parameters at the beginning of the training. GlowTTS uses\n            Activation Normalization that pre-computes normalization stats at the beginning and use the same values\n            for the rest. Defaults to 10.\n\n        speakers_file (str):\n            Path to the file containing the list of speakers. Needed at inference for loading matching speaker ids to\n            speaker names. Defaults to `None`.\n\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n\n        d_vector_dim (int):\n            Dimension of the external speaker embeddings. Defaults to 0.\n\n        optimizer (str):\n            Name of the model optimizer. Defaults to `RAdam`.\n\n        optimizer_params (dict):\n            Arguments of the model optimizer. Defaults to `{\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}`.\n\n        lr_scheduler (str):\n            Name of the learning rate scheduler. Defaults to `Noam`.\n\n        lr_scheduler_params (dict):\n            Arguments of the learning rate scheduler. Defaults to `{\"warmup_steps\": 4000}`.\n\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n\n        grad_clip (float):\n            Gradient norm clipping value. Defaults to `5.0`.\n\n        spec_loss_type (str):\n            Type of the spectrogram loss. Check `ForwardTTSLoss` for possible values. Defaults to `l1`.\n\n        duration_loss_type (str):\n            Type of the duration loss. Check `ForwardTTSLoss` for possible values. Defaults to `huber`.\n\n        use_ssim_loss (bool):\n            Enable/disable the use of SSIM (Structural Similarity) loss. Defaults to True.\n\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n\n        ssim_loss_alpha (float):\n            Weight for the SSIM loss. If set 0, disables the SSIM loss. Defaults to 1.0.\n\n        dur_loss_alpha (float):\n            Weight for the duration predictor's loss. If set 0, disables the huber loss. Defaults to 1.0.\n\n        spec_loss_alpha (float):\n            Weight for the L1 spectrogram loss. If set 0, disables the L1 loss. Defaults to 1.0.\n\n        binary_loss_alpha (float):\n            Weight for the binary loss. If set 0, disables the binary loss. Defaults to 1.0.\n\n        binary_loss_warmup_epochs (float):\n            Number of epochs to gradually increase the binary loss impact. Defaults to 150.\n\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n    \"\"\"\n\n    model: str = \"speedy_speech\"\n    base_model: str = \"forward_tts\"\n\n    # set model args as SpeedySpeech\n    model_args: ForwardTTSArgs = field(\n        default_factory=lambda: ForwardTTSArgs(\n            use_pitch=False,\n            encoder_type=\"residual_conv_bn\",\n            encoder_params={\n                \"kernel_size\": 4,\n                \"dilations\": 4 * [1, 2, 4] + [1],\n                \"num_conv_blocks\": 2,\n                \"num_res_blocks\": 13,\n            },\n            decoder_type=\"residual_conv_bn\",\n            decoder_params={\n                \"kernel_size\": 4,\n                \"dilations\": 4 * [1, 2, 4, 8] + [1],\n                \"num_conv_blocks\": 2,\n                \"num_res_blocks\": 17,\n            },\n            out_channels=80,\n            hidden_channels=128,\n            positional_encoding=True,\n            detach_duration_predictor=True,\n        )\n    )\n\n    # multi-speaker settings\n    num_speakers: int = 0\n    speakers_file: str = None\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n    d_vector_dim: int = 0\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n\n    # loss params\n    spec_loss_type: str = \"l1\"\n    duration_loss_type: str = \"huber\"\n    use_ssim_loss: bool = False\n    ssim_loss_alpha: float = 1.0\n    dur_loss_alpha: float = 1.0\n    spec_loss_alpha: float = 1.0\n    aligner_loss_alpha: float = 1.0\n    binary_align_loss_alpha: float = 0.3\n    binary_loss_warmup_epochs: int = 150\n\n    # overrides\n    min_seq_len: int = 13\n    max_seq_len: int = 200\n    r: int = 1  # DO NOT CHANGE\n\n    # dataset configs\n    compute_f0: bool = False\n    f0_cache_path: str = None\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n\n    def __post_init__(self):\n        # Pass multi-speaker parameters to the model args as `model.init_multispeaker()` looks for it there.\n        if self.num_speakers > 0:\n            self.model_args.num_speakers = self.num_speakers\n\n        # speaker embedding settings\n        if self.use_speaker_embedding:\n            self.model_args.use_speaker_embedding = True\n        if self.speakers_file:\n            self.model_args.speakers_file = self.speakers_file\n\n        # d-vector settings\n        if self.use_d_vector_file:\n            self.model_args.use_d_vector_file = True\n        if self.d_vector_dim is not None and self.d_vector_dim > 0:\n            self.model_args.d_vector_dim = self.d_vector_dim\n        if self.d_vector_file:\n            self.model_args.d_vector_file = self.d_vector_file\n", "TTS/tts/configs/fast_speech_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.forward_tts import ForwardTTSArgs\n\n\n@dataclass\nclass FastSpeechConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastSpeech model.\n\n    Example:\n\n        >>> from TTS.tts.configs.fast_speech_config import FastSpeechConfig\n        >>> config = FastSpeechConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n\n        base_model (str):\n            Name of the base model being configured as this model so that \ud83d\udc38 TTS knows it needs to initiate\n            the base model rather than searching for the `model` implementation. Defaults to `forward_tts`.\n\n        model_args (Coqpit):\n            Model class arguments. Check `FastSpeechArgs` for more details. Defaults to `FastSpeechArgs()`.\n\n        data_dep_init_steps (int):\n            Number of steps used for computing normalization parameters at the beginning of the training. GlowTTS uses\n            Activation Normalization that pre-computes normalization stats at the beginning and use the same values\n            for the rest. Defaults to 10.\n\n        speakers_file (str):\n            Path to the file containing the list of speakers. Needed at inference for loading matching speaker ids to\n            speaker names. Defaults to `None`.\n\n\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n\n        d_vector_dim (int):\n            Dimension of the external speaker embeddings. Defaults to 0.\n\n        optimizer (str):\n            Name of the model optimizer. Defaults to `Adam`.\n\n        optimizer_params (dict):\n            Arguments of the model optimizer. Defaults to `{\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}`.\n\n        lr_scheduler (str):\n            Name of the learning rate scheduler. Defaults to `Noam`.\n\n        lr_scheduler_params (dict):\n            Arguments of the learning rate scheduler. Defaults to `{\"warmup_steps\": 4000}`.\n\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n\n        grad_clip (float):\n            Gradient norm clipping value. Defaults to `5.0`.\n\n        spec_loss_type (str):\n            Type of the spectrogram loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        duration_loss_type (str):\n            Type of the duration loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        use_ssim_loss (bool):\n            Enable/disable the use of SSIM (Structural Similarity) loss. Defaults to True.\n\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n\n        ssim_loss_alpha (float):\n            Weight for the SSIM loss. If set 0, disables the SSIM loss. Defaults to 1.0.\n\n        dur_loss_alpha (float):\n            Weight for the duration predictor's loss. If set 0, disables the huber loss. Defaults to 1.0.\n\n        spec_loss_alpha (float):\n            Weight for the L1 spectrogram loss. If set 0, disables the L1 loss. Defaults to 1.0.\n\n        pitch_loss_alpha (float):\n            Weight for the pitch predictor's loss. If set 0, disables the pitch predictor. Defaults to 1.0.\n\n        binary_loss_alpha (float):\n            Weight for the binary loss. If set 0, disables the binary loss. Defaults to 1.0.\n\n        binary_loss_warmup_epochs (float):\n            Number of epochs to gradually increase the binary loss impact. Defaults to 150.\n\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n    \"\"\"\n\n    model: str = \"fast_speech\"\n    base_model: str = \"forward_tts\"\n\n    # model specific params\n    model_args: ForwardTTSArgs = field(default_factory=lambda: ForwardTTSArgs(use_pitch=False))\n\n    # multi-speaker settings\n    num_speakers: int = 0\n    speakers_file: str = None\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n    d_vector_dim: int = 0\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n\n    # loss params\n    spec_loss_type: str = \"mse\"\n    duration_loss_type: str = \"mse\"\n    use_ssim_loss: bool = True\n    ssim_loss_alpha: float = 1.0\n    dur_loss_alpha: float = 1.0\n    spec_loss_alpha: float = 1.0\n    pitch_loss_alpha: float = 0.0\n    aligner_loss_alpha: float = 1.0\n    binary_align_loss_alpha: float = 1.0\n    binary_loss_warmup_epochs: int = 150\n\n    # overrides\n    min_seq_len: int = 13\n    max_seq_len: int = 200\n    r: int = 1  # DO NOT CHANGE\n\n    # dataset configs\n    compute_f0: bool = False\n    f0_cache_path: str = None\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n\n    def __post_init__(self):\n        # Pass multi-speaker parameters to the model args as `model.init_multispeaker()` looks for it there.\n        if self.num_speakers > 0:\n            self.model_args.num_speakers = self.num_speakers\n\n        # speaker embedding settings\n        if self.use_speaker_embedding:\n            self.model_args.use_speaker_embedding = True\n        if self.speakers_file:\n            self.model_args.speakers_file = self.speakers_file\n\n        # d-vector settings\n        if self.use_d_vector_file:\n            self.model_args.use_d_vector_file = True\n        if self.d_vector_dim is not None and self.d_vector_dim > 0:\n            self.model_args.d_vector_dim = self.d_vector_dim\n        if self.d_vector_file:\n            self.model_args.d_vector_file = self.d_vector_file\n", "TTS/tts/configs/xtts_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.xtts import XttsArgs, XttsAudioConfig\n\n\n@dataclass\nclass XttsConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for XTTS TTS model.\n\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n\n        model_args (XttsArgs):\n            Model architecture arguments. Defaults to `XttsArgs()`.\n\n        audio (XttsAudioConfig):\n            Audio processing configuration. Defaults to `XttsAudioConfig()`.\n\n        model_dir (str):\n            Path to the folder that has all the XTTS models. Defaults to None.\n\n        temperature (float):\n            Temperature for the autoregressive model inference. Larger values makes predictions more creative sacrificing stability. Defaults to `0.2`.\n\n        length_penalty (float):\n            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to the sequence length,\n            which in turn is used to divide the score of the sequence. Since the score is the log likelihood of the sequence (i.e. negative),\n            length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n\n        repetition_penalty (float):\n            The parameter for repetition penalty. 1.0 means no penalty. Defaults to `2.0`.\n\n        top_p (float):\n            If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n            Defaults to `0.8`.\n\n        num_gpt_outputs (int):\n            Number of samples taken from the autoregressive model, all of which are filtered using CLVP.\n            As XTTS is a probabilistic model, more samples means a higher probability of creating something \"great\".\n            Defaults to `16`.\n\n        gpt_cond_len (int):\n            Secs audio to be used as conditioning for the autoregressive model. Defaults to `12`.\n\n        gpt_cond_chunk_len (int):\n            Audio chunk size in secs. Audio is split into chunks and latents are extracted for each chunk. Then the\n            latents are averaged. Chunking improves the stability. It must be <= gpt_cond_len.\n            If gpt_cond_len == gpt_cond_chunk_len, no chunking. Defaults to `4`.\n\n        max_ref_len (int):\n            Maximum number of seconds of audio to be used as conditioning for the decoder. Defaults to `10`.\n\n        sound_norm_refs (bool):\n            Whether to normalize the conditioning audio. Defaults to `False`.\n\n    Note:\n        Check :class:`TTS.tts.configs.shared_configs.BaseTTSConfig` for the inherited parameters.\n\n    Example:\n\n        >>> from TTS.tts.configs.xtts_config import XttsConfig\n        >>> config = XttsConfig()\n    \"\"\"\n\n    model: str = \"xtts\"\n    # model specific params\n    model_args: XttsArgs = field(default_factory=XttsArgs)\n    audio: XttsAudioConfig = field(default_factory=XttsAudioConfig)\n    model_dir: str = None\n    languages: List[str] = field(\n        default_factory=lambda: [\n            \"en\",\n            \"es\",\n            \"fr\",\n            \"de\",\n            \"it\",\n            \"pt\",\n            \"pl\",\n            \"tr\",\n            \"ru\",\n            \"nl\",\n            \"cs\",\n            \"ar\",\n            \"zh-cn\",\n            \"hu\",\n            \"ko\",\n            \"ja\",\n            \"hi\",\n        ]\n    )\n\n    # inference params\n    temperature: float = 0.85\n    length_penalty: float = 1.0\n    repetition_penalty: float = 2.0\n    top_k: int = 50\n    top_p: float = 0.85\n    num_gpt_outputs: int = 1\n\n    # cloning\n    gpt_cond_len: int = 12\n    gpt_cond_chunk_len: int = 4\n    max_ref_len: int = 10\n    sound_norm_refs: bool = False\n", "TTS/tts/configs/tacotron_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig, CapacitronVAEConfig, GSTConfig\n\n\n@dataclass\nclass TacotronConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for Tacotron based models.\n\n    Example:\n\n        >>> from TTS.tts.configs.tacotron_config import TacotronConfig\n        >>> config = TacotronConfig()\n\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Tacotron`.\n        use_gst (bool):\n            enable / disable the use of Global Style Token modules. Defaults to False.\n        gst (GSTConfig):\n            Instance of `GSTConfig` class.\n        gst_style_input (str):\n            Path to the wav file used at inference to set the speech style through GST. If `GST` is enabled and\n            this is not defined, the model uses a zero vector as an input. Defaults to None.\n        use_capacitron_vae (bool):\n            enable / disable the use of Capacitron modules. Defaults to False.\n        capacitron_vae (CapacitronConfig):\n            Instance of `CapacitronConfig` class.\n        num_chars (int):\n            Number of characters used by the model. It must be defined before initializing the model. Defaults to None.\n        num_speakers (int):\n            Number of speakers for multi-speaker models. Defaults to 1.\n        r (int):\n            Initial number of output frames that the decoder computed per iteration. Larger values makes training and inference\n            faster but reduces the quality of the output frames. This must be equal to the largest `r` value used in\n            `gradual_training` schedule. Defaults to 1.\n        gradual_training (List[List]):\n            Parameters for the gradual training schedule. It is in the form `[[a, b, c], [d ,e ,f] ..]` where `a` is\n            the step number to start using the rest of the values, `b` is the `r` value and `c` is the batch size.\n            If sets None, no gradual training is used. Defaults to None.\n        memory_size (int):\n            Defines the number of previous frames used by the Prenet. If set to < 0, then it uses only the last frame.\n            Defaults to -1.\n        prenet_type (str):\n            `original` or `bn`. `original` sets the default Prenet and `bn` uses Batch Normalization version of the\n            Prenet. Defaults to `original`.\n        prenet_dropout (bool):\n            enables / disables the use of dropout in the Prenet. Defaults to True.\n        prenet_dropout_at_inference (bool):\n            enable / disable the use of dropout in the Prenet at the inference time. Defaults to False.\n        stopnet (bool):\n            enable /disable the Stopnet that predicts the end of the decoder sequence. Defaults to True.\n        stopnet_pos_weight (float):\n            Weight that is applied to over-weight positive instances in the Stopnet loss. Use larger values with\n            datasets with longer sentences. Defaults to 0.2.\n        max_decoder_steps (int):\n            Max number of steps allowed for the decoder. Defaults to 50.\n        encoder_in_features (int):\n            Channels of encoder input and character embedding tensors. Defaults to 256.\n        decoder_in_features (int):\n            Channels of decoder input and encoder output tensors. Defaults to 256.\n        out_channels (int):\n            Channels of the final model output. It must match the spectragram size. Defaults to 80.\n        separate_stopnet (bool):\n            Use a distinct Stopnet which is trained separately from the rest of the model. Defaults to True.\n        attention_type (str):\n            attention type. Check ```TTS.tts.layers.attentions.init_attn```. Defaults to 'original'.\n        attention_heads (int):\n            Number of attention heads for GMM attention. Defaults to 5.\n        windowing (bool):\n            It especially useful at inference to keep attention alignment diagonal. Defaults to False.\n        use_forward_attn (bool):\n            It is only valid if ```attn_type``` is ```original```.  Defaults to False.\n        forward_attn_mask (bool):\n            enable/disable extra masking over forward attention. It is useful at inference to prevent\n            possible attention failures. Defaults to False.\n        transition_agent (bool):\n            enable/disable transition agent in forward attention. Defaults to False.\n        location_attn (bool):\n            enable/disable location sensitive attention as in the original Tacotron2 paper.\n            It is only valid if ```attn_type``` is ```original```. Defaults to True.\n        bidirectional_decoder (bool):\n            enable/disable bidirectional decoding. Defaults to False.\n        double_decoder_consistency (bool):\n            enable/disable double decoder consistency. Defaults to False.\n        ddc_r (int):\n            reduction rate used by the coarse decoder when `double_decoder_consistency` is in use. Set this\n            as a multiple of the `r` value. Defaults to 6.\n        speakers_file (str):\n            Path to the speaker mapping file for the Speaker Manager. Defaults to None.\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n        optimizer (str):\n            Optimizer used for the training. Set one from `torch.optim.Optimizer` or `TTS.utils.training`.\n            Defaults to `RAdam`.\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n        lr_scheduler (str):\n            Learning rate scheduler for the training. Use one from `torch.optim.Scheduler` schedulers or\n            `TTS.utils.training`. Defaults to `NoamLR`.\n        lr_scheduler_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to `{\"warmup\": 4000}`.\n        lr (float):\n            Initial learning rate. Defaults to `1e-4`.\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-6`.\n        grad_clip (float):\n            Gradient clipping threshold. Defaults to `5`.\n        seq_len_norm (bool):\n            enable / disable the sequnce length normalization in the loss functions. If set True, loss of a sample\n            is divided by the sequence length. Defaults to False.\n        loss_masking (bool):\n            enable / disable masking the paddings of the samples in loss computation. Defaults to True.\n        decoder_loss_alpha (float):\n            Weight for the decoder loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        postnet_loss_alpha (float):\n            Weight for the postnet loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        postnet_diff_spec_alpha (float):\n            Weight for the postnet differential loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        decoder_diff_spec_alpha (float):\n\n            Weight for the decoder differential loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        decoder_ssim_alpha (float):\n            Weight for the decoder SSIM loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        postnet_ssim_alpha (float):\n            Weight for the postnet SSIM loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        ga_alpha (float):\n            Weight for the guided attention loss. If set less than or equal to zero, it disables the corresponding loss\n            function. Defaults to 5.\n    \"\"\"\n\n    model: str = \"tacotron\"\n    # model_params: TacotronArgs = field(default_factory=lambda: TacotronArgs())\n    use_gst: bool = False\n    gst: GSTConfig = None\n    gst_style_input: str = None\n\n    use_capacitron_vae: bool = False\n    capacitron_vae: CapacitronVAEConfig = None\n\n    # model specific params\n    num_speakers: int = 1\n    num_chars: int = 0\n    r: int = 2\n    gradual_training: List[List[int]] = None\n    memory_size: int = -1\n    prenet_type: str = \"original\"\n    prenet_dropout: bool = True\n    prenet_dropout_at_inference: bool = False\n    stopnet: bool = True\n    separate_stopnet: bool = True\n    stopnet_pos_weight: float = 0.2\n    max_decoder_steps: int = 10000\n    encoder_in_features: int = 256\n    decoder_in_features: int = 256\n    decoder_output_dim: int = 80\n    out_channels: int = 513\n\n    # attention layers\n    attention_type: str = \"original\"\n    attention_heads: int = None\n    attention_norm: str = \"sigmoid\"\n    attention_win: bool = False\n    windowing: bool = False\n    use_forward_attn: bool = False\n    forward_attn_mask: bool = False\n    transition_agent: bool = False\n    location_attn: bool = True\n\n    # advance methods\n    bidirectional_decoder: bool = False\n    double_decoder_consistency: bool = False\n    ddc_r: int = 6\n\n    # multi-speaker settings\n    speakers_file: str = None\n    use_speaker_embedding: bool = False\n    speaker_embedding_dim: int = 512\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n    d_vector_dim: int = None\n\n    # optimizer parameters\n    optimizer: str = \"RAdam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n    seq_len_norm: bool = False\n    loss_masking: bool = True\n\n    # loss params\n    decoder_loss_alpha: float = 0.25\n    postnet_loss_alpha: float = 0.25\n    postnet_diff_spec_alpha: float = 0.25\n    decoder_diff_spec_alpha: float = 0.25\n    decoder_ssim_alpha: float = 0.25\n    postnet_ssim_alpha: float = 0.25\n    ga_alpha: float = 5.0\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n\n    def check_values(self):\n        if self.gradual_training:\n            assert (\n                self.gradual_training[0][1] == self.r\n            ), f\"[!] the first scheduled gradual training `r` must be equal to the model's `r` value. {self.gradual_training[0][1]} vs {self.r}\"\n        if self.model == \"tacotron\" and self.audio is not None:\n            assert self.out_channels == (\n                self.audio.fft_size // 2 + 1\n            ), f\"{self.out_channels} vs {self.audio.fft_size // 2 + 1}\"\n        if self.model == \"tacotron2\" and self.audio is not None:\n            assert self.out_channels == self.audio.num_mels\n", "TTS/tts/configs/fast_pitch_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.forward_tts import ForwardTTSArgs\n\n\n@dataclass\nclass FastPitchConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastPitch model.\n\n    Example:\n\n        >>> from TTS.tts.configs.fast_pitch_config import FastPitchConfig\n        >>> config = FastPitchConfig()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n\n        base_model (str):\n            Name of the base model being configured as this model so that \ud83d\udc38 TTS knows it needs to initiate\n            the base model rather than searching for the `model` implementation. Defaults to `forward_tts`.\n\n        model_args (Coqpit):\n            Model class arguments. Check `FastPitchArgs` for more details. Defaults to `FastPitchArgs()`.\n\n        data_dep_init_steps (int):\n            Number of steps used for computing normalization parameters at the beginning of the training. GlowTTS uses\n            Activation Normalization that pre-computes normalization stats at the beginning and use the same values\n            for the rest. Defaults to 10.\n\n        speakers_file (str):\n            Path to the file containing the list of speakers. Needed at inference for loading matching speaker ids to\n            speaker names. Defaults to `None`.\n\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n\n        d_vector_dim (int):\n            Dimension of the external speaker embeddings. Defaults to 0.\n\n        optimizer (str):\n            Name of the model optimizer. Defaults to `Adam`.\n\n        optimizer_params (dict):\n            Arguments of the model optimizer. Defaults to `{\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}`.\n\n        lr_scheduler (str):\n            Name of the learning rate scheduler. Defaults to `Noam`.\n\n        lr_scheduler_params (dict):\n            Arguments of the learning rate scheduler. Defaults to `{\"warmup_steps\": 4000}`.\n\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n\n        grad_clip (float):\n            Gradient norm clipping value. Defaults to `5.0`.\n\n        spec_loss_type (str):\n            Type of the spectrogram loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        duration_loss_type (str):\n            Type of the duration loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        use_ssim_loss (bool):\n            Enable/disable the use of SSIM (Structural Similarity) loss. Defaults to True.\n\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n\n        ssim_loss_alpha (float):\n            Weight for the SSIM loss. If set 0, disables the SSIM loss. Defaults to 1.0.\n\n        dur_loss_alpha (float):\n            Weight for the duration predictor's loss. If set 0, disables the huber loss. Defaults to 1.0.\n\n        spec_loss_alpha (float):\n            Weight for the L1 spectrogram loss. If set 0, disables the L1 loss. Defaults to 1.0.\n\n        pitch_loss_alpha (float):\n            Weight for the pitch predictor's loss. If set 0, disables the pitch predictor. Defaults to 1.0.\n\n        binary_align_loss_alpha (float):\n            Weight for the binary loss. If set 0, disables the binary loss. Defaults to 1.0.\n\n        binary_loss_warmup_epochs (float):\n            Number of epochs to gradually increase the binary loss impact. Defaults to 150.\n\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n\n        # dataset configs\n        compute_f0(bool):\n            Compute pitch. defaults to True\n\n        f0_cache_path(str):\n            pith cache path. defaults to None\n    \"\"\"\n\n    model: str = \"fast_pitch\"\n    base_model: str = \"forward_tts\"\n\n    # model specific params\n    model_args: ForwardTTSArgs = field(default_factory=ForwardTTSArgs)\n\n    # multi-speaker settings\n    num_speakers: int = 0\n    speakers_file: str = None\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n    d_vector_dim: int = 0\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n\n    # loss params\n    spec_loss_type: str = \"mse\"\n    duration_loss_type: str = \"mse\"\n    use_ssim_loss: bool = True\n    ssim_loss_alpha: float = 1.0\n    spec_loss_alpha: float = 1.0\n    aligner_loss_alpha: float = 1.0\n    pitch_loss_alpha: float = 0.1\n    dur_loss_alpha: float = 0.1\n    binary_align_loss_alpha: float = 0.1\n    binary_loss_warmup_epochs: int = 150\n\n    # overrides\n    min_seq_len: int = 13\n    max_seq_len: int = 200\n    r: int = 1  # DO NOT CHANGE\n\n    # dataset configs\n    compute_f0: bool = True\n    f0_cache_path: str = None\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n\n    def __post_init__(self):\n        # Pass multi-speaker parameters to the model args as `model.init_multispeaker()` looks for it there.\n        if self.num_speakers > 0:\n            self.model_args.num_speakers = self.num_speakers\n\n        # speaker embedding settings\n        if self.use_speaker_embedding:\n            self.model_args.use_speaker_embedding = True\n        if self.speakers_file:\n            self.model_args.speakers_file = self.speakers_file\n\n        # d-vector settings\n        if self.use_d_vector_file:\n            self.model_args.use_d_vector_file = True\n        if self.d_vector_dim is not None and self.d_vector_dim > 0:\n            self.model_args.d_vector_dim = self.d_vector_dim\n        if self.d_vector_file:\n            self.model_args.d_vector_file = self.d_vector_file\n", "TTS/tts/configs/tortoise_config.py": "from dataclasses import dataclass, field\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.tortoise import TortoiseArgs, TortoiseAudioConfig\n\n\n@dataclass\nclass TortoiseConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for Tortoise TTS model.\n\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n\n        model_args (TortoiseArgs):\n            Model architecture arguments. Defaults to `TortoiseArgs()`.\n\n        audio (TortoiseAudioConfig):\n            Audio processing configuration. Defaults to `TortoiseAudioConfig()`.\n\n        model_dir (str):\n            Path to the folder that has all the Tortoise models. Defaults to None.\n\n        temperature (float):\n            Temperature for the autoregressive model inference. Larger values makes predictions more creative sacrificing stability. Defaults to `0.2`.\n\n        length_penalty (float):\n            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to the sequence length,\n            which in turn is used to divide the score of the sequence. Since the score is the log likelihood of the sequence (i.e. negative),\n            length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n\n        reperation_penalty (float):\n            The parameter for repetition penalty. 1.0 means no penalty. Defaults to `2.0`.\n\n        top_p (float):\n            If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n            Defaults to `0.8`.\n\n        cond_free_k (float):\n            Knob that determines how to balance the conditioning free signal with the conditioning-present signal. [0,inf].\n            As cond_free_k increases, the output becomes dominated by the conditioning-free signal.\n            Formula is: output=cond_present_output*(cond_free_k+1)-cond_absenct_output*cond_free_k. Defaults to `2.0`.\n\n        diffusion_temperature (float):\n            Controls the variance of the noise fed into the diffusion model. [0,1]. Values at 0\n            are the \"mean\" prediction of the diffusion network and will sound bland and smeared.\n            Defaults to `1.0`.\n\n        num_autoregressive_samples (int):\n            Number of samples taken from the autoregressive model, all of which are filtered using CLVP.\n            As Tortoise is a probabilistic model, more samples means a higher probability of creating something \"great\".\n            Defaults to `16`.\n\n        diffusion_iterations (int):\n            Number of diffusion steps to perform. [0,4000]. More steps means the network has more chances to iteratively refine\n            the output, which should theoretically mean a higher quality output. Generally a value above 250 is not noticeably better,\n            however. Defaults to `30`.\n\n        sampler (str):\n            Diffusion sampler to be used. `ddim` or `dpm++2m`. Defaults to `ddim`.\n    Note:\n        Check :class:`TTS.tts.configs.shared_configs.BaseTTSConfig` for the inherited parameters.\n\n    Example:\n\n        >>> from TTS.tts.configs.tortoise_config import TortoiseConfig\n        >>> config = TortoiseConfig()\n    \"\"\"\n\n    model: str = \"tortoise\"\n    # model specific params\n    model_args: TortoiseArgs = field(default_factory=TortoiseArgs)\n    audio: TortoiseAudioConfig = field(default_factory=TortoiseAudioConfig)\n    model_dir: str = None\n\n    # settings\n    temperature: float = 0.2\n    length_penalty: float = 1.0\n    repetition_penalty: float = 2.0\n    top_p: float = 0.8\n    cond_free_k: float = 2.0\n    diffusion_temperature: float = 1.0\n\n    # inference params\n    num_autoregressive_samples: int = 16\n    diffusion_iterations: int = 30\n    sampler: str = \"ddim\"\n", "TTS/tts/configs/neuralhmm_tts_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\n\n\n@dataclass\nclass NeuralhmmTTSConfig(BaseTTSConfig):\n    \"\"\"\n    Define parameters for Neural HMM TTS model.\n\n    Example:\n\n        >>> from TTS.tts.configs.overflow_config import OverflowConfig\n        >>> config = OverflowConfig()\n\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Overflow`.\n        run_eval_steps (int):\n            Run evalulation epoch after N steps. If None, waits until training epoch is completed. Defaults to None.\n        save_step (int):\n            Save local checkpoint every save_step steps. Defaults to 500.\n        plot_step (int):\n            Plot training stats on the logger every plot_step steps. Defaults to 1.\n        model_param_stats (bool):\n            Log model parameters stats on the logger dashboard. Defaults to False.\n        force_generate_statistics (bool):\n            Force generate mel normalization statistics. Defaults to False.\n        mel_statistics_parameter_path (str):\n            Path to the mel normalization statistics.If the model doesn't finds a file there it will generate statistics.\n            Defaults to None.\n        num_chars (int):\n            Number of characters used by the model. It must be defined before initializing the model. Defaults to None.\n        state_per_phone (int):\n            Generates N states per phone. Similar, to `add_blank` parameter in GlowTTS but in Overflow it is upsampled by model's encoder. Defaults to 2.\n        encoder_in_out_features (int):\n            Channels of encoder input and character embedding tensors. Defaults to 512.\n        encoder_n_convolutions (int):\n            Number of convolution layers in the encoder. Defaults to 3.\n        out_channels (int):\n            Channels of the final model output. It must match the spectragram size. Defaults to 80.\n        ar_order (int):\n            Autoregressive order of the model. Defaults to 1. In ablations of Neural HMM it was found that more autoregression while giving more variation hurts naturalness of the synthesised audio.\n        sampling_temp (float):\n            Variation added to the sample from the latent space of neural HMM. Defaults to 0.334.\n        deterministic_transition (bool):\n            deterministic duration generation based on duration quantiles as defiend in \"S. Ronanki, O. Watts, S. King, and G. E. Henter, \u201cMedianbased generation of synthetic speech durations using a nonparametric approach,\u201d in Proc. SLT, 2016.\". Defaults to True.\n        duration_threshold (float):\n            Threshold for duration quantiles. Defaults to 0.55. Tune this to change the speaking rate of the synthesis, where lower values defines a slower speaking rate and higher values defines a faster speaking rate.\n        use_grad_checkpointing (bool):\n            Use gradient checkpointing to save memory. In a multi-GPU setting currently pytorch does not supports gradient checkpoint inside a loop so we will have to turn it off then.Adjust depending on whatever get more batch size either by using a single GPU or multi-GPU. Defaults to True.\n        max_sampling_time (int):\n            Maximum sampling time while synthesising latents from neural HMM. Defaults to 1000.\n        prenet_type (str):\n            `original` or `bn`. `original` sets the default Prenet and `bn` uses Batch Normalization version of the\n            Prenet. Defaults to `original`.\n        prenet_dim (int):\n            Dimension of the Prenet. Defaults to 256.\n        prenet_n_layers (int):\n            Number of layers in the Prenet. Defaults to 2.\n        prenet_dropout (float):\n            Dropout rate of the Prenet. Defaults to 0.5.\n        prenet_dropout_at_inference (bool):\n            Use dropout at inference time. Defaults to False.\n        memory_rnn_dim (int):\n            Dimension of the memory LSTM to process the prenet output. Defaults to 1024.\n        outputnet_size (list[int]):\n            Size of the output network inside the neural HMM. Defaults to [1024].\n        flat_start_params (dict):\n            Parameters for the flat start initialization of the neural HMM. Defaults to `{\"mean\": 0.0, \"std\": 1.0, \"transition_p\": 0.14}`.\n            It will be recomputed when you pass the dataset.\n        std_floor (float):\n            Floor value for the standard deviation of the neural HMM. Prevents model cheating by putting point mass and getting infinite likelihood at any datapoint. Defaults to 0.01.\n            It is called `variance flooring` in standard HMM literature.\n        optimizer (str):\n            Optimizer to use for training. Defaults to `adam`.\n        optimizer_params (dict):\n            Parameters for the optimizer. Defaults to `{\"weight_decay\": 1e-6}`.\n        grad_clip (float):\n            Gradient clipping threshold. Defaults to 40_000.\n        lr (float):\n            Learning rate. Defaults to 1e-3.\n        lr_scheduler (str):\n            Learning rate scheduler for the training. Use one from `torch.optim.Scheduler` schedulers or\n            `TTS.utils.training`. Defaults to `None`.\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n    \"\"\"\n\n    model: str = \"NeuralHMM_TTS\"\n\n    # Training and Checkpoint configs\n    run_eval_steps: int = 100\n    save_step: int = 500\n    plot_step: int = 1\n    model_param_stats: bool = False\n\n    # data parameters\n    force_generate_statistics: bool = False\n    mel_statistics_parameter_path: str = None\n\n    # Encoder parameters\n    num_chars: int = None\n    state_per_phone: int = 2\n    encoder_in_out_features: int = 512\n    encoder_n_convolutions: int = 3\n\n    # HMM parameters\n    out_channels: int = 80\n    ar_order: int = 1\n    sampling_temp: float = 0\n    deterministic_transition: bool = True\n    duration_threshold: float = 0.43\n    use_grad_checkpointing: bool = True\n    max_sampling_time: int = 1000\n\n    ## Prenet parameters\n    prenet_type: str = \"original\"\n    prenet_dim: int = 256\n    prenet_n_layers: int = 2\n    prenet_dropout: float = 0.5\n    prenet_dropout_at_inference: bool = True\n    memory_rnn_dim: int = 1024\n\n    ## Outputnet parameters\n    outputnet_size: List[int] = field(default_factory=lambda: [1024])\n    flat_start_params: dict = field(default_factory=lambda: {\"mean\": 0.0, \"std\": 1.0, \"transition_p\": 0.14})\n    std_floor: float = 0.001\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"weight_decay\": 1e-6})\n    grad_clip: float = 40000.0\n    lr: float = 1e-3\n    lr_scheduler: str = None\n\n    # overrides\n    min_text_len: int = 10\n    max_text_len: int = 500\n    min_audio_len: int = 512\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"Be a voice, not an echo.\",\n        ]\n    )\n\n    # Extra needed config\n    r: int = 1\n    use_d_vector_file: bool = False\n    use_speaker_embedding: bool = False\n\n    def check_values(self):\n        \"\"\"Validate the hyperparameters.\n\n        Raises:\n            AssertionError: when the parameters network is not defined\n            AssertionError: transition probability is not between 0 and 1\n        \"\"\"\n        assert self.ar_order > 0, \"AR order must be greater than 0 it is an autoregressive model.\"\n        assert (\n            len(self.outputnet_size) >= 1\n        ), f\"Parameter Network must have atleast one layer check the config file for parameter network. Provided: {self.parameternetwork}\"\n        assert (\n            0 < self.flat_start_params[\"transition_p\"] < 1\n        ), f\"Transition probability must be between 0 and 1. Provided: {self.flat_start_params['transition_p']}\"\n", "TTS/tts/configs/bark_config.py": "import os\nfrom dataclasses import dataclass, field\nfrom typing import Dict\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.layers.bark.model import GPTConfig\nfrom TTS.tts.layers.bark.model_fine import FineGPTConfig\nfrom TTS.tts.models.bark import BarkAudioConfig\nfrom TTS.utils.generic_utils import get_user_data_dir\n\n\n@dataclass\nclass BarkConfig(BaseTTSConfig):\n    \"\"\"Bark TTS configuration\n\n    Args:\n        model (str): model name that registers the model.\n        audio (BarkAudioConfig): audio configuration. Defaults to BarkAudioConfig().\n        num_chars (int): number of characters in the alphabet. Defaults to 0.\n        semantic_config (GPTConfig): semantic configuration. Defaults to GPTConfig().\n        fine_config (FineGPTConfig): fine configuration. Defaults to FineGPTConfig().\n        coarse_config (GPTConfig): coarse configuration. Defaults to GPTConfig().\n        CONTEXT_WINDOW_SIZE (int): GPT context window size. Defaults to 1024.\n        SEMANTIC_RATE_HZ (float): semantic tokens rate in Hz. Defaults to 49.9.\n        SEMANTIC_VOCAB_SIZE (int): semantic vocabulary size. Defaults to 10_000.\n        CODEBOOK_SIZE (int): encodec codebook size. Defaults to 1024.\n        N_COARSE_CODEBOOKS (int): number of coarse codebooks. Defaults to 2.\n        N_FINE_CODEBOOKS (int): number of fine codebooks. Defaults to 8.\n        COARSE_RATE_HZ (int): coarse tokens rate in Hz. Defaults to 75.\n        SAMPLE_RATE (int): sample rate. Defaults to 24_000.\n        USE_SMALLER_MODELS (bool): use smaller models. Defaults to False.\n        TEXT_ENCODING_OFFSET (int): text encoding offset. Defaults to 10_048.\n        SEMANTIC_PAD_TOKEN (int): semantic pad token. Defaults to 10_000.\n        TEXT_PAD_TOKEN ([type]): text pad token. Defaults to 10_048.\n        TEXT_EOS_TOKEN ([type]): text end of sentence token. Defaults to 10_049.\n        TEXT_SOS_TOKEN ([type]): text start of sentence token. Defaults to 10_050.\n        SEMANTIC_INFER_TOKEN (int): semantic infer token. Defaults to 10_051.\n        COARSE_SEMANTIC_PAD_TOKEN (int): coarse semantic pad token. Defaults to 12_048.\n        COARSE_INFER_TOKEN (int): coarse infer token. Defaults to 12_050.\n        REMOTE_BASE_URL ([type]): remote base url. Defaults to \"https://huggingface.co/erogol/bark/tree\".\n        REMOTE_MODEL_PATHS (Dict): remote model paths. Defaults to None.\n        LOCAL_MODEL_PATHS (Dict): local model paths. Defaults to None.\n        SMALL_REMOTE_MODEL_PATHS (Dict): small remote model paths. Defaults to None.\n        CACHE_DIR (str): local cache directory. Defaults to get_user_data_dir().\n        DEF_SPEAKER_DIR (str): default speaker directory to stoke speaker values for voice cloning. Defaults to get_user_data_dir().\n    \"\"\"\n\n    model: str = \"bark\"\n    audio: BarkAudioConfig = field(default_factory=BarkAudioConfig)\n    num_chars: int = 0\n    semantic_config: GPTConfig = field(default_factory=GPTConfig)\n    fine_config: FineGPTConfig = field(default_factory=FineGPTConfig)\n    coarse_config: GPTConfig = field(default_factory=GPTConfig)\n    CONTEXT_WINDOW_SIZE: int = 1024\n    SEMANTIC_RATE_HZ: float = 49.9\n    SEMANTIC_VOCAB_SIZE: int = 10_000\n    CODEBOOK_SIZE: int = 1024\n    N_COARSE_CODEBOOKS: int = 2\n    N_FINE_CODEBOOKS: int = 8\n    COARSE_RATE_HZ: int = 75\n    SAMPLE_RATE: int = 24_000\n    USE_SMALLER_MODELS: bool = False\n\n    TEXT_ENCODING_OFFSET: int = 10_048\n    SEMANTIC_PAD_TOKEN: int = 10_000\n    TEXT_PAD_TOKEN: int = 129_595\n    SEMANTIC_INFER_TOKEN: int = 129_599\n    COARSE_SEMANTIC_PAD_TOKEN: int = 12_048\n    COARSE_INFER_TOKEN: int = 12_050\n\n    REMOTE_BASE_URL = \"https://huggingface.co/erogol/bark/tree/main/\"\n    REMOTE_MODEL_PATHS: Dict = None\n    LOCAL_MODEL_PATHS: Dict = None\n    SMALL_REMOTE_MODEL_PATHS: Dict = None\n    CACHE_DIR: str = str(get_user_data_dir(\"tts/suno/bark_v0\"))\n    DEF_SPEAKER_DIR: str = str(get_user_data_dir(\"tts/bark_v0/speakers\"))\n\n    def __post_init__(self):\n        self.REMOTE_MODEL_PATHS = {\n            \"text\": {\n                \"path\": os.path.join(self.REMOTE_BASE_URL, \"text_2.pt\"),\n                \"checksum\": \"54afa89d65e318d4f5f80e8e8799026a\",\n            },\n            \"coarse\": {\n                \"path\": os.path.join(self.REMOTE_BASE_URL, \"coarse_2.pt\"),\n                \"checksum\": \"8a98094e5e3a255a5c9c0ab7efe8fd28\",\n            },\n            \"fine\": {\n                \"path\": os.path.join(self.REMOTE_BASE_URL, \"fine_2.pt\"),\n                \"checksum\": \"59d184ed44e3650774a2f0503a48a97b\",\n            },\n        }\n        self.LOCAL_MODEL_PATHS = {\n            \"text\": os.path.join(self.CACHE_DIR, \"text_2.pt\"),\n            \"coarse\": os.path.join(self.CACHE_DIR, \"coarse_2.pt\"),\n            \"fine\": os.path.join(self.CACHE_DIR, \"fine_2.pt\"),\n            \"hubert_tokenizer\": os.path.join(self.CACHE_DIR, \"tokenizer.pth\"),\n            \"hubert\": os.path.join(self.CACHE_DIR, \"hubert.pt\"),\n        }\n        self.SMALL_REMOTE_MODEL_PATHS = {\n            \"text\": {\"path\": os.path.join(self.REMOTE_BASE_URL, \"text.pt\")},\n            \"coarse\": {\"path\": os.path.join(self.REMOTE_BASE_URL, \"coarse.pt\")},\n            \"fine\": {\"path\": os.path.join(self.REMOTE_BASE_URL, \"fine.pt\")},\n        }\n        self.sample_rate = self.SAMPLE_RATE  # pylint: disable=attribute-defined-outside-init\n", "TTS/tts/configs/align_tts_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.align_tts import AlignTTSArgs\n\n\n@dataclass\nclass AlignTTSConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for AlignTTS model.\n    Example:\n\n        >>> from TTS.tts.configs.align_tts_config import AlignTTSConfig\n        >>> config = AlignTTSConfig()\n\n    Args:\n        model(str):\n            Model name used for selecting the right model at initialization. Defaults to `align_tts`.\n        positional_encoding (bool):\n            enable / disable positional encoding applied to the encoder output. Defaults to True.\n        hidden_channels (int):\n            Base number of hidden channels. Defines all the layers expect ones defined by the specific encoder or decoder\n            parameters. Defaults to 256.\n        hidden_channels_dp (int):\n            Number of hidden channels of the duration predictor's layers. Defaults to 256.\n        encoder_type (str):\n            Type of the encoder used by the model. Look at `TTS.tts.layers.feed_forward.encoder` for more details.\n            Defaults to `fftransformer`.\n        encoder_params (dict):\n            Parameters used to define the encoder network. Look at `TTS.tts.layers.feed_forward.encoder` for more details.\n            Defaults to `{\"hidden_channels_ffn\": 1024, \"num_heads\": 2, \"num_layers\": 6, \"dropout_p\": 0.1}`.\n        decoder_type (str):\n            Type of the decoder used by the model. Look at `TTS.tts.layers.feed_forward.decoder` for more details.\n            Defaults to `fftransformer`.\n        decoder_params (dict):\n            Parameters used to define the decoder network. Look at `TTS.tts.layers.feed_forward.decoder` for more details.\n            Defaults to `{\"hidden_channels_ffn\": 1024, \"num_heads\": 2, \"num_layers\": 6, \"dropout_p\": 0.1}`.\n        phase_start_steps (List[int]):\n            A list of number of steps required to start the next training phase. AlignTTS has 4 different training\n            phases. Thus you need to define 4 different values to enable phase based training. If None, it\n            trains the whole model together. Defaults to None.\n        ssim_alpha (float):\n            Weight for the SSIM loss. If set <= 0, disables the SSIM loss. Defaults to 1.0.\n        duration_loss_alpha (float):\n            Weight for the duration predictor's loss. Defaults to 1.0.\n        mdn_alpha (float):\n            Weight for the MDN loss. Defaults to 1.0.\n        spec_loss_alpha (float):\n            Weight for the MSE spectrogram loss. If set <= 0, disables the L1 loss. Defaults to 1.0.\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n        noam_schedule (bool):\n            enable / disable the use of Noam LR scheduler. Defaults to False.\n        warmup_steps (int):\n            Number of warm-up steps for the Noam scheduler. Defaults 4000.\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\"\"\"\n\n    model: str = \"align_tts\"\n    # model specific params\n    model_args: AlignTTSArgs = field(default_factory=AlignTTSArgs)\n    phase_start_steps: List[int] = None\n\n    ssim_alpha: float = 1.0\n    spec_loss_alpha: float = 1.0\n    dur_loss_alpha: float = 1.0\n    mdn_alpha: float = 1.0\n\n    # multi-speaker settings\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = None\n    lr_scheduler_params: dict = None\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n\n    # overrides\n    min_seq_len: int = 13\n    max_seq_len: int = 200\n    r: int = 1\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n", "TTS/tts/configs/shared_configs.py": "from dataclasses import asdict, dataclass, field\nfrom typing import Dict, List\n\nfrom coqpit import Coqpit, check_argument\n\nfrom TTS.config import BaseAudioConfig, BaseDatasetConfig, BaseTrainingConfig\n\n\n@dataclass\nclass GSTConfig(Coqpit):\n    \"\"\"Defines the Global Style Token Module\n\n    Args:\n        gst_style_input_wav (str):\n            Path to the wav file used to define the style of the output speech at inference. Defaults to None.\n\n        gst_style_input_weights (dict):\n            Defines the weights for each style token used at inference. Defaults to None.\n\n        gst_embedding_dim (int):\n            Defines the size of the GST embedding vector dimensions. Defaults to 256.\n\n        gst_num_heads (int):\n            Number of attention heads used by the multi-head attention. Defaults to 4.\n\n        gst_num_style_tokens (int):\n            Number of style token vectors. Defaults to 10.\n    \"\"\"\n\n    gst_style_input_wav: str = None\n    gst_style_input_weights: dict = None\n    gst_embedding_dim: int = 256\n    gst_use_speaker_embedding: bool = False\n    gst_num_heads: int = 4\n    gst_num_style_tokens: int = 10\n\n    def check_values(\n        self,\n    ):\n        \"\"\"Check config fields\"\"\"\n        c = asdict(self)\n        super().check_values()\n        check_argument(\"gst_style_input_weights\", c, restricted=False)\n        check_argument(\"gst_style_input_wav\", c, restricted=False)\n        check_argument(\"gst_embedding_dim\", c, restricted=True, min_val=0, max_val=1000)\n        check_argument(\"gst_use_speaker_embedding\", c, restricted=False)\n        check_argument(\"gst_num_heads\", c, restricted=True, min_val=2, max_val=10)\n        check_argument(\"gst_num_style_tokens\", c, restricted=True, min_val=1, max_val=1000)\n\n\n@dataclass\nclass CapacitronVAEConfig(Coqpit):\n    \"\"\"Defines the capacitron VAE Module\n    Args:\n        capacitron_capacity (int):\n            Defines the variational capacity limit of the prosody embeddings. Defaults to 150.\n        capacitron_VAE_embedding_dim (int):\n            Defines the size of the Capacitron embedding vector dimension. Defaults to 128.\n        capacitron_use_text_summary_embeddings (bool):\n            If True, use a text summary embedding in Capacitron. Defaults to True.\n        capacitron_text_summary_embedding_dim (int):\n            Defines the size of the capacitron text embedding vector dimension. Defaults to 128.\n        capacitron_use_speaker_embedding (bool):\n            if True use speaker embeddings in Capacitron. Defaults to False.\n        capacitron_VAE_loss_alpha (float):\n            Weight for the VAE loss of the Tacotron model. If set less than or equal to zero, it disables the\n            corresponding loss function. Defaults to 0.25\n        capacitron_grad_clip (float):\n            Gradient clipping value for all gradients except beta. Defaults to 5.0\n    \"\"\"\n\n    capacitron_loss_alpha: int = 1\n    capacitron_capacity: int = 150\n    capacitron_VAE_embedding_dim: int = 128\n    capacitron_use_text_summary_embeddings: bool = True\n    capacitron_text_summary_embedding_dim: int = 128\n    capacitron_use_speaker_embedding: bool = False\n    capacitron_VAE_loss_alpha: float = 0.25\n    capacitron_grad_clip: float = 5.0\n\n    def check_values(\n        self,\n    ):\n        \"\"\"Check config fields\"\"\"\n        c = asdict(self)\n        super().check_values()\n        check_argument(\"capacitron_capacity\", c, restricted=True, min_val=10, max_val=500)\n        check_argument(\"capacitron_VAE_embedding_dim\", c, restricted=True, min_val=16, max_val=1024)\n        check_argument(\"capacitron_use_speaker_embedding\", c, restricted=False)\n        check_argument(\"capacitron_text_summary_embedding_dim\", c, restricted=False, min_val=16, max_val=512)\n        check_argument(\"capacitron_VAE_loss_alpha\", c, restricted=False)\n        check_argument(\"capacitron_grad_clip\", c, restricted=False)\n\n\n@dataclass\nclass CharactersConfig(Coqpit):\n    \"\"\"Defines arguments for the `BaseCharacters` or `BaseVocabulary` and their subclasses.\n\n    Args:\n        characters_class (str):\n            Defines the class of the characters used. If None, we pick ```Phonemes``` or ```Graphemes``` based on\n            the configuration. Defaults to None.\n\n        vocab_dict (dict):\n            Defines the vocabulary dictionary used to encode the characters. Defaults to None.\n\n        pad (str):\n            characters in place of empty padding. Defaults to None.\n\n        eos (str):\n            characters showing the end of a sentence. Defaults to None.\n\n        bos (str):\n            characters showing the beginning of a sentence. Defaults to None.\n\n        blank (str):\n            Optional character used between characters by some models for better prosody. Defaults to `_blank`.\n\n        characters (str):\n            character set used by the model. Characters not in this list are ignored when converting input text to\n            a list of sequence IDs. Defaults to None.\n\n        punctuations (str):\n            characters considered as punctuation as parsing the input sentence. Defaults to None.\n\n        phonemes (str):\n            characters considered as parsing phonemes. This is only for backwards compat. Use `characters` for new\n            models. Defaults to None.\n\n        is_unique (bool):\n            remove any duplicate characters in the character lists. It is a bandaid for compatibility with the old\n            models trained with character lists with duplicates. Defaults to True.\n\n        is_sorted (bool):\n            Sort the characters in alphabetical order. Defaults to True.\n    \"\"\"\n\n    characters_class: str = None\n\n    # using BaseVocabulary\n    vocab_dict: Dict = None\n\n    # using on BaseCharacters\n    pad: str = None\n    eos: str = None\n    bos: str = None\n    blank: str = None\n    characters: str = None\n    punctuations: str = None\n    phonemes: str = None\n    is_unique: bool = True  # for backwards compatibility of models trained with char sets with duplicates\n    is_sorted: bool = True\n\n\n@dataclass\nclass BaseTTSConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the tts models.\n\n    Args:\n\n        audio (BaseAudioConfig):\n            Audio processor config object instance.\n\n        use_phonemes (bool):\n            enable / disable phoneme use.\n\n        phonemizer (str):\n            Name of the phonemizer to use. If set None, the phonemizer will be selected by `phoneme_language`.\n            Defaults to None.\n\n        phoneme_language (str):\n            Language code for the phonemizer. You can check the list of supported languages by running\n            `python TTS/tts/utils/text/phonemizers/__init__.py`. Defaults to None.\n\n        compute_input_seq_cache (bool):\n            enable / disable precomputation of the phoneme sequences. At the expense of some delay at the beginning of\n            the training, It allows faster data loader time and precise limitation with `max_seq_len` and\n            `min_seq_len`.\n\n        text_cleaner (str):\n            Name of the text cleaner used for cleaning and formatting transcripts.\n\n        enable_eos_bos_chars (bool):\n            enable / disable the use of eos and bos characters.\n\n        test_senteces_file (str):\n            Path to a txt file that has sentences used at test time. The file must have a sentence per line.\n\n        phoneme_cache_path (str):\n            Path to the output folder caching the computed phonemes for each sample.\n\n        characters (CharactersConfig):\n            Instance of a CharactersConfig class.\n\n        batch_group_size (int):\n            Size of the batch groups used for bucketing. By default, the dataloader orders samples by the sequence\n            length for a more efficient and stable training. If `batch_group_size > 1` then it performs bucketing to\n            prevent using the same batches for each epoch.\n\n        loss_masking (bool):\n            enable / disable masking loss values against padded segments of samples in a batch.\n\n        min_text_len (int):\n            Minimum length of input text to be used. All shorter samples will be ignored. Defaults to 0.\n\n        max_text_len (int):\n            Maximum length of input text to be used. All longer samples will be ignored. Defaults to float(\"inf\").\n\n        min_audio_len (int):\n            Minimum length of input audio to be used. All shorter samples will be ignored. Defaults to 0.\n\n        max_audio_len (int):\n            Maximum length of input audio to be used. All longer samples will be ignored. The maximum length in the\n            dataset defines the VRAM used in the training. Hence, pay attention to this value if you encounter an\n            OOM error in training. Defaults to float(\"inf\").\n\n        compute_f0 (int):\n            (Not in use yet).\n\n        compute_energy (int):\n            (Not in use yet).\n\n        compute_linear_spec (bool):\n            If True data loader computes and returns linear spectrograms alongside the other data.\n\n        precompute_num_workers (int):\n            Number of workers to precompute features. Defaults to 0.\n\n        use_noise_augment (bool):\n            Augment the input audio with random noise.\n\n        start_by_longest (bool):\n            If True, the data loader will start loading the longest batch first. It is useful for checking OOM issues.\n            Defaults to False.\n\n        shuffle (bool):\n            If True, the data loader will shuffle the dataset when there is not sampler defined. Defaults to True.\n\n        drop_last (bool):\n            If True, the data loader will drop the last batch if it is not complete. It helps to prevent\n            issues that emerge from the partial batch statistics. Defaults to True.\n\n        add_blank (bool):\n            Add blank characters between each other two characters. It improves performance for some models at expense\n            of slower run-time due to the longer input sequence.\n\n        datasets (List[BaseDatasetConfig]):\n            List of datasets used for training. If multiple datasets are provided, they are merged and used together\n            for training.\n\n        optimizer (str):\n            Optimizer used for the training. Set one from `torch.optim.Optimizer` or `TTS.utils.training`.\n            Defaults to ``.\n\n        optimizer_params (dict):\n            Optimizer kwargs. Defaults to `{\"betas\": [0.8, 0.99], \"weight_decay\": 0.0}`\n\n        lr_scheduler (str):\n            Learning rate scheduler for the training. Use one from `torch.optim.Scheduler` schedulers or\n            `TTS.utils.training`. Defaults to ``.\n\n        lr_scheduler_params (dict):\n            Parameters for the generator learning rate scheduler. Defaults to `{\"warmup\": 4000}`.\n\n        test_sentences (List[str]):\n            List of sentences to be used at testing. Defaults to '[]'\n\n        eval_split_max_size (int):\n            Number maximum of samples to be used for evaluation in proportion split. Defaults to None (Disabled).\n\n        eval_split_size (float):\n            If between 0.0 and 1.0 represents the proportion of the dataset to include in the evaluation set.\n            If > 1, represents the absolute number of evaluation samples. Defaults to 0.01 (1%).\n\n        use_speaker_weighted_sampler (bool):\n            Enable / Disable the batch balancer by speaker. Defaults to ```False```.\n\n        speaker_weighted_sampler_alpha (float):\n            Number that control the influence of the speaker sampler weights. Defaults to ```1.0```.\n\n        use_language_weighted_sampler (bool):\n            Enable / Disable the batch balancer by language. Defaults to ```False```.\n\n        language_weighted_sampler_alpha (float):\n            Number that control the influence of the language sampler weights. Defaults to ```1.0```.\n\n        use_length_weighted_sampler (bool):\n            Enable / Disable the batch balancer by audio length. If enabled the dataset will be divided\n            into 10 buckets considering the min and max audio of the dataset. The sampler weights will be\n            computed forcing to have the same quantity of data for each bucket in each training batch. Defaults to ```False```.\n\n        length_weighted_sampler_alpha (float):\n            Number that control the influence of the length sampler weights. Defaults to ```1.0```.\n    \"\"\"\n\n    audio: BaseAudioConfig = field(default_factory=BaseAudioConfig)\n    # phoneme settings\n    use_phonemes: bool = False\n    phonemizer: str = None\n    phoneme_language: str = None\n    compute_input_seq_cache: bool = False\n    text_cleaner: str = None\n    enable_eos_bos_chars: bool = False\n    test_sentences_file: str = \"\"\n    phoneme_cache_path: str = None\n    # vocabulary parameters\n    characters: CharactersConfig = None\n    add_blank: bool = False\n    # training params\n    batch_group_size: int = 0\n    loss_masking: bool = None\n    # dataloading\n    min_audio_len: int = 1\n    max_audio_len: int = float(\"inf\")\n    min_text_len: int = 1\n    max_text_len: int = float(\"inf\")\n    compute_f0: bool = False\n    compute_energy: bool = False\n    compute_linear_spec: bool = False\n    precompute_num_workers: int = 0\n    use_noise_augment: bool = False\n    start_by_longest: bool = False\n    shuffle: bool = False\n    drop_last: bool = False\n    # dataset\n    datasets: List[BaseDatasetConfig] = field(default_factory=lambda: [BaseDatasetConfig()])\n    # optimizer\n    optimizer: str = \"radam\"\n    optimizer_params: dict = None\n    # scheduler\n    lr_scheduler: str = None\n    lr_scheduler_params: dict = field(default_factory=lambda: {})\n    # testing\n    test_sentences: List[str] = field(default_factory=lambda: [])\n    # evaluation\n    eval_split_max_size: int = None\n    eval_split_size: float = 0.01\n    # weighted samplers\n    use_speaker_weighted_sampler: bool = False\n    speaker_weighted_sampler_alpha: float = 1.0\n    use_language_weighted_sampler: bool = False\n    language_weighted_sampler_alpha: float = 1.0\n    use_length_weighted_sampler: bool = False\n    length_weighted_sampler_alpha: float = 1.0\n", "TTS/tts/configs/overflow_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\n\n\n@dataclass\nclass OverflowConfig(BaseTTSConfig):  # The classname has to be camel case\n    \"\"\"\n    Define parameters for OverFlow model.\n\n    Example:\n\n        >>> from TTS.tts.configs.overflow_config import OverflowConfig\n        >>> config = OverflowConfig()\n\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Overflow`.\n        run_eval_steps (int):\n            Run evalulation epoch after N steps. If None, waits until training epoch is completed. Defaults to None.\n        save_step (int):\n            Save local checkpoint every save_step steps. Defaults to 500.\n        plot_step (int):\n            Plot training stats on the logger every plot_step steps. Defaults to 1.\n        model_param_stats (bool):\n            Log model parameters stats on the logger dashboard. Defaults to False.\n        force_generate_statistics (bool):\n            Force generate mel normalization statistics. Defaults to False.\n        mel_statistics_parameter_path (str):\n            Path to the mel normalization statistics.If the model doesn't finds a file there it will generate statistics.\n            Defaults to None.\n        num_chars (int):\n            Number of characters used by the model. It must be defined before initializing the model. Defaults to None.\n        state_per_phone (int):\n            Generates N states per phone. Similar, to `add_blank` parameter in GlowTTS but in Overflow it is upsampled by model's encoder. Defaults to 2.\n        encoder_in_out_features (int):\n            Channels of encoder input and character embedding tensors. Defaults to 512.\n        encoder_n_convolutions (int):\n            Number of convolution layers in the encoder. Defaults to 3.\n        out_channels (int):\n            Channels of the final model output. It must match the spectragram size. Defaults to 80.\n        ar_order (int):\n            Autoregressive order of the model. Defaults to 1. In ablations of Neural HMM it was found that more autoregression while giving more variation hurts naturalness of the synthesised audio.\n        sampling_temp (float):\n            Variation added to the sample from the latent space of neural HMM. Defaults to 0.334.\n        deterministic_transition (bool):\n            deterministic duration generation based on duration quantiles as defiend in \"S. Ronanki, O. Watts, S. King, and G. E. Henter, \u201cMedianbased generation of synthetic speech durations using a nonparametric approach,\u201d in Proc. SLT, 2016.\". Defaults to True.\n        duration_threshold (float):\n            Threshold for duration quantiles. Defaults to 0.55. Tune this to change the speaking rate of the synthesis, where lower values defines a slower speaking rate and higher values defines a faster speaking rate.\n        use_grad_checkpointing (bool):\n            Use gradient checkpointing to save memory. In a multi-GPU setting currently pytorch does not supports gradient checkpoint inside a loop so we will have to turn it off then.Adjust depending on whatever get more batch size either by using a single GPU or multi-GPU. Defaults to True.\n        max_sampling_time (int):\n            Maximum sampling time while synthesising latents from neural HMM. Defaults to 1000.\n        prenet_type (str):\n            `original` or `bn`. `original` sets the default Prenet and `bn` uses Batch Normalization version of the\n            Prenet. Defaults to `original`.\n        prenet_dim (int):\n            Dimension of the Prenet. Defaults to 256.\n        prenet_n_layers (int):\n            Number of layers in the Prenet. Defaults to 2.\n        prenet_dropout (float):\n            Dropout rate of the Prenet. Defaults to 0.5.\n        prenet_dropout_at_inference (bool):\n            Use dropout at inference time. Defaults to False.\n        memory_rnn_dim (int):\n            Dimension of the memory LSTM to process the prenet output. Defaults to 1024.\n        outputnet_size (list[int]):\n            Size of the output network inside the neural HMM. Defaults to [1024].\n        flat_start_params (dict):\n            Parameters for the flat start initialization of the neural HMM. Defaults to `{\"mean\": 0.0, \"std\": 1.0, \"transition_p\": 0.14}`.\n            It will be recomputed when you pass the dataset.\n        std_floor (float):\n            Floor value for the standard deviation of the neural HMM. Prevents model cheating by putting point mass and getting infinite likelihood at any datapoint. Defaults to 0.01.\n            It is called `variance flooring` in standard HMM literature.\n        hidden_channels_dec (int):\n            Number of base hidden channels used by the decoder WaveNet network. Defaults to 150.\n        kernel_size_dec (int):\n            Decoder kernel size. Defaults to 5\n        dilation_rate (int):\n            Rate to increase dilation by each layer in a decoder block. Defaults to 1.\n        num_flow_blocks_dec (int):\n            Number of decoder layers in each decoder block.  Defaults to 4.\n        dropout_p_dec (float):\n            Dropout rate of the decoder. Defaults to 0.05.\n        num_splits (int):\n            Number of split levels in inversible conv1x1 operation. Defaults to 4.\n        num_squeeze (int):\n            Number of squeeze levels. When squeezing channels increases and time steps reduces by the factor\n            'num_squeeze'. Defaults to 2.\n        sigmoid_scale (bool):\n            enable/disable sigmoid scaling in decoder. Defaults to False.\n        c_in_channels (int):\n            Unused parameter from GlowTTS's decoder. Defaults to 0.\n        optimizer (str):\n            Optimizer to use for training. Defaults to `adam`.\n        optimizer_params (dict):\n            Parameters for the optimizer. Defaults to `{\"weight_decay\": 1e-6}`.\n        grad_clip (float):\n            Gradient clipping threshold. Defaults to 40_000.\n        lr (float):\n            Learning rate. Defaults to 1e-3.\n        lr_scheduler (str):\n            Learning rate scheduler for the training. Use one from `torch.optim.Scheduler` schedulers or\n            `TTS.utils.training`. Defaults to `None`.\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n    \"\"\"\n\n    model: str = \"Overflow\"\n\n    # Training and Checkpoint configs\n    run_eval_steps: int = 100\n    save_step: int = 500\n    plot_step: int = 1\n    model_param_stats: bool = False\n\n    # data parameters\n    force_generate_statistics: bool = False\n    mel_statistics_parameter_path: str = None\n\n    # Encoder parameters\n    num_chars: int = None\n    state_per_phone: int = 2\n    encoder_in_out_features: int = 512\n    encoder_n_convolutions: int = 3\n\n    # HMM parameters\n    out_channels: int = 80\n    ar_order: int = 1\n    sampling_temp: float = 0.334\n    deterministic_transition: bool = True\n    duration_threshold: float = 0.55\n    use_grad_checkpointing: bool = True\n    max_sampling_time: int = 1000\n\n    ## Prenet parameters\n    prenet_type: str = \"original\"\n    prenet_dim: int = 256\n    prenet_n_layers: int = 2\n    prenet_dropout: float = 0.5\n    prenet_dropout_at_inference: bool = False\n    memory_rnn_dim: int = 1024\n\n    ## Outputnet parameters\n    outputnet_size: List[int] = field(default_factory=lambda: [1024])\n    flat_start_params: dict = field(default_factory=lambda: {\"mean\": 0.0, \"std\": 1.0, \"transition_p\": 0.14})\n    std_floor: float = 0.01\n\n    # Decoder parameters\n    hidden_channels_dec: int = 150\n    kernel_size_dec: int = 5\n    dilation_rate: int = 1\n    num_flow_blocks_dec: int = 12\n    num_block_layers: int = 4\n    dropout_p_dec: float = 0.05\n    num_splits: int = 4\n    num_squeeze: int = 2\n    sigmoid_scale: bool = False\n    c_in_channels: int = 0\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"weight_decay\": 1e-6})\n    grad_clip: float = 40000.0\n    lr: float = 1e-3\n    lr_scheduler: str = None\n\n    # overrides\n    min_text_len: int = 10\n    max_text_len: int = 500\n    min_audio_len: int = 512\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"Be a voice, not an echo.\",\n        ]\n    )\n\n    # Extra needed config\n    r: int = 1\n    use_d_vector_file: bool = False\n    use_speaker_embedding: bool = False\n\n    def check_values(self):\n        \"\"\"Validate the hyperparameters.\n\n        Raises:\n            AssertionError: when the parameters network is not defined\n            AssertionError: transition probability is not between 0 and 1\n        \"\"\"\n        assert self.ar_order > 0, \"AR order must be greater than 0 it is an autoregressive model.\"\n        assert (\n            len(self.outputnet_size) >= 1\n        ), f\"Parameter Network must have atleast one layer check the config file for parameter network. Provided: {self.parameternetwork}\"\n        assert (\n            0 < self.flat_start_params[\"transition_p\"] < 1\n        ), f\"Transition probability must be between 0 and 1. Provided: {self.flat_start_params['transition_p']}\"\n", "TTS/tts/configs/__init__.py": "import importlib\nimport os\nfrom inspect import isclass\n\n# import all files under configs/\n# configs_dir = os.path.dirname(__file__)\n# for file in os.listdir(configs_dir):\n#     path = os.path.join(configs_dir, file)\n#     if not file.startswith(\"_\") and not file.startswith(\".\") and (file.endswith(\".py\") or os.path.isdir(path)):\n#         config_name = file[: file.find(\".py\")] if file.endswith(\".py\") else file\n#         module = importlib.import_module(\"TTS.tts.configs.\" + config_name)\n#         for attribute_name in dir(module):\n#             attribute = getattr(module, attribute_name)\n\n#             if isclass(attribute):\n#                 # Add the class to this package's variables\n#                 globals()[attribute_name] = attribute\n", "TTS/tts/configs/tacotron2_config.py": "from dataclasses import dataclass\n\nfrom TTS.tts.configs.tacotron_config import TacotronConfig\n\n\n@dataclass\nclass Tacotron2Config(TacotronConfig):\n    \"\"\"Defines parameters for Tacotron2 based models.\n\n    Example:\n\n        >>> from TTS.tts.configs.tacotron2_config import Tacotron2Config\n        >>> config = Tacotron2Config()\n\n    Check `TacotronConfig` for argument descriptions.\n    \"\"\"\n\n    model: str = \"tacotron2\"\n    out_channels: int = 80\n    encoder_in_features: int = 512\n    decoder_in_features: int = 512\n", "TTS/tts/configs/fastspeech2_config.py": "from dataclasses import dataclass, field\nfrom typing import List\n\nfrom TTS.tts.configs.shared_configs import BaseTTSConfig\nfrom TTS.tts.models.forward_tts import ForwardTTSArgs\n\n\n@dataclass\nclass Fastspeech2Config(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastPitch model.\n\n    Example:\n\n        >>> from TTS.tts.configs.fastspeech2_config import FastSpeech2Config\n        >>> config = FastSpeech2Config()\n\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n\n        base_model (str):\n            Name of the base model being configured as this model so that \ud83d\udc38 TTS knows it needs to initiate\n            the base model rather than searching for the `model` implementation. Defaults to `forward_tts`.\n\n        model_args (Coqpit):\n            Model class arguments. Check `FastPitchArgs` for more details. Defaults to `FastPitchArgs()`.\n\n        data_dep_init_steps (int):\n            Number of steps used for computing normalization parameters at the beginning of the training. GlowTTS uses\n            Activation Normalization that pre-computes normalization stats at the beginning and use the same values\n            for the rest. Defaults to 10.\n\n        speakers_file (str):\n            Path to the file containing the list of speakers. Needed at inference for loading matching speaker ids to\n            speaker names. Defaults to `None`.\n\n        use_speaker_embedding (bool):\n            enable / disable using speaker embeddings for multi-speaker models. If set True, the model is\n            in the multi-speaker mode. Defaults to False.\n\n        use_d_vector_file (bool):\n            enable /disable using external speaker embeddings in place of the learned embeddings. Defaults to False.\n\n        d_vector_file (str):\n            Path to the file including pre-computed speaker embeddings. Defaults to None.\n\n        d_vector_dim (int):\n            Dimension of the external speaker embeddings. Defaults to 0.\n\n        optimizer (str):\n            Name of the model optimizer. Defaults to `Adam`.\n\n        optimizer_params (dict):\n            Arguments of the model optimizer. Defaults to `{\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}`.\n\n        lr_scheduler (str):\n            Name of the learning rate scheduler. Defaults to `Noam`.\n\n        lr_scheduler_params (dict):\n            Arguments of the learning rate scheduler. Defaults to `{\"warmup_steps\": 4000}`.\n\n        lr (float):\n            Initial learning rate. Defaults to `1e-3`.\n\n        grad_clip (float):\n            Gradient norm clipping value. Defaults to `5.0`.\n\n        spec_loss_type (str):\n            Type of the spectrogram loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        duration_loss_type (str):\n            Type of the duration loss. Check `ForwardTTSLoss` for possible values. Defaults to `mse`.\n\n        use_ssim_loss (bool):\n            Enable/disable the use of SSIM (Structural Similarity) loss. Defaults to True.\n\n        wd (float):\n            Weight decay coefficient. Defaults to `1e-7`.\n\n        ssim_loss_alpha (float):\n            Weight for the SSIM loss. If set 0, disables the SSIM loss. Defaults to 1.0.\n\n        dur_loss_alpha (float):\n            Weight for the duration predictor's loss. If set 0, disables the huber loss. Defaults to 1.0.\n\n        spec_loss_alpha (float):\n            Weight for the L1 spectrogram loss. If set 0, disables the L1 loss. Defaults to 1.0.\n\n        pitch_loss_alpha (float):\n            Weight for the pitch predictor's loss. If set 0, disables the pitch predictor. Defaults to 1.0.\n\n        energy_loss_alpha (float):\n            Weight for the energy predictor's loss. If set 0, disables the energy predictor. Defaults to 1.0.\n\n        binary_align_loss_alpha (float):\n            Weight for the binary loss. If set 0, disables the binary loss. Defaults to 1.0.\n\n        binary_loss_warmup_epochs (float):\n            Number of epochs to gradually increase the binary loss impact. Defaults to 150.\n\n        min_seq_len (int):\n            Minimum input sequence length to be used at training.\n\n        max_seq_len (int):\n            Maximum input sequence length to be used at training. Larger values result in more VRAM usage.\n\n        # dataset configs\n        compute_f0(bool):\n            Compute pitch. defaults to True\n\n        f0_cache_path(str):\n            pith cache path. defaults to None\n\n        # dataset configs\n        compute_energy(bool):\n            Compute energy. defaults to True\n\n        energy_cache_path(str):\n            energy cache path. defaults to None\n    \"\"\"\n\n    model: str = \"fastspeech2\"\n    base_model: str = \"forward_tts\"\n\n    # model specific params\n    model_args: ForwardTTSArgs = field(default_factory=lambda: ForwardTTSArgs(use_pitch=True, use_energy=True))\n\n    # multi-speaker settings\n    num_speakers: int = 0\n    speakers_file: str = None\n    use_speaker_embedding: bool = False\n    use_d_vector_file: bool = False\n    d_vector_file: str = False\n    d_vector_dim: int = 0\n\n    # optimizer parameters\n    optimizer: str = \"Adam\"\n    optimizer_params: dict = field(default_factory=lambda: {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6})\n    lr_scheduler: str = \"NoamLR\"\n    lr_scheduler_params: dict = field(default_factory=lambda: {\"warmup_steps\": 4000})\n    lr: float = 1e-4\n    grad_clip: float = 5.0\n\n    # loss params\n    spec_loss_type: str = \"mse\"\n    duration_loss_type: str = \"mse\"\n    use_ssim_loss: bool = True\n    ssim_loss_alpha: float = 1.0\n    spec_loss_alpha: float = 1.0\n    aligner_loss_alpha: float = 1.0\n    pitch_loss_alpha: float = 0.1\n    energy_loss_alpha: float = 0.1\n    dur_loss_alpha: float = 0.1\n    binary_align_loss_alpha: float = 0.1\n    binary_loss_warmup_epochs: int = 150\n\n    # overrides\n    min_seq_len: int = 13\n    max_seq_len: int = 200\n    r: int = 1  # DO NOT CHANGE\n\n    # dataset configs\n    compute_f0: bool = True\n    f0_cache_path: str = None\n\n    # dataset configs\n    compute_energy: bool = True\n    energy_cache_path: str = None\n\n    # testing\n    test_sentences: List[str] = field(\n        default_factory=lambda: [\n            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n            \"Be a voice, not an echo.\",\n            \"I'm sorry Dave. I'm afraid I can't do that.\",\n            \"This cake is great. It's so delicious and moist.\",\n            \"Prior to November 22, 1963.\",\n        ]\n    )\n\n    def __post_init__(self):\n        # Pass multi-speaker parameters to the model args as `model.init_multispeaker()` looks for it there.\n        if self.num_speakers > 0:\n            self.model_args.num_speakers = self.num_speakers\n\n        # speaker embedding settings\n        if self.use_speaker_embedding:\n            self.model_args.use_speaker_embedding = True\n        if self.speakers_file:\n            self.model_args.speakers_file = self.speakers_file\n\n        # d-vector settings\n        if self.use_d_vector_file:\n            self.model_args.use_d_vector_file = True\n        if self.d_vector_dim is not None and self.d_vector_dim > 0:\n            self.model_args.d_vector_dim = self.d_vector_dim\n        if self.d_vector_file:\n            self.model_args.d_vector_file = self.d_vector_file\n"}