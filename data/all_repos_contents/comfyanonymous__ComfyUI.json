{"execution.py": "import sys\nimport copy\nimport logging\nimport threading\nimport heapq\nimport traceback\nimport inspect\nfrom typing import List, Literal, NamedTuple, Optional\n\nimport torch\nimport nodes\n\nimport comfy.model_management\n\ndef get_input_data(inputs, class_def, unique_id, outputs={}, prompt={}, extra_data={}):\n    valid_inputs = class_def.INPUT_TYPES()\n    input_data_all = {}\n    for x in inputs:\n        input_data = inputs[x]\n        if isinstance(input_data, list):\n            input_unique_id = input_data[0]\n            output_index = input_data[1]\n            if input_unique_id not in outputs:\n                input_data_all[x] = (None,)\n                continue\n            obj = outputs[input_unique_id][output_index]\n            input_data_all[x] = obj\n        else:\n            if (\"required\" in valid_inputs and x in valid_inputs[\"required\"]) or (\"optional\" in valid_inputs and x in valid_inputs[\"optional\"]):\n                input_data_all[x] = [input_data]\n\n    if \"hidden\" in valid_inputs:\n        h = valid_inputs[\"hidden\"]\n        for x in h:\n            if h[x] == \"PROMPT\":\n                input_data_all[x] = [prompt]\n            if h[x] == \"EXTRA_PNGINFO\":\n                input_data_all[x] = [extra_data.get('extra_pnginfo', None)]\n            if h[x] == \"UNIQUE_ID\":\n                input_data_all[x] = [unique_id]\n    return input_data_all\n\ndef map_node_over_list(obj, input_data_all, func, allow_interrupt=False):\n    # check if node wants the lists\n    input_is_list = False\n    if hasattr(obj, \"INPUT_IS_LIST\"):\n        input_is_list = obj.INPUT_IS_LIST\n\n    if len(input_data_all) == 0:\n        max_len_input = 0\n    else:\n        max_len_input = max([len(x) for x in input_data_all.values()])\n     \n    # get a slice of inputs, repeat last input when list isn't long enough\n    def slice_dict(d, i):\n        d_new = dict()\n        for k,v in d.items():\n            d_new[k] = v[i if len(v) > i else -1]\n        return d_new\n    \n    results = []\n    if input_is_list:\n        if allow_interrupt:\n            nodes.before_node_execution()\n        results.append(getattr(obj, func)(**input_data_all))\n    elif max_len_input == 0:\n        if allow_interrupt:\n            nodes.before_node_execution()\n        results.append(getattr(obj, func)())\n    else:\n        for i in range(max_len_input):\n            if allow_interrupt:\n                nodes.before_node_execution()\n            results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\n    return results\n\ndef get_output_data(obj, input_data_all):\n    \n    results = []\n    uis = []\n    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\n\n    for r in return_values:\n        if isinstance(r, dict):\n            if 'ui' in r:\n                uis.append(r['ui'])\n            if 'result' in r:\n                results.append(r['result'])\n        else:\n            results.append(r)\n    \n    output = []\n    if len(results) > 0:\n        # check which outputs need concatenating\n        output_is_list = [False] * len(results[0])\n        if hasattr(obj, \"OUTPUT_IS_LIST\"):\n            output_is_list = obj.OUTPUT_IS_LIST\n\n        # merge node execution results\n        for i, is_list in zip(range(len(results[0])), output_is_list):\n            if is_list:\n                output.append([x for o in results for x in o[i]])\n            else:\n                output.append([o[i] for o in results])\n\n    ui = dict()    \n    if len(uis) > 0:\n        ui = {k: [y for x in uis for y in x[k]] for k in uis[0].keys()}\n    return output, ui\n\ndef format_value(x):\n    if x is None:\n        return None\n    elif isinstance(x, (int, float, bool, str)):\n        return x\n    else:\n        return str(x)\n\ndef recursive_execute(server, prompt, outputs, current_item, extra_data, executed, prompt_id, outputs_ui, object_storage):\n    unique_id = current_item\n    inputs = prompt[unique_id]['inputs']\n    class_type = prompt[unique_id]['class_type']\n    class_def = nodes.NODE_CLASS_MAPPINGS[class_type]\n    if unique_id in outputs:\n        return (True, None, None)\n\n    for x in inputs:\n        input_data = inputs[x]\n\n        if isinstance(input_data, list):\n            input_unique_id = input_data[0]\n            output_index = input_data[1]\n            if input_unique_id not in outputs:\n                result = recursive_execute(server, prompt, outputs, input_unique_id, extra_data, executed, prompt_id, outputs_ui, object_storage)\n                if result[0] is not True:\n                    # Another node failed further upstream\n                    return result\n\n    input_data_all = None\n    try:\n        input_data_all = get_input_data(inputs, class_def, unique_id, outputs, prompt, extra_data)\n        if server.client_id is not None:\n            server.last_node_id = unique_id\n            server.send_sync(\"executing\", { \"node\": unique_id, \"prompt_id\": prompt_id }, server.client_id)\n\n        obj = object_storage.get((unique_id, class_type), None)\n        if obj is None:\n            obj = class_def()\n            object_storage[(unique_id, class_type)] = obj\n\n        output_data, output_ui = get_output_data(obj, input_data_all)\n        outputs[unique_id] = output_data\n        if len(output_ui) > 0:\n            outputs_ui[unique_id] = output_ui\n            if server.client_id is not None:\n                server.send_sync(\"executed\", { \"node\": unique_id, \"output\": output_ui, \"prompt_id\": prompt_id }, server.client_id)\n    except comfy.model_management.InterruptProcessingException as iex:\n        logging.info(\"Processing interrupted\")\n\n        # skip formatting inputs/outputs\n        error_details = {\n            \"node_id\": unique_id,\n        }\n\n        return (False, error_details, iex)\n    except Exception as ex:\n        typ, _, tb = sys.exc_info()\n        exception_type = full_type_name(typ)\n        input_data_formatted = {}\n        if input_data_all is not None:\n            input_data_formatted = {}\n            for name, inputs in input_data_all.items():\n                input_data_formatted[name] = [format_value(x) for x in inputs]\n\n        output_data_formatted = {}\n        for node_id, node_outputs in outputs.items():\n            output_data_formatted[node_id] = [[format_value(x) for x in l] for l in node_outputs]\n\n        logging.error(f\"!!! Exception during processing!!! {ex}\")\n        logging.error(traceback.format_exc())\n\n        error_details = {\n            \"node_id\": unique_id,\n            \"exception_message\": str(ex),\n            \"exception_type\": exception_type,\n            \"traceback\": traceback.format_tb(tb),\n            \"current_inputs\": input_data_formatted,\n            \"current_outputs\": output_data_formatted\n        }\n        return (False, error_details, ex)\n\n    executed.add(unique_id)\n\n    return (True, None, None)\n\ndef recursive_will_execute(prompt, outputs, current_item, memo={}):\n    unique_id = current_item\n\n    if unique_id in memo:\n        return memo[unique_id]\n\n    inputs = prompt[unique_id]['inputs']\n    will_execute = []\n    if unique_id in outputs:\n        return []\n\n    for x in inputs:\n        input_data = inputs[x]\n        if isinstance(input_data, list):\n            input_unique_id = input_data[0]\n            output_index = input_data[1]\n            if input_unique_id not in outputs:\n                will_execute += recursive_will_execute(prompt, outputs, input_unique_id, memo)\n\n    memo[unique_id] = will_execute + [unique_id]\n    return memo[unique_id]\n\ndef recursive_output_delete_if_changed(prompt, old_prompt, outputs, current_item):\n    unique_id = current_item\n    inputs = prompt[unique_id]['inputs']\n    class_type = prompt[unique_id]['class_type']\n    class_def = nodes.NODE_CLASS_MAPPINGS[class_type]\n\n    is_changed_old = ''\n    is_changed = ''\n    to_delete = False\n    if hasattr(class_def, 'IS_CHANGED'):\n        if unique_id in old_prompt and 'is_changed' in old_prompt[unique_id]:\n            is_changed_old = old_prompt[unique_id]['is_changed']\n        if 'is_changed' not in prompt[unique_id]:\n            input_data_all = get_input_data(inputs, class_def, unique_id, outputs)\n            if input_data_all is not None:\n                try:\n                    #is_changed = class_def.IS_CHANGED(**input_data_all)\n                    is_changed = map_node_over_list(class_def, input_data_all, \"IS_CHANGED\")\n                    prompt[unique_id]['is_changed'] = is_changed\n                except:\n                    to_delete = True\n        else:\n            is_changed = prompt[unique_id]['is_changed']\n\n    if unique_id not in outputs:\n        return True\n\n    if not to_delete:\n        if is_changed != is_changed_old:\n            to_delete = True\n        elif unique_id not in old_prompt:\n            to_delete = True\n        elif inputs == old_prompt[unique_id]['inputs']:\n            for x in inputs:\n                input_data = inputs[x]\n\n                if isinstance(input_data, list):\n                    input_unique_id = input_data[0]\n                    output_index = input_data[1]\n                    if input_unique_id in outputs:\n                        to_delete = recursive_output_delete_if_changed(prompt, old_prompt, outputs, input_unique_id)\n                    else:\n                        to_delete = True\n                    if to_delete:\n                        break\n        else:\n            to_delete = True\n\n    if to_delete:\n        d = outputs.pop(unique_id)\n        del d\n    return to_delete\n\nclass PromptExecutor:\n    def __init__(self, server):\n        self.server = server\n        self.reset()\n\n    def reset(self):\n        self.outputs = {}\n        self.object_storage = {}\n        self.outputs_ui = {}\n        self.status_messages = []\n        self.success = True\n        self.old_prompt = {}\n\n    def add_message(self, event, data, broadcast: bool):\n        self.status_messages.append((event, data))\n        if self.server.client_id is not None or broadcast:\n            self.server.send_sync(event, data, self.server.client_id)\n\n    def handle_execution_error(self, prompt_id, prompt, current_outputs, executed, error, ex):\n        node_id = error[\"node_id\"]\n        class_type = prompt[node_id][\"class_type\"]\n\n        # First, send back the status to the frontend depending\n        # on the exception type\n        if isinstance(ex, comfy.model_management.InterruptProcessingException):\n            mes = {\n                \"prompt_id\": prompt_id,\n                \"node_id\": node_id,\n                \"node_type\": class_type,\n                \"executed\": list(executed),\n            }\n            self.add_message(\"execution_interrupted\", mes, broadcast=True)\n        else:\n            mes = {\n                \"prompt_id\": prompt_id,\n                \"node_id\": node_id,\n                \"node_type\": class_type,\n                \"executed\": list(executed),\n\n                \"exception_message\": error[\"exception_message\"],\n                \"exception_type\": error[\"exception_type\"],\n                \"traceback\": error[\"traceback\"],\n                \"current_inputs\": error[\"current_inputs\"],\n                \"current_outputs\": error[\"current_outputs\"],\n            }\n            self.add_message(\"execution_error\", mes, broadcast=False)\n        \n        # Next, remove the subsequent outputs since they will not be executed\n        to_delete = []\n        for o in self.outputs:\n            if (o not in current_outputs) and (o not in executed):\n                to_delete += [o]\n                if o in self.old_prompt:\n                    d = self.old_prompt.pop(o)\n                    del d\n        for o in to_delete:\n            d = self.outputs.pop(o)\n            del d\n\n    def execute(self, prompt, prompt_id, extra_data={}, execute_outputs=[]):\n        nodes.interrupt_processing(False)\n\n        if \"client_id\" in extra_data:\n            self.server.client_id = extra_data[\"client_id\"]\n        else:\n            self.server.client_id = None\n\n        self.status_messages = []\n        self.add_message(\"execution_start\", { \"prompt_id\": prompt_id}, broadcast=False)\n\n        with torch.inference_mode():\n            #delete cached outputs if nodes don't exist for them\n            to_delete = []\n            for o in self.outputs:\n                if o not in prompt:\n                    to_delete += [o]\n            for o in to_delete:\n                d = self.outputs.pop(o)\n                del d\n            to_delete = []\n            for o in self.object_storage:\n                if o[0] not in prompt:\n                    to_delete += [o]\n                else:\n                    p = prompt[o[0]]\n                    if o[1] != p['class_type']:\n                        to_delete += [o]\n            for o in to_delete:\n                d = self.object_storage.pop(o)\n                del d\n\n            for x in prompt:\n                recursive_output_delete_if_changed(prompt, self.old_prompt, self.outputs, x)\n\n            current_outputs = set(self.outputs.keys())\n            for x in list(self.outputs_ui.keys()):\n                if x not in current_outputs:\n                    d = self.outputs_ui.pop(x)\n                    del d\n\n            comfy.model_management.cleanup_models(keep_clone_weights_loaded=True)\n            self.add_message(\"execution_cached\",\n                          { \"nodes\": list(current_outputs) , \"prompt_id\": prompt_id},\n                          broadcast=False)\n            executed = set()\n            output_node_id = None\n            to_execute = []\n\n            for node_id in list(execute_outputs):\n                to_execute += [(0, node_id)]\n\n            while len(to_execute) > 0:\n                #always execute the output that depends on the least amount of unexecuted nodes first\n                memo = {}\n                to_execute = sorted(list(map(lambda a: (len(recursive_will_execute(prompt, self.outputs, a[-1], memo)), a[-1]), to_execute)))\n                output_node_id = to_execute.pop(0)[-1]\n\n                # This call shouldn't raise anything if there's an error deep in\n                # the actual SD code, instead it will report the node where the\n                # error was raised\n                self.success, error, ex = recursive_execute(self.server, prompt, self.outputs, output_node_id, extra_data, executed, prompt_id, self.outputs_ui, self.object_storage)\n                if self.success is not True:\n                    self.handle_execution_error(prompt_id, prompt, current_outputs, executed, error, ex)\n                    break\n\n            for x in executed:\n                self.old_prompt[x] = copy.deepcopy(prompt[x])\n            self.server.last_node_id = None\n            if comfy.model_management.DISABLE_SMART_MEMORY:\n                comfy.model_management.unload_all_models()\n\n\n\ndef validate_inputs(prompt, item, validated):\n    unique_id = item\n    if unique_id in validated:\n        return validated[unique_id]\n\n    inputs = prompt[unique_id]['inputs']\n    class_type = prompt[unique_id]['class_type']\n    obj_class = nodes.NODE_CLASS_MAPPINGS[class_type]\n\n    class_inputs = obj_class.INPUT_TYPES()\n    required_inputs = class_inputs['required']\n\n    errors = []\n    valid = True\n\n    validate_function_inputs = []\n    if hasattr(obj_class, \"VALIDATE_INPUTS\"):\n        validate_function_inputs = inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args\n\n    for x in required_inputs:\n        if x not in inputs:\n            error = {\n                \"type\": \"required_input_missing\",\n                \"message\": \"Required input is missing\",\n                \"details\": f\"{x}\",\n                \"extra_info\": {\n                    \"input_name\": x\n                }\n            }\n            errors.append(error)\n            continue\n\n        val = inputs[x]\n        info = required_inputs[x]\n        type_input = info[0]\n        if isinstance(val, list):\n            if len(val) != 2:\n                error = {\n                    \"type\": \"bad_linked_input\",\n                    \"message\": \"Bad linked input, must be a length-2 list of [node_id, slot_index]\",\n                    \"details\": f\"{x}\",\n                    \"extra_info\": {\n                        \"input_name\": x,\n                        \"input_config\": info,\n                        \"received_value\": val\n                    }\n                }\n                errors.append(error)\n                continue\n\n            o_id = val[0]\n            o_class_type = prompt[o_id]['class_type']\n            r = nodes.NODE_CLASS_MAPPINGS[o_class_type].RETURN_TYPES\n            if r[val[1]] != type_input:\n                received_type = r[val[1]]\n                details = f\"{x}, {received_type} != {type_input}\"\n                error = {\n                    \"type\": \"return_type_mismatch\",\n                    \"message\": \"Return type mismatch between linked nodes\",\n                    \"details\": details,\n                    \"extra_info\": {\n                        \"input_name\": x,\n                        \"input_config\": info,\n                        \"received_type\": received_type,\n                        \"linked_node\": val\n                    }\n                }\n                errors.append(error)\n                continue\n            try:\n                r = validate_inputs(prompt, o_id, validated)\n                if r[0] is False:\n                    # `r` will be set in `validated[o_id]` already\n                    valid = False\n                    continue\n            except Exception as ex:\n                typ, _, tb = sys.exc_info()\n                valid = False\n                exception_type = full_type_name(typ)\n                reasons = [{\n                    \"type\": \"exception_during_inner_validation\",\n                    \"message\": \"Exception when validating inner node\",\n                    \"details\": str(ex),\n                    \"extra_info\": {\n                        \"input_name\": x,\n                        \"input_config\": info,\n                        \"exception_message\": str(ex),\n                        \"exception_type\": exception_type,\n                        \"traceback\": traceback.format_tb(tb),\n                        \"linked_node\": val\n                    }\n                }]\n                validated[o_id] = (False, reasons, o_id)\n                continue\n        else:\n            try:\n                if type_input == \"INT\":\n                    val = int(val)\n                    inputs[x] = val\n                if type_input == \"FLOAT\":\n                    val = float(val)\n                    inputs[x] = val\n                if type_input == \"STRING\":\n                    val = str(val)\n                    inputs[x] = val\n            except Exception as ex:\n                error = {\n                    \"type\": \"invalid_input_type\",\n                    \"message\": f\"Failed to convert an input value to a {type_input} value\",\n                    \"details\": f\"{x}, {val}, {ex}\",\n                    \"extra_info\": {\n                        \"input_name\": x,\n                        \"input_config\": info,\n                        \"received_value\": val,\n                        \"exception_message\": str(ex)\n                    }\n                }\n                errors.append(error)\n                continue\n\n            if len(info) > 1:\n                if \"min\" in info[1] and val < info[1][\"min\"]:\n                    error = {\n                        \"type\": \"value_smaller_than_min\",\n                        \"message\": \"Value {} smaller than min of {}\".format(val, info[1][\"min\"]),\n                        \"details\": f\"{x}\",\n                        \"extra_info\": {\n                            \"input_name\": x,\n                            \"input_config\": info,\n                            \"received_value\": val,\n                        }\n                    }\n                    errors.append(error)\n                    continue\n                if \"max\" in info[1] and val > info[1][\"max\"]:\n                    error = {\n                        \"type\": \"value_bigger_than_max\",\n                        \"message\": \"Value {} bigger than max of {}\".format(val, info[1][\"max\"]),\n                        \"details\": f\"{x}\",\n                        \"extra_info\": {\n                            \"input_name\": x,\n                            \"input_config\": info,\n                            \"received_value\": val,\n                        }\n                    }\n                    errors.append(error)\n                    continue\n\n            if x not in validate_function_inputs:\n                if isinstance(type_input, list):\n                    if val not in type_input:\n                        input_config = info\n                        list_info = \"\"\n\n                        # Don't send back gigantic lists like if they're lots of\n                        # scanned model filepaths\n                        if len(type_input) > 20:\n                            list_info = f\"(list of length {len(type_input)})\"\n                            input_config = None\n                        else:\n                            list_info = str(type_input)\n\n                        error = {\n                            \"type\": \"value_not_in_list\",\n                            \"message\": \"Value not in list\",\n                            \"details\": f\"{x}: '{val}' not in {list_info}\",\n                            \"extra_info\": {\n                                \"input_name\": x,\n                                \"input_config\": input_config,\n                                \"received_value\": val,\n                            }\n                        }\n                        errors.append(error)\n                        continue\n\n    if len(validate_function_inputs) > 0:\n        input_data_all = get_input_data(inputs, obj_class, unique_id)\n        input_filtered = {}\n        for x in input_data_all:\n            if x in validate_function_inputs:\n                input_filtered[x] = input_data_all[x]\n\n        #ret = obj_class.VALIDATE_INPUTS(**input_filtered)\n        ret = map_node_over_list(obj_class, input_filtered, \"VALIDATE_INPUTS\")\n        for x in input_filtered:\n            for i, r in enumerate(ret):\n                if r is not True:\n                    details = f\"{x}\"\n                    if r is not False:\n                        details += f\" - {str(r)}\"\n\n                    error = {\n                        \"type\": \"custom_validation_failed\",\n                        \"message\": \"Custom validation failed for node\",\n                        \"details\": details,\n                        \"extra_info\": {\n                            \"input_name\": x,\n                            \"input_config\": info,\n                            \"received_value\": val,\n                        }\n                    }\n                    errors.append(error)\n                    continue\n\n    if len(errors) > 0 or valid is not True:\n        ret = (False, errors, unique_id)\n    else:\n        ret = (True, [], unique_id)\n\n    validated[unique_id] = ret\n    return ret\n\ndef full_type_name(klass):\n    module = klass.__module__\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n\ndef validate_prompt(prompt):\n    outputs = set()\n    for x in prompt:\n        if 'class_type' not in prompt[x]:\n            error = {\n                \"type\": \"invalid_prompt\",\n                \"message\": f\"Cannot execute because a node is missing the class_type property.\",\n                \"details\": f\"Node ID '#{x}'\",\n                \"extra_info\": {}\n            }\n            return (False, error, [], [])\n\n        class_type = prompt[x]['class_type']\n        class_ = nodes.NODE_CLASS_MAPPINGS.get(class_type, None)\n        if class_ is None:\n            error = {\n                \"type\": \"invalid_prompt\",\n                \"message\": f\"Cannot execute because node {class_type} does not exist.\",\n                \"details\": f\"Node ID '#{x}'\",\n                \"extra_info\": {}\n            }\n            return (False, error, [], [])\n\n        if hasattr(class_, 'OUTPUT_NODE') and class_.OUTPUT_NODE is True:\n            outputs.add(x)\n\n    if len(outputs) == 0:\n        error = {\n            \"type\": \"prompt_no_outputs\",\n            \"message\": \"Prompt has no outputs\",\n            \"details\": \"\",\n            \"extra_info\": {}\n        }\n        return (False, error, [], [])\n\n    good_outputs = set()\n    errors = []\n    node_errors = {}\n    validated = {}\n    for o in outputs:\n        valid = False\n        reasons = []\n        try:\n            m = validate_inputs(prompt, o, validated)\n            valid = m[0]\n            reasons = m[1]\n        except Exception as ex:\n            typ, _, tb = sys.exc_info()\n            valid = False\n            exception_type = full_type_name(typ)\n            reasons = [{\n                \"type\": \"exception_during_validation\",\n                \"message\": \"Exception when validating node\",\n                \"details\": str(ex),\n                \"extra_info\": {\n                    \"exception_type\": exception_type,\n                    \"traceback\": traceback.format_tb(tb)\n                }\n            }]\n            validated[o] = (False, reasons, o)\n\n        if valid is True:\n            good_outputs.add(o)\n        else:\n            logging.error(f\"Failed to validate prompt for output {o}:\")\n            if len(reasons) > 0:\n                logging.error(\"* (prompt):\")\n                for reason in reasons:\n                    logging.error(f\"  - {reason['message']}: {reason['details']}\")\n            errors += [(o, reasons)]\n            for node_id, result in validated.items():\n                valid = result[0]\n                reasons = result[1]\n                # If a node upstream has errors, the nodes downstream will also\n                # be reported as invalid, but there will be no errors attached.\n                # So don't return those nodes as having errors in the response.\n                if valid is not True and len(reasons) > 0:\n                    if node_id not in node_errors:\n                        class_type = prompt[node_id]['class_type']\n                        node_errors[node_id] = {\n                            \"errors\": reasons,\n                            \"dependent_outputs\": [],\n                            \"class_type\": class_type\n                        }\n                        logging.error(f\"* {class_type} {node_id}:\")\n                        for reason in reasons:\n                            logging.error(f\"  - {reason['message']}: {reason['details']}\")\n                    node_errors[node_id][\"dependent_outputs\"].append(o)\n            logging.error(\"Output will be ignored\")\n\n    if len(good_outputs) == 0:\n        errors_list = []\n        for o, errors in errors:\n            for error in errors:\n                errors_list.append(f\"{error['message']}: {error['details']}\")\n        errors_list = \"\\n\".join(errors_list)\n\n        error = {\n            \"type\": \"prompt_outputs_failed_validation\",\n            \"message\": \"Prompt outputs failed validation\",\n            \"details\": errors_list,\n            \"extra_info\": {}\n        }\n\n        return (False, error, list(good_outputs), node_errors)\n\n    return (True, None, list(good_outputs), node_errors)\n\nMAXIMUM_HISTORY_SIZE = 10000\n\nclass PromptQueue:\n    def __init__(self, server):\n        self.server = server\n        self.mutex = threading.RLock()\n        self.not_empty = threading.Condition(self.mutex)\n        self.task_counter = 0\n        self.queue = []\n        self.currently_running = {}\n        self.history = {}\n        self.flags = {}\n        server.prompt_queue = self\n\n    def put(self, item):\n        with self.mutex:\n            heapq.heappush(self.queue, item)\n            self.server.queue_updated()\n            self.not_empty.notify()\n\n    def get(self, timeout=None):\n        with self.not_empty:\n            while len(self.queue) == 0:\n                self.not_empty.wait(timeout=timeout)\n                if timeout is not None and len(self.queue) == 0:\n                    return None\n            item = heapq.heappop(self.queue)\n            i = self.task_counter\n            self.currently_running[i] = copy.deepcopy(item)\n            self.task_counter += 1\n            self.server.queue_updated()\n            return (item, i)\n\n    class ExecutionStatus(NamedTuple):\n        status_str: Literal['success', 'error']\n        completed: bool\n        messages: List[str]\n\n    def task_done(self, item_id, outputs,\n                  status: Optional['PromptQueue.ExecutionStatus']):\n        with self.mutex:\n            prompt = self.currently_running.pop(item_id)\n            if len(self.history) > MAXIMUM_HISTORY_SIZE:\n                self.history.pop(next(iter(self.history)))\n\n            status_dict: Optional[dict] = None\n            if status is not None:\n                status_dict = copy.deepcopy(status._asdict())\n\n            self.history[prompt[1]] = {\n                \"prompt\": prompt,\n                \"outputs\": copy.deepcopy(outputs),\n                'status': status_dict,\n            }\n            self.server.queue_updated()\n\n    def get_current_queue(self):\n        with self.mutex:\n            out = []\n            for x in self.currently_running.values():\n                out += [x]\n            return (out, copy.deepcopy(self.queue))\n\n    def get_tasks_remaining(self):\n        with self.mutex:\n            return len(self.queue) + len(self.currently_running)\n\n    def wipe_queue(self):\n        with self.mutex:\n            self.queue = []\n            self.server.queue_updated()\n\n    def delete_queue_item(self, function):\n        with self.mutex:\n            for x in range(len(self.queue)):\n                if function(self.queue[x]):\n                    if len(self.queue) == 1:\n                        self.wipe_queue()\n                    else:\n                        self.queue.pop(x)\n                        heapq.heapify(self.queue)\n                    self.server.queue_updated()\n                    return True\n        return False\n\n    def get_history(self, prompt_id=None, max_items=None, offset=-1):\n        with self.mutex:\n            if prompt_id is None:\n                out = {}\n                i = 0\n                if offset < 0 and max_items is not None:\n                    offset = len(self.history) - max_items\n                for k in self.history:\n                    if i >= offset:\n                        out[k] = self.history[k]\n                        if max_items is not None and len(out) >= max_items:\n                            break\n                    i += 1\n                return out\n            elif prompt_id in self.history:\n                return {prompt_id: copy.deepcopy(self.history[prompt_id])}\n            else:\n                return {}\n\n    def wipe_history(self):\n        with self.mutex:\n            self.history = {}\n\n    def delete_history_item(self, id_to_delete):\n        with self.mutex:\n            self.history.pop(id_to_delete, None)\n\n    def set_flag(self, name, data):\n        with self.mutex:\n            self.flags[name] = data\n            self.not_empty.notify()\n\n    def get_flags(self, reset=True):\n        with self.mutex:\n            if reset:\n                ret = self.flags\n                self.flags = {}\n                return ret\n            else:\n                return self.flags.copy()\n", "cuda_malloc.py": "import os\nimport importlib.util\nfrom comfy.cli_args import args\nimport subprocess\n\n#Can't use pytorch to get the GPU names because the cuda malloc has to be set before the first import.\ndef get_gpu_names():\n    if os.name == 'nt':\n        import ctypes\n\n        # Define necessary C structures and types\n        class DISPLAY_DEVICEA(ctypes.Structure):\n            _fields_ = [\n                ('cb', ctypes.c_ulong),\n                ('DeviceName', ctypes.c_char * 32),\n                ('DeviceString', ctypes.c_char * 128),\n                ('StateFlags', ctypes.c_ulong),\n                ('DeviceID', ctypes.c_char * 128),\n                ('DeviceKey', ctypes.c_char * 128)\n            ]\n\n        # Load user32.dll\n        user32 = ctypes.windll.user32\n\n        # Call EnumDisplayDevicesA\n        def enum_display_devices():\n            device_info = DISPLAY_DEVICEA()\n            device_info.cb = ctypes.sizeof(device_info)\n            device_index = 0\n            gpu_names = set()\n\n            while user32.EnumDisplayDevicesA(None, device_index, ctypes.byref(device_info), 0):\n                device_index += 1\n                gpu_names.add(device_info.DeviceString.decode('utf-8'))\n            return gpu_names\n        return enum_display_devices()\n    else:\n        gpu_names = set()\n        out = subprocess.check_output(['nvidia-smi', '-L'])\n        for l in out.split(b'\\n'):\n            if len(l) > 0:\n                gpu_names.add(l.decode('utf-8').split(' (UUID')[0])\n        return gpu_names\n\nblacklist = {\"GeForce GTX TITAN X\", \"GeForce GTX 980\", \"GeForce GTX 970\", \"GeForce GTX 960\", \"GeForce GTX 950\", \"GeForce 945M\",\n                \"GeForce 940M\", \"GeForce 930M\", \"GeForce 920M\", \"GeForce 910M\", \"GeForce GTX 750\", \"GeForce GTX 745\", \"Quadro K620\",\n                \"Quadro K1200\", \"Quadro K2200\", \"Quadro M500\", \"Quadro M520\", \"Quadro M600\", \"Quadro M620\", \"Quadro M1000\",\n                \"Quadro M1200\", \"Quadro M2000\", \"Quadro M2200\", \"Quadro M3000\", \"Quadro M4000\", \"Quadro M5000\", \"Quadro M5500\", \"Quadro M6000\",\n                \"GeForce MX110\", \"GeForce MX130\", \"GeForce 830M\", \"GeForce 840M\", \"GeForce GTX 850M\", \"GeForce GTX 860M\",\n                \"GeForce GTX 1650\", \"GeForce GTX 1630\", \"Tesla M4\", \"Tesla M6\", \"Tesla M10\", \"Tesla M40\", \"Tesla M60\"\n                }\n\ndef cuda_malloc_supported():\n    try:\n        names = get_gpu_names()\n    except:\n        names = set()\n    for x in names:\n        if \"NVIDIA\" in x:\n            for b in blacklist:\n                if b in x:\n                    return False\n    return True\n\n\nif not args.cuda_malloc:\n    try:\n        version = \"\"\n        torch_spec = importlib.util.find_spec(\"torch\")\n        for folder in torch_spec.submodule_search_locations:\n            ver_file = os.path.join(folder, \"version.py\")\n            if os.path.isfile(ver_file):\n                spec = importlib.util.spec_from_file_location(\"torch_version_import\", ver_file)\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n                version = module.__version__\n        if int(version[0]) >= 2: #enable by default for torch version 2.0 and up\n            args.cuda_malloc = cuda_malloc_supported()\n    except:\n        pass\n\n\nif args.cuda_malloc and not args.disable_cuda_malloc:\n    env_var = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', None)\n    if env_var is None:\n        env_var = \"backend:cudaMallocAsync\"\n    else:\n        env_var += \",backend:cudaMallocAsync\"\n\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = env_var\n", "folder_paths.py": "import os\nimport time\nimport logging\n\nsupported_pt_extensions = set(['.ckpt', '.pt', '.bin', '.pth', '.safetensors', '.pkl'])\n\nfolder_names_and_paths = {}\n\nbase_path = os.path.dirname(os.path.realpath(__file__))\nmodels_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\n\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\n\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"photomaker\"] = ([os.path.join(models_dir, \"photomaker\")], supported_pt_extensions)\n\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\n\noutput_directory = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"input\")\nuser_directory = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"user\")\n\nfilename_list_cache = {}\n\nif not os.path.exists(input_directory):\n    try:\n        os.makedirs(input_directory)\n    except:\n        logging.error(\"Failed to create input directory\")\n\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\n\ndef set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir\n\ndef set_input_directory(input_dir):\n    global input_directory\n    input_directory = input_dir\n\ndef get_output_directory():\n    global output_directory\n    return output_directory\n\ndef get_temp_directory():\n    global temp_directory\n    return temp_directory\n\ndef get_input_directory():\n    global input_directory\n    return input_directory\n\n\n#NOTE: used in http server so don't put folders that should not be accessed remotely\ndef get_directory_by_type(type_name):\n    if type_name == \"output\":\n        return get_output_directory()\n    if type_name == \"temp\":\n        return get_temp_directory()\n    if type_name == \"input\":\n        return get_input_directory()\n    return None\n\n\n# determine base_dir rely on annotation if name is 'filename.ext [annotation]' format\n# otherwise use default_path as base_dir\ndef annotated_filepath(name):\n    if name.endswith(\"[output]\"):\n        base_dir = get_output_directory()\n        name = name[:-9]\n    elif name.endswith(\"[input]\"):\n        base_dir = get_input_directory()\n        name = name[:-8]\n    elif name.endswith(\"[temp]\"):\n        base_dir = get_temp_directory()\n        name = name[:-7]\n    else:\n        return name, None\n\n    return name, base_dir\n\n\ndef get_annotated_filepath(name, default_dir=None):\n    name, base_dir = annotated_filepath(name)\n\n    if base_dir is None:\n        if default_dir is not None:\n            base_dir = default_dir\n        else:\n            base_dir = get_input_directory()  # fallback path\n\n    return os.path.join(base_dir, name)\n\n\ndef exists_annotated_filepath(name):\n    name, base_dir = annotated_filepath(name)\n\n    if base_dir is None:\n        base_dir = get_input_directory()  # fallback path\n\n    filepath = os.path.join(base_dir, name)\n    return os.path.exists(filepath)\n\n\ndef add_model_folder_path(folder_name, full_folder_path):\n    global folder_names_and_paths\n    if folder_name in folder_names_and_paths:\n        folder_names_and_paths[folder_name][0].append(full_folder_path)\n    else:\n        folder_names_and_paths[folder_name] = ([full_folder_path], set())\n\ndef get_folder_paths(folder_name):\n    return folder_names_and_paths[folder_name][0][:]\n\ndef recursive_search(directory, excluded_dir_names=None):\n    if not os.path.isdir(directory):\n        return [], {}\n\n    if excluded_dir_names is None:\n        excluded_dir_names = []\n\n    result = []\n    dirs = {}\n\n    # Attempt to add the initial directory to dirs with error handling\n    try:\n        dirs[directory] = os.path.getmtime(directory)\n    except FileNotFoundError:\n        logging.warning(f\"Warning: Unable to access {directory}. Skipping this path.\")\n\n    logging.debug(\"recursive file list on directory {}\".format(directory))\n    for dirpath, subdirs, filenames in os.walk(directory, followlinks=True, topdown=True):\n        subdirs[:] = [d for d in subdirs if d not in excluded_dir_names]\n        for file_name in filenames:\n            relative_path = os.path.relpath(os.path.join(dirpath, file_name), directory)\n            result.append(relative_path)\n\n        for d in subdirs:\n            path = os.path.join(dirpath, d)\n            try:\n                dirs[path] = os.path.getmtime(path)\n            except FileNotFoundError:\n                logging.warning(f\"Warning: Unable to access {path}. Skipping this path.\")\n                continue\n    logging.debug(\"found {} files\".format(len(result)))\n    return result, dirs\n\ndef filter_files_extensions(files, extensions):\n    return sorted(list(filter(lambda a: os.path.splitext(a)[-1].lower() in extensions or len(extensions) == 0, files)))\n\n\n\ndef get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)\n        if os.path.isfile(full_path):\n            return full_path\n        elif os.path.islink(full_path):\n            logging.warning(\"WARNING path {} exists but doesn't link anywhere, skipping.\".format(full_path))\n\n    return None\n\ndef get_filename_list_(folder_name):\n    global folder_names_and_paths\n    output_list = set()\n    folders = folder_names_and_paths[folder_name]\n    output_folders = {}\n    for x in folders[0]:\n        files, folders_all = recursive_search(x, excluded_dir_names=[\".git\"])\n        output_list.update(filter_files_extensions(files, folders[1]))\n        output_folders = {**output_folders, **folders_all}\n\n    return (sorted(list(output_list)), output_folders, time.perf_counter())\n\ndef cached_filename_list_(folder_name):\n    global filename_list_cache\n    global folder_names_and_paths\n    if folder_name not in filename_list_cache:\n        return None\n    out = filename_list_cache[folder_name]\n\n    for x in out[1]:\n        time_modified = out[1][x]\n        folder = x\n        if os.path.getmtime(folder) != time_modified:\n            return None\n\n    folders = folder_names_and_paths[folder_name]\n    for x in folders[0]:\n        if os.path.isdir(x):\n            if x not in out[1]:\n                return None\n\n    return out\n\ndef get_filename_list(folder_name):\n    out = cached_filename_list_(folder_name)\n    if out is None:\n        out = get_filename_list_(folder_name)\n        global filename_list_cache\n        filename_list_cache[folder_name] = out\n    return list(out[0])\n\ndef get_save_image_path(filename_prefix, output_dir, image_width=0, image_height=0):\n    def map_filename(filename):\n        prefix_len = len(os.path.basename(filename_prefix))\n        prefix = filename[:prefix_len + 1]\n        try:\n            digits = int(filename[prefix_len + 1:].split('_')[0])\n        except:\n            digits = 0\n        return (digits, prefix)\n\n    def compute_vars(input, image_width, image_height):\n        input = input.replace(\"%width%\", str(image_width))\n        input = input.replace(\"%height%\", str(image_height))\n        return input\n\n    filename_prefix = compute_vars(filename_prefix, image_width, image_height)\n\n    subfolder = os.path.dirname(os.path.normpath(filename_prefix))\n    filename = os.path.basename(os.path.normpath(filename_prefix))\n\n    full_output_folder = os.path.join(output_dir, subfolder)\n\n    if os.path.commonpath((output_dir, os.path.abspath(full_output_folder))) != output_dir:\n        err = \"**** ERROR: Saving image outside the output folder is not allowed.\" + \\\n              \"\\n full_output_folder: \" + os.path.abspath(full_output_folder) + \\\n              \"\\n         output_dir: \" + output_dir + \\\n              \"\\n         commonpath: \" + os.path.commonpath((output_dir, os.path.abspath(full_output_folder)))\n        logging.error(err)\n        raise Exception(err)\n\n    try:\n        counter = max(filter(lambda a: os.path.normcase(a[1][:-1]) == os.path.normcase(filename) and a[1][-1] == \"_\", map(map_filename, os.listdir(full_output_folder))))[0] + 1\n    except ValueError:\n        counter = 1\n    except FileNotFoundError:\n        os.makedirs(full_output_folder, exist_ok=True)\n        counter = 1\n    return full_output_folder, filename, counter, subfolder, filename_prefix\n", "nodes.py": "import torch\n\nimport os\nimport sys\nimport json\nimport hashlib\nimport traceback\nimport math\nimport time\nimport random\nimport logging\n\nfrom PIL import Image, ImageOps, ImageSequence, ImageFile\nfrom PIL.PngImagePlugin import PngInfo\n\nimport numpy as np\nimport safetensors.torch\n\nsys.path.insert(0, os.path.join(os.path.dirname(os.path.realpath(__file__)), \"comfy\"))\n\nimport comfy.diffusers_load\nimport comfy.samplers\nimport comfy.sample\nimport comfy.sd\nimport comfy.utils\nimport comfy.controlnet\n\nimport comfy.clip_vision\n\nimport comfy.model_management\nfrom comfy.cli_args import args\n\nimport importlib\n\nimport folder_paths\nimport latent_preview\nimport node_helpers\n\ndef before_node_execution():\n    comfy.model_management.throw_exception_if_processing_interrupted()\n\ndef interrupt_processing(value=True):\n    comfy.model_management.interrupt_current_processing(value)\n\nMAX_RESOLUTION=16384\n\nclass CLIPTextEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}), \"clip\": (\"CLIP\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning\"\n\n    def encode(self, clip, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled}]], )\n\nclass ConditioningCombine:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_1\": (\"CONDITIONING\", ), \"conditioning_2\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"combine\"\n\n    CATEGORY = \"conditioning\"\n\n    def combine(self, conditioning_1, conditioning_2):\n        return (conditioning_1 + conditioning_2, )\n\nclass ConditioningAverage :\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ), \"conditioning_from\": (\"CONDITIONING\", ),\n                              \"conditioning_to_strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"addWeighted\"\n\n    CATEGORY = \"conditioning\"\n\n    def addWeighted(self, conditioning_to, conditioning_from, conditioning_to_strength):\n        out = []\n\n        if len(conditioning_from) > 1:\n            logging.warning(\"Warning: ConditioningAverage conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n\n        cond_from = conditioning_from[0][0]\n        pooled_output_from = conditioning_from[0][1].get(\"pooled_output\", None)\n\n        for i in range(len(conditioning_to)):\n            t1 = conditioning_to[i][0]\n            pooled_output_to = conditioning_to[i][1].get(\"pooled_output\", pooled_output_from)\n            t0 = cond_from[:,:t1.shape[1]]\n            if t0.shape[1] < t1.shape[1]:\n                t0 = torch.cat([t0] + [torch.zeros((1, (t1.shape[1] - t0.shape[1]), t1.shape[2]))], dim=1)\n\n            tw = torch.mul(t1, conditioning_to_strength) + torch.mul(t0, (1.0 - conditioning_to_strength))\n            t_to = conditioning_to[i][1].copy()\n            if pooled_output_from is not None and pooled_output_to is not None:\n                t_to[\"pooled_output\"] = torch.mul(pooled_output_to, conditioning_to_strength) + torch.mul(pooled_output_from, (1.0 - conditioning_to_strength))\n            elif pooled_output_from is not None:\n                t_to[\"pooled_output\"] = pooled_output_from\n\n            n = [tw, t_to]\n            out.append(n)\n        return (out, )\n\nclass ConditioningConcat:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"conditioning_to\": (\"CONDITIONING\",),\n            \"conditioning_from\": (\"CONDITIONING\",),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"concat\"\n\n    CATEGORY = \"conditioning\"\n\n    def concat(self, conditioning_to, conditioning_from):\n        out = []\n\n        if len(conditioning_from) > 1:\n            logging.warning(\"Warning: ConditioningConcat conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n\n        cond_from = conditioning_from[0][0]\n\n        for i in range(len(conditioning_to)):\n            t1 = conditioning_to[i][0]\n            tw = torch.cat((t1, cond_from),1)\n            n = [tw, conditioning_to[i][1].copy()]\n            out.append(n)\n\n        return (out, )\n\nclass ConditioningSetArea:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, width, height, x, y, strength):\n        c = node_helpers.conditioning_set_values(conditioning, {\"area\": (height // 8, width // 8, y // 8, x // 8),\n                                                                \"strength\": strength,\n                                                                \"set_area_to_bounds\": False})\n        return (c, )\n\nclass ConditioningSetAreaPercentage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"height\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"x\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"y\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, width, height, x, y, strength):\n        c = node_helpers.conditioning_set_values(conditioning, {\"area\": (\"percentage\", height, width, y, x),\n                                                                \"strength\": strength,\n                                                                \"set_area_to_bounds\": False})\n        return (c, )\n\nclass ConditioningSetAreaStrength:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, strength):\n        c = node_helpers.conditioning_set_values(conditioning, {\"strength\": strength})\n        return (c, )\n\n\nclass ConditioningSetMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"mask\": (\"MASK\", ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"set_cond_area\": ([\"default\", \"mask bounds\"],),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning\"\n\n    def append(self, conditioning, mask, set_cond_area, strength):\n        set_area_to_bounds = False\n        if set_cond_area != \"default\":\n            set_area_to_bounds = True\n        if len(mask.shape) < 3:\n            mask = mask.unsqueeze(0)\n\n        c = node_helpers.conditioning_set_values(conditioning, {\"mask\": mask,\n                                                                \"set_area_to_bounds\": set_area_to_bounds,\n                                                                \"mask_strength\": strength})\n        return (c, )\n\nclass ConditioningZeroOut:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"zero_out\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def zero_out(self, conditioning):\n        c = []\n        for t in conditioning:\n            d = t[1].copy()\n            if \"pooled_output\" in d:\n                d[\"pooled_output\"] = torch.zeros_like(d[\"pooled_output\"])\n            n = [torch.zeros_like(t[0]), d]\n            c.append(n)\n        return (c, )\n\nclass ConditioningSetTimestepRange:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"start\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"set_range\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def set_range(self, conditioning, start, end):\n        c = node_helpers.conditioning_set_values(conditioning, {\"start_percent\": start,\n                                                                \"end_percent\": end})\n        return (c, )\n\nclass VAEDecode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"latent\"\n\n    def decode(self, vae, samples):\n        return (vae.decode(samples[\"samples\"]), )\n\nclass VAEDecodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"_for_testing\"\n\n    def decode(self, vae, samples, tile_size):\n        return (vae.decode_tiled(samples[\"samples\"], tile_x=tile_size // 8, tile_y=tile_size // 8, ), )\n\nclass VAEEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent\"\n\n    def encode(self, vae, pixels):\n        t = vae.encode(pixels[:,:,:,:3])\n        return ({\"samples\":t}, )\n\nclass VAEEncodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"_for_testing\"\n\n    def encode(self, vae, pixels, tile_size):\n        t = vae.encode_tiled(pixels[:,:,:,:3], tile_x=tile_size, tile_y=tile_size, )\n        return ({\"samples\":t}, )\n\nclass VAEEncodeForInpaint:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ), \"mask\": (\"MASK\", ), \"grow_mask_by\": (\"INT\", {\"default\": 6, \"min\": 0, \"max\": 64, \"step\": 1}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent/inpaint\"\n\n    def encode(self, vae, pixels, mask, grow_mask_by=6):\n        x = (pixels.shape[1] // vae.downscale_ratio) * vae.downscale_ratio\n        y = (pixels.shape[2] // vae.downscale_ratio) * vae.downscale_ratio\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n\n        pixels = pixels.clone()\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % vae.downscale_ratio) // 2\n            y_offset = (pixels.shape[2] % vae.downscale_ratio) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n\n        #grow mask by a few pixels to keep things seamless in latent space\n        if grow_mask_by == 0:\n            mask_erosion = mask\n        else:\n            kernel_tensor = torch.ones((1, 1, grow_mask_by, grow_mask_by))\n            padding = math.ceil((grow_mask_by - 1) / 2)\n\n            mask_erosion = torch.clamp(torch.nn.functional.conv2d(mask.round(), kernel_tensor, padding=padding), 0, 1)\n\n        m = (1.0 - mask.round()).squeeze(1)\n        for i in range(3):\n            pixels[:,:,:,i] -= 0.5\n            pixels[:,:,:,i] *= m\n            pixels[:,:,:,i] += 0.5\n        t = vae.encode(pixels)\n\n        return ({\"samples\":t, \"noise_mask\": (mask_erosion[:,:,:x,:y].round())}, )\n\n\nclass InpaintModelConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"pixels\": (\"IMAGE\", ),\n                             \"mask\": (\"MASK\", ),\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\",\"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/inpaint\"\n\n    def encode(self, positive, negative, pixels, vae, mask):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n\n        orig_pixels = pixels\n        pixels = orig_pixels.clone()\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n\n        m = (1.0 - mask.round()).squeeze(1)\n        for i in range(3):\n            pixels[:,:,:,i] -= 0.5\n            pixels[:,:,:,i] *= m\n            pixels[:,:,:,i] += 0.5\n        concat_latent = vae.encode(pixels)\n        orig_latent = vae.encode(orig_pixels)\n\n        out_latent = {}\n\n        out_latent[\"samples\"] = orig_latent\n        out_latent[\"noise_mask\"] = mask\n\n        out = []\n        for conditioning in [positive, negative]:\n            c = node_helpers.conditioning_set_values(conditioning, {\"concat_latent_image\": concat_latent,\n                                                                    \"concat_mask\": mask})\n            out.append(c)\n        return (out[0], out[1], out_latent)\n\n\nclass SaveLatent:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"latents/ComfyUI\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"_for_testing\"\n\n    def save(self, samples, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n\n        # support save metadata for latent sharing\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = None\n        if not args.disable_metadata:\n            metadata = {\"prompt\": prompt_info}\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        file = f\"{filename}_{counter:05}_.latent\"\n\n        results = list()\n        results.append({\n            \"filename\": file,\n            \"subfolder\": subfolder,\n            \"type\": \"output\"\n        })\n\n        file = os.path.join(full_output_folder, file)\n\n        output = {}\n        output[\"latent_tensor\"] = samples[\"samples\"]\n        output[\"latent_format_version_0\"] = torch.tensor([])\n\n        comfy.utils.save_torch_file(output, file, metadata=metadata)\n        return { \"ui\": { \"latents\": results } }\n\n\nclass LoadLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.endswith(\".latent\")]\n        return {\"required\": {\"latent\": [sorted(files), ]}, }\n\n    CATEGORY = \"_for_testing\"\n\n    RETURN_TYPES = (\"LATENT\", )\n    FUNCTION = \"load\"\n\n    def load(self, latent):\n        latent_path = folder_paths.get_annotated_filepath(latent)\n        latent = safetensors.torch.load_file(latent_path, device=\"cpu\")\n        multiplier = 1.0\n        if \"latent_format_version_0\" not in latent:\n            multiplier = 1.0 / 0.18215\n        samples = {\"samples\": latent[\"latent_tensor\"].float() * multiplier}\n        return (samples, )\n\n    @classmethod\n    def IS_CHANGED(s, latent):\n        image_path = folder_paths.get_annotated_filepath(latent)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, latent):\n        if not folder_paths.exists_annotated_filepath(latent):\n            return \"Invalid latent file: {}\".format(latent)\n        return True\n\n\nclass CheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"config_name\": (folder_paths.get_filename_list(\"configs\"), ),\n                              \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )}}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_checkpoint(self, config_name, ckpt_name):\n        config_path = folder_paths.get_full_path(\"configs\", config_name)\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n        return comfy.sd.load_checkpoint(config_path, ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n\nclass CheckpointLoaderSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders\"\n\n    def load_checkpoint(self, ckpt_name):\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n        out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n        return out[:3]\n\nclass DiffusersLoader:\n    @classmethod\n    def INPUT_TYPES(cls):\n        paths = []\n        for search_path in folder_paths.get_folder_paths(\"diffusers\"):\n            if os.path.exists(search_path):\n                for root, subdir, files in os.walk(search_path, followlinks=True):\n                    if \"model_index.json\" in files:\n                        paths.append(os.path.relpath(root, start=search_path))\n\n        return {\"required\": {\"model_path\": (paths,), }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"advanced/loaders/deprecated\"\n\n    def load_checkpoint(self, model_path, output_vae=True, output_clip=True):\n        for search_path in folder_paths.get_folder_paths(\"diffusers\"):\n            if os.path.exists(search_path):\n                path = os.path.join(search_path, model_path)\n                if os.path.exists(path):\n                    model_path = path\n                    break\n\n        return comfy.diffusers_load.load_diffusers(model_path, output_vae=output_vae, output_clip=output_clip, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n\n\nclass unCLIPCheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\", \"CLIP_VISION\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders\"\n\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n        out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n        return out\n\nclass CLIPSetLastLayer:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\", ),\n                              \"stop_at_clip_layer\": (\"INT\", {\"default\": -1, \"min\": -24, \"max\": -1, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"set_last_layer\"\n\n    CATEGORY = \"conditioning\"\n\n    def set_last_layer(self, clip, stop_at_clip_layer):\n        clip = clip.clone()\n        clip.clip_layer(stop_at_clip_layer)\n        return (clip,)\n\nclass LoraLoader:\n    def __init__(self):\n        self.loaded_lora = None\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\", ),\n                              \"lora_name\": (folder_paths.get_filename_list(\"loras\"), ),\n                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n                              \"strength_clip\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\")\n    FUNCTION = \"load_lora\"\n\n    CATEGORY = \"loaders\"\n\n    def load_lora(self, model, clip, lora_name, strength_model, strength_clip):\n        if strength_model == 0 and strength_clip == 0:\n            return (model, clip)\n\n        lora_path = folder_paths.get_full_path(\"loras\", lora_name)\n        lora = None\n        if self.loaded_lora is not None:\n            if self.loaded_lora[0] == lora_path:\n                lora = self.loaded_lora[1]\n            else:\n                temp = self.loaded_lora\n                self.loaded_lora = None\n                del temp\n\n        if lora is None:\n            lora = comfy.utils.load_torch_file(lora_path, safe_load=True)\n            self.loaded_lora = (lora_path, lora)\n\n        model_lora, clip_lora = comfy.sd.load_lora_for_models(model, clip, lora, strength_model, strength_clip)\n        return (model_lora, clip_lora)\n\nclass LoraLoaderModelOnly(LoraLoader):\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"lora_name\": (folder_paths.get_filename_list(\"loras\"), ),\n                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_lora_model_only\"\n\n    def load_lora_model_only(self, model, lora_name, strength_model):\n        return (self.load_lora(model, None, lora_name, strength_model, 0)[0],)\n\nclass VAELoader:\n    @staticmethod\n    def vae_list():\n        vaes = folder_paths.get_filename_list(\"vae\")\n        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n        sdxl_taesd_enc = False\n        sdxl_taesd_dec = False\n        sd1_taesd_enc = False\n        sd1_taesd_dec = False\n        sd3_taesd_enc = False\n        sd3_taesd_dec = False\n\n        for v in approx_vaes:\n            if v.startswith(\"taesd_decoder.\"):\n                sd1_taesd_dec = True\n            elif v.startswith(\"taesd_encoder.\"):\n                sd1_taesd_enc = True\n            elif v.startswith(\"taesdxl_decoder.\"):\n                sdxl_taesd_dec = True\n            elif v.startswith(\"taesdxl_encoder.\"):\n                sdxl_taesd_enc = True\n            elif v.startswith(\"taesd3_decoder.\"):\n                sd3_taesd_dec = True\n            elif v.startswith(\"taesd3_encoder.\"):\n                sd3_taesd_enc = True\n        if sd1_taesd_dec and sd1_taesd_enc:\n            vaes.append(\"taesd\")\n        if sdxl_taesd_dec and sdxl_taesd_enc:\n            vaes.append(\"taesdxl\")\n        if sd3_taesd_dec and sd3_taesd_enc:\n            vaes.append(\"taesd3\")\n        return vaes\n\n    @staticmethod\n    def load_taesd(name):\n        sd = {}\n        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n\n        encoder = next(filter(lambda a: a.startswith(\"{}_encoder.\".format(name)), approx_vaes))\n        decoder = next(filter(lambda a: a.startswith(\"{}_decoder.\".format(name)), approx_vaes))\n\n        enc = comfy.utils.load_torch_file(folder_paths.get_full_path(\"vae_approx\", encoder))\n        for k in enc:\n            sd[\"taesd_encoder.{}\".format(k)] = enc[k]\n\n        dec = comfy.utils.load_torch_file(folder_paths.get_full_path(\"vae_approx\", decoder))\n        for k in dec:\n            sd[\"taesd_decoder.{}\".format(k)] = dec[k]\n\n        if name == \"taesd\":\n            sd[\"vae_scale\"] = torch.tensor(0.18215)\n            sd[\"vae_shift\"] = torch.tensor(0.0)\n        elif name == \"taesdxl\":\n            sd[\"vae_scale\"] = torch.tensor(0.13025)\n            sd[\"vae_shift\"] = torch.tensor(0.0)\n        elif name == \"taesd3\":\n            sd[\"vae_scale\"] = torch.tensor(1.5305)\n            sd[\"vae_shift\"] = torch.tensor(0.0609)\n        return sd\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae_name\": (s.vae_list(), )}}\n    RETURN_TYPES = (\"VAE\",)\n    FUNCTION = \"load_vae\"\n\n    CATEGORY = \"loaders\"\n\n    #TODO: scale factor?\n    def load_vae(self, vae_name):\n        if vae_name in [\"taesd\", \"taesdxl\", \"taesd3\"]:\n            sd = self.load_taesd(vae_name)\n        else:\n            vae_path = folder_paths.get_full_path(\"vae\", vae_name)\n            sd = comfy.utils.load_torch_file(vae_path)\n        vae = comfy.sd.VAE(sd=sd)\n        return (vae,)\n\nclass ControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n\n    CATEGORY = \"loaders\"\n\n    def load_controlnet(self, control_net_name):\n        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)\n        controlnet = comfy.controlnet.load_controlnet(controlnet_path)\n        return (controlnet,)\n\nclass DiffControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n\n    CATEGORY = \"loaders\"\n\n    def load_controlnet(self, model, control_net_name):\n        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)\n        controlnet = comfy.controlnet.load_controlnet(controlnet_path, model)\n        return (controlnet,)\n\n\nclass ControlNetApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_controlnet\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_controlnet(self, conditioning, control_net, image, strength):\n        if strength == 0:\n            return (conditioning, )\n\n        c = []\n        control_hint = image.movedim(-1,1)\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            c_net = control_net.copy().set_cond_hint(control_hint, strength)\n            if 'control' in t[1]:\n                c_net.set_previous_controlnet(t[1]['control'])\n            n[1]['control'] = c_net\n            n[1]['control_apply_to_uncond'] = True\n            c.append(n)\n        return (c, )\n\n\nclass ControlNetApplyAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end_percent\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\")\n    RETURN_NAMES = (\"positive\", \"negative\")\n    FUNCTION = \"apply_controlnet\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_controlnet(self, positive, negative, control_net, image, strength, start_percent, end_percent):\n        if strength == 0:\n            return (positive, negative)\n\n        control_hint = image.movedim(-1,1)\n        cnets = {}\n\n        out = []\n        for conditioning in [positive, negative]:\n            c = []\n            for t in conditioning:\n                d = t[1].copy()\n\n                prev_cnet = d.get('control', None)\n                if prev_cnet in cnets:\n                    c_net = cnets[prev_cnet]\n                else:\n                    c_net = control_net.copy().set_cond_hint(control_hint, strength, (start_percent, end_percent))\n                    c_net.set_previous_controlnet(prev_cnet)\n                    cnets[prev_cnet] = c_net\n\n                d['control'] = c_net\n                d['control_apply_to_uncond'] = False\n                n = [t[0], d]\n                c.append(n)\n            out.append(c)\n        return (out[0], out[1])\n\n\nclass UNETLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"unet\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_unet\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_unet(self, unet_name):\n        unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n        model = comfy.sd.load_unet(unet_path)\n        return (model,)\n\nclass CLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip\"), ),\n                              \"type\": ([\"stable_diffusion\", \"stable_cascade\", \"sd3\", \"stable_audio\"], ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_clip(self, clip_name, type=\"stable_diffusion\"):\n        if type == \"stable_cascade\":\n            clip_type = comfy.sd.CLIPType.STABLE_CASCADE\n        elif type == \"sd3\":\n            clip_type = comfy.sd.CLIPType.SD3\n        elif type == \"stable_audio\":\n            clip_type = comfy.sd.CLIPType.STABLE_AUDIO\n        else:\n            clip_type = comfy.sd.CLIPType.STABLE_DIFFUSION\n\n        clip_path = folder_paths.get_full_path(\"clip\", clip_name)\n        clip = comfy.sd.load_clip(ckpt_paths=[clip_path], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"), clip_type=clip_type)\n        return (clip,)\n\nclass DualCLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name1\": (folder_paths.get_filename_list(\"clip\"), ),\n                              \"clip_name2\": (folder_paths.get_filename_list(\"clip\"), ),\n                              \"type\": ([\"sdxl\", \"sd3\"], ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_clip(self, clip_name1, clip_name2, type):\n        clip_path1 = folder_paths.get_full_path(\"clip\", clip_name1)\n        clip_path2 = folder_paths.get_full_path(\"clip\", clip_name2)\n        if type == \"sdxl\":\n            clip_type = comfy.sd.CLIPType.STABLE_DIFFUSION\n        elif type == \"sd3\":\n            clip_type = comfy.sd.CLIPType.SD3\n\n        clip = comfy.sd.load_clip(ckpt_paths=[clip_path1, clip_path2], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"), clip_type=clip_type)\n        return (clip,)\n\nclass CLIPVisionLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip_vision\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"loaders\"\n\n    def load_clip(self, clip_name):\n        clip_path = folder_paths.get_full_path(\"clip_vision\", clip_name)\n        clip_vision = comfy.clip_vision.load(clip_path)\n        return (clip_vision,)\n\nclass CLIPVisionEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"image\": (\"IMAGE\",)\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION_OUTPUT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning\"\n\n    def encode(self, clip_vision, image):\n        output = clip_vision.encode_image(image)\n        return (output,)\n\nclass StyleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"style_model_name\": (folder_paths.get_filename_list(\"style_models\"), )}}\n\n    RETURN_TYPES = (\"STYLE_MODEL\",)\n    FUNCTION = \"load_style_model\"\n\n    CATEGORY = \"loaders\"\n\n    def load_style_model(self, style_model_name):\n        style_model_path = folder_paths.get_full_path(\"style_models\", style_model_name)\n        style_model = comfy.sd.load_style_model(style_model_path)\n        return (style_model,)\n\n\nclass StyleModelApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"style_model\": (\"STYLE_MODEL\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_stylemodel\"\n\n    CATEGORY = \"conditioning/style_model\"\n\n    def apply_stylemodel(self, clip_vision_output, style_model, conditioning):\n        cond = style_model.get_cond(clip_vision_output).flatten(start_dim=0, end_dim=1).unsqueeze(dim=0)\n        c = []\n        for t in conditioning:\n            n = [torch.cat((t[0], cond), dim=1), t[1].copy()]\n            c.append(n)\n        return (c, )\n\nclass unCLIPConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_adm\"\n\n    CATEGORY = \"conditioning\"\n\n    def apply_adm(self, conditioning, clip_vision_output, strength, noise_augmentation):\n        if strength == 0:\n            return (conditioning, )\n\n        c = []\n        for t in conditioning:\n            o = t[1].copy()\n            x = {\"clip_vision_output\": clip_vision_output, \"strength\": strength, \"noise_augmentation\": noise_augmentation}\n            if \"unclip_conditioning\" in o:\n                o[\"unclip_conditioning\"] = o[\"unclip_conditioning\"][:] + [x]\n            else:\n                o[\"unclip_conditioning\"] = [x]\n            n = [t[0], o]\n            c.append(n)\n        return (c, )\n\nclass GLIGENLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"gligen_name\": (folder_paths.get_filename_list(\"gligen\"), )}}\n\n    RETURN_TYPES = (\"GLIGEN\",)\n    FUNCTION = \"load_gligen\"\n\n    CATEGORY = \"loaders\"\n\n    def load_gligen(self, gligen_name):\n        gligen_path = folder_paths.get_full_path(\"gligen\", gligen_name)\n        gligen = comfy.sd.load_gligen(gligen_path)\n        return (gligen,)\n\nclass GLIGENTextBoxApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ),\n                              \"clip\": (\"CLIP\", ),\n                              \"gligen_textbox_model\": (\"GLIGEN\", ),\n                              \"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"conditioning/gligen\"\n\n    def append(self, conditioning_to, clip, gligen_textbox_model, text, width, height, x, y):\n        c = []\n        cond, cond_pooled = clip.encode_from_tokens(clip.tokenize(text), return_pooled=\"unprojected\")\n        for t in conditioning_to:\n            n = [t[0], t[1].copy()]\n            position_params = [(cond_pooled, height // 8, width // 8, y // 8, x // 8)]\n            prev = []\n            if \"gligen\" in n[1]:\n                prev = n[1]['gligen'][2]\n\n            n[1]['gligen'] = (\"position\", gligen_textbox_model, prev + position_params)\n            c.append(n)\n        return (c, )\n\nclass EmptyLatentImage:\n    def __init__(self):\n        self.device = comfy.model_management.intermediate_device()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent\"\n\n    def generate(self, width, height, batch_size=1):\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8], device=self.device)\n        return ({\"samples\":latent}, )\n\n\nclass LatentFromBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"batch_index\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 63}),\n                              \"length\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"frombatch\"\n\n    CATEGORY = \"latent/batch\"\n\n    def frombatch(self, samples, batch_index, length):\n        s = samples.copy()\n        s_in = samples[\"samples\"]\n        batch_index = min(s_in.shape[0] - 1, batch_index)\n        length = min(s_in.shape[0] - batch_index, length)\n        s[\"samples\"] = s_in[batch_index:batch_index + length].clone()\n        if \"noise_mask\" in samples:\n            masks = samples[\"noise_mask\"]\n            if masks.shape[0] == 1:\n                s[\"noise_mask\"] = masks.clone()\n            else:\n                if masks.shape[0] < s_in.shape[0]:\n                    masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n                s[\"noise_mask\"] = masks[batch_index:batch_index + length].clone()\n        if \"batch_index\" not in s:\n            s[\"batch_index\"] = [x for x in range(batch_index, batch_index+length)]\n        else:\n            s[\"batch_index\"] = samples[\"batch_index\"][batch_index:batch_index + length]\n        return (s,)\n    \nclass RepeatLatentBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"repeat\"\n\n    CATEGORY = \"latent/batch\"\n\n    def repeat(self, samples, amount):\n        s = samples.copy()\n        s_in = samples[\"samples\"]\n        \n        s[\"samples\"] = s_in.repeat((amount, 1,1,1))\n        if \"noise_mask\" in samples and samples[\"noise_mask\"].shape[0] > 1:\n            masks = samples[\"noise_mask\"]\n            if masks.shape[0] < s_in.shape[0]:\n                masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n            s[\"noise_mask\"] = samples[\"noise_mask\"].repeat((amount, 1,1,1))\n        if \"batch_index\" in s:\n            offset = max(s[\"batch_index\"]) - min(s[\"batch_index\"]) + 1\n            s[\"batch_index\"] = s[\"batch_index\"] + [x + (i * offset) for i in range(1, amount) for x in s[\"batch_index\"]]\n        return (s,)\n\nclass LatentUpscale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"latent\"\n\n    def upscale(self, samples, upscale_method, width, height, crop):\n        if width == 0 and height == 0:\n            s = samples\n        else:\n            s = samples.copy()\n\n            if width == 0:\n                height = max(64, height)\n                width = max(64, round(samples[\"samples\"].shape[3] * height / samples[\"samples\"].shape[2]))\n            elif height == 0:\n                width = max(64, width)\n                height = max(64, round(samples[\"samples\"].shape[2] * width / samples[\"samples\"].shape[3]))\n            else:\n                width = max(64, width)\n                height = max(64, height)\n\n            s[\"samples\"] = comfy.utils.common_upscale(samples[\"samples\"], width // 8, height // 8, upscale_method, crop)\n        return (s,)\n\nclass LatentUpscaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.5, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"latent\"\n\n    def upscale(self, samples, upscale_method, scale_by):\n        s = samples.copy()\n        width = round(samples[\"samples\"].shape[3] * scale_by)\n        height = round(samples[\"samples\"].shape[2] * scale_by)\n        s[\"samples\"] = comfy.utils.common_upscale(samples[\"samples\"], width, height, upscale_method, \"disabled\")\n        return (s,)\n\nclass LatentRotate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"rotation\": ([\"none\", \"90 degrees\", \"180 degrees\", \"270 degrees\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"rotate\"\n\n    CATEGORY = \"latent/transform\"\n\n    def rotate(self, samples, rotation):\n        s = samples.copy()\n        rotate_by = 0\n        if rotation.startswith(\"90\"):\n            rotate_by = 1\n        elif rotation.startswith(\"180\"):\n            rotate_by = 2\n        elif rotation.startswith(\"270\"):\n            rotate_by = 3\n\n        s[\"samples\"] = torch.rot90(samples[\"samples\"], k=rotate_by, dims=[3, 2])\n        return (s,)\n\nclass LatentFlip:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"flip_method\": ([\"x-axis: vertically\", \"y-axis: horizontally\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"flip\"\n\n    CATEGORY = \"latent/transform\"\n\n    def flip(self, samples, flip_method):\n        s = samples.copy()\n        if flip_method.startswith(\"x\"):\n            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[2])\n        elif flip_method.startswith(\"y\"):\n            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[3])\n\n        return (s,)\n\nclass LatentComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples_to\": (\"LATENT\",),\n                              \"samples_from\": (\"LATENT\",),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"feather\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"latent\"\n\n    def composite(self, samples_to, samples_from, x, y, composite_method=\"normal\", feather=0):\n        x =  x // 8\n        y = y // 8\n        feather = feather // 8\n        samples_out = samples_to.copy()\n        s = samples_to[\"samples\"].clone()\n        samples_to = samples_to[\"samples\"]\n        samples_from = samples_from[\"samples\"]\n        if feather == 0:\n            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n        else:\n            samples_from = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n            mask = torch.ones_like(samples_from)\n            for t in range(feather):\n                if y != 0:\n                    mask[:,:,t:1+t,:] *= ((1.0/feather) * (t + 1))\n\n                if y + samples_from.shape[2] < samples_to.shape[2]:\n                    mask[:,:,mask.shape[2] -1 -t: mask.shape[2]-t,:] *= ((1.0/feather) * (t + 1))\n                if x != 0:\n                    mask[:,:,:,t:1+t] *= ((1.0/feather) * (t + 1))\n                if x + samples_from.shape[3] < samples_to.shape[3]:\n                    mask[:,:,:,mask.shape[3]- 1 - t: mask.shape[3]- t] *= ((1.0/feather) * (t + 1))\n            rev_mask = torch.ones_like(mask) - mask\n            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x] * mask + s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] * rev_mask\n        samples_out[\"samples\"] = s\n        return (samples_out,)\n\nclass LatentBlend:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"samples1\": (\"LATENT\",),\n            \"samples2\": (\"LATENT\",),\n            \"blend_factor\": (\"FLOAT\", {\n                \"default\": 0.5,\n                \"min\": 0,\n                \"max\": 1,\n                \"step\": 0.01\n            }),\n        }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"blend\"\n\n    CATEGORY = \"_for_testing\"\n\n    def blend(self, samples1, samples2, blend_factor:float, blend_mode: str=\"normal\"):\n\n        samples_out = samples1.copy()\n        samples1 = samples1[\"samples\"]\n        samples2 = samples2[\"samples\"]\n\n        if samples1.shape != samples2.shape:\n            samples2.permute(0, 3, 1, 2)\n            samples2 = comfy.utils.common_upscale(samples2, samples1.shape[3], samples1.shape[2], 'bicubic', crop='center')\n            samples2.permute(0, 2, 3, 1)\n\n        samples_blended = self.blend_mode(samples1, samples2, blend_mode)\n        samples_blended = samples1 * blend_factor + samples_blended * (1 - blend_factor)\n        samples_out[\"samples\"] = samples_blended\n        return (samples_out,)\n\n    def blend_mode(self, img1, img2, mode):\n        if mode == \"normal\":\n            return img2\n        else:\n            raise ValueError(f\"Unsupported blend mode: {mode}\")\n\nclass LatentCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"crop\"\n\n    CATEGORY = \"latent/transform\"\n\n    def crop(self, samples, width, height, x, y):\n        s = samples.copy()\n        samples = samples['samples']\n        x =  x // 8\n        y = y // 8\n\n        #enfonce minimum size of 64\n        if x > (samples.shape[3] - 8):\n            x = samples.shape[3] - 8\n        if y > (samples.shape[2] - 8):\n            y = samples.shape[2] - 8\n\n        new_height = height // 8\n        new_width = width // 8\n        to_x = new_width + x\n        to_y = new_height + y\n        s['samples'] = samples[:,:,y:to_y, x:to_x]\n        return (s,)\n\nclass SetLatentNoiseMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"mask\": (\"MASK\",),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"set_mask\"\n\n    CATEGORY = \"latent/inpaint\"\n\n    def set_mask(self, samples, mask):\n        s = samples.copy()\n        s[\"noise_mask\"] = mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1]))\n        return (s,)\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = latent_preview.prepare_callback(model, steps)\n    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, ),\n                    \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, ),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                    \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\nclass KSamplerAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": ([\"enable\", \"disable\"], ),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, ),\n                    \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, ),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                    \"start_at_step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n                    \"end_at_step\": (\"INT\", {\"default\": 10000, \"min\": 0, \"max\": 10000}),\n                    \"return_with_leftover_noise\": ([\"disable\", \"enable\"], ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n\n    def sample(self, model, add_noise, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, start_at_step, end_at_step, return_with_leftover_noise, denoise=1.0):\n        force_full_denoise = True\n        if return_with_leftover_noise == \"enable\":\n            force_full_denoise = False\n        disable_noise = False\n        if add_noise == \"disable\":\n            disable_noise = True\n        return common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise, disable_noise=disable_noise, start_step=start_at_step, last_step=end_at_step, force_full_denoise=force_full_denoise)\n\nclass SaveImage:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": \n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image\"\n\n    def save_images(self, images, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        for (batch_number, image) in enumerate(images):\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            metadata = None\n            if not args.disable_metadata:\n                metadata = PngInfo()\n                if prompt is not None:\n                    metadata.add_text(\"prompt\", json.dumps(prompt))\n                if extra_pnginfo is not None:\n                    for x in extra_pnginfo:\n                        metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.png\"\n            img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=self.compress_level)\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n\n        return { \"ui\": { \"images\": results } }\n\nclass PreviewImage(SaveImage):\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + ''.join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n        self.compress_level = 1\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ), },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\nclass LoadImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True})},\n                }\n\n    CATEGORY = \"image\"\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"load_image\"\n    def load_image(self, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        \n        img = node_helpers.pillow(Image.open, image_path)\n        \n        output_images = []\n        output_masks = []\n        w, h = None, None\n\n        excluded_formats = ['MPO']\n        \n        for i in ImageSequence.Iterator(img):\n            i = node_helpers.pillow(ImageOps.exif_transpose, i)\n\n            if i.mode == 'I':\n                i = i.point(lambda i: i * (1 / 255))\n            image = i.convert(\"RGB\")\n\n            if len(output_images) == 0:\n                w = image.size[0]\n                h = image.size[1]\n            \n            if image.size[0] != w or image.size[1] != h:\n                continue\n            \n            image = np.array(image).astype(np.float32) / 255.0\n            image = torch.from_numpy(image)[None,]\n            if 'A' in i.getbands():\n                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n                mask = 1. - torch.from_numpy(mask)\n            else:\n                mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n            output_images.append(image)\n            output_masks.append(mask.unsqueeze(0))\n\n        if len(output_images) > 1 and img.format not in excluded_formats:\n            output_image = torch.cat(output_images, dim=0)\n            output_mask = torch.cat(output_masks, dim=0)\n        else:\n            output_image = output_images[0]\n            output_mask = output_masks[0]\n\n        return (output_image, output_mask)\n\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, image):\n        if not folder_paths.exists_annotated_filepath(image):\n            return \"Invalid image file: {}\".format(image)\n\n        return True\n\nclass LoadImageMask:\n    _color_channels = [\"alpha\", \"red\", \"green\", \"blue\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True}),\n                     \"channel\": (s._color_channels, ), }\n                }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"load_image\"\n    def load_image(self, image, channel):\n        image_path = folder_paths.get_annotated_filepath(image)\n        i = node_helpers.pillow(Image.open, image_path)\n        i = node_helpers.pillow(ImageOps.exif_transpose, i)\n        if i.getbands() != (\"R\", \"G\", \"B\", \"A\"):\n            if i.mode == 'I':\n                i = i.point(lambda i: i * (1 / 255))\n            i = i.convert(\"RGBA\")\n        mask = None\n        c = channel[0].upper()\n        if c in i.getbands():\n            mask = np.array(i.getchannel(c)).astype(np.float32) / 255.0\n            mask = torch.from_numpy(mask)\n            if c == 'A':\n                mask = 1. - mask\n        else:\n            mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n        return (mask.unsqueeze(0),)\n\n    @classmethod\n    def IS_CHANGED(s, image, channel):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, image):\n        if not folder_paths.exists_annotated_filepath(image):\n            return \"Invalid image file: {}\".format(image)\n\n        return True\n\nclass ImageScale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, width, height, crop):\n        if width == 0 and height == 0:\n            s = image\n        else:\n            samples = image.movedim(-1,1)\n\n            if width == 0:\n                width = max(1, round(samples.shape[3] * height / samples.shape[2]))\n            elif height == 0:\n                height = max(1, round(samples.shape[2] * width / samples.shape[3]))\n\n            s = comfy.utils.common_upscale(samples, width, height, upscale_method, crop)\n            s = s.movedim(1,-1)\n        return (s,)\n\nclass ImageScaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, scale_by):\n        samples = image.movedim(-1,1)\n        width = round(samples.shape[3] * scale_by)\n        height = round(samples.shape[2] * scale_by)\n        s = comfy.utils.common_upscale(samples, width, height, upscale_method, \"disabled\")\n        s = s.movedim(1,-1)\n        return (s,)\n\nclass ImageInvert:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"invert\"\n\n    CATEGORY = \"image\"\n\n    def invert(self, image):\n        s = 1.0 - image\n        return (s,)\n\nclass ImageBatch:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image1\": (\"IMAGE\",), \"image2\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"batch\"\n\n    CATEGORY = \"image\"\n\n    def batch(self, image1, image2):\n        if image1.shape[1:] != image2.shape[1:]:\n            image2 = comfy.utils.common_upscale(image2.movedim(-1,1), image1.shape[2], image1.shape[1], \"bilinear\", \"center\").movedim(1,-1)\n        s = torch.cat((image1, image2), dim=0)\n        return (s,)\n\nclass EmptyImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"image\"\n\n    def generate(self, width, height, batch_size=1, color=0):\n        r = torch.full([batch_size, height, width, 1], ((color >> 16) & 0xFF) / 0xFF)\n        g = torch.full([batch_size, height, width, 1], ((color >> 8) & 0xFF) / 0xFF)\n        b = torch.full([batch_size, height, width, 1], ((color) & 0xFF) / 0xFF)\n        return (torch.cat((r, g, b), dim=-1), )\n\nclass ImagePadForOutpaint:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"feathering\": (\"INT\", {\"default\": 40, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"expand_image\"\n\n    CATEGORY = \"image\"\n\n    def expand_image(self, image, left, top, right, bottom, feathering):\n        d1, d2, d3, d4 = image.size()\n\n        new_image = torch.ones(\n            (d1, d2 + top + bottom, d3 + left + right, d4),\n            dtype=torch.float32,\n        ) * 0.5\n\n        new_image[:, top:top + d2, left:left + d3, :] = image\n\n        mask = torch.ones(\n            (d2 + top + bottom, d3 + left + right),\n            dtype=torch.float32,\n        )\n\n        t = torch.zeros(\n            (d2, d3),\n            dtype=torch.float32\n        )\n\n        if feathering > 0 and feathering * 2 < d2 and feathering * 2 < d3:\n\n            for i in range(d2):\n                for j in range(d3):\n                    dt = i if top != 0 else d2\n                    db = d2 - i if bottom != 0 else d2\n\n                    dl = j if left != 0 else d3\n                    dr = d3 - j if right != 0 else d3\n\n                    d = min(dt, db, dl, dr)\n\n                    if d >= feathering:\n                        continue\n\n                    v = (feathering - d) / feathering\n\n                    t[i, j] = v * v\n\n        mask[top:top + d2, left:left + d3] = t\n\n        return (new_image, mask)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"KSampler\": KSampler,\n    \"CheckpointLoaderSimple\": CheckpointLoaderSimple,\n    \"CLIPTextEncode\": CLIPTextEncode,\n    \"CLIPSetLastLayer\": CLIPSetLastLayer,\n    \"VAEDecode\": VAEDecode,\n    \"VAEEncode\": VAEEncode,\n    \"VAEEncodeForInpaint\": VAEEncodeForInpaint,\n    \"VAELoader\": VAELoader,\n    \"EmptyLatentImage\": EmptyLatentImage,\n    \"LatentUpscale\": LatentUpscale,\n    \"LatentUpscaleBy\": LatentUpscaleBy,\n    \"LatentFromBatch\": LatentFromBatch,\n    \"RepeatLatentBatch\": RepeatLatentBatch,\n    \"SaveImage\": SaveImage,\n    \"PreviewImage\": PreviewImage,\n    \"LoadImage\": LoadImage,\n    \"LoadImageMask\": LoadImageMask,\n    \"ImageScale\": ImageScale,\n    \"ImageScaleBy\": ImageScaleBy,\n    \"ImageInvert\": ImageInvert,\n    \"ImageBatch\": ImageBatch,\n    \"ImagePadForOutpaint\": ImagePadForOutpaint,\n    \"EmptyImage\": EmptyImage,\n    \"ConditioningAverage\": ConditioningAverage ,\n    \"ConditioningCombine\": ConditioningCombine,\n    \"ConditioningConcat\": ConditioningConcat,\n    \"ConditioningSetArea\": ConditioningSetArea,\n    \"ConditioningSetAreaPercentage\": ConditioningSetAreaPercentage,\n    \"ConditioningSetAreaStrength\": ConditioningSetAreaStrength,\n    \"ConditioningSetMask\": ConditioningSetMask,\n    \"KSamplerAdvanced\": KSamplerAdvanced,\n    \"SetLatentNoiseMask\": SetLatentNoiseMask,\n    \"LatentComposite\": LatentComposite,\n    \"LatentBlend\": LatentBlend,\n    \"LatentRotate\": LatentRotate,\n    \"LatentFlip\": LatentFlip,\n    \"LatentCrop\": LatentCrop,\n    \"LoraLoader\": LoraLoader,\n    \"CLIPLoader\": CLIPLoader,\n    \"UNETLoader\": UNETLoader,\n    \"DualCLIPLoader\": DualCLIPLoader,\n    \"CLIPVisionEncode\": CLIPVisionEncode,\n    \"StyleModelApply\": StyleModelApply,\n    \"unCLIPConditioning\": unCLIPConditioning,\n    \"ControlNetApply\": ControlNetApply,\n    \"ControlNetApplyAdvanced\": ControlNetApplyAdvanced,\n    \"ControlNetLoader\": ControlNetLoader,\n    \"DiffControlNetLoader\": DiffControlNetLoader,\n    \"StyleModelLoader\": StyleModelLoader,\n    \"CLIPVisionLoader\": CLIPVisionLoader,\n    \"VAEDecodeTiled\": VAEDecodeTiled,\n    \"VAEEncodeTiled\": VAEEncodeTiled,\n    \"unCLIPCheckpointLoader\": unCLIPCheckpointLoader,\n    \"GLIGENLoader\": GLIGENLoader,\n    \"GLIGENTextBoxApply\": GLIGENTextBoxApply,\n    \"InpaintModelConditioning\": InpaintModelConditioning,\n\n    \"CheckpointLoader\": CheckpointLoader,\n    \"DiffusersLoader\": DiffusersLoader,\n\n    \"LoadLatent\": LoadLatent,\n    \"SaveLatent\": SaveLatent,\n\n    \"ConditioningZeroOut\": ConditioningZeroOut,\n    \"ConditioningSetTimestepRange\": ConditioningSetTimestepRange,\n    \"LoraLoaderModelOnly\": LoraLoaderModelOnly,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"KSampler\": \"KSampler\",\n    \"KSamplerAdvanced\": \"KSampler (Advanced)\",\n    # Loaders\n    \"CheckpointLoader\": \"Load Checkpoint With Config (DEPRECATED)\",\n    \"CheckpointLoaderSimple\": \"Load Checkpoint\",\n    \"VAELoader\": \"Load VAE\",\n    \"LoraLoader\": \"Load LoRA\",\n    \"CLIPLoader\": \"Load CLIP\",\n    \"ControlNetLoader\": \"Load ControlNet Model\",\n    \"DiffControlNetLoader\": \"Load ControlNet Model (diff)\",\n    \"StyleModelLoader\": \"Load Style Model\",\n    \"CLIPVisionLoader\": \"Load CLIP Vision\",\n    \"UpscaleModelLoader\": \"Load Upscale Model\",\n    # Conditioning\n    \"CLIPVisionEncode\": \"CLIP Vision Encode\",\n    \"StyleModelApply\": \"Apply Style Model\",\n    \"CLIPTextEncode\": \"CLIP Text Encode (Prompt)\",\n    \"CLIPSetLastLayer\": \"CLIP Set Last Layer\",\n    \"ConditioningCombine\": \"Conditioning (Combine)\",\n    \"ConditioningAverage \": \"Conditioning (Average)\",\n    \"ConditioningConcat\": \"Conditioning (Concat)\",\n    \"ConditioningSetArea\": \"Conditioning (Set Area)\",\n    \"ConditioningSetAreaPercentage\": \"Conditioning (Set Area with Percentage)\",\n    \"ConditioningSetMask\": \"Conditioning (Set Mask)\",\n    \"ControlNetApply\": \"Apply ControlNet\",\n    \"ControlNetApplyAdvanced\": \"Apply ControlNet (Advanced)\",\n    # Latent\n    \"VAEEncodeForInpaint\": \"VAE Encode (for Inpainting)\",\n    \"SetLatentNoiseMask\": \"Set Latent Noise Mask\",\n    \"VAEDecode\": \"VAE Decode\",\n    \"VAEEncode\": \"VAE Encode\",\n    \"LatentRotate\": \"Rotate Latent\",\n    \"LatentFlip\": \"Flip Latent\",\n    \"LatentCrop\": \"Crop Latent\",\n    \"EmptyLatentImage\": \"Empty Latent Image\",\n    \"LatentUpscale\": \"Upscale Latent\",\n    \"LatentUpscaleBy\": \"Upscale Latent By\",\n    \"LatentComposite\": \"Latent Composite\",\n    \"LatentBlend\": \"Latent Blend\",\n    \"LatentFromBatch\" : \"Latent From Batch\",\n    \"RepeatLatentBatch\": \"Repeat Latent Batch\",\n    # Image\n    \"SaveImage\": \"Save Image\",\n    \"PreviewImage\": \"Preview Image\",\n    \"LoadImage\": \"Load Image\",\n    \"LoadImageMask\": \"Load Image (as Mask)\",\n    \"ImageScale\": \"Upscale Image\",\n    \"ImageScaleBy\": \"Upscale Image By\",\n    \"ImageUpscaleWithModel\": \"Upscale Image (using Model)\",\n    \"ImageInvert\": \"Invert Image\",\n    \"ImagePadForOutpaint\": \"Pad Image for Outpainting\",\n    \"ImageBatch\": \"Batch Images\",\n    # _for_testing\n    \"VAEDecodeTiled\": \"VAE Decode (Tiled)\",\n    \"VAEEncodeTiled\": \"VAE Encode (Tiled)\",\n}\n\nEXTENSION_WEB_DIRS = {}\n\ndef load_custom_node(module_path, ignore=set()):\n    module_name = os.path.basename(module_path)\n    if os.path.isfile(module_path):\n        sp = os.path.splitext(module_path)\n        module_name = sp[0]\n    try:\n        logging.debug(\"Trying to load custom node {}\".format(module_path))\n        if os.path.isfile(module_path):\n            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n            module_dir = os.path.split(module_path)[0]\n        else:\n            module_spec = importlib.util.spec_from_file_location(module_name, os.path.join(module_path, \"__init__.py\"))\n            module_dir = module_path\n\n        module = importlib.util.module_from_spec(module_spec)\n        sys.modules[module_name] = module\n        module_spec.loader.exec_module(module)\n\n        if hasattr(module, \"WEB_DIRECTORY\") and getattr(module, \"WEB_DIRECTORY\") is not None:\n            web_dir = os.path.abspath(os.path.join(module_dir, getattr(module, \"WEB_DIRECTORY\")))\n            if os.path.isdir(web_dir):\n                EXTENSION_WEB_DIRS[module_name] = web_dir\n\n        if hasattr(module, \"NODE_CLASS_MAPPINGS\") and getattr(module, \"NODE_CLASS_MAPPINGS\") is not None:\n            for name in module.NODE_CLASS_MAPPINGS:\n                if name not in ignore:\n                    NODE_CLASS_MAPPINGS[name] = module.NODE_CLASS_MAPPINGS[name]\n            if hasattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") and getattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") is not None:\n                NODE_DISPLAY_NAME_MAPPINGS.update(module.NODE_DISPLAY_NAME_MAPPINGS)\n            return True\n        else:\n            logging.warning(f\"Skip {module_path} module for custom nodes due to the lack of NODE_CLASS_MAPPINGS.\")\n            return False\n    except Exception as e:\n        logging.warning(traceback.format_exc())\n        logging.warning(f\"Cannot import {module_path} module for custom nodes: {e}\")\n        return False\n\ndef load_custom_nodes():\n    base_node_names = set(NODE_CLASS_MAPPINGS.keys())\n    node_paths = folder_paths.get_folder_paths(\"custom_nodes\")\n    node_import_times = []\n    for custom_node_path in node_paths:\n        possible_modules = os.listdir(os.path.realpath(custom_node_path))\n        if \"__pycache__\" in possible_modules:\n            possible_modules.remove(\"__pycache__\")\n\n        for possible_module in possible_modules:\n            module_path = os.path.join(custom_node_path, possible_module)\n            if os.path.isfile(module_path) and os.path.splitext(module_path)[1] != \".py\": continue\n            if module_path.endswith(\".disabled\"): continue\n            time_before = time.perf_counter()\n            success = load_custom_node(module_path, base_node_names)\n            node_import_times.append((time.perf_counter() - time_before, module_path, success))\n\n    if len(node_import_times) > 0:\n        logging.info(\"\\nImport times for custom nodes:\")\n        for n in sorted(node_import_times):\n            if n[2]:\n                import_message = \"\"\n            else:\n                import_message = \" (IMPORT FAILED)\"\n            logging.info(\"{:6.1f} seconds{}: {}\".format(n[0], import_message, n[1]))\n        logging.info(\"\")\n\ndef init_custom_nodes():\n    extras_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"comfy_extras\")\n    extras_files = [\n        \"nodes_latent.py\",\n        \"nodes_hypernetwork.py\",\n        \"nodes_upscale_model.py\",\n        \"nodes_post_processing.py\",\n        \"nodes_mask.py\",\n        \"nodes_compositing.py\",\n        \"nodes_rebatch.py\",\n        \"nodes_model_merging.py\",\n        \"nodes_tomesd.py\",\n        \"nodes_clip_sdxl.py\",\n        \"nodes_canny.py\",\n        \"nodes_freelunch.py\",\n        \"nodes_custom_sampler.py\",\n        \"nodes_hypertile.py\",\n        \"nodes_model_advanced.py\",\n        \"nodes_model_downscale.py\",\n        \"nodes_images.py\",\n        \"nodes_video_model.py\",\n        \"nodes_sag.py\",\n        \"nodes_perpneg.py\",\n        \"nodes_stable3d.py\",\n        \"nodes_sdupscale.py\",\n        \"nodes_photomaker.py\",\n        \"nodes_cond.py\",\n        \"nodes_morphology.py\",\n        \"nodes_stable_cascade.py\",\n        \"nodes_differential_diffusion.py\",\n        \"nodes_ip2p.py\",\n        \"nodes_model_merging_model_specific.py\",\n        \"nodes_pag.py\",\n        \"nodes_align_your_steps.py\",\n        \"nodes_attention_multiply.py\",\n        \"nodes_advanced_samplers.py\",\n        \"nodes_webcam.py\",\n        \"nodes_audio.py\",\n        \"nodes_sd3.py\",\n        \"nodes_gits.py\",\n    ]\n\n    import_failed = []\n    for node_file in extras_files:\n        if not load_custom_node(os.path.join(extras_dir, node_file)):\n            import_failed.append(node_file)\n\n    load_custom_nodes()\n\n    if len(import_failed) > 0:\n        logging.warning(\"WARNING: some comfy_extras/ nodes did not import correctly. This may be because they are missing some dependencies.\\n\")\n        for node in import_failed:\n            logging.warning(\"IMPORT FAILED: {}\".format(node))\n        logging.warning(\"\\nThis issue might be caused by new missing dependencies added the last time you updated ComfyUI.\")\n        if args.windows_standalone_build:\n            logging.warning(\"Please run the update script: update/update_comfyui.bat\")\n        else:\n            logging.warning(\"Please do a: pip install -r requirements.txt\")\n        logging.warning(\"\")\n", "latent_preview.py": "import torch\nfrom PIL import Image\nimport struct\nimport numpy as np\nfrom comfy.cli_args import args, LatentPreviewMethod\nfrom comfy.taesd.taesd import TAESD\nimport comfy.model_management\nimport folder_paths\nimport comfy.utils\nimport logging\n\nMAX_PREVIEW_RESOLUTION = 512\n\ndef preview_to_image(latent_image):\n        latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                            .mul(0xFF)  # to 0..255\n                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n\n        return Image.fromarray(latents_ubyte.numpy())\n\nclass LatentPreviewer:\n    def decode_latent_to_preview(self, x0):\n        pass\n\n    def decode_latent_to_preview_image(self, preview_format, x0):\n        preview_image = self.decode_latent_to_preview(x0)\n        return (\"JPEG\", preview_image, MAX_PREVIEW_RESOLUTION)\n\nclass TAESDPreviewerImpl(LatentPreviewer):\n    def __init__(self, taesd):\n        self.taesd = taesd\n\n    def decode_latent_to_preview(self, x0):\n        x_sample = self.taesd.decode(x0[:1])[0].movedim(0, 2)\n        return preview_to_image(x_sample)\n\n\nclass Latent2RGBPreviewer(LatentPreviewer):\n    def __init__(self, latent_rgb_factors):\n        self.latent_rgb_factors = torch.tensor(latent_rgb_factors, device=\"cpu\")\n\n    def decode_latent_to_preview(self, x0):\n        self.latent_rgb_factors = self.latent_rgb_factors.to(dtype=x0.dtype, device=x0.device)\n        latent_image = x0[0].permute(1, 2, 0) @ self.latent_rgb_factors\n        return preview_to_image(latent_image)\n\n\ndef get_previewer(device, latent_format):\n    previewer = None\n    method = args.preview_method\n    if method != LatentPreviewMethod.NoPreviews:\n        # TODO previewer methods\n        taesd_decoder_path = None\n        if latent_format.taesd_decoder_name is not None:\n            taesd_decoder_path = next(\n                (fn for fn in folder_paths.get_filename_list(\"vae_approx\")\n                    if fn.startswith(latent_format.taesd_decoder_name)),\n                \"\"\n            )\n            taesd_decoder_path = folder_paths.get_full_path(\"vae_approx\", taesd_decoder_path)\n\n        if method == LatentPreviewMethod.Auto:\n            method = LatentPreviewMethod.Latent2RGB\n\n        if method == LatentPreviewMethod.TAESD:\n            if taesd_decoder_path:\n                taesd = TAESD(None, taesd_decoder_path, latent_channels=latent_format.latent_channels).to(device)\n                previewer = TAESDPreviewerImpl(taesd)\n            else:\n                logging.warning(\"Warning: TAESD previews enabled, but could not find models/vae_approx/{}\".format(latent_format.taesd_decoder_name))\n\n        if previewer is None:\n            if latent_format.latent_rgb_factors is not None:\n                previewer = Latent2RGBPreviewer(latent_format.latent_rgb_factors)\n    return previewer\n\ndef prepare_callback(model, steps, x0_output_dict=None):\n    preview_format = \"JPEG\"\n    if preview_format not in [\"JPEG\", \"PNG\"]:\n        preview_format = \"JPEG\"\n\n    previewer = get_previewer(model.load_device, model.model.latent_format)\n\n    pbar = comfy.utils.ProgressBar(steps)\n    def callback(step, x0, x, total_steps):\n        if x0_output_dict is not None:\n            x0_output_dict[\"x0\"] = x0\n\n        preview_bytes = None\n        if previewer:\n            preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\n        pbar.update_absolute(step + 1, total_steps, preview_bytes)\n    return callback\n\n", "node_helpers.py": "from PIL import ImageFile, UnidentifiedImageError\n\ndef conditioning_set_values(conditioning, values={}):\n    c = []\n    for t in conditioning:\n        n = [t[0], t[1].copy()]\n        for k in values:\n            n[1][k] = values[k]\n        c.append(n)\n\n    return c\n\ndef pillow(fn, arg):\n    prev_value = None\n    try:\n        x = fn(arg)\n    except (OSError, UnidentifiedImageError, ValueError): #PIL issues #4472 and #2445, also fixes ComfyUI issue #3416\n        prev_value = ImageFile.LOAD_TRUNCATED_IMAGES\n        ImageFile.LOAD_TRUNCATED_IMAGES = True\n        x = fn(arg)\n    finally:\n        if prev_value is not None:\n            ImageFile.LOAD_TRUNCATED_IMAGES = prev_value\n        return x\n", "main.py": "import comfy.options\ncomfy.options.enable_args_parsing()\n\nimport os\nimport importlib.util\nimport folder_paths\nimport time\n\ndef execute_prestartup_script():\n    def execute_script(script_path):\n        module_name = os.path.splitext(script_path)[0]\n        try:\n            spec = importlib.util.spec_from_file_location(module_name, script_path)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            return True\n        except Exception as e:\n            print(f\"Failed to execute startup-script: {script_path} / {e}\")\n        return False\n\n    node_paths = folder_paths.get_folder_paths(\"custom_nodes\")\n    for custom_node_path in node_paths:\n        possible_modules = os.listdir(custom_node_path)\n        node_prestartup_times = []\n\n        for possible_module in possible_modules:\n            module_path = os.path.join(custom_node_path, possible_module)\n            if os.path.isfile(module_path) or module_path.endswith(\".disabled\") or module_path == \"__pycache__\":\n                continue\n\n            script_path = os.path.join(module_path, \"prestartup_script.py\")\n            if os.path.exists(script_path):\n                time_before = time.perf_counter()\n                success = execute_script(script_path)\n                node_prestartup_times.append((time.perf_counter() - time_before, module_path, success))\n    if len(node_prestartup_times) > 0:\n        print(\"\\nPrestartup times for custom nodes:\")\n        for n in sorted(node_prestartup_times):\n            if n[2]:\n                import_message = \"\"\n            else:\n                import_message = \" (PRESTARTUP FAILED)\"\n            print(\"{:6.1f} seconds{}:\".format(n[0], import_message), n[1])\n        print()\n\nexecute_prestartup_script()\n\n\n# Main code\nimport asyncio\nimport itertools\nimport shutil\nimport threading\nimport gc\n\nfrom comfy.cli_args import args\nimport logging\n\nif os.name == \"nt\":\n    logging.getLogger(\"xformers\").addFilter(lambda record: 'A matching Triton is not available' not in record.getMessage())\n\nif __name__ == \"__main__\":\n    if args.cuda_device is not None:\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n        logging.info(\"Set cuda device to: {}\".format(args.cuda_device))\n\n    if args.deterministic:\n        if 'CUBLAS_WORKSPACE_CONFIG' not in os.environ:\n            os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n\n    import cuda_malloc\n\nimport comfy.utils\nimport yaml\n\nimport execution\nimport server\nfrom server import BinaryEventTypes\nfrom nodes import init_custom_nodes\nimport comfy.model_management\n\ndef cuda_malloc_warning():\n    device = comfy.model_management.get_torch_device()\n    device_name = comfy.model_management.get_torch_device_name(device)\n    cuda_malloc_warning = False\n    if \"cudaMallocAsync\" in device_name:\n        for b in cuda_malloc.blacklist:\n            if b in device_name:\n                cuda_malloc_warning = True\n        if cuda_malloc_warning:\n            logging.warning(\"\\nWARNING: this card most likely does not support cuda-malloc, if you get \\\"CUDA error\\\" please run ComfyUI with: --disable-cuda-malloc\\n\")\n\ndef prompt_worker(q, server):\n    e = execution.PromptExecutor(server)\n    last_gc_collect = 0\n    need_gc = False\n    gc_collect_interval = 10.0\n\n    while True:\n        timeout = 1000.0\n        if need_gc:\n            timeout = max(gc_collect_interval - (current_time - last_gc_collect), 0.0)\n\n        queue_item = q.get(timeout=timeout)\n        if queue_item is not None:\n            item, item_id = queue_item\n            execution_start_time = time.perf_counter()\n            prompt_id = item[1]\n            server.last_prompt_id = prompt_id\n\n            e.execute(item[2], prompt_id, item[3], item[4])\n            need_gc = True\n            q.task_done(item_id,\n                        e.outputs_ui,\n                        status=execution.PromptQueue.ExecutionStatus(\n                            status_str='success' if e.success else 'error',\n                            completed=e.success,\n                            messages=e.status_messages))\n            if server.client_id is not None:\n                server.send_sync(\"executing\", { \"node\": None, \"prompt_id\": prompt_id }, server.client_id)\n\n            current_time = time.perf_counter()\n            execution_time = current_time - execution_start_time\n            logging.info(\"Prompt executed in {:.2f} seconds\".format(execution_time))\n\n        flags = q.get_flags()\n        free_memory = flags.get(\"free_memory\", False)\n\n        if flags.get(\"unload_models\", free_memory):\n            comfy.model_management.unload_all_models()\n            need_gc = True\n            last_gc_collect = 0\n\n        if free_memory:\n            e.reset()\n            need_gc = True\n            last_gc_collect = 0\n\n        if need_gc:\n            current_time = time.perf_counter()\n            if (current_time - last_gc_collect) > gc_collect_interval:\n                comfy.model_management.cleanup_models()\n                gc.collect()\n                comfy.model_management.soft_empty_cache()\n                last_gc_collect = current_time\n                need_gc = False\n\nasync def run(server, address='', port=8188, verbose=True, call_on_start=None):\n    await asyncio.gather(server.start(address, port, verbose, call_on_start), server.publish_loop())\n\n\ndef hijack_progress(server):\n    def hook(value, total, preview_image):\n        comfy.model_management.throw_exception_if_processing_interrupted()\n        progress = {\"value\": value, \"max\": total, \"prompt_id\": server.last_prompt_id, \"node\": server.last_node_id}\n\n        server.send_sync(\"progress\", progress, server.client_id)\n        if preview_image is not None:\n            server.send_sync(BinaryEventTypes.UNENCODED_PREVIEW_IMAGE, preview_image, server.client_id)\n    comfy.utils.set_progress_bar_global_hook(hook)\n\n\ndef cleanup_temp():\n    temp_dir = folder_paths.get_temp_directory()\n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n\ndef load_extra_path_config(yaml_path):\n    with open(yaml_path, 'r') as stream:\n        config = yaml.safe_load(stream)\n    for c in config:\n        conf = config[c]\n        if conf is None:\n            continue\n        base_path = None\n        if \"base_path\" in conf:\n            base_path = conf.pop(\"base_path\")\n        for x in conf:\n            for y in conf[x].split(\"\\n\"):\n                if len(y) == 0:\n                    continue\n                full_path = y\n                if base_path is not None:\n                    full_path = os.path.join(base_path, full_path)\n                logging.info(\"Adding extra search path {} {}\".format(x, full_path))\n                folder_paths.add_model_folder_path(x, full_path)\n\n\nif __name__ == \"__main__\":\n    if args.temp_directory:\n        temp_dir = os.path.join(os.path.abspath(args.temp_directory), \"temp\")\n        logging.info(f\"Setting temp directory to: {temp_dir}\")\n        folder_paths.set_temp_directory(temp_dir)\n    cleanup_temp()\n\n    if args.windows_standalone_build:\n        try:\n            import new_updater\n            new_updater.update_windows_updater()\n        except:\n            pass\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    server = server.PromptServer(loop)\n    q = execution.PromptQueue(server)\n\n    extra_model_paths_config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"extra_model_paths.yaml\")\n    if os.path.isfile(extra_model_paths_config_path):\n        load_extra_path_config(extra_model_paths_config_path)\n\n    if args.extra_model_paths_config:\n        for config_path in itertools.chain(*args.extra_model_paths_config):\n            load_extra_path_config(config_path)\n\n    init_custom_nodes()\n\n    cuda_malloc_warning()\n\n    server.add_routes()\n    hijack_progress(server)\n\n    threading.Thread(target=prompt_worker, daemon=True, args=(q, server,)).start()\n\n    if args.output_directory:\n        output_dir = os.path.abspath(args.output_directory)\n        logging.info(f\"Setting output directory to: {output_dir}\")\n        folder_paths.set_output_directory(output_dir)\n\n    #These are the default folders that checkpoints, clip and vae models will be saved to when using CheckpointSave, etc.. nodes\n    folder_paths.add_model_folder_path(\"checkpoints\", os.path.join(folder_paths.get_output_directory(), \"checkpoints\"))\n    folder_paths.add_model_folder_path(\"clip\", os.path.join(folder_paths.get_output_directory(), \"clip\"))\n    folder_paths.add_model_folder_path(\"vae\", os.path.join(folder_paths.get_output_directory(), \"vae\"))\n\n    if args.input_directory:\n        input_dir = os.path.abspath(args.input_directory)\n        logging.info(f\"Setting input directory to: {input_dir}\")\n        folder_paths.set_input_directory(input_dir)\n\n    if args.quick_test_for_ci:\n        exit(0)\n\n    call_on_start = None\n    if args.auto_launch:\n        def startup_server(scheme, address, port):\n            import webbrowser\n            if os.name == 'nt' and address == '0.0.0.0':\n                address = '127.0.0.1'\n            webbrowser.open(f\"{scheme}://{address}:{port}\")\n        call_on_start = startup_server\n\n    try:\n        loop.run_until_complete(run(server, address=args.listen, port=args.port, verbose=not args.dont_print_server, call_on_start=call_on_start))\n    except KeyboardInterrupt:\n        logging.info(\"\\nStopped server\")\n\n    cleanup_temp()\n", "new_updater.py": "import os\nimport shutil\n\nbase_path = os.path.dirname(os.path.realpath(__file__))\n\n\ndef update_windows_updater():\n    top_path = os.path.dirname(base_path)\n    updater_path = os.path.join(base_path, \".ci/update_windows/update.py\")\n    bat_path = os.path.join(base_path, \".ci/update_windows/update_comfyui.bat\")\n\n    dest_updater_path = os.path.join(top_path, \"update/update.py\")\n    dest_bat_path = os.path.join(top_path, \"update/update_comfyui.bat\")\n    dest_bat_deps_path = os.path.join(top_path, \"update/update_comfyui_and_python_dependencies.bat\")\n\n    try:\n        with open(dest_bat_path, 'rb') as f:\n            contents = f.read()\n    except:\n        return\n\n    if not contents.startswith(b\"..\\\\python_embeded\\\\python.exe .\\\\update.py\"):\n        return\n\n    shutil.copy(updater_path, dest_updater_path)\n    try:\n        with open(dest_bat_deps_path, 'rb') as f:\n            contents = f.read()\n            contents = contents.replace(b'..\\\\python_embeded\\\\python.exe .\\\\update.py ..\\\\ComfyUI\\\\', b'call update_comfyui.bat nopause')\n        with open(dest_bat_deps_path, 'wb') as f:\n            f.write(contents)\n    except:\n        pass\n    shutil.copy(bat_path, dest_bat_path)\n    print(\"Updated the windows standalone package updater.\")\n", "server.py": "import os\nimport sys\nimport asyncio\nimport traceback\n\nimport nodes\nimport folder_paths\nimport execution\nimport uuid\nimport urllib\nimport json\nimport glob\nimport struct\nimport ssl\nfrom PIL import Image, ImageOps\nfrom PIL.PngImagePlugin import PngInfo\nfrom io import BytesIO\n\nimport aiohttp\nfrom aiohttp import web\nimport logging\n\nimport mimetypes\nfrom comfy.cli_args import args\nimport comfy.utils\nimport comfy.model_management\n\nfrom app.user_manager import UserManager\n\nclass BinaryEventTypes:\n    PREVIEW_IMAGE = 1\n    UNENCODED_PREVIEW_IMAGE = 2\n\nasync def send_socket_catch_exception(function, message):\n    try:\n        await function(message)\n    except (aiohttp.ClientError, aiohttp.ClientPayloadError, ConnectionResetError) as err:\n        logging.warning(\"send error: {}\".format(err))\n\n@web.middleware\nasync def cache_control(request: web.Request, handler):\n    response: web.Response = await handler(request)\n    if request.path.endswith('.js') or request.path.endswith('.css'):\n        response.headers.setdefault('Cache-Control', 'no-cache')\n    return response\n\ndef create_cors_middleware(allowed_origin: str):\n    @web.middleware\n    async def cors_middleware(request: web.Request, handler):\n        if request.method == \"OPTIONS\":\n            # Pre-flight request. Reply successfully:\n            response = web.Response()\n        else:\n            response = await handler(request)\n\n        response.headers['Access-Control-Allow-Origin'] = allowed_origin\n        response.headers['Access-Control-Allow-Methods'] = 'POST, GET, DELETE, PUT, OPTIONS'\n        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'\n        response.headers['Access-Control-Allow-Credentials'] = 'true'\n        return response\n\n    return cors_middleware\n\nclass PromptServer():\n    def __init__(self, loop):\n        PromptServer.instance = self\n\n        mimetypes.init()\n        mimetypes.types_map['.js'] = 'application/javascript; charset=utf-8'\n\n        self.user_manager = UserManager()\n        self.supports = [\"custom_nodes_from_web\"]\n        self.prompt_queue = None\n        self.loop = loop\n        self.messages = asyncio.Queue()\n        self.number = 0\n\n        middlewares = [cache_control]\n        if args.enable_cors_header:\n            middlewares.append(create_cors_middleware(args.enable_cors_header))\n\n        max_upload_size = round(args.max_upload_size * 1024 * 1024)\n        self.app = web.Application(client_max_size=max_upload_size, middlewares=middlewares)\n        self.sockets = dict()\n        self.web_root = os.path.join(os.path.dirname(\n            os.path.realpath(__file__)), \"web\")\n        routes = web.RouteTableDef()\n        self.routes = routes\n        self.last_node_id = None\n        self.client_id = None\n\n        self.on_prompt_handlers = []\n\n        @routes.get('/ws')\n        async def websocket_handler(request):\n            ws = web.WebSocketResponse()\n            await ws.prepare(request)\n            sid = request.rel_url.query.get('clientId', '')\n            if sid:\n                # Reusing existing session, remove old\n                self.sockets.pop(sid, None)\n            else:\n                sid = uuid.uuid4().hex\n\n            self.sockets[sid] = ws\n\n            try:\n                # Send initial state to the new client\n                await self.send(\"status\", { \"status\": self.get_queue_info(), 'sid': sid }, sid)\n                # On reconnect if we are the currently executing client send the current node\n                if self.client_id == sid and self.last_node_id is not None:\n                    await self.send(\"executing\", { \"node\": self.last_node_id }, sid)\n                    \n                async for msg in ws:\n                    if msg.type == aiohttp.WSMsgType.ERROR:\n                        logging.warning('ws connection closed with exception %s' % ws.exception())\n            finally:\n                self.sockets.pop(sid, None)\n            return ws\n\n        @routes.get(\"/\")\n        async def get_root(request):\n            return web.FileResponse(os.path.join(self.web_root, \"index.html\"))\n\n        @routes.get(\"/embeddings\")\n        def get_embeddings(self):\n            embeddings = folder_paths.get_filename_list(\"embeddings\")\n            return web.json_response(list(map(lambda a: os.path.splitext(a)[0], embeddings)))\n\n        @routes.get(\"/extensions\")\n        async def get_extensions(request):\n            files = glob.glob(os.path.join(\n                glob.escape(self.web_root), 'extensions/**/*.js'), recursive=True)\n            \n            extensions = list(map(lambda f: \"/\" + os.path.relpath(f, self.web_root).replace(\"\\\\\", \"/\"), files))\n            \n            for name, dir in nodes.EXTENSION_WEB_DIRS.items():\n                files = glob.glob(os.path.join(glob.escape(dir), '**/*.js'), recursive=True)\n                extensions.extend(list(map(lambda f: \"/extensions/\" + urllib.parse.quote(\n                    name) + \"/\" + os.path.relpath(f, dir).replace(\"\\\\\", \"/\"), files)))\n\n            return web.json_response(extensions)\n\n        def get_dir_by_type(dir_type):\n            if dir_type is None:\n                dir_type = \"input\"\n\n            if dir_type == \"input\":\n                type_dir = folder_paths.get_input_directory()\n            elif dir_type == \"temp\":\n                type_dir = folder_paths.get_temp_directory()\n            elif dir_type == \"output\":\n                type_dir = folder_paths.get_output_directory()\n\n            return type_dir, dir_type\n\n        def image_upload(post, image_save_function=None):\n            image = post.get(\"image\")\n            overwrite = post.get(\"overwrite\")\n\n            image_upload_type = post.get(\"type\")\n            upload_dir, image_upload_type = get_dir_by_type(image_upload_type)\n\n            if image and image.file:\n                filename = image.filename\n                if not filename:\n                    return web.Response(status=400)\n\n                subfolder = post.get(\"subfolder\", \"\")\n                full_output_folder = os.path.join(upload_dir, os.path.normpath(subfolder))\n                filepath = os.path.abspath(os.path.join(full_output_folder, filename))\n\n                if os.path.commonpath((upload_dir, filepath)) != upload_dir:\n                    return web.Response(status=400)\n\n                if not os.path.exists(full_output_folder):\n                    os.makedirs(full_output_folder)\n\n                split = os.path.splitext(filename)\n\n                if overwrite is not None and (overwrite == \"true\" or overwrite == \"1\"):\n                    pass\n                else:\n                    i = 1\n                    while os.path.exists(filepath):\n                        filename = f\"{split[0]} ({i}){split[1]}\"\n                        filepath = os.path.join(full_output_folder, filename)\n                        i += 1\n\n                if image_save_function is not None:\n                    image_save_function(image, post, filepath)\n                else:\n                    with open(filepath, \"wb\") as f:\n                        f.write(image.file.read())\n\n                return web.json_response({\"name\" : filename, \"subfolder\": subfolder, \"type\": image_upload_type})\n            else:\n                return web.Response(status=400)\n\n        @routes.post(\"/upload/image\")\n        async def upload_image(request):\n            post = await request.post()\n            return image_upload(post)\n\n\n        @routes.post(\"/upload/mask\")\n        async def upload_mask(request):\n            post = await request.post()\n\n            def image_save_function(image, post, filepath):\n                original_ref = json.loads(post.get(\"original_ref\"))\n                filename, output_dir = folder_paths.annotated_filepath(original_ref['filename'])\n\n                # validation for security: prevent accessing arbitrary path\n                if filename[0] == '/' or '..' in filename:\n                    return web.Response(status=400)\n\n                if output_dir is None:\n                    type = original_ref.get(\"type\", \"output\")\n                    output_dir = folder_paths.get_directory_by_type(type)\n\n                if output_dir is None:\n                    return web.Response(status=400)\n\n                if original_ref.get(\"subfolder\", \"\") != \"\":\n                    full_output_dir = os.path.join(output_dir, original_ref[\"subfolder\"])\n                    if os.path.commonpath((os.path.abspath(full_output_dir), output_dir)) != output_dir:\n                        return web.Response(status=403)\n                    output_dir = full_output_dir\n\n                file = os.path.join(output_dir, filename)\n\n                if os.path.isfile(file):\n                    with Image.open(file) as original_pil:\n                        metadata = PngInfo()\n                        if hasattr(original_pil,'text'):\n                            for key in original_pil.text:\n                                metadata.add_text(key, original_pil.text[key])\n                        original_pil = original_pil.convert('RGBA')\n                        mask_pil = Image.open(image.file).convert('RGBA')\n\n                        # alpha copy\n                        new_alpha = mask_pil.getchannel('A')\n                        original_pil.putalpha(new_alpha)\n                        original_pil.save(filepath, compress_level=4, pnginfo=metadata)\n\n            return image_upload(post, image_save_function)\n\n        @routes.get(\"/view\")\n        async def view_image(request):\n            if \"filename\" in request.rel_url.query:\n                filename = request.rel_url.query[\"filename\"]\n                filename,output_dir = folder_paths.annotated_filepath(filename)\n\n                # validation for security: prevent accessing arbitrary path\n                if filename[0] == '/' or '..' in filename:\n                    return web.Response(status=400)\n\n                if output_dir is None:\n                    type = request.rel_url.query.get(\"type\", \"output\")\n                    output_dir = folder_paths.get_directory_by_type(type)\n\n                if output_dir is None:\n                    return web.Response(status=400)\n\n                if \"subfolder\" in request.rel_url.query:\n                    full_output_dir = os.path.join(output_dir, request.rel_url.query[\"subfolder\"])\n                    if os.path.commonpath((os.path.abspath(full_output_dir), output_dir)) != output_dir:\n                        return web.Response(status=403)\n                    output_dir = full_output_dir\n\n                filename = os.path.basename(filename)\n                file = os.path.join(output_dir, filename)\n\n                if os.path.isfile(file):\n                    if 'preview' in request.rel_url.query:\n                        with Image.open(file) as img:\n                            preview_info = request.rel_url.query['preview'].split(';')\n                            image_format = preview_info[0]\n                            if image_format not in ['webp', 'jpeg'] or 'a' in request.rel_url.query.get('channel', ''):\n                                image_format = 'webp'\n\n                            quality = 90\n                            if preview_info[-1].isdigit():\n                                quality = int(preview_info[-1])\n\n                            buffer = BytesIO()\n                            if image_format in ['jpeg'] or request.rel_url.query.get('channel', '') == 'rgb':\n                                img = img.convert(\"RGB\")\n                            img.save(buffer, format=image_format, quality=quality)\n                            buffer.seek(0)\n\n                            return web.Response(body=buffer.read(), content_type=f'image/{image_format}',\n                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n\n                    if 'channel' not in request.rel_url.query:\n                        channel = 'rgba'\n                    else:\n                        channel = request.rel_url.query[\"channel\"]\n\n                    if channel == 'rgb':\n                        with Image.open(file) as img:\n                            if img.mode == \"RGBA\":\n                                r, g, b, a = img.split()\n                                new_img = Image.merge('RGB', (r, g, b))\n                            else:\n                                new_img = img.convert(\"RGB\")\n\n                            buffer = BytesIO()\n                            new_img.save(buffer, format='PNG')\n                            buffer.seek(0)\n\n                            return web.Response(body=buffer.read(), content_type='image/png',\n                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n\n                    elif channel == 'a':\n                        with Image.open(file) as img:\n                            if img.mode == \"RGBA\":\n                                _, _, _, a = img.split()\n                            else:\n                                a = Image.new('L', img.size, 255)\n\n                            # alpha img\n                            alpha_img = Image.new('RGBA', img.size)\n                            alpha_img.putalpha(a)\n                            alpha_buffer = BytesIO()\n                            alpha_img.save(alpha_buffer, format='PNG')\n                            alpha_buffer.seek(0)\n\n                            return web.Response(body=alpha_buffer.read(), content_type='image/png',\n                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n                    else:\n                        return web.FileResponse(file, headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n\n            return web.Response(status=404)\n\n        @routes.get(\"/view_metadata/{folder_name}\")\n        async def view_metadata(request):\n            folder_name = request.match_info.get(\"folder_name\", None)\n            if folder_name is None:\n                return web.Response(status=404)\n            if not \"filename\" in request.rel_url.query:\n                return web.Response(status=404)\n\n            filename = request.rel_url.query[\"filename\"]\n            if not filename.endswith(\".safetensors\"):\n                return web.Response(status=404)\n\n            safetensors_path = folder_paths.get_full_path(folder_name, filename)\n            if safetensors_path is None:\n                return web.Response(status=404)\n            out = comfy.utils.safetensors_header(safetensors_path, max_size=1024*1024)\n            if out is None:\n                return web.Response(status=404)\n            dt = json.loads(out)\n            if not \"__metadata__\" in dt:\n                return web.Response(status=404)\n            return web.json_response(dt[\"__metadata__\"])\n\n        @routes.get(\"/system_stats\")\n        async def get_queue(request):\n            device = comfy.model_management.get_torch_device()\n            device_name = comfy.model_management.get_torch_device_name(device)\n            vram_total, torch_vram_total = comfy.model_management.get_total_memory(device, torch_total_too=True)\n            vram_free, torch_vram_free = comfy.model_management.get_free_memory(device, torch_free_too=True)\n            system_stats = {\n                \"system\": {\n                    \"os\": os.name,\n                    \"python_version\": sys.version,\n                    \"embedded_python\": os.path.split(os.path.split(sys.executable)[0])[1] == \"python_embeded\"\n                },\n                \"devices\": [\n                    {\n                        \"name\": device_name,\n                        \"type\": device.type,\n                        \"index\": device.index,\n                        \"vram_total\": vram_total,\n                        \"vram_free\": vram_free,\n                        \"torch_vram_total\": torch_vram_total,\n                        \"torch_vram_free\": torch_vram_free,\n                    }\n                ]\n            }\n            return web.json_response(system_stats)\n\n        @routes.get(\"/prompt\")\n        async def get_prompt(request):\n            return web.json_response(self.get_queue_info())\n\n        def node_info(node_class):\n            obj_class = nodes.NODE_CLASS_MAPPINGS[node_class]\n            info = {}\n            info['input'] = obj_class.INPUT_TYPES()\n            info['output'] = obj_class.RETURN_TYPES\n            info['output_is_list'] = obj_class.OUTPUT_IS_LIST if hasattr(obj_class, 'OUTPUT_IS_LIST') else [False] * len(obj_class.RETURN_TYPES)\n            info['output_name'] = obj_class.RETURN_NAMES if hasattr(obj_class, 'RETURN_NAMES') else info['output']\n            info['name'] = node_class\n            info['display_name'] = nodes.NODE_DISPLAY_NAME_MAPPINGS[node_class] if node_class in nodes.NODE_DISPLAY_NAME_MAPPINGS.keys() else node_class\n            info['description'] = obj_class.DESCRIPTION if hasattr(obj_class,'DESCRIPTION') else ''\n            info['category'] = 'sd'\n            if hasattr(obj_class, 'OUTPUT_NODE') and obj_class.OUTPUT_NODE == True:\n                info['output_node'] = True\n            else:\n                info['output_node'] = False\n\n            if hasattr(obj_class, 'CATEGORY'):\n                info['category'] = obj_class.CATEGORY\n            return info\n\n        @routes.get(\"/object_info\")\n        async def get_object_info(request):\n            out = {}\n            for x in nodes.NODE_CLASS_MAPPINGS:\n                try:\n                    out[x] = node_info(x)\n                except Exception as e:\n                    logging.error(f\"[ERROR] An error occurred while retrieving information for the '{x}' node.\")\n                    logging.error(traceback.format_exc())\n            return web.json_response(out)\n\n        @routes.get(\"/object_info/{node_class}\")\n        async def get_object_info_node(request):\n            node_class = request.match_info.get(\"node_class\", None)\n            out = {}\n            if (node_class is not None) and (node_class in nodes.NODE_CLASS_MAPPINGS):\n                out[node_class] = node_info(node_class)\n            return web.json_response(out)\n\n        @routes.get(\"/history\")\n        async def get_history(request):\n            max_items = request.rel_url.query.get(\"max_items\", None)\n            if max_items is not None:\n                max_items = int(max_items)\n            return web.json_response(self.prompt_queue.get_history(max_items=max_items))\n\n        @routes.get(\"/history/{prompt_id}\")\n        async def get_history(request):\n            prompt_id = request.match_info.get(\"prompt_id\", None)\n            return web.json_response(self.prompt_queue.get_history(prompt_id=prompt_id))\n\n        @routes.get(\"/queue\")\n        async def get_queue(request):\n            queue_info = {}\n            current_queue = self.prompt_queue.get_current_queue()\n            queue_info['queue_running'] = current_queue[0]\n            queue_info['queue_pending'] = current_queue[1]\n            return web.json_response(queue_info)\n\n        @routes.post(\"/prompt\")\n        async def post_prompt(request):\n            logging.info(\"got prompt\")\n            resp_code = 200\n            out_string = \"\"\n            json_data =  await request.json()\n            json_data = self.trigger_on_prompt(json_data)\n\n            if \"number\" in json_data:\n                number = float(json_data['number'])\n            else:\n                number = self.number\n                if \"front\" in json_data:\n                    if json_data['front']:\n                        number = -number\n\n                self.number += 1\n\n            if \"prompt\" in json_data:\n                prompt = json_data[\"prompt\"]\n                valid = execution.validate_prompt(prompt)\n                extra_data = {}\n                if \"extra_data\" in json_data:\n                    extra_data = json_data[\"extra_data\"]\n\n                if \"client_id\" in json_data:\n                    extra_data[\"client_id\"] = json_data[\"client_id\"]\n                if valid[0]:\n                    prompt_id = str(uuid.uuid4())\n                    outputs_to_execute = valid[2]\n                    self.prompt_queue.put((number, prompt_id, prompt, extra_data, outputs_to_execute))\n                    response = {\"prompt_id\": prompt_id, \"number\": number, \"node_errors\": valid[3]}\n                    return web.json_response(response)\n                else:\n                    logging.warning(\"invalid prompt: {}\".format(valid[1]))\n                    return web.json_response({\"error\": valid[1], \"node_errors\": valid[3]}, status=400)\n            else:\n                return web.json_response({\"error\": \"no prompt\", \"node_errors\": []}, status=400)\n\n        @routes.post(\"/queue\")\n        async def post_queue(request):\n            json_data =  await request.json()\n            if \"clear\" in json_data:\n                if json_data[\"clear\"]:\n                    self.prompt_queue.wipe_queue()\n            if \"delete\" in json_data:\n                to_delete = json_data['delete']\n                for id_to_delete in to_delete:\n                    delete_func = lambda a: a[1] == id_to_delete\n                    self.prompt_queue.delete_queue_item(delete_func)\n\n            return web.Response(status=200)\n\n        @routes.post(\"/interrupt\")\n        async def post_interrupt(request):\n            nodes.interrupt_processing()\n            return web.Response(status=200)\n\n        @routes.post(\"/free\")\n        async def post_free(request):\n            json_data = await request.json()\n            unload_models = json_data.get(\"unload_models\", False)\n            free_memory = json_data.get(\"free_memory\", False)\n            if unload_models:\n                self.prompt_queue.set_flag(\"unload_models\", unload_models)\n            if free_memory:\n                self.prompt_queue.set_flag(\"free_memory\", free_memory)\n            return web.Response(status=200)\n\n        @routes.post(\"/history\")\n        async def post_history(request):\n            json_data =  await request.json()\n            if \"clear\" in json_data:\n                if json_data[\"clear\"]:\n                    self.prompt_queue.wipe_history()\n            if \"delete\" in json_data:\n                to_delete = json_data['delete']\n                for id_to_delete in to_delete:\n                    self.prompt_queue.delete_history_item(id_to_delete)\n\n            return web.Response(status=200)\n\n    def add_routes(self):\n        self.user_manager.add_routes(self.routes)\n\n        # Prefix every route with /api for easier matching for delegation.\n        # This is very useful for frontend dev server, which need to forward\n        # everything except serving of static files.\n        # Currently both the old endpoints without prefix and new endpoints with\n        # prefix are supported.\n        api_routes = web.RouteTableDef()\n        for route in self.routes:\n            # Custom nodes might add extra static routes. Only process non-static\n            # routes to add /api prefix.\n            if isinstance(route, web.RouteDef):\n                api_routes.route(route.method, \"/api\" + route.path)(route.handler, **route.kwargs)\n        self.app.add_routes(api_routes)\n        self.app.add_routes(self.routes)\n\n        for name, dir in nodes.EXTENSION_WEB_DIRS.items():\n            self.app.add_routes([\n                web.static('/extensions/' + urllib.parse.quote(name), dir),\n            ])\n\n        self.app.add_routes([\n            web.static('/', self.web_root),\n        ])\n\n    def get_queue_info(self):\n        prompt_info = {}\n        exec_info = {}\n        exec_info['queue_remaining'] = self.prompt_queue.get_tasks_remaining()\n        prompt_info['exec_info'] = exec_info\n        return prompt_info\n\n    async def send(self, event, data, sid=None):\n        if event == BinaryEventTypes.UNENCODED_PREVIEW_IMAGE:\n            await self.send_image(data, sid=sid)\n        elif isinstance(data, (bytes, bytearray)):\n            await self.send_bytes(event, data, sid)\n        else:\n            await self.send_json(event, data, sid)\n\n    def encode_bytes(self, event, data):\n        if not isinstance(event, int):\n            raise RuntimeError(f\"Binary event types must be integers, got {event}\")\n\n        packed = struct.pack(\">I\", event)\n        message = bytearray(packed)\n        message.extend(data)\n        return message\n\n    async def send_image(self, image_data, sid=None):\n        image_type = image_data[0]\n        image = image_data[1]\n        max_size = image_data[2]\n        if max_size is not None:\n            if hasattr(Image, 'Resampling'):\n                resampling = Image.Resampling.BILINEAR\n            else:\n                resampling = Image.ANTIALIAS\n\n            image = ImageOps.contain(image, (max_size, max_size), resampling)\n        type_num = 1\n        if image_type == \"JPEG\":\n            type_num = 1\n        elif image_type == \"PNG\":\n            type_num = 2\n\n        bytesIO = BytesIO()\n        header = struct.pack(\">I\", type_num)\n        bytesIO.write(header)\n        image.save(bytesIO, format=image_type, quality=95, compress_level=1)\n        preview_bytes = bytesIO.getvalue()\n        await self.send_bytes(BinaryEventTypes.PREVIEW_IMAGE, preview_bytes, sid=sid)\n\n    async def send_bytes(self, event, data, sid=None):\n        message = self.encode_bytes(event, data)\n\n        if sid is None:\n            sockets = list(self.sockets.values())\n            for ws in sockets:\n                await send_socket_catch_exception(ws.send_bytes, message)\n        elif sid in self.sockets:\n            await send_socket_catch_exception(self.sockets[sid].send_bytes, message)\n\n    async def send_json(self, event, data, sid=None):\n        message = {\"type\": event, \"data\": data}\n\n        if sid is None:\n            sockets = list(self.sockets.values())\n            for ws in sockets:\n                await send_socket_catch_exception(ws.send_json, message)\n        elif sid in self.sockets:\n            await send_socket_catch_exception(self.sockets[sid].send_json, message)\n\n    def send_sync(self, event, data, sid=None):\n        self.loop.call_soon_threadsafe(\n            self.messages.put_nowait, (event, data, sid))\n\n    def queue_updated(self):\n        self.send_sync(\"status\", { \"status\": self.get_queue_info() })\n\n    async def publish_loop(self):\n        while True:\n            msg = await self.messages.get()\n            await self.send(*msg)\n\n    async def start(self, address, port, verbose=True, call_on_start=None):\n        runner = web.AppRunner(self.app, access_log=None)\n        await runner.setup()\n        ssl_ctx = None\n        scheme = \"http\"\n        if args.tls_keyfile and args.tls_certfile:\n                ssl_ctx = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS_SERVER, verify_mode=ssl.CERT_NONE)\n                ssl_ctx.load_cert_chain(certfile=args.tls_certfile,\n                                keyfile=args.tls_keyfile)\n                scheme = \"https\"\n\n        site = web.TCPSite(runner, address, port, ssl_context=ssl_ctx)\n        await site.start()\n\n        if verbose:\n            logging.info(\"Starting server\\n\")\n            logging.info(\"To see the GUI go to: {}://{}:{}\".format(scheme, address, port))\n        if call_on_start is not None:\n            call_on_start(scheme, address, port)\n\n    def add_on_prompt_handler(self, handler):\n        self.on_prompt_handlers.append(handler)\n\n    def trigger_on_prompt(self, json_data):\n        for handler in self.on_prompt_handlers:\n            try:\n                json_data = handler(json_data)\n            except Exception as e:\n                logging.warning(f\"[ERROR] An error occurred during the on_prompt_handler processing\")\n                logging.warning(traceback.format_exc())\n\n        return json_data\n", "comfy_extras/nodes_model_merging.py": "import comfy.sd\nimport comfy.utils\nimport comfy.model_base\nimport comfy.model_management\nimport comfy.model_sampling\n\nimport torch\nimport folder_paths\nimport json\nimport os\n\nfrom comfy.cli_args import args\n\nclass ModelMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, ratio):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\nclass ModelSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, multiplier):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, - multiplier, multiplier)\n        return (m, )\n\nclass ModelAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        for k in kp:\n            m.add_patches({k: kp[k]}, 1.0, 1.0)\n        return (m, )\n\n\nclass CLIPMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip1\": (\"CLIP\",),\n                              \"clip2\": (\"CLIP\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, clip1, clip2, ratio):\n        m = clip1.clone()\n        kp = clip2.get_key_patches()\n        for k in kp:\n            if k.endswith(\".position_ids\") or k.endswith(\".logit_scale\"):\n                continue\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\n\nclass CLIPSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip1\": (\"CLIP\",),\n                              \"clip2\": (\"CLIP\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, clip1, clip2, multiplier):\n        m = clip1.clone()\n        kp = clip2.get_key_patches()\n        for k in kp:\n            if k.endswith(\".position_ids\") or k.endswith(\".logit_scale\"):\n                continue\n            m.add_patches({k: kp[k]}, - multiplier, multiplier)\n        return (m, )\n\n\nclass CLIPAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip1\": (\"CLIP\",),\n                              \"clip2\": (\"CLIP\",),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, clip1, clip2):\n        m = clip1.clone()\n        kp = clip2.get_key_patches()\n        for k in kp:\n            if k.endswith(\".position_ids\") or k.endswith(\".logit_scale\"):\n                continue\n            m.add_patches({k: kp[k]}, 1.0, 1.0)\n        return (m, )\n\n\nclass ModelMergeBlocks:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"input\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"middle\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def merge(self, model1, model2, **kwargs):\n        m = model1.clone()\n        kp = model2.get_key_patches(\"diffusion_model.\")\n        default_ratio = next(iter(kwargs.values()))\n\n        for k in kp:\n            ratio = default_ratio\n            k_unet = k[len(\"diffusion_model.\"):]\n\n            last_arg_size = 0\n            for arg in kwargs:\n                if k_unet.startswith(arg) and last_arg_size < len(arg):\n                    ratio = kwargs[arg]\n                    last_arg_size = len(arg)\n\n            m.add_patches({k: kp[k]}, 1.0 - ratio, ratio)\n        return (m, )\n\ndef save_checkpoint(model, clip=None, vae=None, clip_vision=None, filename_prefix=None, output_dir=None, prompt=None, extra_pnginfo=None):\n    full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, output_dir)\n    prompt_info = \"\"\n    if prompt is not None:\n        prompt_info = json.dumps(prompt)\n\n    metadata = {}\n\n    enable_modelspec = True\n    if isinstance(model.model, comfy.model_base.SDXL):\n        if isinstance(model.model, comfy.model_base.SDXL_instructpix2pix):\n            metadata[\"modelspec.architecture\"] = \"stable-diffusion-xl-v1-edit\"\n        else:\n            metadata[\"modelspec.architecture\"] = \"stable-diffusion-xl-v1-base\"\n    elif isinstance(model.model, comfy.model_base.SDXLRefiner):\n        metadata[\"modelspec.architecture\"] = \"stable-diffusion-xl-v1-refiner\"\n    elif isinstance(model.model, comfy.model_base.SVD_img2vid):\n        metadata[\"modelspec.architecture\"] = \"stable-video-diffusion-img2vid-v1\"\n    elif isinstance(model.model, comfy.model_base.SD3):\n        metadata[\"modelspec.architecture\"] = \"stable-diffusion-v3-medium\" #TODO: other SD3 variants\n    else:\n        enable_modelspec = False\n\n    if enable_modelspec:\n        metadata[\"modelspec.sai_model_spec\"] = \"1.0.0\"\n        metadata[\"modelspec.implementation\"] = \"sgm\"\n        metadata[\"modelspec.title\"] = \"{} {}\".format(filename, counter)\n\n    #TODO:\n    # \"stable-diffusion-v1\", \"stable-diffusion-v1-inpainting\", \"stable-diffusion-v2-512\",\n    # \"stable-diffusion-v2-768-v\", \"stable-diffusion-v2-unclip-l\", \"stable-diffusion-v2-unclip-h\",\n    # \"v2-inpainting\"\n\n    extra_keys = {}\n    model_sampling = model.get_model_object(\"model_sampling\")\n    if isinstance(model_sampling, comfy.model_sampling.ModelSamplingContinuousEDM):\n        if isinstance(model_sampling, comfy.model_sampling.V_PREDICTION):\n            extra_keys[\"edm_vpred.sigma_max\"] = torch.tensor(model_sampling.sigma_max).float()\n            extra_keys[\"edm_vpred.sigma_min\"] = torch.tensor(model_sampling.sigma_min).float()\n\n    if model.model.model_type == comfy.model_base.ModelType.EPS:\n        metadata[\"modelspec.predict_key\"] = \"epsilon\"\n    elif model.model.model_type == comfy.model_base.ModelType.V_PREDICTION:\n        metadata[\"modelspec.predict_key\"] = \"v\"\n\n    if not args.disable_metadata:\n        metadata[\"prompt\"] = prompt_info\n        if extra_pnginfo is not None:\n            for x in extra_pnginfo:\n                metadata[x] = json.dumps(extra_pnginfo[x])\n\n    output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n    output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n    comfy.sd.save_checkpoint(output_checkpoint, model, clip, vae, clip_vision, metadata=metadata, extra_keys=extra_keys)\n\nclass CheckpointSave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\",),\n                              \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"checkpoints/ComfyUI\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, model, clip, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        save_checkpoint(model, clip=clip, vae=vae, filename_prefix=filename_prefix, output_dir=self.output_dir, prompt=prompt, extra_pnginfo=extra_pnginfo)\n        return {}\n\nclass CLIPSave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"clip/ComfyUI\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, clip, filename_prefix, prompt=None, extra_pnginfo=None):\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = {}\n        if not args.disable_metadata:\n            metadata[\"prompt\"] = prompt_info\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        comfy.model_management.load_models_gpu([clip.load_model()], force_patch_weights=True)\n        clip_sd = clip.get_sd()\n\n        for prefix in [\"clip_l.\", \"clip_g.\", \"\"]:\n            k = list(filter(lambda a: a.startswith(prefix), clip_sd.keys()))\n            current_clip_sd = {}\n            for x in k:\n                current_clip_sd[x] = clip_sd.pop(x)\n            if len(current_clip_sd) == 0:\n                continue\n\n            p = prefix[:-1]\n            replace_prefix = {}\n            filename_prefix_ = filename_prefix\n            if len(p) > 0:\n                filename_prefix_ = \"{}_{}\".format(filename_prefix_, p)\n                replace_prefix[prefix] = \"\"\n            replace_prefix[\"transformer.\"] = \"\"\n\n            full_output_folder, filename, counter, subfolder, filename_prefix_ = folder_paths.get_save_image_path(filename_prefix_, self.output_dir)\n\n            output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n            output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n            current_clip_sd = comfy.utils.state_dict_prefix_replace(current_clip_sd, replace_prefix)\n\n            comfy.utils.save_torch_file(current_clip_sd, output_checkpoint, metadata=metadata)\n        return {}\n\nclass VAESave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"vae/ComfyUI_vae\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n\n    CATEGORY = \"advanced/model_merging\"\n\n    def save(self, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        prompt_info = \"\"\n        if prompt is not None:\n            prompt_info = json.dumps(prompt)\n\n        metadata = {}\n        if not args.disable_metadata:\n            metadata[\"prompt\"] = prompt_info\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata[x] = json.dumps(extra_pnginfo[x])\n\n        output_checkpoint = f\"{filename}_{counter:05}_.safetensors\"\n        output_checkpoint = os.path.join(full_output_folder, output_checkpoint)\n\n        comfy.utils.save_torch_file(vae.get_sd(), output_checkpoint, metadata=metadata)\n        return {}\n\nNODE_CLASS_MAPPINGS = {\n    \"ModelMergeSimple\": ModelMergeSimple,\n    \"ModelMergeBlocks\": ModelMergeBlocks,\n    \"ModelMergeSubtract\": ModelSubtract,\n    \"ModelMergeAdd\": ModelAdd,\n    \"CheckpointSave\": CheckpointSave,\n    \"CLIPMergeSimple\": CLIPMergeSimple,\n    \"CLIPMergeSubtract\": CLIPSubtract,\n    \"CLIPMergeAdd\": CLIPAdd,\n    \"CLIPSave\": CLIPSave,\n    \"VAESave\": VAESave,\n}\n", "comfy_extras/nodes_photomaker.py": "import torch\nimport torch.nn as nn\nimport folder_paths\nimport comfy.clip_model\nimport comfy.clip_vision\nimport comfy.ops\n\n# code for model from: https://github.com/TencentARC/PhotoMaker/blob/main/photomaker/model.py under Apache License Version 2.0\nVISION_CONFIG_DICT = {\n    \"hidden_size\": 1024,\n    \"image_size\": 224,\n    \"intermediate_size\": 4096,\n    \"num_attention_heads\": 16,\n    \"num_channels\": 3,\n    \"num_hidden_layers\": 24,\n    \"patch_size\": 14,\n    \"projection_dim\": 768,\n    \"hidden_act\": \"quick_gelu\",\n}\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim, use_residual=True, operations=comfy.ops):\n        super().__init__()\n        if use_residual:\n            assert in_dim == out_dim\n        self.layernorm = operations.LayerNorm(in_dim)\n        self.fc1 = operations.Linear(in_dim, hidden_dim)\n        self.fc2 = operations.Linear(hidden_dim, out_dim)\n        self.use_residual = use_residual\n        self.act_fn = nn.GELU()\n\n    def forward(self, x):\n        residual = x\n        x = self.layernorm(x)\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.fc2(x)\n        if self.use_residual:\n            x = x + residual\n        return x\n\n\nclass FuseModule(nn.Module):\n    def __init__(self, embed_dim, operations):\n        super().__init__()\n        self.mlp1 = MLP(embed_dim * 2, embed_dim, embed_dim, use_residual=False, operations=operations)\n        self.mlp2 = MLP(embed_dim, embed_dim, embed_dim, use_residual=True, operations=operations)\n        self.layer_norm = operations.LayerNorm(embed_dim)\n\n    def fuse_fn(self, prompt_embeds, id_embeds):\n        stacked_id_embeds = torch.cat([prompt_embeds, id_embeds], dim=-1)\n        stacked_id_embeds = self.mlp1(stacked_id_embeds) + prompt_embeds\n        stacked_id_embeds = self.mlp2(stacked_id_embeds)\n        stacked_id_embeds = self.layer_norm(stacked_id_embeds)\n        return stacked_id_embeds\n\n    def forward(\n        self,\n        prompt_embeds,\n        id_embeds,\n        class_tokens_mask,\n    ) -> torch.Tensor:\n        # id_embeds shape: [b, max_num_inputs, 1, 2048]\n        id_embeds = id_embeds.to(prompt_embeds.dtype)\n        num_inputs = class_tokens_mask.sum().unsqueeze(0) # TODO: check for training case\n        batch_size, max_num_inputs = id_embeds.shape[:2]\n        # seq_length: 77\n        seq_length = prompt_embeds.shape[1]\n        # flat_id_embeds shape: [b*max_num_inputs, 1, 2048]\n        flat_id_embeds = id_embeds.view(\n            -1, id_embeds.shape[-2], id_embeds.shape[-1]\n        )\n        # valid_id_mask [b*max_num_inputs]\n        valid_id_mask = (\n            torch.arange(max_num_inputs, device=flat_id_embeds.device)[None, :]\n            < num_inputs[:, None]\n        )\n        valid_id_embeds = flat_id_embeds[valid_id_mask.flatten()]\n\n        prompt_embeds = prompt_embeds.view(-1, prompt_embeds.shape[-1])\n        class_tokens_mask = class_tokens_mask.view(-1)\n        valid_id_embeds = valid_id_embeds.view(-1, valid_id_embeds.shape[-1])\n        # slice out the image token embeddings\n        image_token_embeds = prompt_embeds[class_tokens_mask]\n        stacked_id_embeds = self.fuse_fn(image_token_embeds, valid_id_embeds)\n        assert class_tokens_mask.sum() == stacked_id_embeds.shape[0], f\"{class_tokens_mask.sum()} != {stacked_id_embeds.shape[0]}\"\n        prompt_embeds.masked_scatter_(class_tokens_mask[:, None], stacked_id_embeds.to(prompt_embeds.dtype))\n        updated_prompt_embeds = prompt_embeds.view(batch_size, seq_length, -1)\n        return updated_prompt_embeds\n\nclass PhotoMakerIDEncoder(comfy.clip_model.CLIPVisionModelProjection):\n    def __init__(self):\n        self.load_device = comfy.model_management.text_encoder_device()\n        offload_device = comfy.model_management.text_encoder_offload_device()\n        dtype = comfy.model_management.text_encoder_dtype(self.load_device)\n\n        super().__init__(VISION_CONFIG_DICT, dtype, offload_device, comfy.ops.manual_cast)\n        self.visual_projection_2 = comfy.ops.manual_cast.Linear(1024, 1280, bias=False)\n        self.fuse_module = FuseModule(2048, comfy.ops.manual_cast)\n\n    def forward(self, id_pixel_values, prompt_embeds, class_tokens_mask):\n        b, num_inputs, c, h, w = id_pixel_values.shape\n        id_pixel_values = id_pixel_values.view(b * num_inputs, c, h, w)\n\n        shared_id_embeds = self.vision_model(id_pixel_values)[2]\n        id_embeds = self.visual_projection(shared_id_embeds)\n        id_embeds_2 = self.visual_projection_2(shared_id_embeds)\n\n        id_embeds = id_embeds.view(b, num_inputs, 1, -1)\n        id_embeds_2 = id_embeds_2.view(b, num_inputs, 1, -1)\n\n        id_embeds = torch.cat((id_embeds, id_embeds_2), dim=-1)\n        updated_prompt_embeds = self.fuse_module(prompt_embeds, id_embeds, class_tokens_mask)\n\n        return updated_prompt_embeds\n\n\nclass PhotoMakerLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"photomaker_model_name\": (folder_paths.get_filename_list(\"photomaker\"), )}}\n\n    RETURN_TYPES = (\"PHOTOMAKER\",)\n    FUNCTION = \"load_photomaker_model\"\n\n    CATEGORY = \"_for_testing/photomaker\"\n\n    def load_photomaker_model(self, photomaker_model_name):\n        photomaker_model_path = folder_paths.get_full_path(\"photomaker\", photomaker_model_name)\n        photomaker_model = PhotoMakerIDEncoder()\n        data = comfy.utils.load_torch_file(photomaker_model_path, safe_load=True)\n        if \"id_encoder\" in data:\n            data = data[\"id_encoder\"]\n        photomaker_model.load_state_dict(data)\n        return (photomaker_model,)\n\n\nclass PhotoMakerEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"photomaker\": (\"PHOTOMAKER\",),\n                              \"image\": (\"IMAGE\",),\n                              \"clip\": (\"CLIP\", ),\n                              \"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True, \"default\": \"photograph of photomaker\"}),\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_photomaker\"\n\n    CATEGORY = \"_for_testing/photomaker\"\n\n    def apply_photomaker(self, photomaker, image, clip, text):\n        special_token = \"photomaker\"\n        pixel_values = comfy.clip_vision.clip_preprocess(image.to(photomaker.load_device)).float()\n        try:\n            index = text.split(\" \").index(special_token) + 1\n        except ValueError:\n            index = -1\n        tokens = clip.tokenize(text, return_word_ids=True)\n        out_tokens = {}\n        for k in tokens:\n            out_tokens[k] = []\n            for t in tokens[k]:\n                f = list(filter(lambda x: x[2] != index, t))\n                while len(f) < len(t):\n                    f.append(t[-1])\n                out_tokens[k].append(f)\n\n        cond, pooled = clip.encode_from_tokens(out_tokens, return_pooled=True)\n\n        if index > 0:\n            token_index = index - 1\n            num_id_images = 1\n            class_tokens_mask = [True if token_index <= i < token_index+num_id_images else False for i in range(77)]\n            out = photomaker(id_pixel_values=pixel_values.unsqueeze(0), prompt_embeds=cond.to(photomaker.load_device),\n                            class_tokens_mask=torch.tensor(class_tokens_mask, dtype=torch.bool, device=photomaker.load_device).unsqueeze(0))\n        else:\n            out = cond\n\n        return ([[out, {\"pooled_output\": pooled}]], )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"PhotoMakerLoader\": PhotoMakerLoader,\n    \"PhotoMakerEncode\": PhotoMakerEncode,\n}\n\n", "comfy_extras/nodes_canny.py": "from kornia.filters import canny\nimport comfy.model_management\n\n\nclass Canny:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"image\": (\"IMAGE\",),\n                                \"low_threshold\": (\"FLOAT\", {\"default\": 0.4, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01}),\n                                \"high_threshold\": (\"FLOAT\", {\"default\": 0.8, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01})\n                                }}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"detect_edge\"\n\n    CATEGORY = \"image/preprocessors\"\n\n    def detect_edge(self, image, low_threshold, high_threshold):\n        output = canny(image.to(comfy.model_management.get_torch_device()).movedim(-1, 1), low_threshold, high_threshold)\n        img_out = output[1].to(comfy.model_management.intermediate_device()).repeat(1, 3, 1, 1).movedim(1, -1)\n        return (img_out,)\n\nNODE_CLASS_MAPPINGS = {\n    \"Canny\": Canny,\n}\n", "comfy_extras/nodes_video_model.py": "import nodes\nimport torch\nimport comfy.utils\nimport comfy.sd\nimport folder_paths\nimport comfy_extras.nodes_model_merging\n\n\nclass ImageOnlyCheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP_VISION\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n\n    CATEGORY = \"loaders/video_models\"\n\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n        out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=False, output_clipvision=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n        return (out[0], out[3], out[2])\n\n\nclass SVD_img2vid_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 576, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"video_frames\": (\"INT\", {\"default\": 14, \"min\": 1, \"max\": 4096}),\n                              \"motion_bucket_id\": (\"INT\", {\"default\": 127, \"min\": 1, \"max\": 1023}),\n                              \"fps\": (\"INT\", {\"default\": 6, \"min\": 1, \"max\": 1024}),\n                              \"augmentation_level\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, video_frames, motion_bucket_id, fps, augmentation_level):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = comfy.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        if augmentation_level > 0:\n            encode_pixels += torch.randn_like(pixels) * augmentation_level\n        t = vae.encode(encode_pixels)\n        positive = [[pooled, {\"motion_bucket_id\": motion_bucket_id, \"fps\": fps, \"augmentation_level\": augmentation_level, \"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"motion_bucket_id\": motion_bucket_id, \"fps\": fps, \"augmentation_level\": augmentation_level, \"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([video_frames, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent})\n\nclass VideoLinearCFGGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"min_cfg\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.5, \"round\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"sampling/video_models\"\n\n    def patch(self, model, min_cfg):\n        def linear_cfg(args):\n            cond = args[\"cond\"]\n            uncond = args[\"uncond\"]\n            cond_scale = args[\"cond_scale\"]\n\n            scale = torch.linspace(min_cfg, cond_scale, cond.shape[0], device=cond.device).reshape((cond.shape[0], 1, 1, 1))\n            return uncond + scale * (cond - uncond)\n\n        m = model.clone()\n        m.set_model_sampler_cfg_function(linear_cfg)\n        return (m, )\n\nclass VideoTriangleCFGGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"min_cfg\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.5, \"round\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"sampling/video_models\"\n\n    def patch(self, model, min_cfg):\n        def linear_cfg(args):\n            cond = args[\"cond\"]\n            uncond = args[\"uncond\"]\n            cond_scale = args[\"cond_scale\"]\n            period = 1.0\n            values = torch.linspace(0, 1, cond.shape[0], device=cond.device)\n            values = 2 * (values / period - torch.floor(values / period + 0.5)).abs()\n            scale = (values * (cond_scale - min_cfg) + min_cfg).reshape((cond.shape[0], 1, 1, 1))\n\n            return uncond + scale * (cond - uncond)\n\n        m = model.clone()\n        m.set_model_sampler_cfg_function(linear_cfg)\n        return (m, )\n\nclass ImageOnlyCheckpointSave(comfy_extras.nodes_model_merging.CheckpointSave):\n    CATEGORY = \"_for_testing\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip_vision\": (\"CLIP_VISION\",),\n                              \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"checkpoints/ComfyUI\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n\n    def save(self, model, clip_vision, vae, filename_prefix, prompt=None, extra_pnginfo=None):\n        comfy_extras.nodes_model_merging.save_checkpoint(model, clip_vision=clip_vision, vae=vae, filename_prefix=filename_prefix, output_dir=self.output_dir, prompt=prompt, extra_pnginfo=extra_pnginfo)\n        return {}\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageOnlyCheckpointLoader\": ImageOnlyCheckpointLoader,\n    \"SVD_img2vid_Conditioning\": SVD_img2vid_Conditioning,\n    \"VideoLinearCFGGuidance\": VideoLinearCFGGuidance,\n    \"VideoTriangleCFGGuidance\": VideoTriangleCFGGuidance,\n    \"ImageOnlyCheckpointSave\": ImageOnlyCheckpointSave,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"ImageOnlyCheckpointLoader\": \"Image Only Checkpoint Loader (img2vid model)\",\n}\n", "comfy_extras/nodes_stable_cascade.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nimport nodes\nimport comfy.utils\n\n\nclass StableCascade_EmptyLatentImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"width\": (\"INT\", {\"default\": 1024, \"min\": 256, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n            \"height\": (\"INT\", {\"default\": 1024, \"min\": 256, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n            \"compression\": (\"INT\", {\"default\": 42, \"min\": 4, \"max\": 128, \"step\": 1}),\n            \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})\n        }}\n    RETURN_TYPES = (\"LATENT\", \"LATENT\")\n    RETURN_NAMES = (\"stage_c\", \"stage_b\")\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent/stable_cascade\"\n\n    def generate(self, width, height, compression, batch_size=1):\n        c_latent = torch.zeros([batch_size, 16, height // compression, width // compression])\n        b_latent = torch.zeros([batch_size, 4, height // 4, width // 4])\n        return ({\n            \"samples\": c_latent,\n        }, {\n            \"samples\": b_latent,\n        })\n\nclass StableCascade_StageC_VAEEncode:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"image\": (\"IMAGE\",),\n            \"vae\": (\"VAE\", ),\n            \"compression\": (\"INT\", {\"default\": 42, \"min\": 4, \"max\": 128, \"step\": 1}),\n        }}\n    RETURN_TYPES = (\"LATENT\", \"LATENT\")\n    RETURN_NAMES = (\"stage_c\", \"stage_b\")\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent/stable_cascade\"\n\n    def generate(self, image, vae, compression):\n        width = image.shape[-2]\n        height = image.shape[-3]\n        out_width = (width // compression) * vae.downscale_ratio\n        out_height = (height // compression) * vae.downscale_ratio\n\n        s = comfy.utils.common_upscale(image.movedim(-1,1), out_width, out_height, \"bicubic\", \"center\").movedim(1,-1)\n\n        c_latent = vae.encode(s[:,:,:,:3])\n        b_latent = torch.zeros([c_latent.shape[0], 4, (height // 8) * 2, (width // 8) * 2])\n        return ({\n            \"samples\": c_latent,\n        }, {\n            \"samples\": b_latent,\n        })\n\nclass StableCascade_StageB_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"conditioning\": (\"CONDITIONING\",),\n                              \"stage_c\": (\"LATENT\",),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n\n    FUNCTION = \"set_prior\"\n\n    CATEGORY = \"conditioning/stable_cascade\"\n\n    def set_prior(self, conditioning, stage_c):\n        c = []\n        for t in conditioning:\n            d = t[1].copy()\n            d['stable_cascade_prior'] = stage_c['samples']\n            n = [t[0], d]\n            c.append(n)\n        return (c, )\n\nclass StableCascade_SuperResolutionControlnet:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"image\": (\"IMAGE\",),\n            \"vae\": (\"VAE\", ),\n        }}\n    RETURN_TYPES = (\"IMAGE\", \"LATENT\", \"LATENT\")\n    RETURN_NAMES = (\"controlnet_input\", \"stage_c\", \"stage_b\")\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"_for_testing/stable_cascade\"\n\n    def generate(self, image, vae):\n        width = image.shape[-2]\n        height = image.shape[-3]\n        batch_size = image.shape[0]\n        controlnet_input = vae.encode(image[:,:,:,:3]).movedim(1, -1)\n\n        c_latent = torch.zeros([batch_size, 16, height // 16, width // 16])\n        b_latent = torch.zeros([batch_size, 4, height // 2, width // 2])\n        return (controlnet_input, {\n            \"samples\": c_latent,\n        }, {\n            \"samples\": b_latent,\n        })\n\nNODE_CLASS_MAPPINGS = {\n    \"StableCascade_EmptyLatentImage\": StableCascade_EmptyLatentImage,\n    \"StableCascade_StageB_Conditioning\": StableCascade_StageB_Conditioning,\n    \"StableCascade_StageC_VAEEncode\": StableCascade_StageC_VAEEncode,\n    \"StableCascade_SuperResolutionControlnet\": StableCascade_SuperResolutionControlnet,\n}\n", "comfy_extras/nodes_ip2p.py": "import torch\n\nclass InstructPixToPixConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"pixels\": (\"IMAGE\", ),\n                             }}\n\n    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\",\"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/instructpix2pix\"\n\n    def encode(self, positive, negative, pixels, vae):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n\n        concat_latent = vae.encode(pixels)\n\n        out_latent = {}\n        out_latent[\"samples\"] = torch.zeros_like(concat_latent)\n\n        out = []\n        for conditioning in [positive, negative]:\n            c = []\n            for t in conditioning:\n                d = t[1].copy()\n                d[\"concat_latent_image\"] = concat_latent\n                n = [t[0], d]\n                c.append(n)\n            out.append(c)\n        return (out[0], out[1], out_latent)\n\nNODE_CLASS_MAPPINGS = {\n    \"InstructPixToPixConditioning\": InstructPixToPixConditioning,\n}\n", "comfy_extras/nodes_tomesd.py": "#Taken from: https://github.com/dbolya/tomesd\n\nimport torch\nfrom typing import Tuple, Callable\nimport math\n\ndef do_nothing(x: torch.Tensor, mode:str=None):\n    return x\n\n\ndef mps_gather_workaround(input, dim, index):\n    if input.shape[-1] == 1:\n        return torch.gather(\n            input.unsqueeze(-1),\n            dim - 1 if dim < 0 else dim,\n            index.unsqueeze(-1)\n        ).squeeze(-1)\n    else:\n        return torch.gather(input, dim, index)\n\n\ndef bipartite_soft_matching_random2d(metric: torch.Tensor,\n                                     w: int, h: int, sx: int, sy: int, r: int,\n                                     no_rand: bool = False) -> Tuple[Callable, Callable]:\n    \"\"\"\n    Partitions the tokens into src and dst and merges r tokens from src to dst.\n    Dst tokens are partitioned by choosing one randomy in each (sx, sy) region.\n    Args:\n     - metric [B, N, C]: metric to use for similarity\n     - w: image width in tokens\n     - h: image height in tokens\n     - sx: stride in the x dimension for dst, must divide w\n     - sy: stride in the y dimension for dst, must divide h\n     - r: number of tokens to remove (by merging)\n     - no_rand: if true, disable randomness (use top left corner only)\n    \"\"\"\n    B, N, _ = metric.shape\n\n    if r <= 0 or w == 1 or h == 1:\n        return do_nothing, do_nothing\n\n    gather = mps_gather_workaround if metric.device.type == \"mps\" else torch.gather\n    \n    with torch.no_grad():\n        \n        hsy, wsx = h // sy, w // sx\n\n        # For each sy by sx kernel, randomly assign one token to be dst and the rest src\n        if no_rand:\n            rand_idx = torch.zeros(hsy, wsx, 1, device=metric.device, dtype=torch.int64)\n        else:\n            rand_idx = torch.randint(sy*sx, size=(hsy, wsx, 1), device=metric.device)\n        \n        # The image might not divide sx and sy, so we need to work on a view of the top left if the idx buffer instead\n        idx_buffer_view = torch.zeros(hsy, wsx, sy*sx, device=metric.device, dtype=torch.int64)\n        idx_buffer_view.scatter_(dim=2, index=rand_idx, src=-torch.ones_like(rand_idx, dtype=rand_idx.dtype))\n        idx_buffer_view = idx_buffer_view.view(hsy, wsx, sy, sx).transpose(1, 2).reshape(hsy * sy, wsx * sx)\n\n        # Image is not divisible by sx or sy so we need to move it into a new buffer\n        if (hsy * sy) < h or (wsx * sx) < w:\n            idx_buffer = torch.zeros(h, w, device=metric.device, dtype=torch.int64)\n            idx_buffer[:(hsy * sy), :(wsx * sx)] = idx_buffer_view\n        else:\n            idx_buffer = idx_buffer_view\n\n        # We set dst tokens to be -1 and src to be 0, so an argsort gives us dst|src indices\n        rand_idx = idx_buffer.reshape(1, -1, 1).argsort(dim=1)\n\n        # We're finished with these\n        del idx_buffer, idx_buffer_view\n\n        # rand_idx is currently dst|src, so split them\n        num_dst = hsy * wsx\n        a_idx = rand_idx[:, num_dst:, :] # src\n        b_idx = rand_idx[:, :num_dst, :] # dst\n\n        def split(x):\n            C = x.shape[-1]\n            src = gather(x, dim=1, index=a_idx.expand(B, N - num_dst, C))\n            dst = gather(x, dim=1, index=b_idx.expand(B, num_dst, C))\n            return src, dst\n\n        # Cosine similarity between A and B\n        metric = metric / metric.norm(dim=-1, keepdim=True)\n        a, b = split(metric)\n        scores = a @ b.transpose(-1, -2)\n\n        # Can't reduce more than the # tokens in src\n        r = min(a.shape[1], r)\n\n        # Find the most similar greedily\n        node_max, node_idx = scores.max(dim=-1)\n        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n\n        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n        dst_idx = gather(node_idx[..., None], dim=-2, index=src_idx)\n\n    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n        src, dst = split(x)\n        n, t1, c = src.shape\n        \n        unm = gather(src, dim=-2, index=unm_idx.expand(n, t1 - r, c))\n        src = gather(src, dim=-2, index=src_idx.expand(n, r, c))\n        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n\n        return torch.cat([unm, dst], dim=1)\n\n    def unmerge(x: torch.Tensor) -> torch.Tensor:\n        unm_len = unm_idx.shape[1]\n        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n        _, _, c = unm.shape\n\n        src = gather(dst, dim=-2, index=dst_idx.expand(B, r, c))\n\n        # Combine back to the original shape\n        out = torch.zeros(B, N, c, device=x.device, dtype=x.dtype)\n        out.scatter_(dim=-2, index=b_idx.expand(B, num_dst, c), src=dst)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=unm_idx).expand(B, unm_len, c), src=unm)\n        out.scatter_(dim=-2, index=gather(a_idx.expand(B, a_idx.shape[1], 1), dim=1, index=src_idx).expand(B, r, c), src=src)\n\n        return out\n\n    return merge, unmerge\n\n\ndef get_functions(x, ratio, original_shape):\n    b, c, original_h, original_w = original_shape\n    original_tokens = original_h * original_w\n    downsample = int(math.ceil(math.sqrt(original_tokens // x.shape[1])))\n    stride_x = 2\n    stride_y = 2\n    max_downsample = 1\n\n    if downsample <= max_downsample:\n        w = int(math.ceil(original_w / downsample))\n        h = int(math.ceil(original_h / downsample))\n        r = int(x.shape[1] * ratio)\n        no_rand = False\n        m, u = bipartite_soft_matching_random2d(x, w, h, stride_x, stride_y, r, no_rand)\n        return m, u\n\n    nothing = lambda y: y\n    return nothing, nothing\n\n\n\nclass TomePatchModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, ratio):\n        self.u = None\n        def tomesd_m(q, k, v, extra_options):\n            #NOTE: In the reference code get_functions takes x (input of the transformer block) as the argument instead of q\n            #however from my basic testing it seems that using q instead gives better results\n            m, self.u = get_functions(q, ratio, extra_options[\"original_shape\"])\n            return m(q), k, v\n        def tomesd_u(n, extra_options):\n            return self.u(n)\n\n        m = model.clone()\n        m.set_model_attn1_patch(tomesd_m)\n        m.set_model_attn1_output_patch(tomesd_u)\n        return (m, )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"TomePatchModel\": TomePatchModel,\n}\n", "comfy_extras/nodes_differential_diffusion.py": "# code adapted from https://github.com/exx8/differential-diffusion\n\nimport torch\n\nclass DifferentialDiffusion():\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"model\": (\"MODEL\", ),\n                            }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"apply\"\n    CATEGORY = \"_for_testing\"\n    INIT = False\n\n    def apply(self, model):\n        model = model.clone()\n        model.set_model_denoise_mask_function(self.forward)\n        return (model,)\n\n    def forward(self, sigma: torch.Tensor, denoise_mask: torch.Tensor, extra_options: dict):\n        model = extra_options[\"model\"]\n        step_sigmas = extra_options[\"sigmas\"]\n        sigma_to = model.inner_model.model_sampling.sigma_min\n        if step_sigmas[-1] > sigma_to:\n            sigma_to = step_sigmas[-1]\n        sigma_from = step_sigmas[0]\n\n        ts_from = model.inner_model.model_sampling.timestep(sigma_from)\n        ts_to = model.inner_model.model_sampling.timestep(sigma_to)\n        current_ts = model.inner_model.model_sampling.timestep(sigma[0])\n\n        threshold = (current_ts - ts_to) / (ts_from - ts_to)\n\n        return (denoise_mask >= threshold).to(denoise_mask.dtype)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"DifferentialDiffusion\": DifferentialDiffusion,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"DifferentialDiffusion\": \"Differential Diffusion\",\n}\n", "comfy_extras/nodes_model_advanced.py": "import folder_paths\nimport comfy.sd\nimport comfy.model_sampling\nimport comfy.latent_formats\nimport torch\n\nclass LCM(comfy.model_sampling.EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        timestep = self.timestep(sigma).view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        x0 = model_input - model_output * sigma\n\n        sigma_data = 0.5\n        scaled_timestep = timestep * 10.0 #timestep_scaling\n\n        c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n        c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n\n        return c_out * x0 + c_skip * model_input\n\nclass X0(comfy.model_sampling.EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        return model_output\n\nclass ModelSamplingDiscreteDistilled(comfy.model_sampling.ModelSamplingDiscrete):\n    original_timesteps = 50\n\n    def __init__(self, model_config=None):\n        super().__init__(model_config)\n\n        self.skip_steps = self.num_timesteps // self.original_timesteps\n\n        sigmas_valid = torch.zeros((self.original_timesteps), dtype=torch.float32)\n        for x in range(self.original_timesteps):\n            sigmas_valid[self.original_timesteps - 1 - x] = self.sigmas[self.num_timesteps - 1 - x * self.skip_steps]\n\n        self.set_sigmas(sigmas_valid)\n\n    def timestep(self, sigma):\n        log_sigma = sigma.log()\n        dists = log_sigma.to(self.log_sigmas.device) - self.log_sigmas[:, None]\n        return (dists.abs().argmin(dim=0).view(sigma.shape) * self.skip_steps + (self.skip_steps - 1)).to(sigma.device)\n\n    def sigma(self, timestep):\n        t = torch.clamp(((timestep.float().to(self.log_sigmas.device) - (self.skip_steps - 1)) / self.skip_steps).float(), min=0, max=(len(self.sigmas) - 1))\n        low_idx = t.floor().long()\n        high_idx = t.ceil().long()\n        w = t.frac()\n        log_sigma = (1 - w) * self.log_sigmas[low_idx] + w * self.log_sigmas[high_idx]\n        return log_sigma.exp().to(timestep.device)\n\n\ndef rescale_zero_terminal_snr_sigmas(sigmas):\n    alphas_cumprod = 1 / ((sigmas * sigmas) + 1)\n    alphas_bar_sqrt = alphas_cumprod.sqrt()\n\n    # Store old values.\n    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n\n    # Shift so the last timestep is zero.\n    alphas_bar_sqrt -= (alphas_bar_sqrt_T)\n\n    # Scale so the first timestep is back to the old value.\n    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n\n    # Convert alphas_bar_sqrt to betas\n    alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\n    alphas_bar[-1] = 4.8973451890853435e-08\n    return ((1 - alphas_bar) / alphas_bar) ** 0.5\n\nclass ModelSamplingDiscrete:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"eps\", \"v_prediction\", \"lcm\", \"x0\"],),\n                              \"zsnr\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, sampling, zsnr):\n        m = model.clone()\n\n        sampling_base = comfy.model_sampling.ModelSamplingDiscrete\n        if sampling == \"eps\":\n            sampling_type = comfy.model_sampling.EPS\n        elif sampling == \"v_prediction\":\n            sampling_type = comfy.model_sampling.V_PREDICTION\n        elif sampling == \"lcm\":\n            sampling_type = LCM\n            sampling_base = ModelSamplingDiscreteDistilled\n        elif sampling == \"x0\":\n            sampling_type = X0\n\n        class ModelSamplingAdvanced(sampling_base, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        if zsnr:\n            model_sampling.set_sigmas(rescale_zero_terminal_snr_sigmas(model_sampling.sigmas))\n\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        return (m, )\n\nclass ModelSamplingStableCascade:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"shift\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, shift):\n        m = model.clone()\n\n        sampling_base = comfy.model_sampling.StableCascadeSampling\n        sampling_type = comfy.model_sampling.EPS\n\n        class ModelSamplingAdvanced(sampling_base, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        model_sampling.set_parameters(shift)\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        return (m, )\n\nclass ModelSamplingSD3:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"shift\": (\"FLOAT\", {\"default\": 3.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, shift):\n        m = model.clone()\n\n        sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow\n        sampling_type = comfy.model_sampling.CONST\n\n        class ModelSamplingAdvanced(sampling_base, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        model_sampling.set_parameters(shift=shift)\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        return (m, )\n\nclass ModelSamplingContinuousEDM:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"v_prediction\", \"edm_playground_v2.5\", \"eps\"],),\n                              \"sigma_max\": (\"FLOAT\", {\"default\": 120.0, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              \"sigma_min\": (\"FLOAT\", {\"default\": 0.002, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, sampling, sigma_max, sigma_min):\n        m = model.clone()\n\n        latent_format = None\n        sigma_data = 1.0\n        if sampling == \"eps\":\n            sampling_type = comfy.model_sampling.EPS\n        elif sampling == \"v_prediction\":\n            sampling_type = comfy.model_sampling.V_PREDICTION\n        elif sampling == \"edm_playground_v2.5\":\n            sampling_type = comfy.model_sampling.EDM\n            sigma_data = 0.5\n            latent_format = comfy.latent_formats.SDXL_Playground_2_5()\n\n        class ModelSamplingAdvanced(comfy.model_sampling.ModelSamplingContinuousEDM, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        model_sampling.set_parameters(sigma_min, sigma_max, sigma_data)\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        if latent_format is not None:\n            m.add_object_patch(\"latent_format\", latent_format)\n        return (m, )\n\nclass ModelSamplingContinuousV:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"v_prediction\"],),\n                              \"sigma_max\": (\"FLOAT\", {\"default\": 500.0, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              \"sigma_min\": (\"FLOAT\", {\"default\": 0.03, \"min\": 0.0, \"max\": 1000.0, \"step\":0.001, \"round\": False}),\n                              }}\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, sampling, sigma_max, sigma_min):\n        m = model.clone()\n\n        latent_format = None\n        sigma_data = 1.0\n        if sampling == \"v_prediction\":\n            sampling_type = comfy.model_sampling.V_PREDICTION\n\n        class ModelSamplingAdvanced(comfy.model_sampling.ModelSamplingContinuousV, sampling_type):\n            pass\n\n        model_sampling = ModelSamplingAdvanced(model.model.model_config)\n        model_sampling.set_parameters(sigma_min, sigma_max, sigma_data)\n        m.add_object_patch(\"model_sampling\", model_sampling)\n        return (m, )\n\nclass RescaleCFG:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 0.7, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"advanced/model\"\n\n    def patch(self, model, multiplier):\n        def rescale_cfg(args):\n            cond = args[\"cond\"]\n            uncond = args[\"uncond\"]\n            cond_scale = args[\"cond_scale\"]\n            sigma = args[\"sigma\"]\n            sigma = sigma.view(sigma.shape[:1] + (1,) * (cond.ndim - 1))\n            x_orig = args[\"input\"]\n\n            #rescale cfg has to be done on v-pred model output\n            x = x_orig / (sigma * sigma + 1.0)\n            cond = ((x - (x_orig - cond)) * (sigma ** 2 + 1.0) ** 0.5) / (sigma)\n            uncond = ((x - (x_orig - uncond)) * (sigma ** 2 + 1.0) ** 0.5) / (sigma)\n\n            #rescalecfg\n            x_cfg = uncond + cond_scale * (cond - uncond)\n            ro_pos = torch.std(cond, dim=(1,2,3), keepdim=True)\n            ro_cfg = torch.std(x_cfg, dim=(1,2,3), keepdim=True)\n\n            x_rescaled = x_cfg * (ro_pos / ro_cfg)\n            x_final = multiplier * x_rescaled + (1.0 - multiplier) * x_cfg\n\n            return x_orig - (x - x_final * sigma / (sigma * sigma + 1.0) ** 0.5)\n\n        m = model.clone()\n        m.set_model_sampler_cfg_function(rescale_cfg)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"ModelSamplingDiscrete\": ModelSamplingDiscrete,\n    \"ModelSamplingContinuousEDM\": ModelSamplingContinuousEDM,\n    \"ModelSamplingContinuousV\": ModelSamplingContinuousV,\n    \"ModelSamplingStableCascade\": ModelSamplingStableCascade,\n    \"ModelSamplingSD3\": ModelSamplingSD3,\n    \"RescaleCFG\": RescaleCFG,\n}\n", "comfy_extras/nodes_advanced_samplers.py": "import comfy.samplers\nimport comfy.utils\nimport torch\nimport numpy as np\nfrom tqdm.auto import trange, tqdm\nimport math\n\n\n@torch.no_grad()\ndef sample_lcm_upscale(model, x, sigmas, extra_args=None, callback=None, disable=None, total_upscale=2.0, upscale_method=\"bislerp\", upscale_steps=None):\n    extra_args = {} if extra_args is None else extra_args\n\n    if upscale_steps is None:\n        upscale_steps = max(len(sigmas) // 2 + 1, 2)\n    else:\n        upscale_steps += 1\n        upscale_steps = min(upscale_steps, len(sigmas) + 1)\n\n    upscales = np.linspace(1.0, total_upscale, upscale_steps)[1:]\n\n    orig_shape = x.size()\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        x = denoised\n        if i < len(upscales):\n            x = comfy.utils.common_upscale(x, round(orig_shape[-1] * upscales[i]), round(orig_shape[-2] * upscales[i]), upscale_method, \"disabled\")\n\n        if sigmas[i + 1] > 0:\n            x += sigmas[i + 1] * torch.randn_like(x)\n    return x\n\n\nclass SamplerLCMUpscale:\n    upscale_methods = [\"bislerp\", \"nearest-exact\", \"bilinear\", \"area\", \"bicubic\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"scale_ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.1, \"max\": 20.0, \"step\": 0.01}),\n                     \"scale_steps\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 1000, \"step\": 1}),\n                     \"upscale_method\": (s.upscale_methods,),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, scale_ratio, scale_steps, upscale_method):\n        if scale_steps < 0:\n            scale_steps = None\n        sampler = comfy.samplers.KSAMPLER(sample_lcm_upscale, extra_options={\"total_upscale\": scale_ratio, \"upscale_steps\": scale_steps, \"upscale_method\": upscale_method})\n        return (sampler, )\n\nfrom comfy.k_diffusion.sampling import to_d\nimport comfy.model_patcher\n\n@torch.no_grad()\ndef sample_euler_cfgpp(model, x, sigmas, extra_args=None, callback=None, disable=None):\n    extra_args = {} if extra_args is None else extra_args\n\n    temp = [0]\n    def post_cfg_function(args):\n        temp[0] = args[\"uncond_denoised\"]\n        return args[\"denoised\"]\n\n    model_options = extra_args.get(\"model_options\", {}).copy()\n    extra_args[\"model_options\"] = comfy.model_patcher.set_model_options_post_cfg_function(model_options, post_cfg_function, disable_cfg1_optimization=True)\n\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_hat = sigmas[i]\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, temp[0])\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        x = denoised + sigmas[i + 1] * d\n    return x\n\n@torch.no_grad()\ndef sample_euler_cfgpp_alt(model, x, sigmas, extra_args=None, callback=None, disable=None):\n    extra_args = {} if extra_args is None else extra_args\n\n    temp = [0]\n    def post_cfg_function(args):\n        temp[0] = args[\"uncond_denoised\"]\n        return args[\"denoised\"]\n\n    model_options = extra_args.get(\"model_options\", {}).copy()\n    extra_args[\"model_options\"] = comfy.model_patcher.set_model_options_post_cfg_function(model_options, post_cfg_function, disable_cfg1_optimization=True)\n\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_hat = sigmas[i]\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x - denoised + temp[0], sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + d * dt\n    return x\n\nclass SamplerEulerCFGpp:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"version\": ([\"regular\", \"alternative\"],),}\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    # CATEGORY = \"sampling/custom_sampling/samplers\"\n    CATEGORY = \"_for_testing\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, version):\n        if version == \"regular\":\n            sampler = comfy.samplers.KSAMPLER(sample_euler_cfgpp)\n        else:\n            sampler = comfy.samplers.KSAMPLER(sample_euler_cfgpp_alt)\n        return (sampler, )\n\nNODE_CLASS_MAPPINGS = {\n    \"SamplerLCMUpscale\": SamplerLCMUpscale,\n    \"SamplerEulerCFGpp\": SamplerEulerCFGpp,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"SamplerEulerCFGpp\": \"SamplerEulerCFG++\",\n}\n", "comfy_extras/nodes_cond.py": "\n\nclass CLIPTextEncodeControlnet:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"clip\": (\"CLIP\", ), \"conditioning\": (\"CONDITIONING\", ), \"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True})}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"_for_testing/conditioning\"\n\n    def encode(self, clip, conditioning, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        c = []\n        for t in conditioning:\n            n = [t[0], t[1].copy()]\n            n[1]['cross_attn_controlnet'] = cond\n            n[1]['pooled_output_controlnet'] = pooled\n            c.append(n)\n        return (c, )\n\nNODE_CLASS_MAPPINGS = {\n    \"CLIPTextEncodeControlnet\": CLIPTextEncodeControlnet\n}\n", "comfy_extras/nodes_align_your_steps.py": "#from: https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/howto.html\nimport numpy as np\nimport torch\n\ndef loglinear_interp(t_steps, num_steps):\n    \"\"\"\n    Performs log-linear interpolation of a given array of decreasing numbers.\n    \"\"\"\n    xs = np.linspace(0, 1, len(t_steps))\n    ys = np.log(t_steps[::-1])\n\n    new_xs = np.linspace(0, 1, num_steps)\n    new_ys = np.interp(new_xs, xs, ys)\n\n    interped_ys = np.exp(new_ys)[::-1].copy()\n    return interped_ys\n\nNOISE_LEVELS = {\"SD1\": [14.6146412293, 6.4745760956,  3.8636745985,  2.6946151520, 1.8841921177,  1.3943805092,  0.9642583904,  0.6523686016, 0.3977456272,  0.1515232662,  0.0291671582],\n                \"SDXL\":[14.6146412293, 6.3184485287,  3.7681790315,  2.1811480769, 1.3405244945,  0.8620721141,  0.5550693289,  0.3798540708, 0.2332364134,  0.1114188177,  0.0291671582],\n                \"SVD\": [700.00, 54.5, 15.886, 7.977, 4.248, 1.789, 0.981, 0.403, 0.173, 0.034, 0.002]}\n\nclass AlignYourStepsScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model_type\": ([\"SD1\", \"SDXL\", \"SVD\"], ),\n                     \"steps\": (\"INT\", {\"default\": 10, \"min\": 10, \"max\": 10000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model_type, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            if denoise <= 0.0:\n                return (torch.FloatTensor([]),)\n            total_steps = round(steps * denoise)\n\n        sigmas = NOISE_LEVELS[model_type][:]\n        if (steps + 1) != len(sigmas):\n            sigmas = loglinear_interp(sigmas, steps + 1)\n\n        sigmas = sigmas[-(total_steps + 1):]\n        sigmas[-1] = 0\n        return (torch.FloatTensor(sigmas), )\n\nNODE_CLASS_MAPPINGS = {\n    \"AlignYourStepsScheduler\": AlignYourStepsScheduler,\n}\n", "comfy_extras/nodes_sd3.py": "import folder_paths\nimport comfy.sd\nimport comfy.model_management\nimport nodes\nimport torch\n\nclass TripleCLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name1\": (folder_paths.get_filename_list(\"clip\"), ), \"clip_name2\": (folder_paths.get_filename_list(\"clip\"), ), \"clip_name3\": (folder_paths.get_filename_list(\"clip\"), )\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_clip(self, clip_name1, clip_name2, clip_name3):\n        clip_path1 = folder_paths.get_full_path(\"clip\", clip_name1)\n        clip_path2 = folder_paths.get_full_path(\"clip\", clip_name2)\n        clip_path3 = folder_paths.get_full_path(\"clip\", clip_name3)\n        clip = comfy.sd.load_clip(ckpt_paths=[clip_path1, clip_path2, clip_path3], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n        return (clip,)\n\nclass EmptySD3LatentImage:\n    def __init__(self):\n        self.device = comfy.model_management.intermediate_device()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent/sd3\"\n\n    def generate(self, width, height, batch_size=1):\n        latent = torch.ones([batch_size, 16, height // 8, width // 8], device=self.device) * 0.0609\n        return ({\"samples\":latent}, )\n\nclass CLIPTextEncodeSD3:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"clip\": (\"CLIP\", ),\n            \"clip_l\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}),\n            \"clip_g\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}),\n            \"t5xxl\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}),\n            \"empty_padding\": ([\"none\", \"empty_prompt\"], )\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def encode(self, clip, clip_l, clip_g, t5xxl, empty_padding):\n        no_padding = empty_padding == \"none\"\n\n        tokens = clip.tokenize(clip_g)\n        if len(clip_g) == 0 and no_padding:\n            tokens[\"g\"] = []\n\n        if len(clip_l) == 0 and no_padding:\n            tokens[\"l\"] = []\n        else:\n            tokens[\"l\"] = clip.tokenize(clip_l)[\"l\"]\n\n        if len(t5xxl) == 0 and no_padding:\n            tokens[\"t5xxl\"] =  []\n        else:\n            tokens[\"t5xxl\"] = clip.tokenize(t5xxl)[\"t5xxl\"]\n        if len(tokens[\"l\"]) != len(tokens[\"g\"]):\n            empty = clip.tokenize(\"\")\n            while len(tokens[\"l\"]) < len(tokens[\"g\"]):\n                tokens[\"l\"] += empty[\"l\"]\n            while len(tokens[\"l\"]) > len(tokens[\"g\"]):\n                tokens[\"g\"] += empty[\"g\"]\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled}]], )\n\n\nNODE_CLASS_MAPPINGS = {\n    \"TripleCLIPLoader\": TripleCLIPLoader,\n    \"EmptySD3LatentImage\": EmptySD3LatentImage,\n    \"CLIPTextEncodeSD3\": CLIPTextEncodeSD3,\n}\n", "comfy_extras/nodes_custom_sampler.py": "import comfy.samplers\nimport comfy.sample\nfrom comfy.k_diffusion import sampling as k_diffusion_sampling\nimport latent_preview\nimport torch\nimport comfy.utils\nimport node_helpers\n\n\nclass BasicScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"scheduler\": (comfy.samplers.SCHEDULER_NAMES, ),\n                     \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, scheduler, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            if denoise <= 0.0:\n                return (torch.FloatTensor([]),)\n            total_steps = int(steps/denoise)\n\n        sigmas = comfy.samplers.calculate_sigmas(model.get_model_object(\"model_sampling\"), scheduler, total_steps).cpu()\n        sigmas = sigmas[-(steps + 1):]\n        return (sigmas, )\n\n\nclass KarrasScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 7.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min, rho):\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)\n        return (sigmas, )\n\nclass ExponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min):\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=sigma_min, sigma_max=sigma_max)\n        return (sigmas, )\n\nclass PolyexponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, sigma_max, sigma_min, rho):\n        sigmas = k_diffusion_sampling.get_sigmas_polyexponential(n=steps, sigma_min=sigma_min, sigma_max=sigma_max, rho=rho)\n        return (sigmas, )\n\nclass SDTurboScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"steps\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 10}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, steps, denoise):\n        start_step = 10 - int(10 * denoise)\n        timesteps = torch.flip(torch.arange(1, 11) * 100 - 1, (0,))[start_step:start_step + steps]\n        sigmas = model.get_model_object(\"model_sampling\").sigma(timesteps)\n        sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])\n        return (sigmas, )\n\nclass VPScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"beta_d\": (\"FLOAT\", {\"default\": 19.9, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}), #TODO: fix default values\n                     \"beta_min\": (\"FLOAT\", {\"default\": 0.1, \"min\": 0.0, \"max\": 5000.0, \"step\":0.01, \"round\": False}),\n                     \"eps_s\": (\"FLOAT\", {\"default\": 0.001, \"min\": 0.0, \"max\": 1.0, \"step\":0.0001, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, steps, beta_d, beta_min, eps_s):\n        sigmas = k_diffusion_sampling.get_sigmas_vp(n=steps, beta_d=beta_d, beta_min=beta_min, eps_s=eps_s)\n        return (sigmas, )\n\nclass SplitSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                    \"step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",\"SIGMAS\")\n    RETURN_NAMES = (\"high_sigmas\", \"low_sigmas\")\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, sigmas, step):\n        sigmas1 = sigmas[:step + 1]\n        sigmas2 = sigmas[step:]\n        return (sigmas1, sigmas2)\n\nclass SplitSigmasDenoise:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                    \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",\"SIGMAS\")\n    RETURN_NAMES = (\"high_sigmas\", \"low_sigmas\")\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, sigmas, denoise):\n        steps = max(sigmas.shape[-1] - 1, 0)\n        total_steps = round(steps * denoise)\n        sigmas1 = sigmas[:-(total_steps)]\n        sigmas2 = sigmas[-(total_steps + 1):]\n        return (sigmas1, sigmas2)\n\nclass FlipSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, sigmas):\n        if len(sigmas) == 0:\n            return (sigmas,)\n\n        sigmas = sigmas.flip(0)\n        if sigmas[0] == 0:\n            sigmas[0] = 0.0001\n        return (sigmas,)\n\nclass KSamplerSelect:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sampler_name\": (comfy.samplers.SAMPLER_NAMES, ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, sampler_name):\n        sampler = comfy.samplers.sampler_object(sampler_name)\n        return (sampler, )\n\nclass SamplerDPMPP_3M_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, eta, s_noise, noise_device):\n        if noise_device == 'cpu':\n            sampler_name = \"dpmpp_3m_sde\"\n        else:\n            sampler_name = \"dpmpp_3m_sde_gpu\"\n        sampler = comfy.samplers.ksampler(sampler_name, {\"eta\": eta, \"s_noise\": s_noise})\n        return (sampler, )\n\nclass SamplerDPMPP_2M_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"solver_type\": (['midpoint', 'heun'], ),\n                     \"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, solver_type, eta, s_noise, noise_device):\n        if noise_device == 'cpu':\n            sampler_name = \"dpmpp_2m_sde\"\n        else:\n            sampler_name = \"dpmpp_2m_sde_gpu\"\n        sampler = comfy.samplers.ksampler(sampler_name, {\"eta\": eta, \"s_noise\": s_noise, \"solver_type\": solver_type})\n        return (sampler, )\n\n\nclass SamplerDPMPP_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"r\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, eta, s_noise, r, noise_device):\n        if noise_device == 'cpu':\n            sampler_name = \"dpmpp_sde\"\n        else:\n            sampler_name = \"dpmpp_sde_gpu\"\n        sampler = comfy.samplers.ksampler(sampler_name, {\"eta\": eta, \"s_noise\": s_noise, \"r\": r})\n        return (sampler, )\n\nclass SamplerEulerAncestral:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, eta, s_noise):\n        sampler = comfy.samplers.ksampler(\"euler_ancestral\", {\"eta\": eta, \"s_noise\": s_noise})\n        return (sampler, )\n\nclass SamplerLMS:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"order\": (\"INT\", {\"default\": 4, \"min\": 1, \"max\": 100}),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, order):\n        sampler = comfy.samplers.ksampler(\"lms\", {\"order\": order})\n        return (sampler, )\n\nclass SamplerDPMAdaptative:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"order\": (\"INT\", {\"default\": 3, \"min\": 2, \"max\": 3}),\n                     \"rtol\": (\"FLOAT\", {\"default\": 0.05, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"atol\": (\"FLOAT\", {\"default\": 0.0078, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"h_init\": (\"FLOAT\", {\"default\": 0.05, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"pcoeff\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"icoeff\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"dcoeff\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"accept_safety\": (\"FLOAT\", {\"default\": 0.81, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"eta\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise):\n        sampler = comfy.samplers.ksampler(\"dpm_adaptive\", {\"order\": order, \"rtol\": rtol, \"atol\": atol, \"h_init\": h_init, \"pcoeff\": pcoeff,\n                                                              \"icoeff\": icoeff, \"dcoeff\": dcoeff, \"accept_safety\": accept_safety, \"eta\": eta,\n                                                              \"s_noise\":s_noise })\n        return (sampler, )\n\nclass Noise_EmptyNoise:\n    def __init__(self):\n        self.seed = 0\n\n    def generate_noise(self, input_latent):\n        latent_image = input_latent[\"samples\"]\n        return torch.zeros(latent_image.shape, dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n\n\nclass Noise_RandomNoise:\n    def __init__(self, seed):\n        self.seed = seed\n\n    def generate_noise(self, input_latent):\n        latent_image = input_latent[\"samples\"]\n        batch_inds = input_latent[\"batch_index\"] if \"batch_index\" in input_latent else None\n        return comfy.sample.prepare_noise(latent_image, self.seed, batch_inds)\n\nclass SamplerCustom:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": (\"BOOLEAN\", {\"default\": True}),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"sampler\": (\"SAMPLER\", ),\n                    \"sigmas\": (\"SIGMAS\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",\"LATENT\")\n    RETURN_NAMES = (\"output\", \"denoised_output\")\n\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling/custom_sampling\"\n\n    def sample(self, model, add_noise, noise_seed, cfg, positive, negative, sampler, sigmas, latent_image):\n        latent = latent_image\n        latent_image = latent[\"samples\"]\n        latent = latent.copy()\n        latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n        latent[\"samples\"] = latent_image\n\n        if not add_noise:\n            noise = Noise_EmptyNoise().generate_noise(latent)\n        else:\n            noise = Noise_RandomNoise(noise_seed).generate_noise(latent)\n\n        noise_mask = None\n        if \"noise_mask\" in latent:\n            noise_mask = latent[\"noise_mask\"]\n\n        x0_output = {}\n        callback = latent_preview.prepare_callback(model, sigmas.shape[-1] - 1, x0_output)\n\n        disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n        samples = comfy.sample.sample_custom(model, noise, cfg, sampler, sigmas, positive, negative, latent_image, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise_seed)\n\n        out = latent.copy()\n        out[\"samples\"] = samples\n        if \"x0\" in x0_output:\n            out_denoised = latent.copy()\n            out_denoised[\"samples\"] = model.model.process_latent_out(x0_output[\"x0\"].cpu())\n        else:\n            out_denoised = out\n        return (out, out_denoised)\n\nclass Guider_Basic(comfy.samplers.CFGGuider):\n    def set_conds(self, positive):\n        self.inner_set_conds({\"positive\": positive})\n\nclass BasicGuider:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"conditioning\": (\"CONDITIONING\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"GUIDER\",)\n\n    FUNCTION = \"get_guider\"\n    CATEGORY = \"sampling/custom_sampling/guiders\"\n\n    def get_guider(self, model, conditioning):\n        guider = Guider_Basic(model)\n        guider.set_conds(conditioning)\n        return (guider,)\n\nclass CFGGuider:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                     }\n                }\n\n    RETURN_TYPES = (\"GUIDER\",)\n\n    FUNCTION = \"get_guider\"\n    CATEGORY = \"sampling/custom_sampling/guiders\"\n\n    def get_guider(self, model, positive, negative, cfg):\n        guider = comfy.samplers.CFGGuider(model)\n        guider.set_conds(positive, negative)\n        guider.set_cfg(cfg)\n        return (guider,)\n\nclass Guider_DualCFG(comfy.samplers.CFGGuider):\n    def set_cfg(self, cfg1, cfg2):\n        self.cfg1 = cfg1\n        self.cfg2 = cfg2\n\n    def set_conds(self, positive, middle, negative):\n        middle = node_helpers.conditioning_set_values(middle, {\"prompt_type\": \"negative\"})\n        self.inner_set_conds({\"positive\": positive, \"middle\": middle, \"negative\": negative})\n\n    def predict_noise(self, x, timestep, model_options={}, seed=None):\n        negative_cond = self.conds.get(\"negative\", None)\n        middle_cond = self.conds.get(\"middle\", None)\n\n        out = comfy.samplers.calc_cond_batch(self.inner_model, [negative_cond, middle_cond, self.conds.get(\"positive\", None)], x, timestep, model_options)\n        return comfy.samplers.cfg_function(self.inner_model, out[1], out[0], self.cfg2, x, timestep, model_options=model_options, cond=middle_cond, uncond=negative_cond) + (out[2] - out[1]) * self.cfg1\n\nclass DualCFGGuider:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"cond1\": (\"CONDITIONING\", ),\n                    \"cond2\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"cfg_conds\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"cfg_cond2_negative\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                     }\n                }\n\n    RETURN_TYPES = (\"GUIDER\",)\n\n    FUNCTION = \"get_guider\"\n    CATEGORY = \"sampling/custom_sampling/guiders\"\n\n    def get_guider(self, model, cond1, cond2, negative, cfg_conds, cfg_cond2_negative):\n        guider = Guider_DualCFG(model)\n        guider.set_conds(cond1, cond2, negative)\n        guider.set_cfg(cfg_conds, cfg_cond2_negative)\n        return (guider,)\n\nclass DisableNoise:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":{\n                     }\n                }\n\n    RETURN_TYPES = (\"NOISE\",)\n    FUNCTION = \"get_noise\"\n    CATEGORY = \"sampling/custom_sampling/noise\"\n\n    def get_noise(self):\n        return (Noise_EmptyNoise(),)\n\n\nclass RandomNoise(DisableNoise):\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":{\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                     }\n                }\n\n    def get_noise(self, noise_seed):\n        return (Noise_RandomNoise(noise_seed),)\n\n\nclass SamplerCustomAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"noise\": (\"NOISE\", ),\n                    \"guider\": (\"GUIDER\", ),\n                    \"sampler\": (\"SAMPLER\", ),\n                    \"sigmas\": (\"SIGMAS\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",\"LATENT\")\n    RETURN_NAMES = (\"output\", \"denoised_output\")\n\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling/custom_sampling\"\n\n    def sample(self, noise, guider, sampler, sigmas, latent_image):\n        latent = latent_image\n        latent_image = latent[\"samples\"]\n        latent = latent.copy()\n        latent_image = comfy.sample.fix_empty_latent_channels(guider.model_patcher, latent_image)\n        latent[\"samples\"] = latent_image\n\n        noise_mask = None\n        if \"noise_mask\" in latent:\n            noise_mask = latent[\"noise_mask\"]\n\n        x0_output = {}\n        callback = latent_preview.prepare_callback(guider.model_patcher, sigmas.shape[-1] - 1, x0_output)\n\n        disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n        samples = guider.sample(noise.generate_noise(latent), latent_image, sampler, sigmas, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise.seed)\n        samples = samples.to(comfy.model_management.intermediate_device())\n\n        out = latent.copy()\n        out[\"samples\"] = samples\n        if \"x0\" in x0_output:\n            out_denoised = latent.copy()\n            out_denoised[\"samples\"] = guider.model_patcher.model.process_latent_out(x0_output[\"x0\"].cpu())\n        else:\n            out_denoised = out\n        return (out, out_denoised)\n\nclass AddNoise:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"noise\": (\"NOISE\", ),\n                     \"sigmas\": (\"SIGMAS\", ),\n                     \"latent_image\": (\"LATENT\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",)\n\n    FUNCTION = \"add_noise\"\n\n    CATEGORY = \"_for_testing/custom_sampling/noise\"\n\n    def add_noise(self, model, noise, sigmas, latent_image):\n        if len(sigmas) == 0:\n            return latent_image\n\n        latent = latent_image\n        latent_image = latent[\"samples\"]\n\n        noisy = noise.generate_noise(latent)\n\n        model_sampling = model.get_model_object(\"model_sampling\")\n        process_latent_out = model.get_model_object(\"process_latent_out\")\n        process_latent_in = model.get_model_object(\"process_latent_in\")\n\n        if len(sigmas) > 1:\n            scale = torch.abs(sigmas[0] - sigmas[-1])\n        else:\n            scale = sigmas[0]\n\n        if torch.count_nonzero(latent_image) > 0: #Don't shift the empty latent image.\n            latent_image = process_latent_in(latent_image)\n        noisy = model_sampling.noise_scaling(scale, noisy, latent_image)\n        noisy = process_latent_out(noisy)\n        noisy = torch.nan_to_num(noisy, nan=0.0, posinf=0.0, neginf=0.0)\n\n        out = latent.copy()\n        out[\"samples\"] = noisy\n        return (out,)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"SamplerCustom\": SamplerCustom,\n    \"BasicScheduler\": BasicScheduler,\n    \"KarrasScheduler\": KarrasScheduler,\n    \"ExponentialScheduler\": ExponentialScheduler,\n    \"PolyexponentialScheduler\": PolyexponentialScheduler,\n    \"VPScheduler\": VPScheduler,\n    \"SDTurboScheduler\": SDTurboScheduler,\n    \"KSamplerSelect\": KSamplerSelect,\n    \"SamplerEulerAncestral\": SamplerEulerAncestral,\n    \"SamplerLMS\": SamplerLMS,\n    \"SamplerDPMPP_3M_SDE\": SamplerDPMPP_3M_SDE,\n    \"SamplerDPMPP_2M_SDE\": SamplerDPMPP_2M_SDE,\n    \"SamplerDPMPP_SDE\": SamplerDPMPP_SDE,\n    \"SamplerDPMAdaptative\": SamplerDPMAdaptative,\n    \"SplitSigmas\": SplitSigmas,\n    \"SplitSigmasDenoise\": SplitSigmasDenoise,\n    \"FlipSigmas\": FlipSigmas,\n\n    \"CFGGuider\": CFGGuider,\n    \"DualCFGGuider\": DualCFGGuider,\n    \"BasicGuider\": BasicGuider,\n    \"RandomNoise\": RandomNoise,\n    \"DisableNoise\": DisableNoise,\n    \"AddNoise\": AddNoise,\n    \"SamplerCustomAdvanced\": SamplerCustomAdvanced,\n}\n", "comfy_extras/nodes_perpneg.py": "import torch\nimport comfy.model_management\nimport comfy.sampler_helpers\nimport comfy.samplers\nimport comfy.utils\nimport node_helpers\n\ndef perp_neg(x, noise_pred_pos, noise_pred_neg, noise_pred_nocond, neg_scale, cond_scale):\n    pos = noise_pred_pos - noise_pred_nocond\n    neg = noise_pred_neg - noise_pred_nocond\n\n    perp = neg - ((torch.mul(neg, pos).sum())/(torch.norm(pos)**2)) * pos\n    perp_neg = perp * neg_scale\n    cfg_result = noise_pred_nocond + cond_scale*(pos - perp_neg)\n    return cfg_result\n\n#TODO: This node should be removed, it has been replaced with PerpNegGuider\nclass PerpNeg:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"model\": (\"MODEL\", ),\n                             \"empty_conditioning\": (\"CONDITIONING\", ),\n                             \"neg_scale\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\": 0.01}),\n                            }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, empty_conditioning, neg_scale):\n        m = model.clone()\n        nocond = comfy.sampler_helpers.convert_cond(empty_conditioning)\n\n        def cfg_function(args):\n            model = args[\"model\"]\n            noise_pred_pos = args[\"cond_denoised\"]\n            noise_pred_neg = args[\"uncond_denoised\"]\n            cond_scale = args[\"cond_scale\"]\n            x = args[\"input\"]\n            sigma = args[\"sigma\"]\n            model_options = args[\"model_options\"]\n            nocond_processed = comfy.samplers.encode_model_conds(model.extra_conds, nocond, x, x.device, \"negative\")\n\n            (noise_pred_nocond,) = comfy.samplers.calc_cond_batch(model, [nocond_processed], x, sigma, model_options)\n\n            cfg_result = x - perp_neg(x, noise_pred_pos, noise_pred_neg, noise_pred_nocond, neg_scale, cond_scale)\n            return cfg_result\n\n        m.set_model_sampler_cfg_function(cfg_function)\n\n        return (m, )\n\n\nclass Guider_PerpNeg(comfy.samplers.CFGGuider):\n    def set_conds(self, positive, negative, empty_negative_prompt):\n        empty_negative_prompt = node_helpers.conditioning_set_values(empty_negative_prompt, {\"prompt_type\": \"negative\"})\n        self.inner_set_conds({\"positive\": positive, \"empty_negative_prompt\": empty_negative_prompt, \"negative\": negative})\n\n    def set_cfg(self, cfg, neg_scale):\n        self.cfg = cfg\n        self.neg_scale = neg_scale\n\n    def predict_noise(self, x, timestep, model_options={}, seed=None):\n        # in CFGGuider.predict_noise, we call sampling_function(), which uses cfg_function() to compute pos & neg\n        # but we'd rather do a single batch of sampling pos, neg, and empty, so we call calc_cond_batch([pos,neg,empty]) directly\n        \n        positive_cond = self.conds.get(\"positive\", None)\n        negative_cond = self.conds.get(\"negative\", None)\n        empty_cond = self.conds.get(\"empty_negative_prompt\", None)\n\n        (noise_pred_pos, noise_pred_neg, noise_pred_empty) = \\\n            comfy.samplers.calc_cond_batch(self.inner_model, [positive_cond, negative_cond, empty_cond], x, timestep, model_options)\n        cfg_result = perp_neg(x, noise_pred_pos, noise_pred_neg, noise_pred_empty, self.neg_scale, self.cfg)\n\n        # normally this would be done in cfg_function, but we skipped \n        # that for efficiency: we can compute the noise predictions in\n        # a single call to calc_cond_batch() (rather than two)\n        # so we replicate the hook here\n        for fn in model_options.get(\"sampler_post_cfg_function\", []):\n            args = {\n                \"denoised\": cfg_result,\n                \"cond\": positive_cond,\n                \"uncond\": negative_cond,\n                \"model\": self.inner_model,\n                \"uncond_denoised\": noise_pred_neg,\n                \"cond_denoised\": noise_pred_pos,\n                \"sigma\": timestep,\n                \"model_options\": model_options,\n                \"input\": x,\n                # not in the original call in samplers.py:cfg_function, but made available for future hooks\n                \"empty_cond\": empty_cond,\n                \"empty_cond_denoised\": noise_pred_empty,}\n            cfg_result = fn(args)\n\n        return cfg_result\n\nclass PerpNegGuider:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),\n                    \"empty_conditioning\": (\"CONDITIONING\", ),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"neg_scale\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\": 0.01}),\n                     }\n                }\n\n    RETURN_TYPES = (\"GUIDER\",)\n\n    FUNCTION = \"get_guider\"\n    CATEGORY = \"_for_testing\"\n\n    def get_guider(self, model, positive, negative, empty_conditioning, cfg, neg_scale):\n        guider = Guider_PerpNeg(model)\n        guider.set_conds(positive, negative, empty_conditioning)\n        guider.set_cfg(cfg, neg_scale)\n        return (guider,)\n\nNODE_CLASS_MAPPINGS = {\n    \"PerpNeg\": PerpNeg,\n    \"PerpNegGuider\": PerpNegGuider,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"PerpNeg\": \"Perp-Neg (DEPRECATED by PerpNegGuider)\",\n}\n", "comfy_extras/nodes_clip_sdxl.py": "import torch\nfrom nodes import MAX_RESOLUTION\n\nclass CLIPTextEncodeSDXLRefiner:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"ascore\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}), \"clip\": (\"CLIP\", ),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def encode(self, clip, ascore, width, height, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled, \"aesthetic_score\": ascore, \"width\": width,\"height\": height}]], )\n\nclass CLIPTextEncodeSDXL:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_w\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_h\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"text_g\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}), \"clip\": (\"CLIP\", ),\n            \"text_l\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}), \"clip\": (\"CLIP\", ),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"advanced/conditioning\"\n\n    def encode(self, clip, width, height, crop_w, crop_h, target_width, target_height, text_g, text_l):\n        tokens = clip.tokenize(text_g)\n        tokens[\"l\"] = clip.tokenize(text_l)[\"l\"]\n        if len(tokens[\"l\"]) != len(tokens[\"g\"]):\n            empty = clip.tokenize(\"\")\n            while len(tokens[\"l\"]) < len(tokens[\"g\"]):\n                tokens[\"l\"] += empty[\"l\"]\n            while len(tokens[\"l\"]) > len(tokens[\"g\"]):\n                tokens[\"g\"] += empty[\"g\"]\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n        return ([[cond, {\"pooled_output\": pooled, \"width\": width, \"height\": height, \"crop_w\": crop_w, \"crop_h\": crop_h, \"target_width\": target_width, \"target_height\": target_height}]], )\n\nNODE_CLASS_MAPPINGS = {\n    \"CLIPTextEncodeSDXLRefiner\": CLIPTextEncodeSDXLRefiner,\n    \"CLIPTextEncodeSDXL\": CLIPTextEncodeSDXL,\n}\n", "comfy_extras/nodes_post_processing.py": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nimport math\n\nimport comfy.utils\nimport comfy.model_management\n\n\nclass Blend:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",),\n                \"image2\": (\"IMAGE\",),\n                \"blend_factor\": (\"FLOAT\", {\n                    \"default\": 0.5,\n                    \"min\": 0.0,\n                    \"max\": 1.0,\n                    \"step\": 0.01\n                }),\n                \"blend_mode\": ([\"normal\", \"multiply\", \"screen\", \"overlay\", \"soft_light\", \"difference\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"blend_images\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def blend_images(self, image1: torch.Tensor, image2: torch.Tensor, blend_factor: float, blend_mode: str):\n        image2 = image2.to(image1.device)\n        if image1.shape != image2.shape:\n            image2 = image2.permute(0, 3, 1, 2)\n            image2 = comfy.utils.common_upscale(image2, image1.shape[2], image1.shape[1], upscale_method='bicubic', crop='center')\n            image2 = image2.permute(0, 2, 3, 1)\n\n        blended_image = self.blend_mode(image1, image2, blend_mode)\n        blended_image = image1 * (1 - blend_factor) + blended_image * blend_factor\n        blended_image = torch.clamp(blended_image, 0, 1)\n        return (blended_image,)\n\n    def blend_mode(self, img1, img2, mode):\n        if mode == \"normal\":\n            return img2\n        elif mode == \"multiply\":\n            return img1 * img2\n        elif mode == \"screen\":\n            return 1 - (1 - img1) * (1 - img2)\n        elif mode == \"overlay\":\n            return torch.where(img1 <= 0.5, 2 * img1 * img2, 1 - 2 * (1 - img1) * (1 - img2))\n        elif mode == \"soft_light\":\n            return torch.where(img2 <= 0.5, img1 - (1 - 2 * img2) * img1 * (1 - img1), img1 + (2 * img2 - 1) * (self.g(img1) - img1))\n        elif mode == \"difference\":\n            return img1 - img2\n        else:\n            raise ValueError(f\"Unsupported blend mode: {mode}\")\n\n    def g(self, x):\n        return torch.where(x <= 0.25, ((16 * x - 12) * x + 4) * x, torch.sqrt(x))\n\ndef gaussian_kernel(kernel_size: int, sigma: float, device=None):\n    x, y = torch.meshgrid(torch.linspace(-1, 1, kernel_size, device=device), torch.linspace(-1, 1, kernel_size, device=device), indexing=\"ij\")\n    d = torch.sqrt(x * x + y * y)\n    g = torch.exp(-(d * d) / (2.0 * sigma * sigma))\n    return g / g.sum()\n\nclass Blur:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"blur_radius\": (\"INT\", {\n                    \"default\": 1,\n                    \"min\": 1,\n                    \"max\": 31,\n                    \"step\": 1\n                }),\n                \"sigma\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.1,\n                    \"max\": 10.0,\n                    \"step\": 0.1\n                }),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"blur\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def blur(self, image: torch.Tensor, blur_radius: int, sigma: float):\n        if blur_radius == 0:\n            return (image,)\n\n        image = image.to(comfy.model_management.get_torch_device())\n        batch_size, height, width, channels = image.shape\n\n        kernel_size = blur_radius * 2 + 1\n        kernel = gaussian_kernel(kernel_size, sigma, device=image.device).repeat(channels, 1, 1).unsqueeze(1)\n\n        image = image.permute(0, 3, 1, 2) # Torch wants (B, C, H, W) we use (B, H, W, C)\n        padded_image = F.pad(image, (blur_radius,blur_radius,blur_radius,blur_radius), 'reflect')\n        blurred = F.conv2d(padded_image, kernel, padding=kernel_size // 2, groups=channels)[:,:,blur_radius:-blur_radius, blur_radius:-blur_radius]\n        blurred = blurred.permute(0, 2, 3, 1)\n\n        return (blurred.to(comfy.model_management.intermediate_device()),)\n\nclass Quantize:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"colors\": (\"INT\", {\n                    \"default\": 256,\n                    \"min\": 1,\n                    \"max\": 256,\n                    \"step\": 1\n                }),\n                \"dither\": ([\"none\", \"floyd-steinberg\", \"bayer-2\", \"bayer-4\", \"bayer-8\", \"bayer-16\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"quantize\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def bayer(im, pal_im, order):\n        def normalized_bayer_matrix(n):\n            if n == 0:\n                return np.zeros((1,1), \"float32\")\n            else:\n                q = 4 ** n\n                m = q * normalized_bayer_matrix(n - 1)\n                return np.bmat(((m-1.5, m+0.5), (m+1.5, m-0.5))) / q\n\n        num_colors = len(pal_im.getpalette()) // 3\n        spread = 2 * 256 / num_colors\n        bayer_n = int(math.log2(order))\n        bayer_matrix = torch.from_numpy(spread * normalized_bayer_matrix(bayer_n) + 0.5)\n\n        result = torch.from_numpy(np.array(im).astype(np.float32))\n        tw = math.ceil(result.shape[0] / bayer_matrix.shape[0])\n        th = math.ceil(result.shape[1] / bayer_matrix.shape[1])\n        tiled_matrix = bayer_matrix.tile(tw, th).unsqueeze(-1)\n        result.add_(tiled_matrix[:result.shape[0],:result.shape[1]]).clamp_(0, 255)\n        result = result.to(dtype=torch.uint8)\n\n        im = Image.fromarray(result.cpu().numpy())\n        im = im.quantize(palette=pal_im, dither=Image.Dither.NONE)\n        return im\n\n    def quantize(self, image: torch.Tensor, colors: int, dither: str):\n        batch_size, height, width, _ = image.shape\n        result = torch.zeros_like(image)\n\n        for b in range(batch_size):\n            im = Image.fromarray((image[b] * 255).to(torch.uint8).numpy(), mode='RGB')\n\n            pal_im = im.quantize(colors=colors) # Required as described in https://github.com/python-pillow/Pillow/issues/5836\n\n            if dither == \"none\":\n                quantized_image = im.quantize(palette=pal_im, dither=Image.Dither.NONE)\n            elif dither == \"floyd-steinberg\":\n                quantized_image = im.quantize(palette=pal_im, dither=Image.Dither.FLOYDSTEINBERG)\n            elif dither.startswith(\"bayer\"):\n                order = int(dither.split('-')[-1])\n                quantized_image = Quantize.bayer(im, pal_im, order)\n\n            quantized_array = torch.tensor(np.array(quantized_image.convert(\"RGB\"))).float() / 255\n            result[b] = quantized_array\n\n        return (result,)\n\nclass Sharpen:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"sharpen_radius\": (\"INT\", {\n                    \"default\": 1,\n                    \"min\": 1,\n                    \"max\": 31,\n                    \"step\": 1\n                }),\n                \"sigma\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.1,\n                    \"max\": 10.0,\n                    \"step\": 0.01\n                }),\n                \"alpha\": (\"FLOAT\", {\n                    \"default\": 1.0,\n                    \"min\": 0.0,\n                    \"max\": 5.0,\n                    \"step\": 0.01\n                }),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"sharpen\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def sharpen(self, image: torch.Tensor, sharpen_radius: int, sigma:float, alpha: float):\n        if sharpen_radius == 0:\n            return (image,)\n\n        batch_size, height, width, channels = image.shape\n        image = image.to(comfy.model_management.get_torch_device())\n\n        kernel_size = sharpen_radius * 2 + 1\n        kernel = gaussian_kernel(kernel_size, sigma, device=image.device) * -(alpha*10)\n        center = kernel_size // 2\n        kernel[center, center] = kernel[center, center] - kernel.sum() + 1.0\n        kernel = kernel.repeat(channels, 1, 1).unsqueeze(1)\n\n        tensor_image = image.permute(0, 3, 1, 2) # Torch wants (B, C, H, W) we use (B, H, W, C)\n        tensor_image = F.pad(tensor_image, (sharpen_radius,sharpen_radius,sharpen_radius,sharpen_radius), 'reflect')\n        sharpened = F.conv2d(tensor_image, kernel, padding=center, groups=channels)[:,:,sharpen_radius:-sharpen_radius, sharpen_radius:-sharpen_radius]\n        sharpened = sharpened.permute(0, 2, 3, 1)\n\n        result = torch.clamp(sharpened, 0, 1)\n\n        return (result.to(comfy.model_management.intermediate_device()),)\n\nclass ImageScaleToTotalPixels:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"megapixels\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 16.0, \"step\": 0.01}),\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, image, upscale_method, megapixels):\n        samples = image.movedim(-1,1)\n        total = int(megapixels * 1024 * 1024)\n\n        scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))\n        width = round(samples.shape[3] * scale_by)\n        height = round(samples.shape[2] * scale_by)\n\n        s = comfy.utils.common_upscale(samples, width, height, upscale_method, \"disabled\")\n        s = s.movedim(1,-1)\n        return (s,)\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageBlend\": Blend,\n    \"ImageBlur\": Blur,\n    \"ImageQuantize\": Quantize,\n    \"ImageSharpen\": Sharpen,\n    \"ImageScaleToTotalPixels\": ImageScaleToTotalPixels,\n}\n", "comfy_extras/nodes_freelunch.py": "#code originally taken from: https://github.com/ChenyangSi/FreeU (under MIT License)\n\nimport torch\nimport logging\n\ndef Fourier_filter(x, threshold, scale):\n    # FFT\n    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))\n    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))\n\n    B, C, H, W = x_freq.shape\n    mask = torch.ones((B, C, H, W), device=x.device)\n\n    crow, ccol = H // 2, W //2\n    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale\n    x_freq = x_freq * mask\n\n    # IFFT\n    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))\n    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real\n\n    return x_filtered.to(x.dtype)\n\n\nclass FreeU:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.1, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, b1, b2, s1, s2):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n        scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}\n        on_cpu_devices = {}\n\n        def output_block_patch(h, hsp, transformer_options):\n            scale = scale_dict.get(int(h.shape[1]), None)\n            if scale is not None:\n                h[:,:h.shape[1] // 2] = h[:,:h.shape[1] // 2] * scale[0]\n                if hsp.device not in on_cpu_devices:\n                    try:\n                        hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])\n                    except:\n                        logging.warning(\"Device {} does not support the torch.fft functions used in the FreeU node, switching to CPU.\".format(hsp.device))\n                        on_cpu_devices[hsp.device] = True\n                        hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n                else:\n                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n\n            return h, hsp\n\n        m = model.clone()\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nclass FreeU_V2:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.3, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.4, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, b1, b2, s1, s2):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n        scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}\n        on_cpu_devices = {}\n\n        def output_block_patch(h, hsp, transformer_options):\n            scale = scale_dict.get(int(h.shape[1]), None)\n            if scale is not None:\n                hidden_mean = h.mean(1).unsqueeze(1)\n                B = hidden_mean.shape[0]\n                hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)\n                hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)\n                hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)\n\n                h[:,:h.shape[1] // 2] = h[:,:h.shape[1] // 2] * ((scale[0] - 1 ) * hidden_mean + 1)\n\n                if hsp.device not in on_cpu_devices:\n                    try:\n                        hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])\n                    except:\n                        logging.warning(\"Device {} does not support the torch.fft functions used in the FreeU node, switching to CPU.\".format(hsp.device))\n                        on_cpu_devices[hsp.device] = True\n                        hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n                else:\n                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)\n\n            return h, hsp\n\n        m = model.clone()\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"FreeU\": FreeU,\n    \"FreeU_V2\": FreeU_V2,\n}\n", "comfy_extras/nodes_pag.py": "#Modified/simplified version of the node from: https://github.com/pamparamm/sd-perturbed-attention\n#If you want the one with more options see the above repo.\n\n#My modified one here is more basic but has less chances of breaking with ComfyUI updates.\n\nimport comfy.model_patcher\nimport comfy.samplers\n\nclass PerturbedAttentionGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\",),\n                \"scale\": (\"FLOAT\", {\"default\": 3.0, \"min\": 0.0, \"max\": 100.0, \"step\": 0.1, \"round\": 0.01}),\n            }\n        }\n\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, scale):\n        unet_block = \"middle\"\n        unet_block_id = 0\n        m = model.clone()\n\n        def perturbed_attention(q, k, v, extra_options, mask=None):\n            return v\n\n        def post_cfg_function(args):\n            model = args[\"model\"]\n            cond_pred = args[\"cond_denoised\"]\n            cond = args[\"cond\"]\n            cfg_result = args[\"denoised\"]\n            sigma = args[\"sigma\"]\n            model_options = args[\"model_options\"].copy()\n            x = args[\"input\"]\n\n            if scale == 0:\n                return cfg_result\n\n            # Replace Self-attention with PAG\n            model_options = comfy.model_patcher.set_model_options_patch_replace(model_options, perturbed_attention, \"attn1\", unet_block, unet_block_id)\n            (pag,) = comfy.samplers.calc_cond_batch(model, [cond], x, sigma, model_options)\n\n            return cfg_result + (cond_pred - pag) * scale\n\n        m.set_model_sampler_post_cfg_function(post_cfg_function)\n\n        return (m,)\n\nNODE_CLASS_MAPPINGS = {\n    \"PerturbedAttentionGuidance\": PerturbedAttentionGuidance,\n}\n", "comfy_extras/nodes_latent.py": "import comfy.utils\nimport torch\n\ndef reshape_latent_to(target_shape, latent):\n    if latent.shape[1:] != target_shape[1:]:\n        latent = comfy.utils.common_upscale(latent, target_shape[3], target_shape[2], \"bilinear\", \"center\")\n    return comfy.utils.repeat_to_batch_size(latent, target_shape[0])\n\n\nclass LatentAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n        samples_out[\"samples\"] = s1 + s2\n        return (samples_out,)\n\nclass LatentSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n        samples_out[\"samples\"] = s1 - s2\n        return (samples_out,)\n\nclass LatentMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples, multiplier):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1 * multiplier\n        return (samples_out,)\n\nclass LatentInterpolate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",),\n                              \"samples2\": (\"LATENT\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples1, samples2, ratio):\n        samples_out = samples1.copy()\n\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        s2 = reshape_latent_to(s1.shape, s2)\n\n        m1 = torch.linalg.vector_norm(s1, dim=(1))\n        m2 = torch.linalg.vector_norm(s2, dim=(1))\n\n        s1 = torch.nan_to_num(s1 / m1)\n        s2 = torch.nan_to_num(s2 / m2)\n\n        t = (s1 * ratio + s2 * (1.0 - ratio))\n        mt = torch.linalg.vector_norm(t, dim=(1))\n        st = torch.nan_to_num(t / mt)\n\n        samples_out[\"samples\"] = st * (m1 * ratio + m2 * (1.0 - ratio))\n        return (samples_out,)\n\nclass LatentBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"batch\"\n\n    CATEGORY = \"latent/batch\"\n\n    def batch(self, samples1, samples2):\n        samples_out = samples1.copy()\n        s1 = samples1[\"samples\"]\n        s2 = samples2[\"samples\"]\n\n        if s1.shape[1:] != s2.shape[1:]:\n            s2 = comfy.utils.common_upscale(s2, s1.shape[3], s1.shape[2], \"bilinear\", \"center\")\n        s = torch.cat((s1, s2), dim=0)\n        samples_out[\"samples\"] = s\n        samples_out[\"batch_index\"] = samples1.get(\"batch_index\", [x for x in range(0, s1.shape[0])]) + samples2.get(\"batch_index\", [x for x in range(0, s2.shape[0])])\n        return (samples_out,)\n\nclass LatentBatchSeedBehavior:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"seed_behavior\": ([\"random\", \"fixed\"],{\"default\": \"fixed\"}),}}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/advanced\"\n\n    def op(self, samples, seed_behavior):\n        samples_out = samples.copy()\n        latent = samples[\"samples\"]\n        if seed_behavior == \"random\":\n            if 'batch_index' in samples_out:\n                samples_out.pop('batch_index')\n        elif seed_behavior == \"fixed\":\n            batch_number = samples_out.get(\"batch_index\", [0])[0]\n            samples_out[\"batch_index\"] = [batch_number] * latent.shape[0]\n\n        return (samples_out,)\n\nNODE_CLASS_MAPPINGS = {\n    \"LatentAdd\": LatentAdd,\n    \"LatentSubtract\": LatentSubtract,\n    \"LatentMultiply\": LatentMultiply,\n    \"LatentInterpolate\": LatentInterpolate,\n    \"LatentBatch\": LatentBatch,\n    \"LatentBatchSeedBehavior\": LatentBatchSeedBehavior,\n}\n", "comfy_extras/nodes_audio.py": "import torchaudio\nimport torch\nimport comfy.model_management\nimport folder_paths\nimport os\n\nclass EmptyLatentAudio:\n    def __init__(self):\n        self.device = comfy.model_management.intermediate_device()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"_for_testing/audio\"\n\n    def generate(self):\n        batch_size = 1\n        latent = torch.zeros([batch_size, 64, 1024], device=self.device)\n        return ({\"samples\":latent, \"type\": \"audio\"}, )\n\nclass VAEEncodeAudio:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"audio\": (\"AUDIO\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"_for_testing/audio\"\n\n    def encode(self, vae, audio):\n        sample_rate = audio[\"sample_rate\"]\n        if 44100 != sample_rate:\n            waveform = torchaudio.functional.resample(audio[\"waveform\"], sample_rate, 44100)\n        else:\n            waveform = audio[\"waveform\"]\n\n        t = vae.encode(waveform.movedim(1, -1))\n        return ({\"samples\":t}, )\n\nclass VAEDecodeAudio:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"AUDIO\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"_for_testing/audio\"\n\n    def decode(self, vae, samples):\n        audio = vae.decode(samples[\"samples\"]).movedim(-1, 1)\n        return ({\"waveform\": audio, \"sample_rate\": 44100}, )\n\nclass SaveAudio:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"audio\": (\"AUDIO\", ),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"audio/ComfyUI\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_audio\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"_for_testing/audio\"\n\n    def save_audio(self, audio, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        results = list()\n        for (batch_number, waveform) in enumerate(audio[\"waveform\"]):\n            #TODO: metadata\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.flac\"\n            torchaudio.save(os.path.join(full_output_folder, file), waveform, audio[\"sample_rate\"], format=\"FLAC\")\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n\n        return { \"ui\": { \"audio\": results } }\n\nclass LoadAudio:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\": {\"audio\": [sorted(files), ]}, }\n\n    CATEGORY = \"_for_testing/audio\"\n\n    RETURN_TYPES = (\"AUDIO\", )\n    FUNCTION = \"load\"\n\n    def load(self, audio):\n        audio_path = folder_paths.get_annotated_filepath(audio)\n        waveform, sample_rate = torchaudio.load(audio_path)\n        multiplier = 1.0\n        audio = {\"waveform\": waveform.unsqueeze(0), \"sample_rate\": sample_rate}\n        return (audio, )\n\n    @classmethod\n    def IS_CHANGED(s, audio):\n        image_path = folder_paths.get_annotated_filepath(audio)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n\n    @classmethod\n    def VALIDATE_INPUTS(s, audio):\n        if not folder_paths.exists_annotated_filepath(audio):\n            return \"Invalid audio file: {}\".format(audio)\n        return True\n\nNODE_CLASS_MAPPINGS = {\n    \"EmptyLatentAudio\": EmptyLatentAudio,\n    \"VAEEncodeAudio\": VAEEncodeAudio,\n    \"VAEDecodeAudio\": VAEDecodeAudio,\n    \"SaveAudio\": SaveAudio,\n    \"LoadAudio\": LoadAudio,\n}\n", "comfy_extras/nodes_model_downscale.py": "import torch\nimport comfy.utils\n\nclass PatchModelAddDownscale:\n    upscale_methods = [\"bicubic\", \"nearest-exact\", \"bilinear\", \"area\", \"bislerp\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"block_number\": (\"INT\", {\"default\": 3, \"min\": 1, \"max\": 32, \"step\": 1}),\n                              \"downscale_factor\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.1, \"max\": 9.0, \"step\": 0.001}),\n                              \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"end_percent\": (\"FLOAT\", {\"default\": 0.35, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"downscale_after_skip\": (\"BOOLEAN\", {\"default\": True}),\n                              \"downscale_method\": (s.upscale_methods,),\n                              \"upscale_method\": (s.upscale_methods,),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, block_number, downscale_factor, start_percent, end_percent, downscale_after_skip, downscale_method, upscale_method):\n        model_sampling = model.get_model_object(\"model_sampling\")\n        sigma_start = model_sampling.percent_to_sigma(start_percent)\n        sigma_end = model_sampling.percent_to_sigma(end_percent)\n\n        def input_block_patch(h, transformer_options):\n            if transformer_options[\"block\"][1] == block_number:\n                sigma = transformer_options[\"sigmas\"][0].item()\n                if sigma <= sigma_start and sigma >= sigma_end:\n                    h = comfy.utils.common_upscale(h, round(h.shape[-1] * (1.0 / downscale_factor)), round(h.shape[-2] * (1.0 / downscale_factor)), downscale_method, \"disabled\")\n            return h\n\n        def output_block_patch(h, hsp, transformer_options):\n            if h.shape[2] != hsp.shape[2]:\n                h = comfy.utils.common_upscale(h, hsp.shape[-1], hsp.shape[-2], upscale_method, \"disabled\")\n            return h, hsp\n\n        m = model.clone()\n        if downscale_after_skip:\n            m.set_model_input_block_patch_after_skip(input_block_patch)\n        else:\n            m.set_model_input_block_patch(input_block_patch)\n        m.set_model_output_block_patch(output_block_patch)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"PatchModelAddDownscale\": PatchModelAddDownscale,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"PatchModelAddDownscale\": \"PatchModelAddDownscale (Kohya Deep Shrink)\",\n}\n", "comfy_extras/nodes_gits.py": "# from https://github.com/zju-pi/diff-sampler/tree/main/gits-main\nimport numpy as np\nimport torch\n\ndef loglinear_interp(t_steps, num_steps):\n    \"\"\"\n    Performs log-linear interpolation of a given array of decreasing numbers.\n    \"\"\"\n    xs = np.linspace(0, 1, len(t_steps))\n    ys = np.log(t_steps[::-1])\n\n    new_xs = np.linspace(0, 1, num_steps)\n    new_ys = np.interp(new_xs, xs, ys)\n\n    interped_ys = np.exp(new_ys)[::-1].copy()\n    return interped_ys\n\nNOISE_LEVELS = {\n    0.80: [\n        [14.61464119, 7.49001646, 0.02916753],\n        [14.61464119, 11.54541874, 6.77309084, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 3.07277966, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 2.05039096, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 2.05039096, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 8.75849152, 7.49001646, 5.85520077, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 8.75849152, 7.49001646, 5.85520077, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 8.75849152, 7.49001646, 5.85520077, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 5.85520077, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.07277966, 1.56271636, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.1956799, 1.98035145, 0.86115354, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.1956799, 1.98035145, 0.86115354, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.07277966, 1.84880662, 0.83188516, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.88507891, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.07277966, 1.84880662, 0.83188516, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.88507891, 7.49001646, 6.77309084, 5.85520077, 4.86714602, 3.75677586, 2.84484982, 1.78698075, 0.803307, 0.02916753],\n    ],\n    0.85: [\n        [14.61464119, 7.49001646, 0.02916753],\n        [14.61464119, 7.49001646, 1.84880662, 0.02916753],\n        [14.61464119, 11.54541874, 6.77309084, 1.56271636, 0.02916753],\n        [14.61464119, 11.54541874, 7.11996698, 3.07277966, 1.24153244, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.09240818, 2.84484982, 0.95350921, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.09240818, 2.84484982, 0.95350921, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.58536053, 3.1956799, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 8.75849152, 7.49001646, 5.58536053, 3.1956799, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 8.75849152, 7.49001646, 6.14220476, 4.65472794, 3.07277966, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 8.75849152, 7.49001646, 6.14220476, 4.65472794, 3.07277966, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.65472794, 3.07277966, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.65472794, 3.07277966, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.65472794, 3.07277966, 1.84880662, 0.803307, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.60512662, 2.6383388, 1.56271636, 0.72133851, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.46139455, 2.45070267, 1.56271636, 0.72133851, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.46139455, 2.45070267, 1.56271636, 0.72133851, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.46139455, 2.45070267, 1.56271636, 0.72133851, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.46139455, 2.45070267, 1.56271636, 0.72133851, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.90732002, 10.31284904, 9.75859547, 9.24142551, 8.75849152, 8.30717278, 7.88507891, 7.49001646, 6.77309084, 5.85520077, 4.65472794, 3.46139455, 2.45070267, 1.56271636, 0.72133851, 0.02916753],\n    ],\n    0.90: [\n        [14.61464119, 6.77309084, 0.02916753],\n        [14.61464119, 7.49001646, 1.56271636, 0.02916753],\n        [14.61464119, 7.49001646, 3.07277966, 0.95350921, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.54230714, 0.89115214, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 4.86714602, 2.54230714, 0.89115214, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.09240818, 3.07277966, 1.61558151, 0.69515091, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.11996698, 4.86714602, 3.07277966, 1.61558151, 0.69515091, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 2.95596409, 1.61558151, 0.69515091, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.19988537, 1.24153244, 0.57119018, 0.02916753],\n        [14.61464119, 12.96784878, 10.90732002, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.19988537, 1.24153244, 0.57119018, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 9.24142551, 8.30717278, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.19988537, 1.24153244, 0.57119018, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.75677586, 2.84484982, 1.84880662, 1.08895338, 0.52423614, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 4.86714602, 3.75677586, 2.84484982, 1.84880662, 1.08895338, 0.52423614, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.44769001, 5.58536053, 4.45427561, 3.32507086, 2.45070267, 1.61558151, 0.95350921, 0.45573691, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.44769001, 5.58536053, 4.45427561, 3.32507086, 2.45070267, 1.61558151, 0.95350921, 0.45573691, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.86714602, 3.91689563, 3.07277966, 2.27973175, 1.56271636, 0.95350921, 0.45573691, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.86714602, 3.91689563, 3.07277966, 2.27973175, 1.56271636, 0.95350921, 0.45573691, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 4.86714602, 3.91689563, 3.07277966, 2.27973175, 1.56271636, 0.95350921, 0.45573691, 0.02916753],\n        [14.61464119, 13.76078796, 12.96784878, 12.2308979, 11.54541874, 10.31284904, 9.24142551, 8.75849152, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 5.09240818, 4.45427561, 3.60512662, 2.95596409, 2.19988537, 1.51179266, 0.89115214, 0.43325692, 0.02916753],\n    ],\n    0.95: [\n        [14.61464119, 6.77309084, 0.02916753],\n        [14.61464119, 6.77309084, 1.56271636, 0.02916753],\n        [14.61464119, 7.49001646, 2.84484982, 0.89115214, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.36326075, 0.803307, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.95596409, 1.56271636, 0.64427125, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 4.86714602, 2.95596409, 1.56271636, 0.64427125, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 4.86714602, 3.07277966, 1.91321158, 1.08895338, 0.50118381, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.07277966, 1.91321158, 1.08895338, 0.50118381, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 3.07277966, 1.91321158, 1.08895338, 0.50118381, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.19988537, 1.41535246, 0.803307, 0.38853383, 0.02916753],\n        [14.61464119, 12.2308979, 8.75849152, 7.49001646, 5.85520077, 4.65472794, 3.46139455, 2.6383388, 1.84880662, 1.24153244, 0.72133851, 0.34370604, 0.02916753],\n        [14.61464119, 12.96784878, 10.90732002, 8.75849152, 7.49001646, 5.85520077, 4.65472794, 3.46139455, 2.6383388, 1.84880662, 1.24153244, 0.72133851, 0.34370604, 0.02916753],\n        [14.61464119, 12.96784878, 10.90732002, 8.75849152, 7.49001646, 6.14220476, 4.86714602, 3.75677586, 2.95596409, 2.19988537, 1.56271636, 1.05362725, 0.64427125, 0.32104823, 0.02916753],\n        [14.61464119, 12.96784878, 10.90732002, 8.75849152, 7.49001646, 6.44769001, 5.58536053, 4.65472794, 3.60512662, 2.95596409, 2.19988537, 1.56271636, 1.05362725, 0.64427125, 0.32104823, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 9.24142551, 8.30717278, 7.49001646, 6.44769001, 5.58536053, 4.65472794, 3.60512662, 2.95596409, 2.19988537, 1.56271636, 1.05362725, 0.64427125, 0.32104823, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 9.24142551, 8.30717278, 7.49001646, 6.44769001, 5.58536053, 4.65472794, 3.75677586, 3.07277966, 2.45070267, 1.78698075, 1.24153244, 0.83188516, 0.50118381, 0.22545385, 0.02916753],\n        [14.61464119, 12.96784878, 11.54541874, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 5.09240818, 4.45427561, 3.60512662, 2.95596409, 2.36326075, 1.72759056, 1.24153244, 0.83188516, 0.50118381, 0.22545385, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 5.09240818, 4.45427561, 3.60512662, 2.95596409, 2.36326075, 1.72759056, 1.24153244, 0.83188516, 0.50118381, 0.22545385, 0.02916753],\n        [14.61464119, 13.76078796, 12.2308979, 10.90732002, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 5.09240818, 4.45427561, 3.75677586, 3.07277966, 2.45070267, 1.91321158, 1.46270394, 1.05362725, 0.72133851, 0.43325692, 0.19894916, 0.02916753],\n    ],\n    1.00: [\n        [14.61464119, 1.56271636, 0.02916753],\n        [14.61464119, 6.77309084, 0.95350921, 0.02916753],\n        [14.61464119, 6.77309084, 2.36326075, 0.803307, 0.02916753],\n        [14.61464119, 7.11996698, 3.07277966, 1.56271636, 0.59516323, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.84484982, 1.41535246, 0.57119018, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.84484982, 1.61558151, 0.86115354, 0.38853383, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 4.86714602, 2.84484982, 1.61558151, 0.86115354, 0.38853383, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 4.86714602, 3.07277966, 1.98035145, 1.24153244, 0.72133851, 0.34370604, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.07277966, 1.98035145, 1.24153244, 0.72133851, 0.34370604, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.27973175, 1.51179266, 0.95350921, 0.54755926, 0.25053367, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.36326075, 1.61558151, 1.08895338, 0.72133851, 0.41087446, 0.17026083, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.36326075, 1.61558151, 1.08895338, 0.72133851, 0.41087446, 0.17026083, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.84484982, 2.12350607, 1.56271636, 1.08895338, 0.72133851, 0.41087446, 0.17026083, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.84484982, 2.19988537, 1.61558151, 1.162866, 0.803307, 0.50118381, 0.27464288, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 5.85520077, 4.65472794, 3.75677586, 3.07277966, 2.45070267, 1.84880662, 1.36964464, 1.01931262, 0.72133851, 0.45573691, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 6.14220476, 5.09240818, 4.26497746, 3.46139455, 2.84484982, 2.19988537, 1.67050016, 1.24153244, 0.92192322, 0.64427125, 0.43325692, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 8.75849152, 7.49001646, 6.14220476, 5.09240818, 4.26497746, 3.60512662, 2.95596409, 2.45070267, 1.91321158, 1.51179266, 1.12534678, 0.83188516, 0.59516323, 0.38853383, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 12.2308979, 9.24142551, 8.30717278, 7.49001646, 6.14220476, 5.09240818, 4.26497746, 3.60512662, 2.95596409, 2.45070267, 1.91321158, 1.51179266, 1.12534678, 0.83188516, 0.59516323, 0.38853383, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 12.2308979, 9.24142551, 8.30717278, 7.49001646, 6.77309084, 5.85520077, 5.09240818, 4.26497746, 3.60512662, 2.95596409, 2.45070267, 1.91321158, 1.51179266, 1.12534678, 0.83188516, 0.59516323, 0.38853383, 0.22545385, 0.09824532, 0.02916753],\n    ],\n    1.05: [\n        [14.61464119, 0.95350921, 0.02916753],\n        [14.61464119, 6.77309084, 0.89115214, 0.02916753],\n        [14.61464119, 6.77309084, 2.05039096, 0.72133851, 0.02916753],\n        [14.61464119, 6.77309084, 2.84484982, 1.28281462, 0.52423614, 0.02916753],\n        [14.61464119, 6.77309084, 3.07277966, 1.61558151, 0.803307, 0.34370604, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.84484982, 1.56271636, 0.803307, 0.34370604, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.84484982, 1.61558151, 0.95350921, 0.52423614, 0.22545385, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 1.98035145, 1.24153244, 0.74807048, 0.41087446, 0.17026083, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.27973175, 1.51179266, 0.95350921, 0.59516323, 0.34370604, 0.13792117, 0.02916753],\n        [14.61464119, 7.49001646, 5.09240818, 3.46139455, 2.45070267, 1.61558151, 1.08895338, 0.72133851, 0.45573691, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.09240818, 3.46139455, 2.45070267, 1.61558151, 1.08895338, 0.72133851, 0.45573691, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.36326075, 1.61558151, 1.08895338, 0.72133851, 0.45573691, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.45070267, 1.72759056, 1.24153244, 0.86115354, 0.59516323, 0.38853383, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.84484982, 2.19988537, 1.61558151, 1.162866, 0.83188516, 0.59516323, 0.38853383, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.84484982, 2.19988537, 1.67050016, 1.28281462, 0.95350921, 0.72133851, 0.52423614, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.95596409, 2.36326075, 1.84880662, 1.41535246, 1.08895338, 0.83188516, 0.61951244, 0.45573691, 0.32104823, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.65472794, 3.60512662, 2.95596409, 2.45070267, 1.91321158, 1.51179266, 1.20157266, 0.95350921, 0.74807048, 0.57119018, 0.43325692, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 8.30717278, 7.11996698, 5.85520077, 4.65472794, 3.60512662, 2.95596409, 2.45070267, 1.91321158, 1.51179266, 1.20157266, 0.95350921, 0.74807048, 0.57119018, 0.43325692, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 8.30717278, 7.11996698, 5.85520077, 4.65472794, 3.60512662, 2.95596409, 2.45070267, 1.98035145, 1.61558151, 1.32549286, 1.08895338, 0.86115354, 0.69515091, 0.54755926, 0.41087446, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n    ],\n    1.10: [\n        [14.61464119, 0.89115214, 0.02916753],\n        [14.61464119, 2.36326075, 0.72133851, 0.02916753],\n        [14.61464119, 5.85520077, 1.61558151, 0.57119018, 0.02916753],\n        [14.61464119, 6.77309084, 2.45070267, 1.08895338, 0.45573691, 0.02916753],\n        [14.61464119, 6.77309084, 2.95596409, 1.56271636, 0.803307, 0.34370604, 0.02916753],\n        [14.61464119, 6.77309084, 3.07277966, 1.61558151, 0.89115214, 0.4783645, 0.19894916, 0.02916753],\n        [14.61464119, 6.77309084, 3.07277966, 1.84880662, 1.08895338, 0.64427125, 0.34370604, 0.13792117, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.84484982, 1.61558151, 0.95350921, 0.54755926, 0.27464288, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.95596409, 1.91321158, 1.24153244, 0.803307, 0.4783645, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 2.05039096, 1.41535246, 0.95350921, 0.64427125, 0.41087446, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.27973175, 1.61558151, 1.12534678, 0.803307, 0.54755926, 0.36617002, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.32507086, 2.45070267, 1.72759056, 1.24153244, 0.89115214, 0.64427125, 0.45573691, 0.32104823, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 5.09240818, 3.60512662, 2.84484982, 2.05039096, 1.51179266, 1.08895338, 0.803307, 0.59516323, 0.43325692, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 5.09240818, 3.60512662, 2.84484982, 2.12350607, 1.61558151, 1.24153244, 0.95350921, 0.72133851, 0.54755926, 0.41087446, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.45070267, 1.84880662, 1.41535246, 1.08895338, 0.83188516, 0.64427125, 0.50118381, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 5.85520077, 4.45427561, 3.1956799, 2.45070267, 1.91321158, 1.51179266, 1.20157266, 0.95350921, 0.74807048, 0.59516323, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 5.85520077, 4.45427561, 3.46139455, 2.84484982, 2.19988537, 1.72759056, 1.36964464, 1.08895338, 0.86115354, 0.69515091, 0.54755926, 0.43325692, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.46139455, 2.84484982, 2.19988537, 1.72759056, 1.36964464, 1.08895338, 0.86115354, 0.69515091, 0.54755926, 0.43325692, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 11.54541874, 7.49001646, 5.85520077, 4.45427561, 3.46139455, 2.84484982, 2.19988537, 1.72759056, 1.36964464, 1.08895338, 0.89115214, 0.72133851, 0.59516323, 0.4783645, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.09824532, 0.02916753],\n    ], \n    1.15: [\n        [14.61464119, 0.83188516, 0.02916753],\n        [14.61464119, 1.84880662, 0.59516323, 0.02916753],\n        [14.61464119, 5.85520077, 1.56271636, 0.52423614, 0.02916753],\n        [14.61464119, 5.85520077, 1.91321158, 0.83188516, 0.34370604, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.24153244, 0.59516323, 0.25053367, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.51179266, 0.803307, 0.41087446, 0.17026083, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.56271636, 0.89115214, 0.50118381, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 6.77309084, 3.07277966, 1.84880662, 1.12534678, 0.72133851, 0.43325692, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 6.77309084, 3.07277966, 1.91321158, 1.24153244, 0.803307, 0.52423614, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 2.95596409, 1.91321158, 1.24153244, 0.803307, 0.52423614, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 2.05039096, 1.36964464, 0.95350921, 0.69515091, 0.4783645, 0.32104823, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 2.12350607, 1.51179266, 1.08895338, 0.803307, 0.59516323, 0.43325692, 0.29807833, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 2.12350607, 1.51179266, 1.08895338, 0.803307, 0.59516323, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.07277966, 2.19988537, 1.61558151, 1.24153244, 0.95350921, 0.74807048, 0.59516323, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.45070267, 1.78698075, 1.32549286, 1.01931262, 0.803307, 0.64427125, 0.50118381, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.45070267, 1.78698075, 1.32549286, 1.01931262, 0.803307, 0.64427125, 0.52423614, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.45070267, 1.84880662, 1.41535246, 1.12534678, 0.89115214, 0.72133851, 0.59516323, 0.4783645, 0.38853383, 0.32104823, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.45070267, 1.84880662, 1.41535246, 1.12534678, 0.89115214, 0.72133851, 0.59516323, 0.50118381, 0.41087446, 0.34370604, 0.27464288, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.86714602, 3.1956799, 2.45070267, 1.84880662, 1.41535246, 1.12534678, 0.89115214, 0.72133851, 0.59516323, 0.50118381, 0.41087446, 0.34370604, 0.29807833, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.20: [\n        [14.61464119, 0.803307, 0.02916753],\n        [14.61464119, 1.56271636, 0.52423614, 0.02916753],\n        [14.61464119, 2.36326075, 0.92192322, 0.36617002, 0.02916753],\n        [14.61464119, 2.84484982, 1.24153244, 0.59516323, 0.25053367, 0.02916753],\n        [14.61464119, 5.85520077, 2.05039096, 0.95350921, 0.45573691, 0.17026083, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.24153244, 0.64427125, 0.29807833, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.36964464, 0.803307, 0.45573691, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 0.95350921, 0.59516323, 0.36617002, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.67050016, 1.08895338, 0.74807048, 0.50118381, 0.32104823, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.84880662, 1.24153244, 0.83188516, 0.59516323, 0.41087446, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 3.07277966, 1.98035145, 1.36964464, 0.95350921, 0.69515091, 0.50118381, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 6.77309084, 3.46139455, 2.36326075, 1.56271636, 1.08895338, 0.803307, 0.59516323, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 6.77309084, 3.46139455, 2.45070267, 1.61558151, 1.162866, 0.86115354, 0.64427125, 0.50118381, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.12350607, 1.51179266, 1.08895338, 0.83188516, 0.64427125, 0.50118381, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.12350607, 1.51179266, 1.08895338, 0.83188516, 0.64427125, 0.50118381, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.12350607, 1.51179266, 1.08895338, 0.83188516, 0.64427125, 0.50118381, 0.41087446, 0.34370604, 0.27464288, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.19988537, 1.61558151, 1.20157266, 0.92192322, 0.72133851, 0.57119018, 0.45573691, 0.36617002, 0.29807833, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.19988537, 1.61558151, 1.24153244, 0.95350921, 0.74807048, 0.59516323, 0.4783645, 0.38853383, 0.32104823, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 7.49001646, 4.65472794, 3.07277966, 2.19988537, 1.61558151, 1.24153244, 0.95350921, 0.74807048, 0.59516323, 0.50118381, 0.41087446, 0.34370604, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.25: [\n        [14.61464119, 0.72133851, 0.02916753],\n        [14.61464119, 1.56271636, 0.50118381, 0.02916753],\n        [14.61464119, 2.05039096, 0.803307, 0.32104823, 0.02916753],\n        [14.61464119, 2.36326075, 0.95350921, 0.43325692, 0.17026083, 0.02916753],\n        [14.61464119, 2.84484982, 1.24153244, 0.59516323, 0.27464288, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.51179266, 0.803307, 0.43325692, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.36326075, 1.24153244, 0.72133851, 0.41087446, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.36964464, 0.83188516, 0.52423614, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 0.98595673, 0.64427125, 0.43325692, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.67050016, 1.08895338, 0.74807048, 0.52423614, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.72759056, 1.162866, 0.803307, 0.59516323, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.84880662, 1.24153244, 0.86115354, 0.64427125, 0.4783645, 0.36617002, 0.27464288, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.84880662, 1.28281462, 0.92192322, 0.69515091, 0.52423614, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.91321158, 1.32549286, 0.95350921, 0.72133851, 0.54755926, 0.43325692, 0.34370604, 0.27464288, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.91321158, 1.32549286, 0.95350921, 0.72133851, 0.57119018, 0.45573691, 0.36617002, 0.29807833, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.95596409, 1.91321158, 1.32549286, 0.95350921, 0.74807048, 0.59516323, 0.4783645, 0.38853383, 0.32104823, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 3.07277966, 2.05039096, 1.41535246, 1.05362725, 0.803307, 0.61951244, 0.50118381, 0.41087446, 0.34370604, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 3.07277966, 2.05039096, 1.41535246, 1.05362725, 0.803307, 0.64427125, 0.52423614, 0.43325692, 0.36617002, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 3.07277966, 2.05039096, 1.46270394, 1.08895338, 0.83188516, 0.66947293, 0.54755926, 0.45573691, 0.38853383, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.30: [\n        [14.61464119, 0.72133851, 0.02916753],\n        [14.61464119, 1.24153244, 0.43325692, 0.02916753],\n        [14.61464119, 1.56271636, 0.59516323, 0.22545385, 0.02916753],\n        [14.61464119, 1.84880662, 0.803307, 0.36617002, 0.13792117, 0.02916753],\n        [14.61464119, 2.36326075, 1.01931262, 0.52423614, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.36964464, 0.74807048, 0.41087446, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.56271636, 0.89115214, 0.54755926, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.61558151, 0.95350921, 0.61951244, 0.41087446, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.36964464, 0.83188516, 0.54755926, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.41535246, 0.92192322, 0.64427125, 0.45573691, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.6383388, 1.56271636, 1.01931262, 0.72133851, 0.50118381, 0.36617002, 0.27464288, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 1.05362725, 0.74807048, 0.54755926, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 1.08895338, 0.77538133, 0.57119018, 0.43325692, 0.34370604, 0.27464288, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 1.08895338, 0.803307, 0.59516323, 0.45573691, 0.36617002, 0.29807833, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.61558151, 1.08895338, 0.803307, 0.59516323, 0.4783645, 0.38853383, 0.32104823, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.72759056, 1.162866, 0.83188516, 0.64427125, 0.50118381, 0.41087446, 0.34370604, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.72759056, 1.162866, 0.83188516, 0.64427125, 0.52423614, 0.43325692, 0.36617002, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.78698075, 1.24153244, 0.92192322, 0.72133851, 0.57119018, 0.45573691, 0.38853383, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.84484982, 1.78698075, 1.24153244, 0.92192322, 0.72133851, 0.57119018, 0.4783645, 0.41087446, 0.36617002, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ], \n    1.35: [\n        [14.61464119, 0.69515091, 0.02916753],\n        [14.61464119, 0.95350921, 0.34370604, 0.02916753],\n        [14.61464119, 1.56271636, 0.57119018, 0.19894916, 0.02916753],\n        [14.61464119, 1.61558151, 0.69515091, 0.29807833, 0.09824532, 0.02916753],\n        [14.61464119, 1.84880662, 0.83188516, 0.43325692, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.162866, 0.64427125, 0.36617002, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.36964464, 0.803307, 0.50118381, 0.32104823, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.41535246, 0.83188516, 0.54755926, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 0.95350921, 0.64427125, 0.45573691, 0.32104823, 0.22545385, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 0.95350921, 0.64427125, 0.45573691, 0.34370604, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.61558151, 1.01931262, 0.72133851, 0.52423614, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.61558151, 1.01931262, 0.72133851, 0.52423614, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.61558151, 1.05362725, 0.74807048, 0.54755926, 0.43325692, 0.34370604, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.72759056, 1.12534678, 0.803307, 0.59516323, 0.45573691, 0.36617002, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 3.07277966, 1.72759056, 1.12534678, 0.803307, 0.59516323, 0.4783645, 0.38853383, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.45070267, 1.51179266, 1.01931262, 0.74807048, 0.57119018, 0.45573691, 0.36617002, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.6383388, 1.61558151, 1.08895338, 0.803307, 0.61951244, 0.50118381, 0.41087446, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.6383388, 1.61558151, 1.08895338, 0.803307, 0.64427125, 0.52423614, 0.43325692, 0.36617002, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 5.85520077, 2.6383388, 1.61558151, 1.08895338, 0.803307, 0.64427125, 0.52423614, 0.45573691, 0.38853383, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.40: [\n        [14.61464119, 0.59516323, 0.02916753],\n        [14.61464119, 0.95350921, 0.34370604, 0.02916753],\n        [14.61464119, 1.08895338, 0.43325692, 0.13792117, 0.02916753],\n        [14.61464119, 1.56271636, 0.64427125, 0.27464288, 0.09824532, 0.02916753],\n        [14.61464119, 1.61558151, 0.803307, 0.43325692, 0.22545385, 0.09824532, 0.02916753],\n        [14.61464119, 2.05039096, 0.95350921, 0.54755926, 0.34370604, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.24153244, 0.72133851, 0.43325692, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.24153244, 0.74807048, 0.50118381, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.803307, 0.52423614, 0.36617002, 0.27464288, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.803307, 0.54755926, 0.38853383, 0.29807833, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.41535246, 0.86115354, 0.59516323, 0.43325692, 0.32104823, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.51179266, 0.95350921, 0.64427125, 0.45573691, 0.34370604, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.51179266, 0.95350921, 0.64427125, 0.4783645, 0.36617002, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 0.98595673, 0.69515091, 0.52423614, 0.41087446, 0.34370604, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 1.01931262, 0.72133851, 0.54755926, 0.43325692, 0.36617002, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.61558151, 1.05362725, 0.74807048, 0.57119018, 0.45573691, 0.38853383, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.61558151, 1.08895338, 0.803307, 0.61951244, 0.50118381, 0.41087446, 0.36617002, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.61558151, 1.08895338, 0.803307, 0.61951244, 0.50118381, 0.43325692, 0.38853383, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.61558151, 1.08895338, 0.803307, 0.64427125, 0.52423614, 0.45573691, 0.41087446, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.45: [\n        [14.61464119, 0.59516323, 0.02916753],\n        [14.61464119, 0.803307, 0.25053367, 0.02916753],\n        [14.61464119, 0.95350921, 0.34370604, 0.09824532, 0.02916753],\n        [14.61464119, 1.24153244, 0.54755926, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 1.56271636, 0.72133851, 0.36617002, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 1.61558151, 0.803307, 0.45573691, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 1.91321158, 0.95350921, 0.57119018, 0.36617002, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 2.19988537, 1.08895338, 0.64427125, 0.41087446, 0.27464288, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.24153244, 0.74807048, 0.50118381, 0.34370604, 0.25053367, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.24153244, 0.74807048, 0.50118381, 0.36617002, 0.27464288, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.803307, 0.54755926, 0.41087446, 0.32104823, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.803307, 0.57119018, 0.43325692, 0.34370604, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.83188516, 0.59516323, 0.45573691, 0.36617002, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.28281462, 0.83188516, 0.59516323, 0.45573691, 0.36617002, 0.32104823, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.51179266, 0.95350921, 0.69515091, 0.52423614, 0.41087446, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.51179266, 0.95350921, 0.69515091, 0.52423614, 0.43325692, 0.36617002, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 0.98595673, 0.72133851, 0.54755926, 0.45573691, 0.38853383, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 1.01931262, 0.74807048, 0.57119018, 0.4783645, 0.41087446, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.84484982, 1.56271636, 1.01931262, 0.74807048, 0.59516323, 0.50118381, 0.43325692, 0.38853383, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n    1.50: [\n        [14.61464119, 0.54755926, 0.02916753],\n        [14.61464119, 0.803307, 0.25053367, 0.02916753],\n        [14.61464119, 0.86115354, 0.32104823, 0.09824532, 0.02916753],\n        [14.61464119, 1.24153244, 0.54755926, 0.25053367, 0.09824532, 0.02916753],\n        [14.61464119, 1.56271636, 0.72133851, 0.36617002, 0.19894916, 0.09824532, 0.02916753],\n        [14.61464119, 1.61558151, 0.803307, 0.45573691, 0.27464288, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 1.61558151, 0.83188516, 0.52423614, 0.34370604, 0.25053367, 0.17026083, 0.09824532, 0.02916753],\n        [14.61464119, 1.84880662, 0.95350921, 0.59516323, 0.38853383, 0.27464288, 0.19894916, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 1.84880662, 0.95350921, 0.59516323, 0.41087446, 0.29807833, 0.22545385, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 1.84880662, 0.95350921, 0.61951244, 0.43325692, 0.32104823, 0.25053367, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.19988537, 1.12534678, 0.72133851, 0.50118381, 0.36617002, 0.27464288, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.19988537, 1.12534678, 0.72133851, 0.50118381, 0.36617002, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.36326075, 1.24153244, 0.803307, 0.57119018, 0.43325692, 0.34370604, 0.29807833, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.36326075, 1.24153244, 0.803307, 0.57119018, 0.43325692, 0.34370604, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.36326075, 1.24153244, 0.803307, 0.59516323, 0.45573691, 0.36617002, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.36326075, 1.24153244, 0.803307, 0.59516323, 0.45573691, 0.38853383, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.32549286, 0.86115354, 0.64427125, 0.50118381, 0.41087446, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.36964464, 0.92192322, 0.69515091, 0.54755926, 0.45573691, 0.41087446, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n        [14.61464119, 2.45070267, 1.41535246, 0.95350921, 0.72133851, 0.57119018, 0.4783645, 0.43325692, 0.38853383, 0.36617002, 0.34370604, 0.32104823, 0.29807833, 0.27464288, 0.25053367, 0.22545385, 0.19894916, 0.17026083, 0.13792117, 0.09824532, 0.02916753],\n    ],\n}\n\nclass GITSScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"coeff\": (\"FLOAT\", {\"default\": 1.20, \"min\": 0.80, \"max\": 1.50, \"step\": 0.05}),\n                     \"steps\": (\"INT\", {\"default\": 10, \"min\": 2, \"max\": 1000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, coeff, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            if denoise <= 0.0:\n                return (torch.FloatTensor([]),)\n            total_steps = round(steps * denoise)\n\n        if steps <= 20:\n            sigmas = NOISE_LEVELS[round(coeff, 2)][steps-2][:]\n        else:\n            sigmas = NOISE_LEVELS[round(coeff, 2)][-1][:]\n            sigmas = loglinear_interp(sigmas, steps + 1)\n\n        sigmas = sigmas[-(total_steps + 1):]\n        sigmas[-1] = 0\n        return (torch.FloatTensor(sigmas), )\n\nNODE_CLASS_MAPPINGS = {\n    \"GITSScheduler\": GITSScheduler,\n}\n", "comfy_extras/nodes_stable3d.py": "import torch\nimport nodes\nimport comfy.utils\n\ndef camera_embeddings(elevation, azimuth):\n    elevation = torch.as_tensor([elevation])\n    azimuth = torch.as_tensor([azimuth])\n    embeddings = torch.stack(\n        [\n                torch.deg2rad(\n                    (90 - elevation) - (90)\n                ),  # Zero123 polar is 90-elevation\n                torch.sin(torch.deg2rad(azimuth)),\n                torch.cos(torch.deg2rad(azimuth)),\n                torch.deg2rad(\n                    90 - torch.full_like(elevation, 0)\n                ),\n        ], dim=-1).unsqueeze(1)\n\n    return embeddings\n\n\nclass StableZero123_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"elevation\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                              \"azimuth\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/3d_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, batch_size, elevation, azimuth):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = comfy.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        t = vae.encode(encode_pixels)\n        cam_embeds = camera_embeddings(elevation, azimuth)\n        cond = torch.cat([pooled, cam_embeds.to(pooled.device).repeat((pooled.shape[0], 1, 1))], dim=-1)\n\n        positive = [[cond, {\"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent})\n\nclass StableZero123_Conditioning_Batched:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 256, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"elevation\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                              \"azimuth\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                              \"elevation_batch_increment\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                              \"azimuth_batch_increment\": (\"FLOAT\", {\"default\": 0.0, \"min\": -180.0, \"max\": 180.0, \"step\": 0.1, \"round\": False}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/3d_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, batch_size, elevation, azimuth, elevation_batch_increment, azimuth_batch_increment):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = comfy.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        t = vae.encode(encode_pixels)\n\n        cam_embeds = []\n        for i in range(batch_size):\n            cam_embeds.append(camera_embeddings(elevation, azimuth))\n            elevation += elevation_batch_increment\n            azimuth += azimuth_batch_increment\n\n        cam_embeds = torch.cat(cam_embeds, dim=0)\n        cond = torch.cat([comfy.utils.repeat_to_batch_size(pooled, batch_size), cam_embeds], dim=-1)\n\n        positive = [[cond, {\"concat_latent_image\": t}]]\n        negative = [[torch.zeros_like(pooled), {\"concat_latent_image\": torch.zeros_like(t)}]]\n        latent = torch.zeros([batch_size, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent, \"batch_index\": [0] * batch_size})\n\nclass SV3D_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"init_image\": (\"IMAGE\",),\n                              \"vae\": (\"VAE\",),\n                              \"width\": (\"INT\", {\"default\": 576, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 576, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 8}),\n                              \"video_frames\": (\"INT\", {\"default\": 21, \"min\": 1, \"max\": 4096}),\n                              \"elevation\": (\"FLOAT\", {\"default\": 0.0, \"min\": -90.0, \"max\": 90.0, \"step\": 0.1, \"round\": False}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/3d_models\"\n\n    def encode(self, clip_vision, init_image, vae, width, height, video_frames, elevation):\n        output = clip_vision.encode_image(init_image)\n        pooled = output.image_embeds.unsqueeze(0)\n        pixels = comfy.utils.common_upscale(init_image.movedim(-1,1), width, height, \"bilinear\", \"center\").movedim(1,-1)\n        encode_pixels = pixels[:,:,:,:3]\n        t = vae.encode(encode_pixels)\n\n        azimuth = 0\n        azimuth_increment = 360 / (max(video_frames, 2) - 1)\n\n        elevations = []\n        azimuths = []\n        for i in range(video_frames):\n            elevations.append(elevation)\n            azimuths.append(azimuth)\n            azimuth += azimuth_increment\n\n        positive = [[pooled, {\"concat_latent_image\": t, \"elevation\": elevations, \"azimuth\": azimuths}]]\n        negative = [[torch.zeros_like(pooled), {\"concat_latent_image\": torch.zeros_like(t), \"elevation\": elevations, \"azimuth\": azimuths}]]\n        latent = torch.zeros([video_frames, 4, height // 8, width // 8])\n        return (positive, negative, {\"samples\":latent})\n\n\nNODE_CLASS_MAPPINGS = {\n    \"StableZero123_Conditioning\": StableZero123_Conditioning,\n    \"StableZero123_Conditioning_Batched\": StableZero123_Conditioning_Batched,\n    \"SV3D_Conditioning\": SV3D_Conditioning,\n}\n", "comfy_extras/nodes_rebatch.py": "import torch\n\nclass LatentRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"latents\": (\"LATENT\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n\n    FUNCTION = \"rebatch\"\n\n    CATEGORY = \"latent/batch\"\n\n    @staticmethod\n    def get_batch(latents, list_ind, offset):\n        '''prepare a batch out of the list of latents'''\n        samples = latents[list_ind]['samples']\n        shape = samples.shape\n        mask = latents[list_ind]['noise_mask'] if 'noise_mask' in latents[list_ind] else torch.ones((shape[0], 1, shape[2]*8, shape[3]*8), device='cpu')\n        if mask.shape[-1] != shape[-1] * 8 or mask.shape[-2] != shape[-2]:\n            torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(shape[-2]*8, shape[-1]*8), mode=\"bilinear\")\n        if mask.shape[0] < samples.shape[0]:\n            mask = mask.repeat((shape[0] - 1) // mask.shape[0] + 1, 1, 1, 1)[:shape[0]]\n        if 'batch_index' in latents[list_ind]:\n            batch_inds = latents[list_ind]['batch_index']\n        else:\n            batch_inds = [x+offset for x in range(shape[0])]\n        return samples, mask, batch_inds\n\n    @staticmethod\n    def get_slices(indexable, num, batch_size):\n        '''divides an indexable object into num slices of length batch_size, and a remainder'''\n        slices = []\n        for i in range(num):\n            slices.append(indexable[i*batch_size:(i+1)*batch_size])\n        if num * batch_size < len(indexable):\n            return slices, indexable[num * batch_size:]\n        else:\n            return slices, None\n    \n    @staticmethod\n    def slice_batch(batch, num, batch_size):\n        result = [LatentRebatch.get_slices(x, num, batch_size) for x in batch]\n        return list(zip(*result))\n\n    @staticmethod\n    def cat_batch(batch1, batch2):\n        if batch1[0] is None:\n            return batch2\n        result = [torch.cat((b1, b2)) if torch.is_tensor(b1) else b1 + b2 for b1, b2 in zip(batch1, batch2)]\n        return result\n\n    def rebatch(self, latents, batch_size):\n        batch_size = batch_size[0]\n\n        output_list = []\n        current_batch = (None, None, None)\n        processed = 0\n\n        for i in range(len(latents)):\n            # fetch new entry of list\n            #samples, masks, indices = self.get_batch(latents, i)\n            next_batch = self.get_batch(latents, i, processed)\n            processed += len(next_batch[2])\n            # set to current if current is None\n            if current_batch[0] is None:\n                current_batch = next_batch\n            # add previous to list if dimensions do not match\n            elif next_batch[0].shape[-1] != current_batch[0].shape[-1] or next_batch[0].shape[-2] != current_batch[0].shape[-2]:\n                sliced, _ = self.slice_batch(current_batch, 1, batch_size)\n                output_list.append({'samples': sliced[0][0], 'noise_mask': sliced[1][0], 'batch_index': sliced[2][0]})\n                current_batch = next_batch\n            # cat if everything checks out\n            else:\n                current_batch = self.cat_batch(current_batch, next_batch)\n\n            # add to list if dimensions gone above target batch size\n            if current_batch[0].shape[0] > batch_size:\n                num = current_batch[0].shape[0] // batch_size\n                sliced, remainder = self.slice_batch(current_batch, num, batch_size)\n                \n                for i in range(num):\n                    output_list.append({'samples': sliced[0][i], 'noise_mask': sliced[1][i], 'batch_index': sliced[2][i]})\n\n                current_batch = remainder\n\n        #add remainder\n        if current_batch[0] is not None:\n            sliced, _ = self.slice_batch(current_batch, 1, batch_size)\n            output_list.append({'samples': sliced[0][0], 'noise_mask': sliced[1][0], 'batch_index': sliced[2][0]})\n\n        #get rid of empty masks\n        for s in output_list:\n            if s['noise_mask'].mean() == 1.0:\n                del s['noise_mask']\n\n        return (output_list,)\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n\n    FUNCTION = \"rebatch\"\n\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]\n\n        output_list = []\n        all_images = []\n        for img in images:\n            for i in range(img.shape[0]):\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size):\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))\n\n        return (output_list,)\n\nNODE_CLASS_MAPPINGS = {\n    \"RebatchLatents\": LatentRebatch,\n    \"RebatchImages\": ImageRebatch,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"RebatchLatents\": \"Rebatch Latents\",\n    \"RebatchImages\": \"Rebatch Images\",\n}\n", "comfy_extras/nodes_compositing.py": "import numpy as np\nimport torch\nimport comfy.utils\nfrom enum import Enum\n\ndef resize_mask(mask, shape):\n    return torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(shape[0], shape[1]), mode=\"bilinear\").squeeze(1)\n\nclass PorterDuffMode(Enum):\n    ADD = 0\n    CLEAR = 1\n    DARKEN = 2\n    DST = 3\n    DST_ATOP = 4\n    DST_IN = 5\n    DST_OUT = 6\n    DST_OVER = 7\n    LIGHTEN = 8\n    MULTIPLY = 9\n    OVERLAY = 10\n    SCREEN = 11\n    SRC = 12\n    SRC_ATOP = 13\n    SRC_IN = 14\n    SRC_OUT = 15\n    SRC_OVER = 16\n    XOR = 17\n\n\ndef porter_duff_composite(src_image: torch.Tensor, src_alpha: torch.Tensor, dst_image: torch.Tensor, dst_alpha: torch.Tensor, mode: PorterDuffMode):\n    # convert mask to alpha\n    src_alpha = 1 - src_alpha\n    dst_alpha = 1 - dst_alpha\n    # premultiply alpha\n    src_image = src_image * src_alpha\n    dst_image = dst_image * dst_alpha\n\n    # composite ops below assume alpha-premultiplied images\n    if mode == PorterDuffMode.ADD:\n        out_alpha = torch.clamp(src_alpha + dst_alpha, 0, 1)\n        out_image = torch.clamp(src_image + dst_image, 0, 1)\n    elif mode == PorterDuffMode.CLEAR:\n        out_alpha = torch.zeros_like(dst_alpha)\n        out_image = torch.zeros_like(dst_image)\n    elif mode == PorterDuffMode.DARKEN:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image + torch.min(src_image, dst_image)\n    elif mode == PorterDuffMode.DST:\n        out_alpha = dst_alpha\n        out_image = dst_image\n    elif mode == PorterDuffMode.DST_ATOP:\n        out_alpha = src_alpha\n        out_image = src_alpha * dst_image + (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.DST_IN:\n        out_alpha = src_alpha * dst_alpha\n        out_image = dst_image * src_alpha\n    elif mode == PorterDuffMode.DST_OUT:\n        out_alpha = (1 - src_alpha) * dst_alpha\n        out_image = (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.DST_OVER:\n        out_alpha = dst_alpha + (1 - dst_alpha) * src_alpha\n        out_image = dst_image + (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.LIGHTEN:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image + torch.max(src_image, dst_image)\n    elif mode == PorterDuffMode.MULTIPLY:\n        out_alpha = src_alpha * dst_alpha\n        out_image = src_image * dst_image\n    elif mode == PorterDuffMode.OVERLAY:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = torch.where(2 * dst_image < dst_alpha, 2 * src_image * dst_image,\n            src_alpha * dst_alpha - 2 * (dst_alpha - src_image) * (src_alpha - dst_image))\n    elif mode == PorterDuffMode.SCREEN:\n        out_alpha = src_alpha + dst_alpha - src_alpha * dst_alpha\n        out_image = src_image + dst_image - src_image * dst_image\n    elif mode == PorterDuffMode.SRC:\n        out_alpha = src_alpha\n        out_image = src_image\n    elif mode == PorterDuffMode.SRC_ATOP:\n        out_alpha = dst_alpha\n        out_image = dst_alpha * src_image + (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.SRC_IN:\n        out_alpha = src_alpha * dst_alpha\n        out_image = src_image * dst_alpha\n    elif mode == PorterDuffMode.SRC_OUT:\n        out_alpha = (1 - dst_alpha) * src_alpha\n        out_image = (1 - dst_alpha) * src_image\n    elif mode == PorterDuffMode.SRC_OVER:\n        out_alpha = src_alpha + (1 - src_alpha) * dst_alpha\n        out_image = src_image + (1 - src_alpha) * dst_image\n    elif mode == PorterDuffMode.XOR:\n        out_alpha = (1 - dst_alpha) * src_alpha + (1 - src_alpha) * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image\n    else:\n        return None, None\n\n    # back to non-premultiplied alpha\n    out_image = torch.where(out_alpha > 1e-5, out_image / out_alpha, torch.zeros_like(out_image))\n    out_image = torch.clamp(out_image, 0, 1)\n    # convert alpha to mask\n    out_alpha = 1 - out_alpha\n    return out_image, out_alpha\n\n\nclass PorterDuffImageComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"source\": (\"IMAGE\",),\n                \"source_alpha\": (\"MASK\",),\n                \"destination\": (\"IMAGE\",),\n                \"destination_alpha\": (\"MASK\",),\n                \"mode\": ([mode.name for mode in PorterDuffMode], {\"default\": PorterDuffMode.DST.name}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"composite\"\n    CATEGORY = \"mask/compositing\"\n\n    def composite(self, source: torch.Tensor, source_alpha: torch.Tensor, destination: torch.Tensor, destination_alpha: torch.Tensor, mode):\n        batch_size = min(len(source), len(source_alpha), len(destination), len(destination_alpha))\n        out_images = []\n        out_alphas = []\n\n        for i in range(batch_size):\n            src_image = source[i]\n            dst_image = destination[i]\n\n            assert src_image.shape[2] == dst_image.shape[2] # inputs need to have same number of channels\n\n            src_alpha = source_alpha[i].unsqueeze(2)\n            dst_alpha = destination_alpha[i].unsqueeze(2)\n\n            if dst_alpha.shape[:2] != dst_image.shape[:2]:\n                upscale_input = dst_alpha.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = comfy.utils.common_upscale(upscale_input, dst_image.shape[1], dst_image.shape[0], upscale_method='bicubic', crop='center')\n                dst_alpha = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n            if src_image.shape != dst_image.shape:\n                upscale_input = src_image.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = comfy.utils.common_upscale(upscale_input, dst_image.shape[1], dst_image.shape[0], upscale_method='bicubic', crop='center')\n                src_image = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n            if src_alpha.shape != dst_alpha.shape:\n                upscale_input = src_alpha.unsqueeze(0).permute(0, 3, 1, 2)\n                upscale_output = comfy.utils.common_upscale(upscale_input, dst_alpha.shape[1], dst_alpha.shape[0], upscale_method='bicubic', crop='center')\n                src_alpha = upscale_output.permute(0, 2, 3, 1).squeeze(0)\n\n            out_image, out_alpha = porter_duff_composite(src_image, src_alpha, dst_image, dst_alpha, PorterDuffMode[mode])\n\n            out_images.append(out_image)\n            out_alphas.append(out_alpha.squeeze(2))\n\n        result = (torch.stack(out_images), torch.stack(out_alphas))\n        return result\n\n\nclass SplitImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                }\n        }\n\n    CATEGORY = \"mask/compositing\"\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"split_image_with_alpha\"\n\n    def split_image_with_alpha(self, image: torch.Tensor):\n        out_images = [i[:,:,:3] for i in image]\n        out_alphas = [i[:,:,3] if i.shape[2] > 3 else torch.ones_like(i[:,:,0]) for i in image]\n        result = (torch.stack(out_images), 1.0 - torch.stack(out_alphas))\n        return result\n\n\nclass JoinImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"alpha\": (\"MASK\",),\n                }\n        }\n\n    CATEGORY = \"mask/compositing\"\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"join_image_with_alpha\"\n\n    def join_image_with_alpha(self, image: torch.Tensor, alpha: torch.Tensor):\n        batch_size = min(len(image), len(alpha))\n        out_images = []\n\n        alpha = 1.0 - resize_mask(alpha, image.shape[1:])\n        for i in range(batch_size):\n           out_images.append(torch.cat((image[i][:,:,:3], alpha[i].unsqueeze(2)), dim=2))\n\n        result = (torch.stack(out_images),)\n        return result\n\n\nNODE_CLASS_MAPPINGS = {\n    \"PorterDuffImageComposite\": PorterDuffImageComposite,\n    \"SplitImageWithAlpha\": SplitImageWithAlpha,\n    \"JoinImageWithAlpha\": JoinImageWithAlpha,\n}\n\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"PorterDuffImageComposite\": \"Porter-Duff Image Composite\",\n    \"SplitImageWithAlpha\": \"Split Image with Alpha\",\n    \"JoinImageWithAlpha\": \"Join Image with Alpha\",\n}\n", "comfy_extras/nodes_model_merging_model_specific.py": "import comfy_extras.nodes_model_merging\n\nclass ModelMergeSD1(comfy_extras.nodes_model_merging.ModelMergeBlocks):\n    CATEGORY = \"advanced/model_merging/model_specific\"\n    @classmethod\n    def INPUT_TYPES(s):\n        arg_dict = { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",)}\n\n        argument = (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n\n        arg_dict[\"time_embed.\"] = argument\n        arg_dict[\"label_emb.\"] = argument\n\n        for i in range(12):\n            arg_dict[\"input_blocks.{}.\".format(i)] = argument\n\n        for i in range(3):\n            arg_dict[\"middle_block.{}.\".format(i)] = argument\n\n        for i in range(12):\n            arg_dict[\"output_blocks.{}.\".format(i)] = argument\n\n        arg_dict[\"out.\"] = argument\n\n        return {\"required\": arg_dict}\n\n\nclass ModelMergeSDXL(comfy_extras.nodes_model_merging.ModelMergeBlocks):\n    CATEGORY = \"advanced/model_merging/model_specific\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        arg_dict = { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",)}\n\n        argument = (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n\n        arg_dict[\"time_embed.\"] = argument\n        arg_dict[\"label_emb.\"] = argument\n\n        for i in range(9):\n            arg_dict[\"input_blocks.{}\".format(i)] = argument\n\n        for i in range(3):\n            arg_dict[\"middle_block.{}\".format(i)] = argument\n\n        for i in range(9):\n            arg_dict[\"output_blocks.{}\".format(i)] = argument\n\n        arg_dict[\"out.\"] = argument\n\n        return {\"required\": arg_dict}\n\nclass ModelMergeSD3_2B(comfy_extras.nodes_model_merging.ModelMergeBlocks):\n    CATEGORY = \"advanced/model_merging/model_specific\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        arg_dict = { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",)}\n\n        argument = (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n\n        arg_dict[\"pos_embed.\"] = argument\n        arg_dict[\"x_embedder.\"] = argument\n        arg_dict[\"context_embedder.\"] = argument\n        arg_dict[\"y_embedder.\"] = argument\n        arg_dict[\"t_embedder.\"] = argument\n\n        for i in range(24):\n            arg_dict[\"joint_blocks.{}.\".format(i)] = argument\n\n        arg_dict[\"final_layer.\"] = argument\n\n        return {\"required\": arg_dict}\n\nNODE_CLASS_MAPPINGS = {\n    \"ModelMergeSD1\": ModelMergeSD1,\n    \"ModelMergeSD2\": ModelMergeSD1, #SD1 and SD2 have the same blocks\n    \"ModelMergeSDXL\": ModelMergeSDXL,\n    \"ModelMergeSD3_2B\": ModelMergeSD3_2B,\n}\n", "comfy_extras/nodes_mask.py": "import numpy as np\nimport scipy.ndimage\nimport torch\nimport comfy.utils\n\nfrom nodes import MAX_RESOLUTION\n\ndef composite(destination, source, x, y, mask = None, multiplier = 8, resize_source = False):\n    source = source.to(destination.device)\n    if resize_source:\n        source = torch.nn.functional.interpolate(source, size=(destination.shape[2], destination.shape[3]), mode=\"bilinear\")\n\n    source = comfy.utils.repeat_to_batch_size(source, destination.shape[0])\n\n    x = max(-source.shape[3] * multiplier, min(x, destination.shape[3] * multiplier))\n    y = max(-source.shape[2] * multiplier, min(y, destination.shape[2] * multiplier))\n\n    left, top = (x // multiplier, y // multiplier)\n    right, bottom = (left + source.shape[3], top + source.shape[2],)\n\n    if mask is None:\n        mask = torch.ones_like(source)\n    else:\n        mask = mask.to(destination.device, copy=True)\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(source.shape[2], source.shape[3]), mode=\"bilinear\")\n        mask = comfy.utils.repeat_to_batch_size(mask, source.shape[0])\n\n    # calculate the bounds of the source that will be overlapping the destination\n    # this prevents the source trying to overwrite latent pixels that are out of bounds\n    # of the destination\n    visible_width, visible_height = (destination.shape[3] - left + min(0, x), destination.shape[2] - top + min(0, y),)\n\n    mask = mask[:, :, :visible_height, :visible_width]\n    inverse_mask = torch.ones_like(mask) - mask\n\n    source_portion = mask * source[:, :, :visible_height, :visible_width]\n    destination_portion = inverse_mask  * destination[:, :, top:bottom, left:right]\n\n    destination[:, :, top:bottom, left:right] = source_portion + destination_portion\n    return destination\n\nclass LatentCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"LATENT\",),\n                \"source\": (\"LATENT\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"latent\"\n\n    def composite(self, destination, source, x, y, resize_source, mask = None):\n        output = destination.copy()\n        destination = destination[\"samples\"].clone()\n        source = source[\"samples\"]\n        output[\"samples\"] = composite(destination, source, x, y, mask, 8, resize_source)\n        return (output,)\n\nclass ImageCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"IMAGE\",),\n                \"source\": (\"IMAGE\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"composite\"\n\n    CATEGORY = \"image\"\n\n    def composite(self, destination, source, x, y, resize_source, mask = None):\n        destination = destination.clone().movedim(-1, 1)\n        output = composite(destination, source.movedim(-1, 1), x, y, mask, 1, resize_source).movedim(1, -1)\n        return (output,)\n\nclass MaskToImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"mask\": (\"MASK\",),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mask_to_image\"\n\n    def mask_to_image(self, mask):\n        result = mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])).movedim(1, -1).expand(-1, -1, -1, 3)\n        return (result,)\n\nclass ImageToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"channel\": ([\"red\", \"green\", \"blue\", \"alpha\"],),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"image_to_mask\"\n\n    def image_to_mask(self, image, channel):\n        channels = [\"red\", \"green\", \"blue\", \"alpha\"]\n        mask = image[:, :, :, channels.index(channel)]\n        return (mask,)\n\nclass ImageColorToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"image_to_mask\"\n\n    def image_to_mask(self, image, color):\n        temp = (torch.clamp(image, 0, 1.0) * 255.0).round().to(torch.int)\n        temp = torch.bitwise_left_shift(temp[:,:,:,0], 16) + torch.bitwise_left_shift(temp[:,:,:,1], 8) + temp[:,:,:,2]\n        mask = torch.where(temp == color, 255, 0).float()\n        return (mask,)\n\nclass SolidMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"value\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"solid\"\n\n    def solid(self, value, width, height):\n        out = torch.full((1, height, width), value, dtype=torch.float32, device=\"cpu\")\n        return (out,)\n\nclass InvertMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"invert\"\n\n    def invert(self, mask):\n        out = 1.0 - mask\n        return (out,)\n\nclass CropMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"crop\"\n\n    def crop(self, mask, x, y, width, height):\n        mask = mask.reshape((-1, mask.shape[-2], mask.shape[-1]))\n        out = mask[:, y:y + height, x:x + width]\n        return (out,)\n\nclass MaskComposite:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"destination\": (\"MASK\",),\n                \"source\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"operation\": ([\"multiply\", \"add\", \"subtract\", \"and\", \"or\", \"xor\"],),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"combine\"\n\n    def combine(self, destination, source, x, y, operation):\n        output = destination.reshape((-1, destination.shape[-2], destination.shape[-1])).clone()\n        source = source.reshape((-1, source.shape[-2], source.shape[-1]))\n\n        left, top = (x, y,)\n        right, bottom = (min(left + source.shape[-1], destination.shape[-1]), min(top + source.shape[-2], destination.shape[-2]))\n        visible_width, visible_height = (right - left, bottom - top,)\n\n        source_portion = source[:, :visible_height, :visible_width]\n        destination_portion = destination[:, top:bottom, left:right]\n\n        if operation == \"multiply\":\n            output[:, top:bottom, left:right] = destination_portion * source_portion\n        elif operation == \"add\":\n            output[:, top:bottom, left:right] = destination_portion + source_portion\n        elif operation == \"subtract\":\n            output[:, top:bottom, left:right] = destination_portion - source_portion\n        elif operation == \"and\":\n            output[:, top:bottom, left:right] = torch.bitwise_and(destination_portion.round().bool(), source_portion.round().bool()).float()\n        elif operation == \"or\":\n            output[:, top:bottom, left:right] = torch.bitwise_or(destination_portion.round().bool(), source_portion.round().bool()).float()\n        elif operation == \"xor\":\n            output[:, top:bottom, left:right] = torch.bitwise_xor(destination_portion.round().bool(), source_portion.round().bool()).float()\n\n        output = torch.clamp(output, 0.0, 1.0)\n\n        return (output,)\n\nclass FeatherMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"feather\"\n\n    def feather(self, mask, left, top, right, bottom):\n        output = mask.reshape((-1, mask.shape[-2], mask.shape[-1])).clone()\n\n        left = min(left, output.shape[-1])\n        right = min(right, output.shape[-1])\n        top = min(top, output.shape[-2])\n        bottom = min(bottom, output.shape[-2])\n\n        for x in range(left):\n            feather_rate = (x + 1.0) / left\n            output[:, :, x] *= feather_rate\n\n        for x in range(right):\n            feather_rate = (x + 1) / right\n            output[:, :, -x] *= feather_rate\n\n        for y in range(top):\n            feather_rate = (y + 1) / top\n            output[:, y, :] *= feather_rate\n\n        for y in range(bottom):\n            feather_rate = (y + 1) / bottom\n            output[:, -y, :] *= feather_rate\n\n        return (output,)\n    \nclass GrowMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"expand\": (\"INT\", {\"default\": 0, \"min\": -MAX_RESOLUTION, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"tapered_corners\": (\"BOOLEAN\", {\"default\": True}),\n            },\n        }\n    \n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n\n    FUNCTION = \"expand_mask\"\n\n    def expand_mask(self, mask, expand, tapered_corners):\n        c = 0 if tapered_corners else 1\n        kernel = np.array([[c, 1, c],\n                           [1, 1, 1],\n                           [c, 1, c]])\n        mask = mask.reshape((-1, mask.shape[-2], mask.shape[-1]))\n        out = []\n        for m in mask:\n            output = m.numpy()\n            for _ in range(abs(expand)):\n                if expand < 0:\n                    output = scipy.ndimage.grey_erosion(output, footprint=kernel)\n                else:\n                    output = scipy.ndimage.grey_dilation(output, footprint=kernel)\n            output = torch.from_numpy(output)\n            out.append(output)\n        return (torch.stack(out, dim=0),)\n\nclass ThresholdMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"mask\": (\"MASK\",),\n                    \"value\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                }\n        }\n\n    CATEGORY = \"mask\"\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"image_to_mask\"\n\n    def image_to_mask(self, mask, value):\n        mask = (mask > value).float()\n        return (mask,)\n\n\nNODE_CLASS_MAPPINGS = {\n    \"LatentCompositeMasked\": LatentCompositeMasked,\n    \"ImageCompositeMasked\": ImageCompositeMasked,\n    \"MaskToImage\": MaskToImage,\n    \"ImageToMask\": ImageToMask,\n    \"ImageColorToMask\": ImageColorToMask,\n    \"SolidMask\": SolidMask,\n    \"InvertMask\": InvertMask,\n    \"CropMask\": CropMask,\n    \"MaskComposite\": MaskComposite,\n    \"FeatherMask\": FeatherMask,\n    \"GrowMask\": GrowMask,\n    \"ThresholdMask\": ThresholdMask,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"ImageToMask\": \"Convert Image to Mask\",\n    \"MaskToImage\": \"Convert Mask to Image\",\n}\n", "comfy_extras/nodes_hypertile.py": "#Taken from: https://github.com/tfernd/HyperTile/\n\nimport math\nfrom einops import rearrange\n# Use torch rng for consistency across generations\nfrom torch import randint\n\ndef random_divisor(value: int, min_value: int, /, max_options: int = 1) -> int:\n    min_value = min(min_value, value)\n\n    # All big divisors of value (inclusive)\n    divisors = [i for i in range(min_value, value + 1) if value % i == 0]\n\n    ns = [value // i for i in divisors[:max_options]]  # has at least 1 element\n\n    if len(ns) - 1 > 0:\n        idx = randint(low=0, high=len(ns) - 1, size=(1,)).item()\n    else:\n        idx = 0\n\n    return ns[idx]\n\nclass HyperTile:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"tile_size\": (\"INT\", {\"default\": 256, \"min\": 1, \"max\": 2048}),\n                             \"swap_size\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 128}),\n                             \"max_depth\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10}),\n                             \"scale_depth\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"model_patches\"\n\n    def patch(self, model, tile_size, swap_size, max_depth, scale_depth):\n        model_channels = model.model.model_config.unet_config[\"model_channels\"]\n\n        latent_tile_size = max(32, tile_size) // 8\n        self.temp = None\n\n        def hypertile_in(q, k, v, extra_options):\n            model_chans = q.shape[-2]\n            orig_shape = extra_options['original_shape']\n            apply_to = []\n            for i in range(max_depth + 1):\n                apply_to.append((orig_shape[-2] / (2 ** i)) * (orig_shape[-1] / (2 ** i)))\n\n            if model_chans in apply_to:\n                shape = extra_options[\"original_shape\"]\n                aspect_ratio = shape[-1] / shape[-2]\n\n                hw = q.size(1)\n                h, w = round(math.sqrt(hw * aspect_ratio)), round(math.sqrt(hw / aspect_ratio))\n\n                factor = (2 ** apply_to.index(model_chans)) if scale_depth else 1\n                nh = random_divisor(h, latent_tile_size * factor, swap_size)\n                nw = random_divisor(w, latent_tile_size * factor, swap_size)\n\n                if nh * nw > 1:\n                    q = rearrange(q, \"b (nh h nw w) c -> (b nh nw) (h w) c\", h=h // nh, w=w // nw, nh=nh, nw=nw)\n                    self.temp = (nh, nw, h, w)\n                return q, k, v\n\n            return q, k, v\n        def hypertile_out(out, extra_options):\n            if self.temp is not None:\n                nh, nw, h, w = self.temp\n                self.temp = None\n                out = rearrange(out, \"(b nh nw) hw c -> b nh nw hw c\", nh=nh, nw=nw)\n                out = rearrange(out, \"b nh nw (h w) c -> b (nh h nw w) c\", h=h // nh, w=w // nw)\n            return out\n\n\n        m = model.clone()\n        m.set_model_attn1_patch(hypertile_in)\n        m.set_model_attn1_output_patch(hypertile_out)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"HyperTile\": HyperTile,\n}\n", "comfy_extras/nodes_sdupscale.py": "import torch\nimport comfy.utils\n\nclass SD_4XUpscale_Conditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"positive\": (\"CONDITIONING\",),\n                              \"negative\": (\"CONDITIONING\",),\n                              \"scale_ratio\": (\"FLOAT\", {\"default\": 4.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/upscale_diffusion\"\n\n    def encode(self, images, positive, negative, scale_ratio, noise_augmentation):\n        width = max(1, round(images.shape[-2] * scale_ratio))\n        height = max(1, round(images.shape[-3] * scale_ratio))\n\n        pixels = comfy.utils.common_upscale((images.movedim(-1,1) * 2.0) - 1.0, width // 4, height // 4, \"bilinear\", \"center\")\n\n        out_cp = []\n        out_cn = []\n\n        for t in positive:\n            n = [t[0], t[1].copy()]\n            n[1]['concat_image'] = pixels\n            n[1]['noise_augmentation'] = noise_augmentation\n            out_cp.append(n)\n\n        for t in negative:\n            n = [t[0], t[1].copy()]\n            n[1]['concat_image'] = pixels\n            n[1]['noise_augmentation'] = noise_augmentation\n            out_cn.append(n)\n\n        latent = torch.zeros([images.shape[0], 4, height // 4, width // 4])\n        return (out_cp, out_cn, {\"samples\":latent})\n\nNODE_CLASS_MAPPINGS = {\n    \"SD_4XUpscale_Conditioning\": SD_4XUpscale_Conditioning,\n}\n", "comfy_extras/nodes_sag.py": "import torch\nfrom torch import einsum\nimport torch.nn.functional as F\nimport math\n\nfrom einops import rearrange, repeat\nfrom comfy.ldm.modules.attention import optimized_attention\nimport comfy.samplers\n\n# from comfy/ldm/modules/attention.py\n# but modified to return attention scores as well as output\ndef attention_basic_with_sim(q, k, v, heads, mask=None, attn_precision=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),\n    )\n\n    # force cast to fp32 to avoid overflowing\n    if attn_precision == torch.float32:\n        sim = einsum('b i d, b j d -> b i j', q.float(), k.float()) * scale\n    else:\n        sim = einsum('b i d, b j d -> b i j', q, k) * scale\n\n    del q, k\n\n    if mask is not None:\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n\n    # attention, what we cannot get enough of\n    sim = sim.softmax(dim=-1)\n\n    out = einsum('b i j, b j d -> b i d', sim.to(v.dtype), v)\n    out = (\n        out.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return (out, sim)\n\ndef create_blur_map(x0, attn, sigma=3.0, threshold=1.0):\n    # reshape and GAP the attention map\n    _, hw1, hw2 = attn.shape\n    b, _, lh, lw = x0.shape\n    attn = attn.reshape(b, -1, hw1, hw2)\n    # Global Average Pool\n    mask = attn.mean(1, keepdim=False).sum(1, keepdim=False) > threshold\n    ratio = 2**(math.ceil(math.sqrt(lh * lw / hw1)) - 1).bit_length()\n    mid_shape = [math.ceil(lh / ratio), math.ceil(lw / ratio)]\n\n    # Reshape\n    mask = (\n        mask.reshape(b, *mid_shape)\n        .unsqueeze(1)\n        .type(attn.dtype)\n    )\n    # Upsample\n    mask = F.interpolate(mask, (lh, lw))\n\n    blurred = gaussian_blur_2d(x0, kernel_size=9, sigma=sigma)\n    blurred = blurred * mask + x0 * (1 - mask)\n    return blurred\n\ndef gaussian_blur_2d(img, kernel_size, sigma):\n    ksize_half = (kernel_size - 1) * 0.5\n\n    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size)\n\n    pdf = torch.exp(-0.5 * (x / sigma).pow(2))\n\n    x_kernel = pdf / pdf.sum()\n    x_kernel = x_kernel.to(device=img.device, dtype=img.dtype)\n\n    kernel2d = torch.mm(x_kernel[:, None], x_kernel[None, :])\n    kernel2d = kernel2d.expand(img.shape[-3], 1, kernel2d.shape[0], kernel2d.shape[1])\n\n    padding = [kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2]\n\n    img = F.pad(img, padding, mode=\"reflect\")\n    img = F.conv2d(img, kernel2d, groups=img.shape[-3])\n    return img\n\nclass SelfAttentionGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"scale\": (\"FLOAT\", {\"default\": 0.5, \"min\": -2.0, \"max\": 5.0, \"step\": 0.1}),\n                             \"blur_sigma\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.1}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing\"\n\n    def patch(self, model, scale, blur_sigma):\n        m = model.clone()\n\n        attn_scores = None\n\n        # TODO: make this work properly with chunked batches\n        #       currently, we can only save the attn from one UNet call\n        def attn_and_record(q, k, v, extra_options):\n            nonlocal attn_scores\n            # if uncond, save the attention scores\n            heads = extra_options[\"n_heads\"]\n            cond_or_uncond = extra_options[\"cond_or_uncond\"]\n            b = q.shape[0] // len(cond_or_uncond)\n            if 1 in cond_or_uncond:\n                uncond_index = cond_or_uncond.index(1)\n                # do the entire attention operation, but save the attention scores to attn_scores\n                (out, sim) = attention_basic_with_sim(q, k, v, heads=heads, attn_precision=extra_options[\"attn_precision\"])\n                # when using a higher batch size, I BELIEVE the result batch dimension is [uc1, ... ucn, c1, ... cn]\n                n_slices = heads * b\n                attn_scores = sim[n_slices * uncond_index:n_slices * (uncond_index+1)]\n                return out\n            else:\n                return optimized_attention(q, k, v, heads=heads, attn_precision=extra_options[\"attn_precision\"])\n\n        def post_cfg_function(args):\n            nonlocal attn_scores\n            uncond_attn = attn_scores\n\n            sag_scale = scale\n            sag_sigma = blur_sigma\n            sag_threshold = 1.0\n            model = args[\"model\"]\n            uncond_pred = args[\"uncond_denoised\"]\n            uncond = args[\"uncond\"]\n            cfg_result = args[\"denoised\"]\n            sigma = args[\"sigma\"]\n            model_options = args[\"model_options\"]\n            x = args[\"input\"]\n            if min(cfg_result.shape[2:]) <= 4: #skip when too small to add padding\n                return cfg_result\n\n            # create the adversarially blurred image\n            degraded = create_blur_map(uncond_pred, uncond_attn, sag_sigma, sag_threshold)\n            degraded_noised = degraded + x - uncond_pred\n            # call into the UNet\n            (sag,) = comfy.samplers.calc_cond_batch(model, [uncond], degraded_noised, sigma, model_options)\n            return cfg_result + (degraded - sag) * sag_scale\n\n        m.set_model_sampler_post_cfg_function(post_cfg_function, disable_cfg1_optimization=True)\n\n        # from diffusers:\n        # unet.mid_block.attentions[0].transformer_blocks[0].attn1.patch\n        m.set_model_attn1_replace(attn_and_record, \"middle\", 0, 0)\n\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"SelfAttentionGuidance\": SelfAttentionGuidance,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"SelfAttentionGuidance\": \"Self-Attention Guidance\",\n}\n", "comfy_extras/nodes_images.py": "import nodes\nimport folder_paths\nfrom comfy.cli_args import args\n\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngInfo\n\nimport numpy as np\nimport json\nimport os\n\nMAX_RESOLUTION = nodes.MAX_RESOLUTION\n\nclass ImageCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"crop\"\n\n    CATEGORY = \"image/transform\"\n\n    def crop(self, image, width, height, x, y):\n        x = min(x, image.shape[2] - 1)\n        y = min(y, image.shape[1] - 1)\n        to_x = width + x\n        to_y = height + y\n        img = image[:,y:to_y, x:to_x, :]\n        return (img,)\n\nclass RepeatImageBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"repeat\"\n\n    CATEGORY = \"image/batch\"\n\n    def repeat(self, image, amount):\n        s = image.repeat((amount, 1,1,1))\n        return (s,)\n\nclass ImageFromBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"batch_index\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 4095}),\n                              \"length\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"frombatch\"\n\n    CATEGORY = \"image/batch\"\n\n    def frombatch(self, image, batch_index, length):\n        s_in = image\n        batch_index = min(s_in.shape[0] - 1, batch_index)\n        length = min(s_in.shape[0] - batch_index, length)\n        s = s_in[batch_index:batch_index + length].clone()\n        return (s,)\n\nclass SaveAnimatedWEBP:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    methods = {\"default\": 4, \"fastest\": 0, \"slowest\": 6}\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"}),\n                     \"fps\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.01, \"max\": 1000.0, \"step\": 0.01}),\n                     \"lossless\": (\"BOOLEAN\", {\"default\": True}),\n                     \"quality\": (\"INT\", {\"default\": 80, \"min\": 0, \"max\": 100}),\n                     \"method\": (list(s.methods.keys()),),\n                     # \"num_frames\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 8192}),\n                     },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image/animation\"\n\n    def save_images(self, images, fps, filename_prefix, lossless, quality, method, num_frames=0, prompt=None, extra_pnginfo=None):\n        method = self.methods.get(method)\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        pil_images = []\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            pil_images.append(img)\n\n        metadata = pil_images[0].getexif()\n        if not args.disable_metadata:\n            if prompt is not None:\n                metadata[0x0110] = \"prompt:{}\".format(json.dumps(prompt))\n            if extra_pnginfo is not None:\n                inital_exif = 0x010f\n                for x in extra_pnginfo:\n                    metadata[inital_exif] = \"{}:{}\".format(x, json.dumps(extra_pnginfo[x]))\n                    inital_exif -= 1\n\n        if num_frames == 0:\n            num_frames = len(pil_images)\n\n        c = len(pil_images)\n        for i in range(0, c, num_frames):\n            file = f\"{filename}_{counter:05}_.webp\"\n            pil_images[i].save(os.path.join(full_output_folder, file), save_all=True, duration=int(1000.0/fps), append_images=pil_images[i + 1:i + num_frames], exif=metadata, lossless=lossless, quality=quality, method=method)\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n\n        animated = num_frames != 1\n        return { \"ui\": { \"images\": results, \"animated\": (animated,) } }\n\nclass SaveAnimatedPNG:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"}),\n                     \"fps\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.01, \"max\": 1000.0, \"step\": 0.01}),\n                     \"compress_level\": (\"INT\", {\"default\": 4, \"min\": 0, \"max\": 9})\n                     },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"image/animation\"\n\n    def save_images(self, images, fps, compress_level, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        results = list()\n        pil_images = []\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            pil_images.append(img)\n\n        metadata = None\n        if not args.disable_metadata:\n            metadata = PngInfo()\n            if prompt is not None:\n                metadata.add(b\"comf\", \"prompt\".encode(\"latin-1\", \"strict\") + b\"\\0\" + json.dumps(prompt).encode(\"latin-1\", \"strict\"), after_idat=True)\n            if extra_pnginfo is not None:\n                for x in extra_pnginfo:\n                    metadata.add(b\"comf\", x.encode(\"latin-1\", \"strict\") + b\"\\0\" + json.dumps(extra_pnginfo[x]).encode(\"latin-1\", \"strict\"), after_idat=True)\n\n        file = f\"{filename}_{counter:05}_.png\"\n        pil_images[0].save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=compress_level, save_all=True, duration=int(1000.0/fps), append_images=pil_images[1:])\n        results.append({\n            \"filename\": file,\n            \"subfolder\": subfolder,\n            \"type\": self.type\n        })\n\n        return { \"ui\": { \"images\": results, \"animated\": (True,)} }\n\nNODE_CLASS_MAPPINGS = {\n    \"ImageCrop\": ImageCrop,\n    \"RepeatImageBatch\": RepeatImageBatch,\n    \"ImageFromBatch\": ImageFromBatch,\n    \"SaveAnimatedWEBP\": SaveAnimatedWEBP,\n    \"SaveAnimatedPNG\": SaveAnimatedPNG,\n}\n", "comfy_extras/nodes_webcam.py": "import nodes\nimport folder_paths\n\nMAX_RESOLUTION = nodes.MAX_RESOLUTION\n\n\nclass WebcamCapture(nodes.LoadImage):\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"WEBCAM\", {}),\n                \"width\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"capture_on_queue\": (\"BOOLEAN\", {\"default\": True}),\n            }\n        }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"load_capture\"\n\n    CATEGORY = \"image\"\n\n    def load_capture(s, image, **kwargs):\n        return super().load_image(folder_paths.get_annotated_filepath(image))\n\n\nNODE_CLASS_MAPPINGS = {\n    \"WebcamCapture\": WebcamCapture,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"WebcamCapture\": \"Webcam Capture\",\n}", "comfy_extras/nodes_attention_multiply.py": "\ndef attention_multiply(attn, model, q, k, v, out):\n    m = model.clone()\n    sd = model.model_state_dict()\n\n    for key in sd:\n        if key.endswith(\"{}.to_q.bias\".format(attn)) or key.endswith(\"{}.to_q.weight\".format(attn)):\n            m.add_patches({key: (None,)}, 0.0, q)\n        if key.endswith(\"{}.to_k.bias\".format(attn)) or key.endswith(\"{}.to_k.weight\".format(attn)):\n            m.add_patches({key: (None,)}, 0.0, k)\n        if key.endswith(\"{}.to_v.bias\".format(attn)) or key.endswith(\"{}.to_v.weight\".format(attn)):\n            m.add_patches({key: (None,)}, 0.0, v)\n        if key.endswith(\"{}.to_out.0.bias\".format(attn)) or key.endswith(\"{}.to_out.0.weight\".format(attn)):\n            m.add_patches({key: (None,)}, 0.0, out)\n\n    return m\n\n\nclass UNetSelfAttentionMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"q\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"k\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"v\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing/attention_experiments\"\n\n    def patch(self, model, q, k, v, out):\n        m = attention_multiply(\"attn1\", model, q, k, v, out)\n        return (m, )\n\nclass UNetCrossAttentionMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"q\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"k\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"v\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing/attention_experiments\"\n\n    def patch(self, model, q, k, v, out):\n        m = attention_multiply(\"attn2\", model, q, k, v, out)\n        return (m, )\n\nclass CLIPAttentionMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\",),\n                              \"q\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"k\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"v\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing/attention_experiments\"\n\n    def patch(self, clip, q, k, v, out):\n        m = clip.clone()\n        sd = m.patcher.model_state_dict()\n\n        for key in sd:\n            if key.endswith(\"self_attn.q_proj.weight\") or key.endswith(\"self_attn.q_proj.bias\"):\n                m.add_patches({key: (None,)}, 0.0, q)\n            if key.endswith(\"self_attn.k_proj.weight\") or key.endswith(\"self_attn.k_proj.bias\"):\n                m.add_patches({key: (None,)}, 0.0, k)\n            if key.endswith(\"self_attn.v_proj.weight\") or key.endswith(\"self_attn.v_proj.bias\"):\n                m.add_patches({key: (None,)}, 0.0, v)\n            if key.endswith(\"self_attn.out_proj.weight\") or key.endswith(\"self_attn.out_proj.bias\"):\n                m.add_patches({key: (None,)}, 0.0, out)\n        return (m, )\n\nclass UNetTemporalAttentionMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"self_structural\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"self_temporal\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"cross_structural\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"cross_temporal\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n\n    CATEGORY = \"_for_testing/attention_experiments\"\n\n    def patch(self, model, self_structural, self_temporal, cross_structural, cross_temporal):\n        m = model.clone()\n        sd = model.model_state_dict()\n\n        for k in sd:\n            if (k.endswith(\"attn1.to_out.0.bias\") or k.endswith(\"attn1.to_out.0.weight\")):\n                if '.time_stack.' in k:\n                    m.add_patches({k: (None,)}, 0.0, self_temporal)\n                else:\n                    m.add_patches({k: (None,)}, 0.0, self_structural)\n            elif (k.endswith(\"attn2.to_out.0.bias\") or k.endswith(\"attn2.to_out.0.weight\")):\n                if '.time_stack.' in k:\n                    m.add_patches({k: (None,)}, 0.0, cross_temporal)\n                else:\n                    m.add_patches({k: (None,)}, 0.0, cross_structural)\n        return (m, )\n\nNODE_CLASS_MAPPINGS = {\n    \"UNetSelfAttentionMultiply\": UNetSelfAttentionMultiply,\n    \"UNetCrossAttentionMultiply\": UNetCrossAttentionMultiply,\n    \"CLIPAttentionMultiply\": CLIPAttentionMultiply,\n    \"UNetTemporalAttentionMultiply\": UNetTemporalAttentionMultiply,\n}\n", "comfy_extras/nodes_morphology.py": "import torch\nimport comfy.model_management\n\nfrom kornia.morphology import dilation, erosion, opening, closing, gradient, top_hat, bottom_hat\n\n\nclass Morphology:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"image\": (\"IMAGE\",),\n                                \"operation\": ([\"erode\",  \"dilate\", \"open\", \"close\", \"gradient\", \"bottom_hat\", \"top_hat\"],),\n                                \"kernel_size\": (\"INT\", {\"default\": 3, \"min\": 3, \"max\": 999, \"step\": 1}),\n                                }}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"process\"\n\n    CATEGORY = \"image/postprocessing\"\n\n    def process(self, image, operation, kernel_size):\n        device = comfy.model_management.get_torch_device()\n        kernel = torch.ones(kernel_size, kernel_size, device=device)\n        image_k = image.to(device).movedim(-1, 1)\n        if operation == \"erode\":\n            output = erosion(image_k, kernel)\n        elif operation == \"dilate\":\n            output = dilation(image_k, kernel)\n        elif operation == \"open\":\n            output = opening(image_k, kernel)\n        elif operation == \"close\":\n            output = closing(image_k, kernel)\n        elif operation == \"gradient\":\n            output = gradient(image_k, kernel)\n        elif operation == \"top_hat\":\n            output = top_hat(image_k, kernel)\n        elif operation == \"bottom_hat\":\n            output = bottom_hat(image_k, kernel)\n        else:\n            raise ValueError(f\"Invalid operation {operation} for morphology. Must be one of 'erode', 'dilate', 'open', 'close', 'gradient', 'tophat', 'bottomhat'\")\n        img_out = output.to(comfy.model_management.intermediate_device()).movedim(1, -1)\n        return (img_out,)\n\nNODE_CLASS_MAPPINGS = {\n    \"Morphology\": Morphology,\n}\n\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Morphology\": \"ImageMorphology\",\n}", "comfy_extras/nodes_upscale_model.py": "import os\nimport logging\nfrom spandrel import ModelLoader, ImageModelDescriptor\nfrom comfy import model_management\nimport torch\nimport comfy.utils\nimport folder_paths\n\ntry:\n    from spandrel_extra_arches import EXTRA_REGISTRY\n    from spandrel import MAIN_REGISTRY\n    MAIN_REGISTRY.add(*EXTRA_REGISTRY)\n    logging.info(\"Successfully imported spandrel_extra_arches: support for non commercial upscale models.\")\nexcept:\n    pass\n\nclass UpscaleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model_name\": (folder_paths.get_filename_list(\"upscale_models\"), ),\n                             }}\n    RETURN_TYPES = (\"UPSCALE_MODEL\",)\n    FUNCTION = \"load_model\"\n\n    CATEGORY = \"loaders\"\n\n    def load_model(self, model_name):\n        model_path = folder_paths.get_full_path(\"upscale_models\", model_name)\n        sd = comfy.utils.load_torch_file(model_path, safe_load=True)\n        if \"module.layers.0.residual_group.blocks.0.norm1.weight\" in sd:\n            sd = comfy.utils.state_dict_prefix_replace(sd, {\"module.\":\"\"})\n        out = ModelLoader().load_from_state_dict(sd).eval()\n\n        if not isinstance(out, ImageModelDescriptor):\n            raise Exception(\"Upscale model must be a single-image model.\")\n\n        return (out, )\n\n\nclass ImageUpscaleWithModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"upscale_model\": (\"UPSCALE_MODEL\",),\n                              \"image\": (\"IMAGE\",),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"image/upscaling\"\n\n    def upscale(self, upscale_model, image):\n        device = model_management.get_torch_device()\n\n        memory_required = model_management.module_size(upscale_model.model)\n        memory_required += (512 * 512 * 3) * image.element_size() * max(upscale_model.scale, 1.0) * 384.0 #The 384.0 is an estimate of how much some of these models take, TODO: make it more accurate\n        memory_required += image.nelement() * image.element_size()\n        model_management.free_memory(memory_required, device)\n\n        upscale_model.to(device)\n        in_img = image.movedim(-1,-3).to(device)\n\n        tile = 512\n        overlap = 32\n\n        oom = True\n        while oom:\n            try:\n                steps = in_img.shape[0] * comfy.utils.get_tiled_scale_steps(in_img.shape[3], in_img.shape[2], tile_x=tile, tile_y=tile, overlap=overlap)\n                pbar = comfy.utils.ProgressBar(steps)\n                s = comfy.utils.tiled_scale(in_img, lambda a: upscale_model(a), tile_x=tile, tile_y=tile, overlap=overlap, upscale_amount=upscale_model.scale, pbar=pbar)\n                oom = False\n            except model_management.OOM_EXCEPTION as e:\n                tile //= 2\n                if tile < 128:\n                    raise e\n\n        upscale_model.to(\"cpu\")\n        s = torch.clamp(s.movedim(-3,-1), min=0, max=1.0)\n        return (s,)\n\nNODE_CLASS_MAPPINGS = {\n    \"UpscaleModelLoader\": UpscaleModelLoader,\n    \"ImageUpscaleWithModel\": ImageUpscaleWithModel\n}\n", "comfy_extras/nodes_hypernetwork.py": "import comfy.utils\nimport folder_paths\nimport torch\nimport logging\n\ndef load_hypernetwork_patch(path, strength):\n    sd = comfy.utils.load_torch_file(path, safe_load=True)\n    activation_func = sd.get('activation_func', 'linear')\n    is_layer_norm = sd.get('is_layer_norm', False)\n    use_dropout = sd.get('use_dropout', False)\n    activate_output = sd.get('activate_output', False)\n    last_layer_dropout = sd.get('last_layer_dropout', False)\n\n    valid_activation = {\n        \"linear\": torch.nn.Identity,\n        \"relu\": torch.nn.ReLU,\n        \"leakyrelu\": torch.nn.LeakyReLU,\n        \"elu\": torch.nn.ELU,\n        \"swish\": torch.nn.Hardswish,\n        \"tanh\": torch.nn.Tanh,\n        \"sigmoid\": torch.nn.Sigmoid,\n        \"softsign\": torch.nn.Softsign,\n        \"mish\": torch.nn.Mish,\n    }\n\n    if activation_func not in valid_activation:\n        logging.error(\"Unsupported Hypernetwork format, if you report it I might implement it. {}   {} {} {} {} {}\".format(path, activation_func, is_layer_norm, use_dropout, activate_output, last_layer_dropout))\n        return None\n\n    out = {}\n\n    for d in sd:\n        try:\n            dim = int(d)\n        except:\n            continue\n\n        output = []\n        for index in [0, 1]:\n            attn_weights = sd[dim][index]\n            keys = attn_weights.keys()\n\n            linears = filter(lambda a: a.endswith(\".weight\"), keys)\n            linears = list(map(lambda a: a[:-len(\".weight\")], linears))\n            layers = []\n\n            i = 0\n            while i < len(linears):\n                lin_name = linears[i]\n                last_layer = (i == (len(linears) - 1))\n                penultimate_layer = (i == (len(linears) - 2))\n\n                lin_weight = attn_weights['{}.weight'.format(lin_name)]\n                lin_bias = attn_weights['{}.bias'.format(lin_name)]\n                layer = torch.nn.Linear(lin_weight.shape[1], lin_weight.shape[0])\n                layer.load_state_dict({\"weight\": lin_weight, \"bias\": lin_bias})\n                layers.append(layer)\n                if activation_func != \"linear\":\n                    if (not last_layer) or (activate_output):\n                        layers.append(valid_activation[activation_func]())\n                if is_layer_norm:\n                    i += 1\n                    ln_name = linears[i]\n                    ln_weight = attn_weights['{}.weight'.format(ln_name)]\n                    ln_bias = attn_weights['{}.bias'.format(ln_name)]\n                    ln = torch.nn.LayerNorm(ln_weight.shape[0])\n                    ln.load_state_dict({\"weight\": ln_weight, \"bias\": ln_bias})\n                    layers.append(ln)\n                if use_dropout:\n                    if (not last_layer) and (not penultimate_layer or last_layer_dropout):\n                        layers.append(torch.nn.Dropout(p=0.3))\n                i += 1\n\n            output.append(torch.nn.Sequential(*layers))\n        out[dim] = torch.nn.ModuleList(output)\n\n    class hypernetwork_patch:\n        def __init__(self, hypernet, strength):\n            self.hypernet = hypernet\n            self.strength = strength\n        def __call__(self, q, k, v, extra_options):\n            dim = k.shape[-1]\n            if dim in self.hypernet:\n                hn = self.hypernet[dim]\n                k = k + hn[0](k) * self.strength\n                v = v + hn[1](v) * self.strength\n\n            return q, k, v\n\n        def to(self, device):\n            for d in self.hypernet.keys():\n                self.hypernet[d] = self.hypernet[d].to(device)\n            return self\n\n    return hypernetwork_patch(out, strength)\n\nclass HypernetworkLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"hypernetwork_name\": (folder_paths.get_filename_list(\"hypernetworks\"), ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_hypernetwork\"\n\n    CATEGORY = \"loaders\"\n\n    def load_hypernetwork(self, model, hypernetwork_name, strength):\n        hypernetwork_path = folder_paths.get_full_path(\"hypernetworks\", hypernetwork_name)\n        model_hypernetwork = model.clone()\n        patch = load_hypernetwork_patch(hypernetwork_path, strength)\n        if patch is not None:\n            model_hypernetwork.set_model_attn1_patch(patch)\n            model_hypernetwork.set_model_attn2_patch(patch)\n        return (model_hypernetwork,)\n\nNODE_CLASS_MAPPINGS = {\n    \"HypernetworkLoader\": HypernetworkLoader\n}\n", "comfy_extras/chainner_models/model_loading.py": "from spandrel import ModelLoader\n\ndef load_state_dict(state_dict):\n    print(\"WARNING: comfy_extras.chainner_models is deprecated and has been replaced by the spandrel library.\")\n    return ModelLoader().load_from_state_dict(state_dict).eval()\n", "custom_nodes/websocket_image_save.py": "from PIL import Image, ImageOps\nfrom io import BytesIO\nimport numpy as np\nimport struct\nimport comfy.utils\nimport time\n\n#You can use this node to save full size images through the websocket, the\n#images will be sent in exactly the same format as the image previews: as\n#binary images on the websocket with a 8 byte header indicating the type\n#of binary message (first 4 bytes) and the image format (next 4 bytes).\n\n#Note that no metadata will be put in the images saved with this node.\n\nclass SaveImageWebsocket:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),}\n                }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"api/image\"\n\n    def save_images(self, images):\n        pbar = comfy.utils.ProgressBar(images.shape[0])\n        step = 0\n        for image in images:\n            i = 255. * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            pbar.update_absolute(step, images.shape[0], (\"PNG\", img, None))\n            step += 1\n\n        return {}\n\n    def IS_CHANGED(s, images):\n        return time.time()\n\nNODE_CLASS_MAPPINGS = {\n    \"SaveImageWebsocket\": SaveImageWebsocket,\n}\n", ".ci/update_windows/update.py": "import pygit2\nfrom datetime import datetime\nimport sys\nimport os\nimport shutil\nimport filecmp\n\ndef pull(repo, remote_name='origin', branch='master'):\n    for remote in repo.remotes:\n        if remote.name == remote_name:\n            remote.fetch()\n            remote_master_id = repo.lookup_reference('refs/remotes/origin/%s' % (branch)).target\n            merge_result, _ = repo.merge_analysis(remote_master_id)\n            # Up to date, do nothing\n            if merge_result & pygit2.GIT_MERGE_ANALYSIS_UP_TO_DATE:\n                return\n            # We can just fastforward\n            elif merge_result & pygit2.GIT_MERGE_ANALYSIS_FASTFORWARD:\n                repo.checkout_tree(repo.get(remote_master_id))\n                try:\n                    master_ref = repo.lookup_reference('refs/heads/%s' % (branch))\n                    master_ref.set_target(remote_master_id)\n                except KeyError:\n                    repo.create_branch(branch, repo.get(remote_master_id))\n                repo.head.set_target(remote_master_id)\n            elif merge_result & pygit2.GIT_MERGE_ANALYSIS_NORMAL:\n                repo.merge(remote_master_id)\n\n                if repo.index.conflicts is not None:\n                    for conflict in repo.index.conflicts:\n                        print('Conflicts found in:', conflict[0].path)\n                    raise AssertionError('Conflicts, ahhhhh!!')\n\n                user = repo.default_signature\n                tree = repo.index.write_tree()\n                commit = repo.create_commit('HEAD',\n                                            user,\n                                            user,\n                                            'Merge!',\n                                            tree,\n                                            [repo.head.target, remote_master_id])\n                # We need to do this or git CLI will think we are still merging.\n                repo.state_cleanup()\n            else:\n                raise AssertionError('Unknown merge analysis result')\n\npygit2.option(pygit2.GIT_OPT_SET_OWNER_VALIDATION, 0)\nrepo_path = str(sys.argv[1])\nrepo = pygit2.Repository(repo_path)\nident = pygit2.Signature('comfyui', 'comfy@ui')\ntry:\n    print(\"stashing current changes\")\n    repo.stash(ident)\nexcept KeyError:\n    print(\"nothing to stash\")\nbackup_branch_name = 'backup_branch_{}'.format(datetime.today().strftime('%Y-%m-%d_%H_%M_%S'))\nprint(\"creating backup branch: {}\".format(backup_branch_name))\ntry:\n    repo.branches.local.create(backup_branch_name, repo.head.peel())\nexcept:\n    pass\n\nprint(\"checking out master branch\")\nbranch = repo.lookup_branch('master')\nref = repo.lookup_reference(branch.name)\nrepo.checkout(ref)\n\nprint(\"pulling latest changes\")\npull(repo)\n\nprint(\"Done!\")\n\nself_update = True\nif len(sys.argv) > 2:\n    self_update = '--skip_self_update' not in sys.argv\n\nupdate_py_path = os.path.realpath(__file__)\nrepo_update_py_path = os.path.join(repo_path, \".ci/update_windows/update.py\")\n\ncur_path = os.path.dirname(update_py_path)\n\n\nreq_path = os.path.join(cur_path, \"current_requirements.txt\")\nrepo_req_path = os.path.join(repo_path, \"requirements.txt\")\n\n\ndef files_equal(file1, file2):\n    try:\n        return filecmp.cmp(file1, file2, shallow=False)\n    except:\n        return False\n\ndef file_size(f):\n    try:\n        return os.path.getsize(f)\n    except:\n        return 0\n\n\nif self_update and not files_equal(update_py_path, repo_update_py_path) and file_size(repo_update_py_path) > 10:\n    shutil.copy(repo_update_py_path, os.path.join(cur_path, \"update_new.py\"))\n    exit()\n\nif not os.path.exists(req_path) or not files_equal(repo_req_path, req_path):\n    import subprocess\n    try:\n        subprocess.check_call([sys.executable, '-s', '-m', 'pip', 'install', '-r', repo_req_path])\n        shutil.copy(repo_req_path, req_path)\n    except:\n        pass\n", "comfy/sd1_clip.py": "import os\n\nfrom transformers import CLIPTokenizer\nimport comfy.ops\nimport torch\nimport traceback\nimport zipfile\nfrom . import model_management\nimport comfy.clip_model\nimport json\nimport logging\nimport numbers\n\ndef gen_empty_tokens(special_tokens, length):\n    start_token = special_tokens.get(\"start\", None)\n    end_token = special_tokens.get(\"end\", None)\n    pad_token = special_tokens.get(\"pad\")\n    output = []\n    if start_token is not None:\n        output.append(start_token)\n    if end_token is not None:\n        output.append(end_token)\n    output += [pad_token] * (length - len(output))\n    return output\n\nclass ClipTokenWeightEncoder:\n    def encode_token_weights(self, token_weight_pairs):\n        to_encode = list()\n        max_token_len = 0\n        has_weights = False\n        for x in token_weight_pairs:\n            tokens = list(map(lambda a: a[0], x))\n            max_token_len = max(len(tokens), max_token_len)\n            has_weights = has_weights or not all(map(lambda a: a[1] == 1.0, x))\n            to_encode.append(tokens)\n\n        sections = len(to_encode)\n        if has_weights or sections == 0:\n            to_encode.append(gen_empty_tokens(self.special_tokens, max_token_len))\n\n        out, pooled = self.encode(to_encode)\n        if pooled is not None:\n            first_pooled = pooled[0:1].to(model_management.intermediate_device())\n        else:\n            first_pooled = pooled\n\n        output = []\n        for k in range(0, sections):\n            z = out[k:k+1]\n            if has_weights:\n                z_empty = out[-1]\n                for i in range(len(z)):\n                    for j in range(len(z[i])):\n                        weight = token_weight_pairs[k][j][1]\n                        if weight != 1.0:\n                            z[i][j] = (z[i][j] - z_empty[j]) * weight + z_empty[j]\n            output.append(z)\n\n        if (len(output) == 0):\n            return out[-1:].to(model_management.intermediate_device()), first_pooled\n        return torch.cat(output, dim=-2).to(model_management.intermediate_device()), first_pooled\n\nclass SDClipModel(torch.nn.Module, ClipTokenWeightEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    LAYERS = [\n        \"last\",\n        \"pooled\",\n        \"hidden\"\n    ]\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cpu\", max_length=77,\n                 freeze=True, layer=\"last\", layer_idx=None, textmodel_json_config=None, dtype=None, model_class=comfy.clip_model.CLIPTextModel,\n                 special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 49407}, layer_norm_hidden_state=True, enable_attention_masks=False, zero_out_masked=False,\n                 return_projected_pooled=True):  # clip-vit-base-patch32\n        super().__init__()\n        assert layer in self.LAYERS\n\n        if textmodel_json_config is None:\n            textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd1_clip_config.json\")\n\n        with open(textmodel_json_config) as f:\n            config = json.load(f)\n\n        self.transformer = model_class(config, dtype, device, comfy.ops.manual_cast)\n        self.num_layers = self.transformer.num_layers\n\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        self.layer_idx = None\n        self.special_tokens = special_tokens\n\n        self.logit_scale = torch.nn.Parameter(torch.tensor(4.6055))\n        self.enable_attention_masks = enable_attention_masks\n        self.zero_out_masked = zero_out_masked\n\n        self.layer_norm_hidden_state = layer_norm_hidden_state\n        self.return_projected_pooled = return_projected_pooled\n\n        if layer == \"hidden\":\n            assert layer_idx is not None\n            assert abs(layer_idx) < self.num_layers\n            self.set_clip_options({\"layer\": layer_idx})\n        self.options_default = (self.layer, self.layer_idx, self.return_projected_pooled)\n\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        #self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def set_clip_options(self, options):\n        layer_idx = options.get(\"layer\", self.layer_idx)\n        self.return_projected_pooled = options.get(\"projected_pooled\", self.return_projected_pooled)\n        if layer_idx is None or abs(layer_idx) > self.num_layers:\n            self.layer = \"last\"\n        else:\n            self.layer = \"hidden\"\n            self.layer_idx = layer_idx\n\n    def reset_clip_options(self):\n        self.layer = self.options_default[0]\n        self.layer_idx = self.options_default[1]\n        self.return_projected_pooled = self.options_default[2]\n\n    def set_up_textual_embeddings(self, tokens, current_embeds):\n        out_tokens = []\n        next_new_token = token_dict_size = current_embeds.weight.shape[0] - 1\n        embedding_weights = []\n\n        for x in tokens:\n            tokens_temp = []\n            for y in x:\n                if isinstance(y, numbers.Integral):\n                    if y == token_dict_size: #EOS token\n                        y = -1\n                    tokens_temp += [int(y)]\n                else:\n                    if y.shape[0] == current_embeds.weight.shape[1]:\n                        embedding_weights += [y]\n                        tokens_temp += [next_new_token]\n                        next_new_token += 1\n                    else:\n                        logging.warning(\"WARNING: shape mismatch when trying to apply embedding, embedding will be ignored {} != {}\".format(y.shape[0], current_embeds.weight.shape[1]))\n            while len(tokens_temp) < len(x):\n                tokens_temp += [self.special_tokens[\"pad\"]]\n            out_tokens += [tokens_temp]\n\n        n = token_dict_size\n        if len(embedding_weights) > 0:\n            new_embedding = torch.nn.Embedding(next_new_token + 1, current_embeds.weight.shape[1], device=current_embeds.weight.device, dtype=current_embeds.weight.dtype)\n            new_embedding.weight[:token_dict_size] = current_embeds.weight[:-1]\n            for x in embedding_weights:\n                new_embedding.weight[n] = x\n                n += 1\n            new_embedding.weight[n] = current_embeds.weight[-1] #EOS embedding\n            self.transformer.set_input_embeddings(new_embedding)\n\n        processed_tokens = []\n        for x in out_tokens:\n            processed_tokens += [list(map(lambda a: n if a == -1 else a, x))] #The EOS token should always be the largest one\n\n        return processed_tokens\n\n    def forward(self, tokens):\n        backup_embeds = self.transformer.get_input_embeddings()\n        device = backup_embeds.weight.device\n        tokens = self.set_up_textual_embeddings(tokens, backup_embeds)\n        tokens = torch.LongTensor(tokens).to(device)\n\n        attention_mask = None\n        if self.enable_attention_masks:\n            attention_mask = torch.zeros_like(tokens)\n            end_token = self.special_tokens.get(\"end\", -1)\n            for x in range(attention_mask.shape[0]):\n                for y in range(attention_mask.shape[1]):\n                    attention_mask[x, y] = 1\n                    if tokens[x, y] == end_token:\n                        break\n\n        outputs = self.transformer(tokens, attention_mask, intermediate_output=self.layer_idx, final_layer_norm_intermediate=self.layer_norm_hidden_state)\n        self.transformer.set_input_embeddings(backup_embeds)\n\n        if self.layer == \"last\":\n            z = outputs[0].float()\n        else:\n            z = outputs[1].float()\n\n        if self.zero_out_masked and attention_mask is not None:\n            z *= attention_mask.unsqueeze(-1).float()\n\n        pooled_output = None\n        if len(outputs) >= 3:\n            if not self.return_projected_pooled and len(outputs) >= 4 and outputs[3] is not None:\n                pooled_output = outputs[3].float()\n            elif outputs[2] is not None:\n                pooled_output = outputs[2].float()\n\n        return z, pooled_output\n\n    def encode(self, tokens):\n        return self(tokens)\n\n    def load_sd(self, sd):\n        return self.transformer.load_state_dict(sd, strict=False)\n\ndef parse_parentheses(string):\n    result = []\n    current_item = \"\"\n    nesting_level = 0\n    for char in string:\n        if char == \"(\":\n            if nesting_level == 0:\n                if current_item:\n                    result.append(current_item)\n                    current_item = \"(\"\n                else:\n                    current_item = \"(\"\n            else:\n                current_item += char\n            nesting_level += 1\n        elif char == \")\":\n            nesting_level -= 1\n            if nesting_level == 0:\n                result.append(current_item + \")\")\n                current_item = \"\"\n            else:\n                current_item += char\n        else:\n            current_item += char\n    if current_item:\n        result.append(current_item)\n    return result\n\ndef token_weights(string, current_weight):\n    a = parse_parentheses(string)\n    out = []\n    for x in a:\n        weight = current_weight\n        if len(x) >= 2 and x[-1] == ')' and x[0] == '(':\n            x = x[1:-1]\n            xx = x.rfind(\":\")\n            weight *= 1.1\n            if xx > 0:\n                try:\n                    weight = float(x[xx+1:])\n                    x = x[:xx]\n                except:\n                    pass\n            out += token_weights(x, weight)\n        else:\n            out += [(x, current_weight)]\n    return out\n\ndef escape_important(text):\n    text = text.replace(\"\\\\)\", \"\\0\\1\")\n    text = text.replace(\"\\\\(\", \"\\0\\2\")\n    return text\n\ndef unescape_important(text):\n    text = text.replace(\"\\0\\1\", \")\")\n    text = text.replace(\"\\0\\2\", \"(\")\n    return text\n\ndef safe_load_embed_zip(embed_path):\n    with zipfile.ZipFile(embed_path) as myzip:\n        names = list(filter(lambda a: \"data/\" in a, myzip.namelist()))\n        names.reverse()\n        for n in names:\n            with myzip.open(n) as myfile:\n                data = myfile.read()\n                number = len(data) // 4\n                length_embed = 1024 #sd2.x\n                if number < 768:\n                    continue\n                if number % 768 == 0:\n                    length_embed = 768 #sd1.x\n                num_embeds = number // length_embed\n                embed = torch.frombuffer(data, dtype=torch.float)\n                out = embed.reshape((num_embeds, length_embed)).clone()\n                del embed\n                return out\n\ndef expand_directory_list(directories):\n    dirs = set()\n    for x in directories:\n        dirs.add(x)\n        for root, subdir, file in os.walk(x, followlinks=True):\n            dirs.add(root)\n    return list(dirs)\n\ndef load_embed(embedding_name, embedding_directory, embedding_size, embed_key=None):\n    if isinstance(embedding_directory, str):\n        embedding_directory = [embedding_directory]\n\n    embedding_directory = expand_directory_list(embedding_directory)\n\n    valid_file = None\n    for embed_dir in embedding_directory:\n        embed_path = os.path.abspath(os.path.join(embed_dir, embedding_name))\n        embed_dir = os.path.abspath(embed_dir)\n        try:\n            if os.path.commonpath((embed_dir, embed_path)) != embed_dir:\n                continue\n        except:\n            continue\n        if not os.path.isfile(embed_path):\n            extensions = ['.safetensors', '.pt', '.bin']\n            for x in extensions:\n                t = embed_path + x\n                if os.path.isfile(t):\n                    valid_file = t\n                    break\n        else:\n            valid_file = embed_path\n        if valid_file is not None:\n            break\n\n    if valid_file is None:\n        return None\n\n    embed_path = valid_file\n\n    embed_out = None\n\n    try:\n        if embed_path.lower().endswith(\".safetensors\"):\n            import safetensors.torch\n            embed = safetensors.torch.load_file(embed_path, device=\"cpu\")\n        else:\n            if 'weights_only' in torch.load.__code__.co_varnames:\n                try:\n                    embed = torch.load(embed_path, weights_only=True, map_location=\"cpu\")\n                except:\n                    embed_out = safe_load_embed_zip(embed_path)\n            else:\n                embed = torch.load(embed_path, map_location=\"cpu\")\n    except Exception as e:\n        logging.warning(\"{}\\n\\nerror loading embedding, skipping loading: {}\".format(traceback.format_exc(), embedding_name))\n        return None\n\n    if embed_out is None:\n        if 'string_to_param' in embed:\n            values = embed['string_to_param'].values()\n            embed_out = next(iter(values))\n        elif isinstance(embed, list):\n            out_list = []\n            for x in range(len(embed)):\n                for k in embed[x]:\n                    t = embed[x][k]\n                    if t.shape[-1] != embedding_size:\n                        continue\n                    out_list.append(t.reshape(-1, t.shape[-1]))\n            embed_out = torch.cat(out_list, dim=0)\n        elif embed_key is not None and embed_key in embed:\n            embed_out = embed[embed_key]\n        else:\n            values = embed.values()\n            embed_out = next(iter(values))\n    return embed_out\n\nclass SDTokenizer:\n    def __init__(self, tokenizer_path=None, max_length=77, pad_with_end=True, embedding_directory=None, embedding_size=768, embedding_key='clip_l', tokenizer_class=CLIPTokenizer, has_start_token=True, pad_to_max_length=True, min_length=None):\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd1_tokenizer\")\n        self.tokenizer = tokenizer_class.from_pretrained(tokenizer_path)\n        self.max_length = max_length\n        self.min_length = min_length\n\n        empty = self.tokenizer('')[\"input_ids\"]\n        if has_start_token:\n            self.tokens_start = 1\n            self.start_token = empty[0]\n            self.end_token = empty[1]\n        else:\n            self.tokens_start = 0\n            self.start_token = None\n            self.end_token = empty[0]\n        self.pad_with_end = pad_with_end\n        self.pad_to_max_length = pad_to_max_length\n\n        vocab = self.tokenizer.get_vocab()\n        self.inv_vocab = {v: k for k, v in vocab.items()}\n        self.embedding_directory = embedding_directory\n        self.max_word_length = 8\n        self.embedding_identifier = \"embedding:\"\n        self.embedding_size = embedding_size\n        self.embedding_key = embedding_key\n\n    def _try_get_embedding(self, embedding_name:str):\n        '''\n        Takes a potential embedding name and tries to retrieve it.\n        Returns a Tuple consisting of the embedding and any leftover string, embedding can be None.\n        '''\n        embed = load_embed(embedding_name, self.embedding_directory, self.embedding_size, self.embedding_key)\n        if embed is None:\n            stripped = embedding_name.strip(',')\n            if len(stripped) < len(embedding_name):\n                embed = load_embed(stripped, self.embedding_directory, self.embedding_size, self.embedding_key)\n                return (embed, embedding_name[len(stripped):])\n        return (embed, \"\")\n\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        '''\n        Takes a prompt and converts it to a list of (token, weight, word id) elements.\n        Tokens can both be integer tokens and pre computed CLIP tensors.\n        Word id values are unique per word and embedding, where the id 0 is reserved for non word tokens.\n        Returned list has the dimensions NxM where M is the input size of CLIP\n        '''\n        if self.pad_with_end:\n            pad_token = self.end_token\n        else:\n            pad_token = 0\n\n        text = escape_important(text)\n        parsed_weights = token_weights(text, 1.0)\n\n        #tokenize words\n        tokens = []\n        for weighted_segment, weight in parsed_weights:\n            to_tokenize = unescape_important(weighted_segment).replace(\"\\n\", \" \").split(' ')\n            to_tokenize = [x for x in to_tokenize if x != \"\"]\n            for word in to_tokenize:\n                #if we find an embedding, deal with the embedding\n                if word.startswith(self.embedding_identifier) and self.embedding_directory is not None:\n                    embedding_name = word[len(self.embedding_identifier):].strip('\\n')\n                    embed, leftover = self._try_get_embedding(embedding_name)\n                    if embed is None:\n                        logging.warning(f\"warning, embedding:{embedding_name} does not exist, ignoring\")\n                    else:\n                        if len(embed.shape) == 1:\n                            tokens.append([(embed, weight)])\n                        else:\n                            tokens.append([(embed[x], weight) for x in range(embed.shape[0])])\n                    #if we accidentally have leftover text, continue parsing using leftover, else move on to next word\n                    if leftover != \"\":\n                        word = leftover\n                    else:\n                        continue\n                #parse word\n                tokens.append([(t, weight) for t in self.tokenizer(word)[\"input_ids\"][self.tokens_start:-1]])\n\n        #reshape token array to CLIP input size\n        batched_tokens = []\n        batch = []\n        if self.start_token is not None:\n            batch.append((self.start_token, 1.0, 0))\n        batched_tokens.append(batch)\n        for i, t_group in enumerate(tokens):\n            #determine if we're going to try and keep the tokens in a single batch\n            is_large = len(t_group) >= self.max_word_length\n\n            while len(t_group) > 0:\n                if len(t_group) + len(batch) > self.max_length - 1:\n                    remaining_length = self.max_length - len(batch) - 1\n                    #break word in two and add end token\n                    if is_large:\n                        batch.extend([(t,w,i+1) for t,w in t_group[:remaining_length]])\n                        batch.append((self.end_token, 1.0, 0))\n                        t_group = t_group[remaining_length:]\n                    #add end token and pad\n                    else:\n                        batch.append((self.end_token, 1.0, 0))\n                        if self.pad_to_max_length:\n                            batch.extend([(pad_token, 1.0, 0)] * (remaining_length))\n                    #start new batch\n                    batch = []\n                    if self.start_token is not None:\n                        batch.append((self.start_token, 1.0, 0))\n                    batched_tokens.append(batch)\n                else:\n                    batch.extend([(t,w,i+1) for t,w in t_group])\n                    t_group = []\n\n        #fill last batch\n        batch.append((self.end_token, 1.0, 0))\n        if self.pad_to_max_length:\n            batch.extend([(pad_token, 1.0, 0)] * (self.max_length - len(batch)))\n        if self.min_length is not None and len(batch) < self.min_length:\n            batch.extend([(pad_token, 1.0, 0)] * (self.min_length - len(batch)))\n\n        if not return_word_ids:\n            batched_tokens = [[(t, w) for t, w,_ in x] for x in batched_tokens]\n\n        return batched_tokens\n\n\n    def untokenize(self, token_weight_pair):\n        return list(map(lambda a: (a, self.inv_vocab[a[0]]), token_weight_pair))\n\n\nclass SD1Tokenizer:\n    def __init__(self, embedding_directory=None, clip_name=\"l\", tokenizer=SDTokenizer):\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, tokenizer(embedding_directory=embedding_directory))\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[self.clip_name] = getattr(self, self.clip).tokenize_with_weights(text, return_word_ids)\n        return out\n\n    def untokenize(self, token_weight_pair):\n        return getattr(self, self.clip).untokenize(token_weight_pair)\n\n\nclass SD1ClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None, clip_name=\"l\", clip_model=SDClipModel, **kwargs):\n        super().__init__()\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, clip_model(device=device, dtype=dtype, **kwargs))\n\n        self.dtypes = set()\n        if dtype is not None:\n            self.dtypes.add(dtype)\n\n    def set_clip_options(self, options):\n        getattr(self, self.clip).set_clip_options(options)\n\n    def reset_clip_options(self):\n        getattr(self, self.clip).reset_clip_options()\n\n    def encode_token_weights(self, token_weight_pairs):\n        token_weight_pairs = token_weight_pairs[self.clip_name]\n        out, pooled = getattr(self, self.clip).encode_token_weights(token_weight_pairs)\n        return out, pooled\n\n    def load_sd(self, sd):\n        return getattr(self, self.clip).load_sd(sd)\n", "comfy/model_sampling.py": "import torch\nfrom comfy.ldm.modules.diffusionmodules.util import make_beta_schedule\nimport math\n\nclass EPS:\n    def calculate_input(self, sigma, noise):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (noise.ndim - 1))\n        return noise / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input - model_output * sigma\n\n    def noise_scaling(self, sigma, noise, latent_image, max_denoise=False):\n        if max_denoise:\n            noise = noise * torch.sqrt(1.0 + sigma ** 2.0)\n        else:\n            noise = noise * sigma\n\n        noise += latent_image\n        return noise\n\n    def inverse_noise_scaling(self, sigma, latent):\n        return latent\n\nclass V_PREDICTION(EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input * self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2) - model_output * sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\nclass EDM(V_PREDICTION):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input * self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2) + model_output * sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n\nclass CONST:\n    def calculate_input(self, sigma, noise):\n        return noise\n\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input - model_output * sigma\n\n    def noise_scaling(self, sigma, noise, latent_image, max_denoise=False):\n        return sigma * noise + (1.0 - sigma) * latent_image\n\n    def inverse_noise_scaling(self, sigma, latent):\n        return latent / (1.0 - sigma)\n\nclass ModelSamplingDiscrete(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        beta_schedule = sampling_settings.get(\"beta_schedule\", \"linear\")\n        linear_start = sampling_settings.get(\"linear_start\", 0.00085)\n        linear_end = sampling_settings.get(\"linear_end\", 0.012)\n\n        self._register_schedule(given_betas=None, beta_schedule=beta_schedule, timesteps=1000, linear_start=linear_start, linear_end=linear_end, cosine_s=8e-3)\n        self.sigma_data = 1.0\n\n    def _register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if given_betas is not None:\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n\n        # self.register_buffer('betas', torch.tensor(betas, dtype=torch.float32))\n        # self.register_buffer('alphas_cumprod', torch.tensor(alphas_cumprod, dtype=torch.float32))\n        # self.register_buffer('alphas_cumprod_prev', torch.tensor(alphas_cumprod_prev, dtype=torch.float32))\n\n        sigmas = ((1 - alphas_cumprod) / alphas_cumprod) ** 0.5\n        self.set_sigmas(sigmas)\n\n    def set_sigmas(self, sigmas):\n        self.register_buffer('sigmas', sigmas.float())\n        self.register_buffer('log_sigmas', sigmas.log().float())\n\n    @property\n    def sigma_min(self):\n        return self.sigmas[0]\n\n    @property\n    def sigma_max(self):\n        return self.sigmas[-1]\n\n    def timestep(self, sigma):\n        log_sigma = sigma.log()\n        dists = log_sigma.to(self.log_sigmas.device) - self.log_sigmas[:, None]\n        return dists.abs().argmin(dim=0).view(sigma.shape).to(sigma.device)\n\n    def sigma(self, timestep):\n        t = torch.clamp(timestep.float().to(self.log_sigmas.device), min=0, max=(len(self.sigmas) - 1))\n        low_idx = t.floor().long()\n        high_idx = t.ceil().long()\n        w = t.frac()\n        log_sigma = (1 - w) * self.log_sigmas[low_idx] + w * self.log_sigmas[high_idx]\n        return log_sigma.exp().to(timestep.device)\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n        percent = 1.0 - percent\n        return self.sigma(torch.tensor(percent * 999.0)).item()\n\nclass ModelSamplingDiscreteEDM(ModelSamplingDiscrete):\n    def timestep(self, sigma):\n        return 0.25 * sigma.log()\n\n    def sigma(self, timestep):\n        return (timestep / 0.25).exp()\n\nclass ModelSamplingContinuousEDM(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        sigma_min = sampling_settings.get(\"sigma_min\", 0.002)\n        sigma_max = sampling_settings.get(\"sigma_max\", 120.0)\n        sigma_data = sampling_settings.get(\"sigma_data\", 1.0)\n        self.set_parameters(sigma_min, sigma_max, sigma_data)\n\n    def set_parameters(self, sigma_min, sigma_max, sigma_data):\n        self.sigma_data = sigma_data\n        sigmas = torch.linspace(math.log(sigma_min), math.log(sigma_max), 1000).exp()\n\n        self.register_buffer('sigmas', sigmas) #for compatibility with some schedulers\n        self.register_buffer('log_sigmas', sigmas.log())\n\n    @property\n    def sigma_min(self):\n        return self.sigmas[0]\n\n    @property\n    def sigma_max(self):\n        return self.sigmas[-1]\n\n    def timestep(self, sigma):\n        return 0.25 * sigma.log()\n\n    def sigma(self, timestep):\n        return (timestep / 0.25).exp()\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n        percent = 1.0 - percent\n\n        log_sigma_min = math.log(self.sigma_min)\n        return math.exp((math.log(self.sigma_max) - log_sigma_min) * percent + log_sigma_min)\n\n\nclass ModelSamplingContinuousV(ModelSamplingContinuousEDM):\n    def timestep(self, sigma):\n        return sigma.atan() / math.pi * 2\n\n    def sigma(self, timestep):\n        return (timestep * math.pi / 2).tan()\n\n\ndef time_snr_shift(alpha, t):\n    if alpha == 1.0:\n        return t\n    return alpha * t / (1 + (alpha - 1) * t)\n\nclass ModelSamplingDiscreteFlow(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        self.set_parameters(shift=sampling_settings.get(\"shift\", 1.0))\n\n    def set_parameters(self, shift=1.0, timesteps=1000):\n        self.shift = shift\n        ts = self.sigma(torch.arange(1, timesteps + 1, 1))\n        self.register_buffer('sigmas', ts)\n\n    @property\n    def sigma_min(self):\n        return self.sigmas[0]\n\n    @property\n    def sigma_max(self):\n        return self.sigmas[-1]\n\n    def timestep(self, sigma):\n        return sigma * 1000\n\n    def sigma(self, timestep):\n        return time_snr_shift(self.shift, timestep / 1000)\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 1.0\n        if percent >= 1.0:\n            return 0.0\n        return 1.0 - percent\n\nclass StableCascadeSampling(ModelSamplingDiscrete):\n    def __init__(self, model_config=None):\n        super().__init__()\n\n        if model_config is not None:\n            sampling_settings = model_config.sampling_settings\n        else:\n            sampling_settings = {}\n\n        self.set_parameters(sampling_settings.get(\"shift\", 1.0))\n\n    def set_parameters(self, shift=1.0, cosine_s=8e-3):\n        self.shift = shift\n        self.cosine_s = torch.tensor(cosine_s)\n        self._init_alpha_cumprod = torch.cos(self.cosine_s / (1 + self.cosine_s) * torch.pi * 0.5) ** 2\n\n        #This part is just for compatibility with some schedulers in the codebase\n        self.num_timesteps = 10000\n        sigmas = torch.empty((self.num_timesteps), dtype=torch.float32)\n        for x in range(self.num_timesteps):\n            t = (x + 1) / self.num_timesteps\n            sigmas[x] = self.sigma(t)\n\n        self.set_sigmas(sigmas)\n\n    def sigma(self, timestep):\n        alpha_cumprod = (torch.cos((timestep + self.cosine_s) / (1 + self.cosine_s) * torch.pi * 0.5) ** 2 / self._init_alpha_cumprod)\n\n        if self.shift != 1.0:\n            var = alpha_cumprod\n            logSNR = (var/(1-var)).log()\n            logSNR += 2 * torch.log(1.0 / torch.tensor(self.shift))\n            alpha_cumprod = logSNR.sigmoid()\n\n        alpha_cumprod = alpha_cumprod.clamp(0.0001, 0.9999)\n        return ((1 - alpha_cumprod) / alpha_cumprod) ** 0.5\n\n    def timestep(self, sigma):\n        var = 1 / ((sigma * sigma) + 1)\n        var = var.clamp(0, 1.0)\n        s, min_var = self.cosine_s.to(var.device), self._init_alpha_cumprod.to(var.device)\n        t = (((var * min_var) ** 0.5).acos() / (torch.pi * 0.5)) * (1 + s) - s\n        return t\n\n    def percent_to_sigma(self, percent):\n        if percent <= 0.0:\n            return 999999999.9\n        if percent >= 1.0:\n            return 0.0\n\n        percent = 1.0 - percent\n        return self.sigma(torch.tensor(percent))\n", "comfy/samplers.py": "from .k_diffusion import sampling as k_diffusion_sampling\nfrom .extra_samplers import uni_pc\nimport torch\nimport collections\nfrom comfy import model_management\nimport math\nimport logging\nimport comfy.sampler_helpers\n\ndef get_area_and_mult(conds, x_in, timestep_in):\n    dims = tuple(x_in.shape[2:])\n    area = None\n    strength = 1.0\n\n    if 'timestep_start' in conds:\n        timestep_start = conds['timestep_start']\n        if timestep_in[0] > timestep_start:\n            return None\n    if 'timestep_end' in conds:\n        timestep_end = conds['timestep_end']\n        if timestep_in[0] < timestep_end:\n            return None\n    if 'area' in conds:\n        area = list(conds['area'])\n    if 'strength' in conds:\n        strength = conds['strength']\n\n    input_x = x_in\n    if area is not None:\n        for i in range(len(dims)):\n            area[i] = min(input_x.shape[i + 2] - area[len(dims) + i], area[i])\n            input_x = input_x.narrow(i + 2, area[len(dims) + i], area[i])\n\n    if 'mask' in conds:\n        # Scale the mask to the size of the input\n        # The mask should have been resized as we began the sampling process\n        mask_strength = 1.0\n        if \"mask_strength\" in conds:\n            mask_strength = conds[\"mask_strength\"]\n        mask = conds['mask']\n        assert(mask.shape[1:] == x_in.shape[2:])\n\n        mask = mask[:input_x.shape[0]]\n        if area is not None:\n            for i in range(len(dims)):\n                mask = mask.narrow(i + 1, area[len(dims) + i], area[i])\n\n        mask = mask * mask_strength\n        mask = mask.unsqueeze(1).repeat(input_x.shape[0] // mask.shape[0], input_x.shape[1], 1, 1)\n    else:\n        mask = torch.ones_like(input_x)\n    mult = mask * strength\n\n    if 'mask' not in conds and area is not None:\n        rr = 8\n        for i in range(len(dims)):\n            if area[len(dims) + i] != 0:\n                for t in range(rr):\n                    m = mult.narrow(i + 2, t, 1)\n                    m *= ((1.0/rr) * (t + 1))\n            if (area[i] + area[len(dims) + i]) < x_in.shape[i + 2]:\n                for t in range(rr):\n                    m = mult.narrow(i + 2, area[i] - 1 - t, 1)\n                    m *= ((1.0/rr) * (t + 1))\n\n    conditioning = {}\n    model_conds = conds[\"model_conds\"]\n    for c in model_conds:\n        conditioning[c] = model_conds[c].process_cond(batch_size=x_in.shape[0], device=x_in.device, area=area)\n\n    control = conds.get('control', None)\n\n    patches = None\n    if 'gligen' in conds:\n        gligen = conds['gligen']\n        patches = {}\n        gligen_type = gligen[0]\n        gligen_model = gligen[1]\n        if gligen_type == \"position\":\n            gligen_patch = gligen_model.model.set_position(input_x.shape, gligen[2], input_x.device)\n        else:\n            gligen_patch = gligen_model.model.set_empty(input_x.shape, input_x.device)\n\n        patches['middle_patch'] = [gligen_patch]\n\n    cond_obj = collections.namedtuple('cond_obj', ['input_x', 'mult', 'conditioning', 'area', 'control', 'patches'])\n    return cond_obj(input_x, mult, conditioning, area, control, patches)\n\ndef cond_equal_size(c1, c2):\n    if c1 is c2:\n        return True\n    if c1.keys() != c2.keys():\n        return False\n    for k in c1:\n        if not c1[k].can_concat(c2[k]):\n            return False\n    return True\n\ndef can_concat_cond(c1, c2):\n    if c1.input_x.shape != c2.input_x.shape:\n        return False\n\n    def objects_concatable(obj1, obj2):\n        if (obj1 is None) != (obj2 is None):\n            return False\n        if obj1 is not None:\n            if obj1 is not obj2:\n                return False\n        return True\n\n    if not objects_concatable(c1.control, c2.control):\n        return False\n\n    if not objects_concatable(c1.patches, c2.patches):\n        return False\n\n    return cond_equal_size(c1.conditioning, c2.conditioning)\n\ndef cond_cat(c_list):\n    c_crossattn = []\n    c_concat = []\n    c_adm = []\n    crossattn_max_len = 0\n\n    temp = {}\n    for x in c_list:\n        for k in x:\n            cur = temp.get(k, [])\n            cur.append(x[k])\n            temp[k] = cur\n\n    out = {}\n    for k in temp:\n        conds = temp[k]\n        out[k] = conds[0].concat(conds[1:])\n\n    return out\n\ndef calc_cond_batch(model, conds, x_in, timestep, model_options):\n    out_conds = []\n    out_counts = []\n    to_run = []\n\n    for i in range(len(conds)):\n        out_conds.append(torch.zeros_like(x_in))\n        out_counts.append(torch.ones_like(x_in) * 1e-37)\n\n        cond = conds[i]\n        if cond is not None:\n            for x in cond:\n                p = get_area_and_mult(x, x_in, timestep)\n                if p is None:\n                    continue\n\n                to_run += [(p, i)]\n\n    while len(to_run) > 0:\n        first = to_run[0]\n        first_shape = first[0][0].shape\n        to_batch_temp = []\n        for x in range(len(to_run)):\n            if can_concat_cond(to_run[x][0], first[0]):\n                to_batch_temp += [x]\n\n        to_batch_temp.reverse()\n        to_batch = to_batch_temp[:1]\n\n        free_memory = model_management.get_free_memory(x_in.device)\n        for i in range(1, len(to_batch_temp) + 1):\n            batch_amount = to_batch_temp[:len(to_batch_temp)//i]\n            input_shape = [len(batch_amount) * first_shape[0]] + list(first_shape)[1:]\n            if model.memory_required(input_shape) < free_memory:\n                to_batch = batch_amount\n                break\n\n        input_x = []\n        mult = []\n        c = []\n        cond_or_uncond = []\n        area = []\n        control = None\n        patches = None\n        for x in to_batch:\n            o = to_run.pop(x)\n            p = o[0]\n            input_x.append(p.input_x)\n            mult.append(p.mult)\n            c.append(p.conditioning)\n            area.append(p.area)\n            cond_or_uncond.append(o[1])\n            control = p.control\n            patches = p.patches\n\n        batch_chunks = len(cond_or_uncond)\n        input_x = torch.cat(input_x)\n        c = cond_cat(c)\n        timestep_ = torch.cat([timestep] * batch_chunks)\n\n        if control is not None:\n            c['control'] = control.get_control(input_x, timestep_, c, len(cond_or_uncond))\n\n        transformer_options = {}\n        if 'transformer_options' in model_options:\n            transformer_options = model_options['transformer_options'].copy()\n\n        if patches is not None:\n            if \"patches\" in transformer_options:\n                cur_patches = transformer_options[\"patches\"].copy()\n                for p in patches:\n                    if p in cur_patches:\n                        cur_patches[p] = cur_patches[p] + patches[p]\n                    else:\n                        cur_patches[p] = patches[p]\n                transformer_options[\"patches\"] = cur_patches\n            else:\n                transformer_options[\"patches\"] = patches\n\n        transformer_options[\"cond_or_uncond\"] = cond_or_uncond[:]\n        transformer_options[\"sigmas\"] = timestep\n\n        c['transformer_options'] = transformer_options\n\n        if 'model_function_wrapper' in model_options:\n            output = model_options['model_function_wrapper'](model.apply_model, {\"input\": input_x, \"timestep\": timestep_, \"c\": c, \"cond_or_uncond\": cond_or_uncond}).chunk(batch_chunks)\n        else:\n            output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\n\n        for o in range(batch_chunks):\n            cond_index = cond_or_uncond[o]\n            a = area[o]\n            if a is None:\n                out_conds[cond_index] += output[o] * mult[o]\n                out_counts[cond_index] += mult[o]\n            else:\n                out_c = out_conds[cond_index]\n                out_cts = out_counts[cond_index]\n                dims = len(a) // 2\n                for i in range(dims):\n                    out_c = out_c.narrow(i + 2, a[i + dims], a[i])\n                    out_cts = out_cts.narrow(i + 2, a[i + dims], a[i])\n                out_c += output[o] * mult[o]\n                out_cts += mult[o]\n\n    for i in range(len(out_conds)):\n        out_conds[i] /= out_counts[i]\n\n    return out_conds\n\ndef calc_cond_uncond_batch(model, cond, uncond, x_in, timestep, model_options): #TODO: remove\n    logging.warning(\"WARNING: The comfy.samplers.calc_cond_uncond_batch function is deprecated please use the calc_cond_batch one instead.\")\n    return tuple(calc_cond_batch(model, [cond, uncond], x_in, timestep, model_options))\n\ndef cfg_function(model, cond_pred, uncond_pred, cond_scale, x, timestep, model_options={}, cond=None, uncond=None):\n    if \"sampler_cfg_function\" in model_options:\n        args = {\"cond\": x - cond_pred, \"uncond\": x - uncond_pred, \"cond_scale\": cond_scale, \"timestep\": timestep, \"input\": x, \"sigma\": timestep,\n                \"cond_denoised\": cond_pred, \"uncond_denoised\": uncond_pred, \"model\": model, \"model_options\": model_options}\n        cfg_result = x - model_options[\"sampler_cfg_function\"](args)\n    else:\n        cfg_result = uncond_pred + (cond_pred - uncond_pred) * cond_scale\n\n    for fn in model_options.get(\"sampler_post_cfg_function\", []):\n        args = {\"denoised\": cfg_result, \"cond\": cond, \"uncond\": uncond, \"model\": model, \"uncond_denoised\": uncond_pred, \"cond_denoised\": cond_pred,\n                \"sigma\": timestep, \"model_options\": model_options, \"input\": x}\n        cfg_result = fn(args)\n\n    return cfg_result\n\n#The main sampling function shared by all the samplers\n#Returns denoised\ndef sampling_function(model, x, timestep, uncond, cond, cond_scale, model_options={}, seed=None):\n    if math.isclose(cond_scale, 1.0) and model_options.get(\"disable_cfg1_optimization\", False) == False:\n        uncond_ = None\n    else:\n        uncond_ = uncond\n\n    conds = [cond, uncond_]\n    out = calc_cond_batch(model, conds, x, timestep, model_options)\n    return cfg_function(model, out[0], out[1], cond_scale, x, timestep, model_options=model_options, cond=cond, uncond=uncond_)\n\n\nclass KSamplerX0Inpaint:\n    def __init__(self, model, sigmas):\n        self.inner_model = model\n        self.sigmas = sigmas\n    def __call__(self, x, sigma, denoise_mask, model_options={}, seed=None):\n        if denoise_mask is not None:\n            if \"denoise_mask_function\" in model_options:\n                denoise_mask = model_options[\"denoise_mask_function\"](sigma, denoise_mask, extra_options={\"model\": self.inner_model, \"sigmas\": self.sigmas})\n            latent_mask = 1. - denoise_mask\n            x = x * denoise_mask + self.inner_model.inner_model.model_sampling.noise_scaling(sigma.reshape([sigma.shape[0]] + [1] * (len(self.noise.shape) - 1)), self.noise, self.latent_image) * latent_mask\n        out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\n        if denoise_mask is not None:\n            out = out * denoise_mask + self.latent_image * latent_mask\n        return out\n\ndef simple_scheduler(model_sampling, steps):\n    s = model_sampling\n    sigs = []\n    ss = len(s.sigmas) / steps\n    for x in range(steps):\n        sigs += [float(s.sigmas[-(1 + int(x * ss))])]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef ddim_scheduler(model_sampling, steps):\n    s = model_sampling\n    sigs = []\n    ss = max(len(s.sigmas) // steps, 1)\n    x = 1\n    while x < len(s.sigmas):\n        sigs += [float(s.sigmas[x])]\n        x += ss\n    sigs = sigs[::-1]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef normal_scheduler(model_sampling, steps, sgm=False, floor=False):\n    s = model_sampling\n    start = s.timestep(s.sigma_max)\n    end = s.timestep(s.sigma_min)\n\n    if sgm:\n        timesteps = torch.linspace(start, end, steps + 1)[:-1]\n    else:\n        timesteps = torch.linspace(start, end, steps)\n\n    sigs = []\n    for x in range(len(timesteps)):\n        ts = timesteps[x]\n        sigs.append(s.sigma(ts))\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\n\ndef get_mask_aabb(masks):\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device, dtype=torch.int)\n\n    b = masks.shape[0]\n\n    bounding_boxes = torch.zeros((b, 4), device=masks.device, dtype=torch.int)\n    is_empty = torch.zeros((b), device=masks.device, dtype=torch.bool)\n    for i in range(b):\n        mask = masks[i]\n        if mask.numel() == 0:\n            continue\n        if torch.max(mask != 0) == False:\n            is_empty[i] = True\n            continue\n        y, x = torch.where(mask)\n        bounding_boxes[i, 0] = torch.min(x)\n        bounding_boxes[i, 1] = torch.min(y)\n        bounding_boxes[i, 2] = torch.max(x)\n        bounding_boxes[i, 3] = torch.max(y)\n\n    return bounding_boxes, is_empty\n\ndef resolve_areas_and_cond_masks_multidim(conditions, dims, device):\n    # We need to decide on an area outside the sampling loop in order to properly generate opposite areas of equal sizes.\n    # While we're doing this, we can also resolve the mask device and scaling for performance reasons\n    for i in range(len(conditions)):\n        c = conditions[i]\n        if 'area' in c:\n            area = c['area']\n            if area[0] == \"percentage\":\n                modified = c.copy()\n                a = area[1:]\n                a_len = len(a) // 2\n                area = ()\n                for d in range(len(dims)):\n                    area += (max(1, round(a[d] * dims[d])),)\n                for d in range(len(dims)):\n                    area += (round(a[d + a_len] * dims[d]),)\n\n                modified['area'] = area\n                c = modified\n                conditions[i] = c\n\n        if 'mask' in c:\n            mask = c['mask']\n            mask = mask.to(device=device)\n            modified = c.copy()\n            if len(mask.shape) == len(dims):\n                mask = mask.unsqueeze(0)\n            if mask.shape[1:] != dims:\n                mask = torch.nn.functional.interpolate(mask.unsqueeze(1), size=dims, mode='bilinear', align_corners=False).squeeze(1)\n\n            if modified.get(\"set_area_to_bounds\", False): #TODO: handle dim != 2\n                bounds = torch.max(torch.abs(mask),dim=0).values.unsqueeze(0)\n                boxes, is_empty = get_mask_aabb(bounds)\n                if is_empty[0]:\n                    # Use the minimum possible size for efficiency reasons. (Since the mask is all-0, this becomes a noop anyway)\n                    modified['area'] = (8, 8, 0, 0)\n                else:\n                    box = boxes[0]\n                    H, W, Y, X = (box[3] - box[1] + 1, box[2] - box[0] + 1, box[1], box[0])\n                    H = max(8, H)\n                    W = max(8, W)\n                    area = (int(H), int(W), int(Y), int(X))\n                    modified['area'] = area\n\n            modified['mask'] = mask\n            conditions[i] = modified\n\ndef resolve_areas_and_cond_masks(conditions, h, w, device):\n    logging.warning(\"WARNING: The comfy.samplers.resolve_areas_and_cond_masks function is deprecated please use the resolve_areas_and_cond_masks_multidim one instead.\")\n    return resolve_areas_and_cond_masks_multidim(conditions, [h, w], device)\n\ndef create_cond_with_same_area_if_none(conds, c): #TODO: handle dim != 2\n    if 'area' not in c:\n        return\n\n    c_area = c['area']\n    smallest = None\n    for x in conds:\n        if 'area' in x:\n            a = x['area']\n            if c_area[2] >= a[2] and c_area[3] >= a[3]:\n                if a[0] + a[2] >= c_area[0] + c_area[2]:\n                    if a[1] + a[3] >= c_area[1] + c_area[3]:\n                        if smallest is None:\n                            smallest = x\n                        elif 'area' not in smallest:\n                            smallest = x\n                        else:\n                            if smallest['area'][0] * smallest['area'][1] > a[0] * a[1]:\n                                smallest = x\n        else:\n            if smallest is None:\n                smallest = x\n    if smallest is None:\n        return\n    if 'area' in smallest:\n        if smallest['area'] == c_area:\n            return\n\n    out = c.copy()\n    out['model_conds'] = smallest['model_conds'].copy() #TODO: which fields should be copied?\n    conds += [out]\n\ndef calculate_start_end_timesteps(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n\n        timestep_start = None\n        timestep_end = None\n        if 'start_percent' in x:\n            timestep_start = s.percent_to_sigma(x['start_percent'])\n        if 'end_percent' in x:\n            timestep_end = s.percent_to_sigma(x['end_percent'])\n\n        if (timestep_start is not None) or (timestep_end is not None):\n            n = x.copy()\n            if (timestep_start is not None):\n                n['timestep_start'] = timestep_start\n            if (timestep_end is not None):\n                n['timestep_end'] = timestep_end\n            conds[t] = n\n\ndef pre_run_control(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n\n        timestep_start = None\n        timestep_end = None\n        percent_to_timestep_function = lambda a: s.percent_to_sigma(a)\n        if 'control' in x:\n            x['control'].pre_run(model, percent_to_timestep_function)\n\ndef apply_empty_x_to_equal_area(conds, uncond, name, uncond_fill_func):\n    cond_cnets = []\n    cond_other = []\n    uncond_cnets = []\n    uncond_other = []\n    for t in range(len(conds)):\n        x = conds[t]\n        if 'area' not in x:\n            if name in x and x[name] is not None:\n                cond_cnets.append(x[name])\n            else:\n                cond_other.append((x, t))\n    for t in range(len(uncond)):\n        x = uncond[t]\n        if 'area' not in x:\n            if name in x and x[name] is not None:\n                uncond_cnets.append(x[name])\n            else:\n                uncond_other.append((x, t))\n\n    if len(uncond_cnets) > 0:\n        return\n\n    for x in range(len(cond_cnets)):\n        temp = uncond_other[x % len(uncond_other)]\n        o = temp[0]\n        if name in o and o[name] is not None:\n            n = o.copy()\n            n[name] = uncond_fill_func(cond_cnets, x)\n            uncond += [n]\n        else:\n            n = o.copy()\n            n[name] = uncond_fill_func(cond_cnets, x)\n            uncond[temp[1]] = n\n\ndef encode_model_conds(model_function, conds, noise, device, prompt_type, **kwargs):\n    for t in range(len(conds)):\n        x = conds[t]\n        params = x.copy()\n        params[\"device\"] = device\n        params[\"noise\"] = noise\n        default_width = None\n        if len(noise.shape) >= 4: #TODO: 8 multiple should be set by the model\n            default_width = noise.shape[3] * 8\n        params[\"width\"] = params.get(\"width\", default_width)\n        params[\"height\"] = params.get(\"height\", noise.shape[2] * 8)\n        params[\"prompt_type\"] = params.get(\"prompt_type\", prompt_type)\n        for k in kwargs:\n            if k not in params:\n                params[k] = kwargs[k]\n\n        out = model_function(**params)\n        x = x.copy()\n        model_conds = x['model_conds'].copy()\n        for k in out:\n            model_conds[k] = out[k]\n        x['model_conds'] = model_conds\n        conds[t] = x\n    return conds\n\nclass Sampler:\n    def sample(self):\n        pass\n\n    def max_denoise(self, model_wrap, sigmas):\n        max_sigma = float(model_wrap.inner_model.model_sampling.sigma_max)\n        sigma = float(sigmas[0])\n        return math.isclose(max_sigma, sigma, rel_tol=1e-05) or sigma > max_sigma\n\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\n                  \"ipndm\", \"ipndm_v\"]\n\nclass KSAMPLER(Sampler):\n    def __init__(self, sampler_function, extra_options={}, inpaint_options={}):\n        self.sampler_function = sampler_function\n        self.extra_options = extra_options\n        self.inpaint_options = inpaint_options\n\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        extra_args[\"denoise_mask\"] = denoise_mask\n        model_k = KSamplerX0Inpaint(model_wrap, sigmas)\n        model_k.latent_image = latent_image\n        if self.inpaint_options.get(\"random\", False): #TODO: Should this be the default?\n            generator = torch.manual_seed(extra_args.get(\"seed\", 41) + 1)\n            model_k.noise = torch.randn(noise.shape, generator=generator, device=\"cpu\").to(noise.dtype).to(noise.device)\n        else:\n            model_k.noise = noise\n\n        noise = model_wrap.inner_model.model_sampling.noise_scaling(sigmas[0], noise, latent_image, self.max_denoise(model_wrap, sigmas))\n\n        k_callback = None\n        total_steps = len(sigmas) - 1\n        if callback is not None:\n            k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\n\n        samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\n        samples = model_wrap.inner_model.model_sampling.inverse_noise_scaling(sigmas[-1], samples)\n        return samples\n\n\ndef ksampler(sampler_name, extra_options={}, inpaint_options={}):\n    if sampler_name == \"dpm_fast\":\n        def dpm_fast_function(model, noise, sigmas, extra_args, callback, disable):\n            if len(sigmas) <= 1:\n                return noise\n\n            sigma_min = sigmas[-1]\n            if sigma_min == 0:\n                sigma_min = sigmas[-2]\n            total_steps = len(sigmas) - 1\n            return k_diffusion_sampling.sample_dpm_fast(model, noise, sigma_min, sigmas[0], total_steps, extra_args=extra_args, callback=callback, disable=disable)\n        sampler_function = dpm_fast_function\n    elif sampler_name == \"dpm_adaptive\":\n        def dpm_adaptive_function(model, noise, sigmas, extra_args, callback, disable, **extra_options):\n            if len(sigmas) <= 1:\n                return noise\n\n            sigma_min = sigmas[-1]\n            if sigma_min == 0:\n                sigma_min = sigmas[-2]\n            return k_diffusion_sampling.sample_dpm_adaptive(model, noise, sigma_min, sigmas[0], extra_args=extra_args, callback=callback, disable=disable, **extra_options)\n        sampler_function = dpm_adaptive_function\n    else:\n        sampler_function = getattr(k_diffusion_sampling, \"sample_{}\".format(sampler_name))\n\n    return KSAMPLER(sampler_function, extra_options, inpaint_options)\n\n\ndef process_conds(model, noise, conds, device, latent_image=None, denoise_mask=None, seed=None):\n    for k in conds:\n        conds[k] = conds[k][:]\n        resolve_areas_and_cond_masks_multidim(conds[k], noise.shape[2:], device)\n\n    for k in conds:\n        calculate_start_end_timesteps(model, conds[k])\n\n    if hasattr(model, 'extra_conds'):\n        for k in conds:\n            conds[k] = encode_model_conds(model.extra_conds, conds[k], noise, device, k, latent_image=latent_image, denoise_mask=denoise_mask, seed=seed)\n\n    #make sure each cond area has an opposite one with the same area\n    for k in conds:\n        for c in conds[k]:\n            for kk in conds:\n                if k != kk:\n                    create_cond_with_same_area_if_none(conds[kk], c)\n\n    for k in conds:\n        pre_run_control(model, conds[k])\n\n    if \"positive\" in conds:\n        positive = conds[\"positive\"]\n        for k in conds:\n            if k != \"positive\":\n                apply_empty_x_to_equal_area(list(filter(lambda c: c.get('control_apply_to_uncond', False) == True, positive)), conds[k], 'control', lambda cond_cnets, x: cond_cnets[x])\n                apply_empty_x_to_equal_area(positive, conds[k], 'gligen', lambda cond_cnets, x: cond_cnets[x])\n\n    return conds\n\nclass CFGGuider:\n    def __init__(self, model_patcher):\n        self.model_patcher = model_patcher\n        self.model_options = model_patcher.model_options\n        self.original_conds = {}\n        self.cfg = 1.0\n\n    def set_conds(self, positive, negative):\n        self.inner_set_conds({\"positive\": positive, \"negative\": negative})\n\n    def set_cfg(self, cfg):\n        self.cfg = cfg\n\n    def inner_set_conds(self, conds):\n        for k in conds:\n            self.original_conds[k] = comfy.sampler_helpers.convert_cond(conds[k])\n\n    def __call__(self, *args, **kwargs):\n        return self.predict_noise(*args, **kwargs)\n\n    def predict_noise(self, x, timestep, model_options={}, seed=None):\n        return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\n\n    def inner_sample(self, noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed):\n        if latent_image is not None and torch.count_nonzero(latent_image) > 0: #Don't shift the empty latent image.\n            latent_image = self.inner_model.process_latent_in(latent_image)\n\n        self.conds = process_conds(self.inner_model, noise, self.conds, device, latent_image, denoise_mask, seed)\n\n        extra_args = {\"model_options\": self.model_options, \"seed\":seed}\n\n        samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\n        return self.inner_model.process_latent_out(samples.to(torch.float32))\n\n    def sample(self, noise, latent_image, sampler, sigmas, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n        if sigmas.shape[-1] == 0:\n            return latent_image\n\n        self.conds = {}\n        for k in self.original_conds:\n            self.conds[k] = list(map(lambda a: a.copy(), self.original_conds[k]))\n\n        self.inner_model, self.conds, self.loaded_models = comfy.sampler_helpers.prepare_sampling(self.model_patcher, noise.shape, self.conds)\n        device = self.model_patcher.load_device\n\n        if denoise_mask is not None:\n            denoise_mask = comfy.sampler_helpers.prepare_mask(denoise_mask, noise.shape, device)\n\n        noise = noise.to(device)\n        latent_image = latent_image.to(device)\n        sigmas = sigmas.to(device)\n\n        output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n\n        comfy.sampler_helpers.cleanup_models(self.conds, self.loaded_models)\n        del self.inner_model\n        del self.conds\n        del self.loaded_models\n        return output\n\n\ndef sample(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    cfg_guider = CFGGuider(model)\n    cfg_guider.set_conds(positive, negative)\n    cfg_guider.set_cfg(cfg)\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n\n\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\n\ndef calculate_sigmas(model_sampling, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model_sampling.sigma_min), sigma_max=float(model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model_sampling.sigma_min), sigma_max=float(model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model_sampling, steps)\n    elif scheduler_name == \"simple\":\n        sigmas = simple_scheduler(model_sampling, steps)\n    elif scheduler_name == \"ddim_uniform\":\n        sigmas = ddim_scheduler(model_sampling, steps)\n    elif scheduler_name == \"sgm_uniform\":\n        sigmas = normal_scheduler(model_sampling, steps, sgm=True)\n    else:\n        logging.error(\"error invalid scheduler {}\".format(scheduler_name))\n    return sigmas\n\ndef sampler_object(name):\n    if name == \"uni_pc\":\n        sampler = KSAMPLER(uni_pc.sample_unipc)\n    elif name == \"uni_pc_bh2\":\n        sampler = KSAMPLER(uni_pc.sample_unipc_bh2)\n    elif name == \"ddim\":\n        sampler = ksampler(\"euler\", inpaint_options={\"random\": True})\n    else:\n        sampler = ksampler(name)\n    return sampler\n\nclass KSampler:\n    SCHEDULERS = SCHEDULER_NAMES\n    SAMPLERS = SAMPLER_NAMES\n    DISCARD_PENULTIMATE_SIGMA_SAMPLERS = set(('dpm_2', 'dpm_2_ancestral', 'uni_pc', 'uni_pc_bh2'))\n\n    def __init__(self, model, steps, device, sampler=None, scheduler=None, denoise=None, model_options={}):\n        self.model = model\n        self.device = device\n        if scheduler not in self.SCHEDULERS:\n            scheduler = self.SCHEDULERS[0]\n        if sampler not in self.SAMPLERS:\n            sampler = self.SAMPLERS[0]\n        self.scheduler = scheduler\n        self.sampler = sampler\n        self.set_steps(steps, denoise)\n        self.denoise = denoise\n        self.model_options = model_options\n\n    def calculate_sigmas(self, steps):\n        sigmas = None\n\n        discard_penultimate_sigma = False\n        if self.sampler in self.DISCARD_PENULTIMATE_SIGMA_SAMPLERS:\n            steps += 1\n            discard_penultimate_sigma = True\n\n        sigmas = calculate_sigmas(self.model.get_model_object(\"model_sampling\"), self.scheduler, steps)\n\n        if discard_penultimate_sigma:\n            sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n        return sigmas\n\n    def set_steps(self, steps, denoise=None):\n        self.steps = steps\n        if denoise is None or denoise > 0.9999:\n            self.sigmas = self.calculate_sigmas(steps).to(self.device)\n        else:\n            if denoise <= 0.0:\n                self.sigmas = torch.FloatTensor([])\n            else:\n                new_steps = int(steps/denoise)\n                sigmas = self.calculate_sigmas(new_steps).to(self.device)\n                self.sigmas = sigmas[-(steps + 1):]\n\n    def sample(self, noise, positive, negative, cfg, latent_image=None, start_step=None, last_step=None, force_full_denoise=False, denoise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n        if sigmas is None:\n            sigmas = self.sigmas\n\n        if last_step is not None and last_step < (len(sigmas) - 1):\n            sigmas = sigmas[:last_step + 1]\n            if force_full_denoise:\n                sigmas[-1] = 0\n\n        if start_step is not None:\n            if start_step < (len(sigmas) - 1):\n                sigmas = sigmas[start_step:]\n            else:\n                if latent_image is not None:\n                    return latent_image\n                else:\n                    return torch.zeros_like(noise)\n\n        sampler = sampler_object(self.sampler)\n\n        return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n", "comfy/sampler_helpers.py": "import torch\nimport comfy.model_management\nimport comfy.conds\n\ndef prepare_mask(noise_mask, shape, device):\n    \"\"\"ensures noise mask is of proper dimensions\"\"\"\n    noise_mask = torch.nn.functional.interpolate(noise_mask.reshape((-1, 1, noise_mask.shape[-2], noise_mask.shape[-1])), size=(shape[2], shape[3]), mode=\"bilinear\")\n    noise_mask = torch.cat([noise_mask] * shape[1], dim=1)\n    noise_mask = comfy.utils.repeat_to_batch_size(noise_mask, shape[0])\n    noise_mask = noise_mask.to(device)\n    return noise_mask\n\ndef get_models_from_cond(cond, model_type):\n    models = []\n    for c in cond:\n        if model_type in c:\n            models += [c[model_type]]\n    return models\n\ndef convert_cond(cond):\n    out = []\n    for c in cond:\n        temp = c[1].copy()\n        model_conds = temp.get(\"model_conds\", {})\n        if c[0] is not None:\n            model_conds[\"c_crossattn\"] = comfy.conds.CONDCrossAttn(c[0]) #TODO: remove\n            temp[\"cross_attn\"] = c[0]\n        temp[\"model_conds\"] = model_conds\n        out.append(temp)\n    return out\n\ndef get_additional_models(conds, dtype):\n    \"\"\"loads additional models in conditioning\"\"\"\n    cnets = []\n    gligen = []\n\n    for k in conds:\n        cnets += get_models_from_cond(conds[k], \"control\")\n        gligen += get_models_from_cond(conds[k], \"gligen\")\n\n    control_nets = set(cnets)\n\n    inference_memory = 0\n    control_models = []\n    for m in control_nets:\n        control_models += m.get_models()\n        inference_memory += m.inference_memory_requirements(dtype)\n\n    gligen = [x[1] for x in gligen]\n    models = control_models + gligen\n    return models, inference_memory\n\ndef cleanup_additional_models(models):\n    \"\"\"cleanup additional models that were loaded\"\"\"\n    for m in models:\n        if hasattr(m, 'cleanup'):\n            m.cleanup()\n\n\ndef prepare_sampling(model, noise_shape, conds):\n    device = model.load_device\n    real_model = None\n    models, inference_memory = get_additional_models(conds, model.model_dtype())\n    comfy.model_management.load_models_gpu([model] + models, model.memory_required([noise_shape[0] * 2] + list(noise_shape[1:])) + inference_memory)\n    real_model = model.model\n\n    return real_model, conds, models\n\ndef cleanup_models(conds, models):\n    cleanup_additional_models(models)\n\n    control_cleanup = []\n    for k in conds:\n        control_cleanup += get_models_from_cond(conds[k], \"control\")\n\n    cleanup_additional_models(set(control_cleanup))\n", "comfy/options.py": "\nargs_parsing = False\n\ndef enable_args_parsing(enable=True):\n    global args_parsing\n    args_parsing = enable\n", "comfy/sd3_clip.py": "from comfy import sd1_clip\nfrom comfy import sdxl_clip\nfrom transformers import T5TokenizerFast\nimport comfy.t5\nimport torch\nimport os\nimport comfy.model_management\nimport logging\n\nclass T5XXLModel(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", layer=\"last\", layer_idx=None, dtype=None):\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"t5_config_xxl.json\")\n        super().__init__(device=device, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype, special_tokens={\"end\": 1, \"pad\": 0}, model_class=comfy.t5.T5)\n\nclass T5XXLTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, embedding_directory=None):\n        tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"t5_tokenizer\")\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_size=4096, embedding_key='t5xxl', tokenizer_class=T5TokenizerFast, has_start_token=False, pad_to_max_length=False, max_length=99999999, min_length=77)\n\nclass SDT5XXLTokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"t5xxl\", tokenizer=T5XXLTokenizer)\n\nclass SDT5XXLModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"t5xxl\", clip_model=T5XXLModel, **kwargs)\n\n\n\nclass SD3Tokenizer:\n    def __init__(self, embedding_directory=None):\n        self.clip_l = sd1_clip.SDTokenizer(embedding_directory=embedding_directory)\n        self.clip_g = sdxl_clip.SDXLClipGTokenizer(embedding_directory=embedding_directory)\n        self.t5xxl = T5XXLTokenizer(embedding_directory=embedding_directory)\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[\"g\"] = self.clip_g.tokenize_with_weights(text, return_word_ids)\n        out[\"l\"] = self.clip_l.tokenize_with_weights(text, return_word_ids)\n        out[\"t5xxl\"] = self.t5xxl.tokenize_with_weights(text, return_word_ids)\n        return out\n\n    def untokenize(self, token_weight_pair):\n        return self.clip_g.untokenize(token_weight_pair)\n\nclass SD3ClipModel(torch.nn.Module):\n    def __init__(self, clip_l=True, clip_g=True, t5=True, dtype_t5=None, device=\"cpu\", dtype=None):\n        super().__init__()\n        self.dtypes = set()\n        if clip_l:\n            self.clip_l = sd1_clip.SDClipModel(layer=\"hidden\", layer_idx=-2, device=device, dtype=dtype, layer_norm_hidden_state=False, return_projected_pooled=False)\n            self.dtypes.add(dtype)\n        else:\n            self.clip_l = None\n\n        if clip_g:\n            self.clip_g = sdxl_clip.SDXLClipG(device=device, dtype=dtype)\n            self.dtypes.add(dtype)\n        else:\n            self.clip_g = None\n\n        if t5:\n            if dtype_t5 is None:\n                dtype_t5 = dtype\n            elif comfy.model_management.dtype_size(dtype_t5) > comfy.model_management.dtype_size(dtype):\n                dtype_t5 = dtype\n\n            if not comfy.model_management.supports_cast(device, dtype_t5):\n                dtype_t5 = dtype\n\n            self.t5xxl = T5XXLModel(device=device, dtype=dtype_t5)\n            self.dtypes.add(dtype_t5)\n        else:\n            self.t5xxl = None\n\n        logging.debug(\"Created SD3 text encoder with: clip_l {}, clip_g {}, t5xxl {}:{}\".format(clip_l, clip_g, t5, dtype_t5))\n\n    def set_clip_options(self, options):\n        if self.clip_l is not None:\n            self.clip_l.set_clip_options(options)\n        if self.clip_g is not None:\n            self.clip_g.set_clip_options(options)\n        if self.t5xxl is not None:\n            self.t5xxl.set_clip_options(options)\n\n    def reset_clip_options(self):\n        if self.clip_l is not None:\n            self.clip_l.reset_clip_options()\n        if self.clip_g is not None:\n            self.clip_g.reset_clip_options()\n        if self.t5xxl is not None:\n            self.t5xxl.reset_clip_options()\n\n    def encode_token_weights(self, token_weight_pairs):\n        token_weight_pairs_l = token_weight_pairs[\"l\"]\n        token_weight_pairs_g = token_weight_pairs[\"g\"]\n        token_weight_pars_t5 = token_weight_pairs[\"t5xxl\"]\n        lg_out = None\n        pooled = None\n        out = None\n\n        if len(token_weight_pairs_g) > 0 or len(token_weight_pairs_l) > 0:\n            if self.clip_l is not None:\n                lg_out, l_pooled = self.clip_l.encode_token_weights(token_weight_pairs_l)\n            else:\n                l_pooled = torch.zeros((1, 768), device=comfy.model_management.intermediate_device())\n\n            if self.clip_g is not None:\n                g_out, g_pooled = self.clip_g.encode_token_weights(token_weight_pairs_g)\n                if lg_out is not None:\n                    lg_out = torch.cat([lg_out, g_out], dim=-1)\n                else:\n                    lg_out = torch.nn.functional.pad(g_out, (768, 0))\n            else:\n                g_out = None\n                g_pooled = torch.zeros((1, 1280), device=comfy.model_management.intermediate_device())\n\n            if lg_out is not None:\n                lg_out = torch.nn.functional.pad(lg_out, (0, 4096 - lg_out.shape[-1]))\n                out = lg_out\n            pooled = torch.cat((l_pooled, g_pooled), dim=-1)\n\n        if self.t5xxl is not None:\n            t5_out, t5_pooled = self.t5xxl.encode_token_weights(token_weight_pars_t5)\n            if lg_out is not None:\n                out = torch.cat([lg_out, t5_out], dim=-2)\n            else:\n                out = t5_out\n\n        if out is None:\n            out = torch.zeros((1, 77, 4096), device=comfy.model_management.intermediate_device())\n\n        if pooled is None:\n            pooled = torch.zeros((1, 768 + 1280), device=comfy.model_management.intermediate_device())\n\n        return out, pooled\n\n    def load_sd(self, sd):\n        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in sd:\n            return self.clip_g.load_sd(sd)\n        elif \"text_model.encoder.layers.1.mlp.fc1.weight\" in sd:\n            return self.clip_l.load_sd(sd)\n        else:\n            return self.t5xxl.load_sd(sd)\n\ndef sd3_clip(clip_l=True, clip_g=True, t5=True, dtype_t5=None):\n    class SD3ClipModel_(SD3ClipModel):\n        def __init__(self, device=\"cpu\", dtype=None):\n            super().__init__(clip_l=clip_l, clip_g=clip_g, t5=t5, dtype_t5=dtype_t5, device=device, dtype=dtype)\n    return SD3ClipModel_\n", "comfy/t5.py": "import torch\nimport math\nfrom comfy.ldm.modules.attention import optimized_attention_for_device\n\nclass T5LayerNorm(torch.nn.Module):\n    def __init__(self, hidden_size, eps=1e-6, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.empty(hidden_size, dtype=dtype, device=device))\n        self.variance_epsilon = eps\n\n    def forward(self, x):\n        variance = x.pow(2).mean(-1, keepdim=True)\n        x = x * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight.to(device=x.device, dtype=x.dtype) * x\n\nclass T5DenseActDense(torch.nn.Module):\n    def __init__(self, model_dim, ff_dim, dtype, device, operations):\n        super().__init__()\n        self.wi = operations.Linear(model_dim, ff_dim, bias=False, dtype=dtype, device=device)\n        self.wo = operations.Linear(ff_dim, model_dim, bias=False, dtype=dtype, device=device)\n        # self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.wi(x))\n        # x = self.dropout(x)\n        x = self.wo(x)\n        return x\n\nclass T5DenseGatedActDense(torch.nn.Module):\n    def __init__(self, model_dim, ff_dim, dtype, device, operations):\n        super().__init__()\n        self.wi_0 = operations.Linear(model_dim, ff_dim, bias=False, dtype=dtype, device=device)\n        self.wi_1 = operations.Linear(model_dim, ff_dim, bias=False, dtype=dtype, device=device)\n        self.wo = operations.Linear(ff_dim, model_dim, bias=False, dtype=dtype, device=device)\n        # self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x):\n        hidden_gelu = torch.nn.functional.gelu(self.wi_0(x), approximate=\"tanh\")\n        hidden_linear = self.wi_1(x)\n        x = hidden_gelu * hidden_linear\n        # x = self.dropout(x)\n        x = self.wo(x)\n        return x\n\nclass T5LayerFF(torch.nn.Module):\n    def __init__(self, model_dim, ff_dim, ff_activation, dtype, device, operations):\n        super().__init__()\n        if ff_activation == \"gelu_pytorch_tanh\":\n            self.DenseReluDense = T5DenseGatedActDense(model_dim, ff_dim, dtype, device, operations)\n        elif ff_activation == \"relu\":\n            self.DenseReluDense = T5DenseActDense(model_dim, ff_dim, dtype, device, operations)\n\n        self.layer_norm = T5LayerNorm(model_dim, dtype=dtype, device=device, operations=operations)\n        # self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x):\n        forwarded_states = self.layer_norm(x)\n        forwarded_states = self.DenseReluDense(forwarded_states)\n        # x = x + self.dropout(forwarded_states)\n        x += forwarded_states\n        return x\n\nclass T5Attention(torch.nn.Module):\n    def __init__(self, model_dim, inner_dim, num_heads, relative_attention_bias, dtype, device, operations):\n        super().__init__()\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        self.q = operations.Linear(model_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.k = operations.Linear(model_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.v = operations.Linear(model_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.o = operations.Linear(inner_dim, model_dim, bias=False, dtype=dtype, device=device)\n        self.num_heads = num_heads\n\n        self.relative_attention_bias = None\n        if relative_attention_bias:\n            self.relative_attention_num_buckets = 32\n            self.relative_attention_max_distance = 128\n            self.relative_attention_bias = torch.nn.Embedding(self.relative_attention_num_buckets, self.num_heads, device=device)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n        \"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n            relative_position = torch.abs(relative_position)\n        else:\n            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position < max_exact\n\n        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n        relative_position_if_large = max_exact + (\n            torch.log(relative_position.float() / max_exact)\n            / math.log(max_distance / max_exact)\n            * (num_buckets - max_exact)\n        ).to(torch.long)\n        relative_position_if_large = torch.min(\n            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)\n        )\n\n        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length, device):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n        relative_position = memory_position - context_position  # shape (query_length, key_length)\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,  # shape (query_length, key_length)\n            bidirectional=True,\n            num_buckets=self.relative_attention_num_buckets,\n            max_distance=self.relative_attention_max_distance,\n        )\n        values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)\n        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n        return values\n\n    def forward(self, x, mask=None, past_bias=None, optimized_attention=None):\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        if self.relative_attention_bias is not None:\n            past_bias = self.compute_bias(x.shape[1], x.shape[1], x.device)\n\n        if past_bias is not None:\n            if mask is not None:\n                mask = mask + past_bias\n            else:\n                mask = past_bias\n\n        out = optimized_attention(q, k * ((k.shape[-1] / self.num_heads) ** 0.5), v, self.num_heads, mask)\n        return self.o(out), past_bias\n\nclass T5LayerSelfAttention(torch.nn.Module):\n    def __init__(self, model_dim, inner_dim, ff_dim, num_heads, relative_attention_bias, dtype, device, operations):\n        super().__init__()\n        self.SelfAttention = T5Attention(model_dim, inner_dim, num_heads, relative_attention_bias, dtype, device, operations)\n        self.layer_norm = T5LayerNorm(model_dim, dtype=dtype, device=device, operations=operations)\n        # self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x, mask=None, past_bias=None, optimized_attention=None):\n        normed_hidden_states = self.layer_norm(x)\n        output, past_bias = self.SelfAttention(self.layer_norm(x), mask=mask, past_bias=past_bias, optimized_attention=optimized_attention)\n        # x = x + self.dropout(attention_output)\n        x += output\n        return x, past_bias\n\nclass T5Block(torch.nn.Module):\n    def __init__(self, model_dim, inner_dim, ff_dim, ff_activation, num_heads, relative_attention_bias, dtype, device, operations):\n        super().__init__()\n        self.layer = torch.nn.ModuleList()\n        self.layer.append(T5LayerSelfAttention(model_dim, inner_dim, ff_dim, num_heads, relative_attention_bias, dtype, device, operations))\n        self.layer.append(T5LayerFF(model_dim, ff_dim, ff_activation, dtype, device, operations))\n\n    def forward(self, x, mask=None, past_bias=None, optimized_attention=None):\n        x, past_bias = self.layer[0](x, mask, past_bias, optimized_attention)\n        x = self.layer[-1](x)\n        return x, past_bias\n\nclass T5Stack(torch.nn.Module):\n    def __init__(self, num_layers, model_dim, inner_dim, ff_dim, ff_activation, num_heads, dtype, device, operations):\n        super().__init__()\n\n        self.block = torch.nn.ModuleList(\n            [T5Block(model_dim, inner_dim, ff_dim, ff_activation, num_heads, relative_attention_bias=(i == 0), dtype=dtype, device=device, operations=operations) for i in range(num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(model_dim, dtype=dtype, device=device, operations=operations)\n        # self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x, attention_mask=None, intermediate_output=None, final_layer_norm_intermediate=True):\n        mask = None\n        if attention_mask is not None:\n            mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n            mask = mask.masked_fill(mask.to(torch.bool), float(\"-inf\"))\n\n        intermediate = None\n        optimized_attention = optimized_attention_for_device(x.device, mask=attention_mask is not None, small_input=True)\n        past_bias = None\n        for i, l in enumerate(self.block):\n            x, past_bias = l(x, mask, past_bias, optimized_attention)\n            if i == intermediate_output:\n                intermediate = x.clone()\n        x = self.final_layer_norm(x)\n        if intermediate is not None and final_layer_norm_intermediate:\n            intermediate = self.final_layer_norm(intermediate)\n        return x, intermediate\n\nclass T5(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        self.num_layers = config_dict[\"num_layers\"]\n        model_dim = config_dict[\"d_model\"]\n\n        self.encoder = T5Stack(self.num_layers, model_dim, model_dim, config_dict[\"d_ff\"], config_dict[\"dense_act_fn\"], config_dict[\"num_heads\"], dtype, device, operations)\n        self.dtype = dtype\n        self.shared = torch.nn.Embedding(config_dict[\"vocab_size\"], model_dim, device=device)\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, embeddings):\n        self.shared = embeddings\n\n    def forward(self, input_ids, *args, **kwargs):\n        x = self.shared(input_ids)\n        return self.encoder(x, *args, **kwargs)\n", "comfy/model_patcher.py": "import torch\nimport copy\nimport inspect\nimport logging\nimport uuid\n\nimport comfy.utils\nimport comfy.model_management\nfrom comfy.types import UnetWrapperFunction\n\n\ndef weight_decompose(dora_scale, weight, lora_diff, alpha, strength):\n    dora_scale = comfy.model_management.cast_to_device(dora_scale, weight.device, torch.float32)\n    lora_diff *= alpha\n    weight_calc = weight + lora_diff.type(weight.dtype)\n    weight_norm = (\n        weight_calc.transpose(0, 1)\n        .reshape(weight_calc.shape[1], -1)\n        .norm(dim=1, keepdim=True)\n        .reshape(weight_calc.shape[1], *[1] * (weight_calc.dim() - 1))\n        .transpose(0, 1)\n    )\n\n    weight_calc *= (dora_scale / weight_norm).type(weight.dtype)\n    if strength != 1.0:\n        weight_calc -= weight\n        weight += strength * (weight_calc)\n    else:\n        weight[:] = weight_calc\n    return weight\n\n\ndef set_model_options_patch_replace(model_options, patch, name, block_name, number, transformer_index=None):\n    to = model_options[\"transformer_options\"].copy()\n\n    if \"patches_replace\" not in to:\n        to[\"patches_replace\"] = {}\n    else:\n        to[\"patches_replace\"] = to[\"patches_replace\"].copy()\n\n    if name not in to[\"patches_replace\"]:\n        to[\"patches_replace\"][name] = {}\n    else:\n        to[\"patches_replace\"][name] = to[\"patches_replace\"][name].copy()\n\n    if transformer_index is not None:\n        block = (block_name, number, transformer_index)\n    else:\n        block = (block_name, number)\n    to[\"patches_replace\"][name][block] = patch\n    model_options[\"transformer_options\"] = to\n    return model_options\n\ndef set_model_options_post_cfg_function(model_options, post_cfg_function, disable_cfg1_optimization=False):\n    model_options[\"sampler_post_cfg_function\"] = model_options.get(\"sampler_post_cfg_function\", []) + [post_cfg_function]\n    if disable_cfg1_optimization:\n        model_options[\"disable_cfg1_optimization\"] = True\n    return model_options\n\nclass ModelPatcher:\n    def __init__(self, model, load_device, offload_device, size=0, current_device=None, weight_inplace_update=False):\n        self.size = size\n        self.model = model\n        self.patches = {}\n        self.backup = {}\n        self.object_patches = {}\n        self.object_patches_backup = {}\n        self.model_options = {\"transformer_options\":{}}\n        self.model_size()\n        self.load_device = load_device\n        self.offload_device = offload_device\n        if current_device is None:\n            self.current_device = self.offload_device\n        else:\n            self.current_device = current_device\n\n        self.weight_inplace_update = weight_inplace_update\n        self.model_lowvram = False\n        self.lowvram_patch_counter = 0\n        self.patches_uuid = uuid.uuid4()\n\n    def model_size(self):\n        if self.size > 0:\n            return self.size\n        self.size = comfy.model_management.module_size(self.model)\n        return self.size\n\n    def clone(self):\n        n = ModelPatcher(self.model, self.load_device, self.offload_device, self.size, self.current_device, weight_inplace_update=self.weight_inplace_update)\n        n.patches = {}\n        for k in self.patches:\n            n.patches[k] = self.patches[k][:]\n        n.patches_uuid = self.patches_uuid\n\n        n.object_patches = self.object_patches.copy()\n        n.model_options = copy.deepcopy(self.model_options)\n        n.backup = self.backup\n        n.object_patches_backup = self.object_patches_backup\n        return n\n\n    def is_clone(self, other):\n        if hasattr(other, 'model') and self.model is other.model:\n            return True\n        return False\n\n    def clone_has_same_weights(self, clone):\n        if not self.is_clone(clone):\n            return False\n\n        if len(self.patches) == 0 and len(clone.patches) == 0:\n            return True\n\n        if self.patches_uuid == clone.patches_uuid:\n            if len(self.patches) != len(clone.patches):\n                logging.warning(\"WARNING: something went wrong, same patch uuid but different length of patches.\")\n            else:\n                return True\n\n    def memory_required(self, input_shape):\n        return self.model.memory_required(input_shape=input_shape)\n\n    def set_model_sampler_cfg_function(self, sampler_cfg_function, disable_cfg1_optimization=False):\n        if len(inspect.signature(sampler_cfg_function).parameters) == 3:\n            self.model_options[\"sampler_cfg_function\"] = lambda args: sampler_cfg_function(args[\"cond\"], args[\"uncond\"], args[\"cond_scale\"]) #Old way\n        else:\n            self.model_options[\"sampler_cfg_function\"] = sampler_cfg_function\n        if disable_cfg1_optimization:\n            self.model_options[\"disable_cfg1_optimization\"] = True\n\n    def set_model_sampler_post_cfg_function(self, post_cfg_function, disable_cfg1_optimization=False):\n        self.model_options = set_model_options_post_cfg_function(self.model_options, post_cfg_function, disable_cfg1_optimization)\n\n    def set_model_unet_function_wrapper(self, unet_wrapper_function: UnetWrapperFunction):\n        self.model_options[\"model_function_wrapper\"] = unet_wrapper_function\n\n    def set_model_denoise_mask_function(self, denoise_mask_function):\n        self.model_options[\"denoise_mask_function\"] = denoise_mask_function\n\n    def set_model_patch(self, patch, name):\n        to = self.model_options[\"transformer_options\"]\n        if \"patches\" not in to:\n            to[\"patches\"] = {}\n        to[\"patches\"][name] = to[\"patches\"].get(name, []) + [patch]\n\n    def set_model_patch_replace(self, patch, name, block_name, number, transformer_index=None):\n        self.model_options = set_model_options_patch_replace(self.model_options, patch, name, block_name, number, transformer_index=transformer_index)\n\n    def set_model_attn1_patch(self, patch):\n        self.set_model_patch(patch, \"attn1_patch\")\n\n    def set_model_attn2_patch(self, patch):\n        self.set_model_patch(patch, \"attn2_patch\")\n\n    def set_model_attn1_replace(self, patch, block_name, number, transformer_index=None):\n        self.set_model_patch_replace(patch, \"attn1\", block_name, number, transformer_index)\n\n    def set_model_attn2_replace(self, patch, block_name, number, transformer_index=None):\n        self.set_model_patch_replace(patch, \"attn2\", block_name, number, transformer_index)\n\n    def set_model_attn1_output_patch(self, patch):\n        self.set_model_patch(patch, \"attn1_output_patch\")\n\n    def set_model_attn2_output_patch(self, patch):\n        self.set_model_patch(patch, \"attn2_output_patch\")\n\n    def set_model_input_block_patch(self, patch):\n        self.set_model_patch(patch, \"input_block_patch\")\n\n    def set_model_input_block_patch_after_skip(self, patch):\n        self.set_model_patch(patch, \"input_block_patch_after_skip\")\n\n    def set_model_output_block_patch(self, patch):\n        self.set_model_patch(patch, \"output_block_patch\")\n\n    def add_object_patch(self, name, obj):\n        self.object_patches[name] = obj\n\n    def get_model_object(self, name):\n        if name in self.object_patches:\n            return self.object_patches[name]\n        else:\n            if name in self.object_patches_backup:\n                return self.object_patches_backup[name]\n            else:\n                return comfy.utils.get_attr(self.model, name)\n\n    def model_patches_to(self, device):\n        to = self.model_options[\"transformer_options\"]\n        if \"patches\" in to:\n            patches = to[\"patches\"]\n            for name in patches:\n                patch_list = patches[name]\n                for i in range(len(patch_list)):\n                    if hasattr(patch_list[i], \"to\"):\n                        patch_list[i] = patch_list[i].to(device)\n        if \"patches_replace\" in to:\n            patches = to[\"patches_replace\"]\n            for name in patches:\n                patch_list = patches[name]\n                for k in patch_list:\n                    if hasattr(patch_list[k], \"to\"):\n                        patch_list[k] = patch_list[k].to(device)\n        if \"model_function_wrapper\" in self.model_options:\n            wrap_func = self.model_options[\"model_function_wrapper\"]\n            if hasattr(wrap_func, \"to\"):\n                self.model_options[\"model_function_wrapper\"] = wrap_func.to(device)\n\n    def model_dtype(self):\n        if hasattr(self.model, \"get_dtype\"):\n            return self.model.get_dtype()\n\n    def add_patches(self, patches, strength_patch=1.0, strength_model=1.0):\n        p = set()\n        model_sd = self.model.state_dict()\n        for k in patches:\n            offset = None\n            function = None\n            if isinstance(k, str):\n                key = k\n            else:\n                offset = k[1]\n                key = k[0]\n                if len(k) > 2:\n                    function = k[2]\n\n            if key in model_sd:\n                p.add(k)\n                current_patches = self.patches.get(key, [])\n                current_patches.append((strength_patch, patches[k], strength_model, offset, function))\n                self.patches[key] = current_patches\n\n        self.patches_uuid = uuid.uuid4()\n        return list(p)\n\n    def get_key_patches(self, filter_prefix=None):\n        comfy.model_management.unload_model_clones(self)\n        model_sd = self.model_state_dict()\n        p = {}\n        for k in model_sd:\n            if filter_prefix is not None:\n                if not k.startswith(filter_prefix):\n                    continue\n            if k in self.patches:\n                p[k] = [model_sd[k]] + self.patches[k]\n            else:\n                p[k] = (model_sd[k],)\n        return p\n\n    def model_state_dict(self, filter_prefix=None):\n        sd = self.model.state_dict()\n        keys = list(sd.keys())\n        if filter_prefix is not None:\n            for k in keys:\n                if not k.startswith(filter_prefix):\n                    sd.pop(k)\n        return sd\n\n    def patch_weight_to_device(self, key, device_to=None):\n        if key not in self.patches:\n            return\n\n        weight = comfy.utils.get_attr(self.model, key)\n\n        inplace_update = self.weight_inplace_update\n\n        if key not in self.backup:\n            self.backup[key] = weight.to(device=self.offload_device, copy=inplace_update)\n\n        if device_to is not None:\n            temp_weight = comfy.model_management.cast_to_device(weight, device_to, torch.float32, copy=True)\n        else:\n            temp_weight = weight.to(torch.float32, copy=True)\n        out_weight = self.calculate_weight(self.patches[key], temp_weight, key).to(weight.dtype)\n        if inplace_update:\n            comfy.utils.copy_to_param(self.model, key, out_weight)\n        else:\n            comfy.utils.set_attr_param(self.model, key, out_weight)\n\n    def patch_model(self, device_to=None, patch_weights=True):\n        for k in self.object_patches:\n            old = comfy.utils.set_attr(self.model, k, self.object_patches[k])\n            if k not in self.object_patches_backup:\n                self.object_patches_backup[k] = old\n\n        if patch_weights:\n            model_sd = self.model_state_dict()\n            for key in self.patches:\n                if key not in model_sd:\n                    logging.warning(\"could not patch. key doesn't exist in model: {}\".format(key))\n                    continue\n\n                self.patch_weight_to_device(key, device_to)\n\n            if device_to is not None:\n                self.model.to(device_to)\n                self.current_device = device_to\n\n        return self.model\n\n    def patch_model_lowvram(self, device_to=None, lowvram_model_memory=0, force_patch_weights=False):\n        self.patch_model(device_to, patch_weights=False)\n\n        logging.info(\"loading in lowvram mode {}\".format(lowvram_model_memory/(1024 * 1024)))\n        class LowVramPatch:\n            def __init__(self, key, model_patcher):\n                self.key = key\n                self.model_patcher = model_patcher\n            def __call__(self, weight):\n                return self.model_patcher.calculate_weight(self.model_patcher.patches[self.key], weight, self.key)\n\n        mem_counter = 0\n        patch_counter = 0\n        for n, m in self.model.named_modules():\n            lowvram_weight = False\n            if hasattr(m, \"comfy_cast_weights\"):\n                module_mem = comfy.model_management.module_size(m)\n                if mem_counter + module_mem >= lowvram_model_memory:\n                    lowvram_weight = True\n\n            weight_key = \"{}.weight\".format(n)\n            bias_key = \"{}.bias\".format(n)\n\n            if lowvram_weight:\n                if weight_key in self.patches:\n                    if force_patch_weights:\n                        self.patch_weight_to_device(weight_key)\n                    else:\n                        m.weight_function = LowVramPatch(weight_key, self)\n                        patch_counter += 1\n                if bias_key in self.patches:\n                    if force_patch_weights:\n                        self.patch_weight_to_device(bias_key)\n                    else:\n                        m.bias_function = LowVramPatch(bias_key, self)\n                        patch_counter += 1\n\n                m.prev_comfy_cast_weights = m.comfy_cast_weights\n                m.comfy_cast_weights = True\n            else:\n                if hasattr(m, \"weight\"):\n                    self.patch_weight_to_device(weight_key, device_to)\n                    self.patch_weight_to_device(bias_key, device_to)\n                    m.to(device_to)\n                    mem_counter += comfy.model_management.module_size(m)\n                    logging.debug(\"lowvram: loaded module regularly {} {}\".format(n, m))\n\n        self.model_lowvram = True\n        self.lowvram_patch_counter = patch_counter\n        return self.model\n\n    def calculate_weight(self, patches, weight, key):\n        for p in patches:\n            strength = p[0]\n            v = p[1]\n            strength_model = p[2]\n            offset = p[3]\n            function = p[4]\n            if function is None:\n                function = lambda a: a\n\n            old_weight = None\n            if offset is not None:\n                old_weight = weight\n                weight = weight.narrow(offset[0], offset[1], offset[2])\n\n            if strength_model != 1.0:\n                weight *= strength_model\n\n            if isinstance(v, list):\n                v = (self.calculate_weight(v[1:], v[0].clone(), key), )\n\n            if len(v) == 1:\n                patch_type = \"diff\"\n            elif len(v) == 2:\n                patch_type = v[0]\n                v = v[1]\n\n            if patch_type == \"diff\":\n                w1 = v[0]\n                if strength != 0.0:\n                    if w1.shape != weight.shape:\n                        logging.warning(\"WARNING SHAPE MISMATCH {} WEIGHT NOT MERGED {} != {}\".format(key, w1.shape, weight.shape))\n                    else:\n                        weight += function(strength * comfy.model_management.cast_to_device(w1, weight.device, weight.dtype))\n            elif patch_type == \"lora\": #lora/locon\n                mat1 = comfy.model_management.cast_to_device(v[0], weight.device, torch.float32)\n                mat2 = comfy.model_management.cast_to_device(v[1], weight.device, torch.float32)\n                dora_scale = v[4]\n                if v[2] is not None:\n                    alpha = v[2] / mat2.shape[0]\n                else:\n                    alpha = 1.0\n\n                if v[3] is not None:\n                    #locon mid weights, hopefully the math is fine because I didn't properly test it\n                    mat3 = comfy.model_management.cast_to_device(v[3], weight.device, torch.float32)\n                    final_shape = [mat2.shape[1], mat2.shape[0], mat3.shape[2], mat3.shape[3]]\n                    mat2 = torch.mm(mat2.transpose(0, 1).flatten(start_dim=1), mat3.transpose(0, 1).flatten(start_dim=1)).reshape(final_shape).transpose(0, 1)\n                try:\n                    lora_diff = torch.mm(mat1.flatten(start_dim=1), mat2.flatten(start_dim=1)).reshape(weight.shape)\n                    if dora_scale is not None:\n                        weight = function(weight_decompose(dora_scale, weight, lora_diff, alpha, strength))\n                    else:\n                        weight += function(((strength * alpha) * lora_diff).type(weight.dtype))\n                except Exception as e:\n                    logging.error(\"ERROR {} {} {}\".format(patch_type, key, e))\n            elif patch_type == \"lokr\":\n                w1 = v[0]\n                w2 = v[1]\n                w1_a = v[3]\n                w1_b = v[4]\n                w2_a = v[5]\n                w2_b = v[6]\n                t2 = v[7]\n                dora_scale = v[8]\n                dim = None\n\n                if w1 is None:\n                    dim = w1_b.shape[0]\n                    w1 = torch.mm(comfy.model_management.cast_to_device(w1_a, weight.device, torch.float32),\n                                  comfy.model_management.cast_to_device(w1_b, weight.device, torch.float32))\n                else:\n                    w1 = comfy.model_management.cast_to_device(w1, weight.device, torch.float32)\n\n                if w2 is None:\n                    dim = w2_b.shape[0]\n                    if t2 is None:\n                        w2 = torch.mm(comfy.model_management.cast_to_device(w2_a, weight.device, torch.float32),\n                                      comfy.model_management.cast_to_device(w2_b, weight.device, torch.float32))\n                    else:\n                        w2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                          comfy.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                          comfy.model_management.cast_to_device(w2_b, weight.device, torch.float32),\n                                          comfy.model_management.cast_to_device(w2_a, weight.device, torch.float32))\n                else:\n                    w2 = comfy.model_management.cast_to_device(w2, weight.device, torch.float32)\n\n                if len(w2.shape) == 4:\n                    w1 = w1.unsqueeze(2).unsqueeze(2)\n                if v[2] is not None and dim is not None:\n                    alpha = v[2] / dim\n                else:\n                    alpha = 1.0\n\n                try:\n                    lora_diff = torch.kron(w1, w2).reshape(weight.shape)\n                    if dora_scale is not None:\n                        weight = function(weight_decompose(dora_scale, weight, lora_diff, alpha, strength))\n                    else:\n                        weight += function(((strength * alpha) * lora_diff).type(weight.dtype))\n                except Exception as e:\n                    logging.error(\"ERROR {} {} {}\".format(patch_type, key, e))\n            elif patch_type == \"loha\":\n                w1a = v[0]\n                w1b = v[1]\n                if v[2] is not None:\n                    alpha = v[2] / w1b.shape[0]\n                else:\n                    alpha = 1.0\n\n                w2a = v[3]\n                w2b = v[4]\n                dora_scale = v[7]\n                if v[5] is not None: #cp decomposition\n                    t1 = v[5]\n                    t2 = v[6]\n                    m1 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                      comfy.model_management.cast_to_device(t1, weight.device, torch.float32),\n                                      comfy.model_management.cast_to_device(w1b, weight.device, torch.float32),\n                                      comfy.model_management.cast_to_device(w1a, weight.device, torch.float32))\n\n                    m2 = torch.einsum('i j k l, j r, i p -> p r k l',\n                                      comfy.model_management.cast_to_device(t2, weight.device, torch.float32),\n                                      comfy.model_management.cast_to_device(w2b, weight.device, torch.float32),\n                                      comfy.model_management.cast_to_device(w2a, weight.device, torch.float32))\n                else:\n                    m1 = torch.mm(comfy.model_management.cast_to_device(w1a, weight.device, torch.float32),\n                                  comfy.model_management.cast_to_device(w1b, weight.device, torch.float32))\n                    m2 = torch.mm(comfy.model_management.cast_to_device(w2a, weight.device, torch.float32),\n                                  comfy.model_management.cast_to_device(w2b, weight.device, torch.float32))\n\n                try:\n                    lora_diff = (m1 * m2).reshape(weight.shape)\n                    if dora_scale is not None:\n                        weight = function(weight_decompose(dora_scale, weight, lora_diff, alpha, strength))\n                    else:\n                        weight += function(((strength * alpha) * lora_diff).type(weight.dtype))\n                except Exception as e:\n                    logging.error(\"ERROR {} {} {}\".format(patch_type, key, e))\n            elif patch_type == \"glora\":\n                if v[4] is not None:\n                    alpha = v[4] / v[0].shape[0]\n                else:\n                    alpha = 1.0\n\n                dora_scale = v[5]\n\n                a1 = comfy.model_management.cast_to_device(v[0].flatten(start_dim=1), weight.device, torch.float32)\n                a2 = comfy.model_management.cast_to_device(v[1].flatten(start_dim=1), weight.device, torch.float32)\n                b1 = comfy.model_management.cast_to_device(v[2].flatten(start_dim=1), weight.device, torch.float32)\n                b2 = comfy.model_management.cast_to_device(v[3].flatten(start_dim=1), weight.device, torch.float32)\n\n                try:\n                    lora_diff = (torch.mm(b2, b1) + torch.mm(torch.mm(weight.flatten(start_dim=1), a2), a1)).reshape(weight.shape)\n                    if dora_scale is not None:\n                        weight = function(weight_decompose(dora_scale, weight, lora_diff, alpha, strength))\n                    else:\n                        weight += function(((strength * alpha) * lora_diff).type(weight.dtype))\n                except Exception as e:\n                    logging.error(\"ERROR {} {} {}\".format(patch_type, key, e))\n            else:\n                logging.warning(\"patch type not recognized {} {}\".format(patch_type, key))\n\n            if old_weight is not None:\n                weight = old_weight\n\n        return weight\n\n    def unpatch_model(self, device_to=None, unpatch_weights=True):\n        if unpatch_weights:\n            if self.model_lowvram:\n                for m in self.model.modules():\n                    if hasattr(m, \"prev_comfy_cast_weights\"):\n                        m.comfy_cast_weights = m.prev_comfy_cast_weights\n                        del m.prev_comfy_cast_weights\n                    m.weight_function = None\n                    m.bias_function = None\n\n                self.model_lowvram = False\n                self.lowvram_patch_counter = 0\n\n            keys = list(self.backup.keys())\n\n            if self.weight_inplace_update:\n                for k in keys:\n                    comfy.utils.copy_to_param(self.model, k, self.backup[k])\n            else:\n                for k in keys:\n                    comfy.utils.set_attr_param(self.model, k, self.backup[k])\n\n            self.backup.clear()\n\n            if device_to is not None:\n                self.model.to(device_to)\n                self.current_device = device_to\n\n        keys = list(self.object_patches_backup.keys())\n        for k in keys:\n            comfy.utils.set_attr(self.model, k, self.object_patches_backup[k])\n\n        self.object_patches_backup.clear()\n", "comfy/sdxl_clip.py": "from comfy import sd1_clip\nimport torch\nimport os\n\nclass SDXLClipG(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_config_bigg.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype,\n                         special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0}, layer_norm_hidden_state=False)\n\n    def load_sd(self, sd):\n        return super().load_sd(sd)\n\nclass SDXLClipGTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1280, embedding_key='clip_g')\n\n\nclass SDXLTokenizer:\n    def __init__(self, embedding_directory=None):\n        self.clip_l = sd1_clip.SDTokenizer(embedding_directory=embedding_directory)\n        self.clip_g = SDXLClipGTokenizer(embedding_directory=embedding_directory)\n\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[\"g\"] = self.clip_g.tokenize_with_weights(text, return_word_ids)\n        out[\"l\"] = self.clip_l.tokenize_with_weights(text, return_word_ids)\n        return out\n\n    def untokenize(self, token_weight_pair):\n        return self.clip_g.untokenize(token_weight_pair)\n\nclass SDXLClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__()\n        self.clip_l = sd1_clip.SDClipModel(layer=\"hidden\", layer_idx=-2, device=device, dtype=dtype, layer_norm_hidden_state=False)\n        self.clip_g = SDXLClipG(device=device, dtype=dtype)\n        self.dtypes = set([dtype])\n\n    def set_clip_options(self, options):\n        self.clip_l.set_clip_options(options)\n        self.clip_g.set_clip_options(options)\n\n    def reset_clip_options(self):\n        self.clip_g.reset_clip_options()\n        self.clip_l.reset_clip_options()\n\n    def encode_token_weights(self, token_weight_pairs):\n        token_weight_pairs_g = token_weight_pairs[\"g\"]\n        token_weight_pairs_l = token_weight_pairs[\"l\"]\n        g_out, g_pooled = self.clip_g.encode_token_weights(token_weight_pairs_g)\n        l_out, l_pooled = self.clip_l.encode_token_weights(token_weight_pairs_l)\n        return torch.cat([l_out, g_out], dim=-1), g_pooled\n\n    def load_sd(self, sd):\n        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in sd:\n            return self.clip_g.load_sd(sd)\n        else:\n            return self.clip_l.load_sd(sd)\n\nclass SDXLRefinerClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__(device=device, dtype=dtype, clip_name=\"g\", clip_model=SDXLClipG)\n\n\nclass StableCascadeClipGTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=True, embedding_directory=embedding_directory, embedding_size=1280, embedding_key='clip_g')\n\nclass StableCascadeTokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"g\", tokenizer=StableCascadeClipGTokenizer)\n\nclass StableCascadeClipG(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", max_length=77, freeze=True, layer=\"hidden\", layer_idx=-1, dtype=None):\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_config_bigg.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype,\n                         special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 49407}, layer_norm_hidden_state=False, enable_attention_masks=True)\n\n    def load_sd(self, sd):\n        return super().load_sd(sd)\n\nclass StableCascadeClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__(device=device, dtype=dtype, clip_name=\"g\", clip_model=StableCascadeClipG)\n", "comfy/gligen.py": "import torch\nfrom torch import nn\nfrom .ldm.modules.attention import CrossAttention\nfrom inspect import isfunction\nimport comfy.ops\nops = comfy.ops.manual_cast\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = ops.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * torch.nn.functional.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            ops.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            ops.Linear(inner_dim, dim_out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass GatedCrossAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=context_dim,\n            heads=n_heads,\n            dim_head=d_head,\n            operations=ops)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = ops.LayerNorm(query_dim)\n        self.norm2 = ops.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_attn) * self.attn(self.norm1(x), objs, objs)\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass GatedSelfAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = ops.Linear(context_dim, query_dim)\n\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=query_dim,\n            heads=n_heads,\n            dim_head=d_head,\n            operations=ops)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = ops.LayerNorm(query_dim)\n        self.norm2 = ops.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        N_visual = x.shape[1]\n        objs = self.linear(objs)\n\n        x = x + self.scale * torch.tanh(self.alpha_attn) * self.attn(\n            self.norm1(torch.cat([x, objs], dim=1)))[:, 0:N_visual, :]\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass GatedSelfAttentionDense2(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = ops.Linear(context_dim, query_dim)\n\n        self.attn = CrossAttention(\n            query_dim=query_dim, context_dim=query_dim, dim_head=d_head, operations=ops)\n        self.ff = FeedForward(query_dim, glu=True)\n\n        self.norm1 = ops.LayerNorm(query_dim)\n        self.norm2 = ops.LayerNorm(query_dim)\n\n        self.register_parameter('alpha_attn', nn.Parameter(torch.tensor(0.)))\n        self.register_parameter('alpha_dense', nn.Parameter(torch.tensor(0.)))\n\n        # this can be useful: we can externally change magnitude of tanh(alpha)\n        # for example, when it is set to 0, then the entire model is same as\n        # original one\n        self.scale = 1\n\n    def forward(self, x, objs):\n\n        B, N_visual, _ = x.shape\n        B, N_ground, _ = objs.shape\n\n        objs = self.linear(objs)\n\n        # sanity check\n        size_v = math.sqrt(N_visual)\n        size_g = math.sqrt(N_ground)\n        assert int(size_v) == size_v, \"Visual tokens must be square rootable\"\n        assert int(size_g) == size_g, \"Grounding tokens must be square rootable\"\n        size_v = int(size_v)\n        size_g = int(size_g)\n\n        # select grounding token and resize it to visual token size as residual\n        out = self.attn(self.norm1(torch.cat([x, objs], dim=1)))[\n            :, N_visual:, :]\n        out = out.permute(0, 2, 1).reshape(B, -1, size_g, size_g)\n        out = torch.nn.functional.interpolate(\n            out, (size_v, size_v), mode='bicubic')\n        residual = out.reshape(B, -1, N_visual).permute(0, 2, 1)\n\n        # add residual to visual feature\n        x = x + self.scale * torch.tanh(self.alpha_attn) * residual\n        x = x + self.scale * \\\n            torch.tanh(self.alpha_dense) * self.ff(self.norm2(x))\n\n        return x\n\n\nclass FourierEmbedder():\n    def __init__(self, num_freqs=64, temperature=100):\n\n        self.num_freqs = num_freqs\n        self.temperature = temperature\n        self.freq_bands = temperature ** (torch.arange(num_freqs) / num_freqs)\n\n    @torch.no_grad()\n    def __call__(self, x, cat_dim=-1):\n        \"x: arbitrary shape of tensor. dim: cat dim\"\n        out = []\n        for freq in self.freq_bands:\n            out.append(torch.sin(freq * x))\n            out.append(torch.cos(freq * x))\n        return torch.cat(out, cat_dim)\n\n\nclass PositionNet(nn.Module):\n    def __init__(self, in_dim, out_dim, fourier_freqs=8):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        self.fourier_embedder = FourierEmbedder(num_freqs=fourier_freqs)\n        self.position_dim = fourier_freqs * 2 * 4  # 2 is sin&cos, 4 is xyxy\n\n        self.linears = nn.Sequential(\n            ops.Linear(self.in_dim + self.position_dim, 512),\n            nn.SiLU(),\n            ops.Linear(512, 512),\n            nn.SiLU(),\n            ops.Linear(512, out_dim),\n        )\n\n        self.null_positive_feature = torch.nn.Parameter(\n            torch.zeros([self.in_dim]))\n        self.null_position_feature = torch.nn.Parameter(\n            torch.zeros([self.position_dim]))\n\n    def forward(self, boxes, masks, positive_embeddings):\n        B, N, _ = boxes.shape\n        masks = masks.unsqueeze(-1)\n        positive_embeddings = positive_embeddings\n\n        # embedding position (it may includes padding as placeholder)\n        xyxy_embedding = self.fourier_embedder(boxes)  # B*N*4 --> B*N*C\n\n        # learnable null embedding\n        positive_null = self.null_positive_feature.to(device=boxes.device, dtype=boxes.dtype).view(1, 1, -1)\n        xyxy_null = self.null_position_feature.to(device=boxes.device, dtype=boxes.dtype).view(1, 1, -1)\n\n        # replace padding with learnable null embedding\n        positive_embeddings = positive_embeddings * \\\n            masks + (1 - masks) * positive_null\n        xyxy_embedding = xyxy_embedding * masks + (1 - masks) * xyxy_null\n\n        objs = self.linears(\n            torch.cat([positive_embeddings, xyxy_embedding], dim=-1))\n        assert objs.shape == torch.Size([B, N, self.out_dim])\n        return objs\n\n\nclass Gligen(nn.Module):\n    def __init__(self, modules, position_net, key_dim):\n        super().__init__()\n        self.module_list = nn.ModuleList(modules)\n        self.position_net = position_net\n        self.key_dim = key_dim\n        self.max_objs = 30\n        self.current_device = torch.device(\"cpu\")\n\n    def _set_position(self, boxes, masks, positive_embeddings):\n        objs = self.position_net(boxes, masks, positive_embeddings)\n        def func(x, extra_options):\n            key = extra_options[\"transformer_index\"]\n            module = self.module_list[key]\n            return module(x, objs.to(device=x.device, dtype=x.dtype))\n        return func\n\n    def set_position(self, latent_image_shape, position_params, device):\n        batch, c, h, w = latent_image_shape\n        masks = torch.zeros([self.max_objs], device=\"cpu\")\n        boxes = []\n        positive_embeddings = []\n        for p in position_params:\n            x1 = (p[4]) / w\n            y1 = (p[3]) / h\n            x2 = (p[4] + p[2]) / w\n            y2 = (p[3] + p[1]) / h\n            masks[len(boxes)] = 1.0\n            boxes += [torch.tensor((x1, y1, x2, y2)).unsqueeze(0)]\n            positive_embeddings += [p[0]]\n        append_boxes = []\n        append_conds = []\n        if len(boxes) < self.max_objs:\n            append_boxes = [torch.zeros(\n                [self.max_objs - len(boxes), 4], device=\"cpu\")]\n            append_conds = [torch.zeros(\n                [self.max_objs - len(boxes), self.key_dim], device=\"cpu\")]\n\n        box_out = torch.cat(\n            boxes + append_boxes).unsqueeze(0).repeat(batch, 1, 1)\n        masks = masks.unsqueeze(0).repeat(batch, 1)\n        conds = torch.cat(positive_embeddings +\n                          append_conds).unsqueeze(0).repeat(batch, 1, 1)\n        return self._set_position(\n            box_out.to(device),\n            masks.to(device),\n            conds.to(device))\n\n    def set_empty(self, latent_image_shape, device):\n        batch, c, h, w = latent_image_shape\n        masks = torch.zeros([self.max_objs], device=\"cpu\").repeat(batch, 1)\n        box_out = torch.zeros([self.max_objs, 4],\n                              device=\"cpu\").repeat(batch, 1, 1)\n        conds = torch.zeros([self.max_objs, self.key_dim],\n                            device=\"cpu\").repeat(batch, 1, 1)\n        return self._set_position(\n            box_out.to(device),\n            masks.to(device),\n            conds.to(device))\n\n\ndef load_gligen(sd):\n    sd_k = sd.keys()\n    output_list = []\n    key_dim = 768\n    for a in [\"input_blocks\", \"middle_block\", \"output_blocks\"]:\n        for b in range(20):\n            k_temp = filter(lambda k: \"{}.{}.\".format(a, b)\n                            in k and \".fuser.\" in k, sd_k)\n            k_temp = map(lambda k: (k, k.split(\".fuser.\")[-1]), k_temp)\n\n            n_sd = {}\n            for k in k_temp:\n                n_sd[k[1]] = sd[k[0]]\n            if len(n_sd) > 0:\n                query_dim = n_sd[\"linear.weight\"].shape[0]\n                key_dim = n_sd[\"linear.weight\"].shape[1]\n\n                if key_dim == 768:  # SD1.x\n                    n_heads = 8\n                    d_head = query_dim // n_heads\n                else:\n                    d_head = 64\n                    n_heads = query_dim // d_head\n\n                gated = GatedSelfAttentionDense(\n                    query_dim, key_dim, n_heads, d_head)\n                gated.load_state_dict(n_sd, strict=False)\n                output_list.append(gated)\n\n    if \"position_net.null_positive_feature\" in sd_k:\n        in_dim = sd[\"position_net.null_positive_feature\"].shape[0]\n        out_dim = sd[\"position_net.linears.4.weight\"].shape[0]\n\n        class WeightsLoader(torch.nn.Module):\n            pass\n        w = WeightsLoader()\n        w.position_net = PositionNet(in_dim, out_dim)\n        w.load_state_dict(sd, strict=False)\n\n    gligen = Gligen(output_list, w.position_net, key_dim)\n    return gligen\n", "comfy/checkpoint_pickle.py": "import pickle\n\nload = pickle.load\n\nclass Empty:\n    pass\n\nclass Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        #TODO: safe unpickle\n        if module.startswith(\"pytorch_lightning\"):\n            return Empty\n        return super().find_class(module, name)\n", "comfy/sd2_clip.py": "from comfy import sd1_clip\nimport os\n\nclass SD2ClipHModel(sd1_clip.SDClipModel):\n    def __init__(self, arch=\"ViT-H-14\", device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd2_clip_config.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype, special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0})\n\nclass SD2ClipHTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1024)\n\nclass SD2Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"h\", tokenizer=SD2ClipHTokenizer)\n\nclass SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)\n", "comfy/model_detection.py": "import comfy.supported_models\nimport comfy.supported_models_base\nimport comfy.utils\nimport math\nimport logging\nimport torch\n\ndef count_blocks(state_dict_keys, prefix_string):\n    count = 0\n    while True:\n        c = False\n        for k in state_dict_keys:\n            if k.startswith(prefix_string.format(count)):\n                c = True\n                break\n        if c == False:\n            break\n        count += 1\n    return count\n\ndef calculate_transformer_depth(prefix, state_dict_keys, state_dict):\n    context_dim = None\n    use_linear_in_transformer = False\n\n    transformer_prefix = prefix + \"1.transformer_blocks.\"\n    transformer_keys = sorted(list(filter(lambda a: a.startswith(transformer_prefix), state_dict_keys)))\n    if len(transformer_keys) > 0:\n        last_transformer_depth = count_blocks(state_dict_keys, transformer_prefix + '{}')\n        context_dim = state_dict['{}0.attn2.to_k.weight'.format(transformer_prefix)].shape[1]\n        use_linear_in_transformer = len(state_dict['{}1.proj_in.weight'.format(prefix)].shape) == 2\n        time_stack = '{}1.time_stack.0.attn1.to_q.weight'.format(prefix) in state_dict or '{}1.time_mix_blocks.0.attn1.to_q.weight'.format(prefix) in state_dict\n        time_stack_cross = '{}1.time_stack.0.attn2.to_q.weight'.format(prefix) in state_dict or '{}1.time_mix_blocks.0.attn2.to_q.weight'.format(prefix) in state_dict\n        return last_transformer_depth, context_dim, use_linear_in_transformer, time_stack, time_stack_cross\n    return None\n\ndef detect_unet_config(state_dict, key_prefix):\n    state_dict_keys = list(state_dict.keys())\n\n    if '{}joint_blocks.0.context_block.attn.qkv.weight'.format(key_prefix) in state_dict_keys: #mmdit model\n        unet_config = {}\n        unet_config[\"in_channels\"] = state_dict['{}x_embedder.proj.weight'.format(key_prefix)].shape[1]\n        patch_size = state_dict['{}x_embedder.proj.weight'.format(key_prefix)].shape[2]\n        unet_config[\"patch_size\"] = patch_size\n        unet_config[\"out_channels\"] = state_dict['{}final_layer.linear.weight'.format(key_prefix)].shape[0] // (patch_size * patch_size)\n\n        unet_config[\"depth\"] = state_dict['{}x_embedder.proj.weight'.format(key_prefix)].shape[0] // 64\n        unet_config[\"input_size\"] = None\n        y_key = '{}y_embedder.mlp.0.weight'.format(key_prefix)\n        if y_key in state_dict_keys:\n            unet_config[\"adm_in_channels\"] = state_dict[y_key].shape[1]\n\n        context_key = '{}context_embedder.weight'.format(key_prefix)\n        if context_key in state_dict_keys:\n            in_features = state_dict[context_key].shape[1]\n            out_features = state_dict[context_key].shape[0]\n            unet_config[\"context_embedder_config\"] = {\"target\": \"torch.nn.Linear\", \"params\": {\"in_features\": in_features, \"out_features\": out_features}}\n        num_patches_key = '{}pos_embed'.format(key_prefix)\n        if num_patches_key in state_dict_keys:\n            num_patches = state_dict[num_patches_key].shape[1]\n            unet_config[\"num_patches\"] = num_patches\n            unet_config[\"pos_embed_max_size\"] = round(math.sqrt(num_patches))\n\n        rms_qk = '{}joint_blocks.0.context_block.attn.ln_q.weight'.format(key_prefix)\n        if rms_qk in state_dict_keys:\n            unet_config[\"qk_norm\"] = \"rms\"\n\n        unet_config[\"pos_embed_scaling_factor\"] = None #unused for inference\n        context_processor = '{}context_processor.layers.0.attn.qkv.weight'.format(key_prefix)\n        if context_processor in state_dict_keys:\n            unet_config[\"context_processor_layers\"] = count_blocks(state_dict_keys, '{}context_processor.layers.'.format(key_prefix) + '{}.')\n        return unet_config\n\n    if '{}clf.1.weight'.format(key_prefix) in state_dict_keys: #stable cascade\n        unet_config = {}\n        text_mapper_name = '{}clip_txt_mapper.weight'.format(key_prefix)\n        if text_mapper_name in state_dict_keys:\n            unet_config['stable_cascade_stage'] = 'c'\n            w = state_dict[text_mapper_name]\n            if w.shape[0] == 1536: #stage c lite\n                unet_config['c_cond'] = 1536\n                unet_config['c_hidden'] = [1536, 1536]\n                unet_config['nhead'] = [24, 24]\n                unet_config['blocks'] = [[4, 12], [12, 4]]\n            elif w.shape[0] == 2048: #stage c full\n                unet_config['c_cond'] = 2048\n        elif '{}clip_mapper.weight'.format(key_prefix) in state_dict_keys:\n            unet_config['stable_cascade_stage'] = 'b'\n            w = state_dict['{}down_blocks.1.0.channelwise.0.weight'.format(key_prefix)]\n            if w.shape[-1] == 640:\n                unet_config['c_hidden'] = [320, 640, 1280, 1280]\n                unet_config['nhead'] = [-1, -1, 20, 20]\n                unet_config['blocks'] = [[2, 6, 28, 6], [6, 28, 6, 2]]\n                unet_config['block_repeat'] = [[1, 1, 1, 1], [3, 3, 2, 2]]\n            elif w.shape[-1] == 576: #stage b lite\n                unet_config['c_hidden'] = [320, 576, 1152, 1152]\n                unet_config['nhead'] = [-1, 9, 18, 18]\n                unet_config['blocks'] = [[2, 4, 14, 4], [4, 14, 4, 2]]\n                unet_config['block_repeat'] = [[1, 1, 1, 1], [2, 2, 2, 2]]\n        return unet_config\n\n    if '{}transformer.rotary_pos_emb.inv_freq'.format(key_prefix) in state_dict_keys: #stable audio dit\n        unet_config = {}\n        unet_config[\"audio_model\"] = \"dit1.0\"\n        return unet_config\n\n    unet_config = {\n        \"use_checkpoint\": False,\n        \"image_size\": 32,\n        \"use_spatial_transformer\": True,\n        \"legacy\": False\n    }\n\n    y_input = '{}label_emb.0.0.weight'.format(key_prefix)\n    if y_input in state_dict_keys:\n        unet_config[\"num_classes\"] = \"sequential\"\n        unet_config[\"adm_in_channels\"] = state_dict[y_input].shape[1]\n    else:\n        unet_config[\"adm_in_channels\"] = None\n\n    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n    in_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[1]\n\n    out_key = '{}out.2.weight'.format(key_prefix)\n    if out_key in state_dict:\n        out_channels = state_dict[out_key].shape[0]\n    else:\n        out_channels = 4\n\n    num_res_blocks = []\n    channel_mult = []\n    attention_resolutions = []\n    transformer_depth = []\n    transformer_depth_output = []\n    context_dim = None\n    use_linear_in_transformer = False\n\n    video_model = False\n    video_model_cross = False\n\n    current_res = 1\n    count = 0\n\n    last_res_blocks = 0\n    last_channel_mult = 0\n\n    input_block_count = count_blocks(state_dict_keys, '{}input_blocks'.format(key_prefix) + '.{}.')\n    for count in range(input_block_count):\n        prefix = '{}input_blocks.{}.'.format(key_prefix, count)\n        prefix_output = '{}output_blocks.{}.'.format(key_prefix, input_block_count - count - 1)\n\n        block_keys = sorted(list(filter(lambda a: a.startswith(prefix), state_dict_keys)))\n        if len(block_keys) == 0:\n            break\n\n        block_keys_output = sorted(list(filter(lambda a: a.startswith(prefix_output), state_dict_keys)))\n\n        if \"{}0.op.weight\".format(prefix) in block_keys: #new layer\n            num_res_blocks.append(last_res_blocks)\n            channel_mult.append(last_channel_mult)\n\n            current_res *= 2\n            last_res_blocks = 0\n            last_channel_mult = 0\n            out = calculate_transformer_depth(prefix_output, state_dict_keys, state_dict)\n            if out is not None:\n                transformer_depth_output.append(out[0])\n            else:\n                transformer_depth_output.append(0)\n        else:\n            res_block_prefix = \"{}0.in_layers.0.weight\".format(prefix)\n            if res_block_prefix in block_keys:\n                last_res_blocks += 1\n                last_channel_mult = state_dict[\"{}0.out_layers.3.weight\".format(prefix)].shape[0] // model_channels\n\n                out = calculate_transformer_depth(prefix, state_dict_keys, state_dict)\n                if out is not None:\n                    transformer_depth.append(out[0])\n                    if context_dim is None:\n                        context_dim = out[1]\n                        use_linear_in_transformer = out[2]\n                        video_model = out[3]\n                        video_model_cross = out[4]\n                else:\n                    transformer_depth.append(0)\n\n            res_block_prefix = \"{}0.in_layers.0.weight\".format(prefix_output)\n            if res_block_prefix in block_keys_output:\n                out = calculate_transformer_depth(prefix_output, state_dict_keys, state_dict)\n                if out is not None:\n                    transformer_depth_output.append(out[0])\n                else:\n                    transformer_depth_output.append(0)\n\n\n    num_res_blocks.append(last_res_blocks)\n    channel_mult.append(last_channel_mult)\n    if \"{}middle_block.1.proj_in.weight\".format(key_prefix) in state_dict_keys:\n        transformer_depth_middle = count_blocks(state_dict_keys, '{}middle_block.1.transformer_blocks.'.format(key_prefix) + '{}')\n    elif \"{}middle_block.0.in_layers.0.weight\".format(key_prefix) in state_dict_keys:\n        transformer_depth_middle = -1\n    else:\n        transformer_depth_middle = -2\n\n    unet_config[\"in_channels\"] = in_channels\n    unet_config[\"out_channels\"] = out_channels\n    unet_config[\"model_channels\"] = model_channels\n    unet_config[\"num_res_blocks\"] = num_res_blocks\n    unet_config[\"transformer_depth\"] = transformer_depth\n    unet_config[\"transformer_depth_output\"] = transformer_depth_output\n    unet_config[\"channel_mult\"] = channel_mult\n    unet_config[\"transformer_depth_middle\"] = transformer_depth_middle\n    unet_config['use_linear_in_transformer'] = use_linear_in_transformer\n    unet_config[\"context_dim\"] = context_dim\n\n    if video_model:\n        unet_config[\"extra_ff_mix_layer\"] = True\n        unet_config[\"use_spatial_context\"] = True\n        unet_config[\"merge_strategy\"] = \"learned_with_images\"\n        unet_config[\"merge_factor\"] = 0.0\n        unet_config[\"video_kernel_size\"] = [3, 1, 1]\n        unet_config[\"use_temporal_resblock\"] = True\n        unet_config[\"use_temporal_attention\"] = True\n        unet_config[\"disable_temporal_crossattention\"] = not video_model_cross\n    else:\n        unet_config[\"use_temporal_resblock\"] = False\n        unet_config[\"use_temporal_attention\"] = False\n\n    return unet_config\n\ndef model_config_from_unet_config(unet_config, state_dict=None):\n    for model_config in comfy.supported_models.models:\n        if model_config.matches(unet_config, state_dict):\n            return model_config(unet_config)\n\n    logging.error(\"no match {}\".format(unet_config))\n    return None\n\ndef model_config_from_unet(state_dict, unet_key_prefix, use_base_if_no_match=False):\n    unet_config = detect_unet_config(state_dict, unet_key_prefix)\n    model_config = model_config_from_unet_config(unet_config, state_dict)\n    if model_config is None and use_base_if_no_match:\n        return comfy.supported_models_base.BASE(unet_config)\n    else:\n        return model_config\n\ndef unet_prefix_from_state_dict(state_dict):\n    if \"model.model.postprocess_conv.weight\" in state_dict: #audio models\n        unet_key_prefix = \"model.model.\"\n    else:\n        unet_key_prefix = \"model.diffusion_model.\"\n    return unet_key_prefix\n\ndef convert_config(unet_config):\n    new_config = unet_config.copy()\n    num_res_blocks = new_config.get(\"num_res_blocks\", None)\n    channel_mult = new_config.get(\"channel_mult\", None)\n\n    if isinstance(num_res_blocks, int):\n        num_res_blocks = len(channel_mult) * [num_res_blocks]\n\n    if \"attention_resolutions\" in new_config:\n        attention_resolutions = new_config.pop(\"attention_resolutions\")\n        transformer_depth = new_config.get(\"transformer_depth\", None)\n        transformer_depth_middle = new_config.get(\"transformer_depth_middle\", None)\n\n        if isinstance(transformer_depth, int):\n            transformer_depth = len(channel_mult) * [transformer_depth]\n        if transformer_depth_middle is None:\n            transformer_depth_middle =  transformer_depth[-1]\n        t_in = []\n        t_out = []\n        s = 1\n        for i in range(len(num_res_blocks)):\n            res = num_res_blocks[i]\n            d = 0\n            if s in attention_resolutions:\n                d = transformer_depth[i]\n\n            t_in += [d] * res\n            t_out += [d] * (res + 1)\n            s *= 2\n        transformer_depth = t_in\n        transformer_depth_output = t_out\n        new_config[\"transformer_depth\"] = t_in\n        new_config[\"transformer_depth_output\"] = t_out\n        new_config[\"transformer_depth_middle\"] = transformer_depth_middle\n\n    new_config[\"num_res_blocks\"] = num_res_blocks\n    return new_config\n\n\ndef unet_config_from_diffusers_unet(state_dict, dtype=None):\n    match = {}\n    transformer_depth = []\n\n    attn_res = 1\n    down_blocks = count_blocks(state_dict, \"down_blocks.{}\")\n    for i in range(down_blocks):\n        attn_blocks = count_blocks(state_dict, \"down_blocks.{}.attentions.\".format(i) + '{}')\n        res_blocks = count_blocks(state_dict, \"down_blocks.{}.resnets.\".format(i) + '{}')\n        for ab in range(attn_blocks):\n            transformer_count = count_blocks(state_dict, \"down_blocks.{}.attentions.{}.transformer_blocks.\".format(i, ab) + '{}')\n            transformer_depth.append(transformer_count)\n            if transformer_count > 0:\n                match[\"context_dim\"] = state_dict[\"down_blocks.{}.attentions.{}.transformer_blocks.0.attn2.to_k.weight\".format(i, ab)].shape[1]\n\n        attn_res *= 2\n        if attn_blocks == 0:\n            for i in range(res_blocks):\n                transformer_depth.append(0)\n\n    match[\"transformer_depth\"] = transformer_depth\n\n    match[\"model_channels\"] = state_dict[\"conv_in.weight\"].shape[0]\n    match[\"in_channels\"] = state_dict[\"conv_in.weight\"].shape[1]\n    match[\"adm_in_channels\"] = None\n    if \"class_embedding.linear_1.weight\" in state_dict:\n        match[\"adm_in_channels\"] = state_dict[\"class_embedding.linear_1.weight\"].shape[1]\n    elif \"add_embedding.linear_1.weight\" in state_dict:\n        match[\"adm_in_channels\"] = state_dict[\"add_embedding.linear_1.weight\"].shape[1]\n\n    SDXL = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n            'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 10, 10], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 10,\n            'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 2, 2, 2, 10, 10, 10],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_refiner = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 2560, 'dtype': dtype, 'in_channels': 4, 'model_channels': 384,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [0, 0, 4, 4, 4, 4, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 4,\n                    'use_linear_in_transformer': True, 'context_dim': 1280, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21 = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'adm_in_channels': None, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [2, 2, 2, 2],\n            'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1, 'use_linear_in_transformer': True,\n            'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21_uncliph = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 2048, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1,\n                    'use_linear_in_transformer': True, 'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD21_unclipl = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                    'num_classes': 'sequential', 'adm_in_channels': 1536, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                    'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0], 'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1,\n                    'use_linear_in_transformer': True, 'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD15 = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False, 'adm_in_channels': None,\n            'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0],\n            'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1, 'use_linear_in_transformer': False, 'context_dim': 768, 'num_heads': 8,\n            'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_mid_cnet = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                     'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                     'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 0, 0, 1, 1], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 1,\n                     'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 0, 0, 0, 1, 1, 1],\n                     'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_small_cnet = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                       'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n                       'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 0, 0, 0, 0], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 0,\n                       'use_linear_in_transformer': True, 'num_head_channels': 64, 'context_dim': 1, 'transformer_depth_output': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_diffusers_inpaint = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 9, 'model_channels': 320,\n                              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 10, 10], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 10,\n                              'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 2, 2, 2, 10, 10, 10],\n                              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SDXL_diffusers_ip2p = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n                              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 8, 'model_channels': 320,\n                              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 10, 10], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': 10,\n                              'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 0, 2, 2, 2, 10, 10, 10],\n                              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SSD_1B = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 2, 2, 4, 4], 'transformer_depth_output': [0, 0, 0, 1, 1, 2, 10, 4, 4],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': -1, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    Segmind_Vega = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [2, 2, 2], 'transformer_depth': [0, 0, 1, 1, 2, 2], 'transformer_depth_output': [0, 0, 0, 1, 1, 1, 2, 2, 2],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': -1, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    KOALA_700M = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [1, 1, 1], 'transformer_depth': [0, 2, 5], 'transformer_depth_output': [0, 0, 2, 2, 5, 5],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': -2, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    KOALA_1B = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n              'num_classes': 'sequential', 'adm_in_channels': 2816, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320,\n              'num_res_blocks': [1, 1, 1], 'transformer_depth': [0, 2, 6], 'transformer_depth_output': [0, 0, 2, 2, 6, 6],\n              'channel_mult': [1, 2, 4], 'transformer_depth_middle': 6, 'use_linear_in_transformer': True, 'context_dim': 2048, 'num_head_channels': 64,\n              'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n    SD09_XS = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'adm_in_channels': None, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [1, 1, 1],\n            'transformer_depth': [1, 1, 1], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': -2, 'use_linear_in_transformer': True,\n            'context_dim': 1024, 'num_head_channels': 64, 'transformer_depth_output': [1, 1, 1, 1, 1, 1],\n            'use_temporal_attention': False, 'use_temporal_resblock': False, 'disable_self_attentions': [True, False, False]}\n\n    SD_XS = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False,\n            'adm_in_channels': None, 'dtype': dtype, 'in_channels': 4, 'model_channels': 320, 'num_res_blocks': [1, 1, 1],\n            'transformer_depth': [0, 1, 1], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': -2, 'use_linear_in_transformer': False,\n            'context_dim': 768, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 1, 1, 1, 1],\n            'use_temporal_attention': False, 'use_temporal_resblock': False}\n\n\n    supported_models = [SDXL, SDXL_refiner, SD21, SD15, SD21_uncliph, SD21_unclipl, SDXL_mid_cnet, SDXL_small_cnet, SDXL_diffusers_inpaint, SSD_1B, Segmind_Vega, KOALA_700M, KOALA_1B, SD09_XS, SD_XS, SDXL_diffusers_ip2p]\n\n    for unet_config in supported_models:\n        matches = True\n        for k in match:\n            if match[k] != unet_config[k]:\n                matches = False\n                break\n        if matches:\n            return convert_config(unet_config)\n    return None\n\ndef model_config_from_diffusers_unet(state_dict):\n    unet_config = unet_config_from_diffusers_unet(state_dict)\n    if unet_config is not None:\n        return model_config_from_unet_config(unet_config)\n    return None\n\ndef convert_diffusers_mmdit(state_dict, output_prefix=\"\"):\n    depth = count_blocks(state_dict, 'transformer_blocks.{}.')\n    if depth > 0:\n        out_sd = {}\n        sd_map = comfy.utils.mmdit_to_diffusers({\"depth\": depth}, output_prefix=output_prefix)\n        for k in sd_map:\n            weight = state_dict.get(k, None)\n            if weight is not None:\n                t = sd_map[k]\n\n                if not isinstance(t, str):\n                    if len(t) > 2:\n                        fun = t[2]\n                    else:\n                        fun = lambda a: a\n                    offset = t[1]\n                    if offset is not None:\n                        old_weight = out_sd.get(t[0], None)\n                        if old_weight is None:\n                            old_weight = torch.empty_like(weight)\n                            old_weight = old_weight.repeat([3] + [1] * (len(old_weight.shape) - 1))\n\n                        w = old_weight.narrow(offset[0], offset[1], offset[2])\n                    else:\n                        old_weight = weight\n                        w = weight\n                    w[:] = fun(weight)\n                    t = t[0]\n                    out_sd[t] = old_weight\n                else:\n                    out_sd[t] = weight\n                state_dict.pop(k)\n\n    return out_sd\n", "comfy/sample.py": "import torch\nimport comfy.model_management\nimport comfy.samplers\nimport comfy.utils\nimport numpy as np\nimport logging\n\ndef prepare_noise(latent_image, seed, noise_inds=None):\n    \"\"\"\n    creates random noise given a latent image and a seed.\n    optional arg skip can be used to skip and discard x number of noise generations for a given seed\n    \"\"\"\n    generator = torch.manual_seed(seed)\n    if noise_inds is None:\n        return torch.randn(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, generator=generator, device=\"cpu\")\n    \n    unique_inds, inverse = np.unique(noise_inds, return_inverse=True)\n    noises = []\n    for i in range(unique_inds[-1]+1):\n        noise = torch.randn([1] + list(latent_image.size())[1:], dtype=latent_image.dtype, layout=latent_image.layout, generator=generator, device=\"cpu\")\n        if i in unique_inds:\n            noises.append(noise)\n    noises = [noises[i] for i in inverse]\n    noises = torch.cat(noises, axis=0)\n    return noises\n\ndef fix_empty_latent_channels(model, latent_image):\n    latent_channels = model.get_model_object(\"latent_format\").latent_channels #Resize the empty latent image so it has the right number of channels\n    if latent_channels != latent_image.shape[1] and torch.count_nonzero(latent_image) == 0:\n        latent_image = comfy.utils.repeat_to_batch_size(latent_image, latent_channels, dim=1)\n    return latent_image\n\ndef prepare_sampling(model, noise_shape, positive, negative, noise_mask):\n    logging.warning(\"Warning: comfy.sample.prepare_sampling isn't used anymore and can be removed\")\n    return model, positive, negative, noise_mask, []\n\ndef cleanup_additional_models(models):\n    logging.warning(\"Warning: comfy.sample.cleanup_additional_models isn't used anymore and can be removed\")\n\ndef sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False, noise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n    sampler = comfy.samplers.KSampler(model, steps=steps, device=model.load_device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)\n\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.to(comfy.model_management.intermediate_device())\n    return samples\n\ndef sample_custom(model, noise, cfg, sampler, sigmas, positive, negative, latent_image, noise_mask=None, callback=None, disable_pbar=False, seed=None):\n    samples = comfy.samplers.sample(model, noise, positive, negative, cfg, model.load_device, sampler, sigmas, model_options=model.model_options, latent_image=latent_image, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.to(comfy.model_management.intermediate_device())\n    return samples\n", "comfy/utils.py": "import torch\nimport math\nimport struct\nimport comfy.checkpoint_pickle\nimport safetensors.torch\nimport numpy as np\nfrom PIL import Image\nimport logging\nimport itertools\n\ndef load_torch_file(ckpt, safe_load=False, device=None):\n    if device is None:\n        device = torch.device(\"cpu\")\n    if ckpt.lower().endswith(\".safetensors\"):\n        sd = safetensors.torch.load_file(ckpt, device=device.type)\n    else:\n        if safe_load:\n            if not 'weights_only' in torch.load.__code__.co_varnames:\n                logging.warning(\"Warning torch.load doesn't support weights_only on this pytorch version, loading unsafely.\")\n                safe_load = False\n        if safe_load:\n            pl_sd = torch.load(ckpt, map_location=device, weights_only=True)\n        else:\n            pl_sd = torch.load(ckpt, map_location=device, pickle_module=comfy.checkpoint_pickle)\n        if \"global_step\" in pl_sd:\n            logging.debug(f\"Global Step: {pl_sd['global_step']}\")\n        if \"state_dict\" in pl_sd:\n            sd = pl_sd[\"state_dict\"]\n        else:\n            sd = pl_sd\n    return sd\n\ndef save_torch_file(sd, ckpt, metadata=None):\n    if metadata is not None:\n        safetensors.torch.save_file(sd, ckpt, metadata=metadata)\n    else:\n        safetensors.torch.save_file(sd, ckpt)\n\ndef calculate_parameters(sd, prefix=\"\"):\n    params = 0\n    for k in sd.keys():\n        if k.startswith(prefix):\n            params += sd[k].nelement()\n    return params\n\ndef state_dict_key_replace(state_dict, keys_to_replace):\n    for x in keys_to_replace:\n        if x in state_dict:\n            state_dict[keys_to_replace[x]] = state_dict.pop(x)\n    return state_dict\n\ndef state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=False):\n    if filter_keys:\n        out = {}\n    else:\n        out = state_dict\n    for rp in replace_prefix:\n        replace = list(map(lambda a: (a, \"{}{}\".format(replace_prefix[rp], a[len(rp):])), filter(lambda a: a.startswith(rp), state_dict.keys())))\n        for x in replace:\n            w = state_dict.pop(x[0])\n            out[x[1]] = w\n    return out\n\n\ndef transformers_convert(sd, prefix_from, prefix_to, number):\n    keys_to_replace = {\n        \"{}positional_embedding\": \"{}embeddings.position_embedding.weight\",\n        \"{}token_embedding.weight\": \"{}embeddings.token_embedding.weight\",\n        \"{}ln_final.weight\": \"{}final_layer_norm.weight\",\n        \"{}ln_final.bias\": \"{}final_layer_norm.bias\",\n    }\n\n    for k in keys_to_replace:\n        x = k.format(prefix_from)\n        if x in sd:\n            sd[keys_to_replace[k].format(prefix_to)] = sd.pop(x)\n\n    resblock_to_replace = {\n        \"ln_1\": \"layer_norm1\",\n        \"ln_2\": \"layer_norm2\",\n        \"mlp.c_fc\": \"mlp.fc1\",\n        \"mlp.c_proj\": \"mlp.fc2\",\n        \"attn.out_proj\": \"self_attn.out_proj\",\n    }\n\n    for resblock in range(number):\n        for x in resblock_to_replace:\n            for y in [\"weight\", \"bias\"]:\n                k = \"{}transformer.resblocks.{}.{}.{}\".format(prefix_from, resblock, x, y)\n                k_to = \"{}encoder.layers.{}.{}.{}\".format(prefix_to, resblock, resblock_to_replace[x], y)\n                if k in sd:\n                    sd[k_to] = sd.pop(k)\n\n        for y in [\"weight\", \"bias\"]:\n            k_from = \"{}transformer.resblocks.{}.attn.in_proj_{}\".format(prefix_from, resblock, y)\n            if k_from in sd:\n                weights = sd.pop(k_from)\n                shape_from = weights.shape[0] // 3\n                for x in range(3):\n                    p = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"]\n                    k_to = \"{}encoder.layers.{}.{}.{}\".format(prefix_to, resblock, p[x], y)\n                    sd[k_to] = weights[shape_from*x:shape_from*(x + 1)]\n\n    return sd\n\ndef clip_text_transformers_convert(sd, prefix_from, prefix_to):\n    sd = transformers_convert(sd, prefix_from, \"{}text_model.\".format(prefix_to), 32)\n\n    tp = \"{}text_projection.weight\".format(prefix_from)\n    if tp in sd:\n        sd[\"{}text_projection.weight\".format(prefix_to)] = sd.pop(tp)\n\n    tp = \"{}text_projection\".format(prefix_from)\n    if tp in sd:\n        sd[\"{}text_projection.weight\".format(prefix_to)] = sd.pop(tp).transpose(0, 1).contiguous()\n    return sd\n\n\nUNET_MAP_ATTENTIONS = {\n    \"proj_in.weight\",\n    \"proj_in.bias\",\n    \"proj_out.weight\",\n    \"proj_out.bias\",\n    \"norm.weight\",\n    \"norm.bias\",\n}\n\nTRANSFORMER_BLOCKS = {\n    \"norm1.weight\",\n    \"norm1.bias\",\n    \"norm2.weight\",\n    \"norm2.bias\",\n    \"norm3.weight\",\n    \"norm3.bias\",\n    \"attn1.to_q.weight\",\n    \"attn1.to_k.weight\",\n    \"attn1.to_v.weight\",\n    \"attn1.to_out.0.weight\",\n    \"attn1.to_out.0.bias\",\n    \"attn2.to_q.weight\",\n    \"attn2.to_k.weight\",\n    \"attn2.to_v.weight\",\n    \"attn2.to_out.0.weight\",\n    \"attn2.to_out.0.bias\",\n    \"ff.net.0.proj.weight\",\n    \"ff.net.0.proj.bias\",\n    \"ff.net.2.weight\",\n    \"ff.net.2.bias\",\n}\n\nUNET_MAP_RESNET = {\n    \"in_layers.2.weight\": \"conv1.weight\",\n    \"in_layers.2.bias\": \"conv1.bias\",\n    \"emb_layers.1.weight\": \"time_emb_proj.weight\",\n    \"emb_layers.1.bias\": \"time_emb_proj.bias\",\n    \"out_layers.3.weight\": \"conv2.weight\",\n    \"out_layers.3.bias\": \"conv2.bias\",\n    \"skip_connection.weight\": \"conv_shortcut.weight\",\n    \"skip_connection.bias\": \"conv_shortcut.bias\",\n    \"in_layers.0.weight\": \"norm1.weight\",\n    \"in_layers.0.bias\": \"norm1.bias\",\n    \"out_layers.0.weight\": \"norm2.weight\",\n    \"out_layers.0.bias\": \"norm2.bias\",\n}\n\nUNET_MAP_BASIC = {\n    (\"label_emb.0.0.weight\", \"class_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"class_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"class_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"class_embedding.linear_2.bias\"),\n    (\"label_emb.0.0.weight\", \"add_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"add_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"add_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"add_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),\n    (\"out.2.weight\", \"conv_out.weight\"),\n    (\"out.2.bias\", \"conv_out.bias\"),\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\")\n}\n\ndef unet_to_diffusers(unet_config):\n    if \"num_res_blocks\" not in unet_config:\n        return {}\n    num_res_blocks = unet_config[\"num_res_blocks\"]\n    channel_mult = unet_config[\"channel_mult\"]\n    transformer_depth = unet_config[\"transformer_depth\"][:]\n    transformer_depth_output = unet_config[\"transformer_depth_output\"][:]\n    num_blocks = len(channel_mult)\n\n    transformers_mid = unet_config.get(\"transformer_depth_middle\", None)\n\n    diffusers_unet_map = {}\n    for x in range(num_blocks):\n        n = 1 + (num_res_blocks[x] + 1) * x\n        for i in range(num_res_blocks[x]):\n            for b in UNET_MAP_RESNET:\n                diffusers_unet_map[\"down_blocks.{}.resnets.{}.{}\".format(x, i, UNET_MAP_RESNET[b])] = \"input_blocks.{}.0.{}\".format(n, b)\n            num_transformers = transformer_depth.pop(0)\n            if num_transformers > 0:\n                for b in UNET_MAP_ATTENTIONS:\n                    diffusers_unet_map[\"down_blocks.{}.attentions.{}.{}\".format(x, i, b)] = \"input_blocks.{}.1.{}\".format(n, b)\n                for t in range(num_transformers):\n                    for b in TRANSFORMER_BLOCKS:\n                        diffusers_unet_map[\"down_blocks.{}.attentions.{}.transformer_blocks.{}.{}\".format(x, i, t, b)] = \"input_blocks.{}.1.transformer_blocks.{}.{}\".format(n, t, b)\n            n += 1\n        for k in [\"weight\", \"bias\"]:\n            diffusers_unet_map[\"down_blocks.{}.downsamplers.0.conv.{}\".format(x, k)] = \"input_blocks.{}.0.op.{}\".format(n, k)\n\n    i = 0\n    for b in UNET_MAP_ATTENTIONS:\n        diffusers_unet_map[\"mid_block.attentions.{}.{}\".format(i, b)] = \"middle_block.1.{}\".format(b)\n    for t in range(transformers_mid):\n        for b in TRANSFORMER_BLOCKS:\n            diffusers_unet_map[\"mid_block.attentions.{}.transformer_blocks.{}.{}\".format(i, t, b)] = \"middle_block.1.transformer_blocks.{}.{}\".format(t, b)\n\n    for i, n in enumerate([0, 2]):\n        for b in UNET_MAP_RESNET:\n            diffusers_unet_map[\"mid_block.resnets.{}.{}\".format(i, UNET_MAP_RESNET[b])] = \"middle_block.{}.{}\".format(n, b)\n\n    num_res_blocks = list(reversed(num_res_blocks))\n    for x in range(num_blocks):\n        n = (num_res_blocks[x] + 1) * x\n        l = num_res_blocks[x] + 1\n        for i in range(l):\n            c = 0\n            for b in UNET_MAP_RESNET:\n                diffusers_unet_map[\"up_blocks.{}.resnets.{}.{}\".format(x, i, UNET_MAP_RESNET[b])] = \"output_blocks.{}.0.{}\".format(n, b)\n            c += 1\n            num_transformers = transformer_depth_output.pop()\n            if num_transformers > 0:\n                c += 1\n                for b in UNET_MAP_ATTENTIONS:\n                    diffusers_unet_map[\"up_blocks.{}.attentions.{}.{}\".format(x, i, b)] = \"output_blocks.{}.1.{}\".format(n, b)\n                for t in range(num_transformers):\n                    for b in TRANSFORMER_BLOCKS:\n                        diffusers_unet_map[\"up_blocks.{}.attentions.{}.transformer_blocks.{}.{}\".format(x, i, t, b)] = \"output_blocks.{}.1.transformer_blocks.{}.{}\".format(n, t, b)\n            if i == l - 1:\n                for k in [\"weight\", \"bias\"]:\n                    diffusers_unet_map[\"up_blocks.{}.upsamplers.0.conv.{}\".format(x, k)] = \"output_blocks.{}.{}.conv.{}\".format(n, c, k)\n            n += 1\n\n    for k in UNET_MAP_BASIC:\n        diffusers_unet_map[k[1]] = k[0]\n\n    return diffusers_unet_map\n\ndef swap_scale_shift(weight):\n    shift, scale = weight.chunk(2, dim=0)\n    new_weight = torch.cat([scale, shift], dim=0)\n    return new_weight\n\nMMDIT_MAP_BASIC = {\n    (\"context_embedder.bias\", \"context_embedder.bias\"),\n    (\"context_embedder.weight\", \"context_embedder.weight\"),\n    (\"t_embedder.mlp.0.bias\", \"time_text_embed.timestep_embedder.linear_1.bias\"),\n    (\"t_embedder.mlp.0.weight\", \"time_text_embed.timestep_embedder.linear_1.weight\"),\n    (\"t_embedder.mlp.2.bias\", \"time_text_embed.timestep_embedder.linear_2.bias\"),\n    (\"t_embedder.mlp.2.weight\", \"time_text_embed.timestep_embedder.linear_2.weight\"),\n    (\"x_embedder.proj.bias\", \"pos_embed.proj.bias\"),\n    (\"x_embedder.proj.weight\", \"pos_embed.proj.weight\"),\n    (\"y_embedder.mlp.0.bias\", \"time_text_embed.text_embedder.linear_1.bias\"),\n    (\"y_embedder.mlp.0.weight\", \"time_text_embed.text_embedder.linear_1.weight\"),\n    (\"y_embedder.mlp.2.bias\", \"time_text_embed.text_embedder.linear_2.bias\"),\n    (\"y_embedder.mlp.2.weight\", \"time_text_embed.text_embedder.linear_2.weight\"),\n    (\"pos_embed\", \"pos_embed.pos_embed\"),\n    (\"final_layer.adaLN_modulation.1.bias\", \"norm_out.linear.bias\", swap_scale_shift),\n    (\"final_layer.adaLN_modulation.1.weight\", \"norm_out.linear.weight\", swap_scale_shift),\n    (\"final_layer.linear.bias\", \"proj_out.bias\"),\n    (\"final_layer.linear.weight\", \"proj_out.weight\"),\n}\n\nMMDIT_MAP_BLOCK = {\n    (\"context_block.adaLN_modulation.1.bias\", \"norm1_context.linear.bias\"),\n    (\"context_block.adaLN_modulation.1.weight\", \"norm1_context.linear.weight\"),\n    (\"context_block.attn.proj.bias\", \"attn.to_add_out.bias\"),\n    (\"context_block.attn.proj.weight\", \"attn.to_add_out.weight\"),\n    (\"context_block.mlp.fc1.bias\", \"ff_context.net.0.proj.bias\"),\n    (\"context_block.mlp.fc1.weight\", \"ff_context.net.0.proj.weight\"),\n    (\"context_block.mlp.fc2.bias\", \"ff_context.net.2.bias\"),\n    (\"context_block.mlp.fc2.weight\", \"ff_context.net.2.weight\"),\n    (\"x_block.adaLN_modulation.1.bias\", \"norm1.linear.bias\"),\n    (\"x_block.adaLN_modulation.1.weight\", \"norm1.linear.weight\"),\n    (\"x_block.attn.proj.bias\", \"attn.to_out.0.bias\"),\n    (\"x_block.attn.proj.weight\", \"attn.to_out.0.weight\"),\n    (\"x_block.mlp.fc1.bias\", \"ff.net.0.proj.bias\"),\n    (\"x_block.mlp.fc1.weight\", \"ff.net.0.proj.weight\"),\n    (\"x_block.mlp.fc2.bias\", \"ff.net.2.bias\"),\n    (\"x_block.mlp.fc2.weight\", \"ff.net.2.weight\"),\n}\n\ndef mmdit_to_diffusers(mmdit_config, output_prefix=\"\"):\n    key_map = {}\n\n    depth = mmdit_config.get(\"depth\", 0)\n    for i in range(depth):\n        block_from = \"transformer_blocks.{}\".format(i)\n        block_to = \"{}joint_blocks.{}\".format(output_prefix, i)\n\n        offset = depth * 64\n\n        for end in (\"weight\", \"bias\"):\n            k = \"{}.attn.\".format(block_from)\n            qkv = \"{}.x_block.attn.qkv.{}\".format(block_to, end)\n            key_map[\"{}to_q.{}\".format(k, end)] = (qkv, (0, 0, offset))\n            key_map[\"{}to_k.{}\".format(k, end)] = (qkv, (0, offset, offset))\n            key_map[\"{}to_v.{}\".format(k, end)] = (qkv, (0, offset * 2, offset))\n\n            qkv = \"{}.context_block.attn.qkv.{}\".format(block_to, end)\n            key_map[\"{}add_q_proj.{}\".format(k, end)] = (qkv, (0, 0, offset))\n            key_map[\"{}add_k_proj.{}\".format(k, end)] = (qkv, (0, offset, offset))\n            key_map[\"{}add_v_proj.{}\".format(k, end)] = (qkv, (0, offset * 2, offset))\n\n        for k in MMDIT_MAP_BLOCK:\n            key_map[\"{}.{}\".format(block_from, k[1])] = \"{}.{}\".format(block_to, k[0])\n\n    map_basic = MMDIT_MAP_BASIC.copy()\n    map_basic.add((\"joint_blocks.{}.context_block.adaLN_modulation.1.bias\".format(depth - 1), \"transformer_blocks.{}.norm1_context.linear.bias\".format(depth - 1), swap_scale_shift))\n    map_basic.add((\"joint_blocks.{}.context_block.adaLN_modulation.1.weight\".format(depth - 1), \"transformer_blocks.{}.norm1_context.linear.weight\".format(depth - 1), swap_scale_shift))\n\n    for k in map_basic:\n        if len(k) > 2:\n            key_map[k[1]] = (\"{}{}\".format(output_prefix, k[0]), None, k[2])\n        else:\n            key_map[k[1]] = \"{}{}\".format(output_prefix, k[0])\n\n    return key_map\n\ndef repeat_to_batch_size(tensor, batch_size, dim=0):\n    if tensor.shape[dim] > batch_size:\n        return tensor.narrow(dim, 0, batch_size)\n    elif tensor.shape[dim] < batch_size:\n        return tensor.repeat(dim * [1] + [math.ceil(batch_size / tensor.shape[dim])] + [1] * (len(tensor.shape) - 1 - dim)).narrow(dim, 0, batch_size)\n    return tensor\n\ndef resize_to_batch_size(tensor, batch_size):\n    in_batch_size = tensor.shape[0]\n    if in_batch_size == batch_size:\n        return tensor\n\n    if batch_size <= 1:\n        return tensor[:batch_size]\n\n    output = torch.empty([batch_size] + list(tensor.shape)[1:], dtype=tensor.dtype, device=tensor.device)\n    if batch_size < in_batch_size:\n        scale = (in_batch_size - 1) / (batch_size - 1)\n        for i in range(batch_size):\n            output[i] = tensor[min(round(i * scale), in_batch_size - 1)]\n    else:\n        scale = in_batch_size / batch_size\n        for i in range(batch_size):\n            output[i] = tensor[min(math.floor((i + 0.5) * scale), in_batch_size - 1)]\n\n    return output\n\ndef convert_sd_to(state_dict, dtype):\n    keys = list(state_dict.keys())\n    for k in keys:\n        state_dict[k] = state_dict[k].to(dtype)\n    return state_dict\n\ndef safetensors_header(safetensors_path, max_size=100*1024*1024):\n    with open(safetensors_path, \"rb\") as f:\n        header = f.read(8)\n        length_of_header = struct.unpack('<Q', header)[0]\n        if length_of_header > max_size:\n            return None\n        return f.read(length_of_header)\n\ndef set_attr(obj, attr, value):\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    setattr(obj, attrs[-1], value)\n    return prev\n\ndef set_attr_param(obj, attr, value):\n    return set_attr(obj, attr, torch.nn.Parameter(value, requires_grad=False))\n\ndef copy_to_param(obj, attr, value):\n    # inplace update tensor instead of replacing it\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    prev.data.copy_(value)\n\ndef get_attr(obj, attr):\n    attrs = attr.split(\".\")\n    for name in attrs:\n        obj = getattr(obj, name)\n    return obj\n\ndef bislerp(samples, width, height):\n    def slerp(b1, b2, r):\n        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n        \n        c = b1.shape[-1]\n\n        #norms\n        b1_norms = torch.norm(b1, dim=-1, keepdim=True)\n        b2_norms = torch.norm(b2, dim=-1, keepdim=True)\n\n        #normalize\n        b1_normalized = b1 / b1_norms\n        b2_normalized = b2 / b2_norms\n\n        #zero when norms are zero\n        b1_normalized[b1_norms.expand(-1,c) == 0.0] = 0.0\n        b2_normalized[b2_norms.expand(-1,c) == 0.0] = 0.0\n\n        #slerp\n        dot = (b1_normalized*b2_normalized).sum(1)\n        omega = torch.acos(dot)\n        so = torch.sin(omega)\n\n        #technically not mathematically correct, but more pleasing?\n        res = (torch.sin((1.0-r.squeeze(1))*omega)/so).unsqueeze(1)*b1_normalized + (torch.sin(r.squeeze(1)*omega)/so).unsqueeze(1) * b2_normalized\n        res *= (b1_norms * (1.0-r) + b2_norms * r).expand(-1,c)\n\n        #edge cases for same or polar opposites\n        res[dot > 1 - 1e-5] = b1[dot > 1 - 1e-5] \n        res[dot < 1e-5 - 1] = (b1 * (1.0-r) + b2 * r)[dot < 1e-5 - 1]\n        return res\n    \n    def generate_bilinear_data(length_old, length_new, device):\n        coords_1 = torch.arange(length_old, dtype=torch.float32, device=device).reshape((1,1,1,-1))\n        coords_1 = torch.nn.functional.interpolate(coords_1, size=(1, length_new), mode=\"bilinear\")\n        ratios = coords_1 - coords_1.floor()\n        coords_1 = coords_1.to(torch.int64)\n        \n        coords_2 = torch.arange(length_old, dtype=torch.float32, device=device).reshape((1,1,1,-1)) + 1\n        coords_2[:,:,:,-1] -= 1\n        coords_2 = torch.nn.functional.interpolate(coords_2, size=(1, length_new), mode=\"bilinear\")\n        coords_2 = coords_2.to(torch.int64)\n        return ratios, coords_1, coords_2\n\n    orig_dtype = samples.dtype\n    samples = samples.float()\n    n,c,h,w = samples.shape\n    h_new, w_new = (height, width)\n    \n    #linear w\n    ratios, coords_1, coords_2 = generate_bilinear_data(w, w_new, samples.device)\n    coords_1 = coords_1.expand((n, c, h, -1))\n    coords_2 = coords_2.expand((n, c, h, -1))\n    ratios = ratios.expand((n, 1, h, -1))\n\n    pass_1 = samples.gather(-1,coords_1).movedim(1, -1).reshape((-1,c))\n    pass_2 = samples.gather(-1,coords_2).movedim(1, -1).reshape((-1,c))\n    ratios = ratios.movedim(1, -1).reshape((-1,1))\n\n    result = slerp(pass_1, pass_2, ratios)\n    result = result.reshape(n, h, w_new, c).movedim(-1, 1)\n\n    #linear h\n    ratios, coords_1, coords_2 = generate_bilinear_data(h, h_new, samples.device)\n    coords_1 = coords_1.reshape((1,1,-1,1)).expand((n, c, -1, w_new))\n    coords_2 = coords_2.reshape((1,1,-1,1)).expand((n, c, -1, w_new))\n    ratios = ratios.reshape((1,1,-1,1)).expand((n, 1, -1, w_new))\n\n    pass_1 = result.gather(-2,coords_1).movedim(1, -1).reshape((-1,c))\n    pass_2 = result.gather(-2,coords_2).movedim(1, -1).reshape((-1,c))\n    ratios = ratios.movedim(1, -1).reshape((-1,1))\n\n    result = slerp(pass_1, pass_2, ratios)\n    result = result.reshape(n, h_new, w_new, c).movedim(-1, 1)\n    return result.to(orig_dtype)\n\ndef lanczos(samples, width, height):\n    images = [Image.fromarray(np.clip(255. * image.movedim(0, -1).cpu().numpy(), 0, 255).astype(np.uint8)) for image in samples]\n    images = [image.resize((width, height), resample=Image.Resampling.LANCZOS) for image in images]\n    images = [torch.from_numpy(np.array(image).astype(np.float32) / 255.0).movedim(-1, 0) for image in images]\n    result = torch.stack(images)\n    return result.to(samples.device, samples.dtype)\n\ndef common_upscale(samples, width, height, upscale_method, crop):\n        if crop == \"center\":\n            old_width = samples.shape[3]\n            old_height = samples.shape[2]\n            old_aspect = old_width / old_height\n            new_aspect = width / height\n            x = 0\n            y = 0\n            if old_aspect > new_aspect:\n                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)\n            elif old_aspect < new_aspect:\n                y = round((old_height - old_height * (old_aspect / new_aspect)) / 2)\n            s = samples[:,:,y:old_height-y,x:old_width-x]\n        else:\n            s = samples\n\n        if upscale_method == \"bislerp\":\n            return bislerp(s, width, height)\n        elif upscale_method == \"lanczos\":\n            return lanczos(s, width, height)\n        else:\n            return torch.nn.functional.interpolate(s, size=(height, width), mode=upscale_method)\n\ndef get_tiled_scale_steps(width, height, tile_x, tile_y, overlap):\n    return math.ceil((height / (tile_y - overlap))) * math.ceil((width / (tile_x - overlap)))\n\n@torch.inference_mode()\ndef tiled_scale_multidim(samples, function, tile=(64, 64), overlap = 8, upscale_amount = 4, out_channels = 3, output_device=\"cpu\", pbar = None):\n    dims = len(tile)\n    output = torch.empty([samples.shape[0], out_channels] + list(map(lambda a: round(a * upscale_amount), samples.shape[2:])), device=output_device)\n\n    for b in range(samples.shape[0]):\n        s = samples[b:b+1]\n        out = torch.zeros([s.shape[0], out_channels] + list(map(lambda a: round(a * upscale_amount), s.shape[2:])), device=output_device)\n        out_div = torch.zeros([s.shape[0], out_channels] + list(map(lambda a: round(a * upscale_amount), s.shape[2:])), device=output_device)\n\n        for it in itertools.product(*map(lambda a: range(0, a[0], a[1] - overlap), zip(s.shape[2:], tile))):\n            s_in = s\n            upscaled = []\n\n            for d in range(dims):\n                pos = max(0, min(s.shape[d + 2] - overlap, it[d]))\n                l = min(tile[d], s.shape[d + 2] - pos)\n                s_in = s_in.narrow(d + 2, pos, l)\n                upscaled.append(round(pos * upscale_amount))\n            ps = function(s_in).to(output_device)\n            mask = torch.ones_like(ps)\n            feather = round(overlap * upscale_amount)\n            for t in range(feather):\n                for d in range(2, dims + 2):\n                    m = mask.narrow(d, t, 1)\n                    m *= ((1.0/feather) * (t + 1))\n                    m = mask.narrow(d, mask.shape[d] -1 -t, 1)\n                    m *= ((1.0/feather) * (t + 1))\n\n            o = out\n            o_d = out_div\n            for d in range(dims):\n                o = o.narrow(d + 2, upscaled[d], mask.shape[d + 2])\n                o_d = o_d.narrow(d + 2, upscaled[d], mask.shape[d + 2])\n\n            o += ps * mask\n            o_d += mask\n\n            if pbar is not None:\n                pbar.update(1)\n\n        output[b:b+1] = out/out_div\n    return output\n\ndef tiled_scale(samples, function, tile_x=64, tile_y=64, overlap = 8, upscale_amount = 4, out_channels = 3, output_device=\"cpu\", pbar = None):\n    return tiled_scale_multidim(samples, function, (tile_y, tile_x), overlap, upscale_amount, out_channels, output_device, pbar)\n\nPROGRESS_BAR_ENABLED = True\ndef set_progress_bar_enabled(enabled):\n    global PROGRESS_BAR_ENABLED\n    PROGRESS_BAR_ENABLED = enabled\n\nPROGRESS_BAR_HOOK = None\ndef set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\n\nclass ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK\n        self.total = total\n        self.current = 0\n        self.hook = PROGRESS_BAR_HOOK\n\n    def update_absolute(self, value, total=None, preview=None):\n        if total is not None:\n            self.total = total\n        if value > self.total:\n            value = self.total\n        self.current = value\n        if self.hook is not None:\n            self.hook(self.current, self.total, preview)\n\n    def update(self, value):\n        self.update_absolute(self.current + value)\n", "comfy/clip_model.py": "import torch\nfrom comfy.ldm.modules.attention import optimized_attention_for_device\n\nclass CLIPAttention(torch.nn.Module):\n    def __init__(self, embed_dim, heads, dtype, device, operations):\n        super().__init__()\n\n        self.heads = heads\n        self.q_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n        self.k_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n        self.v_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n\n        self.out_proj = operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device)\n\n    def forward(self, x, mask=None, optimized_attention=None):\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        out = optimized_attention(q, k, v, self.heads, mask)\n        return self.out_proj(out)\n\nACTIVATIONS = {\"quick_gelu\": lambda a: a * torch.sigmoid(1.702 * a),\n               \"gelu\": torch.nn.functional.gelu,\n}\n\nclass CLIPMLP(torch.nn.Module):\n    def __init__(self, embed_dim, intermediate_size, activation, dtype, device, operations):\n        super().__init__()\n        self.fc1 = operations.Linear(embed_dim, intermediate_size, bias=True, dtype=dtype, device=device)\n        self.activation = ACTIVATIONS[activation]\n        self.fc2 = operations.Linear(intermediate_size, embed_dim, bias=True, dtype=dtype, device=device)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        return x\n\nclass CLIPLayer(torch.nn.Module):\n    def __init__(self, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations):\n        super().__init__()\n        self.layer_norm1 = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n        self.self_attn = CLIPAttention(embed_dim, heads, dtype, device, operations)\n        self.layer_norm2 = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n        self.mlp = CLIPMLP(embed_dim, intermediate_size, intermediate_activation, dtype, device, operations)\n\n    def forward(self, x, mask=None, optimized_attention=None):\n        x += self.self_attn(self.layer_norm1(x), mask, optimized_attention)\n        x += self.mlp(self.layer_norm2(x))\n        return x\n\n\nclass CLIPEncoder(torch.nn.Module):\n    def __init__(self, num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations):\n        super().__init__()\n        self.layers = torch.nn.ModuleList([CLIPLayer(embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations) for i in range(num_layers)])\n\n    def forward(self, x, mask=None, intermediate_output=None):\n        optimized_attention = optimized_attention_for_device(x.device, mask=mask is not None, small_input=True)\n\n        if intermediate_output is not None:\n            if intermediate_output < 0:\n                intermediate_output = len(self.layers) + intermediate_output\n\n        intermediate = None\n        for i, l in enumerate(self.layers):\n            x = l(x, mask, optimized_attention)\n            if i == intermediate_output:\n                intermediate = x.clone()\n        return x, intermediate\n\nclass CLIPEmbeddings(torch.nn.Module):\n    def __init__(self, embed_dim, vocab_size=49408, num_positions=77, dtype=None, device=None):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim, dtype=dtype, device=device)\n        self.position_embedding = torch.nn.Embedding(num_positions, embed_dim, dtype=dtype, device=device)\n\n    def forward(self, input_tokens):\n        return self.token_embedding(input_tokens) + self.position_embedding.weight\n\n\nclass CLIPTextModel_(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        num_layers = config_dict[\"num_hidden_layers\"]\n        embed_dim = config_dict[\"hidden_size\"]\n        heads = config_dict[\"num_attention_heads\"]\n        intermediate_size = config_dict[\"intermediate_size\"]\n        intermediate_activation = config_dict[\"hidden_act\"]\n\n        super().__init__()\n        self.embeddings = CLIPEmbeddings(embed_dim, dtype=torch.float32, device=device)\n        self.encoder = CLIPEncoder(num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations)\n        self.final_layer_norm = operations.LayerNorm(embed_dim, dtype=dtype, device=device)\n\n    def forward(self, input_tokens, attention_mask=None, intermediate_output=None, final_layer_norm_intermediate=True):\n        x = self.embeddings(input_tokens)\n        mask = None\n        if attention_mask is not None:\n            mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n            mask = mask.masked_fill(mask.to(torch.bool), float(\"-inf\"))\n\n        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(float(\"-inf\")).triu_(1)\n        if mask is not None:\n            mask += causal_mask\n        else:\n            mask = causal_mask\n\n        x, i = self.encoder(x, mask=mask, intermediate_output=intermediate_output)\n        x = self.final_layer_norm(x)\n        if i is not None and final_layer_norm_intermediate:\n            i = self.final_layer_norm(i)\n\n        pooled_output = x[torch.arange(x.shape[0], device=x.device), input_tokens.to(dtype=torch.int, device=x.device).argmax(dim=-1),]\n        return x, i, pooled_output\n\nclass CLIPTextModel(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        self.num_layers = config_dict[\"num_hidden_layers\"]\n        self.text_model = CLIPTextModel_(config_dict, dtype, device, operations)\n        embed_dim = config_dict[\"hidden_size\"]\n        self.text_projection = operations.Linear(embed_dim, embed_dim, bias=False, dtype=dtype, device=device)\n        self.text_projection.weight.copy_(torch.eye(embed_dim))\n        self.dtype = dtype\n\n    def get_input_embeddings(self):\n        return self.text_model.embeddings.token_embedding\n\n    def set_input_embeddings(self, embeddings):\n        self.text_model.embeddings.token_embedding = embeddings\n\n    def forward(self, *args, **kwargs):\n        x = self.text_model(*args, **kwargs)\n        out = self.text_projection(x[2])\n        return (x[0], x[1], out, x[2])\n\n\nclass CLIPVisionEmbeddings(torch.nn.Module):\n    def __init__(self, embed_dim, num_channels=3, patch_size=14, image_size=224, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.class_embedding = torch.nn.Parameter(torch.empty(embed_dim, dtype=dtype, device=device))\n\n        self.patch_embedding = operations.Conv2d(\n            in_channels=num_channels,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=False,\n            dtype=dtype,\n            device=device\n        )\n\n        num_patches = (image_size // patch_size) ** 2\n        num_positions = num_patches + 1\n        self.position_embedding = torch.nn.Embedding(num_positions, embed_dim, dtype=dtype, device=device)\n\n    def forward(self, pixel_values):\n        embeds = self.patch_embedding(pixel_values).flatten(2).transpose(1, 2)\n        return torch.cat([self.class_embedding.to(embeds.device).expand(pixel_values.shape[0], 1, -1), embeds], dim=1) + self.position_embedding.weight.to(embeds.device)\n\n\nclass CLIPVision(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        num_layers = config_dict[\"num_hidden_layers\"]\n        embed_dim = config_dict[\"hidden_size\"]\n        heads = config_dict[\"num_attention_heads\"]\n        intermediate_size = config_dict[\"intermediate_size\"]\n        intermediate_activation = config_dict[\"hidden_act\"]\n\n        self.embeddings = CLIPVisionEmbeddings(embed_dim, config_dict[\"num_channels\"], config_dict[\"patch_size\"], config_dict[\"image_size\"], dtype=torch.float32, device=device, operations=operations)\n        self.pre_layrnorm = operations.LayerNorm(embed_dim)\n        self.encoder = CLIPEncoder(num_layers, embed_dim, heads, intermediate_size, intermediate_activation, dtype, device, operations)\n        self.post_layernorm = operations.LayerNorm(embed_dim)\n\n    def forward(self, pixel_values, attention_mask=None, intermediate_output=None):\n        x = self.embeddings(pixel_values)\n        x = self.pre_layrnorm(x)\n        #TODO: attention_mask?\n        x, i = self.encoder(x, mask=None, intermediate_output=intermediate_output)\n        pooled_output = self.post_layernorm(x[:, 0, :])\n        return x, i, pooled_output\n\nclass CLIPVisionModelProjection(torch.nn.Module):\n    def __init__(self, config_dict, dtype, device, operations):\n        super().__init__()\n        self.vision_model = CLIPVision(config_dict, dtype, device, operations)\n        self.visual_projection = operations.Linear(config_dict[\"hidden_size\"], config_dict[\"projection_dim\"], bias=False)\n\n    def forward(self, *args, **kwargs):\n        x = self.vision_model(*args, **kwargs)\n        out = self.visual_projection(x[2])\n        return (x[0], x[1], out)\n", "comfy/ops.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nimport comfy.model_management\n\ndef cast_bias_weight(s, input):\n    bias = None\n    non_blocking = comfy.model_management.device_should_use_non_blocking(input.device)\n    if s.bias is not None:\n        bias = s.bias.to(device=input.device, dtype=input.dtype, non_blocking=non_blocking)\n        if s.bias_function is not None:\n            bias = s.bias_function(bias)\n    weight = s.weight.to(device=input.device, dtype=input.dtype, non_blocking=non_blocking)\n    if s.weight_function is not None:\n        weight = s.weight_function(weight)\n    return weight, bias\n\nclass CastWeightBiasOp:\n    comfy_cast_weights = False\n    weight_function = None\n    bias_function = None\n\nclass disable_weight_init:\n    class Linear(torch.nn.Linear, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.linear(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class Conv1d(torch.nn.Conv1d, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return self._conv_forward(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class Conv2d(torch.nn.Conv2d, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return self._conv_forward(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class Conv3d(torch.nn.Conv3d, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return self._conv_forward(input, weight, bias)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class GroupNorm(torch.nn.GroupNorm, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.group_norm(input, self.num_groups, weight, bias, self.eps)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n\n    class LayerNorm(torch.nn.LayerNorm, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input):\n            if self.weight is not None:\n                weight, bias = cast_bias_weight(self, input)\n            else:\n                weight = None\n                bias = None\n            return torch.nn.functional.layer_norm(input, self.normalized_shape, weight, bias, self.eps)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class ConvTranspose2d(torch.nn.ConvTranspose2d, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input, output_size=None):\n            num_spatial_dims = 2\n            output_padding = self._output_padding(\n                input, output_size, self.stride, self.padding, self.kernel_size,\n                num_spatial_dims, self.dilation)\n\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.conv_transpose2d(\n                input, weight, bias, self.stride, self.padding,\n                output_padding, self.groups, self.dilation)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    class ConvTranspose1d(torch.nn.ConvTranspose1d, CastWeightBiasOp):\n        def reset_parameters(self):\n            return None\n\n        def forward_comfy_cast_weights(self, input, output_size=None):\n            num_spatial_dims = 1\n            output_padding = self._output_padding(\n                input, output_size, self.stride, self.padding, self.kernel_size,\n                num_spatial_dims, self.dilation)\n\n            weight, bias = cast_bias_weight(self, input)\n            return torch.nn.functional.conv_transpose1d(\n                input, weight, bias, self.stride, self.padding,\n                output_padding, self.groups, self.dilation)\n\n        def forward(self, *args, **kwargs):\n            if self.comfy_cast_weights:\n                return self.forward_comfy_cast_weights(*args, **kwargs)\n            else:\n                return super().forward(*args, **kwargs)\n\n    @classmethod\n    def conv_nd(s, dims, *args, **kwargs):\n        if dims == 2:\n            return s.Conv2d(*args, **kwargs)\n        elif dims == 3:\n            return s.Conv3d(*args, **kwargs)\n        else:\n            raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass manual_cast(disable_weight_init):\n    class Linear(disable_weight_init.Linear):\n        comfy_cast_weights = True\n\n    class Conv1d(disable_weight_init.Conv1d):\n        comfy_cast_weights = True\n\n    class Conv2d(disable_weight_init.Conv2d):\n        comfy_cast_weights = True\n\n    class Conv3d(disable_weight_init.Conv3d):\n        comfy_cast_weights = True\n\n    class GroupNorm(disable_weight_init.GroupNorm):\n        comfy_cast_weights = True\n\n    class LayerNorm(disable_weight_init.LayerNorm):\n        comfy_cast_weights = True\n\n    class ConvTranspose2d(disable_weight_init.ConvTranspose2d):\n        comfy_cast_weights = True\n\n    class ConvTranspose1d(disable_weight_init.ConvTranspose1d):\n        comfy_cast_weights = True\n", "comfy/clip_vision.py": "from .utils import load_torch_file, transformers_convert, state_dict_prefix_replace\nimport os\nimport torch\nimport json\nimport logging\n\nimport comfy.ops\nimport comfy.model_patcher\nimport comfy.model_management\nimport comfy.utils\nimport comfy.clip_model\n\nclass Output:\n    def __getitem__(self, key):\n        return getattr(self, key)\n    def __setitem__(self, key, item):\n        setattr(self, key, item)\n\ndef clip_preprocess(image, size=224):\n    mean = torch.tensor([ 0.48145466,0.4578275,0.40821073], device=image.device, dtype=image.dtype)\n    std = torch.tensor([0.26862954,0.26130258,0.27577711], device=image.device, dtype=image.dtype)\n    image = image.movedim(-1, 1)\n    if not (image.shape[2] == size and image.shape[3] == size):\n        scale = (size / min(image.shape[2], image.shape[3]))\n        image = torch.nn.functional.interpolate(image, size=(round(scale * image.shape[2]), round(scale * image.shape[3])), mode=\"bicubic\", antialias=True)\n        h = (image.shape[2] - size)//2\n        w = (image.shape[3] - size)//2\n        image = image[:,:,h:h+size,w:w+size]\n    image = torch.clip((255. * image), 0, 255).round() / 255.0\n    return (image - mean.view([3,1,1])) / std.view([3,1,1])\n\nclass ClipVisionModel():\n    def __init__(self, json_config):\n        with open(json_config) as f:\n            config = json.load(f)\n\n        self.load_device = comfy.model_management.text_encoder_device()\n        offload_device = comfy.model_management.text_encoder_offload_device()\n        self.dtype = comfy.model_management.text_encoder_dtype(self.load_device)\n        self.model = comfy.clip_model.CLIPVisionModelProjection(config, self.dtype, offload_device, comfy.ops.manual_cast)\n        self.model.eval()\n\n        self.patcher = comfy.model_patcher.ModelPatcher(self.model, load_device=self.load_device, offload_device=offload_device)\n\n    def load_sd(self, sd):\n        return self.model.load_state_dict(sd, strict=False)\n\n    def get_sd(self):\n        return self.model.state_dict()\n\n    def encode_image(self, image):\n        comfy.model_management.load_model_gpu(self.patcher)\n        pixel_values = clip_preprocess(image.to(self.load_device)).float()\n        out = self.model(pixel_values=pixel_values, intermediate_output=-2)\n\n        outputs = Output()\n        outputs[\"last_hidden_state\"] = out[0].to(comfy.model_management.intermediate_device())\n        outputs[\"image_embeds\"] = out[2].to(comfy.model_management.intermediate_device())\n        outputs[\"penultimate_hidden_states\"] = out[1].to(comfy.model_management.intermediate_device())\n        return outputs\n\ndef convert_to_transformers(sd, prefix):\n    sd_k = sd.keys()\n    if \"{}transformer.resblocks.0.attn.in_proj_weight\".format(prefix) in sd_k:\n        keys_to_replace = {\n            \"{}class_embedding\".format(prefix): \"vision_model.embeddings.class_embedding\",\n            \"{}conv1.weight\".format(prefix): \"vision_model.embeddings.patch_embedding.weight\",\n            \"{}positional_embedding\".format(prefix): \"vision_model.embeddings.position_embedding.weight\",\n            \"{}ln_post.bias\".format(prefix): \"vision_model.post_layernorm.bias\",\n            \"{}ln_post.weight\".format(prefix): \"vision_model.post_layernorm.weight\",\n            \"{}ln_pre.bias\".format(prefix): \"vision_model.pre_layrnorm.bias\",\n            \"{}ln_pre.weight\".format(prefix): \"vision_model.pre_layrnorm.weight\",\n        }\n\n        for x in keys_to_replace:\n            if x in sd_k:\n                sd[keys_to_replace[x]] = sd.pop(x)\n\n        if \"{}proj\".format(prefix) in sd_k:\n            sd['visual_projection.weight'] = sd.pop(\"{}proj\".format(prefix)).transpose(0, 1)\n\n        sd = transformers_convert(sd, prefix, \"vision_model.\", 48)\n    else:\n        replace_prefix = {prefix: \"\"}\n        sd = state_dict_prefix_replace(sd, replace_prefix)\n    return sd\n\ndef load_clipvision_from_sd(sd, prefix=\"\", convert_keys=False):\n    if convert_keys:\n        sd = convert_to_transformers(sd, prefix)\n    if \"vision_model.encoder.layers.47.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_g.json\")\n    elif \"vision_model.encoder.layers.30.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_h.json\")\n    elif \"vision_model.encoder.layers.22.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_vitl.json\")\n    else:\n        return None\n\n    clip = ClipVisionModel(json_config)\n    m, u = clip.load_sd(sd)\n    if len(m) > 0:\n        logging.warning(\"missing clip vision: {}\".format(m))\n    u = set(u)\n    keys = list(sd.keys())\n    for k in keys:\n        if k not in u:\n            t = sd.pop(k)\n            del t\n    return clip\n\ndef load(ckpt_path):\n    sd = load_torch_file(ckpt_path)\n    if \"visual.transformer.resblocks.0.attn.in_proj_weight\" in sd:\n        return load_clipvision_from_sd(sd, prefix=\"visual.\", convert_keys=True)\n    else:\n        return load_clipvision_from_sd(sd)\n", "comfy/diffusers_convert.py": "import re\nimport torch\nimport logging\n\n# conversion code from https://github.com/huggingface/diffusers/blob/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n\n# =================#\n# UNet Conversion #\n# =================#\n\nunet_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),\n    (\"out.2.weight\", \"conv_out.weight\"),\n    (\"out.2.bias\", \"conv_out.bias\"),\n]\n\nunet_conversion_map_resnet = [\n    # (stable-diffusion, HF Diffusers)\n    (\"in_layers.0\", \"norm1\"),\n    (\"in_layers.2\", \"conv1\"),\n    (\"out_layers.0\", \"norm2\"),\n    (\"out_layers.3\", \"conv2\"),\n    (\"emb_layers.1\", \"time_emb_proj\"),\n    (\"skip_connection\", \"conv_shortcut\"),\n]\n\nunet_conversion_map_layer = []\n# hardcoded number of downblocks and resnets/attentions...\n# would need smarter logic for other networks.\nfor i in range(4):\n    # loop over downblocks/upblocks\n\n    for j in range(2):\n        # loop over resnets/attentions for downblocks\n        hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n        sd_down_res_prefix = f\"input_blocks.{3 * i + j + 1}.0.\"\n        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))\n\n        if i < 3:\n            # no attention layers in down_blocks.3\n            hf_down_atn_prefix = f\"down_blocks.{i}.attentions.{j}.\"\n            sd_down_atn_prefix = f\"input_blocks.{3 * i + j + 1}.1.\"\n            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))\n\n    for j in range(3):\n        # loop over resnets/attentions for upblocks\n        hf_up_res_prefix = f\"up_blocks.{i}.resnets.{j}.\"\n        sd_up_res_prefix = f\"output_blocks.{3 * i + j}.0.\"\n        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))\n\n        if i > 0:\n            # no attention layers in up_blocks.0\n            hf_up_atn_prefix = f\"up_blocks.{i}.attentions.{j}.\"\n            sd_up_atn_prefix = f\"output_blocks.{3 * i + j}.1.\"\n            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))\n\n    if i < 3:\n        # no downsample in down_blocks.3\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.conv.\"\n        sd_downsample_prefix = f\"input_blocks.{3 * (i + 1)}.0.op.\"\n        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        # no upsample in up_blocks.3\n        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n        sd_upsample_prefix = f\"output_blocks.{3 * i + 2}.{1 if i == 0 else 2}.\"\n        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))\n\nhf_mid_atn_prefix = \"mid_block.attentions.0.\"\nsd_mid_atn_prefix = \"middle_block.1.\"\nunet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\n\nfor j in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n    sd_mid_res_prefix = f\"middle_block.{2 * j}.\"\n    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\n\ndef convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    for k, v in mapping.items():\n        for sd_part, hf_part in unet_conversion_map_layer:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}\n    return new_state_dict\n\n\n# ================#\n# VAE Conversion #\n# ================#\n\nvae_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"nin_shortcut\", \"conv_shortcut\"),\n    (\"norm_out\", \"conv_norm_out\"),\n    (\"mid.attn_1.\", \"mid_block.attentions.0.\"),\n]\n\nfor i in range(4):\n    # down_blocks have two resnets\n    for j in range(2):\n        hf_down_prefix = f\"encoder.down_blocks.{i}.resnets.{j}.\"\n        sd_down_prefix = f\"encoder.down.{i}.block.{j}.\"\n        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))\n\n    if i < 3:\n        hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.\"\n        sd_downsample_prefix = f\"down.{i}.downsample.\"\n        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))\n\n        hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n        sd_upsample_prefix = f\"up.{3 - i}.upsample.\"\n        vae_conversion_map.append((sd_upsample_prefix, hf_upsample_prefix))\n\n    # up_blocks have three resnets\n    # also, up blocks in hf are numbered in reverse from sd\n    for j in range(3):\n        hf_up_prefix = f\"decoder.up_blocks.{i}.resnets.{j}.\"\n        sd_up_prefix = f\"decoder.up.{3 - i}.block.{j}.\"\n        vae_conversion_map.append((sd_up_prefix, hf_up_prefix))\n\n# this part accounts for mid blocks in both the encoder and the decoder\nfor i in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{i}.\"\n    sd_mid_res_prefix = f\"mid.block_{i + 1}.\"\n    vae_conversion_map.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\nvae_conversion_map_attn = [\n    # (stable-diffusion, HF Diffusers)\n    (\"norm.\", \"group_norm.\"),\n    (\"q.\", \"query.\"),\n    (\"k.\", \"key.\"),\n    (\"v.\", \"value.\"),\n    (\"q.\", \"to_q.\"),\n    (\"k.\", \"to_k.\"),\n    (\"v.\", \"to_v.\"),\n    (\"proj_out.\", \"to_out.0.\"),\n    (\"proj_out.\", \"proj_attn.\"),\n]\n\n\ndef reshape_weight_for_sd(w):\n    # convert HF linear weights to SD conv2d weights\n    return w.reshape(*w.shape, 1, 1)\n\n\ndef convert_vae_state_dict(vae_state_dict):\n    mapping = {k: k for k in vae_state_dict.keys()}\n    for k, v in mapping.items():\n        for sd_part, hf_part in vae_conversion_map:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    for k, v in mapping.items():\n        if \"attentions\" in k:\n            for sd_part, hf_part in vae_conversion_map_attn:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}\n    weights_to_convert = [\"q\", \"k\", \"v\", \"proj_out\"]\n    for k, v in new_state_dict.items():\n        for weight_name in weights_to_convert:\n            if f\"mid.attn_1.{weight_name}.weight\" in k:\n                logging.debug(f\"Reshaping {k} for SD format\")\n                new_state_dict[k] = reshape_weight_for_sd(v)\n    return new_state_dict\n\n\n# =========================#\n# Text Encoder Conversion #\n# =========================#\n\n\ntextenc_conversion_lst = [\n    # (stable-diffusion, HF Diffusers)\n    (\"resblocks.\", \"text_model.encoder.layers.\"),\n    (\"ln_1\", \"layer_norm1\"),\n    (\"ln_2\", \"layer_norm2\"),\n    (\".c_fc.\", \".fc1.\"),\n    (\".c_proj.\", \".fc2.\"),\n    (\".attn\", \".self_attn\"),\n    (\"ln_final.\", \"transformer.text_model.final_layer_norm.\"),\n    (\"token_embedding.weight\", \"transformer.text_model.embeddings.token_embedding.weight\"),\n    (\"positional_embedding\", \"transformer.text_model.embeddings.position_embedding.weight\"),\n]\nprotected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}\ntextenc_pattern = re.compile(\"|\".join(protected.keys()))\n\n# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp\ncode2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\n\n# This function exists because at the time of writing torch.cat can't do fp8 with cuda\ndef cat_tensors(tensors):\n    x = 0\n    for t in tensors:\n        x += t.shape[0]\n\n    shape = [x] + list(tensors[0].shape)[1:]\n    out = torch.empty(shape, device=tensors[0].device, dtype=tensors[0].dtype)\n\n    x = 0\n    for t in tensors:\n        out[x:x + t.shape[0]] = t\n        x += t.shape[0]\n\n    return out\n\ndef convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):\n            continue\n        if (\n                k.endswith(\".self_attn.q_proj.weight\")\n                or k.endswith(\".self_attn.k_proj.weight\")\n                or k.endswith(\".self_attn.v_proj.weight\")\n        ):\n            k_pre = k[: -len(\".q_proj.weight\")]\n            k_code = k[-len(\"q_proj.weight\")]\n            if k_pre not in capture_qkv_weight:\n                capture_qkv_weight[k_pre] = [None, None, None]\n            capture_qkv_weight[k_pre][code2idx[k_code]] = v\n            continue\n\n        if (\n                k.endswith(\".self_attn.q_proj.bias\")\n                or k.endswith(\".self_attn.k_proj.bias\")\n                or k.endswith(\".self_attn.v_proj.bias\")\n        ):\n            k_pre = k[: -len(\".q_proj.bias\")]\n            k_code = k[-len(\"q_proj.bias\")]\n            if k_pre not in capture_qkv_bias:\n                capture_qkv_bias[k_pre] = [None, None, None]\n            capture_qkv_bias[k_pre][code2idx[k_code]] = v\n            continue\n\n        text_proj = \"transformer.text_projection.weight\"\n        if k.endswith(text_proj):\n            new_state_dict[k.replace(text_proj, \"text_projection\")] = v.transpose(0, 1).contiguous()\n        else:\n            relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k)\n            new_state_dict[relabelled_key] = v\n\n    for k_pre, tensors in capture_qkv_weight.items():\n        if None in tensors:\n            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n        new_state_dict[relabelled_key + \".in_proj_weight\"] = cat_tensors(tensors)\n\n    for k_pre, tensors in capture_qkv_bias.items():\n        if None in tensors:\n            raise Exception(\"CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing\")\n        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)\n        new_state_dict[relabelled_key + \".in_proj_bias\"] = cat_tensors(tensors)\n\n    return new_state_dict\n\n\ndef convert_text_enc_state_dict(text_enc_dict):\n    return text_enc_dict\n\n\n", "comfy/supported_models_base.py": "import torch\nfrom . import model_base\nfrom . import utils\nfrom . import latent_formats\n\nclass ClipTarget:\n    def __init__(self, tokenizer, clip):\n        self.clip = clip\n        self.tokenizer = tokenizer\n        self.params = {}\n\nclass BASE:\n    unet_config = {}\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,\n    }\n\n    required_keys = {}\n\n    clip_prefix = []\n    clip_vision_prefix = None\n    noise_aug_config = None\n    sampling_settings = {}\n    latent_format = latent_formats.LatentFormat\n    vae_key_prefix = [\"first_stage_model.\"]\n    text_encoder_key_prefix = [\"cond_stage_model.\"]\n    supported_inference_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n\n    manual_cast_dtype = None\n\n    @classmethod\n    def matches(s, unet_config, state_dict=None):\n        for k in s.unet_config:\n            if k not in unet_config or s.unet_config[k] != unet_config[k]:\n                return False\n        if state_dict is not None:\n            for k in s.required_keys:\n                if k not in state_dict:\n                    return False\n        return True\n\n    def model_type(self, state_dict, prefix=\"\"):\n        return model_base.ModelType.EPS\n\n    def inpaint_model(self):\n        return self.unet_config[\"in_channels\"] > 4\n\n    def __init__(self, unet_config):\n        self.unet_config = unet_config.copy()\n        self.sampling_settings = self.sampling_settings.copy()\n        self.latent_format = self.latent_format()\n        for x in self.unet_extra_config:\n            self.unet_config[x] = self.unet_extra_config[x]\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        if self.noise_aug_config is not None:\n            out = model_base.SD21UNCLIP(self, self.noise_aug_config, model_type=self.model_type(state_dict, prefix), device=device)\n        else:\n            out = model_base.BaseModel(self, model_type=self.model_type(state_dict, prefix), device=device)\n        if self.inpaint_model():\n            out.set_inpaint()\n        return out\n\n    def process_clip_state_dict(self, state_dict):\n        state_dict = utils.state_dict_prefix_replace(state_dict, {k: \"\" for k in self.text_encoder_key_prefix}, filter_keys=True)\n        return state_dict\n\n    def process_unet_state_dict(self, state_dict):\n        return state_dict\n\n    def process_vae_state_dict(self, state_dict):\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": self.text_encoder_key_prefix[0]}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_clip_vision_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        if self.clip_vision_prefix is not None:\n            replace_prefix[\"\"] = self.clip_vision_prefix\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_unet_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": \"model.diffusion_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def process_vae_state_dict_for_saving(self, state_dict):\n        replace_prefix = {\"\": self.vae_key_prefix[0]}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def set_inference_dtype(self, dtype, manual_cast_dtype):\n        self.unet_config['dtype'] = dtype\n        self.manual_cast_dtype = manual_cast_dtype\n", "comfy/controlnet.py": "import torch\nimport math\nimport os\nimport logging\nimport comfy.utils\nimport comfy.model_management\nimport comfy.model_detection\nimport comfy.model_patcher\nimport comfy.ops\n\nimport comfy.cldm.cldm\nimport comfy.t2i_adapter.adapter\nimport comfy.ldm.cascade.controlnet\n\n\ndef broadcast_image_to(tensor, target_batch_size, batched_number):\n    current_batch_size = tensor.shape[0]\n    #print(current_batch_size, target_batch_size)\n    if current_batch_size == 1:\n        return tensor\n\n    per_batch = target_batch_size // batched_number\n    tensor = tensor[:per_batch]\n\n    if per_batch > tensor.shape[0]:\n        tensor = torch.cat([tensor] * (per_batch // tensor.shape[0]) + [tensor[:(per_batch % tensor.shape[0])]], dim=0)\n\n    current_batch_size = tensor.shape[0]\n    if current_batch_size == target_batch_size:\n        return tensor\n    else:\n        return torch.cat([tensor] * batched_number, dim=0)\n\nclass ControlBase:\n    def __init__(self, device=None):\n        self.cond_hint_original = None\n        self.cond_hint = None\n        self.strength = 1.0\n        self.timestep_percent_range = (0.0, 1.0)\n        self.global_average_pooling = False\n        self.timestep_range = None\n        self.compression_ratio = 8\n        self.upscale_algorithm = 'nearest-exact'\n\n        if device is None:\n            device = comfy.model_management.get_torch_device()\n        self.device = device\n        self.previous_controlnet = None\n\n    def set_cond_hint(self, cond_hint, strength=1.0, timestep_percent_range=(0.0, 1.0)):\n        self.cond_hint_original = cond_hint\n        self.strength = strength\n        self.timestep_percent_range = timestep_percent_range\n        return self\n\n    def pre_run(self, model, percent_to_timestep_function):\n        self.timestep_range = (percent_to_timestep_function(self.timestep_percent_range[0]), percent_to_timestep_function(self.timestep_percent_range[1]))\n        if self.previous_controlnet is not None:\n            self.previous_controlnet.pre_run(model, percent_to_timestep_function)\n\n    def set_previous_controlnet(self, controlnet):\n        self.previous_controlnet = controlnet\n        return self\n\n    def cleanup(self):\n        if self.previous_controlnet is not None:\n            self.previous_controlnet.cleanup()\n        if self.cond_hint is not None:\n            del self.cond_hint\n            self.cond_hint = None\n        self.timestep_range = None\n\n    def get_models(self):\n        out = []\n        if self.previous_controlnet is not None:\n            out += self.previous_controlnet.get_models()\n        return out\n\n    def copy_to(self, c):\n        c.cond_hint_original = self.cond_hint_original\n        c.strength = self.strength\n        c.timestep_percent_range = self.timestep_percent_range\n        c.global_average_pooling = self.global_average_pooling\n        c.compression_ratio = self.compression_ratio\n        c.upscale_algorithm = self.upscale_algorithm\n\n    def inference_memory_requirements(self, dtype):\n        if self.previous_controlnet is not None:\n            return self.previous_controlnet.inference_memory_requirements(dtype)\n        return 0\n\n    def control_merge(self, control_input, control_output, control_prev, output_dtype):\n        out = {'input':[], 'middle':[], 'output': []}\n\n        if control_input is not None:\n            for i in range(len(control_input)):\n                key = 'input'\n                x = control_input[i]\n                if x is not None:\n                    x *= self.strength\n                    if x.dtype != output_dtype:\n                        x = x.to(output_dtype)\n                out[key].insert(0, x)\n\n        if control_output is not None:\n            for i in range(len(control_output)):\n                if i == (len(control_output) - 1):\n                    key = 'middle'\n                    index = 0\n                else:\n                    key = 'output'\n                    index = i\n                x = control_output[i]\n                if x is not None:\n                    if self.global_average_pooling:\n                        x = torch.mean(x, dim=(2, 3), keepdim=True).repeat(1, 1, x.shape[2], x.shape[3])\n\n                    x *= self.strength\n                    if x.dtype != output_dtype:\n                        x = x.to(output_dtype)\n\n                out[key].append(x)\n        if control_prev is not None:\n            for x in ['input', 'middle', 'output']:\n                o = out[x]\n                for i in range(len(control_prev[x])):\n                    prev_val = control_prev[x][i]\n                    if i >= len(o):\n                        o.append(prev_val)\n                    elif prev_val is not None:\n                        if o[i] is None:\n                            o[i] = prev_val\n                        else:\n                            if o[i].shape[0] < prev_val.shape[0]:\n                                o[i] = prev_val + o[i]\n                            else:\n                                o[i] += prev_val\n        return out\n\nclass ControlNet(ControlBase):\n    def __init__(self, control_model=None, global_average_pooling=False, device=None, load_device=None, manual_cast_dtype=None):\n        super().__init__(device)\n        self.control_model = control_model\n        self.load_device = load_device\n        if control_model is not None:\n            self.control_model_wrapped = comfy.model_patcher.ModelPatcher(self.control_model, load_device=load_device, offload_device=comfy.model_management.unet_offload_device())\n\n        self.global_average_pooling = global_average_pooling\n        self.model_sampling_current = None\n        self.manual_cast_dtype = manual_cast_dtype\n\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:\n            control_prev = self.previous_controlnet.get_control(x_noisy, t, cond, batched_number)\n\n        if self.timestep_range is not None:\n            if t[0] > self.timestep_range[0] or t[0] < self.timestep_range[1]:\n                if control_prev is not None:\n                    return control_prev\n                else:\n                    return None\n\n        dtype = self.control_model.dtype\n        if self.manual_cast_dtype is not None:\n            dtype = self.manual_cast_dtype\n\n        output_dtype = x_noisy.dtype\n        if self.cond_hint is None or x_noisy.shape[2] * self.compression_ratio != self.cond_hint.shape[2] or x_noisy.shape[3] * self.compression_ratio != self.cond_hint.shape[3]:\n            if self.cond_hint is not None:\n                del self.cond_hint\n            self.cond_hint = None\n            self.cond_hint = comfy.utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * self.compression_ratio, x_noisy.shape[2] * self.compression_ratio, self.upscale_algorithm, \"center\").to(dtype).to(self.device)\n        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)\n\n        context = cond.get('crossattn_controlnet', cond['c_crossattn'])\n        y = cond.get('y', None)\n        if y is not None:\n            y = y.to(dtype)\n        timestep = self.model_sampling_current.timestep(t)\n        x_noisy = self.model_sampling_current.calculate_input(t, x_noisy)\n\n        control = self.control_model(x=x_noisy.to(dtype), hint=self.cond_hint, timesteps=timestep.float(), context=context.to(dtype), y=y)\n        return self.control_merge(None, control, control_prev, output_dtype)\n\n    def copy(self):\n        c = ControlNet(None, global_average_pooling=self.global_average_pooling, load_device=self.load_device, manual_cast_dtype=self.manual_cast_dtype)\n        c.control_model = self.control_model\n        c.control_model_wrapped = self.control_model_wrapped\n        self.copy_to(c)\n        return c\n\n    def get_models(self):\n        out = super().get_models()\n        out.append(self.control_model_wrapped)\n        return out\n\n    def pre_run(self, model, percent_to_timestep_function):\n        super().pre_run(model, percent_to_timestep_function)\n        self.model_sampling_current = model.model_sampling\n\n    def cleanup(self):\n        self.model_sampling_current = None\n        super().cleanup()\n\nclass ControlLoraOps:\n    class Linear(torch.nn.Module, comfy.ops.CastWeightBiasOp):\n        def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                    device=None, dtype=None) -> None:\n            factory_kwargs = {'device': device, 'dtype': dtype}\n            super().__init__()\n            self.in_features = in_features\n            self.out_features = out_features\n            self.weight = None\n            self.up = None\n            self.down = None\n            self.bias = None\n\n        def forward(self, input):\n            weight, bias = comfy.ops.cast_bias_weight(self, input)\n            if self.up is not None:\n                return torch.nn.functional.linear(input, weight + (torch.mm(self.up.flatten(start_dim=1), self.down.flatten(start_dim=1))).reshape(self.weight.shape).type(input.dtype), bias)\n            else:\n                return torch.nn.functional.linear(input, weight, bias)\n\n    class Conv2d(torch.nn.Module, comfy.ops.CastWeightBiasOp):\n        def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n            padding_mode='zeros',\n            device=None,\n            dtype=None\n        ):\n            super().__init__()\n            self.in_channels = in_channels\n            self.out_channels = out_channels\n            self.kernel_size = kernel_size\n            self.stride = stride\n            self.padding = padding\n            self.dilation = dilation\n            self.transposed = False\n            self.output_padding = 0\n            self.groups = groups\n            self.padding_mode = padding_mode\n\n            self.weight = None\n            self.bias = None\n            self.up = None\n            self.down = None\n\n\n        def forward(self, input):\n            weight, bias = comfy.ops.cast_bias_weight(self, input)\n            if self.up is not None:\n                return torch.nn.functional.conv2d(input, weight + (torch.mm(self.up.flatten(start_dim=1), self.down.flatten(start_dim=1))).reshape(self.weight.shape).type(input.dtype), bias, self.stride, self.padding, self.dilation, self.groups)\n            else:\n                return torch.nn.functional.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass ControlLora(ControlNet):\n    def __init__(self, control_weights, global_average_pooling=False, device=None):\n        ControlBase.__init__(self, device)\n        self.control_weights = control_weights\n        self.global_average_pooling = global_average_pooling\n\n    def pre_run(self, model, percent_to_timestep_function):\n        super().pre_run(model, percent_to_timestep_function)\n        controlnet_config = model.model_config.unet_config.copy()\n        controlnet_config.pop(\"out_channels\")\n        controlnet_config[\"hint_channels\"] = self.control_weights[\"input_hint_block.0.weight\"].shape[1]\n        self.manual_cast_dtype = model.manual_cast_dtype\n        dtype = model.get_dtype()\n        if self.manual_cast_dtype is None:\n            class control_lora_ops(ControlLoraOps, comfy.ops.disable_weight_init):\n                pass\n        else:\n            class control_lora_ops(ControlLoraOps, comfy.ops.manual_cast):\n                pass\n            dtype = self.manual_cast_dtype\n\n        controlnet_config[\"operations\"] = control_lora_ops\n        controlnet_config[\"dtype\"] = dtype\n        self.control_model = comfy.cldm.cldm.ControlNet(**controlnet_config)\n        self.control_model.to(comfy.model_management.get_torch_device())\n        diffusion_model = model.diffusion_model\n        sd = diffusion_model.state_dict()\n        cm = self.control_model.state_dict()\n\n        for k in sd:\n            weight = sd[k]\n            try:\n                comfy.utils.set_attr_param(self.control_model, k, weight)\n            except:\n                pass\n\n        for k in self.control_weights:\n            if k not in {\"lora_controlnet\"}:\n                comfy.utils.set_attr_param(self.control_model, k, self.control_weights[k].to(dtype).to(comfy.model_management.get_torch_device()))\n\n    def copy(self):\n        c = ControlLora(self.control_weights, global_average_pooling=self.global_average_pooling)\n        self.copy_to(c)\n        return c\n\n    def cleanup(self):\n        del self.control_model\n        self.control_model = None\n        super().cleanup()\n\n    def get_models(self):\n        out = ControlBase.get_models(self)\n        return out\n\n    def inference_memory_requirements(self, dtype):\n        return comfy.utils.calculate_parameters(self.control_weights) * comfy.model_management.dtype_size(dtype) + ControlBase.inference_memory_requirements(self, dtype)\n\ndef load_controlnet(ckpt_path, model=None):\n    controlnet_data = comfy.utils.load_torch_file(ckpt_path, safe_load=True)\n    if \"lora_controlnet\" in controlnet_data:\n        return ControlLora(controlnet_data)\n\n    controlnet_config = None\n    supported_inference_dtypes = None\n\n    if \"controlnet_cond_embedding.conv_in.weight\" in controlnet_data: #diffusers format\n        controlnet_config = comfy.model_detection.unet_config_from_diffusers_unet(controlnet_data)\n        diffusers_keys = comfy.utils.unet_to_diffusers(controlnet_config)\n        diffusers_keys[\"controlnet_mid_block.weight\"] = \"middle_block_out.0.weight\"\n        diffusers_keys[\"controlnet_mid_block.bias\"] = \"middle_block_out.0.bias\"\n\n        count = 0\n        loop = True\n        while loop:\n            suffix = [\".weight\", \".bias\"]\n            for s in suffix:\n                k_in = \"controlnet_down_blocks.{}{}\".format(count, s)\n                k_out = \"zero_convs.{}.0{}\".format(count, s)\n                if k_in not in controlnet_data:\n                    loop = False\n                    break\n                diffusers_keys[k_in] = k_out\n            count += 1\n\n        count = 0\n        loop = True\n        while loop:\n            suffix = [\".weight\", \".bias\"]\n            for s in suffix:\n                if count == 0:\n                    k_in = \"controlnet_cond_embedding.conv_in{}\".format(s)\n                else:\n                    k_in = \"controlnet_cond_embedding.blocks.{}{}\".format(count - 1, s)\n                k_out = \"input_hint_block.{}{}\".format(count * 2, s)\n                if k_in not in controlnet_data:\n                    k_in = \"controlnet_cond_embedding.conv_out{}\".format(s)\n                    loop = False\n                diffusers_keys[k_in] = k_out\n            count += 1\n\n        new_sd = {}\n        for k in diffusers_keys:\n            if k in controlnet_data:\n                new_sd[diffusers_keys[k]] = controlnet_data.pop(k)\n\n        leftover_keys = controlnet_data.keys()\n        if len(leftover_keys) > 0:\n            logging.warning(\"leftover keys: {}\".format(leftover_keys))\n        controlnet_data = new_sd\n\n    pth_key = 'control_model.zero_convs.0.0.weight'\n    pth = False\n    key = 'zero_convs.0.0.weight'\n    if pth_key in controlnet_data:\n        pth = True\n        key = pth_key\n        prefix = \"control_model.\"\n    elif key in controlnet_data:\n        prefix = \"\"\n    else:\n        net = load_t2i_adapter(controlnet_data)\n        if net is None:\n            logging.error(\"error checkpoint does not contain controlnet or t2i adapter data {}\".format(ckpt_path))\n        return net\n\n    if controlnet_config is None:\n        model_config = comfy.model_detection.model_config_from_unet(controlnet_data, prefix, True)\n        supported_inference_dtypes = model_config.supported_inference_dtypes\n        controlnet_config = model_config.unet_config\n\n    load_device = comfy.model_management.get_torch_device()\n    if supported_inference_dtypes is None:\n        unet_dtype = comfy.model_management.unet_dtype()\n    else:\n        unet_dtype = comfy.model_management.unet_dtype(supported_dtypes=supported_inference_dtypes)\n\n    manual_cast_dtype = comfy.model_management.unet_manual_cast(unet_dtype, load_device)\n    if manual_cast_dtype is not None:\n        controlnet_config[\"operations\"] = comfy.ops.manual_cast\n    controlnet_config[\"dtype\"] = unet_dtype\n    controlnet_config.pop(\"out_channels\")\n    controlnet_config[\"hint_channels\"] = controlnet_data[\"{}input_hint_block.0.weight\".format(prefix)].shape[1]\n    control_model = comfy.cldm.cldm.ControlNet(**controlnet_config)\n\n    if pth:\n        if 'difference' in controlnet_data:\n            if model is not None:\n                comfy.model_management.load_models_gpu([model])\n                model_sd = model.model_state_dict()\n                for x in controlnet_data:\n                    c_m = \"control_model.\"\n                    if x.startswith(c_m):\n                        sd_key = \"diffusion_model.{}\".format(x[len(c_m):])\n                        if sd_key in model_sd:\n                            cd = controlnet_data[x]\n                            cd += model_sd[sd_key].type(cd.dtype).to(cd.device)\n            else:\n                logging.warning(\"WARNING: Loaded a diff controlnet without a model. It will very likely not work.\")\n\n        class WeightsLoader(torch.nn.Module):\n            pass\n        w = WeightsLoader()\n        w.control_model = control_model\n        missing, unexpected = w.load_state_dict(controlnet_data, strict=False)\n    else:\n        missing, unexpected = control_model.load_state_dict(controlnet_data, strict=False)\n\n    if len(missing) > 0:\n        logging.warning(\"missing controlnet keys: {}\".format(missing))\n\n    if len(unexpected) > 0:\n        logging.debug(\"unexpected controlnet keys: {}\".format(unexpected))\n\n    global_average_pooling = False\n    filename = os.path.splitext(ckpt_path)[0]\n    if filename.endswith(\"_shuffle\") or filename.endswith(\"_shuffle_fp16\"): #TODO: smarter way of enabling global_average_pooling\n        global_average_pooling = True\n\n    control = ControlNet(control_model, global_average_pooling=global_average_pooling, load_device=load_device, manual_cast_dtype=manual_cast_dtype)\n    return control\n\nclass T2IAdapter(ControlBase):\n    def __init__(self, t2i_model, channels_in, compression_ratio, upscale_algorithm, device=None):\n        super().__init__(device)\n        self.t2i_model = t2i_model\n        self.channels_in = channels_in\n        self.control_input = None\n        self.compression_ratio = compression_ratio\n        self.upscale_algorithm = upscale_algorithm\n\n    def scale_image_to(self, width, height):\n        unshuffle_amount = self.t2i_model.unshuffle_amount\n        width = math.ceil(width / unshuffle_amount) * unshuffle_amount\n        height = math.ceil(height / unshuffle_amount) * unshuffle_amount\n        return width, height\n\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:\n            control_prev = self.previous_controlnet.get_control(x_noisy, t, cond, batched_number)\n\n        if self.timestep_range is not None:\n            if t[0] > self.timestep_range[0] or t[0] < self.timestep_range[1]:\n                if control_prev is not None:\n                    return control_prev\n                else:\n                    return None\n\n        if self.cond_hint is None or x_noisy.shape[2] * self.compression_ratio != self.cond_hint.shape[2] or x_noisy.shape[3] * self.compression_ratio != self.cond_hint.shape[3]:\n            if self.cond_hint is not None:\n                del self.cond_hint\n            self.control_input = None\n            self.cond_hint = None\n            width, height = self.scale_image_to(x_noisy.shape[3] * self.compression_ratio, x_noisy.shape[2] * self.compression_ratio)\n            self.cond_hint = comfy.utils.common_upscale(self.cond_hint_original, width, height, self.upscale_algorithm, \"center\").float().to(self.device)\n            if self.channels_in == 1 and self.cond_hint.shape[1] > 1:\n                self.cond_hint = torch.mean(self.cond_hint, 1, keepdim=True)\n        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)\n        if self.control_input is None:\n            self.t2i_model.to(x_noisy.dtype)\n            self.t2i_model.to(self.device)\n            self.control_input = self.t2i_model(self.cond_hint.to(x_noisy.dtype))\n            self.t2i_model.cpu()\n\n        control_input = list(map(lambda a: None if a is None else a.clone(), self.control_input))\n        mid = None\n        if self.t2i_model.xl == True:\n            mid = control_input[-1:]\n            control_input = control_input[:-1]\n        return self.control_merge(control_input, mid, control_prev, x_noisy.dtype)\n\n    def copy(self):\n        c = T2IAdapter(self.t2i_model, self.channels_in, self.compression_ratio, self.upscale_algorithm)\n        self.copy_to(c)\n        return c\n\ndef load_t2i_adapter(t2i_data):\n    compression_ratio = 8\n    upscale_algorithm = 'nearest-exact'\n\n    if 'adapter' in t2i_data:\n        t2i_data = t2i_data['adapter']\n    if 'adapter.body.0.resnets.0.block1.weight' in t2i_data: #diffusers format\n        prefix_replace = {}\n        for i in range(4):\n            for j in range(2):\n                prefix_replace[\"adapter.body.{}.resnets.{}.\".format(i, j)] = \"body.{}.\".format(i * 2 + j)\n            prefix_replace[\"adapter.body.{}.\".format(i, j)] = \"body.{}.\".format(i * 2)\n        prefix_replace[\"adapter.\"] = \"\"\n        t2i_data = comfy.utils.state_dict_prefix_replace(t2i_data, prefix_replace)\n    keys = t2i_data.keys()\n\n    if \"body.0.in_conv.weight\" in keys:\n        cin = t2i_data['body.0.in_conv.weight'].shape[1]\n        model_ad = comfy.t2i_adapter.adapter.Adapter_light(cin=cin, channels=[320, 640, 1280, 1280], nums_rb=4)\n    elif 'conv_in.weight' in keys:\n        cin = t2i_data['conv_in.weight'].shape[1]\n        channel = t2i_data['conv_in.weight'].shape[0]\n        ksize = t2i_data['body.0.block2.weight'].shape[2]\n        use_conv = False\n        down_opts = list(filter(lambda a: a.endswith(\"down_opt.op.weight\"), keys))\n        if len(down_opts) > 0:\n            use_conv = True\n        xl = False\n        if cin == 256 or cin == 768:\n            xl = True\n        model_ad = comfy.t2i_adapter.adapter.Adapter(cin=cin, channels=[channel, channel*2, channel*4, channel*4][:4], nums_rb=2, ksize=ksize, sk=True, use_conv=use_conv, xl=xl)\n    elif \"backbone.0.0.weight\" in keys:\n        model_ad = comfy.ldm.cascade.controlnet.ControlNet(c_in=t2i_data['backbone.0.0.weight'].shape[1], proj_blocks=[0, 4, 8, 12, 51, 55, 59, 63])\n        compression_ratio = 32\n        upscale_algorithm = 'bilinear'\n    elif \"backbone.10.blocks.0.weight\" in keys:\n        model_ad = comfy.ldm.cascade.controlnet.ControlNet(c_in=t2i_data['backbone.0.weight'].shape[1], bottleneck_mode=\"large\", proj_blocks=[0, 4, 8, 12, 51, 55, 59, 63])\n        compression_ratio = 1\n        upscale_algorithm = 'nearest-exact'\n    else:\n        return None\n\n    missing, unexpected = model_ad.load_state_dict(t2i_data)\n    if len(missing) > 0:\n        logging.warning(\"t2i missing {}\".format(missing))\n\n    if len(unexpected) > 0:\n        logging.debug(\"t2i unexpected {}\".format(unexpected))\n\n    return T2IAdapter(model_ad, model_ad.input_channels, compression_ratio, upscale_algorithm)\n", "comfy/model_base.py": "import torch\nimport logging\nfrom comfy.ldm.modules.diffusionmodules.openaimodel import UNetModel, Timestep\nfrom comfy.ldm.cascade.stage_c import StageC\nfrom comfy.ldm.cascade.stage_b import StageB\nfrom comfy.ldm.modules.encoders.noise_aug_modules import CLIPEmbeddingNoiseAugmentation\nfrom comfy.ldm.modules.diffusionmodules.upscaling import ImageConcatWithNoiseAugmentation\nfrom comfy.ldm.modules.diffusionmodules.mmdit import OpenAISignatureMMDITWrapper\nimport comfy.ldm.audio.dit\nimport comfy.ldm.audio.embedders\nimport comfy.model_management\nimport comfy.conds\nimport comfy.ops\nfrom enum import Enum\nfrom . import utils\nimport comfy.latent_formats\nimport math\n\nclass ModelType(Enum):\n    EPS = 1\n    V_PREDICTION = 2\n    V_PREDICTION_EDM = 3\n    STABLE_CASCADE = 4\n    EDM = 5\n    FLOW = 6\n    V_PREDICTION_CONTINUOUS = 7\n\n\nfrom comfy.model_sampling import EPS, V_PREDICTION, EDM, ModelSamplingDiscrete, ModelSamplingContinuousEDM, StableCascadeSampling, ModelSamplingContinuousV\n\n\ndef model_sampling(model_config, model_type):\n    s = ModelSamplingDiscrete\n\n    if model_type == ModelType.EPS:\n        c = EPS\n    elif model_type == ModelType.V_PREDICTION:\n        c = V_PREDICTION\n    elif model_type == ModelType.V_PREDICTION_EDM:\n        c = V_PREDICTION\n        s = ModelSamplingContinuousEDM\n    elif model_type == ModelType.FLOW:\n        c = comfy.model_sampling.CONST\n        s = comfy.model_sampling.ModelSamplingDiscreteFlow\n    elif model_type == ModelType.STABLE_CASCADE:\n        c = EPS\n        s = StableCascadeSampling\n    elif model_type == ModelType.EDM:\n        c = EDM\n        s = ModelSamplingContinuousEDM\n    elif model_type == ModelType.V_PREDICTION_CONTINUOUS:\n        c = V_PREDICTION\n        s = ModelSamplingContinuousV\n\n    class ModelSampling(s, c):\n        pass\n\n    return ModelSampling(model_config)\n\n\nclass BaseModel(torch.nn.Module):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None, unet_model=UNetModel):\n        super().__init__()\n\n        unet_config = model_config.unet_config\n        self.latent_format = model_config.latent_format\n        self.model_config = model_config\n        self.manual_cast_dtype = model_config.manual_cast_dtype\n\n        if not unet_config.get(\"disable_unet_model_creation\", False):\n            if self.manual_cast_dtype is not None:\n                operations = comfy.ops.manual_cast\n            else:\n                operations = comfy.ops.disable_weight_init\n            self.diffusion_model = unet_model(**unet_config, device=device, operations=operations)\n            if comfy.model_management.force_channels_last():\n                self.diffusion_model.to(memory_format=torch.channels_last)\n                logging.debug(\"using channels last mode for diffusion model\")\n        self.model_type = model_type\n        self.model_sampling = model_sampling(model_config, model_type)\n\n        self.adm_channels = unet_config.get(\"adm_in_channels\", None)\n        if self.adm_channels is None:\n            self.adm_channels = 0\n\n        self.concat_keys = ()\n        logging.info(\"model_type {}\".format(model_type.name))\n        logging.debug(\"adm {}\".format(self.adm_channels))\n\n    def apply_model(self, x, t, c_concat=None, c_crossattn=None, control=None, transformer_options={}, **kwargs):\n        sigma = t\n        xc = self.model_sampling.calculate_input(sigma, x)\n        if c_concat is not None:\n            xc = torch.cat([xc] + [c_concat], dim=1)\n\n        context = c_crossattn\n        dtype = self.get_dtype()\n\n        if self.manual_cast_dtype is not None:\n            dtype = self.manual_cast_dtype\n\n        xc = xc.to(dtype)\n        t = self.model_sampling.timestep(t).float()\n        context = context.to(dtype)\n        extra_conds = {}\n        for o in kwargs:\n            extra = kwargs[o]\n            if hasattr(extra, \"dtype\"):\n                if extra.dtype != torch.int and extra.dtype != torch.long:\n                    extra = extra.to(dtype)\n            extra_conds[o] = extra\n\n        model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\n        return self.model_sampling.calculate_denoised(sigma, model_output, x)\n\n    def get_dtype(self):\n        return self.diffusion_model.dtype\n\n    def is_adm(self):\n        return self.adm_channels > 0\n\n    def encode_adm(self, **kwargs):\n        return None\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        if len(self.concat_keys) > 0:\n            cond_concat = []\n            denoise_mask = kwargs.get(\"concat_mask\", kwargs.get(\"denoise_mask\", None))\n            concat_latent_image = kwargs.get(\"concat_latent_image\", None)\n            if concat_latent_image is None:\n                concat_latent_image = kwargs.get(\"latent_image\", None)\n            else:\n                concat_latent_image = self.process_latent_in(concat_latent_image)\n\n            noise = kwargs.get(\"noise\", None)\n            device = kwargs[\"device\"]\n\n            if concat_latent_image.shape[1:] != noise.shape[1:]:\n                concat_latent_image = utils.common_upscale(concat_latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n            concat_latent_image = utils.resize_to_batch_size(concat_latent_image, noise.shape[0])\n\n            if denoise_mask is not None:\n                if len(denoise_mask.shape) == len(noise.shape):\n                    denoise_mask = denoise_mask[:,:1]\n\n                denoise_mask = denoise_mask.reshape((-1, 1, denoise_mask.shape[-2], denoise_mask.shape[-1]))\n                if denoise_mask.shape[-2:] != noise.shape[-2:]:\n                    denoise_mask = utils.common_upscale(denoise_mask, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n                denoise_mask = utils.resize_to_batch_size(denoise_mask.round(), noise.shape[0])\n\n            for ck in self.concat_keys:\n                if denoise_mask is not None:\n                    if ck == \"mask\":\n                        cond_concat.append(denoise_mask.to(device))\n                    elif ck == \"masked_image\":\n                        cond_concat.append(concat_latent_image.to(device)) #NOTE: the latent_image should be masked by the mask in pixel space\n                else:\n                    if ck == \"mask\":\n                        cond_concat.append(torch.ones_like(noise)[:,:1])\n                    elif ck == \"masked_image\":\n                        cond_concat.append(self.blank_inpaint_image_like(noise))\n            data = torch.cat(cond_concat, dim=1)\n            out['c_concat'] = comfy.conds.CONDNoiseShape(data)\n\n        adm = self.encode_adm(**kwargs)\n        if adm is not None:\n            out['y'] = comfy.conds.CONDRegular(adm)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['c_crossattn'] = comfy.conds.CONDCrossAttn(cross_attn)\n\n        cross_attn_cnet = kwargs.get(\"cross_attn_controlnet\", None)\n        if cross_attn_cnet is not None:\n            out['crossattn_controlnet'] = comfy.conds.CONDCrossAttn(cross_attn_cnet)\n\n        c_concat = kwargs.get(\"noise_concat\", None)\n        if c_concat is not None:\n            out['c_concat'] = comfy.conds.CONDNoiseShape(c_concat)\n\n        return out\n\n    def load_model_weights(self, sd, unet_prefix=\"\"):\n        to_load = {}\n        keys = list(sd.keys())\n        for k in keys:\n            if k.startswith(unet_prefix):\n                to_load[k[len(unet_prefix):]] = sd.pop(k)\n\n        to_load = self.model_config.process_unet_state_dict(to_load)\n        m, u = self.diffusion_model.load_state_dict(to_load, strict=False)\n        if len(m) > 0:\n            logging.warning(\"unet missing: {}\".format(m))\n\n        if len(u) > 0:\n            logging.warning(\"unet unexpected: {}\".format(u))\n        del to_load\n        return self\n\n    def process_latent_in(self, latent):\n        return self.latent_format.process_in(latent)\n\n    def process_latent_out(self, latent):\n        return self.latent_format.process_out(latent)\n\n    def state_dict_for_saving(self, clip_state_dict=None, vae_state_dict=None, clip_vision_state_dict=None):\n        extra_sds = []\n        if clip_state_dict is not None:\n            extra_sds.append(self.model_config.process_clip_state_dict_for_saving(clip_state_dict))\n        if vae_state_dict is not None:\n            extra_sds.append(self.model_config.process_vae_state_dict_for_saving(vae_state_dict))\n        if clip_vision_state_dict is not None:\n            extra_sds.append(self.model_config.process_clip_vision_state_dict_for_saving(clip_vision_state_dict))\n\n        unet_state_dict = self.diffusion_model.state_dict()\n        unet_state_dict = self.model_config.process_unet_state_dict_for_saving(unet_state_dict)\n\n        if self.model_type == ModelType.V_PREDICTION:\n            unet_state_dict[\"v_pred\"] = torch.tensor([])\n\n        for sd in extra_sds:\n            unet_state_dict.update(sd)\n\n        return unet_state_dict\n\n    def set_inpaint(self):\n        self.concat_keys = (\"mask\", \"masked_image\")\n        def blank_inpaint_image_like(latent_image):\n            blank_image = torch.ones_like(latent_image)\n            # these are the values for \"zero\" in pixel space translated to latent space\n            blank_image[:,0] *= 0.8223\n            blank_image[:,1] *= -0.6876\n            blank_image[:,2] *= 0.6364\n            blank_image[:,3] *= 0.1380\n            return blank_image\n        self.blank_inpaint_image_like = blank_inpaint_image_like\n\n    def memory_required(self, input_shape):\n        if comfy.model_management.xformers_enabled() or comfy.model_management.pytorch_attention_flash_attention():\n            dtype = self.get_dtype()\n            if self.manual_cast_dtype is not None:\n                dtype = self.manual_cast_dtype\n            #TODO: this needs to be tweaked\n            area = input_shape[0] * math.prod(input_shape[2:])\n            return (area * comfy.model_management.dtype_size(dtype) / 50) * (1024 * 1024)\n        else:\n            #TODO: this formula might be too aggressive since I tweaked the sub-quad and split algorithms to use less memory.\n            area = input_shape[0] * math.prod(input_shape[2:])\n            return (((area * 0.6) / 0.9) + 1024) * (1024 * 1024)\n\n\ndef unclip_adm(unclip_conditioning, device, noise_augmentor, noise_augment_merge=0.0, seed=None):\n    adm_inputs = []\n    weights = []\n    noise_aug = []\n    for unclip_cond in unclip_conditioning:\n        for adm_cond in unclip_cond[\"clip_vision_output\"].image_embeds:\n            weight = unclip_cond[\"strength\"]\n            noise_augment = unclip_cond[\"noise_augmentation\"]\n            noise_level = round((noise_augmentor.max_noise_level - 1) * noise_augment)\n            c_adm, noise_level_emb = noise_augmentor(adm_cond.to(device), noise_level=torch.tensor([noise_level], device=device), seed=seed)\n            adm_out = torch.cat((c_adm, noise_level_emb), 1) * weight\n            weights.append(weight)\n            noise_aug.append(noise_augment)\n            adm_inputs.append(adm_out)\n\n    if len(noise_aug) > 1:\n        adm_out = torch.stack(adm_inputs).sum(0)\n        noise_augment = noise_augment_merge\n        noise_level = round((noise_augmentor.max_noise_level - 1) * noise_augment)\n        c_adm, noise_level_emb = noise_augmentor(adm_out[:, :noise_augmentor.time_embed.dim], noise_level=torch.tensor([noise_level], device=device))\n        adm_out = torch.cat((c_adm, noise_level_emb), 1)\n\n    return adm_out\n\nclass SD21UNCLIP(BaseModel):\n    def __init__(self, model_config, noise_aug_config, model_type=ModelType.V_PREDICTION, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**noise_aug_config)\n\n    def encode_adm(self, **kwargs):\n        unclip_conditioning = kwargs.get(\"unclip_conditioning\", None)\n        device = kwargs[\"device\"]\n        if unclip_conditioning is None:\n            return torch.zeros((1, self.adm_channels))\n        else:\n            return unclip_adm(unclip_conditioning, device, self.noise_augmentor, kwargs.get(\"unclip_noise_augment_merge\", 0.05), kwargs.get(\"seed\", 0) - 10)\n\ndef sdxl_pooled(args, noise_augmentor):\n    if \"unclip_conditioning\" in args:\n        return unclip_adm(args.get(\"unclip_conditioning\", None), args[\"device\"], noise_augmentor, seed=args.get(\"seed\", 0) - 10)[:,:1280]\n    else:\n        return args[\"pooled_output\"]\n\nclass SDXLRefiner(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)\n        crop_h = kwargs.get(\"crop_h\", 0)\n\n        if kwargs.get(\"prompt_type\", \"\") == \"negative\":\n            aesthetic_score = kwargs.get(\"aesthetic_score\", 2.5)\n        else:\n            aesthetic_score = kwargs.get(\"aesthetic_score\", 6)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([height])))\n        out.append(self.embedder(torch.Tensor([width])))\n        out.append(self.embedder(torch.Tensor([crop_h])))\n        out.append(self.embedder(torch.Tensor([crop_w])))\n        out.append(self.embedder(torch.Tensor([aesthetic_score])))\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0).repeat(clip_pooled.shape[0], 1)\n        return torch.cat((clip_pooled.to(flat.device), flat), dim=1)\n\nclass SDXL(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)\n        crop_h = kwargs.get(\"crop_h\", 0)\n        target_width = kwargs.get(\"target_width\", width)\n        target_height = kwargs.get(\"target_height\", height)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([height])))\n        out.append(self.embedder(torch.Tensor([width])))\n        out.append(self.embedder(torch.Tensor([crop_h])))\n        out.append(self.embedder(torch.Tensor([crop_w])))\n        out.append(self.embedder(torch.Tensor([target_height])))\n        out.append(self.embedder(torch.Tensor([target_width])))\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0).repeat(clip_pooled.shape[0], 1)\n        return torch.cat((clip_pooled.to(flat.device), flat), dim=1)\n\nclass SVD_img2vid(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.V_PREDICTION_EDM, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n\n    def encode_adm(self, **kwargs):\n        fps_id = kwargs.get(\"fps\", 6) - 1\n        motion_bucket_id = kwargs.get(\"motion_bucket_id\", 127)\n        augmentation = kwargs.get(\"augmentation_level\", 0)\n\n        out = []\n        out.append(self.embedder(torch.Tensor([fps_id])))\n        out.append(self.embedder(torch.Tensor([motion_bucket_id])))\n        out.append(self.embedder(torch.Tensor([augmentation])))\n\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0)\n        return flat\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        adm = self.encode_adm(**kwargs)\n        if adm is not None:\n            out['y'] = comfy.conds.CONDRegular(adm)\n\n        latent_image = kwargs.get(\"concat_latent_image\", None)\n        noise = kwargs.get(\"noise\", None)\n        device = kwargs[\"device\"]\n\n        if latent_image is None:\n            latent_image = torch.zeros_like(noise)\n\n        if latent_image.shape[1:] != noise.shape[1:]:\n            latent_image = utils.common_upscale(latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        latent_image = utils.resize_to_batch_size(latent_image, noise.shape[0])\n\n        out['c_concat'] = comfy.conds.CONDNoiseShape(latent_image)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['c_crossattn'] = comfy.conds.CONDCrossAttn(cross_attn)\n\n        if \"time_conditioning\" in kwargs:\n            out[\"time_context\"] = comfy.conds.CONDCrossAttn(kwargs[\"time_conditioning\"])\n\n        out['num_video_frames'] = comfy.conds.CONDConstant(noise.shape[0])\n        return out\n\nclass SV3D_u(SVD_img2vid):\n    def encode_adm(self, **kwargs):\n        augmentation = kwargs.get(\"augmentation_level\", 0)\n\n        out = []\n        out.append(self.embedder(torch.flatten(torch.Tensor([augmentation]))))\n\n        flat = torch.flatten(torch.cat(out)).unsqueeze(dim=0)\n        return flat\n\nclass SV3D_p(SVD_img2vid):\n    def __init__(self, model_config, model_type=ModelType.V_PREDICTION_EDM, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder_512 = Timestep(512)\n\n    def encode_adm(self, **kwargs):\n        augmentation = kwargs.get(\"augmentation_level\", 0)\n        elevation = kwargs.get(\"elevation\", 0) #elevation and azimuth are in degrees here\n        azimuth = kwargs.get(\"azimuth\", 0)\n        noise = kwargs.get(\"noise\", None)\n\n        out = []\n        out.append(self.embedder(torch.flatten(torch.Tensor([augmentation]))))\n        out.append(self.embedder_512(torch.deg2rad(torch.fmod(torch.flatten(90 - torch.Tensor([elevation])), 360.0))))\n        out.append(self.embedder_512(torch.deg2rad(torch.fmod(torch.flatten(torch.Tensor([azimuth])), 360.0))))\n\n        out = list(map(lambda a: utils.resize_to_batch_size(a, noise.shape[0]), out))\n        return torch.cat(out, dim=1)\n\n\nclass Stable_Zero123(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None, cc_projection_weight=None, cc_projection_bias=None):\n        super().__init__(model_config, model_type, device=device)\n        self.cc_projection = comfy.ops.manual_cast.Linear(cc_projection_weight.shape[1], cc_projection_weight.shape[0], dtype=self.get_dtype(), device=device)\n        self.cc_projection.weight.copy_(cc_projection_weight)\n        self.cc_projection.bias.copy_(cc_projection_bias)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        latent_image = kwargs.get(\"concat_latent_image\", None)\n        noise = kwargs.get(\"noise\", None)\n\n        if latent_image is None:\n            latent_image = torch.zeros_like(noise)\n\n        if latent_image.shape[1:] != noise.shape[1:]:\n            latent_image = utils.common_upscale(latent_image, noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        latent_image = utils.resize_to_batch_size(latent_image, noise.shape[0])\n\n        out['c_concat'] = comfy.conds.CONDNoiseShape(latent_image)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            if cross_attn.shape[-1] != 768:\n                cross_attn = self.cc_projection(cross_attn)\n            out['c_crossattn'] = comfy.conds.CONDCrossAttn(cross_attn)\n        return out\n\nclass SD_X4Upscaler(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.V_PREDICTION, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.noise_augmentor = ImageConcatWithNoiseAugmentation(noise_schedule_config={\"linear_start\": 0.0001, \"linear_end\": 0.02}, max_noise_level=350)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        image = kwargs.get(\"concat_image\", None)\n        noise = kwargs.get(\"noise\", None)\n        noise_augment = kwargs.get(\"noise_augmentation\", 0.0)\n        device = kwargs[\"device\"]\n        seed = kwargs[\"seed\"] - 10\n\n        noise_level = round((self.noise_augmentor.max_noise_level) * noise_augment)\n\n        if image is None:\n            image = torch.zeros_like(noise)[:,:3]\n\n        if image.shape[1:] != noise.shape[1:]:\n            image = utils.common_upscale(image.to(device), noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        noise_level = torch.tensor([noise_level], device=device)\n        if noise_augment > 0:\n            image, noise_level = self.noise_augmentor(image.to(device), noise_level=noise_level, seed=seed)\n\n        image = utils.resize_to_batch_size(image, noise.shape[0])\n\n        out['c_concat'] = comfy.conds.CONDNoiseShape(image)\n        out['y'] = comfy.conds.CONDRegular(noise_level)\n        return out\n\nclass IP2P:\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        image = kwargs.get(\"concat_latent_image\", None)\n        noise = kwargs.get(\"noise\", None)\n        device = kwargs[\"device\"]\n\n        if image is None:\n            image = torch.zeros_like(noise)\n\n        if image.shape[1:] != noise.shape[1:]:\n            image = utils.common_upscale(image.to(device), noise.shape[-1], noise.shape[-2], \"bilinear\", \"center\")\n\n        image = utils.resize_to_batch_size(image, noise.shape[0])\n\n        out['c_concat'] = comfy.conds.CONDNoiseShape(self.process_ip2p_image_in(image))\n        adm = self.encode_adm(**kwargs)\n        if adm is not None:\n            out['y'] = comfy.conds.CONDRegular(adm)\n        return out\n\nclass SD15_instructpix2pix(IP2P, BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.process_ip2p_image_in = lambda image: image\n\nclass SDXL_instructpix2pix(IP2P, SDXL):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        if model_type == ModelType.V_PREDICTION_EDM:\n            self.process_ip2p_image_in = lambda image: comfy.latent_formats.SDXL().process_in(image) #cosxl ip2p\n        else:\n            self.process_ip2p_image_in = lambda image: image #diffusers ip2p\n\n\nclass StableCascade_C(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.STABLE_CASCADE, device=None):\n        super().__init__(model_config, model_type, device=device, unet_model=StageC)\n        self.diffusion_model.eval().requires_grad_(False)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        clip_text_pooled = kwargs[\"pooled_output\"]\n        if clip_text_pooled is not None:\n            out['clip_text_pooled'] = comfy.conds.CONDRegular(clip_text_pooled)\n\n        if \"unclip_conditioning\" in kwargs:\n            embeds = []\n            for unclip_cond in kwargs[\"unclip_conditioning\"]:\n                weight = unclip_cond[\"strength\"]\n                embeds.append(unclip_cond[\"clip_vision_output\"].image_embeds.unsqueeze(0) * weight)\n            clip_img = torch.cat(embeds, dim=1)\n        else:\n            clip_img = torch.zeros((1, 1, 768))\n        out[\"clip_img\"] = comfy.conds.CONDRegular(clip_img)\n        out[\"sca\"] = comfy.conds.CONDRegular(torch.zeros((1,)))\n        out[\"crp\"] = comfy.conds.CONDRegular(torch.zeros((1,)))\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['clip_text'] = comfy.conds.CONDCrossAttn(cross_attn)\n        return out\n\n\nclass StableCascade_B(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.STABLE_CASCADE, device=None):\n        super().__init__(model_config, model_type, device=device, unet_model=StageB)\n        self.diffusion_model.eval().requires_grad_(False)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n        noise = kwargs.get(\"noise\", None)\n\n        clip_text_pooled = kwargs[\"pooled_output\"]\n        if clip_text_pooled is not None:\n            out['clip'] = comfy.conds.CONDRegular(clip_text_pooled)\n\n        #size of prior doesn't really matter if zeros because it gets resized but I still want it to get batched\n        prior = kwargs.get(\"stable_cascade_prior\", torch.zeros((1, 16, (noise.shape[2] * 4) // 42, (noise.shape[3] * 4) // 42), dtype=noise.dtype, layout=noise.layout, device=noise.device))\n\n        out[\"effnet\"] = comfy.conds.CONDRegular(prior)\n        out[\"sca\"] = comfy.conds.CONDRegular(torch.zeros((1,)))\n        return out\n\n\nclass SD3(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.FLOW, device=None):\n        super().__init__(model_config, model_type, device=device, unet_model=OpenAISignatureMMDITWrapper)\n\n    def encode_adm(self, **kwargs):\n        return kwargs[\"pooled_output\"]\n\n    def extra_conds(self, **kwargs):\n        out = super().extra_conds(**kwargs)\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            out['c_crossattn'] = comfy.conds.CONDRegular(cross_attn)\n        return out\n\n    def memory_required(self, input_shape):\n        if comfy.model_management.xformers_enabled() or comfy.model_management.pytorch_attention_flash_attention():\n            dtype = self.get_dtype()\n            if self.manual_cast_dtype is not None:\n                dtype = self.manual_cast_dtype\n            #TODO: this probably needs to be tweaked\n            area = input_shape[0] * input_shape[2] * input_shape[3]\n            return (area * comfy.model_management.dtype_size(dtype) * 0.012) * (1024 * 1024)\n        else:\n            area = input_shape[0] * input_shape[2] * input_shape[3]\n            return (area * 0.3) * (1024 * 1024)\n\n\nclass StableAudio1(BaseModel):\n    def __init__(self, model_config, seconds_start_embedder_weights, seconds_total_embedder_weights, model_type=ModelType.V_PREDICTION_CONTINUOUS, device=None):\n        super().__init__(model_config, model_type, device=device, unet_model=comfy.ldm.audio.dit.AudioDiffusionTransformer)\n        self.seconds_start_embedder = comfy.ldm.audio.embedders.NumberConditioner(768, min_val=0, max_val=512)\n        self.seconds_total_embedder = comfy.ldm.audio.embedders.NumberConditioner(768, min_val=0, max_val=512)\n        self.seconds_start_embedder.load_state_dict(seconds_start_embedder_weights)\n        self.seconds_total_embedder.load_state_dict(seconds_total_embedder_weights)\n\n    def extra_conds(self, **kwargs):\n        out = {}\n\n        noise = kwargs.get(\"noise\", None)\n        device = kwargs[\"device\"]\n\n        seconds_start = kwargs.get(\"seconds_start\", 0)\n        seconds_total = kwargs.get(\"seconds_total\", int(noise.shape[-1] / 21.53))\n\n        seconds_start_embed = self.seconds_start_embedder([seconds_start])[0].to(device)\n        seconds_total_embed = self.seconds_total_embedder([seconds_total])[0].to(device)\n\n        global_embed = torch.cat([seconds_start_embed, seconds_total_embed], dim=-1).reshape((1, -1))\n        out['global_embed'] = comfy.conds.CONDRegular(global_embed)\n\n        cross_attn = kwargs.get(\"cross_attn\", None)\n        if cross_attn is not None:\n            cross_attn = torch.cat([cross_attn.to(device), seconds_start_embed.repeat((cross_attn.shape[0], 1, 1)), seconds_total_embed.repeat((cross_attn.shape[0], 1, 1))], dim=1)\n            out['c_crossattn'] = comfy.conds.CONDRegular(cross_attn)\n        return out\n", "comfy/conds.py": "import torch\nimport math\nimport comfy.utils\n\n\ndef lcm(a, b): #TODO: eventually replace by math.lcm (added in python3.9)\n    return abs(a*b) // math.gcd(a, b)\n\nclass CONDRegular:\n    def __init__(self, cond):\n        self.cond = cond\n\n    def _copy_with(self, cond):\n        return self.__class__(cond)\n\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(comfy.utils.repeat_to_batch_size(self.cond, batch_size).to(device))\n\n    def can_concat(self, other):\n        if self.cond.shape != other.cond.shape:\n            return False\n        return True\n\n    def concat(self, others):\n        conds = [self.cond]\n        for x in others:\n            conds.append(x.cond)\n        return torch.cat(conds)\n\nclass CONDNoiseShape(CONDRegular):\n    def process_cond(self, batch_size, device, area, **kwargs):\n        data = self.cond\n        if area is not None:\n            dims = len(area) // 2\n            for i in range(dims):\n                data = data.narrow(i + 2, area[i + dims], area[i])\n\n        return self._copy_with(comfy.utils.repeat_to_batch_size(data, batch_size).to(device))\n\n\nclass CONDCrossAttn(CONDRegular):\n    def can_concat(self, other):\n        s1 = self.cond.shape\n        s2 = other.cond.shape\n        if s1 != s2:\n            if s1[0] != s2[0] or s1[2] != s2[2]: #these 2 cases should not happen\n                return False\n\n            mult_min = lcm(s1[1], s2[1])\n            diff = mult_min // min(s1[1], s2[1])\n            if diff > 4: #arbitrary limit on the padding because it's probably going to impact performance negatively if it's too much\n                return False\n        return True\n\n    def concat(self, others):\n        conds = [self.cond]\n        crossattn_max_len = self.cond.shape[1]\n        for x in others:\n            c = x.cond\n            crossattn_max_len = lcm(crossattn_max_len, c.shape[1])\n            conds.append(c)\n\n        out = []\n        for c in conds:\n            if c.shape[1] < crossattn_max_len:\n                c = c.repeat(1, crossattn_max_len // c.shape[1], 1) #padding with repeat doesn't change result\n            out.append(c)\n        return torch.cat(out)\n\nclass CONDConstant(CONDRegular):\n    def __init__(self, cond):\n        self.cond = cond\n\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(self.cond)\n\n    def can_concat(self, other):\n        if self.cond != other.cond:\n            return False\n        return True\n\n    def concat(self, others):\n        return self.cond\n", "comfy/lora.py": "import comfy.utils\nimport logging\n\nLORA_CLIP_MAP = {\n    \"mlp.fc1\": \"mlp_fc1\",\n    \"mlp.fc2\": \"mlp_fc2\",\n    \"self_attn.k_proj\": \"self_attn_k_proj\",\n    \"self_attn.q_proj\": \"self_attn_q_proj\",\n    \"self_attn.v_proj\": \"self_attn_v_proj\",\n    \"self_attn.out_proj\": \"self_attn_out_proj\",\n}\n\n\ndef load_lora(lora, to_load):\n    patch_dict = {}\n    loaded_keys = set()\n    for x in to_load:\n        alpha_name = \"{}.alpha\".format(x)\n        alpha = None\n        if alpha_name in lora.keys():\n            alpha = lora[alpha_name].item()\n            loaded_keys.add(alpha_name)\n\n        dora_scale_name = \"{}.dora_scale\".format(x)\n        dora_scale = None\n        if dora_scale_name in lora.keys():\n            dora_scale = lora[dora_scale_name]\n            loaded_keys.add(dora_scale_name)\n\n        regular_lora = \"{}.lora_up.weight\".format(x)\n        diffusers_lora = \"{}_lora.up.weight\".format(x)\n        diffusers2_lora = \"{}.lora_B.weight\".format(x)\n        diffusers3_lora = \"{}.lora.up.weight\".format(x)\n        transformers_lora = \"{}.lora_linear_layer.up.weight\".format(x)\n        A_name = None\n\n        if regular_lora in lora.keys():\n            A_name = regular_lora\n            B_name = \"{}.lora_down.weight\".format(x)\n            mid_name = \"{}.lora_mid.weight\".format(x)\n        elif diffusers_lora in lora.keys():\n            A_name = diffusers_lora\n            B_name = \"{}_lora.down.weight\".format(x)\n            mid_name = None\n        elif diffusers2_lora in lora.keys():\n            A_name = diffusers2_lora\n            B_name = \"{}.lora_A.weight\".format(x)\n            mid_name = None\n        elif diffusers3_lora in lora.keys():\n            A_name = diffusers3_lora\n            B_name = \"{}.lora.down.weight\".format(x)\n            mid_name = None\n        elif transformers_lora in lora.keys():\n            A_name = transformers_lora\n            B_name =\"{}.lora_linear_layer.down.weight\".format(x)\n            mid_name = None\n\n        if A_name is not None:\n            mid = None\n            if mid_name is not None and mid_name in lora.keys():\n                mid = lora[mid_name]\n                loaded_keys.add(mid_name)\n            patch_dict[to_load[x]] = (\"lora\", (lora[A_name], lora[B_name], alpha, mid, dora_scale))\n            loaded_keys.add(A_name)\n            loaded_keys.add(B_name)\n\n\n        ######## loha\n        hada_w1_a_name = \"{}.hada_w1_a\".format(x)\n        hada_w1_b_name = \"{}.hada_w1_b\".format(x)\n        hada_w2_a_name = \"{}.hada_w2_a\".format(x)\n        hada_w2_b_name = \"{}.hada_w2_b\".format(x)\n        hada_t1_name = \"{}.hada_t1\".format(x)\n        hada_t2_name = \"{}.hada_t2\".format(x)\n        if hada_w1_a_name in lora.keys():\n            hada_t1 = None\n            hada_t2 = None\n            if hada_t1_name in lora.keys():\n                hada_t1 = lora[hada_t1_name]\n                hada_t2 = lora[hada_t2_name]\n                loaded_keys.add(hada_t1_name)\n                loaded_keys.add(hada_t2_name)\n\n            patch_dict[to_load[x]] = (\"loha\", (lora[hada_w1_a_name], lora[hada_w1_b_name], alpha, lora[hada_w2_a_name], lora[hada_w2_b_name], hada_t1, hada_t2, dora_scale))\n            loaded_keys.add(hada_w1_a_name)\n            loaded_keys.add(hada_w1_b_name)\n            loaded_keys.add(hada_w2_a_name)\n            loaded_keys.add(hada_w2_b_name)\n\n\n        ######## lokr\n        lokr_w1_name = \"{}.lokr_w1\".format(x)\n        lokr_w2_name = \"{}.lokr_w2\".format(x)\n        lokr_w1_a_name = \"{}.lokr_w1_a\".format(x)\n        lokr_w1_b_name = \"{}.lokr_w1_b\".format(x)\n        lokr_t2_name = \"{}.lokr_t2\".format(x)\n        lokr_w2_a_name = \"{}.lokr_w2_a\".format(x)\n        lokr_w2_b_name = \"{}.lokr_w2_b\".format(x)\n\n        lokr_w1 = None\n        if lokr_w1_name in lora.keys():\n            lokr_w1 = lora[lokr_w1_name]\n            loaded_keys.add(lokr_w1_name)\n\n        lokr_w2 = None\n        if lokr_w2_name in lora.keys():\n            lokr_w2 = lora[lokr_w2_name]\n            loaded_keys.add(lokr_w2_name)\n\n        lokr_w1_a = None\n        if lokr_w1_a_name in lora.keys():\n            lokr_w1_a = lora[lokr_w1_a_name]\n            loaded_keys.add(lokr_w1_a_name)\n\n        lokr_w1_b = None\n        if lokr_w1_b_name in lora.keys():\n            lokr_w1_b = lora[lokr_w1_b_name]\n            loaded_keys.add(lokr_w1_b_name)\n\n        lokr_w2_a = None\n        if lokr_w2_a_name in lora.keys():\n            lokr_w2_a = lora[lokr_w2_a_name]\n            loaded_keys.add(lokr_w2_a_name)\n\n        lokr_w2_b = None\n        if lokr_w2_b_name in lora.keys():\n            lokr_w2_b = lora[lokr_w2_b_name]\n            loaded_keys.add(lokr_w2_b_name)\n\n        lokr_t2 = None\n        if lokr_t2_name in lora.keys():\n            lokr_t2 = lora[lokr_t2_name]\n            loaded_keys.add(lokr_t2_name)\n\n        if (lokr_w1 is not None) or (lokr_w2 is not None) or (lokr_w1_a is not None) or (lokr_w2_a is not None):\n            patch_dict[to_load[x]] = (\"lokr\", (lokr_w1, lokr_w2, alpha, lokr_w1_a, lokr_w1_b, lokr_w2_a, lokr_w2_b, lokr_t2, dora_scale))\n\n        #glora\n        a1_name = \"{}.a1.weight\".format(x)\n        a2_name = \"{}.a2.weight\".format(x)\n        b1_name = \"{}.b1.weight\".format(x)\n        b2_name = \"{}.b2.weight\".format(x)\n        if a1_name in lora:\n            patch_dict[to_load[x]] = (\"glora\", (lora[a1_name], lora[a2_name], lora[b1_name], lora[b2_name], alpha, dora_scale))\n            loaded_keys.add(a1_name)\n            loaded_keys.add(a2_name)\n            loaded_keys.add(b1_name)\n            loaded_keys.add(b2_name)\n\n        w_norm_name = \"{}.w_norm\".format(x)\n        b_norm_name = \"{}.b_norm\".format(x)\n        w_norm = lora.get(w_norm_name, None)\n        b_norm = lora.get(b_norm_name, None)\n\n        if w_norm is not None:\n            loaded_keys.add(w_norm_name)\n            patch_dict[to_load[x]] = (\"diff\", (w_norm,))\n            if b_norm is not None:\n                loaded_keys.add(b_norm_name)\n                patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (b_norm,))\n\n        diff_name = \"{}.diff\".format(x)\n        diff_weight = lora.get(diff_name, None)\n        if diff_weight is not None:\n            patch_dict[to_load[x]] = (\"diff\", (diff_weight,))\n            loaded_keys.add(diff_name)\n\n        diff_bias_name = \"{}.diff_b\".format(x)\n        diff_bias = lora.get(diff_bias_name, None)\n        if diff_bias is not None:\n            patch_dict[\"{}.bias\".format(to_load[x][:-len(\".weight\")])] = (\"diff\", (diff_bias,))\n            loaded_keys.add(diff_bias_name)\n\n    for x in lora.keys():\n        if x not in loaded_keys:\n            logging.warning(\"lora key not loaded: {}\".format(x))\n\n    return patch_dict\n\ndef model_lora_keys_clip(model, key_map={}):\n    sdk = model.state_dict().keys()\n\n    text_model_lora_key = \"lora_te_text_model_encoder_layers_{}_{}\"\n    clip_l_present = False\n    for b in range(32): #TODO: clean up\n        for c in LORA_CLIP_MAP:\n            k = \"clip_h.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                lora_key = text_model_lora_key.format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"lora_te1_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                key_map[lora_key] = k\n\n            k = \"clip_l.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                lora_key = text_model_lora_key.format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k\n                lora_key = \"lora_te1_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #SDXL base\n                key_map[lora_key] = k\n                clip_l_present = True\n                lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                key_map[lora_key] = k\n\n            k = \"clip_g.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                if clip_l_present:\n                    lora_key = \"lora_te2_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #SDXL base\n                    key_map[lora_key] = k\n                    lora_key = \"text_encoder_2.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                    key_map[lora_key] = k\n                else:\n                    lora_key = \"lora_te_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #TODO: test if this is correct for SDXL-Refiner\n                    key_map[lora_key] = k\n                    lora_key = \"text_encoder.text_model.encoder.layers.{}.{}\".format(b, c) #diffusers lora\n                    key_map[lora_key] = k\n                    lora_key = \"lora_prior_te_text_model_encoder_layers_{}_{}\".format(b, LORA_CLIP_MAP[c]) #cascade lora: TODO put lora key prefix in the model config\n                    key_map[lora_key] = k\n\n    for k in sdk: #OneTrainer SD3 lora\n        if k.startswith(\"t5xxl.transformer.\") and k.endswith(\".weight\"):\n            l_key = k[len(\"t5xxl.transformer.\"):-len(\".weight\")]\n            lora_key = \"lora_te3_{}\".format(l_key.replace(\".\", \"_\"))\n            key_map[lora_key] = k\n\n    k = \"clip_g.transformer.text_projection.weight\"\n    if k in sdk:\n        key_map[\"lora_prior_te_text_projection\"] = k #cascade lora?\n        # key_map[\"text_encoder.text_projection\"] = k #TODO: check if other lora have the text_projection too\n        key_map[\"lora_te2_text_projection\"] = k #OneTrainer SD3 lora\n\n    k = \"clip_l.transformer.text_projection.weight\"\n    if k in sdk:\n        key_map[\"lora_te1_text_projection\"] = k #OneTrainer SD3 lora, not necessary but omits warning\n\n    return key_map\n\ndef model_lora_keys_unet(model, key_map={}):\n    sd = model.state_dict()\n    sdk = sd.keys()\n\n    for k in sdk:\n        if k.startswith(\"diffusion_model.\") and k.endswith(\".weight\"):\n            key_lora = k[len(\"diffusion_model.\"):-len(\".weight\")].replace(\".\", \"_\")\n            key_map[\"lora_unet_{}\".format(key_lora)] = k\n            key_map[\"lora_prior_unet_{}\".format(key_lora)] = k #cascade lora: TODO put lora key prefix in the model config\n\n    diffusers_keys = comfy.utils.unet_to_diffusers(model.model_config.unet_config)\n    for k in diffusers_keys:\n        if k.endswith(\".weight\"):\n            unet_key = \"diffusion_model.{}\".format(diffusers_keys[k])\n            key_lora = k[:-len(\".weight\")].replace(\".\", \"_\")\n            key_map[\"lora_unet_{}\".format(key_lora)] = unet_key\n\n            diffusers_lora_prefix = [\"\", \"unet.\"]\n            for p in diffusers_lora_prefix:\n                diffusers_lora_key = \"{}{}\".format(p, k[:-len(\".weight\")].replace(\".to_\", \".processor.to_\"))\n                if diffusers_lora_key.endswith(\".to_out.0\"):\n                    diffusers_lora_key = diffusers_lora_key[:-2]\n                key_map[diffusers_lora_key] = unet_key\n\n    if isinstance(model, comfy.model_base.SD3): #Diffusers lora SD3\n        diffusers_keys = comfy.utils.mmdit_to_diffusers(model.model_config.unet_config, output_prefix=\"diffusion_model.\")\n        for k in diffusers_keys:\n            if k.endswith(\".weight\"):\n                to = diffusers_keys[k]\n                key_lora = \"transformer.{}\".format(k[:-len(\".weight\")]) #regular diffusers sd3 lora format\n                key_map[key_lora] = to\n\n                key_lora = \"base_model.model.{}\".format(k[:-len(\".weight\")]) #format for flash-sd3 lora and others?\n                key_map[key_lora] = to\n\n                key_lora = \"lora_transformer_{}\".format(k[:-len(\".weight\")].replace(\".\", \"_\")) #OneTrainer lora\n                key_map[key_lora] = to\n\n    return key_map\n", "comfy/types.py": "import torch\nfrom typing import Callable, Protocol, TypedDict, Optional, List\n\n\nclass UnetApplyFunction(Protocol):\n    \"\"\"Function signature protocol on comfy.model_base.BaseModel.apply_model\"\"\"\n\n    def __call__(self, x: torch.Tensor, t: torch.Tensor, **kwargs) -> torch.Tensor:\n        pass\n\n\nclass UnetApplyConds(TypedDict):\n    \"\"\"Optional conditions for unet apply function.\"\"\"\n\n    c_concat: Optional[torch.Tensor]\n    c_crossattn: Optional[torch.Tensor]\n    control: Optional[torch.Tensor]\n    transformer_options: Optional[dict]\n\n\nclass UnetParams(TypedDict):\n    # Tensor of shape [B, C, H, W]\n    input: torch.Tensor\n    # Tensor of shape [B]\n    timestep: torch.Tensor\n    c: UnetApplyConds\n    # List of [0, 1], [0], [1], ...\n    # 0 means conditional, 1 means conditional unconditional\n    cond_or_uncond: List[int]\n\n\nUnetWrapperFunction = Callable[[UnetApplyFunction, UnetParams], torch.Tensor]\n", "comfy/sd.py": "import torch\nfrom enum import Enum\nimport logging\n\nfrom comfy import model_management\nfrom .ldm.models.autoencoder import AutoencoderKL, AutoencodingEngine\nfrom .ldm.cascade.stage_a import StageA\nfrom .ldm.cascade.stage_c_coder import StageC_coder\nfrom .ldm.audio.autoencoder import AudioOobleckVAE\nimport yaml\n\nimport comfy.utils\n\nfrom . import clip_vision\nfrom . import gligen\nfrom . import diffusers_convert\nfrom . import model_detection\n\nfrom . import sd1_clip\nfrom . import sd2_clip\nfrom . import sdxl_clip\nfrom . import sd3_clip\nfrom . import sa_t5\n\nimport comfy.model_patcher\nimport comfy.lora\nimport comfy.t2i_adapter.adapter\nimport comfy.supported_models_base\nimport comfy.taesd.taesd\n\ndef load_model_weights(model, sd):\n    m, u = model.load_state_dict(sd, strict=False)\n    m = set(m)\n    unexpected_keys = set(u)\n\n    k = list(sd.keys())\n    for x in k:\n        if x not in unexpected_keys:\n            w = sd.pop(x)\n            del w\n    if len(m) > 0:\n        logging.warning(\"missing {}\".format(m))\n    return model\n\ndef load_clip_weights(model, sd):\n    k = list(sd.keys())\n    for x in k:\n        if x.startswith(\"cond_stage_model.transformer.\") and not x.startswith(\"cond_stage_model.transformer.text_model.\"):\n            y = x.replace(\"cond_stage_model.transformer.\", \"cond_stage_model.transformer.text_model.\")\n            sd[y] = sd.pop(x)\n\n    if 'cond_stage_model.transformer.text_model.embeddings.position_ids' in sd:\n        ids = sd['cond_stage_model.transformer.text_model.embeddings.position_ids']\n        if ids.dtype == torch.float32:\n            sd['cond_stage_model.transformer.text_model.embeddings.position_ids'] = ids.round()\n\n    sd = comfy.utils.clip_text_transformers_convert(sd, \"cond_stage_model.model.\", \"cond_stage_model.transformer.\")\n    return load_model_weights(model, sd)\n\n\ndef load_lora_for_models(model, clip, lora, strength_model, strength_clip):\n    key_map = {}\n    if model is not None:\n        key_map = comfy.lora.model_lora_keys_unet(model.model, key_map)\n    if clip is not None:\n        key_map = comfy.lora.model_lora_keys_clip(clip.cond_stage_model, key_map)\n\n    loaded = comfy.lora.load_lora(lora, key_map)\n    if model is not None:\n        new_modelpatcher = model.clone()\n        k = new_modelpatcher.add_patches(loaded, strength_model)\n    else:\n        k = ()\n        new_modelpatcher = None\n\n    if clip is not None:\n        new_clip = clip.clone()\n        k1 = new_clip.add_patches(loaded, strength_clip)\n    else:\n        k1 = ()\n        new_clip = None\n    k = set(k)\n    k1 = set(k1)\n    for x in loaded:\n        if (x not in k) and (x not in k1):\n            logging.warning(\"NOT LOADED {}\".format(x))\n\n    return (new_modelpatcher, new_clip)\n\n\nclass CLIP:\n    def __init__(self, target=None, embedding_directory=None, no_init=False):\n        if no_init:\n            return\n        params = target.params.copy()\n        clip = target.clip\n        tokenizer = target.tokenizer\n\n        load_device = model_management.text_encoder_device()\n        offload_device = model_management.text_encoder_offload_device()\n        params['device'] = offload_device\n        dtype = model_management.text_encoder_dtype(load_device)\n        params['dtype'] = dtype\n\n        self.cond_stage_model = clip(**(params))\n\n        for dt in self.cond_stage_model.dtypes:\n            if not model_management.supports_cast(load_device, dt):\n                load_device = offload_device\n\n        self.tokenizer = tokenizer(embedding_directory=embedding_directory)\n        self.patcher = comfy.model_patcher.ModelPatcher(self.cond_stage_model, load_device=load_device, offload_device=offload_device)\n        self.layer_idx = None\n        logging.debug(\"CLIP model load device: {}, offload device: {}\".format(load_device, offload_device))\n\n    def clone(self):\n        n = CLIP(no_init=True)\n        n.patcher = self.patcher.clone()\n        n.cond_stage_model = self.cond_stage_model\n        n.tokenizer = self.tokenizer\n        n.layer_idx = self.layer_idx\n        return n\n\n    def add_patches(self, patches, strength_patch=1.0, strength_model=1.0):\n        return self.patcher.add_patches(patches, strength_patch, strength_model)\n\n    def clip_layer(self, layer_idx):\n        self.layer_idx = layer_idx\n\n    def tokenize(self, text, return_word_ids=False):\n        return self.tokenizer.tokenize_with_weights(text, return_word_ids)\n\n    def encode_from_tokens(self, tokens, return_pooled=False):\n        self.cond_stage_model.reset_clip_options()\n\n        if self.layer_idx is not None:\n            self.cond_stage_model.set_clip_options({\"layer\": self.layer_idx})\n\n        if return_pooled == \"unprojected\":\n            self.cond_stage_model.set_clip_options({\"projected_pooled\": False})\n\n        self.load_model()\n        cond, pooled = self.cond_stage_model.encode_token_weights(tokens)\n        if return_pooled:\n            return cond, pooled\n        return cond\n\n    def encode(self, text):\n        tokens = self.tokenize(text)\n        return self.encode_from_tokens(tokens)\n\n    def load_sd(self, sd, full_model=False):\n        if full_model:\n            return self.cond_stage_model.load_state_dict(sd, strict=False)\n        else:\n            return self.cond_stage_model.load_sd(sd)\n\n    def get_sd(self):\n        return self.cond_stage_model.state_dict()\n\n    def load_model(self):\n        model_management.load_model_gpu(self.patcher)\n        return self.patcher\n\n    def get_key_patches(self):\n        return self.patcher.get_key_patches()\n\nclass VAE:\n    def __init__(self, sd=None, device=None, config=None, dtype=None):\n        if 'decoder.up_blocks.0.resnets.0.norm1.weight' in sd.keys(): #diffusers format\n            sd = diffusers_convert.convert_vae_state_dict(sd)\n\n        self.memory_used_encode = lambda shape, dtype: (1767 * shape[2] * shape[3]) * model_management.dtype_size(dtype) #These are for AutoencoderKL and need tweaking (should be lower)\n        self.memory_used_decode = lambda shape, dtype: (2178 * shape[2] * shape[3] * 64) * model_management.dtype_size(dtype)\n        self.downscale_ratio = 8\n        self.upscale_ratio = 8\n        self.latent_channels = 4\n        self.output_channels = 3\n        self.process_input = lambda image: image * 2.0 - 1.0\n        self.process_output = lambda image: torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0)\n        self.working_dtypes = [torch.bfloat16, torch.float32]\n\n        if config is None:\n            if \"decoder.mid.block_1.mix_factor\" in sd:\n                encoder_config = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n                decoder_config = encoder_config.copy()\n                decoder_config[\"video_kernel_size\"] = [3, 1, 1]\n                decoder_config[\"alpha\"] = 0.0\n                self.first_stage_model = AutoencodingEngine(regularizer_config={'target': \"comfy.ldm.models.autoencoder.DiagonalGaussianRegularizer\"},\n                                                            encoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Encoder\", 'params': encoder_config},\n                                                            decoder_config={'target': \"comfy.ldm.modules.temporal_ae.VideoDecoder\", 'params': decoder_config})\n            elif \"taesd_decoder.1.weight\" in sd:\n                self.latent_channels = sd[\"taesd_decoder.1.weight\"].shape[1]\n                self.first_stage_model = comfy.taesd.taesd.TAESD(latent_channels=self.latent_channels)\n            elif \"vquantizer.codebook.weight\" in sd: #VQGan: stage a of stable cascade\n                self.first_stage_model = StageA()\n                self.downscale_ratio = 4\n                self.upscale_ratio = 4\n                #TODO\n                #self.memory_used_encode\n                #self.memory_used_decode\n                self.process_input = lambda image: image\n                self.process_output = lambda image: image\n            elif \"backbone.1.0.block.0.1.num_batches_tracked\" in sd: #effnet: encoder for stage c latent of stable cascade\n                self.first_stage_model = StageC_coder()\n                self.downscale_ratio = 32\n                self.latent_channels = 16\n                new_sd = {}\n                for k in sd:\n                    new_sd[\"encoder.{}\".format(k)] = sd[k]\n                sd = new_sd\n            elif \"blocks.11.num_batches_tracked\" in sd: #previewer: decoder for stage c latent of stable cascade\n                self.first_stage_model = StageC_coder()\n                self.latent_channels = 16\n                new_sd = {}\n                for k in sd:\n                    new_sd[\"previewer.{}\".format(k)] = sd[k]\n                sd = new_sd\n            elif \"encoder.backbone.1.0.block.0.1.num_batches_tracked\" in sd: #combined effnet and previewer for stable cascade\n                self.first_stage_model = StageC_coder()\n                self.downscale_ratio = 32\n                self.latent_channels = 16\n            elif \"decoder.conv_in.weight\" in sd:\n                #default SD1.x/SD2.x VAE parameters\n                ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n\n                if 'encoder.down.2.downsample.conv.weight' not in sd and 'decoder.up.3.upsample.conv.weight' not in sd: #Stable diffusion x4 upscaler VAE\n                    ddconfig['ch_mult'] = [1, 2, 4]\n                    self.downscale_ratio = 4\n                    self.upscale_ratio = 4\n\n                self.latent_channels = ddconfig['z_channels'] = sd[\"decoder.conv_in.weight\"].shape[1]\n                if 'quant_conv.weight' in sd:\n                    self.first_stage_model = AutoencoderKL(ddconfig=ddconfig, embed_dim=4)\n                else:\n                    self.first_stage_model = AutoencodingEngine(regularizer_config={'target': \"comfy.ldm.models.autoencoder.DiagonalGaussianRegularizer\"},\n                                                                encoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Encoder\", 'params': ddconfig},\n                                                                decoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Decoder\", 'params': ddconfig})\n            elif \"decoder.layers.0.weight_v\" in sd:\n                self.first_stage_model = AudioOobleckVAE()\n                self.memory_used_encode = lambda shape, dtype: (1000 * shape[2]) * model_management.dtype_size(dtype)\n                self.memory_used_decode = lambda shape, dtype: (1000 * shape[2] * 2048) * model_management.dtype_size(dtype)\n                self.latent_channels = 64\n                self.output_channels = 2\n                self.upscale_ratio = 2048\n                self.downscale_ratio =  2048\n                self.process_output = lambda audio: audio\n                self.process_input = lambda audio: audio\n                self.working_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n            else:\n                logging.warning(\"WARNING: No VAE weights detected, VAE not initalized.\")\n                self.first_stage_model = None\n                return\n        else:\n            self.first_stage_model = AutoencoderKL(**(config['params']))\n        self.first_stage_model = self.first_stage_model.eval()\n\n        m, u = self.first_stage_model.load_state_dict(sd, strict=False)\n        if len(m) > 0:\n            logging.warning(\"Missing VAE keys {}\".format(m))\n\n        if len(u) > 0:\n            logging.debug(\"Leftover VAE keys {}\".format(u))\n\n        if device is None:\n            device = model_management.vae_device()\n        self.device = device\n        offload_device = model_management.vae_offload_device()\n        if dtype is None:\n            dtype = model_management.vae_dtype(self.device, self.working_dtypes)\n        self.vae_dtype = dtype\n        self.first_stage_model.to(self.vae_dtype)\n        self.output_device = model_management.intermediate_device()\n\n        self.patcher = comfy.model_patcher.ModelPatcher(self.first_stage_model, load_device=self.device, offload_device=offload_device)\n        logging.debug(\"VAE load device: {}, offload device: {}, dtype: {}\".format(self.device, offload_device, self.vae_dtype))\n\n    def vae_encode_crop_pixels(self, pixels):\n        dims = pixels.shape[1:-1]\n        for d in range(len(dims)):\n            x = (dims[d] // self.downscale_ratio) * self.downscale_ratio\n            x_offset = (dims[d] % self.downscale_ratio) // 2\n            if x != dims[d]:\n                pixels = pixels.narrow(d + 1, x_offset, x)\n        return pixels\n\n    def decode_tiled_(self, samples, tile_x=64, tile_y=64, overlap = 16):\n        steps = samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x, tile_y, overlap)\n        steps += samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n        steps += samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n        pbar = comfy.utils.ProgressBar(steps)\n\n        decode_fn = lambda a: self.first_stage_model.decode(a.to(self.vae_dtype).to(self.device)).float()\n        output = self.process_output(\n            (comfy.utils.tiled_scale(samples, decode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar) +\n            comfy.utils.tiled_scale(samples, decode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar) +\n             comfy.utils.tiled_scale(samples, decode_fn, tile_x, tile_y, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar))\n            / 3.0)\n        return output\n\n    def decode_tiled_1d(self, samples, tile_x=128, overlap=32):\n        decode_fn = lambda a: self.first_stage_model.decode(a.to(self.vae_dtype).to(self.device)).float()\n        return comfy.utils.tiled_scale_multidim(samples, decode_fn, tile=(tile_x,), overlap=overlap, upscale_amount=self.upscale_ratio, out_channels=self.output_channels, output_device=self.output_device)\n\n    def encode_tiled_(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n        steps = pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x, tile_y, overlap)\n        steps += pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n        steps += pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n        pbar = comfy.utils.ProgressBar(steps)\n\n        encode_fn = lambda a: self.first_stage_model.encode((self.process_input(a)).to(self.vae_dtype).to(self.device)).float()\n        samples = comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x, tile_y, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples += comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples += comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n        samples /= 3.0\n        return samples\n\n    def encode_tiled_1d(self, samples, tile_x=128 * 2048, overlap=32 * 2048):\n        encode_fn = lambda a: self.first_stage_model.encode((self.process_input(a)).to(self.vae_dtype).to(self.device)).float()\n        return comfy.utils.tiled_scale_multidim(samples, encode_fn, tile=(tile_x,), overlap=overlap, upscale_amount=(1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device)\n\n    def decode(self, samples_in):\n        try:\n            memory_used = self.memory_used_decode(samples_in.shape, self.vae_dtype)\n            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n            free_memory = model_management.get_free_memory(self.device)\n            batch_number = int(free_memory / memory_used)\n            batch_number = max(1, batch_number)\n\n            pixel_samples = torch.empty((samples_in.shape[0], self.output_channels) + tuple(map(lambda a: a * self.upscale_ratio, samples_in.shape[2:])), device=self.output_device)\n            for x in range(0, samples_in.shape[0], batch_number):\n                samples = samples_in[x:x+batch_number].to(self.vae_dtype).to(self.device)\n                pixel_samples[x:x+batch_number] = self.process_output(self.first_stage_model.decode(samples).to(self.output_device).float())\n        except model_management.OOM_EXCEPTION as e:\n            logging.warning(\"Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.\")\n            if len(samples_in.shape) == 3:\n                pixel_samples = self.decode_tiled_1d(samples_in)\n            else:\n                pixel_samples = self.decode_tiled_(samples_in)\n\n        pixel_samples = pixel_samples.to(self.output_device).movedim(1,-1)\n        return pixel_samples\n\n    def decode_tiled(self, samples, tile_x=64, tile_y=64, overlap = 16):\n        model_management.load_model_gpu(self.patcher)\n        output = self.decode_tiled_(samples, tile_x, tile_y, overlap)\n        return output.movedim(1,-1)\n\n    def encode(self, pixel_samples):\n        pixel_samples = self.vae_encode_crop_pixels(pixel_samples)\n        pixel_samples = pixel_samples.movedim(-1,1)\n        try:\n            memory_used = self.memory_used_encode(pixel_samples.shape, self.vae_dtype)\n            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n            free_memory = model_management.get_free_memory(self.device)\n            batch_number = int(free_memory / memory_used)\n            batch_number = max(1, batch_number)\n            samples = torch.empty((pixel_samples.shape[0], self.latent_channels) + tuple(map(lambda a: a // self.downscale_ratio, pixel_samples.shape[2:])), device=self.output_device)\n            for x in range(0, pixel_samples.shape[0], batch_number):\n                pixels_in = self.process_input(pixel_samples[x:x+batch_number]).to(self.vae_dtype).to(self.device)\n                samples[x:x+batch_number] = self.first_stage_model.encode(pixels_in).to(self.output_device).float()\n\n        except model_management.OOM_EXCEPTION as e:\n            logging.warning(\"Warning: Ran out of memory when regular VAE encoding, retrying with tiled VAE encoding.\")\n            if len(pixel_samples.shape) == 3:\n                samples = self.encode_tiled_1d(pixel_samples)\n            else:\n                samples = self.encode_tiled_(pixel_samples)\n\n        return samples\n\n    def encode_tiled(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n        pixel_samples = self.vae_encode_crop_pixels(pixel_samples)\n        model_management.load_model_gpu(self.patcher)\n        pixel_samples = pixel_samples.movedim(-1,1)\n        samples = self.encode_tiled_(pixel_samples, tile_x=tile_x, tile_y=tile_y, overlap=overlap)\n        return samples\n\n    def get_sd(self):\n        return self.first_stage_model.state_dict()\n\nclass StyleModel:\n    def __init__(self, model, device=\"cpu\"):\n        self.model = model\n\n    def get_cond(self, input):\n        return self.model(input.last_hidden_state)\n\n\ndef load_style_model(ckpt_path):\n    model_data = comfy.utils.load_torch_file(ckpt_path, safe_load=True)\n    keys = model_data.keys()\n    if \"style_embedding\" in keys:\n        model = comfy.t2i_adapter.adapter.StyleAdapter(width=1024, context_dim=768, num_head=8, n_layes=3, num_token=8)\n    else:\n        raise Exception(\"invalid style model {}\".format(ckpt_path))\n    model.load_state_dict(model_data)\n    return StyleModel(model)\n\nclass CLIPType(Enum):\n    STABLE_DIFFUSION = 1\n    STABLE_CASCADE = 2\n    SD3 = 3\n    STABLE_AUDIO = 4\n\ndef load_clip(ckpt_paths, embedding_directory=None, clip_type=CLIPType.STABLE_DIFFUSION):\n    clip_data = []\n    for p in ckpt_paths:\n        clip_data.append(comfy.utils.load_torch_file(p, safe_load=True))\n\n    class EmptyClass:\n        pass\n\n    for i in range(len(clip_data)):\n        if \"transformer.resblocks.0.ln_1.weight\" in clip_data[i]:\n            clip_data[i] = comfy.utils.clip_text_transformers_convert(clip_data[i], \"\", \"\")\n        else:\n            if \"text_projection\" in clip_data[i]:\n                clip_data[i][\"text_projection.weight\"] = clip_data[i][\"text_projection\"].transpose(0, 1) #old models saved with the CLIPSave node\n\n    clip_target = EmptyClass()\n    clip_target.params = {}\n    if len(clip_data) == 1:\n        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in clip_data[0]:\n            if clip_type == CLIPType.STABLE_CASCADE:\n                clip_target.clip = sdxl_clip.StableCascadeClipModel\n                clip_target.tokenizer = sdxl_clip.StableCascadeTokenizer\n            else:\n                clip_target.clip = sdxl_clip.SDXLRefinerClipModel\n                clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n        elif \"text_model.encoder.layers.22.mlp.fc1.weight\" in clip_data[0]:\n            clip_target.clip = sd2_clip.SD2ClipModel\n            clip_target.tokenizer = sd2_clip.SD2Tokenizer\n        elif \"encoder.block.23.layer.1.DenseReluDense.wi_1.weight\" in clip_data[0]:\n            dtype_t5 = clip_data[0][\"encoder.block.23.layer.1.DenseReluDense.wi_1.weight\"].dtype\n            clip_target.clip = sd3_clip.sd3_clip(clip_l=False, clip_g=False, t5=True, dtype_t5=dtype_t5)\n            clip_target.tokenizer = sd3_clip.SD3Tokenizer\n        elif \"encoder.block.0.layer.0.SelfAttention.k.weight\" in clip_data[0]:\n            clip_target.clip = sa_t5.SAT5Model\n            clip_target.tokenizer = sa_t5.SAT5Tokenizer\n        else:\n            clip_target.clip = sd1_clip.SD1ClipModel\n            clip_target.tokenizer = sd1_clip.SD1Tokenizer\n    elif len(clip_data) == 2:\n        if clip_type == CLIPType.SD3:\n            clip_target.clip = sd3_clip.sd3_clip(clip_l=True, clip_g=True, t5=False)\n            clip_target.tokenizer = sd3_clip.SD3Tokenizer\n        else:\n            clip_target.clip = sdxl_clip.SDXLClipModel\n            clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n    elif len(clip_data) == 3:\n        clip_target.clip = sd3_clip.SD3ClipModel\n        clip_target.tokenizer = sd3_clip.SD3Tokenizer\n\n    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n    for c in clip_data:\n        m, u = clip.load_sd(c)\n        if len(m) > 0:\n            logging.warning(\"clip missing: {}\".format(m))\n\n        if len(u) > 0:\n            logging.debug(\"clip unexpected: {}\".format(u))\n    return clip\n\ndef load_gligen(ckpt_path):\n    data = comfy.utils.load_torch_file(ckpt_path, safe_load=True)\n    model = gligen.load_gligen(data)\n    if model_management.should_use_fp16():\n        model = model.half()\n    return comfy.model_patcher.ModelPatcher(model, load_device=model_management.get_torch_device(), offload_device=model_management.unet_offload_device())\n\ndef load_checkpoint(config_path=None, ckpt_path=None, output_vae=True, output_clip=True, embedding_directory=None, state_dict=None, config=None):\n    logging.warning(\"Warning: The load checkpoint with config function is deprecated and will eventually be removed, please use the other one.\")\n    model, clip, vae, _ = load_checkpoint_guess_config(ckpt_path, output_vae=output_vae, output_clip=output_clip, output_clipvision=False, embedding_directory=embedding_directory, output_model=True)\n    #TODO: this function is a mess and should be removed eventually\n    if config is None:\n        with open(config_path, 'r') as stream:\n            config = yaml.safe_load(stream)\n    model_config_params = config['model']['params']\n    clip_config = model_config_params['cond_stage_config']\n    scale_factor = model_config_params['scale_factor']\n\n    if \"parameterization\" in model_config_params:\n        if model_config_params[\"parameterization\"] == \"v\":\n            m = model.clone()\n            class ModelSamplingAdvanced(comfy.model_sampling.ModelSamplingDiscrete, comfy.model_sampling.V_PREDICTION):\n                pass\n            m.add_object_patch(\"model_sampling\", ModelSamplingAdvanced(model.model.model_config))\n            model = m\n\n    layer_idx = clip_config.get(\"params\", {}).get(\"layer_idx\", None)\n    if layer_idx is not None:\n        clip.clip_layer(layer_idx)\n\n    return (model, clip, vae)\n\ndef load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=False, embedding_directory=None, output_model=True):\n    sd = comfy.utils.load_torch_file(ckpt_path)\n    sd_keys = sd.keys()\n    clip = None\n    clipvision = None\n    vae = None\n    model = None\n    model_patcher = None\n    clip_target = None\n\n    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n    parameters = comfy.utils.calculate_parameters(sd, diffusion_model_prefix)\n    load_device = model_management.get_torch_device()\n\n    model_config = model_detection.model_config_from_unet(sd, diffusion_model_prefix)\n    unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n\n    if model_config is None:\n        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(ckpt_path))\n\n    if model_config.clip_vision_prefix is not None:\n        if output_clipvision:\n            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n\n    if output_model:\n        inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n        offload_device = model_management.unet_offload_device()\n        model = model_config.get_model(sd, diffusion_model_prefix, device=inital_load_device)\n        model.load_model_weights(sd, diffusion_model_prefix)\n\n    if output_vae:\n        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n        vae_sd = model_config.process_vae_state_dict(vae_sd)\n        vae = VAE(sd=vae_sd)\n\n    if output_clip:\n        clip_target = model_config.clip_target(state_dict=sd)\n        if clip_target is not None:\n            clip_sd = model_config.process_clip_state_dict(sd)\n            if len(clip_sd) > 0:\n                clip = CLIP(clip_target, embedding_directory=embedding_directory)\n                m, u = clip.load_sd(clip_sd, full_model=True)\n                if len(m) > 0:\n                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n                    if len(m_filter) > 0:\n                        logging.warning(\"clip missing: {}\".format(m))\n                    else:\n                        logging.debug(\"clip missing: {}\".format(m))\n\n                if len(u) > 0:\n                    logging.debug(\"clip unexpected {}:\".format(u))\n            else:\n                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n\n    left_over = sd.keys()\n    if len(left_over) > 0:\n        logging.debug(\"left over keys: {}\".format(left_over))\n\n    if output_model:\n        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n        if inital_load_device != torch.device(\"cpu\"):\n            logging.info(\"loaded straight to GPU\")\n            model_management.load_model_gpu(model_patcher)\n\n    return (model_patcher, clip, vae, clipvision)\n\n\ndef load_unet_state_dict(sd): #load unet in diffusers format\n    parameters = comfy.utils.calculate_parameters(sd)\n    unet_dtype = model_management.unet_dtype(model_params=parameters)\n    load_device = model_management.get_torch_device()\n\n    if 'transformer_blocks.0.attn.add_q_proj.weight' in sd: #MMDIT SD3\n        new_sd = model_detection.convert_diffusers_mmdit(sd, \"\")\n        if new_sd is None:\n            return None\n        model_config = model_detection.model_config_from_unet(new_sd, \"\")\n        if model_config is None:\n            return None\n    elif \"input_blocks.0.0.weight\" in sd or 'clf.1.weight' in sd: #ldm or stable cascade\n        model_config = model_detection.model_config_from_unet(sd, \"\")\n        if model_config is None:\n            return None\n        new_sd = sd\n\n    else: #diffusers\n        model_config = model_detection.model_config_from_diffusers_unet(sd)\n        if model_config is None:\n            return None\n\n        diffusers_keys = comfy.utils.unet_to_diffusers(model_config.unet_config)\n\n        new_sd = {}\n        for k in diffusers_keys:\n            if k in sd:\n                new_sd[diffusers_keys[k]] = sd.pop(k)\n            else:\n                logging.warning(\"{} {}\".format(diffusers_keys[k], k))\n\n    offload_device = model_management.unet_offload_device()\n    unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n    model = model_config.get_model(new_sd, \"\")\n    model = model.to(offload_device)\n    model.load_model_weights(new_sd, \"\")\n    left_over = sd.keys()\n    if len(left_over) > 0:\n        logging.info(\"left over keys in unet: {}\".format(left_over))\n    return comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device)\n\ndef load_unet(unet_path):\n    sd = comfy.utils.load_torch_file(unet_path)\n    model = load_unet_state_dict(sd)\n    if model is None:\n        logging.error(\"ERROR UNSUPPORTED UNET {}\".format(unet_path))\n        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n    return model\n\ndef save_checkpoint(output_path, model, clip=None, vae=None, clip_vision=None, metadata=None, extra_keys={}):\n    clip_sd = None\n    load_models = [model]\n    if clip is not None:\n        load_models.append(clip.load_model())\n        clip_sd = clip.get_sd()\n\n    model_management.load_models_gpu(load_models, force_patch_weights=True)\n    clip_vision_sd = clip_vision.get_sd() if clip_vision is not None else None\n    sd = model.model.state_dict_for_saving(clip_sd, vae.get_sd(), clip_vision_sd)\n    for k in extra_keys:\n        sd[k] = extra_keys[k]\n\n    comfy.utils.save_torch_file(sd, output_path, metadata=metadata)\n", "comfy/sa_t5.py": "from comfy import sd1_clip\nfrom transformers import T5TokenizerFast\nimport comfy.t5\nimport os\n\nclass T5BaseModel(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", layer=\"last\", layer_idx=None, dtype=None):\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"t5_config_base.json\")\n        super().__init__(device=device, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, dtype=dtype, special_tokens={\"end\": 1, \"pad\": 0}, model_class=comfy.t5.T5, enable_attention_masks=True, zero_out_masked=True)\n\nclass T5BaseTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, embedding_directory=None):\n        tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"t5_tokenizer\")\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_size=768, embedding_key='t5base', tokenizer_class=T5TokenizerFast, has_start_token=False, pad_to_max_length=False, max_length=99999999, min_length=128)\n\nclass SAT5Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"t5base\", tokenizer=T5BaseTokenizer)\n\nclass SAT5Model(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"t5base\", clip_model=T5BaseModel, **kwargs)\n", "comfy/model_management.py": "import psutil\nimport logging\nfrom enum import Enum\nfrom comfy.cli_args import args\nimport torch\nimport sys\nimport platform\n\nclass VRAMState(Enum):\n    DISABLED = 0    #No vram present: no need to move models to vram\n    NO_VRAM = 1     #Very low vram: enable all the options to save vram\n    LOW_VRAM = 2\n    NORMAL_VRAM = 3\n    HIGH_VRAM = 4\n    SHARED = 5      #No dedicated vram: memory shared between CPU and GPU but models still need to be moved between both.\n\nclass CPUState(Enum):\n    GPU = 0\n    CPU = 1\n    MPS = 2\n\n# Determine VRAM State\nvram_state = VRAMState.NORMAL_VRAM\nset_vram_to = VRAMState.NORMAL_VRAM\ncpu_state = CPUState.GPU\n\ntotal_vram = 0\n\nlowvram_available = True\nxpu_available = False\n\nif args.deterministic:\n    logging.info(\"Using deterministic algorithms for pytorch\")\n    torch.use_deterministic_algorithms(True, warn_only=True)\n\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()\n    else:\n        directml_device = torch_directml.device(device_index)\n    logging.info(\"Using directml with device: {}\".format(torch_directml.device_name(device_index)))\n    # torch_directml.disable_tiled_resources(True)\n    lowvram_available = False #TODO: need to find a way to get free memory in directml before this can be enabled by default.\n\ntry:\n    import intel_extension_for_pytorch as ipex\n    if torch.xpu.is_available():\n        xpu_available = True\nexcept:\n    pass\n\ntry:\n    if torch.backends.mps.is_available():\n        cpu_state = CPUState.MPS\n        import torch.mps\nexcept:\n    pass\n\nif args.cpu:\n    cpu_state = CPUState.CPU\n\ndef is_intel_xpu():\n    global cpu_state\n    global xpu_available\n    if cpu_state == CPUState.GPU:\n        if xpu_available:\n            return True\n    return False\n\ndef get_torch_device():\n    global directml_enabled\n    global cpu_state\n    if directml_enabled:\n        global directml_device\n        return directml_device\n    if cpu_state == CPUState.MPS:\n        return torch.device(\"mps\")\n    if cpu_state == CPUState.CPU:\n        return torch.device(\"cpu\")\n    else:\n        if is_intel_xpu():\n            return torch.device(\"xpu\", torch.xpu.current_device())\n        else:\n            return torch.device(torch.cuda.current_device())\n\ndef get_total_memory(dev=None, torch_total_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_total = psutil.virtual_memory().total\n        mem_total_torch = mem_total\n    else:\n        if directml_enabled:\n            mem_total = 1024 * 1024 * 1024 #TODO\n            mem_total_torch = mem_total\n        elif is_intel_xpu():\n            stats = torch.xpu.memory_stats(dev)\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_total_torch = mem_reserved\n            mem_total = torch.xpu.get_device_properties(dev).total_memory\n        else:\n            stats = torch.cuda.memory_stats(dev)\n            mem_reserved = stats['reserved_bytes.all.current']\n            _, mem_total_cuda = torch.cuda.mem_get_info(dev)\n            mem_total_torch = mem_reserved\n            mem_total = mem_total_cuda\n\n    if torch_total_too:\n        return (mem_total, mem_total_torch)\n    else:\n        return mem_total\n\ntotal_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\ntotal_ram = psutil.virtual_memory().total / (1024 * 1024)\nlogging.info(\"Total VRAM {:0.0f} MB, total RAM {:0.0f} MB\".format(total_vram, total_ram))\n\ntry:\n    logging.info(\"pytorch version: {}\".format(torch.version.__version__))\nexcept:\n    pass\n\ntry:\n    OOM_EXCEPTION = torch.cuda.OutOfMemoryError\nexcept:\n    OOM_EXCEPTION = Exception\n\nXFORMERS_VERSION = \"\"\nXFORMERS_ENABLED_VAE = True\nif args.disable_xformers:\n    XFORMERS_IS_AVAILABLE = False\nelse:\n    try:\n        import xformers\n        import xformers.ops\n        XFORMERS_IS_AVAILABLE = True\n        try:\n            XFORMERS_IS_AVAILABLE = xformers._has_cpp_library\n        except:\n            pass\n        try:\n            XFORMERS_VERSION = xformers.version.__version__\n            logging.info(\"xformers version: {}\".format(XFORMERS_VERSION))\n            if XFORMERS_VERSION.startswith(\"0.0.18\"):\n                logging.warning(\"\\nWARNING: This version of xformers has a major bug where you will get black images when generating high resolution images.\")\n                logging.warning(\"Please downgrade or upgrade xformers to a different version.\\n\")\n                XFORMERS_ENABLED_VAE = False\n        except:\n            pass\n    except:\n        XFORMERS_IS_AVAILABLE = False\n\ndef is_nvidia():\n    global cpu_state\n    if cpu_state == CPUState.GPU:\n        if torch.version.cuda:\n            return True\n    return False\n\nENABLE_PYTORCH_ATTENTION = False\nif args.use_pytorch_cross_attention:\n    ENABLE_PYTORCH_ATTENTION = True\n    XFORMERS_IS_AVAILABLE = False\n\nVAE_DTYPES = [torch.float32]\n\ntry:\n    if is_nvidia():\n        torch_version = torch.version.__version__\n        if int(torch_version[0]) >= 2:\n            if ENABLE_PYTORCH_ATTENTION == False and args.use_split_cross_attention == False and args.use_quad_cross_attention == False:\n                ENABLE_PYTORCH_ATTENTION = True\n            if torch.cuda.is_bf16_supported() and torch.cuda.get_device_properties(torch.cuda.current_device()).major >= 8:\n                VAE_DTYPES = [torch.bfloat16] + VAE_DTYPES\n    if is_intel_xpu():\n        if args.use_split_cross_attention == False and args.use_quad_cross_attention == False:\n            ENABLE_PYTORCH_ATTENTION = True\nexcept:\n    pass\n\nif is_intel_xpu():\n    VAE_DTYPES = [torch.bfloat16] + VAE_DTYPES\n\nif args.cpu_vae:\n    VAE_DTYPES = [torch.float32]\n\n\nif ENABLE_PYTORCH_ATTENTION:\n    torch.backends.cuda.enable_math_sdp(True)\n    torch.backends.cuda.enable_flash_sdp(True)\n    torch.backends.cuda.enable_mem_efficient_sdp(True)\n\nif args.lowvram:\n    set_vram_to = VRAMState.LOW_VRAM\n    lowvram_available = True\nelif args.novram:\n    set_vram_to = VRAMState.NO_VRAM\nelif args.highvram or args.gpu_only:\n    vram_state = VRAMState.HIGH_VRAM\n\nFORCE_FP32 = False\nFORCE_FP16 = False\nif args.force_fp32:\n    logging.info(\"Forcing FP32, if this improves things please report it.\")\n    FORCE_FP32 = True\n\nif args.force_fp16:\n    logging.info(\"Forcing FP16.\")\n    FORCE_FP16 = True\n\nif lowvram_available:\n    if set_vram_to in (VRAMState.LOW_VRAM, VRAMState.NO_VRAM):\n        vram_state = set_vram_to\n\n\nif cpu_state != CPUState.GPU:\n    vram_state = VRAMState.DISABLED\n\nif cpu_state == CPUState.MPS:\n    vram_state = VRAMState.SHARED\n\nlogging.info(f\"Set vram state to: {vram_state.name}\")\n\nDISABLE_SMART_MEMORY = args.disable_smart_memory\n\nif DISABLE_SMART_MEMORY:\n    logging.info(\"Disabling smart memory management\")\n\ndef get_torch_device_name(device):\n    if hasattr(device, 'type'):\n        if device.type == \"cuda\":\n            try:\n                allocator_backend = torch.cuda.get_allocator_backend()\n            except:\n                allocator_backend = \"\"\n            return \"{} {} : {}\".format(device, torch.cuda.get_device_name(device), allocator_backend)\n        else:\n            return \"{}\".format(device.type)\n    elif is_intel_xpu():\n        return \"{} {}\".format(device, torch.xpu.get_device_name(device))\n    else:\n        return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n\ntry:\n    logging.info(\"Device: {}\".format(get_torch_device_name(get_torch_device())))\nexcept:\n    logging.warning(\"Could not pick default device.\")\n\n\ncurrent_loaded_models = []\n\ndef module_size(module):\n    module_mem = 0\n    sd = module.state_dict()\n    for k in sd:\n        t = sd[k]\n        module_mem += t.nelement() * t.element_size()\n    return module_mem\n\nclass LoadedModel:\n    def __init__(self, model):\n        self.model = model\n        self.device = model.load_device\n        self.weights_loaded = False\n        self.real_model = None\n        self.currently_used = True\n\n    def model_memory(self):\n        return self.model.model_size()\n\n    def model_memory_required(self, device):\n        if device == self.model.current_device:\n            return 0\n        else:\n            return self.model_memory()\n\n    def model_load(self, lowvram_model_memory=0, force_patch_weights=False):\n        patch_model_to = self.device\n\n        self.model.model_patches_to(self.device)\n        self.model.model_patches_to(self.model.model_dtype())\n\n        load_weights = not self.weights_loaded\n\n        try:\n            if lowvram_model_memory > 0 and load_weights:\n                self.real_model = self.model.patch_model_lowvram(device_to=patch_model_to, lowvram_model_memory=lowvram_model_memory, force_patch_weights=force_patch_weights)\n            else:\n                self.real_model = self.model.patch_model(device_to=patch_model_to, patch_weights=load_weights)\n        except Exception as e:\n            self.model.unpatch_model(self.model.offload_device)\n            self.model_unload()\n            raise e\n\n        if is_intel_xpu() and not args.disable_ipex_optimize:\n            self.real_model = ipex.optimize(self.real_model.eval(), graph_mode=True, concat_linear=True)\n\n        self.weights_loaded = True\n        return self.real_model\n\n    def should_reload_model(self, force_patch_weights=False):\n        if force_patch_weights and self.model.lowvram_patch_counter > 0:\n            return True\n        return False\n\n    def model_unload(self, unpatch_weights=True):\n        self.model.unpatch_model(self.model.offload_device, unpatch_weights=unpatch_weights)\n        self.model.model_patches_to(self.model.offload_device)\n        self.weights_loaded = self.weights_loaded and not unpatch_weights\n        self.real_model = None\n\n    def __eq__(self, other):\n        return self.model is other.model\n\ndef minimum_inference_memory():\n    return (1024 * 1024 * 1024)\n\ndef unload_model_clones(model, unload_weights_only=True, force_unload=True):\n    to_unload = []\n    for i in range(len(current_loaded_models)):\n        if model.is_clone(current_loaded_models[i].model):\n            to_unload = [i] + to_unload\n\n    if len(to_unload) == 0:\n        return True\n\n    same_weights = 0\n    for i in to_unload:\n        if model.clone_has_same_weights(current_loaded_models[i].model):\n            same_weights += 1\n\n    if same_weights == len(to_unload):\n        unload_weight = False\n    else:\n        unload_weight = True\n\n    if not force_unload:\n        if unload_weights_only and unload_weight == False:\n            return None\n\n    for i in to_unload:\n        logging.debug(\"unload clone {} {}\".format(i, unload_weight))\n        current_loaded_models.pop(i).model_unload(unpatch_weights=unload_weight)\n\n    return unload_weight\n\ndef free_memory(memory_required, device, keep_loaded=[]):\n    unloaded_model = []\n    can_unload = []\n\n    for i in range(len(current_loaded_models) -1, -1, -1):\n        shift_model = current_loaded_models[i]\n        if shift_model.device == device:\n            if shift_model not in keep_loaded:\n                can_unload.append((sys.getrefcount(shift_model.model), shift_model.model_memory(), i))\n                shift_model.currently_used = False\n\n    for x in sorted(can_unload):\n        i = x[-1]\n        if not DISABLE_SMART_MEMORY:\n            if get_free_memory(device) > memory_required:\n                break\n        current_loaded_models[i].model_unload()\n        unloaded_model.append(i)\n\n    for i in sorted(unloaded_model, reverse=True):\n        current_loaded_models.pop(i)\n\n    if len(unloaded_model) > 0:\n        soft_empty_cache()\n    else:\n        if vram_state != VRAMState.HIGH_VRAM:\n            mem_free_total, mem_free_torch = get_free_memory(device, torch_free_too=True)\n            if mem_free_torch > mem_free_total * 0.25:\n                soft_empty_cache()\n\ndef load_models_gpu(models, memory_required=0, force_patch_weights=False):\n    global vram_state\n\n    inference_memory = minimum_inference_memory()\n    extra_mem = max(inference_memory, memory_required)\n\n    models = set(models)\n\n    models_to_load = []\n    models_already_loaded = []\n    for x in models:\n        loaded_model = LoadedModel(x)\n        loaded = None\n\n        try:\n            loaded_model_index = current_loaded_models.index(loaded_model)\n        except:\n            loaded_model_index = None\n\n        if loaded_model_index is not None:\n            loaded = current_loaded_models[loaded_model_index]\n            if loaded.should_reload_model(force_patch_weights=force_patch_weights): #TODO: cleanup this model reload logic\n                current_loaded_models.pop(loaded_model_index).model_unload(unpatch_weights=True)\n                loaded = None\n            else:\n                loaded.currently_used = True\n                models_already_loaded.append(loaded)\n\n        if loaded is None:\n            if hasattr(x, \"model\"):\n                logging.info(f\"Requested to load {x.model.__class__.__name__}\")\n            models_to_load.append(loaded_model)\n\n    if len(models_to_load) == 0:\n        devs = set(map(lambda a: a.device, models_already_loaded))\n        for d in devs:\n            if d != torch.device(\"cpu\"):\n                free_memory(extra_mem, d, models_already_loaded)\n        return\n\n    logging.info(f\"Loading {len(models_to_load)} new model{'s' if len(models_to_load) > 1 else ''}\")\n\n    total_memory_required = {}\n    for loaded_model in models_to_load:\n        if unload_model_clones(loaded_model.model, unload_weights_only=True, force_unload=False) == True:#unload clones where the weights are different\n            total_memory_required[loaded_model.device] = total_memory_required.get(loaded_model.device, 0) + loaded_model.model_memory_required(loaded_model.device)\n\n    for device in total_memory_required:\n        if device != torch.device(\"cpu\"):\n            free_memory(total_memory_required[device] * 1.3 + extra_mem, device, models_already_loaded)\n\n    for loaded_model in models_to_load:\n        weights_unloaded = unload_model_clones(loaded_model.model, unload_weights_only=False, force_unload=False) #unload the rest of the clones where the weights can stay loaded\n        if weights_unloaded is not None:\n            loaded_model.weights_loaded = not weights_unloaded\n\n    for loaded_model in models_to_load:\n        model = loaded_model.model\n        torch_dev = model.load_device\n        if is_device_cpu(torch_dev):\n            vram_set_state = VRAMState.DISABLED\n        else:\n            vram_set_state = vram_state\n        lowvram_model_memory = 0\n        if lowvram_available and (vram_set_state == VRAMState.LOW_VRAM or vram_set_state == VRAMState.NORMAL_VRAM):\n            model_size = loaded_model.model_memory_required(torch_dev)\n            current_free_mem = get_free_memory(torch_dev)\n            lowvram_model_memory = int(max(64 * (1024 * 1024), (current_free_mem - 1024 * (1024 * 1024)) / 1.3 ))\n            if model_size <= (current_free_mem - inference_memory): #only switch to lowvram if really necessary\n                lowvram_model_memory = 0\n\n        if vram_set_state == VRAMState.NO_VRAM:\n            lowvram_model_memory = 64 * 1024 * 1024\n\n        cur_loaded_model = loaded_model.model_load(lowvram_model_memory, force_patch_weights=force_patch_weights)\n        current_loaded_models.insert(0, loaded_model)\n    return\n\n\ndef load_model_gpu(model):\n    return load_models_gpu([model])\n\ndef loaded_models(only_currently_used=False):\n    output = []\n    for m in current_loaded_models:\n        if only_currently_used:\n            if not m.currently_used:\n                continue\n\n        output.append(m.model)\n    return output\n\ndef cleanup_models(keep_clone_weights_loaded=False):\n    to_delete = []\n    for i in range(len(current_loaded_models)):\n        if sys.getrefcount(current_loaded_models[i].model) <= 2:\n            if not keep_clone_weights_loaded:\n                to_delete = [i] + to_delete\n            #TODO: find a less fragile way to do this.\n            elif sys.getrefcount(current_loaded_models[i].real_model) <= 3: #references from .real_model + the .model\n                to_delete = [i] + to_delete\n\n    for i in to_delete:\n        x = current_loaded_models.pop(i)\n        x.model_unload()\n        del x\n\ndef dtype_size(dtype):\n    dtype_size = 4\n    if dtype == torch.float16 or dtype == torch.bfloat16:\n        dtype_size = 2\n    elif dtype == torch.float32:\n        dtype_size = 4\n    else:\n        try:\n            dtype_size = dtype.itemsize\n        except: #Old pytorch doesn't have .itemsize\n            pass\n    return dtype_size\n\ndef unet_offload_device():\n    if vram_state == VRAMState.HIGH_VRAM:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef unet_inital_load_device(parameters, dtype):\n    torch_dev = get_torch_device()\n    if vram_state == VRAMState.HIGH_VRAM:\n        return torch_dev\n\n    cpu_dev = torch.device(\"cpu\")\n    if DISABLE_SMART_MEMORY:\n        return cpu_dev\n\n    model_size = dtype_size(dtype) * parameters\n\n    mem_dev = get_free_memory(torch_dev)\n    mem_cpu = get_free_memory(cpu_dev)\n    if mem_dev > mem_cpu and model_size < mem_dev:\n        return torch_dev\n    else:\n        return cpu_dev\n\ndef unet_dtype(device=None, model_params=0, supported_dtypes=[torch.float16, torch.bfloat16, torch.float32]):\n    if args.bf16_unet:\n        return torch.bfloat16\n    if args.fp16_unet:\n        return torch.float16\n    if args.fp8_e4m3fn_unet:\n        return torch.float8_e4m3fn\n    if args.fp8_e5m2_unet:\n        return torch.float8_e5m2\n    if should_use_fp16(device=device, model_params=model_params, manual_cast=True):\n        if torch.float16 in supported_dtypes:\n            return torch.float16\n    if should_use_bf16(device, model_params=model_params, manual_cast=True):\n        if torch.bfloat16 in supported_dtypes:\n            return torch.bfloat16\n    return torch.float32\n\n# None means no manual cast\ndef unet_manual_cast(weight_dtype, inference_device, supported_dtypes=[torch.float16, torch.bfloat16, torch.float32]):\n    if weight_dtype == torch.float32:\n        return None\n\n    fp16_supported = should_use_fp16(inference_device, prioritize_performance=False)\n    if fp16_supported and weight_dtype == torch.float16:\n        return None\n\n    bf16_supported = should_use_bf16(inference_device)\n    if bf16_supported and weight_dtype == torch.bfloat16:\n        return None\n\n    if fp16_supported and torch.float16 in supported_dtypes:\n        return torch.float16\n\n    elif bf16_supported and torch.bfloat16 in supported_dtypes:\n        return torch.bfloat16\n    else:\n        return torch.float32\n\ndef text_encoder_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef text_encoder_device():\n    if args.gpu_only:\n        return get_torch_device()\n    elif vram_state == VRAMState.HIGH_VRAM or vram_state == VRAMState.NORMAL_VRAM:\n        if should_use_fp16(prioritize_performance=False):\n            return get_torch_device()\n        else:\n            return torch.device(\"cpu\")\n    else:\n        return torch.device(\"cpu\")\n\ndef text_encoder_dtype(device=None):\n    if args.fp8_e4m3fn_text_enc:\n        return torch.float8_e4m3fn\n    elif args.fp8_e5m2_text_enc:\n        return torch.float8_e5m2\n    elif args.fp16_text_enc:\n        return torch.float16\n    elif args.fp32_text_enc:\n        return torch.float32\n\n    if is_device_cpu(device):\n        return torch.float16\n\n    return torch.float16\n\n\ndef intermediate_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef vae_device():\n    if args.cpu_vae:\n        return torch.device(\"cpu\")\n    return get_torch_device()\n\ndef vae_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\n\ndef vae_dtype(device=None, allowed_dtypes=[]):\n    global VAE_DTYPES\n    if args.fp16_vae:\n        return torch.float16\n    elif args.bf16_vae:\n        return torch.bfloat16\n    elif args.fp32_vae:\n        return torch.float32\n\n    for d in allowed_dtypes:\n        if d == torch.float16 and should_use_fp16(device, prioritize_performance=False):\n            return d\n        if d in VAE_DTYPES:\n            return d\n\n    return VAE_DTYPES[0]\n\ndef get_autocast_device(dev):\n    if hasattr(dev, 'type'):\n        return dev.type\n    return \"cuda\"\n\ndef supports_dtype(device, dtype): #TODO\n    if dtype == torch.float32:\n        return True\n    if is_device_cpu(device):\n        return False\n    if dtype == torch.float16:\n        return True\n    if dtype == torch.bfloat16:\n        return True\n    return False\n\ndef supports_cast(device, dtype): #TODO\n    if dtype == torch.float32:\n        return True\n    if dtype == torch.float16:\n        return True\n    if is_device_mps(device):\n        return False\n    if directml_enabled: #TODO: test this\n        return False\n    if dtype == torch.bfloat16:\n        return True\n    if dtype == torch.float8_e4m3fn:\n        return True\n    if dtype == torch.float8_e5m2:\n        return True\n    return False\n\ndef device_supports_non_blocking(device):\n    if is_device_mps(device):\n        return False #pytorch bug? mps doesn't support non blocking\n    if is_intel_xpu():\n        return False\n    if args.deterministic: #TODO: figure out why deterministic breaks non blocking from gpu to cpu (previews)\n        return False\n    if directml_enabled:\n        return False\n    return True\n\ndef device_should_use_non_blocking(device):\n    if not device_supports_non_blocking(device):\n        return False\n    return False\n    # return True #TODO: figure out why this causes memory issues on Nvidia and possibly others\n\ndef force_channels_last():\n    if args.force_channels_last:\n        return True\n\n    #TODO\n    return False\n\ndef cast_to_device(tensor, device, dtype, copy=False):\n    device_supports_cast = False\n    if tensor.dtype == torch.float32 or tensor.dtype == torch.float16:\n        device_supports_cast = True\n    elif tensor.dtype == torch.bfloat16:\n        if hasattr(device, 'type') and device.type.startswith(\"cuda\"):\n            device_supports_cast = True\n        elif is_intel_xpu():\n            device_supports_cast = True\n\n    non_blocking = device_should_use_non_blocking(device)\n\n    if device_supports_cast:\n        if copy:\n            if tensor.device == device:\n                return tensor.to(dtype, copy=copy, non_blocking=non_blocking)\n            return tensor.to(device, copy=copy, non_blocking=non_blocking).to(dtype, non_blocking=non_blocking)\n        else:\n            return tensor.to(device, non_blocking=non_blocking).to(dtype, non_blocking=non_blocking)\n    else:\n        return tensor.to(device, dtype, copy=copy, non_blocking=non_blocking)\n\ndef xformers_enabled():\n    global directml_enabled\n    global cpu_state\n    if cpu_state != CPUState.GPU:\n        return False\n    if is_intel_xpu():\n        return False\n    if directml_enabled:\n        return False\n    return XFORMERS_IS_AVAILABLE\n\n\ndef xformers_enabled_vae():\n    enabled = xformers_enabled()\n    if not enabled:\n        return False\n\n    return XFORMERS_ENABLED_VAE\n\ndef pytorch_attention_enabled():\n    global ENABLE_PYTORCH_ATTENTION\n    return ENABLE_PYTORCH_ATTENTION\n\ndef pytorch_attention_flash_attention():\n    global ENABLE_PYTORCH_ATTENTION\n    if ENABLE_PYTORCH_ATTENTION:\n        #TODO: more reliable way of checking for flash attention?\n        if is_nvidia(): #pytorch flash attention only works on Nvidia\n            return True\n        if is_intel_xpu():\n            return True\n    return False\n\ndef force_upcast_attention_dtype():\n    upcast = args.force_upcast_attention\n    try:\n        if platform.mac_ver()[0] in ['14.5']: #black image bug on OSX Sonoma 14.5\n            upcast = True\n    except:\n        pass\n    if upcast:\n        return torch.float32\n    else:\n        return None\n\ndef get_free_memory(dev=None, torch_free_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_free_total = psutil.virtual_memory().available\n        mem_free_torch = mem_free_total\n    else:\n        if directml_enabled:\n            mem_free_total = 1024 * 1024 * 1024 #TODO\n            mem_free_torch = mem_free_total\n        elif is_intel_xpu():\n            stats = torch.xpu.memory_stats(dev)\n            mem_active = stats['active_bytes.all.current']\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_free_torch = mem_reserved - mem_active\n            mem_free_xpu = torch.xpu.get_device_properties(dev).total_memory - mem_reserved\n            mem_free_total = mem_free_xpu + mem_free_torch\n        else:\n            stats = torch.cuda.memory_stats(dev)\n            mem_active = stats['active_bytes.all.current']\n            mem_reserved = stats['reserved_bytes.all.current']\n            mem_free_cuda, _ = torch.cuda.mem_get_info(dev)\n            mem_free_torch = mem_reserved - mem_active\n            mem_free_total = mem_free_cuda + mem_free_torch\n\n    if torch_free_too:\n        return (mem_free_total, mem_free_torch)\n    else:\n        return mem_free_total\n\ndef cpu_mode():\n    global cpu_state\n    return cpu_state == CPUState.CPU\n\ndef mps_mode():\n    global cpu_state\n    return cpu_state == CPUState.MPS\n\ndef is_device_type(device, type):\n    if hasattr(device, 'type'):\n        if (device.type == type):\n            return True\n    return False\n\ndef is_device_cpu(device):\n    return is_device_type(device, 'cpu')\n\ndef is_device_mps(device):\n    return is_device_type(device, 'mps')\n\ndef is_device_cuda(device):\n    return is_device_type(device, 'cuda')\n\ndef should_use_fp16(device=None, model_params=0, prioritize_performance=True, manual_cast=False):\n    global directml_enabled\n\n    if device is not None:\n        if is_device_cpu(device):\n            return False\n\n    if FORCE_FP16:\n        return True\n\n    if device is not None:\n        if is_device_mps(device):\n            return True\n\n    if FORCE_FP32:\n        return False\n\n    if directml_enabled:\n        return False\n\n    if mps_mode():\n        return True\n\n    if cpu_mode():\n        return False\n\n    if is_intel_xpu():\n        return True\n\n    if torch.version.hip:\n        return True\n\n    props = torch.cuda.get_device_properties(\"cuda\")\n    if props.major >= 8:\n        return True\n\n    if props.major < 6:\n        return False\n\n    fp16_works = False\n    #FP16 is confirmed working on a 1080 (GP104) but it's a bit slower than FP32 so it should only be enabled\n    #when the model doesn't actually fit on the card\n    #TODO: actually test if GP106 and others have the same type of behavior\n    nvidia_10_series = [\"1080\", \"1070\", \"titan x\", \"p3000\", \"p3200\", \"p4000\", \"p4200\", \"p5000\", \"p5200\", \"p6000\", \"1060\", \"1050\", \"p40\", \"p100\", \"p6\", \"p4\"]\n    for x in nvidia_10_series:\n        if x in props.name.lower():\n            fp16_works = True\n\n    if fp16_works or manual_cast:\n        free_model_memory = (get_free_memory() * 0.9 - minimum_inference_memory())\n        if (not prioritize_performance) or model_params * 4 > free_model_memory:\n            return True\n\n    if props.major < 7:\n        return False\n\n    #FP16 is just broken on these cards\n    nvidia_16_series = [\"1660\", \"1650\", \"1630\", \"T500\", \"T550\", \"T600\", \"MX550\", \"MX450\", \"CMP 30HX\", \"T2000\", \"T1000\", \"T1200\"]\n    for x in nvidia_16_series:\n        if x in props.name:\n            return False\n\n    return True\n\ndef should_use_bf16(device=None, model_params=0, prioritize_performance=True, manual_cast=False):\n    if device is not None:\n        if is_device_cpu(device): #TODO ? bf16 works on CPU but is extremely slow\n            return False\n\n    if device is not None: #TODO not sure about mps bf16 support\n        if is_device_mps(device):\n            return False\n\n    if FORCE_FP32:\n        return False\n\n    if directml_enabled:\n        return False\n\n    if cpu_mode() or mps_mode():\n        return False\n\n    if is_intel_xpu():\n        return True\n\n    if device is None:\n        device = torch.device(\"cuda\")\n\n    props = torch.cuda.get_device_properties(device)\n    if props.major >= 8:\n        return True\n\n    bf16_works = torch.cuda.is_bf16_supported()\n\n    if bf16_works or manual_cast:\n        free_model_memory = (get_free_memory() * 0.9 - minimum_inference_memory())\n        if (not prioritize_performance) or model_params * 4 > free_model_memory:\n            return True\n\n    return False\n\ndef soft_empty_cache(force=False):\n    global cpu_state\n    if cpu_state == CPUState.MPS:\n        torch.mps.empty_cache()\n    elif is_intel_xpu():\n        torch.xpu.empty_cache()\n    elif torch.cuda.is_available():\n        if force or is_nvidia(): #This seems to make things worse on ROCm so I only do it for cuda\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\ndef unload_all_models():\n    free_memory(1e30, get_torch_device())\n\n\ndef resolve_lowvram_weight(weight, model, key): #TODO: remove\n    print(\"WARNING: The comfy.model_management.resolve_lowvram_weight function will be removed soon, please stop using it.\")\n    return weight\n\n#TODO: might be cleaner to put this somewhere else\nimport threading\n\nclass InterruptProcessingException(Exception):\n    pass\n\ninterrupt_processing_mutex = threading.RLock()\n\ninterrupt_processing = False\ndef interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\n\ndef processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        return interrupt_processing\n\ndef throw_exception_if_processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        if interrupt_processing:\n            interrupt_processing = False\n            raise InterruptProcessingException()\n", "comfy/latent_formats.py": "import torch\n\nclass LatentFormat:\n    scale_factor = 1.0\n    latent_channels = 4\n    latent_rgb_factors = None\n    taesd_decoder_name = None\n\n    def process_in(self, latent):\n        return latent * self.scale_factor\n\n    def process_out(self, latent):\n        return latent / self.scale_factor\n\nclass SD15(LatentFormat):\n    def __init__(self, scale_factor=0.18215):\n        self.scale_factor = scale_factor\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3512,  0.2297,  0.3227],\n                    [ 0.3250,  0.4974,  0.2350],\n                    [-0.2829,  0.1762,  0.2721],\n                    [-0.2120, -0.2616, -0.7177]\n                ]\n        self.taesd_decoder_name = \"taesd_decoder\"\n\nclass SDXL(LatentFormat):\n    scale_factor = 0.13025\n\n    def __init__(self):\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3920,  0.4054,  0.4549],\n                    [-0.2634, -0.0196,  0.0653],\n                    [ 0.0568,  0.1687, -0.0755],\n                    [-0.3112, -0.2359, -0.2076]\n                ]\n        self.taesd_decoder_name = \"taesdxl_decoder\"\n\nclass SDXL_Playground_2_5(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.5\n        self.latents_mean = torch.tensor([-1.6574, 1.886, -1.383, 2.5155]).view(1, 4, 1, 1)\n        self.latents_std = torch.tensor([8.4927, 5.9022, 6.5498, 5.2299]).view(1, 4, 1, 1)\n\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3920,  0.4054,  0.4549],\n                    [-0.2634, -0.0196,  0.0653],\n                    [ 0.0568,  0.1687, -0.0755],\n                    [-0.3112, -0.2359, -0.2076]\n                ]\n        self.taesd_decoder_name = \"taesdxl_decoder\"\n\n    def process_in(self, latent):\n        latents_mean = self.latents_mean.to(latent.device, latent.dtype)\n        latents_std = self.latents_std.to(latent.device, latent.dtype)\n        return (latent - latents_mean) * self.scale_factor / latents_std\n\n    def process_out(self, latent):\n        latents_mean = self.latents_mean.to(latent.device, latent.dtype)\n        latents_std = self.latents_std.to(latent.device, latent.dtype)\n        return latent * latents_std / self.scale_factor + latents_mean\n\n\nclass SD_X4(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.08333\n        self.latent_rgb_factors = [\n            [-0.2340, -0.3863, -0.3257],\n            [ 0.0994,  0.0885, -0.0908],\n            [-0.2833, -0.2349, -0.3741],\n            [ 0.2523, -0.0055, -0.1651]\n        ]\n\nclass SC_Prior(LatentFormat):\n    latent_channels = 16\n    def __init__(self):\n        self.scale_factor = 1.0\n        self.latent_rgb_factors = [\n            [-0.0326, -0.0204, -0.0127],\n            [-0.1592, -0.0427,  0.0216],\n            [ 0.0873,  0.0638, -0.0020],\n            [-0.0602,  0.0442,  0.1304],\n            [ 0.0800, -0.0313, -0.1796],\n            [-0.0810, -0.0638, -0.1581],\n            [ 0.1791,  0.1180,  0.0967],\n            [ 0.0740,  0.1416,  0.0432],\n            [-0.1745, -0.1888, -0.1373],\n            [ 0.2412,  0.1577,  0.0928],\n            [ 0.1908,  0.0998,  0.0682],\n            [ 0.0209,  0.0365, -0.0092],\n            [ 0.0448, -0.0650, -0.1728],\n            [-0.1658, -0.1045, -0.1308],\n            [ 0.0542,  0.1545,  0.1325],\n            [-0.0352, -0.1672, -0.2541]\n        ]\n\nclass SC_B(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 1.0 / 0.43\n        self.latent_rgb_factors = [\n            [ 0.1121,  0.2006,  0.1023],\n            [-0.2093, -0.0222, -0.0195],\n            [-0.3087, -0.1535,  0.0366],\n            [ 0.0290, -0.1574, -0.4078]\n        ]\n\nclass SD3(LatentFormat):\n    latent_channels = 16\n    def __init__(self):\n        self.scale_factor = 1.5305\n        self.shift_factor = 0.0609\n        self.latent_rgb_factors = [\n            [-0.0645,  0.0177,  0.1052],\n            [ 0.0028,  0.0312,  0.0650],\n            [ 0.1848,  0.0762,  0.0360],\n            [ 0.0944,  0.0360,  0.0889],\n            [ 0.0897,  0.0506, -0.0364],\n            [-0.0020,  0.1203,  0.0284],\n            [ 0.0855,  0.0118,  0.0283],\n            [-0.0539,  0.0658,  0.1047],\n            [-0.0057,  0.0116,  0.0700],\n            [-0.0412,  0.0281, -0.0039],\n            [ 0.1106,  0.1171,  0.1220],\n            [-0.0248,  0.0682, -0.0481],\n            [ 0.0815,  0.0846,  0.1207],\n            [-0.0120, -0.0055, -0.0867],\n            [-0.0749, -0.0634, -0.0456],\n            [-0.1418, -0.1457, -0.1259]\n        ]\n        self.taesd_decoder_name = \"taesd3_decoder\"\n\n    def process_in(self, latent):\n        return (latent - self.shift_factor) * self.scale_factor\n\n    def process_out(self, latent):\n        return (latent / self.scale_factor) + self.shift_factor\n\nclass StableAudio1(LatentFormat):\n    latent_channels = 64\n", "comfy/diffusers_load.py": "import os\n\nimport comfy.sd\n\ndef first_file(path, filenames):\n    for f in filenames:\n        p = os.path.join(path, f)\n        if os.path.exists(p):\n            return p\n    return None\n\ndef load_diffusers(model_path, output_vae=True, output_clip=True, embedding_directory=None):\n    diffusion_model_names = [\"diffusion_pytorch_model.fp16.safetensors\", \"diffusion_pytorch_model.safetensors\", \"diffusion_pytorch_model.fp16.bin\", \"diffusion_pytorch_model.bin\"]\n    unet_path = first_file(os.path.join(model_path, \"unet\"), diffusion_model_names)\n    vae_path = first_file(os.path.join(model_path, \"vae\"), diffusion_model_names)\n\n    text_encoder_model_names = [\"model.fp16.safetensors\", \"model.safetensors\", \"pytorch_model.fp16.bin\", \"pytorch_model.bin\"]\n    text_encoder1_path = first_file(os.path.join(model_path, \"text_encoder\"), text_encoder_model_names)\n    text_encoder2_path = first_file(os.path.join(model_path, \"text_encoder_2\"), text_encoder_model_names)\n\n    text_encoder_paths = [text_encoder1_path]\n    if text_encoder2_path is not None:\n        text_encoder_paths.append(text_encoder2_path)\n\n    unet = comfy.sd.load_unet(unet_path)\n\n    clip = None\n    if output_clip:\n        clip = comfy.sd.load_clip(text_encoder_paths, embedding_directory=embedding_directory)\n\n    vae = None\n    if output_vae:\n        sd = comfy.utils.load_torch_file(vae_path)\n        vae = comfy.sd.VAE(sd=sd)\n\n    return (unet, clip, vae)\n", "comfy/cli_args.py": "import argparse\nimport enum\nimport comfy.options\n\nclass EnumAction(argparse.Action):\n    \"\"\"\n    Argparse action for handling Enums\n    \"\"\"\n    def __init__(self, **kwargs):\n        # Pop off the type value\n        enum_type = kwargs.pop(\"type\", None)\n\n        # Ensure an Enum subclass is provided\n        if enum_type is None:\n            raise ValueError(\"type must be assigned an Enum when using EnumAction\")\n        if not issubclass(enum_type, enum.Enum):\n            raise TypeError(\"type must be an Enum when using EnumAction\")\n\n        # Generate choices from the Enum\n        choices = tuple(e.value for e in enum_type)\n        kwargs.setdefault(\"choices\", choices)\n        kwargs.setdefault(\"metavar\", f\"[{','.join(list(choices))}]\")\n\n        super(EnumAction, self).__init__(**kwargs)\n\n        self._enum = enum_type\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        # Convert value back into an Enum\n        value = self._enum(values)\n        setattr(namespace, self.dest, value)\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--listen\", type=str, default=\"127.0.0.1\", metavar=\"IP\", nargs=\"?\", const=\"0.0.0.0\", help=\"Specify the IP address to listen on (default: 127.0.0.1). If --listen is provided without an argument, it defaults to 0.0.0.0. (listens on all)\")\nparser.add_argument(\"--port\", type=int, default=8188, help=\"Set the listen port.\")\nparser.add_argument(\"--tls-keyfile\", type=str, help=\"Path to TLS (SSL) key file. Enables TLS, makes app accessible at https://... requires --tls-certfile to function\")\nparser.add_argument(\"--tls-certfile\", type=str, help=\"Path to TLS (SSL) certificate file. Enables TLS, makes app accessible at https://... requires --tls-keyfile to function\")\nparser.add_argument(\"--enable-cors-header\", type=str, default=None, metavar=\"ORIGIN\", nargs=\"?\", const=\"*\", help=\"Enable CORS (Cross-Origin Resource Sharing) with optional origin or allow all with default '*'.\")\nparser.add_argument(\"--max-upload-size\", type=float, default=100, help=\"Set the maximum upload size in MB.\")\n\nparser.add_argument(\"--extra-model-paths-config\", type=str, default=None, metavar=\"PATH\", nargs='+', action='append', help=\"Load one or more extra_model_paths.yaml files.\")\nparser.add_argument(\"--output-directory\", type=str, default=None, help=\"Set the ComfyUI output directory.\")\nparser.add_argument(\"--temp-directory\", type=str, default=None, help=\"Set the ComfyUI temp directory (default is in the ComfyUI directory).\")\nparser.add_argument(\"--input-directory\", type=str, default=None, help=\"Set the ComfyUI input directory.\")\nparser.add_argument(\"--auto-launch\", action=\"store_true\", help=\"Automatically launch ComfyUI in the default browser.\")\nparser.add_argument(\"--disable-auto-launch\", action=\"store_true\", help=\"Disable auto launching the browser.\")\nparser.add_argument(\"--cuda-device\", type=int, default=None, metavar=\"DEVICE_ID\", help=\"Set the id of the cuda device this instance will use.\")\ncm_group = parser.add_mutually_exclusive_group()\ncm_group.add_argument(\"--cuda-malloc\", action=\"store_true\", help=\"Enable cudaMallocAsync (enabled by default for torch 2.0 and up).\")\ncm_group.add_argument(\"--disable-cuda-malloc\", action=\"store_true\", help=\"Disable cudaMallocAsync.\")\n\n\nfp_group = parser.add_mutually_exclusive_group()\nfp_group.add_argument(\"--force-fp32\", action=\"store_true\", help=\"Force fp32 (If this makes your GPU work better please report it).\")\nfp_group.add_argument(\"--force-fp16\", action=\"store_true\", help=\"Force fp16.\")\n\nfpunet_group = parser.add_mutually_exclusive_group()\nfpunet_group.add_argument(\"--bf16-unet\", action=\"store_true\", help=\"Run the UNET in bf16. This should only be used for testing stuff.\")\nfpunet_group.add_argument(\"--fp16-unet\", action=\"store_true\", help=\"Store unet weights in fp16.\")\nfpunet_group.add_argument(\"--fp8_e4m3fn-unet\", action=\"store_true\", help=\"Store unet weights in fp8_e4m3fn.\")\nfpunet_group.add_argument(\"--fp8_e5m2-unet\", action=\"store_true\", help=\"Store unet weights in fp8_e5m2.\")\n\nfpvae_group = parser.add_mutually_exclusive_group()\nfpvae_group.add_argument(\"--fp16-vae\", action=\"store_true\", help=\"Run the VAE in fp16, might cause black images.\")\nfpvae_group.add_argument(\"--fp32-vae\", action=\"store_true\", help=\"Run the VAE in full precision fp32.\")\nfpvae_group.add_argument(\"--bf16-vae\", action=\"store_true\", help=\"Run the VAE in bf16.\")\n\nparser.add_argument(\"--cpu-vae\", action=\"store_true\", help=\"Run the VAE on the CPU.\")\n\nfpte_group = parser.add_mutually_exclusive_group()\nfpte_group.add_argument(\"--fp8_e4m3fn-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e4m3fn variant).\")\nfpte_group.add_argument(\"--fp8_e5m2-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e5m2 variant).\")\nfpte_group.add_argument(\"--fp16-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp16.\")\nfpte_group.add_argument(\"--fp32-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp32.\")\n\nparser.add_argument(\"--force-channels-last\", action=\"store_true\", help=\"Force channels last format when inferencing the models.\")\n\nparser.add_argument(\"--directml\", type=int, nargs=\"?\", metavar=\"DIRECTML_DEVICE\", const=-1, help=\"Use torch-directml.\")\n\nparser.add_argument(\"--disable-ipex-optimize\", action=\"store_true\", help=\"Disables ipex.optimize when loading models with Intel GPUs.\")\n\nclass LatentPreviewMethod(enum.Enum):\n    NoPreviews = \"none\"\n    Auto = \"auto\"\n    Latent2RGB = \"latent2rgb\"\n    TAESD = \"taesd\"\n\nparser.add_argument(\"--preview-method\", type=LatentPreviewMethod, default=LatentPreviewMethod.NoPreviews, help=\"Default preview method for sampler nodes.\", action=EnumAction)\n\nattn_group = parser.add_mutually_exclusive_group()\nattn_group.add_argument(\"--use-split-cross-attention\", action=\"store_true\", help=\"Use the split cross attention optimization. Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-quad-cross-attention\", action=\"store_true\", help=\"Use the sub-quadratic cross attention optimization . Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-pytorch-cross-attention\", action=\"store_true\", help=\"Use the new pytorch 2.0 cross attention function.\")\n\nparser.add_argument(\"--disable-xformers\", action=\"store_true\", help=\"Disable xformers.\")\n\nupcast = parser.add_mutually_exclusive_group()\nupcast.add_argument(\"--force-upcast-attention\", action=\"store_true\", help=\"Force enable attention upcasting, please report if it fixes black images.\")\nupcast.add_argument(\"--dont-upcast-attention\", action=\"store_true\", help=\"Disable all upcasting of attention. Should be unnecessary except for debugging.\")\n\n\nvram_group = parser.add_mutually_exclusive_group()\nvram_group.add_argument(\"--gpu-only\", action=\"store_true\", help=\"Store and run everything (text encoders/CLIP models, etc... on the GPU).\")\nvram_group.add_argument(\"--highvram\", action=\"store_true\", help=\"By default models will be unloaded to CPU memory after being used. This option keeps them in GPU memory.\")\nvram_group.add_argument(\"--normalvram\", action=\"store_true\", help=\"Used to force normal vram use if lowvram gets automatically enabled.\")\nvram_group.add_argument(\"--lowvram\", action=\"store_true\", help=\"Split the unet in parts to use less vram.\")\nvram_group.add_argument(\"--novram\", action=\"store_true\", help=\"When lowvram isn't enough.\")\nvram_group.add_argument(\"--cpu\", action=\"store_true\", help=\"To use the CPU for everything (slow).\")\n\n\nparser.add_argument(\"--disable-smart-memory\", action=\"store_true\", help=\"Force ComfyUI to agressively offload to regular ram instead of keeping models in vram when it can.\")\nparser.add_argument(\"--deterministic\", action=\"store_true\", help=\"Make pytorch use slower deterministic algorithms when it can. Note that this might not make images deterministic in all cases.\")\n\nparser.add_argument(\"--dont-print-server\", action=\"store_true\", help=\"Don't print server output.\")\nparser.add_argument(\"--quick-test-for-ci\", action=\"store_true\", help=\"Quick test for CI.\")\nparser.add_argument(\"--windows-standalone-build\", action=\"store_true\", help=\"Windows standalone build: Enable convenient things that most people using the standalone windows build will probably enjoy (like auto opening the page on startup).\")\n\nparser.add_argument(\"--disable-metadata\", action=\"store_true\", help=\"Disable saving prompt metadata in files.\")\n\nparser.add_argument(\"--multi-user\", action=\"store_true\", help=\"Enables per-user storage.\")\n\nparser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enables more debug prints.\")\n\n\nif comfy.options.args_parsing:\n    args = parser.parse_args()\nelse:\n    args = parser.parse_args([])\n\nif args.windows_standalone_build:\n    args.auto_launch = True\n\nif args.disable_auto_launch:\n    args.auto_launch = False\n\nimport logging\nlogging_level = logging.INFO\nif args.verbose:\n    logging_level = logging.DEBUG\n\nlogging.basicConfig(format=\"%(message)s\", level=logging_level)\n", "comfy/supported_models.py": "import torch\nfrom . import model_base\nfrom . import utils\n\nfrom . import sd1_clip\nfrom . import sd2_clip\nfrom . import sdxl_clip\nfrom . import sd3_clip\nfrom . import sa_t5\n\nfrom . import supported_models_base\nfrom . import latent_formats\n\nfrom . import diffusers_convert\n\nclass SD15(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    unet_extra_config = {\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    latent_format = latent_formats.SD15\n\n    def process_clip_state_dict(self, state_dict):\n        k = list(state_dict.keys())\n        for x in k:\n            if x.startswith(\"cond_stage_model.transformer.\") and not x.startswith(\"cond_stage_model.transformer.text_model.\"):\n                y = x.replace(\"cond_stage_model.transformer.\", \"cond_stage_model.transformer.text_model.\")\n                state_dict[y] = state_dict.pop(x)\n\n        if 'cond_stage_model.transformer.text_model.embeddings.position_ids' in state_dict:\n            ids = state_dict['cond_stage_model.transformer.text_model.embeddings.position_ids']\n            if ids.dtype == torch.float32:\n                state_dict['cond_stage_model.transformer.text_model.embeddings.position_ids'] = ids.round()\n\n        replace_prefix = {}\n        replace_prefix[\"cond_stage_model.\"] = \"clip_l.\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=True)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        pop_keys = [\"clip_l.transformer.text_projection.weight\", \"clip_l.logit_scale\"]\n        for p in pop_keys:\n            if p in state_dict:\n                state_dict.pop(p)\n\n        replace_prefix = {\"clip_l.\": \"cond_stage_model.\"}\n        return utils.state_dict_prefix_replace(state_dict, replace_prefix)\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sd1_clip.SD1Tokenizer, sd1_clip.SD1ClipModel)\n\nclass SD20(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,\n        \"attn_precision\": torch.float32,\n    }\n\n    latent_format = latent_formats.SD15\n\n    def model_type(self, state_dict, prefix=\"\"):\n        if self.unet_config[\"in_channels\"] == 4: #SD2.0 inpainting models are not v prediction\n            k = \"{}output_blocks.11.1.transformer_blocks.0.norm1.bias\".format(prefix)\n            out = state_dict.get(k, None)\n            if out is not None and torch.std(out, unbiased=False) > 0.09: # not sure how well this will actually work. I guess we will find out.\n                return model_base.ModelType.V_PREDICTION\n        return model_base.ModelType.EPS\n\n    def process_clip_state_dict(self, state_dict):\n        replace_prefix = {}\n        replace_prefix[\"conditioner.embedders.0.model.\"] = \"clip_h.\" #SD2 in sgm format\n        replace_prefix[\"cond_stage_model.model.\"] = \"clip_h.\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=True)\n        state_dict = utils.clip_text_transformers_convert(state_dict, \"clip_h.\", \"clip_h.transformer.\")\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        replace_prefix[\"clip_h\"] = \"cond_stage_model.model\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix)\n        state_dict = diffusers_convert.convert_text_enc_state_dict_v20(state_dict)\n        return state_dict\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sd2_clip.SD2Tokenizer, sd2_clip.SD2ClipModel)\n\nclass SD21UnclipL(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 1536,\n        \"use_temporal_attention\": False,\n    }\n\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 768}\n\n\nclass SD21UnclipH(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 2048,\n        \"use_temporal_attention\": False,\n    }\n\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1024}\n\nclass SDXLRefiner(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 384,\n        \"use_linear_in_transformer\": True,\n        \"context_dim\": 1280,\n        \"adm_in_channels\": 2560,\n        \"transformer_depth\": [0, 0, 4, 4, 4, 4, 0, 0],\n        \"use_temporal_attention\": False,\n    }\n\n    latent_format = latent_formats.SDXL\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        return model_base.SDXLRefiner(self, device=device)\n\n    def process_clip_state_dict(self, state_dict):\n        keys_to_replace = {}\n        replace_prefix = {}\n        replace_prefix[\"conditioner.embedders.0.model.\"] = \"clip_g.\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=True)\n\n        state_dict = utils.clip_text_transformers_convert(state_dict, \"clip_g.\", \"clip_g.transformer.\")\n        state_dict = utils.state_dict_key_replace(state_dict, keys_to_replace)\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        state_dict_g = diffusers_convert.convert_text_enc_state_dict_v20(state_dict, \"clip_g\")\n        if \"clip_g.transformer.text_model.embeddings.position_ids\" in state_dict_g:\n            state_dict_g.pop(\"clip_g.transformer.text_model.embeddings.position_ids\")\n        replace_prefix[\"clip_g\"] = \"conditioner.embedders.0.model\"\n        state_dict_g = utils.state_dict_prefix_replace(state_dict_g, replace_prefix)\n        return state_dict_g\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sdxl_clip.SDXLTokenizer, sdxl_clip.SDXLRefinerClipModel)\n\nclass SDXL(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 10, 10],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\n    latent_format = latent_formats.SDXL\n\n    def model_type(self, state_dict, prefix=\"\"):\n        if 'edm_mean' in state_dict and 'edm_std' in state_dict: #Playground V2.5\n            self.latent_format = latent_formats.SDXL_Playground_2_5()\n            self.sampling_settings[\"sigma_data\"] = 0.5\n            self.sampling_settings[\"sigma_max\"] = 80.0\n            self.sampling_settings[\"sigma_min\"] = 0.002\n            return model_base.ModelType.EDM\n        elif \"edm_vpred.sigma_max\" in state_dict:\n            self.sampling_settings[\"sigma_max\"] = float(state_dict[\"edm_vpred.sigma_max\"].item())\n            if \"edm_vpred.sigma_min\" in state_dict:\n                self.sampling_settings[\"sigma_min\"] = float(state_dict[\"edm_vpred.sigma_min\"].item())\n            return model_base.ModelType.V_PREDICTION_EDM\n        elif \"v_pred\" in state_dict:\n            return model_base.ModelType.V_PREDICTION\n        else:\n            return model_base.ModelType.EPS\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SDXL(self, model_type=self.model_type(state_dict, prefix), device=device)\n        if self.inpaint_model():\n            out.set_inpaint()\n        return out\n\n    def process_clip_state_dict(self, state_dict):\n        keys_to_replace = {}\n        replace_prefix = {}\n\n        replace_prefix[\"conditioner.embedders.0.transformer.text_model\"] = \"clip_l.transformer.text_model\"\n        replace_prefix[\"conditioner.embedders.1.model.\"] = \"clip_g.\"\n        state_dict = utils.state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=True)\n\n        state_dict = utils.state_dict_key_replace(state_dict, keys_to_replace)\n        state_dict = utils.clip_text_transformers_convert(state_dict, \"clip_g.\", \"clip_g.transformer.\")\n        return state_dict\n\n    def process_clip_state_dict_for_saving(self, state_dict):\n        replace_prefix = {}\n        keys_to_replace = {}\n        state_dict_g = diffusers_convert.convert_text_enc_state_dict_v20(state_dict, \"clip_g\")\n        for k in state_dict:\n            if k.startswith(\"clip_l\"):\n                state_dict_g[k] = state_dict[k]\n\n        state_dict_g[\"clip_l.transformer.text_model.embeddings.position_ids\"] = torch.arange(77).expand((1, -1))\n        pop_keys = [\"clip_l.transformer.text_projection.weight\", \"clip_l.logit_scale\"]\n        for p in pop_keys:\n            if p in state_dict_g:\n                state_dict_g.pop(p)\n\n        replace_prefix[\"clip_g\"] = \"conditioner.embedders.1.model\"\n        replace_prefix[\"clip_l\"] = \"conditioner.embedders.0\"\n        state_dict_g = utils.state_dict_prefix_replace(state_dict_g, replace_prefix)\n        return state_dict_g\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sdxl_clip.SDXLTokenizer, sdxl_clip.SDXLClipModel)\n\nclass SSD1B(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 4, 4],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass Segmind_Vega(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 1, 1, 2, 2],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass KOALA_700M(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 2, 5],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass KOALA_1B(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 2, 6],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n    }\n\nclass SVD_img2vid(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 320,\n        \"in_channels\": 8,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [1, 1, 1, 1, 1, 1, 0, 0],\n        \"context_dim\": 1024,\n        \"adm_in_channels\": 768,\n        \"use_temporal_attention\": True,\n        \"use_temporal_resblock\": True\n    }\n\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,\n        \"attn_precision\": torch.float32,\n    }\n\n    clip_vision_prefix = \"conditioner.embedders.0.open_clip.model.visual.\"\n\n    latent_format = latent_formats.SD15\n\n    sampling_settings = {\"sigma_max\": 700.0, \"sigma_min\": 0.002}\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SVD_img2vid(self, device=device)\n        return out\n\n    def clip_target(self, state_dict={}):\n        return None\n\nclass SV3D_u(SVD_img2vid):\n    unet_config = {\n        \"model_channels\": 320,\n        \"in_channels\": 8,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [1, 1, 1, 1, 1, 1, 0, 0],\n        \"context_dim\": 1024,\n        \"adm_in_channels\": 256,\n        \"use_temporal_attention\": True,\n        \"use_temporal_resblock\": True\n    }\n\n    vae_key_prefix = [\"conditioner.embedders.1.encoder.\"]\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SV3D_u(self, device=device)\n        return out\n\nclass SV3D_p(SV3D_u):\n    unet_config = {\n        \"model_channels\": 320,\n        \"in_channels\": 8,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [1, 1, 1, 1, 1, 1, 0, 0],\n        \"context_dim\": 1024,\n        \"adm_in_channels\": 1280,\n        \"use_temporal_attention\": True,\n        \"use_temporal_resblock\": True\n    }\n\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SV3D_p(self, device=device)\n        return out\n\nclass Stable_Zero123(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n        \"in_channels\": 8,\n    }\n\n    unet_extra_config = {\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    required_keys = {\n        \"cc_projection.weight\": None,\n        \"cc_projection.bias\": None,\n    }\n\n    clip_vision_prefix = \"cond_stage_model.model.visual.\"\n\n    latent_format = latent_formats.SD15\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.Stable_Zero123(self, device=device, cc_projection_weight=state_dict[\"cc_projection.weight\"], cc_projection_bias=state_dict[\"cc_projection.bias\"])\n        return out\n\n    def clip_target(self, state_dict={}):\n        return None\n\nclass SD_X4Upscaler(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 256,\n        'in_channels': 7,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n    }\n\n    unet_extra_config = {\n        \"disable_self_attentions\": [True, True, True, False],\n        \"num_classes\": 1000,\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,\n    }\n\n    latent_format = latent_formats.SD_X4\n\n    sampling_settings = {\n        \"linear_start\": 0.0001,\n        \"linear_end\": 0.02,\n    }\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SD_X4Upscaler(self, device=device)\n        return out\n\nclass Stable_Cascade_C(supported_models_base.BASE):\n    unet_config = {\n        \"stable_cascade_stage\": 'c',\n    }\n\n    unet_extra_config = {}\n\n    latent_format = latent_formats.SC_Prior\n    supported_inference_dtypes = [torch.bfloat16, torch.float32]\n\n    sampling_settings = {\n        \"shift\": 2.0,\n    }\n\n    vae_key_prefix = [\"vae.\"]\n    text_encoder_key_prefix = [\"text_encoder.\"]\n    clip_vision_prefix = \"clip_l_vision.\"\n\n    def process_unet_state_dict(self, state_dict):\n        key_list = list(state_dict.keys())\n        for y in [\"weight\", \"bias\"]:\n            suffix = \"in_proj_{}\".format(y)\n            keys = filter(lambda a: a.endswith(suffix), key_list)\n            for k_from in keys:\n                weights = state_dict.pop(k_from)\n                prefix = k_from[:-(len(suffix) + 1)]\n                shape_from = weights.shape[0] // 3\n                for x in range(3):\n                    p = [\"to_q\", \"to_k\", \"to_v\"]\n                    k_to = \"{}.{}.{}\".format(prefix, p[x], y)\n                    state_dict[k_to] = weights[shape_from*x:shape_from*(x + 1)]\n        return state_dict\n\n    def process_clip_state_dict(self, state_dict):\n        state_dict = utils.state_dict_prefix_replace(state_dict, {k: \"\" for k in self.text_encoder_key_prefix}, filter_keys=True)\n        if \"clip_g.text_projection\" in state_dict:\n            state_dict[\"clip_g.transformer.text_projection.weight\"] = state_dict.pop(\"clip_g.text_projection\").transpose(0, 1)\n        return state_dict\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.StableCascade_C(self, device=device)\n        return out\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sdxl_clip.StableCascadeTokenizer, sdxl_clip.StableCascadeClipModel)\n\nclass Stable_Cascade_B(Stable_Cascade_C):\n    unet_config = {\n        \"stable_cascade_stage\": 'b',\n    }\n\n    unet_extra_config = {}\n\n    latent_format = latent_formats.SC_B\n    supported_inference_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n\n    sampling_settings = {\n        \"shift\": 1.0,\n    }\n\n    clip_vision_prefix = None\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.StableCascade_B(self, device=device)\n        return out\n\nclass SD15_instructpix2pix(SD15):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n        \"use_temporal_attention\": False,\n        \"in_channels\": 8,\n    }\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        return model_base.SD15_instructpix2pix(self, device=device)\n\nclass SDXL_instructpix2pix(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 10, 10],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816,\n        \"use_temporal_attention\": False,\n        \"in_channels\": 8,\n    }\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        return model_base.SDXL_instructpix2pix(self, model_type=self.model_type(state_dict, prefix), device=device)\n\nclass SD3(supported_models_base.BASE):\n    unet_config = {\n        \"in_channels\": 16,\n        \"pos_embed_scaling_factor\": None,\n    }\n\n    sampling_settings = {\n        \"shift\": 3.0,\n    }\n\n    unet_extra_config = {}\n    latent_format = latent_formats.SD3\n    text_encoder_key_prefix = [\"text_encoders.\"]\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        out = model_base.SD3(self, device=device)\n        return out\n\n    def clip_target(self, state_dict={}):\n        clip_l = False\n        clip_g = False\n        t5 = False\n        dtype_t5 = None\n        pref = self.text_encoder_key_prefix[0]\n        if \"{}clip_l.transformer.text_model.final_layer_norm.weight\".format(pref) in state_dict:\n            clip_l = True\n        if \"{}clip_g.transformer.text_model.final_layer_norm.weight\".format(pref) in state_dict:\n            clip_g = True\n        t5_key = \"{}t5xxl.transformer.encoder.final_layer_norm.weight\".format(pref)\n        if t5_key in state_dict:\n            t5 = True\n            dtype_t5 = state_dict[t5_key].dtype\n\n        return supported_models_base.ClipTarget(sd3_clip.SD3Tokenizer, sd3_clip.sd3_clip(clip_l=clip_l, clip_g=clip_g, t5=t5, dtype_t5=dtype_t5))\n\nclass StableAudio(supported_models_base.BASE):\n    unet_config = {\n        \"audio_model\": \"dit1.0\",\n    }\n\n    sampling_settings = {\"sigma_max\": 500.0, \"sigma_min\": 0.03}\n\n    unet_extra_config = {}\n    latent_format = latent_formats.StableAudio1\n\n    text_encoder_key_prefix = [\"text_encoders.\"]\n    vae_key_prefix = [\"pretransform.model.\"]\n\n    def get_model(self, state_dict, prefix=\"\", device=None):\n        seconds_start_sd = utils.state_dict_prefix_replace(state_dict, {\"conditioner.conditioners.seconds_start.\": \"\"}, filter_keys=True)\n        seconds_total_sd = utils.state_dict_prefix_replace(state_dict, {\"conditioner.conditioners.seconds_total.\": \"\"}, filter_keys=True)\n        return model_base.StableAudio1(self, seconds_start_embedder_weights=seconds_start_sd, seconds_total_embedder_weights=seconds_total_sd, device=device)\n\n\n    def process_unet_state_dict(self, state_dict):\n        for k in list(state_dict.keys()):\n            if k.endswith(\".cross_attend_norm.beta\") or k.endswith(\".ff_norm.beta\") or k.endswith(\".pre_norm.beta\"): #These weights are all zero\n                state_dict.pop(k)\n        return state_dict\n\n    def clip_target(self, state_dict={}):\n        return supported_models_base.ClipTarget(sa_t5.SAT5Tokenizer, sa_t5.SAT5Model)\n\n\nmodels = [Stable_Zero123, SD15_instructpix2pix, SD15, SD20, SD21UnclipL, SD21UnclipH, SDXL_instructpix2pix, SDXLRefiner, SDXL, SSD1B, KOALA_700M, KOALA_1B, Segmind_Vega, SD_X4Upscaler, Stable_Cascade_C, Stable_Cascade_B, SV3D_u, SV3D_p, SD3, StableAudio]\n\nmodels += [SVD_img2vid]\n", "comfy/ldm/util.py": "import importlib\n\nimport torch\nfrom torch import optim\nimport numpy as np\n\nfrom inspect import isfunction\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))\n        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n\n        try:\n            draw.text((0, 0), lines, fill=\"black\", font=font)\n        except UnicodeEncodeError:\n            print(\"Cant encode string for logging. Skipping.\")\n\n        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n        txts.append(txt)\n    txts = np.stack(txts)\n    txts = torch.tensor(txts)\n    return txts\n\n\ndef ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\n\n\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\n\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n\n\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\n\n\nclass AdamWwithEMAandWings(optim.Optimizer):\n    # credit to https://gist.github.com/crowsonkb/65f7265353f403714fce3b2595e0b298\n    def __init__(self, params, lr=1.e-3, betas=(0.9, 0.999), eps=1.e-8,  # TODO: check hyperparameters before using\n                 weight_decay=1.e-2, amsgrad=False, ema_decay=0.9999,   # ema decay to match previous code\n                 ema_power=1., param_names=()):\n        \"\"\"AdamW that saves EMA versions of the parameters.\"\"\"\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= ema_decay <= 1.0:\n            raise ValueError(\"Invalid ema_decay value: {}\".format(ema_decay))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad, ema_decay=ema_decay,\n                        ema_power=ema_power, param_names=param_names)\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            exp_avg_sqs = []\n            ema_params_with_grad = []\n            state_sums = []\n            max_exp_avg_sqs = []\n            state_steps = []\n            amsgrad = group['amsgrad']\n            beta1, beta2 = group['betas']\n            ema_decay = group['ema_decay']\n            ema_power = group['ema_power']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of parameter values\n                    state['param_exp_avg'] = p.detach().float().clone()\n\n                exp_avgs.append(state['exp_avg'])\n                exp_avg_sqs.append(state['exp_avg_sq'])\n                ema_params_with_grad.append(state['param_exp_avg'])\n\n                if amsgrad:\n                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n\n                # update the steps for each param group update\n                state['step'] += 1\n                # record the step after step update\n                state_steps.append(state['step'])\n\n            optim._functional.adamw(params_with_grad,\n                    grads,\n                    exp_avgs,\n                    exp_avg_sqs,\n                    max_exp_avg_sqs,\n                    state_steps,\n                    amsgrad=amsgrad,\n                    beta1=beta1,\n                    beta2=beta2,\n                    lr=group['lr'],\n                    weight_decay=group['weight_decay'],\n                    eps=group['eps'],\n                    maximize=False)\n\n            cur_ema_decay = min(ema_decay, 1 - state['step'] ** -ema_power)\n            for param, ema_param in zip(params_with_grad, ema_params_with_grad):\n                ema_param.mul_(cur_ema_decay).add_(param.float(), alpha=1 - cur_ema_decay)\n\n        return loss", "comfy/ldm/models/autoencoder.py": "import torch\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom comfy.ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n\nfrom comfy.ldm.util import instantiate_from_config\nfrom comfy.ldm.modules.ema import LitEma\nimport comfy.ops\n\nclass DiagonalGaussianRegularizer(torch.nn.Module):\n    def __init__(self, sample: bool = True):\n        super().__init__()\n        self.sample = sample\n\n    def get_trainable_parameters(self) -> Any:\n        yield from ()\n\n    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n        log = dict()\n        posterior = DiagonalGaussianDistribution(z)\n        if self.sample:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        kl_loss = posterior.kl()\n        kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n        log[\"kl_loss\"] = kl_loss\n        return z, log\n\n\nclass AbstractAutoencoder(torch.nn.Module):\n    \"\"\"\n    This is the base class for all autoencoders, including image autoencoders, image autoencoders with discriminators,\n    unCLIP models, etc. Hence, it is fairly general, and specific features\n    (e.g. discriminator training, encoding, decoding) must be implemented in subclasses.\n    \"\"\"\n\n    def __init__(\n        self,\n        ema_decay: Union[None, float] = None,\n        monitor: Union[None, str] = None,\n        input_key: str = \"jpg\",\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.input_key = input_key\n        self.use_ema = ema_decay is not None\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema:\n            self.model_ema = LitEma(self, decay=ema_decay)\n            logpy.info(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n    def get_input(self, batch) -> Any:\n        raise NotImplementedError()\n\n    def on_train_batch_end(self, *args, **kwargs):\n        # for EMA computation\n        if self.use_ema:\n            self.model_ema(self)\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.parameters())\n            self.model_ema.copy_to(self)\n            if context is not None:\n                logpy.info(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.parameters())\n                if context is not None:\n                    logpy.info(f\"{context}: Restored training weights\")\n\n    def encode(self, *args, **kwargs) -> torch.Tensor:\n        raise NotImplementedError(\"encode()-method of abstract base class called\")\n\n    def decode(self, *args, **kwargs) -> torch.Tensor:\n        raise NotImplementedError(\"decode()-method of abstract base class called\")\n\n    def instantiate_optimizer_from_config(self, params, lr, cfg):\n        logpy.info(f\"loading >>> {cfg['target']} <<< optimizer from config\")\n        return get_obj_from_str(cfg[\"target\"])(\n            params, lr=lr, **cfg.get(\"params\", dict())\n        )\n\n    def configure_optimizers(self) -> Any:\n        raise NotImplementedError()\n\n\nclass AutoencodingEngine(AbstractAutoencoder):\n    \"\"\"\n    Base class for all image autoencoders that we train, like VQGAN or AutoencoderKL\n    (we also restore them explicitly as special cases for legacy reasons).\n    Regularizations such as KL or VQ are moved to the regularizer class.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        encoder_config: Dict,\n        decoder_config: Dict,\n        regularizer_config: Dict,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n        self.encoder: torch.nn.Module = instantiate_from_config(encoder_config)\n        self.decoder: torch.nn.Module = instantiate_from_config(decoder_config)\n        self.regularization: AbstractRegularizer = instantiate_from_config(\n            regularizer_config\n        )\n\n    def get_last_layer(self):\n        return self.decoder.get_last_layer()\n\n    def encode(\n        self,\n        x: torch.Tensor,\n        return_reg_log: bool = False,\n        unregularized: bool = False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, dict]]:\n        z = self.encoder(x)\n        if unregularized:\n            return z, dict()\n        z, reg_log = self.regularization(z)\n        if return_reg_log:\n            return z, reg_log\n        return z\n\n    def decode(self, z: torch.Tensor, **kwargs) -> torch.Tensor:\n        x = self.decoder(z, **kwargs)\n        return x\n\n    def forward(\n        self, x: torch.Tensor, **additional_decode_kwargs\n    ) -> Tuple[torch.Tensor, torch.Tensor, dict]:\n        z, reg_log = self.encode(x, return_reg_log=True)\n        dec = self.decode(z, **additional_decode_kwargs)\n        return z, dec, reg_log\n\n\nclass AutoencodingEngineLegacy(AutoencodingEngine):\n    def __init__(self, embed_dim: int, **kwargs):\n        self.max_batch_size = kwargs.pop(\"max_batch_size\", None)\n        ddconfig = kwargs.pop(\"ddconfig\")\n        super().__init__(\n            encoder_config={\n                \"target\": \"comfy.ldm.modules.diffusionmodules.model.Encoder\",\n                \"params\": ddconfig,\n            },\n            decoder_config={\n                \"target\": \"comfy.ldm.modules.diffusionmodules.model.Decoder\",\n                \"params\": ddconfig,\n            },\n            **kwargs,\n        )\n        self.quant_conv = comfy.ops.disable_weight_init.Conv2d(\n            (1 + ddconfig[\"double_z\"]) * ddconfig[\"z_channels\"],\n            (1 + ddconfig[\"double_z\"]) * embed_dim,\n            1,\n        )\n        self.post_quant_conv = comfy.ops.disable_weight_init.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n\n    def get_autoencoder_params(self) -> list:\n        params = super().get_autoencoder_params()\n        return params\n\n    def encode(\n        self, x: torch.Tensor, return_reg_log: bool = False\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, dict]]:\n        if self.max_batch_size is None:\n            z = self.encoder(x)\n            z = self.quant_conv(z)\n        else:\n            N = x.shape[0]\n            bs = self.max_batch_size\n            n_batches = int(math.ceil(N / bs))\n            z = list()\n            for i_batch in range(n_batches):\n                z_batch = self.encoder(x[i_batch * bs : (i_batch + 1) * bs])\n                z_batch = self.quant_conv(z_batch)\n                z.append(z_batch)\n            z = torch.cat(z, 0)\n\n        z, reg_log = self.regularization(z)\n        if return_reg_log:\n            return z, reg_log\n        return z\n\n    def decode(self, z: torch.Tensor, **decoder_kwargs) -> torch.Tensor:\n        if self.max_batch_size is None:\n            dec = self.post_quant_conv(z)\n            dec = self.decoder(dec, **decoder_kwargs)\n        else:\n            N = z.shape[0]\n            bs = self.max_batch_size\n            n_batches = int(math.ceil(N / bs))\n            dec = list()\n            for i_batch in range(n_batches):\n                dec_batch = self.post_quant_conv(z[i_batch * bs : (i_batch + 1) * bs])\n                dec_batch = self.decoder(dec_batch, **decoder_kwargs)\n                dec.append(dec_batch)\n            dec = torch.cat(dec, 0)\n\n        return dec\n\n\nclass AutoencoderKL(AutoencodingEngineLegacy):\n    def __init__(self, **kwargs):\n        if \"lossconfig\" in kwargs:\n            kwargs[\"loss_config\"] = kwargs.pop(\"lossconfig\")\n        super().__init__(\n            regularizer_config={\n                \"target\": (\n                    \"comfy.ldm.models.autoencoder.DiagonalGaussianRegularizer\"\n                )\n            },\n            **kwargs,\n        )\n", "comfy/ldm/audio/embedders.py": "# code adapted from: https://github.com/Stability-AI/stable-audio-tools\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor, einsum\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\nfrom einops import rearrange\nimport math\nimport comfy.ops\n\nclass LearnedPositionalEmbedding(nn.Module):\n    \"\"\"Used for continuous time\"\"\"\n\n    def __init__(self, dim: int):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.empty(half_dim))\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = rearrange(x, \"b -> b 1\")\n        freqs = x * rearrange(self.weights, \"d -> 1 d\") * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n        fouriered = torch.cat((x, fouriered), dim=-1)\n        return fouriered\n\ndef TimePositionalEmbedding(dim: int, out_features: int) -> nn.Module:\n    return nn.Sequential(\n        LearnedPositionalEmbedding(dim),\n        comfy.ops.manual_cast.Linear(in_features=dim + 1, out_features=out_features),\n    )\n\n\nclass NumberEmbedder(nn.Module):\n    def __init__(\n        self,\n        features: int,\n        dim: int = 256,\n    ):\n        super().__init__()\n        self.features = features\n        self.embedding = TimePositionalEmbedding(dim=dim, out_features=features)\n\n    def forward(self, x: Union[List[float], Tensor]) -> Tensor:\n        if not torch.is_tensor(x):\n            device = next(self.embedding.parameters()).device\n            x = torch.tensor(x, device=device)\n        assert isinstance(x, Tensor)\n        shape = x.shape\n        x = rearrange(x, \"... -> (...)\")\n        embedding = self.embedding(x)\n        x = embedding.view(*shape, self.features)\n        return x  # type: ignore\n\n\nclass Conditioner(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            output_dim: int,\n            project_out: bool = False\n            ):\n\n        super().__init__()\n\n        self.dim = dim\n        self.output_dim = output_dim\n        self.proj_out = nn.Linear(dim, output_dim) if (dim != output_dim or project_out) else nn.Identity()\n\n    def forward(self, x):\n        raise NotImplementedError()\n\nclass NumberConditioner(Conditioner):\n    '''\n        Conditioner that takes a list of floats, normalizes them for a given range, and returns a list of embeddings\n    '''\n    def __init__(self,\n                output_dim: int,\n                min_val: float=0,\n                max_val: float=1\n                ):\n        super().__init__(output_dim, output_dim)\n\n        self.min_val = min_val\n        self.max_val = max_val\n\n        self.embedder = NumberEmbedder(features=output_dim)\n\n    def forward(self, floats, device=None):\n            # Cast the inputs to floats\n            floats = [float(x) for x in floats]\n\n            if device is None:\n                device = next(self.embedder.parameters()).device\n\n            floats = torch.tensor(floats).to(device)\n\n            floats = floats.clamp(self.min_val, self.max_val)\n\n            normalized_floats = (floats - self.min_val) / (self.max_val - self.min_val)\n\n            # Cast floats to same type as embedder\n            embedder_dtype = next(self.embedder.parameters()).dtype\n            normalized_floats = normalized_floats.to(embedder_dtype)\n\n            float_embeds = self.embedder(normalized_floats).unsqueeze(1)\n\n            return [float_embeds, torch.ones(float_embeds.shape[0], 1).to(device)]\n", "comfy/ldm/audio/dit.py": "# code adapted from: https://github.com/Stability-AI/stable-audio-tools\n\nfrom comfy.ldm.modules.attention import optimized_attention\nimport typing as tp\n\nimport torch\n\nfrom einops import rearrange\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\n\nclass FourierFeatures(nn.Module):\n    def __init__(self, in_features, out_features, std=1., dtype=None, device=None):\n        super().__init__()\n        assert out_features % 2 == 0\n        self.weight = nn.Parameter(torch.empty(\n            [out_features // 2, in_features], dtype=dtype, device=device))\n\n    def forward(self, input):\n        f = 2 * math.pi * input @ self.weight.T.to(dtype=input.dtype, device=input.device)\n        return torch.cat([f.cos(), f.sin()], dim=-1)\n\n# norms\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, bias=False, fix_scale=False, dtype=None, device=None):\n        \"\"\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \"\"\"\n        super().__init__()\n\n        self.gamma = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n\n        if bias:\n            self.beta = nn.Parameter(torch.empty(dim, dtype=dtype, device=device))\n        else:\n            self.beta = None\n\n    def forward(self, x):\n        beta = self.beta\n        if self.beta is not None:\n            beta = beta.to(dtype=x.dtype, device=x.device)\n        return F.layer_norm(x, x.shape[-1:], weight=self.gamma.to(dtype=x.dtype, device=x.device), bias=beta)\n\nclass GLU(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        activation,\n        use_conv = False,\n        conv_kernel_size = 3,\n        dtype=None,\n        device=None,\n        operations=None,\n    ):\n        super().__init__()\n        self.act = activation\n        self.proj = operations.Linear(dim_in, dim_out * 2, dtype=dtype, device=device) if not use_conv else operations.Conv1d(dim_in, dim_out * 2, conv_kernel_size, padding = (conv_kernel_size // 2), dtype=dtype, device=device)\n        self.use_conv = use_conv\n\n    def forward(self, x):\n        if self.use_conv:\n            x = rearrange(x, 'b n d -> b d n')\n            x = self.proj(x)\n            x = rearrange(x, 'b d n -> b n d')\n        else:\n            x = self.proj(x)\n\n        x, gate = x.chunk(2, dim = -1)\n        return x * self.act(gate)\n\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.max_seq_len = max_seq_len\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(self, x, pos = None, seq_start_pos = None):\n        seq_len, device = x.shape[1], x.device\n        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n\n        if pos is None:\n            pos = torch.arange(seq_len, device = device)\n\n        if seq_start_pos is not None:\n            pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n\n        pos_emb = self.emb(pos)\n        pos_emb = pos_emb * self.scale\n        return pos_emb\n\nclass ScaledSinusoidalEmbedding(nn.Module):\n    def __init__(self, dim, theta = 10000):\n        super().__init__()\n        assert (dim % 2) == 0, 'dimension must be divisible by 2'\n        self.scale = nn.Parameter(torch.ones(1) * dim ** -0.5)\n\n        half_dim = dim // 2\n        freq_seq = torch.arange(half_dim).float() / half_dim\n        inv_freq = theta ** -freq_seq\n        self.register_buffer('inv_freq', inv_freq, persistent = False)\n\n    def forward(self, x, pos = None, seq_start_pos = None):\n        seq_len, device = x.shape[1], x.device\n\n        if pos is None:\n            pos = torch.arange(seq_len, device = device)\n\n        if seq_start_pos is not None:\n            pos = pos - seq_start_pos[..., None]\n\n        emb = torch.einsum('i, j -> i j', pos, self.inv_freq)\n        emb = torch.cat((emb.sin(), emb.cos()), dim = -1)\n        return emb * self.scale\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        use_xpos = False,\n        scale_base = 512,\n        interpolation_factor = 1.,\n        base = 10000,\n        base_rescale_factor = 1.\n    ):\n        super().__init__()\n        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n        # has some connection to NTK literature\n        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n        base *= base_rescale_factor ** (dim / (dim - 2))\n\n        inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        assert interpolation_factor >= 1.\n        self.interpolation_factor = interpolation_factor\n\n        if not use_xpos:\n            self.register_buffer('scale', None)\n            return\n\n        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n\n        self.scale_base = scale_base\n        self.register_buffer('scale', scale)\n\n    def forward_from_seq_len(self, seq_len, device, dtype):\n        # device = self.inv_freq.device\n\n        t = torch.arange(seq_len, device=device, dtype=dtype)\n        return self.forward(t)\n\n    def forward(self, t):\n        # device = self.inv_freq.device\n        device = t.device\n        dtype = t.dtype\n\n        # t = t.to(torch.float32)\n\n        t = t / self.interpolation_factor\n\n        freqs = torch.einsum('i , j -> i j', t, self.inv_freq.to(dtype=dtype, device=device))\n        freqs = torch.cat((freqs, freqs), dim = -1)\n\n        if self.scale is None:\n            return freqs, 1.\n\n        power = (torch.arange(seq_len, device = device) - (seq_len // 2)) / self.scale_base\n        scale = self.scale.to(dtype=dtype, device=device) ** rearrange(power, 'n -> n 1')\n        scale = torch.cat((scale, scale), dim = -1)\n\n        return freqs, scale\n\ndef rotate_half(x):\n    x = rearrange(x, '... (j d) -> ... j d', j = 2)\n    x1, x2 = x.unbind(dim = -2)\n    return torch.cat((-x2, x1), dim = -1)\n\ndef apply_rotary_pos_emb(t, freqs, scale = 1):\n    out_dtype = t.dtype\n\n    # cast to float32 if necessary for numerical stability\n    dtype = t.dtype #reduce(torch.promote_types, (t.dtype, freqs.dtype, torch.float32))\n    rot_dim, seq_len = freqs.shape[-1], t.shape[-2]\n    freqs, t = freqs.to(dtype), t.to(dtype)\n    freqs = freqs[-seq_len:, :]\n\n    if t.ndim == 4 and freqs.ndim == 3:\n        freqs = rearrange(freqs, 'b n d -> b 1 n d')\n\n    # partial rotary embeddings, Wang et al. GPT-J\n    t, t_unrotated = t[..., :rot_dim], t[..., rot_dim:]\n    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n\n    t, t_unrotated = t.to(out_dtype), t_unrotated.to(out_dtype)\n\n    return torch.cat((t, t_unrotated), dim = -1)\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        mult = 4,\n        no_bias = False,\n        glu = True,\n        use_conv = False,\n        conv_kernel_size = 3,\n        zero_init_output = True,\n        dtype=None,\n        device=None,\n        operations=None,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n\n        # Default to SwiGLU\n\n        activation = nn.SiLU()\n\n        dim_out = dim if dim_out is None else dim_out\n\n        if glu:\n            linear_in = GLU(dim, inner_dim, activation, dtype=dtype, device=device, operations=operations)\n        else:\n            linear_in = nn.Sequential(\n                Rearrange('b n d -> b d n') if use_conv else nn.Identity(),\n                operations.Linear(dim, inner_dim, bias = not no_bias, dtype=dtype, device=device) if not use_conv else operations.Conv1d(dim, inner_dim, conv_kernel_size, padding = (conv_kernel_size // 2), bias = not no_bias, dtype=dtype, device=device),\n                Rearrange('b n d -> b d n') if use_conv else nn.Identity(),\n                activation\n            )\n\n        linear_out = operations.Linear(inner_dim, dim_out, bias = not no_bias, dtype=dtype, device=device) if not use_conv else operations.Conv1d(inner_dim, dim_out, conv_kernel_size, padding = (conv_kernel_size // 2), bias = not no_bias, dtype=dtype, device=device)\n\n        # # init last linear layer to 0\n        # if zero_init_output:\n        #     nn.init.zeros_(linear_out.weight)\n        #     if not no_bias:\n        #         nn.init.zeros_(linear_out.bias)\n\n\n        self.ff = nn.Sequential(\n            linear_in,\n            Rearrange('b d n -> b n d') if use_conv else nn.Identity(),\n            linear_out,\n            Rearrange('b n d -> b d n') if use_conv else nn.Identity(),\n        )\n\n    def forward(self, x):\n        return self.ff(x)\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_heads = 64,\n        dim_context = None,\n        causal = False,\n        zero_init_output=True,\n        qk_norm = False,\n        natten_kernel_size = None,\n        dtype=None,\n        device=None,\n        operations=None,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.dim_heads = dim_heads\n        self.causal = causal\n\n        dim_kv = dim_context if dim_context is not None else dim\n\n        self.num_heads = dim // dim_heads\n        self.kv_heads = dim_kv // dim_heads\n\n        if dim_context is not None:\n            self.to_q = operations.Linear(dim, dim, bias=False, dtype=dtype, device=device)\n            self.to_kv = operations.Linear(dim_kv, dim_kv * 2, bias=False, dtype=dtype, device=device)\n        else:\n            self.to_qkv = operations.Linear(dim, dim * 3, bias=False, dtype=dtype, device=device)\n\n        self.to_out = operations.Linear(dim, dim, bias=False, dtype=dtype, device=device)\n\n        # if zero_init_output:\n        #     nn.init.zeros_(self.to_out.weight)\n\n        self.qk_norm = qk_norm\n\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        context_mask = None,\n        rotary_pos_emb = None,\n        causal = None\n    ):\n        h, kv_h, has_context = self.num_heads, self.kv_heads, context is not None\n\n        kv_input = context if has_context else x\n\n        if hasattr(self, 'to_q'):\n            # Use separate linear projections for q and k/v\n            q = self.to_q(x)\n            q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n\n            k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n            k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = kv_h), (k, v))\n        else:\n            # Use fused linear projection\n            q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n            q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # Normalize q and k for cosine sim attention\n        if self.qk_norm:\n            q = F.normalize(q, dim=-1)\n            k = F.normalize(k, dim=-1)\n\n        if rotary_pos_emb is not None and not has_context:\n            freqs, _ = rotary_pos_emb\n\n            q_dtype = q.dtype\n            k_dtype = k.dtype\n\n            q = q.to(torch.float32)\n            k = k.to(torch.float32)\n            freqs = freqs.to(torch.float32)\n\n            q = apply_rotary_pos_emb(q, freqs)\n            k = apply_rotary_pos_emb(k, freqs)\n\n            q = q.to(q_dtype)\n            k = k.to(k_dtype)\n\n        input_mask = context_mask\n\n        if input_mask is None and not has_context:\n            input_mask = mask\n\n        # determine masking\n        masks = []\n        final_attn_mask = None # The mask that will be applied to the attention matrix, taking all masks into account\n\n        if input_mask is not None:\n            input_mask = rearrange(input_mask, 'b j -> b 1 1 j')\n            masks.append(~input_mask)\n\n        # Other masks will be added here later\n\n        if len(masks) > 0:\n            final_attn_mask = ~or_reduce(masks)\n\n        n, device = q.shape[-2], q.device\n\n        causal = self.causal if causal is None else causal\n\n        if n == 1 and causal:\n            causal = False\n\n        if h != kv_h:\n            # Repeat interleave kv_heads to match q_heads\n            heads_per_kv_head = h // kv_h\n            k, v = map(lambda t: t.repeat_interleave(heads_per_kv_head, dim = 1), (k, v))\n\n        out = optimized_attention(q, k, v, h, skip_reshape=True)\n        out = self.to_out(out)\n\n        if mask is not None:\n            mask = rearrange(mask, 'b n -> b n 1')\n            out = out.masked_fill(~mask, 0.)\n\n        return out\n\nclass ConformerModule(nn.Module):\n    def __init__(\n        self,\n        dim,\n        norm_kwargs = {},\n    ):\n\n        super().__init__()\n\n        self.dim = dim\n\n        self.in_norm = LayerNorm(dim, **norm_kwargs)\n        self.pointwise_conv = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n        self.glu = GLU(dim, dim, nn.SiLU())\n        self.depthwise_conv = nn.Conv1d(dim, dim, kernel_size=17, groups=dim, padding=8, bias=False)\n        self.mid_norm = LayerNorm(dim, **norm_kwargs) # This is a batch norm in the original but I don't like batch norm\n        self.swish = nn.SiLU()\n        self.pointwise_conv_2 = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        x = self.in_norm(x)\n        x = rearrange(x, 'b n d -> b d n')\n        x = self.pointwise_conv(x)\n        x = rearrange(x, 'b d n -> b n d')\n        x = self.glu(x)\n        x = rearrange(x, 'b n d -> b d n')\n        x = self.depthwise_conv(x)\n        x = rearrange(x, 'b d n -> b n d')\n        x = self.mid_norm(x)\n        x = self.swish(x)\n        x = rearrange(x, 'b n d -> b d n')\n        x = self.pointwise_conv_2(x)\n        x = rearrange(x, 'b d n -> b n d')\n\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_heads = 64,\n            cross_attend = False,\n            dim_context = None,\n            global_cond_dim = None,\n            causal = False,\n            zero_init_branch_outputs = True,\n            conformer = False,\n            layer_ix = -1,\n            remove_norms = False,\n            attn_kwargs = {},\n            ff_kwargs = {},\n            norm_kwargs = {},\n            dtype=None,\n            device=None,\n            operations=None,\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.dim_heads = dim_heads\n        self.cross_attend = cross_attend\n        self.dim_context = dim_context\n        self.causal = causal\n\n        self.pre_norm = LayerNorm(dim, dtype=dtype, device=device, **norm_kwargs) if not remove_norms else nn.Identity()\n\n        self.self_attn = Attention(\n            dim,\n            dim_heads = dim_heads,\n            causal = causal,\n            zero_init_output=zero_init_branch_outputs,\n            dtype=dtype,\n            device=device,\n            operations=operations,\n            **attn_kwargs\n        )\n\n        if cross_attend:\n            self.cross_attend_norm = LayerNorm(dim, dtype=dtype, device=device, **norm_kwargs) if not remove_norms else nn.Identity()\n            self.cross_attn = Attention(\n                dim,\n                dim_heads = dim_heads,\n                dim_context=dim_context,\n                causal = causal,\n                zero_init_output=zero_init_branch_outputs,\n                dtype=dtype,\n                device=device,\n                operations=operations,\n                **attn_kwargs\n            )\n\n        self.ff_norm = LayerNorm(dim, dtype=dtype, device=device, **norm_kwargs) if not remove_norms else nn.Identity()\n        self.ff = FeedForward(dim, zero_init_output=zero_init_branch_outputs, dtype=dtype, device=device, operations=operations,**ff_kwargs)\n\n        self.layer_ix = layer_ix\n\n        self.conformer = ConformerModule(dim, norm_kwargs=norm_kwargs) if conformer else None\n\n        self.global_cond_dim = global_cond_dim\n\n        if global_cond_dim is not None:\n            self.to_scale_shift_gate = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(global_cond_dim, dim * 6, bias=False)\n            )\n\n            nn.init.zeros_(self.to_scale_shift_gate[1].weight)\n            #nn.init.zeros_(self.to_scale_shift_gate_self[1].bias)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        global_cond=None,\n        mask = None,\n        context_mask = None,\n        rotary_pos_emb = None\n    ):\n        if self.global_cond_dim is not None and self.global_cond_dim > 0 and global_cond is not None:\n\n            scale_self, shift_self, gate_self, scale_ff, shift_ff, gate_ff = self.to_scale_shift_gate(global_cond).unsqueeze(1).chunk(6, dim = -1)\n\n            # self-attention with adaLN\n            residual = x\n            x = self.pre_norm(x)\n            x = x * (1 + scale_self) + shift_self\n            x = self.self_attn(x, mask = mask, rotary_pos_emb = rotary_pos_emb)\n            x = x * torch.sigmoid(1 - gate_self)\n            x = x + residual\n\n            if context is not None:\n                x = x + self.cross_attn(self.cross_attend_norm(x), context = context, context_mask = context_mask)\n\n            if self.conformer is not None:\n                x = x + self.conformer(x)\n\n            # feedforward with adaLN\n            residual = x\n            x = self.ff_norm(x)\n            x = x * (1 + scale_ff) + shift_ff\n            x = self.ff(x)\n            x = x * torch.sigmoid(1 - gate_ff)\n            x = x + residual\n\n        else:\n            x = x + self.self_attn(self.pre_norm(x), mask = mask, rotary_pos_emb = rotary_pos_emb)\n\n            if context is not None:\n                x = x + self.cross_attn(self.cross_attend_norm(x), context = context, context_mask = context_mask)\n\n            if self.conformer is not None:\n                x = x + self.conformer(x)\n\n            x = x + self.ff(self.ff_norm(x))\n\n        return x\n\nclass ContinuousTransformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        *,\n        dim_in = None,\n        dim_out = None,\n        dim_heads = 64,\n        cross_attend=False,\n        cond_token_dim=None,\n        global_cond_dim=None,\n        causal=False,\n        rotary_pos_emb=True,\n        zero_init_branch_outputs=True,\n        conformer=False,\n        use_sinusoidal_emb=False,\n        use_abs_pos_emb=False,\n        abs_pos_emb_max_length=10000,\n        dtype=None,\n        device=None,\n        operations=None,\n        **kwargs\n        ):\n\n        super().__init__()\n\n        self.dim = dim\n        self.depth = depth\n        self.causal = causal\n        self.layers = nn.ModuleList([])\n\n        self.project_in = operations.Linear(dim_in, dim, bias=False, dtype=dtype, device=device) if dim_in is not None else nn.Identity()\n        self.project_out = operations.Linear(dim, dim_out, bias=False, dtype=dtype, device=device) if dim_out is not None else nn.Identity()\n\n        if rotary_pos_emb:\n            self.rotary_pos_emb = RotaryEmbedding(max(dim_heads // 2, 32))\n        else:\n            self.rotary_pos_emb = None\n\n        self.use_sinusoidal_emb = use_sinusoidal_emb\n        if use_sinusoidal_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(dim)\n\n        self.use_abs_pos_emb = use_abs_pos_emb\n        if use_abs_pos_emb:\n            self.pos_emb = AbsolutePositionalEmbedding(dim, abs_pos_emb_max_length)\n\n        for i in range(depth):\n            self.layers.append(\n                TransformerBlock(\n                    dim,\n                    dim_heads = dim_heads,\n                    cross_attend = cross_attend,\n                    dim_context = cond_token_dim,\n                    global_cond_dim = global_cond_dim,\n                    causal = causal,\n                    zero_init_branch_outputs = zero_init_branch_outputs,\n                    conformer=conformer,\n                    layer_ix=i,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations,\n                    **kwargs\n                )\n            )\n\n    def forward(\n        self,\n        x,\n        mask = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        global_cond = None,\n        return_info = False,\n        **kwargs\n    ):\n        batch, seq, device = *x.shape[:2], x.device\n\n        info = {\n            \"hidden_states\": [],\n        }\n\n        x = self.project_in(x)\n\n        if prepend_embeds is not None:\n            prepend_length, prepend_dim = prepend_embeds.shape[1:]\n\n            assert prepend_dim == x.shape[-1], 'prepend dimension must match sequence dimension'\n\n            x = torch.cat((prepend_embeds, x), dim = -2)\n\n            if prepend_mask is not None or mask is not None:\n                mask = mask if mask is not None else torch.ones((batch, seq), device = device, dtype = torch.bool)\n                prepend_mask = prepend_mask if prepend_mask is not None else torch.ones((batch, prepend_length), device = device, dtype = torch.bool)\n\n                mask = torch.cat((prepend_mask, mask), dim = -1)\n\n        # Attention layers\n\n        if self.rotary_pos_emb is not None:\n            rotary_pos_emb = self.rotary_pos_emb.forward_from_seq_len(x.shape[1], dtype=x.dtype, device=x.device)\n        else:\n            rotary_pos_emb = None\n\n        if self.use_sinusoidal_emb or self.use_abs_pos_emb:\n            x = x + self.pos_emb(x)\n\n        # Iterate over the transformer layers\n        for layer in self.layers:\n            x = layer(x, rotary_pos_emb = rotary_pos_emb, global_cond=global_cond, **kwargs)\n            # x = checkpoint(layer, x, rotary_pos_emb = rotary_pos_emb, global_cond=global_cond, **kwargs)\n\n            if return_info:\n                info[\"hidden_states\"].append(x)\n\n        x = self.project_out(x)\n\n        if return_info:\n            return x, info\n\n        return x\n\nclass AudioDiffusionTransformer(nn.Module):\n    def __init__(self,\n        io_channels=64,\n        patch_size=1,\n        embed_dim=1536,\n        cond_token_dim=768,\n        project_cond_tokens=False,\n        global_cond_dim=1536,\n        project_global_cond=True,\n        input_concat_dim=0,\n        prepend_cond_dim=0,\n        depth=24,\n        num_heads=24,\n        transformer_type: tp.Literal[\"continuous_transformer\"] = \"continuous_transformer\",\n        global_cond_type: tp.Literal[\"prepend\", \"adaLN\"] = \"prepend\",\n        audio_model=\"\",\n        dtype=None,\n        device=None,\n        operations=None,\n        **kwargs):\n\n        super().__init__()\n\n        self.dtype = dtype\n        self.cond_token_dim = cond_token_dim\n\n        # Timestep embeddings\n        timestep_features_dim = 256\n\n        self.timestep_features = FourierFeatures(1, timestep_features_dim, dtype=dtype, device=device)\n\n        self.to_timestep_embed = nn.Sequential(\n            operations.Linear(timestep_features_dim, embed_dim, bias=True, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(embed_dim, embed_dim, bias=True, dtype=dtype, device=device),\n        )\n\n        if cond_token_dim > 0:\n            # Conditioning tokens\n\n            cond_embed_dim = cond_token_dim if not project_cond_tokens else embed_dim\n            self.to_cond_embed = nn.Sequential(\n                operations.Linear(cond_token_dim, cond_embed_dim, bias=False, dtype=dtype, device=device),\n                nn.SiLU(),\n                operations.Linear(cond_embed_dim, cond_embed_dim, bias=False, dtype=dtype, device=device)\n            )\n        else:\n            cond_embed_dim = 0\n\n        if global_cond_dim > 0:\n            # Global conditioning\n            global_embed_dim = global_cond_dim if not project_global_cond else embed_dim\n            self.to_global_embed = nn.Sequential(\n                operations.Linear(global_cond_dim, global_embed_dim, bias=False, dtype=dtype, device=device),\n                nn.SiLU(),\n                operations.Linear(global_embed_dim, global_embed_dim, bias=False, dtype=dtype, device=device)\n            )\n\n        if prepend_cond_dim > 0:\n            # Prepend conditioning\n            self.to_prepend_embed = nn.Sequential(\n                operations.Linear(prepend_cond_dim, embed_dim, bias=False, dtype=dtype, device=device),\n                nn.SiLU(),\n                operations.Linear(embed_dim, embed_dim, bias=False, dtype=dtype, device=device)\n            )\n\n        self.input_concat_dim = input_concat_dim\n\n        dim_in = io_channels + self.input_concat_dim\n\n        self.patch_size = patch_size\n\n        # Transformer\n\n        self.transformer_type = transformer_type\n\n        self.global_cond_type = global_cond_type\n\n        if self.transformer_type == \"continuous_transformer\":\n\n            global_dim = None\n\n            if self.global_cond_type == \"adaLN\":\n                # The global conditioning is projected to the embed_dim already at this point\n                global_dim = embed_dim\n\n            self.transformer = ContinuousTransformer(\n                dim=embed_dim,\n                depth=depth,\n                dim_heads=embed_dim // num_heads,\n                dim_in=dim_in * patch_size,\n                dim_out=io_channels * patch_size,\n                cross_attend = cond_token_dim > 0,\n                cond_token_dim = cond_embed_dim,\n                global_cond_dim=global_dim,\n                dtype=dtype,\n                device=device,\n                operations=operations,\n                **kwargs\n            )\n        else:\n            raise ValueError(f\"Unknown transformer type: {self.transformer_type}\")\n\n        self.preprocess_conv = operations.Conv1d(dim_in, dim_in, 1, bias=False, dtype=dtype, device=device)\n        self.postprocess_conv = operations.Conv1d(io_channels, io_channels, 1, bias=False, dtype=dtype, device=device)\n\n    def _forward(\n        self,\n        x,\n        t,\n        mask=None,\n        cross_attn_cond=None,\n        cross_attn_cond_mask=None,\n        input_concat_cond=None,\n        global_embed=None,\n        prepend_cond=None,\n        prepend_cond_mask=None,\n        return_info=False,\n        **kwargs):\n\n        if cross_attn_cond is not None:\n            cross_attn_cond = self.to_cond_embed(cross_attn_cond)\n\n        if global_embed is not None:\n            # Project the global conditioning to the embedding dimension\n            global_embed = self.to_global_embed(global_embed)\n\n        prepend_inputs = None\n        prepend_mask = None\n        prepend_length = 0\n        if prepend_cond is not None:\n            # Project the prepend conditioning to the embedding dimension\n            prepend_cond = self.to_prepend_embed(prepend_cond)\n\n            prepend_inputs = prepend_cond\n            if prepend_cond_mask is not None:\n                prepend_mask = prepend_cond_mask\n\n        if input_concat_cond is not None:\n\n            # Interpolate input_concat_cond to the same length as x\n            if input_concat_cond.shape[2] != x.shape[2]:\n                input_concat_cond = F.interpolate(input_concat_cond, (x.shape[2], ), mode='nearest')\n\n            x = torch.cat([x, input_concat_cond], dim=1)\n\n        # Get the batch of timestep embeddings\n        timestep_embed = self.to_timestep_embed(self.timestep_features(t[:, None]).to(x.dtype)) # (b, embed_dim)\n\n        # Timestep embedding is considered a global embedding. Add to the global conditioning if it exists\n        if global_embed is not None:\n            global_embed = global_embed + timestep_embed\n        else:\n            global_embed = timestep_embed\n\n        # Add the global_embed to the prepend inputs if there is no global conditioning support in the transformer\n        if self.global_cond_type == \"prepend\":\n            if prepend_inputs is None:\n                # Prepend inputs are just the global embed, and the mask is all ones\n                prepend_inputs = global_embed.unsqueeze(1)\n                prepend_mask = torch.ones((x.shape[0], 1), device=x.device, dtype=torch.bool)\n            else:\n                # Prepend inputs are the prepend conditioning + the global embed\n                prepend_inputs = torch.cat([prepend_inputs, global_embed.unsqueeze(1)], dim=1)\n                prepend_mask = torch.cat([prepend_mask, torch.ones((x.shape[0], 1), device=x.device, dtype=torch.bool)], dim=1)\n\n            prepend_length = prepend_inputs.shape[1]\n\n        x = self.preprocess_conv(x) + x\n\n        x = rearrange(x, \"b c t -> b t c\")\n\n        extra_args = {}\n\n        if self.global_cond_type == \"adaLN\":\n            extra_args[\"global_cond\"] = global_embed\n\n        if self.patch_size > 1:\n            x = rearrange(x, \"b (t p) c -> b t (c p)\", p=self.patch_size)\n\n        if self.transformer_type == \"x-transformers\":\n            output = self.transformer(x, prepend_embeds=prepend_inputs, context=cross_attn_cond, context_mask=cross_attn_cond_mask, mask=mask, prepend_mask=prepend_mask, **extra_args, **kwargs)\n        elif self.transformer_type == \"continuous_transformer\":\n            output = self.transformer(x, prepend_embeds=prepend_inputs, context=cross_attn_cond, context_mask=cross_attn_cond_mask, mask=mask, prepend_mask=prepend_mask, return_info=return_info, **extra_args, **kwargs)\n\n            if return_info:\n                output, info = output\n        elif self.transformer_type == \"mm_transformer\":\n            output = self.transformer(x, context=cross_attn_cond, mask=mask, context_mask=cross_attn_cond_mask, **extra_args, **kwargs)\n\n        output = rearrange(output, \"b t c -> b c t\")[:,:,prepend_length:]\n\n        if self.patch_size > 1:\n            output = rearrange(output, \"b (c p) t -> b c (t p)\", p=self.patch_size)\n\n        output = self.postprocess_conv(output) + output\n\n        if return_info:\n            return output, info\n\n        return output\n\n    def forward(\n        self,\n        x,\n        timestep,\n        context=None,\n        context_mask=None,\n        input_concat_cond=None,\n        global_embed=None,\n        negative_global_embed=None,\n        prepend_cond=None,\n        prepend_cond_mask=None,\n        mask=None,\n        return_info=False,\n        control=None,\n        transformer_options={},\n        **kwargs):\n            return self._forward(\n                x,\n                timestep,\n                cross_attn_cond=context,\n                cross_attn_cond_mask=context_mask,\n                input_concat_cond=input_concat_cond,\n                global_embed=global_embed,\n                prepend_cond=prepend_cond,\n                prepend_cond_mask=prepend_cond_mask,\n                mask=mask,\n                return_info=return_info,\n                **kwargs\n            )\n", "comfy/ldm/audio/autoencoder.py": "# code adapted from: https://github.com/Stability-AI/stable-audio-tools\n\nimport torch\nfrom torch import nn\nfrom typing import Literal, Dict, Any\nimport math\nimport comfy.ops\nops = comfy.ops.disable_weight_init\n\ndef vae_sample(mean, scale):\n        stdev = nn.functional.softplus(scale) + 1e-4\n        var = stdev * stdev\n        logvar = torch.log(var)\n        latents = torch.randn_like(mean) * stdev + mean\n\n        kl = (mean * mean + var - logvar - 1).sum(1).mean()\n\n        return latents, kl\n\nclass VAEBottleneck(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.is_discrete = False\n\n    def encode(self, x, return_info=False, **kwargs):\n        info = {}\n\n        mean, scale = x.chunk(2, dim=1)\n\n        x, kl = vae_sample(mean, scale)\n\n        info[\"kl\"] = kl\n\n        if return_info:\n            return x, info\n        else:\n            return x\n\n    def decode(self, x):\n        return x\n\n\ndef snake_beta(x, alpha, beta):\n    return x + (1.0 / (beta + 0.000000001)) * pow(torch.sin(x * alpha), 2)\n\n# Adapted from https://github.com/NVIDIA/BigVGAN/blob/main/activations.py under MIT license\nclass SnakeBeta(nn.Module):\n\n    def __init__(self, in_features, alpha=1.0, alpha_trainable=True, alpha_logscale=True):\n        super(SnakeBeta, self).__init__()\n        self.in_features = in_features\n\n        # initialize alpha\n        self.alpha_logscale = alpha_logscale\n        if self.alpha_logscale: # log scale alphas initialized to zeros\n            self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n            self.beta = nn.Parameter(torch.zeros(in_features) * alpha)\n        else: # linear scale alphas initialized to ones\n            self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n            self.beta = nn.Parameter(torch.ones(in_features) * alpha)\n\n        # self.alpha.requires_grad = alpha_trainable\n        # self.beta.requires_grad = alpha_trainable\n\n        self.no_div_by_zero = 0.000000001\n\n    def forward(self, x):\n        alpha = self.alpha.unsqueeze(0).unsqueeze(-1).to(x.device) # line up with x to [B, C, T]\n        beta = self.beta.unsqueeze(0).unsqueeze(-1).to(x.device)\n        if self.alpha_logscale:\n            alpha = torch.exp(alpha)\n            beta = torch.exp(beta)\n        x = snake_beta(x, alpha, beta)\n\n        return x\n\ndef WNConv1d(*args, **kwargs):\n    try:\n        return torch.nn.utils.parametrizations.weight_norm(ops.Conv1d(*args, **kwargs))\n    except:\n        return torch.nn.utils.weight_norm(ops.Conv1d(*args, **kwargs)) #support pytorch 2.1 and older\n\ndef WNConvTranspose1d(*args, **kwargs):\n    try:\n        return torch.nn.utils.parametrizations.weight_norm(ops.ConvTranspose1d(*args, **kwargs))\n    except:\n        return torch.nn.utils.weight_norm(ops.ConvTranspose1d(*args, **kwargs)) #support pytorch 2.1 and older\n\ndef get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module:\n    if activation == \"elu\":\n        act = torch.nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = torch.nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation {activation}\")\n\n    if antialias:\n        act = Activation1d(act)\n\n    return act\n\n\nclass ResidualUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation, use_snake=False, antialias_activation=False):\n        super().__init__()\n\n        self.dilation = dilation\n\n        padding = (dilation * (7-1)) // 2\n\n        self.layers = nn.Sequential(\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=out_channels),\n            WNConv1d(in_channels=in_channels, out_channels=out_channels,\n                      kernel_size=7, dilation=dilation, padding=padding),\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=out_channels),\n            WNConv1d(in_channels=out_channels, out_channels=out_channels,\n                      kernel_size=1)\n        )\n\n    def forward(self, x):\n        res = x\n\n        #x = checkpoint(self.layers, x)\n        x = self.layers(x)\n\n        return x + res\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, use_snake=False, antialias_activation=False):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            ResidualUnit(in_channels=in_channels,\n                         out_channels=in_channels, dilation=1, use_snake=use_snake),\n            ResidualUnit(in_channels=in_channels,\n                         out_channels=in_channels, dilation=3, use_snake=use_snake),\n            ResidualUnit(in_channels=in_channels,\n                         out_channels=in_channels, dilation=9, use_snake=use_snake),\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=in_channels),\n            WNConv1d(in_channels=in_channels, out_channels=out_channels,\n                      kernel_size=2*stride, stride=stride, padding=math.ceil(stride/2)),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, use_snake=False, antialias_activation=False, use_nearest_upsample=False):\n        super().__init__()\n\n        if use_nearest_upsample:\n            upsample_layer = nn.Sequential(\n                nn.Upsample(scale_factor=stride, mode=\"nearest\"),\n                WNConv1d(in_channels=in_channels,\n                        out_channels=out_channels,\n                        kernel_size=2*stride,\n                        stride=1,\n                        bias=False,\n                        padding='same')\n            )\n        else:\n            upsample_layer = WNConvTranspose1d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=2*stride, stride=stride, padding=math.ceil(stride/2))\n\n        self.layers = nn.Sequential(\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=in_channels),\n            upsample_layer,\n            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n                         dilation=1, use_snake=use_snake),\n            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n                         dilation=3, use_snake=use_snake),\n            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n                         dilation=9, use_snake=use_snake),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass OobleckEncoder(nn.Module):\n    def __init__(self,\n                 in_channels=2,\n                 channels=128,\n                 latent_dim=32,\n                 c_mults = [1, 2, 4, 8],\n                 strides = [2, 4, 8, 8],\n                 use_snake=False,\n                 antialias_activation=False\n        ):\n        super().__init__()\n\n        c_mults = [1] + c_mults\n\n        self.depth = len(c_mults)\n\n        layers = [\n            WNConv1d(in_channels=in_channels, out_channels=c_mults[0] * channels, kernel_size=7, padding=3)\n        ]\n\n        for i in range(self.depth-1):\n            layers += [EncoderBlock(in_channels=c_mults[i]*channels, out_channels=c_mults[i+1]*channels, stride=strides[i], use_snake=use_snake)]\n\n        layers += [\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=c_mults[-1] * channels),\n            WNConv1d(in_channels=c_mults[-1]*channels, out_channels=latent_dim, kernel_size=3, padding=1)\n        ]\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass OobleckDecoder(nn.Module):\n    def __init__(self,\n                 out_channels=2,\n                 channels=128,\n                 latent_dim=32,\n                 c_mults = [1, 2, 4, 8],\n                 strides = [2, 4, 8, 8],\n                 use_snake=False,\n                 antialias_activation=False,\n                 use_nearest_upsample=False,\n                 final_tanh=True):\n        super().__init__()\n\n        c_mults = [1] + c_mults\n\n        self.depth = len(c_mults)\n\n        layers = [\n            WNConv1d(in_channels=latent_dim, out_channels=c_mults[-1]*channels, kernel_size=7, padding=3),\n        ]\n\n        for i in range(self.depth-1, 0, -1):\n            layers += [DecoderBlock(\n                in_channels=c_mults[i]*channels,\n                out_channels=c_mults[i-1]*channels,\n                stride=strides[i-1],\n                use_snake=use_snake,\n                antialias_activation=antialias_activation,\n                use_nearest_upsample=use_nearest_upsample\n                )\n            ]\n\n        layers += [\n            get_activation(\"snake\" if use_snake else \"elu\", antialias=antialias_activation, channels=c_mults[0] * channels),\n            WNConv1d(in_channels=c_mults[0] * channels, out_channels=out_channels, kernel_size=7, padding=3, bias=False),\n            nn.Tanh() if final_tanh else nn.Identity()\n        ]\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass AudioOobleckVAE(nn.Module):\n    def __init__(self,\n                 in_channels=2,\n                 channels=128,\n                 latent_dim=64,\n                 c_mults = [1, 2, 4, 8, 16],\n                 strides = [2, 4, 4, 8, 8],\n                 use_snake=True,\n                 antialias_activation=False,\n                 use_nearest_upsample=False,\n                 final_tanh=False):\n        super().__init__()\n        self.encoder = OobleckEncoder(in_channels, channels, latent_dim * 2, c_mults, strides, use_snake, antialias_activation)\n        self.decoder = OobleckDecoder(in_channels, channels, latent_dim, c_mults, strides, use_snake, antialias_activation,\n                                      use_nearest_upsample=use_nearest_upsample, final_tanh=final_tanh)\n        self.bottleneck = VAEBottleneck()\n\n    def encode(self, x):\n        return self.bottleneck.encode(self.encoder(x))\n\n    def decode(self, x):\n        return self.decoder(self.bottleneck.decode(x))\n\n", "comfy/ldm/cascade/common.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom comfy.ldm.modules.attention import optimized_attention\n\nclass Linear(torch.nn.Linear):\n    def reset_parameters(self):\n        return None\n\nclass Conv2d(torch.nn.Conv2d):\n    def reset_parameters(self):\n        return None\n\nclass OptimizedAttention(nn.Module):\n    def __init__(self, c, nhead, dropout=0.0, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.heads = nhead\n\n        self.to_q = operations.Linear(c, c, bias=True, dtype=dtype, device=device)\n        self.to_k = operations.Linear(c, c, bias=True, dtype=dtype, device=device)\n        self.to_v = operations.Linear(c, c, bias=True, dtype=dtype, device=device)\n\n        self.out_proj = operations.Linear(c, c, bias=True, dtype=dtype, device=device)\n\n    def forward(self, q, k, v):\n        q = self.to_q(q)\n        k = self.to_k(k)\n        v = self.to_v(v)\n\n        out = optimized_attention(q, k, v, self.heads)\n\n        return self.out_proj(out)\n\nclass Attention2D(nn.Module):\n    def __init__(self, c, nhead, dropout=0.0, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.attn = OptimizedAttention(c, nhead, dtype=dtype, device=device, operations=operations)\n        # self.attn = nn.MultiheadAttention(c, nhead, dropout=dropout, bias=True, batch_first=True, dtype=dtype, device=device)\n\n    def forward(self, x, kv, self_attn=False):\n        orig_shape = x.shape\n        x = x.view(x.size(0), x.size(1), -1).permute(0, 2, 1)  # Bx4xHxW -> Bx(HxW)x4\n        if self_attn:\n            kv = torch.cat([x, kv], dim=1)\n        # x = self.attn(x, kv, kv, need_weights=False)[0]\n        x = self.attn(x, kv, kv)\n        x = x.permute(0, 2, 1).view(*orig_shape)\n        return x\n\n\ndef LayerNorm2d_op(operations):\n    class LayerNorm2d(operations.LayerNorm):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def forward(self, x):\n            return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n    return LayerNorm2d\n\nclass GlobalResponseNorm(nn.Module):\n    \"from https://github.com/facebookresearch/ConvNeXt-V2/blob/3608f67cc1dae164790c5d0aead7bf2d73d9719b/models/utils.py#L105\"\n    def __init__(self, dim, dtype=None, device=None):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim, dtype=dtype, device=device))\n        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim, dtype=dtype, device=device))\n\n    def forward(self, x):\n        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n        return self.gamma.to(device=x.device, dtype=x.dtype) * (x * Nx) + self.beta.to(device=x.device, dtype=x.dtype) + x\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0, dtype=None, device=None, operations=None):  # , num_heads=4, expansion=2):\n        super().__init__()\n        self.depthwise = operations.Conv2d(c, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c, dtype=dtype, device=device)\n        #         self.depthwise = SAMBlock(c, num_heads, expansion)\n        self.norm = LayerNorm2d_op(operations)(c, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.channelwise = nn.Sequential(\n            operations.Linear(c + c_skip, c * 4, dtype=dtype, device=device),\n            nn.GELU(),\n            GlobalResponseNorm(c * 4, dtype=dtype, device=device),\n            nn.Dropout(dropout),\n            operations.Linear(c * 4, c, dtype=dtype, device=device)\n        )\n\n    def forward(self, x, x_skip=None):\n        x_res = x\n        x = self.norm(self.depthwise(x))\n        if x_skip is not None:\n            x = torch.cat([x, x_skip], dim=1)\n        x = self.channelwise(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        return x + x_res\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, c, c_cond, nhead, self_attn=True, dropout=0.0, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.self_attn = self_attn\n        self.norm = LayerNorm2d_op(operations)(c, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.attention = Attention2D(c, nhead, dropout, dtype=dtype, device=device, operations=operations)\n        self.kv_mapper = nn.Sequential(\n            nn.SiLU(),\n            operations.Linear(c_cond, c, dtype=dtype, device=device)\n        )\n\n    def forward(self, x, kv):\n        kv = self.kv_mapper(kv)\n        x = x + self.attention(self.norm(x), kv, self_attn=self.self_attn)\n        return x\n\n\nclass FeedForwardBlock(nn.Module):\n    def __init__(self, c, dropout=0.0, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.norm = LayerNorm2d_op(operations)(c, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.channelwise = nn.Sequential(\n            operations.Linear(c, c * 4, dtype=dtype, device=device),\n            nn.GELU(),\n            GlobalResponseNorm(c * 4, dtype=dtype, device=device),\n            nn.Dropout(dropout),\n            operations.Linear(c * 4, c, dtype=dtype, device=device)\n        )\n\n    def forward(self, x):\n        x = x + self.channelwise(self.norm(x).permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        return x\n\n\nclass TimestepBlock(nn.Module):\n    def __init__(self, c, c_timestep, conds=['sca'], dtype=None, device=None, operations=None):\n        super().__init__()\n        self.mapper = operations.Linear(c_timestep, c * 2, dtype=dtype, device=device)\n        self.conds = conds\n        for cname in conds:\n            setattr(self, f\"mapper_{cname}\", operations.Linear(c_timestep, c * 2, dtype=dtype, device=device))\n\n    def forward(self, x, t):\n        t = t.chunk(len(self.conds) + 1, dim=1)\n        a, b = self.mapper(t[0])[:, :, None, None].chunk(2, dim=1)\n        for i, c in enumerate(self.conds):\n            ac, bc = getattr(self, f\"mapper_{c}\")(t[i + 1])[:, :, None, None].chunk(2, dim=1)\n            a, b = a + ac, b + bc\n        return x * (1 + a) + b\n", "comfy/ldm/cascade/stage_a.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\n\nclass vector_quantize(Function):\n    @staticmethod\n    def forward(ctx, x, codebook):\n        with torch.no_grad():\n            codebook_sqr = torch.sum(codebook ** 2, dim=1)\n            x_sqr = torch.sum(x ** 2, dim=1, keepdim=True)\n\n            dist = torch.addmm(codebook_sqr + x_sqr, x, codebook.t(), alpha=-2.0, beta=1.0)\n            _, indices = dist.min(dim=1)\n\n            ctx.save_for_backward(indices, codebook)\n            ctx.mark_non_differentiable(indices)\n\n            nn = torch.index_select(codebook, 0, indices)\n            return nn, indices\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_indices):\n        grad_inputs, grad_codebook = None, None\n\n        if ctx.needs_input_grad[0]:\n            grad_inputs = grad_output.clone()\n        if ctx.needs_input_grad[1]:\n            # Gradient wrt. the codebook\n            indices, codebook = ctx.saved_tensors\n\n            grad_codebook = torch.zeros_like(codebook)\n            grad_codebook.index_add_(0, indices, grad_output)\n\n        return (grad_inputs, grad_codebook)\n\n\nclass VectorQuantize(nn.Module):\n    def __init__(self, embedding_size, k, ema_decay=0.99, ema_loss=False):\n        \"\"\"\n        Takes an input of variable size (as long as the last dimension matches the embedding size).\n        Returns one tensor containing the nearest neigbour embeddings to each of the inputs,\n        with the same size as the input, vq and commitment components for the loss as a touple\n        in the second output and the indices of the quantized vectors in the third:\n        quantized, (vq_loss, commit_loss), indices\n        \"\"\"\n        super(VectorQuantize, self).__init__()\n\n        self.codebook = nn.Embedding(k, embedding_size)\n        self.codebook.weight.data.uniform_(-1./k, 1./k)\n        self.vq = vector_quantize.apply\n\n        self.ema_decay = ema_decay\n        self.ema_loss = ema_loss\n        if ema_loss:\n            self.register_buffer('ema_element_count', torch.ones(k))\n            self.register_buffer('ema_weight_sum', torch.zeros_like(self.codebook.weight))\n\n    def _laplace_smoothing(self, x, epsilon):\n        n = torch.sum(x)\n        return ((x + epsilon) / (n + x.size(0) * epsilon) * n)\n\n    def _updateEMA(self, z_e_x, indices):\n        mask = nn.functional.one_hot(indices, self.ema_element_count.size(0)).float()\n        elem_count = mask.sum(dim=0)\n        weight_sum = torch.mm(mask.t(), z_e_x)\n\n        self.ema_element_count = (self.ema_decay * self.ema_element_count) + ((1-self.ema_decay) * elem_count)\n        self.ema_element_count = self._laplace_smoothing(self.ema_element_count, 1e-5)\n        self.ema_weight_sum = (self.ema_decay * self.ema_weight_sum) + ((1-self.ema_decay) * weight_sum)\n\n        self.codebook.weight.data = self.ema_weight_sum / self.ema_element_count.unsqueeze(-1)\n\n    def idx2vq(self, idx, dim=-1):\n        q_idx = self.codebook(idx)\n        if dim != -1:\n            q_idx = q_idx.movedim(-1, dim)\n        return q_idx\n\n    def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, c, c_hidden):\n        super().__init__()\n        # depthwise/attention\n        self.norm1 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n        self.depthwise = nn.Sequential(\n            nn.ReplicationPad2d(1),\n            nn.Conv2d(c, c, kernel_size=3, groups=c)\n        )\n\n        # channelwise\n        self.norm2 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n        self.channelwise = nn.Sequential(\n            nn.Linear(c, c_hidden),\n            nn.GELU(),\n            nn.Linear(c_hidden, c),\n        )\n\n        self.gammas = nn.Parameter(torch.zeros(6), requires_grad=True)\n\n        # Init weights\n        def _basic_init(module):\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                torch.nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n\n        self.apply(_basic_init)\n\n    def _norm(self, x, norm):\n        return norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n\n    def forward(self, x):\n        mods = self.gammas\n\n        x_temp = self._norm(x, self.norm1) * (1 + mods[0]) + mods[1]\n        try:\n            x = x + self.depthwise(x_temp) * mods[2]\n        except: #operation not implemented for bf16\n            x_temp = self.depthwise[0](x_temp.float()).to(x.dtype)\n            x = x + self.depthwise[1](x_temp) * mods[2]\n\n        x_temp = self._norm(x, self.norm2) * (1 + mods[3]) + mods[4]\n        x = x + self.channelwise(x_temp.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * mods[5]\n\n        return x\n\n\nclass StageA(nn.Module):\n    def __init__(self, levels=2, bottleneck_blocks=12, c_hidden=384, c_latent=4, codebook_size=8192):\n        super().__init__()\n        self.c_latent = c_latent\n        c_levels = [c_hidden // (2 ** i) for i in reversed(range(levels))]\n\n        # Encoder blocks\n        self.in_block = nn.Sequential(\n            nn.PixelUnshuffle(2),\n            nn.Conv2d(3 * 4, c_levels[0], kernel_size=1)\n        )\n        down_blocks = []\n        for i in range(levels):\n            if i > 0:\n                down_blocks.append(nn.Conv2d(c_levels[i - 1], c_levels[i], kernel_size=4, stride=2, padding=1))\n            block = ResBlock(c_levels[i], c_levels[i] * 4)\n            down_blocks.append(block)\n        down_blocks.append(nn.Sequential(\n            nn.Conv2d(c_levels[-1], c_latent, kernel_size=1, bias=False),\n            nn.BatchNorm2d(c_latent),  # then normalize them to have mean 0 and std 1\n        ))\n        self.down_blocks = nn.Sequential(*down_blocks)\n        self.down_blocks[0]\n\n        self.codebook_size = codebook_size\n        self.vquantizer = VectorQuantize(c_latent, k=codebook_size)\n\n        # Decoder blocks\n        up_blocks = [nn.Sequential(\n            nn.Conv2d(c_latent, c_levels[-1], kernel_size=1)\n        )]\n        for i in range(levels):\n            for j in range(bottleneck_blocks if i == 0 else 1):\n                block = ResBlock(c_levels[levels - 1 - i], c_levels[levels - 1 - i] * 4)\n                up_blocks.append(block)\n            if i < levels - 1:\n                up_blocks.append(\n                    nn.ConvTranspose2d(c_levels[levels - 1 - i], c_levels[levels - 2 - i], kernel_size=4, stride=2,\n                                       padding=1))\n        self.up_blocks = nn.Sequential(*up_blocks)\n        self.out_block = nn.Sequential(\n            nn.Conv2d(c_levels[0], 3 * 4, kernel_size=1),\n            nn.PixelShuffle(2),\n        )\n\n    def encode(self, x, quantize=False):\n        x = self.in_block(x)\n        x = self.down_blocks(x)\n        if quantize:\n            qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n            return qe, x, indices, vq_loss + commit_loss * 0.25\n        else:\n            return x\n\n    def decode(self, x):\n        x = self.up_blocks(x)\n        x = self.out_block(x)\n        return x\n\n    def forward(self, x, quantize=False):\n        qe, x, _, vq_loss = self.encode(x, quantize)\n        x = self.decode(qe)\n        return x, vq_loss\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, c_in=3, c_cond=0, c_hidden=512, depth=6):\n        super().__init__()\n        d = max(depth - 3, 3)\n        layers = [\n            nn.utils.spectral_norm(nn.Conv2d(c_in, c_hidden // (2 ** d), kernel_size=3, stride=2, padding=1)),\n            nn.LeakyReLU(0.2),\n        ]\n        for i in range(depth - 1):\n            c_in = c_hidden // (2 ** max((d - i), 0))\n            c_out = c_hidden // (2 ** max((d - 1 - i), 0))\n            layers.append(nn.utils.spectral_norm(nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1)))\n            layers.append(nn.InstanceNorm2d(c_out))\n            layers.append(nn.LeakyReLU(0.2))\n        self.encoder = nn.Sequential(*layers)\n        self.shuffle = nn.Conv2d((c_hidden + c_cond) if c_cond > 0 else c_hidden, 1, kernel_size=1)\n        self.logits = nn.Sigmoid()\n\n    def forward(self, x, cond=None):\n        x = self.encoder(x)\n        if cond is not None:\n            cond = cond.view(cond.size(0), cond.size(1), 1, 1, ).expand(-1, -1, x.size(-2), x.size(-1))\n            x = torch.cat([x, cond], dim=1)\n        x = self.shuffle(x)\n        x = self.logits(x)\n        return x\n", "comfy/ldm/cascade/controlnet.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom .common import LayerNorm2d_op\n\n\nclass CNetResBlock(nn.Module):\n    def __init__(self, c, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.blocks = nn.Sequential(\n            LayerNorm2d_op(operations)(c, dtype=dtype, device=device),\n            nn.GELU(),\n            operations.Conv2d(c, c, kernel_size=3, padding=1),\n            LayerNorm2d_op(operations)(c, dtype=dtype, device=device),\n            nn.GELU(),\n            operations.Conv2d(c, c, kernel_size=3, padding=1),\n        )\n\n    def forward(self, x):\n        return x + self.blocks(x)\n\n\nclass ControlNet(nn.Module):\n    def __init__(self, c_in=3, c_proj=2048, proj_blocks=None, bottleneck_mode=None, dtype=None, device=None, operations=nn):\n        super().__init__()\n        if bottleneck_mode is None:\n            bottleneck_mode = 'effnet'\n        self.proj_blocks = proj_blocks\n        if bottleneck_mode == 'effnet':\n            embd_channels = 1280\n            self.backbone = torchvision.models.efficientnet_v2_s().features.eval()\n            if c_in != 3:\n                in_weights = self.backbone[0][0].weight.data\n                self.backbone[0][0] = operations.Conv2d(c_in, 24, kernel_size=3, stride=2, bias=False, dtype=dtype, device=device)\n                if c_in > 3:\n                    # nn.init.constant_(self.backbone[0][0].weight, 0)\n                    self.backbone[0][0].weight.data[:, :3] = in_weights[:, :3].clone()\n                else:\n                    self.backbone[0][0].weight.data = in_weights[:, :c_in].clone()\n        elif bottleneck_mode == 'simple':\n            embd_channels = c_in\n            self.backbone = nn.Sequential(\n                operations.Conv2d(embd_channels, embd_channels * 4, kernel_size=3, padding=1, dtype=dtype, device=device),\n                nn.LeakyReLU(0.2, inplace=True),\n                operations.Conv2d(embd_channels * 4, embd_channels, kernel_size=3, padding=1, dtype=dtype, device=device),\n            )\n        elif bottleneck_mode == 'large':\n            self.backbone = nn.Sequential(\n                operations.Conv2d(c_in, 4096 * 4, kernel_size=1, dtype=dtype, device=device),\n                nn.LeakyReLU(0.2, inplace=True),\n                operations.Conv2d(4096 * 4, 1024, kernel_size=1, dtype=dtype, device=device),\n                *[CNetResBlock(1024, dtype=dtype, device=device, operations=operations) for _ in range(8)],\n                operations.Conv2d(1024, 1280, kernel_size=1, dtype=dtype, device=device),\n            )\n            embd_channels = 1280\n        else:\n            raise ValueError(f'Unknown bottleneck mode: {bottleneck_mode}')\n        self.projections = nn.ModuleList()\n        for _ in range(len(proj_blocks)):\n            self.projections.append(nn.Sequential(\n                operations.Conv2d(embd_channels, embd_channels, kernel_size=1, bias=False, dtype=dtype, device=device),\n                nn.LeakyReLU(0.2, inplace=True),\n                operations.Conv2d(embd_channels, c_proj, kernel_size=1, bias=False, dtype=dtype, device=device),\n            ))\n            # nn.init.constant_(self.projections[-1][-1].weight, 0)  # zero output projection\n        self.xl = False\n        self.input_channels = c_in\n        self.unshuffle_amount = 8\n\n    def forward(self, x):\n        x = self.backbone(x)\n        proj_outputs = [None for _ in range(max(self.proj_blocks) + 1)]\n        for i, idx in enumerate(self.proj_blocks):\n            proj_outputs[idx] = self.projections[i](x)\n        return proj_outputs\n", "comfy/ldm/cascade/stage_b.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport math\nimport torch\nfrom torch import nn\nfrom .common import AttnBlock, LayerNorm2d_op, ResBlock, FeedForwardBlock, TimestepBlock\n\nclass StageB(nn.Module):\n    def __init__(self, c_in=4, c_out=4, c_r=64, patch_size=2, c_cond=1280, c_hidden=[320, 640, 1280, 1280],\n                 nhead=[-1, -1, 20, 20], blocks=[[2, 6, 28, 6], [6, 28, 6, 2]],\n                 block_repeat=[[1, 1, 1, 1], [3, 3, 2, 2]], level_config=['CT', 'CT', 'CTA', 'CTA'], c_clip=1280,\n                 c_clip_seq=4, c_effnet=16, c_pixels=3, kernel_size=3, dropout=[0, 0, 0.0, 0.0], self_attn=True,\n                 t_conds=['sca'], stable_cascade_stage=None, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.dtype = dtype\n        self.c_r = c_r\n        self.t_conds = t_conds\n        self.c_clip_seq = c_clip_seq\n        if not isinstance(dropout, list):\n            dropout = [dropout] * len(c_hidden)\n        if not isinstance(self_attn, list):\n            self_attn = [self_attn] * len(c_hidden)\n\n        # CONDITIONING\n        self.effnet_mapper = nn.Sequential(\n            operations.Conv2d(c_effnet, c_hidden[0] * 4, kernel_size=1, dtype=dtype, device=device),\n            nn.GELU(),\n            operations.Conv2d(c_hidden[0] * 4, c_hidden[0], kernel_size=1, dtype=dtype, device=device),\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        )\n        self.pixels_mapper = nn.Sequential(\n            operations.Conv2d(c_pixels, c_hidden[0] * 4, kernel_size=1, dtype=dtype, device=device),\n            nn.GELU(),\n            operations.Conv2d(c_hidden[0] * 4, c_hidden[0], kernel_size=1, dtype=dtype, device=device),\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        )\n        self.clip_mapper = operations.Linear(c_clip, c_cond * c_clip_seq, dtype=dtype, device=device)\n        self.clip_norm = operations.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n\n        self.embedding = nn.Sequential(\n            nn.PixelUnshuffle(patch_size),\n            operations.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1, dtype=dtype, device=device),\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        )\n\n        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0, self_attn=True):\n            if block_type == 'C':\n                return ResBlock(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'A':\n                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'F':\n                return FeedForwardBlock(c_hidden, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'T':\n                return TimestepBlock(c_hidden, c_r, conds=t_conds, dtype=dtype, device=device, operations=operations)\n            else:\n                raise Exception(f'Block type {block_type} not supported')\n\n        # BLOCKS\n        # -- down blocks\n        self.down_blocks = nn.ModuleList()\n        self.down_downscalers = nn.ModuleList()\n        self.down_repeat_mappers = nn.ModuleList()\n        for i in range(len(c_hidden)):\n            if i > 0:\n                self.down_downscalers.append(nn.Sequential(\n                    LayerNorm2d_op(operations)(c_hidden[i - 1], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device),\n                    operations.Conv2d(c_hidden[i - 1], c_hidden[i], kernel_size=2, stride=2, dtype=dtype, device=device),\n                ))\n            else:\n                self.down_downscalers.append(nn.Identity())\n            down_block = nn.ModuleList()\n            for _ in range(blocks[0][i]):\n                for block_type in level_config[i]:\n                    block = get_block(block_type, c_hidden[i], nhead[i], dropout=dropout[i], self_attn=self_attn[i])\n                    down_block.append(block)\n            self.down_blocks.append(down_block)\n            if block_repeat is not None:\n                block_repeat_mappers = nn.ModuleList()\n                for _ in range(block_repeat[0][i] - 1):\n                    block_repeat_mappers.append(operations.Conv2d(c_hidden[i], c_hidden[i], kernel_size=1, dtype=dtype, device=device))\n                self.down_repeat_mappers.append(block_repeat_mappers)\n\n        # -- up blocks\n        self.up_blocks = nn.ModuleList()\n        self.up_upscalers = nn.ModuleList()\n        self.up_repeat_mappers = nn.ModuleList()\n        for i in reversed(range(len(c_hidden))):\n            if i > 0:\n                self.up_upscalers.append(nn.Sequential(\n                    LayerNorm2d_op(operations)(c_hidden[i], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device),\n                    operations.ConvTranspose2d(c_hidden[i], c_hidden[i - 1], kernel_size=2, stride=2, dtype=dtype, device=device),\n                ))\n            else:\n                self.up_upscalers.append(nn.Identity())\n            up_block = nn.ModuleList()\n            for j in range(blocks[1][::-1][i]):\n                for k, block_type in enumerate(level_config[i]):\n                    c_skip = c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0\n                    block = get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i],\n                                      self_attn=self_attn[i])\n                    up_block.append(block)\n            self.up_blocks.append(up_block)\n            if block_repeat is not None:\n                block_repeat_mappers = nn.ModuleList()\n                for _ in range(block_repeat[1][::-1][i] - 1):\n                    block_repeat_mappers.append(operations.Conv2d(c_hidden[i], c_hidden[i], kernel_size=1, dtype=dtype, device=device))\n                self.up_repeat_mappers.append(block_repeat_mappers)\n\n        # OUTPUT\n        self.clf = nn.Sequential(\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device),\n            operations.Conv2d(c_hidden[0], c_out * (patch_size ** 2), kernel_size=1, dtype=dtype, device=device),\n            nn.PixelShuffle(patch_size),\n        )\n\n        # --- WEIGHT INIT ---\n    #     self.apply(self._init_weights)  # General init\n    #     nn.init.normal_(self.clip_mapper.weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.effnet_mapper[0].weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.effnet_mapper[2].weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.pixels_mapper[0].weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.pixels_mapper[2].weight, std=0.02)  # conditionings\n    #     torch.nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n    #     nn.init.constant_(self.clf[1].weight, 0)  # outputs\n    # \n    #     # blocks\n    #     for level_block in self.down_blocks + self.up_blocks:\n    #         for block in level_block:\n    #             if isinstance(block, ResBlock) or isinstance(block, FeedForwardBlock):\n    #                 block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks[0]))\n    #             elif isinstance(block, TimestepBlock):\n    #                 for layer in block.modules():\n    #                     if isinstance(layer, nn.Linear):\n    #                         nn.init.constant_(layer.weight, 0)\n    # \n    # def _init_weights(self, m):\n    #     if isinstance(m, (nn.Conv2d, nn.Linear)):\n    #         torch.nn.init.xavier_uniform_(m.weight)\n    #         if m.bias is not None:\n    #             nn.init.constant_(m.bias, 0)\n\n    def gen_r_embedding(self, r, max_positions=10000):\n        r = r * max_positions\n        half_dim = self.c_r // 2\n        emb = math.log(max_positions) / (half_dim - 1)\n        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n        emb = r[:, None] * emb[None, :]\n        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n        if self.c_r % 2 == 1:  # zero pad\n            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n        return emb\n\n    def gen_c_embeddings(self, clip):\n        if len(clip.shape) == 2:\n            clip = clip.unsqueeze(1)\n        clip = self.clip_mapper(clip).view(clip.size(0), clip.size(1) * self.c_clip_seq, -1)\n        clip = self.clip_norm(clip)\n        return clip\n\n    def _down_encode(self, x, r_embed, clip):\n        level_outputs = []\n        block_group = zip(self.down_blocks, self.down_downscalers, self.down_repeat_mappers)\n        for down_block, downscaler, repmap in block_group:\n            x = downscaler(x)\n            for i in range(len(repmap) + 1):\n                for block in down_block:\n                    if isinstance(block, ResBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  ResBlock)):\n                        x = block(x)\n                    elif isinstance(block, AttnBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  AttnBlock)):\n                        x = block(x, clip)\n                    elif isinstance(block, TimestepBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  TimestepBlock)):\n                        x = block(x, r_embed)\n                    else:\n                        x = block(x)\n                if i < len(repmap):\n                    x = repmap[i](x)\n            level_outputs.insert(0, x)\n        return level_outputs\n\n    def _up_decode(self, level_outputs, r_embed, clip):\n        x = level_outputs[0]\n        block_group = zip(self.up_blocks, self.up_upscalers, self.up_repeat_mappers)\n        for i, (up_block, upscaler, repmap) in enumerate(block_group):\n            for j in range(len(repmap) + 1):\n                for k, block in enumerate(up_block):\n                    if isinstance(block, ResBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  ResBlock)):\n                        skip = level_outputs[i] if k == 0 and i > 0 else None\n                        if skip is not None and (x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2)):\n                            x = torch.nn.functional.interpolate(x, skip.shape[-2:], mode='bilinear',\n                                                                align_corners=True)\n                        x = block(x, skip)\n                    elif isinstance(block, AttnBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  AttnBlock)):\n                        x = block(x, clip)\n                    elif isinstance(block, TimestepBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  TimestepBlock)):\n                        x = block(x, r_embed)\n                    else:\n                        x = block(x)\n                if j < len(repmap):\n                    x = repmap[j](x)\n            x = upscaler(x)\n        return x\n\n    def forward(self, x, r, effnet, clip, pixels=None, **kwargs):\n        if pixels is None:\n            pixels = x.new_zeros(x.size(0), 3, 8, 8)\n\n        # Process the conditioning embeddings\n        r_embed = self.gen_r_embedding(r).to(dtype=x.dtype)\n        for c in self.t_conds:\n            t_cond = kwargs.get(c, torch.zeros_like(r))\n            r_embed = torch.cat([r_embed, self.gen_r_embedding(t_cond).to(dtype=x.dtype)], dim=1)\n        clip = self.gen_c_embeddings(clip)\n\n        # Model Blocks\n        x = self.embedding(x)\n        x = x + self.effnet_mapper(\n            nn.functional.interpolate(effnet, size=x.shape[-2:], mode='bilinear', align_corners=True))\n        x = x + nn.functional.interpolate(self.pixels_mapper(pixels), size=x.shape[-2:], mode='bilinear',\n                                          align_corners=True)\n        level_outputs = self._down_encode(x, r_embed, clip)\n        x = self._up_decode(level_outputs, r_embed, clip)\n        return self.clf(x)\n\n    def update_weights_ema(self, src_model, beta=0.999):\n        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n            self_params.data = self_params.data * beta + src_params.data.clone().to(self_params.device) * (1 - beta)\n        for self_buffers, src_buffers in zip(self.buffers(), src_model.buffers()):\n            self_buffers.data = self_buffers.data * beta + src_buffers.data.clone().to(self_buffers.device) * (1 - beta)\n", "comfy/ldm/cascade/stage_c.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport torch\nfrom torch import nn\nimport math\nfrom .common import AttnBlock, LayerNorm2d_op, ResBlock, FeedForwardBlock, TimestepBlock\n# from .controlnet import ControlNetDeliverer\n\nclass UpDownBlock2d(nn.Module):\n    def __init__(self, c_in, c_out, mode, enabled=True, dtype=None, device=None, operations=None):\n        super().__init__()\n        assert mode in ['up', 'down']\n        interpolation = nn.Upsample(scale_factor=2 if mode == 'up' else 0.5, mode='bilinear',\n                                    align_corners=True) if enabled else nn.Identity()\n        mapping = operations.Conv2d(c_in, c_out, kernel_size=1, dtype=dtype, device=device)\n        self.blocks = nn.ModuleList([interpolation, mapping] if mode == 'up' else [mapping, interpolation])\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n\nclass StageC(nn.Module):\n    def __init__(self, c_in=16, c_out=16, c_r=64, patch_size=1, c_cond=2048, c_hidden=[2048, 2048], nhead=[32, 32],\n                 blocks=[[8, 24], [24, 8]], block_repeat=[[1, 1], [1, 1]], level_config=['CTA', 'CTA'],\n                 c_clip_text=1280, c_clip_text_pooled=1280, c_clip_img=768, c_clip_seq=4, kernel_size=3,\n                 dropout=[0.0, 0.0], self_attn=True, t_conds=['sca', 'crp'], switch_level=[False], stable_cascade_stage=None,\n                 dtype=None, device=None, operations=None):\n        super().__init__()\n        self.dtype = dtype\n        self.c_r = c_r\n        self.t_conds = t_conds\n        self.c_clip_seq = c_clip_seq\n        if not isinstance(dropout, list):\n            dropout = [dropout] * len(c_hidden)\n        if not isinstance(self_attn, list):\n            self_attn = [self_attn] * len(c_hidden)\n\n        # CONDITIONING\n        self.clip_txt_mapper = operations.Linear(c_clip_text, c_cond, dtype=dtype, device=device)\n        self.clip_txt_pooled_mapper = operations.Linear(c_clip_text_pooled, c_cond * c_clip_seq, dtype=dtype, device=device)\n        self.clip_img_mapper = operations.Linear(c_clip_img, c_cond * c_clip_seq, dtype=dtype, device=device)\n        self.clip_norm = operations.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n\n        self.embedding = nn.Sequential(\n            nn.PixelUnshuffle(patch_size),\n            operations.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1, dtype=dtype, device=device),\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6)\n        )\n\n        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0, self_attn=True):\n            if block_type == 'C':\n                return ResBlock(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'A':\n                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'F':\n                return FeedForwardBlock(c_hidden, dropout=dropout, dtype=dtype, device=device, operations=operations)\n            elif block_type == 'T':\n                return TimestepBlock(c_hidden, c_r, conds=t_conds, dtype=dtype, device=device, operations=operations)\n            else:\n                raise Exception(f'Block type {block_type} not supported')\n\n        # BLOCKS\n        # -- down blocks\n        self.down_blocks = nn.ModuleList()\n        self.down_downscalers = nn.ModuleList()\n        self.down_repeat_mappers = nn.ModuleList()\n        for i in range(len(c_hidden)):\n            if i > 0:\n                self.down_downscalers.append(nn.Sequential(\n                    LayerNorm2d_op(operations)(c_hidden[i - 1], elementwise_affine=False, eps=1e-6),\n                    UpDownBlock2d(c_hidden[i - 1], c_hidden[i], mode='down', enabled=switch_level[i - 1], dtype=dtype, device=device, operations=operations)\n                ))\n            else:\n                self.down_downscalers.append(nn.Identity())\n            down_block = nn.ModuleList()\n            for _ in range(blocks[0][i]):\n                for block_type in level_config[i]:\n                    block = get_block(block_type, c_hidden[i], nhead[i], dropout=dropout[i], self_attn=self_attn[i])\n                    down_block.append(block)\n            self.down_blocks.append(down_block)\n            if block_repeat is not None:\n                block_repeat_mappers = nn.ModuleList()\n                for _ in range(block_repeat[0][i] - 1):\n                    block_repeat_mappers.append(operations.Conv2d(c_hidden[i], c_hidden[i], kernel_size=1, dtype=dtype, device=device))\n                self.down_repeat_mappers.append(block_repeat_mappers)\n\n        # -- up blocks\n        self.up_blocks = nn.ModuleList()\n        self.up_upscalers = nn.ModuleList()\n        self.up_repeat_mappers = nn.ModuleList()\n        for i in reversed(range(len(c_hidden))):\n            if i > 0:\n                self.up_upscalers.append(nn.Sequential(\n                    LayerNorm2d_op(operations)(c_hidden[i], elementwise_affine=False, eps=1e-6),\n                    UpDownBlock2d(c_hidden[i], c_hidden[i - 1], mode='up', enabled=switch_level[i - 1], dtype=dtype, device=device, operations=operations)\n                ))\n            else:\n                self.up_upscalers.append(nn.Identity())\n            up_block = nn.ModuleList()\n            for j in range(blocks[1][::-1][i]):\n                for k, block_type in enumerate(level_config[i]):\n                    c_skip = c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0\n                    block = get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i],\n                                      self_attn=self_attn[i])\n                    up_block.append(block)\n            self.up_blocks.append(up_block)\n            if block_repeat is not None:\n                block_repeat_mappers = nn.ModuleList()\n                for _ in range(block_repeat[1][::-1][i] - 1):\n                    block_repeat_mappers.append(operations.Conv2d(c_hidden[i], c_hidden[i], kernel_size=1, dtype=dtype, device=device))\n                self.up_repeat_mappers.append(block_repeat_mappers)\n\n        # OUTPUT\n        self.clf = nn.Sequential(\n            LayerNorm2d_op(operations)(c_hidden[0], elementwise_affine=False, eps=1e-6, dtype=dtype, device=device),\n            operations.Conv2d(c_hidden[0], c_out * (patch_size ** 2), kernel_size=1, dtype=dtype, device=device),\n            nn.PixelShuffle(patch_size),\n        )\n\n        # --- WEIGHT INIT ---\n    #     self.apply(self._init_weights)  # General init\n    #     nn.init.normal_(self.clip_txt_mapper.weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.clip_txt_pooled_mapper.weight, std=0.02)  # conditionings\n    #     nn.init.normal_(self.clip_img_mapper.weight, std=0.02)  # conditionings\n    #     torch.nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n    #     nn.init.constant_(self.clf[1].weight, 0)  # outputs\n    # \n    #     # blocks\n    #     for level_block in self.down_blocks + self.up_blocks:\n    #         for block in level_block:\n    #             if isinstance(block, ResBlock) or isinstance(block, FeedForwardBlock):\n    #                 block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks[0]))\n    #             elif isinstance(block, TimestepBlock):\n    #                 for layer in block.modules():\n    #                     if isinstance(layer, nn.Linear):\n    #                         nn.init.constant_(layer.weight, 0)\n    # \n    # def _init_weights(self, m):\n    #     if isinstance(m, (nn.Conv2d, nn.Linear)):\n    #         torch.nn.init.xavier_uniform_(m.weight)\n    #         if m.bias is not None:\n    #             nn.init.constant_(m.bias, 0)\n\n    def gen_r_embedding(self, r, max_positions=10000):\n        r = r * max_positions\n        half_dim = self.c_r // 2\n        emb = math.log(max_positions) / (half_dim - 1)\n        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n        emb = r[:, None] * emb[None, :]\n        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n        if self.c_r % 2 == 1:  # zero pad\n            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n        return emb\n\n    def gen_c_embeddings(self, clip_txt, clip_txt_pooled, clip_img):\n        clip_txt = self.clip_txt_mapper(clip_txt)\n        if len(clip_txt_pooled.shape) == 2:\n            clip_txt_pooled = clip_txt_pooled.unsqueeze(1)\n        if len(clip_img.shape) == 2:\n            clip_img = clip_img.unsqueeze(1)\n        clip_txt_pool = self.clip_txt_pooled_mapper(clip_txt_pooled).view(clip_txt_pooled.size(0), clip_txt_pooled.size(1) * self.c_clip_seq, -1)\n        clip_img = self.clip_img_mapper(clip_img).view(clip_img.size(0), clip_img.size(1) * self.c_clip_seq, -1)\n        clip = torch.cat([clip_txt, clip_txt_pool, clip_img], dim=1)\n        clip = self.clip_norm(clip)\n        return clip\n\n    def _down_encode(self, x, r_embed, clip, cnet=None):\n        level_outputs = []\n        block_group = zip(self.down_blocks, self.down_downscalers, self.down_repeat_mappers)\n        for down_block, downscaler, repmap in block_group:\n            x = downscaler(x)\n            for i in range(len(repmap) + 1):\n                for block in down_block:\n                    if isinstance(block, ResBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  ResBlock)):\n                        if cnet is not None:\n                            next_cnet = cnet.pop()\n                            if next_cnet is not None:\n                                x = x + nn.functional.interpolate(next_cnet, size=x.shape[-2:], mode='bilinear',\n                                                                  align_corners=True).to(x.dtype)\n                        x = block(x)\n                    elif isinstance(block, AttnBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  AttnBlock)):\n                        x = block(x, clip)\n                    elif isinstance(block, TimestepBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  TimestepBlock)):\n                        x = block(x, r_embed)\n                    else:\n                        x = block(x)\n                if i < len(repmap):\n                    x = repmap[i](x)\n            level_outputs.insert(0, x)\n        return level_outputs\n\n    def _up_decode(self, level_outputs, r_embed, clip, cnet=None):\n        x = level_outputs[0]\n        block_group = zip(self.up_blocks, self.up_upscalers, self.up_repeat_mappers)\n        for i, (up_block, upscaler, repmap) in enumerate(block_group):\n            for j in range(len(repmap) + 1):\n                for k, block in enumerate(up_block):\n                    if isinstance(block, ResBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  ResBlock)):\n                        skip = level_outputs[i] if k == 0 and i > 0 else None\n                        if skip is not None and (x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2)):\n                            x = torch.nn.functional.interpolate(x, skip.shape[-2:], mode='bilinear',\n                                                                align_corners=True)\n                        if cnet is not None:\n                            next_cnet = cnet.pop()\n                            if next_cnet is not None:\n                                x = x + nn.functional.interpolate(next_cnet, size=x.shape[-2:], mode='bilinear',\n                                                                  align_corners=True).to(x.dtype)\n                        x = block(x, skip)\n                    elif isinstance(block, AttnBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  AttnBlock)):\n                        x = block(x, clip)\n                    elif isinstance(block, TimestepBlock) or (\n                            hasattr(block, '_fsdp_wrapped_module') and isinstance(block._fsdp_wrapped_module,\n                                                                                  TimestepBlock)):\n                        x = block(x, r_embed)\n                    else:\n                        x = block(x)\n                if j < len(repmap):\n                    x = repmap[j](x)\n            x = upscaler(x)\n        return x\n\n    def forward(self, x, r, clip_text, clip_text_pooled, clip_img, control=None, **kwargs):\n        # Process the conditioning embeddings\n        r_embed = self.gen_r_embedding(r).to(dtype=x.dtype)\n        for c in self.t_conds:\n            t_cond = kwargs.get(c, torch.zeros_like(r))\n            r_embed = torch.cat([r_embed, self.gen_r_embedding(t_cond).to(dtype=x.dtype)], dim=1)\n        clip = self.gen_c_embeddings(clip_text, clip_text_pooled, clip_img)\n\n        if control is not None:\n            cnet = control.get(\"input\")\n        else:\n            cnet = None\n\n        # Model Blocks\n        x = self.embedding(x)\n        level_outputs = self._down_encode(x, r_embed, clip, cnet)\n        x = self._up_decode(level_outputs, r_embed, clip, cnet)\n        return self.clf(x)\n\n    def update_weights_ema(self, src_model, beta=0.999):\n        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n            self_params.data = self_params.data * beta + src_params.data.clone().to(self_params.device) * (1 - beta)\n        for self_buffers, src_buffers in zip(self.buffers(), src_model.buffers()):\n            self_buffers.data = self_buffers.data * beta + src_buffers.data.clone().to(self_buffers.device) * (1 - beta)\n", "comfy/ldm/cascade/stage_c_coder.py": "\"\"\"\n    This file is part of ComfyUI.\n    Copyright (C) 2024 Stability AI\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\nimport torch\nimport torchvision\nfrom torch import nn\n\n\n# EfficientNet\nclass EfficientNetEncoder(nn.Module):\n    def __init__(self, c_latent=16):\n        super().__init__()\n        self.backbone = torchvision.models.efficientnet_v2_s().features.eval()\n        self.mapper = nn.Sequential(\n            nn.Conv2d(1280, c_latent, kernel_size=1, bias=False),\n            nn.BatchNorm2d(c_latent, affine=False),  # then normalize them to have mean 0 and std 1\n        )\n        self.mean = nn.Parameter(torch.tensor([0.485, 0.456, 0.406]))\n        self.std = nn.Parameter(torch.tensor([0.229, 0.224, 0.225]))\n\n    def forward(self, x):\n        x = x * 0.5 + 0.5\n        x = (x - self.mean.view([3,1,1])) / self.std.view([3,1,1])\n        o = self.mapper(self.backbone(x))\n        return o\n\n\n# Fast Decoder for Stage C latents. E.g. 16 x 24 x 24 -> 3 x 192 x 192\nclass Previewer(nn.Module):\n    def __init__(self, c_in=16, c_hidden=512, c_out=3):\n        super().__init__()\n        self.blocks = nn.Sequential(\n            nn.Conv2d(c_in, c_hidden, kernel_size=1),  # 16 channels to 512 channels\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden),\n\n            nn.Conv2d(c_hidden, c_hidden, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden),\n\n            nn.ConvTranspose2d(c_hidden, c_hidden // 2, kernel_size=2, stride=2),  # 16 -> 32\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 2),\n\n            nn.Conv2d(c_hidden // 2, c_hidden // 2, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 2),\n\n            nn.ConvTranspose2d(c_hidden // 2, c_hidden // 4, kernel_size=2, stride=2),  # 32 -> 64\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 4),\n\n            nn.Conv2d(c_hidden // 4, c_hidden // 4, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 4),\n\n            nn.ConvTranspose2d(c_hidden // 4, c_hidden // 4, kernel_size=2, stride=2),  # 64 -> 128\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 4),\n\n            nn.Conv2d(c_hidden // 4, c_hidden // 4, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.BatchNorm2d(c_hidden // 4),\n\n            nn.Conv2d(c_hidden // 4, c_out, kernel_size=1),\n        )\n\n    def forward(self, x):\n        return (self.blocks(x) - 0.5) * 2.0\n\nclass StageC_coder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.previewer = Previewer()\n        self.encoder = EfficientNetEncoder()\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def decode(self, x):\n        return self.previewer(x)\n", "comfy/ldm/modules/attention.py": "import math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom typing import Optional\nimport logging\n\nfrom .diffusionmodules.util import AlphaBlender, timestep_embedding\nfrom .sub_quadratic_attention import efficient_dot_product_attention\n\nfrom comfy import model_management\n\nif model_management.xformers_enabled():\n    import xformers\n    import xformers.ops\n\nfrom comfy.cli_args import args\nimport comfy.ops\nops = comfy.ops.disable_weight_init\n\nFORCE_UPCAST_ATTENTION_DTYPE = model_management.force_upcast_attention_dtype()\n\ndef get_attn_precision(attn_precision):\n    if args.dont_upcast_attention:\n        return None\n    if FORCE_UPCAST_ATTENTION_DTYPE is not None:\n        return FORCE_UPCAST_ATTENTION_DTYPE\n    return attn_precision\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d\n\n\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\n\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.proj = operations.Linear(dim_in, dim_out * 2, dtype=dtype, device=device)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0., dtype=None, device=None, operations=ops):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            operations.Linear(dim, inner_dim, dtype=dtype, device=device),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim, dtype=dtype, device=device, operations=operations)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            operations.Linear(inner_dim, dim_out, dtype=dtype, device=device)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Normalize(in_channels, dtype=None, device=None):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\n\ndef attention_basic(q, k, v, heads, mask=None, attn_precision=None, skip_reshape=False):\n    attn_precision = get_attn_precision(attn_precision)\n\n    if skip_reshape:\n        b, _, _, dim_head = q.shape\n    else:\n        b, _, dim_head = q.shape\n        dim_head //= heads\n\n    scale = dim_head ** -0.5\n\n    h = heads\n    if skip_reshape:\n         q, k, v = map(\n            lambda t: t.reshape(b * heads, -1, dim_head),\n            (q, k, v),\n        )\n    else:\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(b, -1, heads, dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b * heads, -1, dim_head)\n            .contiguous(),\n            (q, k, v),\n        )\n\n    # force cast to fp32 to avoid overflowing\n    if attn_precision == torch.float32:\n        sim = einsum('b i d, b j d -> b i j', q.float(), k.float()) * scale\n    else:\n        sim = einsum('b i d, b j d -> b i j', q, k) * scale\n\n    del q, k\n\n    if exists(mask):\n        if mask.dtype == torch.bool:\n            mask = rearrange(mask, 'b ... -> b (...)') #TODO: check if this bool part matches pytorch attention\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n        else:\n            if len(mask.shape) == 2:\n                bs = 1\n            else:\n                bs = mask.shape[0]\n            mask = mask.reshape(bs, -1, mask.shape[-2], mask.shape[-1]).expand(b, heads, -1, -1).reshape(-1, mask.shape[-2], mask.shape[-1])\n            sim.add_(mask)\n\n    # attention, what we cannot get enough of\n    sim = sim.softmax(dim=-1)\n\n    out = einsum('b i j, b j d -> b i d', sim.to(v.dtype), v)\n    out = (\n        out.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return out\n\n\ndef attention_sub_quad(query, key, value, heads, mask=None, attn_precision=None, skip_reshape=False):\n    attn_precision = get_attn_precision(attn_precision)\n\n    if skip_reshape:\n        b, _, _, dim_head = query.shape\n    else:\n        b, _, dim_head = query.shape\n        dim_head //= heads\n\n    scale = dim_head ** -0.5\n\n    if skip_reshape:\n        query = query.reshape(b * heads, -1, dim_head)\n        value = value.reshape(b * heads, -1, dim_head)\n        key = key.reshape(b * heads, -1, dim_head).movedim(1, 2)\n    else:\n        query = query.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n        value = value.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n        key = key.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 3, 1).reshape(b * heads, dim_head, -1)\n\n\n    dtype = query.dtype\n    upcast_attention = attn_precision == torch.float32 and query.dtype != torch.float32\n    if upcast_attention:\n        bytes_per_token = torch.finfo(torch.float32).bits//8\n    else:\n        bytes_per_token = torch.finfo(query.dtype).bits//8\n    batch_x_heads, q_tokens, _ = query.shape\n    _, _, k_tokens = key.shape\n    qk_matmul_size_bytes = batch_x_heads * bytes_per_token * q_tokens * k_tokens\n\n    mem_free_total, mem_free_torch = model_management.get_free_memory(query.device, True)\n\n    kv_chunk_size_min = None\n    kv_chunk_size = None\n    query_chunk_size = None\n\n    for x in [4096, 2048, 1024, 512, 256]:\n        count = mem_free_total / (batch_x_heads * bytes_per_token * x * 4.0)\n        if count >= k_tokens:\n            kv_chunk_size = k_tokens\n            query_chunk_size = x\n            break\n\n    if query_chunk_size is None:\n        query_chunk_size = 512\n\n    if mask is not None:\n        if len(mask.shape) == 2:\n            bs = 1\n        else:\n            bs = mask.shape[0]\n        mask = mask.reshape(bs, -1, mask.shape[-2], mask.shape[-1]).expand(b, heads, -1, -1).reshape(-1, mask.shape[-2], mask.shape[-1])\n\n    hidden_states = efficient_dot_product_attention(\n        query,\n        key,\n        value,\n        query_chunk_size=query_chunk_size,\n        kv_chunk_size=kv_chunk_size,\n        kv_chunk_size_min=kv_chunk_size_min,\n        use_checkpoint=False,\n        upcast_attention=upcast_attention,\n        mask=mask,\n    )\n\n    hidden_states = hidden_states.to(dtype)\n\n    hidden_states = hidden_states.unflatten(0, (-1, heads)).transpose(1,2).flatten(start_dim=2)\n    return hidden_states\n\ndef attention_split(q, k, v, heads, mask=None, attn_precision=None, skip_reshape=False):\n    attn_precision = get_attn_precision(attn_precision)\n\n    if skip_reshape:\n        b, _, _, dim_head = q.shape\n    else:\n        b, _, dim_head = q.shape\n        dim_head //= heads\n\n    scale = dim_head ** -0.5\n\n    h = heads\n    if skip_reshape:\n         q, k, v = map(\n            lambda t: t.reshape(b * heads, -1, dim_head),\n            (q, k, v),\n        )\n    else:\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(b, -1, heads, dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b * heads, -1, dim_head)\n            .contiguous(),\n            (q, k, v),\n        )\n\n    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n\n    mem_free_total = model_management.get_free_memory(q.device)\n\n    if attn_precision == torch.float32:\n        element_size = 4\n        upcast = True\n    else:\n        element_size = q.element_size()\n        upcast = False\n\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * element_size\n    modifier = 3\n    mem_required = tensor_size * modifier\n    steps = 1\n\n\n    if mem_required > mem_free_total:\n        steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n        # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n        #      f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n\n    if steps > 64:\n        max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n        raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n                            f'Need: {mem_required/64/gb:0.1f}GB free, Have:{mem_free_total/gb:0.1f}GB free')\n\n    if mask is not None:\n        if len(mask.shape) == 2:\n            bs = 1\n        else:\n            bs = mask.shape[0]\n        mask = mask.reshape(bs, -1, mask.shape[-2], mask.shape[-1]).expand(b, heads, -1, -1).reshape(-1, mask.shape[-2], mask.shape[-1])\n\n    # print(\"steps\", steps, mem_required, mem_free_total, modifier, q.element_size(), tensor_size)\n    first_op_done = False\n    cleared_cache = False\n    while True:\n        try:\n            slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n            for i in range(0, q.shape[1], slice_size):\n                end = i + slice_size\n                if upcast:\n                    with torch.autocast(enabled=False, device_type = 'cuda'):\n                        s1 = einsum('b i d, b j d -> b i j', q[:, i:end].float(), k.float()) * scale\n                else:\n                    s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k) * scale\n\n                if mask is not None:\n                    if len(mask.shape) == 2:\n                        s1 += mask[i:end]\n                    else:\n                        s1 += mask[:, i:end]\n\n                s2 = s1.softmax(dim=-1).to(v.dtype)\n                del s1\n                first_op_done = True\n\n                r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n                del s2\n            break\n        except model_management.OOM_EXCEPTION as e:\n            if first_op_done == False:\n                model_management.soft_empty_cache(True)\n                if cleared_cache == False:\n                    cleared_cache = True\n                    logging.warning(\"out of memory error, emptying cache and trying again\")\n                    continue\n                steps *= 2\n                if steps > 64:\n                    raise e\n                logging.warning(\"out of memory error, increasing steps and trying again {}\".format(steps))\n            else:\n                raise e\n\n    del q, k, v\n\n    r1 = (\n        r1.unsqueeze(0)\n        .reshape(b, heads, -1, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b, -1, heads * dim_head)\n    )\n    return r1\n\nBROKEN_XFORMERS = False\ntry:\n    x_vers = xformers.__version__\n    # XFormers bug confirmed on all versions from 0.0.21 to 0.0.26 (q with bs bigger than 65535 gives CUDA error)\n    BROKEN_XFORMERS = x_vers.startswith(\"0.0.2\") and not x_vers.startswith(\"0.0.20\")\nexcept:\n    pass\n\ndef attention_xformers(q, k, v, heads, mask=None, attn_precision=None, skip_reshape=False):\n    if skip_reshape:\n        b, _, _, dim_head = q.shape\n    else:\n        b, _, dim_head = q.shape\n        dim_head //= heads\n\n    disabled_xformers = False\n\n    if BROKEN_XFORMERS:\n        if b * heads > 65535:\n            disabled_xformers = True\n\n    if not disabled_xformers:\n        if torch.jit.is_tracing() or torch.jit.is_scripting():\n            disabled_xformers = True\n\n    if disabled_xformers:\n        return attention_pytorch(q, k, v, heads, mask)\n\n    if skip_reshape:\n         q, k, v = map(\n            lambda t: t.reshape(b * heads, -1, dim_head),\n            (q, k, v),\n        )\n    else:\n        q, k, v = map(\n            lambda t: t.reshape(b, -1, heads, dim_head),\n            (q, k, v),\n        )\n\n    if mask is not None:\n        pad = 8 - q.shape[1] % 8\n        mask_out = torch.empty([q.shape[0], q.shape[1], q.shape[1] + pad], dtype=q.dtype, device=q.device)\n        mask_out[:, :, :mask.shape[-1]] = mask\n        mask = mask_out[:, :, :mask.shape[-1]]\n\n    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=mask)\n\n    if skip_reshape:\n        out = (\n            out.unsqueeze(0)\n            .reshape(b, heads, -1, dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b, -1, heads * dim_head)\n        )\n    else:\n        out = (\n            out.reshape(b, -1, heads * dim_head)\n        )\n\n    return out\n\ndef attention_pytorch(q, k, v, heads, mask=None, attn_precision=None, skip_reshape=False):\n    if skip_reshape:\n        b, _, _, dim_head = q.shape\n    else:\n        b, _, dim_head = q.shape\n        dim_head //= heads\n        q, k, v = map(\n            lambda t: t.view(b, -1, heads, dim_head).transpose(1, 2),\n            (q, k, v),\n        )\n\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)\n    out = (\n        out.transpose(1, 2).reshape(b, -1, heads * dim_head)\n    )\n    return out\n\n\noptimized_attention = attention_basic\n\nif model_management.xformers_enabled():\n    logging.info(\"Using xformers cross attention\")\n    optimized_attention = attention_xformers\nelif model_management.pytorch_attention_enabled():\n    logging.info(\"Using pytorch cross attention\")\n    optimized_attention = attention_pytorch\nelse:\n    if args.use_split_cross_attention:\n        logging.info(\"Using split optimization for cross attention\")\n        optimized_attention = attention_split\n    else:\n        logging.info(\"Using sub quadratic optimization for cross attention, if you have memory or speed issues try using: --use-split-cross-attention\")\n        optimized_attention = attention_sub_quad\n\noptimized_attention_masked = optimized_attention\n\ndef optimized_attention_for_device(device, mask=False, small_input=False):\n    if small_input:\n        if model_management.pytorch_attention_enabled():\n            return attention_pytorch #TODO: need to confirm but this is probably slightly faster for small inputs in all cases\n        else:\n            return attention_basic\n\n    if device == torch.device(\"cpu\"):\n        return attention_sub_quad\n\n    if mask:\n        return optimized_attention_masked\n\n    return optimized_attention\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., attn_precision=None, dtype=None, device=None, operations=ops):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        self.attn_precision = attn_precision\n\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = operations.Linear(query_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_k = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_v = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)\n\n        self.to_out = nn.Sequential(operations.Linear(inner_dim, query_dim, dtype=dtype, device=device), nn.Dropout(dropout))\n\n    def forward(self, x, context=None, value=None, mask=None):\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        if value is not None:\n            v = self.to_v(value)\n            del value\n        else:\n            v = self.to_v(context)\n\n        if mask is None:\n            out = optimized_attention(q, k, v, self.heads, attn_precision=self.attn_precision)\n        else:\n            out = optimized_attention_masked(q, k, v, self.heads, mask, attn_precision=self.attn_precision)\n        return self.to_out(out)\n\n\nclass BasicTransformerBlock(nn.Module):\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True, ff_in=False, inner_dim=None,\n                 disable_self_attn=False, disable_temporal_crossattention=False, switch_temporal_ca_to_sa=False, attn_precision=None, dtype=None, device=None, operations=ops):\n        super().__init__()\n\n        self.ff_in = ff_in or inner_dim is not None\n        if inner_dim is None:\n            inner_dim = dim\n\n        self.is_res = inner_dim == dim\n        self.attn_precision = attn_precision\n\n        if self.ff_in:\n            self.norm_in = operations.LayerNorm(dim, dtype=dtype, device=device)\n            self.ff_in = FeedForward(dim, dim_out=inner_dim, dropout=dropout, glu=gated_ff, dtype=dtype, device=device, operations=operations)\n\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = CrossAttention(query_dim=inner_dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                              context_dim=context_dim if self.disable_self_attn else None, attn_precision=self.attn_precision, dtype=dtype, device=device, operations=operations)  # is a self-attention if not self.disable_self_attn\n        self.ff = FeedForward(inner_dim, dim_out=dim, dropout=dropout, glu=gated_ff, dtype=dtype, device=device, operations=operations)\n\n        if disable_temporal_crossattention:\n            if switch_temporal_ca_to_sa:\n                raise ValueError\n            else:\n                self.attn2 = None\n        else:\n            context_dim_attn2 = None\n            if not switch_temporal_ca_to_sa:\n                context_dim_attn2 = context_dim\n\n            self.attn2 = CrossAttention(query_dim=inner_dim, context_dim=context_dim_attn2,\n                                heads=n_heads, dim_head=d_head, dropout=dropout, attn_precision=self.attn_precision, dtype=dtype, device=device, operations=operations)  # is self-attn if context is none\n            self.norm2 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n\n        self.norm1 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n        self.norm3 = operations.LayerNorm(inner_dim, dtype=dtype, device=device)\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.switch_temporal_ca_to_sa = switch_temporal_ca_to_sa\n\n    def forward(self, x, context=None, transformer_options={}):\n        extra_options = {}\n        block = transformer_options.get(\"block\", None)\n        block_index = transformer_options.get(\"block_index\", 0)\n        transformer_patches = {}\n        transformer_patches_replace = {}\n\n        for k in transformer_options:\n            if k == \"patches\":\n                transformer_patches = transformer_options[k]\n            elif k == \"patches_replace\":\n                transformer_patches_replace = transformer_options[k]\n            else:\n                extra_options[k] = transformer_options[k]\n\n        extra_options[\"n_heads\"] = self.n_heads\n        extra_options[\"dim_head\"] = self.d_head\n        extra_options[\"attn_precision\"] = self.attn_precision\n\n        if self.ff_in:\n            x_skip = x\n            x = self.ff_in(self.norm_in(x))\n            if self.is_res:\n                x += x_skip\n\n        n = self.norm1(x)\n        if self.disable_self_attn:\n            context_attn1 = context\n        else:\n            context_attn1 = None\n        value_attn1 = None\n\n        if \"attn1_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn1_patch\"]\n            if context_attn1 is None:\n                context_attn1 = n\n            value_attn1 = context_attn1\n            for p in patch:\n                n, context_attn1, value_attn1 = p(n, context_attn1, value_attn1, extra_options)\n\n        if block is not None:\n            transformer_block = (block[0], block[1], block_index)\n        else:\n            transformer_block = None\n        attn1_replace_patch = transformer_patches_replace.get(\"attn1\", {})\n        block_attn1 = transformer_block\n        if block_attn1 not in attn1_replace_patch:\n            block_attn1 = block\n\n        if block_attn1 in attn1_replace_patch:\n            if context_attn1 is None:\n                context_attn1 = n\n                value_attn1 = n\n            n = self.attn1.to_q(n)\n            context_attn1 = self.attn1.to_k(context_attn1)\n            value_attn1 = self.attn1.to_v(value_attn1)\n            n = attn1_replace_patch[block_attn1](n, context_attn1, value_attn1, extra_options)\n            n = self.attn1.to_out(n)\n        else:\n            n = self.attn1(n, context=context_attn1, value=value_attn1)\n\n        if \"attn1_output_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn1_output_patch\"]\n            for p in patch:\n                n = p(n, extra_options)\n\n        x += n\n        if \"middle_patch\" in transformer_patches:\n            patch = transformer_patches[\"middle_patch\"]\n            for p in patch:\n                x = p(x, extra_options)\n\n        if self.attn2 is not None:\n            n = self.norm2(x)\n            if self.switch_temporal_ca_to_sa:\n                context_attn2 = n\n            else:\n                context_attn2 = context\n            value_attn2 = None\n            if \"attn2_patch\" in transformer_patches:\n                patch = transformer_patches[\"attn2_patch\"]\n                value_attn2 = context_attn2\n                for p in patch:\n                    n, context_attn2, value_attn2 = p(n, context_attn2, value_attn2, extra_options)\n\n            attn2_replace_patch = transformer_patches_replace.get(\"attn2\", {})\n            block_attn2 = transformer_block\n            if block_attn2 not in attn2_replace_patch:\n                block_attn2 = block\n\n            if block_attn2 in attn2_replace_patch:\n                if value_attn2 is None:\n                    value_attn2 = context_attn2\n                n = self.attn2.to_q(n)\n                context_attn2 = self.attn2.to_k(context_attn2)\n                value_attn2 = self.attn2.to_v(value_attn2)\n                n = attn2_replace_patch[block_attn2](n, context_attn2, value_attn2, extra_options)\n                n = self.attn2.to_out(n)\n            else:\n                n = self.attn2(n, context=context_attn2, value=value_attn2)\n\n        if \"attn2_output_patch\" in transformer_patches:\n            patch = transformer_patches[\"attn2_output_patch\"]\n            for p in patch:\n                n = p(n, extra_options)\n\n        x += n\n        if self.is_res:\n            x_skip = x\n        x = self.ff(self.norm3(x))\n        if self.is_res:\n            x += x_skip\n\n        return x\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head,\n                 depth=1, dropout=0., context_dim=None,\n                 disable_self_attn=False, use_linear=False,\n                 use_checkpoint=True, attn_precision=None, dtype=None, device=None, operations=ops):\n        super().__init__()\n        if exists(context_dim) and not isinstance(context_dim, list):\n            context_dim = [context_dim] * depth\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = operations.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\n        if not use_linear:\n            self.proj_in = operations.Conv2d(in_channels,\n                                     inner_dim,\n                                     kernel_size=1,\n                                     stride=1,\n                                     padding=0, dtype=dtype, device=device)\n        else:\n            self.proj_in = operations.Linear(in_channels, inner_dim, dtype=dtype, device=device)\n\n        self.transformer_blocks = nn.ModuleList(\n            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim[d],\n                                   disable_self_attn=disable_self_attn, checkpoint=use_checkpoint, attn_precision=attn_precision, dtype=dtype, device=device, operations=operations)\n                for d in range(depth)]\n        )\n        if not use_linear:\n            self.proj_out = operations.Conv2d(inner_dim,in_channels,\n                                                  kernel_size=1,\n                                                  stride=1,\n                                                  padding=0, dtype=dtype, device=device)\n        else:\n            self.proj_out = operations.Linear(in_channels, inner_dim, dtype=dtype, device=device)\n        self.use_linear = use_linear\n\n    def forward(self, x, context=None, transformer_options={}):\n        # note: if no context is given, cross-attention defaults to self-attention\n        if not isinstance(context, list):\n            context = [context] * len(self.transformer_blocks)\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = x.movedim(1, 3).flatten(1, 2).contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        for i, block in enumerate(self.transformer_blocks):\n            transformer_options[\"block_index\"] = i\n            x = block(x, context=context[i], transformer_options=transformer_options)\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = x.reshape(x.shape[0], h, w, x.shape[-1]).movedim(3, 1).contiguous()\n        if not self.use_linear:\n            x = self.proj_out(x)\n        return x + x_in\n\n\nclass SpatialVideoTransformer(SpatialTransformer):\n    def __init__(\n        self,\n        in_channels,\n        n_heads,\n        d_head,\n        depth=1,\n        dropout=0.0,\n        use_linear=False,\n        context_dim=None,\n        use_spatial_context=False,\n        timesteps=None,\n        merge_strategy: str = \"fixed\",\n        merge_factor: float = 0.5,\n        time_context_dim=None,\n        ff_in=False,\n        checkpoint=False,\n        time_depth=1,\n        disable_self_attn=False,\n        disable_temporal_crossattention=False,\n        max_time_embed_period: int = 10000,\n        attn_precision=None,\n        dtype=None, device=None, operations=ops\n    ):\n        super().__init__(\n            in_channels,\n            n_heads,\n            d_head,\n            depth=depth,\n            dropout=dropout,\n            use_checkpoint=checkpoint,\n            context_dim=context_dim,\n            use_linear=use_linear,\n            disable_self_attn=disable_self_attn,\n            attn_precision=attn_precision,\n            dtype=dtype, device=device, operations=operations\n        )\n        self.time_depth = time_depth\n        self.depth = depth\n        self.max_time_embed_period = max_time_embed_period\n\n        time_mix_d_head = d_head\n        n_time_mix_heads = n_heads\n\n        time_mix_inner_dim = int(time_mix_d_head * n_time_mix_heads)\n\n        inner_dim = n_heads * d_head\n        if use_spatial_context:\n            time_context_dim = context_dim\n\n        self.time_stack = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    inner_dim,\n                    n_time_mix_heads,\n                    time_mix_d_head,\n                    dropout=dropout,\n                    context_dim=time_context_dim,\n                    # timesteps=timesteps,\n                    checkpoint=checkpoint,\n                    ff_in=ff_in,\n                    inner_dim=time_mix_inner_dim,\n                    disable_self_attn=disable_self_attn,\n                    disable_temporal_crossattention=disable_temporal_crossattention,\n                    attn_precision=attn_precision,\n                    dtype=dtype, device=device, operations=operations\n                )\n                for _ in range(self.depth)\n            ]\n        )\n\n        assert len(self.time_stack) == len(self.transformer_blocks)\n\n        self.use_spatial_context = use_spatial_context\n        self.in_channels = in_channels\n\n        time_embed_dim = self.in_channels * 4\n        self.time_pos_embed = nn.Sequential(\n            operations.Linear(self.in_channels, time_embed_dim, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, self.in_channels, dtype=dtype, device=device),\n        )\n\n        self.time_mixer = AlphaBlender(\n            alpha=merge_factor, merge_strategy=merge_strategy\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        context: Optional[torch.Tensor] = None,\n        time_context: Optional[torch.Tensor] = None,\n        timesteps: Optional[int] = None,\n        image_only_indicator: Optional[torch.Tensor] = None,\n        transformer_options={}\n    ) -> torch.Tensor:\n        _, _, h, w = x.shape\n        x_in = x\n        spatial_context = None\n        if exists(context):\n            spatial_context = context\n\n        if self.use_spatial_context:\n            assert (\n                context.ndim == 3\n            ), f\"n dims of spatial context should be 3 but are {context.ndim}\"\n\n            if time_context is None:\n                time_context = context\n            time_context_first_timestep = time_context[::timesteps]\n            time_context = repeat(\n                time_context_first_timestep, \"b ... -> (b n) ...\", n=h * w\n            )\n        elif time_context is not None and not self.use_spatial_context:\n            time_context = repeat(time_context, \"b ... -> (b n) ...\", n=h * w)\n            if time_context.ndim == 2:\n                time_context = rearrange(time_context, \"b c -> b 1 c\")\n\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n        if self.use_linear:\n            x = self.proj_in(x)\n\n        num_frames = torch.arange(timesteps, device=x.device)\n        num_frames = repeat(num_frames, \"t -> b t\", b=x.shape[0] // timesteps)\n        num_frames = rearrange(num_frames, \"b t -> (b t)\")\n        t_emb = timestep_embedding(num_frames, self.in_channels, repeat_only=False, max_period=self.max_time_embed_period).to(x.dtype)\n        emb = self.time_pos_embed(t_emb)\n        emb = emb[:, None, :]\n\n        for it_, (block, mix_block) in enumerate(\n            zip(self.transformer_blocks, self.time_stack)\n        ):\n            transformer_options[\"block_index\"] = it_\n            x = block(\n                x,\n                context=spatial_context,\n                transformer_options=transformer_options,\n            )\n\n            x_mix = x\n            x_mix = x_mix + emb\n\n            B, S, C = x_mix.shape\n            x_mix = rearrange(x_mix, \"(b t) s c -> (b s) t c\", t=timesteps)\n            x_mix = mix_block(x_mix, context=time_context) #TODO: transformer_options\n            x_mix = rearrange(\n                x_mix, \"(b s) t c -> (b t) s c\", s=S, b=B // timesteps, c=C, t=timesteps\n            )\n\n            x = self.time_mixer(x_spatial=x, x_temporal=x_mix, image_only_indicator=image_only_indicator)\n\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n        if not self.use_linear:\n            x = self.proj_out(x)\n        out = x + x_in\n        return out\n\n\n", "comfy/ldm/modules/sub_quadratic_attention.py": "# original source:\n#   https://github.com/AminRezaei0x443/memory-efficient-attention/blob/1bc0d9e6ac5f82ea43a375135c4e1d3896ee1694/memory_efficient_attention/attention_torch.py\n# license:\n#   MIT\n# credit:\n#   Amin Rezaei (original author)\n#   Alex Birch (optimized algorithm for 3D tensors, at the expense of removing bias, masking and callbacks)\n# implementation of:\n#   Self-attention Does Not Need O(n2) Memory\":\n#   https://arxiv.org/abs/2112.05682v2\n\nfrom functools import partial\nimport torch\nfrom torch import Tensor\nfrom torch.utils.checkpoint import checkpoint\nimport math\nimport logging\n\ntry:\n\tfrom typing import Optional, NamedTuple, List, Protocol\nexcept ImportError:\n\tfrom typing import Optional, NamedTuple, List\n\tfrom typing_extensions import Protocol\n\nfrom torch import Tensor\nfrom typing import List\n\nfrom comfy import model_management\n\ndef dynamic_slice(\n    x: Tensor,\n    starts: List[int],\n    sizes: List[int],\n) -> Tensor:\n    slicing = [slice(start, start + size) for start, size in zip(starts, sizes)]\n    return x[slicing]\n\nclass AttnChunk(NamedTuple):\n    exp_values: Tensor\n    exp_weights_sum: Tensor\n    max_score: Tensor\n\nclass SummarizeChunk(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> AttnChunk: ...\n\nclass ComputeQueryChunkAttn(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> Tensor: ...\n\ndef _summarize_chunk(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    scale: float,\n    upcast_attention: bool,\n    mask,\n) -> AttnChunk:\n    if upcast_attention:\n        with torch.autocast(enabled=False, device_type = 'cuda'):\n            query = query.float()\n            key_t = key_t.float()\n            attn_weights = torch.baddbmm(\n                torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n                query,\n                key_t,\n                alpha=scale,\n                beta=0,\n            )\n    else:\n        attn_weights = torch.baddbmm(\n            torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n            query,\n            key_t,\n            alpha=scale,\n            beta=0,\n        )\n    max_score, _ = torch.max(attn_weights, -1, keepdim=True)\n    max_score = max_score.detach()\n    attn_weights -= max_score\n    if mask is not None:\n        attn_weights += mask\n    torch.exp(attn_weights, out=attn_weights)\n    exp_weights = attn_weights.to(value.dtype)\n    exp_values = torch.bmm(exp_weights, value)\n    max_score = max_score.squeeze(-1)\n    return AttnChunk(exp_values, exp_weights.sum(dim=-1), max_score)\n\ndef _query_chunk_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    summarize_chunk: SummarizeChunk,\n    kv_chunk_size: int,\n    mask,\n) -> Tensor:\n    batch_x_heads, k_channels_per_head, k_tokens = key_t.shape\n    _, _, v_channels_per_head = value.shape\n\n    def chunk_scanner(chunk_idx: int, mask) -> AttnChunk:\n        key_chunk = dynamic_slice(\n            key_t,\n            (0, 0, chunk_idx),\n            (batch_x_heads, k_channels_per_head, kv_chunk_size)\n        )\n        value_chunk = dynamic_slice(\n            value,\n            (0, chunk_idx, 0),\n            (batch_x_heads, kv_chunk_size, v_channels_per_head)\n        )\n        if mask is not None:\n            mask = mask[:,:,chunk_idx:chunk_idx + kv_chunk_size]\n\n        return summarize_chunk(query, key_chunk, value_chunk, mask=mask)\n\n    chunks: List[AttnChunk] = [\n        chunk_scanner(chunk, mask) for chunk in torch.arange(0, k_tokens, kv_chunk_size)\n    ]\n    acc_chunk = AttnChunk(*map(torch.stack, zip(*chunks)))\n    chunk_values, chunk_weights, chunk_max = acc_chunk\n\n    global_max, _ = torch.max(chunk_max, 0, keepdim=True)\n    max_diffs = torch.exp(chunk_max - global_max)\n    chunk_values *= torch.unsqueeze(max_diffs, -1)\n    chunk_weights *= max_diffs\n\n    all_values = chunk_values.sum(dim=0)\n    all_weights = torch.unsqueeze(chunk_weights, -1).sum(dim=0)\n    return all_values / all_weights\n\n# TODO: refactor CrossAttention#get_attention_scores to share code with this\ndef _get_attention_scores_no_kv_chunking(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    scale: float,\n    upcast_attention: bool,\n    mask,\n) -> Tensor:\n    if upcast_attention:\n        with torch.autocast(enabled=False, device_type = 'cuda'):\n            query = query.float()\n            key_t = key_t.float()\n            attn_scores = torch.baddbmm(\n                torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n                query,\n                key_t,\n                alpha=scale,\n                beta=0,\n            )\n    else:\n        attn_scores = torch.baddbmm(\n            torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n            query,\n            key_t,\n            alpha=scale,\n            beta=0,\n        )\n\n    if mask is not None:\n        attn_scores += mask\n    try:\n        attn_probs = attn_scores.softmax(dim=-1)\n        del attn_scores\n    except model_management.OOM_EXCEPTION:\n        logging.warning(\"ran out of memory while running softmax in  _get_attention_scores_no_kv_chunking, trying slower in place softmax instead\")\n        attn_scores -= attn_scores.max(dim=-1, keepdim=True).values\n        torch.exp(attn_scores, out=attn_scores)\n        summed = torch.sum(attn_scores, dim=-1, keepdim=True)\n        attn_scores /= summed\n        attn_probs = attn_scores\n\n    hidden_states_slice = torch.bmm(attn_probs.to(value.dtype), value)\n    return hidden_states_slice\n\nclass ScannedChunk(NamedTuple):\n    chunk_idx: int\n    attn_chunk: AttnChunk\n\ndef efficient_dot_product_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    query_chunk_size=1024,\n    kv_chunk_size: Optional[int] = None,\n    kv_chunk_size_min: Optional[int] = None,\n    use_checkpoint=True,\n    upcast_attention=False,\n    mask = None,\n):\n    \"\"\"Computes efficient dot-product attention given query, transposed key, and value.\n      This is efficient version of attention presented in\n      https://arxiv.org/abs/2112.05682v2 which comes with O(sqrt(n)) memory requirements.\n      Args:\n        query: queries for calculating attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        key_t: keys for calculating attention with shape of\n          `[batch * num_heads, channels_per_head, tokens]`.\n        value: values to be used in attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        query_chunk_size: int: query chunks size\n        kv_chunk_size: Optional[int]: key/value chunks size. if None: defaults to sqrt(key_tokens)\n        kv_chunk_size_min: Optional[int]: key/value minimum chunk size. only considered when kv_chunk_size is None. changes `sqrt(key_tokens)` into `max(sqrt(key_tokens), kv_chunk_size_min)`, to ensure our chunk sizes don't get too small (smaller chunks = more chunks = less concurrent work done).\n        use_checkpoint: bool: whether to use checkpointing (recommended True for training, False for inference)\n      Returns:\n        Output of shape `[batch * num_heads, query_tokens, channels_per_head]`.\n      \"\"\"\n    batch_x_heads, q_tokens, q_channels_per_head = query.shape\n    _, _, k_tokens = key_t.shape\n    scale = q_channels_per_head ** -0.5\n\n    kv_chunk_size = min(kv_chunk_size or int(math.sqrt(k_tokens)), k_tokens)\n    if kv_chunk_size_min is not None:\n        kv_chunk_size = max(kv_chunk_size, kv_chunk_size_min)\n\n    if mask is not None and len(mask.shape) == 2:\n        mask = mask.unsqueeze(0)\n\n    def get_query_chunk(chunk_idx: int) -> Tensor:\n        return dynamic_slice(\n            query,\n            (0, chunk_idx, 0),\n            (batch_x_heads, min(query_chunk_size, q_tokens), q_channels_per_head)\n        )\n\n    def get_mask_chunk(chunk_idx: int) -> Tensor:\n        if mask is None:\n            return None\n        chunk = min(query_chunk_size, q_tokens)\n        return mask[:,chunk_idx:chunk_idx + chunk]\n\n    summarize_chunk: SummarizeChunk = partial(_summarize_chunk, scale=scale, upcast_attention=upcast_attention)\n    summarize_chunk: SummarizeChunk = partial(checkpoint, summarize_chunk) if use_checkpoint else summarize_chunk\n    compute_query_chunk_attn: ComputeQueryChunkAttn = partial(\n        _get_attention_scores_no_kv_chunking,\n        scale=scale,\n        upcast_attention=upcast_attention\n    ) if k_tokens <= kv_chunk_size else (\n        # fast-path for when there's just 1 key-value chunk per query chunk (this is just sliced attention btw)\n        partial(\n            _query_chunk_attention,\n            kv_chunk_size=kv_chunk_size,\n            summarize_chunk=summarize_chunk,\n        )\n    )\n\n    if q_tokens <= query_chunk_size:\n        # fast-path for when there's just 1 query chunk\n        return compute_query_chunk_attn(\n            query=query,\n            key_t=key_t,\n            value=value,\n            mask=mask,\n        )\n    \n    # TODO: maybe we should use torch.empty_like(query) to allocate storage in-advance,\n    # and pass slices to be mutated, instead of torch.cat()ing the returned slices\n    res = torch.cat([\n        compute_query_chunk_attn(\n            query=get_query_chunk(i * query_chunk_size),\n            key_t=key_t,\n            value=value,\n            mask=get_mask_chunk(i * query_chunk_size)\n        ) for i in range(math.ceil(q_tokens / query_chunk_size))\n    ], dim=1)\n    return res\n", "comfy/ldm/modules/temporal_ae.py": "import functools\nfrom typing import Callable, Iterable, Union\n\nimport torch\nfrom einops import rearrange, repeat\n\nimport comfy.ops\nops = comfy.ops.disable_weight_init\n\nfrom .diffusionmodules.model import (\n    AttnBlock,\n    Decoder,\n    ResnetBlock,\n)\nfrom .diffusionmodules.openaimodel import ResBlock, timestep_embedding\nfrom .attention import BasicTransformerBlock\n\ndef partialclass(cls, *args, **kwargs):\n    class NewCls(cls):\n        __init__ = functools.partialmethod(cls.__init__, *args, **kwargs)\n\n    return NewCls\n\n\nclass VideoResBlock(ResnetBlock):\n    def __init__(\n        self,\n        out_channels,\n        *args,\n        dropout=0.0,\n        video_kernel_size=3,\n        alpha=0.0,\n        merge_strategy=\"learned\",\n        **kwargs,\n    ):\n        super().__init__(out_channels=out_channels, dropout=dropout, *args, **kwargs)\n        if video_kernel_size is None:\n            video_kernel_size = [3, 1, 1]\n        self.time_stack = ResBlock(\n            channels=out_channels,\n            emb_channels=0,\n            dropout=dropout,\n            dims=3,\n            use_scale_shift_norm=False,\n            use_conv=False,\n            up=False,\n            down=False,\n            kernel_size=video_kernel_size,\n            use_checkpoint=False,\n            skip_t_emb=True,\n        )\n\n        self.merge_strategy = merge_strategy\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif self.merge_strategy == \"learned\":\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def get_alpha(self, bs):\n        if self.merge_strategy == \"fixed\":\n            return self.mix_factor\n        elif self.merge_strategy == \"learned\":\n            return torch.sigmoid(self.mix_factor)\n        else:\n            raise NotImplementedError()\n\n    def forward(self, x, temb, skip_video=False, timesteps=None):\n        b, c, h, w = x.shape\n        if timesteps is None:\n            timesteps = b\n\n        x = super().forward(x, temb)\n\n        if not skip_video:\n            x_mix = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n\n            x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n\n            x = self.time_stack(x, temb)\n\n            alpha = self.get_alpha(bs=b // timesteps).to(x.device)\n            x = alpha * x + (1.0 - alpha) * x_mix\n\n            x = rearrange(x, \"b c t h w -> (b t) c h w\")\n        return x\n\n\nclass AE3DConv(ops.Conv2d):\n    def __init__(self, in_channels, out_channels, video_kernel_size=3, *args, **kwargs):\n        super().__init__(in_channels, out_channels, *args, **kwargs)\n        if isinstance(video_kernel_size, Iterable):\n            padding = [int(k // 2) for k in video_kernel_size]\n        else:\n            padding = int(video_kernel_size // 2)\n\n        self.time_mix_conv = ops.Conv3d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=video_kernel_size,\n            padding=padding,\n        )\n\n    def forward(self, input, timesteps=None, skip_video=False):\n        if timesteps is None:\n            timesteps = input.shape[0]\n        x = super().forward(input)\n        if skip_video:\n            return x\n        x = rearrange(x, \"(b t) c h w -> b c t h w\", t=timesteps)\n        x = self.time_mix_conv(x)\n        return rearrange(x, \"b c t h w -> (b t) c h w\")\n\n\nclass AttnVideoBlock(AttnBlock):\n    def __init__(\n        self, in_channels: int, alpha: float = 0, merge_strategy: str = \"learned\"\n    ):\n        super().__init__(in_channels)\n        # no context, single headed, as in base class\n        self.time_mix_block = BasicTransformerBlock(\n            dim=in_channels,\n            n_heads=1,\n            d_head=in_channels,\n            checkpoint=False,\n            ff_in=True,\n        )\n\n        time_embed_dim = self.in_channels * 4\n        self.video_time_embed = torch.nn.Sequential(\n            ops.Linear(self.in_channels, time_embed_dim),\n            torch.nn.SiLU(),\n            ops.Linear(time_embed_dim, self.in_channels),\n        )\n\n        self.merge_strategy = merge_strategy\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif self.merge_strategy == \"learned\":\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def forward(self, x, timesteps=None, skip_time_block=False):\n        if skip_time_block:\n            return super().forward(x)\n\n        if timesteps is None:\n            timesteps = x.shape[0]\n\n        x_in = x\n        x = self.attention(x)\n        h, w = x.shape[2:]\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n\n        x_mix = x\n        num_frames = torch.arange(timesteps, device=x.device)\n        num_frames = repeat(num_frames, \"t -> b t\", b=x.shape[0] // timesteps)\n        num_frames = rearrange(num_frames, \"b t -> (b t)\")\n        t_emb = timestep_embedding(num_frames, self.in_channels, repeat_only=False)\n        emb = self.video_time_embed(t_emb)  # b, n_channels\n        emb = emb[:, None, :]\n        x_mix = x_mix + emb\n\n        alpha = self.get_alpha().to(x.device)\n        x_mix = self.time_mix_block(x_mix, timesteps=timesteps)\n        x = alpha * x + (1.0 - alpha) * x_mix  # alpha merge\n\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n        x = self.proj_out(x)\n\n        return x_in + x\n\n    def get_alpha(\n        self,\n    ):\n        if self.merge_strategy == \"fixed\":\n            return self.mix_factor\n        elif self.merge_strategy == \"learned\":\n            return torch.sigmoid(self.mix_factor)\n        else:\n            raise NotImplementedError(f\"unknown merge strategy {self.merge_strategy}\")\n\n\n\ndef make_time_attn(\n    in_channels,\n    attn_type=\"vanilla\",\n    attn_kwargs=None,\n    alpha: float = 0,\n    merge_strategy: str = \"learned\",\n):\n    return partialclass(\n        AttnVideoBlock, in_channels, alpha=alpha, merge_strategy=merge_strategy\n    )\n\n\nclass Conv2DWrapper(torch.nn.Conv2d):\n    def forward(self, input: torch.Tensor, **kwargs) -> torch.Tensor:\n        return super().forward(input)\n\n\nclass VideoDecoder(Decoder):\n    available_time_modes = [\"all\", \"conv-only\", \"attn-only\"]\n\n    def __init__(\n        self,\n        *args,\n        video_kernel_size: Union[int, list] = 3,\n        alpha: float = 0.0,\n        merge_strategy: str = \"learned\",\n        time_mode: str = \"conv-only\",\n        **kwargs,\n    ):\n        self.video_kernel_size = video_kernel_size\n        self.alpha = alpha\n        self.merge_strategy = merge_strategy\n        self.time_mode = time_mode\n        assert (\n            self.time_mode in self.available_time_modes\n        ), f\"time_mode parameter has to be in {self.available_time_modes}\"\n\n        if self.time_mode != \"attn-only\":\n            kwargs[\"conv_out_op\"] = partialclass(AE3DConv, video_kernel_size=self.video_kernel_size)\n        if self.time_mode not in [\"conv-only\", \"only-last-conv\"]:\n            kwargs[\"attn_op\"] = partialclass(make_time_attn, alpha=self.alpha, merge_strategy=self.merge_strategy)\n        if self.time_mode not in [\"attn-only\", \"only-last-conv\"]:\n            kwargs[\"resnet_op\"] = partialclass(VideoResBlock, video_kernel_size=self.video_kernel_size, alpha=self.alpha, merge_strategy=self.merge_strategy)\n\n        super().__init__(*args, **kwargs)\n\n    def get_last_layer(self, skip_time_mix=False, **kwargs):\n        if self.time_mode == \"attn-only\":\n            raise NotImplementedError(\"TODO\")\n        else:\n            return (\n                self.conv_out.time_mix_conv.weight\n                if not skip_time_mix\n                else self.conv_out.weight\n            )\n", "comfy/ldm/modules/ema.py": "import torch\nfrom torch import nn\n\n\nclass LitEma(nn.Module):\n    def __init__(self, model, decay=0.9999, use_num_upates=True):\n        super().__init__()\n        if decay < 0.0 or decay > 1.0:\n            raise ValueError('Decay must be between 0 and 1')\n\n        self.m_name2s_name = {}\n        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int) if use_num_upates\n        else torch.tensor(-1, dtype=torch.int))\n\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                # remove as '.'-character is not allowed in buffers\n                s_name = name.replace('.', '')\n                self.m_name2s_name.update({name: s_name})\n                self.register_buffer(s_name, p.clone().detach().data)\n\n        self.collected_params = []\n\n    def reset_num_updates(self):\n        del self.num_updates\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int))\n\n    def forward(self, model):\n        decay = self.decay\n\n        if self.num_updates >= 0:\n            self.num_updates += 1\n            decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n\n        one_minus_decay = 1.0 - decay\n\n        with torch.no_grad():\n            m_param = dict(model.named_parameters())\n            shadow_params = dict(self.named_buffers())\n\n            for key in m_param:\n                if m_param[key].requires_grad:\n                    sname = self.m_name2s_name[key]\n                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n                else:\n                    assert not key in self.m_name2s_name\n\n    def copy_to(self, model):\n        m_param = dict(model.named_parameters())\n        shadow_params = dict(self.named_buffers())\n        for key in m_param:\n            if m_param[key].requires_grad:\n                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n            else:\n                assert not key in self.m_name2s_name\n\n    def store(self, parameters):\n        \"\"\"\n        Save the current parameters for restoring later.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            temporarily stored.\n        \"\"\"\n        self.collected_params = [param.clone() for param in parameters]\n\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process. Store the parameters before the\n        `copy_to` method. After validation (or model saving), use this to\n        restore the former parameters.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            updated with the stored parameters.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)\n", "comfy/ldm/modules/encoders/noise_aug_modules.py": "from ..diffusionmodules.upscaling import ImageConcatWithNoiseAugmentation\nfrom ..diffusionmodules.openaimodel import Timestep\nimport torch\n\nclass CLIPEmbeddingNoiseAugmentation(ImageConcatWithNoiseAugmentation):\n    def __init__(self, *args, clip_stats_path=None, timestep_dim=256, **kwargs):\n        super().__init__(*args, **kwargs)\n        if clip_stats_path is None:\n            clip_mean, clip_std = torch.zeros(timestep_dim), torch.ones(timestep_dim)\n        else:\n            clip_mean, clip_std = torch.load(clip_stats_path, map_location=\"cpu\")\n        self.register_buffer(\"data_mean\", clip_mean[None, :], persistent=False)\n        self.register_buffer(\"data_std\", clip_std[None, :], persistent=False)\n        self.time_embed = Timestep(timestep_dim)\n\n    def scale(self, x):\n        # re-normalize to centered mean and unit variance\n        x = (x - self.data_mean.to(x.device)) * 1. / self.data_std.to(x.device)\n        return x\n\n    def unscale(self, x):\n        # back to original data stats\n        x = (x * self.data_std.to(x.device)) + self.data_mean.to(x.device)\n        return x\n\n    def forward(self, x, noise_level=None, seed=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        x = self.scale(x)\n        z = self.q_sample(x, noise_level, seed=seed)\n        z = self.unscale(z)\n        noise_level = self.time_embed(noise_level)\n        return z, noise_level\n", "comfy/ldm/modules/encoders/__init__.py": "", "comfy/ldm/modules/diffusionmodules/model.py": "# pytorch_diffusion + derived encoder decoder\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Optional, Any\nimport logging\n\nfrom comfy import model_management\nimport comfy.ops\nops = comfy.ops.disable_weight_init\n\nif model_management.xformers_enabled_vae():\n    import xformers\n    import xformers.ops\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, num_groups=32):\n    return ops.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        try:\n            x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        except: #operation not implemented for bf16\n            b, c, h, w = x.shape\n            out = torch.empty((b, c, h*2, w*2), dtype=x.dtype, layout=x.layout, device=x.device)\n            split = 8\n            l = out.shape[1] // split\n            for i in range(0, out.shape[1], l):\n                out[:,i:i+l] = torch.nn.functional.interpolate(x[:,i:i+l].to(torch.float32), scale_factor=2.0, mode=\"nearest\").to(x.dtype)\n            del x\n            x = out\n\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.swish = torch.nn.SiLU(inplace=True)\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = ops.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = ops.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout, inplace=True)\n        self.conv2 = ops.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = ops.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = ops.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = self.swish(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(self.swish(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = self.swish(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h\n\ndef slice_attention(q, k, v):\n    r1 = torch.zeros_like(k, device=q.device)\n    scale = (int(q.shape[-1])**(-0.5))\n\n    mem_free_total = model_management.get_free_memory(q.device)\n\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n    modifier = 3 if q.element_size() == 2 else 2.5\n    mem_required = tensor_size * modifier\n    steps = 1\n\n    if mem_required > mem_free_total:\n        steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n\n    while True:\n        try:\n            slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n            for i in range(0, q.shape[1], slice_size):\n                end = i + slice_size\n                s1 = torch.bmm(q[:, i:end], k) * scale\n\n                s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n                del s1\n\n                r1[:, :, i:end] = torch.bmm(v, s2)\n                del s2\n            break\n        except model_management.OOM_EXCEPTION as e:\n            model_management.soft_empty_cache(True)\n            steps *= 2\n            if steps > 128:\n                raise e\n            logging.warning(\"out of memory error, increasing steps and trying again {}\".format(steps))\n\n    return r1\n\ndef normal_attention(q, k, v):\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1\n    return h_\n\ndef xformers_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None)\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n    except NotImplementedError as e:\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\ndef pytorch_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, 1, C, -1).transpose(2, 3).contiguous(),\n        (q, k, v),\n    )\n\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(2, 3).reshape(B, C, H, W)\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"scaled_dot_product_attention OOMed: switched to slice attention\")\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n        if model_management.xformers_enabled_vae():\n            logging.info(\"Using xformers attention in VAE\")\n            self.optimized_attention = xformers_attention\n        elif model_management.pytorch_attention_enabled():\n            logging.info(\"Using pytorch attention in VAE\")\n            self.optimized_attention = pytorch_attention\n        else:\n            logging.info(\"Using split attention in VAE\")\n            self.optimized_attention = normal_attention\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        h_ = self.optimized_attention(q, k, v)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\ndef make_attn(in_channels, attn_type=\"vanilla\", attn_kwargs=None):\n    return AttnBlock(in_channels)\n\n\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        self.use_timestep = use_timestep\n        if self.use_timestep:\n            # timestep embedding\n            self.temb = nn.Module()\n            self.temb.dense = nn.ModuleList([\n                ops.Linear(self.ch,\n                                self.temb_ch),\n                ops.Linear(self.temb_ch,\n                                self.temb_ch),\n            ])\n\n        # downsampling\n        self.conv_in = ops.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            skip_in = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch*in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in+skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = ops.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x, t=None, context=None):\n        #assert x.shape[2] == x.shape[3] == self.resolution\n        if context is not None:\n            # assume aligned context, cat along channel axis\n            x = torch.cat((x, context), dim=1)\n        if self.use_timestep:\n            # timestep embedding\n            assert t is not None\n            temb = get_timestep_embedding(t, self.ch)\n            temb = self.temb.dense[0](temb)\n            temb = nonlinearity(temb)\n            temb = self.temb.dense[1](temb)\n        else:\n            temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n    def get_last_layer(self):\n        return self.conv_out.weight\n\n\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # downsampling\n        self.conv_in = ops.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.in_ch_mult = in_ch_mult\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = ops.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        # timestep embedding\n        temb = None\n        # downsampling\n        h = self.conv_in(x)\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n            if i_level != self.num_resolutions-1:\n                h = self.down[i_level].downsample(h)\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 conv_out_op=ops.Conv2d,\n                 resnet_op=ResnetBlock,\n                 attn_op=AttnBlock,\n                **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.tanh_out = tanh_out\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        logging.debug(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = ops.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = resnet_op(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = attn_op(block_in)\n        self.mid.block_2 = resnet_op(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(resnet_op(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(attn_op(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = conv_out_op(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z, **kwargs):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        h = self.mid.block_1(h, temb, **kwargs)\n        h = self.mid.attn_1(h, **kwargs)\n        h = self.mid.block_2(h, temb, **kwargs)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb, **kwargs)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h, **kwargs)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h, **kwargs)\n        if self.tanh_out:\n            h = torch.tanh(h)\n        return h\n", "comfy/ldm/modules/diffusionmodules/mmdit.py": "import logging\nimport math\nfrom typing import Dict, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom .. import attention\nfrom einops import rearrange, repeat\n\ndef default(x, y):\n    if x is not None:\n        return x\n    return y\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n            dtype=None,\n            device=None,\n            operations=None,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        drop_probs = drop\n        linear_layer = partial(operations.Conv2d, kernel_size=1) if use_conv else operations.Linear\n\n        self.fc1 = linear_layer(in_features, hidden_features, bias=bias, dtype=dtype, device=device)\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs)\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = linear_layer(hidden_features, out_features, bias=bias, dtype=dtype, device=device)\n        self.drop2 = nn.Dropout(drop_probs)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\nclass PatchEmbed(nn.Module):\n    \"\"\" 2D Image to Patch Embedding\n    \"\"\"\n    dynamic_img_pad: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            img_size: Optional[int] = 224,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            embed_dim: int = 768,\n            norm_layer = None,\n            flatten: bool = True,\n            bias: bool = True,\n            strict_img_size: bool = True,\n            dynamic_img_pad: bool = True,\n            dtype=None,\n            device=None,\n            operations=None,\n    ):\n        super().__init__()\n        self.patch_size = (patch_size, patch_size)\n        if img_size is not None:\n            self.img_size = (img_size, img_size)\n            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n            self.num_patches = self.grid_size[0] * self.grid_size[1]\n        else:\n            self.img_size = None\n            self.grid_size = None\n            self.num_patches = None\n\n        # flatten spatial dim and transpose to channels last, kept for bwd compat\n        self.flatten = flatten\n        self.strict_img_size = strict_img_size\n        self.dynamic_img_pad = dynamic_img_pad\n\n        self.proj = operations.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias, dtype=dtype, device=device)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # if self.img_size is not None:\n        #     if self.strict_img_size:\n        #         _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n        #         _assert(W == self.img_size[1], f\"Input width ({W}) doesn't match model ({self.img_size[1]}).\")\n        #     elif not self.dynamic_img_pad:\n        #         _assert(\n        #             H % self.patch_size[0] == 0,\n        #             f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\"\n        #         )\n        #         _assert(\n        #             W % self.patch_size[1] == 0,\n        #             f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\"\n        #         )\n        if self.dynamic_img_pad:\n            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n            x = torch.nn.functional.pad(x, (0, pad_w, 0, pad_h), mode='reflect')\n        x = self.proj(x)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n        x = self.norm(x)\n        return x\n\ndef modulate(x, shift, scale):\n    if shift is None:\n        shift = torch.zeros_like(scale)\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n\n\n#################################################################################\n#                   Sine/Cosine Positional Embedding Functions                  #\n#################################################################################\n\n\ndef get_2d_sincos_pos_embed(\n    embed_dim,\n    grid_size,\n    cls_token=False,\n    extra_tokens=0,\n    scaling_factor=None,\n    offset=None,\n):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n    if scaling_factor is not None:\n        grid = grid / scaling_factor\n    if offset is not None:\n        grid = grid - offset\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token and extra_tokens > 0:\n        pos_embed = np.concatenate(\n            [np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0\n        )\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float64)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\ndef get_1d_sincos_pos_embed_from_grid_torch(embed_dim, pos, device=None, dtype=torch.float32):\n    omega = torch.arange(embed_dim // 2, device=device, dtype=dtype)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n    pos = pos.reshape(-1)  # (M,)\n    out = torch.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n    emb_sin = torch.sin(out)  # (M, D/2)\n    emb_cos = torch.cos(out)  # (M, D/2)\n    emb = torch.cat([emb_sin, emb_cos], dim=1)  # (M, D)\n    return emb\n\ndef get_2d_sincos_pos_embed_torch(embed_dim, w, h, val_center=7.5, val_magnitude=7.5, device=None, dtype=torch.float32):\n    small = min(h, w)\n    val_h = (h / small) * val_magnitude\n    val_w = (w / small) * val_magnitude\n    grid_h, grid_w = torch.meshgrid(torch.linspace(-val_h + val_center, val_h + val_center, h, device=device, dtype=dtype), torch.linspace(-val_w + val_center, val_w + val_center, w, device=device, dtype=dtype), indexing='ij')\n    emb_h = get_1d_sincos_pos_embed_from_grid_torch(embed_dim // 2, grid_h, device=device, dtype=dtype)\n    emb_w = get_1d_sincos_pos_embed_from_grid_torch(embed_dim // 2, grid_w, device=device, dtype=dtype)\n    emb = torch.cat([emb_w, emb_h], dim=1)  # (H*W, D)\n    return emb\n\n\n#################################################################################\n#               Embedding Layers for Timesteps and Class Labels                 #\n#################################################################################\n\n\nclass TimestepEmbedder(nn.Module):\n    \"\"\"\n    Embeds scalar timesteps into vector representations.\n    \"\"\"\n\n    def __init__(self, hidden_size, frequency_embedding_size=256, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            operations.Linear(frequency_embedding_size, hidden_size, bias=True, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(hidden_size, hidden_size, bias=True, dtype=dtype, device=device),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period)\n            * torch.arange(start=0, end=half, dtype=torch.float32, device=t.device)\n            / half\n        )\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat(\n                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n            )\n        if torch.is_floating_point(t):\n            embedding = embedding.to(dtype=t.dtype)\n        return embedding\n\n    def forward(self, t, dtype, **kwargs):\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size).to(dtype)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n\n\nclass VectorEmbedder(nn.Module):\n    \"\"\"\n    Embeds a flat vector of dimension input_dim\n    \"\"\"\n\n    def __init__(self, input_dim: int, hidden_size: int, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            operations.Linear(input_dim, hidden_size, bias=True, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(hidden_size, hidden_size, bias=True, dtype=dtype, device=device),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        emb = self.mlp(x)\n        return emb\n\n\n#################################################################################\n#                                 Core DiT Model                                #\n#################################################################################\n\n\ndef split_qkv(qkv, head_dim):\n    qkv = qkv.reshape(qkv.shape[0], qkv.shape[1], 3, -1, head_dim).movedim(2, 0)\n    return qkv[0], qkv[1], qkv[2]\n\ndef optimized_attention(qkv, num_heads):\n    return attention.optimized_attention(qkv[0], qkv[1], qkv[2], num_heads)\n\nclass SelfAttention(nn.Module):\n    ATTENTION_MODES = (\"xformers\", \"torch\", \"torch-hb\", \"math\", \"debug\")\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        proj_drop: float = 0.0,\n        attn_mode: str = \"xformers\",\n        pre_only: bool = False,\n        qk_norm: Optional[str] = None,\n        rmsnorm: bool = False,\n        dtype=None,\n        device=None,\n        operations=None,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.qkv = operations.Linear(dim, dim * 3, bias=qkv_bias, dtype=dtype, device=device)\n        if not pre_only:\n            self.proj = operations.Linear(dim, dim, dtype=dtype, device=device)\n            self.proj_drop = nn.Dropout(proj_drop)\n        assert attn_mode in self.ATTENTION_MODES\n        self.attn_mode = attn_mode\n        self.pre_only = pre_only\n\n        if qk_norm == \"rms\":\n            self.ln_q = RMSNorm(self.head_dim, elementwise_affine=True, eps=1.0e-6, dtype=dtype, device=device)\n            self.ln_k = RMSNorm(self.head_dim, elementwise_affine=True, eps=1.0e-6, dtype=dtype, device=device)\n        elif qk_norm == \"ln\":\n            self.ln_q = operations.LayerNorm(self.head_dim, elementwise_affine=True, eps=1.0e-6, dtype=dtype, device=device)\n            self.ln_k = operations.LayerNorm(self.head_dim, elementwise_affine=True, eps=1.0e-6, dtype=dtype, device=device)\n        elif qk_norm is None:\n            self.ln_q = nn.Identity()\n            self.ln_k = nn.Identity()\n        else:\n            raise ValueError(qk_norm)\n\n    def pre_attention(self, x: torch.Tensor) -> torch.Tensor:\n        B, L, C = x.shape\n        qkv = self.qkv(x)\n        q, k, v = split_qkv(qkv, self.head_dim)\n        q = self.ln_q(q).reshape(q.shape[0], q.shape[1], -1)\n        k = self.ln_k(k).reshape(q.shape[0], q.shape[1], -1)\n        return (q, k, v)\n\n    def post_attention(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.pre_only\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        qkv = self.pre_attention(x)\n        x = optimized_attention(\n            qkv, num_heads=self.num_heads\n        )\n        x = self.post_attention(x)\n        return x\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(\n        self, dim: int, elementwise_affine: bool = False, eps: float = 1e-6, device=None, dtype=None\n    ):\n        \"\"\"\n        Initialize the RMSNorm normalization layer.\n        Args:\n            dim (int): The dimension of the input tensor.\n            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n        Attributes:\n            eps (float): A small value added to the denominator for numerical stability.\n            weight (nn.Parameter): Learnable scaling parameter.\n        \"\"\"\n        super().__init__()\n        self.eps = eps\n        self.learnable_scale = elementwise_affine\n        if self.learnable_scale:\n            self.weight = nn.Parameter(torch.empty(dim, device=device, dtype=dtype))\n        else:\n            self.register_parameter(\"weight\", None)\n\n    def _norm(self, x):\n        \"\"\"\n        Apply the RMSNorm normalization to the input tensor.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RMSNorm layer.\n        Args:\n            x (torch.Tensor): The input tensor.\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n        \"\"\"\n        x = self._norm(x)\n        if self.learnable_scale:\n            return x * self.weight.to(device=x.device, dtype=x.dtype)\n        else:\n            return x\n\n\nclass SwiGLUFeedForward(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        multiple_of: int,\n        ffn_dim_multiplier: Optional[float] = None,\n    ):\n        \"\"\"\n        Initialize the FeedForward module.\n\n        Args:\n            dim (int): Input dimension.\n            hidden_dim (int): Hidden dimension of the feedforward layer.\n            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n\n        Attributes:\n            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n            w2 (RowParallelLinear): Linear transformation for the second layer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \"\"\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n\nclass DismantledBlock(nn.Module):\n    \"\"\"\n    A DiT block with gated adaptive layer norm (adaLN) conditioning.\n    \"\"\"\n\n    ATTENTION_MODES = (\"xformers\", \"torch\", \"torch-hb\", \"math\", \"debug\")\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        attn_mode: str = \"xformers\",\n        qkv_bias: bool = False,\n        pre_only: bool = False,\n        rmsnorm: bool = False,\n        scale_mod_only: bool = False,\n        swiglu: bool = False,\n        qk_norm: Optional[str] = None,\n        dtype=None,\n        device=None,\n        operations=None,\n        **block_kwargs,\n    ):\n        super().__init__()\n        assert attn_mode in self.ATTENTION_MODES\n        if not rmsnorm:\n            self.norm1 = operations.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        else:\n            self.norm1 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n        self.attn = SelfAttention(\n            dim=hidden_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_mode=attn_mode,\n            pre_only=pre_only,\n            qk_norm=qk_norm,\n            rmsnorm=rmsnorm,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n        if not pre_only:\n            if not rmsnorm:\n                self.norm2 = operations.LayerNorm(\n                    hidden_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device\n                )\n            else:\n                self.norm2 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n        if not pre_only:\n            if not swiglu:\n                self.mlp = Mlp(\n                    in_features=hidden_size,\n                    hidden_features=mlp_hidden_dim,\n                    act_layer=lambda: nn.GELU(approximate=\"tanh\"),\n                    drop=0,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n            else:\n                self.mlp = SwiGLUFeedForward(\n                    dim=hidden_size,\n                    hidden_dim=mlp_hidden_dim,\n                    multiple_of=256,\n                )\n        self.scale_mod_only = scale_mod_only\n        if not scale_mod_only:\n            n_mods = 6 if not pre_only else 2\n        else:\n            n_mods = 4 if not pre_only else 1\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(), operations.Linear(hidden_size, n_mods * hidden_size, bias=True, dtype=dtype, device=device)\n        )\n        self.pre_only = pre_only\n\n    def pre_attention(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n        if not self.pre_only:\n            if not self.scale_mod_only:\n                (\n                    shift_msa,\n                    scale_msa,\n                    gate_msa,\n                    shift_mlp,\n                    scale_mlp,\n                    gate_mlp,\n                ) = self.adaLN_modulation(c).chunk(6, dim=1)\n            else:\n                shift_msa = None\n                shift_mlp = None\n                (\n                    scale_msa,\n                    gate_msa,\n                    scale_mlp,\n                    gate_mlp,\n                ) = self.adaLN_modulation(\n                    c\n                ).chunk(4, dim=1)\n            qkv = self.attn.pre_attention(modulate(self.norm1(x), shift_msa, scale_msa))\n            return qkv, (\n                x,\n                gate_msa,\n                shift_mlp,\n                scale_mlp,\n                gate_mlp,\n            )\n        else:\n            if not self.scale_mod_only:\n                (\n                    shift_msa,\n                    scale_msa,\n                ) = self.adaLN_modulation(\n                    c\n                ).chunk(2, dim=1)\n            else:\n                shift_msa = None\n                scale_msa = self.adaLN_modulation(c)\n            qkv = self.attn.pre_attention(modulate(self.norm1(x), shift_msa, scale_msa))\n            return qkv, None\n\n    def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp):\n        assert not self.pre_only\n        x = x + gate_msa.unsqueeze(1) * self.attn.post_attention(attn)\n        x = x + gate_mlp.unsqueeze(1) * self.mlp(\n            modulate(self.norm2(x), shift_mlp, scale_mlp)\n        )\n        return x\n\n    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n        assert not self.pre_only\n        qkv, intermediates = self.pre_attention(x, c)\n        attn = optimized_attention(\n            qkv,\n            num_heads=self.attn.num_heads,\n        )\n        return self.post_attention(attn, *intermediates)\n\n\ndef block_mixing(*args, use_checkpoint=True, **kwargs):\n    if use_checkpoint:\n        return torch.utils.checkpoint.checkpoint(\n            _block_mixing, *args, use_reentrant=False, **kwargs\n        )\n    else:\n        return _block_mixing(*args, **kwargs)\n\n\ndef _block_mixing(context, x, context_block, x_block, c):\n    context_qkv, context_intermediates = context_block.pre_attention(context, c)\n\n    x_qkv, x_intermediates = x_block.pre_attention(x, c)\n\n    o = []\n    for t in range(3):\n        o.append(torch.cat((context_qkv[t], x_qkv[t]), dim=1))\n    qkv = tuple(o)\n\n    attn = optimized_attention(\n        qkv,\n        num_heads=x_block.attn.num_heads,\n    )\n    context_attn, x_attn = (\n        attn[:, : context_qkv[0].shape[1]],\n        attn[:, context_qkv[0].shape[1] :],\n    )\n\n    if not context_block.pre_only:\n        context = context_block.post_attention(context_attn, *context_intermediates)\n\n    else:\n        context = None\n    x = x_block.post_attention(x_attn, *x_intermediates)\n    return context, x\n\n\nclass JointBlock(nn.Module):\n    \"\"\"just a small wrapper to serve as a fsdp unit\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        pre_only = kwargs.pop(\"pre_only\")\n        qk_norm = kwargs.pop(\"qk_norm\", None)\n        self.context_block = DismantledBlock(*args, pre_only=pre_only, qk_norm=qk_norm, **kwargs)\n        self.x_block = DismantledBlock(*args, pre_only=False, qk_norm=qk_norm, **kwargs)\n\n    def forward(self, *args, **kwargs):\n        return block_mixing(\n            *args, context_block=self.context_block, x_block=self.x_block, **kwargs\n        )\n\n\nclass FinalLayer(nn.Module):\n    \"\"\"\n    The final layer of DiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        patch_size: int,\n        out_channels: int,\n        total_out_channels: Optional[int] = None,\n        dtype=None,\n        device=None,\n        operations=None,\n    ):\n        super().__init__()\n        self.norm_final = operations.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.linear = (\n            operations.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True, dtype=dtype, device=device)\n            if (total_out_channels is None)\n            else operations.Linear(hidden_size, total_out_channels, bias=True, dtype=dtype, device=device)\n        )\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(), operations.Linear(hidden_size, 2 * hidden_size, bias=True, dtype=dtype, device=device)\n        )\n\n    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n        x = modulate(self.norm_final(x), shift, scale)\n        x = self.linear(x)\n        return x\n\nclass SelfAttentionContext(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, dtype=None, device=None, operations=None):\n        super().__init__()\n        dim_head = dim // heads\n        inner_dim = dim\n\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.qkv = operations.Linear(dim, dim * 3, bias=True, dtype=dtype, device=device)\n\n        self.proj = operations.Linear(inner_dim, dim, dtype=dtype, device=device)\n\n    def forward(self, x):\n        qkv = self.qkv(x)\n        q, k, v = split_qkv(qkv, self.dim_head)\n        x = optimized_attention((q.reshape(q.shape[0], q.shape[1], -1), k, v), self.heads)\n        return self.proj(x)\n\nclass ContextProcessorBlock(nn.Module):\n    def __init__(self, context_size, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.norm1 = operations.LayerNorm(context_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.attn = SelfAttentionContext(context_size, dtype=dtype, device=device, operations=operations)\n        self.norm2 = operations.LayerNorm(context_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n        self.mlp = Mlp(in_features=context_size, hidden_features=(context_size * 4), act_layer=lambda: nn.GELU(approximate=\"tanh\"), drop=0, dtype=dtype, device=device, operations=operations)\n\n    def forward(self, x):\n        x += self.attn(self.norm1(x))\n        x += self.mlp(self.norm2(x))\n        return x\n\nclass ContextProcessor(nn.Module):\n    def __init__(self, context_size, num_layers, dtype=None, device=None, operations=None):\n        super().__init__()\n        self.layers = torch.nn.ModuleList([ContextProcessorBlock(context_size, dtype=dtype, device=device, operations=operations) for i in range(num_layers)])\n        self.norm = operations.LayerNorm(context_size, elementwise_affine=False, eps=1e-6, dtype=dtype, device=device)\n\n    def forward(self, x):\n        for i, l in enumerate(self.layers):\n            x = l(x)\n        return self.norm(x)\n\nclass MMDiT(nn.Module):\n    \"\"\"\n    Diffusion model with a Transformer backbone.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int = 32,\n        patch_size: int = 2,\n        in_channels: int = 4,\n        depth: int = 28,\n        # hidden_size: Optional[int] = None,\n        # num_heads: Optional[int] = None,\n        mlp_ratio: float = 4.0,\n        learn_sigma: bool = False,\n        adm_in_channels: Optional[int] = None,\n        context_embedder_config: Optional[Dict] = None,\n        compile_core: bool = False,\n        use_checkpoint: bool = False,\n        register_length: int = 0,\n        attn_mode: str = \"torch\",\n        rmsnorm: bool = False,\n        scale_mod_only: bool = False,\n        swiglu: bool = False,\n        out_channels: Optional[int] = None,\n        pos_embed_scaling_factor: Optional[float] = None,\n        pos_embed_offset: Optional[float] = None,\n        pos_embed_max_size: Optional[int] = None,\n        num_patches = None,\n        qk_norm: Optional[str] = None,\n        qkv_bias: bool = True,\n        context_processor_layers = None,\n        context_size = 4096,\n        dtype = None, #TODO\n        device = None,\n        operations = None,\n    ):\n        super().__init__()\n        self.dtype = dtype\n        self.learn_sigma = learn_sigma\n        self.in_channels = in_channels\n        default_out_channels = in_channels * 2 if learn_sigma else in_channels\n        self.out_channels = default(out_channels, default_out_channels)\n        self.patch_size = patch_size\n        self.pos_embed_scaling_factor = pos_embed_scaling_factor\n        self.pos_embed_offset = pos_embed_offset\n        self.pos_embed_max_size = pos_embed_max_size\n\n        # hidden_size = default(hidden_size, 64 * depth)\n        # num_heads = default(num_heads, hidden_size // 64)\n\n        # apply magic --> this defines a head_size of 64\n        self.hidden_size = 64 * depth\n        num_heads = depth\n\n        self.num_heads = num_heads\n\n        self.x_embedder = PatchEmbed(\n            input_size,\n            patch_size,\n            in_channels,\n            self.hidden_size,\n            bias=True,\n            strict_img_size=self.pos_embed_max_size is None,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n        self.t_embedder = TimestepEmbedder(self.hidden_size, dtype=dtype, device=device, operations=operations)\n\n        self.y_embedder = None\n        if adm_in_channels is not None:\n            assert isinstance(adm_in_channels, int)\n            self.y_embedder = VectorEmbedder(adm_in_channels, self.hidden_size, dtype=dtype, device=device, operations=operations)\n\n        if context_processor_layers is not None:\n            self.context_processor = ContextProcessor(context_size, context_processor_layers, dtype=dtype, device=device, operations=operations)\n        else:\n            self.context_processor = None\n\n        self.context_embedder = nn.Identity()\n        if context_embedder_config is not None:\n            if context_embedder_config[\"target\"] == \"torch.nn.Linear\":\n                self.context_embedder = operations.Linear(**context_embedder_config[\"params\"], dtype=dtype, device=device)\n\n        self.register_length = register_length\n        if self.register_length > 0:\n            self.register = nn.Parameter(torch.randn(1, register_length, self.hidden_size, dtype=dtype, device=device))\n\n        # num_patches = self.x_embedder.num_patches\n        # Will use fixed sin-cos embedding:\n        # just use a buffer already\n        if num_patches is not None:\n            self.register_buffer(\n                \"pos_embed\",\n                torch.empty(1, num_patches, self.hidden_size, dtype=dtype, device=device),\n            )\n        else:\n            self.pos_embed = None\n\n        self.use_checkpoint = use_checkpoint\n        self.joint_blocks = nn.ModuleList(\n            [\n                JointBlock(\n                    self.hidden_size,\n                    num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    attn_mode=attn_mode,\n                    pre_only=i == depth - 1,\n                    rmsnorm=rmsnorm,\n                    scale_mod_only=scale_mod_only,\n                    swiglu=swiglu,\n                    qk_norm=qk_norm,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.final_layer = FinalLayer(self.hidden_size, patch_size, self.out_channels, dtype=dtype, device=device, operations=operations)\n\n        if compile_core:\n            assert False\n            self.forward_core_with_concat = torch.compile(self.forward_core_with_concat)\n\n    def cropped_pos_embed(self, hw, device=None):\n        p = self.x_embedder.patch_size[0]\n        h, w = hw\n        # patched size\n        h = (h + 1) // p\n        w = (w + 1) // p\n        if self.pos_embed is None:\n            return get_2d_sincos_pos_embed_torch(self.hidden_size, w, h, device=device)\n        assert self.pos_embed_max_size is not None\n        assert h <= self.pos_embed_max_size, (h, self.pos_embed_max_size)\n        assert w <= self.pos_embed_max_size, (w, self.pos_embed_max_size)\n        top = (self.pos_embed_max_size - h) // 2\n        left = (self.pos_embed_max_size - w) // 2\n        spatial_pos_embed = rearrange(\n            self.pos_embed,\n            \"1 (h w) c -> 1 h w c\",\n            h=self.pos_embed_max_size,\n            w=self.pos_embed_max_size,\n        )\n        spatial_pos_embed = spatial_pos_embed[:, top : top + h, left : left + w, :]\n        spatial_pos_embed = rearrange(spatial_pos_embed, \"1 h w c -> 1 (h w) c\")\n        # print(spatial_pos_embed, top, left, h, w)\n        # # t = get_2d_sincos_pos_embed_torch(self.hidden_size, w, h, 7.875, 7.875, device=device) #matches exactly for 1024 res\n        # t = get_2d_sincos_pos_embed_torch(self.hidden_size, w, h, 7.5, 7.5, device=device) #scales better\n        # # print(t)\n        # return t\n        return spatial_pos_embed\n\n    def unpatchify(self, x, hw=None):\n        \"\"\"\n        x: (N, T, patch_size**2 * C)\n        imgs: (N, H, W, C)\n        \"\"\"\n        c = self.out_channels\n        p = self.x_embedder.patch_size[0]\n        if hw is None:\n            h = w = int(x.shape[1] ** 0.5)\n        else:\n            h, w = hw\n            h = (h + 1) // p\n            w = (w + 1) // p\n        assert h * w == x.shape[1]\n\n        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n        return imgs\n\n    def forward_core_with_concat(\n        self,\n        x: torch.Tensor,\n        c_mod: torch.Tensor,\n        context: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        if self.register_length > 0:\n            context = torch.cat(\n                (\n                    repeat(self.register, \"1 ... -> b ...\", b=x.shape[0]),\n                    default(context, torch.Tensor([]).type_as(x)),\n                ),\n                1,\n            )\n\n        # context is B, L', D\n        # x is B, L, D\n        for block in self.joint_blocks:\n            context, x = block(\n                context,\n                x,\n                c=c_mod,\n                use_checkpoint=self.use_checkpoint,\n            )\n\n        x = self.final_layer(x, c_mod)  # (N, T, patch_size ** 2 * out_channels)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        t: torch.Tensor,\n        y: Optional[torch.Tensor] = None,\n        context: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass of DiT.\n        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n        t: (N,) tensor of diffusion timesteps\n        y: (N,) tensor of class labels\n        \"\"\"\n\n        if self.context_processor is not None:\n            context = self.context_processor(context)\n\n        hw = x.shape[-2:]\n        x = self.x_embedder(x) + self.cropped_pos_embed(hw, device=x.device).to(dtype=x.dtype, device=x.device)\n        c = self.t_embedder(t, dtype=x.dtype)  # (N, D)\n        if y is not None and self.y_embedder is not None:\n            y = self.y_embedder(y)  # (N, D)\n            c = c + y  # (N, D)\n\n        if context is not None:\n            context = self.context_embedder(context)\n\n        x = self.forward_core_with_concat(x, c, context)\n\n        x = self.unpatchify(x, hw=hw)  # (N, out_channels, H, W)\n        return x[:,:,:hw[-2],:hw[-1]]\n\n\nclass OpenAISignatureMMDITWrapper(MMDiT):\n    def forward(\n        self,\n        x: torch.Tensor,\n        timesteps: torch.Tensor,\n        context: Optional[torch.Tensor] = None,\n        y: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> torch.Tensor:\n        return super().forward(x, timesteps, context=context, y=y)\n\n", "comfy/ldm/modules/diffusionmodules/util.py": "# adopted from\n# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n# and\n# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n# and\n# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n#\n# thanks!\n\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import repeat, rearrange\n\nfrom comfy.ldm.util import instantiate_from_config\n\nclass AlphaBlender(nn.Module):\n    strategies = [\"learned\", \"fixed\", \"learned_with_images\"]\n\n    def __init__(\n        self,\n        alpha: float,\n        merge_strategy: str = \"learned_with_images\",\n        rearrange_pattern: str = \"b t -> (b t) 1 1\",\n    ):\n        super().__init__()\n        self.merge_strategy = merge_strategy\n        self.rearrange_pattern = rearrange_pattern\n\n        assert (\n            merge_strategy in self.strategies\n        ), f\"merge_strategy needs to be in {self.strategies}\"\n\n        if self.merge_strategy == \"fixed\":\n            self.register_buffer(\"mix_factor\", torch.Tensor([alpha]))\n        elif (\n            self.merge_strategy == \"learned\"\n            or self.merge_strategy == \"learned_with_images\"\n        ):\n            self.register_parameter(\n                \"mix_factor\", torch.nn.Parameter(torch.Tensor([alpha]))\n            )\n        else:\n            raise ValueError(f\"unknown merge strategy {self.merge_strategy}\")\n\n    def get_alpha(self, image_only_indicator: torch.Tensor, device) -> torch.Tensor:\n        # skip_time_mix = rearrange(repeat(skip_time_mix, 'b -> (b t) () () ()', t=t), '(b t) 1 ... -> b 1 t ...', t=t)\n        if self.merge_strategy == \"fixed\":\n            # make shape compatible\n            # alpha = repeat(self.mix_factor, '1 -> b () t  () ()', t=t, b=bs)\n            alpha = self.mix_factor.to(device)\n        elif self.merge_strategy == \"learned\":\n            alpha = torch.sigmoid(self.mix_factor.to(device))\n            # make shape compatible\n            # alpha = repeat(alpha, '1 -> s () ()', s = t * bs)\n        elif self.merge_strategy == \"learned_with_images\":\n            if image_only_indicator is None:\n                alpha = rearrange(torch.sigmoid(self.mix_factor.to(device)), \"... -> ... 1\")\n            else:\n                alpha = torch.where(\n                    image_only_indicator.bool(),\n                    torch.ones(1, 1, device=image_only_indicator.device),\n                    rearrange(torch.sigmoid(self.mix_factor.to(image_only_indicator.device)), \"... -> ... 1\"),\n                )\n            alpha = rearrange(alpha, self.rearrange_pattern)\n            # make shape compatible\n            # alpha = repeat(alpha, '1 -> s () ()', s = t * bs)\n        else:\n            raise NotImplementedError()\n        return alpha\n\n    def forward(\n        self,\n        x_spatial,\n        x_temporal,\n        image_only_indicator=None,\n    ) -> torch.Tensor:\n        alpha = self.get_alpha(image_only_indicator, x_spatial.device)\n        x = (\n            alpha.to(x_spatial.dtype) * x_spatial\n            + (1.0 - alpha).to(x_spatial.dtype) * x_temporal\n        )\n        return x\n\n\ndef make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    if schedule == \"linear\":\n        betas = (\n                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n        )\n\n    elif schedule == \"cosine\":\n        timesteps = (\n                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n        alphas = torch.cos(alphas).pow(2)\n        alphas = alphas / alphas[0]\n        betas = 1 - alphas[1:] / alphas[:-1]\n        betas = torch.clamp(betas, min=0, max=0.999)\n\n    elif schedule == \"squaredcos_cap_v2\":  # used for karlo prior\n        # return early\n        return betas_for_alpha_bar(\n            n_timestep,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n\n    elif schedule == \"sqrt_linear\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n    elif schedule == \"sqrt\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n    else:\n        raise ValueError(f\"schedule '{schedule}' unknown.\")\n    return betas\n\n\ndef make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n    if ddim_discr_method == 'uniform':\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == 'quad':\n        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n    else:\n        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n\n    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n    steps_out = ddim_timesteps + 1\n    if verbose:\n        print(f'Selected timesteps for ddim sampler: {steps_out}')\n    return steps_out\n\n\ndef make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n    if verbose:\n        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n        print(f'For the chosen value of eta, which is {eta}, '\n              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n    return sigmas, alphas, alphas_prev\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\ndef extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        ctx.gpu_autocast_kwargs = {\"enabled\": torch.is_autocast_enabled(),\n                                   \"dtype\": torch.get_autocast_gpu_dtype(),\n                                   \"cache_enabled\": torch.is_autocast_cache_enabled()}\n        with torch.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with torch.enable_grad(), \\\n                torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs):\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = torch.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32, device=timesteps.device) / half\n        )\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = repeat(timesteps, 'b -> b d', d=dim)\n    return embedding\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass HybridConditioner(nn.Module):\n\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n\n\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n", "comfy/ldm/modules/diffusionmodules/upscaling.py": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom functools import partial\n\nfrom .util import extract_into_tensor, make_beta_schedule\nfrom comfy.ldm.util import default\n\n\nclass AbstractLowScaleModel(nn.Module):\n    # for concatenating a downsampled image to the latent representation\n    def __init__(self, noise_schedule_config=None):\n        super(AbstractLowScaleModel, self).__init__()\n        if noise_schedule_config is not None:\n            self.register_schedule(**noise_schedule_config)\n\n    def register_schedule(self, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                   cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n    def q_sample(self, x_start, t, noise=None, seed=None):\n        if noise is None:\n            if seed is None:\n                noise = torch.randn_like(x_start)\n            else:\n                noise = torch.randn(x_start.size(), dtype=x_start.dtype, layout=x_start.layout, generator=torch.manual_seed(seed)).to(x_start.device)\n        return (extract_into_tensor(self.sqrt_alphas_cumprod.to(x_start.device), t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod.to(x_start.device), t, x_start.shape) * noise)\n\n    def forward(self, x):\n        return x, None\n\n    def decode(self, x):\n        return x\n\n\nclass SimpleImageConcat(AbstractLowScaleModel):\n    # no noise level conditioning\n    def __init__(self):\n        super(SimpleImageConcat, self).__init__(noise_schedule_config=None)\n        self.max_noise_level = 0\n\n    def forward(self, x):\n        # fix to constant noise level\n        return x, torch.zeros(x.shape[0], device=x.device).long()\n\n\nclass ImageConcatWithNoiseAugmentation(AbstractLowScaleModel):\n    def __init__(self, noise_schedule_config, max_noise_level=1000, to_cuda=False):\n        super().__init__(noise_schedule_config=noise_schedule_config)\n        self.max_noise_level = max_noise_level\n\n    def forward(self, x, noise_level=None, seed=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        z = self.q_sample(x, noise_level, seed=seed)\n        return z, noise_level\n\n\n\n", "comfy/ldm/modules/diffusionmodules/__init__.py": "", "comfy/ldm/modules/diffusionmodules/openaimodel.py": "from abc import abstractmethod\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport logging\n\nfrom .util import (\n    checkpoint,\n    avg_pool_nd,\n    zero_module,\n    timestep_embedding,\n    AlphaBlender,\n)\nfrom ..attention import SpatialTransformer, SpatialVideoTransformer, default\nfrom comfy.ldm.util import exists\nimport comfy.ops\nops = comfy.ops.disable_weight_init\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n#This is needed because accelerate makes a copy of transformer_options which breaks \"transformer_index\"\ndef forward_timestep_embed(ts, x, emb, context=None, transformer_options={}, output_shape=None, time_context=None, num_video_frames=None, image_only_indicator=None):\n    for layer in ts:\n        if isinstance(layer, VideoResBlock):\n            x = layer(x, emb, num_video_frames, image_only_indicator)\n        elif isinstance(layer, TimestepBlock):\n            x = layer(x, emb)\n        elif isinstance(layer, SpatialVideoTransformer):\n            x = layer(x, context, time_context, num_video_frames, image_only_indicator, transformer_options)\n            if \"transformer_index\" in transformer_options:\n                transformer_options[\"transformer_index\"] += 1\n        elif isinstance(layer, SpatialTransformer):\n            x = layer(x, context, transformer_options)\n            if \"transformer_index\" in transformer_options:\n                transformer_options[\"transformer_index\"] += 1\n        elif isinstance(layer, Upsample):\n            x = layer(x, output_shape=output_shape)\n        else:\n            x = layer(x)\n    return x\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, *args, **kwargs):\n        return forward_timestep_embed(self, *args, **kwargs)\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = operations.conv_nd(dims, self.channels, self.out_channels, 3, padding=padding, dtype=dtype, device=device)\n\n    def forward(self, x, output_shape=None):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            shape = [x.shape[2], x.shape[3] * 2, x.shape[4] * 2]\n            if output_shape is not None:\n                shape[1] = output_shape[3]\n                shape[2] = output_shape[4]\n        else:\n            shape = [x.shape[2] * 2, x.shape[3] * 2]\n            if output_shape is not None:\n                shape[0] = output_shape[2]\n                shape[1] = output_shape[3]\n\n        x = F.interpolate(x, size=shape, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=ops):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = operations.conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n        kernel_size=3,\n        exchange_temb_dims=False,\n        skip_t_emb=False,\n        dtype=None,\n        device=None,\n        operations=ops\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.exchange_temb_dims = exchange_temb_dims\n\n        if isinstance(kernel_size, list):\n            padding = [k // 2 for k in kernel_size]\n        else:\n            padding = kernel_size // 2\n\n        self.in_layers = nn.Sequential(\n            operations.GroupNorm(32, channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            operations.conv_nd(dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Upsample(channels, False, dims, dtype=dtype, device=device)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n            self.x_upd = Downsample(channels, False, dims, dtype=dtype, device=device)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.skip_t_emb = skip_t_emb\n        if self.skip_t_emb:\n            self.emb_layers = None\n            self.exchange_temb_dims = False\n        else:\n            self.emb_layers = nn.Sequential(\n                nn.SiLU(),\n                operations.Linear(\n                    emb_channels,\n                    2 * self.out_channels if use_scale_shift_norm else self.out_channels, dtype=dtype, device=device\n                ),\n            )\n        self.out_layers = nn.Sequential(\n            operations.GroupNorm(32, self.out_channels, dtype=dtype, device=device),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            operations.conv_nd(dims, self.out_channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device)\n            ,\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = operations.conv_nd(\n                dims, channels, self.out_channels, kernel_size, padding=padding, dtype=dtype, device=device\n            )\n        else:\n            self.skip_connection = operations.conv_nd(dims, channels, self.out_channels, 1, dtype=dtype, device=device)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = None\n        if not self.skip_t_emb:\n            emb_out = self.emb_layers(emb).type(h.dtype)\n            while len(emb_out.shape) < len(h.shape):\n                emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            h = out_norm(h)\n            if emb_out is not None:\n                scale, shift = th.chunk(emb_out, 2, dim=1)\n                h *= (1 + scale)\n                h += shift\n            h = out_rest(h)\n        else:\n            if emb_out is not None:\n                if self.exchange_temb_dims:\n                    emb_out = emb_out.movedim(1, 2)\n                h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass VideoResBlock(ResBlock):\n    def __init__(\n        self,\n        channels: int,\n        emb_channels: int,\n        dropout: float,\n        video_kernel_size=3,\n        merge_strategy: str = \"fixed\",\n        merge_factor: float = 0.5,\n        out_channels=None,\n        use_conv: bool = False,\n        use_scale_shift_norm: bool = False,\n        dims: int = 2,\n        use_checkpoint: bool = False,\n        up: bool = False,\n        down: bool = False,\n        dtype=None,\n        device=None,\n        operations=ops\n    ):\n        super().__init__(\n            channels,\n            emb_channels,\n            dropout,\n            out_channels=out_channels,\n            use_conv=use_conv,\n            use_scale_shift_norm=use_scale_shift_norm,\n            dims=dims,\n            use_checkpoint=use_checkpoint,\n            up=up,\n            down=down,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n\n        self.time_stack = ResBlock(\n            default(out_channels, channels),\n            emb_channels,\n            dropout=dropout,\n            dims=3,\n            out_channels=default(out_channels, channels),\n            use_scale_shift_norm=False,\n            use_conv=False,\n            up=False,\n            down=False,\n            kernel_size=video_kernel_size,\n            use_checkpoint=use_checkpoint,\n            exchange_temb_dims=True,\n            dtype=dtype,\n            device=device,\n            operations=operations\n        )\n        self.time_mixer = AlphaBlender(\n            alpha=merge_factor,\n            merge_strategy=merge_strategy,\n            rearrange_pattern=\"b t -> b 1 t 1 1\",\n        )\n\n    def forward(\n        self,\n        x: th.Tensor,\n        emb: th.Tensor,\n        num_video_frames: int,\n        image_only_indicator = None,\n    ) -> th.Tensor:\n        x = super().forward(x, emb)\n\n        x_mix = rearrange(x, \"(b t) c h w -> b c t h w\", t=num_video_frames)\n        x = rearrange(x, \"(b t) c h w -> b c t h w\", t=num_video_frames)\n\n        x = self.time_stack(\n            x, rearrange(emb, \"(b t) ... -> b t ...\", t=num_video_frames)\n        )\n        x = self.time_mixer(\n            x_spatial=x_mix, x_temporal=x, image_only_indicator=image_only_indicator\n        )\n        x = rearrange(x, \"b c t h w -> (b t) c h w\")\n        return x\n\n\nclass Timestep(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        return timestep_embedding(t, self.dim)\n\ndef apply_control(h, control, name):\n    if control is not None and name in control and len(control[name]) > 0:\n        ctrl = control[name].pop()\n        if ctrl is not None:\n            try:\n                h += ctrl\n            except:\n                logging.warning(\"warning control could not be applied {} {}\".format(h.shape, ctrl.shape))\n    return h\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        dtype=th.float32,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        adm_in_channels=None,\n        transformer_depth_middle=None,\n        transformer_depth_output=None,\n        use_temporal_resblock=False,\n        use_temporal_attention=False,\n        time_context_dim=None,\n        extra_ff_mix_layer=False,\n        use_spatial_context=False,\n        merge_strategy=None,\n        merge_factor=0.0,\n        video_kernel_size=None,\n        disable_temporal_crossattention=False,\n        max_ddpm_temb_period=10000,\n        attn_precision=None,\n        device=None,\n        operations=ops,\n    ):\n        super().__init__()\n\n        if context_dim is not None:\n            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n            # from omegaconf.listconfig import ListConfig\n            # if type(context_dim) == ListConfig:\n            #     context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n            self.num_res_blocks = num_res_blocks\n\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n\n        transformer_depth = transformer_depth[:]\n        transformer_depth_output = transformer_depth_output[:]\n\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = dtype\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.use_temporal_resblocks = use_temporal_resblock\n        self.predict_codebook_ids = n_embed is not None\n\n        self.default_num_video_frames = None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            operations.Linear(model_channels, time_embed_dim, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n        )\n\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim, dtype=self.dtype, device=device)\n            elif self.num_classes == \"continuous\":\n                logging.debug(\"setting up linear c_adm embedding layer\")\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            elif self.num_classes == \"sequential\":\n                assert adm_in_channels is not None\n                self.label_emb = nn.Sequential(\n                    nn.Sequential(\n                        operations.Linear(adm_in_channels, time_embed_dim, dtype=self.dtype, device=device),\n                        nn.SiLU(),\n                        operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n                    )\n                )\n            else:\n                raise ValueError()\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    operations.conv_nd(dims, in_channels, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n\n        def get_attention_layer(\n            ch,\n            num_heads,\n            dim_head,\n            depth=1,\n            context_dim=None,\n            use_checkpoint=False,\n            disable_self_attn=False,\n        ):\n            if use_temporal_attention:\n                return SpatialVideoTransformer(\n                    ch,\n                    num_heads,\n                    dim_head,\n                    depth=depth,\n                    context_dim=context_dim,\n                    time_context_dim=time_context_dim,\n                    dropout=dropout,\n                    ff_in=extra_ff_mix_layer,\n                    use_spatial_context=use_spatial_context,\n                    merge_strategy=merge_strategy,\n                    merge_factor=merge_factor,\n                    checkpoint=use_checkpoint,\n                    use_linear=use_linear_in_transformer,\n                    disable_self_attn=disable_self_attn,\n                    disable_temporal_crossattention=disable_temporal_crossattention,\n                    max_time_embed_period=max_ddpm_temb_period,\n                    attn_precision=attn_precision,\n                    dtype=self.dtype, device=device, operations=operations\n                )\n            else:\n                return SpatialTransformer(\n                                ch, num_heads, dim_head, depth=depth, context_dim=context_dim,\n                                disable_self_attn=disable_self_attn, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint, attn_precision=attn_precision, dtype=self.dtype, device=device, operations=operations\n                            )\n\n        def get_resblock(\n            merge_factor,\n            merge_strategy,\n            video_kernel_size,\n            ch,\n            time_embed_dim,\n            dropout,\n            out_channels,\n            dims,\n            use_checkpoint,\n            use_scale_shift_norm,\n            down=False,\n            up=False,\n            dtype=None,\n            device=None,\n            operations=ops\n        ):\n            if self.use_temporal_resblocks:\n                return VideoResBlock(\n                    merge_factor=merge_factor,\n                    merge_strategy=merge_strategy,\n                    video_kernel_size=video_kernel_size,\n                    channels=ch,\n                    emb_channels=time_embed_dim,\n                    dropout=dropout,\n                    out_channels=out_channels,\n                    dims=dims,\n                    use_checkpoint=use_checkpoint,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                    down=down,\n                    up=up,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n            else:\n                return ResBlock(\n                    channels=ch,\n                    emb_channels=time_embed_dim,\n                    dropout=dropout,\n                    out_channels=out_channels,\n                    use_checkpoint=use_checkpoint,\n                    dims=dims,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                    down=down,\n                    up=up,\n                    dtype=dtype,\n                    device=device,\n                    operations=operations\n                )\n\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    get_resblock(\n                        merge_factor=merge_factor,\n                        merge_strategy=merge_strategy,\n                        video_kernel_size=video_kernel_size,\n                        ch=ch,\n                        time_embed_dim=time_embed_dim,\n                        dropout=dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations,\n                    )\n                ]\n                ch = mult * model_channels\n                num_transformers = transformer_depth.pop(0)\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or nr < num_attention_blocks[level]:\n                        layers.append(get_attention_layer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_checkpoint=use_checkpoint)\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        get_resblock(\n                            merge_factor=merge_factor,\n                            merge_strategy=merge_strategy,\n                            video_kernel_size=video_kernel_size,\n                            ch=ch,\n                            time_embed_dim=time_embed_dim,\n                            dropout=dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        mid_block = [\n            get_resblock(\n                merge_factor=merge_factor,\n                merge_strategy=merge_strategy,\n                video_kernel_size=video_kernel_size,\n                ch=ch,\n                time_embed_dim=time_embed_dim,\n                dropout=dropout,\n                out_channels=None,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n\n        self.middle_block = None\n        if transformer_depth_middle >= -1:\n            if transformer_depth_middle >= 0:\n                mid_block += [get_attention_layer(  # always uses a self-attn\n                                ch, num_heads, dim_head, depth=transformer_depth_middle, context_dim=context_dim,\n                                disable_self_attn=disable_middle_self_attn, use_checkpoint=use_checkpoint\n                            ),\n                get_resblock(\n                    merge_factor=merge_factor,\n                    merge_strategy=merge_strategy,\n                    video_kernel_size=video_kernel_size,\n                    ch=ch,\n                    time_embed_dim=time_embed_dim,\n                    dropout=dropout,\n                    out_channels=None,\n                    dims=dims,\n                    use_checkpoint=use_checkpoint,\n                    use_scale_shift_norm=use_scale_shift_norm,\n                    dtype=self.dtype,\n                    device=device,\n                    operations=operations\n                )]\n            self.middle_block = TimestepEmbedSequential(*mid_block)\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(self.num_res_blocks[level] + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    get_resblock(\n                        merge_factor=merge_factor,\n                        merge_strategy=merge_strategy,\n                        video_kernel_size=video_kernel_size,\n                        ch=ch + ich,\n                        time_embed_dim=time_embed_dim,\n                        dropout=dropout,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations\n                    )\n                ]\n                ch = model_channels * mult\n                num_transformers = transformer_depth_output.pop()\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or i < num_attention_blocks[level]:\n                        layers.append(\n                            get_attention_layer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_checkpoint=use_checkpoint\n                            )\n                        )\n                if level and i == self.num_res_blocks[level]:\n                    out_ch = ch\n                    layers.append(\n                        get_resblock(\n                            merge_factor=merge_factor,\n                            merge_strategy=merge_strategy,\n                            video_kernel_size=video_kernel_size,\n                            ch=ch,\n                            time_embed_dim=time_embed_dim,\n                            dropout=dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            operations.GroupNorm(32, ch, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            zero_module(operations.conv_nd(dims, model_channels, out_channels, 3, padding=1, dtype=self.dtype, device=device)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n            operations.GroupNorm(32, ch, dtype=self.dtype, device=device),\n            operations.conv_nd(dims, model_channels, n_embed, 1, dtype=self.dtype, device=device),\n            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n        )\n\n    def forward(self, x, timesteps=None, context=None, y=None, control=None, transformer_options={}, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        transformer_options[\"original_shape\"] = list(x.shape)\n        transformer_options[\"transformer_index\"] = 0\n        transformer_patches = transformer_options.get(\"patches\", {})\n\n        num_video_frames = kwargs.get(\"num_video_frames\", self.default_num_video_frames)\n        image_only_indicator = kwargs.get(\"image_only_indicator\", None)\n        time_context = kwargs.get(\"time_context\", None)\n\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n        hs = []\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n        emb = self.time_embed(t_emb)\n\n        if self.num_classes is not None:\n            assert y.shape[0] == x.shape[0]\n            emb = emb + self.label_emb(y)\n\n        h = x\n        for id, module in enumerate(self.input_blocks):\n            transformer_options[\"block\"] = (\"input\", id)\n            h = forward_timestep_embed(module, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n            h = apply_control(h, control, 'input')\n            if \"input_block_patch\" in transformer_patches:\n                patch = transformer_patches[\"input_block_patch\"]\n                for p in patch:\n                    h = p(h, transformer_options)\n\n            hs.append(h)\n            if \"input_block_patch_after_skip\" in transformer_patches:\n                patch = transformer_patches[\"input_block_patch_after_skip\"]\n                for p in patch:\n                    h = p(h, transformer_options)\n\n        transformer_options[\"block\"] = (\"middle\", 0)\n        if self.middle_block is not None:\n            h = forward_timestep_embed(self.middle_block, h, emb, context, transformer_options, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n        h = apply_control(h, control, 'middle')\n\n\n        for id, module in enumerate(self.output_blocks):\n            transformer_options[\"block\"] = (\"output\", id)\n            hsp = hs.pop()\n            hsp = apply_control(hsp, control, 'output')\n\n            if \"output_block_patch\" in transformer_patches:\n                patch = transformer_patches[\"output_block_patch\"]\n                for p in patch:\n                    h, hsp = p(h, hsp, transformer_options)\n\n            h = th.cat([h, hsp], dim=1)\n            del hsp\n            if len(hs) > 0:\n                output_shape = hs[-1].shape\n            else:\n                output_shape = None\n            h = forward_timestep_embed(module, h, emb, context, transformer_options, output_shape, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)\n        h = h.type(x.dtype)\n        if self.predict_codebook_ids:\n            return self.id_predictor(h)\n        else:\n            return self.out(h)\n", "comfy/ldm/modules/distributions/distributions.py": "import torch\nimport numpy as np\n\n\nclass AbstractDistribution:\n    def sample(self):\n        raise NotImplementedError()\n\n    def mode(self):\n        raise NotImplementedError()\n\n\nclass DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n\n    def sample(self):\n        return self.value\n\n    def mode(self):\n        return self.value\n\n\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n\n    def sample(self):\n        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n        return x\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n                                       + self.var - 1.0 - self.logvar,\n                                       dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n                    dim=[1, 2, 3])\n\n    def nll(self, sample, dims=[1,2,3]):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(\n            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n            dim=dims)\n\n    def mode(self):\n        return self.mean\n\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + torch.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n", "comfy/ldm/modules/distributions/__init__.py": "", "comfy/t2i_adapter/adapter.py": "#taken from https://github.com/TencentARC/T2I-Adapter\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if not self.use_conv:\n            padding = [x.shape[2] % 2, x.shape[3] % 2]\n            self.op.padding = padding\n\n        x = self.op(x)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_c, out_c, down, ksize=3, sk=False, use_conv=True):\n        super().__init__()\n        ps = ksize // 2\n        if in_c != out_c or sk == False:\n            self.in_conv = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            # print('n_in')\n            self.in_conv = None\n        self.block1 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(out_c, out_c, ksize, 1, ps)\n        if sk == False:\n            self.skep = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            self.skep = None\n\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=use_conv)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        if self.in_conv is not None:  # edit\n            x = self.in_conv(x)\n\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n        if self.skep is not None:\n            return h + self.skep(x)\n        else:\n            return h + x\n\n\nclass Adapter(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64, ksize=3, sk=False, use_conv=True, xl=True):\n        super(Adapter, self).__init__()\n        self.unshuffle_amount = 8\n        resblock_no_downsample = []\n        resblock_downsample = [3, 2, 1]\n        self.xl = xl\n        if self.xl:\n            self.unshuffle_amount = 16\n            resblock_no_downsample = [1]\n            resblock_downsample = [2]\n\n        self.input_channels = cin // (self.unshuffle_amount * self.unshuffle_amount)\n        self.unshuffle = nn.PixelUnshuffle(self.unshuffle_amount)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        for i in range(len(channels)):\n            for j in range(nums_rb):\n                if (i in resblock_downsample) and (j == 0):\n                    self.body.append(\n                        ResnetBlock(channels[i - 1], channels[i], down=True, ksize=ksize, sk=sk, use_conv=use_conv))\n                elif (i in resblock_no_downsample) and (j == 0):\n                    self.body.append(\n                        ResnetBlock(channels[i - 1], channels[i], down=False, ksize=ksize, sk=sk, use_conv=use_conv))\n                else:\n                    self.body.append(\n                        ResnetBlock(channels[i], channels[i], down=False, ksize=ksize, sk=sk, use_conv=use_conv))\n        self.body = nn.ModuleList(self.body)\n        self.conv_in = nn.Conv2d(cin, channels[0], 3, 1, 1)\n\n    def forward(self, x):\n        # unshuffle\n        x = self.unshuffle(x)\n        # extract features\n        features = []\n        x = self.conv_in(x)\n        for i in range(len(self.channels)):\n            for j in range(self.nums_rb):\n                idx = i * self.nums_rb + j\n                x = self.body[idx](x)\n            if self.xl:\n                features.append(None)\n                if i == 0:\n                    features.append(None)\n                    features.append(None)\n                if i == 2:\n                    features.append(None)\n            else:\n                features.append(None)\n                features.append(None)\n            features.append(x)\n\n        return features\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict([(\"c_fc\", nn.Linear(d_model, d_model * 4)), (\"gelu\", QuickGELU()),\n                         (\"c_proj\", nn.Linear(d_model * 4, d_model))]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass StyleAdapter(nn.Module):\n\n    def __init__(self, width=1024, context_dim=768, num_head=8, n_layes=3, num_token=4):\n        super().__init__()\n\n        scale = width ** -0.5\n        self.transformer_layes = nn.Sequential(*[ResidualAttentionBlock(width, num_head) for _ in range(n_layes)])\n        self.num_token = num_token\n        self.style_embedding = nn.Parameter(torch.randn(1, num_token, width) * scale)\n        self.ln_post = LayerNorm(width)\n        self.ln_pre = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, context_dim))\n\n    def forward(self, x):\n        # x shape [N, HW+1, C]\n        style_embedding = self.style_embedding + torch.zeros(\n            (x.shape[0], self.num_token, self.style_embedding.shape[-1]), device=x.device)\n        x = torch.cat([x, style_embedding], dim=1)\n        x = self.ln_pre(x)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer_layes(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, -self.num_token:, :])\n        x = x @ self.proj\n\n        return x\n\n\nclass ResnetBlock_light(nn.Module):\n    def __init__(self, in_c):\n        super().__init__()\n        self.block1 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n\n    def forward(self, x):\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n\n        return h + x\n\n\nclass extractor(nn.Module):\n    def __init__(self, in_c, inter_c, out_c, nums_rb, down=False):\n        super().__init__()\n        self.in_conv = nn.Conv2d(in_c, inter_c, 1, 1, 0)\n        self.body = []\n        for _ in range(nums_rb):\n            self.body.append(ResnetBlock_light(inter_c))\n        self.body = nn.Sequential(*self.body)\n        self.out_conv = nn.Conv2d(inter_c, out_c, 1, 1, 0)\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=False)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        x = self.in_conv(x)\n        x = self.body(x)\n        x = self.out_conv(x)\n\n        return x\n\n\nclass Adapter_light(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64):\n        super(Adapter_light, self).__init__()\n        self.unshuffle_amount = 8\n        self.unshuffle = nn.PixelUnshuffle(self.unshuffle_amount)\n        self.input_channels = cin // (self.unshuffle_amount * self.unshuffle_amount)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        self.xl = False\n\n        for i in range(len(channels)):\n            if i == 0:\n                self.body.append(extractor(in_c=cin, inter_c=channels[i]//4, out_c=channels[i], nums_rb=nums_rb, down=False))\n            else:\n                self.body.append(extractor(in_c=channels[i-1], inter_c=channels[i]//4, out_c=channels[i], nums_rb=nums_rb, down=True))\n        self.body = nn.ModuleList(self.body)\n\n    def forward(self, x):\n        # unshuffle\n        x = self.unshuffle(x)\n        # extract features\n        features = []\n        for i in range(len(self.channels)):\n            x = self.body[i](x)\n            features.append(None)\n            features.append(None)\n            features.append(x)\n\n        return features\n", "comfy/k_diffusion/sampling.py": "import math\n\nfrom scipy import integrate\nimport torch\nfrom torch import nn\nimport torchsde\nfrom tqdm.auto import trange, tqdm\n\nfrom . import utils\n\n\ndef append_zero(x):\n    return torch.cat([x, x.new_zeros([1])])\n\n\ndef get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n    ramp = torch.linspace(0, 1, n, device=device)\n    min_inv_rho = sigma_min ** (1 / rho)\n    max_inv_rho = sigma_max ** (1 / rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n    return append_zero(sigmas).to(device)\n\n\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n    return append_zero(sigmas)\n\n\ndef get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\n\n\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n    t = torch.linspace(1, eps_s, n, device=device)\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n    return append_zero(sigmas)\n\n\ndef to_d(x, sigma, denoised):\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\n\n\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n    if not eta:\n        return sigma_to, 0.\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n    return sigma_down, sigma_up\n\n\ndef default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)\n\n\nclass BatchedBrownianTree:\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\n\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\n        self.cpu_tree = True\n        if \"cpu\" in kwargs:\n            self.cpu_tree = kwargs.pop(\"cpu\")\n        t0, t1, self.sign = self.sort(t0, t1)\n        w0 = kwargs.get('w0', torch.zeros_like(x))\n        if seed is None:\n            seed = torch.randint(0, 2 ** 63 - 1, []).item()\n        self.batched = True\n        try:\n            assert len(seed) == x.shape[0]\n            w0 = w0[0]\n        except TypeError:\n            seed = [seed]\n            self.batched = False\n        if self.cpu_tree:\n            self.trees = [torchsde.BrownianTree(t0.cpu(), w0.cpu(), t1.cpu(), entropy=s, **kwargs) for s in seed]\n        else:\n            self.trees = [torchsde.BrownianTree(t0, w0, t1, entropy=s, **kwargs) for s in seed]\n\n    @staticmethod\n    def sort(a, b):\n        return (a, b, 1) if a < b else (b, a, -1)\n\n    def __call__(self, t0, t1):\n        t0, t1, sign = self.sort(t0, t1)\n        if self.cpu_tree:\n            w = torch.stack([tree(t0.cpu().float(), t1.cpu().float()).to(t0.dtype).to(t0.device) for tree in self.trees]) * (self.sign * sign)\n        else:\n            w = torch.stack([tree(t0, t1) for tree in self.trees]) * (self.sign * sign)\n\n        return w if self.batched else w[0]\n\n\nclass BrownianTreeNoiseSampler:\n    \"\"\"A noise sampler backed by a torchsde.BrownianTree.\n\n    Args:\n        x (Tensor): The tensor whose shape, device and dtype to use to generate\n            random samples.\n        sigma_min (float): The low end of the valid interval.\n        sigma_max (float): The high end of the valid interval.\n        seed (int or List[int]): The random seed. If a list of seeds is\n            supplied instead of a single integer, then the noise sampler will\n            use one BrownianTree per batch item, each with its own seed.\n        transform (callable): A function that maps sigma to the sampler's\n            internal timestep.\n    \"\"\"\n\n    def __init__(self, x, sigma_min, sigma_max, seed=None, transform=lambda x: x, cpu=False):\n        self.transform = transform\n        t0, t1 = self.transform(torch.as_tensor(sigma_min)), self.transform(torch.as_tensor(sigma_max))\n        self.tree = BatchedBrownianTree(x, t0, t1, seed, cpu=cpu)\n\n    def __call__(self, sigma, sigma_next):\n        t0, t1 = self.transform(torch.as_tensor(sigma)), self.transform(torch.as_tensor(sigma_next))\n        return self.tree(t0, t1) / (t1 - t0).abs().sqrt()\n\n\n@torch.no_grad()\ndef sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        if s_churn > 0:\n            gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n            sigma_hat = sigmas[i] * (gamma + 1)\n        else:\n            gamma = 0\n            sigma_hat = sigmas[i]\n\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        # Euler method\n        x = x + d * dt\n    return x\n\n\n@torch.no_grad()\ndef sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\n@torch.no_grad()\ndef sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        if s_churn > 0:\n            gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n            sigma_hat = sigmas[i] * (gamma + 1)\n        else:\n            gamma = 0\n            sigma_hat = sigmas[i]\n\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        if sigmas[i + 1] == 0:\n            # Euler method\n            x = x + d * dt\n        else:\n            # Heun's method\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n            d_prime = (d + d_2) / 2\n            x = x + d_prime * dt\n    return x\n\n\n@torch.no_grad()\ndef sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        if s_churn > 0:\n            gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n            sigma_hat = sigmas[i] * (gamma + 1)\n        else:\n            gamma = 0\n            sigma_hat = sigmas[i]\n\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Euler method\n            dt = sigmas[i + 1] - sigma_hat\n            x = x + d * dt\n        else:\n            # DPM-Solver-2\n            sigma_mid = sigma_hat.log().lerp(sigmas[i + 1].log(), 0.5).exp()\n            dt_1 = sigma_mid - sigma_hat\n            dt_2 = sigmas[i + 1] - sigma_hat\n            x_2 = x + d * dt_1\n            denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n    return x\n\n\n@torch.no_grad()\ndef sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        if sigma_down == 0:\n            # Euler method\n            dt = sigma_down - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver-2\n            sigma_mid = sigmas[i].log().lerp(sigma_down.log(), 0.5).exp()\n            dt_1 = sigma_mid - sigmas[i]\n            dt_2 = sigma_down - sigmas[i]\n            x_2 = x + d * dt_1\n            denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\ndef linear_multistep_coeff(order, t, i, j):\n    if order - 1 > i:\n        raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k:\n                continue\n            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n        return prod\n    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]\n\n\n@torch.no_grad()\ndef sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigmas_cpu = sigmas.detach().cpu().numpy()\n    ds = []\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        ds.append(d)\n        if len(ds) > order:\n            ds.pop(0)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        cur_order = min(i + 1, order)\n        coeffs = [linear_multistep_coeff(cur_order, sigmas_cpu, i, j) for j in range(cur_order)]\n        x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n    return x\n\n\nclass PIDStepSizeController:\n    \"\"\"A PID controller for ODE adaptive step size control.\"\"\"\n    def __init__(self, h, pcoeff, icoeff, dcoeff, order=1, accept_safety=0.81, eps=1e-8):\n        self.h = h\n        self.b1 = (pcoeff + icoeff + dcoeff) / order\n        self.b2 = -(pcoeff + 2 * dcoeff) / order\n        self.b3 = dcoeff / order\n        self.accept_safety = accept_safety\n        self.eps = eps\n        self.errs = []\n\n    def limiter(self, x):\n        return 1 + math.atan(x - 1)\n\n    def propose_step(self, error):\n        inv_error = 1 / (float(error) + self.eps)\n        if not self.errs:\n            self.errs = [inv_error, inv_error, inv_error]\n        self.errs[0] = inv_error\n        factor = self.errs[0] ** self.b1 * self.errs[1] ** self.b2 * self.errs[2] ** self.b3\n        factor = self.limiter(factor)\n        accept = factor >= self.accept_safety\n        if accept:\n            self.errs[2] = self.errs[1]\n            self.errs[1] = self.errs[0]\n        self.h *= factor\n        return accept\n\n\nclass DPMSolver(nn.Module):\n    \"\"\"DPM-Solver. See https://arxiv.org/abs/2206.00927.\"\"\"\n\n    def __init__(self, model, extra_args=None, eps_callback=None, info_callback=None):\n        super().__init__()\n        self.model = model\n        self.extra_args = {} if extra_args is None else extra_args\n        self.eps_callback = eps_callback\n        self.info_callback = info_callback\n\n    def t(self, sigma):\n        return -sigma.log()\n\n    def sigma(self, t):\n        return t.neg().exp()\n\n    def eps(self, eps_cache, key, x, t, *args, **kwargs):\n        if key in eps_cache:\n            return eps_cache[key], eps_cache\n        sigma = self.sigma(t) * x.new_ones([x.shape[0]])\n        eps = (x - self.model(x, sigma, *args, **self.extra_args, **kwargs)) / self.sigma(t)\n        if self.eps_callback is not None:\n            self.eps_callback()\n        return eps, {key: eps, **eps_cache}\n\n    def dpm_solver_1_step(self, x, t, t_next, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        x_1 = x - self.sigma(t_next) * h.expm1() * eps\n        return x_1, eps_cache\n\n    def dpm_solver_2_step(self, x, t, t_next, r1=1 / 2, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        s1 = t + r1 * h\n        u1 = x - self.sigma(s1) * (r1 * h).expm1() * eps\n        eps_r1, eps_cache = self.eps(eps_cache, 'eps_r1', u1, s1)\n        x_2 = x - self.sigma(t_next) * h.expm1() * eps - self.sigma(t_next) / (2 * r1) * h.expm1() * (eps_r1 - eps)\n        return x_2, eps_cache\n\n    def dpm_solver_3_step(self, x, t, t_next, r1=1 / 3, r2=2 / 3, eps_cache=None):\n        eps_cache = {} if eps_cache is None else eps_cache\n        h = t_next - t\n        eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n        s1 = t + r1 * h\n        s2 = t + r2 * h\n        u1 = x - self.sigma(s1) * (r1 * h).expm1() * eps\n        eps_r1, eps_cache = self.eps(eps_cache, 'eps_r1', u1, s1)\n        u2 = x - self.sigma(s2) * (r2 * h).expm1() * eps - self.sigma(s2) * (r2 / r1) * ((r2 * h).expm1() / (r2 * h) - 1) * (eps_r1 - eps)\n        eps_r2, eps_cache = self.eps(eps_cache, 'eps_r2', u2, s2)\n        x_3 = x - self.sigma(t_next) * h.expm1() * eps - self.sigma(t_next) / r2 * (h.expm1() / h - 1) * (eps_r2 - eps)\n        return x_3, eps_cache\n\n    def dpm_solver_fast(self, x, t_start, t_end, nfe, eta=0., s_noise=1., noise_sampler=None):\n        noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n        if not t_end > t_start and eta:\n            raise ValueError('eta must be 0 for reverse sampling')\n\n        m = math.floor(nfe / 3) + 1\n        ts = torch.linspace(t_start, t_end, m + 1, device=x.device)\n\n        if nfe % 3 == 0:\n            orders = [3] * (m - 2) + [2, 1]\n        else:\n            orders = [3] * (m - 1) + [nfe % 3]\n\n        for i in range(len(orders)):\n            eps_cache = {}\n            t, t_next = ts[i], ts[i + 1]\n            if eta:\n                sd, su = get_ancestral_step(self.sigma(t), self.sigma(t_next), eta)\n                t_next_ = torch.minimum(t_end, self.t(sd))\n                su = (self.sigma(t_next) ** 2 - self.sigma(t_next_) ** 2) ** 0.5\n            else:\n                t_next_, su = t_next, 0.\n\n            eps, eps_cache = self.eps(eps_cache, 'eps', x, t)\n            denoised = x - self.sigma(t) * eps\n            if self.info_callback is not None:\n                self.info_callback({'x': x, 'i': i, 't': ts[i], 't_up': t, 'denoised': denoised})\n\n            if orders[i] == 1:\n                x, eps_cache = self.dpm_solver_1_step(x, t, t_next_, eps_cache=eps_cache)\n            elif orders[i] == 2:\n                x, eps_cache = self.dpm_solver_2_step(x, t, t_next_, eps_cache=eps_cache)\n            else:\n                x, eps_cache = self.dpm_solver_3_step(x, t, t_next_, eps_cache=eps_cache)\n\n            x = x + su * s_noise * noise_sampler(self.sigma(t), self.sigma(t_next))\n\n        return x\n\n    def dpm_solver_adaptive(self, x, t_start, t_end, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None):\n        noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n        if order not in {2, 3}:\n            raise ValueError('order should be 2 or 3')\n        forward = t_end > t_start\n        if not forward and eta:\n            raise ValueError('eta must be 0 for reverse sampling')\n        h_init = abs(h_init) * (1 if forward else -1)\n        atol = torch.tensor(atol)\n        rtol = torch.tensor(rtol)\n        s = t_start\n        x_prev = x\n        accept = True\n        pid = PIDStepSizeController(h_init, pcoeff, icoeff, dcoeff, 1.5 if eta else order, accept_safety)\n        info = {'steps': 0, 'nfe': 0, 'n_accept': 0, 'n_reject': 0}\n\n        while s < t_end - 1e-5 if forward else s > t_end + 1e-5:\n            eps_cache = {}\n            t = torch.minimum(t_end, s + pid.h) if forward else torch.maximum(t_end, s + pid.h)\n            if eta:\n                sd, su = get_ancestral_step(self.sigma(s), self.sigma(t), eta)\n                t_ = torch.minimum(t_end, self.t(sd))\n                su = (self.sigma(t) ** 2 - self.sigma(t_) ** 2) ** 0.5\n            else:\n                t_, su = t, 0.\n\n            eps, eps_cache = self.eps(eps_cache, 'eps', x, s)\n            denoised = x - self.sigma(s) * eps\n\n            if order == 2:\n                x_low, eps_cache = self.dpm_solver_1_step(x, s, t_, eps_cache=eps_cache)\n                x_high, eps_cache = self.dpm_solver_2_step(x, s, t_, eps_cache=eps_cache)\n            else:\n                x_low, eps_cache = self.dpm_solver_2_step(x, s, t_, r1=1 / 3, eps_cache=eps_cache)\n                x_high, eps_cache = self.dpm_solver_3_step(x, s, t_, eps_cache=eps_cache)\n            delta = torch.maximum(atol, rtol * torch.maximum(x_low.abs(), x_prev.abs()))\n            error = torch.linalg.norm((x_low - x_high) / delta) / x.numel() ** 0.5\n            accept = pid.propose_step(error)\n            if accept:\n                x_prev = x_low\n                x = x_high + su * s_noise * noise_sampler(self.sigma(s), self.sigma(t))\n                s = t\n                info['n_accept'] += 1\n            else:\n                info['n_reject'] += 1\n            info['nfe'] += order\n            info['steps'] += 1\n\n            if self.info_callback is not None:\n                self.info_callback({'x': x, 'i': info['steps'] - 1, 't': s, 't_up': s, 'denoised': denoised, 'error': error, 'h': pid.h, **info})\n\n        return x, info\n\n\n@torch.no_grad()\ndef sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(total=n, disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)\n\n\n@torch.no_grad()\ndef sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):\n    \"\"\"DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)\n    if return_info:\n        return x, info\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_2s_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver++(2S) second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigma_down == 0:\n            # Euler method\n            d = to_d(x, sigmas[i], denoised)\n            dt = sigma_down - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver++(2S)\n            t, t_next = t_fn(sigmas[i]), t_fn(sigma_down)\n            r = 1 / 2\n            h = t_next - t\n            s = t + r * h\n            x_2 = (sigma_fn(s) / sigma_fn(t)) * x - (-h * r).expm1() * denoised\n            denoised_2 = model(x_2, sigma_fn(s) * s_in, **extra_args)\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised_2\n        # Noise addition\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    \"\"\"DPM-Solver++ (stochastic).\"\"\"\n    if len(sigmas) <= 1:\n        return x\n\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    seed = extra_args.get(\"seed\", None)\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Euler method\n            d = to_d(x, sigmas[i], denoised)\n            dt = sigmas[i + 1] - sigmas[i]\n            x = x + d * dt\n        else:\n            # DPM-Solver++\n            t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])\n            h = t_next - t\n            s = t + h * r\n            fac = 1 / (2 * r)\n\n            # Step 1\n            sd, su = get_ancestral_step(sigma_fn(t), sigma_fn(s), eta)\n            s_ = t_fn(sd)\n            x_2 = (sigma_fn(s_) / sigma_fn(t)) * x - (t - s_).expm1() * denoised\n            x_2 = x_2 + noise_sampler(sigma_fn(t), sigma_fn(s)) * s_noise * su\n            denoised_2 = model(x_2, sigma_fn(s) * s_in, **extra_args)\n\n            # Step 2\n            sd, su = get_ancestral_step(sigma_fn(t), sigma_fn(t_next), eta)\n            t_next_ = t_fn(sd)\n            denoised_d = (1 - fac) * denoised + fac * denoised_2\n            x = (sigma_fn(t_next_) / sigma_fn(t)) * x - (t - t_next_).expm1() * denoised_d\n            x = x + noise_sampler(sigma_fn(t), sigma_fn(t_next)) * s_noise * su\n    return x\n\n\n@torch.no_grad()\ndef sample_dpmpp_2m(model, x, sigmas, extra_args=None, callback=None, disable=None):\n    \"\"\"DPM-Solver++(2M).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n    old_denoised = None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])\n        h = t_next - t\n        if old_denoised is None or sigmas[i + 1] == 0:\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised\n        else:\n            h_last = t - t_fn(sigmas[i - 1])\n            r = h_last / h\n            denoised_d = (1 + 1 / (2 * r)) * denoised - (1 / (2 * r)) * old_denoised\n            x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised_d\n        old_denoised = denoised\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_2m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    \"\"\"DPM-Solver++(2M) SDE.\"\"\"\n    if len(sigmas) <= 1:\n        return x\n\n    if solver_type not in {'heun', 'midpoint'}:\n        raise ValueError('solver_type must be \\'heun\\' or \\'midpoint\\'')\n\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    old_denoised = None\n    h_last = None\n    h = None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            # DPM-Solver++(2M) SDE\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            eta_h = eta * h\n\n            x = sigmas[i + 1] / sigmas[i] * (-eta_h).exp() * x + (-h - eta_h).expm1().neg() * denoised\n\n            if old_denoised is not None:\n                r = h_last / h\n                if solver_type == 'heun':\n                    x = x + ((-h - eta_h).expm1().neg() / (-h - eta_h) + 1) * (1 / r) * (denoised - old_denoised)\n                elif solver_type == 'midpoint':\n                    x = x + 0.5 * (-h - eta_h).expm1().neg() * (1 / r) * (denoised - old_denoised)\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * eta_h).expm1().neg().sqrt() * s_noise\n\n        old_denoised = denoised\n        h_last = h\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_3m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver++(3M) SDE.\"\"\"\n\n    if len(sigmas) <= 1:\n        return x\n\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    denoised_1, denoised_2 = None, None\n    h, h_1, h_2 = None, None, None\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        if sigmas[i + 1] == 0:\n            # Denoising step\n            x = denoised\n        else:\n            t, s = -sigmas[i].log(), -sigmas[i + 1].log()\n            h = s - t\n            h_eta = h * (eta + 1)\n\n            x = torch.exp(-h_eta) * x + (-h_eta).expm1().neg() * denoised\n\n            if h_2 is not None:\n                r0 = h_1 / h\n                r1 = h_2 / h\n                d1_0 = (denoised - denoised_1) / r0\n                d1_1 = (denoised_1 - denoised_2) / r1\n                d1 = d1_0 + (d1_0 - d1_1) * r0 / (r0 + r1)\n                d2 = (d1_0 - d1_1) / (r0 + r1)\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                phi_3 = phi_2 / h_eta - 0.5\n                x = x + phi_2 * d1 - phi_3 * d2\n            elif h_1 is not None:\n                r = h_1 / h\n                d = (denoised - denoised_1) / r\n                phi_2 = h_eta.neg().expm1() / h_eta + 1\n                x = x + phi_2 * d\n\n            if eta:\n                x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1] * (-2 * h * eta).expm1().neg().sqrt() * s_noise\n\n        denoised_1, denoised_2 = denoised, denoised_1\n        h_1, h_2 = h, h_1\n    return x\n\n@torch.no_grad()\ndef sample_dpmpp_3m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    if len(sigmas) <= 1:\n        return x\n\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_3m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler)\n\n@torch.no_grad()\ndef sample_dpmpp_2m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    if len(sigmas) <= 1:\n        return x\n\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_2m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, solver_type=solver_type)\n\n@torch.no_grad()\ndef sample_dpmpp_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    if len(sigmas) <= 1:\n        return x\n\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, r=r)\n\n\ndef DDPMSampler_step(x, sigma, sigma_prev, noise, noise_sampler):\n    alpha_cumprod = 1 / ((sigma * sigma) + 1)\n    alpha_cumprod_prev = 1 / ((sigma_prev * sigma_prev) + 1)\n    alpha = (alpha_cumprod / alpha_cumprod_prev)\n\n    mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n    if sigma_prev > 0:\n        mu += ((1 - alpha) * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)).sqrt() * noise_sampler(sigma, sigma_prev)\n    return mu\n\ndef generic_step_sampler(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None, step_function=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = step_function(x / torch.sqrt(1.0 + sigmas[i] ** 2.0), sigmas[i], sigmas[i + 1], (x - denoised) / sigmas[i], noise_sampler)\n        if sigmas[i + 1] != 0:\n            x *= torch.sqrt(1.0 + sigmas[i + 1] ** 2.0)\n    return x\n\n\n@torch.no_grad()\ndef sample_ddpm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    return generic_step_sampler(model, x, sigmas, extra_args, callback, disable, noise_sampler, DDPMSampler_step)\n\n@torch.no_grad()\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        x = denoised\n        if sigmas[i + 1] > 0:\n            x = model.inner_model.inner_model.model_sampling.noise_scaling(sigmas[i + 1], noise_sampler(sigmas[i], sigmas[i + 1]), x)\n    return x\n\n\n\n@torch.no_grad()\ndef sample_heunpp2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    # From MIT licensed: https://github.com/Carzit/sd-webui-samplers-scheduler/\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    s_end = sigmas[-1]\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        eps = torch.randn_like(x) * s_noise\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n        denoised = model(x, sigma_hat * s_in, **extra_args)\n        d = to_d(x, sigma_hat, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_hat\n        if sigmas[i + 1] == s_end:\n            # Euler method\n            x = x + d * dt\n        elif sigmas[i + 2] == s_end:\n\n            # Heun's method\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n\n            w = 2 * sigmas[0]\n            w2 = sigmas[i+1]/w\n            w1 = 1 - w2\n\n            d_prime = d * w1 + d_2 * w2\n\n\n            x = x + d_prime * dt\n\n        else:\n            # Heun++\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n            dt_2 = sigmas[i + 2] - sigmas[i + 1]\n\n            x_3 = x_2 + d_2 * dt_2\n            denoised_3 = model(x_3, sigmas[i + 2] * s_in, **extra_args)\n            d_3 = to_d(x_3, sigmas[i + 2], denoised_3)\n\n            w = 3 * sigmas[0]\n            w2 = sigmas[i + 1] / w\n            w3 = sigmas[i + 2] / w\n            w1 = 1 - w2 - w3\n\n            d_prime = w1 * d + w2 * d_2 + w3 * d_3\n            x = x + d_prime * dt\n    return x\n\n\n#From https://github.com/zju-pi/diff-sampler/blob/main/diff-solvers-main/solvers.py\n#under Apache 2 license\ndef sample_ipndm(model, x, sigmas, extra_args=None, callback=None, disable=None, max_order=4):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    x_next = x\n\n    buffer_model = []\n    for i in trange(len(sigmas) - 1, disable=disable):\n        t_cur = sigmas[i]\n        t_next = sigmas[i + 1]\n\n        x_cur = x_next\n\n        denoised = model(x_cur, t_cur * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        d_cur = (x_cur - denoised) / t_cur\n\n        order = min(max_order, i+1)\n        if order == 1:      # First Euler step.\n            x_next = x_cur + (t_next - t_cur) * d_cur\n        elif order == 2:    # Use one history point.\n            x_next = x_cur + (t_next - t_cur) * (3 * d_cur - buffer_model[-1]) / 2\n        elif order == 3:    # Use two history points.\n            x_next = x_cur + (t_next - t_cur) * (23 * d_cur - 16 * buffer_model[-1] + 5 * buffer_model[-2]) / 12\n        elif order == 4:    # Use three history points.\n            x_next = x_cur + (t_next - t_cur) * (55 * d_cur - 59 * buffer_model[-1] + 37 * buffer_model[-2] - 9 * buffer_model[-3]) / 24\n\n        if len(buffer_model) == max_order - 1:\n            for k in range(max_order - 2):\n                buffer_model[k] = buffer_model[k+1]\n            buffer_model[-1] = d_cur\n        else:\n            buffer_model.append(d_cur)\n\n    return x_next\n\n#From https://github.com/zju-pi/diff-sampler/blob/main/diff-solvers-main/solvers.py\n#under Apache 2 license\ndef sample_ipndm_v(model, x, sigmas, extra_args=None, callback=None, disable=None, max_order=4):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n\n    x_next = x\n    t_steps = sigmas\n\n    buffer_model = []\n    for i in trange(len(sigmas) - 1, disable=disable):\n        t_cur = sigmas[i]\n        t_next = sigmas[i + 1]\n\n        x_cur = x_next\n\n        denoised = model(x_cur, t_cur * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        d_cur = (x_cur - denoised) / t_cur\n\n        order = min(max_order, i+1)\n        if order == 1:      # First Euler step.\n            x_next = x_cur + (t_next - t_cur) * d_cur\n        elif order == 2:    # Use one history point.\n            h_n = (t_next - t_cur)\n            h_n_1 = (t_cur - t_steps[i-1])\n            coeff1 = (2 + (h_n / h_n_1)) / 2\n            coeff2 = -(h_n / h_n_1) / 2\n            x_next = x_cur + (t_next - t_cur) * (coeff1 * d_cur + coeff2 * buffer_model[-1])\n        elif order == 3:    # Use two history points.\n            h_n = (t_next - t_cur)\n            h_n_1 = (t_cur - t_steps[i-1])\n            h_n_2 = (t_steps[i-1] - t_steps[i-2])\n            temp = (1 - h_n / (3 * (h_n + h_n_1)) * (h_n * (h_n + h_n_1)) / (h_n_1 * (h_n_1 + h_n_2))) / 2\n            coeff1 = (2 + (h_n / h_n_1)) / 2 + temp\n            coeff2 = -(h_n / h_n_1) / 2 - (1 + h_n_1 / h_n_2) * temp\n            coeff3 = temp * h_n_1 / h_n_2\n            x_next = x_cur + (t_next - t_cur) * (coeff1 * d_cur + coeff2 * buffer_model[-1] + coeff3 * buffer_model[-2])\n        elif order == 4:    # Use three history points.\n            h_n = (t_next - t_cur)\n            h_n_1 = (t_cur - t_steps[i-1])\n            h_n_2 = (t_steps[i-1] - t_steps[i-2])\n            h_n_3 = (t_steps[i-2] - t_steps[i-3])\n            temp1 = (1 - h_n / (3 * (h_n + h_n_1)) * (h_n * (h_n + h_n_1)) / (h_n_1 * (h_n_1 + h_n_2))) / 2\n            temp2 = ((1 - h_n / (3 * (h_n + h_n_1))) / 2 + (1 - h_n / (2 * (h_n + h_n_1))) * h_n / (6 * (h_n + h_n_1 + h_n_2))) \\\n                   * (h_n * (h_n + h_n_1) * (h_n + h_n_1 + h_n_2)) / (h_n_1 * (h_n_1 + h_n_2) * (h_n_1 + h_n_2 + h_n_3))\n            coeff1 = (2 + (h_n / h_n_1)) / 2 + temp1 + temp2\n            coeff2 = -(h_n / h_n_1) / 2 - (1 + h_n_1 / h_n_2) * temp1 - (1 + (h_n_1 / h_n_2) + (h_n_1 * (h_n_1 + h_n_2) / (h_n_2 * (h_n_2 + h_n_3)))) * temp2\n            coeff3 = temp1 * h_n_1 / h_n_2 + ((h_n_1 / h_n_2) + (h_n_1 * (h_n_1 + h_n_2) / (h_n_2 * (h_n_2 + h_n_3))) * (1 + h_n_2 / h_n_3)) * temp2\n            coeff4 = -temp2 * (h_n_1 * (h_n_1 + h_n_2) / (h_n_2 * (h_n_2 + h_n_3))) * h_n_1 / h_n_2\n            x_next = x_cur + (t_next - t_cur) * (coeff1 * d_cur + coeff2 * buffer_model[-1] + coeff3 * buffer_model[-2] + coeff4 * buffer_model[-3])\n\n        if len(buffer_model) == max_order - 1:\n            for k in range(max_order - 2):\n                buffer_model[k] = buffer_model[k+1]\n            buffer_model[-1] = d_cur.detach()\n        else:\n            buffer_model.append(d_cur.detach())\n\n    return x_next\n", "comfy/k_diffusion/utils.py": "from contextlib import contextmanager\nimport hashlib\nimport math\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport warnings\n\nfrom PIL import Image\nimport torch\nfrom torch import nn, optim\nfrom torch.utils import data\n\n\ndef hf_datasets_augs_helper(examples, transform, image_key, mode='RGB'):\n    \"\"\"Apply passed in transforms for HuggingFace Datasets.\"\"\"\n    images = [transform(image.convert(mode)) for image in examples[image_key]]\n    return {image_key: images}\n\n\ndef append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    expanded = x[(...,) + (None,) * dims_to_append]\n    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.\n    # https://github.com/pytorch/pytorch/issues/84364\n    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded\n\n\ndef n_params(module):\n    \"\"\"Returns the number of trainable parameters in a module.\"\"\"\n    return sum(p.numel() for p in module.parameters())\n\n\ndef download_file(path, url, digest=None):\n    \"\"\"Downloads a file if it does not exist, optionally checking its SHA-256 hash.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if digest != file_digest:\n            raise OSError(f'hash of {path} (url: {url}) failed to validate')\n    return path\n\n\n@contextmanager\ndef train_mode(model, mode=True):\n    \"\"\"A context manager that places a model into training mode and restores\n    the previous mode on exit.\"\"\"\n    modes = [module.training for module in model.modules()]\n    try:\n        yield model.train(mode)\n    finally:\n        for i, module in enumerate(model.modules()):\n            module.training = modes[i]\n\n\ndef eval_mode(model):\n    \"\"\"A context manager that places a model into evaluation mode and restores\n    the previous mode on exit.\"\"\"\n    return train_mode(model, False)\n\n\n@torch.no_grad()\ndef ema_update(model, averaged_model, decay):\n    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n    version of a model. It should be called after each optimizer step.\"\"\"\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    assert model_params.keys() == averaged_params.keys()\n\n    for name, param in model_params.items():\n        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    assert model_buffers.keys() == averaged_buffers.keys()\n\n    for name, buf in model_buffers.items():\n        averaged_buffers[name].copy_(buf)\n\n\nclass EMAWarmup:\n    \"\"\"Implements an EMA warmup using an inverse decay schedule.\n    If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n    good values for models you plan to train for a million or more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \"\"\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \"\"\"Returns the state of the class as a :class:`dict`.\"\"\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \"\"\"Gets the current EMA decay rate.\"\"\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \"\"\"Updates the step count.\"\"\"\n        self.last_epoch += 1\n\n\nclass InverseLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an inverse decay learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr.\n    inv_gamma is the number of steps/epochs required for the learning rate to decay to\n    (1 / 2)**power of its original value.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        inv_gamma (float): Inverse multiplicative factor of learning rate decay. Default: 1.\n        power (float): Exponential factor of learning rate decay. Default: 1.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)\n            Default: 0.\n        min_lr (float): The minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n    \"\"\"\n\n    def __init__(self, optimizer, inv_gamma=1., power=1., warmup=0., min_lr=0.,\n                 last_epoch=-1, verbose=False):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        if not 0. <= warmup < 1:\n            raise ValueError('Invalid value for warmup')\n        self.warmup = warmup\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\")\n\n        return self._get_closed_form_lr()\n\n    def _get_closed_form_lr(self):\n        warmup = 1 - self.warmup ** (self.last_epoch + 1)\n        lr_mult = (1 + self.last_epoch / self.inv_gamma) ** -self.power\n        return [warmup * max(self.min_lr, base_lr * lr_mult)\n                for base_lr in self.base_lrs]\n\n\nclass ExponentialLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an exponential learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr. Decays the learning rate\n    continuously by decay (default 0.5) every num_steps steps.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        num_steps (float): The number of steps to decay the learning rate by decay in.\n        decay (float): The factor by which to decay the learning rate every num_steps\n            steps. Default: 0.5.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)\n            Default: 0.\n        min_lr (float): The minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n    \"\"\"\n\n    def __init__(self, optimizer, num_steps, decay=0.5, warmup=0., min_lr=0.,\n                 last_epoch=-1, verbose=False):\n        self.num_steps = num_steps\n        self.decay = decay\n        if not 0. <= warmup < 1:\n            raise ValueError('Invalid value for warmup')\n        self.warmup = warmup\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\")\n\n        return self._get_closed_form_lr()\n\n    def _get_closed_form_lr(self):\n        warmup = 1 - self.warmup ** (self.last_epoch + 1)\n        lr_mult = (self.decay ** (1 / self.num_steps)) ** self.last_epoch\n        return [warmup * max(self.min_lr, base_lr * lr_mult)\n                for base_lr in self.base_lrs]\n\n\ndef rand_log_normal(shape, loc=0., scale=1., device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an lognormal distribution.\"\"\"\n    return (torch.randn(shape, device=device, dtype=dtype) * scale + loc).exp()\n\n\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an optionally truncated log-logistic distribution.\"\"\"\n    min_value = torch.as_tensor(min_value, device=device, dtype=torch.float64)\n    max_value = torch.as_tensor(max_value, device=device, dtype=torch.float64)\n    min_cdf = min_value.log().sub(loc).div(scale).sigmoid()\n    max_cdf = max_value.log().sub(loc).div(scale).sigmoid()\n    u = torch.rand(shape, device=device, dtype=torch.float64) * (max_cdf - min_cdf) + min_cdf\n    return u.logit().mul(scale).add(loc).exp().to(dtype)\n\n\ndef rand_log_uniform(shape, min_value, max_value, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an log-uniform distribution.\"\"\"\n    min_value = math.log(min_value)\n    max_value = math.log(max_value)\n    return (torch.rand(shape, device=device, dtype=dtype) * (max_value - min_value) + min_value).exp()\n\n\ndef rand_v_diffusion(shape, sigma_data=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a truncated v-diffusion training timestep distribution.\"\"\"\n    min_cdf = math.atan(min_value / sigma_data) * 2 / math.pi\n    max_cdf = math.atan(max_value / sigma_data) * 2 / math.pi\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return torch.tan(u * math.pi / 2) * sigma_data\n\n\ndef rand_split_log_normal(shape, loc, scale_1, scale_2, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a split lognormal distribution.\"\"\"\n    n = torch.randn(shape, device=device, dtype=dtype).abs()\n    u = torch.rand(shape, device=device, dtype=dtype)\n    n_left = n * -scale_1 + loc\n    n_right = n * scale_2 + loc\n    ratio = scale_1 / (scale_1 + scale_2)\n    return torch.where(u < ratio, n_left, n_right).exp()\n\n\nclass FolderOfImages(data.Dataset):\n    \"\"\"Recursively finds all images in a directory. It does not support\n    classes/targets.\"\"\"\n\n    IMG_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'}\n\n    def __init__(self, root, transform=None):\n        super().__init__()\n        self.root = Path(root)\n        self.transform = nn.Identity() if transform is None else transform\n        self.paths = sorted(path for path in self.root.rglob('*') if path.suffix.lower() in self.IMG_EXTENSIONS)\n\n    def __repr__(self):\n        return f'FolderOfImages(root=\"{self.root}\", len: {len(self)})'\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, key):\n        path = self.paths[key]\n        with open(path, 'rb') as f:\n            image = Image.open(f).convert('RGB')\n        image = self.transform(image)\n        return image,\n\n\nclass CSVLogger:\n    def __init__(self, filename, columns):\n        self.filename = Path(filename)\n        self.columns = columns\n        if self.filename.exists():\n            self.file = open(self.filename, 'a')\n        else:\n            self.file = open(self.filename, 'w')\n            self.write(*self.columns)\n\n    def write(self, *args):\n        print(*args, sep=',', file=self.file, flush=True)\n\n\n@contextmanager\ndef tf32_mode(cudnn=None, matmul=None):\n    \"\"\"A context manager that sets whether TF32 is allowed on cuDNN or matmul.\"\"\"\n    cudnn_old = torch.backends.cudnn.allow_tf32\n    matmul_old = torch.backends.cuda.matmul.allow_tf32\n    try:\n        if cudnn is not None:\n            torch.backends.cudnn.allow_tf32 = cudnn\n        if matmul is not None:\n            torch.backends.cuda.matmul.allow_tf32 = matmul\n        yield\n    finally:\n        if cudnn is not None:\n            torch.backends.cudnn.allow_tf32 = cudnn_old\n        if matmul is not None:\n            torch.backends.cuda.matmul.allow_tf32 = matmul_old\n", "comfy/extra_samplers/uni_pc.py": "#code taken from: https://github.com/wl-zhao/UniPC and modified\n\nimport torch\nimport torch.nn.functional as F\nimport math\n\nfrom tqdm.auto import trange, tqdm\n\n\nclass NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n        ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n\n            t = self.inverse_lambda(lambda_t)\n\n        ===============================================================\n\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n\n        1. For discrete-time DPMs:\n\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n        \n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \"\"\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == 'linear':\n            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n\ndef model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.,\n    classifier_fn=None,\n    classifier_kwargs={},\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n\n    We support four types of the diffusion model by setting `model_type`:\n\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n    \n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            `` \n\n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            `` \n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n        \n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)         \n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        output = model(x, t_input, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \"classifier-free\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn\n\n\nclass UniPC:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        predict_x0=True,\n        thresholding=False,\n        max_val=1.,\n        variant='bh1',\n    ):\n        \"\"\"Construct a UniPC. \n\n        We support both data_prediction and noise_prediction.\n        \"\"\"\n        self.model = model_fn\n        self.noise_schedule = noise_schedule\n        self.variant = variant\n        self.predict_x0 = predict_x0\n        self.thresholding = thresholding\n        self.max_val = max_val\n\n    def dynamic_thresholding_fn(self, x0, t=None):\n        \"\"\"\n        The dynamic thresholding method. \n        \"\"\"\n        dims = x0.dim()\n        p = self.dynamic_thresholding_ratio\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n        x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with thresholding).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n        if self.thresholding:\n            p = 0.995   # A hyperparameter in the paper of \"Imagen\" [1].\n            s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n            s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n            x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model. \n        \"\"\"\n        if self.predict_x0:\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n        \"\"\"\n        if skip_type == 'logSNR':\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == 'time_uniform':\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == 'time_quadratic':\n            t_order = 2\n            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(\"Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\".format(skip_type))\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [3,] * (K - 2) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [3,] * (K - 1) + [1]\n            else:\n                orders = [3,] * (K - 1) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [2,] * K\n            else:\n                K = steps // 2 + 1\n                orders = [2,] * (K - 1) + [1]\n        elif order == 1:\n            K = steps\n            orders = [1,] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == 'logSNR':\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization. \n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def multistep_uni_pc_update(self, x, model_prev_list, t_prev_list, t, order, **kwargs):\n        if len(t.shape) == 0:\n            t = t.view(-1)\n        if 'bh' in self.variant:\n            return self.multistep_uni_pc_bh_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n        else:\n            assert self.variant == 'vary_coeff'\n            return self.multistep_uni_pc_vary_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n\n    def multistep_uni_pc_vary_update(self, x, model_prev_list, t_prev_list, t, order, use_corrector=True):\n        print(f'using unified predictor-corrector with order {order} (solver type: vary coeff)')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_t = ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = (lambda_prev_i - lambda_prev_0) / h\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        K = len(rks)\n        # build C matrix\n        C = []\n\n        col = torch.ones_like(rks)\n        for k in range(1, K + 1):\n            C.append(col)\n            col = col * rks / (k + 1) \n        C = torch.stack(C, dim=1)\n\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            C_inv_p = torch.linalg.inv(C[:-1, :-1])\n            A_p = C_inv_p\n\n        if use_corrector:\n            print('using corrector')\n            C_inv = torch.linalg.inv(C)\n            A_c = C_inv\n\n        hh = -h if self.predict_x0 else h\n        h_phi_1 = torch.expm1(hh)\n        h_phi_ks = []\n        factorial_k = 1\n        h_phi_k = h_phi_1\n        for k in range(1, K + 2):\n            h_phi_ks.append(h_phi_k)\n            h_phi_k = h_phi_k / hh - 1 / factorial_k\n            factorial_k *= (k + 1)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                sigma_t / sigma_prev_0 * x\n                - alpha_t * h_phi_1 * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - alpha_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        else:\n            log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n            x_t_ = (\n                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                - (sigma_t * h_phi_1) * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - sigma_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        return x_t, model_t\n\n    def multistep_uni_pc_bh_update(self, x, model_prev_list, t_prev_list, t, order, x_t=None, use_corrector=True):\n        # print(f'using unified predictor-corrector with order {order} (solver type: B(h))')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n        dims = x.dim()\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = ((lambda_prev_i - lambda_prev_0) / h)[0]\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        R = []\n        b = []\n\n        hh = -h[0] if self.predict_x0 else h[0]\n        h_phi_1 = torch.expm1(hh) # h\\phi_1(h) = e^h - 1\n        h_phi_k = h_phi_1 / hh - 1\n\n        factorial_i = 1\n\n        if self.variant == 'bh1':\n            B_h = hh\n        elif self.variant == 'bh2':\n            B_h = torch.expm1(hh)\n        else:\n            raise NotImplementedError()\n            \n        for i in range(1, order + 1):\n            R.append(torch.pow(rks, i - 1))\n            b.append(h_phi_k * factorial_i / B_h)\n            factorial_i *= (i + 1)\n            h_phi_k = h_phi_k / hh - 1 / factorial_i \n\n        R = torch.stack(R)\n        b = torch.tensor(b, device=x.device)\n\n        # now predictor\n        use_predictor = len(D1s) > 0 and x_t is None\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            if x_t is None:\n                # for order 2, we use a simplified version\n                if order == 2:\n                    rhos_p = torch.tensor([0.5], device=b.device)\n                else:\n                    rhos_p = torch.linalg.solve(R[:-1, :-1], b[:-1])\n        else:\n            D1s = None\n\n        if use_corrector:\n            # print('using corrector')\n            # for order 1, we use a simplified version\n            if order == 1:\n                rhos_c = torch.tensor([0.5], device=b.device)\n            else:\n                rhos_c = torch.linalg.solve(R, b)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                expand_dims(sigma_t / sigma_prev_0, dims) * x\n                - expand_dims(alpha_t * h_phi_1, dims)* model_prev_0\n            )\n\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        else:\n            x_t_ = (\n                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                - expand_dims(sigma_t * h_phi_1, dims) * model_prev_0\n            )\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        return x_t, model_t\n\n\n    def sample(self, x, timesteps, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n        method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n        atol=0.0078, rtol=0.05, corrector=False, callback=None, disable_pbar=False\n    ):\n        # t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n        # t_T = self.noise_schedule.T if t_start is None else t_start\n        device = x.device\n        steps = len(timesteps) - 1\n        if method == 'multistep':\n            assert steps >= order\n            # timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n            assert timesteps.shape[0] - 1 == steps\n            # with torch.no_grad():\n            for step_index in trange(steps, disable=disable_pbar):\n                if step_index == 0:\n                    vec_t = timesteps[0].expand((x.shape[0]))\n                    model_prev_list = [self.model_fn(x, vec_t)]\n                    t_prev_list = [vec_t]\n                elif step_index < order:\n                    init_order = step_index\n                # Init the first `order` values by lower order multistep DPM-Solver.\n                # for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n                else:\n                    extra_final_step = 0\n                    if step_index == (steps - 1):\n                        extra_final_step = 1\n                    for step in range(step_index, step_index + 1 + extra_final_step):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        # print('this step order:', step_order)\n                        if step == steps:\n                            # print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                if callback is not None:\n                    callback({'x': x, 'i': step_index, 'denoised': model_prev_list[-1]})\n        else:\n            raise NotImplementedError()\n        # if denoise_to_zero:\n        #     x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n\n#############################################################\n# other utility functions\n#############################################################\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand\n\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]\n\n\nclass SigmaConvert:\n    schedule = \"\"\n    def marginal_log_mean_coeff(self, sigma):\n        return 0.5 * torch.log(1 / ((sigma * sigma) + 1))\n\n    def marginal_alpha(self, t):\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\ndef predict_eps_sigma(model, input, sigma_in, **kwargs):\n    sigma = sigma_in.view(sigma_in.shape[:1] + (1,) * (input.ndim - 1))\n    input = input * ((sigma ** 2 + 1.0) ** 0.5)\n    return  (input - model(input, sigma_in, **kwargs)) / sigma\n\n\ndef sample_unipc(model, noise, sigmas, extra_args=None, callback=None, disable=False, variant='bh1'):\n        timesteps = sigmas.clone()\n        if sigmas[-1] == 0:\n            timesteps = sigmas[:]\n            timesteps[-1] = 0.001\n        else:\n            timesteps = sigmas.clone()\n        ns = SigmaConvert()\n\n        noise = noise / torch.sqrt(1.0 + timesteps[0] ** 2.0)\n        model_type = \"noise\"\n\n        model_fn = model_wrapper(\n            lambda input, sigma, **kwargs: predict_eps_sigma(model, input, sigma, **kwargs),\n            ns,\n            model_type=model_type,\n            guidance_type=\"uncond\",\n            model_kwargs=extra_args,\n        )\n\n        order = min(3, len(timesteps) - 2)\n        uni_pc = UniPC(model_fn, ns, predict_x0=True, thresholding=False, variant=variant)\n        x = uni_pc.sample(noise, timesteps=timesteps, skip_type=\"time_uniform\", method=\"multistep\", order=order, lower_order_final=True, callback=callback, disable_pbar=disable)\n        x /= ns.marginal_alpha(timesteps[-1])\n        return x\n\ndef sample_unipc_bh2(model, noise, sigmas, extra_args=None, callback=None, disable=False):\n    return sample_unipc(model, noise, sigmas, extra_args, callback, disable, variant='bh2')", "comfy/taesd/taesd.py": "#!/usr/bin/env python3\n\"\"\"\nTiny AutoEncoder for Stable Diffusion\n(DNN for encoding / decoding SD's latent space)\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nimport comfy.utils\nimport comfy.ops\n\ndef conv(n_in, n_out, **kwargs):\n    return comfy.ops.disable_weight_init.Conv2d(n_in, n_out, 3, padding=1, **kwargs)\n\nclass Clamp(nn.Module):\n    def forward(self, x):\n        return torch.tanh(x / 3) * 3\n\nclass Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = comfy.ops.disable_weight_init.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()\n        self.fuse = nn.ReLU()\n    def forward(self, x):\n        return self.fuse(self.conv(x) + self.skip(x))\n\ndef Encoder(latent_channels=4):\n    return nn.Sequential(\n        conv(3, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, latent_channels),\n    )\n\n\ndef Decoder(latent_channels=4):\n    return nn.Sequential(\n        Clamp(), conv(latent_channels, 64), nn.ReLU(),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), conv(64, 3),\n    )\n\nclass TAESD(nn.Module):\n    latent_magnitude = 3\n    latent_shift = 0.5\n\n    def __init__(self, encoder_path=None, decoder_path=None, latent_channels=4):\n        \"\"\"Initialize pretrained TAESD on the given device from the given checkpoints.\"\"\"\n        super().__init__()\n        self.taesd_encoder = Encoder(latent_channels=latent_channels)\n        self.taesd_decoder = Decoder(latent_channels=latent_channels)\n        self.vae_scale = torch.nn.Parameter(torch.tensor(1.0))\n        self.vae_shift = torch.nn.Parameter(torch.tensor(0.0))\n        if encoder_path is not None:\n            self.taesd_encoder.load_state_dict(comfy.utils.load_torch_file(encoder_path, safe_load=True))\n        if decoder_path is not None:\n            self.taesd_decoder.load_state_dict(comfy.utils.load_torch_file(decoder_path, safe_load=True))\n\n    @staticmethod\n    def scale_latents(x):\n        \"\"\"raw latents -> [0, 1]\"\"\"\n        return x.div(2 * TAESD.latent_magnitude).add(TAESD.latent_shift).clamp(0, 1)\n\n    @staticmethod\n    def unscale_latents(x):\n        \"\"\"[0, 1] -> raw latents\"\"\"\n        return x.sub(TAESD.latent_shift).mul(2 * TAESD.latent_magnitude)\n\n    def decode(self, x):\n        x_sample = self.taesd_decoder((x - self.vae_shift) * self.vae_scale)\n        x_sample = x_sample.sub(0.5).mul(2)\n        return x_sample\n\n    def encode(self, x):\n        return (self.taesd_encoder(x * 0.5 + 0.5) / self.vae_scale) + self.vae_shift\n", "comfy/cldm/cldm.py": "#taken from: https://github.com/lllyasviel/ControlNet\n#and modified\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\nfrom ..ldm.modules.diffusionmodules.util import (\n    zero_module,\n    timestep_embedding,\n)\n\nfrom ..ldm.modules.attention import SpatialTransformer\nfrom ..ldm.modules.diffusionmodules.openaimodel import UNetModel, TimestepEmbedSequential, ResBlock, Downsample\nfrom ..ldm.util import exists\nimport comfy.ops\n\nclass ControlledUnetModel(UNetModel):\n    #implemented in the ldm unet\n    pass\n\nclass ControlNet(nn.Module):\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        hint_channels,\n        num_res_blocks,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        dtype=torch.float32,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        adm_in_channels=None,\n        transformer_depth_middle=None,\n        transformer_depth_output=None,\n        attn_precision=None,\n        device=None,\n        operations=comfy.ops.disable_weight_init,\n        **kwargs,\n    ):\n        super().__init__()\n        assert use_spatial_transformer == True, \"use_spatial_transformer has to be true\"\n        if use_spatial_transformer:\n            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n\n        if context_dim is not None:\n            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n            # from omegaconf.listconfig import ListConfig\n            # if type(context_dim) == ListConfig:\n            #     context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.dims = dims\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n            self.num_res_blocks = num_res_blocks\n\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] >= num_attention_blocks[i], range(len(num_attention_blocks))))\n\n        transformer_depth = transformer_depth[:]\n\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = dtype\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            operations.Linear(model_channels, time_embed_dim, dtype=self.dtype, device=device),\n            nn.SiLU(),\n            operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n        )\n\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n            elif self.num_classes == \"continuous\":\n                print(\"setting up linear c_adm embedding layer\")\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            elif self.num_classes == \"sequential\":\n                assert adm_in_channels is not None\n                self.label_emb = nn.Sequential(\n                    nn.Sequential(\n                        operations.Linear(adm_in_channels, time_embed_dim, dtype=self.dtype, device=device),\n                        nn.SiLU(),\n                        operations.Linear(time_embed_dim, time_embed_dim, dtype=self.dtype, device=device),\n                    )\n                )\n            else:\n                raise ValueError()\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    operations.conv_nd(dims, in_channels, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n                )\n            ]\n        )\n        self.zero_convs = nn.ModuleList([self.make_zero_conv(model_channels, operations=operations, dtype=self.dtype, device=device)])\n\n        self.input_hint_block = TimestepEmbedSequential(\n                    operations.conv_nd(dims, hint_channels, 16, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 16, 16, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 16, 32, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 32, 32, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 32, 96, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 96, 96, 3, padding=1, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 96, 256, 3, padding=1, stride=2, dtype=self.dtype, device=device),\n                    nn.SiLU(),\n                    operations.conv_nd(dims, 256, model_channels, 3, padding=1, dtype=self.dtype, device=device)\n        )\n\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        dtype=self.dtype,\n                        device=device,\n                        operations=operations,\n                    )\n                ]\n                ch = mult * model_channels\n                num_transformers = transformer_depth.pop(0)\n                if num_transformers > 0:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or nr < num_attention_blocks[level]:\n                        layers.append(\n                            SpatialTransformer(\n                                ch, num_heads, dim_head, depth=num_transformers, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint, attn_precision=attn_precision, dtype=self.dtype, device=device, operations=operations\n                            )\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self.zero_convs.append(self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            dtype=self.dtype,\n                            device=device,\n                            operations=operations\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, dtype=self.dtype, device=device, operations=operations\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                self.zero_convs.append(self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device))\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        mid_block = [\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        if transformer_depth_middle >= 0:\n            mid_block += [SpatialTransformer(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth_middle, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_linear=use_linear_in_transformer,\n                            use_checkpoint=use_checkpoint, attn_precision=attn_precision, dtype=self.dtype, device=device, operations=operations\n                        ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                dtype=self.dtype,\n                device=device,\n                operations=operations\n            )]\n        self.middle_block = TimestepEmbedSequential(*mid_block)\n        self.middle_block_out = self.make_zero_conv(ch, operations=operations, dtype=self.dtype, device=device)\n        self._feature_size += ch\n\n    def make_zero_conv(self, channels, operations=None, dtype=None, device=None):\n        return TimestepEmbedSequential(operations.conv_nd(self.dims, channels, channels, 1, padding=0, dtype=dtype, device=device))\n\n    def forward(self, x, hint, timesteps, context, y=None, **kwargs):\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False).to(x.dtype)\n        emb = self.time_embed(t_emb)\n\n        guided_hint = self.input_hint_block(hint, emb, context)\n\n        outs = []\n\n        hs = []\n        if self.num_classes is not None:\n            assert y.shape[0] == x.shape[0]\n            emb = emb + self.label_emb(y)\n\n        h = x\n        for module, zero_conv in zip(self.input_blocks, self.zero_convs):\n            if guided_hint is not None:\n                h = module(h, emb, context)\n                h += guided_hint\n                guided_hint = None\n            else:\n                h = module(h, emb, context)\n            outs.append(zero_conv(h, emb, context))\n\n        h = self.middle_block(h, emb, context)\n        outs.append(self.middle_block_out(h, emb, context))\n\n        return outs\n\n", "app/user_manager.py": "import json\nimport os\nimport re\nimport uuid\nfrom aiohttp import web\nfrom comfy.cli_args import args\nfrom folder_paths import user_directory\nfrom .app_settings import AppSettings\n\ndefault_user = \"default\"\nusers_file = os.path.join(user_directory, \"users.json\")\n\n\nclass UserManager():\n    def __init__(self):\n        global user_directory\n\n        self.settings = AppSettings(self)\n        if not os.path.exists(user_directory):\n            os.mkdir(user_directory)\n            if not args.multi_user:\n                print(\"****** User settings have been changed to be stored on the server instead of browser storage. ******\")\n                print(\"****** For multi-user setups add the --multi-user CLI argument to enable multiple user profiles. ******\")\n\n        if args.multi_user:\n            if os.path.isfile(users_file):\n                with open(users_file) as f:\n                    self.users = json.load(f)\n            else:\n                self.users = {}\n        else:\n            self.users = {\"default\": \"default\"}\n\n    def get_request_user_id(self, request):\n        user = \"default\"\n        if args.multi_user and \"comfy-user\" in request.headers:\n            user = request.headers[\"comfy-user\"]\n\n        if user not in self.users:\n            raise KeyError(\"Unknown user: \" + user)\n\n        return user\n\n    def get_request_user_filepath(self, request, file, type=\"userdata\", create_dir=True):\n        global user_directory\n\n        if type == \"userdata\":\n            root_dir = user_directory\n        else:\n            raise KeyError(\"Unknown filepath type:\" + type)\n\n        user = self.get_request_user_id(request)\n        path = user_root = os.path.abspath(os.path.join(root_dir, user))\n\n        # prevent leaving /{type}\n        if os.path.commonpath((root_dir, user_root)) != root_dir:\n            return None\n\n        parent = user_root\n\n        if file is not None:\n            # prevent leaving /{type}/{user}\n            path = os.path.abspath(os.path.join(user_root, file))\n            if os.path.commonpath((user_root, path)) != user_root:\n                return None\n\n        if create_dir and not os.path.exists(parent):\n            os.mkdir(parent)\n\n        return path\n\n    def add_user(self, name):\n        name = name.strip()\n        if not name:\n            raise ValueError(\"username not provided\")\n        user_id = re.sub(\"[^a-zA-Z0-9-_]+\", '-', name)\n        user_id = user_id + \"_\" + str(uuid.uuid4())\n\n        self.users[user_id] = name\n\n        global users_file\n        with open(users_file, \"w\") as f:\n            json.dump(self.users, f)\n\n        return user_id\n\n    def add_routes(self, routes):\n        self.settings.add_routes(routes)\n\n        @routes.get(\"/users\")\n        async def get_users(request):\n            if args.multi_user:\n                return web.json_response({\"storage\": \"server\", \"users\": self.users})\n            else:\n                user_dir = self.get_request_user_filepath(request, None, create_dir=False)\n                return web.json_response({\n                    \"storage\": \"server\",\n                    \"migrated\": os.path.exists(user_dir)\n                })\n\n        @routes.post(\"/users\")\n        async def post_users(request):\n            body = await request.json()\n            username = body[\"username\"]\n            if username in self.users.values():\n                return web.json_response({\"error\": \"Duplicate username.\"}, status=400)\n\n            user_id = self.add_user(username)\n            return web.json_response(user_id)\n\n        @routes.get(\"/userdata/{file}\")\n        async def getuserdata(request):\n            file = request.match_info.get(\"file\", None)\n            if not file:\n                return web.Response(status=400)\n                \n            path = self.get_request_user_filepath(request, file)\n            if not path:\n                return web.Response(status=403)\n            \n            if not os.path.exists(path):\n                return web.Response(status=404)\n            \n            return web.FileResponse(path)\n\n        @routes.post(\"/userdata/{file}\")\n        async def post_userdata(request):\n            file = request.match_info.get(\"file\", None)\n            if not file:\n                return web.Response(status=400)\n                \n            path = self.get_request_user_filepath(request, file)\n            if not path:\n                return web.Response(status=403)\n\n            body = await request.read()\n            with open(path, \"wb\") as f:\n                f.write(body)\n                \n            return web.Response(status=200)\n", "app/app_settings.py": "import os\nimport json\nfrom aiohttp import web\n\n\nclass AppSettings():\n    def __init__(self, user_manager):\n        self.user_manager = user_manager\n\n    def get_settings(self, request):\n        file = self.user_manager.get_request_user_filepath(\n            request, \"comfy.settings.json\")\n        if os.path.isfile(file):\n            with open(file) as f:\n                return json.load(f)\n        else:\n            return {}\n\n    def save_settings(self, request, settings):\n        file = self.user_manager.get_request_user_filepath(\n            request, \"comfy.settings.json\")\n        with open(file, \"w\") as f:\n            f.write(json.dumps(settings, indent=4))\n\n    def add_routes(self, routes):\n        @routes.get(\"/settings\")\n        async def get_settings(request):\n            return web.json_response(self.get_settings(request))\n\n        @routes.get(\"/settings/{id}\")\n        async def get_setting(request):\n            value = None\n            settings = self.get_settings(request)\n            setting_id = request.match_info.get(\"id\", None)\n            if setting_id and setting_id in settings:\n                value = settings[setting_id]\n            return web.json_response(value)\n\n        @routes.post(\"/settings\")\n        async def post_settings(request):\n            settings = self.get_settings(request)\n            new_settings = await request.json()\n            self.save_settings(request, {**settings, **new_settings})\n            return web.Response(status=200)\n\n        @routes.post(\"/settings/{id}\")\n        async def post_setting(request):\n            setting_id = request.match_info.get(\"id\", None)\n            if not setting_id:\n                return web.Response(status=400)\n            settings = self.get_settings(request)\n            settings[setting_id] = await request.json()\n            self.save_settings(request, settings)\n            return web.Response(status=200)", "script_examples/websockets_api_example.py": "#This is an example that uses the websockets api to know when a prompt execution is done\n#Once the prompt execution is done it downloads the images using the /history endpoint\n\nimport websocket #NOTE: websocket-client (https://github.com/websocket-client/websocket-client)\nimport uuid\nimport json\nimport urllib.request\nimport urllib.parse\n\nserver_address = \"127.0.0.1:8188\"\nclient_id = str(uuid.uuid4())\n\ndef queue_prompt(prompt):\n    p = {\"prompt\": prompt, \"client_id\": client_id}\n    data = json.dumps(p).encode('utf-8')\n    req =  urllib.request.Request(\"http://{}/prompt\".format(server_address), data=data)\n    return json.loads(urllib.request.urlopen(req).read())\n\ndef get_image(filename, subfolder, folder_type):\n    data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n    url_values = urllib.parse.urlencode(data)\n    with urllib.request.urlopen(\"http://{}/view?{}\".format(server_address, url_values)) as response:\n        return response.read()\n\ndef get_history(prompt_id):\n    with urllib.request.urlopen(\"http://{}/history/{}\".format(server_address, prompt_id)) as response:\n        return json.loads(response.read())\n\ndef get_images(ws, prompt):\n    prompt_id = queue_prompt(prompt)['prompt_id']\n    output_images = {}\n    while True:\n        out = ws.recv()\n        if isinstance(out, str):\n            message = json.loads(out)\n            if message['type'] == 'executing':\n                data = message['data']\n                if data['node'] is None and data['prompt_id'] == prompt_id:\n                    break #Execution is done\n        else:\n            continue #previews are binary data\n\n    history = get_history(prompt_id)[prompt_id]\n    for o in history['outputs']:\n        for node_id in history['outputs']:\n            node_output = history['outputs'][node_id]\n            if 'images' in node_output:\n                images_output = []\n                for image in node_output['images']:\n                    image_data = get_image(image['filename'], image['subfolder'], image['type'])\n                    images_output.append(image_data)\n            output_images[node_id] = images_output\n\n    return output_images\n\nprompt_text = \"\"\"\n{\n    \"3\": {\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"cfg\": 8,\n            \"denoise\": 1,\n            \"latent_image\": [\n                \"5\",\n                0\n            ],\n            \"model\": [\n                \"4\",\n                0\n            ],\n            \"negative\": [\n                \"7\",\n                0\n            ],\n            \"positive\": [\n                \"6\",\n                0\n            ],\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"normal\",\n            \"seed\": 8566257,\n            \"steps\": 20\n        }\n    },\n    \"4\": {\n        \"class_type\": \"CheckpointLoaderSimple\",\n        \"inputs\": {\n            \"ckpt_name\": \"v1-5-pruned-emaonly.ckpt\"\n        }\n    },\n    \"5\": {\n        \"class_type\": \"EmptyLatentImage\",\n        \"inputs\": {\n            \"batch_size\": 1,\n            \"height\": 512,\n            \"width\": 512\n        }\n    },\n    \"6\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"masterpiece best quality girl\"\n        }\n    },\n    \"7\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"bad hands\"\n        }\n    },\n    \"8\": {\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\n                \"3\",\n                0\n            ],\n            \"vae\": [\n                \"4\",\n                2\n            ]\n        }\n    },\n    \"9\": {\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"filename_prefix\": \"ComfyUI\",\n            \"images\": [\n                \"8\",\n                0\n            ]\n        }\n    }\n}\n\"\"\"\n\nprompt = json.loads(prompt_text)\n#set the text prompt for our positive CLIPTextEncode\nprompt[\"6\"][\"inputs\"][\"text\"] = \"masterpiece best quality man\"\n\n#set the seed for our KSampler node\nprompt[\"3\"][\"inputs\"][\"seed\"] = 5\n\nws = websocket.WebSocket()\nws.connect(\"ws://{}/ws?clientId={}\".format(server_address, client_id))\nimages = get_images(ws, prompt)\n\n#Commented out code to display the output images:\n\n# for node_id in images:\n#     for image_data in images[node_id]:\n#         from PIL import Image\n#         import io\n#         image = Image.open(io.BytesIO(image_data))\n#         image.show()\n\n", "script_examples/websockets_api_example_ws_images.py": "#This is an example that uses the websockets api and the SaveImageWebsocket node to get images directly without\n#them being saved to disk\n\nimport websocket #NOTE: websocket-client (https://github.com/websocket-client/websocket-client)\nimport uuid\nimport json\nimport urllib.request\nimport urllib.parse\n\nserver_address = \"127.0.0.1:8188\"\nclient_id = str(uuid.uuid4())\n\ndef queue_prompt(prompt):\n    p = {\"prompt\": prompt, \"client_id\": client_id}\n    data = json.dumps(p).encode('utf-8')\n    req =  urllib.request.Request(\"http://{}/prompt\".format(server_address), data=data)\n    return json.loads(urllib.request.urlopen(req).read())\n\ndef get_image(filename, subfolder, folder_type):\n    data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n    url_values = urllib.parse.urlencode(data)\n    with urllib.request.urlopen(\"http://{}/view?{}\".format(server_address, url_values)) as response:\n        return response.read()\n\ndef get_history(prompt_id):\n    with urllib.request.urlopen(\"http://{}/history/{}\".format(server_address, prompt_id)) as response:\n        return json.loads(response.read())\n\ndef get_images(ws, prompt):\n    prompt_id = queue_prompt(prompt)['prompt_id']\n    output_images = {}\n    current_node = \"\"\n    while True:\n        out = ws.recv()\n        if isinstance(out, str):\n            message = json.loads(out)\n            if message['type'] == 'executing':\n                data = message['data']\n                if data['prompt_id'] == prompt_id:\n                    if data['node'] is None:\n                        break #Execution is done\n                    else:\n                        current_node = data['node']\n        else:\n            if current_node == 'save_image_websocket_node':\n                images_output = output_images.get(current_node, [])\n                images_output.append(out[8:])\n                output_images[current_node] = images_output\n\n    return output_images\n\nprompt_text = \"\"\"\n{\n    \"3\": {\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"cfg\": 8,\n            \"denoise\": 1,\n            \"latent_image\": [\n                \"5\",\n                0\n            ],\n            \"model\": [\n                \"4\",\n                0\n            ],\n            \"negative\": [\n                \"7\",\n                0\n            ],\n            \"positive\": [\n                \"6\",\n                0\n            ],\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"normal\",\n            \"seed\": 8566257,\n            \"steps\": 20\n        }\n    },\n    \"4\": {\n        \"class_type\": \"CheckpointLoaderSimple\",\n        \"inputs\": {\n            \"ckpt_name\": \"v1-5-pruned-emaonly.ckpt\"\n        }\n    },\n    \"5\": {\n        \"class_type\": \"EmptyLatentImage\",\n        \"inputs\": {\n            \"batch_size\": 1,\n            \"height\": 512,\n            \"width\": 512\n        }\n    },\n    \"6\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"masterpiece best quality girl\"\n        }\n    },\n    \"7\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"bad hands\"\n        }\n    },\n    \"8\": {\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\n                \"3\",\n                0\n            ],\n            \"vae\": [\n                \"4\",\n                2\n            ]\n        }\n    },\n    \"save_image_websocket_node\": {\n        \"class_type\": \"SaveImageWebsocket\",\n        \"inputs\": {\n            \"images\": [\n                \"8\",\n                0\n            ]\n        }\n    }\n}\n\"\"\"\n\nprompt = json.loads(prompt_text)\n#set the text prompt for our positive CLIPTextEncode\nprompt[\"6\"][\"inputs\"][\"text\"] = \"masterpiece best quality man\"\n\n#set the seed for our KSampler node\nprompt[\"3\"][\"inputs\"][\"seed\"] = 5\n\nws = websocket.WebSocket()\nws.connect(\"ws://{}/ws?clientId={}\".format(server_address, client_id))\nimages = get_images(ws, prompt)\n\n#Commented out code to display the output images:\n\n# for node_id in images:\n#     for image_data in images[node_id]:\n#         from PIL import Image\n#         import io\n#         image = Image.open(io.BytesIO(image_data))\n#         image.show()\n\n", "script_examples/basic_api_example.py": "import json\nfrom urllib import request, parse\nimport random\n\n#This is the ComfyUI api prompt format.\n\n#If you want it for a specific workflow you can \"enable dev mode options\"\n#in the settings of the UI (gear beside the \"Queue Size: \") this will enable\n#a button on the UI to save workflows in api format.\n\n#keep in mind ComfyUI is pre alpha software so this format will change a bit.\n\n#this is the one for the default workflow\nprompt_text = \"\"\"\n{\n    \"3\": {\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"cfg\": 8,\n            \"denoise\": 1,\n            \"latent_image\": [\n                \"5\",\n                0\n            ],\n            \"model\": [\n                \"4\",\n                0\n            ],\n            \"negative\": [\n                \"7\",\n                0\n            ],\n            \"positive\": [\n                \"6\",\n                0\n            ],\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"normal\",\n            \"seed\": 8566257,\n            \"steps\": 20\n        }\n    },\n    \"4\": {\n        \"class_type\": \"CheckpointLoaderSimple\",\n        \"inputs\": {\n            \"ckpt_name\": \"v1-5-pruned-emaonly.ckpt\"\n        }\n    },\n    \"5\": {\n        \"class_type\": \"EmptyLatentImage\",\n        \"inputs\": {\n            \"batch_size\": 1,\n            \"height\": 512,\n            \"width\": 512\n        }\n    },\n    \"6\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"masterpiece best quality girl\"\n        }\n    },\n    \"7\": {\n        \"class_type\": \"CLIPTextEncode\",\n        \"inputs\": {\n            \"clip\": [\n                \"4\",\n                1\n            ],\n            \"text\": \"bad hands\"\n        }\n    },\n    \"8\": {\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\n                \"3\",\n                0\n            ],\n            \"vae\": [\n                \"4\",\n                2\n            ]\n        }\n    },\n    \"9\": {\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"filename_prefix\": \"ComfyUI\",\n            \"images\": [\n                \"8\",\n                0\n            ]\n        }\n    }\n}\n\"\"\"\n\ndef queue_prompt(prompt):\n    p = {\"prompt\": prompt}\n    data = json.dumps(p).encode('utf-8')\n    req =  request.Request(\"http://127.0.0.1:8188/prompt\", data=data)\n    request.urlopen(req)\n\n\nprompt = json.loads(prompt_text)\n#set the text prompt for our positive CLIPTextEncode\nprompt[\"6\"][\"inputs\"][\"text\"] = \"masterpiece best quality man\"\n\n#set the seed for our KSampler node\nprompt[\"3\"][\"inputs\"][\"seed\"] = 5\n\n\nqueue_prompt(prompt)\n\n\n"}